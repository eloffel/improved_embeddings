   #[1]rubik's code    feed [2]rubik's code    comments feed [3]rubik's code
      introduction to convolutional neural networks comments feed
   [4]alternate [5]alternate

   [6]skip to content

     * [7]linkedin
     * [8]twitter
     * [9]git
     * [10]fb

   [11]rubik's code

   machine learning without tears

     * products
          + [12]introducing test driven development in c#     video course
          + [13]test driven development with c# and .net core mvc     video
            course
          + [14]real-world machine learning projects with sci-kit learn
     * [15]articles
     * [16]series
          + [17]id158s series
               o [18]introduction to id158s
               o [19]common neural network id180
               o [20]how do id158s learn?
               o [21]id26 algorithm in artificial neural
                 networks
               o [22]implementing simple neural network in c#
               o [23]introduction to tensorflow     with python example
               o [24]implementing simple neural network using keras     with
                 python example
               o [25]introduction to convolutional neural networks
               o [26]implementation of convolutional neural network using
                 python and keras
               o [27]introduction to recurrent neural networks
               o [28]understanding id137 (lstms)
               o [29]two ways to implement lstm network using python    
                 with tensorflow and keras
          + [30]gan series
               o [31]introduction to id3
                 (gans)
               o [32]implementing gan & dcgan with python
               o [33]introduction to adversarial autoencoders
               o [34]generating images using adversarial autoencoders and
                 python
               o [35]implementing cyclegan using python
               o [36]introduction to cyclegan
          + [37]machine learning with ml.net
               o [38]ultimate guide to machine learning with ml.net
               o [39]using ml.net     introduction to machine learning and
                 ml.net
               o [40]machine learning with ml.net     solving real-world
                 regression problem (bike sharing demands)
               o [41]machine learning with ml.net     solving real-world
                 classification problem (wine quality)
               o [42]machine learning in ml.net     using machine learning
                 model in asp.net core application
               o [43]machine learning with ml.net     comparing data
                 exploration in python with data exploration in ml.net
          + [44]asynchronous programming in .net
               o [45]motivation and unit testing
               o [46]common mistakes and best practices
               o [47]task-based asynchronous pattern (tap)
               o [48]benefits and tradeoffs of using valuetask
          + [49]self- organizing maps series
               o [50]introduction to self-organizing maps
               o [51]implementing self-organizing maps with python and
                 tensorflow
               o [52]implementing self-organizing maps with .net core
               o [53]credit card fraud detection using self-organizing
                 maps and python
          + [54]restricted id82 series
               o [55]introduction to restricted id82s
               o [56]implementing restricted id82 with python
                 and tensorflow
               o [57]implementing restricted id82 with .net
                 core
               o [58]generate music using tensorflow and python
          + [59]mongodb series
               o [60]introduction to nosql and polyglot persistence
               o [61]mongodb basics     part 1.
               o [62]mongo db basics     part 2.
               o [63]using mongodb in c#
               o [64]mean stack crash course     using mongodb with node.js,
                 express and angular 4
               o [65]serverless and playing around with mongodb atlas
          + [66]philosophy as motivational tool for software crafters
            series
               o [67]how to use miyamoto musashi   s philosophy to become
                 better software crafter
               o [68]how to use marcus aurelius   s meditations to become
                 better software crafter
               o [69]   how to use friedrich nietzsche   s philosophy to be
                 better software crafter
               o [70]how to use    art of war    to be better software crafter
     * categories
          + [71]ai
          + [72]machine learning
          + [73].net
          + [74]python
          + [75]nosql
     * [76]about
     * [77]contact

   (button) open a search box close a search box search for:
   ____________________ (button) search

introduction to convolutional neural networks

   [78]february 26, 2018 by [79]rubikscode [80]9 comments

   have you ever wondered how facebook knows how to suggest the right
   friend to tag? speaking of it, how does the google   s image search
   algorithm work? yes, you are right, there is a neural network involved
   in all those tasks. to be more precise, we are talking about
   convolutional neural networks. even though it sounds like a weird
   mixture of biology and computer science (everything related to
   neural networks kinda sound like that) this is one very effective
   mechanism used for image recognition. of course, it is motivated by
   biological systems and the ways the brain works, specifically visual
   cortex.

   [x3rozxj.jpg?w=720&#038;ssl=1]

biology behind the idea

   individual neurons in this section of the brain respond to stimuli only
   in a restricted region of the visual field known as the receptive
   field. because these fields of different neurons overlap, together they
   make the entire visual field. this effectively means that certain
   neurons were activated only if there is a certain attribute in the
   visual field, for example, horizontal edge. so, different neurons will
   be    fired up    if there is a horizontal edge in your visual field, and
   different neurons will be activated if there is, let   s say a vertical
   edge in our visual field. in a nutshell, if there is a certain feature
   in our visual field, specific neurons will be activated and other
   won   t. this was proven by a fascinating experiment done by hubel and
   wiesel in 1962. results of this experiment can be checked out
   in [81]this video.

   so, how are convolutional neural networks using this for image
   recognition? well, they use this idea to differentiate between given
   images and figure out the unique features that make a plane a plane or
   a snake     a snake. this process is happening in our minds
   subconsciously. for example, when we take a look at the picture of a
   plane, we can identify it as a plane by distinguishing features like
   two wings, tale, windows, etc. convolutional neural networks do the
   same thing, but they are first detecting lower level features like
   curves and edges and then they build it up to more abstract concepts.

structure of convolutional neural networks

   in order to achieve the functionality we talked about, convolutional
   neural network processes image through several layers. we will examine
   them in detail in next chapters of this article, but for now, let   s
   just do an overview of them and their purposes:
     * convolutional layer     used to detect features
     * non-linearity layer     introducing non-linearity to the system
     * pooling (downsampling) layer     reduces the number of weights and
       controls overfitting
     * flattening layer     prepares data for classical neural network
     * fully-connected layer     standard neural network used for
       classification

   [tnkq3tf.png?w=720&#038;ssl=1]

   basically, in the end, convolutional neural network uses standard
   neural network for solving classification problem, but it uses other
   layers to prepare data and detect certain features before that. let   s
   dive into details of each layer and their functionalities.

convolutional layer

   this is the main building block of convolutional neural networks. it is
   doing the heavy lifting without which the rest of the activities would
   be impossible. as mentioned already, it is in charge of detecting
   features. this is done by applying a certain filter to the image that
   will extract some of the low level and later higher level of attributes
   on some image. for example, we can use a filter that will detect edges.
   a filter is usually a multi-dimensional array of pixel values     for
   example, 5x5x3. numbers 5 are used to present height and width in
   pixels, while number 3 is used to present depth because images have
   three color channels. so, how do we apply the filter? take a look at
   this image:

   [apffkok.png?w=720&#038;ssl=1]

   we are considering just one channel of this image for the demonstration
   purposes. it is the 5  5 image where each pixel has value 1 or 0. we
   will use the 3  3 filter that is simple and it looks like this:

   [mp9t3pf.png?w=720&#038;ssl=1]

   now take a look how this process of applying the filter, also known as
   convolution, is done:

   [convolution_schematic.gif?w=720]

convolution process     [82]source

   firstly, we position the filter in the first location of the image, top
   left corner. there we use element-wise multiplication between two
   matrixes and store the result to the output matrix. then we move that
   filter to the right by 1 pixel (also known as    stride   ) and repeat the
   process. we do that as long as we can move our filter to the right.
   when we cannot do that anymore, we go to the next row and apply the
   filter in the same way. we do this until our output matrix is not
   complete. the reason our output matrix is 3  3 is that there are 9
   positions in which you can fit the 3  3 feature in the 5  5 image. these
   9 numbers are mapped to the 3  3 matrix.

   what is our output matrix telling to us? this matrix is often called
   feature map. it indicates where the feature, represented by the filter,
   is located in the image. in a nutshell, by moving the filter through
   the image and using simple id127, we detect our
   features. usually, we use more than one filter and detect multiple
   features, which means that we have more than one convolutional layer in
   the network. take a look at the animation below, where this process is
   a little bit more visual:

   [giphy.gif?w=720]

convolution process     [83]source

   when we apply the first filter we are creating one feature map and
   detect one kind of feature. then we use the second filter to create a
   second feature map that detects another kind of feature. these filters
   can be simple as we could see in the example, but they can get quite
   complicated if we want to extract some complex features from the image.
   take a look at the image below, where multiple filters are represented.

   [brsnwcq.png?w=720&#038;ssl=1]

filters     [84]source

   another thing that we already mentioned, but didn   t explain in detail
   is stride. this term is usually used in combination with the term
   padding. stride controls how the filter is convolved around the input
   image. in the example above, stride was 1 pixel, but it can be larger.
   this affects the size of our output, of our feature map. at the early
   stages of the convolution neural network, when we are applying our
   first layers we want to preserve as much information as possible for
   other convolutional layers. that is why padding is used. you may notice
   that feature map is smaller than the original input image. padding
   would add zero values to this map to preserve the size, like on the
   image below:

   [xzusmeu.png?w=720&#038;ssl=1]

padding example

non-linearity

   after every convolutional layer, we usually have a non-linearity layer.
   why is linearity in the image a problem? the problem is that our neural
   network would behave just like a single perception, because the sum of
   all the layers would still be a linear function, meaning the output
   could be calculated as the linear combination of the outputs. this
   layer is also called the activation layer because we use one of
   the [85]id180. in the past, nonlinear functions like
   sigmoid and tahn were used, but it turned out that the function that
   gives the best results when it comes to the speed of training of the
   neural network is [86]rectifier function. so, this layer is often relu
   layer, that removes linearity by setting values that are below 0 to 0
   since rectifier function is described with the     f(x) = max(0, x). here
   is how it looks  once applied to one of the feature maps:

   [aqvnr2y.png?resize=720%2c232&#038;ssl=1]

   on the second image, the feature map, black values are negative ones,
   and after we apply the rectifier function, black ones are removed from
   the image.

pooling layer

   this layer is commonly inserted between successive convolutional layers
   in convolutional neural networks. the functionality of this layer is to
   reduce the spatial size of the representation and with, that the number
   of parameters and computation in the network. this way we are also
   controlling over-fitting in our network. there are different pooling
   operations, but the most popular one is called max pooling and we will
   examine it in this article. the other pooling algorithms, like average
   pooling, or l2-norm pooling, work on the same principle, one which we
   will examine shortly.

   here we will have a sort of a filter once again. take a look at the
   picture below. we used the maxpooling filter size 2  2 on the 4  4 image.
   as you already guessed, filter picks the largest number of the part of
   the image it covers. this way we end up with smaller representations
   that contain enough information for our neural network to make correct
   decisions.

   [pavd2ok.png?w=720&#038;ssl=1]

maxpooling process

   however, a lot of people is against pooling layers, and replace them
   with additional convolutional layers with a larger stride. also, newer
   generative models, such as id5 (vaes) or
   id3 (gans), discard pooling layers
   completely. it seems that this layer will soon be obsolete.

flattening layer

   this is a simple layer that is used to prepare data to be the input of
   the final and most important layer     fully-connected layer (which is
   our neural network). since in general, neural networks receive data in
   one dimension in a form of an array of values, this layer uses data
   passed from pooling layer or convolutional layer and squashed the
   matrixes into arrays. then, these values are used as an input to the
   neural network. here is the visual representation of the flattening
   process:

   [ddyphpb.png?resize=461%2c380&#038;ssl=1]

flattening data


fully-connected layer

   the final layer and the layer that does the actual classification is
   the so-called fully-connected layer. this layer takes input from the
   flattening process and feeds and forwards it through the neural
   network. what we have done basically, is that we fed this network with
   the detection of the features. if you need more information of how
   neural networks work you can check previous articles from [87]this
   series.

architectures of convolutional neural networks

   a common way of building convolutional neural networks is to stack a
   few convolutional layers and after each of them add relu layer. after
   that, they are followed by pool layers and the flattening layer.
   finally, several fully-connected layers along with additional relu
   layers are added. keep in mind that in the end there should always be
   fully-connected layers, which looks something like this:

input image -> [[conv -> relu]*n -> pool?]*m -> flattening -> [fc -> relu]*k
-> fc

   some of the architectures in the field of convolutional networks are
   quite famous and have a name:
     * [88]lenet     this was the first successful application of
       convolutional networks. it was developed by yann lecun in 1990   s
       and it was used to read zip codes, simple digits, etc.
     * [89]alexnet     this was the network that was presented in
       the id163 ilsvrc challenge back in 2012. it is actually the
       network that popularized the convolutional networks since it
       outperformed all other contestants by far. it was developed
       by developed by alex krizhevsky, ilya sutskever, and geoff hinton.
     * [90]googlenet     the winner of the ilsvrc 2014 winner was a
       convolutional network from google. they used average pooling layers
       to dramatically minimize the number of parameters in the network.
       there are several follow-up versions to the googlenet.
     * [91]vggnet     convolutional neural network from karen simonyan and
       andrew zisserman that became known as the vggnet. this network
       proved that depth of the network that is crucial for good
       performances. it has 16 convolutional layers.
     * [92]resnet     developed by kaiming he et al. was the winner of
       ilsvrc 2015. you can watch [93]this cool video in which topic is
       described in depth.

conclusion

   in this article, we covered quite the ground. we examined biological
   motivations behind this type of neural networks as well as the
   mechanisms they work on. there are a lot of topics in here that we just
   scratched the surface of, since the field is wide and somewhat
   complicated. still, it is a nice starting point for going deeper into
   the wide world of convolutional neural networks.

   thanks for reading!
     __________________________________________________________________

   this article is a part of  id158s series, which you
   can check out [94]here.
     __________________________________________________________________

   read more posts from the author at [95]rubik   s code.
     __________________________________________________________________

   [96]codeproject

share:

     * [97]click to share on twitter (opens in new window)
     * [98]click to share on facebook (opens in new window)
     *

like this:

   like loading...

   posted in: [99]ai
   tagged: [100]ai [101]artificaial inteligance [102]artificial neural
   networks [103]conventional neural networks [104]deep learning
   [105]machine learning [106]neural networks [107]software development

post navigation

   previous entry:[108]id158s series
   next entry:[109]implementation of convolutional neural network using
   python and keras

9 comments

    1.
   toni fasth says:
       [110]march 1, 2018 at 3:51 pm
       as usual, very well written article with enough visuals. great job!
       loading...
       [111]reply
         1.
        [112]rubikscode says:
            [113]march 2, 2018 at 6:59 am
            thank you toni, glad you liked it!
            loading...
            [114]reply
    2.
   jeff_t says:
       [115]march 7, 2018 at 3:32 am
       i   m a newbie and having a bit of a hard time following some things.
       can you explain how    there we use element-wise multiplication
       between two matrices and store the result to the output matrix.   
       works and why it is being done or give an example of this in
       another context?
       loading...
       [116]reply
         1.
        [117]rubikscode says:
            [118]march 7, 2018 at 8:36 am
            hi jeff,
            thank you for reading.
            it is a basic multiplication of two matrices. what we are
            doing here is taking every element of the first matrix and
            multiply it with the element of the second matrix in the same
            position. you can see example here    
            [119]https://www.mathworks.com/help/matlab/ref/times.html?s_ti
            d=gn_loc_drop
            by doing so, we are able to detect features.
            loading...
            [120]reply
    3. pingback: [121]introduction to recurrent neural networks    
       developparadise
    4. pingback: [122]implementation of convolutional neural network using
       python and keras     developparadise
    5.
   seductivecoder11 says:
       [123]may 15, 2018 at 3:13 am
       looking for many resources through the past few hours about neural
       network introductions and this is easily the best/most
       striaghtforward!
       loading...
       [124]reply
         1.
        rubikscode says:
            [125]may 15, 2018 at 6:33 am
            thank you! i am glad you liked it     
            loading...
            [126]reply
    6. pingback: [127]convolutional neural networks from the ground up    
       deep in thought

leave a reply [128]cancel reply

   iframe: [129]jetpack_remote_comment

   this site uses akismet to reduce spam. [130]learn how your comment data
   is processed.

subscribe to rubik's code via email

   email address ____________________

   (button) subscribe

video courses

     * introducing test driven development in c#
     * test driven development with c# and .net core mvc
     * real-world machine learning projects with scikit-learn

follow rubik   s code

     * [131]linkedin
     * [132]twitter
     * [133]github
     * [134]facebook

   close and accept
   privacy & cookies: this site uses cookies. by continuing to use this
   website, you agree to their use.
   to find out more, including how to control cookies, see here: [135]info

   (button) go to the top

products

     * [136]introducing test driven development in c#
     * [137]test driven development with c# and .net core mvc
     * [138]real-world machine learning projects with sci-kit learn

series

     * [139]id158s series
     * [140]machine learning with ml.net series
     * [141]asynchronous programming in .net series
     * [142]mongodb series
     * [143]philosophy as motivational tool for software crafters series

rubik   s code

     * [144]about
     * [145]contact

     * [146]linkedin
     * [147]twitter
     * [148]git
     * [149]fb

   iframe: [150]likes-master

   %d bloggers like this:

references

   visible links
   1. https://rubikscode.net/feed/
   2. https://rubikscode.net/comments/feed/
   3. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/feed/
   4. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
   5. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/&format=xml
   6. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#content
   7. https://www.linkedin.com/in/nmzivkovic/
   8. https://twitter.com/nmzivkovic?lang=en
   9. https://github.com/nmzivkovic
  10. https://www.facebook.com/rubikscodenet/
  11. https://rubikscode.net/
  12. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
  13. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
  14. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
  15. https://rubikscode.net/
  16. https://rubikscode.net/category/series/
  17. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  18. https://rubikscode.net/2017/11/13/introduction-to-artificial-neural-networks/
  19. https://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  20. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  21. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  22. https://rubikscode.net/2018/01/29/implementing-simple-neural-network-in-c/
  23. https://rubikscode.net/2018/02/05/introduction-to-tensorflow-with-python-example/
  24. https://rubikscode.net/2018/02/12/implementing-simple-neural-network-using-keras-with-python-example/
  25. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  26. https://rubikscode.net/2018/03/05/implementation-of-convolutional-neural-network-using-python-and-keras/
  27. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  28. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
  29. https://rubikscode.net/2018/03/26/two-ways-to-implement-lstm-network-using-python-with-tensorflow-and-keras/
  30. http://rtrtr.com/
  31. https://rubikscode.net/2018/12/10/introduction-to-generative-adversarial-networks-gans/
  32. https://rubikscode.net/2018/12/17/implementing-gan-dcgan-with-python/
  33. https://rubikscode.net/2019/01/14/introduction-to-adversarial-autoencoders/
  34. https://rubikscode.net/2019/01/21/generating-images-using-adversarial-autoencoders-and-python/
  35. https://rubikscode.net/2019/02/11/implementing-cyclegan-using-python/
  36. https://rubikscode.net/2019/02/04/introduction-to-cyclegan/
  37. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
  38. https://rubikscode.net/2019/02/18/ultimate-guide-to-machine-learning-with-ml-net/
  39. https://rubikscode.net/2018/06/18/using-ml-net-introduction-to-machine-learning-and-ml-net/
  40. https://rubikscode.net/2018/06/25/machine-learning-with-ml-net-solving-real-world-regression-problem-bike-sharing-demands/
  41. https://rubikscode.net/2018/07/02/machine-learning-with-ml-net-solving-real-world-classification-problem-wine-quality/
  42. https://rubikscode.net/2018/07/09/machine-learning-in-ml-net-using-machine-learning-model-in-asp-net-mvc-application/
  43. https://rubikscode.net/2018/07/16/machine-learning-with-ml-net-comparing-data-exploration-in-python-with-data-exploration-in-ml-net/
  44. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
  45. https://rubikscode.net/2018/05/21/asynchronous-programming-in-net-motivation-and-unit-testing/
  46. https://rubikscode.net/2018/05/28/asynchronous-programming-in-net-common-mistakes-and-best-practices/
  47. https://rubikscode.net/2018/06/04/asynchronous-programming-in-net-task-based-asynchronous-pattern-tap/
  48. https://rubikscode.net/2018/06/11/asynchronous-programming-in-net-benefits-and-tradeoffs-of-using-valuetask/
  49. https://rubikscode.net/2018/09/26/self-organizing-maps-series/
  50. https://rubikscode.net/2018/08/20/introduction-to-self-organizing-maps/
  51. https://rubikscode.net/2018/08/27/implementing-self-organizing-maps-with-python-and-tensorflow/
  52. https://rubikscode.net/2018/09/17/implementing-self-organizing-maps-with-net-core/
  53. https://rubikscode.net/2018/09/24/credit-card-fraud-detection-using-self-organizing-maps-and-python/
  54. https://rubikscode.net/2019/01/07/restricted-boltzmann-machine-series/
  55. https://rubikscode.net/2018/10/01/introduction-to-restricted-boltzmann-machines/
  56. https://rubikscode.net/2018/10/22/implementing-restricted-boltzmann-machine-with-python-and-tensorflow/
  57. https://rubikscode.net/2018/10/15/implementing-restricted-boltzmann-machine-with-net-core/
  58. https://rubikscode.net/2018/11/12/generate-music-using-tensorflow-and-python/
  59. https://rubikscode.net/2017/10/16/mongodb-serial/
  60. https://rubikscode.net/2017/07/19/introduction-to-nosql-and-polyglot-persistence/
  61. https://rubikscode.net/2017/07/24/mongo-db-basics-part-1/
  62. https://rubikscode.net/2017/07/31/mongo-db-basics-part-2/
  63. https://rubikscode.net/2017/10/02/using-mongodb-in-c/
  64. https://rubikscode.net/2017/10/09/mean-stack-crash-course-using-mongodb-with-node-js-express-and-angular-4/
  65. https://rubikscode.net/2017/10/15/serverless-and-mongodb-atlas/
  66. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
  67. https://rubikscode.net/2018/04/23/how-to-use-miyamoto-musashis-philosophy-to-become-better-software-crafter/
  68. https://rubikscode.net/2017/11/06/how-to-use-marcus-aureliuss-meditations-to-become-better-software-craftsman/
  69. https://rubikscode.net/2017/06/22/   how-to-use-friedrich-nietzsches-philosophy-to-be-better-software-craftsman/
  70. https://rubikscode.net/2017/05/14/how-to-use-art-of-war-to-be-better-software-craftsman/
  71. https://rubikscode.net/category/ai/
  72. https://rubikscode.net/category/machine-learning/
  73. https://rubikscode.net/category/net/
  74. https://rubikscode.net/category/python/
  75. https://rubikscode.net/category/nosql/
  76. https://rubikscode.net/about/
  77. https://rubikscode.net/contact/
  78. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  79. https://rubikscode.net/author/rubikscode/
  80. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#comments
  81. https://www.youtube.com/watch?v=cw5pkv9rj3o
  82. http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_using_convolution
  83. https://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/
  84. http://cs231n.stanford.edu/
  85. http://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  86. http://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  87. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  88. http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf
  89. https://en.wikipedia.org/wiki/alexnet
  90. https://arxiv.org/abs/1409.4842
  91. http://www.robots.ox.ac.uk/~vgg/research/very_deep/
  92. http://arxiv.org/abs/1512.03385
  93. https://www.youtube.com/watch?v=1pglj-ukt1w
  94. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  95. https://rubikscode.net/
  96. https://www.codeproject.com/
  97. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?share=twitter
  98. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?share=facebook
  99. https://rubikscode.net/category/ai/
 100. https://rubikscode.net/tag/ai/
 101. https://rubikscode.net/tag/artificaial-inteligance/
 102. https://rubikscode.net/tag/artificial-neural-networks/
 103. https://rubikscode.net/tag/conventional-neural-networks/
 104. https://rubikscode.net/tag/deep-learning/
 105. https://rubikscode.net/tag/machine-learning/
 106. https://rubikscode.net/tag/neural-networks/
 107. https://rubikscode.net/tag/software-development/
 108. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
 109. https://rubikscode.net/2018/03/05/implementation-of-convolutional-neural-network-using-python-and-keras/
 110. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#comment-598
 111. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?replytocom=598#respond
 112. http://rubikscode.wordpress.com/
 113. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#comment-599
 114. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?replytocom=599#respond
 115. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#comment-612
 116. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?replytocom=612#respond
 117. http://rubikscode.wordpress.com/
 118. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#comment-614
 119. https://www.mathworks.com/help/matlab/ref/times.html?s_tid=gn_loc_drop
 120. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?replytocom=614#respond
 121. http://devepar.com/archives/2166
 122. http://devepar.com/archives/3068
 123. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#comment-736
 124. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?replytocom=736#respond
 125. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#comment-737
 126. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/?replytocom=737#respond
 127. http://deepinthought.in/convolutional-neural-networks-from-the-ground-up
 128. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/#respond
 129. https://jetpack.wordpress.com/jetpack-comment/?blogid=128253944&postid=7989&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=identicon&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.2.1&show_cookie_consent=10&has_cookie_consent=0&sig=1c9b8ab0c45d1dbb299ec2078b3e5761fb2b62a6#parent=https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
 130. https://akismet.com/privacy/
 131. https://www.linkedin.com/in/nmzivkovic/
 132. https://twitter.com/nmzivkovic
 133. https://github.com/nmzivkovic
 134. https://www.facebook.com/rubikscodenet
 135. https://automattic.com/cookies/
 136. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 137. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 138. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
 139. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
 140. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
 141. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
 142. https://rubikscode.net/2017/10/16/mongodb-serial/
 143. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
 144. https://rubikscode.net/about/
 145. https://rubikscode.net/contact/
 146. https://www.linkedin.com/in/nmzivkovic/
 147. https://twitter.com/nmzivkovic?lang=en
 148. https://github.com/nmzivkovic
 149. https://www.facebook.com/rubikscodenet/
 150. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914

   hidden links:
 152. https://rubikscode.net/
 153. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 154. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 155. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
