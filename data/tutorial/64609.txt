   #[1]rubik's code    feed [2]rubik's code    comments feed [3]rubik's code
      understanding id137 (lstms) comments feed
   [4]alternate [5]alternate

   [6]skip to content

     * [7]linkedin
     * [8]twitter
     * [9]git
     * [10]fb

   [11]rubik's code

   machine learning without tears

     * products
          + [12]introducing test driven development in c#     video course
          + [13]test driven development with c# and .net core mvc     video
            course
          + [14]real-world machine learning projects with sci-kit learn
     * [15]articles
     * [16]series
          + [17]id158s series
               o [18]introduction to id158s
               o [19]common neural network id180
               o [20]how do id158s learn?
               o [21]id26 algorithm in artificial neural
                 networks
               o [22]implementing simple neural network in c#
               o [23]introduction to tensorflow     with python example
               o [24]implementing simple neural network using keras     with
                 python example
               o [25]introduction to convolutional neural networks
               o [26]implementation of convolutional neural network using
                 python and keras
               o [27]introduction to recurrent neural networks
               o [28]understanding id137 (lstms)
               o [29]two ways to implement lstm network using python    
                 with tensorflow and keras
          + [30]gan series
               o [31]introduction to id3
                 (gans)
               o [32]implementing gan & dcgan with python
               o [33]introduction to adversarial autoencoders
               o [34]generating images using adversarial autoencoders and
                 python
               o [35]implementing cyclegan using python
               o [36]introduction to cyclegan
          + [37]machine learning with ml.net
               o [38]ultimate guide to machine learning with ml.net
               o [39]using ml.net     introduction to machine learning and
                 ml.net
               o [40]machine learning with ml.net     solving real-world
                 regression problem (bike sharing demands)
               o [41]machine learning with ml.net     solving real-world
                 classification problem (wine quality)
               o [42]machine learning in ml.net     using machine learning
                 model in asp.net core application
               o [43]machine learning with ml.net     comparing data
                 exploration in python with data exploration in ml.net
          + [44]asynchronous programming in .net
               o [45]motivation and unit testing
               o [46]common mistakes and best practices
               o [47]task-based asynchronous pattern (tap)
               o [48]benefits and tradeoffs of using valuetask
          + [49]self- organizing maps series
               o [50]introduction to self-organizing maps
               o [51]implementing self-organizing maps with python and
                 tensorflow
               o [52]implementing self-organizing maps with .net core
               o [53]credit card fraud detection using self-organizing
                 maps and python
          + [54]restricted id82 series
               o [55]introduction to restricted id82s
               o [56]implementing restricted id82 with python
                 and tensorflow
               o [57]implementing restricted id82 with .net
                 core
               o [58]generate music using tensorflow and python
          + [59]mongodb series
               o [60]introduction to nosql and polyglot persistence
               o [61]mongodb basics     part 1.
               o [62]mongo db basics     part 2.
               o [63]using mongodb in c#
               o [64]mean stack crash course     using mongodb with node.js,
                 express and angular 4
               o [65]serverless and playing around with mongodb atlas
          + [66]philosophy as motivational tool for software crafters
            series
               o [67]how to use miyamoto musashi   s philosophy to become
                 better software crafter
               o [68]how to use marcus aurelius   s meditations to become
                 better software crafter
               o [69]   how to use friedrich nietzsche   s philosophy to be
                 better software crafter
               o [70]how to use    art of war    to be better software crafter
     * categories
          + [71]ai
          + [72]machine learning
          + [73].net
          + [74]python
          + [75]nosql
     * [76]about
     * [77]contact

   (button) open a search box close a search box search for:
   ____________________ (button) search

understanding id137 (lstms)

   [78]march 19, 2018 by [79]rubikscode [80]5 comments

   remember how in the [81]previous article we   ve said that we can predict
   text and make id103 work so well with recurrent neural
   networks? the truth is that all the big accomplishments that we
   assigned to id56s in the previous article are actually achieved using
   special kind of id56s     long short-terms memory units (lstms).

   this upgraded version of id56s solved some of the problems that these
   networks usually have. since the concept of id56s, like most of the
   concepts in this field, has been around for a while, scientists in the
   90s noticed some obstacles in using it. there are two major problems
   that standard id56s have: long-term dependencies problem and
   vanishing-exploding gradient problem.

   [muv9rft.png?w=720&#038;ssl=1]

long-term dependencies problem

   we   ve just mentioned the subject of long-term dependencies in
   the [82]previous article, so let   s get more into details here. one of
   the perks of using id56s is that we can connect previous information to
   the present task and based on that make certain predictions. for
   example, we can try to predict what will be the next word in the
   certain sentence based on the previous word from the sentence. and id56s
   are great for this kind of tasks when we need to look only at the
   previous information to perform a certain task. however, that is really
   the case.

   [8uxo5tl.png?w=720&#038;ssl=1]

   take that example of text prediction in id38. sometimes we
   can predict next word in the text based on the previous one, but more
   often we need more information than just the previous word. we need
   context, we need more words from the sentence. id56s are not good with
   this. the more previous states we need for our prediction the bigger
   problem this represents for standard id56.

vanishing-exploding gradient problem

   as you probably know gradient expresses the change in the weights with
   regards to change in the error. so, if we don   t know the gradient, we
   cannot update weights in a proper direction, and reduce the error. the
   problem that we face in every id158 is that if the
   gradient is vanishingly small,  it will prevent the weights from
   changing their value. for example, when you apply sigmoid function
   multiple times data gets flattened until it has no detectable slope.
   the data is flattened until, for large stretches, it has no detectable
   slope. the same thing happens to the gradient as it passes through many
   layers of a neural network.

   [pyurjp5.png?w=720&#038;ssl=1]

   in id56s this problem is bigger due to the fact that during
   the [83]learning process of recurrent neural networks, we update them
      trough time   . we use [84]id26 through time, and this
   process introduces even more multiplications and operations than
   regular id26. of course, this gradient problem can go the
   other way. the gradient can become progressively bigger and cause
   butterfly effect in the id56, so-called exploding gradient problem.

lstm architecture

   in order to solve obstacles that recurrent neural networks
   faces, [85]hochreiter & schmidhuber (1997) came up with the concept of
   id137. they are working very well on the
   large range of problems and are quite popular now. the structure is
   similar to the structure of standard id56, meaning they feed output
   information back to the input, but these networks don   t struggle with
   problems that standard id56s do. in their architecture, they have
   implemented a mechanism for remembering long dependencies.

   if we observe an unfolded representation of the standard id56 we
   consider it to be a chain of repeating structure. this structure is
   usually pretty simple, such as the single tahn layer.

   [twerhvg.png?w=720&#038;ssl=1]

   lstms even thou they follow the similar principle, have considerately
   more complicated architecture. instead of a single layer, they usually
   have four and they have more operations regarding those four levels.
   example of single lstm unit   s structure can be seen in the image below:

   [tfhlrb0.png?w=720&#038;ssl=1]

   here is the legend for elements from this graph:

   [qoa8ug3.png?w=720&#038;ssl=1]

   as you can see in the image we have four network layers. one layer is
   using the tahn function and two layers are using the sigmoid function.
   also, we are having some pointwise operations and regular operations on
   the vectors, like concatenation. we will explore this in more details
   in the next chapter.

   the important thing to notice however is the upper data stream marked
   with the letter c. this data stream is holding information outside the
   normal flow of the recurrent neural network and is known as the cell
   state. essentially data transferred using the cell state is acting much
   like data in computer   s memory because data can be stored in this cell
   state or read from it. lstm cell decides what information will be added
   to the cell states using analog mechanisms called gates.

   these gates are implemented using element-wise multiplications by
   sigmoids (range 0-1). they are similar to the neural network   s nodes.
   based on the strength input signal, they pass that information to the
   cell state or block it. for this purpose, they have their own set of
   weights that is maintained during the learning process. this means that
   lstm cells are trained to filter certain data and to preserve the
   error, that later can be used in id26. using this mechanism,
   lsmt networks are able to learn over many time steps.

lstms process

   lstm cells operate in few steps:
    1. forget unnecessary information from the cell state
    2. add information to the cell state
    3. calculate the output

   first sigmoid layer in lstm cell is called forget gate level. using
   this layer/gate we lstm cell is deciding how important is the previous
   state in the cell ct-1 is and we are deciding what will be removed.
   since we are using the sigmoid function, that ranges from 0 to 1, so
   data can be completely removed, partially removed, or completely
   preserved. this decision is made by looking at previous state ht-1 and
   current input xt. using this information and sigmoid level, lstm cell
   generates a number between  0 and  1 for each number in the cell
   state ct   1:

   [1jx8wkf.png?w=720&#038;ssl=1]

   when unnecessary information is removed, lstm cell decides which data
   is going to be added to the cell state. this is done by the combination
   of second sigmoid and tahn levels. firstly, second sigmoid layer, also
   known as input gate layer, decides which values in the cell state will
   be updated (output     i) and then tahn layer creates a vector of new
   candidate values that could be added to the cell state (outputs ~ct).

   [j2cljvi.png?w=720&#038;ssl=1]

   effectively, using this information the new value of the cell state is
   calculated     ct.

   [cc5et08.png?w=720&#038;ssl=1]

   the last step we need to do is to calculate the output of lstm cell.
   that is done using third sigmoid level and additional tahn filter. the
   output value is based on values in the cell state, but it is also
   filtered by sigmoid layer. basically, sigmoid layer decides which parts
   of the cell state are going to affect the output value. then we put
   cell state value through the tahn filter (pushes all values between -1
   and 1) and multiply it by the output of the third sigmoid level.

   [pnegk6u.png?w=720&#038;ssl=1]

conclusion

   this upgraded version of id56 is actually what made this type of
   networks so popular. they achieve tremendous results because of their
   ability to preserve data through time. the only shortcoming of these
   networks is that training is quite time-consuming, and require a lot of
   resources. as you can imagine, training this complex structure cannot
   be cheap. apart from that, they are one of the best choices when it
   comes to problems that require processing sequences of data.
   thanks for reading!
     __________________________________________________________________

   this article is a part of  [86]id158s series.
     __________________________________________________________________

   read more posts from the author at [87]rubik   s code.
     __________________________________________________________________

   [88]codeproject

share:

     * [89]click to share on twitter (opens in new window)
     * [90]click to share on facebook (opens in new window)
     *

like this:

   like loading...

   posted in: [91]ai
   tagged: [92]ai [93]artificaial inteligance [94]artificial neural
   networks [95]deep learning [96]long short term memory netwroks [97]long
   short term memory units [98]lstm [99]lstms [100]machine learning
   [101]neural networks [102]recurrent neural networks [103]software
   development

post navigation

   previous entry:[104]introduction to recurrent neural networks
   next entry:[105]two ways to implement lstm network using python     with
   tensorflow and keras

5 comments

    1. pingback: [106]understanding id137
       (lstms)     collective intelligence
    2.
   adrian says:
       [107]march 20, 2018 at 2:26 pm
       excellent, thanks.
       loading...
       [108]reply
    3. pingback: [109]two ways to implement lstm network using python    
       with tensorflow and keras     developparadise
    4. pingback: [110]understanding lstm     deep in thought
    5. pingback: [111]id158s series     deep in thought

leave a reply [112]cancel reply

   iframe: [113]jetpack_remote_comment

   this site uses akismet to reduce spam. [114]learn how your comment data
   is processed.

subscribe to rubik's code via email

   email address ____________________

   (button) subscribe

video courses

     * introducing test driven development in c#
     * test driven development with c# and .net core mvc
     * real-world machine learning projects with scikit-learn

follow rubik   s code

     * [115]linkedin
     * [116]twitter
     * [117]github
     * [118]facebook

   close and accept
   privacy & cookies: this site uses cookies. by continuing to use this
   website, you agree to their use.
   to find out more, including how to control cookies, see here: [119]info

   (button) go to the top

products

     * [120]introducing test driven development in c#
     * [121]test driven development with c# and .net core mvc
     * [122]real-world machine learning projects with sci-kit learn

series

     * [123]id158s series
     * [124]machine learning with ml.net series
     * [125]asynchronous programming in .net series
     * [126]mongodb series
     * [127]philosophy as motivational tool for software crafters series

rubik   s code

     * [128]about
     * [129]contact

     * [130]linkedin
     * [131]twitter
     * [132]git
     * [133]fb

   iframe: [134]likes-master

   %d bloggers like this:

references

   visible links
   1. https://rubikscode.net/feed/
   2. https://rubikscode.net/comments/feed/
   3. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/feed/
   4. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
   5. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/&format=xml
   6. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/#content
   7. https://www.linkedin.com/in/nmzivkovic/
   8. https://twitter.com/nmzivkovic?lang=en
   9. https://github.com/nmzivkovic
  10. https://www.facebook.com/rubikscodenet/
  11. https://rubikscode.net/
  12. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
  13. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
  14. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
  15. https://rubikscode.net/
  16. https://rubikscode.net/category/series/
  17. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  18. https://rubikscode.net/2017/11/13/introduction-to-artificial-neural-networks/
  19. https://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  20. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  21. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  22. https://rubikscode.net/2018/01/29/implementing-simple-neural-network-in-c/
  23. https://rubikscode.net/2018/02/05/introduction-to-tensorflow-with-python-example/
  24. https://rubikscode.net/2018/02/12/implementing-simple-neural-network-using-keras-with-python-example/
  25. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  26. https://rubikscode.net/2018/03/05/implementation-of-convolutional-neural-network-using-python-and-keras/
  27. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  28. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
  29. https://rubikscode.net/2018/03/26/two-ways-to-implement-lstm-network-using-python-with-tensorflow-and-keras/
  30. http://rtrtr.com/
  31. https://rubikscode.net/2018/12/10/introduction-to-generative-adversarial-networks-gans/
  32. https://rubikscode.net/2018/12/17/implementing-gan-dcgan-with-python/
  33. https://rubikscode.net/2019/01/14/introduction-to-adversarial-autoencoders/
  34. https://rubikscode.net/2019/01/21/generating-images-using-adversarial-autoencoders-and-python/
  35. https://rubikscode.net/2019/02/11/implementing-cyclegan-using-python/
  36. https://rubikscode.net/2019/02/04/introduction-to-cyclegan/
  37. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
  38. https://rubikscode.net/2019/02/18/ultimate-guide-to-machine-learning-with-ml-net/
  39. https://rubikscode.net/2018/06/18/using-ml-net-introduction-to-machine-learning-and-ml-net/
  40. https://rubikscode.net/2018/06/25/machine-learning-with-ml-net-solving-real-world-regression-problem-bike-sharing-demands/
  41. https://rubikscode.net/2018/07/02/machine-learning-with-ml-net-solving-real-world-classification-problem-wine-quality/
  42. https://rubikscode.net/2018/07/09/machine-learning-in-ml-net-using-machine-learning-model-in-asp-net-mvc-application/
  43. https://rubikscode.net/2018/07/16/machine-learning-with-ml-net-comparing-data-exploration-in-python-with-data-exploration-in-ml-net/
  44. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
  45. https://rubikscode.net/2018/05/21/asynchronous-programming-in-net-motivation-and-unit-testing/
  46. https://rubikscode.net/2018/05/28/asynchronous-programming-in-net-common-mistakes-and-best-practices/
  47. https://rubikscode.net/2018/06/04/asynchronous-programming-in-net-task-based-asynchronous-pattern-tap/
  48. https://rubikscode.net/2018/06/11/asynchronous-programming-in-net-benefits-and-tradeoffs-of-using-valuetask/
  49. https://rubikscode.net/2018/09/26/self-organizing-maps-series/
  50. https://rubikscode.net/2018/08/20/introduction-to-self-organizing-maps/
  51. https://rubikscode.net/2018/08/27/implementing-self-organizing-maps-with-python-and-tensorflow/
  52. https://rubikscode.net/2018/09/17/implementing-self-organizing-maps-with-net-core/
  53. https://rubikscode.net/2018/09/24/credit-card-fraud-detection-using-self-organizing-maps-and-python/
  54. https://rubikscode.net/2019/01/07/restricted-boltzmann-machine-series/
  55. https://rubikscode.net/2018/10/01/introduction-to-restricted-boltzmann-machines/
  56. https://rubikscode.net/2018/10/22/implementing-restricted-boltzmann-machine-with-python-and-tensorflow/
  57. https://rubikscode.net/2018/10/15/implementing-restricted-boltzmann-machine-with-net-core/
  58. https://rubikscode.net/2018/11/12/generate-music-using-tensorflow-and-python/
  59. https://rubikscode.net/2017/10/16/mongodb-serial/
  60. https://rubikscode.net/2017/07/19/introduction-to-nosql-and-polyglot-persistence/
  61. https://rubikscode.net/2017/07/24/mongo-db-basics-part-1/
  62. https://rubikscode.net/2017/07/31/mongo-db-basics-part-2/
  63. https://rubikscode.net/2017/10/02/using-mongodb-in-c/
  64. https://rubikscode.net/2017/10/09/mean-stack-crash-course-using-mongodb-with-node-js-express-and-angular-4/
  65. https://rubikscode.net/2017/10/15/serverless-and-mongodb-atlas/
  66. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
  67. https://rubikscode.net/2018/04/23/how-to-use-miyamoto-musashis-philosophy-to-become-better-software-crafter/
  68. https://rubikscode.net/2017/11/06/how-to-use-marcus-aureliuss-meditations-to-become-better-software-craftsman/
  69. https://rubikscode.net/2017/06/22/   how-to-use-friedrich-nietzsches-philosophy-to-be-better-software-craftsman/
  70. https://rubikscode.net/2017/05/14/how-to-use-art-of-war-to-be-better-software-craftsman/
  71. https://rubikscode.net/category/ai/
  72. https://rubikscode.net/category/machine-learning/
  73. https://rubikscode.net/category/net/
  74. https://rubikscode.net/category/python/
  75. https://rubikscode.net/category/nosql/
  76. https://rubikscode.net/about/
  77. https://rubikscode.net/contact/
  78. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
  79. https://rubikscode.net/author/rubikscode/
  80. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/#comments
  81. http://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  82. http://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  83. http://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  84. http://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  85. http://www.bioinf.jku.at/publications/older/2604.pdf
  86. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  87. https://rubikscode.net/
  88. https://www.codeproject.com/
  89. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/?share=twitter
  90. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/?share=facebook
  91. https://rubikscode.net/category/ai/
  92. https://rubikscode.net/tag/ai/
  93. https://rubikscode.net/tag/artificaial-inteligance/
  94. https://rubikscode.net/tag/artificial-neural-networks/
  95. https://rubikscode.net/tag/deep-learning/
  96. https://rubikscode.net/tag/long-short-term-memory-netwroks/
  97. https://rubikscode.net/tag/long-short-term-memory-units/
  98. https://rubikscode.net/tag/lstm/
  99. https://rubikscode.net/tag/lstms/
 100. https://rubikscode.net/tag/machine-learning/
 101. https://rubikscode.net/tag/neural-networks/
 102. https://rubikscode.net/tag/recurrent-neural-networks/
 103. https://rubikscode.net/tag/software-development/
 104. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
 105. https://rubikscode.net/2018/03/26/two-ways-to-implement-lstm-network-using-python-with-tensorflow-and-keras/
 106. https://collectiv3intelligenc3.wordpress.com/2018/03/20/understanding-long-short-term-memory-networks-lstms/
 107. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/#comment-638
 108. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/?replytocom=638#respond
 109. http://devepar.com/archives/1643
 110. http://deepinthought.in/understanding-lstm
 111. http://deepinthought.in/artificial-neural-networks-series
 112. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/#respond
 113. https://jetpack.wordpress.com/jetpack-comment/?blogid=128253944&postid=8043&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=identicon&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.2.1&show_cookie_consent=10&has_cookie_consent=0&sig=1d0d38c66fb8dd2bd63941abb994fb50b67f647a#parent=https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
 114. https://akismet.com/privacy/
 115. https://www.linkedin.com/in/nmzivkovic/
 116. https://twitter.com/nmzivkovic
 117. https://github.com/nmzivkovic
 118. https://www.facebook.com/rubikscodenet
 119. https://automattic.com/cookies/
 120. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 121. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 122. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
 123. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
 124. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
 125. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
 126. https://rubikscode.net/2017/10/16/mongodb-serial/
 127. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
 128. https://rubikscode.net/about/
 129. https://rubikscode.net/contact/
 130. https://www.linkedin.com/in/nmzivkovic/
 131. https://twitter.com/nmzivkovic?lang=en
 132. https://github.com/nmzivkovic
 133. https://www.facebook.com/rubikscodenet/
 134. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914

   hidden links:
 136. https://rubikscode.net/
 137. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 138. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 139. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
