   #[1]rubik's code    feed [2]rubik's code    comments feed [3]rubik's code
      how do id158s learn? comments feed [4]alternate
   [5]alternate

   [6]skip to content

     * [7]linkedin
     * [8]twitter
     * [9]git
     * [10]fb

   [11]rubik's code

   machine learning without tears

     * products
          + [12]introducing test driven development in c#     video course
          + [13]test driven development with c# and .net core mvc     video
            course
          + [14]real-world machine learning projects with sci-kit learn
     * [15]articles
     * [16]series
          + [17]id158s series
               o [18]introduction to id158s
               o [19]common neural network id180
               o [20]how do id158s learn?
               o [21]id26 algorithm in artificial neural
                 networks
               o [22]implementing simple neural network in c#
               o [23]introduction to tensorflow     with python example
               o [24]implementing simple neural network using keras     with
                 python example
               o [25]introduction to convolutional neural networks
               o [26]implementation of convolutional neural network using
                 python and keras
               o [27]introduction to recurrent neural networks
               o [28]understanding id137 (lstms)
               o [29]two ways to implement lstm network using python    
                 with tensorflow and keras
          + [30]gan series
               o [31]introduction to id3
                 (gans)
               o [32]implementing gan & dcgan with python
               o [33]introduction to adversarial autoencoders
               o [34]generating images using adversarial autoencoders and
                 python
               o [35]implementing cyclegan using python
               o [36]introduction to cyclegan
          + [37]machine learning with ml.net
               o [38]ultimate guide to machine learning with ml.net
               o [39]using ml.net     introduction to machine learning and
                 ml.net
               o [40]machine learning with ml.net     solving real-world
                 regression problem (bike sharing demands)
               o [41]machine learning with ml.net     solving real-world
                 classification problem (wine quality)
               o [42]machine learning in ml.net     using machine learning
                 model in asp.net core application
               o [43]machine learning with ml.net     comparing data
                 exploration in python with data exploration in ml.net
          + [44]asynchronous programming in .net
               o [45]motivation and unit testing
               o [46]common mistakes and best practices
               o [47]task-based asynchronous pattern (tap)
               o [48]benefits and tradeoffs of using valuetask
          + [49]self- organizing maps series
               o [50]introduction to self-organizing maps
               o [51]implementing self-organizing maps with python and
                 tensorflow
               o [52]implementing self-organizing maps with .net core
               o [53]credit card fraud detection using self-organizing
                 maps and python
          + [54]restricted id82 series
               o [55]introduction to restricted id82s
               o [56]implementing restricted id82 with python
                 and tensorflow
               o [57]implementing restricted id82 with .net
                 core
               o [58]generate music using tensorflow and python
          + [59]mongodb series
               o [60]introduction to nosql and polyglot persistence
               o [61]mongodb basics     part 1.
               o [62]mongo db basics     part 2.
               o [63]using mongodb in c#
               o [64]mean stack crash course     using mongodb with node.js,
                 express and angular 4
               o [65]serverless and playing around with mongodb atlas
          + [66]philosophy as motivational tool for software crafters
            series
               o [67]how to use miyamoto musashi   s philosophy to become
                 better software crafter
               o [68]how to use marcus aurelius   s meditations to become
                 better software crafter
               o [69]   how to use friedrich nietzsche   s philosophy to be
                 better software crafter
               o [70]how to use    art of war    to be better software crafter
     * categories
          + [71]ai
          + [72]machine learning
          + [73].net
          + [74]python
          + [75]nosql
     * [76]about
     * [77]contact

   (button) open a search box close a search box search for:
   ____________________ (button) search

how do id158s learn?

   [78]january 15, 2018february 26, 2018 by [79]rubikscode [80]2 comments
     __________________________________________________________________

   this article is a part of  id158s serial, which you
   can check out [81]here.

   in the previous blog posts, we covered some very interesting topics
   regarding id158s (ann). the basic structure of
   id158s was presented, as well as some of the most
   commonly used id180. nevertheless, we still haven   t
   mentioned the most important aspect of the id158s    
   learning. the biggest power of these systems is that they can be
   familiarized with some kind of problem in the process of training and
   are later able to solve problems of the same class     just like humans
   do! before we dive into that exciting topic let   s have a quick recap of
   some of the most important components of id158s and
   its architecture.

   the smallest and most important unit of the id158
   is the neuron. as in biological neural systems, these neurons are
   connected with each other and together they have the great processing
   power. in general, anns try to replicate the behavior and processes of
   the real brain, and that is why their architecture is modeled based on
   biological observations. the same is with the artificial neuron. it   s
   structure reminiscent of the structure of the real neuron.

   [i2wpfnw.png?w=720&#038;ssl=1]

   every neuron has input connections and output connections.
   these connections simulate the behavior of the synapses in the brain.
   the same way that synapses in the brain transfer the signal from one
   neuron to another, connections pass information between artificial
   neurons. these connections have weights, meaning that value that is
   sent to every connection is multiplied by this factor. again, this is
   inspired by brain synapses, and weights actually simulate the number of
   neurotransmitters that are passed over among biological neurons. so, if
   the connection is important it will have a bigger weight value than
   those connections which are not important.

   since there could be numerous values getting into one of the neurons,
   every neuron has a so-called input function. input values from all
   weighted connections are usually summarized, which is done by weighted
   sum function. this value is then passed to the activation
   function, whose job is to calculate whether some signal should be sent
   to the output of the neuron. you can read more about this in
   the [82]previous article.

   we can (and usually do) have multiple layers of neurons in each ann. it
   looks like something like this:

   [w2nyall.png?w=720&#038;ssl=1]

learning

   if we observe nature, we can see that systems that are able to learn
   are highly adaptable. in their quest to acquire knowledge, these
   systems use input from the outside world and modify information that
   they   ve already collected, or modify their internal structure. that is
   exactly what anns do. they adapt and modify their architecture in order
   to learn. to be more precise, the anns change weights of connections
   based on input and desired output.

      why weights?   , one might ask. well, if you look closer into the
   structure of the anns, there are a few components we could change
   inside of the ann if we want to modify their architecture. for example,
   we could create new connections among neurons, or delete them, or add
   and delete neurons. we could even modify input function or activation
   function. as it turns out, changing weights is the most practical
   approach. plus, most of the other cases could be covered by changing
   weights. deleting a connection, for example, can be done by setting the
   weight to 0. and a neuron can be deleted if we set weights on all its
   connections to zero.

training

   in the first few sentences of this article, i mentioned one very, very
   important word for anns     training. this is a necessary process for
   every ann, and it is a process in which the ann gets familiar with the
   problem it needs to solve. in practice, we usually have some collected
   data based on which we need to create our predictions, or
   classification, or any other processing. this data is called training
   set. in fact, based on behavior during the training and the nature of
   training set, we have a few classes of learning:
     * unsupervised learning     training set contains only inputs. the
       network attempts to identify similar inputs and to put them into
       categories. this type of learning is biologically motivated but it
       is not suitable for all the problems.
     * id23     training set contains inputs, but the
       network is also provided with additional information during the
       training. what happens is that once the network calculates the
       output for one of the inputs, we provide information that indicates
       whether the result was right or wrong and possibly, the nature of
       the mistake that the network made.
     * supervised learning     training set contains inputs and desired
       outputs. this way the network can check its calculated output the
       same as desired output and take appropriate actions based on that.

   supervised learning is most commonly used, so let   s dig a little deeper
   into this topic. basically, we get a training set that contains a
   vector of input values and a vector of desired output values. once the
   network calculates the output for one of the inputs, cost
   function calculates the error vector. this error indicates how close
   our guess is to the desired output. one of the most used cost functions
   is mean squared error function:

   [6emramt.png?w=720&#038;ssl=1]

   here, x is the training input vector, y(x) is the generated output of
   the id158 and a is the desired output. also, one
   can notice that this function is the function that depends
   on wand b, which represents weights and biases, respectively.

   now, this error is sent back to the neural network, and weights are
   modified accordingly. this process is called id26.
   id26 is an advanced mathematical algorithm, using which the
   id158 has the ability to adjust all weights at
   once. since it is a complex topic and would require an entirely
   separate blog post, i suggest you read this [83]article. the important
   thing to remember here is that by using this algorithm, anns are able
   to modify weights in a fast and easy manner.

id119

   the entire point of training is to set the correct values to the
   weights, so we get the desired output in our neural network. this means
   that we are trying to make the value of our error vector as small as
   possible, i.e. to find a global minimum of the cost function. one way
   of solving this problem is to use calculus. we could compute
   derivatives and then use them to find places where is an extremum of
   the cost function. however, the cost function is not a function of one
   or a few variables; it is a function of all weights in the network, so
   these calculations will quickly grow into a monster. that is why we use
   the technique called id119.

   there is one useful analogy that describes this process quite
   well. imagine that you had a ball inside a rounded valley like in the
   picture below. if you let the ball roll, it will go from one side of
   the valley to the other, until it gets to the bottom.

   [ng5nchn.png?w=720&#038;ssl=1]

   essentially, we can look at this behavior like the ball is optimizing
   its position from left to right, and eventually, it comes to the
   bottom, i.e. the lowest point of the valley. the bottom, in this case,
   is the minimum of our error function. this is what id119
   algorithm is doing. it starts from one position in which by calculating
   derivates and some second derivates of cost function c it gets the
   information about where    the ball    should roll. every time we calculate
   derivates we get information about the slope of the side of the valley
   at its current position. this is represented in the picture below with
   the blue line.

   when the slope is negative (downward from left to right), the ball
   should move to the right, otherwise, it should move to the left. be
   aware that the ball is just an analogy, and we are not trying to
   develop an accurate simulation of the laws of physics. we are trying to
   get to the minimum of the function using this alternative method since
   we already realized that using calculus is not optimal.

   [pgjehal.png?w=720&#038;ssl=1]

   in a nutshell, the process goes like this:
    1. put the training set in the neural networks and get the output.
    2. the output is compared with desired output and error is calculated
       using cost function.
    3. based on the error value and used cost function, decision on how
       the weights should be changed is made in order to minimize the
       error value.
    4. the process is repeated until the error is minimal.

   what i   ve just explained has one more name     batch id119.
   this is due to the fact that we put the entire training set in the
   network and then we modify the weights. the problem with this approach
   is that this way, we can hit a local minimum of the error function, but
   not the global one. the previous statement is one of the biggest
   problems in neural networks, and there are multiple ways to solve it.

   however, the common way to avoid the trap of going to a local minimum
   is modifying weights after each processed input of the training set.
   when all inputs from a training set are processed, one epoch is done.
   it is necessary to do multiple epochs to get the best results. the
   explained process is called     stochastical id119. also, by
   doing so we are minimizing the possibility of another problem arising    
   overfitting. overfitting is a situation in which neural networks
   perform well on the training set, but not on the real values later.
   this happens when the weights are set to solve only the specific
   problem we have in the training set.

conclusion

   now, let   s sum this up in a few steps:
    1. we randomly initialize weights in our neural network
    2. we send the first set of input values to the neural network and
       propagate values trough it to get the output value.
    3. we compare output value to the expected output value and calculate
       the error using cost functions.
    4. we propagate the error back to the network and set the weights
       according to that information.
    5. repeat steps from 2 to 4 for every input value we have in our
       training set.
    6. when the entire training set has been sent through the neural
       network, we have finished one epoch. after that, we repeat more
       epochs.

   so, that is an oversimplified representation of how neural networks
   learn. what i haven   t mentioned is that in practice, the training set
   is separated into two parts and the second part of the training set is
   used to validate the work of the network.

   hopefully, this article will provide a good overview of the way neural
   networks learn. since it is a complex topic some things were left
   uncovered (id26 for example), which will be covered in the
   articles to come. it should be mentioned that i tried not to go too
   deep into the math, which leaves plenty of room for research     

   thanks for reading!
     __________________________________________________________________

   this article is a part of  id158s series, which you
   can check out [84]here.
     __________________________________________________________________

   read more posts from the author at [85]rubik   s code.
     __________________________________________________________________

   [86]codeproject

share:

     * [87]click to share on twitter (opens in new window)
     * [88]click to share on facebook (opens in new window)
     *

like this:

   like loading...

   posted in: [89]ai
   tagged: [90]artificaial inteligance [91]id158s
   [92]deep learning [93]neural networks [94]software development

post navigation

   previous entry:[95]common neural network id180
   next entry:[96]id26 algorithm in id158s

2 comments

    1. pingback: [97]implementing simple neural network in c#    
       developparadise
    2. pingback: [98]recurrent neural network detail guide with example
       and applications

leave a reply [99]cancel reply

   iframe: [100]jetpack_remote_comment

   this site uses akismet to reduce spam. [101]learn how your comment data
   is processed.

subscribe to rubik's code via email

   email address ____________________

   (button) subscribe

video courses

     * introducing test driven development in c#
     * test driven development with c# and .net core mvc
     * real-world machine learning projects with scikit-learn

follow rubik   s code

     * [102]linkedin
     * [103]twitter
     * [104]github
     * [105]facebook

   close and accept
   privacy & cookies: this site uses cookies. by continuing to use this
   website, you agree to their use.
   to find out more, including how to control cookies, see here: [106]info

   (button) go to the top

products

     * [107]introducing test driven development in c#
     * [108]test driven development with c# and .net core mvc
     * [109]real-world machine learning projects with sci-kit learn

series

     * [110]id158s series
     * [111]machine learning with ml.net series
     * [112]asynchronous programming in .net series
     * [113]mongodb series
     * [114]philosophy as motivational tool for software crafters series

rubik   s code

     * [115]about
     * [116]contact

     * [117]linkedin
     * [118]twitter
     * [119]git
     * [120]fb

   iframe: [121]likes-master

   %d bloggers like this:

references

   visible links
   1. https://rubikscode.net/feed/
   2. https://rubikscode.net/comments/feed/
   3. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/feed/
   4. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
   5. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/&format=xml
   6. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/#content
   7. https://www.linkedin.com/in/nmzivkovic/
   8. https://twitter.com/nmzivkovic?lang=en
   9. https://github.com/nmzivkovic
  10. https://www.facebook.com/rubikscodenet/
  11. https://rubikscode.net/
  12. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
  13. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
  14. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
  15. https://rubikscode.net/
  16. https://rubikscode.net/category/series/
  17. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  18. https://rubikscode.net/2017/11/13/introduction-to-artificial-neural-networks/
  19. https://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  20. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  21. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  22. https://rubikscode.net/2018/01/29/implementing-simple-neural-network-in-c/
  23. https://rubikscode.net/2018/02/05/introduction-to-tensorflow-with-python-example/
  24. https://rubikscode.net/2018/02/12/implementing-simple-neural-network-using-keras-with-python-example/
  25. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  26. https://rubikscode.net/2018/03/05/implementation-of-convolutional-neural-network-using-python-and-keras/
  27. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  28. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
  29. https://rubikscode.net/2018/03/26/two-ways-to-implement-lstm-network-using-python-with-tensorflow-and-keras/
  30. http://rtrtr.com/
  31. https://rubikscode.net/2018/12/10/introduction-to-generative-adversarial-networks-gans/
  32. https://rubikscode.net/2018/12/17/implementing-gan-dcgan-with-python/
  33. https://rubikscode.net/2019/01/14/introduction-to-adversarial-autoencoders/
  34. https://rubikscode.net/2019/01/21/generating-images-using-adversarial-autoencoders-and-python/
  35. https://rubikscode.net/2019/02/11/implementing-cyclegan-using-python/
  36. https://rubikscode.net/2019/02/04/introduction-to-cyclegan/
  37. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
  38. https://rubikscode.net/2019/02/18/ultimate-guide-to-machine-learning-with-ml-net/
  39. https://rubikscode.net/2018/06/18/using-ml-net-introduction-to-machine-learning-and-ml-net/
  40. https://rubikscode.net/2018/06/25/machine-learning-with-ml-net-solving-real-world-regression-problem-bike-sharing-demands/
  41. https://rubikscode.net/2018/07/02/machine-learning-with-ml-net-solving-real-world-classification-problem-wine-quality/
  42. https://rubikscode.net/2018/07/09/machine-learning-in-ml-net-using-machine-learning-model-in-asp-net-mvc-application/
  43. https://rubikscode.net/2018/07/16/machine-learning-with-ml-net-comparing-data-exploration-in-python-with-data-exploration-in-ml-net/
  44. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
  45. https://rubikscode.net/2018/05/21/asynchronous-programming-in-net-motivation-and-unit-testing/
  46. https://rubikscode.net/2018/05/28/asynchronous-programming-in-net-common-mistakes-and-best-practices/
  47. https://rubikscode.net/2018/06/04/asynchronous-programming-in-net-task-based-asynchronous-pattern-tap/
  48. https://rubikscode.net/2018/06/11/asynchronous-programming-in-net-benefits-and-tradeoffs-of-using-valuetask/
  49. https://rubikscode.net/2018/09/26/self-organizing-maps-series/
  50. https://rubikscode.net/2018/08/20/introduction-to-self-organizing-maps/
  51. https://rubikscode.net/2018/08/27/implementing-self-organizing-maps-with-python-and-tensorflow/
  52. https://rubikscode.net/2018/09/17/implementing-self-organizing-maps-with-net-core/
  53. https://rubikscode.net/2018/09/24/credit-card-fraud-detection-using-self-organizing-maps-and-python/
  54. https://rubikscode.net/2019/01/07/restricted-boltzmann-machine-series/
  55. https://rubikscode.net/2018/10/01/introduction-to-restricted-boltzmann-machines/
  56. https://rubikscode.net/2018/10/22/implementing-restricted-boltzmann-machine-with-python-and-tensorflow/
  57. https://rubikscode.net/2018/10/15/implementing-restricted-boltzmann-machine-with-net-core/
  58. https://rubikscode.net/2018/11/12/generate-music-using-tensorflow-and-python/
  59. https://rubikscode.net/2017/10/16/mongodb-serial/
  60. https://rubikscode.net/2017/07/19/introduction-to-nosql-and-polyglot-persistence/
  61. https://rubikscode.net/2017/07/24/mongo-db-basics-part-1/
  62. https://rubikscode.net/2017/07/31/mongo-db-basics-part-2/
  63. https://rubikscode.net/2017/10/02/using-mongodb-in-c/
  64. https://rubikscode.net/2017/10/09/mean-stack-crash-course-using-mongodb-with-node-js-express-and-angular-4/
  65. https://rubikscode.net/2017/10/15/serverless-and-mongodb-atlas/
  66. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
  67. https://rubikscode.net/2018/04/23/how-to-use-miyamoto-musashis-philosophy-to-become-better-software-crafter/
  68. https://rubikscode.net/2017/11/06/how-to-use-marcus-aureliuss-meditations-to-become-better-software-craftsman/
  69. https://rubikscode.net/2017/06/22/   how-to-use-friedrich-nietzsches-philosophy-to-be-better-software-craftsman/
  70. https://rubikscode.net/2017/05/14/how-to-use-art-of-war-to-be-better-software-craftsman/
  71. https://rubikscode.net/category/ai/
  72. https://rubikscode.net/category/machine-learning/
  73. https://rubikscode.net/category/net/
  74. https://rubikscode.net/category/python/
  75. https://rubikscode.net/category/nosql/
  76. https://rubikscode.net/about/
  77. https://rubikscode.net/contact/
  78. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  79. https://rubikscode.net/author/rubikscode/
  80. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/#comments
  81. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  82. https://rubikscode.net/2017/11/13/introduction-to-artificial-neural-networks/
  83. http://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  84. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  85. https://rubikscode.net/
  86. https://www.codeproject.com/
  87. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/?share=twitter
  88. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/?share=facebook
  89. https://rubikscode.net/category/ai/
  90. https://rubikscode.net/tag/artificaial-inteligance/
  91. https://rubikscode.net/tag/artificial-neural-networks/
  92. https://rubikscode.net/tag/deep-learning/
  93. https://rubikscode.net/tag/neural-networks/
  94. https://rubikscode.net/tag/software-development/
  95. https://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  96. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  97. http://devepar.com/archives/1058
  98. https://techgrabyte.com/recurrent-neural-network-example-applications/
  99. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/#respond
 100. https://jetpack.wordpress.com/jetpack-comment/?blogid=128253944&postid=7906&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=identicon&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.2.1&show_cookie_consent=10&has_cookie_consent=0&sig=46cd6122ee4c598e510a68d8a01961cd9374cc1d#parent=https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
 101. https://akismet.com/privacy/
 102. https://www.linkedin.com/in/nmzivkovic/
 103. https://twitter.com/nmzivkovic
 104. https://github.com/nmzivkovic
 105. https://www.facebook.com/rubikscodenet
 106. https://automattic.com/cookies/
 107. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 108. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 109. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
 110. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
 111. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
 112. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
 113. https://rubikscode.net/2017/10/16/mongodb-serial/
 114. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
 115. https://rubikscode.net/about/
 116. https://rubikscode.net/contact/
 117. https://www.linkedin.com/in/nmzivkovic/
 118. https://twitter.com/nmzivkovic?lang=en
 119. https://github.com/nmzivkovic
 120. https://www.facebook.com/rubikscodenet/
 121. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914

   hidden links:
 123. https://rubikscode.net/
 124. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 125. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 126. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
