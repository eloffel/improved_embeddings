   #[1]rubik's code    feed [2]rubik's code    comments feed [3]rubik's code
      id26 algorithm in id158s comments feed
   [4]alternate [5]alternate

   [6]skip to content

     * [7]linkedin
     * [8]twitter
     * [9]git
     * [10]fb

   [11]rubik's code

   machine learning without tears

     * products
          + [12]introducing test driven development in c#     video course
          + [13]test driven development with c# and .net core mvc     video
            course
          + [14]real-world machine learning projects with sci-kit learn
     * [15]articles
     * [16]series
          + [17]id158s series
               o [18]introduction to id158s
               o [19]common neural network id180
               o [20]how do id158s learn?
               o [21]id26 algorithm in artificial neural
                 networks
               o [22]implementing simple neural network in c#
               o [23]introduction to tensorflow     with python example
               o [24]implementing simple neural network using keras     with
                 python example
               o [25]introduction to convolutional neural networks
               o [26]implementation of convolutional neural network using
                 python and keras
               o [27]introduction to recurrent neural networks
               o [28]understanding id137 (lstms)
               o [29]two ways to implement lstm network using python    
                 with tensorflow and keras
          + [30]gan series
               o [31]introduction to id3
                 (gans)
               o [32]implementing gan & dcgan with python
               o [33]introduction to adversarial autoencoders
               o [34]generating images using adversarial autoencoders and
                 python
               o [35]implementing cyclegan using python
               o [36]introduction to cyclegan
          + [37]machine learning with ml.net
               o [38]ultimate guide to machine learning with ml.net
               o [39]using ml.net     introduction to machine learning and
                 ml.net
               o [40]machine learning with ml.net     solving real-world
                 regression problem (bike sharing demands)
               o [41]machine learning with ml.net     solving real-world
                 classification problem (wine quality)
               o [42]machine learning in ml.net     using machine learning
                 model in asp.net core application
               o [43]machine learning with ml.net     comparing data
                 exploration in python with data exploration in ml.net
          + [44]asynchronous programming in .net
               o [45]motivation and unit testing
               o [46]common mistakes and best practices
               o [47]task-based asynchronous pattern (tap)
               o [48]benefits and tradeoffs of using valuetask
          + [49]self- organizing maps series
               o [50]introduction to self-organizing maps
               o [51]implementing self-organizing maps with python and
                 tensorflow
               o [52]implementing self-organizing maps with .net core
               o [53]credit card fraud detection using self-organizing
                 maps and python
          + [54]restricted id82 series
               o [55]introduction to restricted id82s
               o [56]implementing restricted id82 with python
                 and tensorflow
               o [57]implementing restricted id82 with .net
                 core
               o [58]generate music using tensorflow and python
          + [59]mongodb series
               o [60]introduction to nosql and polyglot persistence
               o [61]mongodb basics     part 1.
               o [62]mongo db basics     part 2.
               o [63]using mongodb in c#
               o [64]mean stack crash course     using mongodb with node.js,
                 express and angular 4
               o [65]serverless and playing around with mongodb atlas
          + [66]philosophy as motivational tool for software crafters
            series
               o [67]how to use miyamoto musashi   s philosophy to become
                 better software crafter
               o [68]how to use marcus aurelius   s meditations to become
                 better software crafter
               o [69]   how to use friedrich nietzsche   s philosophy to be
                 better software crafter
               o [70]how to use    art of war    to be better software crafter
     * categories
          + [71]ai
          + [72]machine learning
          + [73].net
          + [74]python
          + [75]nosql
     * [76]about
     * [77]contact

   (button) open a search box close a search box search for:
   ____________________ (button) search

id26 algorithm in id158s

   [78]january 22, 2018september 29, 2018 by [79]rubikscode [80]3 comments

   in the [81]previous article, we covered the learning process of anns
   using id119. however, in the last few sentences, i   ve
   mentioned that some rocks were left unturned. specifically, explanation
   of the id26 algorithm was skipped. also, i   ve mentioned it
   is a somewhat complicated algorithm and that it deserves the whole
   separate blog post. so here it is, the article about id26!
   don   t get me wrong you could observe this whole process as a black box
   and ignore its details. you will still be able to build artificial
   neural networks using some of the libraries out there. however, knowing
   details will definitely put more light on the whole topic of whole
   learning mechanism of anns and give you a better understanding of it.
   so, let   s dive into it!

   [m2g3jvi.png?w=720&#038;ssl=1]

   like the majority of important aspects of neural networks, we can find
   roots of id26 in the 70s of the last century. however, this
   concept was not appreciated until 1986. when  [82]david
   rumelhart, [83]geoffrey hinton, and [84]ronald williams published their
   [85]paper. here they presented this algorithm as the fastest way to
   update weights in the anns, and today it is one of the most important
   components in anns learning process. one of the main tasks of
   id26 is to give us information on how quickly the error
   changes when weights are changed. behind the scenes, it uses partial
   derivates of the cost function for each weight        c/   w, where c is the
   cost function, ie. function that we use to calculate the error,
   and w is the weight of the specific connection. before we go any
   further let   s set some assumptions regarding cost function necessary
   for this algorithm to work.

cost function assumptions

   in order to make this article easier to understand, from now on we are
   going to use specific cost function      we are going to use quadratic
   cost function, or mean squared error function:

   [6emramt.png?w=720&#038;ssl=1]

   where n is the total number of inputs in the training set, x is the
   individual input from the training set, y(x) is the corresponding
   desired output, a is the vector of actual outputs from the network when
   x is input. this function is most commonly used in anns so i will use
   it here for demonstration purposes too.

   as mentioned, there are some assumptions that we need to make regarding
   this function in order for id26 to be applicable. to be
   exact there are two of them:
    1. the cost function can be written as an average:
       [pkpuo1x.png?w=720&#038;ssl=1] over cost functions c(x) for input
       x.
    2. the cost function it can be written as a function of the outputs
       from the id158.

   you can see that both of these assumptions are applicable to our choice
   of the cost function     quadratic cost function.

id26 algorithm

   we already established that id26 helps us understand how
   changing the weights and biases affects the cost function. this is
   achieved by calculating partial derivatives for each weight and for
   each bias, ie.    c/   w and    c/   b. now, every neuron in the neural network
   generates some sort of an error. this error affects other neurons and
   ultimately it affects the global error, meaning it affects our cost
   function. the middle step of this whole process is calculating this
   value, and use it to align weights accordingly. by calculating the
   partial derivate of cost function by the input of each neuron, we
   define the error:
   [86]\delta = \frac{\partial c}{\partial net}

   where net is the weighted input in the certain neuron.

   we detoured a little bit, haven   t we? let   s see what are the main steps
   of this algorithm. in a nutshell, id26 is happening in two
   main parts. first is called propagation and it is contained from these
   steps:
    1. initialize weights of the neural network
    2. propagate inputs forward through the network to generate the output
       values
    3. calculate the error
    4. propagation of the output back through the network in order to
       generate the error of all output and hidden neurons.

   the second part of id26 updates weights of connections:
    1. the weight   s output error and input are multiplied to find the
       gradient of the weight.
    2. a certain percentage, defined by learning rate (more on this a bit
       later) of the weight   s gradient is subtracted from the weight.

walk-trough

   when we are propagation the error back to the neural network and when
   we are calculating errors, there is a difference depending on where the
   neuron is located in the network. meaning, we use different equations
   to calculate the error of neuron in the output layer from equations
   that we use to calculate the error of the neurons in hidden layers. it
   is a little bit trickier and a little less obvious for neurons in
   hidden layers. good thing is that, when the error is propagated back in
   the neural network, we start from the output layer. so, let   s say that
   we want to calculate partial derivate of the cost function in respect
   to one of the weights of the neuron in output layer. using chain rule
   we will get this equation:

   [87]\frac{\partial c}{\partial w^{ij}} = \frac{\partial c}{\partial
   o^{j}}\frac{\partial o^{j}}{\partial net^{j}}\frac{\partial
   net^{j}}{\partial w^{ij}}

   where o^j is the output of the neuron j, net^j is the weighted input
   value of neuron j and w^ij is the weight of the connection to the
   neuron j. notice that first two parts of this equation are actually     ,
   the error of the output neuron that i   ve mentioned in the previous
   chapter, if we apply a chain rule to it:

   [88]\delta = \frac{\partial c}{\partial o^{j}}\frac{o^{j}}{net^{j}}

   now, by solving each part of this equation we will get the final value
   of this derivate. the first part of the equation     derivate of the cost
   function with respect to the output of the neuron, is pretty
   straightforward to calculate. since the output value of the output
   neuron is the output value of the neural network itself a, this can be
   rewritten like this:

   [89]\frac{\partial c}{\partial o^{j}} = \frac{\partial c}{\partial a} =
   \frac{\partial }{\partial a}\frac{1}{2}\left ( y(x) - a \right ) = a -
   y(x)

   meaning, this can be calculated by subtracting the expected output from
   the actual one in that neuron.

   the second part of the equation     the derivative of the output of
   neuron j with respect to its input is simply the partial derivative of
   the activation function. we are going to assume that we are using the
   logistic function as the activation function of the output neuron:

   [90]\gamma (x) = \frac{1}{1 + e^{-x}}

   then we can write this part of the equation like this:

   [91]\frac{\partial o^{j}}{\partial net^{j}} = \frac{\partial }{\partial
   net^{j}}\gamma (net^{j}) = \gamma (net^{j})(1 - \gamma (net^{j}))

   and finally the third part of the equation     the derivative of weighted
   input to the neuron in respect to the weight of the input connection
   can be calculated like this:

   [92]\frac{\partial net^{j}}{\partial w^{ij}} = \frac{\partial
   }{\partial w^{ij}}w^{ij}o^{j} = o^{j}

   now, let   s put this all together. the formula that we get for
   calculating the error for the neuron in the output layer is:
   [93]\delta = (a - y(x))a(1 - a)

   calculating the error in the neuron that is in the hidden layer is a
   bit more complicated. let   s get back to the equation we started with:

   [94]\frac{\partial c}{\partial w^{ij}} = \frac{\partial c}{\partial
   o^{j}}\frac{\partial o^{j}}{\partial net^{j}}\frac{\partial
   net^{j}}{\partial w^{ij}}

   the only adjustment we need to make is actually in the first part of
   the equation     derivate of the cost function with respect to the output
   of the neuron. the difference is that the output of the neuron in the
   output layer was already calculated, but for the neuron in the hidden
   layer, we need to work a little bit more. we can extend this first
   part, using chain rule again and considering that the cost function is
   a function of all inputs from neurons that are receiving input from
   neuron j. we will mark the set of that neurons with the letter l. then
   this first part of the equation can be written like:

   [95]\frac{\partial c}{\partial o^{j}} = \sum_{l \epsilon
   l}(\frac{\partial c}{net^{l}}\frac{\partial net^{l}}{\partial o^{j}}) =
   \sum_{l \epsilon l}(\frac{\partial c}{\partial o^{l}}\frac{\partial
   o^{l}}{\partial net^{l}}w^{jl})

   this means that this whole derivative can be calculated if derivatives
   of all neurons that are closer to the output are known. meaning, we can
   calculate the error of the neuron in the hidden layer using this
   formula:

   [96]\delta = \sum_{l \epsilon l}(\delta ^{l}w^{jl})a(1-a)

   now, when the error is available, we can finally adjust our weight of
   connection. however, before we do that we need to pick the learning
   rate       . this learning rate is quite important since it is dictating
   next step in the id119 process, and if it is not properly
   adjusted, this may cause minimum to be missed or learning process to be
   very slow. more on this id119 process you can read in
   the [97]previous article. so, the value which will be used to modify
   weight value is:

   [98]\delta w^{ij} = -\eta \frac{\partial c}{\partial w^{ij}} = -\eta
   y^{i}\delta ^{j}

   meaning, that we can get this value by multiplying learning rate, with
   the output value of the input neuron of the connection with the error
   value of the output neuron of the connection. this value is multiplied
   by -1 because it should progress towards the minimum of the cost
   function.

   as you can see there is quite some elegance and beauty to the whole
   process. firstly, error for the output layer is calculated and
   sequentially that error is used to calculate the errors of all neurons
   in hidden layers. when we have this information it is quite easy to
   adjust weights.

conclusion

   id26 is the tool that played quite an important role in the
   field of id158s. it optimized the whole process of
   updating weights and in a way, it helped this field to take off. and
   even thou you can build an id158 with one of the
   powerful libraries on the market, without getting into the math behind
   this algorithm, understanding the math behind this algorithm is
   invaluable.
     __________________________________________________________________

   this article is a part of  id158s series, which you
   can check out [99]here.
     __________________________________________________________________

   read more posts from the author at [100]rubik   s code.
     __________________________________________________________________

   [101]codeproject

share:

     * [102]click to share on twitter (opens in new window)
     * [103]click to share on facebook (opens in new window)
     *

like this:

   like loading...

   posted in: [104]ai
   tagged: [105]ai [106]artificaial inteligance [107]artificial neural
   networks [108]deep learning [109]neural networks [110]software
   development

post navigation

   previous entry:[111]how do id158s learn?
   next entry:[112]implementing simple neural network in c#

3 comments

    1. pingback: [113]id26 algorithm in artificial neural
       networks     collective intelligence
    2.
   bond says:
       [114]march 12, 2018 at 4:15 pm
       hi,
       the images coming from codecogs are not visible.
       loading...
       [115]reply
         1.
        [116]rubikscode says:
            [117]march 13, 2018 at 8:04 am
            that is weird. i can see them.
            i will export them as images and upload them somewhere else.
            thanks!
            loading...
            [118]reply

leave a reply [119]cancel reply

   iframe: [120]jetpack_remote_comment

   this site uses akismet to reduce spam. [121]learn how your comment data
   is processed.

subscribe to rubik's code via email

   email address ____________________

   (button) subscribe

video courses

     * introducing test driven development in c#
     * test driven development with c# and .net core mvc
     * real-world machine learning projects with scikit-learn

follow rubik   s code

     * [122]linkedin
     * [123]twitter
     * [124]github
     * [125]facebook

   close and accept
   privacy & cookies: this site uses cookies. by continuing to use this
   website, you agree to their use.
   to find out more, including how to control cookies, see here: [126]info

   (button) go to the top

products

     * [127]introducing test driven development in c#
     * [128]test driven development with c# and .net core mvc
     * [129]real-world machine learning projects with sci-kit learn

series

     * [130]id158s series
     * [131]machine learning with ml.net series
     * [132]asynchronous programming in .net series
     * [133]mongodb series
     * [134]philosophy as motivational tool for software crafters series

rubik   s code

     * [135]about
     * [136]contact

     * [137]linkedin
     * [138]twitter
     * [139]git
     * [140]fb

   iframe: [141]likes-master

   %d bloggers like this:

references

   visible links
   1. https://rubikscode.net/feed/
   2. https://rubikscode.net/comments/feed/
   3. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/feed/
   4. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
   5. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/&format=xml
   6. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/#content
   7. https://www.linkedin.com/in/nmzivkovic/
   8. https://twitter.com/nmzivkovic?lang=en
   9. https://github.com/nmzivkovic
  10. https://www.facebook.com/rubikscodenet/
  11. https://rubikscode.net/
  12. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
  13. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
  14. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
  15. https://rubikscode.net/
  16. https://rubikscode.net/category/series/
  17. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  18. https://rubikscode.net/2017/11/13/introduction-to-artificial-neural-networks/
  19. https://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  20. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  21. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  22. https://rubikscode.net/2018/01/29/implementing-simple-neural-network-in-c/
  23. https://rubikscode.net/2018/02/05/introduction-to-tensorflow-with-python-example/
  24. https://rubikscode.net/2018/02/12/implementing-simple-neural-network-using-keras-with-python-example/
  25. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  26. https://rubikscode.net/2018/03/05/implementation-of-convolutional-neural-network-using-python-and-keras/
  27. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  28. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
  29. https://rubikscode.net/2018/03/26/two-ways-to-implement-lstm-network-using-python-with-tensorflow-and-keras/
  30. http://rtrtr.com/
  31. https://rubikscode.net/2018/12/10/introduction-to-generative-adversarial-networks-gans/
  32. https://rubikscode.net/2018/12/17/implementing-gan-dcgan-with-python/
  33. https://rubikscode.net/2019/01/14/introduction-to-adversarial-autoencoders/
  34. https://rubikscode.net/2019/01/21/generating-images-using-adversarial-autoencoders-and-python/
  35. https://rubikscode.net/2019/02/11/implementing-cyclegan-using-python/
  36. https://rubikscode.net/2019/02/04/introduction-to-cyclegan/
  37. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
  38. https://rubikscode.net/2019/02/18/ultimate-guide-to-machine-learning-with-ml-net/
  39. https://rubikscode.net/2018/06/18/using-ml-net-introduction-to-machine-learning-and-ml-net/
  40. https://rubikscode.net/2018/06/25/machine-learning-with-ml-net-solving-real-world-regression-problem-bike-sharing-demands/
  41. https://rubikscode.net/2018/07/02/machine-learning-with-ml-net-solving-real-world-classification-problem-wine-quality/
  42. https://rubikscode.net/2018/07/09/machine-learning-in-ml-net-using-machine-learning-model-in-asp-net-mvc-application/
  43. https://rubikscode.net/2018/07/16/machine-learning-with-ml-net-comparing-data-exploration-in-python-with-data-exploration-in-ml-net/
  44. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
  45. https://rubikscode.net/2018/05/21/asynchronous-programming-in-net-motivation-and-unit-testing/
  46. https://rubikscode.net/2018/05/28/asynchronous-programming-in-net-common-mistakes-and-best-practices/
  47. https://rubikscode.net/2018/06/04/asynchronous-programming-in-net-task-based-asynchronous-pattern-tap/
  48. https://rubikscode.net/2018/06/11/asynchronous-programming-in-net-benefits-and-tradeoffs-of-using-valuetask/
  49. https://rubikscode.net/2018/09/26/self-organizing-maps-series/
  50. https://rubikscode.net/2018/08/20/introduction-to-self-organizing-maps/
  51. https://rubikscode.net/2018/08/27/implementing-self-organizing-maps-with-python-and-tensorflow/
  52. https://rubikscode.net/2018/09/17/implementing-self-organizing-maps-with-net-core/
  53. https://rubikscode.net/2018/09/24/credit-card-fraud-detection-using-self-organizing-maps-and-python/
  54. https://rubikscode.net/2019/01/07/restricted-boltzmann-machine-series/
  55. https://rubikscode.net/2018/10/01/introduction-to-restricted-boltzmann-machines/
  56. https://rubikscode.net/2018/10/22/implementing-restricted-boltzmann-machine-with-python-and-tensorflow/
  57. https://rubikscode.net/2018/10/15/implementing-restricted-boltzmann-machine-with-net-core/
  58. https://rubikscode.net/2018/11/12/generate-music-using-tensorflow-and-python/
  59. https://rubikscode.net/2017/10/16/mongodb-serial/
  60. https://rubikscode.net/2017/07/19/introduction-to-nosql-and-polyglot-persistence/
  61. https://rubikscode.net/2017/07/24/mongo-db-basics-part-1/
  62. https://rubikscode.net/2017/07/31/mongo-db-basics-part-2/
  63. https://rubikscode.net/2017/10/02/using-mongodb-in-c/
  64. https://rubikscode.net/2017/10/09/mean-stack-crash-course-using-mongodb-with-node-js-express-and-angular-4/
  65. https://rubikscode.net/2017/10/15/serverless-and-mongodb-atlas/
  66. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
  67. https://rubikscode.net/2018/04/23/how-to-use-miyamoto-musashis-philosophy-to-become-better-software-crafter/
  68. https://rubikscode.net/2017/11/06/how-to-use-marcus-aureliuss-meditations-to-become-better-software-craftsman/
  69. https://rubikscode.net/2017/06/22/   how-to-use-friedrich-nietzsches-philosophy-to-be-better-software-craftsman/
  70. https://rubikscode.net/2017/05/14/how-to-use-art-of-war-to-be-better-software-craftsman/
  71. https://rubikscode.net/category/ai/
  72. https://rubikscode.net/category/machine-learning/
  73. https://rubikscode.net/category/net/
  74. https://rubikscode.net/category/python/
  75. https://rubikscode.net/category/nosql/
  76. https://rubikscode.net/about/
  77. https://rubikscode.net/contact/
  78. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  79. https://rubikscode.net/author/rubikscode/
  80. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/#comments
  81. http://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  82. http://en.wikipedia.org/wiki/david_rumelhart
  83. http://www.cs.toronto.edu/~hinton/
  84. http://en.wikipedia.org/wiki/ronald_j._williams
  85. https://www.nature.com/articles/323533a0
  86. https://www.codecogs.com/eqnedit.php?latex=\delta&space;=&space;\frac{\partial&space;c}{\partial&space;net}
  87. https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;c}{\partial&space;w^{ij}}&space;=&space;\frac{\partial&space;c}{\partial&space;o^{j}}\frac{\partial&space;o^{j}}{\partial&space;net^{j}}\frac{\partial&space;net^{j}}{\partial&space;w^{ij}}
  88. https://www.codecogs.com/eqnedit.php?latex=\delta&space;=&space;\frac{\partial&space;c}{\partial&space;o^{j}}\frac{o^{j}}{net^{j}}
  89. https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;c}{\partial&space;o^{j}}&space;=&space;\frac{\partial&space;c}{\partial&space;a}&space;=&space;\frac{\partial&space;}{\partial&space;a}\frac{1}{2}\left&space;(&space;y(x)&space;-&space;a&space;\right&space;)&space;=&space;a&space;-&space;y(x)
  90. https://www.codecogs.com/eqnedit.php?latex=\gamma&space;(x)&space;=&space;\frac{1}{1&space;+&space;e^{-x}}
  91. https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;o^{j}}{\partial&space;net^{j}}&space;=&space;\frac{\partial&space;}{\partial&space;net^{j}}\gamma&space;(net^{j})&space;=&space;\gamma&space;(net^{j})(1&space;-&space;\gamma&space;(net^{j}))
  92. https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;net^{j}}{\partial&space;w^{ij}}&space;=&space;\frac{\partial&space;}{\partial&space;w^{ij}}w^{ij}o^{j}&space;=&space;o^{j}
  93. https://www.codecogs.com/eqnedit.php?latex=\delta&space;=&space;(a&space;-&space;y(x))a(1&space;-&space;a)
  94. https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;c}{\partial&space;w^{ij}}&space;=&space;\frac{\partial&space;c}{\partial&space;o^{j}}\frac{\partial&space;o^{j}}{\partial&space;net^{j}}\frac{\partial&space;net^{j}}{\partial&space;w^{ij}}
  95. https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;c}{\partial&space;o^{j}}&space;=&space;\sum_{l&space;\epsilon&space;l}(\frac{\partial&space;c}{net^{l}}\frac{\partial&space;net^{l}}{\partial&space;o^{j}})&space;=&space;\sum_{l&space;\epsilon&space;l}(\frac{\partial&space;c}{\partial&space;o^{l}}\frac{\partial&space;o^{l}}{\partial&space;net^{l}}w^{jl})
  96. https://www.codecogs.com/eqnedit.php?latex=\delta&space;=&space;\sum_{l&space;\epsilon&space;l}(\delta&space;^{l}w^{jl})a(1-a)
  97. http://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  98. https://www.codecogs.com/eqnedit.php?latex=\delta&space;w^{ij}&space;=&space;-\eta&space;\frac{\partial&space;c}{\partial&space;w^{ij}}&space;=&space;-\eta&space;y^{i}\delta&space;^{j}
  99. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
 100. https://rubikscode.net/
 101. https://www.codeproject.com/
 102. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/?share=twitter
 103. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/?share=facebook
 104. https://rubikscode.net/category/ai/
 105. https://rubikscode.net/tag/ai/
 106. https://rubikscode.net/tag/artificaial-inteligance/
 107. https://rubikscode.net/tag/artificial-neural-networks/
 108. https://rubikscode.net/tag/deep-learning/
 109. https://rubikscode.net/tag/neural-networks/
 110. https://rubikscode.net/tag/software-development/
 111. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
 112. https://rubikscode.net/2018/01/29/implementing-simple-neural-network-in-c/
 113. https://collectiv3intelligenc3.wordpress.com/2018/01/24/id26-algorithm-in-artificial-neural-networks/
 114. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/#comment-623
 115. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/?replytocom=623#respond
 116. http://rubikscode.wordpress.com/
 117. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/#comment-624
 118. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/?replytocom=624#respond
 119. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/#respond
 120. https://jetpack.wordpress.com/jetpack-comment/?blogid=128253944&postid=7911&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=identicon&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.2.1&show_cookie_consent=10&has_cookie_consent=0&sig=80dec2d2eb5fc0bb3dc5c0b28b37c96943ea3af2#parent=https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
 121. https://akismet.com/privacy/
 122. https://www.linkedin.com/in/nmzivkovic/
 123. https://twitter.com/nmzivkovic
 124. https://github.com/nmzivkovic
 125. https://www.facebook.com/rubikscodenet
 126. https://automattic.com/cookies/
 127. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 128. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 129. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
 130. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
 131. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
 132. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
 133. https://rubikscode.net/2017/10/16/mongodb-serial/
 134. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
 135. https://rubikscode.net/about/
 136. https://rubikscode.net/contact/
 137. https://www.linkedin.com/in/nmzivkovic/
 138. https://twitter.com/nmzivkovic?lang=en
 139. https://github.com/nmzivkovic
 140. https://www.facebook.com/rubikscodenet/
 141. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914

   hidden links:
 143. https://rubikscode.net/
 144. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 145. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 146. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
