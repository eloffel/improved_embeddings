   #[1]rubik's code    feed [2]rubik's code    comments feed [3]rubik's code
      introduction to recurrent neural networks comments feed [4]alternate
   [5]alternate

   [6]skip to content

     * [7]linkedin
     * [8]twitter
     * [9]git
     * [10]fb

   [11]rubik's code

   machine learning without tears

     * products
          + [12]introducing test driven development in c#     video course
          + [13]test driven development with c# and .net core mvc     video
            course
          + [14]real-world machine learning projects with sci-kit learn
     * [15]articles
     * [16]series
          + [17]id158s series
               o [18]introduction to id158s
               o [19]common neural network id180
               o [20]how do id158s learn?
               o [21]id26 algorithm in artificial neural
                 networks
               o [22]implementing simple neural network in c#
               o [23]introduction to tensorflow     with python example
               o [24]implementing simple neural network using keras     with
                 python example
               o [25]introduction to convolutional neural networks
               o [26]implementation of convolutional neural network using
                 python and keras
               o [27]introduction to recurrent neural networks
               o [28]understanding id137 (lstms)
               o [29]two ways to implement lstm network using python    
                 with tensorflow and keras
          + [30]gan series
               o [31]introduction to id3
                 (gans)
               o [32]implementing gan & dcgan with python
               o [33]introduction to adversarial autoencoders
               o [34]generating images using adversarial autoencoders and
                 python
               o [35]implementing cyclegan using python
               o [36]introduction to cyclegan
          + [37]machine learning with ml.net
               o [38]ultimate guide to machine learning with ml.net
               o [39]using ml.net     introduction to machine learning and
                 ml.net
               o [40]machine learning with ml.net     solving real-world
                 regression problem (bike sharing demands)
               o [41]machine learning with ml.net     solving real-world
                 classification problem (wine quality)
               o [42]machine learning in ml.net     using machine learning
                 model in asp.net core application
               o [43]machine learning with ml.net     comparing data
                 exploration in python with data exploration in ml.net
          + [44]asynchronous programming in .net
               o [45]motivation and unit testing
               o [46]common mistakes and best practices
               o [47]task-based asynchronous pattern (tap)
               o [48]benefits and tradeoffs of using valuetask
          + [49]self- organizing maps series
               o [50]introduction to self-organizing maps
               o [51]implementing self-organizing maps with python and
                 tensorflow
               o [52]implementing self-organizing maps with .net core
               o [53]credit card fraud detection using self-organizing
                 maps and python
          + [54]restricted id82 series
               o [55]introduction to restricted id82s
               o [56]implementing restricted id82 with python
                 and tensorflow
               o [57]implementing restricted id82 with .net
                 core
               o [58]generate music using tensorflow and python
          + [59]mongodb series
               o [60]introduction to nosql and polyglot persistence
               o [61]mongodb basics     part 1.
               o [62]mongo db basics     part 2.
               o [63]using mongodb in c#
               o [64]mean stack crash course     using mongodb with node.js,
                 express and angular 4
               o [65]serverless and playing around with mongodb atlas
          + [66]philosophy as motivational tool for software crafters
            series
               o [67]how to use miyamoto musashi   s philosophy to become
                 better software crafter
               o [68]how to use marcus aurelius   s meditations to become
                 better software crafter
               o [69]   how to use friedrich nietzsche   s philosophy to be
                 better software crafter
               o [70]how to use    art of war    to be better software crafter
     * categories
          + [71]ai
          + [72]machine learning
          + [73].net
          + [74]python
          + [75]nosql
     * [76]about
     * [77]contact

   (button) open a search box close a search box search for:
   ____________________ (button) search

introduction to recurrent neural networks

   [78]march 12, 2018march 12, 2018 by [79]rubikscode [80]8 comments

   have you ever wondered how predictive text algorithm works? how exactly
   does that id103 software know our voice? as for image
   classification, [81]convolutional neural networks were turning the
   whiles behind the scene, for these kinds of problems we are using
   recurrent neural networks (id56). these neural networks are very
   powerful and they are especially useful in so-called natural language
   processing (nlp). one might wonder what makes them so special.well, the
   networks we examined so far, [82]standard neural networks and
   [83]convolutional neural networks, are accepting a fixed-size vector as
   input and produce a fixed-sized vector as an output.

   [muv9rft.png?w=720&#038;ssl=1]

   however, humans don   t think like that. we are not throwing everything
   away and start every thought from the scratch. we use context and
   information we received before, as well. as you are reading this
   your understanding of every word is based on your understanding of
   previous words.this means we are thinking in sequences, ie. we are
   using new inputs in every nanosecond, combine them with the previous
   experience and form a thought based on that. that is what recurrent
   neural networks do too (in a way), they operate over sequences of
   inputs and outputs and give us back the result. using them we can make
   much more intelligent systems.

architecture

   the structure of recurrent neural networks is the same as the structure
   of id158s, but with one twist. they are
   propagating output of the network back to the input. wait, what? yes,
   we are using the output of the network in time moment t to calculate
   the output of the network in moment t + 1. take a look at this
   oversimplified representation of id56:

   [ld8jjux.png?w=720&#038;ssl=1]

   this means that output will be sent back to the input and then used
   when next input comes along, in next time step. basically,    state    of
   the network is forward propagated through time. we can unroll this
   structure and get another way to represent the same thing is like this:

   [efjl1p3.png?w=720&#038;ssl=1]

   we can consider this point of view, that id56s have    memory    in which
   they store information about what has been calculated so far as well.
   either way, you get the picture, we are using some sort of a process to
   combine an output of previous time step and input in current time step
   to calculate output in current timestep.

   one might think that sequences of inputs or outputs are rare, however,
   it is important to realize that we can process any data in this manner
   even though inputs/outputs are fixed vectors. for example, in the image
   below we can see how id56 generates the picture of digits by learning to
   sequentially add layer by layer of color to the canvas ([84]gregor et
   al.):

   [m3ldatt.gif?w=720&#038;ssl=1]

math behind id56s

   recurrent neural networks have a simple math representation:

   [p0xqdnm.png?w=720&#038;ssl=1]

   in an essence, this equation is saying that state of the network in
   current time step ht can be described as a function of the state in
   previous time step and input in the current time step. the function f
   is usually a nonlinearity such as [85]tanh or relu. the state of the
   network in current time step ht becomes input value for the next time
   step. if we take the simplest form of id56, the one that uses tanh as
   the activation function, we can represent it like this:

   [dimzcrw.png?w=720&#038;ssl=1]

   where whh are the weights of the recurrent neurons, and wxh are the
   weights of the input neurons. in this example, we are considering just
   one previous time step but in general, we can observe the state of
   multiple time steps. this way our predictions would be more precise.
   the output of this network is then calculated using this current state
   and weights on the output.

simplified example

   as one can see math representation of id56 is not so scary. if we
   transfer that knowledge into a code, we would get a simple id56 class
   implementation. this implementation would expose one method    
   timestep, using which we can simulate time. it would take one input x,
   which would represent the input of the network in that time step. of
   course, this method would return an output y. inside of the class, we
   would keep the information about previous inputs and network states.
   so, here is the simplified implementation of id56 in c#:

   just one note here, for using matrix type you need to
   install mathnet.numerics nuget package. we are using tanh function to
   squashes the activations to the range [-1, 1]. notice that inside that
   function call we are multiplying current state with the recurrent
   weights and sum that with the multiplication of the input with the
   corresponding input weights. just like in the equation from the
   previous chapter. an output is then calculated by multiplying that
   current state with output weights.

   since python is the go-to language when it comes to implementing neural
   networks, here is the implementation using it as well:

   here we used numpy for operations on matrices. we have done the same
   thing, calculated current state h using stored state, input x and
   weights on recurrent and input layers. we used np.tanh function for
   activation function and np.dot function for id127. once
   the state is calculated it is used for calculating the output.

   of course, this is a just simplified representation of the recurrent
   neural network. the idea of this example is to give us the feeling of
   how the state of the networks is preserved through time.

id26 through time (bptt)

   id26 is a mechanism that neural networks use to update
   weights. in a nutshell, during the [86]training process networks
   calculate output for some input training set of data. then they compare
   that result to the desired one and according to that update weights
   going from output layer back to the input layer. this algorithm is a
   bit more complicated and you can read more about it [87]here.

   however, in recurrent neural networks, we have an even more complicated
   situation. that is because we have an extra dimension    
   time. figuratively speaking, we have to go back in time to update the
   weights. that is why this process in recurrent neural networks is
   called id26 through time (bptt).

   [ulxgqll.png?w=720&#038;ssl=1]

   take a look at the image above. it is the same representation of
   unfolded id56, but we have added additional information from the id56
   equation. the unrolled id56 kinda remind us of the standard neural
   network representation. that is why [88]id26 algorithm in
   id56 is similar to the algorithm in standard neural networks. the only
   difference is that we summarize the gradients of the error for all time
   steps. this is done like this because we share parameters across
   layers. here is how it is done!

   [tvcelmz.png?w=720&#038;ssl=1]

   usually, the whole sequence of data is considered to be one training
   example. this simplifies this problem a lot because we can calculate
   the error in each time step and calculate the global error (as the sum
   of all errors). another thing we can notice is that states are
   co-dependent. we can calculate gradient, using stochastic gradient
   descent, and pass this information to the previous time step and use it
   for calculation of their error and gradient, and so on. this is how we
   squash time dimension and use regular id26 algorithm for
   aligning the weights.

conclusion

   recurrent neural networks are one very useful tool with a wide range of
   applications. they are used in various id38 and text
   generators solutions. also, they are used for id103. when
   combined with convolutional neural networks, this kind of neural
   networks are used for creating labels for images that are not labeled.
   it is amazing how this combination works.
   [pjtsnhz.png?resize=720%2c245&#038;ssl=1] [89]source

   recurrent neural networks have one problem though. they are having
   difficulties learning long-range dependencies, meaning they don   t
   understand interactions between data that are several steps apart. for
   example, sometimes we need more context when predicting words than just
   one previous word. this problem is called vanishing gradient problem,
   and it is solved by special kind of recurrent neural networks    
   long-short term memory networks (lstm), the bigger topic that will be
   covered in the later articles.

   thanks for reading!
     __________________________________________________________________

   this article is a part of  [90]id158s series.
     __________________________________________________________________

   read more posts from the author at [91]rubik   s code.
     __________________________________________________________________

   [92]codeproject

share:

     * [93]click to share on twitter (opens in new window)
     * [94]click to share on facebook (opens in new window)
     *

like this:

   like loading...

   posted in: [95]ai
   tagged: [96]ai [97]deep learning [98]machine learning [99]neural
   networks [100]recurrent neural networks [101]software development

post navigation

   previous entry:[102]implementation of convolutional neural network
   using python and keras
   next entry:[103]understanding id137 (lstms)

8 comments

    1. pingback: [104]introduction to recurrent neural networks    
       collective intelligence
    2. pingback: [105]introduction to recurrent neural networks     indiz.de
    3. pingback: [106]introduction to recurrent neural networks (id56):
       https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural
       -networks/     #bigdata #datascience #ai #machinelearning
       #deeplearning #neuralnetworks
       #id26pic.twitter.com/bwsyo0b2al : avata
    4. pingback: [107]understanding id137
       (lstms)     developparadise
    5.
   renato sant anna says:
       [108]june 19, 2018 at 1:02 am
       great article!!!
       loading...
       [109]reply
         1.
        rubikscode says:
            [110]june 19, 2018 at 6:55 am
            thanks!
            loading...
            [111]reply
    6.
   dohyun, kwon says:
       [112]july 25, 2018 at 3:27 pm
       thanks for this article.
       this is not a big issue, but i guess the index of inputs in the
       figure which is above the conclusion section is t-1, t, t+1     
       loading...
       [113]reply
         1.
        rubikscode says:
            [114]july 25, 2018 at 5:45 pm
            hi dohyun,
            thank you for reading.
            yes, you are right, thanks for noticing!
            loading...
            [115]reply

leave a reply [116]cancel reply

   iframe: [117]jetpack_remote_comment

   this site uses akismet to reduce spam. [118]learn how your comment data
   is processed.

subscribe to rubik's code via email

   email address ____________________

   (button) subscribe

video courses

     * introducing test driven development in c#
     * test driven development with c# and .net core mvc
     * real-world machine learning projects with scikit-learn

follow rubik   s code

     * [119]linkedin
     * [120]twitter
     * [121]github
     * [122]facebook

   close and accept
   privacy & cookies: this site uses cookies. by continuing to use this
   website, you agree to their use.
   to find out more, including how to control cookies, see here: [123]info

   (button) go to the top

products

     * [124]introducing test driven development in c#
     * [125]test driven development with c# and .net core mvc
     * [126]real-world machine learning projects with sci-kit learn

series

     * [127]id158s series
     * [128]machine learning with ml.net series
     * [129]asynchronous programming in .net series
     * [130]mongodb series
     * [131]philosophy as motivational tool for software crafters series

rubik   s code

     * [132]about
     * [133]contact

     * [134]linkedin
     * [135]twitter
     * [136]git
     * [137]fb

   iframe: [138]likes-master

   %d bloggers like this:

references

   visible links
   1. https://rubikscode.net/feed/
   2. https://rubikscode.net/comments/feed/
   3. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/feed/
   4. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
   5. https://rubikscode.net/wp-json/oembed/1.0/embed?url=https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/&format=xml
   6. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/#content
   7. https://www.linkedin.com/in/nmzivkovic/
   8. https://twitter.com/nmzivkovic?lang=en
   9. https://github.com/nmzivkovic
  10. https://www.facebook.com/rubikscodenet/
  11. https://rubikscode.net/
  12. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
  13. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
  14. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
  15. https://rubikscode.net/
  16. https://rubikscode.net/category/series/
  17. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  18. https://rubikscode.net/2017/11/13/introduction-to-artificial-neural-networks/
  19. https://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  20. https://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  21. https://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  22. https://rubikscode.net/2018/01/29/implementing-simple-neural-network-in-c/
  23. https://rubikscode.net/2018/02/05/introduction-to-tensorflow-with-python-example/
  24. https://rubikscode.net/2018/02/12/implementing-simple-neural-network-using-keras-with-python-example/
  25. https://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  26. https://rubikscode.net/2018/03/05/implementation-of-convolutional-neural-network-using-python-and-keras/
  27. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  28. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
  29. https://rubikscode.net/2018/03/26/two-ways-to-implement-lstm-network-using-python-with-tensorflow-and-keras/
  30. http://rtrtr.com/
  31. https://rubikscode.net/2018/12/10/introduction-to-generative-adversarial-networks-gans/
  32. https://rubikscode.net/2018/12/17/implementing-gan-dcgan-with-python/
  33. https://rubikscode.net/2019/01/14/introduction-to-adversarial-autoencoders/
  34. https://rubikscode.net/2019/01/21/generating-images-using-adversarial-autoencoders-and-python/
  35. https://rubikscode.net/2019/02/11/implementing-cyclegan-using-python/
  36. https://rubikscode.net/2019/02/04/introduction-to-cyclegan/
  37. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
  38. https://rubikscode.net/2019/02/18/ultimate-guide-to-machine-learning-with-ml-net/
  39. https://rubikscode.net/2018/06/18/using-ml-net-introduction-to-machine-learning-and-ml-net/
  40. https://rubikscode.net/2018/06/25/machine-learning-with-ml-net-solving-real-world-regression-problem-bike-sharing-demands/
  41. https://rubikscode.net/2018/07/02/machine-learning-with-ml-net-solving-real-world-classification-problem-wine-quality/
  42. https://rubikscode.net/2018/07/09/machine-learning-in-ml-net-using-machine-learning-model-in-asp-net-mvc-application/
  43. https://rubikscode.net/2018/07/16/machine-learning-with-ml-net-comparing-data-exploration-in-python-with-data-exploration-in-ml-net/
  44. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
  45. https://rubikscode.net/2018/05/21/asynchronous-programming-in-net-motivation-and-unit-testing/
  46. https://rubikscode.net/2018/05/28/asynchronous-programming-in-net-common-mistakes-and-best-practices/
  47. https://rubikscode.net/2018/06/04/asynchronous-programming-in-net-task-based-asynchronous-pattern-tap/
  48. https://rubikscode.net/2018/06/11/asynchronous-programming-in-net-benefits-and-tradeoffs-of-using-valuetask/
  49. https://rubikscode.net/2018/09/26/self-organizing-maps-series/
  50. https://rubikscode.net/2018/08/20/introduction-to-self-organizing-maps/
  51. https://rubikscode.net/2018/08/27/implementing-self-organizing-maps-with-python-and-tensorflow/
  52. https://rubikscode.net/2018/09/17/implementing-self-organizing-maps-with-net-core/
  53. https://rubikscode.net/2018/09/24/credit-card-fraud-detection-using-self-organizing-maps-and-python/
  54. https://rubikscode.net/2019/01/07/restricted-boltzmann-machine-series/
  55. https://rubikscode.net/2018/10/01/introduction-to-restricted-boltzmann-machines/
  56. https://rubikscode.net/2018/10/22/implementing-restricted-boltzmann-machine-with-python-and-tensorflow/
  57. https://rubikscode.net/2018/10/15/implementing-restricted-boltzmann-machine-with-net-core/
  58. https://rubikscode.net/2018/11/12/generate-music-using-tensorflow-and-python/
  59. https://rubikscode.net/2017/10/16/mongodb-serial/
  60. https://rubikscode.net/2017/07/19/introduction-to-nosql-and-polyglot-persistence/
  61. https://rubikscode.net/2017/07/24/mongo-db-basics-part-1/
  62. https://rubikscode.net/2017/07/31/mongo-db-basics-part-2/
  63. https://rubikscode.net/2017/10/02/using-mongodb-in-c/
  64. https://rubikscode.net/2017/10/09/mean-stack-crash-course-using-mongodb-with-node-js-express-and-angular-4/
  65. https://rubikscode.net/2017/10/15/serverless-and-mongodb-atlas/
  66. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
  67. https://rubikscode.net/2018/04/23/how-to-use-miyamoto-musashis-philosophy-to-become-better-software-crafter/
  68. https://rubikscode.net/2017/11/06/how-to-use-marcus-aureliuss-meditations-to-become-better-software-craftsman/
  69. https://rubikscode.net/2017/06/22/   how-to-use-friedrich-nietzsches-philosophy-to-be-better-software-craftsman/
  70. https://rubikscode.net/2017/05/14/how-to-use-art-of-war-to-be-better-software-craftsman/
  71. https://rubikscode.net/category/ai/
  72. https://rubikscode.net/category/machine-learning/
  73. https://rubikscode.net/category/net/
  74. https://rubikscode.net/category/python/
  75. https://rubikscode.net/category/nosql/
  76. https://rubikscode.net/about/
  77. https://rubikscode.net/contact/
  78. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
  79. https://rubikscode.net/author/rubikscode/
  80. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/#comments
  81. http://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  82. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  83. http://rubikscode.net/2018/02/26/introduction-to-convolutional-neural-networks/
  84. http://arxiv.org/abs/1502.04623
  85. http://rubikscode.net/2017/11/20/common-neural-network-activation-functions/
  86. http://rubikscode.net/2018/01/15/how-artificial-neural-networks-learn/
  87. http://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  88. http://rubikscode.net/2018/01/22/id26-algorithm-in-artificial-neural-networks/
  89. https://cs.stanford.edu/people/karpathy/deepimagesent/
  90. http://rubikscode.net/2018/02/19/artificial-neural-networks-series/
  91. https://rubikscode.net/
  92. https://www.codeproject.com/
  93. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/?share=twitter
  94. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/?share=facebook
  95. https://rubikscode.net/category/ai/
  96. https://rubikscode.net/tag/ai/
  97. https://rubikscode.net/tag/deep-learning/
  98. https://rubikscode.net/tag/machine-learning/
  99. https://rubikscode.net/tag/neural-networks/
 100. https://rubikscode.net/tag/recurrent-neural-networks/
 101. https://rubikscode.net/tag/software-development/
 102. https://rubikscode.net/2018/03/05/implementation-of-convolutional-neural-network-using-python-and-keras/
 103. https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/
 104. https://collectiv3intelligenc3.wordpress.com/2018/03/14/introduction-to-recurrent-neural-networks/
 105. https://indiz.de/2018/03/introduction-to-recurrent-neural-networks/
 106. http://www.contardi.eu/2018/03/20/introduction-to-recurrent-neural-networks-id56-https-rubikscode-net-2018-03-12-introuduction-to-recurrent-neural-networks-bigdata-datascience-ai-machinelearning-deeple
 107. http://devepar.com/archives/2303
 108. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/#comment-1085
 109. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/?replytocom=1085#respond
 110. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/#comment-1088
 111. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/?replytocom=1088#respond
 112. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/#comment-1144
 113. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/?replytocom=1144#respond
 114. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/#comment-1145
 115. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/?replytocom=1145#respond
 116. https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/#respond
 117. https://jetpack.wordpress.com/jetpack-comment/?blogid=128253944&postid=8027&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=identicon&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.2.1&show_cookie_consent=10&has_cookie_consent=0&sig=107bd31eb45030a66dc596cbe0e9297265ad2fde#parent=https://rubikscode.net/2018/03/12/introuduction-to-recurrent-neural-networks/
 118. https://akismet.com/privacy/
 119. https://www.linkedin.com/in/nmzivkovic/
 120. https://twitter.com/nmzivkovic
 121. https://github.com/nmzivkovic
 122. https://www.facebook.com/rubikscodenet
 123. https://automattic.com/cookies/
 124. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 125. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 126. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
 127. https://rubikscode.net/2018/02/19/artificial-neural-networks-series/
 128. https://rubikscode.net/2018/07/23/machine-learning-with-ml-net-series/
 129. https://rubikscode.net/2018/08/06/asynchronous-programming-in-net/
 130. https://rubikscode.net/2017/10/16/mongodb-serial/
 131. https://rubikscode.net/2018/04/30/philosophy-as-motivational-tool-for-software-crafters-series/
 132. https://rubikscode.net/about/
 133. https://rubikscode.net/contact/
 134. https://www.linkedin.com/in/nmzivkovic/
 135. https://twitter.com/nmzivkovic?lang=en
 136. https://github.com/nmzivkovic
 137. https://www.facebook.com/rubikscodenet/
 138. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914

   hidden links:
 140. https://rubikscode.net/
 141. https://www.packtpub.com/application-development/introducing-test-driven-development-c-video
 142. https://www.packtpub.com/application-development/test-driven-development-c-and-net-core-mvc-video
 143. https://www.packtpub.com/big-data-and-business-intelligence/real-world-machine-learning-projects-scikit-learn-video
