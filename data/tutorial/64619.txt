   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

only numpy: vanilla recurrent neural network with activation deriving back
propagation through time practice         part 2/2

   [9]go to the profile of jae duk seo
   [10]jae duk seo (button) blockedunblock (button) followfollowing
   dec 23, 2017

   so today we are going to do the same thing but add one additional
   component which is activation function. for now lets keep it simple and
   use logistic function. (note we will use the notation log(), and when
   implemented in python it could look like something below.)
import numpy as np
function log(x):
   return 1 / ( 1 + np.exp(-1 *x))
function d_log(x):
   return log(x) * (1 - log(x))

   anyways here is the starting point, the training data (x) and ground
   truth data (y) and what we wish to do, count how many ones are there in
   a given line of data.
   [1*bsqwxvibbytlcnmoeadkaa.jpeg]

   but here is the difference, the relationship between current state and
   the next state is defined below.
   [1*abp7ucw_sdymxrb8czhhaw.jpeg]

   (e) is the equation defining the relationship between current state k
   and previous state k-1. now let   s perform forward feed process, that is
   defined at (f). but wait, state 3 does not have a log() function? why?

   logistic function outputs a number between 0 and 1, this means if we
   have more than one 1, the neural network cannot predict the correct
   number of ones!
     __________________________________________________________________

   before moving on i want to do something different, to explain back
   propagation easier i want to divide some terms.
   [1*y_updjwovlggptcyiuobsg.jpeg]

   so the state can have state (out) in which means the output of the
   state, and state (in) in which means the input of the state.

   now lets perform back propagation, again we are going to follow the
   same step as the first tutorial. derive derivative respect to each wx
   and wrec at each time stamp.
   [1*0glgspc107int1ll0partw.jpeg]

   just like my previous post, try to derive the derivatives for time
   stamp at 1, by yourself. (it will really help). also, can you spot the
   difference between having an activation function and not?
   [1*n6qx50kmnzvfjphnespp0g.jpeg]

   the colored boxes are the regions where we get the derivative for
   id180, and that is it!

   unfortunately, for now i don   t have a code to go with this math
   however, i still have a video tutorial as well as code for vanilla id56
   without the id180.

   update dec 23,2017 : here is an interactive code [11]follow this link

   code it github: [12]link

   iframe: [13]/media/4aab296d5fcb47d6104d7951580b6e2d?postid=4110964a9316

   code:
   [14]https://github.com/jaedukseo/only_numpy_basic/blob/master/id56/a_id56
   _simple.py

   hope you learned something!

   for more tutorial check out my website and my youtube channel!

   website: [15]https://jaedukseo.me/

   youtube channel: [16]https://www.youtube.com/c/jaedukseo

     * [17]machine learning
     * [18]recurrent neural network
     * [19]id26
     * [20]id56
     * [21]ai

   (button)
   (button)
   (button) 39 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of jae duk seo

[23]jae duk seo

   [24]https://jaedukseo.me | | | | |your everyday seo, who likes kimchi

     * (button)
       (button) 39
     * (button)
     *
     *

   [25]go to the profile of jae duk seo
   never miss a story from jae duk seo, when you sign up for medium.
   [26]learn more
   never miss a story from jae duk seo
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4110964a9316
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@seojaeduk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@seojaeduk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@seojaeduk?source=post_header_lockup
  10. https://medium.com/@seojaeduk
  11. https://trinket.io/python3/2870d970f2
  12. https://github.com/jaedukseo/only_numpy_basic/blob/master/id56/b_vanilla_id56_with_activation.py
  13. https://medium.com/media/4aab296d5fcb47d6104d7951580b6e2d?postid=4110964a9316
  14. https://github.com/jaedukseo/only_numpy_basic/blob/master/id56/a_id56_simple.py
  15. https://jaedukseo.me/
  16. https://www.youtube.com/c/jaedukseo
  17. https://medium.com/tag/machine-learning?source=post
  18. https://medium.com/tag/recurrent-neural-network?source=post
  19. https://medium.com/tag/id26?source=post
  20. https://medium.com/tag/id56?source=post
  21. https://medium.com/tag/ai?source=post
  22. https://medium.com/@seojaeduk?source=footer_card
  23. https://medium.com/@seojaeduk
  24. https://jaedukseo.me/
  25. https://medium.com/@seojaeduk
  26. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  28. https://medium.com/p/4110964a9316/share/twitter
  29. https://medium.com/p/4110964a9316/share/facebook
  30. https://medium.com/p/4110964a9316/share/twitter
  31. https://medium.com/p/4110964a9316/share/facebook
