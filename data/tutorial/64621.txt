   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

intro to data science

   [16]go to the profile of tiffany souterre
   [17]tiffany souterre (button) blockedunblock (button) followfollowing
   mar 20, 2018

part 2: data wrangling

     * [18]part 1: numpy and pandas
     * [19]part 3: data analysis

   [1*mqaubsf8xolr2oa2nlpfgg.jpeg]
   photo by [20]markus spiske on [21]unsplash

   real world data are often messy and unorganized. one of the most
   important skill that a data scientist should have is the ability to
   extract and clean data. this is usually refered to as data wrangling or
   data munching.

   three of the most common sources from which we can get data are from
   files, from databases or from websites through web apis.

   if we want to process and analyze our data, it is crucial to understand
   the structure of the data itself.

   in part 2 we will learn:
     * the format of data we can acquire online and their structure (csv,
       xml, json)
     * how to load a csv file into a pandas data frame (pd.csv_read())
     * why are relation database useful ? (complex queries, scalability,
       schema)
     * how to formulate simple and complex queries (select, from, limit,
       where, group by, aggregation functions, pandasql.sqldf())
     * what is an api and how to get data from them (request.get(),
       json.loads())
     * sanity check data (describe(), partial deletion, imputation,
       fillna())

acquiring data

   a lot of data online is stored in text files, especially on government
   websites, and it   s often just a matter of downloading files from a
   website. for example, let   s get the database of all major league
   baseball statistics that we can find [22]here. on the page, we can see
   that the data is available in a variety of formats. three of the most
   common formats are csv, xml and json.

   i downloaded the data in csv named    [23]2016         comma-delimited
   version         updated february 26, 2017   . when looking at the file
   allstarfull.csv, the two first lines are :
playerid,yearid,gamenum,gameid,teamid,lgid,gp,startingpos
gomezle01,1933,0,als193307060,nya,al,,1

   in csv format, we usually have a series of rows, with each row
   corresponding to an entry. entries are separated by a comma. the header
   row at the top of the file corresponds to identifiers such as player
   id, yearid    if we look at the entry for the first player, we can see
   the corresponding data in the same order. if a data is missing, we
   would simply see two commas following each other eih no space.

   in the case of an xml document, we end up with something that closely
   resembles html. we can have a document element, which is opened and we
   can have a series of tables, which are also opened. the table has a
   number of children which corresponds to the values discussed above. if
   a data is missing, the field is open with a slash at the end (for
   example : <playerid/>)
<document element>
  <table>
    <playerid>gomezle01</playerid>
    <yearid>1933</yearid>
    <gamenum>0</gamenum>
    <gameid>als193307060</gameid>
    <teamid>nya</teamid>
    <lgid>al</lgid>
    <gp/>
    <startingpos>1</startingpos>

   in the json document, we have a number of json objects indicated in
   curly brackets. a json document is a lot like a python dictionary. we
   have keys, corresponding to the header row in a csv file, followed by
   values. in the case of missing values, we simply open and close the
   quotation marks
{
  "playerid":"gomezle01"
  "yearid":1933
  "gamenum":0
  "gameid":"als193307060"
  "teamid":"nya"
  "lgid":"al"
  "gp":""
  "startingpos":1
}

   the benefit of xml and json is that they support nested structure in a
   way that csv documents cannot although csv still is a very popular way
   to store data.

csv data

   loading csv data into a pandas data frame can be done in one line of
   code. each column can be retrived by mentioning their name. we can also
   create new columns on the data frame by manipulating the columns in a
   vectorized way. for example, if we wanted a new column that was the sum
   of each player   s height and weight, we can write the arithmetic of the
   to column as shown below.

   it is also possible to write the data in a new csv file that include
   the new column we just added with the to_csv() function.

   iframe: [24]/media/f5aeb57ea38ba36bbade0839e49bcf62?postid=75835b9129b4

>>>
    playerid  birthyear  birthmonth  birthday birthcountry ...
0  aardsda01     1981.0        12.0      27.0          usa ...
1  aaronha01     1934.0         2.0       5.0          usa ...
2  aaronto01     1939.0         8.0       5.0          usa ...
3   aasedo01     1954.0         9.0       8.0          usa ...
4   abadan01     1972.0         8.0      25.0          usa ...
>>>
0    290.0
1    252.0
2    265.0
3    265.0
4    257.0

id208

   we now know how to load data from flat files like csv. now let   s see
   how to work with data stored in id208. a relational
   database is similar to a collection of spreadsheets. in each
   spreadsheet, there are columns and rows. a column specifies a value and
   its type such as playerid. each row contains values for each column, we
   call each set of rows and columns a table rather than a spreadsheet and
   the tables are usually related to each other in some way.

   id208 are useful for three main reasons :
     * it is straightforward to extract data with complex queries.
     * it scales well
     * it ensures all data is consistently formatted

   indeed, selecting all records for people where their age is > 50, and
   their weight is < 50, and the city is equal to mumbai is easily done
   with databases compared to flat files.

   it   s not uncommon to have databases with hundreds of thousands or
   millions of entries. since all information is ideally stored in one
   location, it   s easy to update, delete, and add new data to the database
   in a scalable way.

   finally, id208 always have a schema. a schema is a
   blueprint that tells the database how we plan to store our data. for
   instance, people   s age cannot be a string while the age of others is an
   int. also, a schema says that for a given table, every single row or
   entry will have the exact same number of columns that correspond to the
   same values and that each column   s value will be formatted in the same
   way.

queries

   how do we get the data from a relational database ? data is usually
   retrieved from a relational database using structures query language
   (sql). if we want to select all (*) of the data, the query would be:
select * from database_name;

   we can limit the number of rows by just adding limit 20 at the end of
   our sql command.
select * from database_name limit 20;

   we can also just ask for specific columns such as name and age.
select name, age from database_name limit 20;

   let   s write this query in a python file and store the data in
      database_solution   .

   iframe: [25]/media/d12536a1196e85810b683286b4e69063?postid=75835b9129b4

   we can also perform more complex queries. the following query selects
   all data corresponding to the country iceland.
select * from database_name where country = "iceland";

   some function such as group by and aggregate exist in sql. an aggregate
   function takes some set of values, usually numbers and performs a
   mathematical operation on them. for example sum() is an aggregate
   function.

   what are the total number of people over the age of 50 enrolled in the
   database per district? the following query first selects how many men
   and women in each district are enrolled. since we   re using an aggregate
   function, we need to include a group by, with our non-aggregated
   fields, in this case, district and gender. finally, we want to limit
   this to men and women over the age of 50. so, we include the where
   clause, where age > 50, after the table name.
select gender, district, sum(database_generated)
from database_name
where age > 50
group by gender, district

apis (application programming interface)

   we now know how to acquire data from a file or a database, but what
   about data that sites in some website like twitter ? getting all the
   data by searching or id190site could get complex. luckily,
   many companies allow users and developers to access data directly in a
   simple, machine readable format through apis. there are several
   different kinds of apis, but one of the most common types, and the one
   used by twitter, is a representational state transfer or rest api.

   let   s use [26]last.fm api as an example and see how we can interact
   with it. as you can see on the left hand side here, there are a few
   different last.fm api methods that we can talk to and retrieve
   information from. for example, let   s check out the album.getinfo method
   in the purple square in the image.
   [1*qqth-zyskhyzbt_cudmqfw.png]
   [27]last.fm api

   when clicking on [28]album.getinfo link, we see a page describing the
   type of data that the api method will return like artist name, album
   name   
   [1*bs9yulzihjxghfitzfdjfg.png]

   as you can see this page tells us the type of data that the api method
   will return like artist name, album name and a bunch of other
   information such as language or a music brains id from the album.

   but how do we get the data from the api ?
    1. get an [29]api account. you will then receive an api key.
    2. go back to the [30]album.getinfo page and click on one of the
       example urls (in purple square)
    3. you should see an error message saying you need an api key.

   [1*fmhkloffucty5v6sxnvmsa.png]

   4. copy your api key and paste it in the url instead of your_api_key.
   now you should see the data.
   [1*ajgxmccy0me04b8x09vclw.png]

   note there are a few interesting things about the url.

   [31]http://ws.audioscrobbler.com/2.0/?method=album.getinfo&api_key=your
   _api_key&artist=cher&album=believe

   api parameters are defined after the question mark such as method,
   api_key, artist and album. for example if we want the information on
   the album loud from rihanna we can simply write:

   [32]http://ws.audioscrobbler.com/2.0/?method=album.getinfo&api_key=your
   _api_key&artist=rihanna&album=loud

   now let   s implement this in a simple python program that uses the json
   and request libraries.

   iframe: [33]/media/79ce9280333af6781dbb0ab153d7d02e?postid=75835b9129b4

   first we specify a url as we saw previously. the function
   requests.get() will get the data from the url in a .txt. looking at the
   json object we would see a string format pretty hard to work with, but
   the json library allows for very simple interaction with the json data
   thanks to the json.load() function. now the json data is converted into
   a python dictionary.

sanity checking data

   now that we have acquired our data, whether the informal flat file
   sequel like relational database, or an api, we need to do sanity check
   data before doing any analysis. sanity checking allow us to determine
   if the data make sense. we will not go too far into the details but to
   do just the bare amount of sanity checking, pandas dataframes do have a
   useful method called describe().

   to illustrate how panda   s describe function works, we   ll use the
   baseball_data from above.

   iframe: [34]/media/d9803e945205f76d6f5099ccd434883f?postid=75835b9129b4

>>>birthyear  birthmonth  birthday  deathyear  deathmonth
count  18973        18803     18656       9441        9440
mean    1931            6        15       1964           6
std       41            3         8         31           3
min     1820            1         1       1872           1
25%     1895            4         8       1942           3
50%     1937            7        16       1967           6
75%     1969           10        23       1990          10
max     1996           12        31       2017          12
    deathday        weight    height
count   9439         18251    18320
mean      15           186       72
std        8            21        2
min        1            65       43
25%        8           170       71
50%       15           185       72
75%       23           200       74
max       31           320       83

   the function describe() returns a data frame. for every numerical
   column, we see the count, mean, standard deviation, mean, values. we
   can do some quick checking to make sure there are data generally make
   sense like month are comprised between 1 and 12, days between 1 and 3.
   are there any outliers ? see if min and max values are way larger than
   the values corresponding with the 25th or 75th percentile   

   one thing you may notice is that the count is different for each
   column. this shows that we may have a bunch of missing values. there
   are a lot of reasons why values might be missing such as failing in
   collecting data, data loss, non response values. missing values can
   invalidate your findings and it   s important to design data collection
   method consequently. different methods can used to mitigate the effect
   of those missing values on our analysis.

   two approaches exist, partial deletion and imputation. partial deletion
   is exactly what it sounds like, limiting our data set for analysis to
   the data that we have available to us. one method we could use is
   called listwise deletion. in the case where we perform listwise
   deletion, we   d exclude a particular data point from all analyses even
   if some useful values were present.

   we use imputation when we don   t have very much data, or where removing
   our missing values would compromise the representativeness of our
   sample. throwing away a bunch of entries just because they   re missing
   values could severely impact the statistical power of whatever analysis
   we were trying to perform. in this case, it likely makes sense to make
   an intelligent guess at the missing values in our data like
   approximation. we could for example replace all missing values by the
   mean of all others or using id75 to estimate the missing
   values. however, imputation introduces biases and inaccuracies into the
   data set. it is a really hard problem and new techniques are constantly
   being developed. more sophisticated and robust methods exist.

   in the following code, we calculate the mean of the    weight    array and
   impute any missing values in the column    weight    by setting them to the
   average weight.

   iframe: [35]/media/5e0f069cc0c08608ec6672e928489982?postid=75835b9129b4

   in the next article we   ll dive into actual data analysis !

     * [36]data science
     * [37]data wrangling
     * [38]api
     * [39]sql
     * [40]json

   (button)
   (button)
   (button) 273 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [41]go to the profile of tiffany souterre

[42]tiffany souterre

   ph.d.     i like to understand things clearly and convey concepts
   efficiently     [43]https://amagash.github.io

     (button) follow
   [44]towards data science

[45]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 273
     * (button)
     *
     *

   [46]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [47]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/75835b9129b4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/intro-to-data-science-part-2-data-wrangling-75835b9129b4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/intro-to-data-science-part-2-data-wrangling-75835b9129b4&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_uim1hhqeblfy---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@tiffanysouterre?source=post_header_lockup
  17. https://towardsdatascience.com/@tiffanysouterre
  18. https://medium.com/@tiffanysouterre/intro-to-data-science-part-1-numpy-and-pandas-49d98740661b
  19. https://towardsdatascience.com/intro-to-data-science-part-3-data-analysis-71a566c3a8c3
  20. https://unsplash.com/photos/xekxe_vr0ec?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  21. https://unsplash.com/search/photos/wrangling?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  22. http://www.seanlahman.com/baseball-archive/statistics
  23. http://seanlahman.com/files/database/baseballdatabank-2017.1.zip
  24. https://towardsdatascience.com/media/f5aeb57ea38ba36bbade0839e49bcf62?postid=75835b9129b4
  25. https://towardsdatascience.com/media/d12536a1196e85810b683286b4e69063?postid=75835b9129b4
  26. https://www.last.fm/api
  27. https://www.last.fm/api
  28. https://www.last.fm/api/show/album.getinfo
  29. https://www.last.fm/api/account/create
  30. https://www.last.fm/api/show/album.getinfo
  31. http://ws.audioscrobbler.com/2.0/?method=album.getinfo&api_key=your_api_key&artist=cher&album=believe
  32. http://ws.audioscrobbler.com/2.0/?method=album.getinfo&api_key=your_api_key&artist=cher&album=believe
  33. https://towardsdatascience.com/media/79ce9280333af6781dbb0ab153d7d02e?postid=75835b9129b4
  34. https://towardsdatascience.com/media/d9803e945205f76d6f5099ccd434883f?postid=75835b9129b4
  35. https://towardsdatascience.com/media/5e0f069cc0c08608ec6672e928489982?postid=75835b9129b4
  36. https://towardsdatascience.com/tagged/data-science?source=post
  37. https://towardsdatascience.com/tagged/data-wrangling?source=post
  38. https://towardsdatascience.com/tagged/api?source=post
  39. https://towardsdatascience.com/tagged/sql?source=post
  40. https://towardsdatascience.com/tagged/json?source=post
  41. https://towardsdatascience.com/@tiffanysouterre?source=footer_card
  42. https://towardsdatascience.com/@tiffanysouterre
  43. https://amagash.github.io/
  44. https://towardsdatascience.com/?source=footer_card
  45. https://towardsdatascience.com/?source=footer_card
  46. https://towardsdatascience.com/
  47. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  49. https://medium.com/p/75835b9129b4/share/twitter
  50. https://medium.com/p/75835b9129b4/share/facebook
  51. https://medium.com/p/75835b9129b4/share/twitter
  52. https://medium.com/p/75835b9129b4/share/facebook
