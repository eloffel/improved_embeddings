   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

intro to data science

   [16]go to the profile of tiffany souterre
   [17]tiffany souterre (button) blockedunblock (button) followfollowing
   mar 27, 2018

part 3: data analysis

     * [18]part 1: numpy and pandas
     * [19]part 2: data wrangling

   [1*bepiwatrjopmiggew7zwma.jpeg]
   photo by [20]markus spiske on [21]unsplash

   we are going to see the basic methods in statistics and machine
   learning to analyze data. statistics allow us to make sure we are doing
   reasonable id136s from data and check for statistical significance,
   understand confidence intervals    they provide a formalized framework
   for comparing and evaluating data and they enable us to evaluate
   whether perceived effects in our dataset reflect differences across the
   whole population.

   in this part we   ll see :
     * t-test for normal distribution (welch   s test)
     * how to deal with non normal data (shapiro-wilk test), non
       parametric test (mann-whitney u test)
     * id75
     * how to implement the id119 in python

statistical test

   many statistical tests that you might use to analyze data make an
   assumption about the id203 distribution that your data will
   follow. there are many different id203 distributions, but one of
   the most common is the normal distribution, which is also sometimes
   referred to as the gaussian distribution or a bell curve.
   [1*rtszblaztmybgkh0plh89q.png]
   normal distribution

   there are two parameters associated with a normal distribution. the
   mean (  ), and the standard deviation (  ). these two parameters plug in
   to a id203 density function, which describes a gaussian
   distribution. a normal distribution is symetric about its mean. knowing
   about the normal distribution would be helpful for understanding
   parametric tests that we will see next.

t-test

   one of the most common parametric test that we might use to compare two
   sets of data is the t-test. a t-test allows to check if there is a
   significant difference between two sets of data. this is all the
   difference between descriptive and inferential statistics. with
   descriptive stats, such as a mean, you can only describe the sample we
   have but we can   t generalize beyond that (e.g if i flip a coin 10
   times, get 7 heads, and a friend gets 5 heads, it doesn   t mean that i   m
   more likely to get head and we can   t say that i will always get more
   heads). inferential statistic, like a t-test, does not just describe
   the sample we have but it also tells us what we can expect from samples
   we don   t have. it allows us to generalize beyond the sample that we are
   testing. the t-test simply measures the difference between the groups
   and compares it to the difference within the groups. the smaller the
   t-value, the more similar the groups are and inversely, the bigger the
   t-value, the more different the groups are. when we want to compare two
   different sample test, we perform a two sample t-test. we   ll discuss a
   variant of this called welch   s t-test that doesn   t assume equal sample
   size or equal variance. in welch   s t-test, we compute a t-statistic
   using the following equation.
   [1*-kxrvqcr8r6hffrbttjpia.png]

   we   ll also want to estimate the number of degrees of freedom (  ), using
   the following equation.
   [1*e-1ikjirsfux47fitax5xw.png]

   once we have these two values, we can estimate the p-value. the p-value
   is the id203 of obtaining the test statistic at least as extreme
   as the one that was actually observed. if the p-value =0.05, this would
   mean that there is a 5% id203 to have a t-value greater or equal.
   the goal of the t-test is to accept or reject a null hypothesis. a null
   hypothesis is a statement that we are trying to disprove by running our
   test. when performing a statistical test, we usually set a p-critical
   value. if p-value < p-critical, then we would reject the null
   hypothesis.

   calculating the p-value can be tedious, thankfully there   s a simple way
   to do it in python. let   s apply this to an example. we want to
   determine if there is a difference between the performance of
   right-handed and left-handed baseball batters. the null hypothesis is
   then    there is no difference between right-handed and left-handed
   batters   .

   first we need to load the baseball dataset into a pandas dataframe

   iframe: [22]/media/729de7b4531b605a9573e26aedfd8351?postid=71a566c3a8c3

           name handedness height weight    avg   hr
0  brandon hyde          r     75    210  0.000    0
1   carey selph          r     69    175  0.277    0
2  philip nastu          l     74    180  0.040    0
3    kent hrbek          l     76    200  0.282  293
4   bill risley          r     74    215  0.000    0

   then we split the data to compare our two subsets (right and
   left-handed)

   iframe: [23]/media/d23c398c24f5d2d55d59ca58e08d2b50?postid=71a566c3a8c3

              name handedness height weight    avg  hr
0     brandon hyde          r     75    210  0.000   0
1      carey selph          r     69    175  0.277   0
4      bill risley          r     74    215  0.000   0
6  steve gajkowski          r     74    200  0.000   0
7        rick schu          r     72    170  0.246  41
               name handedness height weight    avg   hr
2      philip nastu          l     74    180  0.040    0
3        kent hrbek          l     76    200  0.282  293
9      tom browning          l     73    190  0.153    2
13        tom brown          l     70    168  0.265   64
15  floyd bannister          l     73    190  0.175    0

   we perform the t-test with the ttest_ind() function. when specifying
   equal_var = false, we indicate whether or not we think the variance of
   our two samples is equal. this equal_var=false argument makes this
   particular call of the t-test equal to welch   s t-test. this function
   will return a tuple. the first value is the t-value for your data. the
   second value is the corresponding p-value for a two-tailed test. the
   values returned by the function assumes that we   re performing a
   two-sided t-test where we   re only testing whether the means of our two
   samples are different.

   we choose p-critical = 0.05, this means that if the p-value < 0.05 we
   would reject the null hypothesis and we could say that there is no
   significant difference between right-handed and left-handed batters.

   iframe: [24]/media/cb07b2cd0b31d01af5ef696f9d5bc0bf?postid=71a566c3a8c3

ttest_indresult(statistic=-9.935702226242094, pvalue=3.810274225888738e-23)
there is a significant difference
false

non-normal data

   when performing a t-test, we assume that our data is normal. but we can
   also encounter id203 distribution that are not normal. in this
   case, we have to use other statistical tests. if we are uncertain, we
   first need to determine whether or not our data is normal. the
   shapiro-wilk test measures the likelihood that a sample is drawn from a
   normally distributed population. the shapiro() function returns two
   values. the first, is the shapiro-wilk test statistic. the second value
   is our p-value, which should be interpreted in the same way that we
   would interpret the p-value for a t-test. that is, given the null
   hypothesis that this data is drawn from a normal distribution, what is
   the likelihood that we would observe a value of the shapiro-wilk test
   statistic that was at least as extreme as the one that we see?

   iframe: [25]/media/e6888e4e75a5079b2d51eb63f4f1b6a7?postid=71a566c3a8c3

non-parametric test

   let   s say we just determined that our data was non-normal, there still
   are some non-parametric tests that we can use to compare two samples. a
   non-parametric test is a statistical test that does not assume our data
   is drawn from any particular underlying id203 distribution. one
   such test is the mann-whitney u test which is also sometimes referred
   to as the mann-whitney wilcoxon test. this is a test of the null
   hypothesis that two populations are the same. the mannwhitneyu()
   function returns two values, the mann-whitney test statistic, and the
   one sided p-value for this test.

   iframe: [26]/media/fb2ad95b14bed6890010b73016c8af8c?postid=71a566c3a8c3

   these have just been some of the methods that we can use when
   performing statistical tests on data. as you can imagine, there are a
   number of additional ways to handle data from different id203
   distributions or data that looks like it came from no id203
   distribution whatsoever. data scientists can perform many statistical
   procedures. but it   s crucial to understand the underlying structure of
   the data set and consequently, which statistical tests are appropriate
   given the data that we have.

   now that we know to analyze existing data, we can see if there is a way
   to make predictions about the data.

machine learning

   machine learning is a branch of artificial intelligence focusing on the
   construction of systems that learn from large amounts of data to make
   predictions. but what is the difference between statistics and machine
   learning ?

   in short, the answer is not much. more and more, the 2 fields are
   converging and they share many of the same methods. however there are
   couple of significant philosophical differences between the 2 subjects.
   generally, statistics is focused on analysing existing data, in drawing
   valid conclusions, whereas machine learning is focused on making
   predictions. what this translates to is the following: in statistics,
   we care a lot about how our data was collected, and drawing conclusions
   about that existing data using id203 models. for example, we
   might try to answer the question are left handed hitters statistically
   better than right handed ones? in the case of machine learning, we   re a
   little bit more focused on making accurate predictions and less
   committed to using a id203 model if there   s a more accurate
   method that doesn   t use them at all. as long as our machine learning
   method consistently makes accurate predictions for example, how many
   home runs will a player hit, we aren   t too worried about what
   assumptions the model makes.

   there are many different types of machine learning problems, but two
   common ones are supervised learning and unsupervised learning. machine
   learning typically involves generating some type of model regarding the
   problem we   re trying to solve. we   ll feed data into this model and then
   try to make predictions. in supervised learning, there are labeled
   inputs that we train our model on. training our model welch   s
   testsimply means teaching the model what the answer looks like. an
   example of supervised learning would be estimating the cost of a new
   house, given that we have a number of examples where we know about a
   bunch of features like the square footage, or the number of rooms, or
   the location. and we also know how much that house sold for. we could
   train a model, and then predict how much a future house will sell for,
   given we know all the same parameters. this is an example of
   regression.

   when performing unsupervised learning, we don   t have any such training
   examples. instead we have a bunch of unlabeled data points, and we   re
   trying to understand the structure of the data, often by id91
   similar data points together. for example, if we were to feed an
   unsupervised learning algorithm a bunch of photos, it might split the
   photos into groups, say photos of people, photos of horses, photos of
   buildings, without being told a priori what those groups should be. it
   might not know that the groups are people, or horses, or buildings, but
   it can tell that these distinct groups exist.

id75

   going back to our baseball dataset, we would like to create a model
   that predicts the lifetime number of home runs of a player that is not
   in our dataset. one way that we might be able to solve this problem is
   using id75 and one basic implementation on machine
   learning is to perform id75 by using id119.

   when performing id75, we usually have a number of data
   points (1 through m in the fidure below). each data point has an output
   variable (y) and then a number of input variables (x1 through xn). so
   in our baseball example, y here is lifetime number of home runs, and
   our x1 through xn are things like their height and weight.
   [1*0-n8crner4knx4ob4cdd3q.png]

   the goal is to build a model that predicts values of the output
   variable for each data point by multiplying the input variables by some
   set of coefficients (  1 through   n). we call each    a parameter or a
   weight of our model. it indicates how important is a certain input
   variable x for the prediction of the output variable y. this model is
   built in a way that we multiply each x by the corresponding   , and add
   them up to get y. as you can see from the equation, the smaller   , the
   smaller the product of    and x and inversely.    is calculated so that
   it is small for the input x that are not very important in predicting y
   and big for those who are big contributors. the best equation is the
   one that minimizes the difference across all data points between our
   predicted y and our observed y. what we need to do is find the   s that
   produce the best predictions, that is minimizing this difference thanks
   to id119.
   [1*f4f0nfoiq6bnjrvmp5e2dg.png]

   in order to do id119, we   re first going to need to define
   the following cost function j(  ).
   [1*qaujjzhteinytqnobksgyg.png]

   the cost function is meant to provide a measure of how well our current
   set of   s does at modeling our data. so we want to minimize the value
   of the cost function. the cost function is simply half of the sum of
   the squared errors as we saw in the previous figure. note that x
   superscript i represents our entire collection of x   s. so y predicted
   is the equivalent of the sum of all the products of x and    as we saw
   two figures above.

   so how do we find the correct values of    to minimize our cost function
   j(  )? id119 is an algorithm that takes some initial guess
   for    and iteratively changes   ; such that j(  ) keeps on getting
   smaller and smaller until it converges to some minimum value.
   [1*7-xyceptoa4nnkxwhd192a.png]

implementing the id119 algorithm

   now let   s implement the basic functions of the id119
   algorithm to find the boundary in a small dataset. first, we   ll start
   with the plot_line() function that will help us plot and visualize the
   data. note that the gradient_descent() function takes alpha as a
   parameters. alpha is called the learning rate and basically sets the
   steps at each iteration. taking big step in updating the weights can
   make you overshoot the rights equation. on the other hand taking small
   steps increases the number of iteration you need to reach the
   convergence. it usually involves a fine tuning process to find the
   right balance.

   iframe: [27]/media/57d04c54246bab472a57dfb9e4f625ef?postid=71a566c3a8c3

   the dataset, should look like that :
   [1*4ek3cze3bxlnkmjfu-49iq.png]

   i plotted the initialized line in blue. this line is usually set
   randomly
   [1*ql_qrawrirsrkhdbadd8fg.png]

   every 10 iterations (=epochs), i printed the cost. remember that the
   cost should always decrease until reaching the lowest possible point
   (convergence).
('\n========== epoch', 0, '==========')
cost:  16.004954
('\n========== epoch', 10, '==========')
cost:  7.52057727374
('\n========== epoch', 20, '==========')
cost:  3.71424722592
('\n========== epoch', 30, '==========')
cost:  2.0064263714
('\n========== epoch', 40, '==========')
cost:  1.23996949709
('\n========== epoch', 50, '==========')
cost:  0.895797093343
('\n========== epoch', 60, '==========')
cost:  0.741057191151
('\n========== epoch', 70, '==========')
cost:  0.671295578372
('\n========== epoch', 80, '==========')
cost:  0.639655519089
('\n========== epoch', 90, '==========')
cost:  0.625117716849

   here we can see the different steps where the line was updated towards
   a better regression. feel free to play with this code and tune the
   parameters differently. you can change the epochs, the learning rate
   (alpha), change the dataset   
   [1*jw20kwsrf1pnptazwqpsdq.png]

   the next article will be about data visualization.

     * [28]machine learning
     * [29]data analysis
     * [30]data science
     * [31]towards data science
     * [32]id119

   (button)
   (button)
   (button) 268 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of tiffany souterre

[34]tiffany souterre

   ph.d.     i like to understand things clearly and convey concepts
   efficiently     [35]https://amagash.github.io

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 268
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/71a566c3a8c3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/intro-to-data-science-part-3-data-analysis-71a566c3a8c3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/intro-to-data-science-part-3-data-analysis-71a566c3a8c3&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_padhzkezadwe---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@tiffanysouterre?source=post_header_lockup
  17. https://towardsdatascience.com/@tiffanysouterre
  18. https://medium.com/@tiffanysouterre/intro-to-data-science-part-1-numpy-and-pandas-49d98740661b
  19. https://medium.com/@tiffanysouterre/intro-to-data-science-part-2-data-wrangling-75835b9129b4
  20. https://unsplash.com/photos/skf7hxarcoc?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  21. https://unsplash.com/search/photos/data?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  22. https://towardsdatascience.com/media/729de7b4531b605a9573e26aedfd8351?postid=71a566c3a8c3
  23. https://towardsdatascience.com/media/d23c398c24f5d2d55d59ca58e08d2b50?postid=71a566c3a8c3
  24. https://towardsdatascience.com/media/cb07b2cd0b31d01af5ef696f9d5bc0bf?postid=71a566c3a8c3
  25. https://towardsdatascience.com/media/e6888e4e75a5079b2d51eb63f4f1b6a7?postid=71a566c3a8c3
  26. https://towardsdatascience.com/media/fb2ad95b14bed6890010b73016c8af8c?postid=71a566c3a8c3
  27. https://towardsdatascience.com/media/57d04c54246bab472a57dfb9e4f625ef?postid=71a566c3a8c3
  28. https://towardsdatascience.com/tagged/machine-learning?source=post
  29. https://towardsdatascience.com/tagged/data-analysis?source=post
  30. https://towardsdatascience.com/tagged/data-science?source=post
  31. https://towardsdatascience.com/tagged/towards-data-science?source=post
  32. https://towardsdatascience.com/tagged/gradient-descent?source=post
  33. https://towardsdatascience.com/@tiffanysouterre?source=footer_card
  34. https://towardsdatascience.com/@tiffanysouterre
  35. https://amagash.github.io/
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/71a566c3a8c3/share/twitter
  42. https://medium.com/p/71a566c3a8c3/share/facebook
  43. https://medium.com/p/71a566c3a8c3/share/twitter
  44. https://medium.com/p/71a566c3a8c3/share/facebook
