    #[1]cpuheater    feed [2]cpuheater    comments feed [3]cpuheater   
   introduction to recurrent neural networks in pytorch comments feed
   [4]alternate [5]alternate

   [6]skip to main content

   (button) toggle navigation

   [7]cpuheater
     * [8]home
     * [9]about
     * [10]contact

introduction to recurrent neural networks in pytorch

   [11]1st december 201722nd march 2018 [12]cpuheater [13]deep learning

   this tutorial is intended for someone who wants to understand how
   recurrent neural network works, no prior knowledge about id56 is
   required. we will implement the most simple id56 model     elman recurrent
   neural network. to get a better understanding of id56s, we will build it
   from scratch using pytorch tensor package and autograd library.

   i assume that you have some understanding of feed-forward neural
   network if you are new to pytorch and autograd library checkout
   my [14]tutorial.

elman recurrent neural network

   an elman network was introduced by [15]jeff elman, and was first
   published in a paper entitled [16]finding structure in time.  it   s just
   a  three-layer feed-forward network, in our case, input layer consist
   of one input neuron \(x_{1}\)  and additional units called context
   neurons \(c_{1}\)     \(c_{n}\). context neurons receive input from the
   hidden layer neurons, from previous time step. we have one context
   neuron per neuron in the hidden layer. since the state from previous
   time step is provided as a part of the input, we can say that network
   has a form of memory, context neurons represent a memory.

predicting the sine wave

   we will train our id56 to learn sine function. during training we will
   be feeding our model with one data point at a time, that is why we need
   only one input neuron  \(x_{1}\), and we want to predict the value at
   next time step. our input sequence x consists of 20 data points, and
   the target sequence is the same as the input sequence but it    s shifted
   by one-time step into the future.

implementing the model

   we start by importing the necessary packages.

   import torch________________________________________________
   from torch.autograd import variable_________________________
   import numpy as np__________________________________________
   import pylab as pl__________________________________________
   import torch.nn.init as init________________________________
   1
   2
   3
   4
   5
   import torch
   from torch.autograd import variable
   import numpy as np
   import pylab as pl
   import torch.nn.init as init

   next, we   ll set the model hyperparameters,  the size of the input layer
   is set to 7, which means that we will have 6 context neurons and 1
   input neuron, seq_length defines the length of our input and target
   sequence.

   dtype = torch.floattensor___________________________________
   input_size, hidden_size, output_size = 7, 6, 1______________
   epochs = 300________________________________________________
   seq_length = 20_____________________________________________
   lr = 0.1____________________________________________________
   1
   2
   3
   4
   5
   dtype = torch.floattensor
   input_size, hidden_size, output_size = 7, 6, 1
   epochs = 300
   seq_length = 20
   lr = 0.1

   now we will generate the training data, where x is an input
   sequence and y is a target sequence.

   data_time_steps = np.linspace(2, 10, seq_length + 1)________
   data = np.sin(data_time_steps)______________________________
   data.resize((seq_length + 1, 1))____________________________
   ____________________________________________________________
   x = variable(torch.tensor(data[:-1]).type(dtype), requires_g
   y = variable(torch.tensor(data[1:]).type(dtype), requires_gr
   1
   2
   3
   4
   5
   6
   data_time_steps = np.linspace(2, 10, seq_length + 1)
   data = np.sin(data_time_steps)
   data.resize((seq_length + 1, 1))

   x = variable(torch.tensor(data[:-1]).type(dtype), requires_grad=false)
   y = variable(torch.tensor(data[1:]).type(dtype), requires_grad=false)

   we need to create two weight matrices, w1 of size (input_size,
   hidden_size) for input to hidden connections, and a w2 matrix of size
   (hidden_size, output_size) for hidden to output connection. weights are
   initialized using a normal distribution with zero mean.

   w1 = torch.floattensor(input_size, hidden_size).type(dtype)_
   init.normal(w1, 0.0, 0.4)___________________________________
   w1 =  variable(w1, requires_grad=true)______________________
   w2 = torch.floattensor(hidden_size, output_size).type(dtype)
   init.normal(w2, 0.0, 0.3)___________________________________
   w2 = variable(w2, requires_grad=true)_______________________
   1
   2
   3
   4
   5
   6
   w1 = torch.floattensor(input_size, hidden_size).type(dtype)
   init.normal(w1, 0.0, 0.4)
   w1 =  variable(w1, requires_grad=true)
   w2 = torch.floattensor(hidden_size, output_size).type(dtype)
   init.normal(w2, 0.0, 0.3)
   w2 = variable(w2, requires_grad=true)

   we can now define forward method, it takes input vector, context_state
   vector and two weights matrices as arguments. we will create vector xh
   by concatenating input vector with the context_state vector. we perform
   dot product between the xh vector and weight matrix w1, then
   apply tanh function as nonlinearity, which works better with id56s than
   sigmoid. then we perform another dot product between new context_state
   and weight matrix w2. we want to predict continuous value, so we do not
   apply any nonlinearity at this stage.

   note that context_state vector will be used to populate context neurons
   at the next time step. that is why we return context_state vector along
   with the output of the network.

     this is where the magic happens, context_state vector summarizes the
     history of the sequence it has seen so far.


   def forward(input, context_state, w1, w2):__________________
     xh = torch.cat((input, context_state), 1)_________________
     context_state = torch.tanh(xh.mm(w1))_____________________
     out = context_state.mm(w2)________________________________
     return  (out, context_state)______________________________
   1
   2
   3
   4
   5
   def forward(input, context_state, w1, w2):
     xh = torch.cat((input, context_state), 1)
     context_state = torch.tanh(xh.mm(w1))
     out = context_state.mm(w2)
     return  (out, context_state)

training

   our training loop will be structured as follows.
     * the outer loop iterates over each epoch. epoch is defined as one
       pass of all training data. at the beginning of each epoch, we need
       to initialize our context_state vector with zeros.
     * the inner loop runs through each element of the sequence. we run
       forward method to perform forward pass which returns prediction and
       context_state which will be used for next time step. then we
       compute mean square error (mse),  which is a natural choice when we
       want to predict continuous values.  by running backward() method on
       the loss we calculating gradients, then we update the weights.
       we   re supposed to clear the gradients at each iteration by calling
       zero_() method otherwise gradients will be accumulated. the last
       thing we do is wrapping context_state vector in new variable, to
       detach it from its history.


   for i in range(epochs):_____________________________________
     total_loss = 0____________________________________________
     context_state = variable(torch.zeros((1, hidden_size)).typ
     for j in range(x.size(0)):________________________________
       input = x[j:(j+1)]______________________________________
       target = y[j:(j+1)]_____________________________________
       (pred, context_state) = forward(input, context_state, w1
       loss = (pred - target).pow(2).sum()/2___________________
       total_loss += loss______________________________________
       loss.backward()_________________________________________
       w1.data -= lr * w1.grad.data____________________________
       w2.data -= lr * w2.grad.data____________________________
       w1.grad.data.zero_()____________________________________
       w2.grad.data.zero_()____________________________________
       context_state = variable(context_state.data)____________
     if i % 10 == 0:___________________________________________
        print("epoch: {} loss {}".format(i, total_loss.data[0])
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   for i in range(epochs):
     total_loss = 0
     context_state = variable(torch.zeros((1, hidden_size)).type(dtype),
   requires_grad=true)
     for j in range(x.size(0)):
       input = x[j:(j+1)]
       target = y[j:(j+1)]
       (pred, context_state) = forward(input, context_state, w1, w2)
       loss = (pred - target).pow(2).sum()/2
       total_loss += loss
       loss.backward()
       w1.data -= lr * w1.grad.data
       w2.data -= lr * w2.grad.data
       w1.grad.data.zero_()
       w2.grad.data.zero_()
       context_state = variable(context_state.data)
     if i % 10 == 0:
        print("epoch: {} loss {}".format(i, total_loss.data[0]))

   the output generated during training shows how the loss is decreasing
   with every epoch, which is a good sign. decaying loss means that our
   model is learning.

   epoch: 0 loss 2.777482271194458
   epoch: 10 loss 0.10264662653207779
   epoch: 20 loss 0.1178232803940773
      
   epoch: 280 loss 0.005524573381990194
   epoch: 290 loss 0.005174985621124506

making predictions

   once our model is trained, we can make predictions, at each step of the
   sequence we will feed the model with single data point and ask the
   model to predict one value at the next time step.

   context_state = variable(torch.zeros((1, hidden_size)).type(
   predictions = []____________________________________________
   ____________________________________________________________
   for i in range(x.size(0)):__________________________________
     input = x[i:i+1]__________________________________________
     (pred, context_state) = forward(input, context_state, w1, 
     context_state = context_state_____________________________
     predictions.append(pred.data.numpy().ravel()[0])__________
   1
   2
   3
   4
   5
   6
   7
   8
   context_state = variable(torch.zeros((1, hidden_size)).type(dtype),
   requires_grad=false)
   predictions = []

   for i in range(x.size(0)):
     input = x[i:i+1]
     (pred, context_state) = forward(input, context_state, w1, w2)
     context_state = context_state
     predictions.append(pred.data.numpy().ravel()[0])

   as you can see, our model did a pretty good job.

conclusion

   in this post, we implemented a basic id56 model from scratch using
   pytorch. we learned how to apply id56 to simple sequence prediction
   problem. the [17]full code is available on github.

share this:

     * [18]click to share on twitter (opens in new window)
     * [19]click to share on facebook (opens in new window)
     * [20]click to share on google+ (opens in new window)
     * [21]click to share on linkedin (opens in new window)
     * [22]click to share on tumblr (opens in new window)
     * [23]click to share on pocket (opens in new window)
     * [24]share on skype (opens in new window)
     *

like this:

   like loading...

related

2 thoughts to    introduction to recurrent neural networks in pytorch   

    1.
   alex dc says:
       [25]13th april 2018 at 5:07 pm
       thanks, this was really helpful!     
       [26]reply
         1.
        cpuheater says:
            [27]16th april 2018 at 10:09 am
            i am very happy to hear that.
            [28]reply

leave a reply [29]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   post comment

   [ ] notify me of follow-up comments by email.

   [ ] notify me of new posts by email.

post navigation

   [30]a quick introduction to pytorch

follow me

     * [31]github
     * [32]twitter

recent posts

     * [33]introduction to recurrent neural networks in pytorch
     * [34]a quick introduction to pytorch
     * [35]machine learning with scala     id75
     * [36]id202 for machine learning with scala
     * [37]learn how neural networks work with a neural network simulator

recent comments

     * cpuheater on [38]deep learning interview questions and answers
     * avinash on [39]deep learning interview questions and answers
     * manjunath m c on [40]deep learning interview questions and answers
     * cpuheater on [41]learn how neural networks work with a neural
       network simulator
     * jb on [42]learn how neural networks work with a neural network
       simulator

archives

     * [43]december 2017
     * [44]november 2017
     * [45]september 2017
     * [46]june 2017
     * [47]may 2017

categories

     * [48]deep learning
     * [49]mladdict
     * [50]scala

     * [51]github
     * [52]twitter

   iframe: [53]likes-master

   %d bloggers like this:

references

   1. https://www.cpuheater.com/feed/
   2. https://www.cpuheater.com/comments/feed/
   3. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/feed/
   4. https://www.cpuheater.com/wp-json/oembed/1.0/embed?url=https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/
   5. https://www.cpuheater.com/wp-json/oembed/1.0/embed?url=https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/&format=xml
   6. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/#content
   7. https://www.cpuheater.com/
   8. http://www.cpuheater.com/
   9. https://www.cpuheater.com/about/
  10. https://www.cpuheater.com/contact/
  11. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/
  12. https://www.cpuheater.com/author/cpuheater/
  13. https://www.cpuheater.com/category/deep-learning/
  14. https://www.cpuheater.com/deep-learning/quick-introduction-to-pytorch/
  15. https://en.wikipedia.org/wiki/jeffrey_elman
  16. https://crl.ucsd.edu/~elman/papers/fsit.pdf
  17. https://github.com/cpuheater/pytorch_examples
  18. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?share=twitter
  19. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?share=facebook
  20. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?share=google-plus-1
  21. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?share=linkedin
  22. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?share=tumblr
  23. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?share=pocket
  24. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?share=skype
  25. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/#comment-375
  26. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?replytocom=375#respond
  27. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/#comment-377
  28. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/?replytocom=377#respond
  29. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/#respond
  30. https://www.cpuheater.com/deep-learning/quick-introduction-to-pytorch/
  31. https://github.com/cpuheater
  32. https://twitter.com/cpuheater
  33. https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/
  34. https://www.cpuheater.com/deep-learning/quick-introduction-to-pytorch/
  35. https://www.cpuheater.com/scala/machine-learning-scala-linear-regression/
  36. https://www.cpuheater.com/scala/linear-algebra-machine-learning-scala/
  37. https://www.cpuheater.com/mladdict/learn-how-neural-networks-work-with-neural-network-simulator/
  38. https://www.cpuheater.com/deep-learning/deep-learning-interview-questions-and-answers/#comment-1209
  39. https://www.cpuheater.com/deep-learning/deep-learning-interview-questions-and-answers/#comment-1206
  40. https://www.cpuheater.com/deep-learning/deep-learning-interview-questions-and-answers/#comment-432
  41. https://www.cpuheater.com/mladdict/learn-how-neural-networks-work-with-neural-network-simulator/#comment-386
  42. https://www.cpuheater.com/mladdict/learn-how-neural-networks-work-with-neural-network-simulator/#comment-382
  43. https://www.cpuheater.com/2017/12/
  44. https://www.cpuheater.com/2017/11/
  45. https://www.cpuheater.com/2017/09/
  46. https://www.cpuheater.com/2017/06/
  47. https://www.cpuheater.com/2017/05/
  48. https://www.cpuheater.com/category/deep-learning/
  49. https://www.cpuheater.com/category/mladdict/
  50. https://www.cpuheater.com/category/scala/
  51. https://github.com/cpuheater
  52. https://twitter.com/cpuheater
  53. https://widgets.wp.com/likes/master.html?ver=20160429#ver=20160429&lang=en-gb
