   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

attention mechanism in neural network

   [14]go to the profile of pranoy radhakrishnan
   [15]pranoy radhakrishnan (button) blockedunblock (button)
   followfollowing
   oct 16, 2017

   an encoder reads and encodes a source sentence into a fixed-length
   vector.

   a decoder then outputs a translation from the encoded vector.

limitation

   a potential issue with this encoder   decoder approach is that a neural
   network needs to be able to compress all the necessary information of a
   source sentence into a fixed-length vector.

how attention solves the problem?

   attention mechanism allows the decoder to attend to different parts of
   the source sentence at each step of the output generation.

   instead of encoding the input sequence into a single fixed context
   vector, we let the model learn how to generate a context vector for
   each output time step. that is we let the model learn what to attend
   based on the input sentence and what it has produced so far.

attention mechanism

   [0*jpp6walmjzbjufjp.png]
   attention mechanism

   here, the encoder generates h1,h2,h   .ht from the inputs x1,x2,x3   xt

   then, we have to find out the context vector ci for each of the output
   time step.

how the context vector for each output timestep is computed?

   [1*agii69dmmktagblnucen0g.png]

   a is the alignment model which is a feedforward neural network that is
   trained with all the other components of the proposed system

   the alignment model scores (e) how well each encoded input (h) matches
   the current output of the decoder (s).
   [1*vbalt1kkz16wgvjwpjyo4g.png]

   the alignment scores are normalized using a softmax function.
   [1*uxp_uffxbaqqrjx5g-xyzq.png]

   the context vector is a weighted sum of the annotations (hj) and
   normalized alignment scores.

decoding

   the decoder generates output for i   th timestep by looking into the i   th
   context vector and the previous hidden outputs s(t-1).

reference

       [16]id4 by jointly learning to align and
   translate, 2015.

     * [17]machine learning
     * [18]artificial intelligence
     * [19]deep learning
     * [20]recurrent neural network
     * [21]machine translation

   (button)
   (button)
   (button) 168 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of pranoy radhakrishnan

[23]pranoy radhakrishnan

   deep learning engineer, entrepreneur

     (button) follow
   [24]hacker noon

[25]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 168
     * (button)
     *
     *

   [26]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [27]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/30aaf5e39512
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_3dnqkvzyzzup---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@pranoyradhakrishnan?source=post_header_lockup
  15. https://hackernoon.com/@pranoyradhakrishnan
  16. https://arxiv.org/abs/1409.0473
  17. https://hackernoon.com/tagged/machine-learning?source=post
  18. https://hackernoon.com/tagged/artificial-intelligence?source=post
  19. https://hackernoon.com/tagged/deep-learning?source=post
  20. https://hackernoon.com/tagged/recurrent-neural-network?source=post
  21. https://hackernoon.com/tagged/machine-translation?source=post
  22. https://hackernoon.com/@pranoyradhakrishnan?source=footer_card
  23. https://hackernoon.com/@pranoyradhakrishnan
  24. https://hackernoon.com/?source=footer_card
  25. https://hackernoon.com/?source=footer_card
  26. https://hackernoon.com/
  27. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  29. https://medium.com/p/30aaf5e39512/share/twitter
  30. https://medium.com/p/30aaf5e39512/share/facebook
  31. https://medium.com/p/30aaf5e39512/share/twitter
  32. https://medium.com/p/30aaf5e39512/share/facebook
