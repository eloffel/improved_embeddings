   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]syncedreview
   [7]syncedreview
   [8]sign in[9]get started
     __________________________________________________________________

a brief overview of attention mechanism

   [10]go to the profile of synced
   [11]synced (button) blockedunblock (button) followfollowing
   sep 25, 2017

what is attention?

   attention is simply a vector, often the outputs of dense layer using
   softmax function.

   before attention mechanism, translation relies on reading a complete
   sentence and compress all information into a fixed-length vector, as
   you can image, a sentence with hundreds of words represented by several
   words will surely lead to information loss, inadequate translation,
   etc.

   however, attention partially fixes this problem. it allows machine
   translator to look over all the information the original sentence
   holds, then generate the proper word according to current word it works
   on and the context. it can even allow translator to zoom in or out
   (focus on local or global features).

   attention is not mysterious or complex. it is just an interface
   formulated by parameters and delicate math. you could plug it anywhere
   you find it suitable, and potentially, the result may be enhanced.

why attention?

   the core of probabilistic language model is to assign a id203 to
   a sentence by markov assumption. due to the nature of sentences that
   consist of different numbers of words, id56 is naturally introduced to
   model the id155 among words.
   [0*sx7clvkt8w9j39ed.]

   vanilla id56 (the classic one) often gets trapped when modeling:
    1. structure dilemma: in real world, the length of outputs and inputs
       can be totally different, while vanilla id56 can only handle
       fixed-length problem which is difficult for the alignment. consider
       an en-fr translation examples:    he doesn   t like apples           il
       n   aime pas les pommes   .
    2. mathematical nature: it suffers from gradient vanishing/exploding
       which means it is hard to train when sentences are long enough
       (maybe at most 4 words).

   translation often requires arbitrary input length and out put length,
   to deal with the deficits above, encoder-decoder model is adopted and
   basic id56 cell is changed to gru or lstm cell, hyperbolic tangent
   activation is replaced by relu. we use gru cell here.
   [0*vwqyyhlpdgewsd-2.]

   embedding layer maps discrete words into dense vectors for
   computational efficiency. then embedded word vectors are fed into
   encoder, aka gru cells sequentially. what happened during encoding?
   information flows from left to right and each word vector is learned
   according to not only current input but also all previous words. when
   the sentence is completely read, encoder generates an output and a
   hidden state at timestep 4 for further processing. for encoding part,
   decoder (grus as well) grabs the hidden state from encoder, trained by
   teacher forcing (a mode that previous cell   s output as current input),
   then generate translation words sequentially.

   it seems amazing as this model can be applied to n-to-m sequence, yet
   there still is one main deficit left unsolved: is one hidden state
   really enough?

   yes, attention here.

how does attention work?

   [0*vrrtrruwf2btw4t5.]

   similar to the basic encoder-decoder architecture, this fancy mechanism
   plug a context vector into the gap between encoder and decoder.
   according to the schematic above, blue represents encoder and red
   represents decoder; and we could see that context vector takes all
   cells    outputs as input to compute the id203 distribution of
   source language words for each single word decoder wants to generate.
   by utilizing this mechanism, it is possible for decoder to capture
   somewhat global information rather than solely to infer based on one
   hidden state.

   and to build context vector is fairly simple. for a fixed target word,
   first, we loop over all encoders    states to compare target and source
   states to generate scores for each state in encoders. then we could use
   softmax to normalize all scores, which generates the id203
   distribution conditioned on target states. at last, the weights are
   introduced to make context vector easy to train. that   s it. math is
   shown below:
   [0*4y96bognminvhno8.]

   to understand the seemingly complicated math, we need to keep three key
   points in mind:
    1. during decoding, context vectors are computed for every output
       word. so we will have a 2d matrix whose size is # of target words
       multiplied by # of source words. equation (1) demonstrates how to
       compute a single value given one target word and a set of source
       word.
    2. once context vector is computed, attention vector could be computed
       by context vector, target word, and attention function f.
    3. we need attention mechanism to be trainable. according to equation
       (4), both styles offer the trainable weights (w in luong   s, w1 and
       w2 in bahdanau   s). thus, different styles may result in different
       performance.

conclusion

   we hope you understand the reason why attention is one of the hottest
   topics today, and most importantly, the basic math behind attention.
   implementing your own attention layer is encouraged. there are many
   variants in the cutting-edge researches, and they basically differ in
   the choice of score function and attention function, or of soft
   attention and hard attention (whether differentiable). but basic
   concepts are all the same. if interested, you could check out papers
   below.

   [1] vinyals, oriol, et al. show and tell: a neural image caption
   generator. arxiv:1411.4555 (2014).
   [2] bahdanau, dzmitry, kyunghyun cho, and yoshua bengio. neural machine
   translation by jointly learning to align and translate. arxiv:1409.0473
   (2014).
   [3] cho, kyunghyun, aaron courville, and yoshua bengio. describing
   multimedia content using attention-based encoder   decoder networks.
   arxiv:1507.01053 (2015)
   [4] xu, kelvin, et al. show, attend and tell: neural image caption
   generation with visual attention. arxiv:1502.03044 (2015).
   [5] sukhbaatar, sainbayar, jason weston, and rob fergus. end-to-end
   memory networks. advances in neural information processing systems.
   (2015).
   [6] joulin, armand, and tomas mikolov. inferring algorithmic patterns
   with stack-augmented recurrent nets. arxiv:1503.01007 (2015).
   [7] hermann, karl moritz, et al. teaching machines to read and
   comprehend. advances in neural information processing systems. (2015).
   [8] raffel, colin, and daniel pw ellis. feed-forward networks with
   attention can solve some long-term memory problems. arxiv:1512.08756
   (2015).
   [9] vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., &
   gomez, a. et al. . attention is all you need. arxiv: 1706.03762 (2017).
     __________________________________________________________________

   tech analyst: qingtong wu

     * [12]machine learning
     * [13]id56
     * [14]lstm
     * [15]neural networks

   (button)
   (button)
   (button) 1.2k claps
   (button) (button) (button) 17 (button) (button)

     (button) blockedunblock (button) followfollowing
   [16]go to the profile of synced

[17]synced

   ai technology & industry review
   -[18]www.syncedreview.com&www.jiqizhixin.com|
   newsletter:[19]http://goo.gl/q4cp3b | become synced insight
   partner:[20]https://goo.gl/ucxzdw

     (button) follow
   [21]syncedreview

[22]syncedreview

   we produce professional, authoritative, and thought-provoking content
   relating to artificial intelligence, machine intelligence, emerging
   technologies and industrial insights.

     * (button)
       (button) 1.2k
     * (button)
     *
     *

   [23]syncedreview
   never miss a story from syncedreview, when you sign up for medium.
   [24]learn more
   never miss a story from syncedreview
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/13c578ba9129
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/syncedreview?source=avatar-lo_84ytdtue6xri-863f502aede2
   7. https://medium.com/syncedreview?source=logo-lo_84ytdtue6xri---863f502aede2
   8. https://medium.com/m/signin?redirect=https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@synced?source=post_header_lockup
  11. https://medium.com/@synced
  12. https://medium.com/tag/machine-learning?source=post
  13. https://medium.com/tag/id56?source=post
  14. https://medium.com/tag/lstm?source=post
  15. https://medium.com/tag/neural-networks?source=post
  16. https://medium.com/@synced?source=footer_card
  17. https://medium.com/@synced
  18. http://www.syncedreview.com&www.jiqizhixin.com|/
  19. http://goo.gl/q4cp3b
  20. https://goo.gl/ucxzdw
  21. https://medium.com/syncedreview?source=footer_card
  22. https://medium.com/syncedreview?source=footer_card
  23. https://medium.com/syncedreview
  24. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  26. https://medium.com/p/13c578ba9129/share/twitter
  27. https://medium.com/p/13c578ba9129/share/facebook
  28. https://medium.com/p/13c578ba9129/share/twitter
  29. https://medium.com/p/13c578ba9129/share/facebook
