   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

coding neural network         forward propagation and backpropagtion

   [16]go to the profile of imad dabbura
   [17]imad dabbura (button) blockedunblock (button) followfollowing
   mar 31, 2018

   why neural networks?

   according to universal approximate theorem, neural networks can
   approximate as well as learn and represent any function given a large
   enough layer and desired error margin. the way neural network learns
   the true function is by building complex representations on top of
   simple ones. on each hidden layer, the neural network learns new
   feature space by first compute the affine (linear) transformations of
   the given inputs and then apply non-linear function which in turn will
   be the input of the next layer. this process will continue until we
   reach the output layer. therefore, we can define neural network as
   information flows from inputs through hidden layers towards the output.
   for a 3-layers neural network, the learned function would be: f(x) =
   f_3(f_2(f_1(x))) where:
     * f_1(x): function learned on first hidden layer
     * f_2(x): function learned on second hidden layer
     * f_3(x): function learned on output layer

   therefore, on each layer we learn different representation that gets
   more complicated with later hidden layers.below is an example of a
   3-layers neural network (we don   t count input layer):
   [0*hziq5fs-g8ibpvwq.jpg]
   figure 1: neural network with two hidden layers

   for example, computers can   t understand images directly and don   t know
   what to do with pixels data. however, a neural network can build a
   simple representation of the image in the early hidden layers that
   identifies edges. given the first hidden layer output, it can learn
   corners and contours. given the second hidden layer, it can learn parts
   such as nose. finally, it can learn the object identity.

   since truth is never linear and representation is very critical to the
   performance of a machine learning algorithm, neural network can help us
   build very complex models and leave it to the algorithm to learn such
   representations without worrying about feature engineering that takes
   practitioners very long time and effort to curate a good
   representation.

   the post has two parts:
    1. coding the neural network: this entails writing all the helper
       functions that would allow us to implement a multi-layer neural
       network. while doing so, i   ll explain the theoretical parts
       whenever possible and give some advices on implementations.
    2. application: we   ll implement the neural network we coded in the
       first part on image recognition problem to see if the network we
       built will be able to detect if the image has a cat or a dog and
       see it working :)

   this post will be the first in a series of posts that cover
   implementing neural network in numpy including gradient checking,
   parameter initialization, l2 id173, dropout. the source code
   that created this post can be found [18]here.
# import packages
import h5py import
matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

i. coding the neural network

forward propagation

   the input x provides the initial information that then propagates to
   the hidden units at each layer and finally produce the output
   $\widehat{y}$. the architecture of the network entails determining its
   depth, width, and id180 used on each layer. depth is the
   number of hidden layers. width is the number of units (nodes) on each
   hidden layer since we don   t control neither input layer nor output
   layer dimensions. there are quite a few set of id180
   such rectified linear unit, sigmoid, hyperbolic tangent, etc. research
   has proven that deeper networks outperform networks with more hidden
   units. therefore, it   s always better and won   t hurt to train a deeper
   network (with diminishing returns).

   lets first introduce some notations that will be used throughout the
   post:
   [1*ejygokakbksc466illgslq.png]

   next, we   ll write down the dimensions of a multi-layer neural network
   in the general form to help us in id127 because one of
   the major challenges in implementing a neural network is getting the
   dimensions right.
   [1*om7dk8yf8ev-pjpbternhg.png]

   the two equations we need to implement forward propagations are: these
   computations will take place on each layer.

parameters initialization

   we   ll first initialize the weight matrices and the bias vectors. it   s
   important to note that we shouldn   t initialize all the parameters to
   zero because doing so will lead the gradients to be equal and on each
   iteration the output would be the same and the learning algorithm won   t
   learn anything. therefore, it   s important to randomly initialize the
   parameters to values between 0 and 1. it   s also recommended to multiply
   the random values by small scalar such as 0.01 to make the activation
   units active and be on the regions where id180   
   derivatives are not close to zero.

   iframe: [19]/media/f5b2e5831b2043c03d7348ae3c16ca5c?postid=ccf8cf369f76

id180

   there is no definitive guide for which activation function works best
   on specific problems. it   s a trial and error process where one should
   try different set of functions and see which one works best on the
   problem at hand. we   ll cover 4 of the most commonly used activation
   functions:
     * sigmoid function (  ): g(z) = 1 / (1 + e^{-z}). it   s recommended to
       be used only on the output layer so that we can easily interpret
       the output as probabilities since it has restricted output between
       0 and 1. one of the main disadvantages for using sigmoid function
       on hidden layers is that the gradient is very close to zero over a
       large portion of its domain which makes it slow and harder for the
       learning algorithm to learn.
     * hyperbolic tangent function: g(z) = (e^z -e^{-z}) / (e^z + e^{-z}).
       it   s superior to sigmoid function in which the mean of its output
       is very close to zero, which in other words center the output of
       the activation units around zero and make the range of values very
       small which means faster to learn. the disadvantage that it shares
       with sigmoid function is that the gradient is very small on good
       portion of the domain.
     * rectified linear unit (relu): g(z) = max{0, z}. the models that are
       close to linear are easy to optimize. since relu shares a lot of
       the properties of linear functions, it tends to work well on most
       of the problems. the only issue is that the derivative is not
       defined at z = 0, which we can overcome by assigning the derivative
       to 0 at z = 0. however, this means that for z     0 the gradient is
       zero and again can   t learn.
     * leaky rectified linear unit: g(z) = max{  *z, z}. it overcomes the
       zero gradient issue from relu and assigns    which is a small value
       for z     0.

   if you   re not sure which activation function to choose, start with
   relu. next, we   ll implement the above id180 and draw a
   graph for each one to make it easier to see the domain and range of
   each function.

   iframe: [20]/media/9cef07a51dc0ea616ced1107a994f4b0?postid=ccf8cf369f76

   iframe: [21]/media/187d141895ea630ffcbc11cc6b252c14?postid=ccf8cf369f76

feed forward

   given its inputs from previous layer, each unit computes affine
   transformation z = w^tx + b and then apply an activation function g(z)
   such as relu element-wise. during the process, we   ll store (cache) all
   variables computed and used on each layer to be used in
   back-propagation. we   ll write first two helper functions that will be
   used in the l-model forward propagation to make it easier to debug.
   keep in mind that on each layer, we may have different activation
   function.

   iframe: [22]/media/3026c3f90a97f2a067e07d21e0c21309?postid=ccf8cf369f76

cost

   we   ll use the binary cross-id178 cost. it uses the log-likelihood
   method to estimate its error. the cost is: the above cost function is
   convex; however, neural network usually stuck on a local minimum and is
   not guaranteed to find the optimal parameters. we   ll use here
   gradient-based learning.

   iframe: [23]/media/5cbfff65fe24d029b7ee902557e74e4e?postid=ccf8cf369f76

back-propagation

   allows the information to go back from the cost backward through the
   network in order to compute the gradient. therefore, loop over the
   nodes starting at the final node in reverse topological order to
   compute the derivative of the final node output with respect to each
   edge   s node tail. doing so will help us know who is responsible for the
   most error and change the parameters in that direction. the following
   derivatives    formulas will help us write the back-propagate functions:
   since b^l is always a vector, the sum would be across rows (since each
   column is an example).

   iframe: [24]/media/e982ef4c4f5319dcae77c51dd1f87701?postid=ccf8cf369f76

   iframe: [25]/media/850339d4c359873ea703f3707a1f40e3?postid=ccf8cf369f76

ii. application

   the dataset that we   ll be working on has 209 images. each image is 64 x
   64 pixels on rgb scale. we   ll build a neural network to classify if the
   image has a cat or not. therefore, y^i     {0, 1}.
     * we   ll first load the images.
     * show sample image for a cat.
     * reshape input matrix so that each column would be one example.
       also, since each image is 64 x 64 x 3, we   ll end up having 12,288
       features for each image. therefore, the input matrix would be
       12,288 x 209.
     * standardize the data so that the gradients don   t go out of control.
       also, it will help hidden units have similar range of values. for
       now, we   ll divide every pixel by 255 which shouldn   t be an issue.
       however, it   s better to standardize the data to have a mean of 0
       and a standard deviation of 1.

   iframe: [26]/media/087265b1faf014892ce282629f2c5524?postid=ccf8cf369f76

original dimensions:
--------------------
training: (209, 64, 64, 3), (209,)
test: (50, 64, 64, 3), (50,)
new dimensions:
---------------
training: (12288, 209), (1, 209)
test: (12288, 50), (1, 50)

   [0*9gugxkkznemibwiu.png]
   figure 3: sample image

   now, our dataset is ready to be used and test our neural network
   implementation. let   s first write multi-layer model function to
   implement gradient-based learning using predefined number of iterations
   and learning rate.

   iframe: [27]/media/6d5d485f7044c017e8de6035be6b2ea6?postid=ccf8cf369f76

   next, we   ll train two versions of the neural network where each one
   will use different activation function on hidden layers: one will use
   rectified linear unit (relu) and the second one will use hyperbolic
   tangent function (tanh). finally we   ll use the parameters we get from
   both neural networks to classify training examples and compute the
   training accuracy rates for each version to see which activation
   function works best on this problem.
# setting layers dims
layers_dims = [x_train.shape[0], 5, 5, 1]
# nn with tanh activation fn
parameters_tanh = l_layer_model( x_train, y_train, layers_dims, learning_rate=0.
03, num_iterations=3000, hidden_layers_activation_fn="tanh")
# print the accuracy
accuracy(x_test, parameters_tanh, y_test, activation_fn="tanh")
the cost after 100 iterations is: 0.6556
the cost after 200 iterations is: 0.6468
the cost after 300 iterations is: 0.6447
the cost after 400 iterations is: 0.6441
the cost after 500 iterations is: 0.6440
the cost after 600 iterations is: 0.6440
the cost after 700 iterations is: 0.6440
the cost after 800 iterations is: 0.6439
the cost after 900 iterations is: 0.6439
the cost after 1000 iterations is: 0.6439
the cost after 1100 iterations is: 0.6439
the cost after 1200 iterations is: 0.6439
the cost after 1300 iterations is: 0.6438
the cost after 1400 iterations is: 0.6438
the cost after 1500 iterations is: 0.6437
the cost after 1600 iterations is: 0.6434
the cost after 1700 iterations is: 0.6429
the cost after 1800 iterations is: 0.6413
the cost after 1900 iterations is: 0.6361
the cost after 2000 iterations is: 0.6124
the cost after 2100 iterations is: 0.5112
the cost after 2200 iterations is: 0.5288
the cost after 2300 iterations is: 0.4312
the cost after 2400 iterations is: 0.3821
the cost after 2500 iterations is: 0.3387
the cost after 2600 iterations is: 0.2349
the cost after 2700 iterations is: 0.2206
the cost after 2800 iterations is: 0.1927
the cost after 2900 iterations is: 0.4669
the cost after 3000 iterations is: 0.1040
'the accuracy rate is: 68.00%.'

   [0*is4jiq0zdj7pkyfd.png]
   figure 4: loss curve with tanh activation function
# nn with relu activation fn
parameters_relu = l_layer_model( x_train, y_train, layers_dims, learning_rate=0.
03, num_iterations=3000, hidden_layers_activation_fn="relu")
# print the accuracy
accuracy(x_test, parameters_relu, y_test, activation_fn="relu")
the cost after 100 iterations is: 0.6556
the cost after 200 iterations is: 0.6468
the cost after 300 iterations is: 0.6447
the cost after 400 iterations is: 0.6441
the cost after 500 iterations is: 0.6440
the cost after 600 iterations is: 0.6440
the cost after 700 iterations is: 0.6440
the cost after 800 iterations is: 0.6440
the cost after 900 iterations is: 0.6440
the cost after 1000 iterations is: 0.6440
the cost after 1100 iterations is: 0.6439
the cost after 1200 iterations is: 0.6439
the cost after 1300 iterations is: 0.6439
the cost after 1400 iterations is: 0.6439
the cost after 1500 iterations is: 0.6439
the cost after 1600 iterations is: 0.6439
the cost after 1700 iterations is: 0.6438
the cost after 1800 iterations is: 0.6437
the cost after 1900 iterations is: 0.6435
the cost after 2000 iterations is: 0.6432
the cost after 2100 iterations is: 0.6423
the cost after 2200 iterations is: 0.6395
the cost after 2300 iterations is: 0.6259
the cost after 2400 iterations is: 0.5408
the cost after 2500 iterations is: 0.5262
the cost after 2600 iterations is: 0.4727
the cost after 2700 iterations is: 0.4386
the cost after 2800 iterations is: 0.3493
the cost after 2900 iterations is: 0.1877
the cost after 3000 iterations is: 0.3641
'the accuracy rate is: 42.00%.'

   [0*mipo_t_3vw9gpztn.png]
   figure 5: loss curve with relu activation function

   please note that the accuracy rates above are expected to overestimate
   the generalization accuracy rates.

conclusion

   the purpose of this post is to code deep neural network step-by-step
   and explain the important concepts while doing that. we don   t really
   care about the accuracy rate at this moment since there are tons of
   things we could   ve done to increase the accuracy which would be the
   subject of following posts. below are some takeaways:
     * even if neural network can represent any function, it may fail to
       learn for two reasons:

    1. the optimization algorithm may fail to find the best value for the
       parameters of the desired (true) function. it can stuck in a local
       optimum.
    2. the learning algorithm may find different functional form that is
       different than the intended function due to overfitting.

     * even if neural network rarely converges and always stuck in a local
       minimum, it is still able to reduce the cost significantly and come
       up with very complex models with high test accuracy.
     * the neural network we used in this post is standard fully connected
       network. however, there are two other kinds of networks:

    1. convolutional nn: where not all nodes are connected. it   s best in
       class for image recognition.
    2. recurrent nn: there is a feedback connections where output of the
       model is fed back into itself. it   s used mainly in sequence
       modeling.

     * the fully connected neural network also forgets what happened in
       previous steps and also doesn   t know anything about the output.
     * there are number of hyperparameters that we can tune using cross
       validation to get the best performance of our network:

    1. learning rate (  ): determines how big the step for each update of
       parameters.

   a. small    leads to slow convergence and may become computationally
   very expensive.

   b. large    may lead to overshooting where our learning algorithm may
   never converge.

   2. number of hidden layers (depth): the more hidden layers the better,
   but comes at a cost computationally.

   3. number of units per hidden layer (width): research proven that huge
   number of hidden units per layer doesn   t add to the improvement of the
   network.

   4. activation function: which function to use on hidden layers differs
   among applications and domains. it   s a trial and error process to try
   different functions and see which one works best.

   5. number of iterations.
     * standardize data would help activation units have similar range of
       values and avoid gradients to go out of control.
     __________________________________________________________________

   originally published at [28]imaddabbura.github.io on april 1, 2018.

     * [29]deep learning
     * [30]ai
     * [31]artificial intelligence
     * [32]neural networks
     * [33]machine learning

   (button)
   (button)
   (button) 1.2k claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of imad dabbura

[35]imad dabbura

   data scientist

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.2k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ccf8cf369f76
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ym7ohgrnx7jy---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@imadphd?source=post_header_lockup
  17. https://towardsdatascience.com/@imadphd
  18. https://nbviewer.jupyter.org/github/imaddabbura/blog-posts/blob/master/notebooks/coding-neural-network-forwad-back-propagation.ipynb
  19. https://towardsdatascience.com/media/f5b2e5831b2043c03d7348ae3c16ca5c?postid=ccf8cf369f76
  20. https://towardsdatascience.com/media/9cef07a51dc0ea616ced1107a994f4b0?postid=ccf8cf369f76
  21. https://towardsdatascience.com/media/187d141895ea630ffcbc11cc6b252c14?postid=ccf8cf369f76
  22. https://towardsdatascience.com/media/3026c3f90a97f2a067e07d21e0c21309?postid=ccf8cf369f76
  23. https://towardsdatascience.com/media/5cbfff65fe24d029b7ee902557e74e4e?postid=ccf8cf369f76
  24. https://towardsdatascience.com/media/e982ef4c4f5319dcae77c51dd1f87701?postid=ccf8cf369f76
  25. https://towardsdatascience.com/media/850339d4c359873ea703f3707a1f40e3?postid=ccf8cf369f76
  26. https://towardsdatascience.com/media/087265b1faf014892ce282629f2c5524?postid=ccf8cf369f76
  27. https://towardsdatascience.com/media/6d5d485f7044c017e8de6035be6b2ea6?postid=ccf8cf369f76
  28. https://imaddabbura.github.io/post/coding_neural_network_fwd_bckwd_prop/
  29. https://towardsdatascience.com/tagged/deep-learning?source=post
  30. https://towardsdatascience.com/tagged/ai?source=post
  31. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  32. https://towardsdatascience.com/tagged/neural-networks?source=post
  33. https://towardsdatascience.com/tagged/machine-learning?source=post
  34. https://towardsdatascience.com/@imadphd?source=footer_card
  35. https://towardsdatascience.com/@imadphd
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/ccf8cf369f76/share/twitter
  42. https://medium.com/p/ccf8cf369f76/share/facebook
  43. https://medium.com/p/ccf8cf369f76/share/twitter
  44. https://medium.com/p/ccf8cf369f76/share/facebook
