   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

an introduction to id23

   [11]go to the profile of thomas simonini
   [12]thomas simonini (button) blockedunblock (button) followfollowing
   mar 30, 2018
   [1*0gd5lik1e7rwf3hygxgh-g.png]
   some of the environments you   ll work with

     this article is part of deep id23 course with
     tensorflow        . check the syllabus [13]here

   id23 is an important type of machine learning where
   an agent learn how to behave in a environment by performing actions and
   seeing the results.

   in recent years, we   ve seen a lot of improvements in this fascinating
   area of research. examples include [14]deepmind and the deep id24
   architecture in 2014, [15]beating the champion of the game of go with
   alphago in 2016, [16]openai and the ppo in 2017, amongst others.

   iframe: [17]/media/7f746c334919fe43cd194c4bee6ef478?postid=4339519de419

   deepmind id25

   in this series of articles, we will focus on learning the different
   architectures used today to solve id23 problems.
   these will include q -learning, deep id24, policy gradients,
   actor critic, and ppo.

   in this first article, you   ll learn:
     * what id23 is, and how rewards are the central
       idea
     * the three approaches of id23
     * what the    deep    in deep id23 means

   it   s really important to master these elements before diving into
   implementing deep id23 agents.

   the idea behind id23 is that an agent will learn from
   the environment by interacting with it and receiving rewards for
   performing actions.
   [1*zyssjwywqgerksbjhbtkyg.png]

   learning from interaction with the environment comes from our natural
   experiences. imagine you   re a child in a living room. you see a
   fireplace, and you approach it.
   [1*aquwm51knogiugtgnzoriw.png]

   it   s warm, it   s positive, you feel good (positive reward +1). you
   understand that fire is a positive thing.
   [1*5shp6uzu7xt41vroj7-3gw.png]

   but then you try to touch the fire. ouch! it burns your hand (negative
   reward -1). you   ve just understood that fire is positive when you are a
   sufficient distance away, because it produces warmth. but get too close
   to it and you will be burned.

   that   s how humans learn, through interaction. id23 is
   just a computational approach of learning from action.

the id23 process

   [1*akyfroemmkkybqjovlt2jq.png]

   let   s imagine an agent learning to play super mario bros as a working
   example. the id23 (rl) process can be modeled as a
   loop that works like this:
     * our agent receives state s0 from the environment (in our case we
       receive the first frame of our game (state) from super mario bros
       (environment))
     * based on that state s0, agent takes an action a0 (our agent will
       move right)
     * environment transitions to a new state s1 (new frame)
     * environment gives some reward r1 to the agent (not dead: +1)

   this rl loop outputs a sequence of state, action and reward.

   the goal of the agent is to maximize the expected cumulative reward.

the central idea of the reward hypothesis

   why is the goal of the agent to maximize the expected cumulative
   reward?

   well, id23 is based on the idea of the reward
   hypothesis. all goals can be described by the maximization of the
   expected cumulative reward.

   that   s why in id23, to have the best behavior, we
   need to maximize the expected cumulative reward.

   the cumulative reward at each time step t can be written as:
   [0*ylz4lplmffgqr_g3.]

   which is equivalent to:
   [1*afaum1y8zmso4yb5moapza.png]
   thanks to [18]pierre-luc bacon for the correction

   however, in reality, we can   t just add the rewards like that. the
   rewards that come sooner (in the beginning of the game) are more
   probable to happen, since they are more predictable than the long term
   future reward.
   [1*tcinrjn6pw60-h0piqrixg.png]

   let say your agent is this small mouse and your opponent is the cat.
   your goal is to eat the maximum amount of cheese before being eaten by
   the cat.

   as we can see in the diagram, it   s more probable to eat the cheese near
   us than the cheese close to the cat (the closer we are to the cat, the
   more dangerous it is).

   as a consequence, the reward near the cat, even if it is bigger (more
   cheese), will be discounted. we   re not really sure we   ll be able to eat
   it.

   to discount the rewards, we proceed like this:

   we define a discount rate called gamma. it must be between 0 and 1.
     * the larger the gamma, the smaller the discount. this means the
       learning agent cares more about the long term reward.
     * on the other hand, the smaller the gamma, the bigger the discount.
       this means our agent cares more about the short term reward (the
       nearest cheese).

   our discounted cumulative expected rewards is:
   [1*zrzrtxt8rtwf5fx__kz-yq.png]
   thanks to [19]pierre-luc bacon for the correction

   to be simple, each reward will be discounted by gamma to the exponent
   of the time step. as the time step increases, the cat gets closer to
   us, so the future reward is less and less probable to happen.

episodic or continuing tasks

   a task is an instance of a id23 problem. we can have
   two types of tasks: episodic and continuous.

episodic task

   in this case, we have a starting point and an ending point (a terminal
   state). this creates an episode: a list of states, actions, rewards,
   and new states.

   for instance think about super mario bros, an episode begin at the
   launch of a new mario and ending: when you   re killed or you   re reach
   the end of the level.
   [1*pps51sgatrkjft0iucw6va.png]
   beginning of a new episode

continuous tasks

   these are tasks that continue forever (no terminal state). in this
   case, the agent has to learn how to choose the best actions and
   simultaneously interacts with the environment.

   for instance, an agent that do automated stock trading. for this task,
   there is no starting point and terminal state. the agent keeps running
   until we decide to stop him.
   [1*5t_ta3qauhuemuczev6wyw.jpeg]

monte carlo vs td learning methods

   we have two ways of learning:
     * collecting the rewards at the end of the episode and then
       calculating the maximum expected future reward: monte carlo
       approach
     * estimate the rewards at each step: temporal difference learning

monte carlo

   when the episode ends (the agent reaches a    terminal state   ), the agent
   looks at the total cumulative reward to see how well it did. in monte
   carlo approach, rewards are only received at the end of the game.

   then, we start a new game with the added knowledge. the agent makes
   better decisions with each iteration.
   [1*rllzql4yadpbhplxpa5f6a.png]

   let   s take an example:
   [1*tcinrjn6pw60-h0piqrixg.png]

   if we take the maze environment:
     * we always start at the same starting point.
     * we terminate the episode if the cat eats us or if we move > 20
       steps.
     * at the end of the episode, we have a list of state, actions,
       rewards, and new states.
     * the agent will sum the total rewards gt (to see how well it did).
     * it will then update v(st) based on the formula above.
     * then start a new game with this new knowledge.

   by running more and more episodes, the agent will learn to play better
   and better.

temporal difference learning : learning at each time step

   td learning, on the other hand, will not wait until the end of the
   episode to update the maximum expected future reward estimation: it
   will update its value estimation v for the non-terminal states st
   occurring at that experience.

   this method is called td(0) or one step td (update the value function
   after any individual step).
   [1*llfj11fivpkkzkwq8upi3a.png]

   td methods only wait until the next time step to update the value
   estimates. at time t+1 they immediately form a td target using the
   observed reward rt+1 and the current estimate v(st+1).

   td target is an estimation: in fact you update the previous estimate
   v(st) by updating it towards a one-step target.

exploration/exploitation trade off

   before looking at the different strategies to solve reinforcement
   learning problems, we must cover one more very important topic: the
   exploration/exploitation trade-off.
     * exploration is finding more information about the environment.
     * exploitation is exploiting known information to maximize the
       reward.

   remember, the goal of our rl agent is to maximize the expected
   cumulative reward. however, we can fall into a common trap.
   [1*aplmz8cvgu0oy3sqbvyiuw.png]

   in this game, our mouse can have an infinite amount of small cheese (+1
   each). but at the top of the maze there is a gigantic sum of cheese
   (+1000).

   however, if we only focus on reward, our agent will never reach the
   gigantic sum of cheese. instead, it will only exploit the nearest
   source of rewards, even if this source is small (exploitation).

   but if our agent does a little bit of exploration, it can find the big
   reward.

   this is what we call the exploration/exploitation trade off. we must
   define a rule that helps to handle this trade-off. we   ll see in future
   articles different ways to handle it.

three approaches to id23

   now that we defined the main elements of id23, let   s
   move on to the three approaches to solve a id23
   problem. these are value-based, policy-based, and model-based.

value based

   in value-based rl, the goal is to optimize the value function v(s).

   the value function is a function that tells us the maximum expected
   future reward the agent will get at each state.

   the value of each state is the total amount of the reward an agent can
   expect to accumulate over the future, starting at that state.
   [0*kvtrahbzo-h77iw1.]

   the agent will use this value function to select which state to choose
   at each step. the agent takes the state with the biggest value.
   [1*2_jrk-4o523bcocsy1u31g.png]

   in the maze example, at each step we will take the biggest value: -7,
   then -6, then -5 (and so on) to attain the goal.

policy based

   in policy-based rl, we want to directly optimize the policy function
     (s) without using a value function.

   the policy is what defines the agent behavior at a given time.
   [0*8b4cahvm-k4y9a5u.]
   action = policy(state)

   we learn a policy function. this lets us map each state to the best
   corresponding action.

   we have two types of policy:
     * deterministic: a policy at a given state will always return the
       same action.
     * stochastic: output a distribution id203 over actions.

   [0*dniqgeul1fkunrbb.]
   [1*fii7z01largateajdvloaq.png]

   as we can see here, the policy directly indicates the best action to
   take for each steps.

model based

   in model-based rl, we model the environment. this means we create a
   model of the behavior of the environment.

   the problem is each environment will need a different model
   representation. that   s why we will not speak about this type of
   id23 in the upcoming articles.

introducing deep id23

   deep id23 introduces deep neural networks to solve
   id23 problems         hence the name    deep.   

   for instance, in the next article we   ll work on id24 (classic
   id23) and deep id24.

   you   ll see the difference is that in the first approach, we use a
   traditional algorithm to create a q table that helps us find what
   action to take for each state.

   in the second approach, we will use a neural network (to approximate
   the reward based on state: q value).
   [1*w5guxedz9ivryqm_mluxoq.png]
   schema inspired by the id24 notebook by udacity

   congrats! there was a lot of information in this article. be sure to
   really grasp the material before continuing. it   s important to master
   these elements before entering the fun part: creating ai that plays
   video games.

   important: this article is the first part of a free series of blog
   posts about deep id23. for more information and more
   resources, [20]check out the syllabus.

   next time we   ll work on a id24 agent that learns to play the
   frozen lake game.
   [1*zw-o3-s4jrnplbztkp3u4a.gif]
   frozenlake

   iframe: [21]/media/0c073161eb43580eaf8ce4eb75324965?postid=4339519de419

   if you liked my article, please click the      below as many time as you
   liked the article so other people will see this here on medium. and
   don   t forget to follow me!

   if you have any thoughts, comments, questions, feel free to comment
   below or send me an email: hello@simoninithomas.com, or tweet me
   [22]@thomassimonini.
   [23][1*_yn1fzvefdmlobiysstizg.png]
   [24][1*md-f5vn1swyvhrzabvsu_w.png]
   [25][1*pqiptt-cdi8uwosxufn2dq.png]

   iframe: [26]/media/abe75436fcf1f39ac48a98a7ac598cc3?postid=4339519de419

   cheers!
     __________________________________________________________________

deep id23 course:

     we   re making a video version of the deep id23
     course with tensorflow      where we focus on the implementation part
     with tensorflow [27]here.

   part 1: [28]an introduction to id23

   part 2: [29]diving deeper into id23 with id24

   part 3: [30]an introduction to deep id24: let   s play doom

   part 3+: [31]improvements in deep id24: dueling double id25,
   prioritized experience replay, and fixed q-targets

   part 4: [32]an introduction to policy gradients with doom and cartpole

   part 5: [33]an intro to advantage actor critic methods: let   s play
   sonic the hedgehog!

   part 6: [34]proximal policy optimization (ppo) with sonic the hedgehog
   2 and 3

   part 7: [35]curiosity-driven learning made easy part i

     * [36]machine learning
     * [37]artificial intelligence
     * [38]id23
     * [39]tech
     * [40]deep learning

   (button)
   (button)
   (button) 20k claps
   (button) (button) (button) 36 (button) (button)

     (button) blockedunblock (button) followfollowing
   [41]go to the profile of thomas simonini

[42]thomas simonini

   deep learning engineer / founder of deep id23 course
   [43]https://bit.ly/2mx2mne

     (button) follow
   [44]freecodecamp.org

[45]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 20k
     * (button)
     *
     *

   [46]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [47]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4339519de419
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_3efuruxkzusf---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@thomassimonini?source=post_header_lockup
  12. https://medium.freecodecamp.org/@thomassimonini
  13. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  14. https://deepmind.com/research/id25/
  15. https://deepmind.com/research/alphago/
  16. https://blog.openai.com/openai-baselines-ppo/
  17. https://medium.freecodecamp.org/media/7f746c334919fe43cd194c4bee6ef478?postid=4339519de419
  18. https://twitter.com/pierrelux
  19. https://twitter.com/pierrelux
  20. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  21. https://medium.freecodecamp.org/media/0c073161eb43580eaf8ce4eb75324965?postid=4339519de419
  22. https://twitter.com/thomassimonini
  23. https://twitter.com/thomassimonini
  24. https://www.youtube.com/channel/uc8xusf1ed9af8x8j19ha5og?view_as=subscriber
  25. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  26. https://medium.freecodecamp.org/media/abe75436fcf1f39ac48a98a7ac598cc3?postid=4339519de419
  27. https://youtu.be/q2zoefaaai0
  28. https://medium.com/p/4339519de419/edit
  29. https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe
  30. https://medium.freecodecamp.org/an-introduction-to-deep-id24-lets-play-doom-54d02d8017d8
  31. https://medium.freecodecamp.org/improvements-in-deep-id24-dueling-double-id25-prioritized-experience-replay-and-fixed-58b130cc5682
  32. https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f
  33. https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d
  34. https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e
  35. https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359
  36. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  37. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  38. https://medium.freecodecamp.org/tagged/reinforcement-learning?source=post
  39. https://medium.freecodecamp.org/tagged/tech?source=post
  40. https://medium.freecodecamp.org/tagged/deep-learning?source=post
  41. https://medium.freecodecamp.org/@thomassimonini?source=footer_card
  42. https://medium.freecodecamp.org/@thomassimonini
  43. https://bit.ly/2mx2mne
  44. https://medium.freecodecamp.org/?source=footer_card
  45. https://medium.freecodecamp.org/?source=footer_card
  46. https://medium.freecodecamp.org/
  47. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  49. https://medium.com/p/4339519de419/share/twitter
  50. https://medium.com/p/4339519de419/share/facebook
  51. https://medium.com/p/4339519de419/share/twitter
  52. https://medium.com/p/4339519de419/share/facebook
