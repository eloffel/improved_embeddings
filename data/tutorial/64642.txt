   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

introduction to various id23 algorithms. part ii
(trpo, ppo)

   [16]go to the profile of           steeve huang
   [17]          steeve huang (button) blockedunblock (button) followfollowing
   jan 17, 2018
   [1*o3k5so_pyqnuor55aqzmyg.png]

   in the first part of this series [18]introduction to various
   id23 algorithms. part i (id24, sarsa, id25,
   ddpg), i talked about some basic concepts of id23
   (rl) as well as introducing several basic rl algorithms. in this
   article, i will continue to discuss two more advanced rl algorithms,
   both of which were just published last year. in the end, i am going to
   briefly make a comparison between each of the algorithm i have
   discussed.
     __________________________________________________________________

1. getting started

definition:

     * advantage (a): a(s, a) = q(s, a)- v(s)

   advantage is a term that is commonly used in numerous advanced rl
   algorithms, such as a3c, naf, and the algorithms that i am going to
   discuss (perhaps i will write another blog post for these two
   algorithms). to view it in a more intuitive manner, think of it as how
   good an action is compared to the average action for a specific state.

   but why do we need advantage? isn   t q-value good enough?

   i will use an example posted in this [19]forum to illustrate the idea
   of advantage.

   have you ever played a game called    catch   ? in the game, fruits will be
   dropping down from the top of the screen. you need to move the basket
   right or left in order to catch them.
   [1*9spesk5wd-gvppykdiaoqq.png]
   catch
   ([20]https://datascience.stackexchange.com/questions/15423/understandin
   g-advantage-functions)

   the above image shows a sketch of the game. the circle on the top
   represents a fruit, whereas the small rectangle below is a basket.
   there are three actions, a1, a2, and a3. apparently, the best action is
   a2, not moving at all, as the fruit will directly fall into the basket.
   now, assume that there is no negative reward for any action. in this
   case, the agent does not have the incentive to choose the optimal
   action, a2 in the above scenario. why? let   s use q*(s, a) to denote the
   optimal q value for state s and action a. then we will have:
   [1*nvgy0_bzbtmo4zo_nqluza.png]
   ([21]https://datascience.stackexchange.com/questions/15423/understandin
   g-advantage-functions)

   assume the discount factor      is only slightly smaller than 1. we can
   get
   [1*plduab0jx5_gyhaibxlqoa.png]
   ([22]https://datascience.stackexchange.com/questions/15423/understandin
   g-advantage-functions)

   since there is no negative reward, r(a3) and r(a1) are both greater or
   equal to 0, implying that q*(s, a3) and q*(s, a2) are not very
   different. thus, the agent will only have little preference of a2 over
   a3 in this situation.

   to solve this problem, we can compare the q-value for each action with
   the average of them so that we know how good an action is relative to
   each other. recall from the last blog that the average q-value of a
   state is defined as value (v). essentially, we coin a new operator
   called advantage, which is define by subtracting the q-value for each
   of the action with the value of that state.

2. algorithms illustration

2.1 trust region policy optimization (trpo)

   deep deterministic policy gradient (ddpg) discussed in the last post
   was a break through that allows agent to perform actions in a
   continuous space while maintaining a descent performance. however, the
   main issue of ddpg is that you need to pick the step size that falls
   into the right range. if it is too small, the training progress will be
   extremely slow. if it is too large, conversely, it tends to be
   overwhelmed by the noise, leading to tragic performance. recall that
   the target for calculating the temporal difference (td) error is the
   following:
   [1*htxt8ekobdunpplrntjvqw.png]
   target for td error ([23]https://arxiv.org/pdf/1509.02971.pdf)

   if the step size is selected inappropriately, the target yi derived
   from the networks or function estimators will not be good, leading to a
   even worse sample and worse estimate of the value function.

   therefore, what we need is a way to update parameters that guarantees
   policy improvement. namely, we want the expected discounted long-term
   reward    to be always increasing.
   [1*-jotxwdakk45di71bwigow.png]
   expected discounted long-term reward
   ([24]https://arxiv.org/pdf/1509.02971.pdf)

     warning: there will be numerous mathematical equations and formulas
     in this section. if you are not comfortable with that, feel free to
     jump to the end of this section.

   similar to ddpg, trpo also belongs to the category of policy gradient.
   it adopts the actor-critic architecture, but modifies how the policy
   parameters of the actor are updated.

   for a new policy      ,   (     ) can be viewed as the the expected return of
   policy       in terms of the advantage over   , which is the old policy.
   (since i cannot find    with a curve on it on my keyboard, i will use      
   in the following paragraphs)
   [1*vyrhjledx85xisunoy419q.png]
      for new policy       ([25]https://arxiv.org/pdf/1509.02971.pdf)

   you might wonder why advantage is used. intuitively, you can think of
   it as measuring how good the new policy is with regard to the average
   performance of the old policy.    of the new policy can be rewrite into
   the following form, where     is the discounted visitation frequencies.
   [1*99brnfns9ma5w7fmf4qwhw.png]
   [1*_rvd4e8ubtkxhuiuwtin4g.png]
      rewrite &     ([26]https://arxiv.org/pdf/1509.02971.pdf)

   however, the above formula is hard to be optimized since     is highly
   dependent on the new policy      . therefore, the paper introduced an
   approximation to   (     ), l  (     ):
   [1*uj0qrr6z_9_n4eg7zxxqgw.png]
   approximation of    ([27]https://arxiv.org/pdf/1509.02971.pdf)

   note that we replace       with         , assuming state visitation frequency
   is not too different for the new and the old policies. with this
   equation, we can combine with the well-know policy update method:

   .
   [1*ln3npshgz-fyajrl7llama.png]
   cpi ([28]https://arxiv.org/pdf/1509.02971.pdf)

   here,   _{old} is the current policy, while       is the argument max of
   the policy that maximizes l_{  old}. we will then obtain the following
   theorem (let   s use theorem 1 to denote it).
   [1*em6vxfk-ils06f_4bd87vg.png]
   theorem 1 ([29]https://arxiv.org/pdf/1509.02971.pdf)

   c represents the penalty coefficient, whereas d^{max}_{kl} denotes the
   maximum kl divergence of the two parameter for each of the state. the
   concept of kl divergence was originated from id205,
   describing the information loss. simply put, you can view it as how
   different these two parameters,    and      , are.

   the above formula implies that the expected long-term reward    is
   monotonically improving as long as the right-hand-side term is
   maximized. why? let   s define the right-hand-side of the inequality to
   be m_{i}.
   [1*lu9dweaqhf79m0kefcd7kw.png]
   ([30]https://arxiv.org/pdf/1509.02971.pdf)

   we can then prove the following inequality.
   [1*nubzgst3qljitjpwfxv0qg.png]
   ([31]https://arxiv.org/pdf/1509.02971.pdf)

   the first line can be obtained by simply plugging the definition of
   m_{i} into theorem 1. the second line holds because the kl divergence
   between   _{i} and   _{i} is 0. combining the first and the second line,
   we will get the third line. this shows that as long as m_{i} is
   maximized at every iteration, the objective function    is always
   improving. (i think the last term atthe end of the third line should be
   mi instead of m. not sure if it is a typo if the paper). therefore, the
   complex problem that we are trying to solve now boils down to
   maximizing mi. namely,
   [1*d2lantgkzlzaozb9zobcsw.png]
   objective function 1 ([32]https://arxiv.org/pdf/1509.02971.pdf)

   the following graph visually illustrates the approximation of    with l:
   [1*rduar5coebgz53pvdufhuw.png]
   visual illustration of the approximation
   ([33]https://www.youtube.com/watch?v=xvrrgxcpahy&t=363s)

   in practice, if penalty coefficient is included in the objective
   function, the step size will be very small, leading to long training
   time. consequently, a constraint on the kl divergence is used to allow
   a larger step size while guarantee robust performance.
   [1*fyi0xqnlmwkyyhse2rmeaa.png]
   objective function 2 ([34]https://arxiv.org/pdf/1509.02971.pdf)

   the kl divergence constraint is imposed on every state in the state
   space, the maximum of which should be smaller than a small number     .
   unfortunately, it is not solvable as there are a infinitely large
   number of states. the paper proposed a solution which provides a
   heuristic approximation with the expected kl divergence over states, as
   opposed to finding the maximum kl divergence.
   [1*oz1ntcjtmm20yw0kcjvquw.png]
   kl divergence with state visitation frequency    
   ([35]https://arxiv.org/pdf/1509.02971.pdf)

   now, the objective function becomes the following when we expand the
   first line:
   [1*v3swikyixdvdor5kbuzqbw.png]
   objective function 3 ([36]https://arxiv.org/pdf/1509.02971.pdf)

   by replacing    over states with expectation and    over the actions with
   importance sampling estimator, which is equal to the old policy if
   adopting single path method, we can rewrite the above as:
   [1*mcq6ayvfyxhlvip_dgbqbq.png]
   final objective function ([37]https://arxiv.org/pdf/1509.02971.pdf)

   the objective function is also called a    surrogate    objective function
   as it contains a id203 ratio between current policy and the next
   policy. tpro successfully addresses the problem imposed by ddpg that
   the performance does not improve monotonically. the subset of region
   lies within the constraint is called trust region. as long as the
   policy change is reasonably small, the approximation is not much
   different from the true objective function. by choosing the new policy
   parameters which maximizes the expectation subject to the kl divergence
   constraint, a lower bound of the expected long-term reward    is
   guaranteed. this also implies that you don   t need to worry too much
   about the step size with trpo.

2.2 proximal policy optimization (ppo, openai version)

   although trpo has achieved great and consistent high performance, the
   computation and implementation of it is extremely complicated. in trpo,
   the constraint imposed on the surrogate objective function is the kl
   divergence between the old and the new policy.

   fisher information matrix, a second-order derivative of kl divergence,
   is used to approximate the kl term. this results in computing several
   second-order matrixes, which requires a great amount of computation. in
   the trpo paper, conjugate gradient (cg) algorithm was used to solve the
   constrained optimization problem so that the fisher information matrix
   does not need to be explicitly computed. yet, cg makes implementation
   more complicated.

   ppo gets rid of the computation created by constrained optimization as
   it proposes a clipped surrogate objective function.

   let rt(    ) denotes the ratio between the new and the old policy. the
   surrogate objective function used for approximating long-term reward   
   for trpo becomes the following. note the subscript describes the
   conservative policy iteration (cpi) methods that trpo is based on.
   [1*eo3e07gukhyqxt7o20hwcq.png]
   trpo objective function ([38]https://arxiv.org/pdf/1707.06347.pdf)

   the idea of trpo   s constraint is disallowing the policy to change too
   much. therefore, instead of adding a constraint, ppo slightly modifies
   trpo   s objective function with a penalty for having a too large policy
   update.
   [1*sb-ykxszllh_dhxl8o3zjw.png]
   clipped objective function ([39]https://arxiv.org/pdf/1707.06347.pdf)

   on the right you can see that the id203 ratio rt(    ) is clipped
   between [1-     , 1+    ]. this indicates that if rt(    ) causes the objective
   function to increase to a certain extent, its effectiveness will
   decrease (be clipped). let   s discuss two different cases:
     * case 1: when the advantage   t is greater than 0

   if   t is greater than 0, it means that the action is better than the
   average of all the actions in that state. therefore, the action should
   be encouraged by increasing rt(    ) so that this action has a higher
   chance to be adopted. since the denominator of rt(    ) is constant, the
   old policy, increasing rt(    ) also implies increasing the new policy
         (a, s). namely, increase the chance for taking that action in the
   given state. however, because of the clip, rt(    ) will only grows to as
   much as 1+    .
     * case 2: when the advantage   t is smaller than 0

   by contrast, if   t is smaller than 0, then that action should be
   discouraged. as a result, rt(    ) should be decreased. similarly, due to
   the clip, rt(    ) will only decreases to as little as 1-    .
   [1*fydm-dqksroo5lprqqkfba.png]
   illustration of the clip ([40]https://arxiv.org/pdf/1707.06347.pdf)

   essentially, it restricts the range that the new policy can vary from
   the old one; thus, removing the incentive for the id203 ratio
   rt(    ) to move outside the interval.

   in practice, id168 error and id178 bonus should also be
   considered during implementation as shown below. however, i am not
   going into details of them as the most innovative and important part is
   still the clipped objective function.
   [1*cz0tz14lliu-qimxzvmf4g.png]
   ppo objective function ([41]https://arxiv.org/pdf/1707.06347.pdf)

   comparing the objective function l^{cpi} and l^{clip}, we can observe
   that l^{clip} is in fact a lower bound of the former one. it also
   removes the kl divergence constraint. consequently, the computation for
   optimizing this ppo objective function is much less than that of
   trpo   s. empirically, it also proves that ppo   s performance is better
   than trpo. in fact, thanks to its lightness and ease of implementation,
   ppo has become the default rl algorithm of openai
   ([42]https://blog.openai.com/openai-baselines-ppo/).

3. comparison of discussed algorithms

   [1*beby_ok1mu8wq0haboqevq.png]
   various rl algorithms i have discussed

   all the discussed rl algorithms are model-free. that is, non of them
   are trying to estimate the objective function. instead, they update
   their knowledge based on trial-and-error. among all of them, only sarsa
   is on-policy, learning value based on its current action. id25 was a
   huge improvement from a discrete observation space to a continuous one,
   allowing the agent to handle unseen state. ddpg was another break
   through that enables agent to perform continuous actions with policy
   gradient, broadening the application of rl to more tasks such as
   control. trpo improves the performance of ddpg as it introduces a
   surrogate objective function and a kl divergence constraint,
   guaranteeing non-decreasing long-term reward. ppo further optimizes
   trpo by modifying the surrogate objective function, which improves the
   performance as well as decreasing the complexity of implementation and
   computation.

conclusion

   to sum up, i have introduced two more advanced rl algorithms and
   compared all the rl algorithms that i have discussed. nevertheless, in
   trpo, the mathematical formula is very complicated. although i have
   tried my best to explain it, i believe it might still seems confusing
   to some of you. please feel free to leave a comment below if you have
   any question or follow me on [43]twitter.

     * [44]machine learning
     * [45]id23
     * [46]ai
     * [47]data science
     * [48]towards data science

   (button)
   (button)
   (button) 1.6k claps
   (button) (button) (button) 14 (button) (button)

     (button) blockedunblock (button) followfollowing
   [49]go to the profile of           steeve huang

[50]          steeve huang

   co-founder & cto @ rosetta.ai. kaggle competitions master. beng in cs @
   hkust github: [51]https://github.com/khuangaf linkedin:
   [52]https://goo.gl/7defvr

     (button) follow
   [53]towards data science

[54]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.6k
     * (button)
     *
     *

   [55]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [56]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/87f2c5919bb9
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_xgomc79wt4gg---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@huangkh19951228?source=post_header_lockup
  17. https://towardsdatascience.com/@huangkh19951228
  18. https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-id24-sarsa-id25-ddpg-72a5e0cb6287
  19. https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions
  20. https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions
  21. https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions
  22. https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions
  23. https://arxiv.org/pdf/1509.02971.pdf
  24. https://arxiv.org/pdf/1509.02971.pdf
  25. https://arxiv.org/pdf/1509.02971.pdf
  26. https://arxiv.org/pdf/1509.02971.pdf
  27. https://arxiv.org/pdf/1509.02971.pdf
  28. https://arxiv.org/pdf/1509.02971.pdf
  29. https://arxiv.org/pdf/1509.02971.pdf
  30. https://arxiv.org/pdf/1509.02971.pdf
  31. https://arxiv.org/pdf/1509.02971.pdf
  32. https://arxiv.org/pdf/1509.02971.pdf
  33. https://www.youtube.com/watch?v=xvrrgxcpahy&t=363s
  34. https://arxiv.org/pdf/1509.02971.pdf
  35. https://arxiv.org/pdf/1509.02971.pdf
  36. https://arxiv.org/pdf/1509.02971.pdf
  37. https://arxiv.org/pdf/1509.02971.pdf
  38. https://arxiv.org/pdf/1707.06347.pdf
  39. https://arxiv.org/pdf/1707.06347.pdf
  40. https://arxiv.org/pdf/1707.06347.pdf
  41. https://arxiv.org/pdf/1707.06347.pdf
  42. https://blog.openai.com/openai-baselines-ppo/
  43. https://twitter.com/steeve__huang
  44. https://towardsdatascience.com/tagged/machine-learning?source=post
  45. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  46. https://towardsdatascience.com/tagged/ai?source=post
  47. https://towardsdatascience.com/tagged/data-science?source=post
  48. https://towardsdatascience.com/tagged/towards-data-science?source=post
  49. https://towardsdatascience.com/@huangkh19951228?source=footer_card
  50. https://towardsdatascience.com/@huangkh19951228
  51. https://github.com/khuangaf
  52. https://goo.gl/7defvr
  53. https://towardsdatascience.com/?source=footer_card
  54. https://towardsdatascience.com/?source=footer_card
  55. https://towardsdatascience.com/
  56. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  58. https://medium.com/p/87f2c5919bb9/share/twitter
  59. https://medium.com/p/87f2c5919bb9/share/facebook
  60. https://medium.com/p/87f2c5919bb9/share/twitter
  61. https://medium.com/p/87f2c5919bb9/share/facebook
