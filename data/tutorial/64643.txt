   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id23 w/ keras + openai: id25s

   [16]go to the profile of yash patel
   [17]yash patel (button) blockedunblock (button) followfollowing
   jul 29, 2017

   quick recap

   last time in our keras/openai tutorial, we discussed a very basic
   example of applying deep learning to id23 contexts.
   this was an incredible showing in retrospect! if you looked at the
   training data, the random chance models would usually only be able to
   perform for 60 steps in median. and yet, by training on this seemingly
   very mediocre data, we were able to    beat    the environment (i.e. get
   >200 step performance). how is this possible?

   we can get directly an intuitive feel for this. let   s imagine the
   perfectly random series we used as our training data. it is extremely
   unlikely that any two series will have high overlap with one another,
   since these are generated completely randomly. however, there are key
   features that are common between successful trials, such as pushing the
   cart right when the pole is leaning right and vice versa. and so, by
   training our nn on all these trials data, we extract the shared
   patterns that contributed to them being successful and are able to
   smooth over the details that resulted in their independent failures.

   that being said, the environment we consider this week is significantly
   more difficult than that from last week: the mountaincar.

   more complex environments

   even though it seems we should be able to apply the same technique as
   that we applied last week, there is one key features here that makes
   doing so impossible: we can   t generate training data. unlike the very
   simple cartpole example, taking random movements often simply leads to
   the trial ending in us at the bottom of the hill. that is, we have
   several trials that are all identically -200 in the end. this is
   practically useless to use as training data. imagine you were in a
   class where no matter what answers you put on your exam, you got a 0%!
   how are you going to learn from any of those experiences?
   [1*nbcsvwmys_budz_wajykuw.gif]
   random inputs for the    mountaincar-v0    environment does not produce any
   output that is worthwhile or useful to train on

   in line with that, we have to figure out a way to incrementally improve
   upon previous trials. for this, we use one of the most basic stepping
   stones for id23: id24!

   id25 theory background

   id24 (which doesn   t stand for anything, by the way) is centered
   around creating a    virtual table    that accounts for how much reward is
   assigned to each possible action given the current state of the
   environment. let   s break that down one step at a time:
   [1*wefdub-u4i5hrlunolxj8g.png]
   you can imagine the id25 network as internally maintaining a spreadsheet
   of the values of each of the possible actions that can be taken given
   the current environment state

   what do we mean by    virtual table?    imagine that for each possible
   configuration of the input space, you have a table that assigns a score
   for each of the possible actions you can take. if this were magically
   possible, then it would be extremely easy for you to    beat    the
   environment: simply choose the action that has the highest score! two
   points to note about this score. first, this score is conventionally
   referred to as the    q-score,    which is where the name of the overall
   algorithm comes from. second, as with any other score, these q score
   have no meaning outside the context of their simulation. that is, they
   have no absolute significance, but that   s perfectly fine, since we
   solely need it to do comparisons.

   why then do we need virtual table for each input configuration? why
   can   t we just have one table to rule them all? the reason is that it
   doesn   t make sense to do so: that would be the same as saying the best
   action to take while at the bottom of the valley is exactly that which
   you should take when you are perched on the highest point of the left
   incline.

   now, the main problem with what i described (maintaining a virtual
   table for each input configuration) is that this is impossible: we have
   a continuous (infinite) input space! we could get around this by
   discretizing the input space, but that seems like a pretty hacky
   solution to this problem that we   ll be encountering over and over in
   future situations. so, how do we get around this? by applying neural
   nets to the situation: that   s where the d in id25 comes from!

   id25 agent

   so, we   ve now reduced the problem to finding a way to assign the
   different actions q-scores given the current state. this is the answer
   to a very natural first question to answer when employing any nn: what
   are the inputs and outputs of our model? the extent of the math you
   need to understand for this model is the following equation (don   t
   worry, we   ll break it down):
   [1*iyrdggwa1wsce4nba6u2va.png]

   q, as mentioned, represents the value estimated by our model given the
   current state (s) and action taken (a). the goal, however, is to
   determine the overall value of a state. what do i mean by that? the
   overall value is both the immediate reward you will get and the
   expected rewards you will get in the future from being in that
   position. that is, we want to account for the fact that the value of a
   position often reflects not only its immediate gains but also the
   future gains it enables (damn, deep). in any case, we discount future
   rewards because, if i compare two situations in which i expect to get
   $100 one of the two being in the future, i would always take the
   present deal, since the position of the future one may change between
   when i made the deal and when i receive the money. the gamma factor
   reflects this depreciated value for the expected future returns on the
   state.

   and that   s it: that   s all the math we   ll need for this! time to
   actually move on to some code!

   id25 agent implementation

   the deep q network revolves around continuous learning, meaning that we
   don   t simply accrue a bunch of trial/training data and feed it into the
   model. instead, we create training data through the trials we run and
   feed this information into it directly after running the trial. if this
   all seems somewhat vague right now, don   t worry: time to see some code
   about this. the code largely revolves around defining a id25 class,
   where all the logic of the algorithm will actually be implemented, and
   where we expose a simple set of functions for the actual training.

   id25 hyperparameters

   first off, we   re going to discuss some parameters of relevance for
   id25s. most of them are standard from most neural net implementations:
class id25:
    def __init__(self, env):
        self.env     = env
        self.memory  = deque(maxlen=2000)

        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.01

   let   s step through these one at a time. the first is simply the
   environment, which we supply for convenience when we need to reference
   the shapes in creating our model. the    memory    is a key component of
   id25s: as mentioned previously, the trials are used to continuously
   train the model. however, rather than training on the trials as they
   come in, we add them to memory and train on a random sample of that
   memory. why do this instead of just training on the last x trials as
   our    sample?    the reason is somewhat subtle. imagine instead we were to
   just train on the most recent trials as our sample: in this case, our
   results would only learn on its most recent actions, which may not be
   directly relevant for future predictions. in this environment in
   particular, if we were moving down the right side of the slope,
   training on the most recent trials would entail training on the data
   where you were moving up the hill towards the right. but, this would
   not be at all relevant to determining what actions to take in the
   scenario you would soon be facing of scaling up the left hill. so, by
   taking a random sample, we don   t bias our training set, and instead
   ideally learn about scaling all environments we would encounter equally
   well.

   so, we now discuss hyperparameters of the model: gamma, epsilon/epsilon
   decay, and the learning rate. the first is the future rewards
   depreciation factor (<1) discussed in the earlier equation, and the
   last is the standard learning rate parameter, so i won   t discuss that
   here. the second, however, is an interesting facet of rl that deserves
   a moment to discuss. in any sort of learning experience, we always have
   the choice between exploration vs. exploitation. this isn   t limited to
   computer science or academics: we do this on a day to day basis!

   consider the restaurants in your local neighborhood. when was the last
   time you went to a new one? probably a long time ago. that corresponds
   to your shift from exploration to exploitation: rather than trying to
   find new and better opportunities, you settle with the best one you   ve
   found in your past experiences and maximize your utility from there.
   contrast that to when you moved into your house: at that time, you had
   no idea what restaurants were good or not and so were enticed to
   explore your options. in other words, there   s a clear trend for
   learning: explore all your options when you   re unaware of them, and
   gradually shift over to exploiting once you   ve established opinions on
   some of them. in the same manner, we want our model to capture this
   natural model of learning, and epsilon plays that role.

   epsilon denotes the fraction of time we will dedicate to exploring.
   that is, a fraction self.epsilon of the trials, we will simply take a
   random action rather than the one we would predict to be the best in
   that scenario. as stated, we want to do this more often than not in the
   beginning, before we form stabilizing valuations on the matter, and so
   initialize epsilon to close to 1.0 at the beginning and decay it by
   some fraction <1 at every successive time step.

   id25 models

   there was one key thing that was excluded in the initialization of the
   id25 above: the actual model used for predictions! as in our original
   keras rl tutorial, we are directly given the input and output as
   numeric vectors. so, there   s no need to employ more complex layers in
   our network other than fully connected layers. specifically, we define
   our model just as:
def create_model(self):
        model   = sequential()
        state_shape  = self.env.observation_space.shape
        model.add(dense(24, input_dim=state_shape[0],
            activation="relu"))
        model.add(dense(48, activation="relu"))
        model.add(dense(24, activation="relu"))
        model.add(dense(self.env.action_space.n))
        model.compile(loss="mean_squared_error",
            optimizer=adam(lr=self.learning_rate))
        return model

   and use this to define the model and target model (explained below):
def __init__(self, env):
        self.env     = env
        self.memory  = deque(maxlen=2000)

        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.01
        self.tau = .05
        self.model = self.create_model()
        # "hack" implemented by deepmind to improve convergence
        self.target_model = self.create_model()

   the fact that there are two separate models, one for doing predictions
   and one for tracking    target values    is definitely counter-intuitive.
   to be explicit, the role of the model (self.model) is to do the actual
   predictions on what action to take, and the target model
   (self.target_model) tracks what action we want our model to take.

   why not just have a single model that does both? after all, if
   something is predicting the action to take, shouldn   t it be implicitly
   determine what model we want our model to take? this is actually one of
   those    weird tricks    in deep learning that deepmind developed to get
   convergence in the id25 algorithm. if you use a single model, it can
   (and often does) converge in simple environments (such as the
   cartpole). but, the reason it doesn   t converge in these more complex
   environments is because of how we   re training the model: as mentioned
   previously, we   re training it    on the fly.   

   as a result, we are doing training at each time step and, if we used a
   single network, would also be essentially changing the    goal    at each
   time step. think of how confusing that would be! that would be like if
   a teacher told you to go finish pg. 6 in your textbook and, by the time
   you finished half of it, she changed it to pg. 9, and by the time you
   finished half of that, she told you to do pg. 21! this, therefore,
   causes a lack of convergence by a lack of clear direction in which to
   employ the optimizer, i.e. the gradients are changing too rapidly for
   stable convergence. so, to compensate, we have a network that changes
   more slowly that tracks our eventual goal and one that is trying to
   achieve those.

   id25 training

   the training involves three main steps: remembering, learning, and
   reorienting goals. the first is basically just adding to the memory as
   we go through more trials:
def remember(self, state, action, reward, new_state, done):
        self.memory.append([state, action, reward, new_state, done])

   there   s not much of note here other than that we have to store the done
   phase for how we later update the reward function. moving on to the
   main body of our id25, we have the train function. this is where we make
   use of our stored memory and actively learn from what we   ve seen in the
   past. we start by taking a sample from our entire memory storage. from
   there, we handle each sample different. as we saw in the equation
   before, we want to update the q function as the sum of the current
   reward and expected future rewards (depreciated by gamma). in the case
   we are at the end of the trials, there are no such future rewards, so
   the entire value of this state is just the current reward we received.
   in a non-terminal state, however, we want to see what the maximum
   reward we would receive would be if we were able to take any possible
   action, from which we get:
def replay(self):
        batch_size = 32
        if len(self.memory) < batch_size:
            return
        samples = random.sample(self.memory, batch_size)
        for sample in samples:
            state, action, reward, new_state, done = sample
            target = self.target_model.predict(state)
            if done:
                target[0][action] = reward
            else:
                q_future = max(
                    self.target_model.predict(new_state)[0])
                target[0][action] = reward + q_future * self.gamma
            self.model.fit(state, target, epochs=1, verbose=0)

   and finally, we have to reorient our goals, where we simply copy over
   the weights from the main model into the target one. unlike the main
   train method, however, this target update is called less frequently:
def target_train(self):
        weights = self.model.get_weights()
        target_weights = self.target_model.get_weights()
        for i in range(len(target_weights)):
            target_weights[i] = weights[i]
        self.target_model.set_weights(target_weights)

   id25 action

   the final step is simply getting the id25 to actually perform the
   desired action, which alternates based on the given epsilon parameter
   between taking a random action and one predicated on past training, as
   follows:
def act(self, state):
        self.epsilon *= self.epsilon_decay
        self.epsilon = max(self.epsilon_min, self.epsilon)
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        return np.argmax(self.model.predict(state)[0])

   training agent

   training the agent now follows naturally from the complex agent we
   developed. we have to instantiate it, feed it the experiences as we
   encounter them, train the agent, and update the target network:
def main():
    env     = gym.make("mountaincar-v0")
    gamma   = 0.9
    epsilon = .95
    trials  = 100
    trial_len = 500
    updatetargetnetwork = 1000
    id25_agent = id25(env=env)
    steps = []
    for trial in range(trials):
        cur_state = env.reset().reshape(1,2)
        for step in range(trial_len):
            action = id25_agent.act(cur_state)
            env.render()
            new_state, reward, done, _ = env.step(action)
            reward = reward if not done else -20
            print(reward)
            new_state = new_state.reshape(1,2)
            id25_agent.remember(cur_state, action,
                reward, new_state, done)

            id25_agent.replay()
            id25_agent.target_train()
            cur_state = new_state
            if done:
                break
        if step >= 199:
            print("failed to complete trial")
        else:
            print("completed in {} trials".format(trial))
            break

   complete code

   with that, here is the complete code used for training against the
      mountaincar-v0    environment using id25!

   iframe: [18]/media/d989aded95a0d7dd02464fc56ddbb54e?postid=1eed3a5338c

   keep an eye out for the next keras+openai tutorial!

comment and click that        below to show support!

     * [19]machine learning
     * [20]keras
     * [21]deep leraning
     * [22]id23
     * [23]towards data science

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 9 (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of yash patel

[25]yash patel

   developer interested in id161, graphics, and vr working at
   oculus vr (facebook). graduate from princeton university
   ([26]http://www.ypatel.io)

     (button) follow
   [27]towards data science

[28]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [29]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [30]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1eed3a5338c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-w-keras-openai-id25s-1eed3a5338c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-w-keras-openai-id25s-1eed3a5338c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_oilabv5gosfw---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@yashpatel_86510?source=post_header_lockup
  17. https://towardsdatascience.com/@yashpatel_86510
  18. https://towardsdatascience.com/media/d989aded95a0d7dd02464fc56ddbb54e?postid=1eed3a5338c
  19. https://towardsdatascience.com/tagged/machine-learning?source=post
  20. https://towardsdatascience.com/tagged/keras?source=post
  21. https://towardsdatascience.com/tagged/deep-leraning?source=post
  22. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  23. https://towardsdatascience.com/tagged/towards-data-science?source=post
  24. https://towardsdatascience.com/@yashpatel_86510?source=footer_card
  25. https://towardsdatascience.com/@yashpatel_86510
  26. http://www.ypatel.io/
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/1eed3a5338c/share/twitter
  33. https://medium.com/p/1eed3a5338c/share/facebook
  34. https://medium.com/p/1eed3a5338c/share/twitter
  35. https://medium.com/p/1eed3a5338c/share/facebook
