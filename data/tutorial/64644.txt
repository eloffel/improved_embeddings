   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

how to build a neural network with keras

   [16]go to the profile of niklas donges
   [17]niklas donges (button) blockedunblock (button) followfollowing
   apr 3, 2018
   [1*ijnredchu5_wqotbvdli8w.jpeg]

   keras is one of the most popular deep learning libraries out there at
   the moment and made a big contribution to the commoditization of
   artificial intelligence. it is simple to use and it enables you to
   build powerful neural networks in just a few lines of code. in this
   post, you will discover how you can build a neural network with keras
   that predicts the sentiment of user reviews by categorizing them into
   two categories: positive or negative. this is called id31
   and we will do it with the famous imdb review dataset. the model we
   will build can also be applied to other machine learning problems with
   just a few changes.

   note that we will not go into the details of keras or deep learning.
   this post is intended to provide you with a blueprint of a keras neural
   network and to make you familiar with its implementation.

table of contents:

     * what is keras?
     * what is id31?
     * the imdb dataset
     * import dependencies and get the data
     * exploring the data
     * data preparation
     * building and training the model

what is keras?

   keras is an open source python library that enables you to easily build
   neural networks. the library is capable of running on top of
   tensorflow, microsoft cognitive toolkit, theano, and mxnet. tensorflow
   and theano are the most used numerical platforms in python to build
   deep learning algorithms but they can be quite complex and difficult to
   use. in comparison, keras provides an easy and convenient way to build
   deep learning models. it   s creator fran  ois chollet developed it to
   enable people to build neural networks as fast and easy as possible. he
   laid his focus on extensibility, modularity, minimalism and the support
   of python. keras can be used with gpus and cpus and it supports both
   python 2 and 3. google keras made a big contribution to the
   commoditization of deep learning and artificial intelligence since it
   has commoditized powerful, modern deep learning algorithms that
   previously were not only inaccessible but also unusable as well.

what is id31?

   with id31, we want to determine the attitude (e.g the
   sentiment) of for example a speaker or writer with respect to a
   document, interaction, or event. therefore it is a natural language
   processing problem where text needs to be understood, to predict the
   underlying intent. the sentiment is mostly categorized into positive,
   negative and neutral categories. with the use of id31, we
   want to predict for example a customers opinion and attitude about a
   product based on a review he wrote about it. because of that, sentiment
   analysis is widely applied to things like reviews, surveys, documents
   and much more.
     __________________________________________________________________

the imdb dataset

   the imdb sentiment classification dataset consists of 50,000 movie
   reviews from imdb users that are labeled as either positive (1) or
   negative (0). the reviews are preprocessed and each one is encoded as a
   sequence of word indexes in the form of integers. the words within the
   reviews are indexed by their overall frequency within the dataset. for
   example, the integer    2    encodes the second most frequent word in the
   data. the 50,000 reviews are split into 25,000 for training and 25,000
   for testing. the dataset was created by researchers of the stanford
   university and published in a paper in 2011, where they achieved 88.89%
   accuracy. it was also used within the    bag of words meets bags of
   popcorn    kaggle competition in 2011.

import dependencies and get the data

   we start by importing the required dependencies to preprocess our data
   and to build our model.
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from keras.utils import to_categorical
from keras import models
from keras import layers

   we continue with downloading the imdb dataset, which is fortunately
   already built into keras. since we don   t want to have a 50/50 train
   test split, we will immediately merge the data into data and targets
   after downloading, so that we can do an 80/20 split later on.
from keras.datasets import imdb
(training_data, training_targets), (testing_data, testing_targets) = imdb.load_d
ata(num_words=10000)
data = np.concatenate((training_data, testing_data), axis=0)
targets = np.concatenate((training_targets, testing_targets), axis=0)

exploring the data

   now we can start exploring the dataset:
print("categories:", np.unique(targets))
print("number of unique words:", len(np.unique(np.hstack(data))))

categories: [0 1]
number of unique words: 9998
length = [len(i) for i in data]
print("average review length:", np.mean(length))
print("standard deviation:", round(np.std(length)))

average review length: 234.75892
standard deviation: 173.0

   you can see in the output above that the dataset is labeled into two
   categories, either 0 or 1, which represents the sentiment of the
   review. the whole dataset contains 9998 unique words and the average
   review length is 234 words, with a standard deviation of 173 words.

   now we will look at a single training example:
print("label:", targets[0])

label: 1
print(data[0])

[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 2
56, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112,
167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16,
6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38
, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 3
16, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5,
 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407,
 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 53
0, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2
071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 3
0, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16,
 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]

   above you see the first review of the dataset which is labeled as
   positive (1). the code below retrieves the dictionary mapping word
   indices back into the original words so that we can read them. it
   replaces every unknown word with a    #   . it does this by using the
   [18]get_word_index() function.
index = imdb.get_word_index()
reverse_index = dict([(value, key) for (key, value) in index.items()])
decoded = " ".join( [reverse_index.get(i - 3, "#") for i in data[0]] )
print(decoded)

# this film was just brilliant casting location scenery story direction everyone
's really suited the part they played and you could just imagine being there rob
ert # is an amazing actor and now the same being director # father came from the
 same scottish island as myself so i loved the fact there was a real connection
with this film the witty remarks throughout the film were great it was just bril
liant so much that i bought the film as soon as it was released for # and would
recommend it to everyone to watch and the fly fishing was amazing really cried a
t the end it was so sad and you know what they say if you cry at a film it must
have been good and this definitely was also # to the two little boy's that playe
d the # of norman and paul they were just brilliant children are often left out
of the # list i think because the stars that play them all grown up are such a b
ig profile for the whole film but these children are amazing and should be prais
ed for what they have done don't you think the whole story was so lovely because
 it was true and was someone's life after all that was shared with us all

data preparation

   now it is time to prepare our data. we will vectorize every review and
   fill it with zeros so that it contains exactly 10,000 numbers. that
   means we fill every review that is shorter than 10,000 with zeros. we
   do this because the biggest review is nearly that long and every input
   for our neural network needs to have the same size. we also transform
   the targets into floats.
def vectorize(sequences, dimension = 10000):
results = np.zeros((len(sequences), dimension))
for i, sequence in enumerate(sequences):
results[i, sequence] = 1
return results

data = vectorize(data)
targets = np.array(targets).astype("float32")

   now we split our data into a training and a testing set. the training
   set will contain 40,000 reviews and the testing set 10,000.
test_x = data[:10000]
test_y = targets[:10000]
train_x = data[10000:]
train_y = targets[10000:]

building and training the model

   we can now build our simple neural network. we start by defining the
   type of model we want to build. there are two types of models available
   in keras: [19]the sequential model and [20]the model class used with
   functional api.

   then we simply add the input-, hidden- and output-layers. between them,
   we are using dropout to prevent overfitting. note that you should
   always use a dropout rate between 20% and 50%. at every layer, we use
      dense    which means that the units are fully connected. within the
   hidden-layers, we use the relu function, because this is always a good
   start and yields a satisfactory result most of the time. feel free to
   experiment with other id180. and at the output-layer, we
   use the sigmoid function, which maps the values between 0 and 1. note
   that we set the input-shape to 10,000 at the input-layer, because our
   reviews are 10,000 integers long. the input-layer takes 10,000 as input
   and outputs it with a shape of 50.

   lastly, we let keras print a summary of the model we have just built.
# input - layer
model.add(layers.dense(50, activation = "relu", input_shape=(10000, )))
# hidden - layers
model.add(layers.dropout(0.3, noise_shape=none, seed=none))
model.add(layers.dense(50, activation = "relu")
model.add(layers.dropout(0.2, noise_shape=none, seed=none))
model.add(layers.dense(50, activation = "relu"))
# output- layer
model.add(layers.dense(1, activation = "sigmoid"))model.summary()
model.summary()

_________________________________________________________________
layer (type)                 output shape              param #
=================================================================
dense_1 (dense)              (none, 50)                500050
_________________________________________________________________
dropout_1 (dropout)          (none, 50)                0
_________________________________________________________________
dense_2 (dense)              (none, 50)                2550
_________________________________________________________________
dropout_2 (dropout)          (none, 50)                0
_________________________________________________________________
dense_3 (dense)              (none, 50)                2550
_________________________________________________________________
dense_4 (dense)              (none, 1)                 51
=================================================================
total params: 505,201
trainable params: 505,201
non-trainable params: 0
_________________________________________________________________

   now we need to compile our model, which is nothing but configuring the
   model for training. we use the    adam    optimizer. the optimizer is the
   algorithm that changes the weights and biases during training. we also
   choose binary-crossid178 as loss (because we deal with binary
   classification) and accuracy as our evaluation metric.
model.compile(
 optimizer = "adam",
 loss = "binary_crossid178",
 metrics = ["accuracy"]
)

   we are now able to train our model. we do this with a batch_size of 500
   and only for two epochs because i recognized that the model overfits if
   we train it longer. the batch size defines the number of samples that
   will be propagated through the network and an epoch is an iteration
   over the entire training data. in general a larger batch-size results
   in faster training, but don   t always converges fast. a smaller
   batch-size is slower in training but it can converge faster. this is
   definitely problem dependent and you need to try out a few different
   values. if you start with a problem for the first time, i would you
   recommend to you to first use a batch-size of 32, which is the standard
   size.
results = model.fit(
 train_x, train_y,
 epochs= 2,
 batch_size = 500,
 validation_data = (test_x, test_y)
)

train on 40000 samples, validate on 10000 samples
epoch 1/2
40000/40000 [==============================] - 5s 129us/step - loss: 0.4051 - ac
c: 0.8212 - val_loss: 0.2635 - val_acc: 0.8945
epoch 2/2
40000/40000 [==============================] - 4s 90us/step - loss: 0.2122 - acc
: 0.9190 - val_loss: 0.2598 - val_acc: 0.8950

   it is time to evaluate our model:
print(np.mean(results.history["val_acc"]))

0.894750000536

   awesome! with this simple model, we already beat the accuracy of the
   2011 paper that i mentioned in the beginning. feel free to experiment
   with the hyperparameters and the number of layers.

   you can see the code for the whole model below:
import numpy as np
from keras.utils import to_categorical
from keras import models
from keras import layers
from keras.datasets import imdb
(training_data, training_targets), (testing_data, testing_targets) = imdb.load_d
ata(num_words=10000)
data = np.concatenate((training_data, testing_data), axis=0)
targets = np.concatenate((training_targets, testing_targets), axis=0)
def vectorize(sequences, dimension = 10000):
 results = np.zeros((len(sequences), dimension))
 for i, sequence in enumerate(sequences):
  results[i, sequence] = 1
 return results

data = vectorize(data)
targets = np.array(targets).astype("float32")
test_x = data[:10000]
test_y = targets[:10000]
train_x = data[10000:]
train_y = targets[10000:]
model = models.sequential()
# input - layer
model.add(layers.dense(50, activation = "relu", input_shape=(10000, )))
# hidden - layers
model.add(layers.dropout(0.3, noise_shape=none, seed=none))
model.add(layers.dense(50, activation = "relu"))
model.add(layers.dropout(0.2, noise_shape=none, seed=none))
model.add(layers.dense(50, activation = "relu"))
# output- layer
model.add(layers.dense(1, activation = "sigmoid"))
model.summary()
# compiling the model
model.compile(
 optimizer = "adam",
 loss = "binary_crossid178",
 metrics = ["accuracy"]
)
results = model.fit(
 train_x, train_y,
 epochs= 2,
 batch_size = 500,
 validation_data = (test_x, test_y)
)
print("test-accuracy:", np.mean(results.history["val_acc"]))

summary

   in this post you learned what id31 is and why keras is
   one of the most used deep learning libraries. on top of that you
   learned that keras made a big contribution to the commoditization of
   deep learning and artificial intelligence. you learned how to build a
   simple neural network with six layers that can predict the sentiment of
   movie reviews, which achieves a 89% accuracy. you can now use this
   model to also do binary id31 on other sources of text but
   you need to change them all to a length of 10,000 or you change the
   input-shape of the input layer. you can also apply this model to other
   related machine learning problems with only a few changes.

sources:

   [21]https://keras.io/datasets/

   [22]https://en.wikipedia.org/wiki/sentiment_analysis

   [23]https://machinelearningmastery.com/introduction-python-deep-learnin
   g-library-keras/
   [24]keras documentation
   building a id53 system, an image classification model, a
   id63, or any other model is   keras.io

   this post was initially published at my blog
   ([25]https://machinelearning-blog.com).

     * [26]machine learning
     * [27]towards data science
     * [28]keras
     * [29]deep learning
     * [30]data science

   (button)
   (button)
   (button) 2.2k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of niklas donges

[32]niklas donges

   co-founder: markov-solutions.com     ai solutions & consulting |
   linkedin.com/in/niklas-donges | machinelearning-blog.com

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.2k
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e8faa33d0ae4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_r3j7a9iqrpdv---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@n.donges?source=post_header_lockup
  17. https://towardsdatascience.com/@n.donges
  18. http://get_word_index/
  19. https://keras.io/models/sequential
  20. https://keras.io/models/model
  21. https://keras.io/datasets/
  22. https://en.wikipedia.org/wiki/sentiment_analysis
  23. https://machinelearningmastery.com/introduction-python-deep-learning-library-keras/
  24. https://keras.io/
  25. https://machinelearning-blog.com/
  26. https://towardsdatascience.com/tagged/machine-learning?source=post
  27. https://towardsdatascience.com/tagged/towards-data-science?source=post
  28. https://towardsdatascience.com/tagged/keras?source=post
  29. https://towardsdatascience.com/tagged/deep-learning?source=post
  30. https://towardsdatascience.com/tagged/data-science?source=post
  31. https://towardsdatascience.com/@n.donges?source=footer_card
  32. https://towardsdatascience.com/@n.donges
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://keras.io/
  39. https://medium.com/p/e8faa33d0ae4/share/twitter
  40. https://medium.com/p/e8faa33d0ae4/share/facebook
  41. https://medium.com/p/e8faa33d0ae4/share/twitter
  42. https://medium.com/p/e8faa33d0ae4/share/facebook
