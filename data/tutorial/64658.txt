   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id205 of neural networks

opening the black box    . somewhat

   [16]go to the profile of mukul malik
   [17]mukul malik (button) blockedunblock (button) followfollowing
   apr 4, 2018
   [1*cdrrlhnd5krqwv9is7i8kw.png]

        information: the negative reciprocal value of
     id203.            claude shannon

   aim of this blog is not to understand the underlying mathematical
   concepts behind neural network but to visualise neural networks in
   terms of information manipulation.
     __________________________________________________________________

encoder-decoder

   before we start:

     encoder-decoder is not two id98s/id56s combined together! neither have
     to be neural network in fact!

   [1*icoovmpf3er-hscowjxr6q.jpeg]

   originally, a concept of id205. [18]encoder is simply
   compresses the information and decoder expands the encoded information.

   in case of machine learning, both encoding and decoding are both
   lose-full processes i.e. some information is always lost.

   the encoded output of the encoder is called the context vector and this
   is the input for the decoder.

   there are two ways to setup an encoder-decoder setting:
    1. the decoder in inverse function of encoder. this way the decoder
       tries to reproduce the original information. this is used to
       de-noise the data. this setting has a special name, called
       auto-encoder.
    2. the encoder is a compression algorithm and decoder is generating
       algorithm. this helps to transfer context from one format to
       another.

     example applications:

     auto-encoder: encoder compressing english text into a vector.
     decoder generating original english text from the vector.

     encoder-decoder: encoder compressing english text into a vector.
     decoder generating french translation of the original text from the
     vector.

     encoder-decoder: encoder compressing english text into a vector.
     decoder generating image from the content of text.
     __________________________________________________________________

id205

   now, if i say every neural network, itself, is an encoder-decoder
   setting; it would sound absurd to most.

   let   s re-imagine the neural networks.

   let input layer be x and their real tags/classes (present in the
   training set) be y. now we already know neural networks find the
   underlying function between x and y.

   so x can be seen as high-id178 distribution of y. high id178
   because x contains the information y but it also a lot of other
   information.

     example:

        this boy is good.    contains enough information to tell us about
     it   s    positive    sentiment.

     but.

     it also contains things like:

     1. it   s a specific boy

     2. it   s just one boy

     3. tense of sentence is present

     now no-id178 version of this sentence would be    positive   . yes
     that   s also the output. we   ll come back to that in a while.

   now imagine every hidden layer as a single variable h ( so layers will
   be named as h0, h1    .. h(n-1) )

   now every layer becomes a variable and the neural network becomes a
   [19]markov chain. markov chain because each variable is only dependent
   upon the previous layer only.

   so essentially each layer forms a partisan of information.

   following is a visualisation of a neural network as a markov chain.
   [1*xomkosgt4-icsu8nnqu0mg.jpeg]

   the last layer y_ is supposed to result in least id178 output (with
   respect to    y   , the original tag/class).

   this process of obtaining y_ is squeezing the information in x layer as
   it flows through h layers and retaining only the information most
   relevant to y. this is the information bottleneck.
     __________________________________________________________________

mutual information

   [1*gxaa5kmlrl4p-fdsjdtrda.jpeg]

   i(x,y) = h(x)         h(x|y)

   h -> id178

   h(x) -> id178 of x

   h(x|y) -> conditional id178 of x given y

   in other words, h(x|y) signifies how much uncertainty is removed form x
   if y is known.

properties of mutual information

    1. when you move along the markov chain the mutual information only
       decreases
    2. mutual information is invariant to reparameterization i.e.
       shuffling values in a layer won   t change the output
     __________________________________________________________________

revisiting bottleneck

   in the markov representation of neural network, every layer becomes a
   partition of information.

   in id205 these partitions are known as successive
   refinement of relevant information. you don   t have to worry about the
   details.

   another way of seeing this is the input being encoded and decoded into
   the output.
   [1*fyoyhqxxjnrzn7sxsukija.jpeg]

   so, for enough hidden layers:
    1. the sample complexity of deep neural network is determined by
       encoded mutual information at last hidden layer
    2. the accuracy is determined by decoded mutual information of last
       hidden layer

   [1*dbvnyk7nacjhhvutuzyedg.jpeg]

   sample complexity is the number and variety of examples one needs to
   receive certain accuracy.
     __________________________________________________________________

mutual information along the training phase

   we calculate mutual information between
    1. layer and input
    2. layer and output

   [1*1rm8so6vbddn5owfjedrha.jpeg]
   initial conditions

   initially,weights are randomly initialised. so barely anything is known
   about the correct output. with successive layers the mutual information
   about input decreases and the information in hidden layers about the
   output is low as well.
   [1*-_ybjhj9h_6szr1a6yj5yq.jpeg]
   compression phase

   as we train the neural network the plots start moving up, signifying
   gain of information about the output.

   but.

   plots also start shifting towards the right side, signifying increase
   of information in latter layers about the input.

   this his the longest phase. here the density of the plots is maximum
   and plots are concentrated at the top right. this signifies compression
   of information about the input in relation to the output.

   this is called the compression phase.
   [1*jxnrt_1d2zbxk5l6ejvnnq.jpeg]
   expansion phase

   after the compression phase, the plots starts shifting towards the top
   but also to left side.

   this signifies, with successive layers, there is loss of information
   about the input and what   s retained in the last layer is the lowest
   id178 information about the output.
     __________________________________________________________________

virtualisation

   the markov chain version of neural network highlights one more point,
   learning happens from layer to layer. a layer has all the information
   it needs to predict the output (plus some noise).

   so we use each layer to predict the output. this helps us peep into the
   layer-wise knowledge of the so called black box.

   this gives us a perspective about how many layers are required to make
   an accurate enough prediction of output. if saturation is achieved at
   earlier layers, the layers succeeding this layer can be pruned/dropped.

   these layers are usually very of hundreds or thousands of dimensions.
   our evolution doesn   t allow us to visualise anything beyond the
   3-dimensions. so we use dimension reduction techniques.

   there are various methods of performing dimension reduction. cristopher
   olah has a brilliant [20]blog explaining those methods. i won   t go into
   the details of id167, you can check this [21]blog for details.

   to keep it short, id167 tries to reduce dimension retaining the
   neighbours from higher dimensions in lower dimensions. so this results
   in pretty accurate 2d and 3d plots.

   following are the layer plots of a language model with 2 layers.

   about the plots:
    1. selected 16 words
    2. used the final language model to find n synonyms of the above 16
       words (n=200 for 2d and n=50 for 3d)
    3. find the representation vector for each word at each layer
    4. find 2d and 3d reduced representation of each of above selected
       words and their synonyms using id167
    5. plot the reduced representations

   [1*tkvva6cxm6rcl0ghgps4zq.png]
   [1*7hqptclrdgwcm2t_49wxzq.png]
   2d plots of layer 1 and layer 2
   [1*pby873h94s7r5qkm7gisia.png]
   [1*9m3jifqpi2klhc3iwvaufg.png]
   3d plots of layer 1 and layer 2
     __________________________________________________________________

summary

    1. most every deep neural network acts like an encoder-decoder
    2. most of the training time is spent in compression phase
    3. layers are learned bottom-up
    4. beyond compression phase, more the neural network forgets about the
       input, the more accurate it gets (wiping out the irrelevant part of
       input).

   i have excluded mathematics from this blog. if you are comfortable
   enough with the mathematics of id205, game theory,
   learning theory etc then do watch this [22]video of the mastero naftali
   tishby.
     __________________________________________________________________

     thank you \(-_- )/

     * [23]machine learning
     * [24]id205
     * [25]deep learning
     * [26]artificial intelligence

   (button)
   (button)
   (button) 946 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of mukul malik

[28]mukul malik

   nlu & metacognition researcher | cosmology enthusiast

     (button) follow
   [29]towards data science

[30]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 946
     * (button)
     *
     *

   [31]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [32]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ad4053f8e177
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/information-theory-of-neural-networks-ad4053f8e177&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/information-theory-of-neural-networks-ad4053f8e177&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_p42atffqawbs---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@mukulmalik?source=post_header_lockup
  17. https://towardsdatascience.com/@mukulmalik
  18. https://www.cs.toronto.edu/~hinton/science.pdf
  19. https://brilliant.org/wiki/markov-chains/
  20. http://colah.github.io/posts/2014-10-visualizing-mnist/
  21. https://distill.pub/2016/misread-tsne/
  22. https://www.youtube.com/watch?v=blqjhjxihk8&t=856s
  23. https://towardsdatascience.com/tagged/machine-learning?source=post
  24. https://towardsdatascience.com/tagged/information-theory?source=post
  25. https://towardsdatascience.com/tagged/deep-learning?source=post
  26. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  27. https://towardsdatascience.com/@mukulmalik?source=footer_card
  28. https://towardsdatascience.com/@mukulmalik
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/ad4053f8e177/share/twitter
  35. https://medium.com/p/ad4053f8e177/share/facebook
  36. https://medium.com/p/ad4053f8e177/share/twitter
  37. https://medium.com/p/ad4053f8e177/share/facebook
