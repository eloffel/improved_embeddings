   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

implementing id97 in pytorch (skip-gram model)

   [16]go to the profile of mateusz bednarski
   [17]mateusz bednarski (button) blockedunblock (button) followfollowing
   mar 6, 2018

   you probably have heard about id97 embedding. but do you really
   understand how it works? i though i do. but i have not, until
   implemented it.

   this is why i   m creating this guide.

prerequisites

   i assume you know more-less what id97 is.

   corpus

   in order to be able to track every single step i   m using following nano
   corpus:

   iframe: [18]/media/0f257a3962702611e32fdbe68e16cc35?postid=e6bae040d2fb

creating vocabulary

   very first step is id97 to create the vocabulary. it has to be
   built at the beginning, as extending it is not supported.

   vocabulary is basically a list of unique words with assigned indices.

   corpus is very simple and short. in real implementation we would have
   to perform case id172, removing some punctuation etc, but for
   simplicity let   s use this nice and clean data. anyway, we have to
   tokenize it:

   iframe: [19]/media/06f2dad88564141b326acf8afebb61e7?postid=e6bae040d2fb

   this will give us a list of tokens:
[['he', 'is', 'a', 'king'],
 ['she', 'is', 'a', 'queen'],
 ['he', 'is', 'a', 'man'],
 ['she', 'is', 'a', 'woman'],
 ['warsaw', 'is', 'poland', 'capital'],
 ['berlin', 'is', 'germany', 'capital'],
 ['paris', 'is', 'france', 'capital']]

   we iterate over tokens in corpus, and generate list of unique
   words(tokens). next we create two dictionaries for mapping between word
   and index.

   iframe: [20]/media/2fcd64b2823d40866dfcb25dc4687790?postid=e6bae040d2fb

   which gives us:
 0: 'he',
 1: 'is',
 2: 'a',
 3: 'king',
 4: 'she',
 5: 'queen',
 6: 'man',
 7: 'woman',
 8: 'warsaw',
 9: 'poland',
 10: 'capital',
 11: 'berlin',
 12: 'germany',
 13: 'paris',
 14: 'france'

   we can now generate pairs center word, context word. let   s assume
   context window to be symmetric and equal to 2.

   iframe: [21]/media/88211cd7247680d704a93f58be484116?postid=e6bae040d2fb

   it gives us list of pairs center, context indices:
array([[ 0,  1],
       [ 0,  2],
       ...

   which can be easily translated to words:
he is
he a
is he
is a
is king
a he
a is
a king

   which makes perfect sense.
   [1*uyiqfnruizkdmrmkbwgmpw.png]

defining goal

   now, we are going through details from very first equation to working
   implementation.

   for skip-gram we are interested in predicting context, given center
   word and some parametrization. this is our id203 distribution for
   single pair.
   [0*iamm9c9rxl-hx4zr.]
   e.g.: p(king | is)

   now, we want to maximize it trough all word/context pairs.
   [0*hqjpvlwslmuwu8zd.]

   wait, why?

   as we are interested in predicting context given center word, we want
   to maximize p(context|center) for each context, center pair. as
   id203 sums up to 1         we are implicitly making p(context|center)
   close to 0 for all non-existing context, center pairs. by multiplying
   those probabilities we make this function close to 1 if our model is
   good and close to 0 if it is bad. of course we are pursuing a good
   one         so there is max operator at the beginning.

   this expression is not very suitable to compute. that   s why we are
   going to perform some very common transformations.

   step 1         replace id203 with negative log likelihood.

   recall that neural nets are about minimizing id168. we could
   simply multiply p by minus one , but applying log gives us better
   computational properties. this does not change position of function
   extrema (as log is a strictly monotonic function). so expression is
   changed to:
   [0*lonbnqk0w-94h5gz.]

   step 2         replace products with sums

   next step is to replace products with sums. we can do it because:
   [0*jfp8v0nfis7o8wma.]

   step 3         transform to a proper id168

   and after dividing by number of pars (t) we have our final loss term:
   [0*lak7q89tc39okc_5.]

define p

   great, but how we do define p(context|center)? for now, let   s assume
   that reach word has actually two vectors. one if is present as center
   word (v), and second one if context(u). given that definition for p
   looks following:
   [0*aotdifvvn-jyc93v.]

   that   s scary!

   let me break it down to smaller parts. see following structure:
   [0*-ozb9lj57nbqph3w.]
   softmax!

   this is just a softmax function. now closer look at nominator
   [0*aeub9dww-qajinkr.]

   both u and v are vectors. this expression is just scalar product of
   given center, context pair. bigger as they are more similar to each
   other.

   now, denominator:
   [0*esjlk6osar-z756d.]

   we   re iterating over all words in vocabulary.
   [0*dmc4l4xnndavjmp1.]

   and computing    similarity    for given center word and each word in
   vocabulary treated as context word.

   to sum up:

   for each existing center, context pair in corpus we   re computing their
      similarity score   . and divide it by sum of each theoretically possible
   context         to know whether score is relatively high or low. as softmax
   is guaranteed to take a value between 0 and 1 it defines a valid
   id203 distribution.

great, now let   s code it!

   neural net implementing this concept consists of three layers: input,
   hidden and output.

input layer

   input layer is just the center word encoded in one-hot manner. it
   dimensions are [1, vocabulary_size]

   iframe: [22]/media/dc9106c1aa77d6ea804ff41a06d5b6b3?postid=e6bae040d2fb

hidden layer

   hidden layer makes our v vectors. therefore it has to have
   embedding_dims neurons. to compute it value we have to define w1 weight
   matrix. of course its has to be [embedding_dims, vocabulary_size].
   there is no activation function         just plain id127.

   iframe: [23]/media/bb9e1bb685dd741485e970a9ec3462d2?postid=e6bae040d2fb

   what   s important         each column of w1 stores v vector for single word.
   why? because x is one-hot and if you multiply one-hot vector by matrix,
   result is same as selecting select single column from it. try on your
   own using a piece of paper ;)

output layer

   last layer must have vocabulary_size neurons         because it generates
   probabilities for each word. therefore, w2 is [vocabulary_size,
   embedding_dims] in terms of shape.

   iframe: [24]/media/ad8c1ae9417c679e391b8bef1442d8b7?postid=e6bae040d2fb

   on top on that we have to use softmax layer. pytorch provides optimized
   version of this, combined with log         because regular softmax is not
   really numerically stable:
log_softmax = f.log_softmax(a2, dim=0)

   this is equivalent to compute softmax and after that applying log.

   now we can compute loss. as usual pytorch provides everything we need:
loss = f.nll_loss(log_softmax.view(1,-1), y_true)

   the nll_loss computes negative-log-likelihood on logsoftmax. y_true is
   context word         we want to make this as high as possible         because pair
   x, y_true is from training data         so the are indeed center, context.

backprop

   as we fished forward pass, now it   s time to perform backward pass.
   simply:
loss.backward()

   for optimization sdg is used. it is so simple, that it was faster to
   write it by hand instead of creating optimizer object:
w1.data -= 0.01 * w1.grad.data
w2.data -= 0.01 * w2.grad.data

   last step is to zero gradients to make next pass clear:
w1.grad.data.zero_()
w2.grad.data.zero_()

training loop

   time to compile it into training loop. it can look like:

   iframe: [25]/media/839bb918bd3e649e816c224e2dbb95d3?postid=e6bae040d2fb

   one potentially tricky thing is y_true definition. we do not create
   one-hot explicitly         nll_loss does it by itself.
loss at epo 0: 4.241989389487675
loss at epo 10: 3.8398486052240646
loss at epo 20: 3.5548086541039603
loss at epo 30: 3.343840673991612
loss at epo 40: 3.183084646293095
loss at epo 50: 3.05673006943294
loss at epo 60: 2.953996729850769
loss at epo 70: 2.867735825266157
loss at epo 80: 2.79331214427948
loss at epo 90: 2.727727291413716
loss at epo 100: 2.6690095041479385

extract vectors

   ok, we have trained the network. one, last thing is to extract vectors
   for words. it is possible in three ways:
     * use vector v from w1
     * use vector u from w2
     * use average v and u

   try to think on your own when to use which one ;)

to be continued

   i   m working on online interactive demonstration on this. it should be
   available soon. stay tuned ;)

   you can download code from [26]here.
   [1*-f-igl53j6bnpssjpswpfq.png]
   working on this!

     * [27]machine learning
     * [28]nlp
     * [29]id97
     * [30]pytorch
     * [31]python

   (button)
   (button)
   (button) 709 claps
   (button) (button) (button) 11 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of mateusz bednarski

[33]mateusz bednarski

   ai enthusiast. focused mostly on nlp and good software engineering
   practices for machine learning projects. currently working at roche.

     (button) follow
   [34]towards data science

[35]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 709
     * (button)
     *
     *

   [36]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [37]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e6bae040d2fb
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/implementing-id97-in-pytorch-skip-gram-model-e6bae040d2fb&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/implementing-id97-in-pytorch-skip-gram-model-e6bae040d2fb&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_btjftomfeq8q---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@mbednarski?source=post_header_lockup
  17. https://towardsdatascience.com/@mbednarski
  18. https://towardsdatascience.com/media/0f257a3962702611e32fdbe68e16cc35?postid=e6bae040d2fb
  19. https://towardsdatascience.com/media/06f2dad88564141b326acf8afebb61e7?postid=e6bae040d2fb
  20. https://towardsdatascience.com/media/2fcd64b2823d40866dfcb25dc4687790?postid=e6bae040d2fb
  21. https://towardsdatascience.com/media/88211cd7247680d704a93f58be484116?postid=e6bae040d2fb
  22. https://towardsdatascience.com/media/dc9106c1aa77d6ea804ff41a06d5b6b3?postid=e6bae040d2fb
  23. https://towardsdatascience.com/media/bb9e1bb685dd741485e970a9ec3462d2?postid=e6bae040d2fb
  24. https://towardsdatascience.com/media/ad8c1ae9417c679e391b8bef1442d8b7?postid=e6bae040d2fb
  25. https://towardsdatascience.com/media/839bb918bd3e649e816c224e2dbb95d3?postid=e6bae040d2fb
  26. https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0
  27. https://towardsdatascience.com/tagged/machine-learning?source=post
  28. https://towardsdatascience.com/tagged/nlp?source=post
  29. https://towardsdatascience.com/tagged/id97?source=post
  30. https://towardsdatascience.com/tagged/pytorch?source=post
  31. https://towardsdatascience.com/tagged/python?source=post
  32. https://towardsdatascience.com/@mbednarski?source=footer_card
  33. https://towardsdatascience.com/@mbednarski
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/e6bae040d2fb/share/twitter
  40. https://medium.com/p/e6bae040d2fb/share/facebook
  41. https://medium.com/p/e6bae040d2fb/share/twitter
  42. https://medium.com/p/e6bae040d2fb/share/facebook
