   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

semi-supervised learning and gans

   [16]go to the profile of raghav mehta
   [17]raghav mehta (button) blockedunblock (button) followfollowing
   apr 3, 2018
   [1*03mhbwhjjgvpw4d8-bpysa.jpeg]

   vincent van gogh painted this beautiful art:    the starry night    in 1889
   and today my gan model (i like to call it gan gogh :p) painted some
   mnist digits with only 20% labeled data!! how could it achieve this
   remarkable feat?     let   s find out

introduction

   what is semi-supervised learning?

   most deep learning classifiers require a large amount of labeled
   samples to generalize well, but getting such data is an expensive and
   difficult process. to deal with this limitation semi-supervised
   learning is presented, which is a class of techniques that make use of
   a morsel of labeled data along with a large amount of unlabeled data.
   many machine-learning researchers have found that unlabeled data, when
   used in conjunction with a small amount of labeled data can produce
   considerable improvement in learning accuracy. gans have shown a lot of
   potential in semi-supervised learning where the classifier can obtain a
   good performance with very few labeled data.

   background on gans

   gans are members of deep generative models. they are particularly
   interesting because they don   t explicitly represent a id203
   distribution over the space where the data lies. instead, they provide
   some way of interacting less directly with this id203
   distribution by drawing samples from it.
   [0*ikkugcxdbmtb6ftr.]
   architecture of a vanilla gan

   the basic idea of gan is to set up a game between two players:
     * a generator g: takes random noise z as input and outputs an image
       x. its parameters are tuned to get a high score from the
       discriminator on fake images that it generates.
     * a discriminator d: takes an image x as input and outputs a score
       which reflects its confidence that it is a real image. its
       parameters are tuned to have a high score when it is fed by a real
       image, and a low score when a fake image is fed from the generator.

   i suggest you to go through [18]this and [19]this for more details on
   their working and optimisation objectives. now, let us turn the wheels
   a little and talk about one of the most prominent applications of gans,
   semi-supervised learning.

intuition

   the vanilla architecture of discriminator has only one output neuron
   for classifying the r/f probabilities. we train both the networks
   simultaneously and discard the discriminator after the training as it
   was used only for improving the generator.
   [1*euswmkbnil97ewejs_vdnq.png]

   for the semi-supervised task, in addition to r/f neuron, the
   discriminator will now have 10 more neurons for classification of mnist
   digits. also, this time their roles change and we can discard the
   generator after training, whose only objective was to generate
   unlabeled data to improve the discriminator   s performance.

now the discriminator is turned into an 11-class classifier with 1 neuron
(r/f neuron) representing the fake data output and the other 10 representing
real data with classes. the following has to be kept in mind:

     * to assert r/f neuron output label = 0, when real unsupervised data
       from dataset is fed
     * to assert r/f neuron output label= 1, when fake unsupervised data
       from generator is fed
     * to assert r/f output label = 0 and corresponding label output = 1,
       when real supervised data is fed

   this combination of different sources of data will help the
   discriminator classify more accurately than, if it had been only
   provided with a portion of labeled data.

architecture

   now it   s time to get our hands dirty with some code :d

   the discriminator

   the architecture followed is similar to the one proposed in dcgan
   [20]paper. we use strided convolutions for reducing the dimensions of
   the feature-vectors rather than any pooling layers and apply a series
   of leaky_relu, dropout and bn for all layers to stabilize the learning.
   bn is dropped for input layer and last layer (for the purpose of
   feature matching). in the end, we perform global average pooling to
   take the average over the spatial dimensions of the feature vectors.
   this squashes the tensor dimensions to a single value. after flattening
   the features, a dense layer of 11 classes is added with softmax
   activation for multi-class output.

   iframe: [21]/media/54c50f976c23680912aa8be26e121875?postid=f23bbf4ac683

   the generator

   the generator architecture is designed to mirror the discriminator   s
   spatial outputs. fractional strided convolutions are used to increase
   the spatial dimension of the representation. an input of 4-d tensor of
   noise z is fed which undergoes a series of transposed convolutions,
   relu, bn(except at output layer) and dropout operations. finally tanh
   activation maps the output image in range (-1,1).

   iframe: [22]/media/a0f162ffe2014671b545ab5e30eb43aa?postid=f23bbf4ac683

model loss

   we start by preparing an extended label for the whole batch by
   appending actual label to zeros. this is done to assert the r/f neuron
   output to 0 when the labeled data is fed. the discriminator loss for
   unlabeled data can be thought of as a binary sigmoid loss by asserting
   r/f neuron output to 1 for fake images and 0 for real images.

   iframe: [23]/media/d4ce1f3c4886a09dd8531687de12f912?postid=f23bbf4ac683

   generator loss is a combination of fake_image loss which falsely wants
   to assert r/f neuron output to 0 and feature matching loss which
   penalizes the mean absolute error between the average value of some set
   of features on the training data and the average values of that set of
   features on the generated samples.

   iframe: [24]/media/2406d7d9a56a6f2021aba40e52fac4dc?postid=f23bbf4ac683

training

   the training images are resized from [batch_size, 28 ,28 , 1] to
   [batch_size, 64, 64, 1] to fit the generator/discriminator
   architectures. losses, accuracies and generated samples are calculated
   and are observed to improve over each epoch.

   iframe: [25]/media/8be8cbe3dd4de1fbfa29cecd46f8427b?postid=f23bbf4ac683

conclusion

   the training was done for 5 epochs and 20% labeled_rate due to
   restricted gpu access. for better results more training epochs with
   lesser labeled_rate is advised. the complete code notebook can be found
   [26]here.
   [1*ieu0ydfhjtqivmyqczwrja.jpeg]
   training results

   unsupervised learning is considered as a lacuna in the field of agi. to
   bridge this gap, gans are considered as a potential solution for
   learning complex tasks with low labeled data. with blooming new
   approaches in the domain of semi and unsupervised learning we can
   expect that this gap will lessen.

   i would be remiss not to mention my inspiration from this beautiful
   [27]blog, this [28]implementation along with the assistance of my
   colleague working on similar projects.

   until next time!! kz

     * [29]machine learning
     * [30]artificial intelligence
     * [31]generative adversarial
     * [32]neural networks
     * [33]tensorflow

   (button)
   (button)
   (button) 1.2k claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of raghav mehta

[35]raghav mehta

   salvation lies within.

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.2k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f23bbf4ac683
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/semi-supervised-learning-and-gans-f23bbf4ac683&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/semi-supervised-learning-and-gans-f23bbf4ac683&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_xysvvqscspxw---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@darkcrow?source=post_header_lockup
  17. https://towardsdatascience.com/@darkcrow
  18. https://towardsdatascience.com/overview-of-gans-generative-adversarial-networks-part-i-ac78ec775e31
  19. http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/
  20. https://arxiv.org/pdf/1511.06434.pdf
  21. https://towardsdatascience.com/media/54c50f976c23680912aa8be26e121875?postid=f23bbf4ac683
  22. https://towardsdatascience.com/media/a0f162ffe2014671b545ab5e30eb43aa?postid=f23bbf4ac683
  23. https://towardsdatascience.com/media/d4ce1f3c4886a09dd8531687de12f912?postid=f23bbf4ac683
  24. https://towardsdatascience.com/media/2406d7d9a56a6f2021aba40e52fac4dc?postid=f23bbf4ac683
  25. https://towardsdatascience.com/media/8be8cbe3dd4de1fbfa29cecd46f8427b?postid=f23bbf4ac683
  26. https://github.com/raghav64/semisuper_gan/blob/master/ssgan.py
  27. https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e
  28. https://github.com/nejlag/semi-supervised-learning-gan/blob/master/ssl_gan.ipynb
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  31. https://towardsdatascience.com/tagged/generative-adversarial?source=post
  32. https://towardsdatascience.com/tagged/neural-networks?source=post
  33. https://towardsdatascience.com/tagged/tensorflow?source=post
  34. https://towardsdatascience.com/@darkcrow?source=footer_card
  35. https://towardsdatascience.com/@darkcrow
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/f23bbf4ac683/share/twitter
  42. https://medium.com/p/f23bbf4ac683/share/facebook
  43. https://medium.com/p/f23bbf4ac683/share/twitter
  44. https://medium.com/p/f23bbf4ac683/share/facebook
