   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

hyper-parameters in action! part i         id180

   [16]go to the profile of daniel godoy
   [17]daniel godoy (button) blockedunblock (button) followfollowing
   apr 5, 2018
   [1*2zk3gpvcnoklu0xjzjh48g.png]
   sigmoid activation function in action!

introduction

   this is the first of a series of posts aiming at presenting visually,
   in a clear and concise way, some of the fundamental moving parts of
   training a neural network: the hyper-parameters.

motivation

   deep learning is all about hyper-parameters! maybe this is an
   exaggeration, but having a sound understanding of the effects of
   different hyper-parameters on training a deep neural network is
   definitely going to make your life easier.

   while studying deep learning, you   re likely to find lots of information
   on the importance of properly setting the network   s hyper-parameters:
   id180, weight initializer, optimizer, learning rate,
   mini-batch size, and the network architecture itself, like the number
   of hidden layers and the number of units in each layer.

   so, you learn all the best practices, you set up your network, define
   the hyper-parameters (or just use its default values), start training
   and monitor the progress of your model   s losses and metrics.

   perhaps the experiment doesn   t go so well as you   d expect, so you
   iterate over it, tweaking the network, until you find out the set of
   values that will do the trick for your particular problem.

looking for a deeper understanding (no pun intended!)

   have you ever wondered what exactly is going on under the hood? i did,
   and it turns out that some simple experiments may shed quite some light
   on this matter.

   take id180, for instance, the topic of this post. you
   and i know that the role of id180 is to introduce a
   non-linearity, otherwise the whole neural network could be simply
   replaced by a corresponding [18]affine transformation (that is, a
   linear transformation, such as rotating, scaling or skewing, followed
   by a translation), no matter how deep the network is.

   a neural network having only linear activations (that is, no
   activation!) would have a hard time handling even a quite simple
   classification problem like this (each line has 1,000 points, generated
   for x values equally spaced between -1.0 and 1.0):
   [1*wghkrr6rsjaiww5rkmqwga.png]
   [1*3jgx6yosyxsrsfgbxo2yzw.png]
   figure 1: in this two-dimensional feature space, the blue line
   represents the negative cases (y = 0), while the green line represents
   the positive cases (y= 1).

   if the only thing a network can do is to perform an affine
   transformation, this is likely what it would be able to come up with as
   a solution:
   [1*kad6__subel1p9ycihw3eq.png]
   figure 2: linear boundary         doesn   t look so good, right?

   clearly, this is not even close! some examples of much better solutions
   are:
   [1*bowjjpeuxqtujxfuonu_gq.png]
   figure 3: non-linearities to the rescue!

   these are three fine examples of what non-linear id180
   bring to the table! can you guess which one of the images corresponds
   to a relu?

non-linear boundaries (or are they?)

   how does these non-linear boundaries come to be? well, the actual role
   of the non-linearity is to twist and turn the feature space so much so
   that the boundary turns out to be    linear!

   ok, things are getting more interesting now (at least, i thought so
   first time i laid my eyes on it in this awesome chris olah   s blog
   [19]post, from which i drew my inspiration to write this). so, let   s
   investigate it further!

   next step is to build the simplest possible neural network to tackle
   this particular classification problem. there are two dimensions in our
   feature space (x1 and x2), and the network has a single hidden layer
   with two units, so we preserve the number of dimensions when it comes
   to the outputs of the hidden layer (z1 and z2).
   [1*frni4l9wihqcnvvfciw2za.png]
   figure 4: diagram of a simple neural network with a single 2-unit
   hidden layer

   up to this point, we are still on the realm of affine transformations   
   so, it is time for a non-linear activation function, represented by the
   greek letter sigma, resulting in the activation values (a1 and a2) for
   the hidden layer.

   these activation values represented the twisted and turned feature
   space i referred to in the first paragraph of this section. this is a
   preview of what it looks like, when using a sigmoid as activation
   function:
   [1*znprd0pmxz7i6rhnbpuh1a.png]
   figure 5: two-dimensional feature space: twisted and turned!

   as promised, the boundary is linear! by the way, the plot above
   corresponds to the left-most solution with a non-linear boundary on the
   original feature space (figure 3).

neural network   s basic math recap

   just to make sure you and i are on the same page, i am showing you
   below four representations of the very basic matrix arithmetic
   performed by the neural network up to the hidden layer, before applying
   the activation function (that is, just an affine transformation such as
   xw + b)
   [1*bunxrey2kjknrwmro8es9w.png]
   basic matrix arithmetic: 4 ways of representing the same thing in
   the network

   time to apply the activation function, represented by the greek letter
   sigma on the network diagram.
   [1*9dfvag_peno5mx0eley_kg.png]
   activation function: applied on the results of the affine
   transformations

   voil  ! we went from the inputs to the activation values of the hidden
   layer!

implementing the network in keras

   for the implementation of this simple network, i used keras sequential
   model api. apart from distinct id180, every model
   trained used the very same hyper-parameters:
     * weight initializers: glorot (xavier) normal (hidden layer) and
       random normal (output layer);
     * optimizer: stochastic id119 (sgd);
     * learning rate: 0.05;
     * mini-batch size: 16;
     * number of hidden layers: 1;
     * number of units (in the hidden layer): 2.

   given that this is a binary classification task, the output layer has a
   single unit with a sigmoid activation function and the loss is given by
   binary cross-id178.

   iframe: [20]/media/d504ef8c338890a0d0beab73177cb578?postid=a524bf5bf1c

   code: simple neural network with a single 2-unit hidden layer

id180 in action!

   now, for the juicy part         visualizing the twisted and turned feature
   space as the network trains, using a different activation function each
   time: sigmoid, tanh and relu.

   in addition to showing changes in the feature space, the animations
   also contain:
     * histograms of predicted probabilities for both negative (blue line)
       and positive cases (green line), with misclassified cases shown in
       red bars (using threshold = 0.5);
     * line plots of accuracy and average loss;
     * histogram of losses for every element in the dataset.

sigmoid

   let   s start with the most traditional of the id180, the
   sigmoid, even though, nowadays, its usage is pretty much limited to the
   output layer in classification tasks.
   [1*itamkyoebpphgxqdculyhg.png]
   [1*txzs5gwc3bbqi7ppwcqwdw.png]
   figure 6: sigmoid activation function and its gradient

   as you can see in figure 6, a sigmoid activation function    squashes   
   the inputs values into the range (0, 1) (same range probabilities can
   take, the reason why it is used in the output layer for classification
   tasks). also, remember that the activation values of any given layer
   are the inputs of the following layer and, given the range for the
   sigmoid, the activation values are going to be centered around 0.5,
   instead of zero (as it usually is the case for normalized inputs).

   it is also possible to verify that its gradient peak value is 0.25 (for
   z = 0) and that it gets already close to zero as |z| reaches a value of
   5.

   so, how does using a sigmoid activation function work for this simple
   network? let   s take a look at the animation:

   iframe: [21]/media/101e9299a08c5408803b3bfeca7682c8?postid=a524bf5bf1c

   sigmoid in action!

   there are a couple of observations to be made:
     * epochs 15   40: it is noticeable the typical sigmoid     squashing   
       happening on the horizontal axis;
     * epochs 40   65: the loss stays at a plateau, and there is a
          widening    of the transformed feature space on the vertical axis;
     * epoch 65: at this point, negative cases (blue line) are all
       correctly classified, even though its associated probabilities
       still are distributed up to 0.5; while the positive cases on the
       edges are still misclassified;
     * epochs 65   100: the aforementioned    widening    becomes more and more
       intense, up to the point pretty much all feature space is covered
       again, while the loss falls steadily;
     * epoch 103: thanks to the    widening   , all positive cases are now
       lying within the proper boundary, although some still have
       probabilities barely above the 0.5 threshold;
     * epoch 100   150: there is now some    squashing    happening on the
       vertical axis as well, the loss falls a bit more to what seems to
       be a new plateau and, except for a few of the positive edge cases,
       the network is pretty confident on its predictions.

   so, the sigmoid activation function succeeds in separating both lines,
   but the loss declines slowly, while staying at plateaus for a
   significant portion of the training time.

   can we do better with a different activation function?

tanh

   the tanh activation function was the evolution of the sigmoid, as it
   outputs values with a zero mean, differently from its predecessor.
   [1*70arcslbuw34qmhd7rya9g.png]
   [1*secbb7lfa7kpj-t1mi7grg.png]
   figure 7: tanh activation function and its gradient

   as you can see in figure 7, the tanh activation function    squashes    the
   input values into the range (-1, 1). therefore, being centered at zero,
   the activation values are already (somewhat) normalized inputs for the
   next layer.

   regarding the gradient, it has a much bigger peak value of 1.0 (again,
   for z = 0), but its decrease is even faster, approaching zero to values
   of |z| as low as 3. this is the underlying cause to what is referred to
   as the problem of vanishing gradients, which causes the training of the
   network to be progressively slower.

   now, for the corresponding animation, using tanh as activation
   function:

   iframe: [22]/media/1a282c516b1cbef4bdfb19baec23e2d7?postid=a524bf5bf1c

   tanh in action!

   there are a couple of observations to be made:
     * epochs 10   40: there is a tanh     squashing    happening on the
       horizontal axis, though it less pronounced, while the loss stays at
       a plateau;
     * epochs 40   55: there is still no improvement in the loss, but there
       is a    widening    of the transformed feature space on the vertical
       axis;
     * epoch 55: at this point, negative cases (blue line) are all
       correctly classified, even though its associated probabilities
       still are distributed up to 0.5; while the positive cases on the
       edges are still misclassified;
     * epochs 55   65: the aforementioned    widening    quickly reaches the
       point where pretty much all feature space is covered again, while
       the loss falls abruptly;
     * epoch 69: thanks to the    widening   , all positive cases are now
       lying within the proper boundary, although some still have
       probabilities barely above the 0.5 threshold;
     * epochs 65   90: there is now some    squashing    happening on the
       vertical axis as well, the loss keeps falling until reaching a new
       plateau and the network exhibits a high level of confidence for all
       predictions;
     * epochs 90   150: only small improvements in the predicted
       probabilities happen at this point.

   ok, it seems a bit better    the tanh activation function reached a
   correct classification for all cases faster, with the loss also
   declining faster (when declining, that is), but it also spends a lot of
   time in plateaus.

   what if we get rid of all the    squashing   ?

relu

   rectified linear units, or relus for short, are the commonplace choice
   of activation function these days. a relu addresses the problem of
   vanishing gradients so common in its two predecessors, while also being
   the fastest to compute gradients for.
   [1*ttvsx5g-godrk5juo7wfja.png]
   [1*pisrcnaia2patmd8kvdfkg.png]
   figure 8: relu activation function and its gradient

   as you can see in figure 8, the relu is a totally different beast: it
   does not    squash    the values into a range         it simply preserves
   positive values and turns all negative values into zero.

   the upside of using a relu is that its gradient is either 1 (for
   positive values) or 0 (for negative values)         no more vanishing
   gradients! this pattern leads to a faster convergence of the network.

   on the other hand, this behavior can lead to what it is called a    dead
   neuron   , that is, a neuron whose inputs are consistently negative and,
   therefore, always has an activation value of zero.

   time for the last of the animations, which is quite different from the
   previous two, thanks to the absence of    squashing    in the relu
   activation function:

   iframe: [23]/media/1e1cf590c7a95d333459b1d7d5af4970?postid=a524bf5bf1c

   relu in action!

   there are a couple of observations to be made:
     * epochs 0   10: the loss falls steadily from the very beginning
     * epoch 10: at this point, negative cases (blue line) are all
       correctly classified, even though its associated probabilities
       still are distributed up to 0.5; while the positive cases on the
       edges are still misclassified;
     * epochs 10   60: loss falls until reaching a plateau, all cases are
       already correctly classified since epoch 52, and the network
       already exhibits a high level of confidence for all predictions;
     * epochs 60   150: only small improvements in the predicted
       probabilities happen at this point.

   well, no wonder the relus are the de facto standard for activation
   functions nowadays. the loss kept falling steadily from the beginning
   and only plateaued at a level close to zero, reaching correct
   classification for all cases in about 75% the time it took tanh to do
   it.

showdown

   the animations are cool (ok, i am biased, i made them!), but not very
   handy to compare the overall effect of each and every different
   activation function on the feature space. so, to make it easier for you
   to compare them, there they are, side by side:
   [1*q6pekr1g90y-pj1nrj5_2g.png]
   figure 9: linear boundaries on transformed feature space (top row),
   non-linear boundaries on original feature space (bottom row)

   what about side-by-side accuracy and loss curves, so i can also compare
   the training speeds? sure, here we go:
   [1*tv30vh5inncysphjpsigug.png]
   figure 10: accuracy and loss curves for each activation function

final thoughts

   the example i used to illustrate this post is almost as simple as it
   could possibly be, and the patterns depicted in the animations are
   intended to give you just a general idea of the underlying mechanics of
   each one of the id180.

   besides, i got    lucky    with my initialization of the weights (maybe
   using [24]42 as seed is a good omen?!) and all three networks learned
   to classify correctly all the cases within 150 epochs of training. it
   turns out, training is very sensitive to the initialization, but this
   is a topic for a future post.

   nonetheless, i truly hope this post and its animations can give you
   some insights and maybe even some    a-ha!    moments while learning about
   this fascinating topic that is deep learning.
     __________________________________________________________________

     update (may 10th 2018)

     now you can reproduce the plots and animations yourself :-) i   ve
     released a package         deepreplay         check it out on [25]github and its
     corresponding [26]post.

     thanks to [27]jakukyo friel, you can also check the [28]chinese
     version of this post.

     fixed typo on figure 10: middle figure reads tanh, instead of
     sigmoid.
     __________________________________________________________________

   if you have any thoughts, comments or questions, please leave a comment
   below or contact me on [29]twitter.

     * [30]machine learning
     * [31]deep learning
     * [32]id180
     * [33]towards data science
     * [34]data science

   (button)
   (button)
   (button) 1.9k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of daniel godoy

[36]daniel godoy

   senior data scientist. teacher @ data science retreat

     (button) follow
   [37]towards data science

[38]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.9k
     * (button)
     *
     *

   [39]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [40]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a524bf5bf1c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/hyper-parameters-in-action-a524bf5bf1c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/hyper-parameters-in-action-a524bf5bf1c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_gtmm1z785mxi---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@dvgodoy?source=post_header_lockup
  17. https://towardsdatascience.com/@dvgodoy
  18. https://medium.com/hipster-color-science/computing-2d-affine-transformations-using-only-matrix-multiplication-2ccb31b52181
  19. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  20. https://towardsdatascience.com/media/d504ef8c338890a0d0beab73177cb578?postid=a524bf5bf1c
  21. https://towardsdatascience.com/media/101e9299a08c5408803b3bfeca7682c8?postid=a524bf5bf1c
  22. https://towardsdatascience.com/media/1a282c516b1cbef4bdfb19baec23e2d7?postid=a524bf5bf1c
  23. https://towardsdatascience.com/media/1e1cf590c7a95d333459b1d7d5af4970?postid=a524bf5bf1c
  24. https://en.wikipedia.org/wiki/phrases_from_the_hitchhiker's_guide_to_the_galaxy#answer_to_the_ultimate_question_of_life,_the_universe,_and_everything_(42)
  25. https://github.com/dvgodoy/deepreplay
  26. https://towardsdatascience.com/hyper-parameters-in-action-introducing-deepreplay-31132a7b9631
  27. https://medium.com/@weakish
  28. https://www.jqr.com/article/000161
  29. https://twitter.com/dvgodoy
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/deep-learning?source=post
  32. https://towardsdatascience.com/tagged/activation-functions?source=post
  33. https://towardsdatascience.com/tagged/towards-data-science?source=post
  34. https://towardsdatascience.com/tagged/data-science?source=post
  35. https://towardsdatascience.com/@dvgodoy?source=footer_card
  36. https://towardsdatascience.com/@dvgodoy
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/a524bf5bf1c/share/twitter
  43. https://medium.com/p/a524bf5bf1c/share/facebook
  44. https://medium.com/p/a524bf5bf1c/share/twitter
  45. https://medium.com/p/a524bf5bf1c/share/facebook
