   press j to jump to the feed. press question mark to learn the rest of
   the keyboard shortcuts
   (button) r/machinelearning
   ____________________
   [1]log in[2]sign up
   (button)
   user account menu
   r/

machinelearning

   [3]posts
   (button)
   20
   (button)
   posted by
   [4]u/williamwallace
   [5]3 years ago
   archived

id3 for text

   [rendertimingpixel.png]

   what are some papers where id3 have been
   applied to nlp models? i see plenty for images.
   19 comments
   (button) share
   (button)
   save (button)
   hide (button)
   report
   (button)
   95% upvoted
   this thread is archived
   new comments cannot be posted and votes cannot be cast
   sort by
   (button) best
   [6](button) best[7] (button) top[8] (button) new[9] (button)
   controversial[10] (button) old[11] (button) q&a
   (button)
   (button)
   level 1
   [12]goodfellow_ian
   12 points    [13]3 years ago

   hi there, this is ian goodfellow, inventor of gans (verification:
   [14]http://imgur.com/wdnukgp).

   gans have not been applied to nlp because gans are only defined for
   real-valued data.

   gans work by training a generator network that outputs synthetic data,
   then running a discriminator network on the synthetic data. the
   gradient of the output of the discriminator network with respect to the
   synthetic data tells you how to slightly change the synthetic data to
   make it more realistic.

   you can make slight changes to the synthetic data only if it is based
   on continuous numbers. if it is based on discrete numbers, there is no
   way to make a slight change.

   for example, if you output an image with a pixel value of 1.0, you can
   change that pixel value to 1.0001 on the next step.

   if you output the word "penguin", you can't change that to "penguin +
   .001" on the next step, because there is no such word as "penguin +
   .001". you have to go all the way from "penguin" to "ostrich".

   since all nlp is based on discrete values like words, characters, or
   bytes, no one really knows how to apply gans to nlp yet.

   in principle, you could use the reinforce algorithm, but reinforce
   doesn't work very well, and no one has made the effort to try it yet as
   far as i know.

   i see other people have said that gans don't work for id56s. as far as i
   know, that's wrong; in theory, there's no reason gans should have
   trouble with id56 generators or discriminators. but no one with serious
   neural net credentials has really tried it yet either, so maybe there
   is some obstacle that comes up in practice.

   btw, vaes work with discrete visible units, but not discrete hidden
   units (unless you use reinforce, like with darn/nvil). gans work with
   discrete hidden units, but not discrete visible units (unless, in
   theory, you use reinforce). so the two methods have complementary
   advantages and disadvantages.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [15]hghodrati
   2 points    [16]3 years ago

   thanks a lot for the detailed response ian. i agree with you if text is
   represented in an atomic way. however, text can also be represented in
   the continuous space as vector embeddings (e.g. glove, cbow,
   skip-gram). so regarding your example, it would be one of the
   dimensions of vector(penguin) + .001, which could lead to a
   semantically similar word. what do you think?
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 3
   [17]iamaaditya
   3 points    [18]2 years ago

   problem is total space of embeddings (say vector of size 300 on real
   values [fp32]), is too large compared to the vocabulary. small changes
   on the embeddings vector almost always never leads to another word
   (doing nearest neighbour*), and slightly larger changes might give you
   completely irrelevant words (this is related to how adversarial samples
   are generated).

   *doing nearest neighbour on all your vocab is already a huge problem
   and almost intractable. there are fast 'approximate nearest neighbours'
   but they are still not fast enough to do such operation iteratively
   during training. hth
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [19]williamwallace
   original poster1 point    [20]3 years ago

   thanks for the detailed reply! very much appreciated good sir.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [21]vseledkin
   1 point    [22]3 years ago    edited 3 years ago

   thanks, very insightful. so we need some kind of "smooth" text
   representation or robust mapping of real values to discreet data which
   can be learned to pretend to be discreet text. may be we need something
   from chaos theory where small shifts in initial conditions or
   parameters can lead to very rich and complex discreet features being
   dramatically different from point to point. it would be nice to map
   each sentence into some fractal/bifurcation/dynamic system which is
   parametrized by some z point of "semantic" space.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 1
   [23]adagradlace
   8 points    [24]3 years ago

   "generating sentences from a continuous space"
   [25]http://arxiv.org/pdf/1511.06349v2.pdf

   this work uses an lstm -> variational autoencoder -> lstm architecture
   to build a generative model for text. not a gan, though!
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [26]anvamiba
   1 point    [27]3 years ago

   nice.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [28]williamwallace
   original poster1 point    [29]3 years ago

   thanks!
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 1
   [30]ihsgnef
   6 points    [31]3 years ago

   i heard that it's very hard to propagate the cost through the generator
   id56. people used a lot of tricks to stablize a gan of id98, should be
   harder with id56.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 1
   [32]emansim
   7 points    [33]3 years ago    edited 3 years ago

   as people mentioned here it seems like it is hard to train gans on
   recurrent nets since they are unstable. at the same time while wobbly
   images may look better than blurry images, the same may not apply to
   text.

   also keep in mind that most of success of gans came from unsupervised
   models but not from conditional models which are much more common in
   nlp say machine translation.

   if you want to add some stochasticity to generated text i would suggest
   taking a look at these papers. all of them use some form of variational
   id136.

   [34]http://arxiv.org/abs/1511.06038 [35]http://arxiv.org/abs/1511.06349
   [36]http://arxiv.org/abs/1506.03099
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [37]goodfellow_ian
   1 point    [38]3 years ago

   in general, gans should generate things that people consider to be more
   realistic samples than the alternatives.

   models based on maximum likelihood, like vaes, are intended to always
   assign high id203 to any point that occurs frequently in reality.
   but they also assign high id203 to other points (such as blurry
   images).

   gans are designed to make samples that are realistic. they avoid
   assigning high id203 to points that the discriminator recognizes
   as fake (such as blurry images) but they may also avoid assigning high
   id203 to some of the training data.

   for text, it's not really clear what a "wobbly" sentence would be. but
   gans for text should generate sentences that are hard for a
   discriminator to recognize as being fake, and at the same time they'll
   probably fail to generate some sentences that were in the training set.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 3
   [39]vseledkin
   1 point    [40]3 years ago

   related article [41]http://arxiv.org/pdf/1511.05101v1.pdf
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 3
   [42]emansim
   1 point    [43]3 years ago

     models based on maximum likelihood, like vaes, are intended to
     always assign high id203 to any point that occurs frequently
     in reality. but they also assign high id203 to other points
     (such as blurry images).

   true, but if the dataset is large enough and is more or less
   distributed equally among all possible points then the model should
   avoid doing what you described (aka overfitting). i disagree that
   maximum likelihood models assign high id203 to blurry images for
   no reason as you mentioned. in my opinion it is due to the lack of the
   correct reconstruction error (doing pixel wise error is very bad
   metric) for images as well as bad/very simplistic id136 of vanilla
   vaes (extensions like draw and diffusion models improve on that.)
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 1
   [44]anvamiba
   2 points    [45]3 years ago

   they are conspicuously absent.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 1
   [46]nasenspray
   2 points    [47]3 years ago

   i tried gan with german words and all i got was a new nickname for my
   crush. most of the generated words looked and sounded german, but they
   were total gibberish. same for tweets; it learned to begin with "@" and
   also proper use of spaces to divide words, but the words themselves
   were composed of random letters.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [48]velveteenambush
   3 points    [49]3 years ago

     a new nickname for my crush

   well don't leave us hanging!
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [50]vseledkin
   1 point    [51]3 years ago    edited 3 years ago

   i tried gan with recurrent generator and discriminator on russian and
   have the same result. model learned words separation reasonable
   punctuation placement some words starting from capital letters but
   words are meaningless. it is hard to keep balance between generator and
   discriminator, and learning is very slow. we need more tricks :)
   (button) share
   (button) report (button) save
   level 2
   comment deleted by user[52]3 years ago
   (button)
   (button)
   level 3
   [53]nasenspray
   1 point    [54]3 years ago

   not really. i did the german words first which obviously had to be
   char-based and then simply reran with the tweet data.
   (button) share
   (button) report (button) save
   level 1
   comment deleted by user[55]3 years ago
   (button)
   (button)
   level 2
   [56]williamwallace
   original poster1 point    [57]3 years ago

   where did you see this?
   (button) share
   (button) report (button) save
   community details
   [58]r/machinelearning

   616k

   subscribers

   1.5k

   online

   (button) subscribe[59]create post
   twitter

[60]@slashml

   r/machinelearning rules
   1.
   be nice: no offensive behavior, insults or attacks
   2.
   make your post clear and comprehensive
   3.
   posts without appropriate tag will be removed
   4.
   beginner or career related questions go elsewhere
   5.
   self posts only*
   moderators
   u/kunjaan
   u/cavedave
   naive
   u/olaf_nij
   u/beatlejuce
   u/mtgtraner
   hd hlynsson
   [61]view all moderators
   [62]about[63]careers[64]press
   [65]advertise[66]blog[67]help
   [68]the reddit app[69]reddit coins[70]reddit premium[71]reddit gifts
   [72]content policy|[73] privacy policy
   [74]user agreement|[75] mod policy
      2019 reddit, inc. all rights reserved
   (button) back to top

references

   visible links
   1. https://www.reddit.com/login?dest=https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/
   2. https://www.reddit.com/register?dest=https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/
   3. https://www.reddit.com/r/machinelearning/
   4. https://www.reddit.com/user/williamwallace
   5. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/
   6. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/?sort=confidence
   7. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/?sort=top
   8. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/?sort=new
   9. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/?sort=controversial
  10. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/?sort=old
  11. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/?sort=qa
  12. https://www.reddit.com/user/goodfellow_ian
  13. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyyp0nl/
  14. http://imgur.com/wdnukgp
  15. https://www.reddit.com/user/hghodrati
  16. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/czx4x5o/
  17. https://www.reddit.com/user/iamaaditya
  18. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/d53k6b7/
  19. https://www.reddit.com/user/williamwallace
  20. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyzecyy/
  21. https://www.reddit.com/user/vseledkin
  22. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyzmsq8/
  23. https://www.reddit.com/user/adagradlace
  24. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyva355/
  25. http://arxiv.org/pdf/1511.06349v2.pdf
  26. https://www.reddit.com/user/anvamiba
  27. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyvgabu/
  28. https://www.reddit.com/user/williamwallace
  29. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyvk4t7/
  30. https://www.reddit.com/user/ihsgnef
  31. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyv7shg/
  32. https://www.reddit.com/user/emansim
  33. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyvazv0/
  34. http://arxiv.org/abs/1511.06038
  35. http://arxiv.org/abs/1511.06349
  36. http://arxiv.org/abs/1506.03099
  37. https://www.reddit.com/user/goodfellow_ian
  38. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyype4u/
  39. https://www.reddit.com/user/vseledkin
  40. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyzng6h/
  41. http://arxiv.org/pdf/1511.05101v1.pdf
  42. https://www.reddit.com/user/emansim
  43. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cz2klld/
  44. https://www.reddit.com/user/anvamiba
  45. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyv6cth/
  46. https://www.reddit.com/user/nasenspray
  47. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyv8uwx/
  48. https://www.reddit.com/user/velveteenambush
  49. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyvulkz/
  50. https://www.reddit.com/user/vseledkin
  51. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyxny2h/
  52. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyzvy06/
  53. https://www.reddit.com/user/nasenspray
  54. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cz08q0y/
  55. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyvkx3r/
  56. https://www.reddit.com/user/williamwallace
  57. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/cyvqeo5/
  58. https://www.reddit.com/r/machinelearning/
  59. https://www.reddit.com/r/machinelearning/submit
  60. https://twitter.com/slashml
  61. https://www.reddit.com/r/machinelearning/about/moderators/
  62. https://about.reddit.com/
  63. https://about.reddit.com/careers/
  64. https://about.reddit.com/press/
  65. https://about.reddit.com/advertise/
  66. http://www.redditblog.com/
  67. https://www.reddithelp.com/
  68. https://www.reddit.com/mobile/download
  69. https://www.reddit.com/coins
  70. https://www.reddit.com/premium
  71. http://redditgifts.com/
  72. https://www.reddit.com/help/contentpolicy
  73. https://www.reddit.com/help/privacypolicy
  74. https://www.reddit.com/help/useragreement
  75. https://www.reddit.com/help/healthycommunities/

   hidden links:
  77. https://www.reddit.com/
  78. https://www.reddit.com/
  79. https://www.reddit.com/r/all
  80. https://www.reddit.com/original
  81. https://www.reddit.com/r/machinelearning/
  82. https://reddit.com/message/compose?to=/r/machinelearning
  83. https://www.reddit.com/user/kunjaan
  84. https://www.reddit.com/user/cavedave
  85. https://www.reddit.com/user/olaf_nij
  86. https://www.reddit.com/user/beatlejuce
  87. https://www.reddit.com/user/mtgtraner
