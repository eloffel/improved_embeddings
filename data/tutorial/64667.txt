   [1]lil'log [2]     contact [3]     faq [4]     tags

from gan to wgan

   aug 20, 2017 by lilian weng [5]gan  [6]long-read  [7]generative-model

     this post explains the maths behind a generative adversarial network
     (gan) model and why it is hard to be trained. wasserstein gan is
     intended to improve gans    training by adopting a smooth metric for
     measuring the distance between two id203 distributions.

   [8]generative adversarial network (gan) has shown great results in many
   generative tasks to replicate the real-world rich content such as
   images, human language, and music. it is inspired by game theory: two
   models, a generator and a critic, are competing with each other while
   making each other stronger at the same time. however, it is rather
   challenging to train a gan model, as people are facing issues like
   training instability or failure to converge.

   here i would like to explain the maths behind the generative
   adversarial network framework, why it is hard to be trained, and
   finally introduce a modified version of gan intended to solve the
   training difficulties.

   [updated on 2018-09-30: thanks to yoonju, we have this post translated
   in [9]korean!]
     * [10]kullback   leibler and jensen   shannon divergence
     * [11]generative adversarial network (gan)
          + [12]what is the optimal value for d?
          + [13]what is the global optimal?
          + [14]what does the id168 represent?
     * [15]problems in gans
          + [16]hard to achieve nash equilibrium
          + [17]low dimensional supports
          + [18]vanishing gradient
          + [19]mode collapse
          + [20]lack of a proper evaluation metric
     * [21]improved gan training
     * [22]wasserstein gan (wgan)
          + [23]what is wasserstein distance?
          + [24]why wasserstein is better than js or kl divergence?
          + [25]use wasserstein distance as gan id168
     * [26]example: create new pokemons!
     * [27]references

kullback   leibler and jensen   shannon divergence

   before we start examining gans closely, let us first review two metrics
   for quantifying the similarity between two id203 distributions.

   (1) [28]kl (kullback   leibler) divergence measures how one id203
   distribution diverges from a second expected id203 distribution .

   achieves the minimum zero when == everywhere.

   it is noticeable according to the formula that kl divergence is
   asymmetric. in cases where is close to zero, but is significantly
   non-zero, the    s effect is disregarded. it could cause buggy results
   when we just want to measure the similarity between two equally
   important distributions.

   (2) [29]jensen   shannon divergence is another measure of similarity
   between two id203 distributions, bounded by . js divergence is
   symmetric (yay!) and more smooth. check this [30]quora post if you are
   interested in reading more about the comparison between kl divergence
   and js divergence.

   kl and js divergence

   fig. 1. given two gaussian distribution, with mean=0 and std=1 and with
   mean=1 and std=1. the average of two distributions is labelled as . kl
   divergence is asymmetric but js divergence is symmetric.

   some believe ([31]huszar, 2015) that one reason behind gans    big
   success is switching the id168 from asymmetric kl divergence in
   traditional maximum-likelihood approach to symmetric js divergence. we
   will discuss more on this point in the next section.

generative adversarial network (gan)

   gan consists of two models:
     * a discriminator estimates the id203 of a given sample coming
       from the real dataset. it works as a critic and is optimized to
       tell the fake samples from the real ones.
     * a generator outputs synthetic samples given a noise variable input
       ( brings in potential output diversity). it is trained to capture
       the real data distribution so that its generative samples can be as
       real as possible, or in other words, can trick the discriminator to
       offer a high id203.

   generative adversarial network

   fig. 2. architecture of a generative adversarial network. (image
   source: [32]www.kdnuggets.com/2017/01/generative-   -learning.html)

   these two models compete against each other during the training
   process: the generator is trying hard to trick the discriminator, while
   the critic model is trying hard not to be cheated. this interesting
   zero-sum game between two models motivates both to improve their
   functionalities.

   given,
   symbol                 meaning                        notes
          data distribution over noise input     usually, just uniform.
          the generator   s distribution over data
          data distribution over real sample

   on one hand, we want to make sure the discriminator    s decisions over
   real data are accurate by maximizing . meanwhile, given a fake sample ,
   the discriminator is expected to output a id203, , close to zero
   by maximizing .

   on the other hand, the generator is trained to increase the chances of
   producing a high id203 for a fake example, thus to minimize .

   when combining both aspects together, and are playing a minimax game in
   which we should optimize the following id168:

   ( has no impact on during id119 updates.)

what is the optimal value for d?

   now we have a well-defined id168. let   s first examine what is
   the best value for .

   since we are interested in what is the best value of to maximize , let
   us label

   and then what is inside the integral (we can safely ignore the integral
   because is sampled over all the possible values) is:

   thus, set , we get the best value of the discriminator: .

   once the generator is trained to its optimal, gets very close to . when
   , becomes .

what is the global optimal?

   when both and are at their optimal values, we have and and the loss
   function becomes:

what does the id168 represent?

   according to the formula listed in the [33]previous section, js
   divergence between and can be computed as:

   thus,

   essentially the id168 of gan quantifies the similarity between
   the generative data distribution and the real sample distribution by js
   divergence when the discriminator is optimal. the best that replicates
   the real data distribution leads to the minimum which is aligned with
   equations above.

     other variations of gan: there are many variations of gans in
     different contexts or designed for different tasks. for example, for
     semi-supervised learning, one idea is to update the discriminator to
     output real class labels, , as well as one fake class label . the
     generator model aims to trick the discriminator to output a
     classification label smaller than .

   tensorflow implementation: [34]carpedm20/dcgan-tensorflow

problems in gans

   although gan has shown great success in the realistic image generation,
   the training is not easy; the process is known to be slow and unstable.

hard to achieve nash equilibrium

   [35]salimans et al. (2016) discussed the problem with gan   s
   gradient-descent-based training procedure. two models are trained
   simultaneously to find a [36]nash equilibrium to a two-player
   non-cooperative game. however, each model updates its cost
   independently with no respect to another player in the game. updating
   the gradient of both models concurrently cannot guarantee a
   convergence.

   let   s check out a simple example to better understand why it is
   difficult to find a nash equilibrium in an non-cooperative game.
   suppose one player takes control of to minimize , while at the same
   time the other player constantly updates to minimize .

   because and , we update with and with simulitanously in one iteration,
   where is the learning rate. once and have different signs, every
   following gradient update causes huge oscillation and the instability
   gets worse in time, as shown in fig. 3.

   nash equilibrium example

   fig. 3. a simulation of our example for updating to minimize and
   updating to minimize . the learning rate . with more iterations, the
   oscillation grows more and more unstable.

low dimensional supports

   term explanation
   [37]manifold a topological space that locally resembles euclidean space
   near each point. precisely, when this euclidean space is of dimension ,
   the manifold is referred as -manifold.
   [38]support a real-valued function is the subset of the domain
   containing those elements which are not mapped to zero.

   [39]arjovsky and bottou (2017) discussed the problem of the
   [40]supports of and lying on low dimensional [41]manifolds and how it
   contributes to the instability of gan training thoroughly in a very
   theoretical paper [42]   towards principled methods for training
   id3   .

   the dimensions of many real-world datasets, as represented by , only
   appear to be artificially high. they have been found to concentrate in
   a lower dimensional manifold. this is actually the fundamental
   assumption for [43]manifold learning. thinking of the real world
   images, once the theme or the contained object is fixed, the images
   have a lot of restrictions to follow, i.e., a dog should have two ears
   and a tail, and a skyscraper should have a straight and tall body, etc.
   these restrictions keep images aways from the possibility of having a
   high-dimensional free form.

   lies in a low dimensional manifolds, too. whenever the generator is
   asked to a much larger image like 64x64 given a small dimension, such
   as 100, noise variable input , the distribution of colors over these
   4096 pixels has been defined by the small 100-dimension random number
   vector and can hardly fill up the whole high dimensional space.

   because both and rest in low dimensional manifolds, they are almost
   certainly gonna be disjoint (see fig. 4). when they have disjoint
   supports, we are always capable of finding a perfect discriminator that
   separates real and fake samples 100% correctly. check the [44]paper if
   you are curious about the proof.

   low dimensional manifolds in high dimension space

   fig. 4. low dimensional manifolds in high dimension space can hardly
   have overlaps. (left) two lines in a three-dimension space. (right) two
   surfaces in a three-dimension space.

vanishing gradient

   when the discriminator is perfect, we are guaranteed with and .
   therefore the id168 falls to zero and we end up with no
   gradient to update the loss during learning iterations. fig. 5
   demonstrates an experiment when the discriminator gets better, the
   gradient vanishes fast.

   low dimensional manifolds in high dimension space

   fig. 5. first, a dcgan is trained for 1, 10 and 25 epochs. then, with
   the generator fixed, a discriminator is trained from scratch and
   measure the gradients with the original cost function. we see the
   gradient norms decay quickly (in log scale), in the best case 5 orders
   of magnitude after 4000 discriminator iterations. (image source:
   [45]arjovsky and bottou, 2017)

   as a result, training a gan faces a dilemma:
     * if the discriminator behaves badly, the generator does not have
       accurate feedback and the id168 cannot represent the
       reality.
     * if the discriminator does a great job, the gradient of the loss
       function drops down to close to zero and the learning becomes super
       slow or even jammed.

   this dilemma clearly is capable to make the gan training very tough.

mode collapse

   during the training, the generator may collapse to a setting where it
   always produces same outputs. this is a common failure case for gans,
   commonly referred to as mode collapse. even though the generator might
   be able to trick the corresponding discriminator, it fails to learn to
   represent the complex real-world data distribution and gets stuck in a
   small space with extremely low variety.

   mode collapse in gan

   fig. 6. a dcgan model is trained with an mlp network with 4 layers, 512
   units and relu activation function, configured to lack a strong
   inductive bias for image generation. the results shows a significant
   degree of mode collapse. (image source: [46]arjovsky, chintala, &
   bottou, 2017.)

lack of a proper evaluation metric

   id3 are not born with a good objection
   function that can inform us the training progress. without a good
   evaluation metric, it is like working in the dark. no good sign to tell
   when to stop; no good indicator to compare the performance of multiple
   models.

improved gan training

   the following suggestions are proposed to help stabilize and improve
   the training of gans.

   first five methods are practical techniques to achieve faster
   convergence of gan training, proposed in [47]   improve techniques for
   training gans   . the last two are proposed in [48]   towards principled
   methods for training id3    to solve the
   problem of disjoint distributions.

   (1) feature matching

   feature matching suggests to optimize the discriminator to inspect
   whether the generator   s output matches expected statistics of the real
   samples. in such a scenario, the new id168 is defined as ,
   where can be any computation of statistics of features, such as mean or
   median.

   (2) minibatch discrimination

   with minibatch discrimination, the discriminator is able to digest the
   relationship between training data points in one batch, instead of
   processing each point independently.

   in one minibatch, we approximate the closeness between every pair of
   samples, , and get the overall summary of one data point by summing up
   how close it is to other samples in the same batch, . then is
   explicitly added to the input of the model.

   (3) historical averaging

   for both models, add into the id168, where is the model
   parameter and is how the parameter is configured at the past training
   time . this addition piece penalizes the training speed when is
   changing too dramatically in time.

   (4) one-sided label smoothing

   when feeding the discriminator, instead of providing 1 and 0 labels,
   use soften values such as 0.9 and 0.1. it is shown to reduce the
   networks    vulnerability.

   (5) virtual batch id172 (vbn)

   each data sample is normalized based on a fixed batch (   reference
   batch   ) of data rather than within its minibatch. the reference batch
   is chosen once at the beginning and stays the same through the
   training.

   theano implementation: [49]openai/improved-gan

   (6) adding noises.

   based on the discussion in the [50]previous section, we now know and
   are disjoint in a high dimensional space and it causes the problem of
   vanishing gradient. to artificially    spread out    the distribution and
   to create higher chances for two id203 distributions to have
   overlaps, one solution is to add continuous noises onto the inputs of
   the discriminator .

   (7) use better metric of distribution similarity

   the id168 of the vanilla gan measures the js divergence between
   the distributions of and . this metric fails to provide a meaningful
   value when two distributions are disjoint.

   [51]wasserstein metric is proposed to replace js divergence because it
   has a much smoother value space. see more in the next section.

wasserstein gan (wgan)

what is wasserstein distance?

   [52]wasserstein distance is a measure of the distance between two
   id203 distributions. it is also called earth mover   s distance,
   short for em distance, because informally it can be interpreted as the
   minimum energy cost of moving and transforming a pile of dirt in the
   shape of one id203 distribution to the shape of the other
   distribution. the cost is quantified by: the amount of dirt moved x the
   moving distance.

   let us first look at a simple case where the id203 domain is
   discrete. for example, suppose we have two distributions and , each has
   four piles of dirt and both have ten shovelfuls of dirt in total. the
   numbers of shovelfuls in each dirt pile are assigned as follows:

   in order to change to look like , as illustrated in fig. 7, we:
     * first move 2 shovelfuls from to => match up.
     * then move 2 shovelfuls from to => match up.
     * finally move 1 shovelfuls from to => and match up.

   if we label the cost to pay to make and match as , we would have and in
   the example:

   finally the earth mover   s distance is .

   em distance for discrete case

   fig. 7. step-by-step plan of moving dirt between piles in and to make
   them match.

   when dealing with the continuous id203 domain, the distance
   formula becomes:

   in the formula above, is the set of all possible joint id203
   distributions between and . one joint distribution describes one dirt
   transport plan, same as the discrete example above, but in the
   continuous id203 space. precisely states the percentage of dirt
   should be transported from point to so as to make follows the same
   id203 distribution of . that   s why the marginal distribution over
   adds up to , (once we finish moving the planned amount of dirt from
   every possible to the target , we end up with exactly what has
   according to .) and vice versa .

   when treating as the starting point and as the destination, the total
   amount of dirt moved is and the travelling distance is and thus the
   cost is . the expected cost averaged across all the pairs can be easily
   computed as:

   finally, we take the minimum one among the costs of all dirt moving
   solutions as the em distance. in the definition of wasserstein
   distance, the ([53]infimum, also known as greatest lower bound)
   indicates that we are only interested in the smallest cost.

why wasserstein is better than js or kl divergence?

   even when two distributions are located in lower dimensional manifolds
   without overlaps, wasserstein distance can still provide a meaningful
   and smooth representation of the distance in-between.

   the wgan paper exemplified the idea with a simple example.

   suppose we have two id203 distributions, and :

   simple example

   fig. 8. there is no overlap between and when .

   when :

   but when , two distributions are fully overlapped:

   gives us inifity when two distributions are disjoint. the value of has
   sudden jump, not differentiable at . only wasserstein metric provides a
   smooth measure, which is super helpful for a stable learning process
   using id119s.

use wasserstein distance as gan id168

   it is intractable to exhaust all the possible joint distributions in to
   compute . thus the authors proposed a smart transformation of the
   formula based on the kantorovich-rubinstein duality to:

   where ([54]supremum) is the opposite of (infimum); we want to measure
   the least upper bound or, in even simpler words, the maximum value.

   lipschitz continuity?

   the function in the new form of wasserstein metric is demanded to
   satisfy , meaning it should be [55]k-lipschitz continuous.

   a real-valued function is called -lipschitz continuous if there exists
   a real constant such that, for all ,

   here is known as a lipschitz constant for function . functions that are
   everywhere continuously differentiable is lipschitz continuous, because
   the derivative, estimated as , has bounds. however, a lipschitz
   continuous function may not be everywhere differentiable, such as .

   explaining how the transformation happens on the wasserstein distance
   formula is worthy of a long post by itself, so i skip the details here.
   if you are interested in how to compute wasserstein metric using linear
   programming, or how to transfer wasserstein metric into its dual form
   according to the kantorovich-rubinstein duality, read this [56]awesome
   post.

   suppose this function comes from a family of k-lipschitz continuous
   functions, , parameterized by . in the modified wasserstein-gan, the
      discriminator    model is used to learn to find a good and the loss
   function is configured as measuring the wasserstein distance between
   and .

   thus the    discriminator    is not a direct critic of telling the fake
   samples apart from the real ones anymore. instead, it is trained to
   learn a -lipschitz continuous function to help compute wasserstein
   distance. as the id168 decreases in the training, the
   wasserstein distance gets smaller and the generator model   s output
   grows closer to the real data distribution.

   one big problem is to maintain the -lipschitz continuity of during the
   training in order to make everything work out. the paper presents a
   simple but very practical trick: after every gradient update, clamp the
   weights to a small window, such as , resulting in a compact parameter
   space and thus obtains its lower and upper bounds to preserve the
   lipschitz continuity.

   simple example

   fig. 9. algorithm of wasserstein generative adversarial network. (image
   source: [57]arjovsky, chintala, & bottou, 2017.)

   compared to the original gan algorithm, the wgan undertakes the
   following changes:
     * after every gradient update on the critic function, clamp the
       weights to a small fixed range, .
     * use a new id168 derived from the wasserstein distance, no
       logarithm anymore. the    discriminator    model does not play as a
       direct critic but a helper for estimating the wasserstein metric
       between real and generated data distribution.
     * empirically the authors recommended [58]rmsprop optimizer on the
       critic, rather than a momentum based optimizer such as [59]adam
       which could cause instability in the model training. i haven   t seen
       clear theoretical explanation on this point through.
     __________________________________________________________________

   sadly, wasserstein gan is not perfect. even the authors of the original
   wgan paper mentioned that    weight clipping is a clearly terrible way to
   enforce a lipschitz constraint    (oops!). wgan still suffers from
   unstable training, slow convergence after weight clipping (when
   clipping window is too large), and vanishing gradients (when clipping
   window is too small).

   some improvement, precisely replacing weight clipping with gradient
   penalty, has been discussed in [60]gulrajani et al. 2017. i will leave
   this to a future post.

example: create new pokemons!

   just for fun, i tried out [61]carpedm20/dcgan-tensorflow on a tiny
   dataset, [62]pokemon sprites. the dataset only has 900-ish pokemon
   images, including different levels of same pokemon species.

   let   s check out what types of new pokemons the model is able to create.
   unfortunately due to the tiny training data, the new pokemons only have
   rough shapes without details. the shapes and colors do look better with
   more training epoches! hooray!

   pokemon gan

   fig. 10. train [63]carpedm20/dcgan-tensorflow on a set of pokemon
   sprite images. the sample outputs are listed after training epoches =
   7, 21, 49.

   if you are interested in a commented version of
   [64]carpedm20/dcgan-tensorflow and how to modify it to train wgan and
   wgan with gradient penalty, check
   [65]lilianweng/unified-gan-tensorflow.
     __________________________________________________________________

   if you notice mistakes and errors in this post, don   t hesitate to
   contact me at [lilian dot wengweng at gmail dot com] and i would be
   super happy to correct them right away!

   see you in the next post :d

references

   [1] goodfellow, ian, et al. [66]   generative adversarial nets.    nips,
   2014.

   [2] tim salimans, ian goodfellow, wojciech zaremba, vicki cheung, alec
   radford, and xi chen. [67]   improved techniques for training gans.    in
   advances in neural information processing systems.

   [3] martin arjovsky and l  on bottou. [68]   towards principled methods
   for training id3.    arxiv preprint
   arxiv:1701.04862 (2017).

   [4] martin arjovsky, soumith chintala, and l  on bottou.
   [69]   wasserstein gan.    arxiv preprint arxiv:1701.07875 (2017).

   [4] ishaan gulrajani, faruk ahmed, martin arjovsky, vincent dumoulin,
   aaron courville. [70]improved training of wasserstein gans. arxiv
   preprint arxiv:1704.00028 (2017).

   [5] [71]computing the earth mover   s distance under transformations

   [6] [72]wasserstein gan and the kantorovich-rubinstein duality

   [7] [73]zhuanlan.zhihu.com/p/25071913

   [8] ferenc husz  r. [74]   how (not) to train your generative model:
   scheduled sampling, likelihood, adversary?.    arxiv preprint
   arxiv:1511.05101 (2015).
   [75]    how to explain the prediction of a machine learning model?
   [76]anatomize deep learning with id205    
   please enable javascript to view the [77]comments powered by disqus.

   2019    built by [78]jekyll and [79]minima | view [80]this on github |
   [81]tags | [82]contact | [83]faq

   [84][logo_rss.png] [85][logo_scholar.png] [86][logo_github.png]
   [87][logo_instagram.png] [88][logo_twitter.png]

references

   1. https://lilianweng.github.io/lil-log/
   2. https://lilianweng.github.io/lil-log/contact.html
   3. https://lilianweng.github.io/lil-log/faq.html
   4. https://lilianweng.github.io/lil-log/tags.html
   5. https://lilianweng.github.io/lil-log/tag/gan
   6. https://lilianweng.github.io/lil-log/tag/long-read
   7. https://lilianweng.github.io/lil-log/tag/generative-model
   8. https://arxiv.org/pdf/1406.2661.pdf
   9. https://github.com/yjucho1/articles/blob/master/fromgantowgan/readme.md
  10. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#kullbackleibler-and-jensenshannon-divergence
  11. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#generative-adversarial-network-gan
  12. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#what-is-the-optimal-value-for-d
  13. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#what-is-the-global-optimal
  14. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#what-does-the-loss-function-represent
  15. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#problems-in-gans
  16. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#hard-to-achieve-nash-equilibrium
  17. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#low-dimensional-supports
  18. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#vanishing-gradient
  19. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#mode-collapse
  20. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#lack-of-a-proper-evaluation-metric
  21. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#improved-gan-training
  22. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#wasserstein-gan-wgan
  23. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#what-is-wasserstein-distance
  24. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#why-wasserstein-is-better-than-js-or-kl-divergence
  25. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#use-wasserstein-distance-as-gan-loss-function
  26. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#example-create-new-pokemons
  27. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#references
  28. https://en.wikipedia.org/wiki/kullback   leibler_divergence
  29. https://en.wikipedia.org/wiki/jensen   shannon_divergence
  30. https://www.quora.com/why-isnt-the-jensen-shannon-divergence-used-more-often-than-the-kullback-leibler-since-js-is-symmetric-thus-possibly-a-better-indicator-of-distance
  31. https://arxiv.org/pdf/1511.05101.pdf
  32. http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html
  33. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#kullbackleibler-and-jensenshannon-divergence
  34. https://github.com/carpedm20/dcgan-tensorflow
  35. http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf
  36. https://en.wikipedia.org/wiki/nash_equilibrium
  37. https://en.wikipedia.org/wiki/manifold
  38. https://en.wikipedia.org/wiki/support_(mathematics)
  39. https://arxiv.org/pdf/1701.04862.pdf
  40. https://en.wikipedia.org/wiki/support_(mathematics)
  41. https://en.wikipedia.org/wiki/manifold
  42. https://arxiv.org/pdf/1701.04862.pdf
  43. http://scikit-learn.org/stable/modules/manifold.html
  44. https://arxiv.org/pdf/1701.04862.pdf
  45. https://arxiv.org/pdf/1701.04862.pdf
  46. https://arxiv.org/pdf/1701.07875.pdf
  47. http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf
  48. https://arxiv.org/pdf/1701.04862.pdf
  49. https://github.com/openai/improved-gan
  50. https://lilianweng.github.io/lil-log/2017/08/20/from-gan-to-wgan.html#low-dimensional-supports
  51. https://en.wikipedia.org/wiki/wasserstein_metric
  52. https://en.wikipedia.org/wiki/wasserstein_metric
  53. https://en.wikipedia.org/wiki/infimum_and_supremum
  54. https://en.wikipedia.org/wiki/infimum_and_supremum
  55. https://en.wikipedia.org/wiki/lipschitz_continuity
  56. https://vincentherrmann.github.io/blog/wasserstein/
  57. https://arxiv.org/pdf/1701.07875.pdf
  58. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  59. https://arxiv.org/abs/1412.6980v8
  60. https://arxiv.org/pdf/1704.00028.pdf
  61. https://github.com/carpedm20/dcgan-tensorflow
  62. https://github.com/pokeapi/sprites/
  63. https://github.com/carpedm20/dcgan-tensorflow
  64. https://github.com/carpedm20/dcgan-tensorflow
  65. https://github.com/lilianweng/unified-gan-tensorflow
  66. https://arxiv.org/pdf/1406.2661.pdf
  67. http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf
  68. https://arxiv.org/pdf/1701.04862.pdf
  69. https://arxiv.org/pdf/1701.07875.pdf
  70. https://arxiv.org/pdf/1704.00028.pdf
  71. http://robotics.stanford.edu/~scohen/research/emdg/emdg.html
  72. https://vincentherrmann.github.io/blog/wasserstein/
  73. https://zhuanlan.zhihu.com/p/25071913
  74. https://arxiv.org/pdf/1511.05101.pdf
  75. https://lilianweng.github.io/lil-log/2017/08/01/how-to-explain-the-prediction-of-a-machine-learning-model.html
  76. https://lilianweng.github.io/lil-log/2017/09/28/anatomize-deep-learning-with-information-theory.html
  77. https://disqus.com/?ref_noscript
  78. https://jekyllrb.com/
  79. https://github.com/jekyll/minima/
  80. https://github.com/lilianweng/lil-log/tree/gh-pages
  81. https://lilianweng.github.io/lil-log/tags.html
  82. https://lilianweng.github.io/lil-log/contact.html
  83. https://lilianweng.github.io/lil-log/faq.html
  84. https://lilianweng.github.io/lil-log/feed.xml
  85. https://scholar.google.com/citations?user=dca-pw8aaaaj&hl=en&oi=ao
  86. https://github.com/lilianweng
  87. https://www.instagram.com/lilianweng/
  88. https://twitter.com/lilianweng/
