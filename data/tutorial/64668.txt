   [1]lil'log [2]     contact [3]     faq [4]     tags

a (long) peek into id23

   feb 19, 2018 by lilian weng [5]reinforcement-learning  [6]long-read

     in this post, we are gonna briefly go over the field of
     id23 (rl), from fundamental concepts to classic
     algorithms. hopefully, this review is helpful enough so that newbies
     would not get lost in specialized terms and jargons while starting.
     [warning] this is a long read.

     * [7]what is id23?
          + [8]key concepts
               o [9]model: transition and reward
               o [10]policy
               o [11]value function
               o [12]optimal value and policy
          + [13]id100
          + [14]bellman equations
               o [15]bellman expectation equations
               o [16]bellman optimality equations
     * [17]common approaches
          + [18]id145
               o [19]policy evaluation
               o [20]policy improvement
               o [21]policy iteration
          + [22]monte-carlo methods
          + [23]temporal-difference learning
               o [24]id64
               o [25]value estimation
               o [26]sarsa: on-policy td control
               o [27]id24: off-policy td control
               o [28]deep q-network
          + [29]combining td and mc learning
          + [30]policy gradient
               o [31]policy gradient theorem
               o [32]reinforce
               o [33]actor-critic
               o [34]a3c
          + [35]evolution strategies
     * [36]known problems
          + [37]exploration-exploitation dilemma
          + [38]deadly triad issue
     * [39]case study: alphago zero
     * [40]references

   a couple of exciting news in artificial intelligence (ai) has just
   happened in recent years. alphago defeated the best professional human
   player in the game of go. very soon the extended algorithm alphago zero
   beat alphago by 100-0 without supervised learning on human knowledge.
   top professional game players lost to the bot developed by openai on
   dota2 1v1 competition. after knowing these, it is pretty hard not to be
   curious about the magic behind these algorithms     reinforcement
   learning (rl). i   m writing this post to briefly go over the field. we
   will first introduce several fundamental concepts and then dive into
   classic approaches to solving rl problems. hopefully, this post could
   be a good starting point for newbies, bridging the future study on the
   cutting-edge research.

what is id23?

   say, we have an agent in an unknown environment and this agent can
   obtain some rewards by interacting with the environment. the agent
   ought to take actions so as to maximize cumulative rewards. in reality,
   the scenario could be a bot playing a game to achieve high scores, or a
   robot trying to complete physical tasks with physical items; and not
   just limited to these.

   illustration of a id23 problem

   fig. 1. an agent interacts with the environment, trying to take smart
   actions to maximize cumulative rewards.

   the goal of id23 (rl) is to learn a good strategy for
   the agent from experimental trials and relative simple feedback
   received. with the optimal strategy, the agent is capable to actively
   adapt to the environment to maximize future rewards.

key concepts

   now let   s formally define a set of key concepts in rl.

   the agent is acting in an environment. how the environment reacts to
   certain actions is defined by a model which we may or may not know. the
   agent can stay in one of many states () of the environment, and choose
   to take one of many actions () to switch from one state to another.
   which state the agent will arrive in is decided by transition
   probabilities between states (). once an action is taken, the
   environment delivers a reward () as feedback.

   the model defines the reward function and transition probabilities. we
   may or may not know how the model works and this differentiate two
   circumstances:
     * know the model: planning with perfect information; do model-based
       rl. when we fully know the environment, we can find the optimal
       solution by [41]id145 (dp). do you still remember
          longest increasing subsequence    or    traveling salesmen problem   
       from your algorithms 101 class? lol. this is not the focus of this
       post though.
     * does not know the model: learning with incomplete information; do
       model-free rl or try to learn the model explicitly as part of the
       algorithm. most of the following content serves the scenarios when
       the model is unknown.

   the agent   s policy provides the guideline on what is the optimal action
   to take in a certain state with the goal to maximize the total rewards.
   each state is associated with a value function predicting the expected
   amount of future rewards we are able to receive in this state by acting
   the corresponding policy. in other words, the value function quantifies
   how good a state is. both policy and value functions are what we try to
   learn in id23.

   categorization of rl algorithms

   fig. 2. summary of approaches in rl based on whether we want to model
   the value, policy, or the environment. (image source: reproduced from
   david silver   s rl course [42]lecture 1.)

   the interaction between the agent and the environment involves a
   sequence of actions and observed rewards in time, . during the process,
   the agent accumulates the knowledge about the environment, learns the
   optimal policy, and makes decisions on which action to take next so as
   to efficiently learn the best policy. let   s label the state, action,
   and reward at time step t as , , and , respectively. thus the
   interaction sequence is fully described by one episode (also known as
      trial    or    trajectory   ) and the sequence ends at the terminal state :

   terms you will encounter a lot when diving into different categories of
   rl algorithms:
     * model-based: rely on the model of the environment; either the model
       is known or the algorithm learns it explicitly.
     * model-free: no dependency on the model during learning.
     * on-policy: use the deterministic outcomes or samples from the
       target policy to train the algorithm.
     * off-policy: training on a distribution of transitions or episodes
       produced by a different behavior policy rather than that produced
       by the target policy.

model: transition and reward

   the model is a descriptor of the environment. with the model, we can
   learn or infer how the environment would interact with and provide
   feedback to the agent. the model has two major parts, transition
   id203 function and reward function .

   let   s say when we are in state s, we decide to take action a to arrive
   in the next state s    and obtain reward r. this is known as one
   transition step, represented by a tuple (s, a, s   , r).

   the transition function p records the id203 of transitioning from
   state s to s    after taking action a while obtaining reward r. we use as
   a symbol of    id203   .

   thus the state-transition function can be defined as a function of :

   the reward function r predicts the next reward triggered by one action:

policy

   policy, as the agent   s behavior function , tells us which action to
   take in state s. it is a mapping from state s to action a and can be
   either deterministic or stochastic:
     * deterministic: .
     * stochastic: .

value function

   value function measures the goodness of a state or how rewarding a
   state or an action is by a prediction of future reward. the future
   reward, also known as return, is a total sum of discounted rewards
   going forward. let   s compute the return starting from time t:

   the discounting factor penalize the rewards in the future, because:
     * the future rewards may have higher uncertainty; i.e. stock market.
     * the future rewards do not provide immediate benefits; i.e. as human
       beings, we might prefer to have fun today rather than 5 years later
       ;).
     * discounting provides mathematical convenience; i.e., we don   t need
       to track future steps forever to compute return.
     * we don   t need to worry about the infinite loops in the state
       transition graph.

   the state-value of a state s is the expected return if we are in this
   state at time t, :

   similarly, we define the action-value (   q-value   ; q as    quality    i
   believe?) of a state-action pair as:

   additionally, since we follow the target policy , we can make use of
   the probility distribution over possible actions and the q-values to
   recover the state-value:

   the difference between action-value and state-value is the action
   advantage function (   a-value   ):

optimal value and policy

   the optimal value function produces the maximum return:

   the optimal policy achieves optimal value functions:

   and of course, we have and .

id100

   in more formal terms, almost all the rl problems can be framed as
   id100 (mdps). all states in mdp has    markov   
   property, referring to the fact that the future only depends on the
   current state, not the history:

   or in other words, the future and the past are conditionally
   independent given the present, as the current state encapsulates all
   the statistics we need to decide the future.

   agent-environment interaction in mdp

   fig. 3. the agent-environment interaction in a markov decision process.
   (image source: sec. 3.1 sutton & barto (2017).)

   a markov deicison process consists of five elements , where the symbols
   carry the same meanings as key conceps in the [43]previous section,
   well aligned with rl problem settings:
     * - a set of states;
     * - a set of actions;
     * - transition id203 function;
     * - reward function;
     * - discounting factor for future rewards. in an unknown environment,
       we do not have perfect knowledge about and .

   mdp example

   fig. 4. a fun example of markov decision process: a typical work day.
   (image source: [44]randomant.net/reinforcement-learning-concepts)

bellman equations

   bellman equations refer to a set of equations that decompose the value
   function into the immediate reward plus the discounted future values.

   similarly for q-value,

bellman expectation equations

   the recursive update process can be further decomposed to be equations
   built on both state-value and action-value functions. as we go further
   in future action steps, we extend v and q alternatively by following
   the policy .

   bellman

   fig. 5. illustration of how bellman expection equations update
   state-value and action-value functions.

bellman optimality equations

   if we are only interested in the optimal values, rather than computing
   the expectation following a policy, we could jump right into the
   maximum returns during the alternative updates without using a policy.
   recap: the optimal values and are the best returns we can obtain,
   defined [45]here.

   unsurprisingly they look very similar to bellman expectation equations.

   if we have complete information of the environment, this turns into a
   planning problem, solvable by dp. unfortunately, in most scenarios, we
   do not know or , so we cannot solve mdps by directly applying bellmen
   equations, but it lays the theoretical foundation for many rl
   algorithms.

common approaches

   now it is the time to go through the major approaches and classic
   algorithms for solving rl problems. in future posts, i plan to dive
   into each approach further.

id145

   when the model is fully known, following bellman equations, we can use
   [46]id145 (dp) to iteratively evaluate value functions
   and improve policy.

policy evaluation

   policy evaluation is to compute the state-value for a given policy :

policy improvement

   based on the value functions, policy improvement generates a better
   policy by acting greedily.

policy iteration

   the generalized policy iteration (gpi) algorithm refers to an iterative
   procedure to improve the policy when combining policy evaluation and
   improvement.

   in gpi, the value function is approximated repeatedly to be closer to
   the true value of the current policy and in the meantime, the policy is
   improved repeatedly to approach optimality. this policy iteration
   process works and always converges to the optimality, but why this is
   the case?

   say, we have a policy and then generate an improved version by greedily
   taking actions, . the value of this improved is guaranteed to be better
   because:

monte-carlo methods

   first, let   s recall that . monte-carlo (mc) methods uses a simple idea:
   it learns from episodes of raw experience without modeling the
   environmental dynamics and computes the observed mean return as an
   approximation of the expected return. to compute the empirical return ,
   mc methods need to learn from complete episodes to compute and all the
   episodes must eventually terminate.

   the empirical mean return for state s is:

   where is a binary indicator function. we may count the visit of state s
   every time so that there could exist multiple visits of one state in
   one episode (   every-visit   ), or only count it the first time we
   encounter a state in one episode (   first-visit   ). this way of
   approximation can be easily extended to action-value functions by
   counting (s, a) pair.

   to learn the optimal policy by mc, we iterate it by following a similar
   idea to [47]gpi.

   policy iteration by mc
    1. improve the policy greedily with respect to the current value
       function: .
    2. generate a new episode with the new policy (i.e. using algorithms
       like [48]  -greedy helps us balance between exploitation and
       exploration.)
    3. estimate q using the new episode:

temporal-difference learning

   similar to monte-carlo methods, temporal-difference (td) learning is
   model-free and learns from episodes of experience. however, td learning
   can learn from incomplete episodes and hence we don   t need to track the
   episode up to termination. td learning is so important that sutton &
   barto (2017) in their rl book describes it as    one idea     central and
   novel to id23   .

id64

   td learning methods update targets with regard to existing estimates
   rather than exclusively relying on actual rewards and complete returns
   as in mc methods. this approach is known as id64.

value estimation

   the key idea in td learning is to update the value function towards an
   estimated return (known as    td target   ). to what extent we want to
   update the value function is controlled by the learning rate
   hyperparameter   :

   similarly, for action-value estimation:

   next, let   s dig into the fun part on how to learn optimal policy in td
   learning (aka    td control   ). be prepared, you are gonna see many famous
   names of classic algorithms in this section.

sarsa: on-policy td control

      sarsa    refers to the procedure of updaing q-value by following a
   sequence of . the idea follows the same route of [49]gpi:
    1. at time step t, we start from state and pick action according to q
       values, ;   -greedy is commonly applied.
    2. with action , we observe reward and get into the next state .
    3. then pick the next action in the same way as in step 1.: .
    4. update the action-value function: .
    5. t = t+1 and repeat from step 1.

   in each update of sarsa, we need to choose actions for two steps by
   following the current policy twice (in step 1. & 3.).

id24: off-policy td control

   the development of id24 ([50]watkins & dayan, 1992) is a big
   breakout in the early days of id23.
    1. at time step t, we start from state and pick action according to q
       values, ;   -greedy is commonly applied.
    2. with action , we observe reward and get into the next state .
    3. update the action-value function: .
    4. t = t+1 and repeat from step 1.

   the first two steps are same as in sarsa. in step 3., id24 does
   not follow the current policy to pick the second action but rather
   estimate out of the best q values independently of the current policy.

   sarsa and id24

   fig. 6. the backup diagrams for id24 and sarsa. (image source:
   replotted based on figure 6.5 in sutton & barto (2017))

deep q-network

   theoretically, we can memorize for all state-action pairs in
   id24, like in a gigantic table. however, it quickly becomes
   computationally infeasible when the state and action space are large.
   thus people use functions (i.e. a machine learning model) to
   approximate q values and this is called function approximation. for
   example, if we use a function with parameter to calculate q values, we
   can label q value function as .

   unfortunately id24 may suffer from instability and divergence
   when combined with an nonlinear q-value function approximation and
   [51]id64 (see [52]problems #2).

   deep q-network (   id25   ; mnih et al. 2015) aims to greatly improve and
   stabilize the training procedure of id24 by two innovative
   mechanisms:
     * experience replay: all the episode steps are stored in one replay
       memory . has experience tuples over many episodes. during
       id24 updates, samples are drawn at random from the replay
       memory and thus one sample could be used multiple times. experience
       replay improves data efficiency, removes correlations in the
       observation sequences, and smooths over changes in the data
       distribution.
     * periodically updated target: q is optimized towards target values
       that are only periodically updated. the q network is cloned and
       kept frozen as the optimization target every c steps (c is a
       hyperparameter). this modification makes the training more stable
       as it overcomes the short-term oscillations.

   the id168 looks like this:

   where is a uniform distribution over the replay memory d; is the
   parameters of the frozen target q-network.

   in addition, it is also found to be helpful to clip the error term to
   be between [-1, 1]. (i always get mixed feeling with parameter
   clipping, as many studies have shown that it works empirically but it
   makes the math much less pretty. :/)

   id25 algorithm

   fig. 7. algorithm for id25 with experience replay and occasionally
   frozen optimization target. the prepossessed sequence is the output of
   some processes running on the input images of atari games. don   t worry
   too much about it; just consider them as input feature vectors. (image
   source: mnih et al. 2015)

   there are many extensions of id25 to improve the original design, such
   as id25 with dueling architecture (wang et al. 2016) which estimates
   state-value function v(s) and advantage function a(s, a) with shared
   network parameters.

combining td and mc learning

   in the previous [53]section on value estimation in td learning, we only
   trace one step further down the action chain when calculating the td
   target. one can easily extend it to take multiple steps to estimate the
   return.

   let   s label the estimated return following n steps as , then:
           notes
       td learning

      

      
       mc estimation

   the generalized n-step td learning still has the [54]same form for
   updating the value function:

   td lambda

   we are free to pick any in td learning as we like. now the question
   becomes what is the best ? which gives us the best return
   approximation? a common yet smart solution is to apply a weighted sum
   of all possible n-step td targets rather than to pick a single best n.
   the weights decay by a factor    with n, ; the intuition is similar to
   [55]why we want to discount future rewards when computing the return:
   the more future we look into the less confident we would be. to make
   all the weight (n        ) sum up to 1, we multiply every weight by (1-  ),
   because:

   this weighted sum of many n-step returns is called   -return . td
   learning that adopts   -return for value updating is labeled as td(  ).
   the original version we introduced [56]above is equivalent to td(0).

   backup diagrams

   fig. 8. comparison of the backup diagrams of monte-carlo,
   temporal-difference learning, and id145 for state value
   functions. (image source: david silver   s rl course [57]lecture 4:
      model-free prediction   )

policy gradient

   all the methods we have introduced above aim to learn the state/action
   value function and then to select actions accordingly. policy gradient
   methods instead learn the policy directly with a parameterized function
   respect to , . let   s define the reward function (opposite of loss
   function) as the expected return and train the algorithm with the goal
   to maximize the reward function. my [58]next post described why the
   policy gradient theorem works (proof) and introduced a number of policy
   gradient algorithms.

   in discrete space:

   where is the initial starting state.

   or in continuous space:

   where is stationary distribution of markov chain for . if you are
   unfamiliar with the definition of a    stationary distribution,    please
   check this [59]reference.

   using gradient ascent we can find the best    that produces the highest
   return. it is natural to expect policy-based methods are more useful in
   continuous space, because there is an infinite number of actions and/or
   states to estimate the values for in continuous space and hence
   value-based approaches are computationally much more expensive.

policy gradient theorem

   computing the gradient numerically can be done by perturbing    by a
   small amount    in the k-th dimension. it works even when is not
   differentiable (nice!), but unsurprisingly very slow.

   or analytically,

   actually we have nice theoretical support for (replacing with ):

   check sec 13.1 in sutton & barto (2017) for why this is the case.

   then,

   this result is named    policy gradient theorem    which lays the
   theoretical foundation for various policy gradient algorithms:

reinforce

   reinforce, also known as monte-carlo policy gradient, relies on , an
   estimated return by [60]mc methods using episode samples, to update the
   policy parameter .

   a commonly used variation of reinforce is to subtract a baseline value
   from the return to reduce the variance of gradient estimation while
   keeping the bias unchanged. for example, a common baseline is
   state-value, and if applied, we would use in the gradient ascent
   update.
    1. initialize    at random
    2. generate one episode
    3. for t=1, 2,     , t:
         1. estimate the the return g_t since the time step t.
         2. .

actor-critic

   if the value function is learned in addition to the policy, we would
   get actor-critic algorithm.
     * critic: updates value function parameters w and depending on the
       algorithm it could be action-value or state-value .
     * actor: updates policy parameters   , in the direction suggested by
       the critic, .

   let   s see how it works in an action-value actor-critic algorithm.
    1. initialize s,   , w at random; sample .
    2. for t = 1    t:
         1. sample reward and next state .
         2. then sample the next action .
         3. update policy parameters: .
         4. compute the correction for action-value at time t:
            and use it to update action function parameters:
            .
         5. update and .

   and are two learning rates for policy and value function parameter
   updates, respectively.

a3c

   asynchronous advantage actor-critic (mnih et al., 2016), short for a3c,
   is a classic policy gradient method with the special focus on parallel
   training.

   in a3c, the critics learn the state-value function, , while multiple
   actors are trained in parallel and get synced with global parameters
   from time to time. hence, a3c is good for parallel training by default,
   i.e. on one machine with multi-core cpu.

   the id168 for state-value is to minimize the mean squared
   error, and we use id119 to find the optimal w. this
   state-value function is used as the baseline in the policy gradient
   update.

   here is the algorithm outline:
    1. we have global parameters,    and w; similar thread-specific
       parameters,       and w   .
    2. initialize the time step t = 1
    3. while t <= t_max:
         1. reset gradient: d   = 0 and dw = 0.
         2. synchronize thread-specific parameters with global ones:       =
               and w    = w.
         3. = t and get .
         4. while () and ():
              1. pick the action and receive a new reward and a new state
                 .
              2. update t = t + 1 and t = t + 1.
         5. initialize the variable that holds the return estimation .
         6. for :
              1. ; here r is a mc measure of .
              2. accumulate gradients w.r.t.      : ;
                 accumulate gradients w.r.t. w   : .
         7. update synchronously    using d  , and w using dw.

   a3c enables the parallelism in multiple agent training. the gradient
   accumulation step (6.2) can be considered as a reformation of
   minibatch-based stochastic gradient update: the values of w or    get
   corrected by a little bit in the direction of each training thread
   independently.

evolution strategies

   [61]evolution strategies (es) is a type of model-agnostic optimization
   approach. it learns the optimal solution by imitating darwin   s theory
   of the evolution of species by natural selection. two prerequisites for
   applying es: (1) our solutions can freely interact with the environment
   and see whether they can solve the problem; (2) we are able to compute
   a fitness score of how good each solution is. we don   t have to know the
   environment configuration to solve the problem.

   say, we start with a population of random solutions. all of them are
   capable of interacting with the environment and only candidates with
   high fitness scores can survive (only the fittest can survive in a
   competition for limited resources). a new generation is then created by
   recombining the settings (gene mutation) of high-fitness survivors.
   this process is repeated until the new solutions are good enough.

   very different from the popular mdp-based approaches as what we have
   introduced above, es aims to learn the policy parameter without value
   approximation. let   s assume the distribution over the parameter is an
   [62]isotropic multivariate gaussian with mean and fixed covariance .
   the gradient of is calculated:

   we can rewrite this formula in terms of a    mean    parameter (different
   from the above; this is the base gene for further mutation), and
   therefore . controls how much gaussian noises should be added to create
   mutation:

   ea

   fig. 9. a simple parallel evolution-strategies-based rl algorithm.
   parallel workers share the random seeds so that they can reconstruct
   the gaussian noises with tiny communication bandwidth. (image source:
   salimans et al. 2017.)

   es, as a black-box optimization algorithm, is another approach to rl
   problems (in my original writing, i used the phrase    a nice
   alternative   ; [63]seita pointed me to this [64]discussion and thus i
   updated my wording.). it has a couple of good characteristics (salimans
   et al., 2017) keeping it fast and easy to train:
     * es does not need value function approximation;
     * es does not perform gradient back-propagation;
     * es is invariant to delayed or long-term rewards;
     * es is highly parallelizable with very little data communication.

known problems

exploration-exploitation dilemma

   the problem of exploration vs exploitation dilemma has been discussed
   in my previous [65]post. when the rl problem faces an unknown
   environment, this issue is especially a key to finding a good solution:
   without enough exploration, we cannot learn the environment well
   enough; without enough exploitation, we cannot complete our reward
   optimization task.

   different rl algorithms balance between exploration and exploitation in
   different ways. in [66]mc methods, [67]id24 or many on-policy
   algorithms, the exploration is commonly implemented by [68]  -greedy; in
   [69]es, the exploration is captured by the policy parameter
   perturbation. please keep this into consideration when develop a new rl
   algorithm.

deadly triad issue

   we do seek the efficiency and flexibility of td methods that involve
   id64. however, when off-policy, nonlinear function
   approximation, and id64 are combined in one rl algorithm, the
   training could be unstable and hard to converge. this issue is known as
   the deadly triad (sutton & barto, 2017). many architectures using deep
   learning models were proposed to resolve the problem, including id25 to
   stabilize the training with experience replay and occasionally frozen
   target network.

case study: alphago zero

   the game of [70]go has been an extremely hard problem in the field of
   artificial intelligence for decades until recent years. alphago and
   alphago zero are two programs developed by a team at deepmind. both
   involve deep convolutional neural networks ([71]id98) and monte carlo
   tree search (mcts) and both have been approved to achieve the level of
   professional human go players. different from alphago that relied on
   supervised learning from expert human moves, alphago zero used only
   id23 and self-play without human knowledge beyond the
   basic rules.

   go game board

   fig. 10. the board of go. two players play black and white stones
   alternatively on the vacant intersections of a board with 19 x 19
   lines. a group of stones must have at least one open point (an
   intersection, called a    liberty   ) to remain on the board and must have
   at least two or more enclosed liberties (called    eyes   ) to stay
      alive   . no stone shall repeat a previous position.

   with all the knowledge of rl above, let   s take a look at how alphago
   zero works. the main component is a deep [72]id98 over the game board
   configuration (precisely, a [73]resnet with batch id172 and
   relu). this network outputs two values:
     * : the game board configuration, 19 x 19 x 17 stacked feature
       planes; 17 features for each position, 8 past configurations
       (including current) for the current player + 8 past configurations
       for the opponent + 1 feature indicating the color (1=black,
       0=white). we need to code the color specifically because the
       network is playing with itself and the colors of current player and
       opponents are switching between steps.
     * : the id203 of selecting a move over 19^2 + 1 candidates
       (19^2 positions on the board, in addition to passing).
     * : the winning id203 given the current setting.

   during self-play, mcts further improves the action id203
   distribution and then the action is sampled from this improved policy.
   the reward is a binary value indicating whether the current player
   eventually wins the game. each move generates an episode tuple and it
   is saved into the replay memory. the details on mcts are skipped for
   the sake of space in this post; please read the original [74]paper if
   you are interested.

   alphago zero training

   fig. 11. alphago zero is trained by self-play while mcts improves the
   output policy further in every step. (image source: figure 1a in silver
   et al., 2017).

   the network is trained with the samples in the replay memory to
   minimize the loss:

   where c is a hyperparameter controlling the intensity of l2 penalty to
   avoid overfitting.

   alphago zero simplified alphago by removing supervised learning and
   merging separated policy and value networks into one. it turns out that
   alphago zero achieved largely improved performance with a much shorter
   training time! i strongly recommend reading these [75]two [76]papers
   side by side and compare the difference, super fun.

   i know this is a long read, but hopefully worth it. if you notice
   mistakes and errors in this post, don   t hesitate to contact me at
   [lilian dot wengweng at gmail dot com]. see you in the next post! :)

references

   [1] yuxi li. [77]deep id23: an overview. arxiv
   preprint arxiv:1701.07274. 2017.

   [2] richard s. sutton and andrew g. barto. [78]id23:
   an introduction; 2nd edition. 2017.

   [3] volodymyr mnih, et al. [79]asynchronous methods for deep
   id23. icml. 2016.

   [4] tim salimans, et al. [80]evolution strategies as a scalable
   alternative to id23. arxiv preprint arxiv:1703.03864
   (2017).

   [5] david silver, et al. [81]mastering the game of go without human
   knowledge. nature 550.7676 (2017): 354.

   [6] david silver, et al. [82]mastering the game of go with deep neural
   networks and tree search. nature 529.7587 (2016): 484-489.

   [7] volodymyr mnih, et al. [83]human-level control through deep
   id23. nature 518.7540 (2015): 529.

   [8] ziyu wang, et al. [84]dueling network architectures for deep
   id23. icml. 2016.

   [9] [85]id23 lectures by david silver on youtube.

   [10] openai blog: [86]evolution strategies as a scalable alternative to
   id23

   [11] frank sehnke, et al. [87]parameter-exploring policy gradients.
   neural networks 23.4 (2010): 551-559.

   [12] csaba szepesv  ri. [88]algorithms for id23. 1st
   edition. synthesis lectures on artificial intelligence and machine
   learning 4.1 (2010): 1-103.
     __________________________________________________________________

   if you notice mistakes and errors in this post, please don   t hesitate
   to contact me at [lilian dot wengweng at gmail dot com] and i would be
   super happy to correct them right away!
   [89]    the multi-armed bandit problem and its solutions [90]policy
   gradient algorithms    
   please enable javascript to view the [91]comments powered by disqus.

   2019    built by [92]jekyll and [93]minima | view [94]this on github |
   [95]tags | [96]contact | [97]faq

   [98][logo_rss.png] [99][logo_scholar.png] [100][logo_github.png]
   [101][logo_instagram.png] [102][logo_twitter.png]

references

   1. https://lilianweng.github.io/lil-log/
   2. https://lilianweng.github.io/lil-log/contact.html
   3. https://lilianweng.github.io/lil-log/faq.html
   4. https://lilianweng.github.io/lil-log/tags.html
   5. https://lilianweng.github.io/lil-log/tag/reinforcement-learning
   6. https://lilianweng.github.io/lil-log/tag/long-read
   7. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#what-is-reinforcement-learning
   8. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts
   9. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#model-transition-and-reward
  10. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy
  11. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#value-function
  12. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#optimal-value-and-policy
  13. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#markov-decision-processes
  14. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#bellman-equations
  15. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#bellman-expectation-equations
  16. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#bellman-optimality-equations
  17. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#common-approaches
  18. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#dynamic-programming
  19. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-evaluation
  20. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-improvement
  21. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-iteration
  22. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#monte-carlo-methods
  23. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#temporal-difference-learning
  24. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#id64
  25. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#value-estimation
  26. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control
  27. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#id24-off-policy-td-control
  28. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deep-q-network
  29. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#combining-td-and-mc-learning
  30. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-gradient
  31. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-gradient-theorem
  32. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#reinforce
  33. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#actor-critic
  34. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#a3c
  35. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#evolution-strategies
  36. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#known-problems
  37. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#exploration-exploitation-dilemma
  38. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue
  39. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#case-study-alphago-zero
  40. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#references
  41. https://en.wikipedia.org/wiki/dynamic_programming
  42. https://youtu.be/2pwv7govuf0
  43. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts
  44. https://randomant.net/reinforcement-learning-concepts/
  45. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#optimal-value-and-policy
  46. https://en.wikipedia.org/wiki/dynamic_programming
  47. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-iteration
  48. https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#  -greedy-algorithm
  49. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-iteration
  50. https://link.springer.com/content/pdf/10.1007/bf00992698.pdf
  51. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#id64
  52. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue
  53. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#value-estimation
  54. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#value-estimation
  55. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#value-estimation
  56. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#value-estimation
  57. http://www0.cs.ucl.ac.uk/staff/d.silver/web/teaching_files/mc-td.pdf
  58. https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html
  59. https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/
  60. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#monte-carlo-methods
  61. https://en.wikipedia.org/wiki/evolution_strategy
  62. https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic
  63. https://danieltakeshi.github.io/
  64. https://www.reddit.com/r/machinelearning/comments/6gke6a/d_requesting_openai_to_justify_the_grandiose/dir9wde/
  65. https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#exploitation-vs-exploration
  66. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#monte-carlo-methods
  67. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#id24-off-policy-td-control
  68. https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html#  -greedy-algorithm
  69. https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#evolution-strategies
  70. https://en.wikipedia.org/wiki/go_(game)
  71. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#id98-for-image-classification
  72. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#id98-for-image-classification
  73. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#resnet-he-et-al-2015
  74. https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0
  75. https://pdfs.semanticscholar.org/1740/eb993cc8ca81f1e46ddaadce1f917e8000b5.pdf
  76. https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0
  77. https://arxiv.org/pdf/1701.07274.pdf
  78. http://incompleteideas.net/book/bookdraft2017nov5.pdf
  79. http://proceedings.mlr.press/v48/mniha16.pdf
  80. https://arxiv.org/pdf/1703.03864.pdf
  81. https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0
  82. https://pdfs.semanticscholar.org/1740/eb993cc8ca81f1e46ddaadce1f917e8000b5.pdf
  83. https://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf
  84. https://arxiv.org/pdf/1511.06581.pdf
  85. https://www.youtube.com/playlist?list=pl7-jpktc4r78-wczcqn5iqyuwhbz8foxt
  86. https://blog.openai.com/evolution-strategies/
  87. https://mediatum.ub.tum.de/doc/1287490/file.pdf
  88. https://sites.ualberta.ca/~szepesva/papers/rlalgsinmdps.pdf
  89. https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html
  90. https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html
  91. https://disqus.com/?ref_noscript
  92. https://jekyllrb.com/
  93. https://github.com/jekyll/minima/
  94. https://github.com/lilianweng/lil-log/tree/gh-pages
  95. https://lilianweng.github.io/lil-log/tags.html
  96. https://lilianweng.github.io/lil-log/contact.html
  97. https://lilianweng.github.io/lil-log/faq.html
  98. https://lilianweng.github.io/lil-log/feed.xml
  99. https://scholar.google.com/citations?user=dca-pw8aaaaj&hl=en&oi=ao
 100. https://github.com/lilianweng
 101. https://www.instagram.com/lilianweng/
 102. https://twitter.com/lilianweng/
