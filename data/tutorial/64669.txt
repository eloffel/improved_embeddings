   [1]lil'log [2]     contact [3]     faq [4]     tags

id164 for dummies part 1: gradient vector, hog, and ss

   oct 29, 2017 by lilian weng [5]object-detection  [6]object-recognition

     in this series of posts on    id164 for dummies   , we will
     go through several basic concepts, algorithms, and popular deep
     learning models for image processing and objection detection.
     hopefully, it would be a good read for people with no experience in
     this field but want to learn more. the part 1 introduces the concept
     of gradient vectors, the hog (histogram of oriented gradients)
     algorithm, and selective search for image segmentation.

   i   ve never worked in the field of id161 and has no idea how
   the magic could work when an autonomous car is configured to tell apart
   a stop sign from a pedestrian in a red hat. to motivate myself to look
   into the maths behind object recognition and detection algorithms, i   m
   writing a few posts on this topic    id164 for dummies   . this
   post, part 1, starts with super rudimentary concepts in image
   processing and a few methods for image segmentation. nothing related to
   deep neural networks yet. deep learning models for id164 and
   recognition will be discussed in [7]part 2 and [8]part 3.

     disclaimer: when i started, i was using    object recognition    and
        id164    interchangeably. i don   t think they are the same:
     the former is more about telling whether an object exists in an
     image while the latter needs to spot where the object is. however,
     they are highly related and many object recognition algorithms lay
     the foundation for detection.

     * [9]image gradient vector
          + [10]common image processing kernels
          + [11]example: manu in 2004
     * [12]histogram of oriented gradients (hog)
          + [13]how hog works
          + [14]example: manu in 2004
     * [15]image segmentation (felzenszwalb   s algorithm)
          + [16]graph construction
          + [17]key concepts
          + [18]how image segmentation works
          + [19]example: manu in 2013
     * [20]selective search
          + [21]how selective search works
          + [22]configuration variations
     * [23]references

image gradient vector

   first of all, i would like to make sure we can distinguish the
   following terms. they are very similar, closely related, but not
   exactly the same.
     derivative directional derivative gradient
   value type scalar scalar vector
   definition the rate of change of a function at a point , which is the
   slope of the tangent line at the point. the instantaneous rate of
   change of in the direction of an unit vector . it points in the
   direction of the greatest rate of increase of the function, containing
   all the partial derivative information of a multivariable function.

   in the image processing, we want to know the direction of colors
   changing from one extreme to the other (i.e. black to white on a
   grayscale image). therefore, we want to measure    gradient    on pixels of
   colors. the gradient on an image is discrete because each pixel is
   independent and cannot be further split.

   the [24]image gradient vector is defined as a metric for every
   individual pixel, containing the pixel color changes in both x-axis and
   y-axis. the definition is aligned with the gradient of a continuous
   multi-variable function, which is a vector of partial derivatives of
   all the variables. suppose f(x, y) records the color of the pixel at
   location (x, y), the gradient vector of the pixel (x, y) is defined as
   follows:

   the term is the partial derivative on the x-direction, which is
   computed as the color difference between the adjacent pixels on the
   left and right of the target, f(x+1, y) - f(x-1, y). similarly, the
   term is the partial derivative on the y-direction, measured as f(x,
   y+1) - f(x, y-1), the color difference between the adjacent pixels
   above and below the target.

   there are two important attributes of an image gradient:
     * magnitude is the l2-norm of the vector, .
     * direction is the arctangent of the ratio between the partial
       derivatives on two directions, .

   pixels for gradient vector

   fig. 1. to compute the gradient vector of a target pixel at location
   (x, y), we need to know the colors of its four neighbors (or eight
   surrounding pixels depending on the kernel).

   the gradient vector of the example in fig. 1. is:

   thus,
     * the magnitude is , and
     * the direction is .

   repeating the gradient computation process for every pixel iteratively
   is too slow. instead, it can be well translated into applying a
   convolution operator on the entire image matrix, labeled as using one
   of the specially designed convolutional kernels.

   let   s start with the x-direction of the example in fig 1. using the
   kernel sliding over the x-axis; is the convolution operator:

   similarly, on the y-direction, we adopt the kernel :

   try this in python:
import numpy as np
import scipy.signal as sig
data = np.array([[0, 105, 0], [40, 255, 90], [0, 55, 0]])
g_x = sig.convolve2d(data, np.array([[-1, 0, 1]]), mode='valid')
g_y = sig.convolve2d(data, np.array([[-1], [0], [1]]), mode='valid')

   these two functions return array([[0], [-50], [0]]) and array([[0, 50,
   0]]) respectively. (note that in the numpy array representation, 40 is
   shown in front of 90, so -1 is listed before 1 in the kernel
   correspondingly.)

common image processing kernels

   [25]prewitt operator: rather than only relying on four directly
   adjacent neighbors, the prewitt operator utilizes eight surrounding
   pixels for smoother results.

   [26]sobel operator: to emphasize the impact of directly adjacent pixels
   more, they get assigned with higher weights.

   different kernels are created for different goals, such as edge
   detection, blurring, sharpening and many more. check [27]this wiki page
   for more examples and references.

example: manu in 2004

   let   s run a simple experiment on the photo of manu ginobili in 2004
   [[28]download image] when he still had a lot of hair. for simplicity,
   the photo is converted to grayscale first. for colored images, we just
   need to repeat the same process in each color channel respectively.

   manu 2004

   fig. 2. manu ginobili in 2004 with hair. (image source: [29]manu
   ginobili   s bald spot through the years)
import numpy as np
import scipy
import scipy.signal as sig
# with mode="l", we force the image to be parsed in the grayscale, so it is
# actually unnecessary to convert the photo color beforehand.
img = scipy.misc.imread("manu-2004.jpg", mode="l")

# define the sobel operator kernels.
kernel_x = np.array([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]])
kernel_y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])

g_x = sig.convolve2d(img, kernel_x, mode='same')
g_y = sig.convolve2d(img, kernel_y, mode='same')

# plot them!
fig = plt.figure()
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)

# actually plt.imshow() can handle the value scale well even if i don't do
# the transformation (g_x + 255) / 2.
ax1.imshow((g_x + 255) / 2, cmap='gray'); ax1.set_xlabel("gx")
ax2.imshow((g_y + 255) / 2, cmap='gray'); ax2.set_xlabel("gy")
plt.show()

   sobel operator

   fig. 3. apply sobel operator kernel on the example image.

   you might notice that most area is in gray. because the difference
   between two pixel is between -255 and 255 and we need to convert them
   back to [0, 255] for the display purpose. a simple linear
   transformation ( + 255)/2 would interpret all the zeros (i.e., constant
   colored background shows no change in gradient) as 125 (shown as gray).

histogram of oriented gradients (hog)

   the histogram of oriented gradients (hog) is an efficient way to
   extract features out of the pixel colors for building an object
   recognition classifier. with the knowledge of image gradient vectors,
   it is not hard to understand how hog works. let   s start!

how hog works

   1) preprocess the image, including resizing and color id172.

   2) compute the gradient vector of every pixel, as well as its magnitude
   and direction.

   3) divide the image into many 8x8 pixel cells. in each cell, the
   magnitude values of these 64 cells are binned and cumulatively added
   into 9 buckets of unsigned direction (no sign, so 0-180 degree rather
   than 0-360 degree; this is a practical choice based on empirical
   experiments).
   for better robustness, if the direction of the gradient vector of a
   pixel lays between two buckets, its magnitude does not all go into the
   closer one but proportionally split between two. for example, if a
   pixel   s gradient vector has magnitude 8 and degree 15, it is between
   two buckets for degree 0 and 20 and we would assign 2 to bucket 0 and 6
   to bucket 20.
   this interesting configuration makes the histogram much more stable
   when small distortion is applied to the image.

   histogram construction

   fig. 4. how to split one gradient vector   s magnitude if its degress is
   between two degree bins. (image source:
   https://www.learnopencv.com/histogram-of-oriented-gradients/)

   4) then we slide a 2x2 cells (thus 16x16 pixels) block across the
   image. in each block region, 4 histograms of 4 cells are concatenated
   into one-dimensional vector of 36 values and then normalized to have an
   unit weight. the final hog feature vector is the concatenation of all
   the block vectors. it can be fed into a classifier like id166 for
   learning object recognition tasks.

example: manu in 2004

   let   s reuse the same example image in the previous section. remember
   that we have computed and for the whole image.
n_buckets = 9
cell_size = 8  # each cell is 8x8 pixels
block_size = 2  # each block is 2x2 cells

def assign_bucket_vals(m, d, bucket_vals):
    left_bin = int(d / 20.)
    # handle the case when the direction is between [160, 180)
    right_bin = (int(d / 20.) + 1) % n_buckets
    assert 0 <= left_bin < right_bin < n_buckets

    left_val= m * (right_bin * 20 - d) / 20
    right_val = m * (d - left_bin * 20) / 20
    bucket_vals[left_bin] += left_val
    bucket_vals[right_bin] += right_val

def get_magnitude_hist_cell(loc_x, loc_y):
    # (loc_x, loc_y) defines the top left corner of the target cell.
    cell_x = g_x[loc_x:loc_x + cell_size, loc_y:loc_y + cell_size]
    cell_y = g_y[loc_x:loc_x + cell_size, loc_y:loc_y + cell_size]
    magnitudes = np.sqrt(cell_x * cell_x + cell_y * cell_y)
    directions = np.abs(np.arctan(cell_y / cell_x) * 180 / np.pi)

    buckets = np.linspace(0, 180, n_buckets + 1)
    bucket_vals = np.zeros(n_buckets)
    map(
        lambda (m, d): assign_bucket_vals(m, d, bucket_vals),
        zip(magnitudes.flatten(), directions.flatten())
    )
    return bucket_vals

def get_magnitude_hist_block(loc_x, loc_y):
    # (loc_x, loc_y) defines the top left corner of the target block.
    return reduce(
        lambda arr1, arr2: np.concatenate((arr1, arr2)),
        [get_magnitude_hist_cell(x, y) for x, y in zip(
            [loc_x, loc_x + cell_size, loc_x, loc_x + cell_size],
            [loc_y, loc_y, loc_y + cell_size, loc_y + cell_size],
        )]
    )

   the following code simply calls the functions to construct a histogram
   and plot it.
# random location [200, 200] as an example.
loc_x = loc_y = 200

ydata = get_magnitude_hist_block(loc_x, loc_y)
ydata = ydata / np.linalg.norm(ydata)

xdata = range(len(ydata))
bucket_names = np.tile(np.arange(n_buckets), block_size * block_size)

assert len(ydata) == n_buckets * (block_size * block_size)
assert len(bucket_names) == len(ydata)

plt.figure(figsize=(10, 3))
plt.bar(xdata, ydata, align='center', alpha=0.8, width=0.9)
plt.xticks(xdata, bucket_names * 20, rotation=90)
plt.xlabel('direction buckets')
plt.ylabel('magnitude')
plt.grid(ls='--', color='k', alpha=0.1)
plt.title("hog of block at [%d, %d]" % (loc_x, loc_y))
plt.tight_layout()

   in the code above, i use the block with top left corner located at
   [200, 200] as an example and here is the final normalized histogram of
   this block. you can play with the code to change the block location to
   be identified by a sliding window.

   block histogram

   fig. 5. demonstration of a hog histogram for one block.

   the code is mostly for demonstrating the computation process. there are
   many off-the-shelf libraries with hog algorithm implemented, such as
   [30]opencv, [31]simplecv and [32]scikit-image.

image segmentation (felzenszwalb   s algorithm)

   when there exist multiple objects in one image (true for almost every
   real-world photos), we need to identify a region that potentially
   contains a target object so that the classification can be executed
   more efficiently.

   felzenszwalb and huttenlocher ([33]2004) proposed an algorithm for
   segmenting an image into similar regions using a graph-based approach.
   it is also the initialization method for selective search (a popular
   region proposal algorithm) that we are gonna discuss later.

   say, we use a undirected graph to represent an input image. one vertex
   represents one pixel. one edge connects two vertices and . its
   associated weight measures the dissimilarity between and . the
   dissimilarity can be quantified in dimensions like color, location,
   intensity, etc. the higher the weight, the less similar two pixels are.
   a segmentation solution is a partition of into multiple connected
   components, . intuitively similar pixels should belong to the same
   components while dissimilar ones are assigned to different components.

graph construction

   there are two approaches to constructing a graph out of an image.
     * grid graph: each pixel is only connected with surrounding
       neighbours (8 other cells in total). the edge weight is the
       absolute difference between the intensity values of the pixels.
     * nearest neighbor graph: each pixel is a point in the feature space
       (x, y, r, g, b), in which (x, y) is the pixel location and (r, g,
       b) is the color values in rgb. the weight is the euclidean distance
       between two pixels    feature vectors.

key concepts

   before we lay down the criteria for a good graph partition (aka image
   segmentation), let us define a couple of key concepts:
     * internal difference: , where is the minimum spanning tree of the
       components. a component can still remain connected even when we
       have removed all the edges with weights < .
     * difference between two components: . if there is no edge
       in-between.
     * minimum internal difference: , where helps make sure we have a
       meaningful threshold for the difference between components. with a
       higher , it is more likely to result in larger components.

   the quality of a segmentation is assessed by a pairwise region
   comparison predicate defined for given two regions and :

   only when the predicate holds true, we consider them as two independent
   components; otherwise the segmentation is too fine and they probably
   should be merged.

how image segmentation works

   the algorithm follows a bottom-up procedure. given and :
    1. edges are sorted by weight in ascending order, labeled as .
    2. initially, each pixel stays in its own component, so we start with
       components.
    3. repeat for :
          + the segmentation snapshot at the step is denoted as .
          + we take the k-th edge in the order, .
          + if and belong to the same component, do nothing and thus .
          + if and belong to two different components and as in the
            segmentation , we want to merge them into one if ; otherwise
            do nothing.

   if you are interested in the proof of the segmentation properties and
   why it always exists, please refer to the [34]paper.

   image segmentation indoor scene

   fig. 6. an indoor scene with segmentation detected by the grid graph
   construction in felzenszwalb   s graph-based segmentation algorithm
   (k=300).

example: manu in 2013

   this time i would use the photo of old manu ginobili in 2013
   [[35]image] as the example image when his bald spot has grown up
   strong. still for simplicity, we use the picture in grayscale.

   manu 2013

   fig. 7. manu ginobili in 2013 with bald spot. (image source: [36]manu
   ginobili   s bald spot through the years)

   rather than coding from scratch, let us apply
   [37]skimage.segmentation.felzenszwalb to the image.
import skimage.segmentation
from matplotlib import pyplot as plt

img2 = scipy.misc.imread("manu-2013.jpg", mode="l")
segment_mask1 = skimage.segmentation.felzenszwalb(img2, scale=100)
segment_mask2 = skimage.segmentation.felzenszwalb(img2, scale=1000)

fig = plt.figure(figsize=(12, 5))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)
ax1.imshow(segment_mask1); ax1.set_xlabel("k=100")
ax2.imshow(segment_mask2); ax2.set_xlabel("k=1000")
fig.suptitle("felsenszwalb's efficient graph based image segmentation")
plt.tight_layout()
plt.show()

   the code ran two versions of felzenszwalb   s algorithms as shown in fig.
   8. the left k=100 generates a finer-grained segmentation with small
   regions where manu   s bald spot is identified. the right one k=1000
   outputs a coarser-grained segmentation where regions tend to be larger.

   manu 2013 image segmentation

   fig. 8. felsenszwalb   s efficient graph-based image segmentation is
   applied on the photo of manu in 2013.

selective search

   selective search is a common algorithm to provide region proposals that
   potentially contain objects. it is built on top of the image
   segmentation output and use region-based characteristics (note: not
   just attributes of a single pixel) to do a bottom-up hierarchical
   grouping.

how selective search works

    1. at the initialization stage, apply felzenszwalb and huttenlocher   s
       graph-based image segmentation algorithm to create regions to start
       with.
    2. use a greedy algorithm to iteratively group regions together:
          + first the similarities between all neighbouring regions are
            calculated.
          + the two most similar regions are grouped together, and new
            similarities are calculated between the resulting region and
            its neighbours.
    3. the process of grouping the most similar regions (step 2) is
       repeated until the whole image becomes a single region.

   selective search algorithm

   fig. 9. the detailed algorithm of selective search.

configuration variations

   given two regions , selective search proposed four complementary
   similarity measures:
     * color similarity
     * texture: use algorithm that works well for material recognition
       such as [38]sift.
     * size: small regions are encouraged to merge early.
     * shape: ideally one region can fill the gap of the other.

   by (i) tuning the threshold in felzenszwalb and huttenlocher   s
   algorithm, (ii) changing the color space and (iii) picking different
   combinations of similarity metrics, we can produce a diverse set of
   selective search strategies. the version that produces the region
   proposals with best quality is configured with (i) a mixture of various
   initial segmentation proposals, (ii) a blend of multiple color spaces
   and (iii) a combination of all similarity measures. unsurprisingly we
   need to balance between the quality (the model complexity) and the
   speed.
     __________________________________________________________________

   if you notice mistakes and errors in this post, don   t hesitate to
   contact me at [lilian dot wengweng at gmail dot com] and i would be
   super happy to correct them right away!

   see you in the next post :d

references

   [1] dalal, navneet, and bill triggs. [39]   histograms of oriented
   gradients for human detection.    id161 and pattern recognition
   (cvpr), 2005.

   [2] pedro f. felzenszwalb, and daniel p. huttenlocher. [40]   efficient
   graph-based image segmentation.    intl. journal of id161 59.2
   (2004): 167-181.

   [3] [41]histogram of oriented gradients by satya mallick

   [4] [42]gradient vectors by chris mccormick

   [5] [43]hog person detector tutorial by chris mccormick
   [44]    learning id27 [45]id164 for dummies part 2:
   id98, dpm and overfeat    
   please enable javascript to view the [46]comments powered by disqus.

   2019    built by [47]jekyll and [48]minima | view [49]this on github |
   [50]tags | [51]contact | [52]faq

   [53][logo_rss.png] [54][logo_scholar.png] [55][logo_github.png]
   [56][logo_instagram.png] [57][logo_twitter.png]

references

   1. https://lilianweng.github.io/lil-log/
   2. https://lilianweng.github.io/lil-log/contact.html
   3. https://lilianweng.github.io/lil-log/faq.html
   4. https://lilianweng.github.io/lil-log/tags.html
   5. https://lilianweng.github.io/lil-log/tag/object-detection
   6. https://lilianweng.github.io/lil-log/tag/object-recognition
   7. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html
   8. https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html
   9. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#image-gradient-vector
  10. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#common-image-processing-kernels
  11. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#example-manu-in-2004
  12. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#histogram-of-oriented-gradients-hog
  13. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#how-hog-works
  14. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#example-manu-in-2004-1
  15. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#image-segmentation-felzenszwalbs-algorithm
  16. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#graph-construction
  17. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#key-concepts
  18. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#how-image-segmentation-works
  19. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#example-manu-in-2013
  20. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#selective-search
  21. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#how-selective-search-works
  22. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#configuration-variations
  23. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#references
  24. https://en.wikipedia.org/wiki/image_gradient
  25. https://en.wikipedia.org/wiki/prewitt_operator
  26. https://en.wikipedia.org/wiki/sobel_operator
  27. https://en.wikipedia.org/wiki/kernel_(image_processing)
  28. https://lilianweng.github.io/lil-log/assets/data/manu-2004.jpg
  29. http://ftw.usatoday.com/2013/05/manu-ginobilis-bald-spot-through-the-years
  30. https://github.com/opencv/opencv
  31. http://simplecv.org/
  32. http://scikit-image.org/
  33. http://cvcl.mit.edu/sunseminar/felzenszwalb_ijcv04.pdf
  34. http://fcv2011.ulsan.ac.kr/files/announcement/413/ijcv(2004) efficient graph-based image segmentation.pdf
  35. https://lilianweng.github.io/lil-log/assets/data/manu-2013.jpg
  36. http://ftw.usatoday.com/2013/05/manu-ginobilis-bald-spot-through-the-years
  37. http://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.felzenszwalb
  38. http://www.cs.ubc.ca/~lowe/papers/iccv99.pdf
  39. https://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf
  40. http://cvcl.mit.edu/sunseminar/felzenszwalb_ijcv04.pdf
  41. https://www.learnopencv.com/histogram-of-oriented-gradients/
  42. http://mccormickml.com/2013/05/07/gradient-vectors/
  43. http://mccormickml.com/2013/05/09/hog-person-detector-tutorial/
  44. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html
  45. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html
  46. https://disqus.com/?ref_noscript
  47. https://jekyllrb.com/
  48. https://github.com/jekyll/minima/
  49. https://github.com/lilianweng/lil-log/tree/gh-pages
  50. https://lilianweng.github.io/lil-log/tags.html
  51. https://lilianweng.github.io/lil-log/contact.html
  52. https://lilianweng.github.io/lil-log/faq.html
  53. https://lilianweng.github.io/lil-log/feed.xml
  54. https://scholar.google.com/citations?user=dca-pw8aaaaj&hl=en&oi=ao
  55. https://github.com/lilianweng
  56. https://www.instagram.com/lilianweng/
  57. https://twitter.com/lilianweng/
