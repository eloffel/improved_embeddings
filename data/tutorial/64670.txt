   [1]lil'log [2]     contact [3]     faq [4]     tags

id164 for dummies part 2: id98, dpm and overfeat

   dec 15, 2017 by lilian weng [5]object-detection  [6]object-recognition

     part 2 introduces several classic convolutional neural work
     architecture designs for image classification (alexnet, vgg,
     resnet), as well as dpm (deformable parts model) and overfeat models
     for object recognition.

   [7]part 1 of the    id164 for dummies    series introduced: (1)
   the concept of image gradient vector and how hog algorithm summarizes
   the information across all the gradient vectors in one image; (2) how
   the image segmentation algorithm works to detect regions that
   potentially contain objects; (3) how the selective search algorithm
   refines the outcomes of image segmentation for better region proposal.

   in part 2, we are about to find out more on the classic convolution
   neural network architectures for image classification. they lay the
   foundation for further progress on the deep learning models for object
   detection. go check [8]part 3 if you want to learn more on r-id98 and
   related models.
     * [9]id98 for image classification
          + [10]convolution operation
          + [11]alexnet (krizhevsky et al, 2012)
          + [12]vgg (simonyan and zisserman, 2014)
          + [13]resnet (he et al., 2015)
     * [14]id74: map
     * [15]deformable parts model
     * [16]overfeat
     * [17]reference

id98 for image classification

   id98, short for    convolutional neural network   , is the go-to solution
   for id161 problems in the deep learning world. it was, to
   some extent, [18]inspired by how human visual cortex system works.

convolution operation

   i strongly recommend this [19]guide to convolution arithmetic, which
   provides a clean and solid explanation with tons of visualizations and
   examples. here let   s focus on two-dimensional convolution as we are
   working with images in this post.

   in short, convolution operation slides a predefined [20]kernel (also
   called    filter   ) on top of the input feature map (matrix of image
   pixels), multiplying and adding the values of the kernel and partial
   input features to generate the output. the values form an output
   matrix, as usually, the kernel is much smaller than the input image.

   convolution operation

   fig. 1. an illustration of applying a kernel on the input feature map
   to generate the output. (image source: [21]river trail documentation)

   figure 2 showcases two real examples of how to convolve a 3x3 kernel
   over a 5x5 2d matrix of numeric values to generate a 3x3 matrix. by
   controlling the padding size and the stride length, we can generate an
   output matrix of a certain size.

   convolution operation convolution operation

   fig. 2. two examples of 2d convolution operation: (top) no padding and
   1x1 strides; (bottom) 1x1 border zeros padding and 2x2 strides. (image
   source: [22]deeplearning.net)

alexnet (krizhevsky et al, 2012)

     * 5 convolution [+ optional max pooling] layers + 2 mlp layers + 1 lr
       layer
     * use data augmentation techniques to expand the training dataset,
       such as image translations, horizontal reflections, and patch
       extractions.

   convolution pperation example

   fig. 3. the architecture of alexnet. (image source: [23]link)

vgg (simonyan and zisserman, 2014)

     * the network is considered as    very deep    at its time; 19 layers
     * the architecture is extremely simplified with only 3x3
       convolutional layers and 2x2 pooling layers. the stacking of small
       filters simulates a larger filter with fewer parameters.

resnet (he et al., 2015)

     * the network is indeed very deep; 152 layers of simple architecture.
     * residual block: some input of a certain layer can be passed to the
       component two layers later. residual blocks are essential for
       keeping a deep network trainable and eventually work. without
       residual blocks, the training loss of a plain network does not
       monotonically decrease as the number of layers increases due to
       [24]vanishing and exploding gradients.

   residual block

   fig. 4. an illustration of the residual block of resnet. in some way,
   we can say the design of residual blocks is inspired by v4 getting
   input directly from v1 in the human visual cortex system. (left image
   source: [25]wang et al., 2017)

id74: map

   a common evaluation metric used in many object recognition and
   detection tasks is    map   , short for    mean average precision   . it is a
   number from 0 to 100; higher value is better.
     * combine all detections from all test images to draw a
       precision-recall curve (pr curve) for each class; the    average
       precision    (ap) is the area under the pr curve.
     * given that target objects are in different classes, we first
       compute ap separately for each class, and then average over
       classes.
     * a detection is a true positive if it has    intersection over union   
       (iou) with a ground-truth box greater than some threshold (usually
       0.5; if so, the metric is    map@0.5   )

deformable parts model

   the deformable parts model (dpm) ([26]felzenszwalb et al., 2010)
   recognizes objects with a mixture graphical model (markov random
   fields) of deformable parts. the model consists of three major
   components:
    1. a coarse root filter defines a detection window that approximately
       covers an entire object. a filter specifies weights for a region
       feature vector.
    2. multiple part filters that cover smaller parts of the object. parts
       filters are learned at twice resolution of the root filter.
    3. a spatial model for scoring the locations of part filters relative
       to the root.

   dpm

   fig. 5. the dpm model contains (a) a root filter, (b) multiple part
   filters at twice the resolution, and (c) a model for scoring the
   location and deformation of parts.

   the quality of detecting an object is measured by the score of filters
   minus the deformation costs. the matching score , in laymen   s terms,
   is:

   in which,
     * is an image with a specified position and scale;
     * is a sub region of .
     * is the root filter.
     * is one part filter.
     * cost() measures the penalty of the part deviating from its ideal
       location relative to the root.

   the basic score model is the dot product between the filter and the
   region feature vector : . the feature set can be defined by hog or
   other similar algorithms.

   a root location with high score detects a region with high chances to
   contain an object, while the locations of the parts with high scores
   confirm a recognized object hypothesis. the paper adopted latent id166 to
   model the classifier.

   dpm matching process

   fig. 6. the matching process by dpm. (image source: [27]felzenszwalb et
   al., 2010)

   the author later claimed that dpm and id98 models are not two distinct
   approaches to object recognition. instead, a dpm model can be
   formulated as a id98 by unrolling the dpm id136 algorithm and
   mapping each step to an equivalent id98 layer. (check the details in
   [28]girshick et al., 2015!)

overfeat

   overfeat [[29]paper][[30]code] is a pioneer model of integrating the
   id164, localization and classification tasks all into one
   convolutional neural network. the main idea is to (i) do image
   classification at different locations on regions of multiple scales of
   the image in a sliding window fashion, and (ii) predict the bounding
   box locations with a regressor trained on top of the same convolution
   layers.

   the overfeat model architecture is very similar to [31]alexnet. it is
   trained as follows:

   overfeat training

   fig. 7. the training stages of the overfeat model. (image source:
   [32]link)
    1. train a id98 model (similar to alexnet) on the image classification
       task.
    2. then, we replace the top classifier layers by a regression network
       and train it to predict object bounding boxes at each spatial
       location and scale. the regressor is class-specific, each generated
       for one image class.
          + input: images with classification and bounding box.
          + output: , 4 values in total, representing the coordinates of
            the bounding box edges.
          + loss: the regressor is trained to minimize norm between
            generated bounding box and the ground truth for each training
            example.

   at the detection time,
    1. perform classification at each location using the pretrained id98
       model.
    2. predict object bounding boxes on all classified regions generated
       by the classifier.
    3. merge bounding boxes with sufficient overlap from localization and
       sufficient confidence of being the same object from the classifier.

reference

   [1] vincent dumoulin and francesco visin. [33]   a guide to convolution
   arithmetic for deep learning.    arxiv preprint arxiv:1603.07285 (2016).

   [2] haohan wang, bhiksha raj, and eric p. xing. [34]   on the origin of
   deep learning.    arxiv preprint arxiv:1702.07800 (2017).

   [3] pedro f. felzenszwalb, ross b. girshick, david mcallester, and deva
   ramanan. [35]   id164 with discriminatively trained part-based
   models.    ieee transactions on pattern analysis and machine intelligence
   32, no. 9 (2010): 1627-1645.

   [4] ross b. girshick, forrest iandola, trevor darrell, and jitendra
   malik. [36]   deformable part models are convolutional neural networks.   
   in proc. ieee conf. on id161 and pattern recognition (cvpr),
   pp. 437-446. 2015.

   [5] sermanet, pierre, david eigen, xiang zhang, micha  l mathieu, rob
   fergus, and yann lecun. [37]   overfeat: integrated recognition,
   localization and detection using convolutional networks    arxiv preprint
   arxiv:1312.6229 (2013).
     __________________________________________________________________

   if you notice mistakes and errors in this post, please don   t hesitate
   to contact me at [lilian dot wengweng at gmail dot com] and i would be
   super happy to correct them right away!

   see you in the next post :d
   [38]    id164 for dummies part 1: gradient vector, hog, and ss
   [39]id164 for dummies part 3: r-id98 family    
   please enable javascript to view the [40]comments powered by disqus.

   2019    built by [41]jekyll and [42]minima | view [43]this on github |
   [44]tags | [45]contact | [46]faq

   [47][logo_rss.png] [48][logo_scholar.png] [49][logo_github.png]
   [50][logo_instagram.png] [51][logo_twitter.png]

references

   1. https://lilianweng.github.io/lil-log/
   2. https://lilianweng.github.io/lil-log/contact.html
   3. https://lilianweng.github.io/lil-log/faq.html
   4. https://lilianweng.github.io/lil-log/tags.html
   5. https://lilianweng.github.io/lil-log/tag/object-detection
   6. https://lilianweng.github.io/lil-log/tag/object-recognition
   7. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html
   8. https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html
   9. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#id98-for-image-classification
  10. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#convolution-operation
  11. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#alexnet-krizhevsky-et-al-2012
  12. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#vgg-simonyan-and-zisserman-2014
  13. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#resnet-he-et-al-2015
  14. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#evaluation-metrics-map
  15. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#deformable-parts-model
  16. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#overfeat
  17. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#reference
  18. https://lilianweng.github.io/lil-log/2017/06/21/an-overview-of-deep-learning.html#convolutional-neural-network
  19. https://arxiv.org/pdf/1603.07285.pdf
  20. https://en.wikipedia.org/wiki/kernel_(image_processing)
  21. http://intellabs.github.io/rivertrail/tutorial/
  22. http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html
  23. http://vision03.csail.mit.edu/id98_art/index.html
  24. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  25. https://arxiv.org/pdf/1312.6229.pdf
  26. http://people.cs.uchicago.edu/~pff/papers/lid166-pami.pdf
  27. http://people.cs.uchicago.edu/~pff/papers/lid166-pami.pdf
  28. https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/girshick_deformable_part_models_2015_cvpr_paper.pdf
  29. https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf
  30. https://github.com/sermanet/overfeat
  31. https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#alexnet-krizhevsky-et-al-2012
  32. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf
  33. https://arxiv.org/pdf/1603.07285.pdf
  34. https://arxiv.org/pdf/1702.07800.pdf
  35. http://people.cs.uchicago.edu/~pff/papers/lid166-pami.pdf
  36. https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/girshick_deformable_part_models_2015_cvpr_paper.pdf
  37. https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf
  38. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html
  39. https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html
  40. https://disqus.com/?ref_noscript
  41. https://jekyllrb.com/
  42. https://github.com/jekyll/minima/
  43. https://github.com/lilianweng/lil-log/tree/gh-pages
  44. https://lilianweng.github.io/lil-log/tags.html
  45. https://lilianweng.github.io/lil-log/contact.html
  46. https://lilianweng.github.io/lil-log/faq.html
  47. https://lilianweng.github.io/lil-log/feed.xml
  48. https://scholar.google.com/citations?user=dca-pw8aaaaj&hl=en&oi=ao
  49. https://github.com/lilianweng
  50. https://www.instagram.com/lilianweng/
  51. https://twitter.com/lilianweng/
