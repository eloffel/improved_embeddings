   [1]lil'log [2]     contact [3]     faq [4]     tags

learning id27

   oct 15, 2017 by lilian weng [5]nlp

     id27 is a dense representation of words in the form of
     numeric vectors. it can be learned using a variety of language
     models. the id27 representation is able to reveal many
     hidden relationships between words. for example, vector(   cat   ) -
     vector(   kitten   ) is similar to vector(   dog   ) - vector(   puppy   ). this
     post introduces several models for learning id27 and how
     their id168s are designed for the purpose.

   human vocabulary comes in free text. in order to make a machine
   learning model understand and process the natural language, we need to
   transform the free-text words into numeric values. one of the simplest
   transformation approaches is to do a one-hot encoding in which each
   distinct word stands for one dimension of the resulting vector and a
   binary value indicates whether the word presents (1) or not (0).

   however, one-hot encoding is impractical computationally when dealing
   with the entire vocabulary, as the representation demands hundreds of
   thousands of dimensions. id27 represents words and phrases in
   vectors of (non-binary) numeric values with much lower and thus denser
   dimensions. an intuitive assumption for good id27 is that
   they can approximate the similarity between words (i.e.,    cat    and
      kitten    are similar words, and thus they are expected to be close in
   the reduced vector space) or disclose hidden semantic relationships
   (i.e., the relationship between    cat    and    kitten    is an analogy to the
   one between    dog    and    puppy   ). contextual information is super useful
   for learning word meaning and relationship, as similar words may appear
   in the similar context often.
     * [6]count-based vector space model
     * [7]context-based: skip-gram model
     * [8]context-based: continuous bag-of-words (cbow)
     * [9]id168s
          + [10]full softmax
          + [11]hierarchical softmax
          + [12]cross id178
          + [13]noise contrastive estimation (nce)
          + [14]negative sampling (neg)
     * [15]other tips for learning id27
     * [16]glove: global vectors
     * [17]examples: id97 on    game of thrones   
     * [18]references

   there are two main approaches for learning id27, both relying
   on the contextual knowledge.
     * count-based: the first one is unsupervised, based on matrix
       factorization of a global word co-occurrence matrix. raw
       co-occurrence counts do not work well, so we want to do smart
       things on top.
     * context-based: the second approach is supervised. given a local
       context, we want to design a model to predict the target words and
       in the meantime, this model learns the efficient id27
       representation.

count-based vector space model

   count-based vector space models heavily rely on the word frequency and
   co-occurrence matrix with the assumption that words in the same
   contexts share similar or related semantic meanings. the models map
   count-based statistics like co-occurrences between neighboring words
   down to a small and dense word vectors. pca, topic models, and neural
   probabilistic language models are all good examples of this category.
     __________________________________________________________________

   different from the count-based approaches, context-based methods build
   predictive models that directly target at predicting a word given its
   neighbors. the dense word vectors are part of the model parameters. the
   best vector representation of each word is learned during the model
   training process.

context-based: skip-gram model

   suppose that you have a sliding window of a fixed size moving along a
   sentence: the word in the middle is the    target    and those on its left
   and right within the sliding window are the context words. the
   skip-gram model ([19]mikolov et al., 2013) is trained to predict the
   probabilities of a word being a context word for the given target.

   the following example demonstrates multiple pairs of target and context
   words as training samples, generated by a 5-word window sliding along
   the sentence.

        the man who passes the sentence should swing the sword.        ned
     stark

       sliding window (size = 5)     target word           context
   [the man who]                     the         man, who
   [the man who passes]              man         the, who, passes
   [the man who passes the]          who         the, man, passes, the
   [man who passes the sentence]     passes      man, who, the, sentence
                                                        
   [sentence should swing the sword] swing       sentence, should, the, sword
   [should swing the sword]          the         should, swing, sword
   [swing the sword]                 sword       swing, the

   each context-target pair is treated as a new observation in the data.
   for example, the target word    swing    in the above case produces four
   training samples: (   swing   ,    sentence   ), (   swing   ,    should   ), (   swing   ,
      the   ), and (   swing   ,    sword   ).

   skip-gram model

   fig. 1. the skip-gram model. both the input vector and the output are
   one-hot encoded word representations. the hidden layer is the word
   embedding of size .

   given the vocabulary size , we are about to learn id27
   vectors of size . the model learns to predict one context word (output)
   using one target word (input) at a time.

   according to fig. 1,
     * both input word and the output word are one-hot encoded into binary
       vectors and of size .
     * first, the multiplication of the binary vector and the word
       embedding matrix of size gives us the embedding vector of the input
       word : the i-th row of the matrix .
     * this newly discovered embedding vector of dimension forms the
       hidden layer.
     * the multiplication of the hidden layer and the word context matrix
       of size produces the output one-hot encoded vector .
     * the output context matrix encodes the meanings of words as context,
       different from the embedding matrix . note: despite the name, is
       independent of , not a transpose or inverse or whatsoever.

context-based: continuous bag-of-words (cbow)

   the continuous bag-of-words (cbow) is another similar model for
   learning word vectors. it predicts the target word (i.e.    swing   ) from
   source context words (i.e.,    sentence should the sword   ).

   cbow model

   fig. 2. the cbow model. word vectors of multiple context words are
   averaged to get a fixed-length vector as in the hidden layer. other
   symbols have the same meanings as in fig 1.

   because there are multiple contextual words, we average their
   corresponding word vectors, constructed by the multiplication of the
   input vector and the matrix . because the averaging stage smoothes over
   a lot of the distributional information, some people believe the cbow
   model is better for small dataset.

id168s

   both the skip-gram model and the cbow model should be trained to
   minimize a well-designed loss/objective function. there are several
   id168s we can incorporate to train these language models. in
   the following discussion, we will use the skip-gram model as an example
   to describe how the loss is computed.

full softmax

   the skip-gram model defines the embedding vector of every word by the
   matrix and the context vector by the output matrix . given an input
   word , let us label the corresponding row of as vector (embedding
   vector) and its corresponding column of as (context vector). the final
   output layer applies softmax to compute the id203 of predicting
   the output word given , and therefore:

   this is accurate as presented in fig. 1. however, when is extremely
   large, calculating the denominator by going through all the words for
   every single sample is computationally impractical. the demand for more
   efficient id155 estimation leads to the new methods
   like hierarchical softmax.

hierarchical softmax

   morin and bengio ([20]2005) proposed hierarchical softmax to make the
   sum calculation faster with the help of a binary tree structure. the
   hierarchical softmax encodes the language model   s output softmax layer
   into a tree hierarchy, where each leaf is one word and each internal
   node stands for relative probabilities of the children nodes.

   hierarchical softmax

   fig. 3. an illustration of the hierarchical softmax binary tree. the
   leaf nodes in white are words in the vocabulary. the gray inner nodes
   carry information on the probabilities of reaching its child nodes. one
   path starting from the root to the leaf . denotes the j-th node on this
   path. (image source: [21]id97 parameter learning explained)

   each word has a unique path from the root down to its corresponding
   leaf. the id203 of picking this word is equivalent to the
   id203 of taking this path from the root down through the tree
   branches. since we know the embedding vector of the internal node , the
   id203 of getting the word can be computed by the product of
   taking left or right turn at every internal node stop.

   according to fig. 3, the id203 of one node is ( is the sigmoid
   function):

   the final id203 of getting a context word given an input word is:

   where is the depth of the path leading to the word and is a specially
   indicator function which returns 1 if is the left child of otherwise
   -1. the internal nodes    embeddings are learned during the model
   training. the tree structure helps greatly reduce the complexity of the
   denominator estimation from o(v) (vocabulary size) to o(log v) (the
   depth of the tree) at the training time. however, at the prediction
   time, we still to compute the id203 of every word and pick the
   best, as we don   t know which leaf to reach for in advance.

   a good tree structure is crucial to the model performance. several
   handy principles are: group words by frequency like what is implemented
   by huffman tree for simple speedup; group similar words into same or
   close branches (i.e. use predefined word clusters, id138).

cross id178

   another approach completely steers away from the softmax framework.
   instead, the id168 measures the cross id178 between the
   predicted probabilities and the true binary labels .

   first, let   s recall that the cross id178 between two distributions
   and is measured as . in our case, the true label is 1 only when is the
   output word; is 0 otherwise. the id168 of the model with
   parameter config aims to minimize the cross id178 between the
   prediction and the ground truth, as lower cross id178 indicates high
   similarity between two distributions.

   recall that,

   therefore,

   to start training the model using back-propagation with sgd, we need to
   compute the gradient of the id168. for simplicity, let   s label
   .

   where is the distribution of noise samples.

   according to the formula above, the correct output word has a positive
   reinforcement according to the first term (the larger the better loss
   we have), while other words have a negative impact as captured by the
   second term.

   how to estimate with a sample set of noise words rather than scanning
   through the entire vocabulary is the key of using cross-id178-based
   sampling approach.

noise contrastive estimation (nce)

   the noise contrastive estimation (nce) metric intends to differentiate
   the target word from noise samples using a id28
   classifier ([22]gutmann and hyv  rinen, 2010).

   given an input word , the correct output word is known as . in the
   meantime, we sample other words from the noise sample distribution ,
   denoted as . let   s label the decision of the binary classifier as and
   can only take a binary value.

   when is big enough, according to [23]the law of large numbers,

   to compute the id203 , we can start with the joint id203 .
   among , we have 1 out of (n+1) chance to pick the true word , which is
   sampled from the id155 ; meanwhile, we have n out of
   (n+1) chances to pick a noise word, each sampled from . thus,

   then we can figure out and :

   finally the id168 of nce   s binary classifier becomes:

   however, still involves summing up the entire vocabulary in the
   denominator. let   s label the denominator as a partition function of the
   input word, . a common assumption is given that we expect the softmax
   output layer to be normalized ([24]minh and teh, 2012). then the loss
   function is simplified to:

   the noise distribution is a tunable parameter and we would like to
   design it in a way so that:
     * intuitively it should be very similar to the real data
       distribution; and
     * it should be easy to sample from.

   for example, the sampleing implementation
   ([25]log_uniform_candidate_sampler) of nce loss in tensorflow assumes
   that such noise samples follow a log-uniform distribution, also known
   as [26]zipfian   s law. the id203 of a given word in logarithm is
   expected to be reversely proportional to its rank, while high-frequency
   words are assigned with lower ranks. in this case, , where is the rank
   of a word by frequency in descending order.

negative sampling (neg)

   the negative sampling (neg) proposed by mikolov et al. ([27]2013) is a
   simplified variation of nce loss. it is especially famous for training
   google   s [28]id97 project. different from nce loss which attempts
   to approximately maximize the log id203 of the softmax output,
   negative sampling did further simplification because it focuses on
   learning high-quality id27 rather than modeling the word
   distribution in natural language.

   neg approximates the binary classifier   s output with sigmoid functions
   as follows:

   the final nce id168 looks like:

other tips for learning id27

   mikolov et al. ([29]2013) suggested several helpful practices that
   could result in good id27 learning outcomes.
     * soft sliding window. when pairing the words within the sliding
       window, we could assign less weight to more distant words. one
       heuristic is     given a maximum window size parameter defined, , the
       actual window size is randomly sampled between 1 and for every
       training sample. thus, each context word has the id203 of
       1/(its distance to the target word) being observed, while the
       adjacent words are always observed.
     * subsampling frequent words. extremely frequent words might be too
       general to differentiate the context (i.e. think about stopwords).
       while on the other hand, rare words are more likely to carry
       distinct information. to balance the frequent and rare words,
       mikolov et al. proposed to discard words with id203 during
       sampling. here is the word frequency and is an adjustable
       threshold.
     * learning phrases first. a phrase often stands as a conceptual unit,
       rather than a simple composition of individual words. for example,
       we cannot really tell    new york    is a city name even we know the
       meanings of    new    and    york   . learning such phrases first and
       treating them as word units before training the id27
       model improves the outcome quality. a simple data-driven approach
       is based on unigram and bigram counts: , where is simple count of
       an unigram or bigram and is a discounting threshold to prevent
       super infrequent words and phrases. higher scores indicate higher
       chances of being phrases. to form phrases longer than two words, we
       can scan the vocabulary multiple times with decreasing score cutoff
       values.

glove: global vectors

   the global vector (glove) model proposed by pennington et al.
   ([30]2014) aims to combine the count-based id105 and the
   context-based skip-gram model together.

   we all know the counts and co-occurrences can reveal the meanings of
   words. to distinguish from in the context of a id27 word, we
   would like to define the co-ocurrence id203 as:

   counts the co-occurrence between words and .

   say, we have two words, =   ice    and =   steam   . the third word =   solid    is
   related to    ice    but not    steam   , and thus we expect to be much larger
   than and therefore to be very large. if the third word =    water    is
   related to both or =    fashion    is unrelated to either of them, is
   expected to be close to one.

   the intuition here is that the word meanings are captured by the ratios
   of co-occurrence probabilities rather than the probabilities
   themselves. the global vector models the relationship between two words
   regarding to the third context word as:

   further, since the goal is to learn meaningful word vectors, is
   designed to be a function of the linear difference between two words :

   with the consideration of being symmetric between target words and
   context words, the final solution is to model as an exponential
   function. please read the original paper ([31]pennington et al., 2014)
   for more details of the equations.

   finally,

   since the second term is independent of , we can add bias term for to
   capture . to keep the symmetric form, we also add in a bias for .

   the id168 for the glove model is designed to preserve the above
   formula by minimizing the sum of the squared errors:

   the weighting schema is a function of the co-occurrence of and and it
   is an adjustable model configuration. it should be close to zero as ;
   should be non-decreasing as higher co-occurrence should have more
   impact; should saturate when become extremely large. the paper proposed
   the following weighting function.

examples: id97 on    game of thrones   

   after reviewing all the theoretical knowledge above, let   s try a little
   experiment in id27 extracted from [32]the games of thrones
   corpus. the process is super straightforward using [33]gensim.

   step 1: extract words
import sys
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize

stop_words = set(stopwords.words('english'))

def get_words(txt):
    return filter(
        lambda x: x not in stop_words,
        re.findall(r'\b(\w+)\b', txt)
    )

def parse_sentence_words(input_file_names):
   """returns a list of a list of words. each sublist is a sentence."""
    sentence_words = []
    for file_name in input_file_names:
        for line in open(file_name):
            line = line.strip().lower()
            line = line.decode('unicode_escape').encode('ascii','ignore')
            sent_words = map(get_words, sent_tokenize(line))
            sent_words = filter(lambda sw: len(sw) > 1, sent_words)
            if len(sent_words) > 1:
                sentence_words += sent_words
    return sentence_words

# you would see five .txt files after unzip 'a_song_of_ice_and_fire.zip'
input_file_names = ["001ssb.txt", "002ssb.txt", "003ssb.txt",
                    "004ssb.txt", "005ssb.txt"]
got_sentence_words= parse_sentence_words(input_file_names)

   step 2: feed a id97 model
from gensim.models import id97

# size: the dimensionality of the embedding vectors.
# window: the maximum distance between the current and predicted word within a s
entence.
model = id97(got_sentence_words, size=128, window=3, min_count=5, workers=4)
model.wv.save_id97_format("got_id97.txt", binary=false)

   step 3: check the results

   in the got id27 space, the top similar words to    king    and
      queen    are:
   model.most_similar('king', topn=10)
   (word, similarity with    king   ) model.most_similar('queen', topn=10)
   (word, similarity with    queen   )
   (   kings   , 0.897245) (   cersei   , 0.942618)
   (   baratheon   , 0.809675) (   joffrey   , 0.933756)
   (   son   , 0.763614) (   margaery   , 0.931099)
   (   robert   , 0.708522) (   sister   , 0.928902)
   (   lords   , 0.698684) (   prince   , 0.927364)
   (   joffrey   , 0.696455) (   uncle   , 0.922507)
   (   prince   , 0.695699) (   varys   , 0.918421)
   (   brother   , 0.685239) (   ned   , 0.917492)
   (   aerys   , 0.684527) (   melisandre   , 0.915403)
   (   stannis   , 0.682932) (   robb   , 0.915272)
     __________________________________________________________________

   if you notice mistakes and errors in this post, don   t hesitate to
   contact me at [lilian dot wengweng at gmail dot com] and i would be
   super happy to correct them right away!

   see you in the next post :d

references

   [1] tensorflow tutorial [34]vector representations of words.

   [2] [35]   id97 tutorial - the skip-gram model    by chris mccormick.

   [3] [36]   on id27s - part 2: approximating the softmax    by
   sebastian ruder.

   [4] xin rong. [37]id97 parameter learning explained

   [5] mikolov, tomas, kai chen, greg corrado, and jeffrey dean.
   [38]   efficient estimation of word representations in vector space.   
   arxiv preprint arxiv:1301.3781 (2013).

   [6] frederic morin and yoshua bengio. [39]   hierarchical probabilistic
   neural network language model.    aistats. vol. 5. 2005.

   [7] michael gutmann and aapo hyv  rinen. [40]   noise-contrastive
   estimation: a new estimation principle for unnormalized statistical
   models.    proc. intl. conf. on artificial intelligence and statistics.
   2010.

   [8] tomas mikolov, ilya sutskever, kai chen, greg corrado, and jeffrey
   dean. [41]   distributed representations of words and phrases and their
   compositionality.    advances in neural information processing systems.
   2013.

   [9] tomas mikolov, kai chen, greg corrado, and jeffrey dean.
   [42]   efficient estimation of word representations in vector space.   
   arxiv preprint arxiv:1301.3781 (2013).

   [10] marco baroni, georgiana dinu, and germ  n kruszewski. [43]   don   t
   count, predict! a systematic comparison of context-counting vs.
   context-predicting semantic vectors.    acl (1). 2014.

   [11] jeffrey pennington, richard socher, and christopher manning.
   [44]   glove: global vectors for word representation.    proc. conf. on
   empirical methods in natural language processing (emnlp). 2014.
   [45]    anatomize deep learning with id205 [46]object
   detection for dummies part 1: gradient vector, hog, and ss    
   please enable javascript to view the [47]comments powered by disqus.

   2019    built by [48]jekyll and [49]minima | view [50]this on github |
   [51]tags | [52]contact | [53]faq

   [54][logo_rss.png] [55][logo_scholar.png] [56][logo_github.png]
   [57][logo_instagram.png] [58][logo_twitter.png]

references

   1. https://lilianweng.github.io/lil-log/
   2. https://lilianweng.github.io/lil-log/contact.html
   3. https://lilianweng.github.io/lil-log/faq.html
   4. https://lilianweng.github.io/lil-log/tags.html
   5. https://lilianweng.github.io/lil-log/tag/nlp
   6. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#count-based-vector-space-model
   7. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#context-based-skip-gram-model
   8. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#context-based-continuous-bag-of-words-cbow
   9. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions
  10. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#full-softmax
  11. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#hierarchical-softmax
  12. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-id178
  13. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#noise-contrastive-estimation-nce
  14. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#negative-sampling-neg
  15. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#other-tips-for-learning-word-embedding
  16. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#glove-global-vectors
  17. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#examples-id97-on-game-of-thrones
  18. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#references
  19. https://arxiv.org/pdf/1301.3781.pdf
  20. https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf
  21. https://arxiv.org/pdf/1411.2738.pdf
  22. http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf
  23. https://en.wikipedia.org/wiki/law_of_large_numbers
  24. https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf
  25. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/candidate_sampling_ops.py#l83
  26. https://en.wikipedia.org/wiki/zipf's_law
  27. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  28. https://code.google.com/archive/p/id97/
  29. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  30. http://www.aclweb.org/anthology/d14-1162
  31. http://www.aclweb.org/anthology/d14-1162
  32. https://lilianweng.github.io/lil-log/assets/data/a_song_of_ice_and_fire.zip
  33. https://radimrehurek.com/gensim/models/id97.html
  34. https://www.tensorflow.org/tutorials/id97
  35. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  36. http://ruder.io/word-embeddings-softmax/
  37. https://arxiv.org/pdf/1411.2738.pdf
  38. https://arxiv.org/pdf/1301.3781.pdf
  39. https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf
  40. http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf
  41. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  42. https://arxiv.org/pdf/1301.3781.pdf
  43. http://anthology.aclweb.org/p/p14/p14-1023.pdf
  44. http://www.aclweb.org/anthology/d14-1162
  45. https://lilianweng.github.io/lil-log/2017/09/28/anatomize-deep-learning-with-information-theory.html
  46. https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html
  47. https://disqus.com/?ref_noscript
  48. https://jekyllrb.com/
  49. https://github.com/jekyll/minima/
  50. https://github.com/lilianweng/lil-log/tree/gh-pages
  51. https://lilianweng.github.io/lil-log/tags.html
  52. https://lilianweng.github.io/lil-log/contact.html
  53. https://lilianweng.github.io/lil-log/faq.html
  54. https://lilianweng.github.io/lil-log/feed.xml
  55. https://scholar.google.com/citations?user=dca-pw8aaaaj&hl=en&oi=ao
  56. https://github.com/lilianweng
  57. https://www.instagram.com/lilianweng/
  58. https://twitter.com/lilianweng/
