
   (button)
     * [1]home
     * [2]research
          + [3]publications
          + [4]alphago
          + [5]id25
          + [6]dnc
          + [7]open source
          + [8]access to science
     * [9]applied
          + [10]deepmind health
          + [11]deepmind for google
          + [12]deepmind ethics & society
     * [13]news & blog
     * [14]about us
     * [15]careers

   (button) search (button) search
   [deepmind_logo_swirl.png]

[16]research

highlighted research

     * [17]alphago
     * [18]id25
     * [19]dnc

[20]publications

[21]open source

latest research news

   [22]towards robust and verified ai: specification testing, robust
   training, and formal verification

[23]applied

[24]deepmind health

[25]deepmind for google

[26]deepmind ethics & society

latest applied news

   [27]scaling streams with google

[28]careers

     * [29]home
     * [30]news & blog
     * [31]about us
     * [32]press
     * [33]terms and conditions
     * [34]privacy policy     updated

     *
     *
     *

   [blog-hero-img-interoperability-19.2e16d0ba.fill-1100x400.png]

understanding deep learning through neuron deletion

   deep neural networks are composed of many individual neurons, which
   combine in complex and counterintuitive ways to solve a wide range of
   challenging tasks. this complexity grants neural networks their power
   but also earns them their reputation as confusing and opaque black
   boxes.

   understanding how deep neural networks function is critical for
   explaining their decisions and enabling us to build more powerful
   systems. for instance, imagine the difficulty of trying to build a
   clock without understanding how individual gears fit together. one
   approach to understanding neural networks, both in neuroscience and
   deep learning, is to investigate the role of individual neurons,
   especially those which are easily interpretable.

   our investigation into the [35]importance of single directions for
   generalisation, soon to appear at the sixth international conference on
   learning representations ([36]iclr), uses an approach inspired by
   decades of experimental neuroscience     exploring the impact of damage    
   to determine: how important are small groups of neurons in deep neural
   networks? are more easily interpretable neurons also more important to
   the network   s computation?

   we measured the performance impact of damaging the network by deleting
   individual neurons as well as groups of neurons. our experiments led to
   two surprising findings:

     * although many previous studies have focused on understanding easily
       interpretable individual neurons (e.g.    cat neurons   , or neurons in
       the hidden layers of deep networks which are only active in
       response to images of cats), we found that these interpretable
       neurons are no more important than confusing neurons with
       difficult-to-interpret activity.
     * networks which correctly classify unseen images are more resilient
       to neuron deletion than networks which can only classify images
       they have seen before. in other words, networks which generalise
       well are much less reliant on single directions than those which
       memorise.

   cat neurons    may be more interpretable, but they   re not more important

   in both neuroscience and deep learning, easily interpretable neurons
   (   selective    neurons) which are only active in response to images of a
   single input category, such as dogs, have been analysed extensively. in
   deep learning, this has led to the emphasis on [37]cat
   neurons, [38]sentiment neurons, and [39]parentheses neurons; in
   neuroscience, [40]jennifer aniston neurons, among others. however, the
   relative importance of these few highly selective neurons compared to
   the majority of neurons which have low selectivity and more puzzling,
   hard-to-interpret activity has remained unknown.
   [selectivity_figure.jpg]

   neurons with clear response patterns (e.g., active for cats, inactive
   for everything else) are much more interpretable than confusing neurons
   which are active and inactive in response to seemingly random sets of
   images.

   to evaluate neuron importance, we measured how network performance on
   image classification tasks changes when a neuron is deleted. if a
   neuron is very important, deleting it should be highly damaging and
   substantially decrease network performance, while the deletion of an
   unimportant neuron should have little impact. neuroscientists routinely
   perform similar experiments, although they cannot achieve the
   fine-grained precision which is necessary for these experiments and
   readily available in id158s.

   iframe:
   [41]https://storage.googleapis.com/deepmind-media/research/understandin
   g-through-neuron-deletion/ablation/index.html

   conceptual diagram of the impact of deletion on a simple neural
   network. darker neurons are more active. try clicking hidden layer
   neurons to delete them and see how the activity of the output neurons
   changes. notice that deleting only one or two neurons has a small
   impact on the output, while deleting most of the neurons has a large
   impact, and that some neurons are more important than others!

   surprisingly, we found that there was little relationship between
   selectivity and importance. in other words,    cat neurons    were no more
   important than confusing neurons. this finding echoes recent work in
   neuroscience which has demonstrated that confusing neurons can actually
   be quite informative, and suggests that we must look beyond the most
   easily interpretable neurons in order to understand deep neural
   networks.

   iframe:
   [42]https://storage.googleapis.com/deepmind-media/research/understandin
   g-through-neuron-deletion/selectivity/index.html

   while "cat neurons" may be more interpretable, they are no more
   important than confusing neurons that have no obvious preference. try
   clicking the plot to see what we   d expect for different relationships
   between importance and interpretability!

   although interpretable neurons are easier to understand intuitively
   (   it likes dogs   ), they are no more important than confusing neurons
   with no obvious preference.

networks which generalise better are harder to break

   we seek to build intelligent systems, and we can only call a system
   intelligent if it can generalise to new situations. for example, an
   image classification network which can only classify specific dog
   images that it has seen before, but not new images of the same dog, is
   useless. it is only in the intelligent categorisation of new examples
   that these systems gain their utility. a [43]recent collaborative paper
   from google brain, berkeley, and deepmind which won best paper at iclr
   2017 showed that deep nets can simply memorise each and every image on
   which they are trained instead of learning in a more human-like way
   (e.g., understanding the abstract notion of a "dog").

   however, it is often unclear whether a network has learned a solution
   which will generalise to new situations or not. by deleting
   progressively larger and larger groups of neurons, we found that
   networks which generalise well were much more robust to deletions than
   networks which simply memorised images that were previously seen during
   training. in other words, networks which generalise better are harder
   to break (although they can definitely still be broken).

   as increasingly large groups of neurons are deleted, the performance of
   networks which generalise well decreases much more slowly than the
   performance of networks which memorise.

   by measuring network robustness in this way, we can evaluate whether a
   network is exploiting undesirable memorisation to    cheat.   
   understanding how networks change when they memorise will help us to
   build new networks which memorise less and generalise more.

neuroscience-inspired analysis

   together, these findings demonstrate the power of using techniques
   inspired by experimental neuroscience to understand neural networks.
   using these methods, we found that highly selective individual neurons
   are no more important than non-selective neurons, and that networks
   which generalise well are much less reliant on individual neurons than
   those which simply memorise the training data. these results imply that
   individual neurons may be much less important than a first glance may
   suggest.

   by working to explain the role of all neurons, not just those which are
   easy-to-interpret, we hope to better understand the inner workings of
   neural networks, and critically, to use this understanding to build
   more intelligent and general systems.
     __________________________________________________________________

   read the full paper [44]here.

   this work was done by ari s morcos, david gt barrett, neil c
   rabinowitz, and matthew botvinick.

   visualisations were created by paul lewis, adam cain, and doug fritz.

   share article
     *
     *
     *
     *
     * [ ]
          + [45]linkedin
          + whatsapp
          + sms
          + [46]reddit

   authors
   wednesday, 21 march 2018
     * [ari_morcos.2e16d0ba.fill-80x80.png]
       ari morcos
       research scientist, deepmind
     * [barrettdavid.2e16d0ba.fill-80x80_vczmk5a.png]
       david barrett
       research scientist, deepmind

   ____________________
   ____________________
   [47]show all results
   (button) close

[48]deepmind logo

   follow
     *
     *
     *

     * [49]research [50]research
     * [51]applied [52]applied
     * [53]news & blog [54]news & blog
     * [55]about us [56]about us
     * [57]careers [58]careers

     * [59]press
     * [60]terms and conditions
     * [61]privacy policy     updated
     * [62]modern slavery statement
     * [63]alphabet inc

      2019 deepmind technologies limited

   deepmind.com uses cookies to help give you the best possible user
   experience and to allow us to see how the site is used. by using this
   site, you agree that we can set and use these cookies. for more
   information on cookies and how to change your settings, see our
   [64]privacy policy.

references

   visible links
   1. https://deepmind.com/
   2. https://deepmind.com/research/
   3. https://deepmind.com/research/publications/
   4. https://deepmind.com/research/alphago/
   5. https://deepmind.com/research/id25/
   6. https://deepmind.com/research/dnc/
   7. https://deepmind.com/research/open-source/
   8. https://deepmind.com/research/access-science/
   9. https://deepmind.com/applied/
  10. https://deepmind.com/applied/deepmind-health/
  11. https://deepmind.com/applied/deepmind-google/
  12. https://deepmind.com/applied/deepmind-ethics-society/
  13. https://deepmind.com/blog/
  14. https://deepmind.com/about/
  15. https://deepmind.com/careers/
  16. https://deepmind.com/research/
  17. https://deepmind.com/research/alphago/
  18. https://deepmind.com/research/id25/
  19. https://deepmind.com/research/dnc/
  20. https://deepmind.com/research/publications/
  21. https://deepmind.com/research/open-source/
  22. https://deepmind.com/blog/robust-and-verified-ai/
  23. https://deepmind.com/applied/
  24. https://deepmind.com/applied/deepmind-health/
  25. https://deepmind.com/applied/deepmind-google/
  26. https://deepmind.com/applied/deepmind-ethics-society/
  27. https://deepmind.com/blog/scaling-streams-google/
  28. https://deepmind.com/careers/
  29. https://deepmind.com/
  30. https://deepmind.com/blog/
  31. https://deepmind.com/about/
  32. https://deepmind.com/press/
  33. https://deepmind.com/terms-and-conditions/
  34. https://deepmind.com/privacy-policy/
  35. https://arxiv.org/abs/1803.06959
  36. https://iclr.cc/
  37. https://www.wired.com/2012/06/google-x-neural-network/
  38. https://blog.openai.com/unsupervised-sentiment-neuron/
  39. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  40. https://www.newscientist.com/article/dn7567-why-your-brain-has-a-jennifer-aniston-cell/
  41. https://storage.googleapis.com/deepmind-media/research/understanding-through-neuron-deletion/ablation/index.html
  42. https://storage.googleapis.com/deepmind-media/research/understanding-through-neuron-deletion/selectivity/index.html
  43. https://arxiv.org/abs/1611.03530
  44. https://arxiv.org/abs/1803.06959
  45. https://www.linkedin.com/sharearticle?mini=true&url=https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/&title=understanding deep learning through neuron deletion&summary=&source=google deepmind
  46. http://www.reddit.com/submit?url=https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/&title=understanding deep learning through neuron deletion
  47. https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/
  48. https://deepmind.com/
  49. https://deepmind.com/research/
  50. https://deepmind.com/research/
  51. https://deepmind.com/applied/
  52. https://deepmind.com/applied/
  53. https://deepmind.com/blog/
  54. https://deepmind.com/blog/
  55. https://deepmind.com/about/
  56. https://deepmind.com/about/
  57. https://deepmind.com/careers/
  58. https://deepmind.com/careers/
  59. https://deepmind.com/press/
  60. https://deepmind.com/terms-and-conditions/
  61. https://deepmind.com/privacy-policy/
  62. https://storage.googleapis.com/deepmind-media/modern_slavery/final_201_google_modern_slavery_statement.pdf
  63. https://abc.xyz/
  64. https://deepmind.com/privacy-policy/

   hidden links:
  66. https://twitter.com/deepmindai
  67. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  68. https://plus.google.com/+deepmindai
  69. http://twitter.com/intent/tweet/?text=&url=https%3a//deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/
  70. http://www.facebook.com/share.php?u=https%3a//deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/&t=
  71. https://plus.google.com/share?url=https%3a//deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/
  72. mailto:?subject=understanding%20deep%20learning%20through%20neuron%20deletion&body=%0d%0a%0d%0ahttps%3a//deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/
  73. https://twitter.com/deepmindai
  74. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  75. https://plus.google.com/+deepmindai
