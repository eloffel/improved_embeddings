   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]programming
   [7]become a member[8]sign in[9]get started
     __________________________________________________________________

software 2.0

   [10]go to the profile of andrej karpathy
   [11]andrej karpathy (button) blockedunblock (button) followfollowing
   nov 11, 2017

   i sometimes see people refer to neural networks as just    another tool
   in your machine learning toolbox   . they have some pros and cons, they
   work here or there, and sometimes you can use them to win kaggle
   competitions. unfortunately, this interpretation completely misses the
   forest for the trees. neural networks are not just another classifier,
   they represent the beginning of a fundamental shift in how we write
   software. they are software 2.0.

   the    classical stack    of software 1.0 is what we   re all familiar
   with         it is written in languages such as python, c++, etc. it consists
   of explicit instructions to the computer written by a programmer. by
   writing each line of code, the programmer identifies a specific point
   in program space with some desirable behavior.
   [1*chcu2l0nmazwcpqgms1bya.jpeg]

   in contrast, software 2.0 can be written in much more abstract, human
   unfriendly language, such as the weights of a neural network. no human
   is involved in writing this code because there are a lot of weights
   (typical networks might have millions), and coding directly in weights
   is kind of hard (i tried).
   [1*6eb1xue1wm_qp0iizxphqa.png]

   instead, our approach is to specify some goal on the behavior of a
   desirable program (e.g.,    satisfy a dataset of input output pairs of
   examples   , or    win a game of go   ), write a rough skeleton of the code
   (e.g. a neural net architecture), that identifies a subset of program
   space to search, and use the computational resources at our disposal to
   search this space for a program that works. in the specific case of
   neural networks, we restrict the search to a continuous subset of the
   program space where the search process can be made (somewhat
   surprisingly) efficient with id26 and stochastic gradient
   descent.
   [1*5ng3u8msatqmqpjkr_-uow.png]

   it turns out that a large portion of real-world problems have the
   property that it is significantly easier to collect the data (or more
   generally, identify a desirable behavior) than to explicitly write the
   program. in these cases, the programmers will split into two teams. the
   2.0 programmers manually curate, maintain, massage, clean and label
   datasets; each labeled example literally programs the final system
   because the dataset gets compiled into software 2.0 code via the
   optimization. meanwhile, the 1.0 programmers maintain the surrounding
   tools, analytics, visualizations, labeling interfaces, infrastructure,
   and the training code.

ongoing transition

   let   s briefly examine some concrete examples of this ongoing
   transition. in each of these areas we   ve seen improvements over the
   last few years when we give up on trying to address a complex problem
   by writing explicit code and instead transition the code into the 2.0
   stack.

   visual recognition used to consist of engineered features with a bit of
   machine learning sprinkled on top at the end (e.g., an id166). since
   then, we discovered much more powerful visual features by obtaining
   large datasets (e.g. id163) and searching in the space of
   convolutional neural network architectures. more recently, we don   t
   even trust ourselves to hand-code the architectures and we   ve begun
   [12]searching over those as well.

   id103 used to involve a lot of preprocessing, gaussian
   mixture models and id48, but [13]today consist almost
   entirely of neural net stuff. a very related, often cited humorous
   quote attributed to fred jelinek from 1985 reads    every time i fire a
   linguist, the performance of our id103 system goes up   .

   id133 has historically been approached with various
   stitching mechanisms, but today the state of the art models are large
   convnets (e.g. [14]wavenet) that produce raw audio signal outputs.

   machine translation has usually been approaches with phrase-based
   statistical techniques, but neural networks are quickly becoming
   dominant. my favorite architectures are trained in the [15]multilingual
   setting, where a single model translates from any source language to
   any target language, and in weakly supervised (or entirely
   [16]unsupervised) settings.

   games. explicitly hand-coded go playing programs have been developed
   for a long while, but [17]alphago zero (a convnet that looks at the raw
   state of the board and plays a move) has now become by far the
   strongest player of the game. i expect we   re going to see very similar
   results in other areas, e.g. [18]dota 2, or [19]starcraft.

   databases. more traditional systems outside of artificial intelligence
   are also seeing early hints of a transition. for instance,    [20]the
   case for learned index structures    replaces core components of a data
   management system with a neural network, outperforming cache-optimized
   b-trees by up to 70% in speed while saving an order-of-magnitude in
   memory.

   you   ll notice that many of my links above involve work done at google.
   this is because google is currently at the forefront of re-writing
   large chunks of itself into software 2.0 code.    [21]one model to rule
   them all    provides an early sketch of what this might look like, where
   the statistical strength of the individual domains is amalgamated into
   one consistent understanding of the world.

the benefits of software 2.0

   why should we prefer to port complex programs into software 2.0?
   clearly, one easy answer is that they work better in practice. however,
   there are a lot of other convenient reasons to prefer this stack. let   s
   take a look at some of the benefits of software 2.0 (think: a convnet)
   compared to software 1.0 (think: a production-level c++ code base).
   software 2.0 is:

   computationally homogeneous. a typical neural network is, to the first
   order, made up of a sandwich of only two operations: matrix
   multiplication and thresholding at zero (relu). compare that with the
   instruction set of classical software, which is significantly more
   heterogenous and complex. because you only have to provide software 1.0
   implementation for a small number of the core computational primitives
   (e.g. matrix multiply), it is much easier to make various
   correctness/performance guarantees.

   simple to bake into silicon. as a corollary, since the instruction set
   of a neural network is relatively small, it is significantly easier to
   implement these networks much closer to silicon, e.g. with custom
   [22]asics, [23]neuromorphic chips, and so on. the world will change
   when low-powered intelligence becomes pervasive around us. e.g., small,
   inexpensive chips could come with a pretrained convnet, a speech
   recognizer, and a wavenet id133 network all integrated in a
   small protobrain that you can attach to stuff.

   constant running time. every iteration of a typical neural net forward
   pass takes exactly the same amount of flops. there is zero variability
   based on the different execution paths your code could take through
   some sprawling c++ code base. of course, you could have dynamic compute
   graphs but the execution flow is normally still significantly
   constrained. this way we are also almost guaranteed to never find
   ourselves in unintended infinite loops.

   constant memory use. related to the above, there is no dynamically
   allocated memory anywhere so there is also little possibility of
   swapping to disk, or memory leaks that you have to hunt down in your
   code.

   it is highly portable. a sequence of matrix multiplies is significantly
   easier to run on arbitrary computational configurations compared to
   classical binaries or scripts.

   it is very agile. if you had a c++ code and someone wanted you to make
   it twice as fast (at cost of performance if needed), it would be highly
   non-trivial to tune the system for the new spec. however, in software
   2.0 we can take our network, remove half of the channels, retrain, and
   there         it runs exactly at twice the speed and works a bit worse. it   s
   magic. conversely, if you happen to get more data/compute, you can
   immediately make your program work better just by adding more channels
   and retraining.

   modules can meld into an optimal whole. our software is often
   decomposed into modules that communicate through public functions,
   apis, or endpoints. however, if two software 2.0 modules that were
   originally trained separately interact, we can easily backpropagate
   through the whole. think about how amazing it could be if your web
   browser could automatically re-design the low-level system instructions
   10 stacks down to achieve a higher efficiency in loading web pages.
   with 2.0, this is the default behavior.

   it is better than you. finally, and most importantly, a neural network
   is a better piece of code than anything you or i can come up with in a
   large fraction of valuable verticals, which currently at the very least
   involve anything to do with images/video and sound/speech.

the limitations of software 2.0

   the 2.0 stack also has some of its own disadvantages. at the end of the
   optimization we   re left with large networks that work well, but it   s
   very hard to tell how. across many applications areas, we   ll be left
   with a choice of using a 90% accurate model we understand, or 99%
   accurate model we don   t.

   the 2.0 stack can [24]fail in unintuitive and embarrassing ways ,or
   worse, they can    silently fail   , e.g., by silently adopting biases in
   their training data, which are very difficult to properly analyze and
   examine when their sizes are easily in the millions in most cases.

   finally, we   re still discovering some of the peculiar properties of
   this stack. for instance, the existence of [25]adversarial examples and
   [26]attacks highlights the unintuitive nature of this stack.

programming in the 2.0 stack

   software 1.0 is code we write. software 2.0 is code written by the
   optimization based on an evaluation criterion (such as    classify this
   training data correctly   ). it is likely that any setting where the
   program is not obvious but one can repeatedly evaluate the performance
   of it (e.g.         did you classify some images correctly? do you win games
   of go?) will be subject to this transition, because the optimization
   can find much better code than what a human can write.
   [1*7atcuemw8obriqkyobunva.png]

   the lens through which we view trends matters. if you recognize
   software 2.0 as a new and emerging programming paradigm instead of
   simply treating neural networks as a pretty good classifier in the
   class of machine learning techniques, the extrapolations become more
   obvious, and it   s clear that there is much more work to do.

   in particular, we   ve built up a vast amount of tooling that assists
   humans in writing 1.0 code, such as powerful ides with features like
   syntax highlighting, debuggers, profilers, go to def, git integration,
   etc. in the 2.0 stack, the programming is done by accumulating,
   massaging and cleaning datasets. for example, when the network fails in
   some hard or rare cases, we do not fix those predictions by writing
   code, but by including more labeled examples of those cases. who is
   going to develop the first software 2.0 ides, which help with all of
   the workflows in accumulating, visualizing, cleaning, labeling, and
   sourcing datasets? perhaps the ide bubbles up images that the network
   suspects are mislabeled based on the per-example loss, or assists in
   labeling by seeding labels with predictions, or suggests useful
   examples to label based on the uncertainty of the network   s
   predictions.

   similarly, github is a very successful home for software 1.0 code. is
   there space for a software 2.0 github? in this case repositories are
   datasets and commits are made up of additions and edits of the labels.

   in the short/medium term, software 2.0 will become increasingly
   prevalent in any domain where repeated evaluation is possible and
   cheap, and where the algorithm itself is difficult to design
   explicitly. and in the long run, the future of this paradigm is bright
   because it is increasingly clear to many that when we develop agi, it
   will certainly be written in software 2.0.

     * [27]machine learning
     * [28]artificial intelligence
     * [29]programming
     * [30]software development
     * [31]future

   (button)
   (button)
   (button) 43k claps
   (button) (button) (button) 153 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of andrej karpathy

[33]andrej karpathy

   director of ai at tesla. previously research scientist at openai and
   phd student at stanford. i like to train deep neural nets on large
   datasets.

     * (button)
       (button) 43k
     * (button)
     *
     *

   [34]go to the profile of andrej karpathy
   never miss a story from andrej karpathy, when you sign up for medium.
   [35]learn more
   never miss a story from andrej karpathy
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a64152b37c35
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/topic/programming
   7. https://medium.com/membership?source=upgrade_membership---nav_full
   8. https://medium.com/m/signin?redirect=https://medium.com/@karpathy/software-2-0-a64152b37c35&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/@karpathy/software-2-0-a64152b37c35&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@karpathy?source=post_header_lockup
  11. https://medium.com/@karpathy
  12. https://arxiv.org/abs/1703.01041
  13. https://github.com/syhw/wer_are_we
  14. https://deepmind.com/blog/wavenet-launches-google-assistant/
  15. https://arxiv.org/abs/1611.04558
  16. https://arxiv.org/abs/1710.11041
  17. https://deepmind.com/blog/alphago-zero-learning-scratch/
  18. https://blog.openai.com/more-on-dota-2/
  19. https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/
  20. https://arxiv.org/abs/1712.01208
  21. https://arxiv.org/abs/1706.05137
  22. https://www.forbes.com/sites/moorinsights/2017/08/04/will-asic-chips-become-the-next-big-thing-in-ai/#7d6d7c0511d9
  23. https://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-learningor-obscurity
  24. https://motherboard.vice.com/en_us/article/nz7798/weve-already-taught-artificial-intelligence-to-be-racist-sexist
  25. https://blog.openai.com/adversarial-example-research/
  26. https://github.com/yenchenlin/awesome-adversarial-machine-learning
  27. https://medium.com/tag/machine-learning?source=post
  28. https://medium.com/tag/artificial-intelligence?source=post
  29. https://medium.com/tag/programming?source=post
  30. https://medium.com/tag/software-development?source=post
  31. https://medium.com/tag/future?source=post
  32. https://medium.com/@karpathy?source=footer_card
  33. https://medium.com/@karpathy
  34. https://medium.com/@karpathy
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/a64152b37c35/share/twitter
  38. https://medium.com/p/a64152b37c35/share/facebook
  39. https://medium.com/p/a64152b37c35/share/twitter
  40. https://medium.com/p/a64152b37c35/share/facebook
