   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*isiuygbsyna9d_w3rwoigg.jpeg]

   cracking    morse code with id56s

   [16]go to the profile of sandeep bhupatiraju
   [17]sandeep bhupatiraju (button) blockedunblock (button)
   followfollowing
   mar 4, 2018

   spoiler alert: morse code doesn   t really need cracking. its useful
   because messages can be sent using this code with minimal equipment,
   and i say it doesn   t need cracking because the code is well known and
   what the combinations of dots and dashes stand for is no secret. but,
   in theory, it is a substitution cipher         where each letter of the
   alphabet (and each digit) has some representation using dots and
   dashes, as illustrated below.
   [1*k5akqbmrdkagwh2tl_e-dg.png]
   international morse code

   let   s suspend our skepticism and suppose we received messages in morse
   code, but we didn   t know how to read them. suppose also that we had a
   list of examples of some codes and their corresponding words. now, we
   could potentially guess that it is a substitution cipher and then
   eventually figure out the codes for each alphabet; thereby decoding the
   messages.

   or         we could construct an encoder-decoder model to guess (almost) all
   the words!! being masochists, we will of course choose the latter. with
   that said, let us spur rocinante and embark on our journey to fight the
   windmills.

   here is the problem at hand; we have a few examples of coded sequences
   and their intelligible counterparts. using these examples we must learn
   some patterns and use that information to predict what new coded tokens
   (words) could be. unlike the usual regression problems where we predict
   numerical outcomes, we have a sequence-to-sequence learning problem at
   hand, where there is temporal structure in the data. this is an instant
   cue that recurrent neural nets (id56s) could be helpful (the dictum
   being id56s for speech and language data, id98s for image data and
   combined id56s with id98s for image captioning). roughly speaking, this
   falls into a category of problems which also contains the problem of
   machine translation; the structure of that model serves as the
   inspiration here. for much more on this topic refer to [1]. we won   t
   spend time on the theory of id56s here but for a clear and concise
   introduction to this topic consult the series of posts in [2].

   for those of you wondering if this problem can be tackled differently;
   yes, a id115 would work to get similar results. in
   that case we would follow the procedure mentioned in the first example
   in the excellent paper [3].

general idea

   roughly speaking, we want to predict some output sequence (y_1,    , y_m)
   from an input sequence (x_1,    x_n) which involves learning the
   id155
   [1*mdyi2_u-0c8475_vi597mg.png]

   a major hurdle here is to predict variable sized output from variable
   sized inputs. at a meta level, this is overcome by combining two id56s
   the first of which maps variable sized inputs to fixed length output
   and the other which takes fixed length input and returns variable
   length outputs. the fixed length intermediary vector, called the
   context vector, encapsulates the information from the input sequence
   which is fed one character at a time. the mechanism by which the
   context vector is produced is what makes id56s useful for capturing the
   temporal structure         the context vector is either the hidden state of
   the id56 after the final timestep or some function of it. the above
   id155 is computed using the chain rule
   [1*ojolgutyc8vrdoif2pdghw.png]

   where h is the context vector. finally, a id155 on
   the right in the equation above can be computed using the softmax
   function which takes as input the one-hot encoded vectors of the
   characters y_{i-1},    , y_1, the output of the recurrent layer in the
   second id56 and the context vector. the particular type of id56 used here
   is the lstm which effectively overcomes the limitations of simple id56s
   which suffer from vanishing gradients problems and are better at
   capturing long range dependencies.

data preparation

   we will introduce a special character (*) to denote the space between
   the code for each letter. for instance the code for sos is going to be
   denoted by    . . . *                *. . .    (instead of    . . .                     . . .   ). we
   do this to ensure that a word corresponding to a given code is unique.
   next, instead of producing random collections of letters as words for
   our data, we will use english words from the data set (words_alpha)
   compiled [18]here. to get a sense of the data consider the histogram of
   word lengths given below. as is evident from the histogram there are
   many more long words (length greater than 5) than short ones.
   [1*57ti8km3fzzjv0p5lw9j9a.png]

   networks trained on the data containing long encoded words tends to
   predict long words on average. remember that the network will not
   figure out the    formula    producing the data, i.e., it will not learn
   the chart in figure 1.

   we begin the data preparation with the construction of a function which
   will take as input an english word and output its morse code.
import random
import numpy as np
import matplotlib.pyplot as plt

# construct the morse dictionary
alphabet = " ".join("abcdefghijklmnopqrstuvwxyz").split()
values = ['.-', '-...', '-.-.', '-..', '.', '..-.', '--.', '....', '..', '.---',
 '-.-', '.-..', '--', '-.','---', '.--.', '--.-',
.-.', '...', '-', '..-', '...-', '.--', '-..-', '-.--', '--..']
morse_dict = dict(zip(alphabet, values))
def morse_encode(word):
    return "*".join([dict_morse_encode[i]for i
                             in " ".join(word).split()]

   for the purpose of illustration, we will produce the training and
   validation data from words of a given fixed length. here we fix this
   length to be 9 as the number of words of length 9 is sufficiently large
   (refer the histogram above). note that this means that the output words
   from the network will be of a fixed length but the input morse codes
   will not all be of the same length. another liberty we take is that we
   assume we know that each alphabet is encoded by a string of length at
   most length four (we needn   t make this specific assumption, we could
   instead choose the length of the longest morse code in the training
   data as the max_length_x value to follow). so if the word if of length
   n then the morse code corresponding to it will be of length at most
   4n+(n-1), where the n-1 term corresponds to the number of *s. we pad
   the codes by white spaces on the left to make them all the same length,
   this implies that our input character vocabulary is {   .   ,                ,    *   ,    
      }, and for generality we let the character vocabulary for the output
   be all the alphabets and the special character of a white space.
   returning to the comment about networks guessing long words on average,
   we mean the network will tend to guess fewer white spaces because of
   the imbalance caused by the number of long words. in the code snippet
   below, the output_list will contain the english words and the
   input_list will contain the padded morse codes.
import random
word_len = 9
max_len_x = 4*word_len + (word_len-1)
max_len_y = len_word
def data_gen(n):

    with open('words_alpha.txt', 'r') as f:
        all_words = f.read().lower().split('\n')
        words = [word for word in all_words if len(word)==n]

        # shuffle the list since the words are ordered
        random.shuffle(words)

        g_out = lambda x: ' '*(max_len_y -len(x)) + x
        output_list = [g_out(word) for word in words]

        g_in = lambda x: morse_encode(x)+' '*(max_len_x
                                             - len(morse_encode(x)))
        input_list = [g_in(word) for word in words]

        return output_list, input_list
output_list, input_list = data_gen(9)

   now, we construct the one hot encoded vectors of the characters in the
   input to make the input data palatable for the neural network. for this
   purpose, we construct a class object (similar to the example in the
   keras documentation) which will be useful for encoding and decoding
   into arrays the morse codes as well as the english words. we assign the
   classes to objects with the appropriate character sets.
class chartable(object):
    def __init__(self, chars):
        self.chars = sorted(set(chars))
        self.char_indices = dict((c, i) for i, c in
                                            enumerate(self.chars))
        self.indices_char = dict((i, c) for i, c in
                                            enumerate(self.chars))
    def encode(self, token, num_rows):
        x = np.zeros((num_rows, len(self.chars)))
        for i, c in enumerate(token):
            x[i, self.char_indices[c]] = 1
        return x
    def decode(self, x, calc_argmax=true):
        if calc_argmax:
            x = x.argmax(axis=-1)
        return ''.join(self.indices_char[x] for x in x)
# we include the white space as a character in both cases below.
chars_in = '*-. '
chars_out = 'abcdefghijklmnopqrstuvwxyz '
ctable_in = chartable(chars_in)
ctable_out = chartable(chars_out)

   split the data to produce a training set x_train, y_train out of a
   quarter of the entire data set x, y and we will keep the remaining
   three fourths as the validation set x_val, y_val. note that we should
   ideally split a part of the training set as our validation set and the
   rest as our test set but given our toy set-up we are more interested in
   the model construction than parameter tuning. we now have our training
   and test (validation) data ready and can proceed to tinker with the
   network.
x = np.zeros((len(input_list), max_len_x, len(chars_in)))
y = np.zeros((len(output_list), max_len_y, len(chars_out)))
for i, token in enumerate(input_list):
    x[i] = ctable_in.encode(token, max_len_x)
for i, token in enumerate(output_list):
    y[i] = ctable_out.encode(token, max_len_y)


m = len(x)// 4
(x_train, x_val) = x[:m], x[m:]
(y_train, y_val) = y[:m], y[m:]

   the easiest way to construct a neural network is to work with the keras
   model and sequential apis. since we don   t need the full functionality
   and flexibility of tensorflow let   s stick to keras.

model construction (encoder-decoder model)

   the model topology we choose will incorporate a powerful variant of the
   simple id56, called the long short term memory (lstm) network.
   [1*uv8ehkdz7ec2ykpxat2avw.png]

   the first lstm will serve as the encoder, taking in a variable length
   input sequence, one character at a time, and converting it to an
   internal latent representation of a fixed length. another lstm will
   serve as a decoder taking the latent representation as input and will
   pass its output into a dense layer which uses a softmax function to
   make predictions one character at a time.

   the encoder and decoder components of the model could potentially have
   multiple layers of lstms, and its usually not clear a priori which
   topology would work best. for machine translation it is usually the
   case that deep networks work better. as a rule of thumb we expect
   stacked layers to be capable of learning higher level temporal
   representations, and as a result we use it when the data has some
   hierarchical structure. for us just one layer each would suffice.

   the model is constructed using sequential( ), and each layer is added
   one at a time. the first lstm layer takes as input a 3d tensors and
   requires the user to specify the input dimensions. this can be
   concisely done with input_shape as specified in the code, where the
   first component represents the number of time steps and the second is
   the number of features. for us, the number of features is the number of
   elements in our vocabulary of the input sequence, namely 4, since we
   have    .   ,                ,    *    and the white space character        . the number of
   time steps is max_len_x since we feed each of one-hot encoded vectors
   one at a time. we will also specify the number of memory cells (or
   blocks) in the layer (denoted here by the latent_dim parameter, for
   which we use 256), which is the dimension of the latent representation.
   note that we want to return the final hidden state of the lstm as the
   latent representation, this will have the information from all the time
   steps, i.e., the full input sequence. if we used the return_sequences =
   true option we would get a hidden state output for each time step, but
   that would contain information about the sequence only up to that step.
model = sequential()
model.add(layers.lstm(latent_dim, input_shape=(max_x_length,
                                                  len(chars_in))))

   this concludes out simple encoder model. next we construct a similar
   layer as our decoder. but, the output of the above code snippet will be
   a 2d array. we convert it to a 3d tensor by repeating this output
   max_len_y nunber of times using the convenient repeatvector layer and
   use that as input for our next lstm layer (decoder). now, we use the
   return_sequences=true option in this lstm to output the sequence of
   hidden states, and we need to use this information to make a
   predictions. for this purpose we use use a timedistributed dense layer
   which outputs a vector of length max_len_y, over which we use a softmax
   activation function to pick the most likely alphabets. for a quick
   introduction to the purpose of the time distributed layer refer to this
   [19]blog post.
model.add(layers.lstm(latent_dim, return_sequences=true))
model.add(layers.timedistributed(layers.dense(len(chars_out))))
model.add(layers.activation('softmax'))
model.compile(loss='categorical_crossid178', optimizer='adam',
                                           metrics=['accuracy'])
model.summary()

   here is a quick summary of the network and the dimensions of the
   various inputs and outputs.
   [1*u7ggqoy8v093-qj6nhkpiq.png]

   we fit the model to the data, training on the set x_train, y_train and
   using x_val and y_val to see how well we have done. the final set of
   parameters we need to set are the number of epochs and the batch size.
   the batch size is the size of the part of the training set passed
   through the network in the id119 algorithm after which an
   update is made to the weights in the network. usually the batch size is
   set to the largest value which your computers memory can handle. an
   epoch is a full run through the training data using these batches. here
   we set a batch size of 1024 and use 120 epochs, and it can be seen in
   the graph below that there is no appreciable gain in the accuracy after
   about 100 epochs. generally, it is a matter of trial and error to see
   which parameters work. we now fit the model using the fit( ) method.
epochs = 120
batch_size = 1024
hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=
                            epochs, validation_data=(x_val, y_val))
plt.figure(figsize=(20,5))
plt.subplot(121)
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.subplot(122)
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()

   [1*ea9s7_stgmhbqe4af9u5jw.png]

   finally, as can be seen from the graphs above, we can get an accuracy
   of about 93% on the validation set, which is not bad. of course if we
   increased the size of the training data we could do much better. here
   are a few predictions for a randomly chosen set of words.
   [1*gupdpo6x0kla-ewcdhibgg.png]
   input codes on the left, the corresponding words in the middle and the
   predictions on the right. the word is green if predicted correctly and
   red if not.

   as you can see the wrong predictions are not too bad either. we must
   remind ourselves that the codes have not been cracked by cracking the
   cipher, i.e., by figuring out what each letter stands for. in fact, we
   could input the codes for the alphabets and see what the network
   predicts for the codes of the single letters, and as you can see below,
   we are way off the mark!
   [1*u2jralaaaiexfnxy-zsm5q.png]

   as an another example of the encoder-decoder model, you could try and
   work with the [20]caesar cipher or another code to see how effective
   this method could be.

   references:

   [1]
   [21]http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with
   -neural-networks.pdf

   [2] [22]http://colah.github.io/posts/2015-08-understanding-lstms/

   [3]
   [23]http://www.ams.org/journals/bull/2009-46-02/s0273-0979-08-01238-x/s
   0273-0979-08-01238-x.pdf

     * [24]machine learning
     * [25]id56
     * [26]ai
     * [27]data science

   (button)
   (button)
   (button) 387 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of sandeep bhupatiraju

[29]sandeep bhupatiraju

   mathematician and data scientist

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 387
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e5883355a6f3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/cracking-morse-code-with-id56s-e5883355a6f3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/cracking-morse-code-with-id56s-e5883355a6f3&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_j5htumgxphpk---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@sandeepbhupatiraju?source=post_header_lockup
  17. https://towardsdatascience.com/@sandeepbhupatiraju
  18. https://github.com/dwyl/english-words/blob/master/words_alpha.txt
  19. https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/
  20. https://en.wikipedia.org/wiki/caesar_cipher
  21. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
  22. http://colah.github.io/posts/2015-08-understanding-lstms/
  23. http://www.ams.org/journals/bull/2009-46-02/s0273-0979-08-01238-x/s0273-0979-08-01238-x.pdf
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/id56?source=post
  26. https://towardsdatascience.com/tagged/ai?source=post
  27. https://towardsdatascience.com/tagged/data-science?source=post
  28. https://towardsdatascience.com/@sandeepbhupatiraju?source=footer_card
  29. https://towardsdatascience.com/@sandeepbhupatiraju
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/e5883355a6f3/share/twitter
  36. https://medium.com/p/e5883355a6f3/share/facebook
  37. https://medium.com/p/e5883355a6f3/share/twitter
  38. https://medium.com/p/e5883355a6f3/share/facebook
