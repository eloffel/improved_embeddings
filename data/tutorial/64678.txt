   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

what is wrong with convolutional neural networks ?

   [16]go to the profile of mahmoud tarrasse
   [17]mahmoud tarrasse (button) blockedunblock (button) followfollowing
   jan 17, 2018

introduction

   of course convolutional neural networks (id98s) are fascinating and
   strong tool, maybe it   s one of the reasons deep learning is so popular
   these days, since alex krizhevsky, ilya sutskever, and geoffrey hinton
   published    id163 classification with deep convolutional networks    in
   2012, id98 has been the winning card in id161 achieving
   superhuman performance in many tasks, but are id98   s flawless? is that
   the best that we can do? i guess from the title you figured that the
   answer is no.

   december 4, 2014, geoffrey hinton gave a speech in mit about a project
   of his called id22, and he discussed the problems with
   id98   s and why pooling is very bad and the fact that it   s working so
   well is a disaster

     if you are familiar with id98   s you can skip to what   s wrong?

convolutional layers

   a convolutional layer have a set of matrices that get multiplied by the
   previous layer output in a process called the convolution to detect
   some features this features could be basic features (e.g. edge, color
   grade or pattern) or complex one (e.g. shape, nose, or a mouth) so,
   those matrices are called filters or kernels
   [1*gci7g-jlaqieocon7xfbhg.gif]
   [18](source)

pooling layers

   there is more than one type of pooling layer (max pooling, avg
   pooling    ), the most common -this days- is max pooling because it gives
   transational variance         poor but good enough for some tasks         and it
   reduces the dimensionality of the network so cheaply (with no
   parameters)
   max pooling layers is actually very simple, you predefine a filter (a
   window) and swap this window across the input taking the max of the
   values contained in the window to be the output
   [1*gksqn5xy8hppiddm5wzm7a.jpeg]
   max pooling with filter size 2*2 [19](source)
   [1*lbutgianqloo1gfsc9phtg.gif]
   [20](source)

what is wrong?

1- id26

   backprob is a method to find the contribution of every weight in the
   error after a batch of data is prepossessed and most of good
   optimization algorithms (sgd, adam     ) uses id26 to find the
   gradients

   id26 has been doing so good in the last years but is not an
   efficient way of learning, because it needs huge dataset
   i believe that we can do better

2- translation invariance

   when we say translational invariance we mean that the same object with
   slightly change of orientation or position might not fire up the neuron
   that is supposed to recognize that object
   [1*6kn_h3jql62i9d8vz33tyq.png]
   [21](source)

   as in the image above if we assumed that there is a neuron that is
   supposed to detect cats it   s value will change with the change of the
   position and rotation of the cat, data augmentation partially solves
   the problem but it does not get rid of it totally

3- pooling layers

   pooling layers is a big mistake because it loses a lot of valuable
   information and it ignores the relation between the part and the whole
   if we are talking about a face detector so we have to combine some
   features (mouth, 2 eyes, face oval and a nose) to say that is a face
   id98 would say if those 5 features present with high id203 this
   would be a face
   [1*wsf4tsoh77t1lpylpuihba.png]
   [22](source)

   so the output of the two images might be similar which is not good

conclusion

   id98   s are awesome but it have 2 very dangerous flaws translation
   invariance and pooling layers, luckily we can reduce the danger with
   data augmentation but something is coming up (id22) we have
   to be ready and open to the change

sources

   [23]cs231n convolutional neural networks for visual recognition
   course materials and notes for stanford class cs231n: convolutional
   neural networks for visual recognition.cs231n.github.io
   [24]understanding hinton   s id22. part i: intuition.
   part of understanding hinton   s id22 series:medium.com
   [25]convolutional neural networks
   this is a preliminary version of the lab series of deep learning for
   healthcare in cse6250 big data analytics for   www.cc.gatech.edu

   iframe: [26]/media/d224dda902be929a78ee2c65c1c3c1c1?postid=75c2ba8fbd6f

     * [27]machine learning
     * [28]convolutional
     * [29]deep learning
     * [30]geoffrey hinton
     * [31]neural networks

   (button)
   (button)
   (button) 500 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of mahmoud tarrasse

[33]mahmoud tarrasse

     (button) follow
   [34]towards data science

[35]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 500
     * (button)
     *
     *

   [36]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [37]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/75c2ba8fbd6f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-is-wrong-with-convolutional-neural-networks-75c2ba8fbd6f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-is-wrong-with-convolutional-neural-networks-75c2ba8fbd6f&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_6juiiwbugcbl---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@mahmoudtarrasse?source=post_header_lockup
  17. https://towardsdatascience.com/@mahmoudtarrasse
  18. https://www.cc.gatech.edu/~san37/post/dlhc-id98/
  19. http://cs231n.github.io/convolutional-networks/
  20. https://www.cc.gatech.edu/~san37/post/dlhc-id98/
  21. https://www.cc.gatech.edu/~san37/post/dlhc-id98/
  22. https://medium.com/ai  -theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b
  23. http://cs231n.github.io/
  24. https://medium.com/ai  -theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b
  25. https://www.cc.gatech.edu/~san37/post/dlhc-id98/
  26. https://towardsdatascience.com/media/d224dda902be929a78ee2c65c1c3c1c1?postid=75c2ba8fbd6f
  27. https://towardsdatascience.com/tagged/machine-learning?source=post
  28. https://towardsdatascience.com/tagged/convolutional?source=post
  29. https://towardsdatascience.com/tagged/deep-learning?source=post
  30. https://towardsdatascience.com/tagged/geoffrey-hinton?source=post
  31. https://towardsdatascience.com/tagged/neural-networks?source=post
  32. https://towardsdatascience.com/@mahmoudtarrasse?source=footer_card
  33. https://towardsdatascience.com/@mahmoudtarrasse
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. http://cs231n.github.io/
  40. https://medium.com/ai%c2%b3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b
  41. https://www.cc.gatech.edu/~san37/post/dlhc-id98/
  42. https://medium.com/p/75c2ba8fbd6f/share/twitter
  43. https://medium.com/p/75c2ba8fbd6f/share/facebook
  44. https://medium.com/p/75c2ba8fbd6f/share/twitter
  45. https://medium.com/p/75c2ba8fbd6f/share/facebook
