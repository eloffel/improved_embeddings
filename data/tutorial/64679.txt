   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]yellowblog
     * [9]about us
     * [10]contact
     * [11]website
     __________________________________________________________________

unsupervised sentence representation with deep learning

   [12]go to the profile of yonatan hadar
   [13]yonatan hadar (button) blockedunblock (button) followfollowing
   apr 8, 2018

   in recent years, the field of natural-language processing (nlp) has
   gained a major boost in performance thanks to the technique of
   representing words as a continuous vector instead of a sparse one-hot
   encoded vector (id97).
   [1*sxnxyfaqflueidxpco130w.png]
   id97 examples

   while id97 works very well and creates nice semantics like
   king         man + woman = queen, sometimes we are not interested in the
   representation of a word but in the representation of a sentence.

   in this post, we will review and share code examples for several
   unsupervised, deep-learning methods of sentence representation. we will
   present their effectiveness as a pre-processing step for a specific
   text classification task.
     __________________________________________________________________

the classification task

   the data that we used to present the different sentence representation
   methods is based on 10,000 news articles that we scraped from the world
   wide web. the task is to classify each article into one of 10 possible
   topics (the data has topic labels, and is therefore a supervised task).
   for the sake of the presentation, i will use a id28
   model, each time using a different pre-processing representation method
   that we will apply to the article headline.
     __________________________________________________________________

baseline model         average id97

   we will start with a very simple baseline. we will represent the
   headline by averaging the headline words in their id97
   representation. as previously mentioned, id97 is a machine-learning
   method for representing words as vectors. the id97 model is trained
   by predicting words close to the target word with a shallow neural
   network. you can read more about how the algorithm works [14]here.

   we can train our own id97 with [15]gensim but in this example we
   will use a google pre-trained id97 model that was built based on
   google news data. after representing each word as a vector, we will
   represent a sentence (the headline) as an average of its words
   (vectors) and run id28 for classification of the
   article   s category.
#load data and id97 model
df = pd.read_csv("news_dataset.csv")
data = df[['body','headline','category']]
w2v = gensim.models.keyedvectors.load_id97_format('/googlenews-vectors-negat
ive300.bin', binary=true)
#build x and y
x = np.random.rand(len(data),300)
for i in range(len(data)):
    k = 0
    non = 0
    values = np.zeros(300)
    for j in data['headline'].iloc[i].split(' '):
        if j in w2v:
            values+= w2v[j]
            k+=1
    if k > 0:
        x[i,:]=values/k
    else: non+=1
y = labelencoder().fit_transform(data['category'].values)
msk = np.random.rand(len(data)) < 0.8
x_train,y_train,x_test,y_test = x[msk],y[msk],x[~msk],y[~msk]
#train the model
lr = logisticregression().fit(x_train,y_train)
lr.score(x_test,y_test)

   our baseline average id97 model achieved an accuracy score of 68%.
   this is good, but let   s see if we can do better.

   the average id97 method has two main weaknesses: it is a
   bag-of-words model that doesn   t relate to the word order, and all the
   words are treated with the same weight. we will try to address these
   issues in the next methods by leveraging id56 architectures for sentence
   representation.

autoencoder

   an autoencoder is an unsupervised deep learning model that attempts to
   copy its input to its output. the trick of autoencoders is that the
   dimension of the middle-hidden layer is lower than that of the input
   data. thus, the neural network must represent the input in a smart and
   compact way in order to reconstruct it successfully. in many cases,
   using the autoencoder for feature extraction has proved to be very
   effective.
   [1*hibrgsv2epftcsof_lgpcq.png]

   our autoencoder is a simple sequnce2sequence architecture built from an
   input layer followed by an embedding layer, an lstm layer, and a
   softmax layer. both the input and the output of the entire architecture
   are the headline, and we will use the output of the lstm to represent
   the headline. after getting the representation from the encoder, we
   will use id28 to predict categories. in order to gain
   more data, we will train the autoencoder with all the sentences in the
   articles and not just the headline.
#parse all sentences
sentenses = []
for i in data['body'].values:
    for j in nltk.sent_tokenize(i):
        sentenses.append(j)
#preprocess for keras
num_words=2000
maxlen=20
tokenizer = tokenizer(num_words = num_words, split=' ')
tokenizer.fit_on_texts(sentenses)
seqs = tokenizer.texts_to_sequences(sentenses)
pad_seqs = []
for i in seqs:
    if len(i)>4:
        pad_seqs.append(i)
pad_seqs = pad_sequences(pad_seqs,maxlen)
#the model
embed_dim = 150
latent_dim = 128
batch_size = 64
#### encoder model ####
encoder_inputs = input(shape=(maxlen,), name='encoder-input')
emb_layer = embedding(num_words, embed_dim,input_length = maxlen, name='body-wor
d-embedding', mask_zero=false)
# word embeding for encoder (ex: issue body)
x = emb_layer(encoder_inputs)
state_h = gru(latent_dim, name='encoder-last-gru')(x)
encoder_model = model(inputs=encoder_inputs, outputs=state_h, name='encoder-mode
l')
id195_encoder_out = encoder_model(encoder_inputs)
#### decoder model ####
decoded = repeatvector(maxlen)(id195_encoder_out)
decoder_gru = gru(latent_dim, return_sequences=true, name='decoder-gru-before')
decoder_gru_output = decoder_gru(decoded)
decoder_dense = dense(num_words, activation='softmax', name='final-output-dense-
before')
decoder_outputs = decoder_dense(decoder_gru_output)
#### id195 model ####
#id195_decoder_out = decoder_model([decoder_inputs, id195_encoder_out])
id195_model = model(encoder_inputs,decoder_outputs )
id195_model.compile(optimizer=optimizers.nadam(lr=0.001), loss='sparse_categor
ical_crossid178')
history = id195_model.fit(pad_seqs, np.expand_dims(pad_seqs, -1),
          batch_size=batch_size,
          epochs=5,
          validation_split=0.12)
#feature extraction
headlines = tokenizer.texts_to_sequences(data['headline'].values)
headlines = pad_sequences(headlines,maxlen=maxlen)x = encoder_model.predict(head
lines)
#classifier
x_train,y_train,x_test,y_test = x[msk],y[msk],x[~msk],y[~msk]
lr = logisticregression().fit(x_train,y_train)
lr.score(x_test,y_test)

   we achieved an accuracy score of 60%, which is actually worse than our
   baseline. we can probably improve this score by optimizing the
   hyperparameters, increasing the number of training epochs, or training
   the model on more data.

language model

   our second method is training a language model to represent our
   sentences. a language model describes the id203 of a text
   existing in a language. for example, the sentence    i like eating
   bananas    would be more probable than    i like eating convolutions.    we
   train a language model by slicing windows of n words and predicting
   what the next word will be in the text. you can read more about
   id38 with id56 [16]here. by building a language model, we
   gain an understanding of how    journalistic english    is built, and the
   model should be capable of focusing on important words in its
   representation.
   [1*b5rfwo2gidaj7jj9xf8o4g.png]

   our architecture is similar to the autoencoder architecture, but
   instead of predicting a sequence of words, we will only be predicting
   one word. the input will contain windows of 20 words from the articles,
   and the label will be the 21st word. after training the language model,
   we will take the headline representation from the lstm output hidden
   state and run id28 to predict categories.
#building x and y
num_words=2000
maxlen=20
tokenizer = tokenizer(num_words = num_words, split=' ')
tokenizer.fit_on_texts(df['body'].values)
seqs = tokenizer.texts_to_sequences(df['body'].values)
seq = []
for i in seqs:
    seq+=i

x = []
y = []
for i in tqdm(range(len(seq)-maxlen-1)):
    x.append(seq[i:i+maxlen])
    y.append(seq[i+maxlen+1])
x = pd.dataframe(x)
y = pd.dataframe(y)
y[0]=y[0].astype('category')
y =pd.get_dummies(y)
#buidling the network
embed_dim = 150
lstm_out = 128
batch_size= 128
model = sequential()
model.add(embedding(num_words, embed_dim,input_length = maxlen))
model.add(bidirectional(lstm(lstm_out)))
model.add(dense(y.shape[1],activation='softmax'))
adam = adam(lr=0.001, beta_1=0.7, beta_2=0.99, epsilon=none, decay=0.0, amsgrad=
false)
model.compile(loss = 'categorical_crossid178', optimizer=adam)
model.summary()
print('fit')
model.fit(x, y, batch_size =batch_size,validation_split=0.1, epochs = 5,  verbos
e = 1)
#feature extraction
headlines = tokenizer.texts_to_sequences(data['headline'].values)
headlines = pad_sequences(headlines,maxlen=maxlen)
inp = model.input
outputs = [model.layers[1].output]
functor = k.function([inp]+ [k.learning_phase()], outputs )
x = functor([headlines, 1.])[0]
#classifier
x_train,y_train,x_test,y_test = x[msk],y[msk],x[~msk],y[~msk]
lr = logisticregression().fit(x_train,y_train)
lr.score(x_test,y_test)

   we achieved an accuracy score of 72%. this is better than our baseline,
   but let   s see if we can make it better still.

skip-thought vectors

   in the skip-thought [17]paper from 2015, the authors took the same
   intuition from the language model. however, in skip thought, instead of
   predicting the next word, we are predicting the previous and the next
   sentence. this gives the model more context for the sentence, thus we
   can build better representations of sentences. for more information
   about the model, check out [18]this great blog post.
   [1*zlpr8o3ft0nravbkx0op5a.png]
   example from the skip thought [19]paper

   we will build a id195 architecture similar to the autoencoder
   architecture, but with two main differences. first, we will have two
   output layers with lstms         one for the previous sentence and one for
   the next sentence. second, we will use teacher forcing in the output
   lstms. this means that instead of giving the output lstm the previous
   hidden state only, we will also give it the real previous word (you can
   see an illustration of the input in the picture above, in the bottom
   line of the outputs).
#build x and y
num_words=2000
maxlen=20
tokenizer = tokenizer(num_words = num_words, split=' ')
tokenizer.fit_on_texts(sentenses)
seqs = tokenizer.texts_to_sequences(sentenses)
pad_seqs = pad_sequences(seqs,maxlen)
x_skip = []
y_before = []
y_after = []
for i in tqdm(range(1,len(seqs)-1)):
    if len(seqs[i])>4:
        x_skip.append(pad_seqs[i].tolist())
        y_before.append(pad_seqs[i-1].tolist())
        y_after.append(pad_seqs[i+1].tolist())
x_before = np.matrix([[0]+i[:-1] for i in y_before])
x_after =np.matrix([[0]+i[:-1] for i in y_after])
x_skip = np.matrix(x_skip)
y_before = np.matrix(y_before)
y_after = np.matrix(y_after)
#building the model
embed_dim = 150
latent_dim = 128
batch_size = 64
#### encoder model ####
encoder_inputs = input(shape=(maxlen,), name='encoder-input')
emb_layer = embedding(num_words, embed_dim,input_length = maxlen, name='body-wor
d-embedding', mask_zero=false)
x = emb_layer(encoder_inputs)
_, state_h = gru(latent_dim, return_state=true, name='encoder-last-gru')(x)
encoder_model = model(inputs=encoder_inputs, outputs=state_h, name='encoder-mode
l')
id195_encoder_out = encoder_model(encoder_inputs)
#### decoder model ####
decoder_inputs_before = input(shape=(none,), name='decoder-input-before')  # for
 teacher forcing
dec_emb_before = emb_layer(decoder_inputs_before)
decoder_gru_before = gru(latent_dim, return_state=true, return_sequences=true, n
ame='decoder-gru-before')
decoder_gru_output_before, _ = decoder_gru_before(dec_emb_before, initial_state=
id195_encoder_out)
decoder_dense_before = dense(num_words, activation='softmax', name='final-output
-dense-before')
decoder_outputs_before = decoder_dense_before(decoder_gru_output_before)
decoder_inputs_after = input(shape=(none,), name='decoder-input-after')  # for t
eacher forcing
dec_emb_after = emb_layer(decoder_inputs_after)
decoder_gru_after = gru(latent_dim, return_state=true, return_sequences=true, na
me='decoder-gru-after')
decoder_gru_output_after, _ = decoder_gru_after(dec_emb_after, initial_state=seq
2seq_encoder_out)
decoder_dense_after = dense(num_words, activation='softmax', name='final-output-
dense-after')
decoder_outputs_after = decoder_dense_after(decoder_gru_output_after)
#### id195 model ####
id195_model = model([encoder_inputs, decoder_inputs_before,decoder_inputs_afte
r], [decoder_outputs_before,decoder_outputs_after])
id195_model.compile(optimizer=optimizers.nadam(lr=0.001), loss='sparse_categor
ical_crossid178')
id195_model.summary()
history = id195_model.fit([x_skip,x_before, x_after], [np.expand_dims(y_before
, -1),np.expand_dims(y_after, -1)],
          batch_size=batch_size,
          epochs=10,
          validation_split=0.12)
#feature extraction
headlines = tokenizer.texts_to_sequences(data['headline'].values)
headlines = pad_sequences(headlines,maxlen=maxlen)x = encoder_model.predict(head
lines)
#classifier
x_train,y_train,x_test,y_test = x[msk],y[msk],x[~msk],y[~msk]
lr = logisticregression().fit(x_train,y_train)
lr.score(x_test,y_test)

   we achieved an accuracy score of 74%. this is the best score we   ve
   achieved so far!

conclusion

   in this post, we reviewed three unsupervised methods for creating
   vector representations of sentences with id56s and presented their
   effectiveness in solving a supervised task. the results of the
   autoencoder method were worse than the results for the baseline model
   (possibly because of the relatively small dataset used). the language
   and skip-thought vector models, both of which used context for
   predicting words or sentences, obtained the best result.

   the many options available for improving the methods we   ve presented
   include tuning the hyperparameters, training using more epochs, using a
   pre-trained embedding matrix, changing the neural network architecture,
   and more. in theory, such advanced tuning work might change some of the
   results, but i believe the basic intuition of each pre-processing
   method can be achieved with the examples i   ve shared above.

   hope you enjoyed my post, and you   re more than welcomed to read and
   follow our blog at [20]yellowblog.

     * [21]machine learning
     * [22]deep learning
     * [23]nlp
     * [24]artificial intelligence
     * [25]python

   (button)
   (button)
   (button) 330 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of yonatan hadar

[27]yonatan hadar

     (button) follow
   [28]yellowblog

[29]yellowblog

   blog written by yellowroad         when machine learning and big data meet
   business

     * (button)
       (button) 330
     * (button)
     *
     *

   [30]yellowblog
   never miss a story from yellowblog, when you sign up for medium.
   [31]learn more
   never miss a story from yellowblog
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.myyellowroad.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/104b90079a93
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.myyellowroad.com/unsupervised-sentence-representation-with-deep-learning-104b90079a93&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.myyellowroad.com/unsupervised-sentence-representation-with-deep-learning-104b90079a93&source=--------------------------nav_reg&operation=register
   8. https://blog.myyellowroad.com/?source=logo-lo_fsaccxsmojxr---4981ab970712
   9. https://blog.myyellowroad.com/about
  10. https://blog.myyellowroad.com/whats-your-story-ddd43c7095fb
  11. http://www.myyellowroad.com/
  12. https://blog.myyellowroad.com/@yonatan.hadar?source=post_header_lockup
  13. https://blog.myyellowroad.com/@yonatan.hadar
  14. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  15. https://radimrehurek.com/gensim/
  16. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  17. https://arxiv.org/abs/1506.06726
  18. https://medium.com/@sanyamagarwal/my-thoughts-on-skip-thoughts-a3e773605efa
  19. https://arxiv.org/abs/1506.06726
  20. https://blog.myyellowroad.com/
  21. https://blog.myyellowroad.com/tagged/machine-learning?source=post
  22. https://blog.myyellowroad.com/tagged/deep-learning?source=post
  23. https://blog.myyellowroad.com/tagged/nlp?source=post
  24. https://blog.myyellowroad.com/tagged/artificial-intelligence?source=post
  25. https://blog.myyellowroad.com/tagged/python?source=post
  26. https://blog.myyellowroad.com/@yonatan.hadar?source=footer_card
  27. https://blog.myyellowroad.com/@yonatan.hadar
  28. https://blog.myyellowroad.com/?source=footer_card
  29. https://blog.myyellowroad.com/?source=footer_card
  30. https://blog.myyellowroad.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/104b90079a93/share/twitter
  34. https://medium.com/p/104b90079a93/share/facebook
  35. https://medium.com/p/104b90079a93/share/twitter
  36. https://medium.com/p/104b90079a93/share/facebook
