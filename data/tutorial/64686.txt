   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

only numpy: understanding back propagation for transpose convolution in multi
layer id98 with example and interactive code.

   [16]go to the profile of jae duk seo
   [17]jae duk seo (button) blockedunblock (button) followfollowing
   jan 29, 2018
   [1*ds548dyicdhduoxlgkz5dg.gif]
   gif from this [18]website

   so for the past two days i was having a hard time understanding
   transpose convolution operation. but i finally got it, and today, we
   are going to train a simple id98 that have two convolution layer, as
   shown below.
   [1*pdllwvjyz2ptoxkun-mfwq.jpeg]
   very simple id98 arch.
     __________________________________________________________________

   funny story of where i got the answer.

   yesterday, i was invited to a dinner for a gathering. the reason was
   one of very knowledgeable master student finished her defense
   successfully, so we were celebrating. however, for the past two days i
   wasn   t able to fully understand the whole back propagation process of
   id98.
   as soon as i tried to perform back propagation after the most outer
   layer of convolution layer i hit a wall.

   but at that dinner, it finally hit me.
   [1*crv_mjhw8whnn_hk5hpqya.jpeg]

   looking at the corns on my plate, i realize that all this time, i was
   trying to understand the back propagation process in id98 as
   deconvolution. this was my original thinking.

   let red box be 2*2 output image
   let green box be 3*3 kernel
   let blue box be 4*4 input image

      since we get a 2*2 output image after performing convolution on 4 * 4
   image, then, while performing back propagation we need to do perform
   some operation on 2*2 output image to get some image that have 4*4
   dimension.   

   but the corns (lol) made me realize that our goal isn   t to restore the
   original image. rather we are trying to get the error rate respect to
   the every weight in the network. and in the case of multi layer id98, we
   need to back propagate that error. so haha, that   s where i got my
   solution, and let me    try    to explain what i mean by a concrete example
   and code.
     __________________________________________________________________

   network architecture
   [1*f0dudrwuv9t3pxzuk41hkw.jpeg]

   as seen above, the network arch is very simple, just two layer of
   convolution and one layer of fully connected layer. please take note,
   while performing convolution we need to transpose (rotate) the kernel
   by 180 degrees, so take note of the green boxes in above photo.

   also, please take note that i didn   t draw activation layer for
   simplicity. but in the interactive code, i used either tanh() or
   archtan().
   [1*tbflnbdhsh9hjczgq5kd2w.png]
   image [19]from techieme

   if you are not sure about matrix rotation, [20]please read this
   article.
     __________________________________________________________________

   forward feed operation
   [1*oqnoh2txdpds875nt6msxg.jpeg]

     ** update ** i made a mistake on the column, the two column pointed
     by the green arrows must be switched. thank you abraham kan for
     pointing out.

   so as seen above, convolution operations can be written in one line.
   and for reasons that i will explain later, please take a careful note
   of red boxed variables, they are basically input of next layers. but
   this information becomes crucial, while performing back propagation.
     __________________________________________________________________

   back propagation respect to green weight
   [1*uajff5zd-eniirqbbi8mpq.jpeg]

   the yellow box means learning rate, also, the whole back propagation is
   a standard process. and i put down the gradient update equation as
   well. finally, please take note of the symbol    k    in the red box, i
   will use this symbol over and over to represent (out         y).
     __________________________________________________________________

   back propagation respect to red weight
   [1*dtj3owhcnk9grrgktldgyw.jpeg]

   red box     (out         y)
   yellow box     learning rate
   black box     rotating the kernel 180 degrees (or transpose) before
   convolution operation         remember in convolution operation we rotate the
   kernel.

   everything is very simple and straight forward except for the purple
   boxes, what is that doing?

   purple box     rotating the matrix to fit calculate the respect
   derivative to each weight.

   the question now arises, why? why do we do this?

   remember when i told you guys to pay attention to the input of the each
   layer? well lets go back one more time.
   [1*tkf4dkjvqrgp5t_tygoaiw.jpeg]

   please take a close look at the colored boxes.

   orange box     inputs that is being multiplied by red w(2,2)
   light green box     inputs that is being multiplied by red w(2,1)
   blue box     inputs that is being multiplied by red w(1,2)
   pink box     inputs that is being multiplied by red w(1,1)

   great that was easy, but what does that have to do with transposing the
   kernel? well, let   s do this, since (please look at the black boxed
   equation) out can be written in one line, lets take the derivative
   respect toe red weights as shown below.
   [1*xqkv7g_fzrnzp4vmepxl8q.jpeg]

     ** update ** april 8         thank you [21]dae woo and abraham kang for
     pointing out the error, there was a small error in the coordinates.

   dark green boxed numbers     sorry i didn   t have a green colored pen but
   they represent the green weights.

   as seen, when taking the derivative respect to each red weights, we can
   see that the xx coordinates differs from one input to another. we need
   to match those coordinates respect to each weights, that is the reason
   why we rotate (transpose) the matrix by 180 degrees.
     __________________________________________________________________

   back propagation respect to blue weight part 1
   [1*ed5yulurdbya6mpg5zqu5q.jpeg]

   blue box     calculated convolution between (k * green weight) and
   (padded red weight)
   orange box     again rotating the matrix to get the derivative respect to
   each weight.
   black box     same story, rotating the kernel before convolution
   operation.

   now, the question arises, why the padding (purple box)? why do we need
   to pad the red weight?

   good question, and i   ll explain in a bit, but for now please just read
   along.
     __________________________________________________________________

   back propagation respect to blue weight part 2
   [1*d6mrsus7mhbl9ejde00azq.png]

   blue box     calculated matrix in part 1.
   black box     transposing the kernel before convolution operation.
   orange, light green, blue, pink box     calculation for each derivative
   respect to each blue weights.
   [1*xeanqi9ufuivxsoepxc45a.jpeg]
   close look of the rotated kernel

   above, is a closer look on the rotated kernel, while performing
   convolution operation. but now let   s take a look at the inputs again.
   [1*fciwdsoevzzdwux7_p6owa.jpeg]

   once more, since out can be written in one line, lets take the
   derivative respect to blue weights as shown below.
   [1*u38xwuuntuygvxfjr7vebw.jpeg]

   green box number     again, sorry did not have a green pen
   orange box     calculated gradient respect to blue weight(2,2)
   pink box     calculated gradient respect to blue weight (1,1)

   so, again, we rotated (or transposed) the matrix to match the gradient
   for each weight.

   also, now the reason why we padded the red weight is clear, it was to
   get the gradient for each weights, i   ll show you guys once again what i
   mean by padded red weights. (please look at the purple star)
   [1*io68rre3vn6afbroqrut_g.jpeg]
     __________________________________________________________________

   interactive code id180
   [1*fckgvfwn0_sdz9zy8czl7q.png]

   green box     derivative of id180, since they have the
   same dimensional we can just perform element wise multiplication

   red box     rotating the kernel to match the gradient

   blue box     padding the red weight (named w2) with zeros
     __________________________________________________________________

   interactive code
   [1*oe84uzaofj_i-dpbgxafjq.png]

   please[22] click here to access the interactive code.
     __________________________________________________________________

final words

   it was very satisfying to understand back propagation in id98, i was
   struggling to understand this concept for two days. and i could not
   have done it without being invited to [23]@sarah__asiri graduation
   dinner, thank you sarah!

   if any errors are found, please email me at jae.duk.seo@gmail.com.

   meanwhile follow me on my twitter [24]here, and visit [25]my website,
   or my [26]youtube channel for more content. i also did comparison of
   decoupled neural network [27]here if you are interested.
     __________________________________________________________________

   reference
    1. d. (2015, october 25). matrix rotation and matrix transpose.
       retrieved january 28, 2018, from
       [28]http://techieme.in/matrix-rotation/
    2. what are deconvolutional layers? (n.d.). retrieved january 28,
       2018, from
       [29]https://datascience.stackexchange.com/questions/6107/what-are-d
       econvolutional-layers
    3. [30]http://courses.cs.tau.ac.il/caffe_workshop/bootcamp/pdf_lecture
       s/lecture%203%20id98%20-%20id26.pdf
    4. dumoulin, v., & visin, f. (2016). a guide to convolution arithmetic
       for deep learning. arxiv preprint arxiv:1603.07285.
    5. [31]https://www.cc.gatech.edu/classes/ay2018/cs7643_fall/slides/l7_
       id98s_annotated.pdf

   [1*6gfnvvkmrftjvswf7vkcla.png]

this story is published in [32]the startup, medium   s largest entrepreneurship
publication followed by 295,232+ people.

subscribe to receive [33]our top stories here.

   [1*6gfnvvkmrftjvswf7vkcla.png]

     * [34]machine learning
     * [35]convolutional network
     * [36]neural networks
     * [37]artificial intelligence
     * [38]id26

   (button)
   (button)
   (button) 340 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of jae duk seo

[40]jae duk seo

   [41]https://jaedukseo.me | | | | |your everyday seo, who likes kimchi

     (button) follow
   [42]towards data science

[43]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 340
     * (button)
     *
     *

   [44]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [45]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c0a07d191981
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/only-numpy-understanding-back-propagation-for-transpose-convolution-in-multi-layer-id98-with-c0a07d191981&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/only-numpy-understanding-back-propagation-for-transpose-convolution-in-multi-layer-id98-with-c0a07d191981&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_yytprbmgnpbb---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@seojaeduk?source=post_header_lockup
  17. https://towardsdatascience.com/@seojaeduk
  18. https://giphy.com/gifs/math-35v2aus45pure/download
  19. http://techieme.in/matrix-rotation/
  20. http://techieme.in/matrix-rotation/
  21. https://medium.com/@ak16?source=responses---------1----------------
  22. https://repl.it/@jae_dukduk/transpose-conv
  23. https://twitter.com/sarah__asiri
  24. https://twitter.com/jaedukseo
  25. https://jaedukseo.me/
  26. https://www.youtube.com/c/jaedukseo
  27. https://becominghuman.ai/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af
  28. http://techieme.in/matrix-rotation/
  29. https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers
  30. http://courses.cs.tau.ac.il/caffe_workshop/bootcamp/pdf_lectures/lecture 3 id98 - id26.pdf
  31. https://www.cc.gatech.edu/classes/ay2018/cs7643_fall/slides/l7_id98s_annotated.pdf
  32. https://medium.com/swlh
  33. http://growthsupply.com/the-startup-newsletter/
  34. https://towardsdatascience.com/tagged/machine-learning?source=post
  35. https://towardsdatascience.com/tagged/convolutional-network?source=post
  36. https://towardsdatascience.com/tagged/neural-networks?source=post
  37. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  38. https://towardsdatascience.com/tagged/id26?source=post
  39. https://towardsdatascience.com/@seojaeduk?source=footer_card
  40. https://towardsdatascience.com/@seojaeduk
  41. https://jaedukseo.me/
  42. https://towardsdatascience.com/?source=footer_card
  43. https://towardsdatascience.com/?source=footer_card
  44. https://towardsdatascience.com/
  45. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  47. https://medium.com/p/c0a07d191981/share/twitter
  48. https://medium.com/p/c0a07d191981/share/facebook
  49. https://medium.com/p/c0a07d191981/share/twitter
  50. https://medium.com/p/c0a07d191981/share/facebook
