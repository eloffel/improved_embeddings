5
1
0
2

 
r
p
a
2

 

 
 
]
l
m

.
t
a
t
s
[
 
 

1
v
1
4
6
0
0

.

4
0
5
1
:
v
i
x
r
a

a probabilistic theory of deep learning

ankit b. patel, tan nguyen, richard g. baraniuk

department of electrical and computer engineering

rice university

{abp4, mn15, richb}@rice.edu

april 2, 2015

a grand challenge in machine learning is the development of computational al-
gorithms that match or outperform humans in perceptual id136 tasks such
as visual object and id103. the key factor complicating such tasks
is the presence of numerous nuisance variables, for instance, the unknown
object position, orientation, and scale in object recognition or the unknown
voice pronunciation, pitch, and speed in id103. recently, a new
breed of deep learning algorithms have emerged for high-nuisance id136
tasks; they are constructed from many layers of alternating linear and nonlin-
ear processing units and are trained using large-scale algorithms and massive
amounts of training data. the recent success of deep learning systems is im-
pressive     they now routinely yield pattern recognition systems with near-
or super-human capabilities     but a fundamental question remains: why do
they work? intuitions abound, but a coherent framework for understanding,
analyzing, and synthesizing deep learning architectures has remained elusive.
we answer this question by developing a new probabilistic framework for deep
learning based on a bayesian generative probabilistic model that explicitly cap-
tures variation due to nuisance variables. the graphical structure of the model
enables it to be learned from data using classical expectation-maximization
techniques. furthermore, by relaxing the generative model to a discriminative
one, we can recover two of the current leading deep learning systems, deep
convolutional neural networks (dcns) and random decision forests (rdfs),
providing insights into their successes and shortcomings as well as a princi-
pled route to their improvement.

1

contents
1

introduction

2 a deep probabilistic model for nuisance variation

2.1 the rendering model: capturing nuisance variation . . . . . . . . . . . . . .
2.2 deriving the key elements of one layer of a deep convolutional

network from the rendering model

id136 in the deep rendering model
2.4.1 what about the softmax regression layer?

. . . . . . . . . . . . . . . . . . . . . . .
2.3 the deep rendering model: capturing levels of abstraction . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
2.4
. . . . . . . . . . . . . .
2.5 dcns are probabilistic message passing networks . . . . . . . . . . . . . . .
2.5.1 deep rendering model and message passing . . . . . . . . . . . . . .
2.5.2 a uni   cation of neural networks and probabilistic id136 . . . . .
2.5.3 the probabilistic role of max-pooling . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
2.6.1 em algorithm for the shallow rendering model
. . . . . . . . . . . .
2.6.2 em algorithm for the deep rendering model . . . . . . . . . . . . . .
2.6.3 what about dropout training? . . . . . . . . . . . . . . . . . . . . .
2.7 from generative to discriminative classi   ers . . . . . . . . . . . . . . . . . .
. . . .
from the deep rendering model to deep convolutional networks . . .

2.7.1 transforming a generative classi   er into a discriminative one
2.7.2

2.6 learning the rendering models

3 new insights into deep convolutional networks

3.1 dcns possess full probabilistic semantics
. . . . . . . . . . . . . . . . . . .
3.2 class appearance models and activity maximization . . . . . . . . . . . . . .
3.3

(dis)entanglement: supervised learning of task targets is
intertwined with unsupervised learning of latent task nuisances . . . . . . .

4 from the deep rendering model to random decision forests

4.1 the evolutionary deep rendering model: a hierarchy of categories
4.2

. . . . .
id136 with the e-drm yields a decision tree . . . . . . . . . . . . . . . .
4.2.1 what about the leaf histograms? . . . . . . . . . . . . . . . . . . . .

4.3 bootstrap aggregation to prevent over   tting yields a decision

forest

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 em learning for the e-drm yields the infomax principle . . . . . . . . . . .

.

.

.

.

.

.

.

.

.

.

4

5
6

9
12
15
17
17
17
18
18
18
20
21
23
23
24
26

27
27
27

30

30
32
33
34

34
36

2

5 relation to prior work

5.1 relation to mixture of factor analyzers . . . . . . . . . . . . . . . . . . . . .
i-theory: invariant representations inspired by sensory cortex . . . . . . . .
5.2
5.3 scattering transform: achieving invariance via wavelets . . . . . . . . . . . .
5.4 learning deep architectures via sparsity . . . . . . . . . . . . . . . . . . . .
5.5 google facenet: learning useful representations with dcns . . . . . . . . .
5.6 reid172 theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.7 summary of key distinguishing features of the drm . . . . . . . . . . . . .

6 new directions

soft id136 . .

. . . . . . . . . . . . . . . . . . . . . . . .
6.1 more realistic rendering models
6.2 new id136 algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2.2 top-down convolutional nets: top-down id136 via the drm . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3.1 derivative-free learning . . . . . . . . . . . . . . . . . . . . . . . . .
6.3.2 dynamics: learning from video . . . . . . . . . . . . . . . . . . . . .
6.3.3 training from labeled and unlabeled data . . . . . . . . . . . . . . .

6.3 new learning algorithms

a supplemental information

a.1 from the gaussian rendering model classi   er to deep dcns . . . . . . . . .
a.2 generalizing to arbitrary mixtures of exponential family

distributions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
a.3 id173 schemes: deriving the dropout algorithm . . . . . . . . . . .

. .

. .

. .

36
36
37
38
38
39
40
40

41
41
43
43
43
44
44
44
44

46
46

49
49

3

1

introduction

humans are expert at a wide array of complicated sensory id136 tasks, from recognizing
objects in an image to understanding phonemes in a speech signal, despite signi   cant variations
such as the position, orientation, and scale of objects and the pronunciation, pitch, and volume
of speech. indeed, the main challenge in many sensory perception tasks in vision, speech, and
natural language processing is a high amount of such nuisance variation. nuisance variations
complicate perception, because they turn otherwise simple statistical id136 problems with
a small number of variables (e.g., class label) into much higher-dimensional problems. for ex-
ample, images of a car taken from different camera viewpoints lie on a highly curved, nonlinear
manifold in high-dimensional space that is intertwined with the manifolds of myriad other ob-
jects. the key challenge in developing an id136 algorithm is then how to factor out all of
the nuisance variation in the input. over the past few decades, a vast literature that approaches
this problem from myriad different perspectives has developed, but the most dif   cult id136
problems have remained out of reach.

recently, a new breed of machine learning algorithms have emerged for high-nuisance in-
ference tasks, resulting in pattern recognition systems with sometimes super-human capabili-
ties (1). these so-called deep learning systems share two common hallmarks. first, architec-
turally, they are constructed from many layers of alternating linear and nonlinear processing
units. second, computationally, their parameters are learned using large-scale algorithms and
massive amounts of training data. two examples of such architectures are the deep convolu-
tional neural network (dcn), which has seen great success in tasks like visual object recogni-
tion and localization (2), id103 (3), and part-of-id103 (4), and random
decision forests (rdfs) (5) for image segmentation. the success of deep learning systems is
impressive, but a fundamental question remains: why do they work? intuitions abound to ex-
plain their success. some explanations focus on properties of feature invariance and selectivity
developed over multiple layers, while others credit raw computational power and the amount
of available training data (1). however, beyond these intuitions, a coherent theoretical frame-
work for understanding, analyzing, and synthesizing deep learning architectures has remained
elusive.

in this paper, we develop a new theoretical framework that provides insights into both the
successes and shortcomings of deep learning systems, as well as a principled route to their de-
sign and improvement. our framework is based on a generative probabilistic model that explic-
itly captures variation due to latent nuisance variables. the rendering model (rm) explicitly
models nuisance variation through a rendering function that combines the task-speci   c vari-
ables of interest (e.g., object class in an object recognition task) and the collection of nuisance
variables. the deep rendering model (drm) extends the rm in a hierarchical fashion by ren-

4

dering via a product of af   ne nuisance transformations across multiple levels of abstraction. the
graphical structures of the rm and drm enable id136 via message passing, using, for ex-
ample, the sum-product or max-sum algorithms, and training via the expectation-maximization
(em) algorithm. a key element of the framework is the relaxation of the rm/drm generative
model to a discriminative one in order to optimize the id160.

the drm unites and subsumes two of the current leading deep learning based systems as
max-sum message passing networks. that is, con   guring the drm with two different nuisance
structures     gaussian translational nuisance or evolutionary additive nuisance     leads directly
to dcns and rdfs, respectively. the intimate connection between the drm and these deep
learning systems provides a range of new insights into how and why they work, answering
several open questions. moreover, the drm provides insights into how and why deep learning
fails and suggests pathways to their improvement.

it is important to note that our theory and methods apply to a wide range of different in-
ference tasks (including, for example, classi   cation, estimation, regression, etc.) that feature a
number of task-irrelevant nuisance variables (including, for example, object and speech recog-
nition). however, for concreteness of exposition, we will focus below on the classi   cation
problem underlying visual object recognition.

this paper is organized as follows. section 2 introduces the rm and drm and demonstrates
step-by-step how they map onto dcns. section 3 then summarizes some of the key insights
that the drm provides into the operation and performance of dcns. section 4 proceeds in a
similar fashion to derive rdfs from a variant of the drm that models a hierarchy of categories.
section 6 closes the paper by suggesting a number of promising avenues for research, including
several that should lead to improvement in deep learning system performance and generality.
the proofs of several results appear in the appendix.

2 a deep probabilistic model for nuisance variation
this section develops the rm, a generative probabilistic model that explicitly captures nuisance
transformations as latent variables. we show how id136 in the rm corresponds to oper-
ations in a single layer of a dcn. we then extend the rm by de   ning the drm, a rendering
model with layers representing different scales or levels of abstraction. finally, we show that,
after the application of a discriminative relaxation, id136 and learning in the drm corre-
spond to feedforward propagation and back propagation training in the dcn. this enables us to
conclude that dcns are probabilistic message passing networks, thus unifying the probabilistic
and neural network perspectives.

5

2.1 the rendering model: capturing nuisance variation
visual object recognition is naturally formulated as a statistical classi   cation problem.1 we
are given a d-pixel, multi-channel image i of an object, with intensity i(x,   ) at pixel x and
channel    (e.g.,    ={red, green, blue}). we seek to infer the object   s identity (class) c     c,
where c is a    nite set of classes.2 we will use the terms    object    and    class    interchangeably.
given a joint probabilistic model p(i, c) for images and objects, we can classify a particular
image i using the maximum a posteriori (map) classi   er

  c(i) = argmax

c   c

p(c|i) = argmax

c   c

p(i|c)p(c),

(1)

where p(i|c) is the image likelihood, p(c) is the prior distribution over the classes, and p(c|i)    
p(i|c)p(c) by bayes    rule.
object recognition, like many other id136 tasks, is complicated by a high amount of
variation due to nuisance variables, which the above formation ignores. we advocate explicitly
modeling nuisance variables by encapsulating all of them into a (possibly high-dimensional)
parameter g     g, where g is the set of all nuisances. in some cases, it is natural for g to be a
transformation and for g to be endowed with a (semi-)group structure.
we now propose a generative model for images that explicitly models the relationship be-
tween images i of the same object c subject to nuisance g. first, given c, g, and other auxiliary
parameters, we de   ne the rendering function r(c, g) that renders (produces) an image. in image
id136 problems, for example, r(c, g) might be a photorealistic computer graphics engine
(c.f., pixar). a particular realization of an image is then generated by adding noise to the output
of the renderer:

i|c, g = r(c, g) + noise.

(2)

we assume that the noise distribution is from the exponential family, which includes a large
number of practically useful distributions (e.g., gaussian, poisson). also we assume that the
noise is independent and identically distributed (iid) as a function of pixel location x and that
the class and nuisance variables are independently distributed according to categorical distri-
butions.3 with these assumptions, eq. 2 then becomes the probabilistic (shallow) rendering

1recall that we focus on object recognition from images only for concreteness of exposition.
2the restriction for c to be    nite can be removed by using a nonparametric prior such as a chinese restaurant
3independence is merely a convenient approximation; in practice, g can depend on c. for example, humans

process (crp) (6)

have dif   culty recognizing and discriminating upside down faces (7).

6

model (rm)

c     cat({  c}c   c),
g     cat({  g}g   g),

i|c, g     q(  cg).

(3)

here q(  cg) denotes a distribution from the exponential family with parameters   cg, which
include the mixing probabilities   cg, natural parameters   (  cg), suf   cient statistics t (i) and
whose mean is the rendered template   cg = r(c, g).

an important special case is when q(  cg) is gaussian, and this de   nes the gaussian ren-

dering model (grm), in which images are generated according to

i|c, g     n (i|  cg = r(c, g),   cg =   21),

(4)

where 1 is the identity matrix. the grm generalizes both the gaussian na    ve bayes classi   er
(gnbc) and the gaussian mixture model (gmm) by allowing variation in the image to depend
on an observed class label c, like a gnbc, and on an unobserved nuisance label g, like a gmm.
the gnbc, gmm and the (g)rm can all be conveniently described as a directed graphical
model (8). figure 1a depicts the id114 for the gnbc and gmm, while fig. 1b
shows how they are combined to form the (g)rm.

finally, since the world is spatially varying and an image can contain a number of different
objects, it is natural to break the image up into a number of (overlapping) subimages, called
patches, that are indexed by spatial location x. thus, a patch is de   ned here as a collection of
pixels centered on a single pixel x. in general, patches can overlap, meaning that (i) they do not
tile the image, and (ii) an image pixel x can belong to more than one patch. given this notion of
pixels and patches, we allow the class and nuisance variables to depend on pixel/patch location:
i.e., local image class c(x) and local nuisance g(x) (see fig. 2a). we will omit the dependence
on x when it is clear from context.

the notion of a rendering operator is quite general and can refer to any function that maps
a target variable c and nuisance variables g into a pattern or template r(c, g). for example, in
id103, c might be a phoneme, in which case g represents volume, pitch, speed, and
accent, and r(c, g) is the amplitude of the acoustic signal (or alternatively the time-frequency
representation). in natural language processing, c might be the grammatical part-of-speech, in
which case g represents syntax and grammar, and r(c, g) is a clause, phrase or sentence.

to perform object recognition with the rm via eq. 1, we must marginalize out the nuisance
variables g. we consider two approaches for doing so, one conventional and one unconven-
tional. the sum-product rm classi   er (sp-rmc) sums over all nuisance variables g     g and

7

figure 1. graphical depiction of the naive bayes classi   er (a, left), gaussian mixture model (a, right),
the shallow rendering model (b) and the deep rendering model (c). all dependence on pixel location
x has been suppressed for clarity.

then chooses the most likely class:

  csp(i) = argmax

c   c

= argmax

c   c

1

|g|(cid:88)g   g
|g|(cid:88)g   g

1

p(i|c, g)p(c)p(g)

exp(cid:104)  (  cg)|t (i)(cid:105),

(5)

where (cid:104)  |  (cid:105) is the bra-ket notation for inner products and in the last line we have used the def-
inition of an exponential family distribution. thus the sp-rm computes the marginal of the
posterior distribution over the target variable, given the input image. this is the conventional
approach used in most probabilistic modeling with latent variables.

an alternative and less conventional approach is to use the max-sum rm classi   er (ms-

rmc), which maximizes over all g     g and then chooses the most likely class:

  cms(i) = argmax

c   c

= argmax

c   c

g   g p(i|c, g)p(c)p(g)
max
max
g   g (cid:104)  (  cg)|t (i)(cid:105).

(6)

the ms-rmc computes the mode of the posterior distribution over the target and nuisance
variables, given the input image. equivalently, it computes the most likely global con   guration
of target and nuisance variables for the image. intuitively, this is an effective strategy when
    g that dominates all other explanations g (cid:54)= g   . this condition
there is one explanation g   

8

i  l  l   1  1...glgl   1g1acbicgiicgnaive bayes classifiermixture modelrendering modeldeep rendering modelis justi   ed in settings where the rendering function is deterministic or nearly noise-free. this
approach to classi   cation is unconventional in both the machine learning and computational
neuroscience literatures, where the sum-product approach is most commonly used, although it
has received some recent attention (9).

both the sum-product and max-sum classi   ers amount to applying an af   ne transformation
to the input image i (via an inner product that performs feature detection via template match-
ing), followed by a sum or max nonlinearity that marginalizes over the nuisance variables.

throughout the paper we will assume isotropic or diagonal gaussian noise for simplicity,
but the treatment presented here can be generalized to any distribution from the exponential
family in a straightforward manner. note that such an extension may require a non-linear trans-
formation (i.e. quadratic or logarithmic t (i)), depending on the speci   c exponential family.
please see supplement section a.2 for more details.

2.2 deriving the key elements of one layer of a deep convolutional

network from the rendering model

having formulated the rendering model (rm), we now show how to connect the rm with deep
convolutional networks (dcns). we will see that the ms-rmc (after imposing a few additional
assumptions on the rm) gives rise to most commonly used dcn layer types.

our    rst assumption is that the noise added to the rendered template is isotropically gaus-
sian (grm) i.e. each pixel has the same noise variance   2 that is independent of the con   g-
uration (c, g). then, assuming the image is normalized (cid:107)i(cid:107)2 = 1, eq. 6 yields the max-sum
gaussian rm classi   er (see appendix a.1 for a detailed proof)

  cms(i) = argmax

c   c

    argmax

c   c

max

  2   cg(cid:12)(cid:12)(cid:12)i(cid:29)    
g   g (cid:28) 1

max
g   g (cid:104)wcg|i(cid:105) + bcg,

1
2  2(cid:107)  cg(cid:107)2

2 + ln   c  g

(7)

where we have de   ned the natural parameters        {wcg, bcg} in terms of the traditional pa-
rameters        {  2,   cg,   c,   g} according to4
1
  2   cg =
1
2  2(cid:107)  cg(cid:107)2

1
  2 r(c, g)
2 + ln   c  g.

wcg    
bcg    

(8)

note that we have suppressed the parameters    dependence on pixel location x.

4since the gaussian distribution of the noise is in the exponential family, it can be reparametrized in terms of

the natural parameters. this is known as canonical form.

9

we will now demonstrate that the sequence of operations in the ms-rmc in eq. 7 coincides
exactly with the operations involved in one layer of a dcn (or, more generally, a max-out neural
network (10)): image id172, linear template matching, thresholding, and max pooling.
see fig. 2c. we now explore each operation in eq. 7 in detail to make the link precise.

first, the image is normalized. until recently, there were several different types of normal-
ization typically employed in dcns: local response id172, and local contrast normal-
ization (11, 12). however, the most recent highly performing dcns employ a different form of
id172, known as batch-id172 (13). we will come back to this later when we
show how to derive batch id172 from a principled approach. one implication of this is
that it is unclear what probabilistic assumption the older forms of id172 arise from, if
any.

second, the image is    ltered with a set of noise-scaled rendered templates wcg. the size
of the templates depends on the size of the objects of class c and the values of the nuisance
variables g. large objects will have large templates, corresponding to a fully connected layer in
a dcn (14), while small objects will have small templates, corresponding to a locally connected
layer in a dcn (15). if the distribution of objects depends on the pixel position x (e.g., cars are
more likely on the ground while planes are more likely in the air) then, in general, we will need
different rendered templates at each x. in this case, the locally connected layer is appropriate.
if, on the other hand, all objects are equally likely to be present at all locations throughout the
entire image, then we can assume translational invariance in the rm. this yields a global set of
templates that are used at all pixels x, corresponding to a convolutional layer in a dcn (14) (see
appendix a.2 for a detailed proof). if the    lter sizes are large relative to the scale at which the
image variation occurs and the    lters are overcomplete, then adjacent    lters overlap and waste
computation. in these case, it is appropriate to use a strided convolution, where the output of the
traditional convolution is down-sampled by some factor; this saves some computation without
losing information.

third, the resulting activations (log-probabilities of the hypotheses) are passed through a
pooling layer; i.e., if g is a translational nuisance, then taking the maximum over g corresponds
to max pooling in a dcn.

fourth, recall that a given image pixel x will reside in several overlapping image patches,
each rendered by its own parent class c(x) and the nuisance location g(x) (fig. 2a). thus
we must consider the possibility of collisions: i.e. when two different parents c(x1) (cid:54)= c(x2)
might render the same pixel (or patch). to avoid such undesirable collisions, it is natural to
force the rendering to be locally sparse: i.e. we must enforce that only one renderer in a local
neighborhood can be    active   .

to formalize this, we endow each parent renderer with an on/off state via a switching
variable a     a     {on, off}. if a = on = 1, then the rendered image patch is left untouched,

10

figure 2. an example of mapping from the deep rendering model (drm) to its corresponding factor
graph to a deep convolutional network (dcn) showing only the transformation from level (cid:96) of the
hierarchy of abstraction to level (cid:96) + 1. (a) drm generative model: a single super pixel x(cid:96)+1 at level
(cid:96) + 1 (green, upper) renders down to a 3    3 image patch at level (cid:96) (green, lower), whose location
is speci   ed by g(cid:96)+1 (red). (b) factor graph representation of the drm model that supports ef   cient
id136 algorithms such as the max-sum message passing shown here. (c) computational network that
implements the max-sum message passing algorithm from (b) explicitly; its structure exactly matches
that of a dcn.

11

1	
   2	
   3	
   4	
   gl+1onoffal+11	
   2	
   4	
   3	
   xlclxl+1cl+1   cl+1    cl =   l+1 gl+1    cl+1 noise...1	
   2	
   4	
   3	
   xlclxl+1cl+1...dcnconvolutionmaxpool+reluinputfeaturemapoutputfeaturemapil+1id136 discriminative  counterpart probabilistic model (deep rendering model) afactor graph (id136 via max-sum) bneural network (deep convolutional network) c1	
   2	
   4	
   3	
   xlclxl+1cl+1maxgl+1{  }   cl+1 |   cl ,gl+1...max summessagemax summessagefactoril+1hwl+1|  i1	
   2	
   3	
   4	
   gl+1onoffal+1ililrenderingid136id136.........i0i0i0whereas if a = off = 0, the image patch is masked with zeros after rendering. thus, the
switching variable a models (in)active parent renderers.

however, these switching variables have strong correlations due to the crowding out effect:
if one is on, then its neighbors must be off in order to prevent rendering collisions. although
natural for realistic rendering, this complicates id136. thus, we employ an approxima-
tion by instead assuming that the state of each renderer on or off completely at random and
thus independent of any other variables, including the measurements (i.e., the image itself).
of course, an approximation to real rendering, but it simpli   es id136, and leads directly
to recti   ed linear units, as we show below. such approximations to true sparsity have been
extensively studied, and are known as spike-and-slab sparse coding models (16, 17).

since the switching variables are latent (unobserved), we must max-marginalize over them
during classi   cation, as we did with nuisance variables g in the last section (one can think of a
as just another nuisance). this leads to (see appendix a.3 for a more detailed proof)

  c(i) = argmax

c   c

c   c

    argmax
= argmax

c   c

1
2  2 ((cid:107)a  cg(cid:107)2

2 + (cid:107)i(cid:107)2

2)) + ln   c  g  a

a   a(cid:28) 1

  2 a  cg(cid:12)(cid:12)(cid:12)i(cid:29)    

g   g max
max
max
g   g max
g   g relu ((cid:104)wcg|i(cid:105) + bcg) ,
max

a   a a((cid:104)wcg|i(cid:105) + bcg) + bcga

(9)

where bcga and bcg are bias terms and relu(u)     (u)+ = max{u, 0} denotes the soft-thresholding
operation performed by the recti   ed linear units (relu) in modern dcns (18). in the last
line, we have assumed that the prior   cg is uniform so that bcga is independent of c and g and
can be dropped.

2.3 the deep rendering model: capturing levels of abstraction
the world is summarizable at varying levels of abstraction, and hierarchical bayesian models
(hbms) can exploit this fact to accelerate learning. in particular, the power of abstraction allows
the higher levels of an hbm to learn concepts and categories far more rapidly than lower levels,
due to stronger inductive biases and exposure to more data (19). this is informally known as
the blessing of abstraction (19). in light of these bene   ts, it is natural for us to extend the rm
into an hbm, thus giving it the power to summarize data at different levels of abstraction.

in order to illustrate this concept, consider the example of rendering an image of a face, at
different levels of detail (cid:96)     {l, l   1, . . . , 0}. at level (cid:96) = l (the coarsest level of abstraction),
we specify only the identity of the face cl and its overall location and pose gl without specifying
any    ner-scale details such as the locations of the eyes or type of facial expression. at level
(cid:96) = l     1, we specify    ner-grained details, such as the existence of a left eye (cl   1) with a

12

figure 3. this sculpture by henri matisse illustrates the deep rendering model (drm). the sculpture
in the leftmost panel is analogous to a fully rendered image at the lowest abstraction level (cid:96) = 0. moving
from left to right, the sculptures become progressively more abstract, until the in the rightmost panel we
reach the highest abstraction level (cid:96) = 3. the    ner-scale details in the    rst three panels that are lost
in the fourth are the nuisance parameters g, whereas the coarser-scale details in the last panel that are
preserved are the target c.

certain location, pose, and state (e.g., gl   1 = open or closed), again without specifying any
   ner-scale parameters (such as eyelash length or pupil shape). we continue in this way, at
each level (cid:96) adding    ner-scaled information that was unspeci   ed at level (cid:96)     1, until at level
(cid:96) = 0 we have fully speci   ed the image   s pixel intensities, leaving us with the fully rendered,
multi-channel image i 0(x(cid:96),   (cid:96)). here x(cid:96) refers to a pixel location at level (cid:96).

for another illustrative example, consider the back series of sculptures by the artist henri
matisse (fig. 3). as one moves from left to right, the sculptures become increasingly abstract,
losing low-level features and details, while preserving high-level features essential for the over-
all meaning: i.e. (cl, gl) =    woman with her back facing us.    conversely, as one moves from
right to left, the sculptures become increasingly concrete, progressively gaining    ner-scale de-
tails (nuisance parameters g(cid:96), (cid:96) = l   1, . . . , 0) and culminating in a rich and textured rendering.
we formalize this process of progressive rendering by de   ning the deep rendering model
(drm). analogous to the matisse sculptures, the image generation process in a drm starts at
the highest level of abstraction ((cid:96) = l), with the random choice of the object class cl and overall
pose gl. it is then followed by generation of the lower-level details g(cid:96), and a progressive level-
by-level ((cid:96)     (cid:96)     1) rendering of a set of intermediate rendered    images      (cid:96), each with more
detailed information. the process    nally culminates in a fully rendered d0     d-dimensional

13

image   0 = i 0     i ((cid:96) = 0). mathematically,

cl     cat(  (cl)),
g(cid:96)+1     cat(  (g(cid:96)+1)),

cl     cl,
g(cid:96)+1     g(cid:96)+1,
  (cl, g) =   (g)  (cl)       1(g1)         l(gl)      (cl),
i(cl, g) =   (cl, g) + n (0,   21d)     rd.

(cid:96) = l     1, l     2, . . . , 0

g = {g(cid:96)}l

(cid:96)=1

(10)
here c(cid:96),g(cid:96) are the sets of all target-relevant and target-irrelevant nuisance variables at level
(cid:96), respectively. the rendering path is de   ned as the sequence (cl, gl, . . . , g(cid:96), . . . , g1) from
the root (overall class) down to the individual pixels at (cid:96) = 0.   (cl) is an abstract template
for the high-level class cl, and   (g)     (cid:81)(cid:96)   (cid:96)(g(cid:96)) represents the sequence of local nuisance

transformations that renders    ner-scale details as one moves from abstract to concrete. note
that each   (cid:96)(g(cid:96)) is an af   ne transformation with a bias term   (g(cid:96)) that we have suppressed
for clarity5. figure 2a illustrates the corresponding graphical model. as before, we have
suppressed the dependence of c(cid:96), g(cid:96) on the pixel location x(cid:96) at level (cid:96) of the hierarchy.

we can cast the drm into an incremental form by de   ning an intermediate class c(cid:96)    
(cl, gl, . . . , g(cid:96)+1) that intuitively represents a partial rendering path up to level (cid:96). then, the
partial rendering from level (cid:96) + 1 to (cid:96) can be written as an af   ne transformation

  (c(cid:96)) =   (cid:96)+1(g(cid:96)+1)      (c(cid:96)+1) +   (g(cid:96)+1) + n (0,   (cid:96)+1),

(11)

where we have shown the bias term    explicitly and introduced noise6 with a diagonal co-
variance   (cid:96)+1.
it is important to note that c(cid:96), g(cid:96) can correspond to different kinds of target
relevant and irrelevant features at different levels. for example, when rendering faces, c1(x1)
might correspond to different edge orientations and g1(x1) to different edge locations in patch
x1, whereas c2(x2) might correspond to different eye types and g2(x2) to different eye gaze
directions in patch x2.

the drm generates images at intermediate abstraction levels via the incremental rendering
functions in eq. 11 (see fig. 2a). hence the complete rendering function r(c, g) from eq. 2
is a composition of incremental rendering functions, amounting to a product of af   ne trans-
formations as in eq. 10. compared to the shallow rm, the factorized structure of the drm

results in an exponential reduction in the number of free parameters, from d0 |cl|(cid:81)(cid:96) |g(cid:96)| to
|cl|(cid:80)(cid:96) d(cid:96)|g(cid:96)| where d(cid:96) is the number of pixels in the intermediate image   (cid:96), thus enabling

more ef   cient id136 and learning, and most importantly, better generalization.

5this assumes that we are using an exponential family with linear suf   cient statistics i.e. t (i) = (i, 1)t .
however, note that the family we use here is not gaussian, it is instead a factor analyzer, a different probabilistic
model.

6we introduce noise for two reasons: (1) it will make it easier to connect later to existing em algorithms for
factor analyzers and (2) we can always take the noise-free limit to impose cluster well-separatedness if needed.
indeed, if the rendering process is deterministic or nearly noise-free, then the latter is justi   ed.

14

the drm as formulated here is distinct from but related to several other id187,
such as the deep mixture of factor analyzers (dmfa) (20) and the deep gaussian mixture
model (21), both of which are essentially compositions of another model     the mixture of
factor analyzers (mfa) (22). we will highlight the similarities and differences with these
models in more detail in section 5.

2.4 id136 in the deep rendering model
id136 in the drm is similar to id136 in the shallow rm. for example, to classify images
we can use either the sum-product (eq. 5) or the max-sum (eq. 6) classi   er. the key difference
between the deep and shallow rms is that the drm yields iterated layer-by-layer updates, from
   ne-to-coarse abstraction (bottom-up) and from coarse-to-   ne abstraction (top-down). in the
case we are only interested in inferring the high-level class cl, we only need the    ne-to-coarse
pass and so we will only consider it in this section.

importantly, the bottom-up pass leads directly to dcns, implying that dcns ignore poten-
tially useful top-down information. this maybe an explanation for their dif   culties in vision
tasks with occlusion and clutter, where such top-down information is essential for disambiguat-
ing local bottom-up hypotheses. later on in section 6.2.2, we will describe the coarse-to-   ne
pass and a new class of top-down dcns that do make use of such information.

given an input image i 0, the max-sum classi   er infers the most likely global con   guration
{c(cid:96), g(cid:96)}, (cid:96) = 0, 1, . . . , l by executing the max-sum message passing algorithm in two stages: (i)
from    ne-to-coarse levels of abstraction to infer the overall class label   cl
ms and (ii) from coarse-
ms at all intermediate levels (cid:96).
to-   ne levels of abstraction to infer the latent variables   c(cid:96)
as mentioned above, we will focus on the    ne-to-coarse pass. since the drm is an rm with
a hierarchical prior on the rendered templates, we can use eq. 7 to derive the    ne-to-coarse

ms and   g(cid:96)

15

max-sum drm classi   er (ms-drmc) as:

  cm s(i) = argmax

cl   c

= argmax

cl   c

= argmax

cl   c

= argmax

cl   c

= argmax

cl   c

max

g   g (cid:104)  (cl, g)|     1|i 0(cid:105)
g   g (cid:104)  (g)  (cl)|(  (g)  (g)t )   

max

  (cid:96)(g(cid:96))   

|i 0(cid:105)

  (cid:96)(g(cid:96))   

1(cid:89)(cid:96)=l

max
g(cid:96)   g(cid:96)

g   g (cid:104)  (cl)|
max
1(cid:89)(cid:96)=l
(cid:104)  (cl)|
(cid:104)  (cl)| max
gl   gl

  l(gl)   

  l(gl)   

|i 0(cid:105)
       max
g1   g1

(cid:124)
(cid:124)

       max
g2   g2

       max
g3   g3

    argmax
cl   c

(cid:104)  (cl)| max
gl   gl

    argmax
cl   c
...
    argmax
cl   c

(cid:104)  (cl)| max
gl   gl

  l(gl)   

(cid:104)  (cl)|i l(cid:105),

|i 0(cid:105)

  1(g1)   

    i 1
  2(g2)   

    i 2
  3(g3)   

(cid:123)(cid:122)
(cid:123)(cid:122)

(cid:105)

|i 0
(cid:125)
|i 1
(cid:125)
|i 2(cid:105)

(cid:105)

(12)

where          (g)  (g)t is the covariance of the rendered image i and (cid:104)x|m|y(cid:105)     xt m y. note
the signi   cant change with respect to the shallow rm: the covariance    is no longer diagonal
due to the iterative af   ne transformations during rendering (eq. 11), and so we must decorrelate
the input image (via      1i 0 in the    rst line) in order to classify accurately.

note also that we have omitted the bias terms for clarity and that m    is the pseudoinverse
of matrix m. in the fourth line, we used the distributivity of max over products7 and in the last
lines de   ned the intermediate quantities

i (cid:96)+1     max

|i (cid:96)(cid:105)

   w (cid:96)+1

g(cid:96)+1   g(cid:96)+1(cid:104)(  (cid:96)+1(g(cid:96)+1))   
(cid:125)
g(cid:96)+1   g(cid:96)+1(cid:104)w (cid:96)+1(g(cid:96)+1)|i (cid:96)(cid:105)

(cid:123)(cid:122)
    maxpool(conv(i (cid:96))).

= max

(cid:124)

(13)

here i (cid:96) = i (cid:96)(x(cid:96), c(cid:96)) is the feature map output of layer (cid:96) indexed by channels c(cid:96) and   (c(cid:96), g(cid:96))    
  (c(cid:96), g(cid:96)) are the natural parameters (i.e., intermediate rendered templates) for level (cid:96).

7 for a > 0, max{ab, ac} = a max{b, c}.

16

if we care only about inferring the overall class of the image cl(i 0), then the    ne-to-coarse
pass suf   ces, since all information relevant to determining the overall class has been integrated.
that is, for high-level classi   cation, we need only iterate eqs. 12 and 13. note that eq. 12
simpli   es to eq. 9 when we assume sparse patch rendering as in section 2.2.

coming back to dcns, we have see that the (cid:96)-th iteration of eq. 12 or eq. 9 corresponds to
feedforward propagation in the (cid:96)-th layer of a dcn. thus a dcn   s operation has a probabilistic
interpretation as    ne-to-coarse id136 of the most probable global con   guration in the drm.

2.4.1 what about the softmax regression layer?

it is important to note that we have not fully reconstituted the architecture of modern a dcn
as yet. in particular, the softmax regression layer, typically attached to the end of network, is
missing. this means that the high-level class cl in the drm (eq. 12) is not necessarily the
same as the training data class labels   c given in the dataset. in fact, the two labels   c and cl are
in general distinct.

but then how are we to interpret cl? the answer is that the most probable global con-
   guration (cl, g   ) inferred by a dcn can be interpreted as a good representation of the input
image, i.e., one that disentangles the many nuisance factors into (nearly) independent compo-
nents cl, g   . 8 under this interpretation, it becomes clear that the high-level class cl in the
disentangled representation need not be the same as the training data class label   c.

the disentangled representation for cl lies in the penultimate layer activations:   al(in) =
ln p(cl, g   
|in). given this representation, we can infer the class label   c by using a simple linear
classi   er such as the softmax regression9. explicitly, the softmax regression layer computes
p(  c|  al;   softmax) =   (w l+1  al + bl+1), and then chooses the most likely class. here   (  ) is the
softmax function and   softmax     {w l+1, bl+1} are the parameters of the softmax regression
layer.

2.5 dcns are probabilistic message passing networks
2.5.1 deep rendering model and message passing

encouraged by the correspondence identi   ed in section 2.4, we step back for a moment to
reinterpret all of the major elements of dcns in a probabilistic light. our derivation of the
drm id136 algorithm above is mathematically equivalent to performing max-sum message
passing on the factor graph representation of the drm, which is shown in fig. 2b. the factor

8in this sense, the drm can be seen as a deep (nonlinear) generalization of independent components analysis

9note that this implicitly assumes that a good disentangled representation of an image will be useful for the

(23).

classi   cation task at hand.

17

graph encodes the same information as the generative model but organizes it in a manner that
simpli   es the de   nition and execution of id136 algorithms (24). such id136 algorithms
are called message passing algorithms, because they work by passing real-valued functions
called messages along the edges between nodes. in the drm/dcn, the messages sent from
   ner to coarser levels are the feature maps i (cid:96)(x(cid:96), c(cid:96)). however, unlike the input image i 0, the
channels c(cid:96) in these feature maps do not refer to colors (e.g, red, green, blue) but instead to
more abstract features (e.g., edge orientations or the open/closed state of an eyelid).

2.5.2 a uni   cation of neural networks and probabilistic id136

the factor graph formulation provides a powerful interpretation that the convolution, max-
pooling and relu operations in a dcn correspond to max-sum id136 in a drm. thus,
we see that architectures and layer types commonly used in today   s dcns are not ad hoc; rather
they can be derived from precise probabilistic assumptions that entirely determine their struc-
ture. thus the drm uni   es two perspectives     neural network and probabilistic id136. a
summary of the relationship between the two perspectives is given in table 1.

2.5.3 the probabilistic role of max-pooling

consider the role of max-pooling from the message passing perspective. we see that it can
be interpreted as the    max    in max-sum, thus executing a max-marginalization over nuisance
variables g. typically, this operation would be intractable, since there are exponentially many
con   gurations g     g. but here the drm   s model of abstraction     a deep product of af   ne
transformations     comes to the rescue.
it enables us to convert an otherwise intractable
max-marginalization over g into a tractable sequence of iterated max-marginalizations over
abstraction levels g(cid:96) (eqs. 12, 13).10 thus, the max-pooling operation implements probabilistic
marginalization, so is absolutely essential to the dcn   s ability to factor out nuisance variation.
indeed, since the relu can also be cast as a max-pooling over on/off switching variables,
we conclude that the most important operation in dcns is max-pooling. this is in con   ict with
some recent claims to the contrary (27).

2.6 learning the rendering models
since the rm and drm are id114 with latent variables, we can learn their pa-
rameters from training data using the id83 (28). we    rst
develop the em algorithm for the shallow rm from section 2.1 and then extend it to the drm
from section 2.3.

10this can be seen, equivalently, as the execution of the max-product algorithm (26).

18

table 1. summary of probabilistic and neural network perspectives for dcns. the drm provides an
exact correspondence between the two, providing a probabilistic interpretation for all of the common
elements of dcns relating to the underlying model, id136 algorithm, and learning rules. [bn] =
reference (25).

19

aspect!neural'nets'perspective'!deep$convnets$(dcns)!probabilistic+perspective+!deep$rendering$model$(drm)!model!weights and biases of filters at a given layer  partial rendering at a given abstraction level/scale !number of layers number of abstraction levels !number of filters in a layer number of clusters/classes at a given abstraction level  !implicit in network weights; can be computed by product of weights over all layers or by activity maximization category prototypes are finely detailed versions of coarser-scale super-category prototypes. fine details are modeled with affine nuisance transformations. id136!forward propagation thru dcn exact bottom-up id136 via max-sum message passing (with max-product for nuisance factorization). !input and output feature maps probabilistic max-sum messages (real-valued functions of variables nodes) !template matching at a given layer (convolutional, locally or fully connected) local computation at factor node (log-likelihood of measurements) !max-pooling over local pooling region max-marginalization over latent translational nuisance transformations !rectified linear unit (relu). sparsifies output activations. max-marginalization over latent switching state of renderer. low prior id203 of being on. learning!stochastic id119 batch discriminative em algorithm with fine-to-coarse e-step + gradient m-step. no coarse-to-fine pass in e-step. !n/a full em algorithm !batch-normalized sgd (google state-of-the-art [bn]) discriminative approximation to full em (assumes diagonal pixel covariance) !2.6.1 em algorithm for the shallow rendering model
n=1, each iteration of the em algorithm con-
given a dataset of labeled training images {in, cn}n
sists of an e-step that infers the latent variables given the observed variables and the    old    pa-
rameters     old
gen from the last m-step, followed by an m-step that updates the parameter estimates
according to

e-step:   ncg = p(c, g|in;     old
gen),
   (cid:88)n (cid:88)cg
m-step:

     = argmax

  ncgl(  ).

here   ncg are the posterior probabilities over the latent mixture components (also called the

l(  ) is the complete-data log-likelihood for the model.

responsibilities), the sum(cid:80)cg is over all possible global con   gurations (c, g)     c    g, and
for the rm, the parameters are de   ned as        {  c,   g,   cg,   2} and include the prior prob-
abilities of the different classes   c and nuisance variables   g along with the rendered templates
  cg and the pixel noise variance   2. if, instead of an isotropic gaussian rm, we use a full-
covariance gaussian rm or an rm with a different exponential family distribution, then the
suf   cient statistics and the rendered template parameters would be different (e.g. quadratic for
a full covariance gaussian).

when the clusters in the rm are well-separated (or equivalently, when the rendering intro-
duces little noise), each input image can be assigned to its nearest cluster in a    hard    e-step,
wherein we care only about the most likely con   guration of the c(cid:96) and g(cid:96) given the input i 0. in
this case, the responsibility   (cid:96)
ncg = 1 if c(cid:96) and g(cid:96) in image in are consistent with the most likely
con   guration; otherwise it equals 0. thus, we can compute the responsibilities using max-sum
message passing according to eqs. 12 and 14. in this case, the hard em algorithm reduces to

(14)

(15)

(16)

hard e-step:   (cid:96)

m-step:

n, g   

n)(cid:75)

ncg =(cid:74)(c, g) = (c   
cg =(cid:88)n

  (cid:96)
ncg

  n (cid:96)

    (cid:96)
cg =

    (cid:96)
cg =

ncgi (cid:96)
  (cid:96)
n

  n (cid:96)
cg
n
1
  n (cid:96)
1
  n (cid:96)

cg(cid:88)n
cg(cid:88)n

20

(    2

cg)(cid:96) =

  (cid:96)
ncg(cid:107)i (cid:96)

n       (cid:96)

cg(cid:107)2
2,

where we have used the iversen bracket to denote a boolean expression, i.e.,(cid:74)b(cid:75)     1 if b is true
and(cid:74)b(cid:75)     0 if b is false.

(17)

2.6.2 em algorithm for the deep rendering model

for high-nuisance tasks, the em algorithm for the shallow rm is computationally intractable,
since it requires recomputing the responsibilities and parameters for all possible con   gurations
       (cl, gl, . . . g1).
there are exponentially many such con   gurations ( |cl|(cid:81)(cid:96) |g(cid:96)|), one for each possible
rendering tree rooted at cl. however, the crux of the drm is the factorized form of the rendered
templates (eq. 11), which results in a dramatic reduction in the number of parameters. this
enables us to ef   ciently infer the most probable con   guration exactly11 via eq. 12 and thus
avoid the need to resort to slower, approximate sampling techniques (e.g. id150),
which are commonly used for approximate id136 in deep hbms (20, 21). we will exploit
this realization below in the drm e-step.

guided by the em algorithm for mfa (22), we can extend the em algorithm for the shallow
rm from the previous section into one for the drm. the drm e-step performs id136,
   nding the most likely rendering tree con   guration      
n)    given the current
training input i 0
n. the drm m-step updates the parameters in each layer     the weights and
biases     via a responsibility-weighted regression of output activations off of input activations.
this can be interpreted as each layer learning how to summarize its input feature map into a
coarser-grained output feature map, the essence of abstraction.

n     (cl

n , . . . , g1

n, gl

in the following it will be convenient to de   ne and use the augmented form12 for certain
parameters so that af   ne transformations can be recast as linear ones. mathematically, a single

11note that this is exact for the spike-n-slab approximation to the truly sparse rendering model where only one
renderer per neighborhood is active, as described in section 2.2. technically, this approximation is not a tree, but
instead a so-called polytree. nevertheless, max-sum is exact for trees and polytrees (29).

12y = mx + b       mt   x, where   m     [m|b] and   x     [x|1] are the augmented forms for the parameters and input.

21

em iteration for the drm is then de   ned as

e-step:

  n   =(cid:74)   =      

n(cid:75) where      
n       (cid:96)(g(cid:96)))     w (cid:96)(g(cid:96))i (cid:96)   1

n     argmax

  

{ln p(  |in)}

n + b(cid:96)(g(cid:96))

e(cid:2)  (cid:96)(c(cid:96))(cid:3) =   (cid:96)(g(cid:96))   (i (cid:96)   1

e(cid:2)  (cid:96)(c(cid:96))  (cid:96)(c(cid:96))t(cid:3) = 1       (cid:96)(g(cid:96))     (cid:96)(g(cid:96))+

  (cid:96)(g(cid:96))   (i (cid:96)   1

n       (cid:96)(g(cid:96)))(i (cid:96)   1

n       (cid:96)(g(cid:96)))t (  (cid:96)(g(cid:96))   )t

m-step:

1

  n  

  (   ) =

n(cid:88)n
    (cid:96)(g(cid:96))    (cid:2)  (cid:96)(g(cid:96))|   (cid:96)(g(cid:96))(cid:3)
=(cid:32)(cid:88)n
diag(cid:40)(cid:88)n

  n   i (cid:96)   1

  (cid:96) =

1
n

n

e(cid:2)    (cid:96)(c(cid:96))(cid:3)t(cid:33)(cid:32)(cid:88)n
  n  (cid:16)i (cid:96)   1

  n  e(cid:2)    (cid:96)(c(cid:96))    (cid:96)(c(cid:96))t(cid:3)(cid:33)   1
)t(cid:41) ,

n

n         (cid:96)(g(cid:96))e(cid:2)    (cid:96)(c(cid:96))(cid:3)(cid:17) (i (cid:96)   1

where   (cid:96)(g(cid:96))   
that the nuisance variables g(cid:96) comprise both the translational and the switching variables that
were introduced earlier for dcns.

      (cid:96)(g(cid:96))t (  (cid:96) +   (cid:96)(g(cid:96))(  (cid:96)(g(cid:96)))t )   1 and e(cid:2)    (cid:96)(c(cid:96))(cid:3) = (cid:2)e(cid:2)  (cid:96)(c(cid:96))(cid:3) | 1(cid:3). note

note that this new em algorithm is a derivative-free alternative to the back propagation

algorithm for training dcns that is fast, easy to implement, and intuitive.

a powerful learning rule discovered recently and independently by google (25) can be
seen as an approximation to the above em algorithm, whereby eq. 18 is approximated by
normalizing the input activations with respect to each training batch and introducing scaling
and bias parameters according to

(18)

(19)

(20)

(21)

(22)

(23)

(24)

here   i (cid:96)   1
are the batch-normalized activations, and   ib and   b are the batch mean and standard
deviation vector of the input activations, respectively. note that the division is element-wise,

n

e(cid:2)  (cid:96)(c(cid:96))(cid:3) =   (cid:96)(g(cid:96))   (i (cid:96)   1
            i (cid:96)   1
n +   
         (cid:18) i (cid:96)   1
n       ib
  b

n       (cid:96)(g(cid:96)))
(cid:19) +   .

22

since each activation is normalized independently to avoid a costly full covariance calculation.
the diagonal matrix    and bias vector    are parameters that are introduced to compensate
for any distortions due to the batch-id172.
in light of our em algorithm derivation
for the drm, it is clear that this scheme is a crude approximation to the true id172
step in eq. 18, whose decorrelation scheme uses the nuisance-dependent mean   (g(cid:96)) and full
covariance   (cid:96)(g(cid:96))   . nevertheless, the excellent performance of the google algorithm bodes
well for the performance of the exact em algorithm for the drm developed above.

2.6.3 what about dropout training?

we did not mention the most common id173 scheme used with dcns     dropout (30).
dropout training consists of units in the dcn dropping their outputs at random. this can be
seen as a kind of noise corruption, and encourages the learning of features that are robust to
missing data and prevents feature co-adaptation as well (18, 30). dropout is not speci   c to
dcns; it can be used with other architectures as well. for brevity, we refer the reader to the
proof of the dropout algorithm in appendix a.7. there we show that dropout can be derived
from the em algorithm.

2.7 from generative to discriminative classi   ers
we have constructed a correspondence between the drm and dcns, but the mapping de   ned
so far is not exact. in particular, note the constraints on the weights and biases in eq. 8. these
are re   ections of the distributional assumptions underlying the gaussian drm. dcns do not
have such constraints     their weights and biases are free parameters. as a result, when faced
with training data that violates the drm   s underlying assumptions (model misspeci   cation),
the dcn will have more freedom to compensate. in order to complete our mapping and create
an exact correspondence between the drm and dcns, we relax these parameter constraints,
allowing the weights and biases to be free and independent parameters. however, this seems an
ad hoc approach. can we instead theoretically motivate such a relaxation?

it turns out that the distinction between the drm and dcn classi   ers is fundamental: the
former is known as a generative classi   er while the latter is known as a discriminative clas-
si   er (31, 32). the distinction between generative and discriminative models has to do with
the id160. on the one hand, generative models have strong distributional as-
sumptions, and thus introduce signi   cant model bias in order to lower model variance (i.e., less
risk of over   tting). on the other hand, discriminative models relax some of the distributional
assumptions in order to lower the model bias and thus    let the data speak for itself   , but they do
so at the cost of higher variance (i.e., more risk of over   tting) (31, 32). practically speaking, if
a generative model is misspeci   ed and if enough labeled data is available, then a discriminative

23

model will achieve better performance on a speci   c task (32). however, if the generative model
really is the true data-generating distribution (or there is not much labeled data for the task),
then the generative model will be the better choice.

having motivated the distinction between the two types of models, in this section we will
de   ne a method for transforming one into the other that we call a discriminative relaxation. we
call the resulting discriminative classi   er a discriminative counterpart of the generative classi-
   er.13 we will then show that applying this procedure to the generative drm classi   er (with
constrained weights) yields the discriminative dcn classi   er (with free weights). although we
will focus again on the gaussian drm, the treatment can be generalized to other exponential
family distributions with a few modi   cations (see appendix a.6 for more details).

2.7.1 transforming a generative classi   er into a discriminative one

before we formally de   ne the procedure, some preliminary de   nitions and remarks will be
helpful. a generative classi   er models the joint distribution p(c, i) of the input features and
the class labels. it can then classify inputs by using bayes rule to calculate p(c|i)     p(c, i) =
p(i|c)p(c) and picking the most likely label c. training such a classi   er is known as generative
learning, since one can generate synthetic features i by sampling the joint distribution p(c, i).
therefore, a generative classi   er learns an indirect map from input features i to labels c by
modeling the joint distribution p(c, i) of the labels and the features.

in contrast, a discriminative classi   er parametrically models p(c|i) = p(c|i;   d) and then
trains on a dataset of input-output pairs {(in, cn)}n
n=1 in order to estimate the parameter   d. this
is known as discriminative learning, since we directly discriminate between different labels c
given an input feature i. therefore, a discriminative classi   er learns a direct map from input
features i to labels c by directly modeling the conditional distribution p(c|i) of the labels given
the features.
given these de   nitions, we can now de   ne the discriminative relaxation procedure for con-
verting a generative classi   er into a discriminative one. starting with the standard learning
objective for a generative classi   er, we will employ a series of transformations and relaxations

13the discriminative relaxation procedure is a many-to-one mapping: several generative models might have the

same discriminative model as their counterpart.

24

to obtain the learning objective for a discriminative classi   er. mathematically, we have

max

  

lgen(  )     max

ln p(cn, in|  )

ln p(cn|in,   ) + ln p(in|  )
ln p(cn|in,   ) + ln p(in|    )

(a)
= max

(b)
= max

(c)

    max

   (cid:88)n
   (cid:88)n
  ,    :  =    (cid:88)n
   (cid:88)n
(cid:124)
  :  =  (  )(cid:88)n
   (cid:88)n
(cid:124)

(e)

    max

(d)
= max

ln p(cn|in,   )
   lcond(  )
ln p(cn|in,   )

(cid:123)(cid:122)

(cid:125)

ln p(cn|in,   )
   ldis(  )

(cid:123)(cid:122)

(cid:125)

,

(25)

where the l   s are the generative, conditional and discriminative log-likelihoods, respectively.
in line (a), we used the chain rule of id203. in line (b), we introduced an extra set of
parameters      while also introducing a constraint that enforces equality with the old set of gen-
erative parameters   . in line (c), we relax the equality constraint (   rst introduced by bishop,
laserre and minka in (31)), allowing the classi   er parameters    to differ from the image gener-
ation parameters     . in line (d), we pass to the natural parametrization of the exponential family
distribution i|c, where the natural parameters    =   (  ) are a    xed function of the conventional
parameters   . this constraint on the natural parameters ensures that optimization of lcond(  )
yields the same answer as optimization of lcond(  ). and    nally, in line (e) we relax the nat-
ural parameter constraint to get the learning objective for a discriminative classi   er, where the
parameters    are now free to be optimized.

in summary, starting with a generative classi   er with learning objective lgen(  ), we com-
plete steps (a) through (e) to arrive at a discriminative classi   er with learning objective ldis(  ).
we refer to this process as a discriminative relaxation of a generative classi   er and the resulting
classi   er is a discriminative counterpart to the generative classi   er.

figure 4 illustrates the discriminative relaxation procedure as applied to the rm (or drm).
if we consider a gaussian (d)rm, then    simply comprises the mixing probabilities   cg and
the mixture parameters   cg, and so that we have    = {  cg,   cg,   2}. the corresponding relaxed
discriminative parameters are the weights and biases   dis     {wcg, bcg}.
intuitively, we can interpret the discriminative relaxation as a brain-world transformation
applied to a generative model. according to this interpretation, instead of the world generating

25

figure 4. graphical depiction of discriminative relaxation procedure. (a) the rendering model (rm) is
depicted graphically, with mixing id203 parameters   cg and rendered template parameters   cg. the
brain-world transformation converts the rm (a) to an equivalent graphical model (b), where an extra
set of parameters      and constraints (arrows from    to      to   ) have been introduced. discriminatively
relaxing these constraints (b, red x   s) yields the single-layer dcn as the discriminative counterpart to
the original generative rm classi   er in (a).

images and class labels (fig. 4a), we instead imagine the world generating images in via the
rendering parameters            world while the brain generates labels cn, gn via the classi   er param-
eters   dis       brain (fig. 4b). note that the graphical model depicted in fig. 4b is equivalent to
that in fig. 4a, except for the relaxation of the parameter constraints (red      s) that represent
the discriminative relaxation.

2.7.2 from the deep rendering model to deep convolutional networks

we can now apply the above to show that the dcn is a discriminative relaxation of the drm.
first, we apply the brain-world transformation (eq. 25) to the drm. the resulting classi   er is
precisely a deep maxout neural network (10) as discussed earlier. second, we impose trans-
lational invariance at the    ner scales of abstraction (cid:96) and introduce switching variables a to
model inactive renderers. this yields convolutional layers with relu id180, as in
section 2.1. third, the learning algorithm for the generative drm classi   er     the em algo-
rithm in eqs. 18   23     must be modi   ed according to eq. 25 to account for the discriminative
relaxation. in particular, note that the new discriminative e-step is only    ne-to-coarse and cor-
responds to forward propagation in dcns. as for the discriminative m-step, there are a variety
of choices: any general purpose optimization algorithm can be used (e.g., newton-raphson,
conjugate gradient, etc.). choosing id119 this leads to the classical back propagation
algorithm for neural network training (33). typically, modern-day dcns are trained using a
variant of back propagation called stochastic id119 (sgd), in which gradients are
computed using one mini-batch of data at a time (instead of the entire dataset). in light of our
developments here, we can re-interpret sgd as a discriminative counterpart to the generative
batch em algorithm (34, 35).

this completes the mapping from the drm to dcns. we have shown that dcn classi-

26

icg   cg cgicg   x"discrimina+ve"relaxa+on"x"   (  )ab         brain   brain           world   ers are a discriminative relaxation of drm classi   ers, with forward propagation in a dcn
corresponding to id136 of the most probable con   guration in a drm.14 we have also re-
interpreted learning: sgd back propagation training in dcns is a discriminative relaxation of
a batch em learning algorithm for the drm. we have provided a principled motivation for
passing from the generative drm to its discriminative counterpart dcn by showing that the
discriminative relaxation helps alleviate model misspeci   cation issues by increasing the drm   s
   exibility, at the cost of slower learning and requiring more training data.

3 new insights into deep convolutional networks

in light of the intimate connection between drms and dcns, the drm provides new insights
into how and why dcns work, answering many open questions. and importantly, drms also
show us how and why dcns fail and what we can do to improve them (see section 6). in this
section, we explore some of these insights.

3.1 dcns possess full probabilistic semantics
the factor graph formulation of the drm (fig. 2b) provides a useful interpretation of dcns: it
shows us that the convolutional and max-pooling layers correspond to standard message pass-
ing operations, as applied inside factor nodes in the factor graph of the drm. in particular, the
max-sum algorithm corresponds to a max-pool-conv neural network, whereas the sum-product
algorithm corresponds to a mean-pool-conv neural network. more generally, we see that archi-
tectures and layer types used commonly in successful dcns are neither arbitrary nor ad hoc;
rather they can be derived from precise probabilistic assumptions that almost entirely determine
their structure. a summary of the two perspectives     neural network and probabilistic     are
given in table 1.

3.2 class appearance models and activity maximization
our derivation of id136 in the drm enables us to understand just how trained dcns distill
and store knowledge from past experiences in their parameters. speci   cally, the drm generates
rendered templates   (cl, g)       (cl, gl, . . . , g1) via a product of af   ne transformations, thus
implying that class appearance models in dcns (and drms) are stored in a factorized form
across multiple levels of abstraction. thus, we can explain why past attempts to understand
how dcns store memories by examining    lters at each layer were a fruitless exercise: it is the

14as mentioned in section 2.4.1, this is typically followed by a softmax regression layer at the end. this layer
classi   es the hidden representation (the penultimate layer activations   al(in)) into the class labels   cn used for
training. see section 2.4.1 for more details.

27

product of all the    lters/weights over all layers that yield meaningful images of objects. indeed,
this fact is encapsulated mathematically in eqs. 10, 11. notably, recent studies in computational
neuroscience have also shown a strong similarity between representations in primate visual
cortex and a highly trained dcn (36), suggesting that the brain might also employ factorized
class appearance models.

we can also shed new light on another approach to understanding dcn memories that
proceeds by searching for input images that maximize the activity of a particular class unit
(say, cat) (37), a technique we call activity maximization. results from activity maximization
on a high performance dcn trained on 15 million images from (37) is shown in fig. 5. the
resulting images are striking and reveal much about how dcns store memories. we now derive
a closed-form expression for the activity-maximizing images as a function of the underlying
drm model   s learned parameters. mathematically, we seek the image i that maximizes the
score s(c|i) of a speci   c object class. using the drm, we have
1
  2   (c(cid:96), g(cid:96))|i(cid:105)
(cid:104)  (c(cid:96), g)|i(cid:105)
ipp (cid:104)  (c(cid:96), g)|(cid:88)pi   p
ipi (cid:104)  (c(cid:96), g)|ipi(cid:105)
max
(c(cid:96), g)(cid:105)

s(c(cid:96)|i) = max
    max
= max

max
g   g (cid:104)
g   g max
g   g max

ip1        max

ipi(cid:105)

= max

= max

max

pi

i

i

i

g   g (cid:88)pi   p
g   g (cid:88)pi   p (cid:104)  (c(cid:96), g)|i   
= (cid:88)pi   p (cid:104)  (c(cid:96), g)|i   

pi

(c(cid:96), g   

pi(cid:105),

i   
pi

(c(cid:96), g)

argmaxipi

where
   
   
argmaxg   g (cid:104)  (c(cid:96), g)|i   
the image i is decomposed into p
pi
patches ipi of the same size as i, with all pixels outside of the patch pi set to zero. the
maxg   g operator    nds the most probable g   
pi within each patch. the solution i    of the activity
maximization is then the sum of the individual activity-maximizing patches

(cid:104)  (c(cid:96), g)|ipi(cid:105)
in the third line,

(c(cid:96), g)(cid:105).

and

=

g   (c(cid:96),pi)

g   
pi

i   

    (cid:88)pi   p

)     (cid:88)pi   p

i   
pi

(c(cid:96), g   
pi

  (c(cid:96), g   
pi

).

(27)

eq. 27 implies that i    contains multiple appearances of the same object but in various poses.
each activity-maximizing patch has its own pose (i.e. g   
pi), in agreement with fig. 5. such
images provide strong con   rming evidence that the underlying model is a mixture over nuisance
(pose) parameters, as is expected in light of the drm.

28

(26)

figure 5. results of activity maximization on id163 dataset. for a given class c, activity-maximizing
inputs are superpositions of various poses of the object, with distinct patches pi containing distinct poses
g   
pi, as predicted by eq. 27. figure adapted from (37) with permission from the authors.

29

dumbbell cup dalmatian bell pepper lemon husky washing machine computer keyboard kit fox goose limousine ostrich figure1:numericallycomputedimages,illustratingtheclassappearancemodels,learntbyaconvnet,trainedonilsvrc-2013.notehowdifferentaspectsofclassappearancearecapturedinasingleimage.betterviewedincolour.33.3 (dis)entanglement: supervised learning of task targets is

intertwined with unsupervised learning of latent task nuisances

a key goal of representation learning is to disentangle the factors of variation that contribute
to an image   s appearance. given our formulation of the drm, it is clear that dcns are dis-
criminative classi   ers that capture these factors of variation with latent nuisance variables g. as
such, the theory presented here makes a clear prediction that for a dcn, supervised learning
of task targets will lead inevitably to unsupervised learning of latent task nuisance variables.
from the perspective of manifold learning, this means that the architecture of dcns is designed
to learn and disentangle the intrinsic dimensions of the data manifold.

in order to test this prediction, we trained a dcn to classify synthetically rendered images
of naturalistic objects, such as cars and planes. since we explicitly used a renderer, we have
the power to systematically control variation in factors such as pose, location, and lighting. af-
ter training, we probed the layers of the trained dcn to quantify how much linearly separable
information exists about the task target c and latent nuisance variables g. figure 6 shows that
the trained dcn possesses signi   cant information about latent factors of variation and, further-
more, the more nuisance variables, the more layers are required to disentangle the factors. this
is strong evidence that depth is necessary and that the amount of depth required increases with
the complexity of the class models and the nuisance variations.

in light of these results, when we talk about training dcns, the traditional distinction be-
tween supervised and unsupervised learning is ill-de   ned at worst and misleading at best. this
is evident from the initial formulation of the rm, where c is the task target and g is a latent vari-
able capturing all nuisance parameters (fig. 1). put another way, our derivation above shows
that dcns are discriminative classi   ers with latent variables that capture nuisance variation.
we believe the main reason this was not noticed earlier is probably that latent nuisance variables
in a dcn are hidden within the max-pooling units, which serve the dual purpose of learning
and marginalizing out the latent nuisance variables.

4 from the deep rendering model to random decision

forests

random decision forests (rdf)s (5, 38) are one of the best performing but least understood
classi   ers in machine learning. while intuitive, their structure does not seem to arise from a
proper probabilistic model. their success in a vast array of ml tasks is perplexing, with no
clear explanation or theoretical understanding. in particular, they have been quite successful in
real-time image and video segmentation tasks, the most prominent example being their use for
pose estimation and body part tracking in the microsoft kinect gaming system (39). they also

30

figure 6. manifold entanglement and disentanglement as illustrated in a 5-layer max-out dcn trained
to classify synthetically rendered images of planes (top) and naturalistic objects (bottom) in different
poses, locations, depths and lighting conditions. the amount of linearly separable information about
the target variable (object identity, red) increases with layer depth while information about nuisance
variables (slant, tilt, left-right location, depth location) follows an inverted u-shaped curve. layers
with increasing information correspond to disentanglement of the manifold     factoring variation into
independent parameters     whereas layers with decreasing information correspond to marginalization
over the nuisance parameters. note that disentanglement of the latent nuisance parameters is achieved
progressively over multiple layers, without requiring the network to explicitly train for them. due to
the complexity of the variation induced, several layers are required for successful disentanglement, as
predicted by our theory.

31

01234layer0.00.20.40.60.81.0accuracy rateaccuracy of classifying classes and latent variables vs layerobj_accu_rateslant_accu_ratetilt_accu_ratelocx_accu_ratelocy_accu_ratelocz_accu_rateenergy_accu_rate01234layer0.00.20.40.60.81.0accuracy rateaccuracy of classifying classes and latent variables vs layerobj_accu_rateslant_accu_ratetilt_accu_ratelocx_accu_ratelocy_accu_ratehave had great success in medical image segmentation problems (5,38), wherein distinguishing
different organs or cell types is quite dif   cult and typically requires expert annotators.

in this section we show that, like dcns, rdfs can also be derived from the drm model,
but with a different set of assumptions regarding the nuisance structure. instead of translational
and switching nuisances, we will show that an additive mutation nuisance process that generates
a hierarchy of categories (e.g., evolution of a taxonomy of living organisms) is at the heart of
the rdf. as in the drm to dcn derivation, we will start with a generative classi   er and then
derive its discriminative relaxation. as such, rdfs possess a similar interpretation as dcns in
that they can be cast as max-sum message passing networks.

a decision tree classi   er takes an input image i and asks a series of questions about it. the
answer to each question determines which branch in the tree to follow. at the next node, another
question is asked. this pattern is repeated until a leaf b of the tree is reached. at the leaf, there
is a class posterior id203 distribution p(c|i, b) that can be used for classi   cation. different
leaves contain different class posteriors. an rdf is an ensemble of decision tree classi   ers
t     t . to classify an input i, it is sent as input to each decision tree t     t individually,
and each decision tree outputs a class posterior p(c|i, b, t). these are then averaged to obtain
an overall posterior p(c|i) =(cid:80)t p(c|i, b, t)p(t), from which the most likely class c is chosen.
typically we assume p(t) = 1/|t |.
4.1 the evolutionary deep rendering model: a hierarchy of categories
we de   ne the evolutionary drm (e-drm) as a drm with an evolutionary tree of categories.
samples from the model are generated by starting from the root ancestor template and randomly
mutating the templates. each child template is an additive mutation of its parent, where the spe-
ci   c mutation does not depend on the parent (see eq. 29 below). repeating this pattern at each
child node, an entire evolutionary tree of templates is generated. we assume for simplicity that
we are working with a gaussian e-drm so that at the leaves of the tree a sample is generated
by adding gaussian pixel noise. of course, as described earlier, this can be extended to handle
other noise distributions from the exponential family. mathematically, we have

cl     cat(  (cl)),
g(cid:96)+1     cat(  (g(cid:96)+1)),

cl     cl,
g(cid:96)+1     g(cid:96)+1,

(cid:96) = l     1, l     2, . . . , 0

  (cl, g) =   (g)  (cl)       1(g1)         l(gl)      (cl)

=   (cl) +   (gl) +        +   (g1),
i(cl, g) =   (cl, g) + n (0,   21d)     rd.

g = {g(cid:96)}l

(cid:96)=1

(28)

32

here,   (cid:96)(g(cid:96)) has a special structure due to the additive mutation process:   (cid:96)(g(cid:96)) = [1|   (g(cid:96))],
where 1 is the identity matrix. as before, c(cid:96),g(cid:96) are the sets of all target-relevant and target-
irrelevant nuisance variables at level (cid:96), respectively. (the target here is the same as with the
drm and dcns     the overall class label cl.) the rendering path represents template evo-
lution and is de   ned as the sequence (cl, gl, . . . , g(cid:96), . . . , g1) from the root ancestor template
down to the individual pixels at (cid:96) = 0.   (cl) is an abstract template for the root ancestor

accumulation of many additive mutations.

cl, and(cid:80)(cid:96)   (g(cid:96)) represents the sequence of local nuisance transformations, in this case, the
as with the drm, we can cast the e-drm into an incremental form by de   ning an inter-
mediate class c(cid:96)     (cl, gl, . . . , g(cid:96)+1) that intuitively represents a partial evolutionary path up to
level (cid:96). then, the mutation from level (cid:96) + 1 to (cid:96) can be written as

  (c(cid:96)) =   (cid:96)+1(g(cid:96)+1)      (c(cid:96)+1) =   (c(cid:96)+1) +   (g(cid:96)+1),

(29)

where   (g(cid:96)) is the mutation added to the template at level (cid:96) in the evolutionary tree.

as a generative model, the e-drm is a mixture of evolutionary paths, where each path starts
at the root and ends at a leaf species in the tree. each leaf species is associated with a rendered
template   (cl, gl, . . . , g1).

4.2 id136 with the e-drm yields a decision tree
since the e-drm is an rm with a hierarchical prior on the rendered templates, we can use
eq. 7 to derive the e-drm id136 algorithm as:

  cm s(i) = argmax
cl   cl
= argmax
cl   cl
= argmax
cl   cl
= argmax
cl   cl

g   g (cid:104)  (cl, g)|i 0(cid:105)
max
g   g (cid:104)  (g)  (cl)|i 0(cid:105)
max
g1   g1        max
max
max
g1   g1        max

= argmax
cl   cl
...
    argmax

cl   cl (cid:104)  (cl, g   )|i 0(cid:105),

gl   gl (cid:104)  (cl) +   (gl) +        +   (g1)|i 0(cid:105)
gl   1   gl   1 (cid:104)   (cl) +   (gl   )
+       +   (g1)|i 0(cid:105)
(cid:125)
gl   1   gl   1 (cid:104)  (cl   1) +   (gl   1) +        +   (g1)|i 0(cid:105)

     (cl,gl   )=  (cl   1)

(cid:123)(cid:122)

(cid:124)

max
g1   g1        max

(30)

note that we have explicitly shown the bias terms here, since they represent the additive muta-
tions. in the last lines, we repeatedly use the distributivity of max over sums, resulting in the

33

iteration

g(cid:96)+1(c(cid:96)+1)   

(cid:124)

   w (cid:96)+1

|i 0(cid:105)

= argmax

    argmax

g(cid:96)+1   g(cid:96)+1(cid:104)  (c(cid:96)+1, g(cid:96)+1)
(cid:125)
g(cid:96)+1   g(cid:96)+1(cid:104)w (cid:96)+1(c(cid:96)+1, g(cid:96)+1)|i 0(cid:105)
    choosechild(filter(i 0)).

(cid:123)(cid:122)

(31)

note the key differences from the drn/dcn id136 derivation in eq. 12: (i) the input
to each layer is always the input image i 0, (ii) the iterations go from coarse-to-   ne (from root
ancestor to leaf species) rather than    ne-to-coarse, and (iii) the resulting network is not a neural
network but rather a deep decision tree of single-layer neural networks. these differences
are due to the special additive structure of the mutational nuisances and the evolutionary tree
process underlying the generation of category templates.

4.2.1 what about the leaf histograms?

the mapping to a single decision tree is not yet complete; the leaf label histograms (5, 38) are
missing. analogous to the missing softmax regression layers with dcns (sec 2.4.1), the high-
level representation class label cl inferred by the e-drm in eq. 30 need not be the training
data class label   c. for clarity, we treat the two as separate in general.

but then how do we understand cl? we can interpret the inferred con   guration       =
(cl   , g   ) as a disentangled representation of the input, wherein the different factors in      , in-
cluding cl, vary independently in the world. in contrast to dcns, the class labels   c in a decision
tree are instead inferred from the discrete evolutionary path variable       through the use of the
leaf histograms p(  c|     ). note that id90 also have label histograms at all internal (non-
leaf) nodes, but that they are not needed for id136. however, they do play a critical role in
learning, as we will see below.

we are almost    nished with our mapping from id136 in gaussian e-drms to decision
trees. to    nish the mapping, we need only apply the discriminative relaxation (eq. 25) in order
to allow the weights and biases that de   ne the decision functions in the internal nodes to be
free. note that this is exactly analogous to steps in section 2.7 for mapping from the gaussian
drm to dcns.

4.3 bootstrap aggregation to prevent over   tting yields a decision

forest

thus far we have derived the id136 algorithm for the e-drm and shown that its discrimi-
native counterpart is indeed a single decision tree. but how to relate to this result to the entire

34

forest? this is important, since it is well known that individual id90 are notoriously
good at over   tting data. indeed, the historical motivation for introducing a forest of decision
trees has been in order to prevent such over   tting by averaging over many different models,
each trained on a randomly drawn subset of the data. this technique is known as bootstrap ag-
gregation or id112 for short, and was    rst introduced by breiman in the context of decision
trees (38). for completeness, in this section we review id112, thus completing our mapping
from the e-drm to the rdf.

in order to derive id112, it will be necessary in the following to make explicit the de-
n=1, i.e.
pendence of learned id136 parameters    on the training data dci     {(cn, in)}n
   =   (dci). this dependence is typically suppressed in most work, but is necessary here as
id112 entails training different id90 t on different subsets dt     d of the full training
data. in other words,   t =   t(dt).
mathematically, we perform id136 as follows: given all previously seen data dci and
an unseen image i, we classify i by computing the posterior distribution

1

(a)

p(c, a|i,dci)

p(c|i,dci, a)p(a)

p(c|i,dci) =(cid:88)a
=(cid:88)a
    ea[p(c|i,dci, a)]
t (cid:88)t   t
   
t (cid:88)t   t (cid:90) d  t p(c|i,   t) p(  t|dci, at)
t (cid:88)t   t
(cid:124)

p(c|i,      
(cid:125)
(cid:123)(cid:122)

p(c|i,dci, at)

     
t     max

decision forest classi   er

   

(b)
=

t )

(c)

1

1

,

  

p(  |dci(at)).

(32)

here at     (atn)n
n=1 is a collection of switching variables that indicates which data points are
included, i.e., atn = 1 if data point n is included in dataset dt     dci(at). in this way, we have
randomly subsampled the full dataset dci (with replacement) t times in line (a), approximating
the true marginalization over all possible subsets of the data. in line (b), we perform bayesian
model averaging over all possible values of the e-drm/decision tree parameters   t. since this
is intractable, we approximate it with the map estimate      
t in line (c). the overall result is
that each e-drm (or decision tree) t is trained separately on a randomly drawn subset dt    
dci(at)     dci of the entire dataset, and the    nal output of the classi   er is an average over the
individual classi   ers.

35

4.4 em learning for the e-drm yields the infomax principle
one approach to train an e-drm classi   er is to maximize the mutual information between the
given training labels   c and the inferred (partial) rendering path    (cid:96)     (cl, gl, . . . , gl) at each
level. note that   c and    (cid:96) are both discrete random variables.
this mutual information-based classi   er (mic) plays the same role as the softmax regres-
sion layer in dcns, predicting the class labels   c given a good disentangled representation    (cid:96)   
of the input i. in order to train the mic classi   er, we update the classi   er parameters   mic in
each m-step as the solution to the optimization:

max

  

m i(  c, (cl, gl, . . . , g1)) = max

  1

       max

  l

1(cid:88)l=l

m i(  c, gl

n ;   l)

n|gl+1

=

=

1(cid:88)l=l
1(cid:88)l=l

max

  l

max

  l

m i(  c, gl

n ;   l)

n|gl+1

h[  c]     h[  c|gl
(cid:123)(cid:122)
(cid:124)
   information gain

(cid:125)

n;   l]

.

(33)

here m i(  ,  ) is the mutual information between two random variables, h[  ] is the id178 of a
random variable, and   (cid:96) are the parameters at layer (cid:96). in the    rst line, we have used the layer-
by-layer structure of the e-drm to split the mutual information calculation across levels, from
coarse to    ne. in the second line, we have used the max-sum algorithm (id145)
to split up the optimization into a sequence of optimizations from (cid:96) = l     (cid:96) = 1. in the third
line, we have used the information-theoretic relationship m i(x, y )     h[x]     h[y |x]. this
algorithm is known as infomax in the literature (5).

5 relation to prior work
5.1 relation to mixture of factor analyzers
as mentioned above, on a high level, the drm is related to id187 based on the
mixture of factor analyzers (mfa) (22). indeed, if we add noise to each partial rendering step
from level (cid:96) to (cid:96)     1 in the drm, then eq. 11 becomes

i (cid:96)   1     n(cid:0)  (cid:96)(g(cid:96))  (cid:96)(c(cid:96)) +   (cid:96)(g(cid:96)),   (cid:96)(cid:1) ,

where we have introduced the diagonal noise covariance   (cid:96). this is equivalent to the mfa
model. the drm and dmfa both employ parameter sharing, resulting in an exponential re-
duction in the number of parameters, as compared to the collapsed or shallow version of the
models. this serves as a strong regularizer to prevent over   tting.

(34)

36

despite the high-level similarities, there are several essential differences between the drm
and the mfa-based models, all of which are critical for reproducing dcns. first, in the drm
the only randomness is due to the choice of the g(cid:96) and the observation noise after rendering.
this naturally leads to id136 of the most probable con   guration via the max-sum algorithm,
which is equivalent to max-pooling in the dcn. second, the drm   s af   ne transformations   (cid:96)
act on multi-channel images at level (cid:96) + 1 to produce multi-channel images at level (cid:96). this
structure is important, because it leads directly to the notion of (multi-channel) feature maps in
dcns. third, a drm   s layers vary in connectivity from sparse to dense, as they give rise to
convolutional, locally connected, and fully connected layers in the resulting dcn. fourth, the
drm has switching variables that model (in)active renderers (section 2.1). the manifestation
of these variables in the dcn are the relus (eq. 9). thus, the critical elements of the dcn
architecture arise directly from aspects of the drm structure that are absent in mfa-based
models.

i-theory: invariant representations inspired by sensory cortex

5.2
representational invariance and selectivity (ri) are important ideas that have developed in
the computational neuroscience community. according to this perspective, the main purpose
of the feedforward aspects of visual cortical processing in the ventral stream are to compute
a representation for a sensed image that is invariant to irrelevant transformations (e.g., pose,
lighting etc.) (40, 41). in this sense, the ri perspective is quite similar to the drm in its basic
motivations. however, the ri approach has remained qualitative in its explanatory power until
recently, when a theory of invariant representations in deep architectures     dubbed i-theory
    was proposed (42, 43). inspired by neuroscience and models of the visual cortex, it is the
   rst serious attempt at explaining the success of deep architectures, formalizing intuitions about
invariance and selectivity in a rigorous and quantitatively precise manner.

the i-theory posits a representation that employs group averages and orbits to explicitly
insure invariance to speci   c types of nuisance transformations. these transformation must pos-
sess a mathematical semi-group structure; as a result, the invariance constraint is relaxed to a
notion of partial invariance, which is built up slowly over multiple layers of the architecture.

at a high level, the drm shares similar goals with i-theory in that it attempts to capture
explicitly the notion of nuisance transformations. however, the drm differs from i-theory in
two critical ways. first, it does not impose a semi-group structure on the set of nuisance trans-
formations. this provides the drm the    exibility to learn a representation that is invariant to a
wider class of nuisance transformations, including non-rigid ones. second, the drm does not
   x the representation for images in advance. instead, the representation emerges naturally out
of the id136 process. for instance, sum- and max-pooling emerge as probabilistic marginal-

37

ization over nuisance variables and thus are necessary for proper id136. the deep iterative
nature of the dcn also arises as a direct mathematical consequence of the drm   s rendering
model, which comprises multiple levels of abstraction.

this is the most important difference between the two theories. despite these differences,
i-theory is complementary to our approach in several ways, one of which is that it spends a good
deal of energy focusing on questions such as: how many templates are required for accurate
discrimination? how many samples are needed for learning? we plan to pursue these questions
for the drm in future work.

5.3 scattering transform: achieving invariance via wavelets
we have used the drm, with its notion of target and nuisance variables, to explain the power
of dcn for learning selectivity and invariance to nuisance transformations. another theoretical
approach to learning selectivity and invariance is the scattering transform (st) (44,45), which
consists of a series of linear wavelet transforms interleaved by nonlinear modulus-pooling of
the wavelet coef   cients. the goal is to explicitly hand-design invariance to a speci   c set of
nuisance transformations (translations, rotations, scalings, and small deformations) by using
the properties of wavelet transforms.

if we ignore the modulus-pooling for a moment, then the st implicitly assumes that images
can be modeled as linear combinations of pre-determined wavelet templates. thus the st ap-
proach has a maximally strong model bias, in that there is no learning at all. the st performs
well on tasks that are consistent with its strong model bias, i.e., on small datasets for which
successful performance is therefore contingent on strong model bias. however, the st will be
more challenged on dif   cult real-world tasks with complex nuisance structure for which large
datasets are available. this contrasts strongly with the approach presented here and that of the
machine learning community at large, where hand-designed features have been outperformed
by learned features in the vast majority of tasks.

5.4 learning deep architectures via sparsity
what is the optimal machine learning architecture to use for a given task? this question has
typically been answered by exhaustively searching over many different architectures. but is
there a way to learn the optimal architecture directly from the data? arora et al. (46) provide
some of the    rst theoretical results in this direction. in order to retain theoretical tractability,
they assume a simple sparse neural network as the generative model for the data. then, given the
data, they design a greedy learning algorithm that reconstructs the architecture of the generating
neural network, layer-by-layer.

38

they prove that their algorithm is optimal under a certain set of restrictive assumptions.
indeed, as a consequence of these restrictions, their results do not directly apply to the drm or
other plausible generative models of natural images. however, the core message of the paper
has nonetheless been in   uential in the development of the inception architecture (13), which
has recently achieved the highest accuracy on the id163 classi   cation benchmark (25).

how does the sparse reconstruction approach relate to the drm? the drm is indeed also a
sparse generative model: the act of rendering an image is approximated as a sequence of af   ne
transformations applied to an abstract high-level class template. thus, the drm can potentially
be represented as a sparse neural network. another similarity between the two approaches is the
focus on id91 highly correlated activations in the next coarser layer of abstraction. indeed
the drm is a composition of sparse factor analyzers, and so each higher layer (cid:96) + 1 in a dcn
really does decorrelate and cluster the layer (cid:96) below, as quanti   ed by eq. 18.

but despite these high-level similarities, the two approaches differ signi   cantly in their over-
all goals and results. first, our focus has not been on recovering the architectural parameters;
instead we have focused on the class of architectures that are well-suited to the task of factor-
ing out large amounts of nuisance variation. in this sense the goals of the two approaches are
different and complementary. second, we are able to derive the structure of dcns and rdfs
exactly from the drm. this enables us to bring to bear the full power of probabilistic analy-
sis for solving high-nuisance problems; moreover, it will enable us to build better models and
representations for hard tasks by addressing limitations of current approaches in a principled
manner.

5.5 google facenet: learning useful representations with dcns
recently, google developed a new face recognition architecture called facenet (47) that illus-
trates the power of learning good representations. it achieves state-of-the-art accuracy in face
recognition and id91 on several public benchmarks. facenet uses a dcn architecture, but
crucially, it was not trained for classi   cation. instead, it is trained to optimize a novel learning
objective called triplet    nding that learns good representations in general.

the basic idea behind their new representation-based learning objective is to encourage
the dcn   s latent representation to embed images of the same class close to each other while
embedding images of different classes far away from each other, an idea that is similar to the
numax algorithm (48). in other words, the learning objective enforces a well-separatedness
criterion. in light of our work connecting drms to dcns, we will next show how this new
learning objective can be understood from the perspective of the drm.

the correspondence between the drm and the triplet learning objective is simple. since
rendering is a deterministic (or nearly noise-free) function of the global con   guration (c, g), one

39

explanation should dominate for any given input image i = r(c, g), or equivalently, the clus-
ters (c, g) should be well-separated. thus, the noise-free, deterministic, and well-separated
drm are all equivalent.
indeed, we implicitly used the well-separatedness criterion when
we employed the hard em algorithm to establish the correspondence between drms and
dcns/rdfs.

5.6 reid172 theory
given the drm   s notion of irrelevant (nuisance) transformations and multiple levels of abstrac-
tion, we can interpret a dcn   s action as an iterative coarse-graining of an image, thus relating
our work to another recent approach to understanding deep learning that draws upon an analogy
from reid172 theory in physics (49). this approach constructs an exact correspondence
between the restricted id82 (rbm) and block-spin reid172     an iter-
ative coarse-graining technique from physics that compresses a con   guration of binary random
variables (spins) to a smaller con   guration with less variables. the goal is to preserve as much
information about the longer-range correlations as possible, while integrating out shorter-range
   uctuations.

our work here shows that this analogy goes even further as we have created an exact map-
ping between the dcn and the drm, the latter of which can be interpreted as a new real-space
reid172 scheme. indeed, the drm   s main goal is to factor out irrelevant features over
multiple levels of detail, and it thus bears a strong resemblance to the core tenets of renormal-
ization theory. as a result, we believe this will be an important avenue for further research.

5.7 summary of key distinguishing features of the drm
the key features that distinguish the drm approach from others in the literature can be summa-
rized as: (i) the drm explicitly models nuisance variation across multiple levels of abstraction
via a product of af   ne transformations. this factorized linear structure serves dual purposes:
it enables (ii) exact id136 (via the max-sum/max-product algorithm) and (iii) it serves as a
regularizer, preventing over   tting by a novel exponential reduction in the number of parame-
ters. critically, (iv) the id136 is not performed for a single variable of interest but instead
for the full global con   guration. this is justi   ed in low-noise settings, i.e., when the rendering
process is nearly deterministic, and suggests the intriguing possibility that vision is less about
probabilities and more about inverting a complicated (but deterministic) rendering transforma-
tion.

40

6 new directions

we have shown that the drm is a powerful generative model that underlies both dcns and
rdfs, the two most powerful vision paradigms currently employed in machine learning. de-
spite the power of the drm/dcn/rdf, it has limitations, and there is room for improvement.
(since both dcns and rdfs stem from drms, we will loosely refer to them both as dcns in
the following, although technically an rdf corresponds to a kind of tree of dcns.)

in broad terms, most of the limitations of the dcn framework can be traced back to the
fact that it is a discriminative classi   er whose underlying generative model was not known.
without a generative model, many important tasks are very dif   cult or impossible, including
sampling, model re   nement, top-down id136, faster learning, model selection, and learning
from unlabeled data. with a generative model, these tasks become feasible. moreover, the
dcn models rendering as a sequence of af   ne transformations, which severely limits its ability
to capture many important real-world visual phenomena, including    gure-ground segmentation,
occlusion/clutter, and refraction. it also lacks several operations that appear to be fundamental
in the brain: feed-back, dynamics, and 3d geometry. finally, it is unable to learn from unlabeled
data and to generalize from few examples. as a result, dcns require enormous amounts of
labeled data for training.

these limitations can be overcome by designing new deep networks based on new model
structures (extended drms), new message-passing id136 algorithms, and new learning
rules, as summarized in table 2. we now explore these solutions in more detail.

6.1 more realistic rendering models
we can improve dcns by designing better generative models incorporating more realistic as-
sumptions about the rendering process by which latent variables cause images. these assump-
tions should include symmetries of translation, rotation, scaling (44), perspective, and non-rigid
deformations, as rendered by computer graphics and multi-view geometry.

in order to encourage more intrinsic computer graphics-based representations, we can en-
force these symmetries on the parameters during learning (50, 51). initially, we could use local
af   ne approximations to these transformations (52). for example, we could impose weight ty-
ing based on 3d rotations in depth. other nuisance transformations are also of interest, such
as scaling (i.e., motion towards or away from a camera). indeed, scaling-based templates are
already in use by the state-of-the-art dcns such as the inception architectures developed by
google (13), and so this approach has already shown substantial promise.

we can also perform intrinsic transformations directly on 3d scene representations. for ex-
ample, we could train networks with depth maps, in which a subset of channels in input feature
maps encode pixel z-depth. these augmented input features will help de   ne useful higher-level

41

table 2. limitations of current dcns and potential solutions using extended drms.

42

 area problem (dcn) proposed solution (drm) model rendering model applies nuisance transformations to extrinsic representation (2d images or feature maps). modify drm so that rendering applies nuisance transformations to intrinsic representations (e.g. 3d geometry).  difficulty handling occlusion, clutter and classifying objects that are slender, transparent, metallic.  modify drm to include intrinsic computer-graphics-based representations, transformations and phototrealistic rendering.   model is static and thus cannot learn from videos. incorporate time into the drm (dynamic drm). id136 infers most probable global configuration (max-product), ignoring alternative hypotheses. use softer message-passing, i.e. higher temperature or sum-product, to encode uncertainty.  no top-down id136/feedback is possible, so vision tasks involving lower-level variables (e.g. clutter, occlusion, segmentation) are difficult. compute contextual priors as top-down messages for low-level tasks. learning hard-em algorithm and its discriminative relaxation tend to confuse signal and noise use soft-em or id58-em.  nuisance variation makes learning intrinsic latent factors difficult. use dynamic drm with movies to learn that only a few nuisance parameters change per frame.  discriminative models cannot learn from unlabeled data. use drm to do hybrid generative-discriminative learning that simultaneously incorporates labeled, unlabeled, and weakly labeled data.  features for 2d image features, and thereby transfer representational bene   ts even to test im-
ages that do not provide depth information (53). with these richer geometric representations,
learning and id136 algorithms can be modi   ed to account for 3d constraints according to
the equations of multi-view geometry (53).

another important limitation of the dcn is its restriction to static images. there is no notion
of time or dynamics in the corresponding drm model. as a result, dcn training on large-scale
datasets requires millions of images in order to learn the structure of high-dimensional nuisance
variables, resulting in a glacial learning process. in contrast, learning from natural videos should
result in an accelerated learning process, as typically only a few nuisance variables change from
frame to frame. this property should enable substantial acceleration in learning, as id136
about which nuisance variables have changed will be faster and more accurate (54). see sec-
tion 6.3.2 below for more details.

6.2 new id136 algorithms
6.2.1 soft id136

we showed above in section 2.4 that dcns implicitly infer the most probable global interpre-
tation of the scene, via the max-sum algorithm (55). however, there is potentially major com-
ponent missing in this algorithm: max-sum message passing only propagates the most likely
hypothesis to higher levels of abstraction, which may not be the optimal strategy, in general,
especially if uncertainty in the measurements is high (e.g., vision in a fog or at nighttime). con-
sequently, we can consider a wider variety of softer id136 algorithms by de   ning a temper-
ature parameter that enables us to smoothly interpolate between the max-sum and sum-product
algorithms, as well as other message-passing variants such as the approximate id58
em (56). to the best of our knowledge, this notion of a soft dcn is novel.

6.2.2 top-down convolutional nets: top-down id136 via the drm

the dcn id136 algorithm lacks any form of top-down id136 or feedback. performance
on tasks using low-level features is then suboptimal, because higher-level information informs
low-level variables neither for id136 nor for learning. we can solve this problem by using
the drm, since it is a proper generative model and thus enables us to implement top-down
message passing properly.

employing the same steps as outlined in section 2, we can convert the drm into a top-down
dcn, a neural network that implements both the bottom-up and top-down passes of id136
via the max-sum message passing algorithm. this kind of top-down id136 should have a
dramatic impact on scene understanding tasks that require segmentation such as target detection

43

with occlusion and clutter, where local bottom-up hypotheses about features are ambiguous. to
the best of our knowledge, this is the    rst principled approach to de   ning top-down dcns.

6.3 new learning algorithms
6.3.1 derivative-free learning

back propagation is often used in deep learning algorithms due to its simplicity. we have
shown above that back propagation in dcns is actually an inef   cient implementation of an
approximate em algorithm, whose e-step consists of bottom-up id136 and whose m-step
is a id119 step that fails to take advantage of the underlying probabilistic model
(the drm). to the contrary, our above em algorithm (eqs. 18   23) is both much faster and
more accurate, because it directly exploits the drm   s structure. its e-step incorporates bottom-
up and top-down id136, and its m-step is a fast computation of suf   cient statistics (e.g.,
sample counts, means, and covariances). the speed-up in ef   ciency should be substantial, since
generative learning is typically much faster than discriminative learning due to the bias-variance
tradeoff (32); moreover, the em-algorithm is intrinsically more parallelizable (57).

6.3.2 dynamics: learning from video

although deep nns have incorporated time and dynamics for auditory tasks (58   60), dcns
for visual tasks have remained predominantly static (images as opposed to videos) and are
trained on static inputs. latent causes in the natural world tend to change little from frame-
to-frame, such that previous frames serve as partial self-supervision during learning (61). a
dynamic version of the drm would train without external supervision on large quantities of
video data (using the corresponding em algorithm). we can supplement video recordings of
natural dynamic scenes with synthetically rendered videos of objects traveling along smooth
trajectories, which will enable the training to focus on learning key nuisance factors that cause
dif   culty (e.g., occlusion).

6.3.3 training from labeled and unlabeled data

dcns are purely discriminative techniques and thus cannot bene   t from unlabeled data. how-
ever, armed with a generative model we can perform hybrid discriminative-generative train-
ing (31) that enables training to bene   t from both labeled and unlabeled data in a principled
manner. this should dramatically increase the power of pre-training, by encouraging rep-
resentations of the input that have disentangled factors of variation. this hybrid generative-
discriminative learning is achieved by the optimization of a novel objective function for learn-
ing, that relies on both the generative model and its discriminative relaxation. in particular, the

44

learning objective will have terms for both, as described in (31). recall from section 2.7 that
the discriminative relaxation of a generative model is performed by relaxing certain parameter
constraints during learning, according to

max

  

  :  =  (  )

lgen(  ;dci) = max
    max
    max

  

  :  =  (  )

lnat(  ;dci)
lcond(  ;dc|i)

ldis(  ;dc|i),

(35)

where the l   s are the model   s generative, naturally parametrized generative, conditional, and
discriminative likelihoods. here    are the natural parameters expressed as a function of the
traditional parameters   , dci is the training dataset of labels and images, and dc|i is the training
dataset of labels given images. although the discriminative relaxation is optional, it is very
important for achieving high performance in real-world classi   ers as discriminative models
have less model bias and, therefore, are less sensitive to model mis-speci   cations (32). thus,
we will design new principled training algorithms that span the spectrum from discriminative
(e.g., stochastic id119 with back propagation) to generative (e.g., em algorithm).

acknowledgments

thanks to cj barberan for help with the manuscript and to mayank kumar, ali mousavi,
salman asif and andreas tolias for comments and discussions. thanks to karen simonyan
for providing the activity maximization    gure. a special thanks to xaq pitkow whose keen
insight, criticisms and detailed feedback on this work have been instrumental in its develop-
ment. thanks to ruchi kukreja for her unwavering support and her humor and to raina patel
for providing inspiration.

45

a supplemental information
a.1 from the gaussian rendering model classi   er to deep dcns
proposition a.1 (maxout nns). the discriminative relaxation of a noise-free grm classi   er
is a single layer nn consisting of a local template matching operation followed by a piecewise
linear activation function (also known as a maxout nn (10)).
proof. in order to teach the reader, we prove this claim exhaustively. later claims will have
simple proofs that exploit the fact that the rm   s distribution is from the exponential family.

  c(i)     argmax
= argmax

c   c

c   c

p(c|i)
{p(i|c)p(c)}

= argmax

(a)
= argmax

= argmax

(b)
= argmax

c   c (cid:40)(cid:88)h   h
c   c (cid:26)max
c   c (cid:26)max
c   c (cid:40)max
c   c (cid:40)max
c   c (cid:40)max
c   c (cid:26)exp(cid:18)max
c   c (cid:26)max

p(i|c, h)p(c, h)(cid:41)
h   h p(i|c, h)p(c, h)(cid:27)
h   h exp (ln p(i|c, h) + ln p(c, h))(cid:27)
h   h exp(cid:32)(cid:88)  
h   h exp(cid:32)   
h   h exp(cid:32)(cid:88)   (cid:104)w  
h   h {wch (cid:63)lc i}(cid:27)

ch|i   (cid:105) + b  
h   h {wch (cid:63)lc i}(cid:19)(cid:27)

ln p(i   |c, h) + ln p(c, h)(cid:33)(cid:41)
2(cid:88)   (cid:10)i            
ch|     1
ch |i            
ch(cid:33)(cid:41)

1

= argmax

(d)

    argmax

= argmax

(c)
= argmax

= choose{maxoutpool(localtemplatematch(i))}
= maxout-nn(i;   ).

ch(cid:11) + ln p(c, h)    

ln|  ch|(cid:33)(cid:41)

d
2

in line (a), we take the noise-free limit of the grm, which means that one hypothesis (c, h)
dominates all others in likelihood. in line (b), we assume that the image i consists of multi-
ple channels           , that are conditionally independent given the global con   guration (c, h).

46

typically, for input images these are color channels and         {r, g, b} but in general     can
be more abstract (e.g. as in feature maps). in line (c), we assume that the pixel noise covari-
ance is isotropic and conditionally independent given the global con   guration (c, h), so that
x1d is proportional to the d    d identity matrix 1d. in line (d), we de   ned the locally
  ch =   2
connected template matching operator (cid:63)lc , which is a location-dependent template matching
operation.

note that the nuisance variables h     h are (max-)marginalized over, after the application

of a local template matching operation against a set of    lters/templates w     {wch}c   c,h   h
lemma a.2 (translational nuisance    d dcn convolution). the maxout template match-
ing and pooling operation (from proposition a.1) for a set of translational nuisance variables
h     gt reduces to the traditional dcn convolution and max-pooling operation.
proof. let the activation for a single output unit be yc(i). then we have

yc(i)     max
= max

= max

h   h {wch (cid:63)lc i}
g   gt {(cid:104)wcg|i(cid:105)}
g   gt {(cid:104)tgwc|i(cid:105)}
g   gt {(cid:104)wc|t   gi(cid:105)}
g   gt {(wc (cid:63)dcn i)g}
= maxpool(wc (cid:63)dcn i).

= max

= max

finally, vectorizing in c gives us the desired result y(i) = maxpool(w (cid:63)dcn i).
proposition a.3 (max pooling dcns with relu activations). the discriminative relaxation
of a noise-free grm with translational nuisances and random missing data is a single convo-
lutional layer of a traditional dcn. the layer consists of a generalized convolution operation,
followed by a relu activation function and a max-pooling operation.
proof. we will model completely random missing data as a nuisance transformation a     a    
{keep, drop}, where a = keep = 1 leaves the rendered image data untouched, while a =
drop = 0 throws out the entire image after rendering. thus, the switching variable a models
missing data. critically, whether the data is missing is assumed to be completely random and
thus independent of any other task variables, including the measurements (i.e. the image itself).
since the missingness of the evidence is just another nuisance, we can invoke proposition a.1
to conclude that the discriminative relaxation of a noise-free grm with random missing data is
also a maxout-dcn, but with a specialized structure which we now derive.

47

mathematically, we decompose the nuisance variable h     h into two parts h = (g, a)    

h = g    a, and then, following a similar line of reasoning as in proposition a.1, we have

  c(i) = argmax

c   c

max
h   h p(c, h|i)

= argmax

(b)
= argmax

(a)
= argmax

h   h {wch (cid:63)lc i}(cid:27)

c   c (cid:26)max
c   c (cid:26)max
c   c (cid:26)max
c   c (cid:26)max
c   c (cid:26)max

a   a(cid:8)a((cid:104)wcg|i(cid:105) + bcg) + b(cid:48)
g   g max
g   g {max{(wc (cid:63)dcn i)g, 0} + b(cid:48)
cg}(cid:27)
g   g {max{(wc (cid:63)dcn i)g, 0} + b(cid:48)
g   g {max{(wc (cid:63)dcn i)g, 0}}(cid:27)
= choose{maxpool(relu(dcnconv(i)))}
= dcn(i;   ).

(d)
= argmax

(c)
= argmax

cg + b(cid:48)

cg + ba + b(cid:48)

i(cid:9)(cid:27)
i}(cid:27)
drop + b(cid:48)

in line (a) we calculated the log-posterior

ln p(c, h|i) = ln p(c, g, a|i)

= ln p(i|c, g, a) + ln p(c, g, a)
((cid:107)a  cg(cid:107)2
=
cg + ba + b(cid:48)
i,

x(cid:104)a  cg|i(cid:105)    

1
2  2
x

1
2  2

    a((cid:104)wcg|i(cid:105) + bcg) + b(cid:48)
cg     ln p(c, g), b(cid:48)

2 + (cid:107)i(cid:107)2

2)) + ln p(c, g, a)

2  2

x(cid:107)i(cid:107)2

i         1

where a     {0, 1}, ba     ln p(a), b(cid:48)
2. in line (b), we use lemma a.2
to write the expression in terms of the dcn convolution operator, after which we invoke the
identity max{u, v} = max{u     v, 0} + v     relu(u     v) + v for real numbers u, v     r. here
we   ve de   ned b(cid:48)
drop     ln p(a = keep) and we   ve used a slightly modi   ed dcn convolution
operator (cid:63)dcn de   ned by wcg (cid:63)dcn i     wcg (cid:63) i + ln(cid:16) p(a=keep)
p(a=drop)(cid:17). also, we observe that all

the primed constants are independent of a and so can be pulled outside of the maxa. in line(c),
the two primed constants that are also independent of c, g can be dropped due to the argmaxcg.
finally, in line (d), we assume a uniform prior over c, g. the resulting sequence of operations
corresponds exactly to those applied in a single convolutional layer of a traditional dcn.

remark a.4 (the probabilistic origin of the recti   ed linear unit). note the origin of
it compares the relative (log-)likelihood of two hypotheses
the relu in the proof above:

48

a = keep and a = drop, i.e. whether the current measurements (image data i) are avail-
able/relevant/important or instead missing/irrelevant/unimportant for hypothesis (c, g). in this
way, the relu also promotes sparsity in the activations.

a.2 generalizing to arbitrary mixtures of exponential family

distributions

in the last section, we showed that the grm     a mixture of gaussian nuisance classi   ers    
has as its discriminative relaxation a maxout nn. in this section, we generalize this result to an
arbitrary mixture of exponential family nuisance classi   ers. for example, consider a laplacian
rm (lrm) or a poisson rm (prm).

de   nition a.5 (exponential family distributions). a distribution p(x;   ) is in the exponential
family if it can be written in the form

p(x;   ) = h(x) exp((cid:104)  (  )|t (x)(cid:105)     a(  )),

where   (  ) is the vector of natural parameters, t (x)is the vector of suf   cient statistics,
a(  (  )) is the log-partition function.

by generalizing to the exponential family, we will see that derivations of the discriminative
relations will simplify greatly, with the key roles being played by familiar concepts such as nat-
ural parameters, suf   cient statistics and log-partition functions. furthermore, most importantly,
we will see that the resulting discriminative counter parts are still maxout nns. thus maxout
nns are quite a robust class, as most e-family mixtures have maxout nns as d-counterparts.

theorem a.6 (discriminative counterparts to exponential family mixtures are maxout neu-
ral nets). let mg be a nuisance mixture classi   er from the exponential family. then the
discriminative counterpart md of mg is a maxout nn.
proof. the proof is analogous to the proof of proposition a.1, except we generalize by using
the de   nition of an exponential family distribution (above). we simply use the fact that all
exponential family distributions have a natural or canonical form as described above in the
de   nition a.5. thus the natural parameters will serve as generalized weights and biases, while
the suf   cient statistic serves as the generalized input. note that this may require a non-linear
transformation i.e. quadratic or logarithmic, depending on the speci   c exponential family.

a.3 id173 schemes: deriving the dropout algorithm
despite the large amount of labeled data available in many real-world vision applications of
deep dcns, id173 schemes are still a critical part of training, essential for avoiding

49

over   tting the data. the most important such scheme is dropout (30) and it consist of training
with unreliable neurons and synapses. unreliability is modeled by a    dropout    id203 pd
that the neuron will not    re (i.e. output activation is zero) or that the synapse won   t send its
output to the receiving neuron. intuitively, downstream neurons cannot rely on every piece of
data/evidence always being there, and thus are forced to develop a robust set of features. this
prevents the co-adaptation of feature detectors that undermines generalization ability.

in this section, we answer the question: can we derive the dropout algorithm from the
generative modeling perspective? here we show that the answer is yes. dropout can be de-
rived from the grm generative model via the use of the em algorithm under the condition of
(completely random) missing data.

proposition a.7. the discriminative relaxation of a noise-free grm with completely random
missing data is a dropout dcn (18) with max-pooling.

proof. since we have data that is missing completely at random, we can use the em algorithm
to train the grm (56). our strategy is to show that a single iteration of the em-algorithm
corresponds to a full epoch of dropout dcn training (i.e. one pass thru the entire dataset).
note that typically an em-algorithm is used to train generative models; here we utilize the em-
algorithm in a novel way, performing a discriminative relaxation in the m-step. in this way, we
use the generative em algorithm to de   ne a discriminative em algorithm (d-em).

the d-e-step is equivalent to usual generative e-step. given the observed data x and the
current parameter estimate     t, we will compute the posterior of the latent variables z = (h, a)
where a is the missing data indicator matrix i.e. anp = 1 iff the p-th feature (e.g. pixel
intensity) of the input data in (e.g. natural image) is available. h contains all other latent
nuisance variables (e.g. pose) that are important for the classi   cation task. since we assume a
noise-free grm, we will actually execute a hybrid e-step: hard in h and soft in a. the hard-e
step will yield the max-sum message passing algorithm, while the soft e-step will yield the
ensemble average that is the characteristic feature of dropout (18).

in the d-m-step, we will start out by maximizing the complete-data log-likelihood
(cid:96)(  ; h, a, x), just as in the usual generative m-step. however, near the end of the deriva-
tion we will employ a discriminative relaxation that will free us from the rigid distributional
assumptions of the generative model   g and instead leave us with a much more    exible set of
assumptions, as embodied in the discriminative modeling problem for   d.

mathematically, we have a single e-step and m-step that leads to a parameter update as

50

follows:

(cid:96)(    new)     max

= max

= max

= max

   (cid:110)ez|x[(cid:96)(  ; z, x)](cid:111)
   (cid:110)eaeh|x[(cid:96)(  ; h, a, x)](cid:111)
   (cid:110)eaeh|x(cid:104)(cid:96)(  ; c, h|i, a) + (cid:96)(  ; i) + (cid:96)(  ; a)(cid:105)(cid:111)
  d   d  g(cid:110)eaeh|x(cid:104)(cid:96)(  d; c, h|i, a) + (cid:96)(  g; i)(cid:105)(cid:111)
  d (cid:110)eaeh|x(cid:104)(cid:96)(  d; c, h|i, a)(cid:105)(cid:111)
  d (cid:110)eamh|x(cid:104)(cid:96)(  d; c, h|i, a)(cid:105)(cid:111)
|i, a)(cid:105)(cid:111)
  d (cid:110)ea(cid:104)(cid:96)(  d; c, h   
|i, a)(cid:111)
  d (cid:110)(cid:88)a
p(a)    (cid:96)(  d; c, h   
  d (cid:110)(cid:88)a   t
|i, a)(cid:111)
p(a)    (cid:96)(  d; c, h   
  d (cid:110)(cid:88)a   t
p(a)    (cid:88)n   ddropout

;   d)(cid:111).

ln p(cn, h   

n|idropout

ci

n

    max
= max

    max
= max

    max

= max

here we have de   ned the conditional likelihood (cid:96)(  ; d1|d2)     ln p(d1|d2;   ), and d =
(d1, d2) is some partition of the data. this de   nition allows us to write (cid:96)(  ; d) =
(cid:96)(  ; d1|d2) + (cid:96)(  ; d2) by invoking the id155 law p(d|  ) = p(d1|d2;   )   
the symbol mh|x[f (h)]     maxh{p(h|x)f (h)} and the reduced dataset
p(d2|  ).
ddropout
(a) is simply the original dataset of labels and features less the missing data (as speci   ed
ci
by a).

the    nal objective function left for us to optimize is a mixture of exponentially-many dis-
criminative models, each trained on a different random subset of the training data, but all sharing
parameters (weights and biases). since the sum over a is intractable, we approximate the sums
by monte carlo sampling of a (the soft part of the e-step), yielding an ensemble e     {a(i)}.
the resulting optimization corresponds exactly to the dropout algorithm.

51

references and notes
1. j. schmidhuber,    deep learning in neural networks: an overview,    neural networks,

vol. 61, pp. 85   117, 2015.

2. m. d. zeiler and r. fergus,    visualizing and understanding convolutional networks,    in

id161   eccv 2014. springer, 2014, pp. 818   833.

3. a. hannun, c. case, j. casper, b. catanzaro, g. diamos, e. elsen, r. prenger, s. satheesh,
s. sengupta, a. coates et al.,    deepspeech: scaling up end-to-end id103,   
arxiv preprint arxiv:1412.5567, 2014.

4. h. schmid,    part-of-speech tagging with neural networks,    in proceedings of the 15th
conference on computational linguistics - volume 1, ser. coling    94. stroudsburg,
pa, usa: association for computational linguistics, 1994, pp. 172   176. [online].
available: http://dx.doi.org/10.3115/991886.991915

5. a. criminisi and j. shotton, decision forests for id161 and medical image
analysis, ser. advances in id161 and pattern recognition. springer london,
2013. [online]. available: https://books.google.com/books?id=f6a-naeacaaj

6. d. grif   ths and m. tenenbaum,    hierarchical topic models and the nested chinese restau-

rant process,    advances in neural information processing systems, vol. 16, p. 17, 2004.

7. j. h. searcy and j. c. bartlett,    inversion and processing of component and spatial-
relational information in faces.    journal of experimental psychology. human perception
and performance, vol. 22, no. 4, pp. 904   915, aug. 1996.

8. m. i. jordan and t. j. sejnowski, id114: foundations of neural computation.

mit press, 2001.

9. y. bengio, a. courville, and p. vincent,    representation learning: a review and new per-
spectives,    pattern analysis and machine intelligence, ieee transactions on, vol. 35, no. 8,
pp. 1798   1828, 2013.

10. i. j. goodfellow, d. warde-farley, m. mirza, a. courville, and y. bengio,    maxout net-

works,    arxiv preprint arxiv:1302.4389, 2013.

11. a. krizhevsky, i. sutskever, and g. hinton,    id163 classi   cation with deep convolu-

tional neural networks,    nips, pp. 1   9, nov. 2012.

52

12. k. jarrett, k. kavukcuoglu, m. ranzato, and y. lecun,    what is the best multi-stage
architecture for object recognition?    in id161, 2009 ieee 12th international
conference on.

ieee, 2009, pp. 2146   2153.

13. c. szegedy, w. liu, y. jia, p. sermanet, s. reed, d. anguelov, d. erhan, v. vanhoucke,
and a. rabinovich,    going deeper with convolutions,    arxiv preprint arxiv:1409.4842,
2014.

14. y. lecun, l. bottou, y. bengio, and p. haffner,    gradient-based learning applied to docu-

ment recognition,    proceedings of the ieee, vol. 86, no. 11, pp. 2278   2324, 1998.

15. y. taigman, m. yang, m. ranzato, and l. wolf,    deepface: closing the gap to human-level
performance in face veri   cation,    in id161 and pattern recognition (cvpr),
2014 ieee conference on.

ieee, 2014, pp. 1701   1708.

16. j. l  ucke and a.-s. sheikh,    closed-form em for sparse coding and its application to source
separation,    in latent variable analysis and signal separation. springer, 2012, pp. 213   
221.

17. i. goodfellow, a. courville, and y. bengio,    large-scale id171 with spike-and-

slab sparse coding,    arxiv preprint arxiv:1206.6407, 2012.

18. g. e. dahl, t. n. sainath, and g. e. hinton,    improving deep neural networks for lvcsr
using recti   ed linear units and dropout,    in acoustics, speech and signal processing
(icassp), 2013 ieee international conference on.

ieee, 2013, pp. 8609   8613.

19. j. b. tenenbaum, c. kemp, t. l. grif   ths, and n. d. goodman,    how to grow a mind:

statistics, structure, and abstraction,    science, vol. 331, no. 6022, pp. 1279   1285, 2011.

20. y. tang, r. salakhutdinov, and g. hinton,    deep mixtures of factor analysers,    arxiv

preprint arxiv:1206.4635, 2012.

21. a. van den oord and b. schrauwen,    factoring variations in natural images with deep
gaussian mixture models,    in advances in neural information processing systems, 2014,
pp. 3518   3526.

22. z. ghahramani, g. e. hinton et al.,    the em algorithm for mixtures of factor analyzers,   

technical report crg-tr-96-1, university of toronto, tech. rep., 1996.

23. a. hyv  arinen, j. karhunen, and e. oja, independent component analysis.

john wiley &

sons, 2004, vol. 46.

53

24. f. r. kschischang, b. j. frey, and h.-a. loeliger,    factor graphs and the sum-product
algorithm,    id205, ieee transactions on, vol. 47, no. 2, pp. 498   519, 2001.

25. s. ioffe and c. szegedy,    batch id172: accelerating deep network training by

reducing internal covariate shift,    arxiv preprint arxiv:1502.03167, 2015.

26. p. f. felzenszwalb and d. p. huttenlocher,    ef   cient belief propagation for early vision,   

international journal of id161, vol. 70, no. 1, pp. 41   54, 2006.

27. g. hinton,    what   s wrong with convolutional nets?    2014, available from the mit techtv

website.

28. s. roweis and z. ghahramani,    learning nonlinear dynamical systems using the
expectation   maximization algorithm,    kalman    ltering and neural networks, p. 175, 2001.

29. t. v  amos,    judea pearl: probabilistic reasoning in intelligent systems,    decision support

systems, vol. 8, no. 1, pp. 73   75, 1992.

30. g. e. hinton, n. srivastava, a. krizhevsky, i. sutskever, and r. r. salakhutdinov,    im-
proving neural networks by preventing co-adaptation of feature detectors,    arxiv preprint
arxiv:1207.0580, 2012.

31. c. m. bishop, j. lasserre et al.,    generative or discriminative? getting the best of both

worlds,    bayesian statistics, vol. 8, pp. 3   24, 2007.

32. a. jordan,    on discriminative vs. generative classi   ers: a comparison of id28
and naive bayes,    advances in neural information processing systems, vol. 14, p. 841, 2002.
33. b. m. wilamowski, s. iplikci, o. kaynak, and m.   o. efe,    an algorithm for fast conver-
gence in training neural networks,    in proceedings of the international joint conference on
neural networks, vol. 2, 2001, pp. 1778   1782.

34. o. capp  e and e. moulines,    online em algorithm for latent data models,    journal of the

royal statistical society, 2008.

35. m. jordan, learning in id114, ser. adaptive computation and machine
https://books.google.com/books?id=

[online]. available:

london, 1998.

learning.
zac7l4lbntuc

36. d. l. yamins, h. hong, c. f. cadieu, e. a. solomon, d. seibert, and j. j. dicarlo,
   performance-optimized id187 predict neural responses in higher visual cor-
tex,    proceedings of the national academy of sciences, vol. 111, no. 23, pp. 8619   8624,
2014.

54

37. k. simonyan, a. vedaldi, and a. zisserman,    deep inside convolutional networks: visu-
alising image classi   cation models and saliency maps,    arxiv preprint arxiv:1312.6034,
2013.

38. l. breiman,    id79s,    machine learning, vol. 45, no. 1, pp. 5   32, 2001.

39. n.-q. pham, h.-s. le, d.-d. nguyen, and t.-g. ngo,    a study of feature combination in
gesture recognition with kinect,    in knowledge and systems engineering. springer, 2015,
pp. 459   471.

40. n. pinto, d. d. cox, and j. j. dicarlo,    why is real-world visual object recognition

hard?    plos computational biology, vol. 4, no. 1, p. e27, 2008.

41. j. j. dicarlo, d. zoccolan, and n. c. rust,    perspective,    neuron, vol. 73, no. 3, pp. 415   

434, feb. 2012.

42. f. anselmi, j. mutch, and t. poggio,    magic materials,    proceedings of the national

academy of sciences, vol. 104, no. 51, pp. 20 167   20 172, dec. 2007.

43. f. anselmi, l. rosasco, and t. poggio,    on invariance and selectivity in representation

learning,    arxiv preprint arxiv:1503.05938, 2015.

44. j. bruna and s. mallat,    invariant scattering convolution networks,    pattern analysis and

machine intelligence, ieee transactions on, vol. 35, no. 8, pp. 1872   1886, 2013.

45. s. mallat,    group invariant scattering,    communications on pure and applied mathemat-

ics, vol. 65, no. 10, pp. 1331   1398, 2012.

46. s. arora, a. bhaskara, r. ge, and t. ma,    provable bounds for learning some deep repre-

sentations,    arxiv preprint arxiv:1310.6343, 2013.

47. f. schroff, d. kalenichenko, and j. philbin,    facenet: a uni   ed embedding for face recog-

nition and id91,    arxiv preprint arxiv:1503.03832, 2015.

48. c. hegde, a. sankaranarayanan, w. yin, and r. baraniuk,    a convex approach for learning

near-isometric linear embeddings,    preparation, august, 2012.

49. p. mehta and d. j. schwab,    an exact mapping between the variational reid172

group and deep learning,    arxiv preprint arxiv:1410.3831, 2014.

50. x. miao and r. p. rao,    learning the lie groups of visual invariance,    neural computation,

vol. 19, no. 10, pp. 2665   2693, 2007.

55

51. f. anselmi, j. z. leibo, l. rosasco, j. mutch, a. tacchetti, and t. poggio,    unsuper-
vised learning of invariant representations in hierarchical architectures,    arxiv preprint
arxiv:1311.4158, 2013.

52. j. sohl-dickstein, j. c. wang, and b. a. olshausen,    an unsupervised algorithm for learn-

ing lie group transformations,    arxiv preprint arxiv:1001.1027, 2010.

53. r. hartley and a. zisserman, multiple view geometry in id161.

cambridge

university press, 2003.

54. v. michalski, r. memisevic, and k. konda,    modeling sequential data using higher-order

relational features and predictive training,    arxiv preprint arxiv:1402.2333, 2014.

55. j. pearl,    probabilistic reasoning in intelligent systems: networks of plausible id136.

morgan kauffman pub,    1988.

56. c. m. bishop et al., pattern recognition and machine learning.

springer new york, 2006,

vol. 4, no. 4.

57. n. kumar, s. satoor, and i. buck,    fast parallel expectation maximization for gaussian mix-
ture models on gpus using cuda,    in high performance computing and communications,
2009. hpcc   09. 11th ieee international conference on.

ieee, 2009, pp. 103   109.

58. s. hochreiter and j. schmidhuber,    long short-term memory,    neural computation, vol. 9,

no. 8, pp. 1735   1780, 1997.

59. a. graves, a.-r. mohamed, and g. hinton,    id103 with deep recurrent neural
networks,    in acoustics, speech and signal processing (icassp), 2013 ieee international
conference on.

ieee, 2013, pp. 6645   6649.

60. a. graves, n. jaitly, and a.-r. mohamed,    hybrid id103 with deep bidi-
rectional lstm,    in automatic id103 and understanding (asru), 2013 ieee
workshop on.

ieee, 2013, pp. 273   278.

61. l. wiskott,    how does our visual system achieve shift and size invariance,    jl van hemmen

and tj sejnowski, editors, vol. 23, pp. 322   340, 2006.

56

