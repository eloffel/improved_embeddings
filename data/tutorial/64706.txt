5
1
0
2

 

v
o
n
4
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
1
9
7
0

.

1
1
5
1
:
v
i
x
r
a

natural language understanding with

distributed representation

kyunghyun cho

courant institute of mathematical sciences and

center for data science,

new york university

november 26, 2015

abstract

this is a lecture note for the course ds-ga 3001 (cid:104)natural language understanding
with distributed representation(cid:105) at the center for data science1, new york university
in fall, 2015. as the name of the course suggests, this lecture note introduces readers
to a neural network based approach to natural language understanding/processing. in
order to make it as self-contained as possible, i spend much time on describing basics of
machine learning and neural networks, only after which how they are used for natural
languages is introduced. on the language front, i almost solely focus on language
modelling and machine translation, two of which i personally    nd most fascinating
and most fundamental to natural language understanding.

after about a month of lectures and about 40 pages of writing this lecture note, i
found this fascinating note [47] by yoav goldberg on neural network models for natural
language processing. this note deals with wider topics on natural language processing
with distributed representations in more details, and i highly recommend you to read it
(hopefully along with this lecture note.) i seriously wish yoav had written it earlier so
that i could   ve simply used his excellent note for my course.

this lecture note had been written quite hastily as the course progressed, meaning
that i could spare only about 100 hours in total for this note. this is my lame excuse
for likely many mistakes in this lecture note, and i kindly ask for your understanding
in advance. again, how grateful i would   ve been had i found yoav   s note earlier.

i am planning to update this lecture note gradually over time, hoping that i will
be able to convince the center for data science to let me teach the same course next
year. the latest version will always be available both in pdf and in latex source code
from https://github.com/nyu-dl/nlp_dl_lecture_note. the arxiv
version will be updated whenever a major revision is made.

i thank all the students and non-students who took2 this course and david rosen-

berg for feedback.

1 http://cds.nyu.edu/
2 in fact, they are still taking the course as of 24 nov 2015. they have two guest lectures and a    nal exam

left until the end of the course.

contents

1

introduction
1.1 route we will not take

. . .

. . . . . .
. . . . . . . . . . . . .
. . .
1.1.1 what is language? .
. . . . . . . . . . . . . . . . . . . . . .
1.1.2 language understanding . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
1.2.1 language as a function . . . . . . . . . . . . . . . . . . . .
1.2.2 language understanding as a function approximation . . . .

. . . . . .

. . .

. .

1.2 road we will take .

2 function approximation as supervised learning

2.1 function approximation: parametric approach . . . . . . . . . . . .
2.1.1 expected cost function . . . . . . . . . . . . . . . . . . . .
2.1.2 empirical cost function . . . . . . . . . . . . . . . . . . . .
2.2 learning as optimization . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 gradient-based local iterative optimization . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
2.2.2
2.3 when do we stop learning? . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 early stopping . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
2.3.2 model selection . .
2.4 evaluation .
. . . . . . . . . . . . . . . . . . . . . . . . .
2.5 id75 for non-linear functions . . . . . . . . . . . . . .
feature extraction . . . . . . . . . . . . . . . . . . . . . . .

stochastic id119

2.5.1

. . .

.

.

.

5
5
5
6
8
8
8

11
11
11
12
13
13
14
16
16
18
19
20
20

3 neural networks and id26 algorithm

3.1 conditional distribution approximation . . . . . . . . . . . . . . . .
3.1.1 why do we want to do this? . . . . . . . . . . . . . . . . . .
3.1.2 other distributions . . . . . . . . . . . . . . . . . . . . . . .
3.2 feature extraction is also a function . . . . . . . . . . . . . . . . . .
3.3 multilayer id88 . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .

22
22
24
25
25
26
3.3.1 example: binary classi   cation with a single hidden unit
27
3.3.2 example: binary classi   cation with more than one hidden units 29
31
32

3.4 automating id26 . . . . . . . . . . . . . . . . . . . . . .
. . .

3.4.1 what if a function is not differentiable?

. . . . .

. . .

2

4 recurrent neural networks and id149

. . .

. . . .

. . . . . . .

fixed-size output y

4.2 id149

4.1 recurrent neural networks . . . . . . . . . . . . . . . . . . . . . . .
4.1.1
. . . . . . . .
4.1.2 multiple child nodes and derivatives . . . . . . . . . . . . .
4.1.3 example: id31
. . . . . . . . . . . . . . . . .
4.1.4 variable-length output y: |x| = |y|
. . . .
. . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 making simple recurrent neural networks realistic . . . .
.
4.2.2 id149 . . . . . . . . . . . . . . . . . . . . .
4.2.3 long short-term memory . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
4.3.1 recti   ers explode .
. . . . . . . . . . . . . . . . . . . . . .
4.3.2
is tanh a blessing? .
4.3.3 are we doomed? . .
. . . . . . . . . . . . . . . . . . . . . .
4.3.4 id149 address vanishing gradient . . . . . .

4.3 why not recti   ers? .

. . . . .

. . . . .

.

5 neural language models

5.1 id38: first step . . . . . . . . . . . . . . . . . . . . .
5.1.1 what if those linguistic structures do exist . . . . . . . . . . .
5.1.2 quick note on linguistic units
. . . . . . . . . . . . . . . .
5.2 statistical id38 . . . . . . . . . . . . . . . . . . . . .
5.2.1 data sparsity/scarcity . . . . . . . . . . . . . . . . . . . . .
id165 language model . .
. . . . . . . . . . . . . . . . . . . . . .
5.3.1
smoothing and back-off . . . . . . . . . . . . . . . . . . . .
5.3.2 lack of generalization . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
5.4.1 how does neural language model generalize to unseen n-
grams?     distributional hypothesis . . . . . . . . . . . . . .

5.4 neural language model

5.3

5.4.2 continuous bag-of-words language model:

5.4.3

maximum pseudo   likelihood approach . . . . . . . . . . .
semi-supervised learning with pretrained id27s
5.5 recurrent language model . . . . . . . . . . . . . . . . . . . . . . .
5.6 how do id165 language model, neural language model and id56-lm
. . . . . . . . . . . . . . . . . . . . . . .

compare? .

. . .

.

.

.

.

.

6 id4

6.1 statistical approach to machine translation . . . . . . . . . . . . . .
6.1.1
parallel corpora: training data for machine translation . . .
6.1.2 automatic evaluation metric . . . . . . . . . . . . . . . . . .

6.2 id4:

simple encoder-decoder model
6.2.1

. . . . . . . . . . . . . . . . . . . .
sampling vs. decoding . . . . . . . . . . . . . . . . . . . . .
6.3 attention-based id4 . . . . . . . . . . . . . .
. . . . . . . . . . .
6.4 warren weaver   s memorandum . . . . . . . . . . . . . . . . . . . .

6.3.1 what does the attention mechanism do?

35
35
37
38
39
40
43
43
44
46
47
47
49
52
53

55
55
56
57
58
59
61
62
65
66

68

71
74
76

79

82
82
84
87

90
91
97
101
103

3

7 final words

7.1 multimedia description generation as translation . . . . . . . . . . .
7.2 language understanding with world knowledge . . . . . . . . . . .
7.3 larger-context language understanding:

beyond sentences and beyond words . . . . . . . . . . . . . . . . .
7.4 warning and summary . . . . . . . . . . . . . . . . . . . . . . . . .

107
107
109

112
113

4

chapter 1

introduction

this lecture is going to be the only one where i discuss some philosophical, meaning
nonpractical, arguments, because according to chris manning and hinrich schuetze,
   even practically-minded people have to confront the issue of what prior knowledge to
try to build into their model    [77].

1.1 route we will not take
1.1.1 what is language?
the very    rst question we must ask ourselves before starting this course is the ques-
tion of what natural language is. of course, the rest of this course does not in any
way require us to know what natural language is, but it is a philosophical question i
recommend everyone, including myself, to ponder upon once a while.

when i start talking about languages with anyone, there is a single person who
never misses to be mentioned, that is noam chomsky. his view has greatly in   uenced
the modern linguistics, and although many linguists i have talked to claim that their
work and    eld have long moved on from chomsky   s, i can feel his shadow all over
them.

my    rst encounter with chomsky was at the classroom of <automata> from my
early undergrad years. i was not the most attentive student back then, and all i can
remember is chomsky   s hierarchy and how it has shaped our view on languages, in this
context, programming/computer languages. a large part of the course was dedicated
to explaining which class of languages emerges given a set of constraints on a set of
generating rules, or production rules.

for instance, if we are given a set of generating rules that do not depend on the con-
text/meaning of non-terminal symbols (context-free grammar, id18), we get a context-
free language. if we put a bit of constraints to id18 that each generating rule is such
that a non-terminal symbol is replaced by either a terminal symbol, a terminal symbol
by a non-terminal symbol or an empty symbol, then we get a regular grammar. sim-
ilarly to id18, we get a regular language from the regular grammar, and the regular

5

language is a subset of the context-free language.

what chomsky believes is that this kind of approach applies also to human lan-
guages, or natural languages. there exists a set of generating rules that generates a
natural language. but, then, the obvious question to follow is where those generating
rules are. where are they stored? how are they stored? do we have separate generating
rules for different languages?

1.1.2 language understanding
understanding human language those questions are interesting, but out of scope
for this course. those questions are the ones linguists try to answer. generative linguis-
tics aims at    guring out what those rules are, how they are combined to form a valid
sentence, how they are adapted to different languages and so on. we will leave these to
linguists and continue on to our journey of building a machine that understands human
languages.

natural language understanding so, let   s put these questions aside and trust chom-
sky that we, humans, are specially designed to store those generating rules somewhere
in the brain [30, 21]. or, better yet, let   s trust chomsky that there   s a universal gram-
mar built in our brain. in other words, let   s say we were born with this set of generating
rules for natural languages, and while growing, we have adapted this universal gram-
mar toward our native tongue (language variation).

when we decide to speak of something (whatever that is and however implausi-
ble that is), our brain quickly picks up a sequence of some of those generating rules
and starts generating a sentence accordingly. of course, those rules do not generate a
sentence directly, but generates a sequence of control signals to move our muscles to
make sound. when heard by other people who understand your language, the sound
becomes a sentence.

in our case, we are more interested in a machine hearing that sound, or a sentence
from here on. when a machine heard this sentence, what would/should a language un-
derstanding machine do to understand a language, or more simply a sentence? again,
we are assuming that this sentence was generated from applying a sequence of the
existing generating rules.

under our assumption, a natural    rst step that comes to my mind is to    gure out
that sequence of the generating rules which led to the sentence. once the sequence is
found, or in a fancier term, inferred, the next step will be to    gure out what kind of
mental state of the speaker led to those generating rules.

let   s take an example sentence    our company is training workers    (from sec. 1.3
of [77]), which is a horrible choice, because this was used as an example of ambiguity
in parsing. regardless, a speaker obviously has an awesome image of her company
which trains its workers and wants to tell a machine about this. this mental state is
used to select the following generating rules (assuming a phrase structure grammar)1:

(root

1 stanford parser: http://nlp.stanford.edu:8080/parser

6

(s

(np (prp$ our) (nn company))

(vp (vbz is)

(vp (vbg training)

(np (nns workers))))))

figure 1.1: a parse of    our company is training workers   

the machine hears the sentence    our company is training workers    and infers
the parse in fig. 1.1. then, we can make a simple set of rules (again!)
to let the
machine answer questions about this sentence, kinds of questions that imply that the
machine has understood the sentence (language). for instance, given a question    who
is training workers?   , the machine can answer by noticing that the question is asking
for the subject of the verb phrase    is training    acted on the object    workers    and that
the subject is    our company   .

side note: bayesian language understanding this generative view of languages
   ts quite well with bayesian modelling (see, e.g., [84].) there exists a hidden mecha-
nism, or a set of generating rules and a rule governing their composition, which can be
modelled as a latent variable z. given these rules, a language or a sentence x is gen-
erated according to the conditional distribution p(x|z). then, understanding language
(by humans) is equivalent to computing the posterior distribution over all possible sets
of generating rules and their compositional rules (i.e., p(z|x).) this answers the ques-
tion of what is the most likely mechanism underlying the observed language.

furthermore, from the perspective of machines, bayesian approach is attractive. in
this case, we assume to know the set of rules in advance and let the latent variable z
denote the speci   c con   guration (use) of those rules. given this sequence of applying
the rules, a sentence x is generated via the conditional distribution p(x|z). machine
understanding of language is equivalent to inferring the posterior distribution over z
given x.

for more details about bayesian approaches (in the context of machine learning),
please, refer to [13] or take the course ds-ga 1005 id136 and representation by
prof. david sontag.

7

pa1.3theambiguityoflanguage:whynlpisdi   cult17philosophically,thisbringsusclosetothepositionadoptedinthelaterwritingsofwittgenstein(thatis,wittgenstein1968),wherethemean-ingofawordisde   nedbythecircumstancesofitsuse(ausetheoryofusetheoryofmeaningmeaning)   seethequotationsatthebeginningofthechapter.underthisconception,muchofstatisticalnlpresearchdirectlytacklesquestionsofmeaning.1.3theambiguityoflanguage:whynlpisdi   cultannlpsystemneedstodeterminesomethingofthestructureoftext   normallyatleastenoughthatitcananswer   whodidwhattowhom?   conventionalparsingsystemstrytoanswerthisquestiononlyintermsofpossiblestructuresthatcouldbedeemedgrammaticalforsomechoiceofwordsofacertaincategory.forexample,givenareasonablegrammar,astandardnlpsystemwillsaythatsentence(1.10)has3syntacticanal-yses,oftencalledparses:(1.10)ourcompanyistrainingworkers.thethreedi   eringparsesmightberepresentedasin(1.11):(1.11)a.snpourcompanyvpauxisvpvtrainingnpworkersb.snpourcompanyvpvisnpvpvtrainingnpworkersunderstanding vs. using what   s clear from this example is that in this generative
view of languages, there is a clear separation between understanding and using. in-
ferring the generating rules from a given sentence is understanding, and answering a
question based on this understanding, using, is a separate activity. understanding part
is done when the underlying (true) structure has been determined regardless of how
this understanding be used.

to put it in a slightly different wording, language understanding does not require its
use, or downstream tasks. in this road that we will not take in this course, understanding
exists as it is, regardless of what the understood insight/knowledge will be used for.
and, this is the reason why we do not walk down this road.

1.2 road we will take
1.2.1 language as a function
in this course, we will view a natural/human language as    a system intended to com-
municate ideas from a speaker to a hearer    [110]. what this means is that we do not
view a language as a separate entity that exists on its own. rather, we view a whole
system or behaviour of communication as a language. furthermore, this view dictates
that we must take into account the world surrounding a speaker and a hearer in order
to understand language.

under this view of language, language or rather its usage become somewhat similar
to action or behaviour. speaking of something is equivalent to acting on a listener, as
both of them in   uence the listener in one way or another. the purpose of language
is then to in   uence another by ef   ciently communicate one   s will or intention.2 this
hints at how language came to be (or may have come to be): (evolution) language
has evolved to facilitate the exchange of ideas among people (learning) humans learn
language by being either encouraged or punished for the use of language. this latter
view on how language came to be is similar in spirit to the behaviourism of b. f.
skinner (   necessary mediation of reinforcement by another organism    [97].)

this is a radical departure from the generative view of human language, where
language existed on its own and its understanding does not necessarily require the
existence of the outside world nor the existence of a listener.
it is no wonder why
chomsky was so harsh in criticizing skinner   s work in [30]. this departure, as i see
it, is the departure toward a functional view of language. language is a function of
communication.

1.2.2 language understanding as a function approximation
let   s make a large jump here such that we consider this function as a mathematical
function. this function (called language) takes as input the state of the surrounding
world, the speaker   s speech, either written, spoken or signed and the listener   s mental

2 chomsky does not agree:    it is wrong to think of human use of language as characteristically informa-

tive, in fact or in intention.    [31].

8

state3 inside the function, the listener   s mental state is updated to incorporate the new
idea from the speaker   s speech. the function then returns a response by the listener
(which may include    no response    as well) and a set of non-verbal action sequences
(what would be the action sequence if the speaker insulted the listener?).

in this case, language understanding, both from humans    and machines    perspec-
tive, boils down to    guring out the internal working of this function. in other words, we
understand language by learning the internal mechanism of the function. furthermore,
this view suggests that the underlying structures of language are heavily dependent on
the surrounding environment (context) as well as on the target task. the former (con-
text dependence) is quite clear, as the function takes as input the context, but the latter
may be confusing now. hopefully, this will become clearer later in the course.

how can we approximate this function? how can we    gure out the internal working

mechanism of this function? what tools do we have?

language understanding by machine learning this functional view of languages
suddenly makes machine learning a very appealing tool for understanding human lan-
guages. after all, function approximation is the core of machine learning. classi   ca-
tion is a classical example of function approximation, id91 is a function approxi-
mation where the target is not given, generative modeling learns a function that returns
a id203 of an input, and so on.

when we approximate a function in machine learning, the prime ingredient is data.
we are given data which was either generated from this function (unsupervised learn-
ing) or well    t this function (supervised learning), based on which we adjust our ap-
proximation to the function, often iteratively, to best    t the data. but, i must note here
that it does not matter how well the approximated function    ts the data it was    tted to,
but matters how well this approximation    ts unseen data.4

in language understanding, this means that we collect a large data set of input and
output pairs (or conversations together with the recording of the surrounding environ-
ment) and    t some arbitrary function to well predict the output given an input. we
probably want to evaluate this approximation in a novel conversation. if this function
makes a conversation just like a person, voil`a, we made a machine that passed the
turing test. simple, right?

problem unfortunately, as soon as we try to do this, we run into a big problem. this
problem is not from machine learning nor languages, but the de   nition of this function
of language.

properly approximating this function requires us to either simulate or record the
whole world (in fact, the whole universe.) for, this function takes as input and main-
tains as internal state the surrounding world (context) and the mental state of the in-
dividual (speaker.) this is unavoidable, if we wanted to very well approximate this
function as a whole.

it is unclear, however, whether we want to approximate the full function. for a
human to survive, yes, it is likely that the full function is needed. but, if our goal is

3 we assume here that a such thing exists however it is represented in our brain.
4 this is a matter of generalization, and we will talk about this more throughout the course.

9

restricted to a certain task (such as translation, language modelling, and so on), we may
not want to approximate this function fully. we probably want to approximate only a
subset of this whole function. for instance, if our goal is to understand the process
of translation from one language to another, we can perhaps ignore all but the speech
input to the function and all but the speech output from the function, because often a
(trained) person can translate a sentence in one language to another without knowing
the whole context.

this latter approach to language understanding   approximating a partial function
of languages    will be at the core of this course. we will talk about various language
tasks that are a part of this whole function of language. these tasks will include, but
are not limited to, language modelling, machine translation, image/video description
generation and id53. for these tasks and potentially more, we will study
how to use machine learning, or more speci   cally deep learning, to solve these tasks
by approximating sub-functions of language.

10

chapter 2

function approximation as
supervised learning

throughout this course, we will extensively use arti   cial neural networks1 to approx-
imate (a part of) the function of natural language. this makes it necessary for us to
study the basics of neural networks    rst, and this lecture and a couple of subsequent
ones are designed to serve this purpose.

2.1 function approximation: parametric approach
2.1.1 expected cost function
let us start by de   ning a data distribution pdata. pdata is de   ned over a pair of input
and output vectors, x     id and y     ok, respectively. i and o are respectively sets of
all possible input and output values, such as r, {0,1} and {0,1, . . . ,l}. this data
distribution is not known to us.
the goal is to    nd a relationship between x and y. more speci   cally, we are in-
terested in    nding a function f : rd     ok that generates the output y given its corre-
sponding input x. the very    rst thing we should do is to put some constraints on the
function f to make our search for the correct f a bit less impossible. in this lecture,
and throughout the course, i will consider only a parametric function f , in which case
the function is fully speci   ed with a set of parameters   .
next, we must de   ne a way to measure how well the function f approximates
the underlying mechanism of generation (x     y). let   s denote by   y the output of the
function with a particular set    of parameters and a given input x:

1 from here on, i will simply drop arti   cial and call them neural networks. whenever i say    neural

network   , it refers to arti   cial neural networks.

  y = f   (x)

11

how well f approximates the true generating function is equivalent to how far   y is from
the correct output y. let   s use d(  y,y) for now call this distance2 between   y and y
it is clear that we want to    nd    that minimizes d(  y,y) for every pair in the space
(rrd   ok). but, wait, every pair equally likely? probably not, for we do not care how
well f   approximates the true function, when a pair of input x and output y is unlikely,
meaning we do not care how bad the approximation is, if pdata(x,y) is small. however,
this is a bit dif   cult to take into account, as we must decided on the threshold below
which we consider any pair irrelevant.

hence, we weight the distance between the approximated   y and the correct y of
each pair (x,y) in the space by its id203 p(x,y). mathematically saying, we want
to    nd

where the integral (cid:82) should be replaced with the summation     if any of x and y is

pdata(x,y)d(  y,y)dxdy,

argmin

  

x

y

(cid:90)

(cid:90)

discrete.

we call this quantity being minimized with respect to the parameters    a cost func-
tion c(   ). this is equivalent to computing the expected distance between the predicted
output   y and the correct one y:

c(   ) =

pdata(x,y)d(  y,y)dxdy,

(cid:90)

(cid:90)
=e(x,y)   pdata [d(  y,y)]

x

y

(2.1)

(2.2)

this is often called an expected loss or risk, and minimizing this cost function is re-
ferred to as expected risk minimization [105].

unfortunately c(   ) cannot be (exactly) computed for a number of reasons. the
most important reason among them is simply that we don   t know what the data distri-
bution pdata is. even if we have access to pdata, we can exactly compute c(   ) only with
heavy assumptions on both the data distribution and the distance function.3

2.1.2 empirical cost function
this does not mean that we are doomed from the beginning. instead of the full-blown
description of the data distribution pdata, we will assume that someone miraculously
gave us a    nite set of pairs drawn from the data distribution. we will call this a training
set:

(cid:8)(x1,y1), . . . , (xn,yn)(cid:9) .

as we have access to the samples from the data distribution, we can use monte

carlo method to approximate the expected cost function c(   ) such that

c(   )       c(   ) =

1
n

n

   

n=1

d(   yn,yn).

(2.3)

2 note that we do not require this distance to satisfy the triangular inequality, meaning that it does not

have to be a distance. however, i will just call it distance for now.

3why?

12

we call this approximate   c(   ) of the expected cost function, an empirical cost function
(or empirical risk or empirical loss.)

because empirical cost function is readily computable, we will mainly work with
the empirical cost function not with the expected cost function. however, keep in mind
that at the end of the day, the goal is to    nd a set of parameters that minimizes the
expected cost.

2.2 learning as optimization

we often call this process of    nding a good set of parameters that minimizes the ex-
pected cost learning. this term is used from the perspective of a machine which imple-
ments the function f   , as it learns to approximate the true generating function f from
training data.

from what i have described so far, it may have become clear even without me men-
tioning that learning is optimization. we have a clearly de   ned function (the empirical
cost function   c) which needs to be minimized with respect to its input   .

2.2.1 gradient-based local iterative optimization
there are many optimization algorithms one can use to    nd a set of parameters that
minimizes   c. sometimes, you can even    nd the optimal set of parameters in a closed
form equation.4 in most cases, because there is no known closed-form solution, it is
typical to use an iterative optimization algorithm (see [42] for in-depth discussion on
optimization.)

by an iterative optimization, i mean an algorithm which re   nes its estimate of the
optimal set of parameters little by little until the values of the parameters converge to
the optimal (expected) cost function. also, it is worthwhile to note that most iterative
optimization algorithms are local, in the sense that they do not require us to evaluate
the whole parameter space, but only a small subset along the path from the starting
point to the convergence point.5

here i will describe the simplest one among those local iterative optimization algo-
rithms, called id119 (gd) algorithm. as the name suggests, this algorithm
depends entirely on the gradient of the cost function.6

4 one such example is a id75 where
    f   ={w}(x) = wx
    d(  y,y) = 1

2(cid:107)  y    y(cid:107)2
in this case, the optimal w is

where

w = yx(cid:62)(xx(cid:62))   1,

x =(cid:2)x1; . . .;xn(cid:3) ,y =(cid:2)y1; . . .;yn(cid:3) .

(2.4)

try it yourself!
5 there are global optimization algorithms, but they are out of scope for this course. see, for instance,

[18] for one such algorithm called bayesian optimization.

6 from here on, i will use the cost function to refer to the empirical cost function.

13

(blue)

figure 2.1:
sin(10x) + x.
x =    0.6.
gradient at x =    0.6.

f (x) =
(red) a gradient at
(magenta) a negative

the gradient of a function       c is a vector whose direction points to the direction of
the greatest rate of increase in the function   s value and whose magnitude measures this
rate. at each point    t in the parameter space, the gradient of the cost function       c(   t )
is the opposite direction toward which we want to move the parameters. see fig. 2.1
for graphical illustration.

one important point of gd that needs to be mentioned here is on how large a
step one takes each time. as clear from the magenta line (the direction opposite to
the direction given by the gradient) in fig. 2.1, if too large a step is taken toward the
negative gradient direction, the optimization process will overshoot and miss the (local)
minimum around x =    0.8. this step size, or sometimes called learning rate,    is one
most important hyperparameter of the gd algorithm.

now we have all the ingredients for the gd algorithm:       c and   . the gd algo-

rithm iterates the following step:

                      c(   ).

(2.5)

the iteration continues until a certain stopping criterion is met, which we will discuss
shortly.

2.2.2 stochastic id119
this simple gd algorithm works surprisingly quite well, and it is a fundamental basis
upon which many advanced optimization algorithms have been built. i will present a
list of few of those advanced algorithms later on and discuss them brie   y. but, before
going into those advanced algorithms, let   s solve one tiny, but signi   cant issue of the
gd algorithm.

this tiny, but signi   cant issue arises especially often in machine learning. that is,
it is computationally very expensive to compute   c and consequently its gradient       c,
thanks to the ever increasing size of the training set d.

why is the growing size of the training set making it more and more computation-
ally demanding to compute   c and       c? this is because both of them are essentially

14

the sum of as many per-sample costs as there are examples in the training set. in other
words,

  c(   ) =

1
n

      c(   ) =

n

   

n=1
n
1
n

   

n=1

  c(xn,yn|   ),

      c(xn,yn|   ).

and, n goes up to millions or billions very easily these days.

this enormous computational cost involved in each gd step has motivated the

stochastic id119 (sgd) algorithm [88, 15].

first, recall from eq. (2.3) that the cost function we minimize is the empirical
cost function   c which is the sample-based approximation to the expected cost function
c. this approximation was done by assuming that the training examples were drawn
randomly from the data distribution pdata:

c(   )       c(   ) =

1
n

n

   

n=1

d(   yn,yn).

in fact, as long as this assumption on the training set holds, we can always approximate
the expected cost function with a fewer number of training examples:

c(   )       cm (   ) =

1
|m|    
m   m

d(   ym,ym),

where m (cid:28) n and m is the indices of the examples in this much smaller subset of the
training set. we call this small subset a minibatch.

similarly, this leads to a minibatch-based estimate of the gradient as well:

      cm (   ) =

1
|m|    
m   m

   d(   ym,ym).

it must now be clear to you where i am headed toward. at each gd step, instead
of using the full training set, we will use a small subset m which is randomly selected
to compute the gradient estimate. in other words, we use   cm instead of   c, and       cm
instead of       c, in eq. (2.5).

because computing   cm and       cm is independent of the size of the training set, we
can use sgd to make as many steps as we want without worrying about the growing
size of training examples. this is highly bene   cial, as regardless of how many train-
ing examples you used to compute the gradient, we can only take a tiny step toward
that descending direction. furthermore, the increased level of noisy in the gradient
estimate due to the small sample size has been suspected to help reaching a better so-
lution in high-dimensional non-convex problems (such as those in training deep neural
networks) [71].7

7 why would this be the case? it is worth thinking about this issue further.

15

we can set m to be any constant, and in an extreme, we can set it to 1 as well. in
this case, we call it online sgd.8 surprisingly, already in 1951, it was shown that using
a single example each time is enough for the sgd to converge to a minimum (under
certain conditions, obviously) [88].

this sgd algorithm will be at the core of this course and will be discussed further

in the future lectures.

2.3 when do we stop learning?

from here on, i assume that we approximate the ground truth function by iteratively
re   ning its set of parameters, in most cases using stochastic id119. in other
words, learning of a machine that approximates the true generating function f happens
gradually as the machine goes over the training examples little by little over time.

let us go over again what kind of constraints/issue we have    rst:

1. lack of access to the expected cost function c(   )
2. computationally expensive empirical cost function   c(   )
3. (potential) non-convexity of the empirical cost function   c(   )
the most severe issue is that we do not have access to the expected cost function
which is the one we want to minimize in order to work well with any pair of input x
and output y. instead, we have access to the empirical cost function which is a    nite
sample approximation to the expected cost function.

why is this a problem? because, we do not have a guarantee that the (local) mini-
mum of the empirical cost function corresponds to the (local) minimum of the expected
cost function. an example of this mismatch between the expected and empirical cost
functions is shown in fig. 2.2.

as in the case shown in fig. 2.2, it is not desirable to minimize the empirical cost
function perfectly. the parameters that perfectly minimize the empirical cost function
(in the case of fig. 2.2, the slope a of a linear function f (x) = ax) will likely be a
sub-optimal cost for the expected cost function about which we really care.

2.3.1 early stopping
what should we do? there are many ways to avoid this weird contradiction where
we want to optimize the cost function well but not too well. among those, one most
important trick is early stopping, which is only applicable when iterative optimization
is used.

first, we will split the training set d into two partitions dtrain and dval.9 we call
them a training set and a validation set, respectively. in practice it is a good idea to
keep d much larger than d(cid:48), because of the reasons that will become clear shortly.

8 okay, this is not true in a strict sense. sgd is an online algorithm with m = 1 originally, and using
m > 1, is a variant of sgd, often called, minibatch sgd. however, as using minibatches (m > 1) is almost
always the case in practice, i will refer to minibatch sgd as sgd, and to the original sgd as online sgd.

9 later on, we will split it further into three partitions.

16

figure 2.2: (blue) expected cost
function c(   ).
(red) empirical
cost function   c(   ).
the un-
derlying true generating function
was f (x) = sin(10x) + x. the
cost function uses the squared eu-
clidean distance.
the empiri-
cal cost function was computed
based on 10 noisy examples of
which x   s were sampled from the
uniform distribution between 0
and 1. for each sample input x,
noise from zero-mean gaussian
distribution with standard devia-
tion 0.01 was added to f (x) to
emulate the noisy measurement
channel.

further, let us de   ne the training cost as

  c(   ) = ctrain(   ) =

1
|dtrain|    

(x,y)   dtrain

dtrain(   y,y),

(2.6)

and the validation cost as

cval(   ) =

1
|dval|    
(x,y)   dval

d(   y,y).

(2.7)

with these two cost functions we are all ready to use early stopping now.

after every few updates using sgd (or gd), the validation cost function is evalu-
ated with the current set of parameters. the parameters are updated (i.e., the training
cost function is optimized) until the validation cost does not decrease, or starts to in-
crease instead of decreasing.

that   s it! it is almost free, as long as the size of the validation set is reasonable,
since each evaluation is at most as expensive as computing the gradient of the empirical
cost function. because of the simplicity and effectiveness, this early stopping strategy
has become de facto standard in deep learning and in general machine learning.

the question that needs to be asked here is what the validation cost function does
here. clearly, it approximates the expected cost function c, similarly to the empirical
cost function   c as well as the training cost function ctrain. in the in   nite limit of the
size of either training or validation set, they should coincide, but in the case of a    nite
set, those two cost functions differ by the noise in sampling (sampling pairs from the
data distribution) and observation (noise in y = f (x).)

the fact that we explicitly optimize the training cost function implies that there is
a possibility (in fact, almost surely in practice) that the set of parameters found by this
optimization process may capture not only the underlying generating function but also

17

noise in the observation and sampling procedure. this is an issue, because we want our
machine to approximate the true generating function not the noise process involved.

the validation cost function measures both the true generating structure as well as
noise injected during sampling and observation. however, assuming that noise is not
correlated with the underlying generating function, noise introduced in the validation
cost function differs from that in the training cost function. in other words, the set
of parameters that perfectly minimizes the training cost function (thereby capturing
even noise in the training set) will be penalized when measured by the validation cost
function.

2.3.2 model selection
in fact, the use of the validation cost does not stop at the early stopping. rather, it has a
more general role in model selection. first, we must talk about model selection itself.
this whole procedure of optimization, or learning, can be cast as a process of
searching for the best hypothesis over the entire space h of hypotheses. here, each
hypothesis corresponds to each possible function (with a unique set of parameters and
a unique functional form) that takes the input x and output y. in the case of regression
(x     rd and y     r), the hypothesis space includes an n-th order polynomial function

f (x) =

   

k=1 ik=n,ik   0
   d

ai1,i2,...,ik

d

   
k(cid:48)=1

xik
k(cid:48),

where ai1,i2,...,ik   s are the coef   cients, and any other functional form that you can imag-
ine as long as it can process x and return a real-valued scalar. in the case of neural
networks, this space includes all the possible model architectures which are de   ned by
the number of layers, the type of nonlinearities, the number of hidden units in each
layer and so on.
let us use m     h to denote one hypothesis.10 one important thing to remember is
that the parameter space is only a subset of the hypothesis space, because the parameter
space is de   ned by a family of hypotheses (the parameter space of a linear function
cannot include a set of parameters for a second-order polynomial function.)

given a de   nition of expected cost function, we can score each hypothesis m by
the corresponding cost cm. then, the whole goal of function approximation boils down
to the search for a hypothesis m with the minimal expected cost function c. but, of
course, we do not have access to the expected cost function and resort to the empirical
cost function based on a given training set.

the optimization-based approach we discussed so far searches for the best hypoth-
esis based on the empirical cost iteratively. however, because of the issue of over   tting
which means that the optimization algorithm overshot and missed the local minimum
of the expected cost function (because it was aimed at the local minimum of the empir-
ical cost function), i introduced the concept of early stopping based on the validation
cost.

10 m, because each hypothesis corresponds to one learning machine.

18

this is unfortunately not satisfactory, as we have only searched for the best hypoth-
esis inside a small subset of the whole hypothesis space h . what if another subset
of the hypothesis space includes a function that better suits the underlying generating
function f ? are we doomed?

it is clearly better to try more than one subsets of the hypothesis space. for in-
stance, for a regression task, we can try linear functions (h1), quadratic (second-order
polynomial) functions (h2) and sinusoidal functions (h3). let   s say for each of these
subsets, we found the best hypothesis (using iterative optimization and early stopping);
mh1, mh2 and mh3. then, the question is how we should choose one of those hy-
potheses.

similar to what we   ve done with early stopping, we can use the validation cost to
compare these hypotheses. among those three we choose one that has the smallest
validation cost cval(m).

this is one way to do model selection, and we will talk about another way to do

this later.

2.4 evaluation
but, wait, if this is an argument for using the validation cost to early stop the optimiza-
tion (or learning), one needs to notice something weird. what is it?

because we used the validation cost to stop the optimization, there is a chance
that the set of parameters we found is optimal for the validation set (whose structure
consists of both the true generating function and sampling/observation noise), but not
to the general data distribution. this means that we cannot tell whether the function
estimate   f approximating the true generating function f is a good    t by simply early
stopping based on the validation cost. once the optimization is done, we need yet
another metric to see how well the learned function estimate   f approximates f .

therefore, we need to split the training set not into two partitions but into three
partitions. we call them a training set dtrain, a validation set dval and a test set dtest.
consequently, we will have three cost functions; a training cost function ctrain, a vali-
dation cost function cval and a test cost function ctest, similarly to eqs. 2.6   2.7.

this test cost function is the one we use to compare different hypotheses, or models,
fairly. any hypothesis that worked best in terms of the test cost is the one that you
choose.

let   s not cheat one most important lesson here is that you must never look at a test
set. as soon as you take a peak at the test set, it will in   uence your choice in the model
structure as well as any other hyperparameters biasing toward a better test cost. the
best option is to never ever look at the test set until it is absolutely needed (e.g., need
to present your result.)

19

2.5 id75 for non-linear functions

let us start with a simple linear function to approximate a true generating function such
that

  y = f (x) = w(cid:62)x,

where w     rd  l is the weight matrix.
parameter, i.e.,    = {w}.

the empirical cost function is then

  c(   ) =

1
n

n

   

n=1

the gradient of the empirical cost function is

in this case, this weight matrix is the only

1
2

(cid:13)(cid:13)(cid:13)yn     w(cid:62)xn(cid:13)(cid:13)(cid:13)2
(cid:16)
yn     w(cid:62)xn(cid:17)(cid:62)

2

.

xn.

(2.8)

      c(   ) =     1
n

n

   

n=1

with these two well de   ned, we can use the iterative optimization algorithm, such
as gd or sgd, to    nd the best w that minimizes the empirical cost function.11 or,
better is to use a validation set to stop the optimization algorithm at the point of the
minimal validation cost function (remember early stopping?)

now, but we are not too satis   ed with a linear network, are we?

2.5.1 feature extraction
why are we not satis   ed?

first, we are not sure whether the true generating function f was a linear function.
if it is not, can we expect id75 to approximate the true function well? of
course, not. we will talk about this shortly.

second, because we were given x (meaning we did not have much control over what
we want to measure as x), it is unclear how well x represents the input. for instance,
consider doing a sales forecast of air conditioner at one store which opened    ve years
ago. the input x is the number of days since the opening date of the store (1 jan 2009),
and the output y is the number of units sold on each day.

clearly, in this example, the relationship between x and y is not linear. furthermore,
perhaps the most important feature for predicting the sales of air conditioners is missing
from the input x, which is a month (or a season, if you prefer.) it is likely that the
sales bottoms out during the winter (perhaps sometime around december, january and
february,) and it hits the peak during summer months (around may, june and july.)
in other words, if we look at how far the month is away from july, we can predict the
sales quite well even with id75.

11 in fact, looking at eq. (2.8), it   s quite clear that you can compute the optimal w analytically. see

eq. (2.4).

20

let us call this quantity    (x), or equivalent feature, such that

   (x) = |m(x)      | ,

(2.9)
where m(x)     {1,2, . . . ,12} is the month of x and    = 5.5. with this feature, we can    t
id75 to better approximate the sales    gure of air conditioners. furthermore,
we can add yet another feature to improve the predictive performance. for instance,
one such feature can be which day of week x is.

this whole process of extracting a good set of features that will make our choice
of parametric function family (such as id75 in this case) is called feature
extraction. this feature extraction is an important step in machine learning and has
often been at the core of many applications such as id161 (the representative
example is sift [74].)

feature extraction often requires heavy knowledge of the domain in which this
function approximation is applied. to use id75 for id161, it is
a good idea to use id161 knowledge to extract a good set of features. if we
want to use it for environmental problems, we must    rst notice which features must be
important and how they should be represented for id75 to work.

this is okay for a machine learning practitioner in a particular    eld, because the
person has in-depth knowledge about the    eld. there are however many cases where
there   s simply not enough domain knowledge to exploit. to make the matter worse, it
is likely that the domain knowledge is not correct, making the whole business of using
manually extracted features futile.

21

chapter 3

neural networks and
id26 algorithm

3.1 conditional distribution approximation

i have mainly described so far as if the function we approximate or the function we
use to approximate returns only a constant value, as in one point y in the output space.
this is however not true, and in fact, the function can return anything including a
distribution [17, 35, 12].

let   s    rst decompose the data distribution pdata into the product of two terms:

pdata(x,y) = pdata(x)pdata(y|x).

it becomes clear that one way to sample from pdata is to sample an input xn from
pdata(x) and subsequently sample the corresponding output yn from the conditional
distribution pdata(y|xn).
this implies that the function approximation of the generating function ( f : x     y)
is effectively equivalent to approximating the conditional distribution pdata(y|x). this
may suddenly sound much more complicated, but it should not alarm you at all. as
long as we choose to use a distribution parametrized by a small number of param-
eters to approximate the conditional distribution pdata(y|x), this is quite manageable
without almost any modi   cation to the expected and empirical cost functions we have
discussed.
approximating the true, underlying id203 distribution pdata(y|x). as the notation
suggests, the function now returns the parameters of the distribution    (x) given the
input x.
for example, let   s say y     {0,1}k is a binary vector and we chose to use inde-
pendent bernoulli distribution to approximate the conditional distribution pdata(y|x).
in this case, the parameters that de   ne the conditional distribution are the means of k

let us use    (x) to denote a set of parameters for the id203 distribution   p(y|x,   (x))

22

dimensions:

  p(y|x) =

k

   
k(cid:48)=1

p(yk(cid:48)|x) =

k

   
k(cid:48)=1

k(cid:48) (1      k(cid:48))1   yk(cid:48) .
  yk(cid:48)

(3.1)

then the function    (x) should output a k-dimensional vector of which each element is
between 0 and 1.
another example: let   s say y     rk is a real-valued vector. it is quite natural to use a
gaussian distribution with a diagonal covariance matrix to approximate the conditional
distribution p(y|x):

(cid:32)

(cid:33)

  p(y|x) =

k

   
k(cid:48)=1

1   
2    k(cid:48)

exp

(yk(cid:48)       k(cid:48))2

2   2
k(cid:48)

.

(3.2)

the parameters for this conditional distribution are    (x) ={  1,   2, . . . ,   k,  1,  2, . . . ,  k},
where   k     r and   k     r>0.

in this case of id203 approximation, it is natural to use kullback-leibler (kl)
divergence to measure the distance.1 the kl divergence from one distribution p to the
other q is de   ned2 by

kl(p(cid:107)q) =

p(x)log

p(x)
q(x)

dx.

in our case of function/distribution approximation, we want to minimize the kl di-
vergence from the data distribution pdata(y|x) to the approximate distribution   p(y|x)
averaged over the data distribution pdata(x):

(cid:90)

c(   ) =

pdata(x)kl(pdata(cid:107)   p)dx =

pdata(x)

(cid:90)

pdata(y|x)log

pdata(y|x)
  p(y|x)

dydx.

but again we do not have access to pdata and cannot compute this expected cost func-
tion.

similarly to how we de   ned the empirical cost function earlier, we must approxi-

mate this expected kl divergence using the training set:

  c(   ) =

1
n

n

   

n=1

   log   p(yn|xn).

(3.3)

as an example, if we choose to return the binary vector y as in eq. (3.1), the empirical
cost function will be

  c(   ) =     1
n

n

   

n=1

yk(cid:48) log   k(cid:48) + (1    yk(cid:48))log(1      k(cid:48)),

k

   
k(cid:48)=1

1 again, we use a loose de   nition of the distance where triangular inequality is not enforced.
2 why don   t i say the kl divergence between two distributions here? because, the kl divergence is not

a symmetric measure, i.e., kl(p(cid:107)q) (cid:54)= kl(q(cid:107)p).

23

(cid:90)

(cid:90)

which is often called a cross id178 cost. in the case of eq. (3.2),

  c(   ) =     1
n

n

   

n=1

k

   
k(cid:48)=1

(yk(cid:48)       k(cid:48))2

2   2
k(cid:48)

    log  k(cid:48).

(3.4)

do you see something interesting in eq. (3.4)? if we assume that the function
outputs 1 for all   k(cid:48)   s, we see that this cost function reduces to that using the euclidean
distance between the true output y and the mean   . what does this mean?

there will be many occasions later on to discuss more about this perspective when
we discuss language modelling. however, one thing we must keep in our mind is that
there is nothing different between approximating a function and a distribution.

3.1.1 why do we want to do this?
before we move on to the main topic of today   s lecture, let   s try to understand why
we want to output the distribution. unlike returning a single point in the space, the
distribution returned by the function f incorporates both the most likely outcome   y as
well as the uncertainty associated with this value.

in the case of the gaussian output in eq. (3.2), the standard deviation   k(cid:48), or the
variance    2
k(cid:48), indicates how uncertain the function is about the output centered at   k(cid:48).
similarly, the mean   k(cid:48) of the bernoulli output in eq. (3.1) is directly proportional to
the function   s con   dence in predicting that the k(cid:48)-th dimension of the output is 1.

figure 3.1: is this a duck or a rab-
bit? [68] at the end of the day,
we want our function f to return
a conditional distribution saying
that p(duck|x) = p(rabbit|x), in-
stead of returning the answer out
of these two possible answers.

this is useful in many aspects, but one important aspect is that it re   ects the natural
uncertainty of the underlying generating function. one input x may be interpreted in
more than one ways, leading to two possible outputs, which happens more often than
not in the real world. for instance, the famous picture in fig. 3.1 can be viewed as a
picture of a duck or a picture of a rabbit, in which case the function needs to output the
id203 distribution by which the same id203 mass is assigned to both a duck
and a rabbit. furthermore, there is observational noise that cannot easily be identi   ed
and ignored by the function, in which case the function should return the uncertainty
due to the observational noise along with the most likely (or the average) prediction.

24

3.1.2 other distributions
i have described two distributions (densities) that are widely used:

    bernoulli distribution: binary classi   cation
    gaussian distribution: real value regression

here, let me present one more distribution which we will use almost everyday through
this course.

categorical distribution: multi-class classi   cation multi-class classi   cation is a
task in which each example belongs to one of k classes. for each input x, the problem
reduces to    nd a id203 pk(x) of the k-th class under the constraint that

k

   

k=1

pk(x) = 1

it is clear that in this case, the function f returns k values {  1,   2, . . . ,   k}, each
of which is between 0 and 1. furthermore, the sum of   k   s must sum to 1. this can be
achieved easily by letting f to compute af   ne transformation of x (or    (x)) to return k
(unbounded) real values followed by a so called softmax function [17]:

  k =

exp(w(cid:62)
k(cid:48)=1 exp(w(cid:62)
   k

k    (x) + bk)

k(cid:48)   (x) + bk)

,

where wk     rdim(   (x)) and bk     r are the parameters of af   ne transformation.
in this case, the (empirical) cost function based on the kl divergence is

where

ik=yn =

c(   ) =     1
n

n

   

n=1

k

   

k=1

ik=yn  k,

(cid:26) 1,

if k = yn
0, otherwise

(3.5)

(3.6)

(3.7)

3.2 feature extraction is also a function

we talked about the manual feature extraction in the previous lecture (see sec. 2.5.1.
but, this is quite unsatisfactory, because this whole process of manual feature extraction
is heavily dependent on the domain knowledge, meaning that we cannot have a generic
principle on which we design features. this raises a question: instead of manually
designing features ourselves, is it possible for this to happen automatically?

one thing we notice is that the feature extraction process    (x) is nothing but a
function. a function of a function is a function, right? in other words, we will extend
our de   nition of the function to include the feature extraction function:

  y = f (   (x)).

25

we will assume that the feature extraction function    is also parametrized, and its
parameters are included in the set of parameters which includes those of f . as an
example,    in eq. (2.9) is a parameter of the feature extraction   .

a natural next question is which family of parametric functions we should use for
  . we run into the same issue we talked about earlier in sec. 2.3: the size of hypothesis
space is simply too large!

instead of choosing one great feature extraction function, we can go for a stack of
simple transformations which are all learned.3 each transformation can be as simple
as af   ne transformation followed by a simple point-wise nonlinearity:

(3.8)
where w0 is the weight matrix, b0 is the bias and g is a point-wise nonlinearity such
as tanh.4

  0(x) = g(w0x + b0),

one interesting thing is that if the dimensionality of the transformed feature vector
  0(x) is much larger than that of x, the function f (  0(x)) can approximate any func-
tion from x to y under some assumptions, even when the parameters w0 and b0 are
randomly selected! [34]

the problem solved, right? we just put a huge matrix w0, apply some nonlinear
function g to it and    t id75 as i described earlier. we don   t even need to
touch w0 and b0. all we need to do is replace the input xn of all the pairs in the training
set to   0(xn).

in fact, there is a group of researchers claiming to have    gured this out by them-
selves less than a decade ago (as of 2015) who call this model an extreme learning
machine [54]. there have been some debates about this so-called extreme learning
machine. here i will not make any comment myself, but would be a good exercise for
you to    gure out why there has been debates about this.

but, regardlessly, this is not what we want.5 what we want is to fully tune the

whole thing.

3.3 multilayer id88

the basic idea of multilayer id88 is to stack a large number of those feature
extraction layers in eq. (3.8) between the input and the output. this idea is as old as
the whole    eld of neural network research, dating back to early 1960s [89]. however,
it took many more years for people to    gure out a way to tune the whole network, both
f and      s together. see [91] and [70], if you are interested in the history.

3 a great article about

this was posted recently in http://colah.github.io/posts/

2014-03-nn-manifolds-topology/.
4 some of the widely used nonlinearities are
    sigmoid:    (x) =
    hyperbolic function: tanh(x) = 1   exp(   2x)
1+exp(   2x)
    recti   ed linear unit: rect(x) = max(0,x)

1+exp(   x)

1

5 and, more importantly, i will not accept any    nal project proposal whose main model is based on the

elm.

26

3.3.1 example: binary classi   cation with a single hidden unit
let us start with the simplest example. the input x     r is a real-valued scalar, and
the output y     {0,1} is a binary value corresponding to the input   s label. the feature
extractor    is de   ned as

   (x) =    (ux + c),

(3.9)

where u and c are the parameters. the function f returns the mean of the bernoulli
conditional distribution p(y|x):

   = f (x) =    (w   (x) + b).

in both of these equations,    is a sigmoid function:

   (x) =

1

1 + exp(   x)

.

(3.10)

(3.11)

we use the kl divergence to measure the distance between the true conditional

distribution p(y|x) and the predicted conditional distribution   p(y|x).

kl(p(cid:107)   p) =    
y   {0,1}
=    
y   {0,1}

p(y|x)log

p(y|x)
  p(y|x)

p(y|x)log p(y|x)    p(y|x)log   p(y|x).

note that the    rst term in the summation p(y|x)log p(y|x) can be safely ignored in our
case. why? because, this does not concern   p which is one we change in order to
minimize this kl divergence.
let   s approximate this kl divergence with a single sample from p(y|x) and leave

only the relevant part. we will call this a per-sample cost:

cx =    log   p(y|x)

=    log   y(1      )1   y
=    ylog        (1    y)log(1      ),

(3.12)
(3.13)
(3.14)

where    is from eq. (3.10).
it is okay to work with this per-sample cost function
instead of the full cost function, because the full cost function is almost always the
(unweighted) sum of these per-sample cost functions. see eq. (2.3).

we now need to compute the gradient of this cost function cx with respect to all the

parameters w, b, u and c. first, let   s start with w:

   cx
    w =

   cx
      

      
      

      
    w ,

which is a simple application of chain rule of derivatives. compare this to

   cx
    b =

   cx
      

      
      

      
    b .

27

in both equations,    = w   (x) + b which is the input to f .

      
both of these derivatives share    cx
       , where
      
   y + y   +        y  

      

=     y
  

  (cid:48) +

1    y
1      

  (cid:48) =

  (1      )

   cx
      

      (cid:124)(cid:123)(cid:122)(cid:125)

=  (cid:48)

  (cid:48) =

       y
  (1      )

  (cid:48) =        y,

(3.15)

because the derivative of the sigmoid function       
       is
  (cid:48) =   (1      ).

note that this corresponds to computing the difference between the correct label y and
the predicted label (id203)   .
given this output derivative    cx

       , all we need to compute are

      
    w =    (x)
      
    b = 1.

from these computations, we see that
   cx
    w = (       y)   (x),
   cx
    b = (       y).

(3.16)

(3.17)

let us continue on to u and c. we can again rewrite the derivatives w.r.t. these into

   cx
    u =
   cx
    c =

   cx
      
   cx
      

      
     
      
     

     
     
     
     

     
    u
     
    c ,

where    is the input to    similarly to    was to the input to   .

there are two things to notice here. first, we already have    cx

derivatives w.r.t. w and b, meaning there is no need to re-compute it. second,       
shared between the derivatives w.r.t. u and c.

       from computing the
      is

therefore, we    rst compute       
      :

      
     

     

     (cid:124)(cid:123)(cid:122)(cid:125)

=  (cid:48)

= w  (cid:48) = w   (x)(1       (x))

28

next, we compute

     
    u = x
     
    c = 1.

now all the ingredients are there:

   cx
    u =(       y)w   (x)(1       (x))x
   cx
    c =(       y)w   (x)(1       (x)).

the most important lession to learn from here is that most of the computations
needed to get the derivatives in this seemingly complicated multilayered computational
graph (multilayer id88) are shared. at the end of the day, the amount of compu-
tation needed to compute the gradient of the cost function w.r.t. all the parameters in
the network is only as expensive as computing the cost function itself.

3.3.2 example: binary classi   cation with more than one hidden

units

let us try to generalize this simple, or rather simplest model, into a slightly more
general setting. we will still look at the binary classi   cation but with multiple hidden
units and a multidimensional input such that:

   (x) = ux + c,

where u     rl  d and c     rl. consequently, w will be a l-dimensional vector.

the output derivative    cx
      

      
       stays same as before. see eq. (3.15). however, we
note that the derivative of    with respect to w should now differ, because it   s a vector.6
let   s look at what this means.
the    can be expressed as

   = w(cid:62)   (x) + b =

l

   

i=1

wi  i(x) + b.

(3.18)

in this case, we can start computing the derivative with respect to each element of wi
separately:

      
    wi

=   i(x),

6 the matrix cookbook [85] is a good reference for this section.

29

and will put them into a vector:

(cid:20)       

    w1

      
    w =

      
    w2

,

, . . . ,

      
    wl

(cid:21)(cid:62)

(cid:62)
= [  1(x),  2(x), . . . ,  l(x)]

=    (x)

then, the derivative of the cost function cy with respect to w can be written as

   cy
    w = (       y)   (x),

now, let   s look at    cy

in which case nothing really changed from the case of a single hidden unit in eq. (3.16).
      . again, because    (x) is now a vector, there has to be some
      . in fact, the
    w due to the symmetry

changes. because    cy
procedure for computing this is identical to that for computing       
in eq. (3.18). that is,

       is already computed, we only need to look at       

      
      = w

next, what about      

      ? because the nonlinear activation function    is applied
element-wise, we can simply compute this derivative for each element in    (x) such
that

(cid:16)(cid:2)  (cid:48)

     
      = diag

1(x),  (cid:48)

2(x), . . . ,  (cid:48)

l (x)(cid:3)(cid:62)(cid:17)

,

where diag returns a diagonal matrix of the input vector. in short, we will denote this
as   (cid:48)

overall so far, we have got

   cy
      = (       y)w(cid:62)  (cid:48)(x) = (       y)(w(cid:12) diag(  (cid:48)(x))),

where (cid:12) is an element-wise multiplication.

now it is time to compute      
   u :

     
   u =

   u(cid:62)x
   u = x,

according to the matrix cookbook [85]. then, let   s look at the whole derivative w.r.t.
u:

   cy
   u = (       y)(w(cid:12) diag(  (cid:48)(x)))x(cid:62).

note that all the vectors in this lecture note are column vectors.

for c, it   s straightforward, since

     
    c = 1.

30

3.4 automating id26
this procedure, presented as two examples, is called a id26 algorithm. if
you read textbooks on neural networks, you see a fancier way to explain this back-
propagation algorithm by introducing a lot of fancy terms such as local error    and
so on. but, personally i    nd it much easier to understand id26 as a clever
application of the chain rule of derivatives to a directed acyclic graph (dag) in which
each node computes a certain function    using the output of the previous nodes. i will
refer to this dag as a computational graph from here on.

(a)

(b)

figure 3.2: (a) a graphical representation of the computational graph of the example
network from sec. 3.3.2. (b) a graphical illustration of a function node (   : forward
pass,    : backward pass.)

a typical computational graph looks like the one in fig. 3.2 (a). this computational
graph has two types of nodes; (1) function node ((cid:13)) and (2) variable node (2). there
are four different types of function nodes; (1) matmul(a,b) = ab, (2) matsum(a,b) =
a+b, (3)   : element-wise sigmoid function and (4) cy: cost node. the variables nodes
correspond to either parameters or data (x and y.) each function node has a number
associated with it to distinguish between the nodes of the same function.

now, in this computational graph, let us start computing the gradient using the
    y and    cy
      1 .
    matsum1 and multiply it

id26 algorithm. we start from the last code, cy, by computing    cy
then, the function node    1 will compute its own derivative
with    cy

      1 passed back from the function node cy. so far we   ve computed

      1

   cy

    matsum1 =

   cy
      1

      1

    matsum1

(3.19)

the function node matsum1 has two inputs b and the output of matmul1. thus,
    matmul1 . each of these is multiplied
    matsum1 from eq. (3.19). at this point, we already

this node computes two derivatives     matsum1
with the backpropagated derivative
have the derivative of the cost function cy w.r.t. one of the parameters b:

and     matsum1

    b
   cy

   cy
    b =

   cy

    matsum1

    matsum1

    b

31

111222this process continues mechanically until the very beginning of the graph (a set
of root variable nodes) is reached. all we need in this process of backpropagating the
derivatives is that each function node implements both forward computation as well
as backward computation. in the backward computation, the function node received
the derivative from the next function node, evaluates its own derivative with respect to
the inputs (at the point of the forward activation) and passes theses derivatives to the
corresponding previous nodes. see fig. 3.2 (b) for the graphical illustration.

importantly, the inner mechanism of a function node does not change depending on
its context (or equivalently where the node is placed in a computational graph.) in other
words, if each type of function nodes is implemented in advance, it becomes trivial to
build a complicated neural network (including multilayer id88s) and compute
the gradient of the cost function (which is one such function node in the graph) with
respect to all the parameters as well as all the inputs.

this is a special case, called the reverse mode, of automatic differentiation.7 it
is probably the most valuable tool in deep learning, and fortunately many widely used
toolkits such as theano [10, 4] have implemented this reverse mode of automatic differ-
entiation with an extensive number of function nodes used in deep learning everyday.
before    nishing this discussion on automating id26, i   d like you to
think of pushing this even further. for instance, you can think of each function node
returning not its numerical derivative on its backward pass, but a computational sub-
graph computing its derivative. this means that it will return a computational graph
of gradient, where the output is the derivatives of all the variable nodes (or a subset
of them.) then, we can use the same facility to compute the second-order derivatives,
right?

3.4.1 what if a function is not differentiable?
from the description so far, one thing we notice is that id26 works only
when each and every function node (in a computational graph) is differentiable. in
other words, the nonlinear activation function must be chosen such that almost every-
where it is differentiable. all three id180 i have presented so far have
this property.

logistic functions a sigmoid function is de   ned as

and its derivative is

   (x) =

1

1 + exp(   x)

,

  (cid:48)(x) =    (x)(1       (x)).

a hyperbolic tangent function is

tanh(x) =

exp(2x)    1
exp(2x) + 1

,

7 if anyone   s interested in digging more into the whole    eld of automatic differentiation, try to google it

and you   ll    nd tons of materials. one such reference is [5].

32

and its derivative is

(cid:18)

tanh(cid:48)(x) =

(cid:19)2

.

2

exp(x) + exp(   x)

piece-wise linear functions
[81, 46]) earlier:

i described a recti   ed linear unit (recti   er or relu,

rect(x) = max(0,x).

it is clear that this function is not strictly differentiable, because of the discontinuity
at x = 0. however, the chance of the input to this recti   er lands exactly at 0 has
zero id203, meaning that we can forget about this extremely unlikely event. the
derivative of the recti   er in this case is
rect(cid:48)(x) =

(cid:26) 1,

if x > 0
if x     0

0,

although the recti   er has become the most widely used nonlinearity, especially,
in deep learning   s applications to id161,8 there is a small issue with the
recti   er. that is, for a half of the input space, the derivative is zero, meaning that the
error (the output derivative from eq. (3.15)) will be not well propagated through the
recti   er function node.

in [48], the recti   er was extended to a maxout unit so as to avoid this issue of the
existence of zero-derivative region in the input to the recti   er. the maxout unit of rank
k is de   ned as

maxout(x1, . . . ,xk) = max(x1, . . . ,xk),

and its derivative as

    maxout

    xi

(x1, . . . ,xk) =

(cid:26) 1,

if max(x1, . . . ,xk) = xi

0, otherwise

this means that the derivative is backpropagated only through one of the k inputs.

stochastic variables these id180 work well with the id26
algorithm, because they are differentiable almost everywhere in the input space. how-
ever, what happens if a function is non-differentiable at all. one such example is a
binary stochastic node, which is computed by

1. compute p =    (x), where x is the input to the function node.
2. consider p as a mean of a bernoulli distribution, i.e., b(p).
3. generate one sample s     {0,1} from the bernoulli distribution.
4. output s.

8 almost all the winning entries in id163 large scale visual recognition challenges (ilsvrc) use a
convolutional neural network with recti   ers. see http://image-net.org/challenges/lsvrc/.

33

clearly there is no derivative of this function node.

does it mean that we   re doomed in this case? fortunately, no. although i will not
discuss about this any further in this course, bengio et al. [7] provide an extensive list
of approaches we can take in order to compute the derivative of the stochastic function
nodes.

34

chapter 4

recurrent neural networks and
id149

after the last lecture i hope that it has become clear how to build a multilayer percep-
tron. of course, there are so many details that i did not mention, but are extremely im-
portant in practice. for instance, how many layers of simple transformations eq. (3.8)
should a multilayer id88 have for a certain task? how wide (equiv. dim(  0(x)))
should each transformation be? what other transformation layers are there? what kind
of learning rate    (see eq. (2.5)) should we use? how should we schedule this learning
rate over training? answers to many of these questions are unfortunately heavily task-,
data- and model-dependent, and i cannot provide any general answer to them.

4.1 recurrent neural networks
instead, i will move on to describing how we can build a neural network1 to handle
a variable length input. until now the input x was assumed to be either a scalar or
a vector of the    xed number of dimensions. from here on however, we remove this
assumption of a    xed size input and consider the case of having a variable length input
x.

what do i mean by a variable length input? a variable length input x is a sequence
where each input x has a different number of elements. for instance, the    rst training
example   s input x1 may consist of l1 elements such that

x1 = (x1

1,x1

2, . . . ,x1

l1).

meanwhile, another example   s input xn may be a sequence of ln (cid:54)= l1 elements:

xn = (xn

1,xn

2, . . . ,xn

ln).

let   s go back to very basic about dealing with these kinds of sequences. further-
more, let us assume that each element xi is binary, meaning that it is either 0 or 1. what

1 now, let me begin using a term neural network instead of a general function.

35

would be the most natural way to write a function that returns the number of 1   s in
an input sequence x = (x1,x2, . . . ,xl)? my answer is to    rst build a recursive function
called add1, shown in alg. 1. this function add1 will be called for each element of
the input x, as in alg. 2.

algorithm 1 a function add1

s     0
function add1(v,s)

if v = 0 then return s
else return s + 1
end if

end function

algorithm 2 a function add1

s     0
for i     1,2, . . . ,l do s     add1(xi,s)
end for

there are two important components in this implementation. first, there is a mem-
ory s which counts the number of 1   s in the input sequence x. second, a single function
add1 is applied to each symbol in the sequence one at a time together with the mem-
ory s. thanks to these two properties, our implementation of the function add1 can be
used with the input sequence of any length.

now let us generalize this idea of having a memory and a recursive function that
works over a variable length sequence. one likely most general case of this idea is
a digital computer we use everyday. a computer program is a sequence x of instruc-
tions xi. a central processing unit (cpu) reads each instruction of this program and
manipulates its registers according to what the instruction says. manipulating registers
is often equivalent to manipulating any input   output (i/o) device attached to the cpu.
once one instruction is executed, the cpu moves on to the next instruction which will
be executed with the content of the registers from the previous step. in other words,
these registers work as a memory in this case (s from alg. 2,) and the execution of an
instruction by the cpu corresponds to a recursive function (add1 from alg. 1.)

both add1 and cpu are hard coded in the sense that they do what they have been
designed and manufactured to do. clearly, this is not what we want, because nobody
knows how to design a cpu or a recursive function for natural language understanding,
which is our ultimate goal. instead what we want is to have a parametric recursive
function that is able to read a sequence of (linguistic) symbols and use a memory in
order to understand natural languages.

to build this parametric recursive function2 that works on a variable-length input
sequence x = (x1,x2, . . . ,xl), we now know that there needs to be a memory. we will
use one vector h     rdh as this memory vector. as is clear from alg. 1, this recursive
function takes as input both one input symbol xt and the memory vector h, and it

2 in neural network research, we call this function a recurrent neural network.

36

returns the updated memory vector. it often helps to time index the memory vector
as well, such that the input to this function is ht   1 (the memory after processing the
previous symbol xt   1,) and we use ht to denote the memory vector returned by the
function. this function is then

ht = f (xt ,ht   1)

now the big question is what kind of parametric form this recursive function f
takes? we will follow the simple transformation layer from eq. (3.8), in which case we
get

f (xt ,ht   1) = g(w   (xt ) + uht   1),

(4.1)
where    (xt ) is a function that transforms the input symbol (often discrete) into a d-
dimensional real-valued vector. w     rdh  d and udh  dh are parameters of this function.
a nonlinear activation function g can be any function, but for now, we will assume that
it is an element-wise nonlinear function such as tanh.

4.1.1 fixed-size output y
because our goal is to approximate an underlying, true function, we now need to think
of how we use this recursive function to return an output y. as with the case of variable-
length sequence input x, y can only be either a    xed-size output, such as a category to
which the input x belongs, or a variable-length sequence output. here let us discuss the
case of having a    xed-size output y.

the most natural approach is to use the last memory vector hl to produce the output
(or more often output distribution.) consider a task of binary classi   cation where y is
either positive (1) or negative (0), in which case a bernoulli distribution    ts perfectly.
a bernoulli distribution is fully characterized by a single parameter   . hence,

   =    (v(cid:62)hl),

where v     rdh is a weight vector, and    is a sigmoid function.

this now looks very much like the multilayer id88 from sec. 3.3. the whole

function given an input sequence x computes
(cid:125)
   =    (v(cid:62) g(w   (xl) + ug(w   (xl   1) + ug(w   (xl   2) +      g(w   (x1) + uh0)       )))
),

(cid:123)(cid:122)

(cid:124)

(a) recurrence

(4.2)

where h0 is an initial memory state which can be simply set to an all-zero vector.

the main difference is that the input is not given only to the    rst simple trans-
formation layer, but is given to all those transformation layers (one at a time.) also,
each transformation layer shares the parameters w and u.3 the    rst two steps of the
3 note that for brevity, i have omitted bias vectors. this should not matter much, as having a bias vector

is equivalent to augmenting the input with a constant element whose value is    xed at 1. why? because,

(cid:21)

(cid:20) x

1

[w;b]

= wx + b

note that as i have declared before all vectors are column vectors.

37

recurrence part (a) of eq. (4.2) are shown as a computational graph in fig. 4.1.

figure 4.1: sample computational graph of the recurrence in eq. (4.2).

as this is not any special computational graph, the whole discussion on how to au-
tomate id26 (computing the gradient of the cost function w.r.t. the parame-
ters) in sec. 3.4 applies to recurrent neural networks directly, except for one potentially
confusing point.

4.1.2 multiple child nodes and derivatives
it may be confusing how to handle those parameters that are shared across multiple
time steps; w and u in fig. 4.1. in fact, in the earlier section (sec. 3.4), we did not
discuss about what to do when the output of one node is fed into multiple function
nodes. mathematically saying, what do we do in the case of

c = g( f1(x), f2(x), . . . , fn(x))?

g can be any function, but let us look at two widely used cases:
    addition: g( f1(x), . . . , fn(x)) =    n

    multiplication: g( f1(x), . . . , fn(x)) =    n

    c
    x =

i   {1,2,...,n}

i=1 fi(x)
    c
    g    
(cid:32)
   
j(cid:54)=i

i=1 fi(x)

    fi
    x .

(cid:33)

f j(x)

    c
    x =

    c
    g    

i   {1,2,...,n}

    fi
    x .

from these two cases, we can see that in general

    c
    x =

    c
    g    

i   {1,2,...,n}

    g
    fi

    fi
    x .

38

this means that when multiple derivatives are backpropagated into a single node, the
node should    rst sum them and multiply its summed derivative with its own derivative.
what does this mean for the shared parameters of the recurrent neural network? in

an equation,

    matsuml

    matsuml
    matmull

    matmull

   w

(4.3)

   c
   w =

+

+

(cid:124)
(cid:124)
(cid:124)

   c

(a)
   c

(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:123)(cid:122)

(a)

(a)
   c

(cid:125)
(cid:125)
(cid:125)

(cid:124)
(cid:124)

+       ,

(b)

(cid:123)(cid:122)
(cid:123)(cid:122)

(b)

(cid:125)
(cid:125)

(cid:124)

    matsuml
    matsuml   1

    matsuml   1
    matmull   1

    matmull   1

   w

    matsuml

    matsuml
    matsuml   1

    matsuml   1
    matsuml   2

    matsuml   2
    matmull   2

    matmull   2

   w

    matsuml

(cid:123)(cid:122)

(c)

(cid:125)

where the superscript l of each function node denotes the layer at which the function
node resides.

similarly to what we   ve observed in sec. 3.4, many derivatives are shared across
the terms inside the summation in eq. (4.3). this allows us to compute the derivative
of the cost function w.r.t. the parameter w ef   ciently by simply running the recurrent
neural network backward.

4.1.3 example: id31
there is a task in natural language processing called id31. as the name
suggests, the goal of this task is to predict the sentiment of a given text. this is de   -
nitely one function that a human can do fairly well: when you read a critique   s review
of a movie, you can easily tell whether the critique likes, hates or is neutral to the
movie. also, even without a star rating of a product on amazon, you can quite easily
tell whether a user like it by reading her/his review of the product.

in this task, an input sequence x is a given text, and the    xed-size output is its label
which is almost always one of positive, negative or neutral. let us assume for now
that the input is a sequence of words, where each word xi is represented as a so-called
one-hot vector.4 in this case, we can use

in eq. (4.1).

   (xt ) = xt

4 a one-hot vector is a way to represent a discrete symbol as a binary vector. the one-hot vector vi of a

symbol i     v = {1,2, . . . ,|v|} is

(cid:124) (cid:123)(cid:122) (cid:125)

vi = [0, . . . ,0
1,...,i   1

, 1(cid:124)(cid:123)(cid:122)(cid:125)

i

(cid:124) (cid:123)(cid:122) (cid:125)

, 0, . . . ,0
i+1,...,|v|

](cid:62).

39

once the input sequence, or paragraph in this speci   c example, is read, we get
the last memory state hl of the recurrent neural network. we will af   ne-transform hl
followed by the softmax function to obtain the conditional distribution of the output
y     {1,2,3} (1: positive, 2: neutral and 3: negative):

(cid:62)
   = [  1,   2,   3]

= softmax(vhl),

(4.4)

where   1,   2 and   3 are the probabilities of    positive   ,    neural    and    negative   . see
eq. (3.5) for more details on the softmax function.

because this network returns a categorial distribution, it is natural to use the (cate-
gorical) cross id178 as the cost function. see eq. (3.6). a working example of this
sentiment analyzer based on recurrent neural networks will be introduced and discussed
during the lab session.5
4.1.4 variable-length output y: |x| = |y|
let   s generalize what we have discussed so far to recurrent neural networks here. in-
stead of a    xed-size output y, we will assume that the goal is to label each input symbol,
resulting in the output sequence y = (y1,y2, . . . ,yl) of the same length as the input se-
quence x.

what kind of applications can you think of that returns the output sequence as long
as the input sequence? one of the most widely studied problems in natural language
processing is a problem of classifying each word in a sentence into one of part-of-
speech tags, often called id52 (see sec. 3.1 of [77].) unfortunately, in my
personal opinion, this is perhaps the least interesting problem of all time in natural
language understanding, but perhaps the most well suited problem for this section.

in its simplest form, we can view this problem of id52 as classifying each
word in a sentence as one of noun, verb, adjective and others. as an example, given
the following input sentence x

the goal is to output

x = (children,eat,sweet,candy),

y = (noun,verb,adjective,noun).

this task can be solved by a recurrent neural network from the preceding section
(sec. 4.1.1) after a quite trivial modi   cation. instead of waiting until the end of the
sentence to get the last memory state of the recurrent neural network, we will use the
immediate memory state to predict the label at each time step t.

at each time t, we get the immediate memory state ht by

ht = f (xt ,ht   1),

(4.5)

where f is from eq. (4.1). instead of continuing on to processing the next word, we
will    rst predict the label of the t-th input word xt.

5 for those eager to learn more, see http://deeplearning.net/tutorial/lstm.html in

advance of the lab session.

40

this can be done by

(cid:62)
  t = [  t,1,   t,2,   t,3,   t,4]

= softmax(vht ).

(4.6)

four   t,i   s correspond to the probabilities of the four categories; (1) noun, (2) verb, (3)
adjective and (4) others.

from this output distribution at time step t, we can de   ne a per-step, per-sample

cost function:

cx,t (   ) =     k
   

k=1

ik=y  t,k,

(4.7)

where k is the number of categories, four in this case. we discussed earlier in eq. (3.6).
naturally a per-sample cost function is de   ned as the sum of these per-step, per-sample
cost functions:

cx(   ) =     l
   

t=1

k

   

k=1

ik=y  t,k.

(4.8)

incorporating the output structures this formulation of the cost function is equiv-
alent to maximizing the log-id203 of the correct output sequence given an input
sequence, where the conditional log-id203 is de   ned as

log p(yt|x1, . . . ,xt )

.

(4.9)

(cid:124)

log p(y|x) =

l

t=1

   
(cid:124)

(cid:123)(cid:122)
(cid:123)(cid:122)

eq. (4.7)

eq. (4.8)

(cid:125)
(cid:125)

this means that the network is predicting the label of the t-th input symbol using only
the input symbols read up to that point (i.e., x1,x2, . . . ,xt.)

in other words, this means that the recurrent neural network is not taking into ac-
count the structure of the output sequence. for instance, even without looking at the
input sequence, in english it is well known that the id203 of the next word being a
noun increases if the current word is an adjective.6 this kind of structures in the output
are effectively ignored in this formulation.

why is this so in this formulation? because, we have made an assumption that
the output symbols y1,y2, . . . ,yl are mutually independent conditioned on the input se-
quence. this is clear from eq. (4.9) and the de   nition of the conditional independence:

y1 and y2 are conditionally independent dependent on x

       p(y1,y2|x) = p(y1|x)p(y2|x).

if the underlying, true conditional distribution obeyed this assumption of condi-
tional independence, there is no worry. however, this is a very strong assumption for

6 okay, this requires a more thorough analysis, but for the sake of the argument, which does not have to

do anything with actual pos tags, let   s believe that this is indeed the case.

41

many of the tasks we run into, apparently from the example of id52. then,
how can we exploit the structure in the output sequence?

one simple way is to make a less strong assumption about the conditional proba-

bility of the output sequence y given x. for instance, we can assume that

log p(y|x) =

l

   

i=1

log p(yi|y<i,x   i),

where y<i and x   i denote all the output symbols before the i-th one and all the input
symbols up to the i-th one, respectively.

now the question is how we can incorporate this into the existing formulation of
a recurrent neural network from eq. (4.5). it turned out that the answer is extremely
simple. all we need to do is to compute the memory state of the recurrent neural
network based not only on the current input symbol xt and the previous memory state
ht   1, but also on the previous output symbol yt   1 such that

ht = f (xt ,yt   1,ht   1).

similarly to eq. (4.1), we can think of implementing f as

f (xt ,yt   1,ht   1) = g(wx  x(xt ) + wy  y(yt   1) + whht   1).

there are two questions naturally arising from this formulation. first, what do we
do when computing h1? this is equivalent to saying what   y(y0) is. there are two
potential answers to this question:

1. fix   y(y0) to an all-zero vector
2. consider   y(y0) as an additional parameter

in the latter case,   y(y0) will be estimated together with all the other parameters such
as those weight matrices wx, wy, wh and v.

id136 the second question involves how to handle yt   1. during training, it is
quite straightforward, as our cost function (kl-divergence between the underlying,
true distribution and the parametric conditional distribution p(y|x), approximated by
monte carlo method) says that we use the groundtruth value for yt   1   s.

it is however not clear what we should do when we test the trained network, because
then we are not given the groundtruth output sequence. this process of    nding an
output that maximizes the conditional (log-)id203 is called id1367:

  y = argmax

y

log p(y|x)

7 okay, i confess. the term id136 refers to a much larger class of problems, even if we consider only
machine learning. however, let me simply use this term to refer to a task of    nding the most likely output of
a function.

42

the exact id136 is quite straightforward. one can simply evaluate log p(y|x) for
every possible output sequence and choose the one with the highest conditional proba-
bility. unfortunately, this is almost always intractable, as the number of every possible
output sequence grows exponentially with respect to the length of the sequence:

|y | = kl,

where y , k and l are the set of all possible output sequences, the number of labels and
the length of the sequence, respectively. thus, this is necessary to resort to approximate
search over the set y .

the most naive approach to approximate id136 is a greedy one. with the trained
model, you predict the    rst output symbol   y1 based on the    rst input symbol x1 by
selecting the category of the highest id203 p(y1|x1). now, given   y1, x1 and x2,
we compute p(y2|x1,x2,y1) from which we select the next output symbol   y2 with the
highest id203. we continue this process iteratively until the last output symbol   yl
is selected.

this is greedy in the sense that any early choice with a high id155
may turn out to be unlikely one due to extremely low conditional probabilities later on.
it is highly related to the so-called garden path sentence problem. to know more about
this, read, for instance, sec. 3.2.4 of [77].

it is possible to alleviate this issue by considering n < k best hypotheses of the
output sequence at each time step. this procedure is called id125, and we will
discuss more about this in a later lecture on id4.

4.2 id149
4.2.1 making simple recurrent neural networks realistic
let us get back to the analogy we made in sec. 4.1. we compared a recurrent neural
network to how cpu works. executing a recurrent function f is equivalent to executing
one of the instructions on cpu, and the memory state of the recurrent neural network is
equivalent to the registers of the cpu. this analogy does sound plausible, except that
it is not.

in fact, how a simple recurrent neural network works is far from being similar to
how cpu works. i am now talking about how they are implemented in practice, but
rather i   m talking at the conceptual level. what is it at the conceptual level that makes
the simple recurrent neural network unrealistic?

an important observation we make about the simple recurrent neural network is
that it refreshes the whole memory state at each time step. this is almost opposite to
how the registers on a cpu are maintained. each time an instruction is executed, the
cpu does not clear up the whole registers and repopulate them. rather, it works only
on a small number of registers. all the other registers    values are stored as they were
before the execution of the instruction.

let   s try to write this procedure mathematically. each time, based on the choice
of instruction to be executed, a subset of the registers of a cpu, or a subset of the

43

elements in the memory state of a recurrent neural network, is selected. this can be
written down as a binary vector u     {0,1}nh:

(cid:26) 0,

1,

ui =

if the register   s value does not change
if the register   s value will change

with this binary vector, which i will call an update gate, a new memory state or a

new register value at time t can be computed as a convex interpolation such that

ht = (1    u)(cid:12) ht   1 + u(cid:12)   ht ,

(4.10)
where (cid:12) is as usual an element-wise multiplication.   ht denotes a new memory state or
a new register value, after executing the instruction at time t.

another unrealistic point about the simple recurrent neural network is that each
execution considers the whole registers. it is almost impossible to imagine designing
an instruction on a cpu that requires to read the values of all the registers. instead,
what almost always happens is that each instruction will consider only a small subset
of the registers, which again we can use a binary vector to represent. let me call it a
reset gate r     {0,1}nh:

(cid:26) 0,

1,

ri =

if the register   s value will not be used
if the register   s value will be used

this reset gate can be multiplied to the register values before being used by the

instruction at time t.8 if we use a recursive function f from eq. (4.1), it means that

  ht = f (xt ,r(cid:12) ht   1) = g(w   (xt ) + u(r(cid:12) ht   1)).

(4.11)

now, let us put these two gates that are necessary to make the simple recurrent
neural network more realistic into one piece. at each time step, the candidate memory
state is computed based on a subset of the elements of the previous memory state:

  ht = g(w   (xt ) + u(r(cid:12) ht   1))

a new memory state is computed as a linear interpolation between the previous mem-
ory state and this candidate memory state using the update gate:

ht = (1    u)(cid:12) ht   1 + u(cid:12)   ht

see fig. 4.2 for the graphical illustration.

4.2.2 id149
now here goes a big question: how are the update u and reset r gates computed?

if we stick to our analogy to the cpu, those gates must be pre-con   gured per
instruction. those binary gates are dependent on the instruction. again however, this

8 it is important to note that this is not resetting the actual values of the registers, but only the input to the

instruction/recursive function.

44

figure 4.2: a graphical illustration of a
gated recurrent unit [29].

is not what we want to do in our case. there is no set of prede   ned instructions, but the
execution of any instruction corresponds to computing a recurrent function based on the
input symbol and the memory state from the previous time step (see, e.g., eq. (4.1).)
similarly to this what we want with the update and reset gates is that they are computed
by a function which depends on the input symbol and the previous memory state.

this sounds like quite straightforward, except that we de   ned the gates to be binary.
this means that whatever the function we use to compute those gates, the function will
be a discontinuous function with zero derivative almost everywhere, except at the point
where a sharp transition from 0 to 1 happens. we discussed the consequence of having
an activation function with zero derivative almost everywhere in sec. 3.4.1, and the
conclusion was that it becomes very dif   cult to compute the gradient of the cost func-
tion ef   ciently and exactly with these discrete id180 in a computational
graph.

one simple solution which turned out to be extremely ef   cient is to consider those
gates not as binary vectors but as real-valued coef   cient vectors. in other words, we
rede   ne the update and reset gates to be

u     [0,1]nh ,r     [0,1]nh .

this approach makes these gates leaky in the sense that they always allow some leak
of information through the gate.

in the case of the reset gate, rather than making a hard decision on which subset
of the registers, or the elements of the memory state, will be used, it now decides how
much information from the previous memory state will be used. the update gate on
the other hand now controls how much content in the memory state will be replaced,
which is equivalent to saying that it controls how much information will be kept from
the previous memory state.

under this de   nition we can simply use a sigmoid function from eq. (3.11) to

compute these gates:

r =   (wr   (xt ) + urht   1),
u =   (wu   (xt ) + uu(r(cid:12) ht   1)),

where wr, ur, wu and uu are the additional parameters.9 since the sigmoid function
is differentiable everywhere, we can use the id26 algorithm (see sec. 3.4)

9 note that this is not the formulation available for computing the reset and update gates. for instance,

45

urhh~xto compute the derivatives of the cost function with respect to these parameters and
estimate them together with all the other parameters.

we call this recurrent activation function with the reset and update gates a gated
recurrent unit (gru), and a recurrent neural network having this gru as a gated re-
current network.

4.2.3 long short-term memory
the gated recurrent unit (gru) is highly motivated by a much earlier work on long
short-term memory (lstm) units [53].10 the lstm was proposed in 1997 with
the goal of building a recurrent neural network that can learn long-term dependen-
cies across many number of timsteps, which was deemed to be dif   cult to do so with a
simple recurrent neural network.

unlike the element-wise nonlinearity of the simple recurrent neural network and the
gated recurrent unit, the lstm explicitly separates the memory state ct and the output
ht. the output is a small subset of the hidden memory state, and only this subset of the
memory state is visibly exposed to any other part of the whole network.

how does a recurrent neural network with lstm units decide how much of the
memory state it will reveal? as perhaps obvious at this point, the lstm uses a so-
called output gate o to achieve this goal. similarly to the reset and update gates of the
gru, the output gate is computed by

o =    (wo   (xt ) + uoht   1).

this output vector is multiplied to the memory state ct point-wise to result in the output:

ht = o(cid:12) tanh(ct ).

updating the memory state ct closely resembles how it is updated in the gru (see
eq. (4.10).) a major difference is that instead of using a single update gate, the lstm
uses two gates, forget and input gates, such that

ct = f(cid:12) ct   1 + i(cid:12)   ct ,

where f     rnh, i     rnh and   ct are the forget gate, input gate and the candidate memory
state, respectively.

the roles of those two gates are quite clear from their names. the forget gate
decides how much information from the memory state will be forgotten, while the
input gate controls how much informationa about the new input (consisting of the input

one can use the following de   nitions of the reset and update gates:
r =   (wr   (xt ) + urht   1),
u =   (wu   (xt ) + uuht   1),

which is more parallelizable than the original formulation from [29]. this is because there is no more direct
dependency between r and u, which makes it possible to compute them in parallel.

10 okay, let me confess here. i was not well aware of long short-term memory when i was designing the

gated recurrent unit together with yoshua bengio and caglar gulcehre in 2014.

46

symbol and the previous output) will be inputted to the memory. they are computed
by

f =   (w f    (xt ) + u f ht   1),
i =   (wi   (xt ) + uiht   1).

(4.12)

the candidate memory state is computed similarly to how it was done with the

gru in eq. (4.11):

  ct = g(wc   (xt ) + ucht   1),

(4.13)

where g is often an element-wise tanh.

all the additional parameters speci   c to the lstm   wo,uo,w f ,u f ,wi,ui,wc
and uc    are estimated together with all the other parameters. again, every function
inside the lstm is differentiable everywhere, and we can use the id26
algorithm to ef   cient compute the gradient of the cost function with respect to all the
parameters.

although i have described one formulation of the long short-term memory unit
here, there are many other variants proposed over more than a decade since it was    rst
proposed. for instance, the forget gate in eq. (4.12) was not present in the original
work [53] but was    xed to 1. gers et al. [45] proposed the forget gate few years after
the lstm was originally proposed, and it turned out to be one of the most crucial
component in the lstm. for more variants of the lstm, i suggest you to read [49,
58].11

4.3 why not recti   ers?
4.3.1 recti   ers explode
let us go back to the simple recurrent neural network which uses the simple transfor-
mation layer from eq. (4.1):

f (xt ,ht   1) = g(w   (xt ) + uht   1),

where g is an element-wise nonlinearity.

one of the most widely used nonlinearities is a hyperbolic tangent function tanh.
this is unlike the case in feedforward neural networks (multilayer id88s) where a
(unbounded) piecewise linear function, such as a recti   er and maxout, has become stan-
dard. in the case of feedforward neural networks, you can safely assume that everyone
uses some kind of piecewise linear function as an activation function in the network.
this has become pretty much standard since krizhevsky et al. [67] shocked the (com-
puter vision) research community by outperforming all the more traditional computer
vision teams in the id163 large scale visual recognition challenge 2012.12

11 interestingly, based on the observation in [58], it seems like the plain lstm with a forget gate and the

gru seem to be close to the optimal gated unit we can    nd.

12 http://image-net.org/challenges/lsvrc/2012/results.html

47

the main difference between logistic functions (tanh and sigmoid function) and
piecewise linear functions (recti   ers and maxout) is that the former is bounded from
both above and below, while the latter is bounded only from below (or in some cases,
not bounded at all [50].13)

this unbounded nature of piece-wise linear functions makes it dif   cult for them to

be used in recurrent neural networks. why is this so?

let us consider the simplest case of unbounded element-wise nonlinearity; a linear

function:

g(a) = a.

the hidden state after l symbols is
hl =u(u(u(u(       ) + w   (xl   3)) + w   (xl   2)) + w   (xl   1)) + w   (xl)

w   (x2) +       + uw   (xl   1) + w   (xl),

(4.14)

(cid:32) l   2

(cid:33)

u

   
l(cid:48)=1

u

(cid:32) l   1
(cid:33)
(cid:32) l   t
   
l(cid:48)=1
(cid:124)
   

   
l(cid:48)=1

t=1

l

=

=

w   (x1) +

u

w   (xt )

(cid:125)

(cid:33)
(cid:123)(cid:122)

(a)

where l is the length of the input sequence.

let us assume that
    u is a full rank matrix
    the input sequence is sparse:    l
    [w   (x)]i > 0 for all i
and consider eq. (4.14) (a):

t=1 i   (xt )(cid:54)=0 = c, where c = o(1)

(cid:32)l   t(cid:48)

   
l(cid:48)=1

ht(cid:48)
l =

(cid:33)

u

w   (xt(cid:48)).

(4.15)

now, let   s look at what happens to eq. (4.15). first, the eigendecomposition of the

matrix u:

u = qsq   1,

where s is a diagonal matrix whose non-zero entries are eigenvalues. q is an orthogo-
nal matrix. then

13 a parametric recti   er, or prelu, is de   ned as

l   t(cid:48)
   
l(cid:48)=1

g(x) =

u = qsl   t(cid:48)q   1,
(cid:26) x,

if x     0
otherwise ,

ax,

where a is a parameter to be estimated together with all the other parameters of a network.

48

and

(cid:33)

u

(cid:32)l   t(cid:48)

   
l(cid:48)=1

w   (xt(cid:48)) = diag(sl   t(cid:48)

(cid:124) (cid:123)(cid:122) (cid:125)

)(cid:12) (qq   1
=i

w   (xt(cid:48) )),

where (cid:12) is an element-wise product.
what happens if the largest eigenvalue emax = maxdiag(s) is larger than 1, the
norm of hl will explode, i.e., (cid:107)hl(cid:107)        . furthermore, due to the assumption that
w   (xt(cid:48)) > 0, each element of hl will explode to in   nity as well. the rate of growth is
exponentially with respect to the length of the input sequence, meaning that even when
the input sequence is not too long, the norm of the memory state grows quickly if emax
is reasonably larger than 1.

this happens, because the nonlinearity g is unbounded. if g is bounded from both
above and below, such as the case with tanh, the norm of the memory state is also
bounded. in the case of tanh : r     [   1,1],

(cid:107)hl(cid:107)     dim(hl).

this is one reason why a logistic function, such as tanh and   , is most widely used
with recurrent neural networks, compared to piecewise linear functions.14 i will call
this recurrent neural network with tanh as an element-wise nonlinear function a simple
recurrent neural network.

is tanh a blessing?

4.3.2
now, the argument in the previous section may sound like tanh and    are the nonlinear
functions that one should use. this seems quite convincing for recurrent neural net-
works, and perhaps so for feedforward neural networks as well, if the network is deep
enough.

here let me try to convince you otherwise by looking at how the norm of backprop-
agated derivative behaves. again, this is much easier to see if we assume the following:

    u is a full rank matrix
    the input sequence is sparse:    l
similarly to eq. (4.14), let us consider a forward computational path until hl, how-

t=1 i   (xt )(cid:54)=0 = c, where c = o(1)

ever without assuming a linear activation function:
hl = g (ug (ug (ug (u (       ) + w   (xl   3)) + w   (xl   2)) + w   (xl   1)) + w   (xl)) .
we will consider a subsequence of this process, in which all the input symbols are 0
except for the    rst symbol:

hl1 = g(cid:0)ug(cid:0)u(cid:0)      g(cid:0)uhl0 + w  (cid:0)xl0+1

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) .

14 however, it is not to say that piecewise linear functions are never used for recurrent neural networks.

see, for instance, [69, 6].

49

it should be noted that as l approaches in   nity, there will be at least one such sub-
sequence whose length also approaches in   nity due to the sparsity of the input we
assumed.

from this equation, let   s look at

     (cid:0)xl0+1

   hl1

(cid:1) .

this measures the effect of the (l0 +1)-th input symbol xl0+1 on the l1-th memory state
of the simple recurrent neural network. this is also the crucial derivative that needs to
be computed in order to compute the gradient of the cost function using the automated
id26 procedure described in sec. 3.4.

this derivative can be rewritten as

among these three terms in the left hand side, we will focus on the    rst one (a) which
can be further expanded as

   hl1

     (cid:0)xl0+1
                  

(cid:124) (cid:123)(cid:122) (cid:125)

   hl1
   hl1   1

(c)

(cid:1) =

   hl0+1
   hl0+1

   hl1
   hl0+1

(cid:124) (cid:123)(cid:122) (cid:125)

(a)

                     hl1   1

(cid:124) (cid:123)(cid:122) (cid:125)

   hl1   1

(b)

   hl1   1
   hl1   2

(cid:124) (cid:123)(cid:122) (cid:125)

(c)

(cid:1) .

   hl0+1

     (cid:0)xl0+1
                     hl0+2
                        

(cid:124) (cid:123)(cid:122) (cid:125)

   hl0+2

(b)

                     hl1

(cid:124)(cid:123)(cid:122)(cid:125)

   hl1
(b)

                   .

   hl0+2
   hl0+1

(cid:124) (cid:123)(cid:122) (cid:125)

(c)

   hl1
   hl0+1

=

because this is a recurrent neural network, we can see that the analytical forms for
the terms grouped by the parentheses in the above equation are identical except for the
subscripts indicating the time index. in other words, we can simply only on one of
those groups, and the resulting analytical form will be generally applicable to all the
other groups.

first, we look at eq. (4.16) (b), which is nothing but a derivative of a nonlinear
activation function used in this simple recurrent neural network. the derivatives of the
widely used logistic functions are

  (cid:48)(x) =   (x)(1       (x)),

tanh(cid:48)(x) =1    tanh2(x),

as described earlier in sec. 3.4.1. both of these functions    derivatives are bounded:

(4.16)

(4.17)
(4.18)

in the simplest case in which g is a linear function (i.e., x = g(x),) we do not even

need to look at

from eq. (4.16).

0 <   (cid:48)(x)     0.25,
0 < tanh(cid:48)(x)     1.

(cid:13)(cid:13)(cid:13)    ht

   ht

(cid:13)(cid:13)(cid:13). we simply ignore all the    ht

   ht

50

next, consider eq. (4.16) (c). in this case of simple recurrent neural network, we
notice that we have already learned how to compute this derivative earlier in sec. 3.3.2:

from these two, we get

   hl1
   hl0+1

=

=

   ht+1
   ht

= u

(cid:19)(cid:18)   hl1   1
(cid:18)   hl1
(cid:18)   ht
(cid:19)
   hl1
l1   

   hl1   1

u

u

.

   ht

t=l0+2

(cid:19)

u

      

(cid:18)   hl0+2

   hl0+2

(cid:19)

u

do you see how similar it looks like eq. (4.15)? if the recurrent activation function

f is linear, this whole term reduces to
   hl1
   hl0+1

= ul1   l0+1,

which according to sec. 4.3.1, will explode as l         if

emax > 1,

where emax is the largest eigenvalue of u. when emax < 1, it will vanish, i.e., (cid:107)    hl1
   hl0+1
0, exponentially fast.
what if the recurrent activation function f is not linear at all? let   s look at    ht
   ht

(cid:107)   

u as

               
(cid:124)

   ht
   ht

u =

(cid:0)qsq   1(cid:1) ,

               
(cid:125)

(cid:19)

(cid:12) s

q   1.

f (cid:48)
1
0
...
0

0
f (cid:48)
2
...
0

=diag

(cid:18)

0
0
...
f (cid:48)
nh

(cid:19)

      
      
      
(cid:123)(cid:122)
      
(cid:17)
(cid:16)    ht
(cid:18)   ht

   ht

   ht

   ht
   ht

u = q

diag

where we used the eigendecomposition of u = qsq   1. this can be re-written into

this means that the eigenvalue of u will be scaled by the derivative of the recurrent ac-
tivation function at each timestep. in this case, we can bound the maximum eigenvalue
of    ht
   ht

u by

max        emax,
et
where    is the upperbound on g(cid:48) =    ht
. see eqs. (4.17)   (4.18) for the upperbounds of
   ht
the sigmoid and hyperbolic tangent functions.

51

in other words, if the largest eigenvalue of u is larger than 1

   , it is likely that this
temporal derivative of hl1 with respect to hl0+1 will explode, meaning that its norm will
grow exponentially large. in the opposite case of emax < 1
   , the norm of the temporal
derivative likely shrinks toward 0. the former case is referred to as exploding gradient,
and the latter vanishing gradient. these cases were studied already at the very early
years of research in recurrent neural networks [9, 52].

using tanh is a blessing in recurrent neural networks when running the network
forward, as i described in the previous section. this is however not necessarily true in
the case of backpropagating derivaties. especially because, there is a higher chance of
vanishing gradient with tanh, or even worse with   . why? because 1
   > 1 for almost
everywhere.

4.3.3 are we doomed?
exploding gradient fortunately it turned out that the phenomenon of exploding
gradient is quite easy to address. first, it is straightforward to detect whether the ex-
ploding gradient happened by inspecting the norm of the gradient fo the cost with

respect to the parameters(cid:13)(cid:13)        c(cid:13)(cid:13). if the gradient   s norm is larger than some prede   ned

threhold    > 0, we can simply renormalize the norm of the gradient to be   . otherwise,
we leave it as it is.
in mathematics,

(cid:40)

      =

      (cid:107)   (cid:107) ,
   ,

if (cid:107)   (cid:107) >   
otherwise

,

      is a rescaled gradient update
where we used the shorthand notiation     for         c.
direction which will be used by the stochastic id119 (sgd) algorithm from
sec. 2.2.2. this algorithm is referred to as gradient clipping [83].

vanishing gradient what about vanishing gradient? but,    rst, what does vanishing
gradient mean? we need to understand the meaning of this phenomenon in order to tell
whether this is a problem at all from the beginning.
let us consider a case the variable-length output where |x| = |y| from sec. 4.1.4.
let   s assume that there exists a clear dependency between the output label yt and the
input symbol xt(cid:48), where t(cid:48) (cid:28) t. this means that the empirical cost will decrease when
the weights are adjusted such that

log p(yt = y   

t | . . . ,   (xt(cid:48)), . . .)

is maximized, where y   
has great in   uence on the t-th output yt, and the in   uence can be measured by

t is the ground truth output label at time t. the value of    (xt(cid:48))

    log p(yt = y   
      (xt(cid:48))
instead of exactly computing     log p(yt =y   
t |...)
      (xt(cid:48) )

t | . . .)

.

, we can approximate it by the    nite
difference method. let        rdim(   (xt(cid:48) )) be a vector of which each element is a very

52

small real value (       0.) then,
t | . . .)

    log p(yt = y   
      (xt(cid:48))

= lim
     0

(log p(yt = y   
    log p(yt = y   

t | . . . ,   (xt(cid:48)) +   , . . .)
t | . . . ,   (xt(cid:48)), . . . , ))(cid:11)   ,

where (cid:11) is an element-wise division. this shows that     log p(yt =y   
computes the
      (xt(cid:48) )
difference in the t-th output id203 with respect to the change in the value of the
t(cid:48)-th input.

t |...)

in other words     log p(yt =y   
      (xt(cid:48) )

directly re   ects the degree to which the t-th output
yt depends on the t(cid:48)-th input xt(cid:48), according to the network. to put it in another way,
    log p(yt =y   
re   ects how much dependency the recurrent neural network has cap-
      (xt(cid:48) )
tured the dependency between yt and xt(cid:48).

t |...)

t |...)

let   s rewrite

    log p(yt = y   
      (xt(cid:48))

t | . . .)

    log p(yt = y   

t | . . .)

   ht

=

   ht
   ht   1

(cid:124)

(cid:123)(cid:122)
(cid:125)
          ht(cid:48)+1
   ht(cid:48)

(a)

   ht(cid:48)
      (xt )

.

the terms marked with (a) looks exactly identical to eq. (4.16). we have already seen
that this term can easily vanish toward zero with a high id203 (see sec. 4.3.2.)

this means that the recurrent neural network is unlikely to capture this dependency.
this is especially true when the (temporal) distance between the output and input, i.e.,
|t    t(cid:48)| (cid:29) 0.
the biggest issue with this vanishing behaviour is that there is no straightforward
way to avoid it. we cannot tell whether     log p(yt =y   
    0 is due to the lack of this
      (xt(cid:48) )
dependency in the true, underlying function or due to the wrong con   guration (param-
eter setting) of the recurrent neural network. if we are certain that there are indeed
these long-term dependencies, we may simultaneously minimize the following auxil-
iary term together with the cost function:

t |...)

      1   

(cid:13)(cid:13)(cid:13)       c
(cid:13)(cid:13)(cid:13)       c

   ht+1

   ht+1

(cid:13)(cid:13)(cid:13)

   ht+1
   ht

(cid:13)(cid:13)(cid:13)

      2

.

t

   

t=1

this term, which was introduced in [83], is minimized when the norm of the derivative
does not change as it is being backpropagated, effectively forcing the gradient not to
vanish.

this term however was found to help signi   cantly only when the target task, or the
underlying function, does indeed exhibit long-term dependencies. how can we know
in advance? pascanu et al. [83] showed this with the well-known toy tasks which were
speci   cally designed to exhibit long-term dependencies [52].

4.3.4 id149 address vanishing gradient
will the same problems of vanishing gradient happen with the id149
(gru) or the long short-term memory units (lstm)? let us write the memory state at

53

time t:

ht =ut (cid:12)   ht + (1    ut )(cid:12)(cid:0)ut   1 (cid:12)   ht   1 + (1    ut   1)(cid:12)(cid:0)ut   2 (cid:12)   ht   2 + (1    ut   2)(cid:12) (       )(cid:1)(cid:1)

=ut (cid:12)   ht + (1    ut )(cid:12) ut   1 (cid:12)   ht   1 + (1    ut )(cid:12) (1    ut   1)(cid:12) ut   2 (cid:12)   ht   2 +      

let   s be more speci   c and see what happens to this with respect to xt(cid:48):

ht =ut (cid:12)   ht + (1    ut )(cid:12) ut   1 (cid:12)   ht   1 + (1    ut )(cid:12) (1    ut   1)(cid:12) ut   2 (cid:12)   ht   2 +      

(cid:33)

+

   

k=t,...,t(cid:48)+1

(1    uk)

(cid:123)(cid:122)

(a)

(cid:12)tanh (w   (xt(cid:48)) + u (rt(cid:48) (cid:12) ht(cid:48)   1)) ,

(4.19)

(cid:12) ut(cid:48)
(cid:125)

(cid:32)
(cid:124)

where     is for element-wise multiplication.

what this implies is that the gru effectively introduces a shortcut from time t(cid:48) to
t. the change in xt(cid:48) will directly in   uence the value of ht, and subsequently the t-th
output symbol yt. in other words, all the issue with the simple recurrent neural network
we discussed earlier in sec. 4.3.3.

the update gate controls the strength of these shortcuts. let   s assume for now that
the update gate is    xed to some prede   ned value between 0 and 1. this effectively
makes the gru a leaky integration unit [6]. however, as it is perhaps clear from
eq. (4.19) that we will inevitably run into an issue. why is this so?

let   s say we are sure that there are many long-term dependencies in the data. it is
natural to choose a large coef   cient for the leaky integration unit, meaning the update
gate is close to 1. this will de   nitely help carrying the dependency across many time
steps, but this inevitably carries unnecessary information as well. this means that
much of the representational power of the output function gout(ht ) is wasted in ignoring
those unnecessary information.

if the update gate is    xed to something substantially smaller than 1, all the shortcuts
(see eq. (4.19) (a)) will exponentially vanish. why? because it is a repeated multipli-
cation of a scalar small than 1. in other words, it does not really help to have a leaky
integration unit in the place of a simple tanh unit.

this is however not the case with the actual gru or lstm, because those update
gates are not    xed but are adaptive with respect to the input. if the network detects
that there is an important dependency being captured, the update gate will be closed
(u j     0.) this will effectively strengthen the shortcut connection (see eq. (4.19) (a).)
when the network detects that there is no dependency anymore, it will open the update
gate (u j     1), which effectively cuts off the shortcut. how does the network know, or
detect, the existence or lack of these dependencies? do we need to manually code this
up? i will leave these questions for you to    gure out.

54

chapter 5

neural language models

5.1 id38: first step

what does it mean for a machine to understand natural language? in other words, how
can we tell that the machine understood natural language? these are the two equivalent
questions that are at the core of this course.

one of the most basic capability of a machine that can signal us that it indeed
understands natural language is for the machine to tell how likely a given sentence
is. of course, this is extremely ill-de   ned, as we probably cannot de   ne the likeliness
of a sentence, because there are many different types of unlikeliness. for instance,
a sentence    colorless green ideas sleep furiously    from chomsky   s [32] is unlikely
according to our common sense, because

1. an object (   idea   ) cannot be both    colorless    and    green.   
2. an object cannot    sleep       furiously.   
3. an    idea    does not    sleep.   

on the other hand, this sentence is a grammatically correct sentence.

let   s take a look at another sentence    jane and me went to see a movie yesterday.   
grammatically, this is not the most correct sentence one can make. it should be    jane
and i went to see a movie yesterday.    even with a grammatical error in the original
sentence however, the meaning of the sentence is clear to me, and perhaps is much more
understandable than the sentence    colorless green ideas sleep furiously.    furthermore,
many people likely say this (saying    me    instead of    i   ) quite often. this sentence is
thus likely according to our common sense, but is not likely according to the grammar.
this observation makes us wonder what is the criterion to use. is it correct for a
machine to tell whether the sentence is likely by analyzing its grammatical correctness?
or, is it possible that the machine should deem a sentence likely only when its meaning
agrees well with common sense regardless of its grammatical correctness in the most
strict sense?

as we discussed in the    rst lecture of the course, we are more interested in ap-
proaching natural language as a means for one to communicate ideas to a listener. in

55

this sense, language use is a function which takes as input the surrounding environ-
ment, including the others    speech, and returns linguistic response, and this function
is not given but learned via observing others    use of language and the reinforcement
by the surrounding environment [97]. also, throughout this course, we are not con-
cerned too much about the existing syntactic, or grammatical, structures underlying
natural language, which makes it dif   cult for us to say anything about the grammatical
correctness of a given sentence.

in short, we take the route here that the likeliness of a sentence be determined
based on its agreement with common sense. the common sense here is captured by
everyday use of natural language, which consequently implies that the statistics of
natural language use can be a strong indicator for determining the likely of a natural
language sentence.

5.1.1 what if those linguistic structures do exist
of course, as we discussed earlier in sec. 1.1 and in this section, not everyone agrees.
this is due to the fact that a perfect grammatical sentence may be considered unlikely,
just because it does not happen often. in other words, statistical approaches to language
modeling may conclude that a sentence with perfectly valid grammatical construction
is unlikely. is this a problem?

this problem of telling how likely a given sentence is can be viewed very naturally
as building a probabilistic model of sentences. in other words, given a sentence s, what
is the id203 p(s) of s? let us brie   y talk about what this means for the case of
viewing the likeliness of a sentence as equivalent to its grammatical correctness.1

we    rst assume that there is an underlying linguistic structure g which has gener-
ated the observed sentence s. of course, we do not know the correct g in advance, and
unfortunately no one will tell us what the correct g is.2 thus, g is a hidden variable
in this case. this hidden structure g generates the observed sentence s according to an
unknown conditional distribution p(s|g). each and every grammatical structure g is
assigned a prior id203 which is also unknown in advance.3
with the conditional distribution s|g and the prior distribution g, we easily get the

joint distribution s,g by

p(s,g) = p(s|g)p(g),

from the de   nition of id155.4 from this joint distribution we get the

1 why brie   y and why here? because, we will not pursue this line at all after this section.
2 here, the correct g means the g that generated s, not the whole structure of g which is assumed to

exist according to a certain set of rules.

3 this is not necessarily true. if we believe that each and every grammatical correct sentence is equally
likely and that each correct grammatical structure generates a single corresponding sentence, the prior dis-
tribution over the hidden linguistic structure is such that any correct structure is given an equal id203
while any incorrect structure is given a zero id203. but, of course, if we think about it, there are clearly
certain structures that are more prevalent and others that are not.

4 a id155 of a given b is de   ned as
p(a|b) =

p(a,b)
p(b)

56

distribution over a given sentence s by marginalizing out g:

p(s) =    

g

p(s,g).

this means that we should compute how likely a given sentence s is with respect to all
possible underlying linguistic structure. this is very likely intractable, because there
must be in   nite possible such structures.

instead of computing p(s) exactly we can simply look at its lowerbound. for in-

stance, one simplest, and probably not the best, way to do so is

p(s) =    

g

p(s,g)     p(s,   g),

where   g = argmaxg p(s,g) = argmaxg p(g|s).5
this lowerbound is tight, i.e., p(s) = p(s,   g), when there is only a single true un-
derlying linguistic structure   g given s. what this says is that there is no other possible
linguistic structure possible for a single observed sentence, i.e., no ambiguity in infer-
ring the correct linguistic structure. in other words, we can compute the id203 or
likeliness of a given sentence by inferring its correct underlying linguistic structure.

however, there are a few issues here. first, it is not clear which formalism g
follows, and we have brie   y discussed about this at the very beginning of this course.
second, it is quite well known that most of the formalisms do indeed have uncertainty
in id136. again, we looked at one particular example in sec. 1.1.2. these two
issues make many people, including myself, quite uneasy about this type of model-
based approaches.

in the remaining of this chapter, i will thus talk about model-free approaches (as

opposed to these model-based approaches.)

5.1.2 quick note on linguistic units
before continuing, there is one question that must be bugging you, or at least has
bugged me a lot: what is the minimal linguistic unit?

if we think about written text, the minimal unit does seem like a character. with
spoken language, the minimal unit seems to be a phoneme. but, is this the level at
which we want to model the process of understanding natural language? in fact, to
most of the existing natural language processing researchers as well as some (or most)
linguists, the answer to this question is a hard    no.   

the main reason is that these low-level units, both characters and phonemes, do not
convey any meaning themselves. does a latin alphabet    q    have its own meaning? the
answer by most of the people will be    no.    then, starting from this alphabet    q   , how
far should we climb up in the hierarchy of linguistic units to reach a level at which the
unit begins to convey its own meaning?    qu    does not seem to have its own meaning
still.    qui    in french means    who   , but in english it does not really say much.    quit   
in english is a valid word that has its own meaning, and similarly    quiet    is a valid
word that has its own meaning, quite apart from that of    quit.   

5 this inequality holds due to the de   nition of id203, which states that p(x)     0 and    x p(x) = 1.

57

it looks like a word is the level at which meaning begins to form itself. however,

this raises a follow-up question on the de   nition of a word: what is a word?

it is tempting to say that a sequence of non-blank characters is a word. this makes
everyone   s life so much easier, because we can simply split each sentence by a blank
space to get a sequence of words. unfortunately this is a very bad strategy. the sim-
plest counter example to this de   nition of words is a token (which i will use to refer to
a sequence of non-blank characters) consisting of a word followed by a punctuation.
if we simply split a sentence into words by blank spaces, we will get a bunch of re-
dundant words. for instance,    llama   ,    llama,   ,    llama.   ,    llama?   ,       llama   ,    llama      
and    llama!    will all be distinct words. we will run into an issue of exploding vocab-
ulary with any morphologically rich language. furthermore, in some languages such
as chinese, there is no blank space at all inside a sentence, in which case this simple
strategy will completely fail to give us any meaningful, small linguistic unit other than
sentences.

now at this point it almost seems like the best strategy is to use each character
as a linguistic unit. this is not necessarily true due to the highly nonlinear nature of
orthography.6 there are many examples in which this nonlinear nature shows its dif-
   culty. one such example is to consider the following three words:    quite   ,    quiet   
and    quit   .7 all three character sequences have near identical forms, but their corre-
sponding meanings differ from each other substantially. in other words, any function
that maps from these character sequences to the corresponding meanings will have to
be extremely nonlinear and thus dif   cult to be learned from data. of course, this is an
area with active research, and i hope i am not giving you an impression that characters
are not the units to use (see, e.g., [61].)

now then the question is whether there is some middle ground between characters
and words (or blank-space-separated tokens) that are more suitable to be used as ele-
mentary linguistic units (see, e.g., [93].) unfortunately this is again an area with active
research. hopefully, we will have time later in the course to discuss this issue further.
for now, we will simply use blank-space-separated tokens as our linguistic units.

5.2 statistical id38
regardless of which linguistic unit we use, any natural language sentence s can be
represented as a sequence of t discrete symbols such that

s = (w1,w2, . . . ,wt ).

each symbol is one element from a vocabulary v which contains all possible symbols:

v =(cid:8)v1,v2, . . . ,v|v|(cid:9) ,

where |v| is used to mean the size of the vocabulary, or the number of all symbols.

6 orthography is de   ned as    the study of spelling and how letters combine to represent sounds and form

words.   

7 i would like to thank bart van merrienboer for this example.

58

the problem of id38 is equivalent to    nding a model that assigns a

id203 p(s) to a sentence:

p(s) = p(w1,w2, . . . ,wt ).

(5.1)

of course, we are not given this distribution and need to learn this from data.

let   s say we are given data d which contains n sentences such that

d =(cid:8)s1,s2, . . . ,sn(cid:9) ,

where each sentence sn is

2, . . . ,wn
meaning that each sentence has a different length.

sn = (wn

1,wn

t n),

given this data d, let us estimate the id203 of a certain sentence s. this is

quite straightforward:

where i is the indicator function de   ned earlier in eq. (3.7) which is de   ned as

,

(5.2)

p(s) =

n=1 is=sn
   n

n

(cid:26) 1,

is=sn =

if s = sn
0, otherwise

this is equivalent to counting how many times s occurs in the data.8

5.2.1 data sparsity/scarcity
has this solved the whole problem of language model? no, unfortunately not. the
very major issue here is that however large your corpus is, it is unlikely to contain all
reasonable sentences in the world. let   s do simple counting here.
there are |v| symbols in a vocabulary. each sentence can be as long as t symbols.
then, there are |v|t possible sentences. a reasonable range for the sentence length t
is roughly between 1 to 50, meaning that there are

|v|t

50

   

t =1

possible sentences. as it   s quite clear, this is a huge space of sentences.

of course, not all those sentences are plausible. this is however conceivable that
even the fraction of that space will be gigantic, especially considering that the size of
vocabulary often goes up to 100k to 1m words. many of the plausible sentences will
not appear in the corpus. is this true? in fact, yes, it is.

it is quite easy to    nd such an example. for instance, google books ngram
viewer9 lets you search for a sentence or a sequence of up to    ve english words from

8 a data set consisting of (written) text is often referred to as a corpus.
9 https://books.google.com/ngrams

59

figure 5.1: a picture of a llama lying down. from https://en.wikipedia.
org/wiki/llama

the gigantic corpus of google books. let me try to search for a very plausible sen-
tence    i like llama,    and the google books ngram10 viewer returns an error saying
that    ngrams not found: i like llama.    (see fig. 5.1 in the case you are not familiar
with a llama.) see fig. 5.2 as an evidence.

figure 5.2: a resulting page of google books ngram viewer for the query    i like
llama   .

what does this mean for the estimate in eq. (5.2)? it means that this estimator will
be too harsh for many of the plausible sentences that do not occur in the data. as soon
as a given sentence does not appear exactly as it is in the corpus, this estimator will
say that there is a zero id203 of the given sentence. although the sentence    i like
llama    is a likely sentence, according to this estimator in eq. (5.2), it will be deemed
extremely unlikely.

this problem is due to the issue of data sparsity. data sparsity here refers to the

10 we will discuss what ngrams are in the later sections.

60

phenomenon where a training set does not cover the whole space of input suf   ciently.
in more concrete terms, most of the points in the input space, which have non-zero
probabilities according to the true, underlying distribution, do not appear in the training
set. if the size of a training set is assumed to be    xed, the severity of data sparsity
increases as the average, or maximum length of the sentences. this follows from the
fact that the size of the input space, the set of all possible sentences, grows with respect
to the maximum possible length of a sentence.

in the next section, we will discuss the most straightforward approach to addressing

this issue of data sparsity.

5.3

id165 language model

the fact that the issue of data sparsity worsens as the maximum length of sentences
grows hints us a straightforward approach to addressing this: limit the maximum length
of phrases/sentences we estimate a id203 on. this idea is a foundation on which
a so-called id165 language model is based.

in the id165 language model, we    rst rewrite the id203 of a given sentence

s from eq. (5.1) into

p(s) = p(w1,w2, . . . ,wt ) = p(w1)p(w2|w1)       p(wk|w<k)

       p(wt|w<t ),

(5.3)

(cid:124)

(cid:123)(cid:122)

(a)

(cid:125)

where w<k denotes all the symbols before the k-th symbol wk. from this, the n-
gram language model makes an important assumption that each id155
(eq. (5.3) (a)) is only conditioned on the n    1 preceding symbols only, meaning

p(wk|w<k)     p(wk|wk   n,wk   n+1, . . . ,wk   1).

this results in

p(s)     t
   

t=1

p(wt|wt   n, . . . ,wt   1).

what does this mean? under this assumption we are saying that any symbol in a
sentence is predictable based on the n    1 preceding symbols. this is in fact a quite
reasonable assumption in many languages. for instance, let us consider a phrase    i am
from   . even without any more context information surrounding this phrase, such as
surrounding words and the identity of a speaker, we know that the word following this
phrase will be likely a name of place or country. in other words, the id203 of a
name of place or country given the three preceding words    i am from    is higher than
that of any other words.

but, of course, this assumption does not always work. for instance, consider a
.    let us focus on

phrase    in korea, more than half of all the residents speak korean

(cid:124) (cid:123)(cid:122) (cid:125)

(a)

the last word    korean    (marked with (a).) we immediately see that it will be useful
to condition its id155 on the second word    korea   . why is this so?

61

because the id155 of    korean    following    speak    should signi   cantly
increase over all the other words (that correspond to other languages) knowing the fact
that the sentence is talking about the residents of    korea   . this requires the conditional
distribution to be conditioned on at least 10 words (   ,    is considered a separate word,)
and this certainly will not be captured by id165 language model with n < 9.

from these examples it is clear that there   s a natural trade-off between the quality
of id203 estimate and statistical ef   ciency based on the choice of n in id165
id38. the higher n the longer context the conditional distribution has,
leading to a better model/estimate (second example,) however resulting in a situation
of more sever data sparsity (see sec. 5.2.1.) on the other hand, the lower n leads to
the worse id38 (second example), but this will avoid the issue of data
sparsity.

id165 id203 estimation we can estimate the id165 id155
p(wk|wk   n, . . . ,wk   1) from the training corpus. since it is a id155, we
need to rewrite it according to the de   nition of the id155:

p(wk|wk   n, . . . ,wk   1) =

p(wk   n, . . . ,wk   1,wk)

p(wk   n, . . . ,wk   1)

(5.4)

this rewrite implies that the id165 id203 is equivalent to counting the occur-
rences of the id165 (wk   n, . . . ,wk) among all id165s starting with (wk   n, . . . ,wk   1).
let us consider the denominator    rst. the denominator can be computed by the
marginalizing the k-th word (w(cid:48) below):

p(wk   n, . . . ,wk   1) =    
w(cid:48)   v

p(wk   n, . . . ,wk   1,w(cid:48)).

(5.5)

from eq. (5.2), we know how to estimate p(wk   n, . . . ,wk   1,w(cid:48)):
p(wk   n, . . . ,wk   1,w(cid:48))     c(wk   n, . . . ,wk   1,w(cid:48))

nn

,

(5.6)

where c(  ) is the number of occurrences of the given id165 in the training corpus, and
nn is the number of all id165s in the training corpus.

now let   s plug eq. (5.6) into eqs. (5.4)   (5.5):

p(wk|wk   n, . . . ,wk   1) =

  1
nn

c(wk   n, . . . ,wk   1,wk)

nn    w(cid:48)   v c(wk   n, . . . ,wk   1,w(cid:48))
  1

(5.7)

5.3.1 smoothing and back-off
note that i am missing many references this section, as i am writing this on my travel.
i will    ll in missing references once i   m back from my travel.

the biggest issue of having an id165 that never occurs in the training corpus is
that any sentence containing the id165 will be given a zero id203 regardless
of how likely all the other id165s are. let us continue with the example of    i like

62

llama   . with an id165 language model built using all the books in google books, the
following, totally valid sentence11 will be given a zero id203:

       i like llama which is a domesticated south american camelid.12

why is this so? because the id203 of this sentence is given as a product of all
possible trigrams:

p(   i   ,    like   ,    llama   ,    which   ,    is   ,    a   ,    domesticated   ,    south   ,    american   ,    camelid   )
=p(   i   )p(   like   |   i   ) p(   llama   |   i   ,   like   )

       p(   camelid   |   south   ,   american   )

(cid:124)

(cid:123)(cid:122)

=0

(cid:125)

=0

one may mistakenly believe that we can simply increase the size of corpus (col-
lecting even more data) to avoid this issue. however, remember that    data sparsity is
almost always an issue in statistical modeling    [24], which means that more data call
for better statistical models with often more parameters leading to the issue of data
sparsity.

one way to alleviate this problem is to assign a small id203 to all unseen
id165s. at least, in this case, we will assign some small, non-zero id203 to
any sentence, thereby avoiding a valid, but zero-id203 sentence under the id165
language model. one simplest implementation of this approach is to assume that each
and every id165 occurs at least    times and any occurrence in the training corpus is
in addition to this background occurrence.

in this case, the estimate of an id165 becomes

p(wk|wk   n, . . . ,wk   1) =

=

   + c(wk   n,wk   n+1, . . . ,wk)

   w(cid:48)   v (   + c(wk   n,wk   n+1, . . . ,w(cid:48)))
  |v| +    w(cid:48)   v c(wk   n,wk   n+1, . . . ,w(cid:48))

   + c(wk   n,wk   n+1, . . . ,wk)

,

where c(wk   n,wk   n+1, . . . ,wk) is the number of occurrences of the given id165 in the
training corpus. c(wk   n,wk   n+1, . . . ,w(cid:48)) is the number of occurrences of the given n-
gram if the last word wk is substituted with a word w(cid:48) from the vocabulary v .    is often
set to be a scalar such that 0 <        1. see the difference from the original estimate in
eq. (5.7).

it is quite easy to see that this is a quite horrible estimator: how does it make sense
to say that every unseen id165 occurs with the same frequency? also, knowing that
this is a horrible approach, what can we do about this?
one possibility is to smooth the id165 id203 by interpolating between the
estimate of the id165 id203 in eq. (5.7) and the estimate of the (n    1)-gram
id203. this can written down as
ps(wk|wk   n, . . . ,wk   1) =   (wk   n, . . . ,wk   1)p(wk|wk   n, . . . ,wk   1)

+ (1       (wk   n, . . . ,wk   1))ps(wk|wk   n+1, . . . ,wk   1).

(5.8)

11 this is not strictly true, as i should put    a    in front of the llama.
12 the description of a llama taken from wikipedia: https://en.wikipedia.org/wiki/llama

63

this implies that the id165 (smoothed) id203 is computed recursively by the
lower-order id165 probabilities. this is clearly an effective strategy, considering that
falling off to the lower-order id165s contains at least some information of the original
id165, unlike the previous approach of adding a scalar    to every possible id165.

now a big question here is how the interpolation coef   cient    is computed. the
simplest approach we can think of is to    t it to the data as well. however, the situ-
ation is not that easy, as using the same training corpus, which was used to estimate
p(wk|wk   n, . . . ,wk   1) according to eq. (5.7), will lead to a degenerate case. what is
this degenerate case? if the same corpus is used to    t both the non-smoothed id165
id203 and       s, the optimal solution is to simply set all       s to 1, as that will assign
the high probabilities to all the id165s. therefore, one needs to use a separate corpus
to    t       s.

more generally, we may rewrite eq. (5.8) as

(cid:26)   (wk|wk   n, . . . ,wk   1), if c(wk   n, . . . ,wk   1,wk) > 0

  (wk   n+1, . . . ,wk)ps(wk|wk   n+1, . . . ,wk   1), otherwise
(5.9)

ps(wk|wk   n, . . . ,wk   1) =

following the notation introduced in [63]. speci   c choices of    and    lead to a number
of different smoothing techniques. for an extensive list of these smoothing techniques,
see [24].

before ending this section on smoothing techniques for id165 id38,
let me brie   y describe one of the most widely used smoothing technique, called the
modi   ed kneser-ney smoothing (kn smoothing), described in [24]. this modi   ed
kn smoothing is ef   ciently implemented in the open-source software package called
kenlm [51].

first, let us de   ne some quantities. we will use nk to denote the total number of
id165s that occur exactly k times in the training corpus. with this, we de   ne the
following so-called discounting factors:

n1

y =
n1 + 2n2
n2
d1 =1    2y
n1
n3
d2 =2    3y
n2
n4
d3+ =3    4y
n3

.

also, let us de   ne the following quantities describing the number of all possible words
following a given id165 with a speci   ed frequency l:

nl(wk   n, . . . ,wk   1) = |{c(wk   n, . . . ,wk   1,wk) = l}|

the modi   ed kn smoothing then de   nes    in eq. (5.9) to be
  (wk|wk   n, . . . ,wk   1) =

c(wk   n, . . . ,wk   1,wk)    d(c(wk   n, . . . ,wk   1,wk))

   w(cid:48)   v c(wk   n, . . . ,wk   1,w(cid:48))

,

64

where d is

and,    is de   ned as

  (wk   n, . . . ,wk   1) =

                     

d(c) =

0,
d1,
d2,
d3+,

if c = 0
if c = 1
if c = 2
if c     3

d1n1(wk   n, . . . ,wk   1) + d2n2(wk   n, . . . ,wk   1) + d3+n3+(wk   n, . . . ,wk   1)

   w(cid:48)   v c(wk   n, . . . ,wk   1,w(cid:48))

.

for details on how this modi   ed kn smoothing has been designed, see [24].

5.3.2 lack of generalization
although id165 language modelling works like a charm in many cases. this is still
not totally satisfactory, because of the lack of generalization. what do i mean by
generalization here?

consider an example where three trigrams13 were observed from a training corpus:
   chases a cat   ,    chases a dog    and    chases a rabbit   . there is a clear pattern here. the
pattern is that it is highly likely that    chases a    will be followed by an animal.

how do we know this? this is a trivial example of humans    generalization abil-
ity. we have noticed a higher-level concept, in this case an animal, from observing
words such as    cat   ,    dog    and    rabbit   , and based on this concept, we generalize this
knowledge (that    chases a    is followed by an animal) to unseen trigrams in the form of
   chases a [animal]   .

this however does not happen with id165 language model. as an example, let   s
consider a trigram    chases a llama   . unless this speci   c trigram occurred more than
once in the training corpus, the id155 given by id165 language mod-
eling will be zero.14 this issue is closely related to data sparsity, but the main differ-
ence is that it is not the lack of data, or id165s, but the lack of world knowledge. in
other words, there exist relevant id165s in the training corpus, but id165 language
modelling is not able to exploit these.

at this point, it almost seems trivial to address this issue by incorporating existing
knowledge into language modelling. for instance, one can think of using a dictionary
to    nd the de   nition of a word in interest (continuing on from the previous example,
the de   nition of    llama   ) and letting the language model notice that    llama    is a    a

13 is    trigram    a proper term? certainly not, but it is widely accepted by the whole community of natural
language processing researchers. here   s an interesting discussion on how id165s should be referred to
as, from [77]:    these alternatives are usually referred to as a bigram, a trigram, and a four-gram model,
respectively. revealing this will surely be enough to cause any classicists who are reading this book to stop,
and to leave the    eld to uneducated engineering sorts ... with the declining levels of education in recent
decades ... some people do make an attempt at appearing educated by saying quadgram    

14 here we assume that no smoothing or backoff is used. however, even when these techniques are used,
we cannot be satis   ed, since the id203 assigned to this trigram will be at best reasonable up to the point
that the id165 language model is giving as high id203 as the bigram    chases a   . in other words, we do
not get any generalization based on the fact that a    llama    is an animal similar to a    cat   ,    dog    or    rabbit   .

65

domesticated pack animal of the camel family found in the andes, valued for its soft
woolly    eece.    based on this, the language model should    gure out that the id203
of    chases a llama    should be similar to    chases a cat   ,    chases a dog    or    chases a
rabbit    because all    cat   ,    dog    and    rabbit    are animals according to the dictionary.

this is however not satisfactory for us. first, those de   nitions are yet another
natural language text, and letting the model understand it becomes equivalent to nat-
ural language understanding (which is the end-goal of this whole course!) second,
a dictionary or any human-curated knowledge base is an inherently limited resource.
these are limited in the sense that they are often static (not changing rapidly to re   ect
the changes in language use) and are often too generic, potentially not capturing any
domain-speci   c knowledge.

in the next section, i will describe an approach purely based on statistics of natural

language that is able to alleviate this lack of generalization.

5.4 neural language model
one thing we notice from id165 language modelling is that this boils down to com-
puting the conditional distribution of a next word wk given n     1 preceding words
in other words, the goal of id165 id38 is to    nd a
wk   n, . . . ,wk   1.
function that takes as input n    1 words and returns a id155 of a next
word:

p(wk|wk   n, . . . ,wk   1) = f wk

   (wk   n, . . . ,wk   1).

this is almost exactly what we have learned in chapter 2.
first, we should de   ne the input to this language modelling function. clearly the
input will be a sequence of n    1 words, but the question is how each of these words
will be represented. since our goal is to put the least amount of prior knowledge, we
want to represent each word such that each and every word in the vocabulary is equi-
distant away from the others. one encoding scheme that achieves this goal is 1-of-k
coding.

in this 1-of-k coding scheme, each word i in the vocabulary v is represented as a
binary vector wi whose sum equals 1. to denote the i-th word with the vector wi, we
set the i-th element of the vector wi to be 1 (and consequently all the other elements
are set to zero.) mathematically,

wi = [0,0, . . . ,

, . . . ,0](cid:62)     {0,1}|v|

(5.10)

this kind of vector is often called a one-hot vector.

it is easy to see that this encoding scheme perfectly    ts our goal of having minimal

prior, because

|wi     w j| =

if i (cid:54)= j
otherwise

1(cid:124)(cid:123)(cid:122)(cid:125)

i-th element

(cid:26) 1,

0,

66

now the input to our function is a sequence of n     1 such vectors, which i will
denote by (w1,w2, . . . ,wn   1). as we will use a neural network as a function approx-
imator here,15 these vectors will be multiplied with a weight matrix e. after this, we
get a sequence of continuous vectors (p1,p2, . . . ,pn   1), where

p j = e(cid:62)w j

(5.11)

and e     r|v|  d.

before continuing to build this function, let us see what it means to multiply the
transpose of a matrix with an one-hot vector from left. since only one of the elements
of the one-hot vector is non-zero, all the rows of the matrix will be ignored except for
the row corresponding to the index of the non-zero element of the one-hot vector. this
row is multiplied by 1, which simply gives us the same row as the result of this whole
matrix   vector multiplication. in short, the multiplication of the transpose of a matrix
with an one-hot vector is equivalent to slicing out a single row from the matrix.

in other words, let

                ,

               

e1
e2
...
e|v|

e =

(5.12)

where ei     rd. then,

e(cid:62)wi = ei.

this view has two consequences. first, in practice, it will be much more ef   cient
computationally to implement this multiplication as a simple table look-up. for in-
stance, in python with numpy, do

p = e[i,:]

instead of

p = numpy.dot(e.t, w_i)

second, from this perspective, we can see each row of the matrix e as a continuous-
space representation of a corresponding word. ei will be a vector representation of the
i-th word in the vocabulary v . this representation is often called a id27
and should re   ect the underlying meaning of the word. we will discuss this further
shortly.

closely following [8], we will simply concatenate the continuous-space represen-

tations of the input words such that

p =(cid:2)p1;p2; . . .;pn   1(cid:3)(cid:62)

15 obviously, this does not have to be true, but at the end of the day, it is unclear if there is any parametric

function approximation other than neural networks.

67

this vector p is a representation of n    1 input words in a continuous vector space and
often referred to as a context vector.

this context vector is fed through a composition of nonlinear feature extraction
layers. we can for instance apply the simple transformation layer from eq. (3.8) such
that

h = tanh(wp + b),

(5.13)

where w and b are the parameters.

once a set of nonlinear layers has been applied to the context vector, it   s time to
compute the output id203 distribution. in this case of language modelling, the
distribution outputted by the function is a categorical distribution. we discussed how
we can build a function to return a categorical distribution already in sec. 3.1.2.

as a recap, a categorical distribution de   nes a id203 of one event happening
among k discrete events. the id203 of the k-th event happening is often denoted
as   k, and

k

   

k=1

  k = 1.

therefore, the function needs to return a k-dimensional vector [  1,   2, . . . ,   k]. in this
case of language modelling, k = |v| and   i corresponds to the id203 of the i-th
word in the vocabulary for the next word.

as discussed earlier in sec. 3.1.2, we can use softmax to compute each of those

output probabilities:

p(wn = k|w1,w2, . . . ,wn   1) =   k =

   

k h + ck)

exp(u(cid:62)
|v|
k(cid:48)=1 exp(u(cid:62)

k(cid:48)h + ck(cid:48))

,

(5.14)

where uk     rdim(h).

this whole function is called a neural language model. see fig. 5.3 (a) for the

graphical illustration of neural language model.

5.4.1 how does neural language model generalize to unseen n-

grams?     distributional hypothesis

now that we have described neural language model, let us take a look into what hap-
pens inside. especially, we will focus on how the model generalizes to unseen id165s.
the previously described neural language model can be thought of as a composite
of two function (g     f ). the    rst stage f projects a sequence of context words, or
preceding n    1 words to a continuous vector space:

f : {0,1}|v|  n   1     rd

we will call the resulting vector h a context vector. the second stage g maps this
continuous vector h to the target word id203, by applying af   ne transformation to
the vector h followed by softmax id172.

68

(a)

(b)

figure 5.3: (a) schematics of neural language model.
language model generalizes to an unseen id165.

(b) example of how neural

let us look more closely at what g does in eq. (5.14). if we ignore the effect of
the bias ck for now, we can clearly see that the id203 of the k-th word in the
vocabulary is large when the output vector uk (or the k-th row of the output matrix u)
is well aligned with the context vector h. in other words, the id203 of the next
word being the k-th word in the vocabulary is roughly proportional to the inner product
between the context vector h and the corresponding target word vector uk.

now let us consider two context vectors h j and hk. these contexts are followed by
a similar set of words, meaning that the conditional distributions of the next word are
similar to each other. although these distributions are de   ned over all possibility target
words, let us look at the probabilities of only one of the target words wl:

(cid:16)
(cid:16)

(cid:17)
(cid:17)

,

.

j =p(wl|h j) =
pl
k =p(wl|hk) =
pl
(cid:16)

k is then16

=

exp

zk
z j

pl
j
pl
k

1
z j
1
zk

exp

exp

w(cid:62)
l h j
w(cid:62)
l hk

(cid:17)
w(cid:62)
l (h j     hk)

.

the ratio between pl

j and pl

from this, we can clearly see that in order for the ratio pl
j
pl
k

to be 1, i.e., pl

j = pl
k,

w(cid:62)
l (h j     hk) = 0.

(5.15)

16 note that both pl

j and pl

k are positive due to our use of softmax.

69

1-of-k codingcontinuous-spaceword representationsoftmaxnonlinear projectionthreefourteamsgroupsnow let us assume that wl is not an all-zero vector, as otherwise it will be too dull
a case. in this case, the way to achieve the equality in eq. (5.15) is to drive the context
vectors h j and hk to each other. in other words, the context vectors must be similar
to each other (in terms of euclidean distance) in order to result in similar conditional
distributions of the next word.
what does this mean? this means that the neural language model must project
(n    1)-grams that are followed by the same word to nearby points in the context vec-
tor space, while keeping the other id165s away from that neighbourhood. this is
necessary in order to give a similar id203 to the same word. if two (n    1)-grams,
which are followed by the same word in the training corpus, are projected to far away
points in the context vector space, it naturally follows from this argument that the prob-
ability over the next word will differ substantially, resulting in a bad language model.
let us consider an extreme example, where we do bigram modeling with the train-

ing corpus comprising only three sentences:

    there are three teams left for the quali   cation.
    four teams have passed the    rst round.
    four groups are playing in the    eld.

we will focus on the bold-faced phrases;    three teams   ,    four teams    and    four group   .
the    rst word of each of these bigrams is a context word, and neural language model
is asked to compute the id203 of the word following the context word.

it is important to notice that neural language model must project    three    and    four   
to nearby points in the context space (see eq. (5.13).) this is because the context
vectors from these two words need to give a similar id203 to the word    teams   .
this naturally follows from our discussion earlier on how dot product preserves the
ordering in the space. and, from these two context vectors (which are close to each
other), the model assigns similar probabilities to    teams    and    groups   , because they
occur in the training corpus. in other words, the target word vector uteams and ugroups
will also be similar to each other, because otherwise the id203 of    teams    given
   four    (p(teams|four)) and    groups    given    four    (p(groups|four)) will be very differ-
ent despite the fact that they occurred equally likely in the training corpus.

now, let   s assume the case where we use the neural language model trained on
this tiny training corpus to assign a id203 to an unseen bigram    three groups   .
the neural language model will project the context word    three    to a point in the con-
text space close to the point of    four   . from this context vector, the neural language
model will have to assign a high id203 to the word    groups   , because the context
vector hthree and the target word vector ugroups well align. thereby, even without ever
seeing the bigram    three groups   , the neural language model can assign a reasonable
id203. see fig. 5.3 (b) for graphical illustration.

what this example shows is that neural language model automatically learns the
similarity among different context words (via context vectors h), and also among dif-
ferent target words (via target word vectors uk), by exploiting co-occurrences of words.
in this example, the neural language model learned that    four    and    three    are similar
from the fact that both of them occur together with    teams   . similarly, in the target

70

side, the neural language model was able to capture the similarity between    teams   
and    groups    by noticing that they both follow a common word    four   .

this is a clear, real-world demonstration of the so-called distributional hypothe-
sis. distributional hypothesis states that    words which are similar in meaning appear
in similar distributional contexts    [41]. by observing which words a given word co-
occurs together, it is possible to peek into the word   s underlying meaning. of course,
this is only a partial picture17 into the underlying meaning of each word, or as a mat-
ter of fact a phrase, but surely still a very interesting property that is being naturally
exploited by neural language model.

in neural language model, the most direct way to observe the effect of this dis-
tributional hypothesis/structure is to investigate the    rst layer   s weight matrix e in
eq. (5.12). this weight matrix can be considered as a set of dense vectors of the

words in the input vocabulary(cid:8)e1,e2, . . . ,e|v|(cid:9), and any visualization technique, such

as principal component analysis (pca) or id167 [104], can be used to project each
high-dimensional word vector into a lower-dimensional space (often 2-d or 3-d).

5.4.2 continuous bag-of-words language model:

maximum pseudo   likelihood approach

this is about time someone asks a question why we are only considering the preceding
words when doing language modelling. is it a good assumption that the conditional
distribution over a word is only dependent on preceding words?

in fact, we do not have to do so. we can certainly model a natural language sentence
such that each word in a sentence is conditioned on 2n surrounding words (n words to
the left and n words to the right.) in this case, we get a markov random    eld (mrf)
language model [56].

figure 5.4: an example markov random    eld language model (mrf-lm) with the
order n = 1.

in a markov random    eld (mrf) language model (mrf-lm), we say each word in
a given sentence is a random variable wi. we connect each word with its 2n surrounding
words with undirected edges, and these edges represent the conditional dependency
structure of the whole mrf-lm. an example of an mrf-lm with n = 1 is shown in
fig. 5.4.

a id203 over a markov random    eld is de   ned as a product of clique po-
tentials. a potential is de   ned for each clique as a positive function whose input is
the values of the random variables in the clique.
in the case of mrf-lm, we will
assign 1 as a potential to every clique except for cliques of two random variables (in

17 we will discuss why this is only a partial picture later on.

71

other words, we only use pairwise potentials only.) the pairwise potential between the
words i and j is de   ned as

(cid:16)
(e(cid:62)wi)(cid:62)e(cid:62)w j(cid:17)

(cid:16)

(cid:17)

,

= exp

e(cid:62)
wiew j

   (wi,w j) = exp

where e is from eq. (5.12), and wi is the one-hot vector of the i-th word. one must
note that this is one possible implementation of the pairwise potential, and there may be
other possibilities, such as to replace the dot product between the word vectors (e(cid:62)
wiew j)
with a deeper network.

with this pairwise potential, the id203 over the whole sentence is de   ned as

p(w1,w2, . . . ,wt ) =

1
z

t   n
   

t=1

t+n

   

j=t

   (wt ,w j) =

1
z

exp

(cid:32)t   n

   

t=1

(cid:33)

e(cid:62)
wt ew j

,

where z is the id172 constant. this id172 constant makes the product
of the potentials to be a id203 and often is at the core of computational intractabil-
ity in markov random    elds.

figure 5.5: gray nodes indicate the markov blank of the fourth word.

although compute the full sentence id203 is intractable in this mrf-lm, it is
quite straightforward to compute the id155 of each word wi given all
the other words. when computing the id155, we must    rst notice that
the id155 of wi only depends on the values of other words included
in its markov blanket. in the case of markov random    elds, the markov blanket of
a random variable is de   ned as a set of all immediate neighbours, and it implies that
the id155 of wi is dependent only on n preceding words and the n
following words. see fig. 5.5 for an example.

keeping this in mind, we can easily see that

p(wi|wi   n, . . . ,wi   1,wi+1, . . . ,wi+n) =

1
z(cid:48) exp
where z(cid:48) is a id172 constant computed by

(cid:32)

(cid:32) n

z(cid:48) =    
v   v

exp

e(cid:62)
v

ewi   k +

n

   

k=1

   

k=1

ewi+k

.

(cid:33)(cid:33)

ewi   k +

ewi+k

,

n

   

k=1

(cid:32) n

(cid:32)

e(cid:62)
wi

k=1

   
(cid:33)(cid:33)

do you see a stark similarity to neural language model we discussed earlier? this
id155 is a shallow neural network with a single linear hidden layer

72

figure 5.6: continuous bag-of-words model approximates the conditional distribution
over the j-th word w j under the mrf-lm.

whose input are the context words (n preceding and n following words) and the output
is the conditional distribution of the center word wi. we will talk about this shortly in
more depth. see fig. 5.6 for graphical illustration.

now we know that it is often dif   cult to compute the full sentence id203
p(w1, . . . ,wt ) due to the intractable id172 constant z. we however know how
to compute the conditional probabilities (for all words) quite tractably. the former
fact implies that it is perhaps not the best idea to maximize log-likelihood to train this
model.18 the latter however sheds a bit of light, because we can train a model to
maximize pseudo   likelihood [11] instead.19

pseudo   likelihood of the mrf-lm is de   ned as

logpl =

t

   

i=1

log p(wi|wi   n, . . . ,wi   1,wi+1, . . . ,wi+n).

(5.16)

maximizing this pseudo   likelihood is equivalent to training a neural network in fig. 5.6
which approximates each conditional distribution p(wi|wi   n, . . . ,wi   1,wi+1, . . . ,wi+n)
to give a higher id203 to the ground-truth center word in the training corpus.

unfortunately, even after training the model by maximizing the pseudo   likelihood
in eq. (5.16), we do not have a good way to compute the full sentence id203 under
this model. under certain conditions maximizing pseudo   likelihood indeed converges
to the maximum likelihood solution, but this does not mean that we can use the product
of all the conditionals as a replacement of the full sentence id203. however, this
does not mean that we cannot use this mrf-lm as a language model, since given
a    xed model, the pseudo   id203 (the product of all the conditionals) can score
different sentences.

18 however this is not to say maximum likelihood in this case is impossible. there are different ways to

approximate the full sentence id203 under this model. see [56] for one such approach.

19 see the note by amir globerson (later modi   ed by david sontag) available at http://cs.nyu.

edu/  dsontag/courses/id13614/slides/pseudolikelihood_notes.pdf.

73

1-of-k codingsoftmax+continuous bag-of-wordsthis is in contrast to the neural language model we discussed earlier in sec. 5.4. in
the case of neural language model, we were able to compute the id203 of a given
sentence by computing the id155 of each word, reading from left until
the end of the sentence. this is perhaps one of the reasons why the mrf-lm is not
often used in practice as a language model. then, you must ask why i even bothered
to explain this mrf-lm in the    rst place.

this approach, which was proposed in [79] as a continuous bag-of-words (cbow)
model,20 was found to exhibit an interesting property. that is, the id27
matrix e learned as a part of this cbow model very well re   ects underlying structures
of words, and this has become one of the darling models by natural language processing
researchers in recent years. we will discuss further in the next section.

skip-gram and implicit id105 in [79], another model, called skip-
gram, is proposed. the skip-gram model is built by    ipping the continuous bag-of-
words model. instead of trying to predict the middle word given 2n surrounding words,
the skip-gram model tries to predict randomly chosen one of the 2n surrounding words
given the middle word. from this description alone, it is quite clear that this skip-gram
model is not going to be great as a language model. however, it turned out that the
word vectors obtained by training a skip-gram model were as good as those obtained by
either a continuous bag-of-words model or any other neural language model. of course,
it is debatable which criterion be used to determine the goodness of word vectors, but in
many of the existing so-called    intrinsic    evaluations, those obtained from a skip-gram
model have been shown to excel.

the authors of [72] recently showed that training a skip-gram model with negative
sampling (see [79]) is equivalent to factorizing a positive point-wise mutual informa-
tion matrix (ppmi) into two lower-dimensional matrices. the left lower-dimensional
matrix corresponds to the input id27 matrix e in a skip-gram model. in
other words, training a skip-gram model implicitly factorizes a ppmi matrix.

their work drew a nice connection between the existing works on distributional
word representations from natural language processing, or even computational linguis-
tics and these more recent neural approaches. i will not go into any further detail in
this course, but i encourage readers to read [72].

5.4.3 semi-supervised learning with pretrained id27s
one thing i want to emphasize in these language models, including id165 language
model, neural language model and continuous bag-of-words model, is that they are
purely unsupervised, meaning that all we need is a large corpus of unannotated text.
this is one thing that makes this statistical approach to language modelling much more
appealing than any other approach based on linguistic structures (see sec. 5.1.1 for a
brief discussion.)

20 one difference between the model we derived in this section starting from the mrf-lm and the one
proposed in [79] is that in our derivation, the neural network shares a single weight matrix e for both the
input and output layers.

74

when it comes to neural language model and continuous bag-of-words model, we
now know that these networks learn continuous vector representations of input words,
target words and the context phrase (h from eq. (5.13).) we also discussed how these
vector representations encode similarities among different linguistic units, be it a word
or a phrase.

what this implies is that once we train this type of language model on a large, or
effectively in   nite,21 corpus of unlabelled text, we get good vectors for those linguistic
units for free. among these, word vectors, the rows of the input weight matrix e in
eq. (5.12), have been extensively used in many natural language processing applica-
tions in recent years since [103, 33, 79].

let us consider an extreme example of classifying each english word as either
   positive    or    negative   . for instance,    happy    is positive, while    sad    is negative. a
training set of 2 examples   1 positive and 1 negative words    is given. how would one
build a classi   er?22

there are two issues here. first, it is unclear how we should represent the input, in
this case a word. a good reader who has read this note so far will be clearly ready to
use an one-hot vector and use a softmax layer in the output, and i commend you for
that. however, this still does not solve a more serious issue which is that we have only
two training examples! all the word vectors, save for two vectors corresponding to the
words in the training set, will not be updated at all.

one way to overcome these two issues is to make somewhat strong, but reasonable
assumption that similar input will have similar sentiments. this assumption is at the
heart of semi-supervised learning [23]. it says that high-dimensional data points in
effect lies on a lower-dimensional manifold, and the target values of the points on this
manifold change smoothly. under this assumption, if we can well model this lower-
dimensional data manifold using unlabelled training examples, we can train a good
classi   er23

and, guess what? we have access to this lower-dimensional manifold, which is
represented by the set of pretrained word vectors e. believing that similar words have
similar sentiment and that these pretrained word vectors indeed well re   ect similarities
among words, let me build a simple nearest neighbour (nn) classi   er which uses the
pretrained word vectors:

nn(w) =

if cos(ew,ehappy) > cos(ew,ebad)
otherwise
where cos(  ,  ) is a cosine similarity de   ned as

negative,

,

cos(ei,e j) =

e(cid:62)
i e j
(cid:107)ei(cid:107)(cid:107)e j(cid:107) .

21 why? because of almost universal broadband access to the internet!
22 although the setting of 2 training examples is extreme, but the task itself turned out to be not-so-
extreme. in fact, there is multiple dictionaries of words    sentiment maintained. for instance, check http:
//sentiid138.isti.cnr.it/search.php?q=llama.

23 what do i mean by a good classi   er? a good classi   er is a classi   er that classi   es unseen test examples

well. see sec. 2.3.

75

(cid:26) positive,

this use of a term    similarity    almost makes this set of pretrained word vectors
look like some kind of magical wand that can solve everything.24 this is however not
true, and using pretrained word vectors must be done with caution.

why should we be careful in using these pretrained word vectors? we must remem-
ber that these word vectors were obtained by training a neural network to maximize a
certain objective, or to minimize a certain cost function. this means that these word
vectors capture certain aspects of words    underlying structures that are necessary to
achieve the training objective, and that there is no reason for these word vectors to
capture any other properties of the words that are not necessary for maximizing the
training objective. in other words,    similarity    among multiple words has many dif-
ferent aspects, and these word vectors will capture only a few of these many aspects.
which few aspects will be determined by the choice of training objective.

the hope is that language modelling is a good training objective that will encourage
the word vectors to capture as many aspects of similarity as possible.25 but, is this true
in general?

let   s consider an example of words describing emotions, such as    happy   ,    sad   
and    angry   , in the context of a continuous bag-of-words model. these emotion-
describing words often follow some forms of a verb    feel   , such as    feel   ,    feels   ,
   felt    and    feeling   . this means that those emotion-describing words will have to be
projected nearby in the context space in order to give a high id203 to those forms
of    feel    as a middle word. this is understandable and agrees quite well with our in-
tuition. all those emotion-describing words are similar to each other in the sense that
they all describe emotion. but, wait, this aspect of similarity is not going to help sen-
timent classi   cation of words. in fact, this aspect of similarity will hurt the sentiment
classi   er, because a positive word    happy    will be close to negative words    sad    and
   angry    in this word vector space!

the lesson here is that when you are solving a language-related task with very little
data, it is a good idea to consider using a set of pretrained word vectors from neural
language models. however, you must do so in caution, and perhaps try to pretrain your
own word vectors by training a neural network to maximize a certain objective that
better suits your    nal task.

but, then, what other training objectives are there? we will get to that later.

5.5 recurrent language model
neural language model indeed avoids the lack of generalization in the conventional n-
gram id38. it still assumes the n-th order markov property, meaning that
it looks only as far back into the past as n    1 words. in sec. 5.3, i gave an example of
   in korea, more than half of all the residents speak korean   . in this example, the con-
ditional distribution over the last word in the sentence clearly will be better estimated

24 for future reference, i must say there were many papers claiming that the pretrained word vectors are
indeed magic wands at three top-tier natural language processing conferences (acl, emnlp, naacl) in
2014 and 2015.

25 some may ask how a single vector, which is a point in a space, can capture multiple aspects of similarity.

this is possible because these word vectors are high-dimensional.

76

(a)

(b)

figure 5.7:
network language model.

(a) a recurrent neural network from sec. 4.1.4. (b) a recurrent neural

if it is conditioned on the second word of the sentence which is more than 10 words
back in the past.

let us recall what we learned in sec. 4.1.4. there, we learn how to build a recurrent
neural network to read a variable-length sequence and return a variable-length output
sequence. an example we considered back then was a task of part-of-speech tagging,
where the input is a sentence such as

x = (children,eat,sweet,candy),

and the target output is a sequence of part-of-speech tags such as

y = (noun,verb,adjective,noun).

in order to make less of an assumption on the conditional independence of the
predicted tags, we made a small adjustment such that the prediction yt at each timestep
was fed back into the recurrent neural network in the next timestep together with the
input xt+1. see fig. 5.7 (a) for graphical illustration.

why am i talking about this again, after saying that the task of part-of-speech tag-
ging is not even going to be considered as a valid topic for the    nal project? because
the very same model for part-of-speech tagging will be turned into the very recurrent
neural network language model in this section.

let us start by considering a single conditional distribution, marked (a) below, from

the full sentence id203:

p(w1,w2, . . . ,wt ) =

t

   

t=1

(cid:124)

p(wt|w1, . . . ,wt   1)

.

(cid:123)(cid:122)

(a)

(cid:125)

this id155 can be approximated by a neural network, as we   ve been
doing over and over again throughout this course, that takes as input (w1, . . . ,wt   1) and
returns the id203 over all possible words in the vocabulary v . this is not unlike
neural language model we discussed earlier in sec. 5.4, except that the input is now a
variable-length sequence.

77

figure 5.8: a recurrent neural network language model

ht(cid:48)

in this case, we can use a recurrent neural network which is capable of summariz-
ing/memorizing a variable-length input sequence. a recurrent neural network summa-
rizes a given input sequence (w1, . . . ,wt   1) into a memory state ht   1:

(cid:26) 0,
f (ewt(cid:48) ,ht(cid:48)   1),
where t(cid:48) runs from 0 to t     1.
f is a recurrent function which can be any of a naive
transition function from eq. (4.1), a gated recurrent unit or a long short-term memory
unit from sec. 4.2.2. ewt(cid:48) is a word vector corresponding to the word wt(cid:48)
this summary ht   1 is af   ne-transformed followed by a softmax nonlinear function
to compute the id155 of wt. hopefully, everyone remembers how it is
done. as in eq. (4.6),

if t(cid:48) = 0
otherwise ,

(5.17)

=

.

   = softmax(vht   1),

where    is a vector of probabilities of all the words in the vocabulary.

one thing to notice here is that the iteration procedure in eq. (5.17) computes a
sequence of every memory state vector ht by simply reading the input sentence once.
in other words, we can let the recurrent neural network read one word wt at a time,
update the memory state ht and compute the id155 of the next word
p(wt+1|w   t ).

this procedure is illustrated in fig. 5.7 (b).26 this language model is called a

recurrent neural network language model (id56-lm, [80]).

but, wait, from looking at figs. 5.7 (a)   (b), there is a clear difference between
the recurrent neural networks for part-of-speech tagging and language model. that is,
there is no feedback connection from the output of the previous time step back into the
recurrent neural network in the id56-lm. this is simply an illusion from the limitation
in the graphical illustration, because the input wt+1 in the next time step is in fact the
output wt+1 at the current time step. this becomes clearer by drawing the same    gure
in a slightly different way, as in fig. 5.8.

26 in the    gure, you should notice the beginning-of-the-sentence symbol (cid:104)s(cid:105). this is necessary in order to
use the very same recurrent function f to compute the id155 of the    rst word in the input
sentence.

78

5.6 how do id165 language model, neural language

model and id56-lm compare?

now the question is which one of these language models we should use in practice. in
order to answer this, we must    rst discuss the metric most commonly used for evaluat-
ing language models.

the most commonly used metric is a perplexity. in the context of language mod-

elling, the perplexity ppl of a model m is computed by
n=1 logb pm (wn|w<n),

ppl = b    1

n    n

(5.18)

where n is the number of all the words in the validation/test corpus, and b is some
constant that is often 2 or 10 in practice.

what is this perplexed metric? i totally agree with you on this one. of course, there
is a quite well principled way to explain what this perplexity is based on information
theory. this is however not necessary for us to understand this metric called perplexity.
as the exponential function (with base b in the case of perplexity in eq. (5.18))
is a monotonically increasing function, we see that the ordering of different language
models based on the perplexity will not change even if we only consider the exponent:

    1
n

n

   

n=1

logb pm (wn|w<n).

furthermore, assuming that b > 1, we can simply replace logb with log (natural loga-
rithm) without changing the order of different language models:

    1
n

n

   

n=1

log pm (wn|w<n).

now, this looks awfully similar to the cost function, or negative log-likelihood, we
minimize in order to train a neural network (see chapter 2.)

let   s take a look at a single term inside the summation above:

log pm (wn|w<n).

this is simply measuring how high a id203 the language model m is assigning to
a correct next word given all the previous words. again, because log is a monotonically
increasing function.

in summary, the (inverse) perplexity measures how high a id203 the language
model m assigns to correct next words in the test/validation corpus on average. there-
fore, a better language model is the one with a lower perplexity. there is nothing so
perplexing about the perplexity, once we start viewing it from this perspective.

we are now ready to compare different language models, or to be more precise,
three different classes of language models   count-based id165 language model, neural
id165 language model and recurrent neural network language model. the biggest
challenge in doing so is that this comparison will depend on many factors that are not
easy to control. to list a few of them,

    language

79

    genre/topic of training, validation and test corpora
    size of a training corpus
    size of a language model

figure 5.9: the perplexity, word error rate (wer) and character error rate (cer) of
an automatic id103 system using different language models. note that all
the results by neural or recurrent language models are by interpolating these models
with the count-based id165 language model. reprinted from [100].

because of this dif   culty, this kind of comparison has often been done in the con-
text of a speci   c downstream application. this choice of a downstream application
often puts rough constraints on the size of available, or commonly used, corpus, target
language and reasonably accepted size of language models. for instance, the authors
of [3] compared the conventional id165 language model and neural language model,
with various approximation techniques, with machine translation as a    nal task. in
[100], the authors compared all the three classes of language model in the context of
automatic id103.

first, let us look at one observation made in [100]. from fig. 5.9, we can see that
it is bene   cial to use a recurrent neural network language model (id56-lm) compared
to a usual neural language model. especially when long short-term memory units were

80

figure 5.10: the trend of perplexity as the size of language model changes. reprinted
from [100].

used, the improvement over the neural language model was signi   cant. furthermore,
we see that it is possible to improve these language models by simply increasing their
size.

similarly, in fig. 5.10 from the same paper [100], it is observed that larger language
models tend to get better/lower perplexity and that id56-lm in general outperforms
neural language models.

these two observations do seem to suggest that neural and recurrent language mod-
els are better candidates as language model. however, this is not to be taken as an
evidence for choosing neural or recurrent language models. it has been numerously
observed over years that the best performance, both in terms of perplexity and in terms
of performance in the downstream applications such as machine translation and auto-
matic id103, is achieved by combining a count-based id165 language
model and a neural, or recurrent, language model. see, for instance, [92].

this superiority of combined, or hybrid, language model suggests that the count-
based, or conventional, id165 language model, neural language model and recurrent
neural network language model are capturing underlying structures of natural language
sentences that are complement to each other. however, it is not crystal clear how these
captured structures differ from each other.

81

chapter 6

id4

finally, we have come to the point in this course where we discuss an actual natural
language task. in this chapter, we will discuss how translation from one language to
another can be done with statistical methods, more speci   cally neural networks.

6.1 statistical approach to machine translation
let   s    rst think of what it means to translate one sentence x in a source language to an
equivalent sentence y in a target language which is different from the source language.
a process of translation is a function that takes as input the source sentence x and
returns a correct translation y , and it is clear that there may be more than one correct
translations. the latter fact implies that this function of translation should return not a
single correct translation, but a id203 distribution that assigns high probabilities
to more than one likely translations.

now, let us write it in a more formal way. first, the input is a sequence of words

where tx is the length of the source sentence. a target sentence is

x = (x1,x2, . . . ,xtx ),

y = (y1,y2, . . . ,yty).

similarly, ty is the length of the target sentence.

the translation function f then reads the input sequence x and computes the prob-

ability over target sentences. in other words,

f : v +

x     c+|vy|   1
is a set of all possible source sentences of any

(6.1)

where vx is a source vocabulary, and v +
x
length tx > 0. vy is a target vocabulary, and ck is a standard k-simplex.

what is a standard k-simplex? it is a set de   ned by

(cid:40)

(cid:41)

ck =

(t0, . . . ,tk)     rk+1

tk = 1 and ti     0 for all i

.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k

   

i=1

82

figure 6.1: graphical illustration of id151

in short, this set contains all possible settings for categorical distributions of k + 1
possible outcomes. this means that the translation function f returns a id203
distribution p(y|x) over all possible translations of length ty > 1.
given a source sentence x, this translation function f returns the conditional prob-
ability of a translation y : p(y|x). let us rewrite this id155 according
to what we have discussed in chapter 5:

p(y|x) =

ty
   

t=1

(cid:124)

p(yt|y1, . . . ,yt   1,

x(cid:124)(cid:123)(cid:122)(cid:125)

)
conditional

(cid:125)

(cid:123)(cid:122)

language modelling

(6.2)

looking at it in this way, it is clear that this is nothing but conditional language mod-
elling. this means that we can use any of the techniques we have used earlier in
chapter 5 for id151.

training can be trivially done by maximizing the log-likelihood or equivalently

minimizing the negative log-likelihood (see sec. 3.1):

given a training set

  c(   ) =     1
n

n

   

ty
   

n=1

t=1

log p(yn

t |yn

<t ,x n),

d =(cid:8)(x 1,y 1), (x 2,y 2), . . . , (x n,y n)(cid:9)

(6.3)

(6.4)

consisting of n training pairs.

all these look extremely straightforward and do not deviate too much from what we
have learned so far in this course. a big picture on this process translation is shown in
fig. 6.1. more speci   cally, building a id151 model is simple,
because we have learned how to

1. assign a id203 to a sentence in sec. 5.2.
2. handle variable-length sequences with recurrent neural networks in sec. 4.1.
3. compute the gradient of an empirical cost function   c with respect to the param-

eters    of a recurrent neural network in sec. 4.1.2 and sec. 3.4.

83

corporaf = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)e = (economic, growth, has, slowed, down, in, recent, years, .)4. use stochastic id119 to minimize the cost function in sec. 2.2.2.

of course, simply knowing all these does not get you a working neural network that
translates from one language to another. we will discuss in detail how we can build
such a neural network in the next section. before going to the next section, we must
   rst discuss two issues; (1) where do we get training data? (2) how do we evaluate
machine translation systems?

6.1.1 parallel corpora: training data for machine translation
first, let us consider again what the problem we   re trying to solve here. it is machine
translation, and from the description in the previous section and from eqs. (6.1)   (6.2),
it is a sentence-to-sentence translation task. we approach this problem by building a
model that takes as input a source sentence s and computes the id203 p(y|x) of
a target sentence y , equivalently a translation. in order for this model to translate, we
must train it with a training set of pairs of a source sentence and its correct translation.
the very    rst problem we run into is where we can    nd this training set which is
often called a parallel corpus. it is not easy to think of documents which have been
translated into multiple languages. let   s take for instance all the books that are being
translated each year. according to [86], approximately 3% of titles published each year
in english are translations from another language.1 a few international news agencies
publish some of their news articles in multiple languages. for instance, afp publishes
1,500 stories in french, 700 stories in english, 400 stories in spanish, 250 stories in
arabic, 200 stories in german and 150 stories in portuguese each day, and there are
some overlapping stories across these six languages.2 online commerce sites, such as
ebay, often list their products in international sites with their descriptions in multiple
languages.3

unfortunately these sources of multiple languages of the same content are not suit-
able for our purpose. why is this so? most importantly, they are often copy-righted
and sold for personal use only. we cannot buy more than 14,400 books in order to
train a translation model. we will likely go broke before completing the purchase,
and even if so, it is unclear whether it is acceptable under copyright to use these text
to build a translation model. because we are mixing multiple sources of which each
is protected under copyright, is the translation model trained from a mix of all these
materials considered a derivative work?4

this issue is nothing new, and has been there since the very    rst statistical machine
translation system was proposed in [19]. fortunately, it turned out that there are a
number of legitimate sources where we can get documents translated in more than
one languages, often very faithfully to their content. these sources are parliamentary
proceedings of bilingual, or multilingual countries.

1    according to the information bowker released in october of 2005, in 2004 there were 375,000 new
books published in english.    ..    of that total, approx. 14,440 were new translations, which is slightly more
than 3% of all books published.    [86].

2 http://www.afp.com/en/products/services/text
3 http://sellercentre.ebay.co.uk/international-selling-tools
4 http://copyright.gov/circs/circ14.pdf

84

brown et al. [19] used the proceedings from the canadian parliament, which are by
law kept in both french and english. all of these proceedings are digitally available
and called hansards. you can check it yourself online at http://www.parl.gc.
ca/, and here   s an excerpt from the prayers of the 2nd session, 41st parliament, issue
152:5

    french:    elizabeth deux, par la gr  ace de dieu, reine du royaume-
uni, du canada et de ses autres royaumes et territoires, chef du commonwealth,
d  efenseur de la foi.   

    english:    elizabeth the second, by the grace of god of the united
kingdom, canada and her other realms and territories queen, head of the
commonwealth, defender of the faith.   

every single word spoken in the canadian parliament is translated either into french
or into english. a more recent version of hansards preprocessed for research can be
found at http://www.isi.edu/natural-language/download/hansard/.

similarly, the european parliament used to provided the parliamentary proceedings
in all 23 of   cial languages.6 this is a unique data in the sense that each and every
sentence is translated into either 11 or 26 of   cial languages. for instance, here is one
example [65]:

    danish: det er n  sten en personlig rekord for mig dette efter  ar.
    german: das ist f  ur mich fast pers  onlicher rekord in diesem herbst .
    greek: (omitted)
    english: that is almost a personal record for me this autumn !
    spanish: es la mejor marca que he alcanzado este oto  no .
    finnish: se on melkein minun enn  atykseni t  an  a syksyn  a !
    french: c     est pratiquement un record personnel pour moi , cet automne !
    italian: e     quasi il mio record personale dell     autunno .
    dutch: dit is haast een persoonlijk record deze herfst .
    portuguese:   e quase o meu recorde pessoal deste semestre !
    swedish: det   ar n  astan personligt rekord f  or mig denna h  ost !
the european proceedings has been an invaluable resource for machine translation
research. at least, the existing multilingual proceedings (up to 2011) can be still used,
and it is known in the    eld as the    europarl    corpus [65] and can be downloaded from
http://www.statmt.org/europarl/.

these proceedings-based parallel corpora have two distinct advantages. first, in
many cases, the sentences in those corpora are well-formed, and their translations are

5 this is one political lesson here: canada is still headed by the queen of the united kingdom.
6 unfortunately, the european parliament decided to stop translating its proceedings into all 23 of   -
cial languages on 21 nov 2011 as an effort toward budget cut. see http://www.euractiv.com/
culture/parliament-cuts-translation-budg-news-516201.

85

done by professionals, meaning the quality of the corpora is guaranteed. second, sur-
prisingly, the topics discussed in those proceedings are quite diverse. clearly the mem-
bers of the parliament do not often chitchat too often, but they do discuss a diverse set
of topics. here   s one such example from the europarl corpus:

    english: although there are now two finnish channels and one portuguese one,
there is still no dutch channel, which is what i had requested because dutch
people here like to be able to follow the news too when we are sent to this place
of exile every month.

    french: il y a bien deux cha    nes    nnoises et une cha    ne portugaise, mais il
n   y a toujours aucune cha    ne n  eerlandaise. pourtant je vous avais demand  e une
cha    ne n  eerlandaise, car les n  eerlandais aussi d  esirent pouvoir suivre les actu-
alit  es chaque mois lorsqu   ils sont envoy  es en cette terre d   exil.

one apparent limitation is that these proceedings cover only a handful of languages
in the world, mostly west european languages. this is not desirable. why? according
to ethnologue (2014)7, the top-   ve most spoken languages in the world are

1. chinese: approx. 1.2 billion
2. spanish: approx. 414 million
3. english: approx. 335 million
4. hindi: approx. 260 million
5. arabic: approx. 237 million

there are only two european languages in this list.

so, then, where can we get all data for all these non-european languages? there

are a number of resources you can use, and let me list a few of them here:

you can    nd the translated subtitle of the ted talks at the web inventory of
transcribed and translated talks (wit3, https://wit3.fbk.eu/) [22].
it is
a quite small corpus, but includes 104 languages. for russian   english data, yandex
released a parallel corpus of one million sentence pairs. you can get it at https:
//translate.yandex.ru/corpus?lang=en. you can continue with other
languages by googling very hard, but eventually you run into a hard wall.

this hard wall is not only the lack of any resource, but also lack of enough resource.
for instance, i quickly googled for korean   english parallel corpora and found the
following resources:

    swrc english-korean multilingual corpus: 60,000 sentence pairs http://

semanticweb.kaist.ac.kr/home/index.php/corpus10

    jungyeul   s english-korean parallel corpus: 94,123 sentence pairs https://

github.com/jungyeul/korean-parallel-corpora

this is just not large enough.

one way to avoid this or mitigate this problem is to automatically mine parallel
corpora from the internet. there have been quite some work in this direction as a way

7 http://www.ethnologue.com/world

86

to increase the size of parallel corpora [87, 112]. the idea is to build an algorithm that
crawls the internet and    nd a pair of corresponding pages in two different languages.
one of the largest preprocessed corpus of multiple languages from the internet is the
common crawl parallel corpus created by smith et al. [98] available at http://
www.statmt.org/wmt13/training-parallel-commoncrawl.tgz.

6.1.2 automatic evaluation metric
let   s say we have trained a machine translation model on a training corpus. a big
question follows: how do we evaluate this model?

in the case of classi   cation, evaluation is quite straightforward. all we need to do is
to classify held-out test examples with a trained classi   er and see how many examples
were correctly classi   ed. this is however not true in the case of translation.

there are a number of issues, but let us discuss two most important problems here.
first, there may be many correct translations given a single source sentence. for in-
stance, the following three sentences are the translations made by a human translator
given a single chinese sentence [82]:

    it is a guide to action that ensures that the military will forever heed party com-

mands.

    it is the guiding principle which guarantees the military forces always being

under the command of the party.

    it is the practical guide for the army always to heed the directions of the party.
they all clearly differ from each other, although they are the translations of a single
source sentence.

second, the quality of translation cannot be measured as either success or failure.
it is rather a smooth measure between success and failure. let us consider an english
translation of a french sentence    j   aime un llama, qui est un animal mignon qui vit en
am  erique du sud   .8

one possible english translation of this french sentence is    i like a llama which is a
cute animal living in south america   . let   s give this translation a score 100 (success).
according to google translate, the french sentence above is    i like a llama, a cute
animal that lives in south america   . i see that google translate has omitted    qui est   
from the original sentence, but the whole meaning has well been captured. let us give
this translation a slightly lower score of 90.

then, how about    i like a llama from south america   ? this is certainly not a
correct translation, but except for the part about a llama being cute, this sentence does
communicate most of what the original french sentence tried to communicate. maybe,
we can give this translation a score of 50.

how about    i do not like a llama which is an animal from south america   ? this
translation correctly describes the characteristics of llama exactly as described in the
source sentence. however this translation incorrectly states that i do not like a llama,
when i like a llama according to the original french sentence. what kind of score
would you give this translation?

8 i would like to thank laurent dinh for the french translation.

87

even worse, we want an automated evaluation algorithm. we cannot look at thou-
sands of validation or test sentence pairs to tell how well a machine translation model
does. even if we somehow did it for a single model, in order to compare this translation
model against others, we must do it for every single machine translation model under
comparison. we must have an automatic evaluation metric in order to ef   ciently test
and compare different machine translation models.

id7 one of the most widely used automatic evaluation metric for assessing the
quality of translations is id7 proposed in [82]. id7 computes the geometric mean
of the modi   ed id165 precision scores multiplied by brevity penalty. let me describe
this in detail here.

first, we de   ne the modi   ed id165 precision pn of a translation y as

pn =

   s   c    ngram   s   c(ngram)
   s   c    ngram   s c(ngram)

,

where c is a corpus of all the sentences/translations, and s is a set of all unique id165s
in one sentence in c. c(ngram) is the count of the id165, and   c(ngram) is

  c(ngram) = min(c(ngram),cref(ngram)).

cref(ngram) is the count of the id165 in reference sentences.

what does this modi   ed id165 precision measure? it measures the ratio between
the number of id165s in the translation and the number of those id165s actually
occurred in a reference (ground-truth) translation. if there is no id165 from the trans-
lation in the reference, this modi   ed precision will be zero because cref(  ) will be zero
all the time.

it is common to use the geometric average of modi   ed 1-, 2-, 3- and 4-gram preci-

sions, which is computed by

(cid:33)

(cid:32)

1
4

4

   

n=1

p4
1 = exp

log pn

.

if we use this geometric average p as it is, there is a big loophole. one can get
a high average modi   ed precision by making as short a translation as possible. for
instance, a reference translation is

    i like a llama, a cute animal that lives in south america .

and a translation we are trying to evaluate is

    cute animal that lives

this is clearly a very bad translation, but the modi   ed 1-, 2-, 3- and 4-gram precisions

88

will be high. the modi   ed precisions are

=

4
4

= 1

= 1

p1 =

p2 =

p3 =

p4 =

=

1 + 1 + 1 + 1
1 + 1 + 1 + 1
3
1 + 1 + 1
1 + 1 + 1
3
2
1 + 1
1 + 1
2
1
1
1
1

= 1.

=

=

= 1

their geometric average is then

p4
1 = exp

(cid:18)1

4

(cid:19)

(0 + 0 + 0 + 0)

= 1

which is the maximum modi   ed precision you can get!

in order to avoid this behaviour, id7 penalizes the geometric average of the
modi   ed id165 precisions by the ratio of the lengths between the reference r and
translation l. this is done by    rst computing a brevity penalty:

(cid:26) 1
exp(cid:0)1    r

l

(cid:1)

bp =

, if l     r
, if l < r

if the translation is longer than the reference, it uses the geometric average of the
modi   ed id165 precisions as it is. otherwise, it will penalize it by multiplying the
average precision with a scalar less than 1. in the case of the example above, the brevity
penalty is 0.064, and the    nal id7 score is 0.064.

(a)

(b)

figure 6.2: (a) id7 vs. bilingual and monolingual judgements of three machine
translation systems (s1, s2 and s3) and two humans (h1 and h2). reprinted from
[82].
(b) id7 vs. human judgement (adequacy and    uency separately) of three
machine translation systems (two statistical and one rule-based systems). reprinted
from [20].

the id7 was shown to correlate well with human judgements in the original
article [82]. fig. 6.2 (a) shows how id7 correlates with the human judgements in
comparing different translation systems.

89

this is however not to be taken as a message saying that the id7 is the perfect
automatic evaluation metric.
it has been shown that the id7 is only adequate in
comparing two similar machine translation systems, but not too much so in comparing
two very different systems. for instance, callison-burch et al. [20] observed that the
id7 underestimates the quality of the machine translation system that is not a phrase-
based statistical system. see fig. 6.2 (b) for an example.

id7 is de   nitely not a perfect metric, and many researchers strive to build a better
evaluation metric for machine translation systems. some of the alternatives available
at the moment are meteor [36] and ter [99].

6.2 id4:

simple encoder-decoder model

from the previous section and from eq. 6.2, it is clear that we need to model each
conditional distribution inside the product as a function. this function will take as
input all the previous words in the target sentence y = (y1, . . . ,yt   1) and the whole
source sentence x = (x1, . . . ,xtx ). given these inputs the function will compute the
probabilities of all the words in the target vocabulary vy. in this section, i will describe
an approach that was proposed multiple times independently over 17 years in [43, 28,
101].

let us start by tackling how to handle the source sentence x = (x1, . . . ,xtx ). since
this is a variable-length sequence, we can readily use a recurrent neural network from
chapter 4. however, unlike the previous examples, there is no explicit target/output in
this case. all we need is a (vector) summary of the source sentence.

we call this recurrent neural network an encoder, as it encodes the source sentence

into a (continuous vector) code. it is implemented as
ht   1,e(cid:62)
x xt

ht =   enc

(cid:16)

(cid:17)

.

(6.5)

as usual,   enc can be any recurrent activation function, but it is highly recommended to
use either id149 (see sec. 4.2.2) or long short-term memory units (see
sec. 4.2.3.) ex     r|vx|  d is an input weight matrix containing word vectors as its rows
(see eq. (5.12) in sec. 5.4,) and xt is an one-hot vector representation of the word xt
(see eq. (5.10) in sec. 5.4.) h0 is initialized as an all-zero vector.

after reading the whole sentence up to xtx, the last memory state htx of the encoder
summarizes the whole source sentence into a single vector, as shown in fig. ?? (a).
thanks to this encoder, we can now work with a single vector instead of a whole
sequence of source words. let us denote this vector as c and call it a context vector.

we now need to design a decoder, again, using a recurrent neural network. as i
mentioned earlier, the decoder is really nothing but a language model, except that it is
conditioned on the source sentence x. what this means is that we can build a recurrent
neural network language model from sec. 5.5 but feeding also the context vector at
each time step. in other words,

zt =   dec

zt   1,

e(cid:62)
y yt   1;c

(6.6)

(cid:105)(cid:17)

(cid:16)

(cid:104)

90

(a)

(b)

figure 6.3: (a) the encoder and (b) the decoder of a simple id4
model

(cid:16)

(cid:17)

do you see the similarity and dissimilarity to eq. (5.17) from sec. 5.5? it   s essentially
same, except that the input at time t is a concatenated vector of the word vector of the
previous word yt   1 and the context vector c.

once the decoder   s memory state is updated, we can compute the probabilities of

all possible target words by

e(cid:62)
w(cid:48)zt

p(yt = w(cid:48)|y<t ,x)     exp

,

(6.7)
where ew(cid:48) is the target word vector associated the word w(cid:48). this is equivalent to af   ne-
transforming zt followed by a softmax function from eq. (3.5) from sec. 3.1.

now, should we again initialize z0 to be an all-zero vector? maybe, or maybe not.
one way to view what this decoder does is that the decoder models a trajectory in
a continuous vector space, and each point in the trajectory is zt. then, z0 acts as a
starting point of this trajectory, and it is natural to initialize this starting point to be a
point relevant to the source sentence. because we have access to the source sentence   s
content via c, we can again use it to initialize z0 as

z0 =   init (c) .

(6.8)

see fig. 6.3 (b) for the graphical illustration of the decoder.

although i have used c as if it is a separate variable, this is not true. c is simply
a shorthand notation of the last memory state of the encoder which is a function of
the whole source sentence. what does this mean? it means that we can compute the
gradient of the empirical cost function in eq. (6.3) with respect to all the parameters of
both the encoder and decoder and maximize the cost function using stochastic gradient
descent, just like any other neural network we have learned so far in this course.

6.2.1 sampling vs. decoding
sampling we are ready to compute the conditional distribution p(y|x) over all pos-
sible translations given a source sentence. when we have a distribution, the    rst thing
we can try is to sample from this distribution. often, it is not straightforward to gen-
erate samples from a distribution, but fortunately, in this case, we can readily generate
exact samples from the distribution p(y|x).

91

we simply iterate over the following steps until a token indicating the end of a

sentence ((cid:104)eos(cid:105)):

1. compute c (eq. (6.5))
2. initialize z0 with c (eq. (6.8))
3. compute zt given zt   1, yt   1 and c (eq. (6.6))
4. compute p(yt|y<t ,x) (eq. (6.7))
5. sample   yt from the compute distribution
6. repeat (3)   (5) until   yt = (cid:104)eos(cid:105)
after taking these steps, we get a sample   y =

(cid:16)

(cid:17)

  y1, . . . ,   y|   y|

given a source sentence
x. of course, there is no guarantee that this will be a good translation of x. in order to
   nd a good translation, meaning a translation with a high id203 p(   y|x), we need
to repeatedly sample multiple translations from p(y|x) and choose one with the high
id203.
this is not too desirable, as it is not clear how many translations we need to sample
from p(y|x) and also it will likely be computationally expensive. we must wonder
whether we can solve the following optimization problem directly:

  y = argmax

y

logp(y|x).

unfortunately, the exact solution to this requires evaluating p(y|x) for every possible
y . even if we limit our search space of y to consist of only sentences of length up
to a    nite number, it will likely become too large (the cardinality of the set grows
exponentially with respect to the number of words in a translation.) thus, it only
makes sense to solving the optimization problem above approximately.

approximate decoding: beamsearch although it is quite clear that    nding a trans-
lation   y that maximizes the log-id203 logp(   y|x) is extremely expensive, we will
regardlessly try it here.

one very natural way to enumerate all possible target sentences and simultaneously
computing the log-id203 of each and every one of them is to start from all possible
   rst word, compute the probabilities of them, and from each potential    rst word branch
into all possible second words, and so on. this procedure forms a tree, and any path
from the root of this tree to any intermediate node is a valid, but perhaps very unlikely,
sentence. see fig. 6.4 for the illustration. the conditional probabilities of all these
paths, or sentences, can be computed as we expand this tree down by simply following
eq. (6.2).

of course, we cannot compute the conditional probabilities of all possible sen-
tences. hence, we must resort to some kind of approximate search. wait, search? yes,
this whole procedure of    nding the most likely translation is equivalent to searching
through a space, in this case a tree, of all possible sentences for one sentence that has
the highest id155.

92

(a)

(b)

figure 6.4: (a) search space depicted as a tree. (b) greedy search.

the most basic approach to approximately searching for the most likely translation

is to choose only a single branch at each time step t. in other words,

  yt = argmax

w(cid:48)   v

log p(yt = w(cid:48)|   y<t ,x),

where the id155 is de   ned in eq. (6.7), and   y<t = (   y1,   y2, . . . ,   yt   1) is
a sequence of greedily-selected target words up to the (t     1)-th step. this procedure
is repeated until the selected   yt is a symbol corresponding to the end of the translation
(often denoted as (cid:104)eos(cid:105).) see fig. 6.4 (b) for illustration.

there is a big problem of this greedy search. that is, as soon as it makes one
mistake at one time step, there is no way for this search procedure to recover from this
mistake. this happens because the conditional distributions at later steps depend on
the choices made earlier.

consider the following two sequences: (w1,w2) and (w(cid:48)

1,w2). these sequences   

probabilities are

p(w1,w2) = p(w1)p(w2|w1),
p(w(cid:48)
1)p(w2|w(cid:48)
1)

1,w2) = p(w(cid:48)

93

let   s assume that

where 0 <    < 1, meaning that p(w1) > p(w(cid:48)
choose w1 over w(cid:48)

1 and ignore w(cid:48)
1.

   p(w1) = p(w(cid:48)
1),

1). in this case, the greedy search will

now we can see that there   s a problem with this. let   s assume that
p(w2|w(cid:48)
1)

   p(w2|w1) < p(w2|w(cid:48)

1)        p(w2|w1) <

1
  

where    was de   ned earlier. in this case,

p(w1,w2) =p(w1)p(w2|w1) =    p(w(cid:48)
1) = p(w(cid:48)

<    p(w(cid:48)
1)

p(w2|w(cid:48)

1)p(w2|w1)
1)p(w2|w(cid:48)

1
   

1) = p(w(cid:48)

1,w2).

in short,

it means that the sequence (w(cid:48)
algorithm is unable to notice this, because simply p(w1) > p(w(cid:48)
1).

p(w1,w2) < p(w(cid:48)
1,w2) is more likely than (w1,w2), but the greedy search

1,w2).

unfortunately, the only way to completely avoid this undesirable situation is to
consider all the possible paths starting from the very    rst time step. this is exactly the
reason why we introduced the greedy search in the    rst place, but the greedy search
is too greedy. the question is then whether there is something in between the exact
search and the greedy search.

id125 let us start from the very    rst position t = 1. first, we compute the
conditional probabilities of all the words in the vocabulary:

p(y1 = w|x) for all w     v.

among these, we choose the k most likely words and initialize the k hypotheses:

(w1

1), (w1

2), . . . , (w1
k)

we use the subscript to denote the hypothesis and the subscript the time step. as an
example, w1

1 is the    rst hypothesis at time step 1.

for each hypothesis, we compute the next conditional probabilities of all the words

in the vocabulary:

p(y2 = w|y<1 = (w1

i ),x) for all w     v,

where i = 1, . . . ,k. we then have k   |v| candidates with the corresponding probabil-
ities:

                                 

(cid:124)

k

p(w1
p(w1

1,w2
2,w2

c,|v|)
c,|v|)

p(w1

k,w2

c,|v|)

(cid:125)

p(w1
p(w1

1,w2
2,w2

c,1),
c,1),

p(w1

k,w2

c,1),

. . . ,
. . . ,

...
(cid:123)(cid:122)

. . . ,
|v|

94

figure 6.5: id125 with the beam width set to 3.

among these k   |v| candidates, we choose the k most likely candidates:

(w1

1,w2

1), (w1

2,w2

2), . . . , (w1

k,w2

k).

starting from these k new hypotheses, we repeat the process of computing the proba-
bilities of all k   |v| possible candidates and choosing among them the k most likely
new hypotheses.
it should be clear that this procedure, called id125 and shown in fig. 6.5,
becomes equivalent to the exact search, as k        . also, when k = 1, this procedure is
equivalent to the greedy search. in other words, this id125 interpolates between
the exact search, which is computationally intractable but exact, and the greedy search,
which is computationally very cheap but probably quite inexact, by changing the size
k of hypotheses maintained throughout the search procedure.

how do we choose k? one might mistakenly think that we can simply use as large
k as possible given the constraints on computation and memory. unfortunately, this is
not necessarily true, as this interpolation by k is not monotonic. that is, the quality of
the translation found by the id125 with a larger k is not necessarily better than
the translation found with a smaller k.
let us consider the case of vocabulary having three symbols {a,b,c} and any valid

translation being of a length 3. in the    rst step, we have

p(a) = 0.5, p(b) = 0.15, p(c) = 0.45.

in the case of k = 1, i.e., greedy search, we choose a. if k = 2, we will keep (a) and
(c).

given a as the    rst symbol, we have

p(a|a) = 0.4, p(b|a) = 0.3, p(c|a) = 0.3,

95

in which case, we keep (a,a) with k = 1. with k = 2, we should check also

p(a|c) = 0.45, p(b|c) = 0.45, p(c|c) = 0.1,

from which we maintain the hypotheses (c,a) and (c,b) (0.45   0.45 and 0.45   0.45,
respectively.) note that with k = 2, we have discarded (a,a).

now, the greedy search ends by computing the last conditional probabilities:

p(a|a,a) = 0.9, p(b|a,a) = 0.05, p(c|a,a) = 0.05.

the    nal verdict from the greedy search is therefore (a,a,a) with its id203 being
0.5   0.4   0.9 = 0.18.

what happens with the id125 having k = 2? we need to check the following

conditional probabilities:

p(a|c,a) = 0.7, p(b|c,a) = 0.2, p(c|c,a) = 0.1
p(a|c,b) = 0.4, p(b|c,b) = 0.0, p(c|c,b) = 0.6

from here we consider (c,a,a) and (c,b,c) with the corresponding probabilities 0.45  
0.45   0.7 = 0.14175 and 0.45   0.45   0.6 = 0.1215. among these two, (c,a,a) is
   nally chosen, due to its higher id203 than that of (c,b,c).

in summary, the greedy search found (a,a,a) whose id203 is

p(a,a,a) = 0.18,

and the id125 with k = 2 found (c,a,a) whose id203 is

p(c,a,a) = 0.14175.

even with a larger k, the id125 found a worse translation!
now, clearly, what one can do is to set the maximum beam width   k and try with
all possible 1     k       k. among the translations given by   k id125 procedures,
the best translation can be selected based on their corresponding probabilities. from
the point of view of computational complexity, this is perhaps the best approach to
upper-bound the worst-case memory consumption. doing the id125 once with
  k or multiple id125es with k = 1, . . . ,   k are equivalent in terms of memory con-
sumption, i.e., both are o(k|v|). furthermore, the worst-case computation is o(k|v|)
(assuming a constant time computation for computing each id155.) in
practice however, the constant in front of k|v| does matter, and we often choose k
based on the translation quality of the validation set, after trying a number of values   
{1,2,4,8,16}.

if you   re interested in how to improve id125 by backtracking so that the
id125 becomes complete, refer to, e.g., [44, 113]. if you   re interested in general
search strategies, refer to [90]. also, in the context of id151, it
is useful to read [64].

96

6.3 attention-based id4

one important property of the simple encoder-decoder model for neural machine trans-
lation (from sec. 6.2) is that a whole source sentence is compressed into a single real-
valued vector c. this sounds okay, since the space of all possible source sentences is
countable, while the context vector space [   1,1]d is uncountable. there exists a map-
ping from this sentence space to the context vector space, and all we need to ensure is
that training the simple encoder-decoder model    nds this mapping. this is conditioned
on the assumption that the hypothesis space9 de   ned by the model architecture   the
number of hidden units and parameters    includes this mapping from any source sen-
tence to a context vector.

unfortunately, considering the complexity of any natural language sentence, it is
quite easy to guess that this mapping must be highly nonlinear and will require a huge
encoder, and consequently, a huge decoder to map back from a context vector to a target
sentence. in fact, this fact was empirically validated last year (2014), when the almost
identical models from two groups [101, 27] showed vastly different performances on
the same english   french translation task. the only difference there was that the au-
thors of [101] used a much larger model than the authors of [27] did.

at a more fundamental level there   s a question of whether a natural language sen-
tence should be fully represented as a single vector. for instance, there is now a famous
quote by prof. raymond mooney10 of the university of texas at austin:    you can   t
cram the meaning of a whole %&!$# sentence into a single $&!#* vector!   11 though,
our goal is not in answering this fundamental question from linguistics.

our goal is rather to investigate the possibility of avoiding this situation of having
to learn a highly nonlinear, complex mapping from a source sentence to a single vector.
the question we are more interested in is whether there exists a neural network that
can handle a variable-length sentence by building a variable-length representation of
it. especially, we are interested in whether we can build a id4
system that can exploit a variable-length context representation.

variable-length context representation in the simple encoder-decoder model, a
source sentence, regardless of its length, was mapped to a single context vector by a
recurrent neural network:

(cid:16)

(cid:17)

ht =   enc

ht   1,e(cid:62)
x xt

.

see eq. (6.5) and the surrounding text for more details.

instead, here we will encode a source sentence x = (x1,x2, . . . ,xtx ) with a set c of
context vectors ht   s. this is achieved by having two recurrent neural networks rather
than a single recurrent neural networks, as in the simple encoder-decoder model. the
   rst recurrent neural network, to which we will refer as a forward recurrent neural
network, reads the source sentence as usual and results in a set of forward memory

9 see sec. 2.3.2.
10 https://www.cs.utexas.edu/  mooney/
11 http://nlpers.blogspot.com/2014/09/amr-not-semantics-but-close-maybe.

html

97

figure 6.6: an encoder with a bidirectional recurrent neural network

      
h t, for t = 1, . . . ,tx. the second recurrent neural network, a reverse recurrent
states
neural network, reads the source sentence in a reverse order, starting from xtx to x1.
      
h t, for t =
this reverse network will output a sequence of reverse memory states
1, . . . ,tx.

for each xt, we will concatenate

      
h t and

(cid:35)

      
(cid:34)       
h t to form a context-dependent vector ht:
h t      
h t

(6.9)

ht =

we will form a context set with these context-dependent vectors c = {h1,h2, . . . ,htx}.
see fig. 6.6 for the graphical illustration of this process.
now, why is ht a context-dependent vector? we should look at what the input was

to a function that computed ht. the    rst half of ht,

(cid:16)

      
h t =   fenc

  fenc

      
h t, was computed by

(cid:17)

(cid:17)

x xt   1

,e(cid:62)
x xt

,

      
h t was

(cid:16)
(cid:105)(cid:62)

where   fenc is a forward recurrent activation function. from this we see that
computed by all the source words up to t, i.e., x   t. similarly,
,e(cid:62)
x xt
      
h t depends on all the source

      
h t =   renc

x xt+1

  renc

(cid:17)

(cid:17)

,

where   renc is a reverse recurrent activation function, and
words from t to the end, i.e., x   t.
      
h (cid:62)
t

in summary, ht =

h (cid:62)
t ;

(cid:104)      

is a vector representation of the t-th word, xt, with
respect to all the other words in the source sentence. this is why ht is a context-
dependent representation. but, then, what is the difference among all those context-
dependent representations {h1, . . . ,htx}? we will discuss this shortly.
(cid:105)(cid:17)(cid:104)

decoder with attention mechanism before anything let us think of what the mem-
ory state zt of the decoder (from eq. (6.6)) does:

(cid:105)(cid:17)

(cid:105)(cid:17)

(cid:16)

(cid:16)

(cid:104)

(cid:104)

e(cid:62)
y yt   3;c

e(cid:62)
y yt   2;c

,

e(cid:62)
y yt   1;c

zt =   dec

  dec

  dec

(cid:16)       ,

(cid:16)       ,e(cid:62)
(cid:16)       ,e(cid:62)

98

figure 6.7: illustration of how the relevance score e2,3 of the second context vector h2
at time step 3 (dashed curves and box.)

it is computed based on all the generated target words so far (   y1,   y2, . . . ,   yt   1) and
the context vector12 c which is the summary of the source sentence. the very reason
why i designed the decoder in this way is so that the memory state zt is informative of
which target word should be generated at time t after generating the    rst t     1 target
words given the source sentence. in order to do so, zt must encode what have been
translated so far among the words that are supposed to be translated (which is encoded
in the context vector c.) let   s keep this in mind.
in order to compute the new memory state zt with a context set c ={h1,h2, . . . ,htx},
we must    rst get one vector out of tx context vectors. why is this necessary? because
we cannot have an in   nitely large number of parameters to cope with any number of
context vectors. then, how can we get a single vector from an unspeci   ed number of
context vectors ht   s?

first, let us score each context vector h j ( j = 1, . . . ,tx) based on how relevant it is
for translating a next target word. this scoring needs to be based on (1) the previous
memory state zt   1 which summarizes what has been translated up to the (t     2)-th
word13, (2) the previously generated target word   yt   1, and (3) the j-th context vector
h j:

e j,t = fscore(zt   1,e(cid:62)

y   yt   1,h j).

(6.10)

conceptually, the score e j,t will be computed by comparing (zt   1,   yt   1) with the con-
text vector c j. see fig. 6.7 for graphical illustration.

12 we will shortly switch to using a context set instead.
13 think of why this is only up to the (t     2)-th word not up to the (t     1)-th one.

99

figure 6.8: computing the new memory state zt of the decoder based on the previous
memory state zt   1, the previous target word   yt   1 and the weighted average of context
vectors according to the attention weights.

once the scores for all the context vectors h j   s ( j = 1, . . . ,tx) are computed by

fscore, we normalize them with a softmax function:
exp(e j,t )
j(cid:48)=1 exp(e j(cid:48),t )

   j,t =

   tx

.

(6.11)

we call these normalized scores the attention weights, as they correspond to how much
the decoder attends to each of the context vectors. this whole process of computing
the attention weights is often referred to as an attention mechanism (see, e.g., [26].)

we take the weighted average of the context vectors with these attention weights:

ct =

tx   

j=1

   j,th j

(6.12)

this weighted average is used to compute the new memory state zt of the decoder,
which is identical to the decoder   s update equation from the simple encoder-decoder
model (see eq. (6.6)) except that ct is used instead of c ((a) in the equation below):

         zt   1,

         e(cid:62)
y yt   1; ct(cid:124)(cid:123)(cid:122)(cid:125)

         
         

(a)

zt =   dec

100

see fig. 6.8 for the graphical illustration of how it works.

given the new memory state zt of the decoder, the output probabilities of all the
target words in a vocabulary happen without any change from the simple encoder-
decoder model in sec. 6.2.

we will call this model, which has a bidirectional recurrent neural network as an en-
coder and a decoder with the attention mechanism, an attention-based encoder-decoder
model. this approach was proposed last year (2014) in the context of machine transla-
tion in [2] and has been studied extensively in [76].

6.3.1 what does the attention mechanism do?
one important thing to notice is that this attention-based encoder-decoder model can be
reduced to the simple encoder-decoder model easily. this happens when the attention
mechanism fscore in eq. (6.10) returns a constant regardless of its input. when this
happens, the context vector ct at each time step t (see eq. (6.12)) is same for all the
time steps t = 1, . . . ,ty:

ct =

1
tx

tx   

j=1

h j.

the encoder effectively maps the whole input sentence into a single vector, which was
at the core of the simple encoder-decoder model from sec. 6.2.

this is not the only situation in which this type of behaviour happens. another
      
h 1, of
possible scenario is for the encoder to make the last memory states,
the forward and reverse recurrent neural networks to have a special mark telling that
these are the last states. the attention mechanism then can exploit this to assign a large
score to these two memory states (but still constant across time t.) this will become
even closer to the simple encoder-decoder model.

      
h tx and

the question is how we can avoid these degenerate cases. or, is it necessary for us
to explicitly make these degenerate cases unlikely? of course, there is no single answer
to this question. let me give you my answer, which may differ from others    answer:
no.

the goal of introducing a novel network architecture is to guide a model according
to our intuition or scienti   c observation so that it will do a better job at a target task. in
our case, the attention mechanism was introduced based on our observation, and some
intuition, that it is not desirable to ask the encoder to compress a whole source sentence
into a single vector.

this incorporation of prior knowledge however should not put a hard constraint.
we give a model a possibility of exploiting this prior knowledge, but should not force
the model to use this prior knowledge exclusively. as this prior knowledge, based
on our observation of a small portion of data, is not likely to be true in general, the
model must be able to ignore this, if the data does not exhibit the underlying structure
corresponding to this prior knowledge. in this case of attention-based encoder-decoder
model, the existence of those degenerate cases above is a direct evidence of what this
attention-based model can do, if there is no such underlying structure present in the
data.

101

then, a natural next question is whether there are such structures that can be well
exploited by this attention mechanism in real data.
if we train this attention-based
encoder-decoder model on the parallel corpora we discussed earlier in sec. 6.1.1, what
kind of structure does this attention-based model learn?

in order to answer this question, we must    rst realize that we can easily visualize
what is happening inside this attention-based model. first, note that given a pair of
source x and target y sentences,14 the attention-based model computes an alignment
matrix a     [0,1]

|x|  |y|:

               

a =

  1,2
  1,1
  2,2
  2,1
...
...
  |x|,1   |x|,2

  1,|y|
  2,|y|
...

      
      
...
         |x|,|y|

                ,

where    j,t is de   ned in eq. (6.11).

each column at of this alignment matrix a is how well each source word (based
on its context-dependent vector representation from eq. (6.9)) is aligned to the t-th
target word. each row b j similarly shows how well each target word is aligned to the
content-dependent vector of the j-th source word. in other words, we can simply draw
the alignment matrix a as if it were a gray scale 2-d image.

in fig. 6.9, the visualization of four alignment matrices is presented. it is quite
clear, especially to a french-english bilingual speaker, that the model indeed captured
the underlying structure of word/phrase mapping between two languages. for instance,
focus on    european economic area    in fig. 6.9 (a). the model correctly noticed
that    area    corresponds to    zone   ,    economic    to      economique   , and    european    to
   europ  eenne   , without any supervision about this type of alignment.

this is nice to see that the model was able to notice these regularities from data
without any explicit supervision. however, the goal of introducing the attention mech-
anism was not to get these pretty    gures. after all, our goal is not to build an inter-
pretable model, but a model that is predictive of the correct output given an input (see
chapter 1 and [16].) in this regard, how much does the introduction of the attention
mechanism help?

in [2], the attention-based encoder-decoder model was compared against the sim-
ple encoder-decoder model in the task of english-french translation. they observed
the relative improvement of up to 60% (in terms of id7, see sec. 6.1.2,) as shown in
table 6.1. furthermore, by using some of the latest techniques, such as handling large
vocabularies [55], building a vocabulary of subword units [93] and variants of the atten-
tion mechanism [76], it has been found possible to achieve a better translation quality
with id4 than the existing state-of-the-art translation systems.

14 note that if you   re given only a source sentence, you can let the model translate and align simultane-

ously.

102

model

simple enc   dec

attention-based enc   dec

attention-based enc   dec (lv)
attention-based enc   dec (lv)(cid:63)

state-of-the-art smt   

   

id7 rel. improvement
17.82
28.45
34.11
37.19
37.03

+59.7%
+90.7%
+106.0%

   

table 6.1: the translation performances and the relative improvements over the simple
encoder-decoder model on an english-to-french translation task (wmt   14), measured
by id7 [2, 55]. (cid:63): an ensemble of multiple attention-based models.    : the state-of-
the-art phrase-based id151 system [39].

6.4 warren weaver   s memorandum
in 1949 warren weaver15 wrote a memorandum, titled (cid:104)translation(cid:105) on machine trans-
lation [108]. although this text was written way before computers have become ubiq-
uitous,16 there are many interesting ideas that are closely related to what we have dis-
cussed so far in this chapter. let us go over some parts of the weaver   s memorandum
and see how the ideas there corresponds to modern-day machine translation.

necessity of linguistic knowledge weaver talks about a distinguished mathemati-
cian p who was surprised by his colleague. his colleague    had an amateur interest in
cryptography   , and one day presented p his method to    decipher    an encrypted turkish
text successfully.    the most important point   , according to weaver, from this instance
is that    the decoding was done by someone who did not know turkish.    now, this
sounds familiar, doesn   t it?

as long as there was a parallel corpus, we are able to use neural machine transla-
tion models, described throughout this chapter, without ever caring about which lan-
guages we are training a model to translate between. especially if we decide to consider
each sentence as a sequence of characters,17 there is almost no need for any linguistic
knowledge when building these id4 systems.

this lack of necessity for linguistic knowledge is not new. in fact, the most widely
studied and used machine translation approach, which is (count-based) statistical ma-
chine translation [19, 66], does not require any prior knowledge about source and target
languages. all it needs is a large corpus.

importance of context recall from sec. 6.3 that the encoder of an attention-based
id4 uses a bidirectional recurrent neural network in order to ob-
tain a context set. each vector in the context set was considered a context-dependent

15 yes, this is the very same weaver after which the building of the courant institute of mathematical

sciences has been named.

16 although weaver talks about modern computers over and over in his memorandum, what he refers to

is not exactly what we think of computers as these days.

17 in fact, only very recently people have started investigating the possibility of building a machine trans-
lation system based on character sequences [73]. this has been made possible due to the recent success of
id4.

103

vector, as it represents what the center word means with respect to all the surround-
ing words. this context dependency is a necessary component in making the whole
attention-based id4, as it helps disambiguating the meaning of
each word and also distinguishing multiple occurrences of a single word by their con-
text.

weaver discusses this extensively in sec. 3   4 in his memorandum. first, to weaver,
it was    amply clear that a translation procedure that does little more than handle a one-
to-one correspondence of words can not hope to be useful ..
in which the problems
of .. multiple meanings .. are frequent.    in other words, it is simply not possible to
look at each word separately from surrounding words (or context) and translate it to a
corresponding target word, because there is uncertainty in the meaning of the source
word which can only be resolved by taking into account its context.

so, what does weaver propose in order to address this issue? he proposes in sec. 5
that if    one can see not only the central word in question, but also say n words on
either side, then if [sic] n is large enough one can unambiguously decide the meaning
of the central word.    if we consider only a single sentence and take the in   nite limit of
n        , we see that what weaver refers to is exactly the bidirectional recurrent neural
network used by the encoder of the attention-based translation system. furthermore,
we see that the continuous bag-of-words language model, or markov random    eld
based language model, from sec. 5.4.2 exactly does what weaver proposed by setting
n to a    nite number.

in sec. 5.2.1, we talked about the issue of data sparsity, and how it is desirable to
have a larger n but it   s often not a good idea statistically to do so. weaver was also
worried about this by saying that    it would hardly be practical to do this by means of
a generalized dictionary which contains all possible phases [sic] 2n + 1 words long;
for the number of such phases [sic] is horrifying.    we learned that this issue of data
sparsity can be largely avoided by adopting a fully parametric approach instead of a
table-based approach in sec. 5.4.

common base of human communications weaver suggested in the last section of
his memorandum that    perhaps the way    for translation    is to descend, from each lan-
guage, down to the common base of human communication     the real but as yet undis-
covered universal language     and then re-emerge by whatever particular route is conve-
nient.    he speci   cally talked about a    universal language   , and this makes me wonder
if we can consider the memory state of the recurrent neural networks (both of the en-
coder and decoder) as this kind of intermediate language. this intermediate language
radically departs from our common notion of natural languages. unlike conventional
languages, it does not use discrete symbols, but uses continuous vectors. this use of
continuous vectors allows us to use simple arithmetics to manipulate the meaning, as
well as its surface realization.18

this view may sound radical, considering that what we   ve discussed so far has been
con   ned to translating from one language to another. after all, this universal language

18 if you    nd this view too radical or fascinating, i suggest you to look at the presentation slides by
geoff hinton at https://drive.google.com/file/d/0b16rwcmqqrtdmwfaethbtc1mzkk/
view?usp=sharing

104

of ours is very speci   c to only a single source language with respect to a single target
language. this is however not a constraint on the id4 by design,
but simply a consequence of our having focused on this speci   c case.

indeed, in this year (2015), researchers have begun to report that it is possible to
build a id4 model that considers multiple languages, and even
further multiple tasks [38, 75]. more works in this line are expected, and it will be
interesting to see if weaver   s prediction again turns out to be true.

105

(a)

(b)

(c)

(d)

figure 6.9: visualizations of the four sample alignment matrices. the alignment
matrices were computed from an attention-based translation model trained to translate
a sentence in english to french. reprinted from [2].

106

theagreementontheeuropeaneconomicareawassignedinaugust1992.<end>l'accordsurlazone  conomiqueeurop  ennea  t  sign  enao  t1992.<end>itshouldbenotedthatthemarineenvironmentistheleastknownofenvironments.<end>ilconvientdenoterquel'environnementmarinestlemoinsconnudel'environnement.<end>destructionoftheequipmentmeansthatsyriacannolongerproducenewchemicalweapons.<end>ladestructiondel'  quipementsignifiequelasyrienepeutplusproduiredenouvellesarmeschimiques.<end>"thiswillchangemyfuturewithmyfamily,"themansaid.<end>"celavachangermonaveniravecmafamille",aditl'homme.<end>chapter 7

final words

let me wrap up this lecture note by describing some aspects of natural language under-
standing with distributed representations that i have not discussed in this course. these
are the topics that i would have spent time on, had the course been scheduled to last
twice the duration as it is now. afterward, i will    nalize this whole lecture note with a
short summary.

7.1 multimedia description generation as translation

those who have followed this course closely so far must have noticed that the neural
machine translation model described in the previous chapter is quite general in the
sense that the input to the model does not have to be a sentence. in the case of the
simple encoder-decoder model from sec. 6.2, it is clear that any type of input x can be
used instead of a sentence, as long as there is a feature extractor that returns the vector
representation c of the input.

and, fortunately, we already learned how to build a feature extractor throughout
this course. almost every single model (that is, a neural network in our case) converts
an input into a continuous vector. let us take a multilayer id88 from sec. 3.3
as an example. any classi   er built as a multilayer id88 can be considered as a
two-stage process (see sec. 3.3.2.) first, the feature vector of the input is extracted (see
eq. (3.9)):

   (x) =    (ux + c).

the extracted feature vector    (x) is then af   ne-transformed, followed by softmax func-
tion. this results in a conditional distribution over all possible labels (see eq. (4.4).)

this means that we can make the simple encoder-decoder model to work with non-
language input simply by replacing the recurrent neural network based encoder with
the feature extraction stage of the multilayer id88. furthermore, it is possible to
pretrain this feature extractor by training the whole multilayer id88 on a separate
classi   cation dataset.1

1 this way of using a feature extractor pretrained from another network has become a de facto standard

107

this approach of using the encoder-decoder model for describing non-language
input has become popular in recent years (especially, 2014 and 2015,) and has been
applied to many applications, including image/video description generation and speech
recognition. for an extensive list of these applications, i refer the readers to a recent
review article by cho et al. [26].

example: image id134 let me take as an example the task of im-
age id134. the possibility of using the encoder-decoder model for image
id134 was noticed by several research groups (almost simultaneously) last
year (2014) [62, 106, 59, 78, 37, 40, 25].2 the success of id4
in [101] and earlier success of deep convolutional network on object recognition (see,
e.g., [67, 96, 102]) inspired them the idea to use the deep convolutional network   s fea-
ture extractor together with the recurrent neural network decoder for the task of image
id134.

right after these, xu et al. [111]
realized that it is possible to use the
attention-based encoder-decoder model
from sec. 6.3 for image caption gen-
eration. unlike the simple model, the
attention-based model requires a context
set instead of a context vector. the con-
text set should contain multiple context
vectors, and each vector should repre-
sent a spatial location with respect to
the whole image, meaning each context
vector is a spatially-localized, context-
dependent image descriptor. this was
achieved by using the last convolutional
layer   s activations of the pretrained deep
convolutional network instead of the last
fully-connected layer   s. see fig. 7.1 for
graphical illustration of this approach.

figure 7.1: image id134 with
the attention-based encoder-decoder model
[111].

these approaches based on neural
networks, or in other words based on dis-
tributed representations, have been suc-
cessful at image id134. four out of    ve top rankers in the recent microsoft
coco image captioning challenge 20153 were using variants of the neural encoder-
decoder model, based on human evaluation of the captions.

in many of the id161 tasks [94]. this is also closely related to semi-supervised learning with
pretrained id27s which we discussed in sec. 5.4.3. in that case, it was only the    rst input layer
that was pretrained and used later (see eqs. (5.11)   (5.12).)

2 i must however make a note that kiros et al. [62] proposed a fully neural network based image caption

generation earlier than all the others cited here did.

3 http://mscoco.org/dataset/#captions-leaderboard

108

annotationvectorsword ssampleuirecurrentstatezif = (a,   man,   is,   jumping,   into,   a,   lake,   .)+hjattentionmechanismaattention        weightjaj  =1convolutional neural network7.2 language understanding with world knowledge

in sec. 1.2, we talked about how we view natural languages as a function. this function
of natural language maps from a tuple of a speaker   s speech, a listener   s mental state
and the surrounding world to the listener   s reaction, often as a form of natural language
response. unfortunately, in order to make it manageable, we decided to build a model
that approximates only a part of this true function.

immediate state of the surrounding world in this course of action, one thing we
have dropped out is the surrounding world. the surrounding world may mean many
different things. one of them is the current state of the surrounding world. as an
example, when i say    look at this cute llama,    it is quite likely that the surrounding
world at the current state contains either an actual llama or at least a picture of a llama.
a listener then understands easily what a llama is even without having known what a
llama is in advance. by looking at the picture of llama, the listener makes a mental note
that the llama looks similar to a camel and therefore must be a four-legged animal.

if the surrounding world is not taken into account, as we   ve been doing so far,
the listener can only generalize based on the context words. just like how the neural
language model from sec. 5.4 generalized to unseen, or rarely seen words, the listener
can infer that    llama    must be a type of animal by remembering that the phrase    look
at this cute    has mainly been followed by an animal such as    cat    or    dog   . however,
it is quite clear that    look at this cute    is also followed by many other nouns, including
   baby   ,    book    and so on.

the question is then how to exploit this. how can we incorporate, for instance,

vision information from the surrounding world into natural language understanding?

the simplest approach is to simply concatenate a id27 vector (see
eq. (5.12)) and a corresponding image vector (obtained from an existing feature ex-
tractor, see above) [60]. this can be applied to any existing language models such as
neural language model (see sec. 5.4) and id4 model (see chap-
ter 6.) this approach gives a strong signal to the model the similarities among different
words based on the corresponding objects    appearances. this approach of concatenat-
ing vectors of two different modalities, e.g., language and vision, was earlier proposed
in [109].

a more sophisticated approach is to design and train a model to solve a task that
requires tight interaction between language and other modalities. as our original goal
is to build a natural language function, all we need to do is to build a function approxi-
mator that takes as input both language and other modalities. recently, antol et al. [1]
built a large-scale dataset of question-answer-image triplets, called visual question an-
swering (vqa) for this speci   c purpose. they have carefully built the dataset such
that many, if not most, questions can only be answered when the accompanying image
is taken into consideration. any model that   s able to solve the questions in this dataset
well will have to consider both language and vision.

knowledge base: lost in a library so far, we have talked about incorporating an
immediate state of the surrounding world. however, our use of languages is more

109

sophisticated. this is especially apparent in written languages. let us take an example
of me writing this lecture note. it is not the case where i simply sit and start writing
the whole text based purely on my mental state (with memory of my past research) and
the immediate surrounding world state (which has almost nothing to do with.) rather,
a large part of this writing process is spent on going through various research articles
and books written by others in order to    nd relevant details of the topic.

in this case, the surrounding world is a database in which human knowledge is
stored. you can think of a library or the internet. as the amount of knowledge is
simply too large to be memorized in the entirety, it is necessary for a person to be able
to search through the vast knowledge base. but, wait, what does it have to do with
natural language understanding?

consider the case where the context phrase is    llama is a domesticated camelid
from   . without access to the knowledge base, or in this speci   c instance, access to
wikipedia, any language model can only say as much as that this context phrase is
likely followed by a name of some place. this is especially true, if we assume that the
training corpus did not mention    llama    at all. however, if the language model is able
to search wikipedia and condition on its search result, it suddenly becomes so obvious
that this context phrase is followed by    south america    or the name of any region on
andean mountain rages.

although this may sound too complicated a task to incorporate into a neural net-
work, the concept of how to incorporate this is not necessarily complicated. in fact, we
can use the attention mechanism, discussed in sec. 6.3, almost as it is. let us describe
here a conceptual picture of how this can be done.
let d = {d1,d2, . . . ,dm} be a set of knowledge vectors. each knowledge vector
di is a vector representation of a piece of knowledge. for instance, di can be a vector
representation of one wikipedia article. it is certainly unclear what is the best way to
obtain this vector representation of an entire article, but let us assume that an oracle
gave us a means to do so.

let us focus on recurrent language modelling from sec. 5.5.4 at each time step,

we have access to the following vectors:

1. context vector ht   1: the summary all the preceding words
2. current word wt: the current input word

similarly to what we have done in sec. 6.3, we will de   ne a scoring function fscore
which scores each knowledge vector di with respect to the context vector and the cur-
rent word:

  i,t     exp ( fscore(di,ht   1,ewt )) ,
where ewt is a vector representation of the current word wt.

this score re   ects the relevance of the knowledge in predicting the next word, and
once it is computed for every knowledge vector, we compute the weighted sum of all

4 this approach of using attention mechanism for external knowledge pieces has been proposed recently
in [14], in the context of question-answering. here, we stick to language modelling, as the course has not
dealt with question-answering tasks.

110

the knowledge:

  dt =

m

   

i=1

  i,tdi.

this vector   dt is a vector summary of the knowledge relevant to the next word, taking
into account the context phrase. in the case of an earlier example, the scoring function
gives a high score to the wikipedia article on    llama    based on the history of preceding
words    llama is a domesticated camelid from   .

this knowledge vector is used when updating the memory state of the recurrent

neural network:

(cid:0)ht   1,ewt ,   dt

(cid:1) .

ht = frec

from this updated memory state, which also contains the knowledge extracted from
the selected knowledge vector, the next word   s distribution is computed according to
eq. (4.6).

one important issue with this approach is that the size of knowledge set d is often
extremely large. for instance, english wikipedia contains more than 5m articles as of
23 nov 2015.5 it easily becomes impossible to score each and every knowledge vector,
not to mention to extract knowledge vectors of all the articles.6 it is an open question
how this unreasonable amount of computation needed for search can be avoided.

why is this any signi   cant? one may naively think that if we train a large enough
network with a large enough data which contains all those world knowledge, a trained
network will be able to contain all those world knowledge (likely in a compressed
form) in its parameters together with its network architecture. this is true up to a
certain level, but there are many issues here.

first, the world knowledge we   re talking about here contains all the knowledge
accumulated so far. even a human brain, arguably the best working neural network
to date, cannot store all the world knowledge and must resort to searching over the
external database of knowledge. it is no wonder we have libraries where people can go
and look for relevant knowledge.

second, the world knowledge is dynamic. every day some parts of the world
knowledge become obsolete, and at the same time previously unknown facts are added
to the world knowledge. if anyone looked up    facebook    before 2004, they would   ve
ended up with yearly facebooks from american universities. nowadays, it is almost
certain that when a person looks up    facebook   , they will    nd information on    face-
book    the social network site. having all the current world knowledge encoded in the
model   s parameters is not ideal in this sense.

5 https://en.wikipedia.org/wiki/wikipedia:statistics
6 this is true especially when those knowledge vectors are also updated during training.

111

7.3 larger-context language understanding:

beyond sentences and beyond words

if we view natural language as a function, it becomes clear that what we   ve discussed
so far throughout the course is heavily restrictive. there are two reasons behind this
restriction.

first, what we have discussed so far has narrowly focused on handling a sentence.
in sec. 5.2, i have described language model as a way to model a sentence id203
p(s). this is a bit weird in the sense that we   ve been using a term    language    modelling
not    sentence    modelling. keeping it in mind, we can start looking at a id203 of a
document or discourse d as a whole rather than as a product of sentence probabilities:

p(d) =

p(sk|s<k),

n

   

k=1

where the document d consists of n sentences. this approach is readily integrated into
the language modelling approaches we discussed earlier in chapter 5 by

p(d) =

n

   

k=1

p(w j|w< j,s<k).

tk   

j=1

this is applicable to any language-related models we have discussed so far, includ-
ing neural language model from sec. 5.4, recurrent language model from sec. 5.5,
markov random    eld language model from sec. 5.4.2 and id4
from chapter 6.

in the context of language modelling, two recent articles proposed to explore this

direction. i refer the readers to [107] and [57].

second, we have stuck to representing a sentence as a sequence of words so far,
despite a short discussion in sec. 5.1.2 where i strongly claim that this does not have
to be. this is indeed true, and in fact, even if we replace most occurrence of    word   
in this course with, for instance,    character   , all the arguments stand. of course, by
using smaller units than words, we run into many practical and theoretical issues. one
most severe practical issue is that each sentence suddenly becomes much longer. one
most sever theoretical issue is that it is a highly nonlinear mapping from a sequence
of characters to its meaning, as we discussed earlier in sec. 5.1.2. nevertheless, the
advance in computing and deep neural networks, which are capable of learning such
a highly nonlinear mapping, have begun to let researchers directly work on this prob-
lem of using subword units (see, e.g., [61, 73].) note that i am not trying to say that
characters are the only possible sub-word units, and recently an effective statistical ap-
proach to deriving sub-word units off-line was proposed and applied to neural machine
translation in [93].

112

7.4 warning and summary

before i    nish this lecture note with the summary of what we have discussed through-
out this course, let me warn you by quoting claude shannon [95]:7

it will be all too easy for our somewhat arti   cial prosperity to collapse
overnight when it is realized that the use of a few exciting words like in-
formation, id178, redundancy, do not solve all our problems.

natural language understanding with distributed representation is a fascinating topic
that has recently gathered large interest from both machine learning and natural lan-
guage processing communities. this may give a wrong sign that this approach with
neural networks is an ultimate winner in natural language understanding/processing,
though without any ill intention. as shannon pointed out, this prosperity of distributed
representation based natural language understanding may collapse overnight, as can
any other approaches out there.8 therefore, i warn the readers, especially students, to
keep this quote in their mind and remember that it is not a few recent successes of this
approach to natural language understanding but the fundamental ideas underlying this
approach that matter and should be remembered after this course.

summary finally, here goes the summary of what we have learned throughout this
semester. we began our journey by a brief discussion on how we view human language
as, and we decided to stick to the idea that a language is a function not an entity existing
independent of the surrounding world, including speakers and listeners. is this a correct
way to view a human language? maybe, maybe not.. i will leave it up to you to decide.
in order to build a machine that can approximate this language function, in chap-
ter 2, we studied basic ideas behind supervised learning in machine learning. we de-
   ned what a cost function is, how we can minimize it using an iterative optimization
algorithm, speci   cally stochastic id119, and learned the importance of hav-
ing a validation set for both early-stopping and model selection. these are all basic
topics that are dealt in almost any basic machine learning course, and the only thing
that i would like to emphasize is the importance of not looking at a held-out test set.
one must always select anything related to learning, e.g., hyperparameters, networks
architectures and so on, based solely on a validation set. as soon as one tunes any
of those based on the test set performance, any result from this tuning easily becomes
invalid, or at least highly disputable.

in chapter 3, we    nally talked about deep neural networks, or more traditionally
called multilayer id88.9 i tried to go over basic, but important details as slowly as
possible, including how to build a deep neural network based classi   er, how to de   ne
a cost function and how to compute the gradient w.r.t. the parameters of the network.
however, i must confess that there are better materials for this topic than this lecture
note.

7 i would like to thank adam lopez for pointing me to this quote.
8 though, it is interesting to note that id205 never really collapsed overnight. rather its
prosperity has been continuing for more than half a century since shannon warned us about its potential
overnight collapse in 1956.

9 i personally prefer    multilayer id88   , but it seems like it has gone out of fashion.

113

we then moved on to recurrent neural networks in chapter 4. this was a necessary
step in order to build a neural network based model that can handle both variable-length
input and output. again, my goal here was to take as much time as it is needed to moti-
vate the need of recurrent networks and to give you basic ideas underlying them. also,
i spent quite some time on why it has been considered dif   cult to train recurrent neural
networks by stochastic id119 like algorithms, and as a remedy, introduced
id149 and long short-term memory units.

only after these long four to    ve weeks, have i started talking about how to handle
language data in chapter 5. i motivated neural language models by the lack of general-
ization and the curse of data sparsity. it is my regret that i have not spent much time on
discussing the existing techniques for count-based id165 language models, but again,
there are much better materials and better lecturers for these techniques already. af-
ter the introduction of neural language model, i spent some time on describing how
this neural language model is capable of generalizing to unseen phrases. continuing
from this neural language model, in sec. 5.5, language modelling using recurrent neu-
ral networks was introduced as a way to avoid markov assumption of id165 language
model.

this discussion on neural language model naturally continued on to neural machine
translation in chapter 6. rather than going directly into describing neural machine
translation models, i have spent a full week on two issues that are often overlooked;
data preparation in sec. 6.1.1 and evaluation in sec. 6.1.2. i wish the discussion of these
two topics has reminded students that machine learning is not only about algorithms
and models but is about a full pipeline starting from data collection to evaluation (often
with loops here and there.) this chapter    nished with where we are in 2015, compared
to what weaver predicted in 1949.

of course, there are so many interesting topics in this area of natural language
understanding. i am not quali   ed nor knowledgeable to teach many, if not most, of
those topics unfortunately, and have focused on those few topics that i have worked on
myself. i hope this lecture note will serve at least as a useful starting point into more
advanced topics in natural language understanding with distributed representations.

114

bibliography

[1] s. antol, a. agrawal, j. lu, m. mitchell, d. batra, c. l. zitnick, and d. parikh.
vqa: visual id53. in international conference on computer vi-
sion (iccv), 2015.

[2] d. bahdanau, k. cho, and y. bengio. id4 by jointly

learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

[3] p. baltescu and p. blunsom. pragmatic neural language modelling in machine

translation. arxiv preprint arxiv:1412.7119, 2014.

[4] f. bastien, p. lamblin, r. pascanu, j. bergstra, i. goodfellow, a. bergeron,
n. bouchard, d. warde-farley, and y. bengio. theano: new features and speed
improvements. arxiv preprint arxiv:1211.5590, 2012.

[5] a. g. baydin, b. a. pearlmutter, and a. a. radul. automatic differentiation in

machine learning: a survey. arxiv preprint arxiv:1502.05767, 2015.

[6] y. bengio, n. boulanger-lewandowski, and r. pascanu. advances in optimiz-
ing recurrent networks. in acoustics, speech and signal processing (icassp),
2013 ieee international conference on, pages 8624   8628. ieee, 2013.

[7] y. bengio, n. l  eonard, and a. courville. estimating or propagating gradi-
ents through stochastic neurons for conditional computation. arxiv preprint
arxiv:1308.3432, 2013.

[8] y. bengio, h. schwenk, j.-s. sen  ecal, f. morin, and j.-l. gauvain. neural
probabilistic language models. in innovations in machine learning, pages 137   
186. springer berlin heidelberg, 2006.

[9] y. bengio, p. simard, and p. frasconi. learning long-term dependencies with
id119 is dif   cult. neural networks, ieee transactions on, 5(2):157   
166, 1994.

[10] j. bergstra, o. breuleux, f. bastien, p. lamblin, r. pascanu, g. desjardins,
j. turian, d. warde-farley, and y. bengio. theano: a cpu and gpu math expres-
sion compiler. in proceedings of the python for scienti   c computing conference
(scipy), volume 4, page 3. austin, tx, 2010.

115

[11] j. besag. statistical analysis of non-lattice data. the statistician, pages 179   195,

1975.

[12] c. m. bishop. mixture density networks. 1994.

[13] c. m. bishop. pattern recognition and machine learning. springer, 2006.

[14] a. bordes, n. usunier, s. chopra, and j. weston. large-scale simple question

answering with memory networks. arxiv preprint arxiv:1506.02075, 2015.

[15] l. bottou. online algorithms and stochastic approximations. in d. saad, edi-
tor, online learning and neural networks. cambridge university press, cam-
bridge, uk, 1998.

[16] l. breiman et al. statistical modeling: the two cultures (with comments and a

rejoinder by the author). statistical science, 16(3):199   231, 2001.

[17] j. s. bridle. training stochastic model recognition algorithms as networks can
lead to maximum mutual information estimation of parameters. in d. touretzky,
editor, advances in neural information processing systems 2, pages 211   217.
morgan-kaufmann, 1990.

[18] e. brochu, v. m. cora, and n. de freitas. a tutorial on bayesian optimization
of expensive cost functions, with application to active user modeling and hierar-
chical id23. arxiv:1012.2599 [cs.lg], dec. 2010.

[19] p. f. brown, j. cocke, s. a. d. pietra, v. j. d. pietra, f. jelinek, j. d. lafferty,
r. l. mercer, and p. s. roossin. a statistical approach to machine translation.
computational linguistics, 16(2):79   85, 1990.

[20] c. callison-burch, m. osborne, and p. koehn. re-evaluation the role of id7 in

machine translation research. in eacl, volume 6, pages 249   256, 2006.

[21] a. carnie. syntax: a generative introduction. john wiley & sons, 2013.

[22] m. cettolo, c. girardi, and m. federico. wit3: web inventory of transcribed
and translated talks. in proceedings of the 16th conference of the european as-
sociation for machine translation (eamt), pages 261   268, trento, italy, may
2012.

[23] o. chapelle, b. sch  olkopf, and a. zien, editors. semi-supervised learning.

mit press, cambridge, ma, 2006.

[24] s. f. chen and j. goodman. an empirical study of smoothing techniques for
id38. in proceedings of the 34th annual meeting on association
for computational linguistics, pages 310   318. association for computational
linguistics, 1996.

[25] x. chen and c. l. zitnick. learning a recurrent visual representation for image

id134. arxiv:1411.5654, 2014.

116

[26] k. cho, a. courville, and y. bengio. describing multimedia content using

attention-based encoder   decoder networks. 2015.

[27] k. cho, b. van merri  enboer, d. bahdanau, and y. bengio. on the properties
of id4: encoder-decoder approaches. arxiv preprint
arxiv:1409.1259, 2014.

[28] k. cho, b. van merri  enboer, c. gulcehre, d. bahdanau, f. bougares,
h. schwenk, and y. bengio. learning phrase representations using id56 encoder-
decoder for id151. arxiv preprint arxiv:1406.1078,
2014.

[29] k. cho, b. van merrienboer, c. gulcehre, f. bougares, h. schwenk, and y. ben-
gio. learning phrase representations using id56 encoder-decoder for statistical
machine translation. in proceedings of the empiricial methods in natural lan-
guage processing (emnlp 2014), oct. 2014.

[30] n. chomsky. a review of b. f. skinner   s verbal behavior. language, 35(1):26   

58, 1959.

[31] n. chomsky. linguistic contributions to the study of mind (future). language

and thinking, pages 323   364, 1968.

[32] n. chomsky. syntactic structures. walter de gruyter, 2002.

[33] r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuoglu, and p. kuksa.
natural language processing (almost) from scratch. the journal of machine
learning research, 12:2493   2537, 2011.

[34] t. m. cover. geometrical and statistical properties of systems of linear inequal-
ities with applications in pattern recognition. ieee transactions on electronic
computers, ec-14(3):326   334, 1965.

[35] j. denker and y. lecun. transforming neural-net output levels to id203
distributions. in advances in neural information processing systems 3. citeseer,
1991.

[36] m. denkowski and a. lavie. meteor universal: language speci   c translation
evaluation for any target language. in proceedings of the eacl 2014 workshop
on id151, 2014.

[37] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach, s. venugopalan,
k. saenko, and t. darrell. long-term recurrent convolutional networks for vi-
sual recognition and description. arxiv:1411.4389, 2014.

[38] d. dong, h. wu, w. he, d. yu, and h. wang. id72 for multiple

language translation. acl, 2015.

117

[39] n. durrani, b. haddow, p. koehn, and k. hea   eld. edinburgh   s phrase-based
machine translation systems for wmt-14. in proceedings of the ninth work-
shop on id151, pages 97   104. association for com-
putational linguistics baltimore, md, usa, 2014.

[40] h. fang, s. gupta, f. iandola, r. srivastava, l. deng, p. doll  ar, j. gao, x. he,
m. mitchell, j. c. platt, c. l. zitnick, and g. zweig. from captions to visual
concepts and back. arxiv:1411.4952, 2014.

[41] j. r. firth. a synopsis of linguistic theory 1930-1955. oxford: philological

society, 1957.

[42] r. fletcher. practical methods of optimization. wiley-interscience, new york,

ny, usa, 2nd edition, 1987.

[43] m. l. forcada and r. p.   neco. recursive hetero-associative memories for trans-
lation. in biological and arti   cial computation: from neuroscience to tech-
nology, pages 453   462. springer, 1997.

[44] d. furcy and s. koenig. limited discrepancy id125.

125   131, 2005.

in ijcai, pages

[45] f. a. gers, j. schmidhuber, and f. cummins. learning to forget: continual

prediction with lstm. neural computation, 12(10):2451   2471, 2000.

[46] x. glorot, a. bordes, and y. bengio. deep sparse recti   er neural networks.
in international conference on arti   cial intelligence and statistics, pages 315   
323, 2011.

[47] y. goldberg. a primer on neural network models for natural language process-

ing. arxiv preprint arxiv:1510.00726, 2015.

[48] i. goodfellow, d. warde-farley, m. mirza, a. courville, and y. bengio. maxout
in proceedings of the 30th international conference on machine

networks.
learning (icml-13), pages 1319   1327, 2013.

[49] k. greff, r. k. srivastava, j. koutn    k, b. r. steunebrink, and j. schmidhuber.

lstm: a search space odyssey. arxiv preprint arxiv:1503.04069, 2015.

[50] k. he, x. zhang, s. ren, and j. sun. delving deep into recti   ers: sur-
passing human-level performance on id163 classi   cation. arxiv preprint
arxiv:1502.01852, 2015.

[51] k. hea   eld, i. pouzyrevsky, j. h. clark, and p. koehn. scalable modi   ed
in proceedings of the 51st annual
kneser-ney language model estimation.
meeting of the association for computational linguistics, pages 690   696, so   a,
bulgaria, august 2013.

[52] s. hochreiter, y. bengio, p. frasconi, and j. schmidhuber. gradient    ow in
recurrent nets: the dif   culty of learning long-term dependencies, volume 1. a
   eld guide to dynamical recurrent neural networks. ieee press, 2001.

118

[53] s. hochreiter and j. schmidhuber. long short-term memory. neural computa-

tion, 9(8):1735   1780, 1997.

[54] g.-b. huang, q.-y. zhu, and c.-k. siew. extreme learning machine: theory

and applications. neurocomputing, 70(1   3):489   501, 2006.

[55] s. jean, k. cho, r. memisevic, and y. bengio. on using very large target

vocabulary for id4. in acl 2015, 2014.

[56] y. jernite, a. m. rush, and d. sontag. a fast variational approach for learn-
ing markov random    eld language models. 32nd international conference on
machine learning (icml), 2015.

[57] y. ji, t. cohn, l. kong, c. dyer, and j. eisenstein. document context language

models. arxiv preprint arxiv:1511.03962, 2015.

[58] r. jozefowicz, w. zaremba, and i. sutskever. an empirical exploration of recur-
rent network architectures. in proceedings of the 32nd international conference
on machine learning (icml-15), pages 2342   2350, 2015.

[59] a. karpathy and f.-f. li. deep visual-semantic alignments for generating image

descriptions. arxiv:1412.2306, 2014.

[60] d. kiela and l. bottou. learning image embeddings using convolutional neural
networks for improved multi-modal semantics. proceedings of emnlp, 2014,
2014.

[61] y. kim, y. jernite, d. sontag, and a. m. rush. character-aware neural language

models. arxiv preprint arxiv:1508.06615, 2015.

[62] r. kiros, r. salakhutdinov, and r. zemel. multimodal neural language models.

in icml   2014, 2014.

[63] r. kneser and h. ney. improved backing-off for m-gram id38.
in acoustics, speech, and signal processing, 1995. icassp-95., 1995 interna-
tional conference on, volume 1, pages 181   184. ieee, 1995.

[64] p. koehn. pharaoh: a id125 decoder for phrase-based statistical machine
translation models. in machine translation: from real users to research, pages
115   124. springer, 2004.

[65] p. koehn. europarl: a parallel corpus for id151. in mt

summit, volume 5, pages 79   86. citeseer, 2005.

[66] p. koehn, f. j. och, and d. marcu. statistical phrase-based translation. in pro-
ceedings of the 2003 conference of the north american chapter of the associa-
tion for computational linguistics on human language technology-volume 1,
pages 48   54. association for computational linguistics, 2003.

119

[67] a. krizhevsky, i. sutskever, and g. e. hinton. id163 classi   cation with deep
convolutional neural networks. in advances in neural information processing
systems, pages 1097   1105, 2012.

[68] t. s. kuhn. the structure of scienti   c revolutions. university of chicago press,

2012.

[69] q. v. le, n. jaitly, and g. e. hinton. a simple way to initialize recurrent net-

works of recti   ed linear units. arxiv preprint arxiv:1504.00941, 2015.

[70] y. lecun, y. bengio, and g. hinton. deep learning. nature, 521(7553):436   

444, 2015.

[71] y. lecun, l. bottou, g. orr, and k. r. m  uller. ef   cient backprop. in g. orr
and k. m  uller, editors, neural networks: tricks of the trade, volume 1524 of
lecture notes in computer science, pages 5   50. springer verlag, 1998.

[72] o. levy and y. goldberg. neural id27 as implicit matrix factor-
ization. in z. ghahramani, m. welling, c. cortes, n. lawrence, and k. wein-
berger, editors, advances in neural information processing systems 27, pages
2177   2185. curran associates, inc., 2014.

[73] w. ling, i. trancoso, c. dyer, and a. w. black. character-based neural machine

translation. arxiv preprint arxiv:1511.04586, 2015.

[74] d. g. lowe. object recognition from local scale-invariant features. in computer
vision, 1999. the proceedings of the seventh ieee international conference on,
volume 2, pages 1150   1157. ieee, 1999.

[75] m.-t. luong, q. v. le, i. sutskever, o. vinyals, and l. kaiser. multi-task

sequence to sequence learning. arxiv preprint arxiv:1511.06114, 2015.

[76] m.-t. luong, h. pham, and c. d. manning. effective approaches to attention-

based id4. arxiv preprint arxiv:1508.04025, 2015.

[77] c. d. manning and h. sch  utze. foundations of statistical natural language

processing. mit press, 1999.

[78] j. mao, w. xu, y. yang, j. wang, and a. l. yuille. explain images with multi-

modal recurrent neural networks. arxiv:1410.1090, 2014.

[79] t. mikolov, k. chen, g. corrado, and j. dean. ef   cient estimation of word

representations in vector space. arxiv preprint arxiv:1301.3781, 2013.

[80] t. mikolov, m. kara     at, l. burget, j. cernock`y, and s. khudanpur. recurrent
neural network based language model. in interspeech 2010, pages 1045   
1048, 2010.

[81] v. nair and g. e. hinton. recti   ed linear units improve restricted boltzmann
in proceedings of the 27th international conference on machine

machines.
learning (icml-10), pages 807   814, 2010.

120

[82] k. papineni, s. roukos, t. ward, and w.-j. zhu. id7: a method for automatic
evaluation of machine translation. in proceedings of the 40th annual meeting
on association for computational linguistics, pages 311   318. association for
computational linguistics, 2002.

[83] r. pascanu, t. mikolov, and y. bengio. on the dif   culty of training recurrent
neural networks. in proceedings of the 30th international conference on ma-
chine learning, pages 1310   1318, 2013.

[84] a. perfors, j. tenenbaum, and t. regier. poverty of the stimulus? a rational

approach. in annual conference, 2006.

[85] k. b. petersen, m. s. pedersen, et al. the matrix cookbook. technical university

of denmark, 7:15, 2008.

[86] c. w. post. the three percent problem: rants and responses on publishing,

translation, and the future of reading. open letter, 2011.

[87] p. resnik and n. a. smith. the web as a parallel corpus. computational lin-

guistics, 29(3):349   380, 2003.

[88] h. robbins and s. monro. a stochastic approximation method. the annals of

mathematical statistics, 22(3):400   407, 1951.

[89] f. rosenblatt. principles of neurodynamics: id88s and the theory of brain
mechanisms. report (cornell aeronautical laboratory). spartan books, 1962.

[90] s. russell and p. norvig. arti   cial intelligence: a modern approach. 1995.

[91] j. schmidhuber. deep learning in neural networks: an overview. neural net-

works, 61:85   117, 2015.

[92] h. schwenk. continuous space language models. computer speech & lan-

guage, 21(3):492   518, 2007.

[93] r. sennrich, b. haddow, and a. birch. id4 of rare words

with subword units. arxiv preprint arxiv:1508.07909, 2015.

[94] p. sermanet, d. eigen, x. zhang, m. mathieu, r. fergus, and y. lecun. over-
feat: integrated recognition, localization and detection using convolutional net-
works. arxiv preprint arxiv:1312.6229, 2013.

[95] c. shannon. the bandwagon (edtl.). ire transactions on id205,

1(2):3, 1956.

[96] k. simonyan and a. zisserman. very deep convolutional networks for large-

scale image recognition. arxiv preprint arxiv:1409.1556, 2014.

[97] b. f. skinner. verbal behavior. bf skinner foundation, 2014.

121

[98] j. r. smith, h. saint-amand, m. plamada, p. koehn, c. callison-burch, and
a. lopez. dirt cheap web-scale parallel text from the common crawl. in acl
(1), pages 1374   1383, 2013.

[99] m. snover, b. dorr, r. schwartz, l. micciulla, and j. makhoul. a study of trans-
lation edit rate with targeted human annotation. in proceedings of association
for machine translation in the americas, pages 223   231, 2006.

[100] m. sundermeyer, h. ney, and r. schluter. from feedforward to recurrent lstm
neural networks for id38. audio, speech, and language process-
ing, ieee/acm transactions on, 23(3):517   529, 2015.

[101] i. sutskever, o. vinyals, and q. v. le. sequence to sequence learning with
neural networks. in advances in neural information processing systems, pages
3104   3112, 2014.

[102] c. szegedy, w. liu, y. jia, p. sermanet, s. reed, d. anguelov, d. erhan, v. van-
houcke, and a. rabinovich. going deeper with convolutions. arxiv preprint
arxiv:1409.4842, 2014.

[103] j. turian, l. ratinov, and y. bengio. word representations: a simple and general
method for semi-supervised learning. in proceedings of the 48th annual meeting
of the association for computational linguistics, pages 384   394. association for
computational linguistics, 2010.

[104] l. van der maaten and g. e. hinton. visualizing data using id167. journal of

machine learning research, 9:2579   2605, november 2008.

[105] v. vapnik. the nature of statistical learning theory. springer-verlag new

york, inc., new york, ny, usa, 1995.

[106] o. vinyals, a. toshev, s. bengio, and d. erhan. show and tell: a neural image

caption generator. arxiv preprint arxiv:1411.4555, 2014.

[107] t. wang and k. cho. larger-context language modelling. arxiv preprint

arxiv:1511.03729, 2015.

[108] w. weaver. translation. machine translation of languages, 14:15   23, 1955.

[109] j. weston, s. bengio, and n. usunier. large scale image annotation: learning to
rank with joint word-image embeddings. machine learning, 81(1):21   35, 2010.

[110] t. winograd. understanding natural language. cognitive psychology, 3(1):1   

191, 1972.

[111] k. xu, j. ba, r. kiros, k. cho, a. courville, r. salakhutdinov, r. zemel, and
y. bengio. show, attend and tell: neural image id134 with visual
attention. in international conference on machine learning, 2015.

122

[112] y. zhang, k. wu, j. gao, and p. vines. automatic acquisition of chinese   english
parallel corpus from the web. in advances in information retrieval, pages 420   
431. springer, 2006.

[113] r. zhou and e. a. hansen. beam-stack search: integrating backtracking with

id125. in icaps, pages 90   98, 2005.

123

