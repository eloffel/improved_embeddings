deep id23: an overview

yuxi li (yuxili@gmail.com)

abstract

we give an overview of recent exciting achievements of deep reinforcement learn-
ing (rl). we discuss six core elements, six important mechanisms, and twelve
applications. we start with background of machine learning, deep learning and
id23. next we discuss core rl elements, including value func-
tion, in particular, deep q-network (id25), policy, reward, model and planning,
exploration, and knowledge. after that, we discuss important mechanisms for rl,
including attention and memory, unsupervised learning, id21, multi-
agent rl, hierarchical rl, and learning to learn. then we discuss various appli-
cations of rl, including games, in particular, alphago, robotics, natural language
processing, including dialogue systems, machine translation, and text generation,
id161, business management,    nance, healthcare, education, industry
4.0, smart grid, intelligent transportation systems, and computer systems. we
mention topics not reviewed yet, and list a collection of rl resources. after pre-
senting a brief summary, we close with discussions.
this is the    rst overview about deep id23 publicly available on-
line. it is comprehensive. comments and criticisms are welcome. (this particular
version is incomplete.)

please see deep id23, https://arxiv.org/abs/
1810.06339, for a signi   cant update to this manuscript.

8
1
0
2

 

v
o
n
6
2

 

 
 
]

g
l
.
s
c
[
 
 

6
v
4
7
2
7
0

.

1
0
7
1
:
v
i
x
r
a

1

contents

1 introduction

2 background

.

.
.

.
.

.
.

. .

problem setup .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1 machine learning .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 deep learning .
.
2.3 id23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.2 exploration vs exploitation . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.3 value function .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.4 id145 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.5 temporal difference learning . . . . . . . . . . . . . . . . . . . . . . . .
2.3.6 multi-step id64 . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.7
function approximation . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.8
policy optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.9 deep id23 . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.10 rl parlance .
.
2.3.11 brief summary .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.

.

3 core elements

.

.

.

.

.

.

.

.

.

.

.
.

3.2 policy .

3.1 value function .

.
.
policy gradient .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 deep q-network (id25) and extensions . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 actor-critic .
3.2.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
3.2.3 combining policy gradient with off-policy rl . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
3.3 reward .
3.4 model and planning .
.
3.5 exploration .
3.6 knowledge .
.

.
.
.
.

.
.
.
.

.
.

.
.

.
.

.

.

.

.

.

4 important mechanisms

.

.

4.1 attention and memory .
4.2 unsupervised learning .
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 horde .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.2 unsupervised auxiliary learning . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
4.2.3 id3
4.3 id21 .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 multi-agent id23 . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

.

2

5

7
7
8
9
9
9
9
10
10
11
11
12
14
14
14

15
15
15
17
17
18
20
20
21
22
23

23
23
24
24
24
25
25
26

4.5 hierarchical id23 . . . . . . . . . . . . . . . . . . . . . . . .
4.6 learning to learn .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.6.1 learning to learn/optimize . . . . . . . . . . . . . . . . . . . . . . . . .
4.6.2 zero/one/few-shot learning . . . . . . . . . . . . . . . . . . . . . . . .
4.6.3 neural architecture design . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

5 applications
5.1 games .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

. .

5.2 robotics .

5.4 id161 .

5.1.1
5.1.2
5.1.3 video games .
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
perfect information board games . . . . . . . . . . . . . . . . . . . . . .
imperfect information board games . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.1 guided policy search . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.2 learn to navigate .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 natural language processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3.1 dialogue systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3.2 machine translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
5.3.3 text generation .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .
.
5.4.1 background .
.
.
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4.2 recognition .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .
5.4.3 motion analysis
scene understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4.4
integration with nlp . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4.5
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4.6 visual control
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 business management
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
5.6 finance .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
5.7 healthcare .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
5.8 education .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
5.9
industry 4.0 .
5.10 smart grid .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
5.11 intelligent transportation systems . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.12 computer systems
5.12.1 resource allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.12.2 performance optimization . . . . . . . . . . . . . . . . . . . . . . . . . .
5.12.3 security & privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.

.

.

6 more topics

3

26
27
27
28
28

28
30
30
33
34
34
35
35
35
36
37
37
38
38
39
39
39
40
40
40
41
41
41
41
42
42
42
43
43
43

44

45
45
45
46
46
47
47
47
48
49

50

52

7 resources

.

.

.
.

.
.

.
.

.
.
.
.
.

.
.
.
.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
7.1 books .
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
7.2 more books .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 surveys and reports
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
7.4 courses .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.5 tutorials .
.
7.6 conferences, journals and workshops . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.7 blogs
.
7.8 testbeds .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.9 algorithm implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

8 brief summary

9 discussions

4

1

introduction

id23 (rl) is about an agent interacting with the environment, learning an optimal
policy, by trial and error, for sequential decision making problems in a wide range of    elds in both
natural and social sciences, and engineering (sutton and barto, 1998; 2018; bertsekas and tsitsiklis,
1996; bertsekas, 2012; szepesv  ari, 2010; powell, 2011).
the integration of id23 and neural networks has a long history (sutton and barto,
2018; bertsekas and tsitsiklis, 1996; schmidhuber, 2015). with recent exciting achievements of
deep learning (lecun et al., 2015; goodfellow et al., 2016), bene   ting from big data, powerful
computation, new algorithmic techniques, mature software packages and architectures, and strong
   nancial support, we have been witnessing the renaissance of id23 (krakovsky,
2016), especially, the combination of deep neural networks and id23, i.e., deep
id23 (deep rl).
deep learning, or deep neural networks, has been prevailing in id23 in the last
several years, in games, robotics, natural language processing, etc. we have been witnessing break-
throughs, like deep q-network (mnih et al., 2015) and alphago (silver et al., 2016a); and novel ar-
chitectures and applications, like differentiable neural computer (graves et al., 2016), asynchronous
methods (mnih et al., 2016), dueling network architectures (wang et al., 2016b), value iteration
networks (tamar et al., 2016), unsupervised reinforcement and auxiliary learning (jaderberg et al.,
2017; mirowski et al., 2017), neural architecture design (zoph and le, 2017), dual learning for
machine translation (he et al., 2016a), spoken dialogue systems (su et al., 2016b), information
extraction (narasimhan et al., 2016), guided policy search (levine et al., 2016a), and generative ad-
versarial imitation learning (ho and ermon, 2016), etc. creativity would push the frontiers of deep
rl further with respect to core elements, mechanisms, and applications.
why has deep learning been helping id23 make so many and so enormous achieve-
ments? representation learning with deep learning enables automatic feature engineering and end-
to-end learning through id119, so that reliance on domain knowledge is signi   cantly
reduced or even removed. feature engineering used to be done manually and is usually time-
consuming, over-speci   ed, and incomplete. deep, distributed representations exploit the hierar-
chical composition of factors in data to combat the exponential challenges of the curse of dimen-
sionality. generality, expressiveness and    exibility of deep neural networks make some tasks easier
or possible, e.g., in the breakthroughs and novel architectures and applications discussed above.
deep learning, as a speci   c class of machine learning, is not without limitations, e.g., as a black-box
lacking interpretability, as an    alchemy    without clear and suf   cient scienti   c principles to work
with, and without human intelligence not able to competing with a baby in some tasks. however,
there are lots of works to improve deep learning, machine learning, and ai in general.
deep learning and id23, being selected as one of the mit technology review 10
breakthrough technologies in 2013 and 2017 respectively, will play their crucial role in achieving
arti   cial general intelligence. david silver, the major contributor of alphago (silver et al., 2016a;
2017), even made a formula: arti   cial intelligence = id23 + deep learning (silver,
2016).
the outline of this overview follows. first we discuss background of machine learning, deep learn-
ing and id23 in section 2. next we discuss core rl elements, including value
function in section 3.1, policy in section 3.2, reward in section 3.3, model and planning in sec-
tion 3.4, exploration in section 3.5, and knowledge in section 3.6. then we discuss important mech-
anisms for rl, including attention and memory in section 4.1, unsupervised learning in section 4.2,
id21 in section 4.3, multi-agent rl in section 4.4, hierarchical rl in section 4.5, and,
learning to learn in section 4.6. after that, we discuss various rl applications, including games in
section 5.1, robotics in section 5.2, natural language processing in section 5.3, id161 in
section 5.4, business management in section 5.5,    nance in section 5.6, healthcare in section 5.7,
education in section 5.8, industry 4.0 in section 5.9, smart grid in section 5.10, intelligent trans-
portation systems in section 5.11, and computer systems in section 5.12. we present a list of topics
not reviewed yet in section 6, give a brief summary in section 8, and close with discussions in
section 9.

5

figure 1: conceptual organization of the overview

in section 7, we list a collection of rl resources including books, surveys, reports, online courses,
tutorials, conferences, journals and workshops, blogs, and open sources. if picking a single rl
resource, it is sutton and barto   s rl book (sutton and barto, 2018), 2nd edition in preparation. it
covers rl fundamentals and re   ects new progress, e.g., in deep q-network, alphago, policy gra-
dient methods, as well as in psychology and neuroscience. deng and dong (2014) and goodfellow
et al. (2016) are recent deep learning books. bishop (2011), hastie et al. (2009), and murphy (2012)
are popular machine learning textbooks; james et al. (2013) gives an introduction to machine learn-
ing; provost and fawcett (2013) and kuhn and johnson (2013) discuss practical issues in machine
learning applications; and simeone (2017) is a brief introduction to machine learning for engineers.
figure 1 illustrates the conceptual organization of the overview. the agent-environment interac-
tion sits in the center, around which are core elements: value function, policy, reward, model and
planning, exploration, and knowledge. next come important mechanisms: attention and memory,
unsupervised learning, id21, multi-agent rl, hierarchical rl, and learning to learn.
then come various applications: games, robotics, nlp (natural language processing), computer vi-
sion, business management,    nance, healthcare, education, industry 4.0, smart grid, its (intelligent
transportation systems), and computer systems.
the main readers of this overview would be those who want to get more familiar with deep re-
inforcement learning. we endeavour to provide as much relevant information as possible. for
id23 experts, as well as new comers, we hope this overview would be helpful as
a reference. in this overview, we mainly focus on contemporary work in recent couple of years, by
no means complete, and make slight effort for discussions of historical context, for which the best
material to consult is sutton and barto (2018).
in this version, we endeavour to provide a wide coverage of fundamental and contemporary rl
issues, about core elements, important mechanisms, and applications. in the future, besides further
re   nements for the width, we will also improve the depth by conducting deeper analysis of the issues
involved and the papers discussed. comments and criticisms are welcome.

6

2 background

in this section, we brie   y introduce concepts and fundamentals in machine learning, deep learn-
ing (goodfellow et al., 2016) and id23 (sutton and barto, 2018). we do not give
detailed background introduction for machine learning and deep learning. instead, we recommend
the following recent nature/science survey papers: jordan and mitchell (2015) for machine learn-
ing, and lecun et al. (2015) for deep learning. we cover some rl basics. however, we recommend
the textbook, sutton and barto (2018), and the recent nature survey paper, littman (2015), for
id23. we also collect relevant resources in section 7.

2.1 machine learning

machine learning is about learning from data and making predictions and/or decisions.
usually we categorize machine learning as supervised, unsupervised, and id23.1
in supervised learning, there are labeled data; in unsupervised learning, there are no labeled data; and
in id23, there are evaluative feedbacks, but no supervised signals. classi   cation
and regression are two types of supervised learning problems, with categorical and numerical outputs
respectively.
unsupervised learning attempts to extract information from data without labels, e.g., id91 and
density estimation. representation learning is a classical type of unsupervised learning. however,
training feedforward networks or convolutional neural networks with supervised learning is a kind of
representation learning. representation learning    nds a representation to preserve as much informa-
tion about the original data as possible, at the same time, to keep the representation simpler or more
accessible than the original data, with low-dimensional, sparse, and independent representations.
deep learning, or deep neural networks, is a particular machine learning scheme, usually for su-
pervised or unsupervised learning, and can be integrated with id23, usually as a
function approximator. supervised and unsupervised learning are usually one-shot, myopic, consid-
ering instant reward; while id23 is sequential, far-sighted, considering long-term
accumulative reward.
machine learning is based on id203 theory and statistics (hastie et al., 2009) and optimiza-
tion (boyd and vandenberghe, 2004), is the basis for big data, data science (blei and smyth, 2017;
provost and fawcett, 2013), predictive modeling (kuhn and johnson, 2013), data mining, informa-
tion retrieval (manning et al., 2008), etc, and becomes a critical ingredient for id161, nat-
ural language processing, robotics, etc. id23 is kin to optimal control (bertsekas,
2012), and operations research and management (powell, 2011), and is also related to psychology
and neuroscience (sutton and barto, 2018). machine learning is a subset of arti   cial intelligence
(ai), and is evolving to be critical for all    elds of ai.
a machine learning algorithm is composed of a dataset, a cost/id168, an optimization pro-
cedure, and a model (goodfellow et al., 2016). a dataset is divided into non-overlapping training,
validation, and testing subsets. a cost/id168 measures the model performance, e.g., with
respect to accuracy, like mean square error in regression and classi   cation error rate. training error
measures the error on the training data, minimizing which is an optimization problem. generaliza-
tion error, or test error, measures the error on new input data, which differentiates machine learning
from optimization. a machine learning algorithm tries to make the training error, and the gap be-
tween training error and testing error small. a model is under-   tting if it can not achieve a low
training error; a model is over-   tting if the gap between training error and test error is large.
a model   s capacity measures the range of functions it can    t. vc dimension measures the capacity
of a binary classi   er. occam   s razor states that, with the same expressiveness, simple models are
preferred. training error and generalization error versus model capacity usually form a u-shape
relationship. we    nd the optimal capacity to achieve low training error and small gap between train-
ing error and generalization error. bias measures the expected deviation of the estimator from the
true value; while variance measures the deviation of the estimator from the expected value, or vari-
ance of the estimator. as model capacity increases, bias tends to decrease, while variance tends to

1is id23 part of machine learning, or more than it, and somewhere close to arti   cial

intelligence? we raise this question without elaboration.

7

increase, yielding another u-shape relationship between generalization error versus model capacity.
we try to    nd the optimal capacity point, of which under-   tting occurs on the left and over-   tting
occurs on the right. id173 add a penalty term to the cost function, to reduce the general-
ization error, but not training error. no free lunch theorem states that there is no universally best
model, or best regularizor. an implication is that deep learning may not be the best model for some
problems. there are model parameters, and hyperparameters for model capacity and id173.
cross-validation is used to tune hyperparameters, to strike a balance between bias and variance, and
to select the optimal model.
id113 (id113) is a common approach to derive good estimation of param-
eters. for issues like numerical under   ow, the product in id113 is converted to summation to obtain
negative log-likelihood (nll). id113 is equivalent to minimizing kl divergence, the dissimilarity
between the empirical distribution de   ned by the training data and the model distribution. minimiz-
ing kl divergence between two distributions corresponds to minimizing the cross-id178 between
the distributions. in short, maximization of likelihood becomes minimization of the negative log-
likelihood (nll), or equivalently, minimization of cross id178.
id119 is a common approach to solve optimization problems. stochastic id119
extends id119 by working with a single sample each time, and usually with minibatches.
importance sampling is a technique to estimate properties of a particular distribution, by samples
from a different distribution, to lower the variance of the estimation, or when sampling from the
distribution of interest is dif   cult.
frequentist statistics estimates a single value, and characterizes variance by con   dence interval;
bayesian statistics considers the distribution of an estimate when making predictions and decisions.
generative vs discriminative

2.2 deep learning

deep learning is in contrast to    shallow    learning. for many machine learning algorithms, e.g.,
id75, id28, support vector machines (id166s), id90, and boosting,
we have input layer and output layer, and the inputs may be transformed with manual feature en-
gineering before training. in deep learning, between input and output layers, we have one or more
hidden layers. at each layer except input layer, we compute the input to each unit, as the weighted
sum of units from the previous layer; then we usually use nonlinear transformation, or activation
function, such as logistic, tanh, or more popular recently, recti   ed linear unit (relu), to apply to
the input of a unit, to obtain a new representation of the input from previous layer. we have weights
on links between units from layer to layer. after computations    ow forward from input to output, at
output layer and each hidden layer, we can compute error derivatives backward, and backpropagate
gradients towards the input layer, so that weights can be updated to optimize some id168.
a feedforward deep neural network or multilayer id88 (mlp) is to map a set of input values
to output values with a mathematical function formed by composing many simpler functions at
each layer. a convolutional neural network (id98) is a feedforward deep neural network, with
convolutional layers, pooling layers and fully connected layers. id98s are designed to process
data with multiple arrays, e.g., colour image, language, audio spectrogram, and video, bene   t from
the properties of such signals:
local connections, shared weights, pooling and the use of many
layers, and are inspired by simple cells and complex cells in visual neuroscience (lecun et al.,
2015). resnets (he et al., 2016d) are designed to ease the training of very deep neural networks
by adding shortcut connections to learn residual functions with reference to the layer inputs. a
recurrent neural network (id56) is often used to process sequential inputs like speech and language,
element by element, with hidden units to store history of past elements. a id56 can be seen as a
multilayer neural network with all layers sharing the same weights, when being unfolded in time of
forward computation. it is hard for id56 to store information for very long time and the gradient
may vanish. long short term memory networks (lstm) (hochreiter and schmidhuber, 1997) and
gated recurrent unit (gru) (chung et al., 2014) were proposed to address such issues, with gating
mechanisms to manipulate information through recurrent cells. gradient id26 or its
variants can be used for training all deep neural networks mentioned above.

8

dropout (srivastava et al., 2014) is a id173 strategy to train an ensemble of sub-networks
by removing non-output units randomly from the original network. batch id172 (ioffe and
szegedy, 2015) performs the id172 for each training mini-batch, to accelerate training by
reducing internal covariate shift, i.e., the change of parameters of previous layers will change each
layer   s inputs distribution.
deep neural networks learn representations automatically from raw inputs to recover the compo-
sitional hierarchies in many natural signals, i.e., higher-level features are composed of lower-level
ones, e.g., in images, the hierarch of objects, parts, motifs, and local combinations of edges. dis-
tributed representation is a central idea in deep learning, which implies that many features may
represent each input, and each feature may represent many inputs. the exponential advantages of
deep, distributed representations combat the exponential challenges of the curse of dimensionality.
the notion of end-to-end training refers to that a learning model uses raw inputs without manual
feature engineering to generate outputs, e.g., alexnet (krizhevsky et al., 2012) with raw pixels for
image classi   cation, id195 (sutskever et al., 2014) with raw sentences for machine translation,
and id25 (mnih et al., 2015) with raw pixels and score to play games.

2.3 id23

we provide background of id23 brie   y in this section. after setting up the rl
problem, we discuss value function, temporal difference learning, function approximation, policy
optimization, deep rl, rl parlance, and close this section with a brief summary. to have a good
understanding of deep id23, it is essential to have a good understanding of rein-
forcement learning    rst.

2.3.1 problem setup

a rl agent interacts with an environment over time. at each time step t, the agent receives a state
st in a state space s and selects an action at from an action space a, following a policy   (at|st),
which is the agent   s behavior, i.e., a mapping from state st to actions at, receives a scalar reward
rt, and transitions to the next state st+1, according to the environment dynamics, or model, for
reward function r(s, a) and state transition id203 p(st+1|st, at) respectively. in an episodic
problem, this process continues until the agent reaches a terminal state and then it restarts. the return
k=0   krt+k is the discounted, accumulated reward with the discount factor        (0, 1]. the
agent aims to maximize the expectation of such long term return from each state. the problem is set
up in discrete state and action spaces. it is not hard to extend it to continuous spaces.

rt =(cid:80)   

2.3.2 exploration vs exploitation

multi-arm bandit
various exploration techniques

2.3.3 value function

a   (a|s)(cid:80)

v  (s) =(cid:80)
lowing policy   . q  (s, a) decomposes into the bellman equation: q  (s, a) =(cid:80)
  (cid:80)
bellman equation: q   (s, a) = (cid:80)

a value function is a prediction of the expected, accumulative, discounted, future reward, measur-
ing how good each state, or state-action pair, is. the state value v  (s) = e[rt|st = s] is the
expected return for following policy    from state s. v  (s) decomposes into the bellman equation:
s(cid:48),r p(s(cid:48), r|s, a)[r +   v  (s(cid:48))]. an optimal state value v   (s) = max   v  (s) =
(cid:80)
maxa q      (s, a) is the maximum state value achievable by any policy for state s. v   (s) decom-
s(cid:48),r p(s(cid:48), r|s, a)[r +   v   (s(cid:48))]. the action value
poses into the bellman equation: v   (s) = maxa
q  (s, a) = e[rt|st = s, at = a] is the expected return for selecting action a in state s and then fol-
s(cid:48),r p(s(cid:48), r|s, a)[r +
a(cid:48)   (a(cid:48)|s(cid:48))q  (s(cid:48), a(cid:48))]. an optimal action value function q   (s, a) = max   q  (s, a) is the maxi-
mum action value achievable by any policy for state s and action a. q   (s, a) decomposes into the
s(cid:48),r p(s(cid:48), r|s, a)[r +    maxa(cid:48) q   (s(cid:48), a(cid:48))]. we denote an optimal
policy by      .

9

2.3.4 id145

2.3.5 temporal difference learning

when a rl problem satis   es the markov property, i.e., the future depends only on the current state
and action, but not on the past, it is formulated as a markov decision process (mdp), de   ned by
the 5-tuple (s,a,p,r,   ). when the system model is available, we use id145
methods: policy evaluation to calculate value/action value function for a policy, value iteration and
policy iteration for    nding an optimal policy. when there is no model, we resort to rl methods.
rl methods also work when the model is available. additionally, a rl environment can be a multi-
armed bandit, an mdp, a pomdp, a game, etc.
temporal difference (td) learning is central in rl. td learning is usually refer to the learning
methods for value function evaluation in sutton (1988). sarsa (sutton and barto, 2018) and q-
learning (watkins and dayan, 1992) are also regarded as temporal difference learning.
td learning (sutton, 1988) learns value function v (s) directly from experience with td error,
with id64, in a model-free, online, and fully incremental way. td learning is a prediction
problem. the update rule is v (s)     v (s) +   [r +   v (s(cid:48))     v (s)], where    is a learning rate, and
r +   v (s(cid:48))    v (s) is called td error. algorithm 1 presents the pseudo code for tabular td learning.
precisely, it is tabular td(0) learning, where    0    indicates it is based on one-step return.
id64, like the td update rule, estimates state or action value based on subsequent esti-
mates, is common in rl, like td learning, id24, and actor-critic. id64 methods are
usually faster to learn, and enable learning to be online and continual. id64 methods are
not instances of true gradient decent, since the target depends on the weights to be estimated. the
concept of semi-id119 is then introduced (sutton and barto, 2018).

input: the policy    to be evaluated
output: value function v
initialize v arbitrarily, e.g., to 0 for all states
for each episode do
initialize state s
for each step of episode, state s is not terminal do

a     action given by    for s
take action a, observe r, s(cid:48)
v (s)     v (s) +   [r +   v (s(cid:48))     v (s)]
s     s(cid:48)

end

end

algorithm 1: td learning, adapted from sutton and barto (2018)

output: action value function q
initialize q arbitrarily, e.g., to 0 for all states, set action value for terminal states as 0
for each episode do
initialize state s
for each step of episode, state s is not terminal do
a     action for s derived by q, e.g.,  -greedy
take action a, observe r, s(cid:48)
a(cid:48)     action for s(cid:48) derived by q, e.g.,  -greedy
q(s, a)     q(s, a) +   [r +   q(s(cid:48), a(cid:48))     q(s, a)]
s     s(cid:48), a     a(cid:48)

end

end

algorithm 2: sarsa, adapted from sutton and barto (2018)

10

output: action value function q
initialize q arbitrarily, e.g., to 0 for all states, set action value for terminal states as 0
for each episode do
initialize state s
for each step of episode, state s is not terminal do
a     action for s derived by q, e.g.,  -greedy
take action a, observe r, s(cid:48)
q(s, a)     q(s, a) +   [r +    maxa(cid:48) q(s(cid:48), a(cid:48))     q(s, a)]
s     s(cid:48)

end

end

algorithm 3: id24, adapted from sutton and barto (2018)

sarsa, representing state, action, reward, (next) state, (next) action, is an on-policy control method
to    nd the optimal policy, with the update rule, q(s, a)     q(s, a) +   [r +   q(s(cid:48), a(cid:48))     q(s, a)].
algorithm 2 presents the pseudo code for tabular sarsa, precisely tabular sarsa(0).
id24 is an off-policy control method to    nd the optimal policy. id24 learns action value
function, with the update rule, q(s, a)     q(s, a) +   [r +    maxa(cid:48) q(s(cid:48), a(cid:48))    q(s, a)]. id24
re   nes the policy greedily with respect to action values by the max operator. algorithm 3 presents
the pseudo code for id24, precisely, tabular q(0) learning.
td-learning, id24 and sarsa converge under certain conditions. from an optimal action
value function, we can derive an optimal policy.

2.3.6 multi-step id64

the above algorithms are referred to as td(0) and q(0), learning with one-step return. we have td
learning and id24 variants and monte-carlo approach with multi-step return in the forward
view. the eligibility trace from the backward view provides an online, incremental implementation,
resulting in td(  ) and q(  ) algorithms, where        [0, 1]. td(1) is the same as the monte carlo
approach.
eligibility trace is a short-term memory, usually lasting within an episode, assists the learning pro-
cess, by affecting the weight vector. the weight vector is a long-term memory, lasting the whole
duration of the system, determines the estimated value. eligibility trace helps with the issues of
long-delayed rewards and non-markov tasks (sutton and barto, 2018).
td(  ) uni   es one-step td prediction, td(0), with monte carlo methods, td(1), using eligibility
traces and the decay parameter   , for prediction algorithms. de asis et al. (2018) made uni   cation
for multi-step td control algorithms.

2.3.7 function approximation

we discuss the tabular cases above, where a value function or a policy is stored in a tabular form.
function approximation is a way for generalization when the state and/or action spaces are large or
continuous. function approximation aims to generalize from examples of a function to construct
an approximate of the entire function; it is usually a concept in supervised learning, studied in the
   elds of machine learning, patten recognition, and statistical curve    tting; function approximation in
id23 usually treats each backup as a training example, and encounters new issues
like nonstationarity, id64, and delayed targets (sutton and barto, 2018). linear function
approximation is a popular choice, partially due to its desirable theoretical properties, esp. before the
work of deep q-network (mnih et al., 2015). however, the integration of id23
and neural networks dated back a long time ago (sutton and barto, 2018; bertsekas and tsitsiklis,
1996; schmidhuber, 2015).
algorithm 4 presents the pseudo code for td(0) with function approximation.   v(s, w) is the ap-
proximate value function, w is the value function weight vector,      v(s, w) is the gradient of the
approximate value function with respect to the weight vector, and the weight vector is updated fol-
lowing the update rule, w     w +   [r +     v(s(cid:48), w)       v(s, w)]     v(s, w).

11

input: the policy    to be evaluated
input: a differentiable value function   v(s, w),   v(terminal,  ) = 0
output: value function   v(s, w)
initialize value function weight w arbitrarily, e.g., w = 0
for each episode do
initialize state s
for each step of episode, state s is not terminal do
a       (  |s)
take action a, observe r, s(cid:48)
w     w +   [r +     v(s(cid:48), w)       v(s, w)]     v(s, w)
s     s(cid:48)

end

end

algorithm 4: td(0) with function approximation, adapted from sutton and barto (2018)

= (cid:80)

a   (a|s)(cid:80)

tion for value function is v  (s) = (cid:80)
a   (a|s)(cid:80)
b  v  . bellman error for the function approximation case is then(cid:80)

when combining off-policy, function approximation, and id64, instability and divergence
may occur (tsitsiklis and van roy, 1997), which is called the deadly triad issue (sutton and barto,
2018). all these three elements are necessary: function approximation for scalability and gener-
alization, id64 for computational and data ef   ciency, and off-policy learning for freeing
behaviour policy from target policy. what is the root cause for the instability? learning or sampling
are not, since id145 suffers from divergence with function approximation; explo-
ration, greedi   cation, or control are not, since prediction alone can diverge; local minima or complex
non-linear function approximation are not, since linear function approximation can produce instabil-
ity (sutton, 2016). it is unclear what is the root cause for instability     each single factor mentioned
above is not     there are still many open problems in off-policy learning (sutton and barto, 2018).
table 1 presents various algorithms that tackle various issues (sutton, 2016). deep rl algorithms
like deep q-network (mnih et al., 2015) and a3c (mnih et al., 2016) are not presented here, since
they do not have theoretical guarantee, although they achieve stunning performance empirically.
before explaining table 1, we introduce some background de   nitions. recall that bellman equa-
s(cid:48),r p(s(cid:48), r|s, a)[r +   v  (s(cid:48))]. bellman operator
s(cid:48),r p(s(cid:48), r|s, a)[r +   v  (s(cid:48))]. td    x point is then v   =
is de   ned as (b  v)(s)
s(cid:48),r p(s(cid:48), r|s, a)[r +
    v  (s(cid:48), w)]       v  (s, w), the right side of bellman equation with function approximation minus the
left side. it can be written as b  vw     vw. bellman error is the expectation of the td error.
adp algorithms refer to id145 algorithms like policy evaluation, policy iteration,
and value iteration, with function approximation. least square temporal difference (lstd) (bradtke
and barto, 1996) computes td    x-point directly in batch mode. lstd is data ef   cient, yet with
squared time complexity. lspe (nedi  c and bertsekas, 2003) extended lstd. fitted-q algo-
rithms (ernst et al., 2005; riedmiller, 2005) learn action values in batch mode. residual gradi-
ent algorithms (baird, 1995) minimize bellman error. gradient-td (sutton et al., 2009a;b; mah-
mood et al., 2014) methods are true gradient algorithms, perform sgd in the projected bellman
error (pbe), converge robustly under off-policy training and non-linear function approximation.
emphatic-td (sutton et al., 2016) emphasizes some updates and de-emphasizes others by reweight-
ing, improving computational ef   ciency, yet being a semi-gradient method. see sutton and barto
(2018) for more details. du et al. (2017) proposed variance reduction techniques for policy eval-
uation to achieve fast convergence. white and white (2016) performed empirical comparisons of
linear td methods, and made suggestions about their practical use.

a   (a|s)(cid:80)

.

2.3.8 policy optimization

in contrast to value-based methods like td learning and id24, policy-based methods opti-
mize the policy   (a|s;   ) (with function approximation) directly, and update the parameters    by
gradient ascent on e[rt]. reinforce (williams, 1992) is a policy gradient method, updating   
in the direction of       log   (at|st;   )rt. usually a baseline bt(st) is subtracted from the return to
reduce the variance of gradient estimate, yet keeping its unbiasedness, to yield the gradient direction
      log   (at|st;   )(rt     bt(st)). using v (st) as the baseline bt(st), we have the advantage func-

12

td(  )

sarsa(  ) adp
(cid:88)

(cid:88)

algorithm

lstd(  )
lspe(  )

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

fitted-q

(cid:88)

(cid:88)

residual
gradient

gtd(  )
gq(  )

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

e
u
s
s
i

linear

computation

nonlinear
convergent
off-policy
convergent
model-free,

online

converges to

pbe = 0

table 1: rl issues vs. algorithms

tion a(at, st) = q(at, st)     v (st), since rt is an estimate of q(at, st). algorithm 5 presents the
pseudo code for reinforce algorithm in the episodic case.

input: policy   (a|s,   ),   v(s, w)
parameters: step sizes,    > 0,    > 0
output: policy   (a|s,   )
initialize policy parameter    and state-value weights w
for true do

generate an episode s0, a0, r1,        , st   1, at   1, rt , following   (  |  ,   )
for each step t of episode 0,        , t     1 do

gt     return from step t
       gt       v(st, w)
w     w +        w   v(st, w)
          +     t       log  (at|st,   )

end

end

algorithm 5: reinforce with baseline (episodic), adapted from sutton and barto (2018)

in actor-critic algorithms, the critic updates action-value function parameters, and the actor updates
policy parameters, in the direction suggested by the critic. algorithm 6 presents the pseudo code for
one-step actor-critic algorithm in the episodic case.

input: policy   (a|s,   ),   v(s, w)
parameters: step sizes,    > 0,    > 0
output: policy   (a|s,   )
initialize policy parameter    and state-value weights w
for true do
initialize s, the    rst state of the episode
i     1
for s is not terminal do

a       (  |s,   )
take action a, observe s(cid:48), r
       r +     v(s(cid:48), w)       v(s, w) (if s(cid:48) is terminal,   v(s(cid:48), w)
w     w +        w   v(st, w)
          +   i       log  (at|st,   )
i       i
s     s(cid:48)

.
= 0)

end

end

algorithm 6: actor-critic (episodic), adapted from sutton and barto (2018)

13

policy iteration alternates between policy evaluation and policy improvement, to generate a sequence
of improving policies. in policy evaluation, the value function of the current policy is estimated from
the outcomes of sampled trajectories. in policy improvement, the current value function is used to
generate a better policy, e.g., by selecting actions greedily with respect to the value function.

2.3.9 deep id23

we obtain deep id23 (deep rl) methods when we use deep neural networks to
approximate any of the following components of id23: value function,   v(s;   )
or   q(s, a;   ), policy   (a|s;   ), and model (state transition function and reward function). here, the
parameters    are the weights in deep neural networks. when we use    shallow    models, like linear
function, id90, tile coding and so on as the function approximator, we obtain    shallow   
rl, and the parameters    are the weight parameters in these models. note, a shallow model, e.g.,
id90, may be non-linear. the distinct difference between deep rl and    shallow    rl is what
function approximator is used. this is similar to the difference between deep learning and    shallow   
machine learning. we usually utilize stochastic id119 to update weight parameters in deep
rl. when off-policy, function approximation, in particular, non-linear function approximation, and
id64 are combined together, instability and divergence may occur (tsitsiklis and van roy,
1997). however, recent work like deep q-network (mnih et al., 2015) and alphago (silver et al.,
2016a) stabilized the learning and achieved outstanding results.

2.3.10 rl parlance

we explain some terms in rl parlance.
the prediction problem, or policy evaluation, is to compute the state or action value function for a
policy. the control problem is to    nd the optimal policy. planning constructs a value function or a
policy with a model.
on-policy methods evaluate or improve the behavioural policy, e.g., sarsa    ts the action-value
function to the current policy, i.e., sarsa evaluates the policy based on samples from the same
policy, then re   nes the policy greedily with respect to action values. in off-policy methods, an agent
learns an optimal value function/policy, maybe following an unrelated behavioural policy, e.g., q-
learning attempts to    nd action values for the optimal policy directly, not necessarily    tting to the
policy generating the data, i.e., the policy id24 obtains is usually different from the policy that
generates the samples. the notion of on-policy and off-policy can be understood as same-policy and
different-policy.
the exploration-exploitation dilemma is about the agent needs to exploit the currently best action
to maximize rewards greedily, yet it has to explore the environment to    nd better actions, when the
policy is not optimal yet, or the system is non-stationary.
in model-free methods, the agent learns with trail-and-error from experience explicitly; the model
(state transition function) is not known or learned from experience. rl methods that use models are
model-based methods.
in online mode, training algorithms are executed on data acquired in sequence. in of   ine mode, or
batch mode, models are trained on the entire data set.
with id64, an estimate of state or action value is updated from subsequent estimates.

2.3.11 brief summary

a rl problem is formulated as an mdp when the observation about the environment satis   es the
markov property. an mdp is de   ned by the 5-tuple (s,a,p,r,   ). a central concept in rl
is value function. bellman equations are cornerstone for developing rl algorithms. temporal
difference learning algorithms are fundamental for evaluating/predicting value functions. control
algorithms    nd optimal policies. id23 algorithms may be based on value func-
tion and/or policy, model-free or model-based, on-policy or off-policy, with function approximation
or not, with sample backups (td and monte carlo) or full backups (id145 and
exhaustive search), and about the depth of backups, either one-step return (td(0) and dynamic pro-
gramming) or multi-step return (td(  ), monte carlo, and exhaustive search). when combining

14

off-policy, function approximation, and id64, we face instability and divergence (tsitsiklis
and van roy, 1997), the deadly triad issue (sutton and barto, 2018). theoretical guarantee has been
established for linear function approximation, e.g., gradient-td (sutton et al., 2009a;b; mahmood
et al., 2014), emphatic-td (sutton et al., 2016) and du et al. (2017). with non-linear function ap-
proximation, in particular deep learning, algorithms like deep q-network (mnih et al., 2015) and
alphago (silver et al., 2016a; 2017) stabilized the learning and achieved stunning results, which is
the focus of this overview.

3 core elements

a rl agent executes a sequence of actions and observe states and rewards, with major components
of value function, policy and model. a rl problem may be formulated as a prediction, control or
planning problem, and solution methods may be model-free or model-based, with value function
and/or policy. exploration-exploitation is a fundamental tradeoff in rl. knowledge would be crit-
ical for rl. in this section, we discuss core rl elements: value function in section 3.1, policy in
section 3.2, reward in section 3.3, model and planning in section 3.4, exploration in section 3.5,
and knowledge in section 3.6.

3.1 value function

value function is a fundamental concept in id23, and temporal difference (td)
learning (sutton, 1988) and its extension, id24 (watkins and dayan, 1992), are classical algo-
rithms for learning state and action value functions respectively. in the following, we focus on deep
q-network (mnih et al., 2015), a recent breakthrough, and its extensions.

3.1.1 deep q-network (id25) and extensions

mnih et al. (2015) introduced deep q-network (id25) and ignited the    eld of deep rl. we present
id25 pseudo code in algorithm 7.

input: the pixels and the game score
output: q action value function (from which we obtain policy and select action)
initialize replay memory d
initialize action-value function q with random weight   
initialize target action-value function   q with weights       =   
for episode = 1 to m do

initialize sequence s1 = {x1} and preprocessed sequence   1 =   (s1)
for t = 1 to t do

(cid:26)a random action

following  -greedy policy, select at =
execute action ai in emulator and observe reward rt and image xt+1
set st+1 = st, at, xt+1 and preprocess   t+1 =   (st+1)
store transition (  t, at, rt,   t+1) in d
// experience replay
sample random minibatch of transitions (  j, aj, rj,   j+1) from d

arg maxa q(  (st), a;   ) otherwise

(cid:26)rj

rj +    maxa(cid:48)   q(  j+1, a(cid:48);      ) otherwise

set yj =
perform a id119 step on (yj     q(  j, aj;   ))2 w.r.t. the network parameter   
// periodic update of target network
every c steps reset   q = q, i.e., set       =   

if episode terminates at step j + 1

with id203  

end

end

algorithm 7: deep q-nework (id25), adapted from mnih et al. (2015)

before id25, it is well known that rl is unstable or even divergent when action value function is
approximated with a nonlinear function like neural networks. id25 made several important contri-
butions: 1) stabilize the training of action value function approximation with deep neural networks

15

(id98) using experience replay (lin, 1992) and target network; 2) designing an end-to-end rl ap-
proach, with only the pixels and the game score as inputs, so that only minimal domain knowledge
is required; 3) training a    exible network with the same algorithm, network architecture and hyper-
parameters to perform well on many different tasks, i.e., 49 atari games (bellemare et al., 2013),
and outperforming previous algorithms and performing comparably to a human professional tester.
see chapter 16 in sutton and barto (2018) for a detailed and intuitive description of deep q-
network. see deepmind   s description of id25 at https://deepmind.com/research/id25/.

double id25

van hasselt et al. (2016a) proposed double id25 (d-id25) to tackle the over-estimate problem in
id24. in standard id24, as well as in id25, the parameters are updated as follows:

  t+1 =   t +   (yq

t     q(st, at;   t))     tq(st, at;   t),

where

yq
t = rt+1 +    max

a

q(st+1, a;   t),

so that the max operator uses the same values to both select and evaluate an action. as a conse-
quence, it is more likely to select over-estimated values, and results in over-optimistic value esti-
mates. van hasselt et al. (2016a) proposed to evaluate the greedy policy according to the online
network, but to use the target network to estimate its value. this can be achieved with a minor
change to the id25 algorithm, replacing yq

t with

yd   id25

t

= rt+1 +   q(st+1, arg max

a

q(st+1, at;   t);      
t ),

where   t is the parameter for online network and      
reference, yq

t can be written as

t

is the parameter for target network. for

yq
t = rt+1 +   q(st+1, arg max

a

q(st+1, at;   t);   t).

d-id25 found better policies than id25 on atari games.

prioritized experience replay

in id25, experience transitions are uniformly sampled from the replay memory, regardless of the
signi   cance of experiences. schaul et al. (2016) proposed to prioritize experience replay, so that
important experience transitions can be replayed more frequently, to learn more ef   ciently. the
importance of experience transitions are measured by td errors. the authors designed a stochastic
prioritization based on the td errors, using importance sampling to avoid the bias in the update
distribution. the authors used prioritized experience replay in id25 and d-id25, and improved
their performance on atari games.

dueling architecture

wang et al. (2016b) proposed the dueling network architecture to estimate state value function v (s)
and associated advantage function a(s, a), and then combine them to estimate action value function
q(s, a), to converge faster than id24. in id25, a id98 layer is followed by a fully connected
(fc) layer. in dueling architecture, a id98 layer is followed by two streams of fc layers, to estimate
value function and advantage function separately; then the two streams are combined to estimate
action value function. usually we use the following to combine v (s) and a(s, a) to obtain q(s, a),

q(s, a;   ,   ,   ) = v (s;   ,   ) +(cid:0)a(s, a;   ,   )     max
q(s, a;   ,   ,   ) = v (s;   ,   ) +(cid:0)a(s, a;   ,   )     a

a(cid:48) a(s, a(cid:48);   ,   )(cid:1)
|a| a(s, a(cid:48);   ,   )(cid:1)

where    and    are parameters of the two streams of fc layers. wang et al. (2016b) proposed to
replace max operator with average as the following for better stability,

dueling architecture implemented with d-id25 and prioritized experience replay improved previous
work, id25 and d-id25 with prioritized experience replay, on atari games.

16

distributional value function

bellemare et al. (2017)

rainbow

hessel et al. (2018)

more id25 extensions

id25 has been receiving much attention. we list several extensions/improvements here.

with shallow rl.

ous q-values estimates.

mization approach, to propagate reward faster, and to improve accuracy over id25.

    anschel et al. (2017) proposed to reduce variability and instability by an average of previ-
    he et al. (2017) proposed to accelerate id25 by optimality tightening, a constrained opti-
    liang et al. (2016) attempted to understand the success of id25 and reproduced results
    o   donoghue et al. (2017) proposed policy gradient and id24 (pgq), as discussed in
    oh et al. (2015) proposed spatio-temporal video prediction conditioned on actions and
    osband et al. (2016) designed better exploration strategy to improve id25.
    hester et al. (2018) proposed to learn from demonstration with new id168s, as dis-

previous video frames with deep neural networks in atari games.

section 3.2.3.

cussed in section 4.2.

3.2 policy

a policy maps state to action, and policy optimization is to    nd an optimal mapping. as in peters
and neumann (2015), the spectrum from direct policy search to value-based rl includes: evo-
lutionary strategies, cma-es (covariance matrix adaptation evolution strategy), episodic reps
(relative id178 policy search), policy gradients, pilco (probabilistic id136 for learning con-
trol) (deisenroth and rasmussen, 2011), model-based reps, policy search by trajectory optimiza-
tion, actor critic, natural actor critic, enac (episodic natural actor critic), advantage weighted re-
gression, conservative policy iteration, lspi (least square policy iteration) (lagoudakis and parr,
2003), id24, and    tted q, as well as important extensions, contextual policy search, and hier-
archical policy search.
we discuss actor-critic (mnih et al., 2016). then we discuss policy gradient, including deterministic
policy gradient (silver et al., 2014; lillicrap et al., 2016), trust region policy optimization (schulman
et al., 2015), and, benchmark results (duan et al., 2016). next we discuss the combination of policy
gradient and off-policy rl (o   donoghue et al., 2017; nachum et al., 2017; gu et al., 2017).
see retrace algorithm (munos et al., 2016), a safe and ef   cient return-based off-policy control
algorithm, and its actor-critic extension, reactor (gruslys et al., 2017), for retrace-actor. see dis-
tributed proximal policy optimization (heess et al., 2017). mcallister and rasmussen (2017) ex-
tended pilco to pomdps.

3.2.1 actor-critic

an actor-critic algorithm learns both a policy and a state-value function, and the value function
is used for id64, i.e., updating a state from subsequent estimates, to reduce variance and
accelerate learning (sutton and barto, 2018). in the following, we focus on asynchronous advantage
actor-critic (a3c) (mnih et al., 2016). mnih et al. (2016) also discussed asynchronous one-step
sarsa, one-step id24 and n-step id24.
in a3c, parallel actors employ different exploration policies to stabilize training, so that experience
replay is not utilized. different from most deep learning algorithms, asynchronous methods can
run on a single multi-core cpu. for atari games, a3c ran much faster yet performed better than

17

or comparably with id25, gorila (nair et al., 2015), d-id25, dueling d-id25, and prioritized
d-id25. a3c also succeeded on continuous motor control problems: torcs car racing games
and mujoco physics manipulation and locomotion, and labyrinth, a navigating task in random 3d
mazes using visual inputs, in which an agent will face a new maze in each new episode, so that it
needs to learn a general strategy to explore random mazes.
global shared parameter vectors    and   v, thread-speci   c parameter vectors   (cid:48) and   (cid:48)
global shared counter t = 0, tmax
initialize step counter t     1
for t     tmax do

v

reset gradients, d       0 and d  v     0
synchronize thread-speci   c parameters   (cid:48) =    and   (cid:48)
set tstart = t, get state st
for st not terminal and t     tstart     tmax do

take at according to policy   (at|st;   (cid:48))
receive reward rt and new state st+1
t     t + 1, t     t + 1

v =   v

end

(cid:26)0

for terminal st

v (st,   (cid:48)

r =
v) otherwise
for i     {t     1, ..., tstart} do

r     ri +   r
accumulate gradients wrt   (cid:48): d       d   +      (cid:48) log   (ai|si;   (cid:48))(r     v (si;   (cid:48)
accumulate gradients wrt   (cid:48)

(r     v (si;   (cid:48)

v: d  v     d  v +      (cid:48)
end
update asynchronously    using d  , and   v using d  v

v))2

v

v))

algorithm 8: a3c, each actor-learner thread, based on mnih et al. (2016)

end

we present pseudo code for asynchronous advantage actor-critic for each actor-learner thread
in algorithm 8. a3c maintains a policy   (at|st;   ) and an estimate of the value function
v (st;   v), being updated with n-step returns in the forward view, after every tmax actions or
reaching a terminal state, similar to using minibatches. the gradient update can be seen as
i=0   irt+i +   kv (st+k;   v)    

     (cid:48) log   (at|st;   (cid:48))a(st, at;   ,   v), where a(st, at;   ,   v) = (cid:80)k   1

v (st;   v) is an estimate of the advantage function, with k upbounded by tmax.
wang et al. (2017b) proposed a stable and sample ef   cient actor-critic deep rl model using experi-
ence replay, with truncated importance sampling, stochastic dueling network (wang et al., 2016b) as
discussed in section 3.1.1, and trust region policy optimization (schulman et al., 2015) as discussed
in section 3.2.2. babaeizadeh et al. (2017) proposed a hybrid cpu/gpu implementation of a3c.

3.2.2 policy gradient

reinforce (williams, 1992; sutton et al., 2000) is a popular policy gradient method. relatively
speaking, id24 as discussed in section 3.1 is sample ef   cient, while policy gradient is stable.

deterministic policy gradient

policies are usually stochastic. however, silver et al. (2014) and lillicrap et al. (2016) proposed
deterministic policy gradient (dpg) for ef   cient estimation of policy gradients.
silver et al. (2014) introduced the deterministic policy gradient (dpg) algorithm for rl problems
with continuous action spaces. the deterministic policy gradient is the expected gradient of the
action-value function, which integrates over the state space; whereas in the stochastic case, the pol-
icy gradient integrates over both state and action spaces. consequently, the deterministic policy
gradient can be estimated more ef   ciently than the stochastic policy gradient. the authors intro-
duced an off-policy actor-critic algorithm to learn a deterministic target policy from an exploratory
behaviour policy, and to ensure unbiased policy gradient with the compatible function approxima-
tion for deterministic policy gradients. empirical results showed its superior to stochastic policy

18

gradients, in particular in high dimensional tasks, on several problems: a high-dimensional bandit;
standard benchmark rl tasks of mountain car and pendulum and 2d puddle world with low dimen-
sional action spaces; and controlling an octopus arm with a high-dimensional action space. the
experiments were conducted with tile-coding and linear function approximators.
lillicrap et al. (2016) proposed an actor-critic, model-free, deep deterministic policy gradient
(ddpg) algorithm in continuous action spaces, by extending id25 (mnih et al., 2015) and dpg (sil-
ver et al., 2014). with actor-critic as in dpg, ddpg avoids the optimization of action at every time
step to obtain a greedy policy as in id24, which will make it infeasible in complex action
spaces with large, unconstrained function approximators like deep neural networks. to make the
learning stable and robust, similar to id25, ddpq deploys experience replay and an idea similar to
target network,    soft    target, which, rather than copying the weights directly as in id25, updates the
soft target network weights   (cid:48) slowly to track the learned networks weights   :   (cid:48)           + (1       )  (cid:48),
with    (cid:28) 1. the authors adapted batch id172 to handle the issue that the different com-
ponents of the observation with different physical units. as an off-policy algorithm, ddpg learns
an actor policy from experiences from an exploration policy by adding noise sampled from a noise
process to the actor policy. more than 20 simulated physics tasks of varying dif   culty in the mu-
joco environment were solved with the same learning algorithm, network architecture and hyper-
parameters, and obtained policies with performance competitive with those found by a planning
algorithm with full access to the underlying physical model and its derivatives. ddpg can solve
problems with 20 times fewer steps of experience than id25, although it still needs a large number
of training episodes to    nd solutions, as in most model-free rl methods. it is end-to-end, with raw
pixels as input. ddpq paper also contains links to videos for illustration.
hausknecht and stone (2016) considers parameterization of action space.

trust region policy optimization

schulman et al. (2015) introduced an iterative procedure to monotonically improve policies theoreti-
cally, guaranteed by optimizing a surrogate objective function. the authors then proposed a practical
algorithm, trust region policy optimization (trpo), by making several approximations, includ-
ing, introducing a trust region constraint, de   ned by the kl divergence between the new policy and
the old policy, so that at every point in the state space, the kl divergence is bounded; approximat-
ing the trust region constraint by the average kl divergence constraint; replacing the expectations
and q value in the optimization problem by sample estimates, with two variants: in the single path
approach, individual trajectories are sampled; in the vine approach, a rollout set is constructed and
multiple actions are performed from each state in the rollout set; and, solving the constrained opti-
mization problem approximately to update the policy   s parameter vector. the authors also uni   ed
policy iteration and policy gradient with analysis, and showed that policy iteration, policy gradient,
and natural policy gradient (kakade, 2002) are special cases of trpo. in the experiments, trpo
methods performed well on simulated robotic tasks of swimming, hopping, and walking, as well as
playing atari games in an end-to-end manner directly from raw images.
wu et al. (2017) proposed scalable trpo with kronecker-factored approximation to the curvature.
https://blog.openai.com/openai-baselines-ppo/

benchmark results

duan et al. (2016) presented a benchmark for continuous control tasks, including classic tasks like
cart-pole, tasks with very large state and action spaces such as 3d humanoid locomotion and tasks
with partial observations, and tasks with hierarchical structure, implemented various algorithms,
including batch algorithms: reinforce, truncated natural policy gradient (tnpg), reward-
weighted regression (rwr), relative id178 policy search (reps), trust region policy opti-
mization (trpo), cross id178 method (cem), covariance matrix adaption evolution strategy
(cma-es); online algorithms: deep deterministic policy gradient (ddpg); and recurrent variants
of batch algorithms. the open source is available at: https://github.com/rllab/rllab.
duan et al. (2016) compared various algorithms, and showed that ddpg, trpo, and truncated nat-
ural policy gradient (tnpg) (schulman et al., 2015) are effective in training deep neural network
policies, yet better algorithms are called for hierarchical tasks.

19

islam et al. (2017)
tassa et al. (2018)

3.2.3 combining policy gradient with off-policy rl

o   donoghue et al. (2017) proposed to combine policy gradient with off-policy id24 (pgq),
to bene   t from experience replay. usually actor-critic methods are on-policy. the authors also
showed that action value    tting techniques and actor-critic methods are equivalent, and interpreted
regularized policy gradient techniques as advantage function learning algorithms. empirically, the
authors showed that pgq outperformed id25 and a3c on atari games.
nachum et al. (2017) introduced the notion of softmax temporal consistency, to generalize the hard-
max bellman consistency as in off-policy id24, and in contrast to the average consistency
as in on-policy sarsa and actor-critic. the authors established the correspondence and a mutual
compatibility property between softmax consistent action values and the optimal policy maximizing
id178 regularized expected discounted reward. the authors proposed path consistency learning,
attempting to bridge the gap between value and policy based rl, by exploiting multi-step path-wise
consistency on traces from both on and off policies.
gu et al. (2017) proposed q-prop to take advantage of the stability of policy gradients and the
sample ef   ciency of off-policy rl. schulman et al. (2017) showed the equivalence between id178-
regularized id24 and policy gradient.
gu et al. (2017)

3.3 reward

rewards provide evaluative feedbacks for a rl agent to make decisions. rewards may be sparse
so that it is challenging for learning algorithms, e.g., in computer go, a reward occurs at the end of
a game. there are unsupervised ways to harness environmental signals, see section 4.2. reward
function is a mathematical formulation for rewards. reward shaping is to modify reward function
to facilitate learning while maintaining optimal policy. reward functions may not be available for
some rl problems, which is the focus of this section.
in imitation learning, an agent learns to perform a task from expert demonstrations, with samples of
trajectories from the expert, without reinforcement signal, without additional data from the expert
while training; two main approaches for imitation learning are behavioral cloning and inverse rein-
forcement learning. behavioral cloning, or apprenticeship learning, or learning from demonstration,
is formulated as a supervised learning problem to map state-action pairs from expert trajectories
to policy, without learning the reward function (ho et al., 2016; ho and ermon, 2016). inverse
id23 (irl) is the problem of determining a reward function given observations
of optimal behaviour (ng and russell, 2000). abbeel and ng (2004) approached apprenticeship
learning via irl.
in the following, we discuss learning from demonstration (hester et al., 2018), and imitation learning
with id3 (gans) (ho and ermon, 2016; stadie et al., 2017). we will
discuss gans, a recent unsupervised learning framework, in section 4.2.3.
su et al. (2016b) proposed to train dialogue policy jointly with reward model. christiano et al. (2017)
proposed to learn reward function by human preferences from comparisons of trajectory segments.
see also had   eld-menell et al. (2016); merel et al. (2017); wang et al. (2017); van seijen et al.
(2017).
amin et al. (2017)

learning from demonstration

hester et al. (2018) proposed deep id24 from demonstrations (dqfd) to attempt to accel-
erate learning by leveraging demonstration data, using a combination of temporal difference (td),
supervised, and regularized losses. in dqfq, reward signal is not available for demonstration data;
however, it is available in id24. the supervised large margin classi   cation loss enables the
policy derived from the learned value function to imitate the demonstrator; the td loss enables the

20

validity of value function according to the bellman equation and its further use for learning with
rl; the id173 id168 on network weights and biases prevents over   tting on small
demonstration dataset. in the pre-training phase, dqfd trains only on demonstration data, to obtain
a policy imitating the demonstrator and a value function for continual rl learning. after that, dqfd
self-generates samples, and mixes them with demonstration data according to certain proportion to
obtain training data. the authors showed that, on atari games, dqfd in general has better initial
performance, more average rewards, and learns faster than id25.
in alphago (silver et al., 2016a), to be discussed in section 5.1.1, the supervised learning policy
network is learned from expert moves as learning from demonstration; the results initialize the rl
policy network. see also kim et al. (2014); p  erez-d   arpino and shah (2017). see argall et al.
(2009) for a survey of robot learning from demonstration.
ve  cer    k et al. (2017)

generative adversarial imitation learning

with irl, an agent learns a reward function    rst, then from which derives an optimal policy. many
irl algorithms have high time complexity, with a rl problem in the inner loop.
ho and ermon (2016) proposed generative adversarial imitation learning algorithm to learn poli-
cies directly from data, bypassing the intermediate irl step. generative adversarial training was
deployed to    t the discriminator, the distribution of states and actions that de   nes expert behavior,
and the generator, the policy.
generative adversarial imitation learning    nds a policy      so that a discriminator dr can not dis-
tinguish states following the expert policy   e and states following the imitator policy     , hence
forcing dr to take 0.5 in all cases and      not distinguishable from   e in the equillibrium. such a
game is formulated as:

max
    

mindr

   e     [log dr(s)]     e  e [log(1     dr(s))]

the authors represented both      and dr as deep neural networks, and found an optimal solution
by repeatedly performing gradient updates on each of them. dr can be trained with supervised
learning with a data set formed from traces from a current      and expert traces. for a    xed dr, an
optimal      is sought. hence it is a policy optimization problem, with     log dr(s) as the reward.
the authors trained      by trust region policy optimization (schulman et al., 2015).
li et al. (2017)

third person imitation learning

stadie et al. (2017) argued that previous works in imitation learning, like ho and ermon (2016) and
finn et al. (2016b), have the limitation of    rst person demonstrations, and proposed to learn from
unsupervised third person demonstration, mimicking human learning by observing other humans
achieving goals.

3.4 model and planning

a model is an agent   s representation of the environment, including the transition model and the
reward model. usually we assume the reward model is known. we discuss how to handle unknown
reward models in section 3.3. model-free rl approaches handle unknown dynamical systems,
however, they usually require large number of samples, which may be costly or prohibitive to obtain
for real physical systems. model-based rl approaches learn value function and/or policy in a data-
ef   cient way, however, they may suffer from the issue of model identi   cation so that the estimated
models may not be accurate, and the performance is limited by the estimated model. planning
constructs a value function or a policy usually with a model, so that planning is usually related to
model-based rl methods.
chebotar et al. (2017) attempted to combine the advantages of both model-free and model-based rl
approaches. the authors focused on time-varying linear-gaussian policies, and integrated a model-

21

based linear quadratic regulator (lqr) algorithm with a model-free path integral policy improve-
ment algorithm. to generalize the method for arbitrary parameterized policies such as deep neural
networks, the authors combined the proposed approach with guided policy search (gps) (levine
et al., 2016a). the proposed approach does not generate synthetic samples with estimated models to
avoid degradation from modelling errors. see recent work on model-based learning, e.g., gu et al.
(2016b); henaff et al. (2017); hester and stone (2017); oh et al. (2017); watter et al. (2015).
tamar et al. (2016) introduced value iteration networks (vin), a fully differentiable id98 plan-
ning module to approximate the value iteration algorithm, to learn to plan, e.g, policies in rl. in
contrast to conventional planning, vin is model-free, where reward and transition id203 are
part of the neural network to be learned, so that it may avoid issues with system identi   cation. vin
can be trained end-to-end with id26. vin can generalize in a diverse set of tasks: sim-
ple gridworlds, mars rover navigation, continuous control and webnav challenge for wikipedia
links navigation (nogueira and cho, 2016). one merit of value iteration network, as well as du-
eling network(wang et al., 2016b), is that they design novel deep neural networks architectures
for id23 problems. see a blog about vin at https://github.com/karpathy/paper-
notes/blob/master/vin.md.
silver et al. (2016b) proposed the predictron to integrate learning and planning into one end-to-end
training procedure with raw input in markov reward process, which can be regarded as markov
decision process without actions. see classical dyna-q (sutton, 1990).
weber et al. (2017)
andrychowicz et al. (2017)

3.5 exploration

a rl agent usually uses exploration to reduce its uncertainty about the reward function and tran-
sition probabilities of the environment. in tabular cases, this uncertainty can be quanti   ed as con-
   dence intervals or posterior of environment parameters, which are related to the state-action visit
counts. with count-based exploration, a rl agent uses visit counts to guide its behaviour to re-
duce uncertainty. however, count-based methods are not directly useful in large domains. intrinsic
motivation suggests to explore what is surprising, typically in learning process based on change in
prediction error. intrinsic motivation methods do not require markov property and tabular repre-
sentation as count-based methods require. bellemare et al. (2016) proposed pseudo-count, a density
model over the state space, to unify count-based exploration and intrinsic motivation, by introducing
information gain, to relate to con   dence intervals in count-based exploration, and to relate to learn-
ing progress in intrinsic motivation. the author established pseudo-count   s theoretical advantage
over previous intrinsic motivation methods, and validated it with atari games.
nachum et al. (2017) proposed an under-appreciated reward exploration technique to avoid the pre-
vious ineffective, undirected exploration strategies of the reward landscape, as in  -greedy and en-
tropy id173, and to promote directed exploration of the regions, in which the log-id203
of an action sequence under the current policy under-estimates the resulting reward. the under-
appreciated reward exploration strategy resulted from importance sampling from the optimal policy,
and combined a mode seeking and a mean seeking terms to tradeoff exploration and exploitation.
the authors implemented the proposed exploration strategy with minor modi   cations to rein-
force, and validated it, for the    rst time with a rl method, on several algorithmic tasks.
osband et al. (2016) proposed bootstrapped id25 to combine deep exploration with deep neural
networks to achieve ef   cient learning. houthooft et al. (2016) proposed variational information
maximizing exploration for continuous state and action spaces. fortunato et al. (2017) proposed
noisynet for ef   cient exploration by adding parametric noise added to weights of deep neural net-
works. see also azar et al. (2017); jiang et al. (2016); ostrovski et al. (2017).
tang et al. (2017)
fu et al. (2017)

22

3.6 knowledge

(this section would be an open-ended discussion.)
knowledge would be critical for further development of rl. knowledge may be incorporated into
rl in various ways, through value, reward, policy, model, exploration strategy, etc. during a per-
sonal conversation with rich sutton, he mentioned that it is still wide open how to incorporate
knowledge into rl.
human intelligence, lake et al. (2016), developmental start-up software     intuitive physics, intu-
itive psychology; learning as rapid model building     compositionality, causality; learning to learn;
thinking fast     approximate id136 in structured models, model-based and model-free reinforce-
ment learning
consciousness prior, bengio (2017)
ml with knowledge, song and roth (2017)
causality, pearl (2018), johansson et al. (2016)
interpretability, zhang and zhu (2018) surveyed visual interpretability for deep learning, dong et al.
(2017)
george et al. (2017)
yang and mitchell (2017)

4

important mechanisms

in this section, we discuss important mechanisms for the development of (deep) reinforcement learn-
ing, including attention and memory, unsupervised learning, id21, multi-agent reinforce-
ment learning, hierarchical rl, and learning to learn. we note that we do not discuss in detail some
important mechanisms, like bayesian rl (ghavamzadeh et al., 2015), pomdp (hausknecht and
stone, 2015), and semi-supervised rl (audiffren et al., 2015; finn et al., 2017; zhu and goldberg,
2009).

4.1 attention and memory

attention is a mechanism to focus on the salient parts. memory provides data storage for long time,
and attention is an approach for memory addressing.
graves et al. (2016) proposed differentiable neural computer (dnc), in which, a neural network
can read from and write to an external memory, so that dnc can solve complex, structured prob-
lems, which a neural network without read-write memory can not solve. dnc minimizes memory
allocation interference and enables long-term storage. similar to a conventional computer, in a
dnc, the neural network is the controller and the external memory is the random-access memory;
and a dnc represents and manipulates complex data structures with the memory. differently, a
dnc learns such representation and manipulation end-to-end with id119 from data in a
goal-directed manner. when trained with supervised learning, a dnc can solve synthetic question
answering problems, for reasoning and id136 in natural language; it can solve the shortest path
   nding problem between two stops in transportation networks and the relationship id136 prob-
lem in a family tree. when trained with id23, a dnc can solve a moving blocks
puzzle with changing goals speci   ed by symbol sequences. dnc outperformed normal neural net-
work like lstm or dnc   s precursor id63 (graves et al., 2014); with harder
problems, an lstm may simply fail. although these experiments are relatively small-scale, we
expect to see further improvements and applications of dnc. see deepmind   s description of dnc
at https://deepmind.com/blog/differentiable-neural-computers/.
mnih et al. (2014) applied attention to image classi   cation and id164. xu et al. (2015)
integrated attention to image captioning. we brie   y discuss application of attention in computer
vision in section 5.4. the attention mechanism is also deployed in nlp, e.g., in bahdanau et al.
(2015; 2017), and with external memory, in differentiable neural computer (graves et al., 2016) as
discussed above. most works follow a soft attention mechanism (bahdanau et al., 2015), a weighted

23

addressing scheme to all memory locations. there are endeavours for hard attention (gulcehre et al.,
2016; liang et al., 2017a; luo et al., 2016; xu et al., 2015; zaremba and sutskever, 2015), which is
the way conventional computers access memory.
see recent work on attention and/or memory, e.g., ba et al. (2014; 2016); chen et al. (2016b);
danihelka et al. (2016); duan et al. (2017); eslami et al. (2016); gregor et al. (2015); jader-
berg et al. (2015); kaiser and bengio (2016); kadlec et al. (2016); luo et al. (2016); oh
et al. (2016); oquab et al. (2015); vaswani et al. (2017); weston et al. (2015); sukhbaatar
et al. (2015); yang et al. (2015); zagoruyko and komodakis (2017); zaremba and sutskever
(2015). see http://distill.pub/2016/augmented-id56s/ and http://www.wildml.com/2016/01/attention-
and-memory-in-deep-learning-and-nlp/ for blogs about attention and memory.

4.2 unsupervised learning

unsupervised learning is a way to take advantage of the massive amount of data, and would be a crit-
ical mechanism to achieve general arti   cial intelligence. unsupervised learning is categorized into
non-probabilistic models, like sparse coding, autoencoders, id116 etc, and probabilistic (gen-
erative) models, where density functions are concerned, either explicitly or implicitly (salakhut-
dinov, 2016). among probabilistic (generative) models with explicit density functions, some are
with tractable models, like fully observable belief nets, neural autoregressive distribution estima-
tors, and pixelid56, etc; some are with non-tractable models, like botlzmann machines, variational
autoencoders, helmhotz machines, etc. for probabilistic (generative) models with implicit density
functions, we have id3, moment matching networks, etc.
in the following, we discuss horde (sutton et al., 2011), and unsupervised auxiliary learning (jader-
berg et al., 2017), two ways to take advantages of possible non-reward training signals in environ-
ments. we also discuss id3 (goodfellow et al., 2014). see also le et al.
(2012), chen et al. (2016), liu et al. (2017).
artetxe et al. (2017)

4.2.1 horde

sutton et al. (2011) proposed to represent knowledge with general value function, where policy,
termination function, reward function, and terminal reward function are parameters. the authors
then proposed horde, a scalable real-time architecture for learning in parallel general value functions
for independent sub-agents from unsupervised sensorimotor interaction, i.e., nonreward signals and
observations. horde can learn to predict the values of many sensors, and policies to maximize those
sensor values, with general value functions, and answer predictive or goal-oriented questions. horde
is off-policy, i.e., it learns in real-time while following some other behaviour policy, and learns with
gradient-based temporal difference learning methods, with constant time and memory complexity
per time step.

4.2.2 unsupervised auxiliary learning

environments may contain abundant possible training signals, which may help to expedite achieving
the main goal of maximizing the accumulative rewards, e.g., pixel changes may imply important
events, and auxiliary reward tasks may help to achieve a good representation of rewarding states.
this may be even helpful when the extrinsic rewards are rarely observed.
jaderberg et al. (2017) proposed unsupervised reinforcement and auxiliary learning (unreal)
to improve learning ef   ciency by maximizing pseudo-reward functions, besides the usual cumulative
reward, while sharing a common representation. unreal is composed of id56-lstm base agent,
pixel control, reward prediction, and value function replay. the base agent is trained on-policy with
a3c (mnih et al., 2016). experiences of observations, rewards and actions are stored in a reply
buffer, for being used by auxiliary tasks. the auxiliary policies use the base id98 and lstm,
together with a deconvolutional network, to maximize changes in pixel intensity of different regions
of the input images. the reward prediction module predicts short-term extrinsic reward in next
frame by observing the last three frames, to tackle the issue of reward sparsity. value function
replay further trains the value function. unreal improved a3c   s performance on atari games,
and performed well on 3d labyrinth game. unreal has a shared representation among signals,

24

while horde trains each value function separately with distinct weights. see deepmind   s description
of unreal at https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/.
we discuss robotics navigation with similar unsupervised auxiliary learning (mirowski et al., 2017)
in section 5.2. see also lample and chaplot (2017).

4.2.3 id3

goodfellow et al. (2014) proposed generative adversarial nets (gans) to estimate generative models
via an adversarial process by training two models simultaneously, a generative model g to capture
the data distribution, and a discriminative model d to estimate the id203 that a sample comes
from the training data but not the generative model g.
goodfellow et al. (2014) modelled g and d with multilayer id88s: g(z :   g) and d(x :   d),
where   g and   d are parameters, x are data points, and z are input noise variables. de   ne a prior on
input noise variable pz(z). g is a differentiable function and d(x) outputs a scalar as the id203
that x comes from the training data rather than pg, the generative distribution we want to learn.
d will be trained to maximize the id203 of assigning labels correctly to samples from both
training data and g. simultaneously, g will be trained to minimize such classi   cation accuracy,
log(1     d(g(z))). as a result, d and g form the two-player minimax game as follows:

min

g

max

d

ex   pdata(x)[log d(x)] + ez   pz(z)[log(1     d(g(z)))]

goodfellow et al. (2014) showed that as g and d are given enough capacity, generative adversarial
nets can recover the data generating distribution, and provided a training algorithm with backpropa-
gation by minibatch stochastic id119.
see goodfellow (2017) for ian goodfellow   s summary of his nips 2016 tutorial on gans. gans
have received much attention and many works have been appearing after the tutorial.
gans are notoriously hard to train. see arjovsky et al. (2017) for wasserstein gan (wgan) as a
stable gans model. gulrajani et al. (2017) proposed to improve stability of wgan by penalizing
the norm of the gradient of the discriminator with respect to its input, instead of clipping weights as
in arjovsky et al. (2017). mao et al. (2016) proposed least squares gans (lsgans), another sta-
ble model. berthelot et al. (2017) proposed began to improve wgan by an equilibrium enforcing
model, and set a new milestone in visual quality for image generation. bellemare et al. (2017) pro-
posed cram  er gan to satisfy three machine learning properties of id203 divergences: sum
invariance, scale sensitivity, and unbiased sample gradients. hu et al. (2017) uni   ed gans and
id5 (vaes).
we discuss imitation learning with gans in section 3.3, including generative adversarial imitation
learning, and third person imitation learning. finn et al. (2016a) established a connection between
gans, inverse rl, and energy-based models. pfau and vinyals (2016) established the connection
between gans and actor-critic algorithms. see an answer on quora, http://bit.ly/2sgtpx8, by prof
sridhar mahadevan.

4.3 id21

id21 is about transferring knowledge learned from different domains, possibly with
different feature spaces and/or different data distributions (taylor and stone, 2009; pan and yang,
2010; weiss et al., 2016). as reviewed in pan and yang (2010), id21 can be inductive,
transductive, or unsupervised; inductive id21 includes self-taught learning and multi-
task learning; and transductive id21 includes id20 and sample selection
bias/covariance shift.
bousmalis et al. (2017)
https://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html
gupta et al. (2017a) formulated the multi-skill problem for two agents to learn multiple skills, de-
   ned the common representation using which to map states and to project the execution of skills, and

25

designed an algorithm for two agents to transfer the informative feature space maximally to trans-
fer new skills, with similarity loss metric, autoencoder, and id23. the authors
validated their proposed approach with two simulated robotic manipulation tasks.
see also recent work in id21 e.g., andreas et al. (2017); dong et al. (2015); ganin et al.
(2016); kaiser et al. (2017a); kansky et al. (2017); long et al. (2015; 2016); maurer et al. (2016);
mo et al. (2016); parisotto et al. (2016); papernot et al. (2017); p  erez-d   arpino and shah (2017);
rajendran et al. (2017); whye teh et al. (2017); yosinski et al. (2014). see ruder (2017) for an
overview about id72. see nips 2015 transfer and id72: trends and
new perspectives workshop.
long et al. (2017)
killian et al. (2017)
barreto et al. (2017)
mccann et al. (2017)

4.4 multi-agent id23

multi-agent rl (marl) is the integration of multi-agent systems (shoham and leyton-brown,
2009; stone and veloso, 2000) with rl, thus it is at the intersection of game theory (leyton-brown
and shoham, 2008) and rl/ai communities. besides issues in rl like convergence and curse-of-
dimensionality, there are new issues like multiple equilibria, and even fundamental issues like what
is the question for multi-agent learning, whether convergence to an equilibrium is an appropriate
goal, etc. consequently, multi-agent learning is challenging both technically and conceptually, and
demands clear understanding of the problem to be solved, the criteria for evaluation, and coherent
research agendas (shoham et al., 2007).
multi-agent systems have many applications, e.g., as we will discuss, games in section 5.1, robotics
in section 5.2, smart grid in section 5.10, intelligent transportation systems in section 5.11, and
compute systems in section 5.12.
busoniu et al. (2008) surveyed works in multi-agent rl. there are several recent works, about new
deep marl algorithms (foerster et al., 2018; foerster et al., 2017; lowe et al., 2017; omidsha   ei
et al., 2017), new communication mechanisms in marl (foerster et al., 2016; sukhbaatar et al.,
2016), and sequential social dilemmas with marl (leibo et al., 2017).
bansal et al. (2017)
al-shedivat et al. (2017a)
ghavamzadeh et al. (2006)
foerster et al. (2017)
perolat et al. (2017)
lanctot et al. (2017)
had   eld-menell et al. (2016)
had   eld-menell et al. (2017)
mhamdi et al. (2017)
lowe et al. (2017)
hoshen (2017)

4.5 hierarchical id23

hierarchical rl is a way to learn, plan, and represent knowledge with spatio-temporal abstraction
at multiple levels. hierarchical rl is an approach for issues of sparse rewards and/or long hori-
zons (sutton et al., 1999; dietterich, 2000; barto and mahadevan, 2003).

26

vezhnevets et al. (2016) proposed strategic attentive writer (straw), a deep recurrent neural net-
work architecture, for learning high-level temporally abstracted macro-actions in an end-to-end man-
ner based on observations from the environment. macro-actions are sequences of actions commonly
occurring. straw builds a multi-step action plan, updated periodically based on observing re-
wards, and learns for how long to commit to the plan by following it without replanning. straw
learns to discover macro-actions automatically from data, in contrast to the manual approach in pre-
vious work. vezhnevets et al. (2016) validated straw on next character prediction in text, 2d
maze navigation, and atari games.
kulkarni et al. (2016) proposed hierarchical-id25 (h-id25) by organizing goal-driven intrinsically
motivated deep rl modules hierarchically to work at different time-scales. h-id25 integrates a
top level action value function and a lower level action value function; the former learns a policy
over intrinsic sub-goals, or options (sutton et al., 1999); the latter learns a policy over raw actions
to satisfy given sub-goals.
in a hard atari game, montezuma   s revenge, h-id25 outperformed
previous methods, including id25 and a3c.
florensa et al. (2017) proposed to pre-train a large span of skills using stochastic neural networks
with an information-theoretic regularizer, then on top of these skills, to train high-level policies
for downstream tasks. pre-training is based on a proxy reward signal, which is a form of intrinsic
motivation to explore agent   s own capabilities; its design requires minimal domain knowledge about
the downstream tasks. their method combined hierarchical methods with intrinsic motivation, and
the pre-training follows an unsupervised way.
tessler et al. (2017) proposed a hierarchical deep rl network architecture for lifelong learning.
reusable skills, or sub-goals, are learned to transfer knowledge to new tasks. the authors tested
their approach on the game of minecraft.
see also bacon et al. (2017), kompella et al. (2017), machado et al. (2017), peng et al. (2017a),
schaul et al. (2015), sharma et al. (2017), vezhnevets et al. (2017), yao et al. (2014). see a survey
on hierarchical rl (barto and mahadevan, 2003).
harutyunyan et al. (2018)

4.6 learning to learn

learning to learn, also know as meta-learning, is about learning to adapt rapidly to new tasks. it
is related to id21, id72, representation learning, and one/few/zero-shot
learning. we can also see hyper-parameter learning and neural architecture design as learning to
learn. it is a core ingredient to achieve strong ai (lake et al., 2016).
hypermarameter tuning, e.g., jaderberg et al. (2017)
sutton (1992)

4.6.1 learning to learn/optimize

li and malik (2017) proposed to automate unconstrained continuous optimization algorithms with
guided policy search (levine et al., 2016a) by representing a particular optimization algorithm as a
policy, and convergence rate as reward. see also andrychowicz et al. (2016).
duan et al. (2016) and wang et al. (2016) proposed to learn a    exible id56 model to handle a family
of rl tasks, to improve sample ef   ciency, learn new tasks in a few samples, and bene   t from prior
knowledge.
combinatorial optimization, e.g., vinyals et al. (2015), bello et al. (2016), dai et al. (2017)
xu et al. (2017)
smith et al. (2017)
li and malik (2017)

27

4.6.2 zero/one/few-shot learning

lake et al. (2015) proposed an one-shot concept learning model, for handwritten characters in par-
ticular, with probabilistic program induction. koch et al. (2015) proposed siamese neural networks
with metric learning for one-shot image recognition. vinyals et al. (2016) designed matching net-
works for one-shot classi   cation. duan et al. (2017) proposed a model for one-shot imitation learn-
ing with attention for robotics. ravi and larochelle (2017) proposed a meta-learning model for few
shot learning. johnson et al. (2016) presented zero-shot translation for google   s multilingual neural
machine translation system. kaiser et al. (2017b) designed a large scale memory module for life-
long id62 to remember rare events. kansky et al. (2017) proposed schema networks
for zero-shot transfer with a generative causal model of intuitive physics. snell et al. (2017) pro-
posed prototypical networks for few/zero-shot classi   cation by learning a metric space to compute
distances to prototype representations of each class.

4.6.3 neural architecture design

neural networks architecture design is a notorious, nontrivial engineering issue. neural architecture
search provides a promising avenue to explore.
zoph and le (2017) proposed the neural architecture search to generate neural networks architec-
tures with an id56 trained by rl, in particular, reinforce, searching from scratch in variable-
length architecture space, to maximize the expected accuracy of the generated architectures on a
validation set. in the rl formulation, a controller generates hyperparameters as a sequence of to-
kens, which are actions chosen from hyperparameters spaces; each gradient update to the policy
parameters corresponds to training one generated network to convergence; an accuracy on a valida-
tion set is the reward signal. the neural architecture search can generate convolutional layers, with
skip connections or branching layers, and recurrent cell architecture. the authors designed a param-
eter server approach to speed up training. comparing with state-of-the-art methods, the proposed
approach achieved competitive results for an image classi   cation task with cifar-10 dataset; and
better results for a id38 task with id32.
zoph et al. (2017) proposed to transfer the architectural building block learned with the neural archi-
tecture search (zoph and le, 2017) on small dataset to large dataset for scalable image recognition.
baker et al. (2017) proposed a meta-learning approach, using id24 with  -greedy exploration
and experience replay, to generate id98 architectures automatically for a given learning task. zhong
et al. (2017) proposed to construct network blocks to reduce the search space of network design,
trained by id24. see also bello et al. (2017).
there are recent works exploring new neural architectures. kaiser et al. (2017a) proposed to train a
single model, multimodel, which is composed of convolutional layers, an attention mechanism, and
sparsely-gated layers, to learn multiple tasks from various domains, including image classi   cation,
image captioning and machine translation. vaswani et al. (2017) proposed a new achichitecture for
translation that replaces id98 and id56 with attention and positional encoding. wang et al. (2016b)
proposed the dueling network architecture to estimate state value function and associated advantage
function, to combine them to estimate action value function for faster convergence. tamar et al.
(2016) introduced value iteration networks, a fully differentiable id98 planning module to approx-
imate the value iteration algorithm, to learn to plan. silver et al. (2016b) proposed the predictron
to integrate learning and planning into one end-to-end training procedure with raw input in markov
reward process.
liu et al. (2017)
liu et al. (2017)

5 applications

id23 has a wide range of applications. we discuss games in section 5.1 and
robotics in section 5.2, two classical rl application areas. games will still be important testbeds
for rl/ai. robotics will be critical in the era of ai. next we discuss natural language processing
in section 5.3, which enjoys wide and deep applications of rl recently. id161 follows
in section 5.4, in which, there are efforts for integration of vision and language. combinatorial

28

figure 2: deep rl applications

optimization including neural architecture design in section ?? is an exciting application of rl. in
section 5.5, we discuss business management, like ads, recommendation, customer management,
and marketing. we discuss    nance in section 5.6. business and    nance have natural problems for
rl. we discuss healthcare in section 5.7, which receives much attention recently, esp. after the
success of deep learning. we discuss industry 4.0 in section 5.9. many countries have made plans
to integrate ai with manufacturing. we discuss smart grid in section 5.10, intelligent transportation
systems in section 5.11, and computer systems in section 5.12. there are optimization and control
problems in these areas, and many of them are concerned with networking and graphs. these appli-
cation areas may overlap with each other, e.g., a robot may need skills for many of the application
areas. we present deep rl applications brie   y in figure 2.
rl is usually for sequential decision making problems. however, some problems, seemingly non-
sequential on surface, like machine translation and neural network architecture design, have been
approached by rl. rl applications abound; and creativity would be the boundary.
id23 is widely used in operations research (powell, 2011), e.g., supply chain,
inventory management, resource management, etc; we do not list it as an application area     it is
implicitly a component in application areas like intelligent transportation system and industry 4.0.
we do not list smart city, an important application area of ai, as it includes several application
areas here: healthcare, intelligent transportation system, smart grid, etc. we do not discuss some
interesting applications, like music generation (briot et al., 2017; jaques et al., 2017), and retrosyn-
thesis (segler et al., 2017). see previous work on lists of rl applications at: http://bit.ly/2pdes1q,
and http://bit.ly/2rjsmaz. we may only touch the surface of some application areas. it is desirable to
do a deeper analysis of all application areas listed in the following, which we leave as a future work.

29

5.1 games

games provide excellent testbeds for rl/ai algorithms. we discuss deep q-network (id25) in
section 3.1.1 and its extensions, all of which experimented with atari games. we discuss mnih
et al. (2016) in section 3.2.1, jaderberg et al. (2017) in section 4.2, and mirowski et al. (2017)
in section 5.2, and they used labyrinth as the testbed. see yannakakis and togelius (2018) for a
book on arti   cial intelligence and games. we discuss multi-agent rl in section 4.4, which is at the
intersection of game theory and rl/ai.
backgammon and computer go are perfect information board games.
in section 5.1.1, we dis-
cuss brie   y backgammon, and focus on computer go, in particular, alphago. variants of card
games, including majiang/mahjong, are imperfect information board games, which we discuss in
section 5.1.2, and focus on texas hold   em poker. in video games, information may be perfect or
imperfect, and game theory may be deployed or not. we discuss video games in section 5.1.3. we
will see more achievements in imperfect information games and video games, and their applications.

5.1.1 perfect information board games

board games like backgammon, go, chess, checker and othello, are classical testbeds for rl/ai al-
gorithms. in such games, players reveal prefect information. tesauro (1994) approached backgam-
mon by using neural networks to approximate value function learned with td learning, and achieved
human level performance. we focus on computer go, in particular, alphago (silver et al., 2016a;
2017), for its signi   cance.

computer go

the challenge of solving computer go comes from not only the gigantic search space of size 250150,
an astronomical number, but also the hardness of position evaluation (m  uller, 2002), which was
successfully used in solving many other games, like backgammon and chess.
alphago (silver et al., 2016a), a computer go program, won the human european go champion, 5
games to 0, in october 2015, and became the    rst computer go program to won a human profes-
sional go player without handicaps on a full-sized 19    19 board. soon after that in march 2016,
alphago defeated lee sedol, an 18-time world champion go player, 4 games to 1, making headline
news worldwide. this set a landmark in ai. alphago defeated ke jie 3:0 in may 2017. alphago
zero (silver et al., 2017) further improved previous versions by learning a superhuman computer
go program without human knowledge.

alphago: training pipeline and mcts

we discuss brie   y how alphago works based on silver et al. (2016a) and sutton and barto (2018).
see sutton and barto (2018) for a detailed and intuitive description of alphago. see deepmind   s
description of alphago at goo.gl/lzoq1d.
alphago was built with techniques of deep convolutional neural networks, supervised learning,
id23, and id169 (mcts) (browne et al., 2012; gelly and silver,
2007; gelly et al., 2012). alphago is composed of two phases: neural network training pipeline and
mcts. the training pipeline phase includes training a supervised learning (sl) policy network from
expert moves, a fast rollout policy, a rl policy network, and a rl value network.
the sl policy network has convolutional layers, relu nonlinearities, and an output softmax layer
representing id203 distribution over legal moves. the inputs to the id98 are 19    19    48
image stacks, where 19 is the dimension of a go board and 48 is the number of features. state-
action pairs are sampled from expert moves to train the network with stochastic gradient ascent to
maximize the likelihood of the move selected in a given state. the fast rollout policy uses a linear
softmax with small pattern features.
the rl policy network improves sl policy network, with the same network architecture, and the
weights of sl policy network as initial weights, and policy gradient for training. the reward function
is +1 for winning and -1 for losing in the terminal states, and 0 otherwise. games are played between
the current policy network and a random, previous iteration of the policy network, to stabilize the

30

learning and to avoid over   tting. weights are updated by stochastic gradient ascent to maximize the
expected outcome.
the rl value network still has the same network architecture as sl policy network, except the out-
put is a single scalar predicting the value of a position. the value network is learned in a monte
carlo policy evaluation approach. to tackle the over   tting problem caused by strongly correlated
successive positions in games, data are generated by self-play between the rl policy network and
itself until game termination. the weights are trained by regression on state-outcome pairs, us-
ing stochastic id119 to minimize the mean squared error between the prediction and the
corresponding outcome.
in mcts phase, alphago selects moves by lookahead search. it builds a partial game tree starting
from the current state, in the following stages: 1) select a promising node to explore further, 2)
expand a leaf node guided by the sl policy network and collected statistics, 3) evaluate a leaf node
with a mixture of the rl value network and the rollout policy, 4) backup evaluations to update the
action values. a move is then selected.

alphago zero

alphago zero can be understood as an approximation policy iteration, incorporating mcts inside
the training loop to perform both policy improvement and policy evaluation. mcts may be regarded
as a policy improvement operator. it outputs move probabilities stronger than raw probabilities of
the neural network. self-play with search may be regarded as a policy evaluation operator. it uses
mcts to select moves, and game winners as samples of value function. then the policy iteration
procedure updates the neural network   s weights to match the move probabilities and value more
closely with the improved search probabilities and self-play winner, and conduct self-play with
updated neural network weights in the next iteration to make the search stronger.
the features of alphago zero (silver et al., 2017), comparing with alphago (silver et al., 2016a),
are: 1) it learns from random play, with self-play id23, without human data or
supervision; 2) it uses black and white stones from the board as input, without any manual feature
engineering; 3) it uses a single neural network to represent both policy and value, rather than separate
policy network and value network; and 4) it utilizes the neural network for position evaluation and
move sampling for mcts, and it does not perform monte carlo rollouts. alphago zero deploys
several recent achievements in neural networks: residual convolutional neural networks (resnets),
batch id172, and recti   er nonlinearities.
alphago zero has three main components in its self-play training pipeline executed in parallel asyn-
chronously: 1) optimize neural network weights from recent self-play data continually; 2) evaluate
players continually; 3) use the strongest player to generate new self-play data.
when alphago zero playing a game against an opponent, mcts searches from the current state,
with the trained neural network weights, to generate move probabilities, and then selects a move.
we present a brief, conceptual pseudo code in algorithm 9 for training in alphago zero, conducive
for easier understanding. refer to the original paper (silver et al., 2017) for details.
silver et al. (2017)

discussions

alphago zero is a id23 algorithm. it is neither supervised learning nor unsu-
pervised learning. the game score is a reward signal, not a supervision label. optimizing the loss
function l is supervised learning. however, it performs policy evaluation and policy improvement,
as one iteration in policy iteration.
alphago zero is not only a heuristic search algorithm. alphago zero is a policy iteration proce-
dure, in which, heuristic search, in particular, mcts, plays a critical role, but within the scheme of
id23 policy iteration, as illustrated in the pseudo code in algorithm 9. mcts can
be viewed as a policy improvement operator.

31

input: the raw board representation of the position, its history, and the colour to play as 19    19
images; game rules; a game scoring function; invariance of game rules under rotation and
re   ection, and invariance to colour transposition except for komi
output: policy (move probabilities) p, value v

initialize neural network weights   0 randomly
//alphago zero follows a policy iteration procedure
for each iteration i do

// termination conditions:
// 1. both players pass
// 2. the search value drops below a resignation threshold
// 3. the game exceeds a maximum length

initialize s0
for each step t, until termination at step t do

// mcts can be viewed as a policy improvement operator
// search algorithm: asynchronous policy and value mcts algorithm (apv-mcts)
// execute an mcts search   t =     i   1 (st) with previous neural network f  i   1
// each edge (s, a) in the search tree stores a prior id203 p (s, a), a visit count n (s, a),
and an action value q(s, a)
while computational resource remains do

select: each simulation traverses the tree by selecting the edge with maximum upper
con   dence bound q(s, a) + u (s, a), where u (s, a)     p (s, a)/(1 + n (s, a))
expand and evaluate: the leaf node is expanded and the associated position s is
evaluated by the neural network, (p (s,  ), v (s)) = f  i(s); the vector of p values are
stored in the outgoing edges from s
backup: each edge (s, a) traversed in the simulation is updated to increment its visit
count n (s, a), and to update its action value to the mean evaluation over these
a simulation eventually reached s(cid:48) after taking move a from position s

simulations, q(s, a) = 1/n (s, a)(cid:80)

s(cid:48)|s,a   s(cid:48) v (s(cid:48)), where s(cid:48)|s, a     s(cid:48) indicates that

end
// self-play with search can be viewed as a policy evaluation operator: select each move
with the improved mcts-based policy, uses the game winner as a sample of the value
play: once the search is complete, search probabilities        n 1/   are returned, where n is
the visit count of each move from root and    is a parameter controlling temperature; play
a move by sampling the search probabilities   t, transition to next state st+1

end
score the game to give a    nal reward rt     {   1, +1}
for each step t in the last game do

zt       rt , the game winner from the perspective of the current player
store data as (st,   t, zt)

end
sample data (s,   , z) uniformly among all time-steps of the last iteration(s) of self-play

//train neural network weights   i
//optimizing id168 l performs both policy evaluation, via (z     v)2, and policy
improvement, via      t log p, in a single step
adjust the neural network (p, v) = f  i(s):
to minimize the error between the predicted value v and the self-play winner z, and
to maximize similarity of neural network move probabilities p to search probabilities   
speci   cally, adjust the parameters    by id119 on a id168
(p, v) = f  i(s) and l = (z     v)2       t log p + c(cid:107)  i(cid:107)2
l sums over the mean-squared error and cross-id178 losses, respectively
c is a parameter controlling the level of l2 weight id173 to prevent over   tting

evaluate the checkpoint every 1000 training steps to decide if replacing the current best player
(neural network weights) for generating next batch of self-play games

end

algorithm 9: alphago zero training pseudo code, based on silver et al. (2017)

32

alphago attains a superhuman level. it may con   rm that professionals have developed effective
strategies. however, it does not need to mimic professional plays. thus it does not need to predict
their moves correctly.
the inputs to alphago zero include the raw board representation of the position, its history, and the
colour to play as 19    19 images; game rules; a game scoring function; invariance of game rules
under rotation and re   ection, and invariance to colour transposition except for komi. an additional
and critical input is solid research and development experiences.
alphago zero utilized 64 gpu workers (each maybe with multiple gpus) and 19 cpu parameter
servers (each with multiple cpus) for training, around 2000 tpus for data generation, and 4 tpus
for game playing. the computation cost is too formidable for replicating alphago zero.
alphago requires huge amount of data for training, so it is still a big data issue. however, the data
can be generated by self play, with a perfect model or precise game rules.
due to the perfect model or precise game rules for computer go, alphago algorithms have their
limitations. for example, in healthcare, robotics and self driving problems, it is usually hard to
collect a large amount of data, and it is hard or impossible to have a close enough or even perfect
model. as such, it is nontrivial to directly apply alphago algorithms to such applications.
on the other hand, alphago algorithms, especially, the underlying techniques, namely, deep learn-
ing, id23, and id169, have many applications. silver et al.
(2016a) and silver et al. (2017) recommended the following applications: general game-playing
(in particular, video games), classical planning, partially observed planning, scheduling, constraint
satisfaction, robotics, industrial control, and online id126s. alphago zero blog
mentioned the following structured problems: protein folding, reducing energy consumption, and
searching for revolutionary new materials.2
alphago has made tremendous progress, and set a landmark in ai. however, we are still far away
from attaining arti   cial general intelligence (agi).
it is interesting to see how strong a raw deep neural network in alphago can become, and how soon
a very strong computer go program would be available on a mobile phone.

5.1.2

imperfect information board games

imperfect information games, or game theory in general, have many applications, e.g., security and
medical decision support. it is interesting to see more progress of deep rl in such applications, and
the full version of texas hold   em.
heinrich and silver (2016) proposed neural fictitious self-play (nfsp) to combine    ctitious self-
play with deep rl to learn approximate nash equilibria for games of imperfect information in a
scalable end-to-end approach without prior domain knowledge. nfsp was evaluated on two-player
zero-sum games. in leduc poker, nfsp approached a nash equilibrium, while common rl methods
diverged. in limit texas hold   em, a real-world scale imperfect-information game, nfsp performed
similarly to state-of-the-art, superhuman algorithms which are based on signi   cant domain expertise.
heads-up limit hold   em poker was essentially solved (bowling et al., 2015) with counterfactual
regret minimization (cfr), which is an iterative method to approximate a nash equilibrium of an
extensive-form game with repeated self-play between two regret-minimizing algorithms.

2andrej karpathy posted a blog titled    alphago, in context   , after alphago defeated ke jie in may 2017.
he characterized properties of computer go as: fully deterministic, fully observable, discrete action space,
accessible perfect simulator, relatively short episode/game, clear and fast evaluation conducive for many trail-
and-errors, and huge datasets of human play games, to illustrate the narrowness of alphago. it is true that
computer go has limitations in the problem setting and thus potential applications, and is far from arti   cial
general intelligence. however, we see the success of alphago as the triumph of ai, in particular, alphago   s
underlying techniques, i.e., learning from demonstration (as supervised learning), deep learning, reinforcement
learning, and id169; these techniques are present in many recent achievements in ai. as
a whole technique, alphago will probably shed lights on classical ai areas, like planning, scheduling, and
id124 (silver et al., 2016a), and new areas for ai, like retrosynthesis (segler et al., 2017).
reportedly, the success of alphago   s conquering titanic search space inspired quantum physicists to solve the
quantum many-body problem (carleo and troyer, 2017).

33

deepstack

recently, signi   cant progress has been made for heads-up no-limit hold   em poker (morav  c    k
et al., 2017), the deepstack computer program defeated professional poker players for the    rst
time. deepstack utilized the recursive reasoning of cfr to handle information asymmetry, focusing
computation on speci   c situations arising when making decisions and use of value functions trained
automatically, with little domain knowledge or human expert games, without abstraction and of   ine
computation of complete strategies as before.

5.1.3 video games

video games would be great testbeds for arti   cial general intelligence.
wu and tian (2017) deployed a3c with id98 to train an agent in a partially observable 3d envi-
ronment, doom, from recent four raw frames and game variables, to predict next action and value
function, following the curriculum learning (bengio et al., 2009) approach of starting with simple
tasks and gradually transition to harder ones. it is nontrivial to apply a3c to such 3d games directly,
partly due to sparse and long term reward. the authors won the champion in track 1 of vizdoom
competition by a large margin, and plan the following future work: a map from an unknown envi-
ronment, localization, a global plan to act, and visualization of the reasoning process.
dosovitskiy and koltun (2017) approached the problem of sensorimotor control in immersive en-
vironments with supervised learning, and won the full deathmatch track of the visual doom ai
competition. we list it here since it is usually a rl problem, yet it was solved with supervised
learning. lample and chaplot (2017) also discussed how to tackle doom.
peng et al. (2017b) proposed a multiagent actor-critic framework, with a bidirectionally-coordinated
network to form coordination among multiple agents in a team, deploying the concept of dynamic
grouping and parameter sharing for better scalability. the authors used starcraft as the testbed.
without human demonstration or labelled data as supervision, the proposed approach learned strate-
gies for coordination similar to the level of experienced human players, like move without collision,
hit and run, cover attack, and focus    re without overkill. usunier et al. (2017); justesen and risi
(2017) also studied starcraft.
oh et al. (2016) and tessler et al. (2017) studied minecraft, chen and yi (2017); firoiu et al. (2017)
studied super smash bros, and kansky et al. (2017) proposed schema networks and empirically
studied variants of breakout in atari games.
see justesen et al. (2017) for a survey about applying deep (reinforcement) learning to video games.
see onta  n  on et al. (2013) for a survey about starcraft. check aiide and cig starcraft ai compe-
titions, and its history at https://www.cs.mun.ca/  dchurchill/starcraftaicomp/history.shtml. see lin
et al. (2017) for starcraft dataset.

5.2 robotics

robotics is a classical area for id23. see kober et al. (2013) for a survey of rl in
robotics, deisenroth et al. (2013) for a survey on policy search for robotics, and argall et al. (2009)
for a survey of robot learning from demonstration. see the journal science robotics. it is interesting
to note that from nips 2016 invited talk, boston dynamics robots did not use machine learning.
in the following, we discuss guided policy search (levine et al., 2016a) and learn to navi-
gate (mirowski et al., 2017). see more recent robotics papers, e.g., chebotar et al. (2016; 2017);
duan et al. (2017); finn and levine (2016); gu et al. (2016a); lee et al. (2017); levine et al.
(2016b); mahler et al. (2017); p  erez-d   arpino and shah (2017); popov et al. (2017); yahya et al.
(2016); zhu et al. (2017b).
we recommend pieter abbeel   s nips 2017 keynote speech, deep learning for robotics, slides at,
https://www.dropbox.com/s/fdw7q8mx3x4wr0c/

34

5.2.1 guided policy search

levine et al. (2016a) proposed to train the perception and control systems jointly end-to-end, to map
raw image observations directly to torques at the robot   s motors. the authors introduced guided
policy search (gps) to train policies represented as id98, by transforming policy search into su-
pervised learning to achieve data ef   ciency, with training data provided by a trajectory-centric rl
method operating under unknown dynamics. gps alternates between trajectory-centric rl and su-
pervised learning, to obtain the training data coming from the policy   s own state distribution, to
address the issue that supervised learning usually does not achieve good, long-horizon performance.
gps utilizes pre-training to reduce the amount of experience data to train visuomotor policies. good
performance was achieved on a range of real-world manipulation tasks requiring localization, visual
tracking, and handling complex contact dynamics, and simulated comparisons with previous policy
search methods. as the authors mentioned,    this is the    rst method that can train deep visuomotor
policies for complex, high-dimensional manipulation skills with direct torque control   .

5.2.2 learn to navigate

mirowski et al. (2017) obtained the navigation ability by solving a rl problem maximizing cumu-
lative reward and jointly considering un/self-supervised tasks to improve data ef   ciency and task
performance. the authors addressed the sparse reward issues by augmenting the loss with two
auxiliary tasks, 1) unsupervised reconstruction of a low-dimensional depth map for representation
learning to aid obstacle avoidance and short-term trajectory planning; 2) self-supervised loop clo-
sure classi   cation task within a local trajectory. the authors incorporated a stacked lstm to use
memory at different time scales for dynamic elements in the environments. the proposed agent
learn to navigate in complex 3d mazes end-to-end from raw sensory input, and performed similarly
to human level, even when start/goal locations change frequently.
in this approach, navigation is a by-product of the goal-directed rl optimization problem, in con-
trast to conventional approaches such as simultaneous localisation and mapping (slam), where
explicit position id136 and mapping are used for navigation. this may have the chance to replace
the popular slam, which usually requires manual processing.

5.3 natural language processing

in the following we talk about natural language processing (nlp), dialogue systems in section 5.3.1,
machine translation in section 5.3.2, and text generation in section 5.3.3. there are many interesting
issues in nlp, and we list some in the following.

(2016), xiong et al. (2017a), and wang et al. (2017a), choi et al. (2017)

    language tree-structure learning, e.g., socher et al. (2011; 2013); yogatama et al. (2017)
    id29, e.g., liang et al. (2017b)
    id53, e.g., celikyilmaz et al. (2017), shen et al. (2017), trischler et al.
    summarization, e.g., paulus et al. (2017); zhang and lapata (2017)
    id31 (liu, 2012; zhang et al., 2018), e.g., radford et al. (2017)
    information retrieval (manning et al., 2008), e.g., zhang et al. (2016), and mitra and
    information extraction, e.g., narasimhan et al. (2016)
    automatic query reformulation, e.g., nogueira and cho (2017)
    language to executable program, e.g., guu et al. (2017)
    id13 reasoning, e.g., xiong et al. (2017c)
    text games, e.g., wang et al. (2016a), he et al. (2016b), and narasimhan et al. (2015)

craswell (2017)

deep learning has been permeating into many subareas in nlp, and helping make signi   cant
progress. the above is a partial list. it appears that nlp is still a    eld, more about synergy than
competition, for deep learning vs. non-deep learning algorithms, and for approaches based on no
domain knowledge (end-to-end) vs linguistics knowledge. some non-deep learning algorithms are

35

effective and perform well, e.g., id97 (mikolov et al., 2013; mikolov et al., 2017) and fast-
text (joulin et al., 2017), and many works that study syntax and semantics of languages, see a
recent example in id14 (he et al., 2017). some deep learning approaches to nlp
problems incorporate explicitly or implicitly linguistics knowledge, e.g., socher et al. (2011; 2013);
yogatama et al. (2017). see an article by christopher d. manning, titled    last words: computa-
tional linguistics and deep learning, a look at the importance of natural language processing   ,
at http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning.
melis et al. (2017)

5.3.1 dialogue systems

in dialogue systems, conversational agents, or chatbots, human and computer interacts with natu-
ral language. we intentionally remove    spoken    before    dialogue systems    to accommodate both
spoken and written language user interface (ui). jurafsky and martin (2017) categorize dialogue
systems as task-oriented dialog agents and chatbots; the former are set up to have short conversa-
tions to help complete particular tasks; the latter are set up to mimic human-human interactions
with extended conversations, sometimes with entertainment value. as in deng (2017), there are
four categories: social chatbots, infobots (interactive id53), task completion bots
(task-oriented or goal-oriented) and personal assistant bots. we have seen generation one dialogue
systems: symbolic rule/template based, and generation two: data driven with (shallow) learning. we
are now experiencing generation three: data driven with deep learning, and id23
usually play an important role. a dialogue system usually include the following modules: (spoken)
language understanding, dialogue manager (dialogue state tracker and dialogue policy learning),
and a id86 (young et al., 2013). in task-oriented systems, there is usually a
knowledge base to query. a deep learning approach, as usual, attempts to make the learning of the
system parameters end-to-end. see deng (2017) for more details. see a survey paper on applying
machine learning to id103 (deng and li, 2013).
li et al. (2017b) presented an end-to-end task-completion neural dialogue system with parameters
learned by supervised and id23. the proposed framework includes a user sim-
ulator (li et al., 2016d) and a neural dialogue system. the user simulator consists of user agenda
modelling and id86. the neural dialogue system is composed of language
understanding and dialogue management (dialogue state tracking and policy learning). the authors
deployed rl to train dialogue management end-to-end, representing the dialogue policy as a deep
q-network (mnih et al., 2015), with the tricks of a target network and a customized experience re-
play, and using a rule-based agent to warm-start the system with supervised learning. the source
code is available at http://github.com/miulab/tc-bot.
dhingra et al. (2017) proposed kb-infobot, a goal-oriented dialogue system for multi-turn infor-
mation access. kb-infobot is trained end-to-end using rl from user feedback with differentiable
operations, including those for accessing external knowledge database (kb). in previous work, e.g.,
li et al. (2017b) and wen et al. (2017), a dialogue system accesses real world knowledge from
kb by symbolic, sql-like operations, which is non-differentiable and disables the dialogue system
from fully end-to-end trainable. kb-infobot achieved the differentiability by inducing a soft pos-
terior distribution over the kb entries to indicate which ones the user is interested in. the authors
designed a modi   ed version of the episodic reinforce algorithm to explore and learn both the
policy to select dialogue acts and the posterior over the kb entries for correct retrievals.the authors
deployed imitation learning from rule-based belief trackers and policy to warm up the system.
su et al. (2016b) proposed an on-line learning framework to train the dialogue policy jointly with
the reward model via active learning with a gaussian process model, to tackle the issue that it is
unreliable and costly to use explicit user feedback as the reward signal. the authors showed em-
pirically that the proposed framework reduced manual data annotations signi   cantly and mitigated
noisy user feedback in dialogue policy learning.
li et al. (2016c) proposed to use deep rl to generate dialogues to model future reward for better
informativity, coherence, and ease of answering, to attempt to address the issues in the sequence
to sequence models based on sutskever et al. (2014): the myopia and misalignment of maximizing
the id203 of generating a response given the previous dialogue turn, and the in   nite loop of
repetitive responses. the authors designed a reward function to re   ect the above desirable properties,

36

and deployed policy gradient to optimize the long term reward. it would be interesting to investigate
the reward model with the approach in su et al. (2016b) or with inverse rl and imitation learning
as discussed in section 3.3, although su et al. (2016b) mentioned that such methods are costly, and
humans may not act optimally.
some recent papers follow: asri et al. (2016), bordes et al. (2017), chen et al. (2016c), eric and
manning (2017), fatemi et al. (2016), kandasamy et al. (2017), lewis et al. (2017), li et al. (2016a),
li et al. (2017a), li et al. (2017b), lipton et al. (2016), mesnil et al. (2015), mo et al. (2016), peng
et al. (2017a), saon et al. (2016), serban et al. (2017), shah et al. (2016), she and chai (2017),
su et al. (2016a), weiss et al. (2017), wen et al. (2015a), wen et al. (2017), williams and zweig
(2016), williams et al. (2017), xiong et al. (2017b), xiong et al. (2017), yang et al. (2016), zhang
et al. (2017a), zhang et al. (2017c), zhao and eskenazi (2016), zhou et al. (2017). see serban et al.
(2015) for a survey of corpora for building dialogue systems.
see nips 2016 workshop on end-to-end learning for speech and audio processing, and nips
2015 workshop on machine learning for spoken language understanding and interactions.

5.3.2 machine translation

id4 (kalchbrenner and blunsom, 2013; cho et al., 2014; sutskever et al.,
2014; bahdanau et al., 2015) utilizes end-to-end deep learning for machine translation, and becomes
dominant, against the traditional id151 techniques. the neural machine
translation approach usually    rst encodes a variable-length source sentence, and then decodes it to
a variable-length target sentence. cho et al. (2014) and sutskever et al. (2014) used two id56s to
encode a sentence to a    x-length vector and then decode the vector into a target sentence. bahdanau
et al. (2015) introduced the soft-attention technique to learn to jointly align and translate.
he et al. (2016a) proposed dual learning mechanism to tackle the data hunger issue in machine
translation, inspired by the observation that the information feedback between the primal, translation
from language a to language b, and the dual, translation from b to a, can help improve both
translation models, with a policy gradient method, using the language model likelihood as the reward
signal. experiments showed that, with only 10% bilingual data for warm start and monolingual
data, the dual learning approach performed comparably with previous id4
methods with full bilingual data in english to french tasks. the dual learning mechanism may
have extensions to many tasks, if the task has a dual form, e.g., id103 and text to
speech, image caption and image generation, id53 and question generation, search
and keyword extraction, etc.
see wu et al. (2016); johnson et al. (2016) for google   s id4 system;
gehring et al. (2017) for convolutional sequence to sequence learning for fast neural machine trans-
lation; klein et al. (2017) for openid4, an open source id4 system; cheng
et al. (2016) for semi-supervised learning for id4, and wu et al. (2017c) for
adversarial id4. see vaswani et al. (2017) for a new approach for translation
that replaces id98 and id56 with attention and positional encoding. see zhang et al. (2017b) for
an open source toolkit for id4. see monroe (2017) for a gentle introduction
to translation.
artetxe et al. (2017)

5.3.3 text generation

text generation is the basis for many nlp problems, like conversational response generation, ma-
chine translation, abstractive summarization, etc.
text generation models are usually based on id165, feed-forward neural networks, or recurrent
neural networks, trained to predict next word given the previous ground truth words as inputs; then
in testing, the trained models are used to generate a sequence word by word, using the generated
words as inputs. the errors will accumulate on the way, causing the exposure bias issue. moreover,
these models are trained with word level losses, e.g., cross id178, to maximize the id203 of
next word; however, the models are evaluated on a different metrics like id7.

37

ranzato et al. (2016) proposed mixed incremental cross-id178 reinforce (mixer) for sequence
prediction, with incremental learning and a id168 combining both reinforce and cross-
id178. mixer is a sequence level training algorithm, aligning training and testing objective, such
as id7, rather than predicting the next word as in previous works.
bahdanau et al. (2017) proposed an actor-critic algorithm for sequence prediction, attempting to
further improve ranzato et al. (2016). the authors utilized a critic network to predict the value of a
token, i.e., the expected score following the sequence prediction policy, de   ned by an actor network,
trained by the predicted value of tokens. some techniques are deployed to improve performance:
sarsa rather than monter-carlo method to lessen the variance in estimating value functions; target
network for stability; sampling prediction from a delayed actor whose weights are updated more
slowly than the actor to be trained, to avoid the feedback loop when actor and critic need to be
trained based on the output of each other; reward shaping to avoid the issue of sparse training signal.
yu et al. (2017) proposed seqgan, sequence generative adversarial nets with policy gradient, inte-
grating the adversarial scheme in goodfellow et al. (2014). li et al. (2017a) proposed to improve
sequence generation by considering the knowledge about the future.

5.4 id161

id161 is about how computers gain understanding from digital images or videos. in the
following, after presenting background in id161, we discuss recognition, motion analysis,
scene understanding, integration with nlp, and visual control.
id23 would be an important ingredient for interactive perception (bohg et al.,
2017), where perception and interaction with the environment would be helpful to each other, in
tasks like object segmentation, articulation model estimation, object dynamics learning and haptic
property estimation, object recognition or categorization, multimodal object model learning, object
pose estimation, grasp planning, and manipulation skill learning.
more topics about applying deep rl to id161:

    liu et al. (2017) for id29 of large-scale 3d point clouds;
    devrim kaba et al. (2017) for view planning, which is a set cover problem;
    cao et al. (2017) for face hallucination, i.e., generating a high-resolution face image from

a low-resolution input image;

    brunner et al. (2018) for learning to read maps;
    bhatti et al. (2016) for slam-augmented id25.

5.4.1 background

todo: alexnet (krizhevsky et al., 2012), resnets (he et al., 2016d) densenets (huang et al., 2017),
fast r-id98 (girshick, 2015), faster r-id98 ren et al. (2015), mask r-id98 he et al. (2017),
shrivastava et al. (2017), vaes (variational autoencoder) (diederik p kingma, 2014)
todo: gans (goodfellow et al., 2014; goodfellow, 2017); cyclegan (zhu et al., 2017a), dual-
gan (yi et al., 2017); see arjovsky et al. (2017) for wasserstein gan (wgan) as a stable gans
model. gulrajani et al. (2017) proposed to improve stability of wgan by penalizing the norm of
the gradient of the discriminator with respect to its input, instead of clipping weights as in arjovsky
et al. (2017). mao et al. (2016) proposed least squares gans (lsgans), another stable model.
connection with rl: finn et al. (2016a) established a connection between gans, inverse rl, and
energy-based models. pfau and vinyals (2016) established the connection between gans and actor-
critic algorithms. ho and ermon (2016) and li et al. (2017) studied the connection between gans
and imitation learning.
autoencoder (hinton and salakhutdinov, 2006)
for disentangled factor learning, kulkarni et al. (2015) proposed dc-ign, the deep convolution
inverse graphics network, which follows a semi-supervised way; and chen et al. (2016a) proposed
infogan, an information-theoretic extension to the generative adversarial network, which follows

38

an unsupervised way. zhou et al. (2015) showed that object detectors emerge from learning to
recognize scenes, without supervised labels for objects.
higgins et al. (2017) proposed   -vae to automatically discover interpretable, disentangled, fac-
torised, latent representations from raw images in an unsupervised way. the hyperparameter   
balances latent channel capacity and independence constraints with reconstruction accuracy. when
   = 1,   -vae is the same as the original vaes.
eslami et al. (2016) proposed the framework of attend-infer-repeat for ef   cient id136 in struc-
tured image models to reason about objects explicitly. the authors deployed a recurrent neural
network to design an iterative process for id136, by attending to one object at a time, and for
each image, learning an appropriate number of id136 steps. the authors showed that, in an un-
supervised way, the proposed approach can learn generative models to identify multiple objects for
both 2d and 3d problems.
zhang and zhu (2018) surveyed visual interpretability for deep learning.

5.4.2 recognition

rl can improve ef   ciency for image classi   cation by focusing only on salient parts. for visual
object localization and detection, rl can improve ef   ciency over approaches with exhaustive spatial
hypothesis search and sliding windows, and strikes a balance between sampling more regions for
better accuracy and stopping the search when suf   cient con   dence is obtained about the target   s
location.
mnih et al. (2014) introduced the recurrent attention model (ram) to focus on selected sequence
of regions or locations from an image or video for image classi   cation and id164.
the authors used reinforce to train the model, to overcome the issue that the model is non-
differentiable, and experimented on an image classi   cation task and a dynamic visual control prob-
lem.
caicedo and lazebnik (2015) proposed an active detection model for object localization with id25,
by deforming a bounding box with transformation actions to determine the most speci   c location
for target objects. jie et al. (2016) proposed a tree-structure rl approach to search for objects se-
quentially, considering both the current observation and previous search paths, by maximizing the
long-term reward associated with localization accuracy over all objects with id25. mathe et al.
(2016) proposed to use policy search for visual id164. kong et al. (2017) deployed col-
laborative multi-agent rl with inter-agent communication for joint object search. welleck et al.
(2017) proposed a hierarchical visual architecture with an attention mechanism for multi-label im-
age classi   cation. rao et al. (2017) proposed an attention-aware deep rl method for video face
recognition.
krull et al. (2017) for 6d object pose estimation

5.4.3 motion analysis

in tracking, an agent needs to follow a moving object. supan  ci  c and ramanan (2017) proposed
online decision-making process for tracking, formulated it as a partially observable decision-making
process (pomdp), and learned policies with deep rl algorithms, to decide where to look for the
object, when to reinitialize, and when to update the appearance model for the object, where image
frames may be ambiguous and computational budget may be constrained. yun et al. (2017) also
studied visual tracking with deep rl.
rhinehart and kitani (2017) proposed discovering agent rewards for k-futures online (darko)
to model and forecast    rst-person camera wearer   s long-term goals, together with states, transitions,
and rewards from streaming data, with online inverse id23.

5.4.4 scene understanding

wu et al. (2017b) studied the problem of scene understanding, and attempted to obtain a compact,
expressive, and interpretable representation to encode scene information like objects, their cate-
gories, poses, positions, etc, in a semi-supervised way. in contrast to encoder-decoder based neural

39

architectures as in previous works, wu et al. (2017b) proposed to replace the decoder with a deter-
ministic rendering function, to map a structured and disentangled scene description, scene xml, to
an image; consequently, the encoder transforms an image to the scene xml by inverting the ren-
dering operation, a.k.a., de-rendering. the authors deployed a variant of reinforce algorithm to
overcome the non-differentiability issue of graphics rendering engines.
wu et al. (2017a) proposed a paradigm with three major components, a convolutional perception
module, a physics engine, and a graphics engine, to understand physical scenes without human an-
notations. the perception module recovers a physical world representation by inverting the graphics
engine, inferring the physical object state for each segment proposal in input and combining them.
the generative physics and graphics engines then run forward with the world representation to re-
construct the visual data. the authors showed results on both neural, differentiable and more mature
but non-differentiable physics engines.
there are recent works about physics learning, e.g., agrawal et al. (2016); battaglia et al. (2016);
denil et al. (2017); watters et al. (2017); wu et al. (2015).

5.4.5

integration with nlp

some are integrating id161 with natural language processing. xu et al. (2015) integrated
attention to image captioning, trained the hard version attention with reinforce, and showed
the effectiveness of attention on flickr8k, flickr30k, and ms coco datasets. rennie et al. (2017)
introduced self-critical sequence training, using the output of test-time id136 algorithm as the
baseline in reinforce to normalize the rewards it experiences, for image captioning. see also
liu et al. (2016), lu et al. (2016), and ren et al. (2017) for image captioning. strub et al. (2017)
proposed end-to-end optimization with deep rl for goal-driven and visually grounded dialogue
systems for guesswhat?! game. das et al. (2017) proposed to learn cooperative visual dialog
agents with deep rl. see also kottur et al. (2017). see pasunuru and bansal (2017) for video
captioning. see liang et al. (2017d) for visual relationship and attribute detection.

5.4.6 visual control

visual control is about deriving a policy from visual inputs, e.g., in games (mnih et al., 2015; silver
et al., 2016a; 2017; oh et al., 2015; wu and tian, 2017; dosovitskiy and koltun, 2017; lample
and chaplot, 2017; jaderberg et al., 2017), robotics (finn and levine, 2016; gupta et al., 2017b;
lee et al., 2017; levine et al., 2016a; mirowski et al., 2017; zhu et al., 2017b), and self-driving
vehicles (bojarski et al., 2016; bojarski et al., 2017; zhou and tuzel, 2017). 3

5.5 business management

id23 has many applications in business management, like ads, recommendation,
customer management, and marketing.
li et al. (2010) formulated personalized news articles recommendation as a contextual bandit prob-
lem, to learn an algorithm to select articles sequentially for users based on contextual information
of the user and articles, such as historical activities of the user and descriptive information and cate-
gories of content, and to take user-click feedback to adapt article selection policy to maximize total
user clicks in the long run.
theocharous et al. (2015) formulated a personalized ads id126s as a rl problem
to maximize life-time value (ltv) with theoretical guarantees. this is in contrast to a myopic
solution with supervised learning or contextual bandit formulation, usually with the performance
metric of click through rate (ctr). as the models are hard to learn, the authors deployed a model-
free approach to compute a lower-bound on the expected return of a policy to address the off-policy
evaluation problem, i.e., how to evaluate a rl policy without deployment.

3although we include visual control here, it is not very clear if we should categorize the following type of
problems, e.g., id25 (mnih et al., 2015) and alphago (silver et al., 2016a; 2017), into id161: pixels
(id25) or problem setting (go board status) as the input, some deep neural networks as the architecture, and an
end-to-end id119/ascent algorithm as the optimization method to    nd a policy, without any further
knowledge of id161. or we may see this as part of the synergy of id161 and reinforcement
learning.

40

li et al. (2015) also attempted to maximize lifetime value of customers. silver et al. (2013) pro-
posed concurrent id23 for the customer interaction problem. see sutton and barto
(2018) for a detailed and intuitive description of some topics discussed here under the section title
of personalized web services.

5.6 finance

rl is a natural solution to some    nance and economics problems (hull, 2014; luenberger, 1997),
like option pricing (longstaff and schwartz, 2001; tsitsiklis and van roy, 2001; li et al., 2009),
and multi-period portfolio optimization (brandt et al., 2005; neuneier, 1997), where value function
based rl methods were used. moody and saffell (2001) proposed to utilize policy search to learn
to trade; deng et al. (2016) extended it with deep neural networks. deep (reinforcement) learning
would provide better solutions in some issues in risk management (hull, 2014; yu et al., 2009). the
market ef   ciency hypothesis is fundamental in    nance. however, there are well-known behavioral
biases in human decision-making under uncertainty, in particular, prospect theory (prashanth et al.,
2016). a reconciliation is the adaptive markets hypothesis (lo, 2004), which may be approached by
id23.
it is nontrivial for    nance and economics academia to accept blackbox methods like neural networks;
heaton et al. (2016) may be regarded as an exception. however, there is a lecture in afa 2017
annual meeting: machine learning and prediction in economics and finance. we may also be
aware that    nancial    rms would probably hold state-of-the-art research/application results.
fintech has been attracting attention, especially after the notion of big data. fintech employs
machine learning techniques to deal with issues like fraud detection (phua et al., 2010), consumer
credit risk (khandani et al., 2010), etc.

5.7 healthcare

there are many opportunities and challenges in healthcare for machine learning (miotto et al., 2017;
saria, 2014). personalized medicine is getting popular in healthcare. it systematically optimizes the
patient   s health care, in particular, for chronic conditions and cancers using individual patient infor-
mation, potentially from electronic health/medical record (ehr/emr). dynamic treatment regimes
(dtrs) or adaptive treatment strategies are sequential decision making problems. some issues in
dtrs are not in standard rl. shortreed et al. (2011) tackled the missing data problem, and designed
methods to quantify the evidence of the learned optimal policy. goldberg and kosorok (2012) pro-
posed methods for censored data (patients may drop out during the trial) and    exible number of
stages. see chakraborty and murphy (2014) for a recent survey, and kosorok and moodie (2015)
for an edited book about recent progress in dtrs. currently id24 is the rl method in dtrs.
ling et al. (2017) applied deep rl to the problem of inferring patient phenotypes.
the intersection of machine learning and healthcare are: nips
some recent workshops at
2016 workshop on machine learning for health (http://www.nipsml4hc.ws) and nips 2015
workshop on machine learning in healthcare (https://sites.google.com/site/nipsmlhc15/). see
icml 2017 tutorial on deep learning for health care applications: challenges and solutions
(https://sites.google.com/view/icml2017-deep-health-tutorial/home).

5.8 education

mandel et al. (2014)
liu et al. (2014)

5.9

industry 4.0

the ear of industry 4.0 is approaching, e.g., see o   donovan et al. (2015), and preuveneers and
ilie-zudor (2017). id23 in particular, arti   cial intelligence in general, will be
critical enabling techniques for many aspects of industry 4.0, e.g., predictive maintenance, real-
time diagnostics, and management of manufacturing activities and processes. robots will prevail in
industry 4.0, and we discuss robotics in section 5.2.

41

liu and tomizuka (2016; 2017) studied how to make robots and people to collaborate to achieve
both    exibility and productivity in production lines. see a blog titled towards intelligent industrial
co-robots, at http://bair.berkeley.edu/blog/2017/12/12/corobots/
hein et al. (2017) designed a benchmark for the rl community to attempt to bridge the gap between
academic research and real industrial problems. its open source based on openai gym is available
at https://github.com/siemens/industrialbenchmark.
surana et al. (2016) proposed to apply guided policy search (levine et al., 2016a) as discussed in
section 5.2.1 to optimize trajectory policy of cold spray nozzle dynamics, to handle complex trajec-
tories traversing by robotic agents. the authors generated cold spray surface simulation pro   les to
train the model.

5.10 smart grid

a smart grid is a power grid utilizing modern information technologies to create an intelligent elec-
tricity delivery network for electricity generation, transmission, distribution, consumption, and con-
trol (fang et al., 2012). an important aspect is adaptive control (anderson et al., 2011). glavic et al.
(2017) reviewed application of rl for electric power system decision and control. here we brie   y
discuss demand response (wen et al., 2015b; ruelens et al., 2016).
demand response systems motivate users to dynamically adapt electrical demands in response to
changes in grid signals, like electricity price, temperature, and weather, etc. with suitable electricity
prices, load of peak consumption may be rescheduled/lessened, to improve ef   ciency, reduce costs,
and reduce risks. wen et al. (2015b) proposed to design a fully automated energy management
system with model-free id23, so that it doesn   t need to specify a disutility function
to model users    dissatisfaction with job rescheduling. the authors decomposed the rl formulation
over devices, so that the computational complexity grows linearly with the number of devices, and
conducted simulations using id24. ruelens et al. (2016) tackled the demand response problem
with batch rl. wen et al. (2015b) took the exogenous prices as states, and ruelens et al. (2016)
utilized the average as feature extractor to construct states.

5.11

intelligent transportation systems

intelligent transportation systems (bazzan and kl  ugl, 2014) apply advanced information technolo-
gies for tackling issues in transport networks, like congestion, safety, ef   ciency, etc., to make trans-
port networks, vehicles and users smart.
an important issue in intelligent transportation systems is adaptive traf   c signal control. el-tantawy
et al. (2013) proposed to model the adaptive traf   c signal control problem as a multiple player
stochastic game, and solve it with the approach of multi-agent rl (shoham et al., 2007; busoniu
et al., 2008). multi-agent rl integrates single agent rl with game theory, facing challenges of
stability, nonstationarity, and curse of dimensionality. el-tantawy et al. (2013) approached the issue
of coordination by considering agents at neighbouring intersections. the authors validated their
proposed approach with simulations, and real traf   c data from the city of toronto. el-tantawy
et al. (2013) didn   t explore function approximation. see also van der pol and oliehoek (2017) for a
recent work, and mannion et al. (2016) for an experimental review, about applying rl to adaptive
traf   c signal control.
self-driving vehicle is also a topic of intelligent transportation systems. see bojarski et al. (2016),
bojarski et al. (2017), zhou and tuzel (2017).
see nips 2017, 2016 workshop on machine learning for intelligent transportation systems. check
for a special issue of ieee transactions on neural networks and learning systems on deep re-
inforcement learning and adaptive id145, tentative publication date december
2017.

5.12 computer systems

computer systems are indispensable in our daily life and work, e.g., mobile phones, computers, and
cloud computing. control and optimization problems abound in computer systems, e,g., mestres

42

et al. (2016) proposed knowledge-de   ned networks, gavrilovska et al. (2013) reviewed learning and
reasoning techniques in cognitive radio networks, and haykin (2005) discussed issues in cognitive
radio, like channel state prediction and resource allocation. we also note that internet of things
(iot)(xu et al., 2014) and wireless sensor networks (alsheikh et al., 2014) play an important role in
industry 4.0 as discussed in section 5.9, in smart grid as discussed in section 5.10, and in intelligent
transportation systems as discussed in section 5.11.

5.12.1 resource allocation

mao et al. (2016) studied resource management in systems and networking with deep rl. the au-
thors proposed to tackle multi-resource cluster scheduling with policy gradient, in an online manner
with dynamic job arrivals, optimizing various objectives like average job slowdown or completion
time. the authors validated their proposed approach with simulation.
liu et al. (2017) proposed a hierarchical framework to tackle resource allocation and power man-
agement in cloud computing with deep rl. the authors decomposed the problem as a global tier
for virtual machines resource allocation and a local tier for servers power management. the au-
thors validated their proposed approach with actual google cluster traces. such hierarchical frame-
work/decomposition approach was to reduce state/action space, and to enable distributed operation
of power management.
google deployed machine learning for data centre power management, reducing energy consump-
tion by 40%, https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/.
optimizing memory control is discussed in sutton and barto (2018).

5.12.2 performance optimization

mirhoseini et al. (2017) proposed to optimize device placement for tensor   ow computational graphs
with rl. the authors deployed a seuqence-to-sequence model to predict how to place subsets of
operations in a tensor   ow graph on available devices, using the execution time of the predicted
placement as reward signal for reinforce algorithm. the proposed method found placements
of tensor   ow operations on devices for inception-v3, recurrent neural language model and neural
machine translation, yielding shorter execution time than those placements designed by human ex-
perts. computation burden is one concern for a rl approach to search directly in the solution space
of a combinatorial problem. we discuss combinatorial optimization in section ??.

5.12.3 security & privacy

adversarial attacks, e.g., huang et al. (2017),
http://rll.berkeley.edu/adversarial/
papernot et al. (2016)
abadi et al. (2016)
balle et al. (2016)
delle fave et al. (2014)
carlini and wagner (2017b)
defenders ian j. goodfellow (2015); carlini and wagner (2017a); madry et al. (2017); tram`er et al.
(2017)
anderson et al. (2017) https://github.com/endgameinc/gym-malware
evtimov et al. (2017) see a blog titled physical adversarial examples against deep neural
networks at http://bair.berkeley.edu/blog/2017/12/30/yolo-attack/, which contains a brief survey
of attack and defence algorithms. check acm conference on computer and communications
security (ccs 2016) tutorial on adversarial data mining: big data meets cyber security,
https://www.sigsac.org/ccs/ccs2016/tutorials/index.html

43

6 more topics

we list more interesting and/or important topics we have not discussed in this overview as below,
hoping it would provide pointers for those who may be interested in studying them further. some
topics/papers may not contain rl yet. however, we believe these are interesting and/or important
directions for rl in the sense of either theory or application. it would be de   nitely more desirable
if we could    nish reviewing these, however, we leave it as future work.

    understanding deep learning, daniely et al. (2016); li et al. (2016b); karpathy et al. (2016);
kawaguchi et al. (2017); koh and liang (2017); neyshabur et al. (2017); shalev-shwartz
et al. (2017); shwartz-ziv and tishby (2017); zhang et al. (2017)
    interpretability, e.g., al-shedivat et al. (2017b); doshi-velez and kim (2017); harrison
et al. (2017); lei et al. (2016); lipton (2016); miller (2017); ribeiro et al. (2016); huk
park et al. (2016)

    nips 2017 interpretable machine learning symposium
    icml 2017 tutorial on interpretable machine learning
    nips 2016 workshop on interpretable ml for complex systems
    icml workshop on human interpretability in machine learning 2017, 2016

    usable machine learning, bailis et al. (2017)

    cloud automl: making ai accessible to every business,
https://www.blog.google/topics/google-cloud/cloud-automl-making-ai-accessible-every-
business/

(2017)

    expressivity, raghu et al. (2016)
    testing, pei et al. (2017)
    deep learning ef   ciency, e.g., han et al. (2016), spring and shrivastava (2017), sze et al.
    deep learning compression
    optimization, e.g., wilson et al. (2017), czarnecki et al. (2017)
    id172, klambauer et al. (2017), van hasselt et al. (2016b)
    curriculum learning, graves et al. (2017), held et al. (2017), matiisen et al. (2017)
    professor forcing, lamb et al. (2016)
    new q-value operators, kavosh and littman (2017), haarnoja et al. (2017)
    large action space, e.g., dulac-arnold et al. (2015); he et al. (2016c)
    predictive state representation, downey et al. (2017), venkatraman et al. (2017)
    safe rl, e.g., berkenkamp et al. (2017)
    agent modelling, e.g., albrechta and stone (2018)
    semi-supervised learning, e.g., audiffren et al. (2015); cheng et al. (2016); dai et al.
(2017); finn et al. (2017); kingma et al. (2014); papernot et al. (2017); yang et al. (2017);
zhu and goldberg (2009)

    neural episodic control, pritzel et al. (2017)
    continual learning, chen and liu (2016); kirkpatrick et al. (2017); lopez-paz and ranzato

(2017)

forcement learning summer school 2017

    satinder singh, steps towards continual learning, tutorial at deep learning and rein-
    symbolic learning, evans and grefenstette (2017); liang et al. (2017a); parisotto et al.
    pathnet, fernando et al. (2017)
    evolution strategies, petroski such et al. (2017), salimans et al. (2017) , lehman et al.

(2017)

(2017)

44

    capsules, sabour et al. (2017)
    deepforest, zhou and feng (2017); feng and zhou (2017)
    deep probabilistic programming, tran et al. (2017)
    active learning, e.g., fang et al. (2017)
    deep learning games, schuurmans and zinkevich (2016)
    program learning, e.g., balog et al. (2017); cai et al. (2017); denil et al. (2017); parisotto
    relational reasoning, e.g., santoro et al. (2017), watters et al. (2017)
    proving, e.g., loos et al. (2017); rockt  aschel and riedel (2017)
    education, e.g.,
    music generation, e.g., jaques et al. (2017)
    retrosynthesis, e.g., segler et al. (2017)
    quantum rl, e.g., crawford et al. (2016)

et al. (2017); reed and de freitas (2016)

    nips 2015 workshop on quantum machine learning

7 resources

we list a collection of deep rl resources including books, surveys, reports, online courses, tutorials,
conferences, journals and workshops, blogs, testbed, and open source algorithm implementations.
this by no means is complete.
it is essential to have a good understanding of id23, before having a good under-
standing of deep id23. we recommend to start with the textbook by sutton and
barto (sutton and barto, 2018), the rl courses by rich sutton and by david silver as the    rst two
items in the courses subsection below.
in the current information/social media age, we are overwhelmed by information, e.g., from twitter,
arxiv, google+, etc. the skill to ef   ciently select the best information becomes essential. the wild
week in ai (http://www.wildml.com) is an excellent series of weekly summary blogs. in an ear
of ai, we expect to see an ai agent to do such tasks like intelligently searching and summarizing
relevant news, blogs, research papers, etc.

7.1 books

    the de   nitive and intuitive id23 book by richard s. sutton and andrew
    deep learning books (deng and dong, 2014; goodfellow et al., 2016)

g. barto (sutton and barto, 2018)

7.2 more books

    theoretical rl books (bertsekas, 2012; bertsekas and tsitsiklis, 1996; szepesv  ari, 2010)
    an operations research oriented rl book (powell, 2011)
    an edited rl book (wiering and van otterlo, 2012)
    id100 (puterman, 2005)
    machine learning (bishop, 2011; hastie et al., 2009; haykin, 2008; james et al., 2013;
kuhn and johnson, 2013; murphy, 2012; provost and fawcett, 2013; simeone, 2017; zhou,
2016)

    arti   cial intelligence (russell and norvig, 2009)
    natural language processing (nlp) (deng and liu, 2017; goldberg, 2017; jurafsky and
    semi-supervised learning (zhu and goldberg, 2009)
    game theory (leyton-brown and shoham, 2008)

martin, 2017)

45

7.3 surveys and reports

    id23 (littman, 2015; kaelbling et al., 1996; geramifard et al., 2013;
grondman et al., 2012; roijers et al., 2013); deep id23 (arulkumaran
et al., 2017) 4
    deep learning (lecun et al., 2015; schmidhuber, 2015; bengio, 2009; wang and raj, 2017)
    ef   cient processing of deep neural networks (sze et al., 2017)
    machine learning (jordan and mitchell, 2015)
    practical machine learning advices (domingos, 2012; smith, 2017; zinkevich, 2017)
    natural language processing (nlp) (hirschberg and manning, 2015; cho, 2015; young
    spoken dialogue systems (deng and li, 2013; hinton et al., 2012; he and deng, 2013;
    robotics (kober et al., 2013)
    id21 (taylor and stone, 2009; pan and yang, 2010; weiss et al., 2016)
    bayesian rl (ghavamzadeh et al., 2015)
    ai safety (amodei et al., 2016; garc`  a and fern`andez, 2015)
    id169 (mcts) (browne et al., 2012; gelly et al., 2012)

young et al., 2013)

et al., 2017)

7.4 courses

(goo.gl/7bvrkt)

forcement learning, goo.gl/eyvlfg

stats 385), https://stats385.github.io

spring 2017, https://katefvision.github.io

spring 2017, http://rll.berkeley.edu/deeprlcourse/

http://www.incompleteideas.net/sutton/609%20dropbox/

    richard sutton, id23, 2016, slides, assignments, reading materials, etc.
    david silver, id23, 2015, slides (goo.gl/uqaxlo), video-lectures
    sergey levine, john schulman and chelsea finn, cs 294: deep id23,
    katerina fragkiadaki, ruslan satakhutdinov, deep id23 and control,
    emma brunskill, cs234: id23, http://web.stanford.edu/class/cs234/
    charles isbell, michael littman and pushkar kolhe, udacity: machine learning: rein-
    david donoho, hatef monajemi, and vardan papyan, theories of deep learning (stanford
    nando de freitas, deep learning lectures, https://www.youtube.com/user/profnandodf
    fei-fei li, andrej karpathy and justin johnson, cs231n: convolutional neural networks
    richard socher, cs224d: deep learning for natural language processing,
    brendan shillingford, yannis assael, chris dyer, oxford deep nlp 2017 course,
    pieter abbeel, advanced robotics, fall 2015, https://people.eecs.berkeley.edu/ pabbeel/cs287-
    emo todorov, intelligent control through learning and optimization,
    abdeslam boularias, robot learning seminar, http://www.abdeslam.net/robotlearningseminar
    mit 6.s094: deep learning for self-driving cars, http://selfdrivingcars.mit.edu
    jeremy howard, practical deep learning for coders, http://course.fast.ai
    andrew ng, deep learning specialization

fa15/
http://homes.cs.washington.edu/   todorov/courses/amath579/index.html

for visual recognition, http://cs231n.stanford.edu

https://github.com/oxford-cs-deepnlp-2017

http://cs224d.stanford.edu

https://www.coursera.org/specializations/deep-learning

4our overview is much more comprehensive, and was online much earlier, than this brief survey.

46

7.5 tutorials

    rich sutton, introduction to id23 with function approximation,

https://www.microsoft.com/en-us/research/video/tutorial-introduction-to-reinforcement-
learning-with-function-approximation/

    deep id23
    david silver, icml 2016
    david silver, 2nd multidisciplinary conference on id23 and de-

cision making (rldm), edmonton, alberta, canada, 2015;
http://videolectures.net/rldm2015 silver id23/

    john schulman, deep learning school, 2016
    pieter abbeel, deep learning summer school, 2016;

http://videolectures.net/deeplearning2016 abbeel deep reinforcement/

    pieter abbeel and john schulman, deep id23 through policy op-

timization, nips 2016

and control, icml 2017

inforcement learning workshop, nips 2016

2016; http://videolectures.net/deeplearning2016 pineau id23/

    sergey levine and chelsea finn, deep id23, decision making,
    john schulman, the nuts and bolts of deep id23 research, deep re-
    joelle pineau, introduction to id23, deep learning summer school,
    andrew ng, nuts and bolts of building applications using deep learning, nips 2016
    deep learning summer school, 2016, 2015
    deep learning and id23 summer schools, 2017
    simons institute interactive learning workshop, 2017
    simons institute representation learning workshop, 2017
    simons institute computational challenges in machine learning workshop, 2017

7.6 conferences, journals and workshops
    nips: neural information processing systems
    icml: international conference on machine learning
    iclr: international conference on learning representation
    rldm: multidisciplinary conference on id23 and decision making
    ewrl: european workshop on id23
    aaai, ijcai, acl, emnlp, sigdial, icra, iros, kdd, sigir, cvpr, etc.
    nature machine intelligence, science robotics, jmlr, mlj, aij, jair, pami, etc
    nature may 2015, science july 2015, survey papers on machine learning/ai
    science, july 7, 2017 issue, the cyberscientist, a special issue about ai
    deep id23 workshop, nips 2016, 2015; ijcai 2016
    deep learning workshop, icml 2016
    http://distill.pub

7.7 blogs

    deepmind blog, https://deepmind.com/blog/

   
   

    google research blog, https://research.googleblog.com, goo.gl/ok88b7

    the google brain team     looking back on 2017, goo.gl/1g7jnb, goo.gl/ucwdlr

47

    the google brain team     looking back on 2016,

http://www.marcgbellemare.info/blog/

www.wildml.com, esp. goo.gl/myrwdc

    berkeley ai research blog, http://bair.berkeley.edu/blog/
    openai blog, https://blog.openai.com
    marc bellemare, classic and modern id23,
    denny britz, the wild week in ai, a weekly ai & deep learning newsletter,
    andrej karpathy, karpathy.github.io, esp. goo.gl/1hkkrb
    junling hu, id23 explained - learning to act based on long-term payoffs
    li deng, how deep id23 can help chatbots
https://venturebeat.com/2016/08/01/how-deep-reinforcement-learning-can-help-chatbots/
    id23, https://www.technologyreview.com/s/603501/10-breakthrough-
    deep learning, https://www.technologyreview.com/s/513696/deep-learning/

https://www.oreilly.com/ideas/reinforcement-learning-explained

technologies-2017-reinforcement-learning/

7.8 testbeds

2017c), http://ray.readthedocs.io/en/latest/rllib.html

a framework composed of atari 2600 games to develop and evaluate ai agents.

    the arcade learning environment (ale) (bellemare et al., 2013; machado et al., 2017) is
    ray rllib: a composable and scalable id23 library (liang et al.,
    openai gym (https://gym.openai.com) is a toolkit for the development of rl algorithms,
consisting of environments, e.g., atari games and simulated robots, and a site for the com-
parison and reproduction of results.
    openai universe (https://universe.openai.com) is used to turn any program into a gym
environment. universe has already integrated many environments, including atari games,
   ash games, browser tasks like mini world of bits and real-world browser tasks. recently,
gta v was added to universe for self-driving vehicle simulation.

such as starcraft: brood war.

https://github.com/davechurchill/commandcenter

    deepmind control suite, tassa et al. (2018)
    deepmind released a    rst-person 3d game platform deepmind lab (beattie et al., 2016).
deepmind and blizzard will collaborate to release the starcraft ii ai research environment
(goo.gl/ptiwfg).
    psychlab: a psychology laboratory for deep id23 agents (leibo
    fair torchcraft (synnaeve et al., 2016) is a library for real-time strategy (rts) games
    deepmind pysc2 - starcraft ii learning environment, https://github.com/deepmind/pysc2
    david churchill, commandcenter: starcraft 2 ai bot,
    parlai is a framework for dialogue research, implemented in python, open-sourced by
    elf, an extensive, lightweight and    exible platform for rl research (tian et al., 2017)
    project malmo (https://github.com/microsoft/malmo), from microsoft, is an ai research
    twitter open-sourced torch-twrl, a framework for rl development.
    vizdoom is a doom-based ai research platform for visual rl (kempka et al., 2016).
    baidu apollo project, self-driving open-source, http://apollo.auto
    torcs is a car racing simulator (bernhard wymann et al., 2014).
    mujoco, multi-joint dynamics with contact, is a physics engine, http://www.mujoco.org.

and experimentation platform built on top of minecraft.

facebook. https://github.com/facebookresearch/parlai

et al., 2018)

48

    nogueira and cho (2016) presented webnav challenge for wikipedia links navigation.
    rlglue (tanner and white, 2009) is a language-independent software for rl experiments.
    rlpy (geramifard et al., 2015) is a value-function-based id23 frame-

it may need extensions to accommodate progress in deep learning.

work for education and research.

7.9 algorithm implementations

we collect implementations of algorithms, either classical ones as in a textbook like sutton and
barto (2018) or in recent papers.

https://github.com/rllab/rllab

https://github.com/openai/baselines

https://github.com/devsisters/id25-tensor   ow

https://github.com/carpedm20/deep-rl-tensor   ow

https://github.com/iassael/torch-bootstrapped-id25

http://www.wildml.com/2016/10/learning-reinforcement-learning/

rl course, https://github.com/shangtongzhang/reinforcement-learning-an-introduction

    shangtong zhang, python code to accompany sutton & barto   s rl book and david silver   s
    learning id23 (with code, exercises and solutions),
    openai baselines: high-quality implementations of id23 algorithms,
    tensorflow implementation of deep id23 papers,
    deep id23 for keras, https://github.com/matthiasplappert/keras-rl
    code implementations for nips 2016 papers, http://bit.ly/2hsaoyx
    benchmark results of various policy optimization algorithms (duan et al., 2016),
    tensor2tensor (t2t) (vaswani et al., 2017; kaiser et al., 2017a;b)
    id25 (mnih et al., 2015), https://sites.google.com/a/deepmind.com/id25/
    tensor   ow implementation of id25 (mnih et al., 2015),
    deep id24 with keras and gym, https://keon.io/deep-id24/
    deep exploration via bootstrapped id25 (osband et al., 2016), a torch implementation,
    darkforest, the facebook go engine (github), https://github.com/facebookresearch/darkforestgo
    using keras and deep q-network to play flappybird,
    deep deterministic policy gradients (lillicrap et al., 2016) in tensorflow,
    deep deterministic policy gradient (lillicrap et al., 2016) to play torcs,
    id23 with unsupervised auxiliary tasks (jaderberg et al., 2017),
    learning to communicate with deep multi-agent id23,
    deep id23: playing a racing game - byte tank, http://bit.ly/2pvip4i
    differentiable neural computer (dnc) (graves et al., 2016),
    playing fps games with deep id23 (lample and chaplot, 2017),
    learning to learn (reed and de freitas, 2016) in tensorflow,
    value iteration networks (tamar et al., 2016) in tensor   ow,

https://yanpanlau.github.io/2016/07/10/flappybird-keras.html

http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html

https://yanpanlau.github.io/2016/10/11/torcs-keras.html

https://github.com/iassael/learning-to-communicate

https://github.com/deepmind/learning-to-learn

https://github.com/miyosuda/unreal

https://github.com/glample/arnold

https://github.com/deepmind/dnc

https://github.com/theabhikumar/tensor   ow-value-iteration-networks

49

https://github.com/zhongwen/predictron

    tensor   ow implementation of the predictron (silver et al., 2016b),
    meta id23 (wang et al., 2016) in tensor   ow,
    generative adversarial imitation learning (ho and ermon, 2016), containing an im-
plementation of trust region policy optimization (trpo) (schulman et al., 2015),
https://github.com/openai/imitation

https://github.com/awjuliani/meta-rl

https://github.com/openai/evolution-strategies-starter

    starter code for evolution strategies (salimans et al., 2017),
    id21 (long et al., 2015; 2016), https://github.com/thuml/transfer-caffe
    deepforest (zhou and feng, 2017), http://lamda.nju.edu.cn/   les/gcforest.zip

8 brief summary

we list some rl issues and corresponding proposed approaches covered in this overview, as well
as some classical work. one direction of future work is to further re   ne this section, especially for
issues and solutions in applications.

    issue: prediction, policy evaluation

proposed approaches:

    temporal difference (td) learning (sutton, 1988)
    issue: control,    nding optimal policy (classical work)

proposed approaches:

    id24 (watkins and dayan, 1992)
    policy gradient (williams, 1992)

(cid:5) reduce variance of gradient estimate: baseline, advantage function (williams,
1992; sutton et al., 2000)
    actor-critic (barto et al., 1983)
    sarsa (sutton and barto, 2018)

the deadly triad:

instability and divergence when combining off-policy, function

    issue:

approximation, and id64
proposed approaches:

    id25 with experience replay (lin, 1992) and target network (mnih et al., 2015)

(cid:5) overestimate problem in id24: double id25 (van hasselt et al., 2016a)
(cid:5) prioritized experience replay (schaul et al., 2016)
(cid:5) better exploration strategy (osband et al., 2016)
(cid:5) optimality tightening to accelerate id25 (he et al., 2017)
(cid:5) reduce variability and instability with averaged-id25 (anschel et al., 2017)

    dueling architecture (wang et al., 2016b)
    asynchronous methods (mnih et al., 2016)
    trust region policy optimization (schulman et al., 2015)
    distributed proximal policy optimization (heess et al., 2017)
    combine policy gradient and id24 (o   donoghue et al., 2017; nachum et al.,

2017; gu et al., 2017; schulman et al., 2017)

    gtd (sutton et al., 2009a;b; mahmood et al., 2014)
    emphatic-td (sutton et al., 2016)

    issue: train perception and control jointly end-to-end

proposed approaches:

    guided policy search (levine et al., 2016a)

    issue: data/sample ef   ciency

50

proposed approaches:

    id24, actor-critic
    model-based policy search, e.g., pilco deisenroth and rasmussen (2011)
    actor-critic with experience replay (wang et al., 2017b)
    pgq, policy gradient and id24 (o   donoghue et al., 2017)
    q-prop, policy gradient with off-policy critic (gu et al., 2017)
    return-based off-policy control, retrace (munos et al., 2016), reactor (gruslys et al.,

2017)

    learning to learn, e.g., duan et al. (2016); wang et al. (2016); lake et al. (2015)

    issue: reward function not available

proposed approaches:
    imitation learning
    inverse rl (ng and russell, 2000)
    learn from demonstration (hester et al., 2018)
    imitation learning with gans (ho and ermon, 2016; stadie et al., 2017)
    train dialogue policy jointly with reward model (su et al., 2016b)

    issue: exploration-exploitation tradeoff

proposed approaches:

    unify count-based exploration and intrinsic motivation (bellemare et al., 2016)
    under-appreciated reward exploration (nachum et al., 2017)
    deep exploration via bootstrapped id25 (osband et al., 2016)
    variational information maximizing exploration (houthooft et al., 2016)

    issue: model-based learning

proposed approaches:

    dyna-q (sutton, 1990)
    combine model-free and model-based rl (chebotar et al., 2017)

    issue: model-free planning

proposed approaches:

    value iteration networks (tamar et al., 2016)
    predictron (silver et al., 2016b)

    issue: focus on salient parts

proposed approaches: attention

    id164 (mnih et al., 2014)
    id4 (bahdanau et al., 2015)
    image captioning (xu et al., 2015)
    replace id98 and id56 with attention in sequence modelling (vaswani et al., 2017)

    issue: data storage over long time, separating from computation

proposed approaches: memory

    differentiable neural computer (dnc) with external memory (graves et al., 2016)

    issue: bene   t from non-reward training signals in environments

proposed approaches: unsupervised learning

    horde (sutton et al., 2011)
    unsupervised reinforcement and auxiliary learning (jaderberg et al., 2017)
    learn to navigate with unsupervised auxiliary learning (mirowski et al., 2017)
    id3 (gans) (goodfellow et al., 2014)

    issue: learn knowledge from different domains

51

proposed approaches:
weiss et al., 2016)

id21 (taylor and stone, 2009; pan and yang, 2010;

    learn invariant features to transfer skills (gupta et al., 2017a)

    issue: bene   t from both labelled and unlabelled data

proposed approaches: semi-supervised learning (zhu and goldberg, 2009)

    learn with mdps both with and without reward functions (finn et al., 2017)
    learn with expert   s trajectories and those may not from experts (audiffren et al., 2015)
    issue: learn, plan, and represent knowledge with spatio-temporal abstraction at multiple

levels
proposed approaches: hierarchical rl (barto and mahadevan, 2003)

    options (sutton et al., 1999), maxq (dietterich, 2000)
    strategic attentive writer to learn macro-actions (vezhnevets et al., 2016)
    integrate temporal abstraction with intrinsic motivation (kulkarni et al., 2016)
    stochastic neural networks for hierarchical rl (florensa et al., 2017)
    lifelong learning with hierarchical rl (tessler et al., 2017)

    issue: adapt rapidly to new tasks

proposed approaches: learning to learn

    learn to optimize (li and malik, 2017)
    learn a    exible id56 model to handle a family of rl tasks (duan et al., 2016; wang

et al., 2016)

    one/few/zero-shot learning (duan et al., 2017; johnson et al., 2016; kaiser et al.,
2017b; koch et al., 2015; lake et al., 2015; ravi and larochelle, 2017; vinyals et al.,
2016)

    issue: gigantic search space

proposed approaches:

    integrate supervised learning, id23, and monte-carlo tree search as

in alphago (silver et al., 2016a)

    issue: neural networks architecture design

proposed approaches:

    neural architecture search (bello et al., 2017; baker et al., 2017; zoph and le, 2017)
    new architectures, e.g., kaiser et al. (2017a), silver et al. (2016b), tamar et al. (2016),

vaswani et al. (2017), wang et al. (2016b)

9 discussions

it is both the best and the worst of times for the    eld of deep rl, for the same reason: it has been
growing so fast and so enormously. we have been witnessing breakthroughs, exciting new methods
and applications, and we expect to see much more and much faster. as a consequence, this overview
is incomplete, in the sense of both depth and width. however, we attempt to summarize important
achievements and discuss potential directions and applications in this amazing    eld.
in this overview, we summarize six core elements     value function, policy, reward, model and plan-
ning, exploration, and knowledge; six important mechanisms     attention and memory, unsupervised
learning, id21, multi-agent rl, hierarchical rl, and learning to learn; and twelve ap-
plications     games, robotics, natural language processing, id161, business management,
   nance, healthcare, education, industry 4.0, smart grid, intelligent transportation systems, and com-
puter systems. we also discuss background of machine learning, deep learning, and reinforcement
learning, and list a collection of rl resources.
we have seen breakthroughs about deep rl, including deep q-network (mnih et al., 2015) and
alphago (silver et al., 2016a). there have been many extensions to, improvements for and applica-
tions of deep q-network (mnih et al., 2015).

52

novel architectures and applications using deep rl were recognized in top tier conferences as best
papers in 2016: dueling network architectures (wang et al., 2016b) at icml, spoken dialogue
systems (su et al., 2016b) at acl (student), information extraction (narasimhan et al., 2016) at
emnlp, and, value iteration networks (tamar et al., 2016) at nips. gelly and silver (2007) was
the recipient of test of time award at icml 2017. in 2017, the following were recognized as
best papers, kottur et al. (2017) at emnlp (short), and, bacon et al. (2017) at aaai (student).
exciting achievements abound: differentiable neural computer (graves et al., 2016), unsupervised
reinforcement and auxiliary learning (jaderberg et al., 2017), asynchronous methods (mnih et al.,
2016), dual learning for machine translation (he et al., 2016a), guided policy search (levine et al.,
2016a), generative adversarial imitation learning (ho and ermon, 2016), and neural architecture de-
sign (zoph and le, 2017), etc. creativity would push the frontiers of deep rl further with respect
to core elements, mechanisms, and applications.
state of the art control of atari games using shallow rl was accepted at aamas. it was also
nominated for the best paper award
value function is central to id23, e.g., in deep q-network and its many exten-
tions. policy optimization approaches have been gaining traction, in many, diverse applications,
e.g., robotics, neural architecture design, spoken dialogue systems, machine translation, attention,
and learning to learn, and this list is boundless. new learning mechanisms have emerged, e.g.,
using transfer/unsupervised/semi-supervised learning to improve the quality and speed of learn-
ing, and more new mechanisms will be emerging. this is the renaissance of reinforcement learn-
ing (krakovsky, 2016). in fact, id23 and deep learning have been making steady
progress even during the last ai winter.
a popular criticism about deep learning is that it is a blackbox, or even the    alchemy    as a com-
ment during nips 2017 test of time award (rahimi and recht, 2007) speech, so it is not clear
how it works. this should not be the reason not to accept deep learning; rather, having a better un-
derstanding of how deep learning works is helpful for deep learning and general machine learning
community. there are works in this direction as well as for interpretability of deep learning as we
list in section 6.
it is essential to consider issues of learning models, like stability, convergence, accuracy, data ef   -
ciency, scalability, speed, simplicity, interpretability, robustness, and safety, etc. it is important to
investigate comments/criticisms, e.g., from conginitive science, like intuitive physics, intuitive psy-
chology, causal model, compositionality, learning to learn, and act in real time (lake et al., 2016),
for stronger ai. it is interesting to check deepmind   s commentary (botvinick et al., 2017) about
one additional ingredient, autonomy, so that agents can build and exploit their own internal models,
with minimal human manual engineering, and investigate the connection between neuroscience and
rl/ai (hassabis et al., 2017). see also peter norvig   s perspective at http://bit.ly/2qpehcd. see sto-
ica et al. (2017) for systems challenges for ai.
nature in may 2015 and science in july 2015 featured survey papers on machine learning/ai. sci-
ence robotics launched in 2016. science has a special issue on july 7, 2017 about ai on the
cyberscientist. nature machine intelligence will launch in january 2019. the coverage of ai by
premier journals like nature and science and the launch of science robotics and nature machine
intelligence illustrate the apparent importance of ai. it is interesting to mention that nips 2017
main conference was sold out only two weeks after opening for registration.
it is worthwhile to envision deep rl considering perspectives of government, academia and industry
on ai, e.g., arti   cial intelligence, automation, and the economy, executive of   ce of the president,
usa; arti   cial intelligence and life in 2030 - one hundred year study on arti   cial intelligence:
report of the 2015-2016 study panel, stanford university (stone et al., 2016); and ai, machine
learning and data fuel the future of productivity by the goldman sachs group, inc., etc. see also
the recent ai frontiers conference, https://www.aifrontiers.com.
deep learning was among mit technology review 10 breakthrough technologies in 2013. we
have been witnessing the dramatic development of deep learning in both academia and industry in
the last few years. id23 was among mit technology review 10 breakthrough
technologies in 2017. deep learning has made many achievements, has    conquered    speech recog-
nition, id161, and now nlp, is more mature and well-accepted, and has been validated by
products and market. in contrast, rl has lots of (potential, promising) applications, yet few products

53

so far, may still need better algorithms, may still need products and market validation. however, it
is probably the right time to nurture, educate and lead the market. we will see both deep learning
and id23 prospering in the coming years and beyond. prediction is very dif   cult,
especially about the future. however, we expect that 2018 for id23 would be 2010
for deep learning.
deep learning, in this third wave of ai, will have deeper in   uences, as we have already seen from
its many achievements. id23, as a more general learning and decision making
paradigm, will deeply in   uence deep learning, machine learning, and arti   cial intelligence in gen-
eral. deepmind, conducting leading research in deep id23, recently opened its
   rst ever international ai research of   ce in alberta, canada, co-locating with the major research
center for id23 led by rich sutton. it is interesting to mention that when pro-
fessor rich sutton started working in the university of alberta in 2003, he named his lab rlai:
id23 and arti   cial intelligence.

ackowledgement

we appreciate comments from baochun bai, kan deng, hai fang, hua he, junling hu, ruitong
huang, aravind lakshminarayanan, jinke li, lihong li, bhairav mehta, dale schuurmans, david
silver, rich sutton, csaba szepesv  ari, arash tavakoli, cameron upright, yi wan, qing yu, yao-
liang yu, attendants of various seminars and webinars, in particular, a seminar at mit on alphago:
key techniques and applications, and an ai seminar at the university of alberta on deep rein-
forcement learning: an overview. any remaining issues and errors are our own.

references

abadi, m., chu, a., goodfellow, i., mcmahan, h. b., mironov, i., talwar, k., and zhang, l. (2016).
deep learning with differential privacy. in acm conference on computer and communications
security (acm ccs).

abbeel, p. and ng, a. y. (2004). apprenticeship learning via inverse id23. in the

international conference on machine learning (icml).

agrawal, p., nair, a., abbeel, p., malik, j., and levine, s. (2016). learning to poke by poking:
experiential learning of intuitive physics. in the annual conference on neural information pro-
cessing systems (nips).

al-shedivat, m., bansal, t., burda, y., sutskever, i., mordatch, i., and abbeel, p. (2017a). con-
tinuous adaptation via meta-learning in nonstationary and competitive environments. arxiv
e-prints.

al-shedivat, m., dubey, a., and xing, e. p. (2017b). contextual explanation networks. arxiv

e-prints.

albrechta, s. v. and stone, p. (2018). autonomous agents modelling other agents: a comprehensive

survey and open problems. arti   cial intelligence.

alsheikh, m. a., lin, s., niyato, d., and tan, h.-p. (2014). machine learning in wireless sensor
networks: algorithms, strategies, and applications. ieee communications surveys & tutorials,
16(4):1996   2018.

amin, k., jiang, n., and singh, s. (2017). repeated inverse id23. in the annual

conference on neural information processing systems (nips).

amodei, d., olah, c., steinhardt, j., christiano, p., schulman, j., and man  e, d. (2016). concrete

problems in ai safety. arxiv e-prints.

anderson, h. s., kharkar, a., filar, b., and roth, p. (2017). evading machine learning malware

detection. in black hat usa.

54

anderson, r. n., boulanger, a., powell, w. b., and scott, w. (2011). adaptive stochastic control

for the smart grid. proceedings of the ieee, 99(6):1098   1115.

andreas, j., klein, d., and levine, s. (2017). modular multitask id23 with policy

sketches. in the international conference on machine learning (icml).

andrychowicz, m., denil, m., colmenarejo, s. g., hoffman, m. w., pfau, d., schaul, t., shilling-
ford, b., and de freitas, n. (2016). learning to learn by id119 by id119. in
the annual conference on neural information processing systems (nips).

andrychowicz, m., wolski, f., ray, a., schneider, j., fong, r., welinder, p., mcgrew, b., tobin,
j., abbeel, p., and zaremba, w. (2017). hindsight experience replay. in the annual conference
on neural information processing systems (nips).

anschel, o., baram, n., and shimkin, n. (2017). averaged-id25: variance reduction and stabi-
lization for deep id23. in the international conference on machine learning
(icml).

argall, b. d., chernova, s., veloso, m., and browning, b. (2009). a survey of robot learning from

demonstration. robotics and autonomous systems, 57(5):469   483.

arjovsky, m., chintala, s., and bottou, l. (2017). wasserstein gan. arxiv e-prints.

artetxe, m., labaka, g., agirre, e., and cho, k. (2017). unsupervised id4.

arxiv e-prints.

arulkumaran, k., deisenroth, m. p., brundage, m., and bharath, a. a. (2017). a brief survey of

deep id23. arxiv e-prints.

asri, l. e., he, j., and suleman, k. (2016). a sequence-to-sequence model for user simulation
in annual meeting of the international speech communication

in spoken dialogue systems.
association (interspeech).

audiffren, j., valko, m., lazaric, a., and ghavamzadeh, m. (2015). maximum id178 semi-
in the international joint conference on arti   cial

supervised inverse id23.
intelligence (ijcai).

azar, m. g., osband, i., and munos, r. (2017). minimax regret bounds for id23.

in the international conference on machine learning (icml).

ba, j., hinton, g. e., mnih, v., leibo, j. z., and ionescu, c. (2016). using fast weights to attend to

the recent past. in the annual conference on neural information processing systems (nips).

ba, j., mnih, v., and kavukcuoglu, k. (2014). multiple object recognition with visual attention. in

the international conference on learning representations (iclr).

babaeizadeh, m., frosio, i., tyree, s., clemons, j., and kautz, j. (2017). reinforcement learn-
ing through asynchronous advantage actor-critic on a gpu. in the international conference on
learning representations (iclr).

bacon, p.-l., harb, j., and precup, d. (2017). the option-critic architecture. in the aaai conference

on arti   cial intelligence (aaai).

bahdanau, d., brakel, p., xu, k., goyal, a., lowe, r., pineau, j., courville, a., and bengio, y.
(2017). an actor-critic algorithm for sequence prediction. in the international conference on
learning representations (iclr).

bahdanau, d., cho, k., and bengio, y. (2015). id4 by jointly learning to

align and translate. in the international conference on learning representations (iclr).

bailis, p., olukoton, k., re, c., and zaharia, m. (2017). infrastructure for usable machine learning:

the stanford dawn project. arxiv e-prints.

baird, l. (1995). residual algorithms: id23 with function approximation. in the

international conference on machine learning (icml).

55

baker, b., gupta, o., naik, n., and raskar, r. (2017). designing neural network architectures using

id23. in the international conference on learning representations (iclr).

balle, b., gomrokchi, m., and precup, d. (2016). differentially private policy evaluation. in the

international conference on machine learning (icml).

balog, m., gaunt, a. l., brockschmidt, m., nowozin, s., and tarlow, d. (2017). deepcoder:
learning to write programs. in the international conference on learning representations (iclr).

bansal, t., pachocki, j., sidor, s., sutskever, i., and mordatch, i. (2017). emergent complexity via

multi-agent competition. arxiv e-prints.

barreto, a., munos, r., schaul, t., and silver, d. (2017). successor features for transfer in rein-
forcement learning. in the annual conference on neural information processing systems (nips).

barto, a. g. and mahadevan, s. (2003). recent advances in hierarchical id23.

discrete event dynamic systems, 13(4):341   379.

barto, a. g., sutton, r. s., and anderson, c. w. (1983). neuronlike elements that can solve dif   cult

learning control problems. ieee transactions on systems, man, and cybernetics, 13:835   846.

battaglia, p. w., pascanu, r., lai, m., rezende, d., and kavukcuoglu, k. (2016). interaction net-
in the annual conference on neural

works for learning about objects, relations and physics.
information processing systems (nips).

bazzan, a. l. and kl  ugl, f. (2014). introduction to intelligent systems in traf   c and transportation.

morgan & claypool.

beattie, c., leibo, j. z., teplyashin, d., ward, t., wainwright, m., k  uttler, h., lefrancq, a., green,
s., vald  es, v., sadik, a., schrittwieser, j., anderson, k., york, s., cant, m., cain, a., bolton,
a., gaffney, s., king, h., hassabis, d., legg, s., and petersen, s. (2016). deepmind lab. arxiv
e-prints.

bellemare, m. g., dabney, w., and munos, r. (2017). a distributional perspective on reinforcement

learning. in the international conference on machine learning (icml).

bellemare, m. g., danihelka, i., dabney, w., mohamed, s., lakshminarayanan, b., hoyer, s., and
munos, r. (2017). the cramer distance as a solution to biased wasserstein gradients. arxiv
e-prints.

bellemare, m. g., naddaf, y., veness, j., and bowling, m. (2013). the arcade learning environment:
an evaluation platform for general agents. journal of arti   cial intelligence research, 47:253   
279.

bellemare, m. g., schaul, t., srinivasan, s., saxton, d., ostrovski, g., and munos, r. (2016).
unifying count-based exploration and intrinsic motivation. in the annual conference on neural
information processing systems (nips).

bello, i., pham, h., le, q. v., norouzi, m., and bengio, s. (2016). neural combinatorial optimiza-

tion with id23. arxiv e-prints.

bello, i., zoph, b., vasudevan, v., and le, q. v. (2017). neural optimizer search with reinforcement

learning. in the international conference on machine learning (icml).

bengio, y. (2009). learning deep architectures for ai. foundations and trends r(cid:13)in machine learn-

ing, 2(1):1   127.

bengio, y. (2017). the consciousness prior. arxiv e-prints.

bengio, y., louradour, j., collobert, r., and weston, j. (2009). curriculum learning. in the inter-

national conference on machine learning (icml).

berkenkamp, f., turchetta, m., schoellig, a. p., and krause, a. (2017). safe model-based rein-
forcement learning with stability guarantees. in the annual conference on neural information
processing systems (nips).

56

bernhard wymann, e. e., guionneau, c., dimitrakakis, c., and r  emi coulom, a. s. (2014).

torcs, the open racing car simulator.    http://www.torcs.org   .

berthelot, d., schumm, t., and metz, l. (2017). began: boundary equilibrium generative ad-

versarial networks. arxiv e-prints.

bertsekas, d. p. (2012). id145 and optimal control (vol. ii, 4th edition: approxi-

mate id145). athena scienti   c, massachusetts, usa.

bertsekas, d. p. and tsitsiklis, j. n. (1996). neuro-id145. athena scienti   c.

bhatti, s., desmaison, a., miksik, o., nardelli, n., siddharth, n., and torr, p. h. s. (2016). playing

doom with slam-augmented deep id23. arxiv e-prints.

bishop, c. (2011). pattern recognition and machine learning. springer.

blei, d. m. and smyth, p. (2017). science and data science. pnas, 114(33):8689   8692.

bohg, j., hausman, k., sankaran, b., brock, o., kragic, d., schaal, s., and sukhatme, g. s. (2017).
interactive perception: leveraging action in perception and perception in action. ieee transac-
tions on robotics, 33(6):1273   1291.

bojarski, m., testa, d. d., dworakowski, d., firner, b., flepp, b., goyal, p., jackel, l. d., monfort,
m., muller, u., zhang, j., zhang, x., zhao, j., and zieba, k. (2016). end to end learning for
self-driving cars. arxiv e-prints.

bojarski, m., yeres, p., choromanska, a., choromanski, k., firner, b., jackel, l., and muller, u.
(2017). explaining how a deep neural network trained with end-to-end learning steers a car.
arxiv e-prints.

bordes, a., boureau, y.-l., and weston, j. (2017). learning end-to-end goal-oriented dialog. in the

international conference on learning representations (iclr).

botvinick, m., barrett, d. g. t., battaglia, p., de freitas, n., kumaran, d., leibo, j. z., lillicrap,
t., modayil, j., mohamed, s., rabinowitz, n. c., rezende, d. j., santoro, a., schaul, t., sum-
mer   eld, c., wayne, g., weber, t., wierstra, d., legg, s., and hassabis, d. (2017). building
machines that learn and think for themselves. behavioral and brain sciences, 40.

bousmalis, k., irpan, a., wohlhart, p., bai, y., kelcey, m., kalakrishnan, m., downs, l., ibarz, j.,
pastor, p., konolige, k., levine, s., and vanhoucke, v. (2017). using simulation and domain
adaptation to improve ef   ciency of deep robotic grasping. arxiv e-prints.

bowling, m., burch, n., johanson, m., and tammelin, o. (2015). heads-up limit hold   em poker is

solved. science, 347(6218):145   149.

boyd, s. and vandenberghe, l. (2004). id76. cambridge university press.

bradtke, s. j. and barto, a. g. (1996). linear least-squares algorithms for temporal difference

learning. machine learning, 22(1-3):33   57.

brandt, m. w., goyal, a., santa-clara, p., and stroud, j. r. (2005). a simulation approach to
dynamic portfolio choice with an application to learning about return predictability. the review
of financial studies, 18(3):831   873.

briot, j.-p., hadjeres, g., and pachet, f. (2017). deep learning techniques for music generation -

a survey. arxiv e-prints.

browne, c., powley, e., whitehouse, d., lucas, s., cowling, p. i., rohlfshagen, p., tavener, s.,
perez, d., samothrakis, s., and colton, s. (2012). a survey of id169 methods.
ieee transactions on computational intelligence and ai in games, 4(1):1   43.

brunner, g., richter, o., wang, y., and wattenhofer, r. (2018). teaching a machine to read maps

with deep id23. in the aaai conference on arti   cial intelligence (aaai).

57

busoniu, l., babuska, r., and schutter, b. d. (2008). a comprehensive survey of multiagent rein-
forcement learning. ieee transactions on systems, man, and cybernetics - part c: applications
and reviews, 38(2).

cai, j., shin, r., and song, d. (2017). making neural programming architectures generalize via

recursion. in the international conference on learning representations (iclr).

caicedo, j. c. and lazebnik, s. (2015). active object localization with deep id23.

in the ieee international conference on id161 (iccv).

cao, q., lin, l., shi, y., liang, x., and li, g. (2017). attention-aware face hallucination via deep
id23. in the ieee conference on id161 and pattern recognition
(cvpr).

carleo, g. and troyer, m. (2017). solving the quantum many-body problem with arti   cial neural

networks. science, 355(6325):602   606.

carlini, n. and wagner, d. (2017a). adversarial examples are not easily detected: bypassing ten

detection methods. in acm ccs 2017 workshop on arti   cial intelligence and security.

carlini, n. and wagner, d. (2017b). towards evaluating the robustness of neural networks. in ieee

symposium on security and privacy.

celikyilmaz, a., deng, l., li, l., and wang, c. (2017). scaffolding networks: incremental learn-

ing and teaching through questioning. arxiv e-prints.

chakraborty, b. and murphy, s. a. (2014). dynamic treatment regimes. annual review of statistics

and its application, 1:447   464.

chebotar, y., hausman, k., zhang, m., sukhatme, g., schaal, s., and levine, s. (2017). com-
bining model-based and model-free updates for trajectory-centric id23. in the
international conference on machine learning (icml).

chebotar, y., kalakrishnan, m., yahya, a., li, a., schaal, s., and levine, s. (2016). path integral

guided policy search. arxiv e-prints.

chen, j., huang, p.-s., he, x., gao, j., and deng, l. (2016). unsupervised learning of predictors

from unpaired input-output samples. arxiv e-prints.

chen, x., duan, y., houthooft, r., schulman, j., sutskever, i., and abbeel, p. (2016a). infogan:
interpretable representation learning by information maximizing generative adversarial nets. in
the annual conference on neural information processing systems (nips).

chen, y.-n., hakkani-tur, d., tur, g., celikyilmaz, a., gao, j., and deng, l. (2016b). knowledge

as a teacher: knowledge-guided structural attention networks. arxiv e-prints.

chen, y.-n. v., hakkani-t  ur, d., tur, g., gao, j., and deng, l. (2016c). end-to-end memory
in annual

networks with knowledge carryover for multi-turn spoken language understanding.
meeting of the international speech communication association (interspeech).

chen, z. and liu, b. (2016). lifelong machine learning. morgan & claypool publishers.

chen, z. and yi, d. (2017). the game imitation: deep supervised convolutional networks for

quick video game ai. arxiv e-prints.

cheng, y., xu, w., he, z., he, w., wu, h., sun, m., and liu, y. (2016). semi-supervised learning
for id4. in the association for computational linguistics annual meeting
(acl).

cho, k. (2015). natural language understanding with distributed representation. arxiv e-prints.

cho, k., van merrienboer, b., gulcehre, c., bougares, f., schwenk, h., and bengio, y. (2014).
learning phrase representations using id56 encoder-decoder for id151.
in conference on empirical methods in natural language processing (emnlp).

58

choi, e., hewlett, d., polosukhin, i., lacoste, a., uszkoreit, j., and berant, j. (2017). coarse-to-   ne
id53 for long documents. in the association for computational linguistics annual
meeting (acl).

christiano, p., leike, j., brown, t. b., martic, m., legg, s., and amodei, d. (2017). deep rein-
forcement learning from human preferences. in the annual conference on neural information
processing systems (nips).

chung, j., gulcehre, c., cho, k., and bengio, y. (2014). empirical evaluation of gated recurrent
neural networks on sequence modeling. in nips 2014 deep learning and representation learn-
ing workshop.

crawford, d., levit, a., ghadermarzy, n., oberoi, j. s., and ronagh, p. (2016). reinforcement

learning using quantum id82s. arxiv e-prints.

czarnecki, w. m.,   swirszcz, g., jaderberg, m., osindero, s., vinyals, o., and kavukcuoglu, k.

(2017). understanding synthetic gradients and decoupled neural interfaces. arxiv e-prints.

dai, h., khalil, e. b., zhang, y., dilkina, b., and song, l. (2017). learning combinatorial opti-
mization algorithms over graphs. in the annual conference on neural information processing
systems (nips).

dai, z., yang, z., yang, f., cohen, w. w., and salakhutdinov, r. (2017). good semi-supervised

learning that requires a bad gan. arxiv e-prints.

daniely, a., frostig, r., and singer, y. (2016). toward deeper understanding of neural networks:
the power of initialization and a dual view on expressivity. in the annual conference on neural
information processing systems (nips).

danihelka, i., wayne, g., uria, b., kalchbrenner, n., and graves, a. (2016). associative long

short-term memory. in the international conference on machine learning (icml).

das, a., kottur, s., moura, j. m. f., lee, s., and batra, d. (2017). learning cooperative visual dialog
in the ieee international conference on computer

agents with deep id23.
vision (iccv).

de asis, k., hernandez-garcia, j. f., zacharias holland, g., and sutton, r. s. (2018). multi-step
id23: a unifying algorithm. in the aaai conference on arti   cial intelligence
(aaai).

deisenroth, m. p., neumann, g., and peters, j. (2013). a survey on policy search for robotics.

foundations and trend in robotics, 2:1   142.

deisenroth, m. p. and rasmussen, c. e. (2011). pilco: a model-based and data-ef   cient approach

to policy search. in the international conference on machine learning (icml).

delle fave, f. m., jiang, a. x., yin, z., zhang, c., tambe, m., kraus, s., and sullivan, j. p. (2014).
game-theoretic security patrolling with dynamic execution uncertainty and a case study on a real
transit system. 50:321   367.

deng, l.

(2017).

frontiers conference.

ai
li-deng-three-generations-of-spoken-dialogue-systems-bots.

talk at
https://www.slideshare.net/aifrontiers/

spoken dialogue systems

(bots),

three generations of

deng, l. and dong, y. (2014). deep learning: methods and applications. now publishers inc.

deng, l. and li, x. (2013). machine learning paradigms for id103: an overview. ieee

transactions on audio, speech, and language processing, 21(5):1060   1089.

deng, l. and liu, y. (2017). deep learning in natural language processing (edited book, sched-

uled august 2017). springer.

deng, y., bao, f., kong, y., ren, z., and dai, q. (2016). deep direct id23 for
   nancial signal representation and trading. ieee transactions on neural networks and learning
systems.

59

denil, m., agrawal, p., kulkarni, t. d., erez, t., battaglia, p., and de freitas, n. (2017). learning
to perform physics experiments via deep id23. in the international conference
on learning representations (iclr).

denil, m., g  omez colmenarejo, s., cabi, s., saxton, d., and de freitas, n. (2017). programmable

agents. arxiv e-prints.

devrim kaba, m., gokhan uzunbas, m., and nam lim, s. (2017). a id23 ap-
proach to the view planning problem. in the ieee conference on id161 and pattern
recognition (cvpr).

dhingra, b., li, l., li, x., gao, j., chen, y.-n., ahmed, f., and deng, l. (2017). end-to-end
id23 of dialogue agents for information access. in the association for compu-
tational linguistics annual meeting (acl).

diederik p kingma, m. w. (2014). auto-encoding id58. in the international conference

on learning representations (iclr).

dietterich, t. g. (2000). hierarchical id23 with the maxq value function de-

composition. journal of arti   cial intelligence research, 13(1):227   303.

domingos, p. (2012). a few useful things to know about machine learning. communications of the

acm, 55(10):78   87.

dong, d., wu, h., he, w., yu, d., and wang, h. (2015). id72 for multiple language

translation. in the association for computational linguistics annual meeting (acl).

dong, y., su, h., zhu, j., and zhang, b. (2017). improving interpretability of deep neural networks
with semantic information. in the ieee conference on id161 and pattern recognition
(cvpr).

doshi-velez, f. and kim, b. (2017). towards a rigorous science of interpretable machine learn-

ing. arxiv e-prints.

dosovitskiy, a. and koltun, v. (2017). learning to act by predicting the future. in the international

conference on learning representations (iclr).

downey, c., hefny, a., li, b., boots, b., and gordon, g. (2017). predictive state recurrent neural

networks. in the annual conference on neural information processing systems (nips).

du, s. s., chen, j., li, l., xiao, l., and zhou, d. (2017). stochastic variance reduction methods for

policy evaluation. in the international conference on machine learning (icml).

duan, y., andrychowicz, m., stadie, b. c., ho, j., schneider, j., sutskever, i., abbeel, p., and
zaremba, w. (2017). one-shot imitation learning. in the annual conference on neural informa-
tion processing systems (nips).

duan, y., chen, x., houthooft, r., schulman, j., and abbeel, p. (2016). benchmarking deep rein-
forcement learning for continuous control. in the international conference on machine learning
(icml).

duan, y., schulman, j., chen, x., bartlett, p. l., sutskever, i., and abbeel, p. (2016). rl2: fast

id23 via slow id23. arxiv e-prints.

dulac-arnold, g., evans, r., van hasselt, h., sunehag, p., lillicrap, t., hunt, j., mann, t., weber,
t., degris, t., and coppin, b. (2015). deep id23 in large discrete action
spaces. arxiv e-prints.

el-tantawy, s., abdulhai, b., and abdelgawad, h. (2013). multiagent id23 for
integrated network of adaptive traf   c signal controllers (marlin-atsc): methodology and large-
scale application on downtown toronto. ieee transactions on intelligent transportation systems,
14(3):1140   1150.

eric, m. and manning, c. d. (2017). a copy-augmented sequence-to-sequence architecture gives

good performance on task-oriented dialogue. arxiv e-prints.

60

ernst, d., geurts, p., and wehenkel, l. (2005). tree-based batch mode id23. the

journal of machine learning research, 6:503   556.

eslami, s. m. a., heess, n., weber, t., tassa, y., szepesv  ari, d., kavukcuoglu, k., and hinton,
in the

g. e. (2016). attend, infer, repeat: fast scene understanding with generative models.
annual conference on neural information processing systems (nips).

evans, r. and grefenstette, e. (2017). learning explanatory rules from noisy data. arxiv e-prints.

evtimov, i., eykholt, k., fernandes, e., kohno, t., li, b., prakash, a., rahmati, a., and song, d.

(2017). robust physical-world attacks on deep learning models. arxiv e-prints.

fang, m., li, y., and cohn, t. (2017). learning how to active learn: a deep id23

approach. in conference on empirical methods in natural language processing (emnlp).

fang, x., misra, s., xue, g., and yang, d. (2012). smart grid - the new and improved power grid:

a survey. ieee communications surveys tutorials, 14(4):944   980.

fatemi, m., asri, l. e., schulz, h., he, j., and suleman, k. (2016). policy networks with two-
stage training for dialogue systems. in the annual sigdial meeting on discourse and dialogue
(sigdial).

feng, j. and zhou, z.-h. (2017). autoencoder by forest. arxiv e-prints.

fernando, c., banarse, d., blundell, c., zwols, y., ha, d., rusu, a. a., pritzel, a., and wierstra,
d. (2017). pathnet: evolution channels id119 in super neural networks. arxiv
e-prints.

finn, c., christiano, p., abbeel, p., and levine, s. (2016a). a connection between gans, inverse
id23, and energy-based models. in nips 2016 workshop on adversarial train-
ing.

finn, c. and levine, s. (2016). deep visual foresight for planning robot motion. in ieee interna-

tional conference on robotics and automation (icra).

finn, c., levine, s., and abbeel, p. (2016b). guided cost learning: deep inverse optimal control via

policy optimization. in the international conference on machine learning (icml).

finn, c., yu, t., fu, j., abbeel, p., and levine, s. (2017). generalizing skills with semi-supervised

id23. in the international conference on learning representations (iclr).

firoiu, v., whitney, w. f., and tenenbaum, j. b. (2017). beating the world   s best at super smash

bros. with deep id23. arxiv e-prints.

florensa, c., duan, y., and abbeel, p. (2017). stochastic neural networks for hierarchical reinforce-

ment learning. in the international conference on learning representations (iclr).

foerster, j., assael, y. m., de freitas, n., and whiteson, s. (2016). learning to communicate
with deep multi-agent id23. in the annual conference on neural information
processing systems (nips).

foerster, j., farquhar, g., afouras, t., nardelli, n., and whiteson, s. (2018). counterfactual multi-

agent policy gradients. in the aaai conference on arti   cial intelligence (aaai).

foerster, j., nardelli, n., farquhar, g., torr, p. h. s., kohli, p., and whiteson, s. (2017). stabilising
experience replay for deep multi-agent id23. in the international conference
on machine learning (icml).

foerster, j. n., chen, r. y., al-shedivat, m., whiteson, s., abbeel, p., and mordatch, i. (2017).

learning with opponent-learning awareness. arxiv e-prints.

fortunato, m., gheshlaghi azar, m., piot, b., menick, j., osband, i., graves, a., mnih, v., munos,
r., hassabis, d., pietquin, o., blundell, c., and legg, s. (2017). noisy networks for exploration.
arxiv e-prints.

61

fu, j., co-reyes, j. d., and levine, s. (2017). ex2: exploration with exemplar models for deep
in the annual conference on neural information processing systems

id23.
(nips).

ganin, y., ustinova, e., ajakan, h., germain, p., larochelle, h., laviolette, f., marchand, m.,
and lempitsky, v. (2016). domain-adversarial training of neural networks. journal of machine
learning research, 17(59):1   35.

garc`  a, j. and fern`andez, f. (2015). a comprehensive survey on safe id23. the

journal of machine learning research, 16:1437   1480.

gavrilovska, l., atanasovski, v., macaluso, i., and dasilva, l. a. (2013). learning and reasoning

in cognitive radio networks. ieee communications surveys tutorials, 15(4):1761   1777.

gehring, j., auli, m., grangier, d., yarats, d., and dauphin, y. n. (2017). convolutional sequence

to sequence learning. arxiv e-prints.

gelly, s., schoenauer, m., sebag, m., teytaud, o., kocsis, l., silver, d., and szepesv  ari, c. (2012).
the grand challenge of computer go: id169 and extensions. communications
of the acm, 55(3):106   113.

gelly, s. and silver, d. (2007). combining online and of   ine knowledge in uct. in the international

conference on machine learning (icml).

george, d., lehrach, w., kansky, k., l  azaro-gredilla, m., laan, c., marthi, b., lou, x., meng, z.,
liu, y., wang, h., lavin, a., and phoenix, d. s. (2017). a generative vision model that trains
with high data ef   ciency and breaks text-based captchas. science.

geramifard, a., dann, c., klein, r. h., dabney, w., and how, j. p. (2015). rlpy: a value-function-
based id23 framework for education and research. journal of machine learning
research, 16:1573   1578.

geramifard, a., walsh, t. j., tellex, s., chowdhary, g., roy, n., and how, j. p. (2013). a tutorial on
linear function approximators for id145 and id23. foundations
and trends in machine learning, 6(4):375   451.

ghavamzadeh, m., mahadevan, s., and makar, r. (2006). hierarchical multi-agent reinforcement

learning. autonomous agents and multi-agent systems, 13(2):197   229.

ghavamzadeh, m., mannor, s., pineau, j., and tamar, a. (2015). bayesian id23:

a survey. foundations and trends in machine learning, 8(5-6):359   483.

girshick, r. (2015). fast r-id98.

(iccv).

in the ieee international conference on id161

glavic, m., fonteneau, r., and ernst, d. (2017). id23 for electric power system
decision and control: past considerations and perspectives. in the 20th world congress of the
international federation of automatic control.

goldberg, y. (2017). neural network methods for natural language processing. morgan & clay-

pool publishers.

goldberg, y. and kosorok, m. r. (2012). id24 with censored data. annals of statistics,

40(1):529   560.

goodfellow, i. (2017). nips 2016 tutorial: id3. arxiv e-prints.

goodfellow, i., bengio, y., and courville, a. (2016). deep learning. mit press.

goodfellow, i., pouget-abadie, j., mirza, m., xu, b., warde-farley, d., ozair, s., courville, a., ,
and bengio, y. (2014). generative adversarial nets. in the annual conference on neural infor-
mation processing systems (nips), page 2672?2680.

graves, a., bellemare, m. g., menick, j., munos, r., and kavukcuoglu, k. (2017). automated

curriculum learning for neural networks. arxiv e-prints.

62

graves, a., wayne, g., and danihelka, i. (2014). id63s. arxiv e-prints.

graves, a., wayne, g., reynolds, m., harley, t., danihelka, i., grabska-barwi  nska, a., col-
menarejo, s. g., grefenstette, e., ramalho, t., agapiou, j., nech badia, a. p., hermann, k. m.,
zwols, y., ostrovski, g., cain, a., king, h., summer   eld, c., blunsom, p., kavukcuoglu, k., and
hassabis, d. (2016). hybrid computing using a neural network with dynamic external memory.
nature, 538:471   476.

gregor, k., danihelka, i., graves, a., rezende, d., and wierstra, d. (2015). draw: a recurrent
in the international conference on machine learning

neural network for image generation.
(icml).

grondman, i., busoniu, l., lopes, g. a., and babu  ska, r. (2012). a survey of actor-critic rein-
forcement learning: standard and natural policy gradients. ieee transactions on systems, man,
and cybernetics, part c (applications and reviews), 42(6):1291   1307.

gruslys, a., gheshlaghi azar, m., bellemare, m. g., and munos, r. (2017). the reactor: a

sample-ef   cient actor-critic architecture. arxiv e-prints.

gu, s., holly, e., lillicrap, t., and levine, s. (2016a). deep id23 for robotic

manipulation with asynchronous off-policy updates. arxiv e-prints.

gu, s., lillicrap, t., ghahramani, z., turner, r. e., and levine, s. (2017). q-prop: sample-
ef   cient policy gradient with an off-policy critic. in the international conference on learning
representations (iclr).

gu, s., lillicrap, t., ghahramani, z., turner, r. e., sch  olkopf, b., and levine, s. (2017). interpo-
lated policy gradient: merging on-policy and off-policy gradient estimation for deep reinforce-
ment learning. in the annual conference on neural information processing systems (nips).

gu, s., lillicrap, t., sutskever, i., and levine, s. (2016b). continuous deep id24 with model-

based acceleration. in the international conference on machine learning (icml).

gulcehre, c., chandar, s., cho, k., and bengio, y. (2016). dynamic id63 with

soft and hard addressing schemes. arxiv e-prints.

gulrajani, i., ahmed, f., arjovsky, m., dumoulin, v., and courville, a. (2017). improved training
of wasserstein gans. in the annual conference on neural information processing systems (nips).

gupta, a., devin, c., liu, y., abbeel, p., and levine, s. (2017a). learning invariant feature spaces
to transfer skills with id23. in the international conference on learning rep-
resentations (iclr).

gupta, s., davidson, j., levine, s., sukthankar, r., and malik, j. (2017b). cognitive mapping
in the ieee conference on id161 and pattern

and planning for visual navigation.
recognition (cvpr).

guu, k., pasupat, p., liu, e. z., and liang, p. (2017). from language to programs: bridging
id23 and maximum marginal likelihood. in the association for computational
linguistics annual meeting (acl).

haarnoja, t., tang, h., abbeel, p., and levine, s. (2017). id23 with deep energy-

based policies. in the international conference on machine learning (icml).

had   eld-menell, d., dragan, a., abbeel, p., and russell, s. (2016). cooperative inverse reinforce-

ment learning. in the annual conference on neural information processing systems (nips).

had   eld-menell, d., milli, s., abbeel, p., russell, s., and dragan, a. (2017). inverse reward design.

in the annual conference on neural information processing systems (nips).

han, s., mao, h., and dally, w. j. (2016). deep compression: compressing deep neural net-
works with pruning, trained quantization and huffman coding. in the international conference
on learning representations (iclr).

63

harrison, b., ehsan, u., and riedl, m. o. (2017). rationalization: a id4

approach to generating natural language explanations. arxiv e-prints.

harutyunyan, a., vrancx, p., bacon, p.-l., precup, d., and nowe, a. (2018). learning with options

that terminate off-policy. in the aaai conference on arti   cial intelligence (aaai).

hassabis, d., kumaran, d., summer   eld, c., and botvinick, m. (2017). neuroscience-inspired

arti   cial intelligence. neuron, 95:245   258.

hastie, t., tibshirani, r., and friedman, j. (2009). the elements of statistical learning: data

mining, id136, and prediction. springer.

hausknecht, m. and stone, p. (2015). deep recurrent id24 for partially observable mdps. in

the aaai conference on arti   cial intelligence (aaai).

hausknecht, m. and stone, p. (2016). deep id23 in parameterized action space.

in the international conference on learning representations (iclr).

haykin, s. (2005). cognitive radio: brain-empowered wireless communications. ieee journal on

selected areas in communications, 23(2):201   220.

haykin, s. (2008). neural networks and learning machines (third edition). prentice hall.

he, d., xia, y., qin, t., wang, l., yu, n., liu, t.-y., and ma, w.-y. (2016a). dual learning
for machine translation. in the annual conference on neural information processing systems
(nips).

he, f. s., liu, y., schwing, a. g., and peng, j. (2017). learning to play in a day: faster deep
in the international conference on learning

id23 by optimality tightening.
representations (iclr).

he, j., chen, j., he, x., gao, j., li, l., deng, l., and ostendorf, m. (2016b). deep reinforcement
learning with a natural language action space. in the association for computational linguistics
annual meeting (acl).

he, j., ostendorf, m., he, x., chen, j., gao, j., li, l., and deng, l. (2016c). deep reinforcement
learning with a combinatorial action space for predicting popular reddit threads. in conference
on empirical methods in natural language processing (emnlp).

he, k., gkioxari, g., doll  ar, p., and girshick, r. (2017). mask r-id98. in the ieee international

conference on id161 (iccv).

he, k., zhang, x., ren, s., and sun, j. (2016d). deep residual learning for image recognition. in

the ieee conference on id161 and pattern recognition (cvpr).

he, l., lee, k., lewis, m., and zettlemoyer, l. (2017). deep id14: what works

and what   s next. in the association for computational linguistics annual meeting (acl).

he, x. and deng, l. (2013). speech-centric information processing: an optimization-oriented

approach. proceedings of the ieee     vol. 101, no. 5, may 2013, 101(5):1116   1135.

heaton, j. b., polson, n. g., and witte, j. h. (2016). deep learning for    nance: deep portfolios.

applied stochastic models in business and industry.

heess, n., tb, d., sriram, s., lemmon, j., merel, j., wayne, g., tassa, y., erez, t., wang, z.,
eslami, a., riedmiller, m., and silver, d. (2017). emergence of locomotion behaviours in rich
environments. arxiv e-prints.

hein, d., depeweg, s., tokic, m., udluft, s., hentschel, a., runkler, t. a., and sterzing, v. (2017).
in ieee symposium on

a benchmark environment motivated by industrial control problems.
adaptive id145 and id23 (ieee adprl   17).

heinrich, j. and silver, d. (2016). deep id23 from self-play in imperfect-

information games. in nips 2016 deep id23 workshop.

64

held, d., geng, x., florensa, c., and abbeel, p. (2017). automatic goal generation for reinforce-

ment learning agents. arxiv e-prints.

henaff, m., whitney, w. f., and lecun, y. (2017). model-based planning in discrete action

spaces. arxiv e-prints.

hessel, m., modayil, j., van hasselt, h., schaul, t., ostrovski, g., dabney, w., horgan, d., piot,
b., azar, m., and silver, d. (2018). rainbow: combining improvements in deep reinforcement
learning. in the aaai conference on arti   cial intelligence (aaai).

hester, t. and stone, p. (2017). intrinsically motivated model learning for developing curious robots.

arti   cial intelligence, 247:170   86.

hester, t., vecerik, m., pietquin, o., lanctot, m., schaul, t., piot, b., horgan, d., quan, j.,
sendonaris, a., dulac-arnold, g., osband, i., agapiou, j., leibo, j. z., and gruslys, a. (2018).
deep id24 from demonstrations. in the aaai conference on arti   cial intelligence (aaai).

higgins, i., matthey, l., pal, a., burgess, c., glorot, x., botvinick, m., mohamed, s., and lerchner,
a. (2017).   -vae: learning basic visual concepts with a constrained variational framework. in
the international conference on learning representations (iclr).

hinton, g., deng, l., yu, d., dahl, g. e., rahman mohamed, a., jaitly, n., senior, a., vanhoucke,
v., nguyen, p., sainath, t. n., , and kingsbury, b. (2012). deep neural networks for acoustic
modeling in id103. ieee signal processing magazine, 82.

hinton, g. e. and salakhutdinov, r. r. (2006). reducing the dimensionality of data with neural

networks. science, 313(5786):504   507.

hirschberg, j. and manning, c. d. (2015). advances in natural language processing. science,

349(6245):261   266.

ho, j. and ermon, s. (2016). generative adversarial imitation learning. in the annual conference

on neural information processing systems (nips).

ho, j., gupta, j. k., and ermon, s. (2016). model-free imitation learning with policy optimization.

in the international conference on machine learning (icml).

hochreiter, s. and schmidhuber, j. (1997). long short-term memory. neural computation, 9:1735   

1780.

hoshen, y. (2017). vain: attentional multi-agent predictive modeling. in the annual conference

on neural information processing systems (nips).

houthooft, r., chen, x., duan, y., schulman, j., turck, f. d., and abbeel, p. (2016). vime:
variational information maximizing exploration. in the annual conference on neural information
processing systems (nips).

hu, z., yang, z., salakhutdinov, r., and xing, e. p. (2017). on unifying deep generative models.

arxiv e-prints.

huang, g., liu, z., weinberger, k. q., and van der maaten, l. (2017). densely connected convolu-
tional networks. in the ieee conference on id161 and pattern recognition (cvpr).

huang, s., papernot, n., goodfellow, i., duan, y., and abbeel, p. (2017). adversarial attacks on

neural network policies. arxiv e-prints.

huk park, d., hendricks, l. a., akata, z., schiele, b., darrell, t., and rohrbach, m. (2016). atten-

tive explanations: justifying decisions and pointing to the evidence. arxiv e-prints.

hull, j. c. (2014). options, futures and other derivatives (9th edition). prentice hall.

ian j. goodfellow, jonathon shlens, c. s. (2015). explaining and harnessing adversarial examples.

in the international conference on learning representations (iclr).

65

ioffe, s. and szegedy, c. (2015). batch id172: accelerating deep network training by
reducing internal covariate shift. in the international conference on machine learning (icml).

islam, r., henderson, p., gomrokchi, m., and precup, d. (2017). reproducibility of benchmarked
deep id23 tasks for continuous control. in icml 2017 reproducibility in ma-
chine learning workshop.

jaderberg, m., dalibard, v., osindero, s., czarnecki, w. m., donahue, j., razavi, a., vinyals, o.,
green, t., dunning, i., simonyan, k., fernando, c., and kavukcuoglu, k. (2017). population
based training of neural networks. arxiv e-prints.

jaderberg, m., mnih, v., czarnecki, w., schaul, t., leibo, j. z., silver, d., and kavukcuoglu, k.
(2017). id23 with unsupervised auxiliary tasks. in the international confer-
ence on learning representations (iclr).

jaderberg, m., simonyan, k., zisserman, a., and kavukcuoglu, k. (2015). spatial transformer

networks. in the annual conference on neural information processing systems (nips).

james, g., witten, d., hastie, t., and tibshirani, r. (2013). an introduction to statistical learning

with applications in r. springer.

jaques, n., gu, s., turner, r. e., and eck, d. (2017). tuning recurrent neural networks with

id23. submitted to int   l conference on learning representations.

jiang, n., krishnamurthy, a., agarwal, a., langford, j., and schapire, r. e. (2016). contextual

decision processes with low bellman rank are pac-learnable. arxiv e-prints.

jie, z., liang, x., feng, j., jin, x., lu, w. f., and yan, s. (2016). tree-structured reinforcement
in the annual conference on neural information

learning for sequential object localization.
processing systems (nips).

johansson, f. d., shalit, u., and sontag, d. (2016). learning representations for counterfactual

id136. in the international conference on machine learning (icml).

johnson, m., schuster, m., le, q. v., krikun, m., wu, y., chen, z., thorat, n., vi  egas, f., watten-
berg, m., corrado, g., hughes, m., and dean, j. (2016). google   s multilingual neural machine
translation system: enabling zero-shot translation. arxiv e-prints.

jordan, m. i. and mitchell, t. (2015). machine learning: trends, perspectives, and prospects. sci-

ence, 349(6245):255   260.

joulin, a., grave, e., bojanowski, p., and mikolov, t. (2017). bag of tricks for ef   cient text clas-
si   cation. in proceedings of the 15th conference of the european chapter of the association for
computational linguistics (eacl).

jurafsky, d. and martin, j. h. (2017). speech and language processing (3rd ed. draft). prentice

hall.

justesen, n., bontrager, p., togelius, j., and risi, s. (2017). deep learning for video game playing.

arxiv e-prints.

justesen, n. and risi, s. (2017). learning macromanagement in starcraft from replays using deep

learning. in ieee conference on computational intelligence and games (cig).

kadlec, r., schmid, m., bajgar, o., and kleindienst, j. (2016). text understanding with the atten-

tion sum reader network. arxiv e-prints.

kaelbling, l. p., littman, m. l., and moore, a. (1996). id23: a survey. journal

of arti   cial intelligence research, 4:237   285.

kaiser, l. and bengio, s. (2016). can active memory replace attention? in the annual conference

on neural information processing systems (nips).

kaiser, l., gomez, a. n., shazeer, n., vaswani, a., parmar, n., jones, l., and uszkoreit, j. (2017a).

one model to learn them all. arxiv e-prints.

66

kaiser,   ., nachum, o., roy, a., and bengio, s. (2017b). learning to remember rare events. in

the international conference on learning representations (iclr).

kakade, s. (2002). a natural policy gradient. in the annual conference on neural information

processing systems (nips).

kalchbrenner, n. and blunsom, p. (2013). recurrent continuous translation models. in conference

on empirical methods in natural language processing (emnlp).

kandasamy, k., bachrach, y., tomioka, r., tarlow, d., and carter, d. (2017). batch policy gradient
methods for improving neural conversation models. in the international conference on learning
representations (iclr).

kansky, k., silver, t., m  ely, d. a., eldawy, m., l  azaro-gredilla, m., lou, x., dorfman, n., sidor,
s., phoenix, s., and george, d. (2017). schema networks: zero-shot transfer with a generative
causal model of intuitive physics. in the international conference on machine learning (icml).

karpathy, a., johnson, j., and fei-fei, l. (2016). visualizing and understanding recurrent networks.

in iclr 2016 workshop.

kavosh and littman, m. l. (2017). a new softmax operator for id23.

international conference on machine learning (icml).

in the

kawaguchi, k., pack kaelbling, l., and bengio, y. (2017). generalization in deep learning. arxiv

e-prints.

kempka, m., wydmuch, m., runc, g., toczek, j., and jas  kowski, w. (2016). vizdoom: a doom-
based ai research platform for visual id23. in ieee conference on computa-
tional intelligence and games.

khandani, a. e., kim, a. j., and lo, a. w. (2010). consumer credit-risk models via machine-

learning algorithms. journal of banking & finance, 34:2767   2787.

killian, t., daulton, s., konidaris, g., and doshi-velez, f. (2017). robust and ef   cient transfer
learning with hidden-parameter id100. in the annual conference on neural
information processing systems (nips).

kim, b., massoud farahmand, a., pineau, j., and precup, d. (2014). learning from limited demon-

strations. in the annual conference on neural information processing systems (nips).

kingma, d. p., rezende, d. j., mohamed, s., and welling, m. (2014). semi-supervised learning with
deep generative models. in the annual conference on neural information processing systems
(nips).

kirkpatrick, j., pascanu, r., rabinowitz, n., veness, j., desjardins, g., rusu, a. a., milan, k.,
quan, j., ramalho, t., grabska-barwinska, a., hassabis, d., clopath, c., kumaran, d., and
hadsell, r. (2017). overcoming catastrophic forgetting in neural networks. pnas, 114(13):3521   
3526.

klambauer, g., unterthiner, t., mayr, a., and hochreiter, s. (2017). self-normalizing neural

networks. arxiv e-prints.

klein, g., kim, y., deng, y., senellart, j., and rush, a. m. (2017). openid4: open-source toolkit

for id4. arxiv e-prints.

kober, j., bagnell, j. a., and peters, j. (2013). id23 in robotics: a survey.

international journal of robotics research, 32(11):1238   1278.

koch, g., zemel, r., and salakhutdinov, r. (2015). siamese neural networks for one-shot image

recognition. in the international conference on machine learning (icml).

koh, p. w. and liang, p. (2017). understanding black-box predictions via in   uence functions. in

the international conference on machine learning (icml).

67

kompella, v. r., stollenga, m., luciw, m., and schmidhuber, j. (2017). continual curiosity-driven
skill acquisition from high-dimensional video inputs for humanoid robots. arti   cial intelligence,
247:313   335.

kong, x., xin, b., wang, y., and hua, g. (2017). collaborative deep id23 for
joint object search. in the ieee conference on id161 and pattern recognition (cvpr).

kosorok, m. r. and moodie, e. e. m. (2015). adaptive treatment strategies in practice: plan-
ning trials and analyzing data for personalized medicine. asa-siam series on statistics and
applied id203.

kottur, s., moura, j. m., lee, s., and batra, d. (2017). natural language does not emerge    naturally   
in conference on empirical methods in natural language processing

in multi-agent dialog.
(emnlp).

krakovsky, m. (2016). reinforcement renaissance. communications of the acm, 59(8):12   14.

krizhevsky, a., sutskever, i., and hinton, g. e. (2012). id163 classi   cation with deep convo-
lutional neural networks. in the annual conference on neural information processing systems
(nips).

krull, a., brachmann, e., nowozin, s., michel, f., shotton, j., and rother, c. (2017). poseagent:
budget-constrained 6d object pose estimation via id23. in the ieee conference
on id161 and pattern recognition (cvpr).

kuhn, m. and johnson, k. (2013). applied predictive modeling. springer.

kulkarni, t. d., narasimhan, k. r., saeedi, a., and tenenbaum, j. b. (2016). hierarchical deep
id23: integrating temporal abstraction and intrinsic motivation. in the annual
conference on neural information processing systems (nips).

kulkarni, t. d., whitney, w., kohli, p., and tenenbaum, j. b. (2015). deep convolutional inverse
graphics network. in the annual conference on neural information processing systems (nips).

lagoudakis, m. g. and parr, r. (2003). least-squares policy iteration. the journal of machine

learning research, 4:1107     1149.

lake, b. m., salakhutdinov, r., and tenenbaum, j. b. (2015). human-level concept learning through

probabilistic program induction. science, 350(6266):1332   1338.

lake, b. m., ullman, t. d., tenenbaum, j. b., and gershman, s. j. (2016). building machines that

learn and think like people. behavioral and brain sciences, 24:1   101.

lamb, a., goyal, a., zhang, y., zhang, s., courville, a., and bengio, y. (2016). professor forcing:
a new algorithm for training recurrent networks. in the annual conference on neural information
processing systems (nips).

lample, g. and chaplot, d. s. (2017). playing fps games with deep id23. in the

aaai conference on arti   cial intelligence (aaai).

lanctot, m., zambaldi, v., gruslys, a., lazaridou, a., tuyls, k., perolat, j., silver, d., and graepel,
t. (2017). a uni   ed game-theoretic approach to multiagent id23. in the annual
conference on neural information processing systems (nips).

le, q. v., ranzato, m., monga, r., devin, m., chen, k., corrado, g. s., dean, j., and ng, a. y.
(2012). building high-level features using large scale unsupervised learning. in the international
conference on machine learning (icml).

lecun, y., bengio, y., and hinton, g. (2015). deep learning. nature, 521:436   444.

lee, a. x., levine, s., and abbeel, p. (2017). learning visual servoing with deep features and trust

region    tted q-iteration. in the international conference on learning representations (iclr).

lehman, j., chen, j., clune, j., and stanley, k. o. (2017). safe mutations for deep and recurrent

neural networks through output gradients. arxiv e-prints.

68

lei, t., barzilay, r., and jaakkola, t. (2016). rationalizing neural predictions. in conference on

empirical methods in natural language processing (emnlp).

leibo, j. z., de masson d   autume, c., zoran, d., amos, d., beattie, c., anderson, k., garc    a
casta  neda, a., sanchez, m., green, s., gruslys, a., legg, s., hassabis, d., and botvinick, m. m.
(2018). psychlab: a psychology laboratory for deep id23 agents. arxiv
e-prints.

leibo, j. z., zambaldi, v., lanctot, m., marecki, j., and graepel, t. (2017). multi-agent reinforce-
in the international conference on autonomous

ment learning in sequential social dilemmas.
agents & multiagent systems (aamas).

levine, s., finn, c., darrell, t., and abbeel, p. (2016a). end-to-end training of deep visuomotor

policies. the journal of machine learning research, 17:1   40.

levine, s., pastor, p., krizhevsky, a., and quillen, d. (2016b). learning hand-eye coordination

for robotic grasping with deep learning and large-scale data collection. arxiv e-prints.

lewis, m., yarats, d., dauphin, y. n., parikh, d., and batra, d. (2017). deal or no deal? end-to-end

learning for negotiation dialogues. in fair.

leyton-brown, k. and shoham, y. (2008). essentials of game theory: a concise, multidisciplinary

introduction. morgan & claypool publishers.

li, j., miller, a. h., chopra, s., ranzato, m., and weston, j. (2017a). dialogue learning with

human-in-the-loop. in the international conference on learning representations (iclr).

li, j., miller, a. h., chopra, s., ranzato, m., and weston, j. (2017b). learning through dialogue
interactions by asking questions. in the international conference on learning representations
(iclr).

li, j., monroe, w., and jurafsky, d. (2016a). a simple, fast diverse decoding algorithm for neural

generation. arxiv e-prints.

li, j., monroe, w., and jurafsky, d. (2016b). understanding neural networks through representa-

tion erasure. arxiv e-prints.

li, j., monroe, w., and jurafsky, d. (2017a). learning to decode for future success. arxiv e-prints.

li, j., monroe, w., ritter, a., galley, m., gao, j., and jurafsky, d. (2016c). deep reinforcement
in conference on empirical methods in natural language

learning for dialogue generation.
processing (emnlp).

li, k. and malik, j. (2017). learning to optimize. in the international conference on learning

representations (iclr).

li, k. and malik, j. (2017). learning to optimize neural nets. arxiv e-prints.

li, l., chu, w., langford, j., and schapire, r. e. (2010). a contextual-bandit approach to person-
alized news article recommendation. in the international world wide web conference (www).

li, x., chen, y.-n., li, l., and gao, j. (2017b). end-to-end task-completion neural dialogue

systems. arxiv e-prints.

li, x., li, l., gao, j., he, x., chen, j., deng, l., and he, j. (2015). recurrent reinforcement

learning: a hybrid approach. arxiv e-prints.

li, x., lipton, z. c., dhingra, b., li, l., gao, j., and chen, y.-n. (2016d). a user simulator for

task-completion dialogues. arxiv e-prints.

li, y., song, j., and ermon, s. (2017). infogail: interpretable imitation learning from visual demon-

strations. in the annual conference on neural information processing systems (nips).

li, y., szepesv  ari, c., and schuurmans, d. (2009). learning exercise policies for american options.

in international conference on arti   cial intelligence and statistics (aistats09).

69

liang, c., berant, j., le, q., forbus, k. d., and lao, n. (2017a). neural symbolic machines: learn-
ing semantic parsers on freebase with weak supervision. in the association for computational
linguistics annual meeting (acl).

liang, c., berant, j., le, q., forbus, k. d., and lao, n. (2017b). neural symbolic machines: learn-
ing semantic parsers on freebase with weak supervision. in the association for computational
linguistics annual meeting (acl).

liang, e., liaw, r., nishihara, r., moritz, p., fox, r., gonzalez, j., goldberg, k., and stoica, i.
in nips 2017

(2017c). ray rllib: a composable and scalable id23 library.
deep id23 symposium.

liang, x., lee, l., and xing, e. p. (2017d). deep variation-structured id23 for
in the ieee conference on id161 and

visual relationship and attribute detection.
pattern recognition (cvpr).

liang, y., machado, m. c., talvitie, e., and bowling, m. (2016). state of the art control of atari
in the international conference on autonomous

games using shallow id23.
agents & multiagent systems (aamas).

lillicrap, t. p., hunt, j. j., pritzel, a., heess, n., erez, t., tassa, y., silver, d., and wierstra, d.
(2016). continuous control with deep id23. in the international conference on
learning representations (iclr).

lin, l.-j. (1992). self-improving reactive agents based on id23, planning and

teaching. machine learning, 8(3):293   321.

lin, z., gehring, j., khalidov, v., and synnaeve, g. (2017). stardata: a starcraft ai research dataset.

in aaai conference on arti   cial intelligence and interactive digital entertainment (aiide).

ling, y., hasan, s. a., datla, v., qadir, a., lee, k., liu, j., and farri, o. (2017). diagnostic infer-
encing via improving clinical concept extraction with deep id23: a preliminary
study. in machine learning for healthcare.

lipton, z. c. (2016). the mythos of model interpretability. arxiv e-prints.

lipton, z. c., gao, j., li, l., li, x., ahmed, f., and deng, l. (2016). ef   cient exploration for

dialogue policy learning with bbq networks & replay buffer spiking. arxiv e-prints.

littman, m. l. (2015). id23 improves behaviour from evaluative feedback. na-

ture, 521:445   451.

liu, b. (2012). id31 and opinion mining. morgan & claypool publishers.

liu, c. and tomizuka, m. (2016). algorithmic safety measures for intelligent industrial co-robots.

in ieee international conference on robotics and automation (icra).

liu, c. and tomizuka, m. (2017). designing the robot behavior for safe human robot interactions,
in trends in control and decision-making for human-robot collaboration systems (y. wang and
f. zhang (eds.)). springer.

liu, c., zoph, b., shlens, j., hua, w., li, l.-j., fei-fei, l., yuille, a., huang, j., and murphy, k.

(2017). progressive neural architecture search. arxiv e-prints.

liu, f., li, s., zhang, l., zhou, c., ye, r., wang, y., and lu, j. (2017). 3did98-id25-id56: a
deep id23 framework for id29 of large-scale 3d point clouds. in
the ieee international conference on id161 (iccv).

liu, h., simonyan, k., vinyals, o., fernando, c., and kavukcuoglu, k. (2017). hierarchical rep-

resentations for ef   cient architecture search. arxiv e-prints.

liu, n., li, z., xu, z., xu, j., lin, s., qiu, q., tang, j., and wang, y. (2017). a hierarchical frame-
work of cloud resource allocation and power management using deep id23. in
37th ieee international conference on distributed computing (icdcs 2017).

70

liu, s., zhu, z., ye, n., guadarrama, s., and murphy, k. (2016). improved image captioning via

policy gradient optimization of spider. arxiv e-prints.

liu, y., chen, j., and deng, l. (2017). unsupervised sequence classi   cation using sequential

output statistics. arxiv e-prints.

liu, y.-e., mandel, t., brunskill, e., and popovi  c, z. (2014). trading off scienti   c knowledge and

user learning with multi-armed bandits. in educational data mining (edm).

lo, a. w. (2004). the adaptive markets hypothesis: market ef   ciency from an evolutionary

perspective. journal of portfolio management, 30:15   29.

long, m., cao, y., wang, j., and jordan, m. i. (2015). learning transferable features with deep

adaptation networks. in the international conference on machine learning (icml).

long, m., cao, z., wang, j., and yu, p. s. (2017). learning multiple tasks with multilinear relation-

ship networks. in the annual conference on neural information processing systems (nips).

long, m., zhu, h., wang, j., and jordan, m. i. (2016). unsupervised id20 with
residual transfer networks. in the annual conference on neural information processing systems
(nips).

longstaff, f. a. and schwartz, e. s. (2001). valuing american options by simulation: a simple

least-squares approach. the review of financial studies, 14(1):113   147.

loos, s., irving, g., szegedy, c., and kaliszyk, c. (2017). deep network guided proof search.

arxiv e-prints.

lopez-paz, d. and ranzato, m. (2017). gradient episodic memory for continuum learning. arxiv

e-prints.

lowe, r., wu, y., tamar, a., harb, j., abbeel, p., and mordatch, i. (2017). multi-agent actor-critic
for mixed cooperative-competitive environments. in the annual conference on neural informa-
tion processing systems (nips).

lu, j., xiong, c., parikh, d., and socher, r. (2016). knowing when to look: adaptive attention

via a visual sentinel for image captioning. arxiv e-prints.

luenberger, d. g. (1997). investment science. oxford university press.

luo, y., chiu, c.-c., jaitly, n., and sutskever, i. (2016). learning online alignments with contin-

uous rewards policy gradient. arxiv e-prints.

machado, m. c., bellemare, m. g., and bowling, m. (2017). a laplacian framework for option dis-
covery in id23. in the international conference on machine learning (icml).

machado, m. c., bellemare, m. g., talvitie, e., veness, j., hausknecht, m., and bowling, m.
(2017). revisiting the arcade learning environment: evaluation protocols and open problems
for general agents. arxiv e-prints.

madry, a., makelov, a., schmidt, l., tsipras, d., and vladu, a. (2017). towards deep learning

models resistant to adversarial attacks. arxiv e-prints.

mahler, j., liang, j., niyaz, s., laskey, m., doan, r., liu, x., aparicio ojea, j., and goldberg, k.
(2017). dex-net 2.0: deep learning to plan robust grasps with synthetic point clouds and analytic
grasp metrics. in robotics: science and systems (rss).

mahmood, a. r., van hasselt, h., and sutton, r. s. (2014). weighted importance sampling for
in the annual conference on neural

off-policy learning with linear function approximation.
information processing systems (nips).

mandel, t., liu, y. e., levine, s., brunskill, e., and popovi  c, z. (2014). of   ine policy evaluation
across representations with applications to educational games. in the international conference
on autonomous agents & multiagent systems (aamas).

71

manning, c. d., raghavan, p., and sch  utze, h. (2008). introduction to information retrieval. cam-

bridge university press.

mannion, p., duggan, j., and howley, e. (2016). an experimental review of reinforcement learn-
ing algorithms for adaptive traf   c signal control. autonomic road transport support systems,
edited by mccluskey, t., kotsialos, a., m  uller, j., kl  ugl, f., rana, o., and schumann r., springer
international publishing, cham, pages 47   66.

mao, h., alizadeh, m., menache, i., and kandula, s. (2016). resource management with deep

id23. in acm workshop on hot topics in networks (hotnets).

mao, x., li, q., xie, h., lau, r. y. k., and wang, z. (2016). least squares generative adversarial

networks. arxiv e-prints.

mathe, s., pirinen, a., and sminchisescu, c. (2016). id23 for visual object

detection. in the ieee conference on id161 and pattern recognition (cvpr).

matiisen, t., oliver, a., cohen, t., and schulman, j. (2017). teacher-student curriculum learning.

arxiv e-prints.

maurer, a., pontil, m., and romera-paredes, b. (2016). the bene   t of multitask representation

learning. the journal of machine learning research, 17(81):1   32.

mcallister, r. and rasmussen, c. e. (2017). data-ef   cient id23 in continuous-

state pomdps. in the annual conference on neural information processing systems (nips).

mccann, b., bradbury, j., xiong, c., and socher, r. (2017). learned in translation: contextualized

word vectors. arxiv e-prints.

melis, g., dyer, c., and blunsom, p. (2017). on the state of the art of evaluation in neural

language models. arxiv e-prints.

merel, j., tassa, y., tb, d., srinivasan, s., lemmon, j., wang, z., wayne, g., and heess, n. (2017).

learning human behaviors from motion capture by adversarial imitation. arxiv e-prints.

mesnil, g., dauphin, y., yao, k., bengio, y., deng, l., he, x., heck, l., tur, g., hakkani-t  ur,
d., yu, d., and zweig, g. (2015). using recurrent neural networks for slot    lling in spoken
language understanding. ieee/acm transactions on audio, speech, and language processing,
23(3):530   539.

mestres, a., rodriguez-natal, a., carner, j., barlet-ros, p., alarc  on, e., sol  e, m., munt  es, v.,
meyer, d., barkai, s., hibbett, m. j., estrada, g., ma`ruf, k., coras, f., ermagan, v., latapie,
h., cassar, c., evans, j., maino, f., walrand, j., and cabellos, a. (2016). knowledge-de   ned
networking. arxiv e-prints.

mhamdi, e. m. e., guerraoui, r., hendrikx, h., and maurer, a. (2017). dynamic safe interrupt-
ibility for decentralized multi-agent id23. in the annual conference on neural
information processing systems (nips).

mikolov, t., chen, k., corrado, g., and dean, j. (2013). ef   cient estimation of word representations

in vector space. in the international conference on learning representations (iclr).

mikolov, t., grave, e., bojanowski, p., puhrsch, c., and joulin, a. (2017). advances in pre-training

distributed word representations. arxiv e-prints.

miller, t. (2017). explanation in arti   cial intelligence: insights from the social sciences. arxiv

e-prints.

miotto, r., wang, f., wang, s., jiang, x., and dudley, j. t. (2017). deep learning for healthcare:

review, opportunities and challenges. brie   ngs in bioinformatics, pages 1   11.

mirhoseini, a., pham, h., le, q. v., steiner, b., larsen, r., zhou, y., kumar, n., and moham-
mad norouzi, samy bengio, j. d. (2017). device placement optimization with reinforcement
learning. in the international conference on machine learning (icml).

72

mirowski, p., pascanu, r., viola, f., soyer, h., ballard, a., banino, a., denil, m., goroshin, r.,
sifre, l., kavukcuoglu, k., kumaran, d., and hadsell, r. (2017). learning to navigate in complex
environments. in the international conference on learning representations (iclr).

mitra, b. and craswell, n. (2017). neural models for information retrieval. arxiv e-prints.

mnih, v., badia, a. p., mirza, m., graves, a., harley, t., lillicrap, t. p., silver, d., and
in the in-

kavukcuoglu, k. (2016). asynchronous methods for deep id23.
ternational conference on machine learning (icml).

mnih, v., heess, n., graves, a., and kavukcuoglu, k. (2014). recurrent models of visual attention.

in the annual conference on neural information processing systems (nips).

mnih, v., kavukcuoglu, k., silver, d., rusu, a. a., veness, j., bellemare, m. g., graves, a.,
riedmiller, m., fidjeland, a. k., ostrovski, g., petersen, s., beattie, c., sadik, a., antonoglou,
i., king, h., kumaran, d., wierstra, d., legg, s., and hassabis, d. (2015). human-level control
through deep id23. nature, 518(7540):529   533.

mo, k., li, s., zhang, y., li, j., and yang, q. (2016). personalizing a dialogue system with transfer

learning. arxiv e-prints.

monroe, d. (2017). deep learning takes on translation. communications of the acm, 60(6):12   14.

moody, j. and saffell, m. (2001). learning to trade via direct reinforcement. ieee transactions on

neural networks, 12(4):875   889.

morav  c    k, m., schmid, m., burch, n., lis  y, v., morrill, d., bard, n., davis, t., waugh, k., jo-
hanson, m., and bowling, m. (2017). deepstack: expert-level arti   cial intelligence in heads-up
no-limit poker. science.

m  uller, m. (2002). computer go. arti   cial intelligence, 134(1-2):145   179.

munos, r., stepleton, t., harutyunyan, a., and bellemare, m. g. (2016). safe and ef   cient off-
policy id23. in the annual conference on neural information processing sys-
tems (nips).

murphy, k. p. (2012). machine learning: a probabilistic perspective. the mit press.

nachum, o., norouzi, m., and schuurmans, d. (2017).

improving policy gradient by exploring
under-appreciated rewards. in the international conference on learning representations (iclr).

nachum, o., norouzi, m., xu, k., and schuurmans, d. (2017). bridging the gap between value and
policy based id23. in the annual conference on neural information processing
systems (nips).

nair, a., srinivasan, p., blackwell, s., alcicek, c., fearon, r., de maria, a., panneershelvam,
v., suleyman, m., beattie, c., petersen, s., legg, s., mnih, v., kavukcuoglu, k., and silver,
in icml 2015 deep
d. (2015). massively parallel methods for deep id23.
learning workshop.

narasimhan, k., kulkarni, t., and barzilay, r. (2015). language understanding for text-based
games using deep id23. in conference on empirical methods in natural lan-
guage processing (emnlp).

narasimhan, k., yala, a., and barzilay, r. (2016). improving information extraction by acquiring
external evidence with id23. in conference on empirical methods in natural
language processing (emnlp).

nedi  c, a. and bertsekas, d. p. (2003). least squares policy evaluation algorithms with linear func-

tion approximation. discrete event dynamic systems: theory and applications, 13:79   110.

neuneier, r. (1997). enhancing id24 for optimal asset allocation. in the annual conference

on neural information processing systems (nips).

73

neyshabur, b., tomioka, r., salakhutdinov, r., and srebro, n. (2017). geometry of optimization

and implicit id173 in deep learning. arxiv e-prints.

ng, a. and russell, s. (2000). algorithms for inverse id23. in the international

conference on machine learning (icml).

nogueira, r. and cho, k. (2016). end-to-end goal-driven web navigation. arxiv e-prints.

nogueira, r. and cho, k. (2017). task-oriented query reformulation with reinforcement learn-

ing. arxiv e-prints.

o   donoghue, b., munos, r., kavukcuoglu, k., and mnih, v. (2017). pgq: combining policy

gradient and id24. in the international conference on learning representations (iclr).

o   donovan, p., leahy, k., bruton, k., and o   sullivan, d. t. j. (2015). big data in manufacturing:

a systematic mapping study. journal of big data, 2(20).

oh, j., chockalingam, v., singh, s., and lee, h. (2016). control of memory, active perception, and

action in minecraft. in the international conference on machine learning (icml).

oh, j., guo, x., lee, h., lewis, r., and singh, s. (2015). action-conditional video prediction
using deep networks in atari games. in the annual conference on neural information processing
systems (nips).

oh, j., singh, s., and lee, h. (2017). value prediction network. in the annual conference on neural

information processing systems (nips).

omidsha   ei, s., pazis, j., amato, c., how, j. p., and vian, j. (2017). deep decentralized multi-task
multi-agent id23 under partial observability. in the international conference
on machine learning (icml).

onta  n  on, s., synnaeve, g., uriarte, a., richoux, f., churchill, d., and preuss, m. (2013). a
survey of real-time strategy game ai research and competition in starcraft. ieee transactions on
computational intelligence and ai in games, 5(4):293   311.

oquab, m., bottou, l., laptev, i., and sivic, j. (2015). is object localization for free?     weakly-
supervised learning with convolutional neural networks. in the ieee conference on computer
vision and pattern recognition (cvpr).

osband, i., blundell, c., pritzel, a., and roy, b. v. (2016). deep exploration via bootstrapped id25.

in the annual conference on neural information processing systems (nips).

ostrovski, g., bellemare, m. g., van den oord, a., and munos, r. (2017). count-based exploration

with neural density models. arxiv e-prints.

pan, s. j. and yang, q. (2010). a survey on id21. ieee transactions on knowledge

and data engineering, 22(10):1345     1359.

papernot, n., abadi, m., erlingsson,   u., goodfellow, i., and talwar, k. (2017). semi-supervised
knowledge transfer for deep learning from private training data. in the international conference
on learning representations (iclr).

papernot, n., goodfellow, i., sheatsley, r., feinman, r., and mcdaniel, p. (2016). cleverhans

v1.0.0: an adversarial machine learning library. arxiv e-prints.

parisotto, e., ba, j. l., and salakhutdinov, r. (2016). actor-mimic: deep multitask and transfer

id23. in the international conference on learning representations (iclr).

parisotto, e., rahman mohamed, a., singh, r., li, l., zhou, d., and kohli, p. (2017). neuro-
in the international conference on learning representations

symbolic program synthesis.
(iclr).

pasunuru, r. and bansal, m. (2017). reinforced video captioning with entailment rewards.

conference on empirical methods in natural language processing (emnlp).

in

74

paulus, r., xiong, c., and socher, r. (2017). a deep reinforced model for abstractive summa-

rization. arxiv e-prints.

pearl, j. (2018). theoretical impediments to machine learning with seven sparks from the causal

revolution. arxiv e-prints.

pei, k., cao, y., yang, j., and jana, s. (2017). deepxplore: automated whitebox testing of deep

learning systems. arxiv e-prints.

peng, b., li, x., li, l., gao, j., celikyilmaz, a., lee, s., and wong, k.-f. (2017a). composite
task-completion dialogue system via hierarchical deep id23. in conference on
empirical methods in natural language processing (emnlp).

peng, p., yuan, q., wen, y., yang, y., tang, z., long, h., and wang, j. (2017b). multiagent
bidirectionally-coordinated nets for learning to play starcraft combat games. arxiv e-prints.

p  erez-d   arpino, c. and shah, j. a. (2017). c-learn: learning geometric constraints from demon-
strations for multi-step manipulation in shared autonomy. in ieee international conference on
robotics and automation (icra).

perolat, j., leibo, j. z., zambaldi, v., beattie, c., tuyls, k., and graepel, t. (2017). a multi-agent
id23 model of common-pool resource appropriation. in the annual conference
on neural information processing systems (nips).

peters, j. and neumann, g. (2015). policy search: methods and applications. icml 2015 tutorial.

petroski such, f., madhavan, v., conti, e., lehman, j., stanley, k. o., and clune, j. (2017). deep
neuroevolution: id107 are a competitive alternative for training deep neural
networks for id23. arxiv e-prints.

pfau, d. and vinyals, o. (2016). connecting id3 and actor-critic

methods. arxiv e-prints.

phua, c., lee, v., smith, k., and gayler, r. (2010). a comprehensive survey of data mining-based

fraud detection research. arxiv e-prints.

popov, i., heess, n., lillicrap, t., hafner, r., barth-maron, g., vecerik, m., lampe, t., tassa, y.,
erez, t., and riedmiller, m. (2017). data-ef   cient deep id23 for dexterous
manipulation. arxiv e-prints.

powell, w. b. (2011). approximate id145: solving the curses of dimensionality

(2nd edition). john wiley and sons.

prashanth, l., jie, c., fu, m., marcus, s., and szepes  ari, c. (2016). cumulative prospect theory
meets id23: prediction and control. in the international conference on machine
learning (icml).

preuveneers, d. and ilie-zudor, e. (2017). the intelligent industry of the future: a survey on emerg-
ing trends, research challenges and opportunities in industry 4.0. journal of ambient intelligence
and smart environments, 9(3):287   298.

pritzel, a., uria, b., srinivasan, s., puigdom`enech, a., vinyals, o., hassabis, d., wierstra, d., and

blundell, c. (2017). neural episodic control. arxiv e-prints.

provost, f. and fawcett, t. (2013). data science for business. o   reilly media.

puterman, m. l. (2005). id100 : discrete stochastic id145.

wiley-interscience.

radford, a., jozefowicz, r., and sutskever, i. (2017). learning to generate reviews and discover-

ing sentiment. arxiv e-prints.

raghu, m., poole, b., kleinberg, j., ganguli, s., and sohl-dickstein, j. (2016). survey of expres-

sivity in deep neural networks. arxiv e-prints.

75

rahimi, a. and recht, b. (2007). random features for large-scale kernel machines. in the annual

conference on neural information processing systems (nips).

rajendran, j., lakshminarayanan, a., khapra, m. m., p, p., and ravindran, b. (2017). attend, adapt
and transfer: attentive deep architecture for adaptive transfer from multiple sources in the same
domain. the international conference on learning representations (iclr).

ranzato, m., chopra, s., auli, m., and zaremba, w. (2016). sequence level training with recurrent

neural networks. in the international conference on learning representations (iclr).

rao, y., lu, j., and zhou, j. (2017). attention-aware deep id23 for video face

recognition. in the ieee international conference on id161 (iccv).

ravi, s. and larochelle, h. (2017). optimization as a model for few-shot learning. in the interna-

tional conference on learning representations (iclr).

reed, s. and de freitas, n. (2016). neural programmer-interpreters. in the international conference

on learning representations (iclr).

ren, s., he, k., girshick, r., and sun, j. (2015). faster r-id98: towards real-time id164
in the annual conference on neural information processing

with region proposal networks.
systems (nips).

ren, z., wang, x., zhang, n., lv, x., and li, l.-j. (2017). deep id23-based
in the ieee conference on id161 and

image captioning with embedding reward.
pattern recognition (cvpr).

rennie, s. j., marcheret, e., mroueh, y., ross, j., and goel, v. (2017). self-critical sequence training
in the ieee conference on id161 and pattern recognition

for image captioning.
(cvpr).

rhinehart, n. and kitani, k. m. (2017). first-person activity forecasting with online inverse rein-

forcement learning. in the ieee international conference on id161 (iccv).

ribeiro, m. t., singh, s., and guestrin, c. (2016).    why should i trust you?    explaining the pre-
dictions of any classi   er. in acm international conference on knowledge discovery and data
mining (sigkdd).

riedmiller, m. (2005). neural    tted q iteration -    rst experiences with a data ef   cient neural rein-

forcement learning method. in european conference on machine learning (ecml).

rockt  aschel, t. and riedel, s. (2017). end-to-end differentiable proving. arxiv e-prints.

roijers, d. m., vamplew, p., whiteson, s., and dazeley, r. (2013). a survey of multi-objective

sequential decision-making. journal of arti   cial intelligence research, 48:67   113.

ruder, s. (2017). an overview of id72 in deep neural networks. arxiv e-prints.

ruelens, f., claessens, b. j., vandael, s., schutter, b. d., babu  ska, r., and belmans, r. (2016). res-
idential demand response of thermostatically controlled loads using batch id23.
ieee transactions on smart grid, pp(99):1   11.

russell, s. and norvig, p. (2009). arti   cial intelligence: a modern approach (3rd edition). pearson.

sabour, s., frosst, n., and hinton, g. e. (2017). dynamic routing between capsules. in the annual

conference on neural information processing systems (nips).

salakhutdinov, r. (2016). foundations of unsupervised deep learning, a talk at deep learn-
ing school, https://www.bayareadlschool.org. https://www.youtube.com/watch?v=
rk6bchqean8.

salimans, t., ho, j., chen, x., and sutskever, i. (2017). evolution strategies as a scalable alterna-

tive to id23. arxiv e-prints.

76

santoro, a., raposo, d., barrett, d. g. t., malinowski, m., pascanu, r., battaglia, p., and lillicrap,

t. (2017). a simple neural network module for relational reasoning. arxiv e-prints.

saon, g., sercu, t., rennie, s., and kuo, h.-k. j. (2016). the ibm 2016 english conversational
telephone id103 system. in annual meeting of the international speech commu-
nication association (interspeech).

saria, s. (2014). a $3 trillion challenge to computational scientists: transforming healthcare deliv-

ery. ieee intelligent systems, 29(4):82   87.

schaul, t., horgan, d., gregor, k., and silver, d. (2015). universal value function approximators.

in the international conference on machine learning (icml).

schaul, t., quan, j., antonoglou, i., and silver, d. (2016). prioritized experience replay. in the

international conference on learning representations (iclr).

schmidhuber, j. (2015). deep learning in neural networks: an overview. neural networks, 61:85   

117.

schulman, j., abbeel, p., and chen, x. (2017). equivalence between policy gradients and soft

id24. arxiv e-prints.

schulman, j., levine, s., moritz, p., jordan, m. i., and abbeel, p. (2015). trust region policy

optimization. in the international conference on machine learning (icml).

schuurmans, d. and zinkevich, m. (2016). deep learning games. in the annual conference on

neural information processing systems (nips).

segler, m. h. s., preuss, m., and waller, m. p. (2017). learning to plan chemical syntheses. arxiv

e-prints.

serban, i. v., lowe, r., charlin, l., and pineau, j. (2015). a survey of available corpora for building

data-driven dialogue systems. arxiv e-prints, abs/1512.05742.

serban, i. v., sankar, c., germain, m., zhang, s., lin, z., subramanian, s., kim, t., pieper, m.,
chandar, s., ke, n. r., mudumba, s., de brebisson, a., sotelo, j. m. r., suhubdy, d., michalski,
v., nguyen, a., pineau, j., and bengio, y. (2017). a deep id23 chatbot.
arxiv e-prints.

shah, p., hakkani-t  ur, d., and heck, l. (2016). interactive id23 for task-oriented

dialogue management. in nips 2016 deep learning for action and interaction workshop.

shalev-shwartz, s., shamir, o., and shammah, s. (2017). failures of gradient-based deep learning.

in the international conference on machine learning (icml).

sharma, s., lakshminarayanan, a. s., and ravindran, b. (2017). learning to repeat: fine grained
action repetition for deep id23. in the international conference on learning
representations (iclr).

she, l. and chai, j. (2017). interactive learning for acquisition of grounded verb semantics towards
human-robot communication. in the association for computational linguistics annual meeting
(acl).

shen, y., huang, p.-s., gao, j., and chen, w. (2017). reasonet: learning to stop reading in machine
comprehension. in acm international conference on knowledge discovery and data mining
(sigkdd).

shoham, y. and leyton-brown, k. (2009). multiagent systems: algorithmic, game-theoretic, and

logical foundations. cambridge university press.

shoham, y., powers, r., and grenager, t. (2007). if multi-agent learning is the answer, what is the

question? arti   cial intelligence, 171:365   377.

77

shortreed, s. m., laber, e., lizotte, d. j., stroup, t. s., pineau, j., and murphy, s. a. (2011). in-
forming sequential clinical decision-making through id23: an empirical study.
machine learning, 84:109   136.

shrivastava, a., p   ster, t., tuzel, o., susskind, j., wang, w., and webb, r. (2017). learning
from simulated and unsupervised images through adversarial training. in the ieee conference
on id161 and pattern recognition (cvpr).

shwartz-ziv, r. and tishby, n. (2017). opening the black box of deep neural networks via

information. arxiv e-prints.

silver, d. (2016). deep id23, a tutorial at icml 2016. http://icml.cc/

2016/tutorials/deep_rl_tutorial.pdf.

silver, d., huang, a., maddison, c. j., guez, a., sifre, l., van den driessche, g., schrittwieser, j.,
antonoglou, i., panneershelvam, v., lanctot, m., et al. (2016a). mastering the game of go with
deep neural networks and tree search. nature, 529(7587):484   489.

silver, d., hubert, t., schrittwieser, j., antonoglou, i., lai, m., guez, a., lanctot, m., sifre, l.,
kumaran, d., graepel, t., lillicrap, t., simonyan, k., and hassabis, d. (2017). mastering chess
and shogi by self-play with a general id23 algorithm. arxiv e-prints.

silver, d., lever, g., heess, n., degris, t., wierstra, d., and riedmiller, m. (2014). deterministic

policy gradient algorithms. in the international conference on machine learning (icml).

silver, d., newnham, l., barker, d., weller, s., and mcfall, j. (2013). concurrent reinforce-
ment learning from customer interactions. in the international conference on machine learning
(icml).

silver, d., schrittwieser, j., simonyan, k., antonoglou, i., huang, a., guez, a., hubert, t., baker,
l., lai, m., bolton, a., chen, y., lillicrap, t., hui, f., sifre, l., van den driessche, g., graepel,
t., and hassabis, d. (2017). mastering the game of go without human knowledge. nature,
550:354   359.

silver, d., van hasselt, h., hessel, m., schaul, t., guez, a., harley, t., dulac-arnold, g., reichert,
d., rabinowitz, n., barreto, a., and degris, t. (2016b). the predictron: end-to-end learning and
planning. in nips 2016 deep id23 workshop.

simeone, o. (2017). a brief introduction to machine learning for engineers. arxiv e-prints.

smith, l. n. (2017). best practices for applying deep learning to novel applications. arxiv

e-prints.

smith, v., chiang, c.-k., sanjabi, m., and talwalkar, a. (2017). federated id72. in

the annual conference on neural information processing systems (nips).

snell, j., swersky, k., and zemel, r. s. (2017). prototypical networks for few-shot learning.

arxiv e-prints.

socher, r., pennington, j., huang, e. h., ng, a. y., and manning, c. d. (2011). semi-supervised re-
cursive autoencoders for predicting sentiment distributions. in conference on empirical methods
in natural language processing (emnlp).

socher, r., perelygin, a., wu, j., chuang, j., manning, c., ng, a., and potts, c. (2013). recur-
sive deep models for semantic compositionality over a sentiment tree- bank. in conference on
empirical methods in natural language processing (emnlp).

song, y. and roth, d. (2017). machine learning with world knowledge: the position and survey.

arxiv e-prints.

spring, r. and shrivastava, a. (2017). scalable and sustainable deep learning via randomized hash-

ing. in acm international conference on knowledge discovery and data mining (sigkdd).

78

srivastava, n., hinton, g., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2014). dropout:
a simple way to prevent neural networks from over   tting. the journal of machine learning
research, 15:1929   1958.

stadie, b. c., abbeel, p., and sutskever, i. (2017). third person imitation learning. in the interna-

tional conference on learning representations (iclr).

stoica, i., song, d., popa, r. a., patterson, d. a., mahoney, m. w., katz, r. h., joseph, a. d.,
jordan, m., hellerstein, j. m., gonzalez, j., goldberg, k., ghodsi, a., culler, d. e., and abbeel,
p. (2017). a berkeley view of systems challenges for ai. technical report no. ucb/eecs-2017-
159.

stone, p., brooks, r., brynjolfsson, e., calo, r., etzioni, o., hager, g., hirschberg, j., kalyanakr-
ishnan, s., kamar, e., kraus, s., leyton-brown, k., parkes, d., press, w., saxenian, a., shah,
j., tambe, m., and teller, a. (2016). arti   cial intelligence and life in 2030 - one hundred
year study on arti   cial intelligence: report of the 2015-2016 study panel. stanford university,
stanford, ca.

stone, p. and veloso, m. (2000). multiagent systems: a survey from a machine learning perspective.

autonomous robots, 8(3):345   383.

strub, f., de vries, h., mary, j., piot, b., courville, a., and pietquin, o. (2017). end-to-end

optimization of goal-driven and visually grounded dialogue systems. arxiv e-prints.

su, p.-h., gasic, m., mrksic, n., rojas-barahona, l., ultes, s., vandyke, d., wen, t.-h., and young,

s. (2016a). continuously learning neural dialogue management. arxiv e-prints.

su, p.-h., gas  i  c, m., mrks  i  c, n., rojas-barahona, l., ultes, s., vandyke, d., wen, t.-h., and young,
s. (2016b). on-line active reward learning for policy optimisation in spoken dialogue systems. in
the association for computational linguistics annual meeting (acl).

sukhbaatar, s., szlam, a., and fergus, r. (2016). learning multiagent communication with back-

propagation. in the annual conference on neural information processing systems (nips).

sukhbaatar, s., weston, j., and fergus, r. (2015). end-to-end memory networks. in the annual

conference on neural information processing systems (nips).

supan  ci  c, iii, j. and ramanan, d. (2017). tracking as online decision-making: learning a policy
in the ieee international conference on

from streaming videos with id23.
id161 (iccv).

surana, a., sarkar, s., and reddy, k. k. (2016). guided deep id23 for additive

manufacturing control application. in nips 2016 deep id23 workshop.

sutskever, i., vinyals, o., and le, q. v. (2014). sequence to sequence learning with neural networks.

in the annual conference on neural information processing systems (nips).

sutton, r. (2016). id23 for arti   cial intelligence, course slides. http://www.

incompleteideas.net/sutton/609%20dropbox/.

sutton, r. s. (1988). learning to predict by the methods of temporal differences. machine learning,

3(1):9   44.

sutton, r. s. (1990). integrated architectures for learning, planning, and reacting based on approxi-

mating id145. in the international conference on machine learning (icml).

sutton, r. s. (1992). adapting bias by id119: an incremental version of delta-bar-delta.

in the aaai conference on arti   cial intelligence (aaai).

sutton, r. s. and barto, a. g. (1998). id23: an introduction. mit press.

sutton, r. s. and barto, a. g. (2018). id23: an introduction (2nd edition, in

preparation). mit press.

79

sutton, r. s., maei, h. r., precup, d., bhatnagar, s., silver, d., szepesv  ari, c., and wiewiora,
e. (2009a). fast gradient-descent methods for temporal-difference learning with linear function
approximation. in the international conference on machine learning (icml).

sutton, r. s., mahmood, a. r., and white, m. (2016). an emphatic approach to the problem of

off-policy temporal-difference learning. the journal of machine learning research, 17:1   29.

sutton, r. s., mcallester, d., singh, s., and mansour, y. (2000). id189 for rein-
forcement learning with function approximation. in the annual conference on neural information
processing systems (nips).

sutton, r. s., modayil, j., delp, m., degris, t., pilarski, p. m., white, a., and precup, d. (2011).
horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor
interaction, , proc. of 10th. in international conference on autonomous agents and multiagent
systems (aamas).

sutton, r. s., precup, d., and singh, s. (1999). between mdps and semi-mdps: a framework for

temporal abstraction in id23. arti   cial intelligence, 112(1-2):181   211.

sutton, r. s., szepesv  ari, c., and maei, h. r. (2009b). a convergent o(n) algorithm for off-policy
temporal-difference learning with linear function approximation. in the annual conference on
neural information processing systems (nips).

synnaeve, g., nardelli, n., auvolat, a., chintala, s., lacroix, t., lin, z., richoux, f., and usunier,
n. (2016). torchcraft: a library for machine learning research on real-time strategy games.
arxiv e-prints.

sze, v., chen, y.-h., yang, t.-j., and emer, j. (2017). ef   cient processing of deep neural net-

works: a tutorial and survey. arxiv e-prints.

szepesv  ari, c. (2010). algorithms for id23. morgan & claypool.

tamar, a., wu, y., thomas, g., levine, s., and abbeel, p. (2016). value iteration networks. in the

annual conference on neural information processing systems (nips).

tang, h., houthooft, r., foote, d., stooke, a., chen, x., duan, y., schulman, j., turck, f. d.,
and abbeel, p. (2017). exploration: a study of count-based exploration for deep reinforcement
learning. in the annual conference on neural information processing systems (nips).

tanner, b. and white, a. (2009). rl-glue : language-independent software for reinforcement-

learning experiments. journal of machine learning research, 10:2133   2136.

tassa, y., doron, y., muldal, a., erez, t., li, y., de las casas, d., budden, d., abdolmaleki, a.,
merel, j., lefrancq, a., lillicrap, t., and riedmiller, m. (2018). deepmind control suite. arxiv
e-prints.

taylor, m. e. and stone, p. (2009). id21 for id23 domains: a survey.

journal of machine learning research, 10:1633   1685.

tesauro, g. (1994). td-gammon, a self-teaching backgammon program, achieves master-level

play. neural computation, 6(2):215   219.

tessler, c., givony, s., zahavy, t., mankowitz, d. j., and mannor, s. (2017). a deep hierarchical
in the aaai conference on arti   cial intelligence

approach to lifelong learning in minecraft.
(aaai).

theocharous, g., thomas, p. s., and ghavamzadeh, m. (2015). personalized ad recommendation
systems for life-time value optimization with guarantees. in the international joint conference
on arti   cial intelligence (ijcai).

tian, y., gong, q., shang, w., wu, y., and zitnick, l. (2017). elf: an extensive, lightweight and

flexible research platform for real-time strategy games. arxiv e-prints.

tram`er, f., kurakin, a., papernot, n., boneh, d., and mcdaniel, p. (2017). ensemble adversarial

training: attacks and defenses. arxiv e-prints.

80

tran, d., hoffman, m. d., saurous, r. a., brevdo, e., murphy, k., and blei, d. m. (2017). deep
probabilistic programming. in the international conference on learning representations (iclr).

trischler, a., ye, z., yuan, x., and suleman, k. (2016). natural language comprehension with the

epireader. in conference on empirical methods in natural language processing (emnlp).

tsitsiklis, j. n. and van roy, b. (1997). an analysis of temporal-difference learning with function

approximation. ieee transactions on automatic control, 42(5):674   690.

tsitsiklis, j. n. and van roy, b. (2001). regression methods for pricing complex american-style

options. ieee transactions on neural networks, 12(4):694   703.

usunier, n., synnaeve, g., lin, z., and chintala, s. (2017). episodic exploration for deep de-
in the international

terministic policies: an application to starcraft micromanagement tasks.
conference on learning representations (iclr).

van der pol, e. and oliehoek, f. a. (2017). coordinated deep reinforcement learners for traf   c light

control. in nips   16 workshop on learning, id136 and control of multi-agent systems.

van hasselt, h., guez, a., , and silver, d. (2016a). deep id23 with double q-

learning. in the aaai conference on arti   cial intelligence (aaai).

van hasselt, h., guez, a., hessel, m., mnih, v., and silver, d. (2016b). learning values across
many orders of magnitude. in the annual conference on neural information processing systems
(nips).

van seijen, h., fatemi, m., romoff, j., laroche, r., barnes, t., and tsang, j. (2017). hybrid
reward architecture for id23. in the annual conference on neural information
processing systems (nips).

vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a. n., kaiser, l., and polo-
sukhin, i. (2017). attention is all you need. in the annual conference on neural information
processing systems (nips).

venkatraman, a., rhinehart, n., sun, w., pinto, l., hebert, m., boots, b., kitani, k. m., and
bagnell, j. a. (2017). predictive-state decoders: encoding the future into recurrent networks. in
the annual conference on neural information processing systems (nips).

ve  cer    k, m., hester, t., scholz, j., wang, f., pietquin, o., piot, b., heess, n., roth  orl, t., lampe,
t., and riedmiller, m. (2017). leveraging demonstrations for deep id23 on
robotics problems with sparse rewards. in the annual conference on neural information pro-
cessing systems (nips).

vezhnevets, a. s., mnih, v., agapiou, j., osindero, s., graves, a., vinyals, o., and kavukcuoglu,
k. (2016). strategic attentive writer for learning macro-actions. in the annual conference on
neural information processing systems (nips).

vezhnevets, a. s., osindero, s., schaul, t., heess, n., jaderberg, m., silver, d., and kavukcuoglu,
k. (2017). feudal networks for hierarchical id23. in the international confer-
ence on machine learning (icml).

vinyals, o., blundell, c., lillicrap, t., kavukcuoglu, k., and wierstra, d. (2016). matching net-
works for one shot learning. in the annual conference on neural information processing systems
(nips).

vinyals, o., fortunato, m., and jaitly, n. (2015). id193. in the annual conference on

neural information processing systems (nips).

wang, h. and raj, b. (2017). on the origin of deep learning. arxiv e-prints.

wang, j. x., kurth-nelson, z., tirumala, d., soyer, h., leibo, j. z., munos, r., blundell, c.,

kumaran, d., and botvinick, m. (2016). learning to reinforcement learn. arxiv e-prints.

wang, s. i., liang, p., and manning, c. d. (2016a). learning language games through interaction.

in the association for computational linguistics annual meeting (acl).

81

wang, w., yang, n., wei, f., chang, b., and zhou, m. (2017a). gated self-matching networks for
reading comprehension and id53. in the association for computational linguistics
annual meeting (acl).

wang, z., bapst, v., heess, n., mnih, v., munos, r., kavukcuoglu, k., and de freitas, n. (2017b).
sample ef   cient actor-critic with experience replay. in the international conference on learning
representations (iclr).

wang, z., merel, j., reed, s., wayne, g., de freitas, n., and heess, n. (2017). robust imitation of

diverse behaviors. arxiv e-prints.

wang, z., schaul, t., hessel, m., van hasselt, h., lanctot, m., and de freitas, n. (2016b). du-
eling network architectures for deep id23. in the international conference on
machine learning (icml).

watkins, c. j. c. h. and dayan, p. (1992). id24. machine learning, 8:279   292.

watter, m., springenberg, j. t., boedecker, j., and riedmiller, m. (2015). embed to control: a
locally linear latent dynamics model for control from raw images. in the annual conference on
neural information processing systems (nips).

watters, n., tacchetti, a., weber, t., pascanu, r., battaglia, p., and zoran, d. (2017). visual
in the annual conference on

interaction networks: learning a physics simulator from video.
neural information processing systems (nips).

weber, t., racani`ere, s., reichert, d. p., buesing, l., guez, a., jimenez rezende, d., puig-
dom`enech badia, a., vinyals, o., heess, n., li, y., pascanu, r., battaglia, p., silver, d., and
wierstra, d. (2017). imagination-augmented agents for deep id23. in the an-
nual conference on neural information processing systems (nips).

weiss, k., khoshgoftaar, t. m., and wang, d. (2016). a survey of id21. journal of big

data, 3(9).

weiss, r. j., chorowski, j., jaitly, n., wu, y., and chen, z. (2017). sequence-to-sequence models

can directly transcribe foreign speech. arxiv e-prints.

welleck, s., mao, j., cho, k., and zhang, z. (2017). saliency-based sequential image attention with
multiset prediction. in the annual conference on neural information processing systems (nips).

wen, t.-h., gasic, m., mrksic, n., su, p.-h., vandyke, d., and young, s. (2015a). semantically con-
ditioned lstm-based id86 for spoken dialogue systems. in conference
on empirical methods in natural language processing (emnlp).

wen, t.-h., vandyke, d., mrksic, n., gasic, m., rojas-barahona, l. m., su, p.-h., ultes, s., and
young, s. (2017). a network-based end-to-end trainable task-oriented dialogue system. in pro-
ceedings of the 15th conference of the european chapter of the association for computational
linguistics (eacl).

wen, z., o   neill, d., and maei, h. (2015b). optimal demand response using device-based rein-

forcement learning. ieee transactions on smart grid, 6(5):2312   2324.

weston, j., chopra, s., and bordes, a. (2015). memory networks. in the international conference

on learning representations (iclr).

white, a. and white, m. (2016). investigating practical linear temporal difference learning. in the

international conference on autonomous agents & multiagent systems (aamas).

whye teh, y., bapst, v., czarnecki, w. m., quan, j., kirkpatrick, j., hadsell, r., heess, n., and
pascanu, r. (2017). distral: robust multitask id23. in the annual conference
on neural information processing systems (nips).

wiering, m. and van otterlo, m. (2012). id23: state-of-the-art (edited book).

springer.

82

williams, j. d., asadi, k., and zweig, g. (2017). hybrid code networks: practical and ef   cient
in the association for

end-to-end dialog control with supervised and id23.
computational linguistics annual meeting (acl).

williams, j. d. and zweig, g. (2016). end-to-end lstm-based dialog control optimized with

supervised and id23. arxiv e-prints.

williams, r. j. (1992). simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. machine learning, 8(3):229   256.

wilson, a. c., roelofs, r., stern, m., srebro, n., and recht, b. (2017). the marginal value of

adaptive gradient methods in machine learning. arxiv e-prints.

wu, j., lu, e., kohli, p., freeman, b., and tenenbaum, j. (2017a). learning to see physics via visual

de-animation. in the annual conference on neural information processing systems (nips).

wu, j., tenenbaum, j. b., and kohli, p. (2017b). neural scene de-rendering. in the ieee conference

on id161 and pattern recognition (cvpr).

wu, j., yildirim, i., lim, j. j., freeman, b., and tenenbaum, j. (2015). galileo: perceiving physical
object properties by integrating a physics engine with deep learning. in the annual conference
on neural information processing systems (nips).

wu, l., xia, y., zhao, l., tian, f., qin, t., lai, j., and liu, t.-y. (2017c). adversarial neural

machine translation. arxiv e-prints.

wu, y., mansimov, e., liao, s., grosse, r., and ba, j. (2017). scalable trust-region method for
deep id23 using kronecker-factored approximation. in the annual conference
on neural information processing systems (nips).

wu, y., schuster, m., chen, z., le, q. v., norouzi, m., macherey, w., krikun, m., cao, y., gao,
q., macherey, k., klingner, j., shah, a., johnson, m., liu, x., kaiser, l., gouws, s., kato, y.,
kudo, t., kazawa, h., stevens, k., kurian, g., patil, n., wang, w., young, c., smith, j., riesa,
j., rudnick, a., vinyals, o., corrado, g., hughes, m., and dean, j. (2016). google   s neural
machine translation system: bridging the gap between human and machine translation. arxiv
e-prints.

wu, y. and tian, y. (2017). training agent for    rst-person shooter game with actor-critic curriculum

learning. in the international conference on learning representations (iclr).

xiong, c., zhong, v., and socher, r. (2017a). dynamic coattention networks for question answer-

ing. in the international conference on learning representations (iclr).

xiong, w., droppo, j., huang, x., seide, f., seltzer, m., stolcke, a., yu, d., and zweig, g. (2017b).
the microsoft 2016 conversational id103 system. in the ieee international con-
ference on acoustics, speech and signal processing (icassp).

xiong, w., hoang, t., and wang, w. y. (2017c). deeppath: a id23 method for
id13 reasoning. in conference on empirical methods in natural language process-
ing (emnlp).

xiong, w., wu, l., alleva, f., droppo, j., huang, x., and stolcke, a. (2017). the microsoft 2017

conversational id103 system. arxiv e-prints.

xu, d., nair, s., zhu, y., gao, j., garg, a., fei-fei, l., and savarese, s. (2017). neural task

programming: learning to generalize across hierarchical tasks. arxiv e-prints.

xu, k., ba, j. l., kiros, r., cho, k., courville, a., salakhutdinov, r., zemel, r. s., and bengio,
y. (2015). show, attend and tell: neural image id134 with visual attention. in the
international conference on machine learning (icml).

xu, l. d., he, w., and li, s. (2014). internet of things in industries: a survey. ieee transactions

on industrial informatics, 10(4):2233   2243.

83

yahya, a., li, a., kalakrishnan, m., chebotar, y., and levine, s. (2016). collective robot reinforce-

ment learning with distributed asynchronous guided policy search. arxiv e-prints.

yang, b. and mitchell, t. (2017). leveraging knowledge bases in lstms for improving machine

reading. in the association for computational linguistics annual meeting (acl).

yang, x., chen, y.-n., hakkani-tur, d., crook, p., li, x., gao, j., and deng, l. (2016). end-to-end

joint learning of natural language understanding and dialogue manager. arxiv e-prints.

yang, z., he, x., gao, j., deng, l., and smola, a. (2015). stacked attention networks for image

id53. arxiv e-prints.

yang, z., hu, j., salakhutdinov, r., and cohen, w. w. (2017). semi-supervised qa with generative

domain-adaptive nets. in the association for computational linguistics annual meeting (acl).

yannakakis, g. n. and togelius, j. (2018). arti   cial intelligence and games. springer.

yao, h., szepesvari, c., sutton, r. s., modayil, j., and bhatnagar, s. (2014). universal option

models. in the annual conference on neural information processing systems (nips).

yi, z., zhang, h., tan, p., and gong, m. (2017). dualgan: unsupervised dual learning for image-

to-image translation. in the ieee international conference on id161 (iccv).

yogatama, d., blunsom, p., dyer, c., grefenstette, e., and ling, w. (2017). learning to compose
words into sentences with id23. in the international conference on learning
representations (iclr).

yosinski, j., clune, j., bengio, y., and lipson, h. (2014). how transferable are features in deep
neural networks? in the annual conference on neural information processing systems (nips).

young, s., ga  si  c, m., thomson, b., and williams, j. d. (2013). pomdp-based statistical spoken

dialogue systems: a review. proc ieee, 101(5):1160   1179.

young, t., hazarika, d., poria, s., and cambria, e. (2017). recent trends in deep learning based

natural language processing. arxiv e-prints.

yu, l., zhang, w., wang, j., and yu, y. (2017). seqgan: sequence generative adversarial nets with

policy gradient. in the aaai conference on arti   cial intelligence (aaai).

yu, y.-l., li, y., szepesv  ari, c., and schuurmans, d. (2009). a general projection property for dis-
tribution families. in the annual conference on neural information processing systems (nips).

yun, s., choi, j., yoo, y., yun, k., and young choi, j. (2017). action-decision networks for visual
in the ieee conference on id161 and

tracking with deep id23.
pattern recognition (cvpr).

zagoruyko, s. and komodakis, n. (2017). paying more attention to attention: improving the per-
formance of convolutional neural networks via attention transfer. in the international conference
on learning representations (iclr).

zaremba, w. and sutskever, i. (2015). id23 id63s - revised.

arxiv e-prints.

zhang, c., bengio, s., hardt, m., recht, b., and vinyals, o. (2017). understanding deep learning
requires rethinking generalization. in the international conference on learning representations
(iclr).

zhang, h., yu, h., and xu, w. (2017a). listen, interact and talk: learning to speak via interaction.

arxiv e-prints.

zhang, j., ding, y., shen, s., cheng, y., sun, m., luan, h., and liu, y. (2017b). thumt: an open

source toolkit for id4. arxiv e-prints.

zhang, l., wang, s., and liu, b. (2018). deep learning for id31 : a survey. arxiv

e-prints.

84

zhang, q. and zhu, s.-c. (2018). visual interpretability for deep learning: a survey. frontiers of

information technology & electronic engineering, 19(1):27   39.

zhang, x. and lapata, m. (2017). sentence simpli   cation with deep id23. in

conference on empirical methods in natural language processing (emnlp).

zhang, y., musta   zur rahman, m., braylan, a., dang, b., chang, h.-l., kim, h., mcnamara, q.,
angert, a., banner, e., khetan, v., mcdonnell, t., thanh nguyen, a., xu, d., wallace, b. c.,
and lease, m. (2016). neural information retrieval: a literature review. arxiv e-prints.

zhang, y., pezeshki, m., brakel, p., zhang, s., yoshua bengio, c. l., and courville, a. (2017c).
towards end-to-end id103 with deep convolutional neural networks. arxiv e-
prints.

zhao, t. and eskenazi, m. (2016). towards end-to-end learning for dialog state tracking and man-
agement using deep id23. in the annual sigdial meeting on discourse and
dialogue (sigdial).

zhong, z., yan, j., and liu, c.-l. (2017). practical network blocks design with id24. arxiv

e-prints.

zhou, b., khosla, a., lapedriza, a., oliva, a., and torralba, a. (2015). object detectors emerge in

deep scene id98s. in the international conference on learning representations (iclr).

zhou, h., huang, m., zhang, t., zhu, x., and liu, b. (2017). emotional chatting machine: emo-

tional conversation generation with internal and external memory. arxiv e-prints.

zhou, y. and tuzel, o. (2017). voxelnet: end-to-end learning for point cloud based 3d object

detection. arxiv e-prints.

zhou, z.-h. (2016). machine learning (in chinese). tsinghua university press, beijing, china.

zhou, z.-h. and feng, j. (2017). deep forest: towards an alternative to deep neural networks. in

the international joint conference on arti   cial intelligence (ijcai).

zhu, j.-y., park, t., isola, p., and efros, a. a. (2017a). unpaired image-to-image translation using
cycle-consistent adversarial networks. in the ieee international conference on id161
(iccv).

zhu, x. and goldberg, a. b. (2009). introduction to semi-supervised learning. morgan & claypool.

zhu, y., mottaghi, r., kolve, e., lim, j. j., gupta, a., li, f.-f., and farhadi, a. (2017b). target-
driven visual navigation in indoor scenes using deep id23. in ieee interna-
tional conference on robotics and automation (icra).

zinkevich, m. (2017).

rules of machine learning: best practices for ml engineering.

http://martin.zinkevich.org/rules of ml/rules of ml.pdf.

zoph, b. and le, q. v. (2017). neural architecture search with id23.

international conference on learning representations (iclr).

in the

zoph, b., vasudevan, v., shlens, j., and le, q. v. (2017). learning transferable architectures for

scalable image recognition. arxiv e-prints.

85

