8
1
0
2

 

v
o
n
5
2

 

 
 
]
l
c
.
s
c
[
 
 

8
v
9
0
7
2
0

.

8
0
7
1
:
v
i
x
r
a

recent trends in deep learning based

natural language processing

tom young      , devamanyu hazarika      , soujanya poria      , erik cambria(cid:53)   

1

    school of information and electronics, beijing institute of technology, china

    school of computing, national university of singapore, singapore
    temasek laboratories, nanyang technological university, singapore

(cid:53) school of computer science and engineering, nanyang technological university, singapore

abstract

deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced
state-of-the-art results in many domains. recently, a variety of model designs and methods have blossomed in the context of
natural language processing (nlp). in this paper, we review signi   cant deep learning related models and methods that have been
employed for numerous nlp tasks and provide a walk-through of their evolution. we also summarize, compare and contrast the
various models and put forward a detailed understanding of the past, present and future of deep learning in nlp.

natural language processing, deep learning, id97, attention, recurrent neural networks, convolutional neural net-
works, lstm, id31, id53, dialogue systems, parsing, named-entity recognition, id52,
id14

index terms

i. introduction

natural language processing (nlp) is a theory-motivated range of computational techniques for the automatic analysis and
representation of human language. nlp research has evolved from the era of punch cards and batch processing, in which the
analysis of a sentence could take up to 7 minutes, to the era of google and the likes of it, in which millions of webpages can
be processed in less than a second [1]. nlp enables computers to perform a wide range of natural language related tasks at
all levels, ranging from parsing and part-of-speech (pos) tagging, to machine translation and dialogue systems.

deep learning architectures and algorithms have already made impressive advances in    elds such as id161 and
pattern recognition. following this trend, recent nlp research is now increasingly focusing on the use of new deep learning
methods (see figure 1). for decades, machine learning approaches targeting nlp problems have been based on shallow models
(e.g., id166 and id28) trained on very high dimensional and sparse features. in the last few years, neural networks
based on dense vector representations have been producing superior results on various nlp tasks. this trend is sparked by
the success of id27s [2, 3] and deep learning methods [4]. deep learning enables multi-level automatic feature
representation learning. in contrast, traditional machine learning based nlp systems liaise heavily on hand-crafted features.
such hand-crafted features are time-consuming and often incomplete.

collobert et al. [5] demonstrated that a simple deep learning framework outperforms most state-of-the-art approaches in
several nlp tasks such as named-entity recognition (ner), id14 (srl), and id52. since then,
numerous complex deep learning based algorithms have been proposed to solve dif   cult nlp tasks. we review major deep
learning related models and methods applied to natural language tasks such as convolutional neural networks (id98s), recurrent
neural networks (id56s), and id56s. we also discuss memory-augmenting strategies, attention mechanisms
and how unsupervised models, id23 methods and recently, deep generative models have been employed for
language-related tasks.

to the best of our knowledge, this work is the    rst of its type to comprehensively cover the most popular deep learning
methods in nlp research today 1. the work by goldberg [6] only presented the basic principles for applying neural networks
to nlp in a tutorial manner. we believe this paper will give readers a more comprehensive idea of current practices in this
domain.

the structure of the paper is as follows: section ii introduces the concept of distributed representation, the basis of
sophisticated deep learning models; next, sections iii, iv, and v discuss popular models such as convolutional, recurrent,
and id56s, as well as their use in various nlp tasks; following, section vi lists recent applications of
id23 in nlp and new developments in unsupervised sentence representation learning; later, section vii

    means authors contributed equally
    corresponding author (e-mail: cambria@ntu.edu.sg)
1we intend to update this article with time as and when signi   cant advances are proposed and used by the community

2

fig. 1: percentage of deep learning papers in acl, emnlp, eacl, naacl over the last 6 years (long papers).

illustrates the recent trend of coupling deep learning models with memory modules;    nally, section viii summarizes the
performance of a series of deep learning methods on standard datasets about major nlp topics.

statistical nlp has emerged as the primary option for modeling complex natural language tasks. however, in its beginning,
it often used to suffer from the notorious curse of dimensionality while learning joint id203 functions of language models.
this led to the motivation of learning distributed representations of words existing in low-dimensional space [7].

ii. distributed representation

a. id27s

distributional vectors or id27s (fig. 2) essentially follow the distributional hypothesis, according to which words
with similar meanings tend to occur in similar context. thus, these vectors try to capture the characteristics of the neighbors of a
word. the main advantage of distributional vectors is that they capture similarity between words. measuring similarity between
vectors is possible, using measures such as cosine similarity. id27s are often used as the    rst data processing layer
in a deep learning model. typically, id27s are pre-trained by optimizing an auxiliary objective in a large unlabeled
corpus, such as predicting a word based on its context [8, 3], where the learned word vectors can capture general syntactical
and semantic information. thus, these embeddings have proven to be ef   cient in capturing context similarity, analogies and
due to its smaller dimensionality, are fast and ef   cient in processing core nlp tasks.

over the years, the models that create such embeddings have been shallow neural networks and there has not been need
for deep networks to create good embeddings. however, deep learning based nlp models invariably represent their words,
phrases and even sentences using these embeddings. this is in fact a major difference between traditional word count based
models and deep learning based models. id27s have been responsible for state-of-the-art results in a wide range
of nlp tasks [9, 10, 11, 12].

for example, glorot et al. [13] used embeddings along with stacked denoising autoencoders for id20 in senti-
ment classi   cation and hermann and blunsom [14] presented combinatory categorial autoencoders to learn the compositionality
of sentence. their wide usage across the recent literature shows their effectiveness and importance in any deep learning model
performing a nlp task.

distributed representations (embeddings) are mainly learned through context. during 1990s, several research develop-
ments [15] marked the foundations of research in id65. a more detailed summary of these early trends is

fig. 2: distributional vectors represented by a d-dimensional vector where d << v, where v is size of vocabulary. figure
source: http://veredshwartz.blogspot.sg.

king(-) man(+) womanqueen3

fig. 3: neural language model (figure reproduced from bengio et al. [7]). c(i) is the ith id27.

provided in [16, 17]. later developments were adaptations of these early works, which led to creation of topic models like
id44 [18] and language models [7]. these works laid out the foundations of representation learning in
natural language.

in 2003, bengio et al. [7] proposed a neural language model which learned distributed representations for words (fig. 3).
authors argued that these word representations, once compiled into sentence representations using joint id203 of word
sequences, achieved an exponential number of semantically neighboring sentences. this, in turn, helped in generalization
since unseen sentences could now gather higher con   dence if word sequences with similar words (in respect to nearby word
representation) were already seen.

collobert and weston [19] were the    rst work to show the utility of pre-trained id27s. they proposed a neural
network architecture that forms the foundation to many current approaches. the work also establishes id27s as
a useful tool for nlp tasks. however, the immense popularization of id27s was arguably due to mikolov et al.
[3] who proposed the continuous bag-of-words (cbow) and skip-gram models to ef   ciently construct high-quality distributed
vector representations. propelling their popularity was the unexpected side effect of the vectors exhibiting compositionality, i.e.,
adding two word vectors results in a vector that is a semantic composite of the individual words, e.g.,    man    +    royal    =    king   .
the theoretical justi   cation for this behavior was recently given by gittens et al. [20], which stated that compositionality is
seen only when certain assumptions are held, e.g., the assumption that words need to be uniformly distributed in the embedding
space.

glove by pennington et al. [21] is another famous id27 method which is essentially a    count-based    model.
here, the word co-occurrence count matrix is pre-processed by normalizing the counts and log-smoothing operation. this
matrix is then factorized to get lower dimensional representations which is done by minimizing a    reconstruction loss   .

below, we provide a brief description of the id97 method proposed by mikolov et al. [3].

b. id97

id27s were revolutionized by mikolov et al. [8, 3] who proposed the cbow and skip-gram models. cbow
computes the id155 of a target word given the context words surrounding it across a window of size k. on
the other hand, the skip-gram model does the exact opposite of the cbow model, by predicting the surrounding context words
given the central target word. the context words are assumed to be located symmetrically to the target words within a distance
equal to the window size in both directions. in unsupervised settings, the id27 dimension is determined by the
accuracy of prediction. as the embedding dimension increases, the accuracy of prediction also increases until it converges at
some point, which is considered the optimal embedding dimension as it is the shortest without compromising accuracy.

let us consider a simpli   ed version of the cbow model where only one word is considered in the context. this essentially

replicates a bigram language model.

as shown in fig. 4, the cbow model is a simple fully connected neural network with one hidden layer. the input layer,
which takes the one-hot vector of context word has v neurons while the hidden layer has n neurons. the output layer is softmax
id203 over all words in the vocabulary. the layers are connected by weight matrix w     rv   n and w
(cid:48)     rh  v ,

table look-up using matrix cword indexword indexithoutput=p(wt=i   context)c(wt   n+1)c(wt   1)wt   n+1wt   1softmax classi   cationtanh activationconcatenation4

fig. 4: model for cbow (figure source: rong [22])

respectively. each word from the vocabulary is    nally represented as two learned vectors vc and vw, corresponding to context
and target word representations, respectively. thus, kth word in the vocabulary will have

vc = w(k,.) and vw = w

(cid:48)
(.,k)

(1)

overall, for any word wi with given context word c as input,

(2)
the parameters    = {vw, vc}w,c     vocab are learned by de   ning the objective function as the log-likelihood and    nding its
gradient as

where, ui = vt
wi

i=1 eui

= yi =

.vc

p

c

(cid:16) wi

(cid:17)

eui(cid:80)v
(cid:88)

(cid:16)

(cid:16) w
(cid:17)(cid:17)
(cid:17)(cid:17)
(cid:16) w

p

c

l(  ) =

log

w   vocab

   l(  )
   vw

= vc

1     p

c

(cid:16)

in the general cbow model, all the one-hot vectors of context words are taken as input simultaneously, i.e,

h = wt(x1 + x2 + ... + xc)

one limitation of individual id27s is their inability to represent phrases [3], where the combination of two or
more words     e.g., idioms like    hot potato    or named entities such as    boston globe        does not represent the combination
of meanings of individual words. one solution to this problem, as explored by mikolov et al. [3], is to identify such phrases
based on word co-occurrence and train embeddings for them separately. later methods have explored directly learning id165
embeddings from unlabeled data [23].

another limitation comes from learning embeddings based only on a small window of surrounding words, sometimes
words such as good and bad share almost the same embedding [24], which is problematic if used in tasks such as sentiment
analysis [25]. at times these embeddings cluster semantically-similar words which have opposing sentiment polarities. this
leads the downstream model used for the id31 task to be unable to identify this contrasting polarities leading to
poor performance. tang et al. [26] addressed this problem by proposing sentiment speci   c id27 (sswe). authors
incorporated the supervised sentiment polarity of text in their id168s while learning the embeddings.

a general caveat for id27s is that they are highly dependent on the applications in which it is used. labutov
and lipson [27] proposed task speci   c embeddings which retrain the id27s to align them in the current task space.
this is very important as training embeddings from scratch requires large amount of time and resource. mikolov et al. [8] tried
to address this issue by proposing negative sampling which does frequency-based sampling of negative terms while training
the id97 model.

traditional id27 algorithms assign a distinct vector to each word. this makes them unable to account for
polysemy. in a recent work, upadhyay et al. [28] provided an innovative way to address this de   cit. the authors leveraged
multilingual parallel data to learn multi-sense id27s. for example, the english word bank, when translated to
french provides two different words: banc and banque representing    nancial and geographical meanings, respectively. such
multilingual distributional information helped them in accounting for polysemy.

table i provides a directory of existing frameworks that are frequently used for creating embeddings which are further

incorporated into deep learning models.

(3)

(4)

(5)

5

language

java
java
python
python
python
python
python

url

https://github.com/fozziethebeat/s-space

https://github.com/semanticvectors/
https://radimrehurek.com/gensim/

https://github.com/jimmycallin/pydsm

http://clic.cimec.unitn.it/composes/toolkit/

https://fasttext.cc/

https://tfhub.dev/google/elmo/2

framework

s-space

semanticvectors

gensim
pydsm
dissect
fasttext

elmo

table i: frameworks providing id27 tools and methods.

c. character embeddings

id27s are able to capture syntactic and semantic information, yet for tasks such as pos-tagging and ner, intra-
word morphological and shape information can also be very useful. generally speaking, building natural language understanding
systems at the character level has attracted certain research attention [29, 30, 31, 32]. better results on morphologically rich
languages are reported in certain nlp tasks. santos and guimaraes [31] applied character-level representations, along with
id27s for ner, achieving state-of-the-art results in portuguese and spanish corpora. kim et al. [29] showed positive
results on building a neural language model using only character embeddings. ma et al. [33] exploited several embeddings,
including character trigrams, to incorporate prototypical and hierarchical information for learning pre-trained label embeddings
in the context of ner.

a common phenomenon for languages with large vocabularies is the unknown word issue, also known as out-of-vocabulary
(oov) words. character embeddings naturally deal with it since each word is considered as no more than a composition
of individual letters. in languages where text is not composed of separated words but individual characters and the semantic
meaning of words map to its compositional characters (such as chinese), building systems at the character level is a natural
choice to avoid id40 [34]. thus, works employing deep learning applications on such languages tend to prefer
character embeddings over word vectors [35]. for example, peng et al. [36] proved that radical-level processing could greatly
improve sentiment classi   cation performance. in particular, the authors proposed two types of chinese radical-based hierarchical
embeddings, which incorporate not only semantics at radical and character level, but also sentiment information. bojanowski
et al. [37] also tried to improve the representation of words by using character-level information in morphologically-rich
languages. they approached the skip-gram method by representing words as bag-of-character id165s. their work thus had
the effectiveness of the skip-gram model along with addressing some persistent issues of id27s. the method was
also fast, which allowed training models on large corpora quickly. popularly known as fasttext, such a method stands out over
previous methods in terms of speed, scalability, and effectiveness.

apart from character embeddings, different approaches have been proposed for oov handling. herbelot and baroni [38]
provided on-the-   y oov handling by initializing the unknown words as the sum of the context words and re   ning these
words with a high learning rate. however, their approach is yet to be tested on typical nlp tasks. pinter et al. [39] provided
an interesting approach of training a character-based model to recreate pre-trained embeddings. this allowed them to learn a
compositional mapping form character to id27, thus tackling the oov problem.

despite the ever growing popularity of distributional vectors, recent discussions on their relevance in the long run have
cropped up. for example, lucy and gauthier [40] has recently tried to evaluate how well the word vectors capture the
necessary facets of conceptual meaning. the authors have discovered severe limitations in perceptual understanding of the
concepts behind the words, which cannot be inferred from id65 alone. a possible direction for mitigating
these de   ciencies will be grounded learning, which has been gaining popularity in this research domain.

d. contextualized id27s

the quality of word representations is generally gauged by its ability to encode syntactical information and handle polysemic
behavior (or word senses). these properties result in improved semantic word representations. recent approaches in this area
encode such information into its embeddings by leveraging the context. these methods provide deeper networks that calculate
word representations as a function of its context.

traditional id27 methods such as id97 and glove consider all the sentences where a word is present in
order to create a global vector representation of that word. however, a word can have completely different senses or meanings
in the contexts. for example, lets consider these two sentences - 1)    the bank will not be accepting cash on saturdays    2)
   the river over   owed the bank.   . the word senses of bank are different in these two sentences depending on its context.
reasonably, one might want two different vector representations of the word bank based on its two different word senses.
the new class of models adopt this reasoning by diverging from the concept of global word representations and proposing
contextual id27s instead.

embedding from language model (elmo) [41] is one such method that provides deep contextual embeddings. elmo
produces id27s for each context where the word is used, thus allowing different representations for varying senses

6

(6)

(7)

(8)

k

of the same word. speci   cally, for n different sentences where a word w is present, elmo generates n different representations
of w i.e., w1, w2,   ,wn .

the mechanism of elmo is based on the representation obtained from a bidirectional language model. a bidirectional
language model (bilm) constitutes of two language models (lm) 1) forward lm and 2) backward lm. a forward lm takes
input representation xlm
for each of the kth token and passes it through l layers of forward lstm to get representations
      
(cid:81)n
k,j where j = 1, . . . , l. each of these representations, being hidden representations of recurrent neural networks, is context
h lm
dependent. a forward lm can be seen as a method to model the joint id203 of a sequence of tokens: p (t1, t2, . . . , tn ) =
k=1 p (tk|t1, t2, . . . , tk   1). at a timestep k    1 the forward lm predicts the next token tk given the previous observed tokens
tokens: p (t1, t2, . . . , tn ) =(cid:81)n
t1, t2, ..., tk. this is typically achieved by placing a softmax layer on top of the    nal lstm in a forward lm. on the other
hand, a backward lm models the same joint id203 of the sequence by predicting the previous token given the future
k=1 p (tk|tk+1, tk+2, . . . , tn ). in other words, a backward lm is similar to forward lm which
processes a sequence with the order being reversed. the training of the bilm model involves modeling the log-likelihood of
both the sentence orientations. finally, hidden representations from both lms are concetenated to compose the    nal token
vectors [42].

for each tokem, elmo extracts the intermediate layer representations from the bilm and performs a linear combination

based on the given downstream task. a l-layer bilm contains 2l + 1 set of representations as shown below -

(cid:111)

      
k,j |j = 1, . . . , l
h lm

rk =

,

k

xlm

      
h lm
k,j ,

(cid:110)
=(cid:8)hlm
k,j |j = 0, . . . , l(cid:9)
(cid:104)      

      
h lm
k,j

(cid:105)    j = 1, . . . , l.
l(cid:88)

k = e(cid:0)rk;   task(cid:1) =   task

elmotask

stask
j hlm
k,j

k,0 is the token representation at the lowest level. one can use either character or id27s to initialize

here, hlm
k,0 . for other values of j,
hlm

h lm
k,j ,
elmo    attens all layers in r in a single vector such that -

hlm
k,j =

j=0

j

in eq. 8, stask
is the softmax-normalized weight vector to combine the representations of different layers.   task is a hyper-
parameter which helps in optimization and task speci   c scaling of the elmo representation. elmo produces varied word
representations for the same word in different sentences. according to peters et al. [41], it is always bene   cial to combine
elmo word representations with standard global word representations like glove and id97.

off-late, there has been a surge of interest in pre-trained language models for myriad of natural language tasks [43].
id38 is chosen as the pre-training objective as it is widely considered to incorporate multiple traits of natual
language understanding and generation. a good language model requires learning complex characteristics of language involving
syntactical properties and also semantical coherence. thus, it is believed that unsupervised training on such objectives would
infuse better linguistic knowledge into the networks than random initialization. the generative pre-training and discriminative
   ne-tuning procedure is also desirable as the pre-training is unsupervised and does not require any manual labeling.

radford et al. [44] proposed similar pre-trained model, the openai-gpt, by adapting the transformer (see section iv-e).
recently, devlin et al. [45] proposed bert which utilizes a transformer network to pre-train a language model for extracting
contextual id27s. unlike elmo and openai-gpt, bert uses different pre-training tasks for id38.
in one of the tasks, bert randomly masks a percentage of words in the sentences and only predicts those masked words. in
the other task, bert predicts the next sentence given a sentence. this task in particular tries to model the relationship among
two sentences which is supposedly not captured by traditional bidirectional language models. consequently, this particular
pre-training scheme helps bert to outperform state-of-the-art techniques by a large margin on key nlp tasks such as qa,
natural language id136 (nli) where understanding relation among two sentences is very important. we discuss the impact
of these proposed models and the performance achieved by them in section viii-i.

the described approaches for contextual id27s promises better quality representations for words. the pre-trained
deep language models also provide a headstart for downstream tasks in the form of id21. this approach has been
extremely popular in id161 tasks. whether there would be similar trends in the nlp community, where researchers
and practitioners would prefer such models over traditional variants remains to be seen in the future.

iii. convolutional neural networks

following the popularization of id27s and its ability to represent words in a distributed space, the need arose
for an effective feature function that extracts higher-level features from constituting words or id165s. these abstract features
would then be used for numerous nlp tasks such as id31, summarization, machine translation, and question
answering (qa). id98s turned out to be the natural choice given their effectiveness in id161 tasks [46, 47, 48].

7

fig. 5: id98 framework used to perform word wise class prediction (figure source: collobert and weston [19])

the use of id98s for sentence modeling traces back to collobert and weston [19]. this work used id72 to
output multiple predictions for nlp tasks such as pos tags, chunks, named-entity tags, semantic roles, semantically-similar
words and a language model. a look-up table was used to transform each word into a vector of user-de   ned dimensions.
thus, an input sequence {s1, s2, ...sn} of n words was transformed into a series of vectors {ws1, ws2 , ...wsn} by applying
the look-up table to each of its words (fig. 5).

this can be thought of as a primitive id27 method whose weights were learned in the training of the network.
in [5], collobert extended his work to propose a general id98-based framework to solve a plethora of nlp tasks. both these
works triggered a huge popularization of id98s amongst nlp researchers. given that id98s had already shown their mettle
for id161 tasks, it was easier for people to believe in their performance.

id98s have the ability to extract salient id165 features from the input sentence to create an informative latent semantic
representation of the sentence for downstream tasks. this application was pioneered by collobert et al. [5], kalchbrenner et al.
[49], kim [50], which led to a huge proliferation of id98-based networks in the succeeding literature. below, we describe the
working of a simple id98-based sentence modeling network:

a. basic id98

1) sentence modeling: for each sentence, let wi     rd represent the id27 for the ith word in the sentence,
where d is the dimension of the id27. given that a sentence has n words, the sentence can now be represented as
an embedding matrix w     rn  d. fig. 6 depicts such a sentence as an input to the id98 framework.
let wi:i+j refer to the concatenation of vectors wi, wi+1, ...wj. convolution is performed on this input embedding layer.
it involves a    lter k     rhd which is applied to a window of h words to produce a new feature. for example, a feature ci is
generated using the window of words wi:i+h   1 by

(9)
here, b     r is the bias term and f is a non-linear activation function, for example the hyperbolic tangent. the    lter k is
applied to all possible windows using the same weights to create the feature map.

ci = f (wi:i+h   1.kt + b)

c = [c1, c2, ..., cn   h+1]

(10)

in a id98, a number of convolutional    lters, also called kernels (typically hundreds), of different widths slide over the
entire id27 matrix. each kernel extracts a speci   c pattern of id165. a convolution layer is usually followed by
a max-pooling strategy,   c = max{c}, which subsamples the input typically by applying a max operation on each    lter. this
strategy has two primary reasons.

wown   1input sentencelookup tablefeature 1feature kconvolution layermax-pool over timefully connected layersoftmax classi   cationw18

fig. 6: id98 modeling on text (figure source: zhang and wallace [51])

firstly, max pooling provides a    xed-length output which is generally required for classi   cation. thus, regardless the size of
the    lters, max pooling always maps the input to a    xed dimension of outputs. secondly, it reduces the output   s dimensionality
while keeping the most salient id165 features across the whole sentence. this is done in a translation invariant manner where
each    lter is now able to extract a particular feature (e.g., negations) from anywhere in the sentence and add it to the    nal
sentence representation.

the id27s can be initialized randomly or pre-trained on a large unlabeled corpora (as in section ii). the
latter option is sometimes found bene   cial to performance, especially when the amount of labeled data is limited [50]. this
combination of convolution layer followed by max pooling is often stacked to create deep id98 networks. these sequential
convolutions help in improved mining of the sentence to grasp a truly abstract representations comprising rich semantic
information. the kernels through deeper convolutions cover a larger part of the sentence until    nally covering it fully and
creating a global summarization of the sentence features.

2) window approach: the above-mentioned architecture allows for modeling of complete sentences into sentence repre-
sentations. however, many nlp tasks, such as ner, id52, and srl, require word-based predictions. to adapt id98s
for such tasks, a window approach is used, which assumes that the tag of a word primarily depends on its neighboring words.
for each word, thus, a    xed-size window surrounding itself is assumed and the sub-sentence ranging within the window is
considered. a standalone id98 is applied to this sub-sentence as explained earlier and predictions are attributed to the word
in the center of the window. following this approach, poria et al. [52] employed a multi-level deep id98 to tag each word in
a sentence as a possible aspect or non-aspect. coupled with a set of linguistic patterns, their ensemble classi   er managed to
perform well in aspect detection.

the ultimate goal of word-level classi   cation is generally to assign a sequence of labels to the entire sentence. in such cases,
id170 techniques such as conditional random    eld (crf) are sometimes employed to better capture dependencies
between adjacent class labels and    nally generate cohesive label sequence giving maximum score to the whole sentence [53].
to get a larger contextual range, the classic window approach is often coupled with a time-delay neural network (tdnn) [54].
here, convolutions are performed across all windows throughout the sequence. these convolutions are generally constrained
by de   ning a kernel having a certain width. thus, while the classic window approach only considers the words in the window
around the word to be labeled, tdnn considers all windows of words in the sentence at the same time. at times, tdnn
layers are also stacked like id98 architectures to extract local features in lower layers and global features in higher layers [5].

b. applications

in this section, we present some of the crucial works that employed id98s on nlp tasks to set state-of-the-art benchmarks

in their respective times.

kim [50] explored using the above architecture for a variety of sentence classi   cation tasks, including sentiment, subjectivity
and question type classi   cation, showing competitive results. this work was quickly adapted by researchers given its simple
yet effective network. after training for a speci   c task, the randomly initialized convolutional kernels became speci   c id165
feature detectors that were useful for that target task (fig. 7). this simple network, however, had many shortcomings with the
id98   s inability to model long distance dependencies standing as the main issue.

9

(a) figure a

(b) figure b

fig. 7: top 7-grams by four learned 7-gram kernels; each kernel
source: kalchbrenner et al. [49])

is sensitive to a speci   c kind of 7-gram (figure

this issue was partly handled by kalchbrenner et al. [49], who published a prominent paper where they proposed a dynamic
convolutional neural network (did98) for semantic modeling of sentences. they proposed dynamic k-max pooling strategy
which, given a sequence p selects the k most active features. the selection preserved the order of the features but was insensitive
to their speci   c positions (fig. 8). built on the concept of tdnn, they added this dynamic k-max pooling strategy to create
a sentence model. this combination allowed    lters with small width to span across a long range within the input sentence,
thus accumulating crucial information across the sentence. in the induced subgraph (fig. 8), higher order features had highly
variable ranges that could be either short and focused or global and long as the input sentence. they applied their model on
multiple tasks, including sentiment prediction and question type classi   cation, achieving signi   cant results. overall, this work
commented on the range of individual kernels while trying to model contextual semantics and proposed a way to extend their
reach.

tasks involving id31 also require effective extraction of aspects along with their sentiment polarities [55].
ruder et al. [56] applied a id98 where in the input they concatenated an aspect vector with the id27s to get
competitive results. id98 modeling approach varies amongst different length of texts. such differences were seen in many
works like johnson and zhang [23], where performance on longer text worked well as opposed to shorter texts. wang et al.
[57] proposed the usage of id98 for modeling representations of short texts, which suffer from the lack of available context
and, thus, require extra efforts to create meaningful representations. the authors proposed semantic id91 which introduced
multi-scale semantic units to be used as external knowledge for the short texts. id98 was used to combine these units and
form the overall representation. in fact, this requirement of high context information can be thought of as a caveat for id98-
based models. nlp tasks involving microtexts using id98-based methods often require the need of additional information and
external knowledge to perform as per expectations. this fact was also observed in [58], where authors performed sarcasm
detection in twitter texts using a id98 network. auxiliary support, in the form of pre-trained networks trained on emotion,
sentiment and personality datasets was used to achieve state-of-the-art performance.

id98s have also been extensively used in other tasks. for example, denil et al. [59] applied did98 to map meanings of
words that constitute a sentence to that of documents for summarization. the did98 learned convolution    lters at both the
sentence and document level, hierarchically learning to capture and compose low-level lexical features into high-level semantic
concepts. the focal point of this work was the introduction of a novel visualization technique of the learned representations,
which provided insights not only in the learning process but also for id54 of texts.

id98 models are also suitable for certain nlp tasks that require semantic matching beyond classi   cation [60]. a similar
model to the above id98 architecture (fig. 6) was explored in [61] for information retrieval. the id98 was used for projecting
queries and documents to a    xed-dimension semantic space, where cosine similarity between the query and documents was
used for ranking documents regarding a speci   c query. the model attempted to extract rich contextual structures in a query
or a document by considering a temporal context window in a word sequence. this captured the contextual features at the
word id165 level. the salient word id165s is then discovered by the convolution and max-pooling layers which are then
aggregated to form the overall sentence vector.

in the domain of qa, yih et al. [62] proposed to measure the semantic similarity between a question and entries in a
knowledge base (kb) to determine what supporting fact in the kb to look for when answering a question. to create semantic
representations, a id98 similar to the one in fig. 6 was used. unlike the classi   cation setting, the supervision signal came
from positive or negative text pairs (e.g., query-document), instead of class labels. subsequently, dong et al. [63] introduced
a multi-column id98 (mcid98) to analyze and understand questions from multiple aspects and create their representations.
mcid98 used multiple column networks to extract information from aspects comprising answer types and context from the
input questions. by representing entities and relations in the kb with low-dimensional vectors, they used question-answer
pairs to train the id98 model so as to rank candidate answers. severyn and moschitti [64] also used id98 network to model
optimal representations of question and answer sentences. they proposed additional features in the embeddings in the form
of relational information given by matching words between the question and answer pair. these parameters were tuned by the

10

fig. 8: did98 subgraph. with dynamic pooling, a    lter with small width at the higher layers can relate phrases far apart in
the input sentence (figure source: kalchbrenner et al. [49])

network. this simple network was able to produce comparable results to state-of-the-art methods.

id98s are wired in a way to capture the most important information in a sentence. traditional max-pooling strategies
perform this in a translation invariant form. however, this often misses valuable information present in multiple facts within
the sentence. to overcome this loss of information for multiple-event modeling, chen et al. [65] proposed a modi   ed pooling
strategy: dynamic multi-pooling id98 (dmid98). this strategy used a novel dynamic multi-pooling layer that, as the name
suggests, incorporates event triggers and arguments to reserve more crucial information from the pooling layer.

id98s inherently provide certain required features like local connectivity, weight sharing, and pooling. this puts forward
some degree of invariance which is highly desired in many tasks. id103 also requires such invariance and, thus,
abdel-hamid et al. [66] used a hybrid id98-id48 model which provided invariance to frequency shifts along the frequency
axis. this variability is often found in speech signals due to speaker differences. they also performed limited weight sharing
which led to a smaller number of pooling parameters, resulting in lower computational complexity. palaz et al. [67] performed
extensive analysis of id98-based id103 systems when given raw speech as input. they showed the ability of
id98s to directly model the relationship between raw input and phones, creating a robust automatic id103 system.
tasks like machine translation require perseverance of sequential information and long-term dependency. thus, structurally
they are not well suited for id98 networks, which lack these features. nevertheless, tu et al. [68] addressed this task by
considering both the semantic similarity of the translation pair and their respective contexts. although this method did not
address the sequence perseverance problem, it allowed them to get competitive results amongst other benchmarks.

overall, id98s are extremely effective in mining semantic clues in contextual windows. however, they are very data heavy
models. they include a large number of trainable parameters which require huge training data. this poses a problem when
scarcity of data arises. another persistent issue with id98s is their inability to model long-distance contextual information and
preserving sequential order in their representations [49, 68]. other networks like recursive models (explained below) reveal
themselves as better suited for such learning.

iv. recurrent neural networks

id56s [69] use the idea of processing sequential information. the term    recurrent    applies as they perform the same task
over each instance of the sequence such that the output is dependent on the previous computations and results. generally, a
   xed-size vector is produced to represent a sequence by feeding tokens one by one to a recurrent unit. in a way, id56s have
   memory    over previous computations and use this information in current processing. this template is naturally suited for
many nlp tasks such as id38 [2, 70, 71], machine translation [72, 73, 74], id103 [75, 76, 77, 78],
image captioning [79]. this made id56s increasingly popular for nlp applications in recent years.

a. need for recurrent networks

in this section, we analyze the fundamental properties that favored the popularization of id56s in a multitude of nlp tasks.
given that an id56 performs sequential processing by modeling units in sequence, it has the ability to capture the inherent
sequential nature present in language, where units are characters, words or even sentences. words in a language develop their
semantical meaning based on the previous words in the sentence. a simple example stating this would be the difference in
meaning between    dog    and    hot dog   . id56s are tailor-made for modeling such context dependencies in language and similar
sequence modeling tasks, which resulted to be a strong motivation for researchers to use id56s over id98s in these areas.

xnx2x1xnx2x111

fig. 9: simple id56 network (figure source: lecun et al. [90])

another factor aiding id56   s suitability for sequence modeling tasks lies in its ability to model variable length of text,
including very long sentences, paragraphs and even documents [80]. unlike id98s, id56s have    exible computational steps
that provide better modeling capability and create the possibility to capture unbounded context. this ability to handle input of
arbitrary length became one of the selling points of major works using id56s [81].

many nlp tasks require semantic modeling over the whole sentence. this involves creating a gist of the sentence in
a    xed dimensional hyperspace. id56   s ability to summarize sentences led to their increased usage for tasks like machine
translation [82] where the whole sentence is summarized to a    xed vector and then mapped back to the variable-length target
sequence.

id56 also provides the network support to perform time distributed joint processing. most of the sequence labeling
tasks like id52 [32] come under this domain. more speci   c use cases include applications such as multi-label text
categorization [83], multimodal id31 [84, 85, 86], and subjectivity detection [87].

the above points enlist some of the focal reasons that motivated researchers to opt for id56s. however, it would be gravely
wrong to make conclusions on the superiority of id56s over other deep networks. recently, several works provided contrasting
evidence on the superiority of id98s over id56s. even in id56-suited tasks like id38, id98s achieved competitive
performance over id56s [88]. both id98s and id56s have different objectives when modeling a sentence. while id56s try
to create a composition of an arbitrarily long sentence along with unbounded context, id98s try to extract the most important
id165s. although they prove an effective way to capture id165 features, which is approximately suf   cient in certain sentence
classi   cation tasks, their sensitivity to word order is restricted locally and long-term dependencies are typically ignored.

yin et al. [89] provided interesting insights on the comparative performance between id56s and id98s. after testing on
multiple nlp tasks that included sentiment classi   cation, qa, and id52, they concluded that there is no clear winner:
the performance of each network depends on the global semantics required by the task itself.

below, we discuss some of the id56 models extensively used in the literature.

b. id56 models

1) simple id56: in the context of nlp, id56s are primarily based on elman network [69] and they are originally three-
layer networks. fig. 9 illustrates a more general id56 which is unfolded across time to accommodate a whole sequence. in
the    gure, xt is taken as the input to the network at time step t and st represents the hidden state at the same time step.
calculation of st is based as per the equation:

st = f (u xt + w st   1)

(11)

thus, st is calculated based on the current input and the previous time step   s hidden state. the function f is taken to be a
non-linear transformation such as tanh, relu and u, v, w account for weights that are shared across time. in the context of
nlp, xt typically comprises of one-hot encodings or embeddings. at times, they can also be abstract representations of textual
content. ot illustrates the output of the network which is also often subjected to non-linearity, especially when the network
contains further layers downstream.

the hidden state of the id56 is typically considered to be its most crucial element. as stated before, it can be considered
as the network   s memory element that accumulates information from other time steps. in practice, however, these simple id56
networks suffer from the infamous vanishing gradient problem, which makes it really hard to learn and tune the parameters
of the earlier layers in the network.

this limitation was overcome by various networks such as long short-term memory (lstm), id149 (grus),

and residual networks (resnets), where the    rst two are the most used id56 variants in nlp applications.

wotot   1ot+1unfoldvvvvwwuuuwuxtxt   1xt+1xohhtht   1ht+112

fig. 10: illustration of an lstm and gru gate (figure source: chung et al. [81])

(cid:21)

(cid:20) ht   1

xt

x =

2) long short-term memory: lstm [91, 92] (fig. 10) has additional    forget    gates over the simple id56. its unique

mechanism enables it to overcome both the vanishing and exploding gradient problem.

unlike the vanilla id56, lstm allows the error to back-propagate through unlimited number of time steps. consisting of
three gates: input, forget and output gates, it calculates the hidden state by taking a combination of these three gates as per
the equations below:

(12)

ft =   (wf .x + bf )
it =   (wi.x + bi)
ot =   (wo.x + bo)

(13)
(14)
(15)
(16)
(17)
3) id149: another gated id56 variant called gru [82] (fig. 10) of lesser complexity was invented with
empirically similar performances to lstm in most tasks. gru comprises of two gates, reset gate and update gate, and handles
the    ow of information like an lstm sans a memory unit. thus, it exposes the whole hidden content without any control.
being less complex, gru can be a more ef   cient id56 than lstm. the working of gru is as follows:

ct = ft (cid:12) ct   1 + it (cid:12) tanh(wc.x + bc)

ht = ot (cid:12) tanh(ct)

st = tanh(uz.xt + ws.(ht   1 (cid:12) r))

z =   (uz.xt + wz.ht   1)
r =   (ur.xt + wr.ht   1)

(18)
(19)
(20)
(21)
researchers often face the dilemma of choosing the appropriate gated id56. this also extends to developers working in
nlp. throughout the history, most of the choices over the id56 variant tended to be heuristic. chung et al. [81] did a critical
comparative evaluation of the three id56 variants mentioned above, although not on nlp tasks. they evaluated their work on
tasks relating to polyphonic music modeling and speech signal modeling. their evaluation clearly demonstrated the superiority
of the gated units (lstm and gru) over the traditional simple id56 (in their case, using tanh activation) (fig. 11). however,
they could not make any concrete conclusion about which of the two gating units was better. this fact has been noted in other
works too and, thus, people often leverage on other factors like computing power while choosing between the two.

ht = (1     z) (cid:12) st + z (cid:12) ht   1

c. applications

1) id56 for word-level classi   cation: id56s have had a huge presence in the    eld of word-level classi   cation. many of
their applications stand as state of the art in their respective tasks. lample et al. [93] proposed to use bidirectional lstm

tanhxt  coicht   1htxtszrht(1) long short-term memory(2) gated recurrent unit13

fig. 11: learning curves for training and validation sets of different types of units with respect to (top) the number of iterations
and (bottom) the wall clock time. y-axis corresponds to the negative log likelihood of the model shown in log-scale (figure
source: chung et al. [81])

for ner. the network captured arbitrarily long context information around the target word (curbing the limitation of a    xed
window size) resulting in two    xed-size vector, on top of which another fully-connected layer was built. they used a crf
layer at last for the    nal entity tagging.

id56s have also shown considerable improvement in id38 over traditional methods based on count statistics.
pioneering work in this    eld was done by graves [94], who introduced the effectiveness of id56s in modeling complex
sequences with long range context structures. he also proposed deep id56s where multiple layers of hidden states were used
to enhance the modeling. this work established the usage of id56s on tasks beyond the context of nlp. later, sundermeyer
et al. [95] compared the gain obtained by replacing a feed-forward neural network with an id56 when conditioning the
prediction of a word on the words ahead. in their work, they proposed a typical hierarchy in neural network architectures
where feed-forward neural networks gave considerable improvement over traditional count-based language models, which in
turn were superseded by id56s and later by lstms. an important point that they mentioned was the applicability of their
conclusions to a variety of other tasks such as id151 [96].

2) id56 for sentence-level classi   cation: wang et al. [25] proposed encoding entire tweets with lstm, whose hidden
state is used for predicting sentiment polarity. this simple strategy proved competitive to the more complex did98 structure
by kalchbrenner et al. [49] designed to endow id98 models with ability to capture long-term dependencies. in a special case
studying negation phrase, the authors also showed that the dynamics of lstm gates can capture the reversal effect of the word
not.

similar to id98, the hidden state of an id56 can also be used for semantic matching between texts. in dialogue systems,
lowe et al. [97] proposed to match a message with candidate responses with dual-lstm, which encodes both as    xed-size
vectors and then measure their inner product as the basis to rank candidate responses.

3) id56 for generating language: a challenging task in nlp is generating natural language, which is another natural
application of id56s. conditioned on textual or visual data, deep lstms have been shown to generate reasonable task-speci   c
text in tasks such as machine translation, image captioning, etc. in such cases, the id56 is termed a decoder.

in [74], the authors proposed a general deep lstm encoder-decoder framework that maps a sequence to another sequence.
one lstm is used to encode the    source    sequence as a    xed-size vector, which can be text in the original language (machine
translation), the question to be answered (qa) or the message to be replied to (dialogue systems). the vector is used as the
initial state of another lstm, named the decoder. during id136, the decoder generates tokens one by one, while updating
its hidden state with the last generated token. id125 is often used to approximate the optimal sequence.

sutskever et al. [74] experimented with 4-layer lstm on a machine translation task in an end-to-end fashion, showing
competitive results. in [99], the same encoder-decoder framework is employed to model human conversations. when trained
on more than 100 million message-response pairs, the lstm decoder is able to generate very interesting responses in the open

14

fig. 12: lstm decoder combined with a id98 image embedder to generate image captioning (figure source: vinyals et al.
[98])

domain. it is also common to condition the lstm decoder on additional signal to achieve certain effects. in [100], the authors
proposed to condition the decoder on a constant persona vector that captures the personal information of an individual speaker.
in the above cases, language is generated based mainly on the semantic vector representing textual input. similar frameworks
have also been successfully used in image-based language generation, where visual features are used to condition the lstm
decoder (fig. 12).

visual qa is another task that requires language generation based on both textual and visual clues. malinowski et al. [101]
were the    rst to provide an end-to-end deep learning solution where they predicted the answer as a set of words conditioned
on the input image modeled by a id98 and text modeled by an lstm (fig. 13).

kumar et al. [102] tackled this problem by proposing an elaborated network termed dynamic memory network (dmn),
which had four sub-modules. the idea was to repeatedly attend to the input text and image to form episodes of information
improved at each iteration. attention networks were used for    ne-grained focus on input text phrases.

d. attention mechanism

one potential problem that the traditional encoder-decoder framework faces is that the encoder at times is forced to encode
information which might not be fully relevant to the task at hand. the problem arises also if the input is long or very
information-rich and selective encoding is not possible.

for example, the task of text summarization can be cast as a sequence-to-sequence learning problem, where the input is the
original text and the output is the condensed version. intuitively, it is unrealistic to expect a    xed-size vector to encode all
information in a piece of text whose length can potentially be very long. similar problems have also been reported in machine
translation [103].

in tasks such as text summarization and machine translation, certain alignment exists between the input text and the output
text, which means that each token generation step is highly related to a certain part of the input text. this intuition inspires the

fig. 13: neural-image qa (figure source: malinowski et al. [101])

id98lstmlstmlstmimagewn   1p1pn   1output true image descriptionp2w2w115

attention mechanism. this mechanism attempts to ease the above problems by allowing the decoder to refer back to the input
sequence. speci   cally during decoding, in addition to the last hidden state and generated token, the decoder is also conditioned
on a    context    vector calculated based on the input hidden state sequence. the attention mechanism can be broadly seen as
mapping a query and a set of key-value pairs to an output, where all the mentioned components are vectors. the output is a
combination of the values whose weights are determined by the compatibility between the query and the corresponding keys.
this output amounts to the    context    of the input used in decoding the output.

bahdanau et al. [103]    rst applied the attention mechanism to machine translation, which improved the performance especially
for long sequences. in their work, the attention signal over the input hidden state sequence is determined with a multi-layer
id88 by the last hidden state of the decoder. by visualizing the attention signal over the input sequence during each
decoding step, a clear alignment between the source and target language can be demonstrated (fig. 14).

a similar approach was applied to the task of summarization by rush et al. [104] where each output word in the summary
was conditioned on the input sentence through an attention mechanism. the authors performed abstractive summarization which
is not very conventional as opposed to extractive summarization, but can be scaled up to large data with minimal linguistic
input.

in image captioning, xu et al. [105] conditioned the lstm decoder on different parts of the input image during each
decoding step. attention signal was determined by the previous hidden state and id98 features. in [106], the authors casted the
syntactical parsing problem as a sequence-to-sequence learning task by linearizing the parsing tree. the attention mechanism
proved to be more data-ef   cient in this work. a further step in referring to the input sequence was to directly copy words
or sub-sequences of the input onto the output sequence under a certain condition [107], which was useful in tasks such as
dialogue generation and text summarization. copying or generation was chosen at each time step during decoding [108].

in aspect-based id31, wang et al. [109]
proposed an attention-based solution where they used aspect
embeddings to provide additional support during classi   ca-
tion (fig. 15). the attention module focused on selective
regions of the sentence which affected the aspect
to be
classi   ed. this can be seen in fig. 17 where, for the aspect
service in (a), the attention module dynamically focused on
the phrase    fastest delivery times    and in (b) with the aspect
food, it identi   ed multiple key-points across the sentence
that
included    tasteless    and    too sweet   . recently, ma
et al. [110] augmented lstm with a hierarchical atten-
tion mechanism consisting of a target-level attention and a
sentence-level attention to exploit commonsense knowledge
for targeted aspect-based id31.

on the other hand, tang et al. [111] adopted a solution
based on a memory network (also known as memnet [112]),
which employed multiple-hop attention. the multiple at-
tention computation layer on the memory led to improved
lookup for most informational regions in the memory and
subsequently aided the classi   cation. their work stands as
the state of the art in this domain.

adopted for an increasing number of applications.

given the intuitive applicability of attention modules, they are still being actively investigated by nlp researchers and

fig. 14: word alignment matrix (figure source: bahdanau et al.
[103])

e. parallelized attention: the transformer

both id98s and id56s have been crucial in sequence transduction applications involving the encoder-decoder architecture.
attention-based mechanisms, as described above, have further boosted the capabilities of these models. however, one of the
bottlenecks suffered by these architectures is the sequential processing at the encoding step. to address this, vaswani et al.
[113] proposed the transformer which dispensed the recurrence and convolutions involved in the encoding step entirely and
based models only on attention mechanisms to capture the global relations between input and output. as a result, the overall
architecture became more parallelizable and required lesser time to train along with positive results on tasks ranging from
translation to parsing.

the transformer consists stacked layers in both encoder and decoder components. each layer has two sub-layers comprising
multi-head attention layer (figure 17) followed by a position-wise feed forward network. for set of queries q, keys k and

16

(22)
(23)

(24)

fig. 15: aspect classi   cation using attention (figure source: wang et al. [109])

fig. 16: multi-head attention: vaswani et al. [113])

values v , the multi-head attention module performs attention h times where the computation can be seen as:

multihead(q, k, v ) = concat(head1, head2, . . . , headh)w o
, v w v
i )

where headi = attention(qw q

i , kw k
i

and attention(q, k, v ) = softmax

(cid:19)

(cid:18) qk t   

dk

v

here, w [.]
and w o are projection parameters. incorporating other techniques such as residual connections [114], layer nor-
i
malization [115], dropouts, positional encodings, and others, the model achieves state-of-the-art results in english-german and
english-french translation and constituency parsing.

v. id56s

recurrent neural networks represent a natural way to model sequences. arguably, however, language exhibits a natural
recursive structure, where words and sub-phrases combine into phrases in a hierarchical manner. such structure can be
represented by a constituency parsing tree. thus, tree-structured models have been used to better make use of such syntactic
interpretations of sentence structure [4]. speci   cally, in a id56, the representation of each non-terminal
node in a parsing tree is determined by the representations of all its children.

weighted combinationlstmlstmlstmhidden representationinput sentencew1wnaspect embeddingh1hn  attentionw2a. basic model

17

in this section, we describe the basic structure of id56s. as shown in fig. 18a and 18b, the network g
de   nes a compositional function on the representations of phrases or words (b, c or a, p1) to compute the representation of a
higher-level phrase (p1 or p2). the representations of all nodes take the same form.

socher et al. [4] described multiple variations of this model. in its simplest form, g is de   ned as:

p1 = tanh

w

, p2 = tanh

w

(cid:18)

(cid:21)(cid:19)

(cid:20) b

c

(cid:18)

(cid:20) cb

(cid:21)(cid:19)

(cid:18)

(cid:18)

(cid:21)(cid:19)

(cid:20) a

p1

(cid:20) b

(cid:21)(cid:19)

in which the representation for each node is a d-dimensional vector and w     rd  2d.

another variation is the mv-id56 [116]. the idea is to represent every word and phrase as both a matrix and a vector.

when two constituents are combined, the matrix of one is multiplied with the vector of the other:

(26)
in which b, c, p1     rd, b, c, p1     rd  d, and wm     rd  2d. compared to the vanilla form, mv-id56 parameterizes the
compositional function with matrices corresponding to the constituents.

, p1 = tanh

p1 = tanh

wm

bc

w

c

the recursive neural tensor network (rntn) is proposed to introduce more interaction between the input vectors without

making the number of parameters exceptionally large like mv-id56. rntn is de   ned by:

c
where v     r2d  2d  d is a tensor that de   nes multiple bilinear forms.

c

p1 = tanh

v [1:d]

+ w

(cid:32)(cid:20) b

(cid:21)t

(cid:20) b

(cid:21)

(cid:21)(cid:33)

(cid:20) b

c

(25)

(27)

b. applications

one natural application of id56s is parsing [10]. a scoring function is de   ned on the phrase representation
to calculate the plausibility of that phrase. id125 is usually applied for searching the best tree. the model is trained
with the max-margin objective [117].

based on id56s and the parsing tree, socher et al. [4] proposed a phrase-level id31

framework (fig. 19), where each node in the parsing tree can be assigned a sentiment label.

socher et al. [116] classi   ed semantic relationships such as cause-effect or topic-message between nominals in a sentence by
building a single id152 for the minimal constituent including both terms. bowman et al. [118] proposed to
classify the logical relationship between sentences with id56s. the representations for both sentences are
fed to another neural network for relationship classi   cation. they show that both vanilla and tensor versions of the recursive
unit performed competitively in a id123 dataset.

to avoid the gradient vanishing problem, lstm units have also been applied to tree structures in [119]. the authors
showed improved sentence representation over linear lstm models, as clear improvement in id31 and sentence
relatedness test was observed.

a. id23 for sequence generation

vi. deep reinforced models and deep unsupervised learning

id23 is a method of training an agent to perform discrete actions before obtaining a reward. in nlp, tasks

concerning language generation can sometimes be cast as id23 problems.

fig. 17: focus of attention module on the sentence for certain aspects (figure source: wang et al. [109])

18

(a) id56s for phrase-level sentiment
classi   cation (figure source: socher et al. [4])

(b) id56s iteratively form high-level
representation from lower-level representations.

fig. 18: id56s

fig. 19: id56s applied on a sentence for sentiment classi   cation. note that    but    plays a crucial role on
determining the sentiment of the whole sentence (figure source: socher et al. [4])

in its original formulation, id56 language generators are typically trained by maximizing the likelihood of each token in the
ground-truth sequence given the current hidden state and the previous tokens. termed    teacher forcing   , this training scheme
provides the real sequence pre   x to the generator during each generation (loss evaluation) step. at test time, however, ground-
truth tokens are then replaced by a token generated by the model itself. this discrepancy between training and id136,
termed    exposure bias    [120, 121], can yield errors that can accumulate quickly along the generated sequence.

another problem with the word-level maximum likelihood strategy, when training auto-regressive language generation
models, is that the training objective is different from the test metric. it is unclear how the id165 overlap based metrics
(id7, id8) used to evaluate these tasks (machine translation, dialogue systems, etc.) can be optimized with the word-
level training strategy. empirically, dialogue systems trained with word-level maximum likelihood also tend to produce dull
and short-sighted responses [122], while text summarization tends to produce incoherent or repetitive summaries [108].

id23 offers a prospective to solve the above problems to a certain extent. in order to optimize the non-
differentiable id74 directly, ranzato et al. [121] applied the reinforce algorithm [123] to train id56-based
models for several sequence generation tasks (e.g., text summarization, machine translation and image captioning), leading to
improvements compared to previous supervised learning methods. in such a framework, the generative model (id56) is viewed
as an agent, which interacts with the external environment (the words and the context vector it sees as input at every time step).
the parameters of this agent de   nes a policy, whose execution results in the agent picking an action, which refers to predicting
the next word in the sequence at each time step. after taking an action the agent updates its internal state (the hidden units
of id56). once the agent has reached the end of a sequence, it observes a reward. this reward can be any developer-de   ned
metric tailored to a speci   c task. for example, li et al. [122] de   ned 3 rewards for a generated sentence based on ease of
answering, information    ow, and semantic coherence.

there are two well-known shortcomings of id23. to make id23 tractable, it is desired
to carefully handle the state and action space [124, 125], which in the end may restrict expressive power and learning capacity
of the model. secondly, the need for training the reward functions makes such models hard to design and measure at run
time [126, 127].

another approach for sequence-level supervision is to use the adversarial training technique [128], where the training objective

for the language generator is to fool another discriminator trained to distinguish generated sequences from real sequences. the
generator g and the discriminator d are trained jointly in a min-max game which ideally leads to g, generating sequences
indistinguishable from real ones. this approach can be seen as a variation of id3 in [128], where
g and d are conditioned on certain stimuli (for example, the source image in the task of image captioning). in practice,
the above scheme can be realized under the id23 paradigm with policy gradient. for dialogue systems, the
discriminator is analogous to a human turing tester, who discriminates between human and machine-produced dialogues [129].

b. unsupervised sentence representation learning

19

similar to id27s, distributed representation for sentences can also be learned in an unsupervised fashion. the
result of such unsupervised learning are    sentence encoders   , which map arbitrary sentences to    xed-size vectors that can
capture their semantic and syntactic properties. usually an auxiliary task has to be de   ned for the learning process.

similar to the skip-gram model [8] for learning id27s, the skip-thought model [130] was proposed for learning
sentence representation, where the auxiliary task was to predict two adjacent sentences (before and after) based on the given
sentence. the id195 model was employed for this learning task. one lstm encoded the sentence to a vector (distributed
representation). two other lstms decoded such representation to generate the target sequences. standard id195 training
process was used. after training, the encoder could be seen as a generic feature extractor (id27s were also learned
in the same time).

kiros et al. [130] veri   ed the quality of the learned sentence encoder on a range of sentence classi   cation tasks, showing
competitive results with a simple linear model based on the static feature vectors. however, the sentence encoder can also
be    ne-tuned in the supervised learning task as part of the classi   er. dai and le [43] investigated the use of the decoder to
reconstruct the encoded sentence itself, which resembled an autoencoder [131].

id38 could also be used as an auxiliary task when training lstm encoders, where the supervision signal
came from the prediction of the next token. dai and le [43] conducted experiments on initializing lstm models with learned
parameters on a variety of tasks. they showed that pre-training the sentence encoder on a large unsupervised corpus yielded
better accuracy than only pre-training id27s. also, predicting the next token turned out to be a worse auxiliary
objective than reconstructing the sentence itself, as the lstm hidden state was only responsible for a rather short-term objective.

c. deep generative models

recent success in generating realistic images has driven a series of efforts on applying deep generative models to text data.
the promise of such research is to discover rich structure in natural language while generating realistic sentences from a latent
code space. in this section, we review recent research on achieving this goal with id5 (vaes) [132] and
id3 (gans) [128].

fig. 20: id56-based vae for sentence generation (figure source: bowman et al. [133])

standard sentence autoencoders, as in the last section, do not impose any constraint on the latent space, as a result, they
fail when generating realistic sentences from arbitrary latent representations [133]. the representations of these sentences may
often occupy a small region in the hidden space and most of regions in the hidden space do not necessarily map to a realistic
sentence [134]. they cannot be used to assign probabilities to sentences or to sample novel sentences [133].

the vae imposes a prior distribution on the hidden code space which makes it possible to draw proper samples from
the model. it modi   es the autoencoder architecture by replacing the deterministic encoder function with a learned posterior
recognition model. the model consists of encoder and generator networks which encode data examples to latent representation
and generate samples from the latent space, respectively. it is trained by maximizing a variational lower bound on the log-
likelihood of observed data under the generative model.

lstmlstmlinearlinearlstm    zlstmlstmlstmx1x2x3<eos><eos>y1y2y1y2decoder encoder 20

fig. 21: multiple supporting facts were retrieved from the memory in order to answer a speci   c question using an attention
mechanism. the    rst hop uncovered the need for additional hops (figure source: sukhbaatar et al. [138])

bowman et al. [133] proposed an id56-based variational autoencoder generative model that incorporated distributed latent
representations of entire sentences (fig. 20). unlike vanilla id56 language models, this model worked from an explicit global
sentence representation. samples from the prior over these sentence representations produced diverse and well-formed sentences.
hu et al. [135] proposed generating sentences whose attributes are controlled by learning disentangled latent representations
with designated semantics. the authors augmented the latent code in the vae with a set of structured variables, each targeting
a salient and independent semantic feature of sentences. the model incorporated vae and attribute discriminators, in which
the vae component trained the generator to reconstruct real sentences for generating plausible text, while the discriminators
forced the generator to produce attributes coherent with the structured code. when trained on a large number of unsupervised
sentences and a small number of labeled sentences, hu et al. [135] showed that the model was able to generate plausible
sentences conditioned on two major attributes of english: tense and sentiment.

gan is another class of generative model composed of two competing networks. a generative neural network decodes latent
representation to a data instance, while the discriminative network is simultaneously taught to discriminate between instances
from the true data distribution and synthesized instances produced by the generator. gan does not explicitly represent the true
data distribution p(x).

zhang et al. [134] proposed a framework for employing lstm and id98 for adversarial training to generate realistic text. the
latent code z was fed to the lstm generator at every time step. id98 acted as a binary sentence classi   er which discriminated
between real data and generated samples. one problem with applying gan to text is that the gradients from the discriminator
cannot properly back-propagate through discrete variables. in [134], this problem was solved by making the word prediction
at every time    soft    at the id27 space. yu et al. [136] proposed to bypass this problem by modeling the generator
as a stochastic policy. the reward signal came from the gan discriminator judged on a complete sequence, and was passed
back to the intermediate state-action steps using monte carlo search.

the evaluation of deep generative models has been challenging. for text, it is possible to create oracle training data from a
   xed set of grammars and then evaluate generative models based on whether (or how well) the generated samples agree with
the prede   ned grammar [137]. another strategy is to evaluate id7 scores of samples on a large amount of unseen test data.
the ability to generate similar sentences to unseen real data is considered a measurement of quality [136].

vii. memory-augmented networks

the attention mechanism stores a series of hidden vectors of the encoder, which the decoder is allowed to access during the
generation of each token. here, the hidden vectors of the encoder can be seen as entries of the model   s    internal memory   .
recently, there has been a surge of interest in coupling neural networks with a form of memory, which the model can interact
with.

in [112], the authors proposed memory networks for qa tasks. in synthetic qa, a series of statements (memory entries)
were provided to the model as potential supporting facts to the question. the model learned to retrieve one entry at a time
from memory based on the question and previously retrieved memory. in large-scale realistic qa, a large set of commonsense
knowledge in the form of (subject, relation, object) triples were used as memory. sukhbaatar et al. [138] extended this work and
proposed end-to-end memory networks, where memory entries were retrieved in a    soft    manner with attention mechanism,
thus enabling end-to-end training. multiple rounds (hops) of information retrieval from memory were shown to be essential to
good performance and the model was able to retrieve and reason about several supporting facts to answer a speci   c question
(fig. 21). sukhbaatar et al. [138] also showed a special use of the model for id38, where each word in the
sentence was seen as a memory entry. with multiple hops, the model yielded results comparable to deep lstm models.

furthermore, dynamic memory networks (dmn) [102] improved upon previous memory-based models by employing neural
network models for input representation, attention, and answer mechanisms. the resulting model was applicable to a wide

21

range of nlp tasks (qa, id52, and id31), as every task could be cast to the <memory, question, answer>
triple format. xiong et al. [139] applied the same model to visual qa and proved that the memory module was applicable to
visual signals.

viii. performance of different models on different nlp tasks

we summarize the performance of a series of deep learning methods on standard datasets developed in recent years on some
major nlp topics. our goal is to show the readers common datasets used in the community and state-of-the-art results with
different models.

a. id52

the wsj-ptb (the wall street journal part of the id32 dataset) corpus contains 1.17 million tokens and has been
widely used for developing and evaluating id52 systems. gim  enez and marquez [140] employed one-against-all id166
based on manually-de   ned features within a seven-word window, in which some basic id165 patterns were evaluated to form
binary features such as:    previous word is the   ,    two preceding tags are dt nn   , etc. one characteristic of the id52
problem was the strong dependency between adjacent tags. with a simple left-to-right tagging scheme, this method modeled
dependencies between adjacent tags only by feature engineering. in an effort to reduce feature engineering, collobert et al. [5]
relied on only id27s within the word window by a multi-layer id88. incorporating crf was proven useful
in [5]. santos and zadrozny [32] concatenated id27s with character embeddings to better exploit morphological
clues. in [32], the authors did not consider crf, but since word-level decision was made on a context window, it can be
seen that dependencies were modeled implicitly. huang et al. [141] concatenated id27s and manually-designed
word-level features and employed bidirectional lstm to model arbitrarily long context. a series of ablative analysis suggested
that bi-directionality and crf both boosted performance. andor et al. [142] showed a transition-based approach that produces
competitive result with a simple feed-forward neural network. when applied to sequence tagging tasks, dmns [102] essentially
allowed for attending over the context multiple times by treating each id56 hidden state as a memory entry, each time focusing
on different parts of the context.

table ii: id52

paper

model

gim  enez and marquez [140]

collobert et al. [5]

santos and zadrozny [32]

id166 with manual feature pattern
mlp with id27s + crf
mlp with character+id27s

huang et al. [141]
huang et al. [141]
huang et al. [141]
huang et al. [141]
andor et al. [142]
kumar et al. [102]

lstm

bidirectional lstm

lstm-crf

bidirectional lstm-crf

transition-based neural network

dmn

wsj-ptb (per-token accuracy %)

97.16
97.29
97.32
97.29
97.40
97.54
97.55
97.45
97.56

b. parsing

there are two types of parsing: id33, which connects individual words with their relations, and constituency
parsing, which iteratively breaks text into sub-phrases. transition-based methods are a popular choice since they are linear in
the length of the sentence. the parser makes a series of decisions that read words sequentially from a buffer and combine them
incrementally into the syntactic structure [143]. at each time step, the decision is made based on a stack containing available
tree nodes, a buffer containing unread words and the obtained set of dependency arcs. chen and manning [143] modeled the
decision making at each time step with a neural network with one hidden layer. the input layer contained embeddings of
certain words, pos tags and arc labels, which came from the stack, the buffer and the set of arc labels.

tu et al. [68] extended the work of chen and manning [143] by employing a deeper model with 2 hidden layers. however,
both tu et al. [68] and chen and manning [143] relied on manual feature selecting from the parser state, and they only took
into account a limited number of latest tokens. dyer et al. [144] proposed stack-lstms to model arbitrarily long history. the
end pointer of the stack changed position as the stack of tree nodes could be pushed and popped. zhou et al. [145] integrated
id125 and contrastive learning for better optimization.

transition-based models were applied to constituency parsing as well. zhu et al. [146] based each transition action on
features such as the pos tags and constituent labels of the top few words of the stack and the buffer. by uniquely representing
the parsing tree with a linear sequence of labels, vinyals et al. [106] applied the seq2seid24 method to this problem.

table iii: parsing (uas/las = unlabeled/labeled attachment score; wsj = the wall street journal section of id32)

22

parsing type

id33

constituency parsing

paper

chen and manning [143]

weiss et al. [147]
dyer et al. [144]
zhou et al. [145]
petrov et al. [148]
socher et al. [10]
zhu et al. [146]

vinyals et al. [106]

model

fully-connected nn with features including pos

deep fully-connected nn with features including pos

stack-lstm

beam contrastive model

id140 (pid18)

id56s

feature-based transition parsing

seq2seid24 with lstm+attention

wsj

91.8/89.6 (uas/las)
94.3/92.4 (uas/las)
93.1/90.9 (uas/las)
93.31/92.37 (uas/las)

91.8 (f1 score)
90.29 (f1 score)
91.3 (f1 score)
93.5 (f1 score)

table iv: named-entity recognition

chiu and nichols [150]

bi-lstm with word+char+lexicon embeddings

paper

collobert et al. [5]
passos et al. [149]

luo et al. [151]
lample et al. [93]
lample et al. [93]
strubell et al. [152]

model

mlp with id27s+gazetteer
lexicon infused phrase embeddings

semi-crf jointly trained with linking

bi-lstm-crf with word+char embeddings

bi-lstm with word+char embeddings

dilated id98 with crf

conll 2003 (f1 %)

89.59
90.90
90.77
91.20
90.94
89.15
90.54

c. named-entity recognition

conll 2003 has been a standard english dataset for ner, which concentrates on four types of named entities: people,
locations, organizations and miscellaneous entities. ner is one of the nlp problems where lexicons can be very useful.
collobert et al. [5]    rst achieved competitive results with neural structures augmented by gazetteer features. chiu and nichols
[150] concatenated lexicon features, character embeddings and id27s and fed them as input to a bidirectional
lstm. on the other hand, lample et al. [93] only relied on character and id27s, with pre-training embeddings
on large unsupervised corpora, they achieved competitive results without using any lexicon. similar to id52, crf also
boosted the performance of ner, as demonstrated by the comparison in [93]. overall, we see that bidirectional lstm with
crf acts as a strong model for nlp problems related to id170.

passos et al. [149] proposed to modify skip-gram models to better learn entity-type related id27s that can leverage
information from relevant lexicons. luo et al. [151] jointly optimized the entities and the linking of entities to a kb. strubell
et al. [152] proposed to use dilated convolutions, de   ned over a wider effective input width by skipping over certain inputs at
a time, for better parallelization and context modeling. the model showed signi   cant speedup while retaining accuracy.

d. id14

id14 (srl) aims to discover the predicate-argument structure of each predicate in a sentence. for each
target verb (predicate), all constituents in the sentence which take a semantic role of the verb are recognized. typical semantic
arguments include agent, patient, instrument, etc., and also adjuncts such as locative, temporal, manner, cause, etc. [153].
table v shows the performance of different models on the conll 2005 and 2012 datasets.

traditional srl systems consist of several stages: producing a parse tree, identifying which parse tree nodes represent the
arguments of a given verb, and    nally classifying these nodes to determine the corresponding srl tags. each classi   cation
process usually entails extracting numerous features and feeding them into statistical models [5].

given a predicate, t  ackstr  om et al. [154] scored a constituent span and its possible role to that predicate with a series of
features based on the parse tree. they proposed a id145 algorithm for ef   cient id136. collobert et al. [5]
achieved comparable results with a convolution neural networks augmented by parsing information provided in the form of
additional look-up tables. zhou and xu [153] proposed to use bidirectional lstm to model arbitrarily long context, which
proved to be successful without any parsing tree information. he et al. [155] further extended this work by introducing highway
connections [156], more advanced id173 and ensemble of multiple experts.

table v: id14

paper

collobert et al. [5]

t  ackstr  om et al. [154]
zhou and xu [153]

he et al. [155]

model

id98 with parsing features

manual features with dp for id136

bidirectional lstm

bidirectional lstm with highway connections

conll2005 (f1 %)

76.06
78.6
81.07
83.2

conll2012 (f1 %)

79.4
81.27
83.4

e. sentiment classi   cation

23

the stanford sentiment treebank (sst) dataset contains sentences taken from the movie review website rotten tomatoes.
it was proposed by pang and lee [157] and subsequently extended by socher et al. [4]. the annotation scheme has inspired
a new dataset for id31, called cmu-mosi, where sentiment is studied in a multimodal setup [158].

[4] and [119] were both recursive networks that relied on constituency parsing trees. their difference shows the effectiveness
of lstm over vanilla id56 in modeling sentences. on the other hand, tree-lstm performed better than linear bidirectional
lstm, implying that tree structures can potentially better capture the syntactical property of natural sentences. yu et al. [159]
proposed to re   ne pre-trained id27s with a sentiment lexicon, observing improved results based on [119].

kim [50] and kalchbrenner et al. [49] both used convolutional layers. the model [50] was similar to the one in fig. 6,
while kalchbrenner et al. [49] constructed the model in a hierarchical manner by interweaving k-max pooling layers with
convolutional layers.

table vi: sentiment classi   cation (sst-1 = stanford sentiment treebank,    ne-grained 5 classes socher et al. [4]; sst-2: the binary
version of sst-1; numbers are accuracies (%))

paper

socher et al. [4]

kim [50]

kalchbrenner et al. [49]

tai et al. [119]

le and mikolov [160]

tai et al. [119]
yu et al. [159]

kumar et al. [102]

model

recursive neural tensor network

multichannel id98

did98 with k-max pooling

bidirectional lstm

paragraph vector

constituency tree-lstm

tree-lstm with re   ned id27s

dmn

sst-1
45.7
47.4
48.5
48.5
48.7
51.0
54.0
52.1

sst-2
85.4
88.1
86.8
87.2
87.8
88.0
90.3
88.6

f. machine translation

the phrase-based smt framework [161] factorized the translation model into the translation probabilities of matching
phrases in the source and target sentences. cho et al. [82] proposed to learn the translation id203 of a source phrase
to a corresponding target phrase with an id56 encoder-decoder. such a scheme of scoring phrase pairs improved translation
performance. sutskever et al. [74], on the other hand, re-scored the top 1000 best candidate translations produced by an
smt system with a 4-layer lstm id195 model. dispensing the traditional smt system entirely, wu et al. [162] trained
a deep lstm network with 8 encoder and 8 decoder layers with residual connections as well as attention connections. wu
et al. [162] then re   ned the model by using id23 to directly optimize id7 scores, but they found that
the improvement in id7 scores by this method did not re   ect in human evaluation of translation quality. recently, gehring
et al. [163] proposed a id98-based seq2seid24 model for machine translation. the representation for each word in the
input is computed by id98 in a parallelized style for the attention mechanism. the decoder state is also determined by id98
with words that are already produced. vaswani et al. [113] proposed a self-attention-based model and dispensed convolutions
and recurrences entirely.

table vii: machine translation (numbers are id7 scores)

model

wmt2014 english2german wmt2014 english2french

paper

cho et al. [82]

sutskever et al. [74]

wu et al. [162]

gehring et al. [163]
vaswani et al. [113]

phrase table with neural features

reranking phrase-based smt best list with lstm id195
residual lstm id195 + id23 re   ning

id195 with id98
attention mechanism

26.30
26.36
28.4

34.50
36.5
41.16
41.29
41.0

g. id53

qa problems take many forms. some rely on large kbs to answer open-domain questions, while others answer a question
based on a few sentences or a paragraph (reading comprehension). for the former, we list (see table viii) several experiments
conducted on a large-scale qa dataset introduced by [164], where 14m commonsense knowledge triples are considered as the
kb. each question can be answered with a single-relation query. for the latter, we consider (see table viii) (1) the synthetic
dataset of babi [165], which requires the model to reason over multiple related facts to produce the right answer. it contains
20 synthetic tasks that test a model   s ability to retrieve relevant facts and reason over them. each task focuses on a different
skill such as basic coreference and size reasoning. (2) the stanford id53 dataset (squad) [166], consisting
of 100,000+ questions posed by crowdworkers on a set of wikipedia articles. the answer to each question is a segment of
text from the corresponding article.

the central problem of learning to answer single-relation queries is to    nd the single supporting fact in the database. fader
et al. [164] proposed to tackle this problem by learning a lexicon that maps natural language patterns to database concepts

24

(entities, relations, question patterns) based on a question id141 dataset. bordes et al. [167] embedded both questions
and kb triples as dense vectors and scored them with inner product.

weston et al. [112] took a similar approach by treating the kb as long-term memory, while casting the problem in the
framework of a memory network. on the babi dataset, sukhbaatar et al. [138] improved upon the original memory networks
model [112] by making the training procedure agnostic of the actual supporting fact, while kumar et al. [102] used neural
sequence models (gru) instead of neural bag-of-words models as in [138] and [112] to embed memories.

for models on the squad dataset, the goal is to determine the start point and end point of the answer segment. chen
et al. [168] encoded both the question and the words in context using lstms and used a bilinear matrix for calculating the
similarity between the two. shen et al. [169] proposed reasonet, a model that read a document repeatedly with attention on
different parts each time until a satisfying answer is found. yu et al. [170] replaced id56s with convolution and self-attention
for encoding the question and the context with signi   cant speed improvement.

table viii: id53
babi (mean accuracy %)

model

paraphrase-driven lexicon learning

weekly supervised embedding

memory networks

sukhbaatar et al. [138]

end-to-end memory networks

paper

fader et al. [164]
bordes et al. [167]
weston et al. [112]

kumar et al. [102]
chen et al. [168]
shen et al. [169]
yu et al. [170]

dmn

document reader

reasonet

qanet

93.3
88.4
93.6

farbes (accuracy %)

0.54
0.73
0.83

squad (em/f1 %)

70.0/79.0
69.1/78.9
76.2/84.6

h. dialogue systems

two types of dialogue systems have been developed: generation-based models and retrieval-based models.
in table ix, the twitter conversation triple dataset is typically used for evaluating generation-based dialogue systems,
containing 3-turn twitter conversation instances. one commonly used evaluation metric is id7 [171], although it is commonly
acknowledged that most automatic id74 are not completely reliable for dialogue evaluation and additional human
evaluation is often necessary. ritter et al. [172] employed the phrase-based id151 (smt) framework
to    translate    the message to its appropriate response. sordoni et al. [173] reranked the 1000 best responses produced by
smt with a context-sensitive id56 encoder-decoder framework, observing substantial gains. li et al. [174] reported results on
replacing the traditional maximum log likelihood training objective with the maximum mutual information training objective, in
an effort to produce interesting and diverse responses, both of which are tested on a 4-layer lstm encoder-decoder framework.
the response retrieval task is de   ned as selecting the best response from a repository of candidate responses. such a model
can be evaluated by the recall1@k metric, where the ground-truth response is mixed with k    1 random responses. the ubuntu
dialogue dataset was constructed by scraping multi-turn ubuntu trouble-shooting dialogues from an online chatroom [97].
lowe et al. [97] used lstms to encode the message and response, and then inner product of the two sentence embeddings is
used to rank candidates.

zhou et al. [175] proposed to better exploit the multi-turn nature of human conversation by employing the lstm encoder
on top of sentence-level id98 embeddings, similar to [176]. dodge et al. [177] cast the problem in the framework of a memory
network, where the past conversation was treated as memory and the latest utterance was considered as a    question    to be
responded to. the authors showed that using simple neural bag-of-id27 for sentences can yield competitive results.

table ix: dialogue systems

paper

ritter et al. [172]
sordoni et al. [173]

li et al. [174]
li et al. [174]
lowe et al. [97]
dodge et al. [177]
zhou et al. [175]

model
smt

smt+neural reranking

lstm id195

lstm id195 with mmi objective

dual lstm encoders for semantic matching

memory networks

sentence-level id98-lstm encoder

twitter conversation
triple dataset (id7)

3.60
4.44
4.51
5.22

ubuntu dialogue

dataset (recall 1@10 %)

55.22
63.72
66.15

i. contextual embeddings

in this section, we explore some of the recent results based on contextual embeddings as explained in section ii-d. elmo
has contributed signi   cantly towards the recent advancement of nlp. in various nlp tasks, elmo outperformed state of the
art by signi   cant margin (table x). however, latest mode bert surpass elmo to establish itself as the state-of-the-art in
multiple tasks as summarized in table xi.

table x: comparison of elmo + baseline with the previous state of the art (sota) on various nlp tasks. the table has
been adapted from [41]. sota results have been taken from [41]; squad [166]: qa task; snli [178]: stanford natural
language id136 task; srl [153]: semantic role labelling; coref [179]: coreference resolution; ner [180]: named entity
recognition; sst-5 [4]: stanford sentiment treebank 5-class classi   cation;

25

task
squad
snli
srl
coref
ner
sst-5

previous sota
liu et al. [181]
qian et al. [182]
luheng et al. [183]
kenton et al. [184]
matthew et al. [185]
bryan et al. [186]

previous
sota results
84.4
88.6
81.7
67.2
91.93    0.19
53.7

baseline
81.1
88.0
81.4
67.2
90.15
51.4

elmo +
baseline
85.8
88.70    0.17
84.6
70.4
92.22    0.10
54.7 0.5

increase
(absolute/relative)
4.7 / 24.9%
0.7 / 5.8%
3.2 / 17.2%
3.2 / 9.8%
2.06 / 21%
3.3 / 6.8%

task
qnli
sst-2
sts-b
rte
squad
ner

bilstm+
elmo+attn
79.9
90.9
73.3
56.8
85.8
92.2

bert
91.1
94.9
86.5
70.1
91.1
92.8

table xi: qnli [187]: question natural language id136 task; sst-2 [4]: stanford sentiment treebank binary classi-
   cation; sts-b [188]: semantic textual similarity benchmark; rte [189]: recognizing id123; squad [166]:
qa task; ner [180]: id39.

ix. conclusion

deep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. with
distributed representation, various deep models have become the new state-of-the-art methods for nlp problems. supervised
learning is the most popular practice in recent deep learning research for nlp. in many real-world scenarios, however, we have
unlabeled data which require advanced unsupervised or semi-supervised approaches. in cases where there is lack of labeled data
for some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should
be employed. these learning schemes are still in their developing phase but we expect deep learning based nlp research to be
driven in the direction of making better use of unlabeled data. we expect such trend to continue with more and better model
designs. we expect to see more nlp applications that employ id23 methods, e.g., dialogue systems. we also
expect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated
with) other signals.

finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)
is enriched with an external memory (top-down knowledge inherited from a kb). coupling symbolic and sub-symbolic ai
will be key for stepping forward in the path from nlp to natural language understanding. relying on machine learning, in
fact, is good to make a    good guess    based on past experience, because sub-symbolic methods encode correlation and their
decision-making process is probabilistic. natural language understanding, however, requires much more than that. to use noam
chomsky   s words,    you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer
and doing statistical analysis of them: that   s not the way you understand things, you have to have theoretical insights   .

references

[1] e. cambria and b. white,    jumping nlp curves: a review of natural language processing research,    ieee computational

intelligence magazine, vol. 9, no. 2, pp. 48   57, 2014.

[2] t. mikolov, m. kara     at, l. burget, j. cernock`y, and s. khudanpur,    recurrent neural network based language model.   

in interspeech, vol. 2, 2010, p. 3.

[3] t. mikolov, i. sutskever, k. chen, g. s. corrado, and j. dean,    distributed representations of words and phrases and

their compositionality,    in advances in neural information processing systems, 2013, pp. 3111   3119.

[4] r. socher, a. perelygin, j. y. wu, j. chuang, c. d. manning, a. y. ng, c. potts et al.,    recursive deep models for
semantic compositionality over a sentiment treebank,    in proceedings of the conference on empirical methods in natural
language processing (emnlp), vol. 1631, 2013, p. 1642.

[5] r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuoglu, and p. kuksa,    natural language processing (almost)

from scratch,    journal of machine learning research, vol. 12, no. aug, pp. 2493   2537, 2011.

[6] y. goldberg,    a primer on neural network models for natural language processing,    journal of arti   cial intelligence

[7] y. bengio, r. ducharme, p. vincent, and c. jauvin,    a neural probabilistic language model,    journal of machine learning

research, vol. 57, pp. 345   420, 2016.

research, vol. 3, no. feb, pp. 1137   1155, 2003.

26

[8] t. mikolov, k. chen, g. corrado, and j. dean,    ef   cient estimation of word representations in vector space,    arxiv

[9] j. weston, s. bengio, and n. usunier,    wsabie: scaling up to large vocabulary image annotation,    in ijcai, vol. 11,

preprint arxiv:1301.3781, 2013.

2011, pp. 2764   2770.

[10] r. socher, c. c. lin, c. manning, and a. y. ng,    parsing natural scenes and natural language with recursive neural

networks,    in proceedings of the 28th international conference on machine learning (icml-11), 2011, pp. 129   136.

[11] p. d. turney and p. pantel,    from frequency to meaning: vector space models of semantics,    journal of arti   cial

[12] e. cambria, s. poria, a. gelbukh, and m. thelwall,    id31 is a big suitcase,    ieee intelligent systems,

intelligence research, vol. 37, pp. 141   188, 2010.

vol. 32, no. 6, pp. 74   80, 2017.

[13] x. glorot, a. bordes, and y. bengio,    id20 for large-scale sentiment classi   cation: a deep learning

approach,    in proceedings of the 28th international conference on machine learning (icml-11), 2011, pp. 513   520.

[14] k. m. hermann and p. blunsom,    the role of syntax in vector space models of id152,    in proceedings
of the 51st annual meeting of the association for computational linguistics (volume 1: long papers). association
for computational linguistics, 2013.

[15] j. l. elman,    distributed representations, simple recurrent networks, and grammatical structure,    machine learning,

vol. 7, no. 2-3, pp. 195   225, 1991.

[16] a. m. glenberg and d. a. robertson,    symbol grounding and meaning: a comparison of high-dimensional and embodied

theories of meaning,    journal of memory and language, vol. 43, no. 3, pp. 379   401, 2000.

[17] s. t. dumais,    latent semantic analysis,    annual review of information science and technology, vol. 38, no. 1, pp.

[18] d. m. blei, a. y. ng, and m. i. jordan,    id44,    journal of machine learning research, vol. 3, no.

188   230, 2004.

jan, pp. 993   1022, 2003.

[19] r. collobert and j. weston,    a uni   ed architecture for natural language processing: deep neural networks with multitask

learning,    in proceedings of the 25th international conference on machine learning. acm, 2008, pp. 160   167.

[20] a. gittens, d. achlioptas, and m. w. mahoney,    skip-gram-zipf+ uniform= vector additivity,    in proceedings of the
55th annual meeting of the association for computational linguistics (volume 1: long papers), vol. 1, 2017, pp. 69   76.
[21] j. pennington, r. socher, and c. d. manning,    glove: global vectors for word representation.    in emnlp, vol. 14,

2014, pp. 1532   1543.

[22] x. rong,    id97 parameter learning explained,    arxiv preprint arxiv:1411.2738, 2014.
[23] r. johnson and t. zhang,    semi-supervised convolutional neural networks for text categorization via region embedding,   

in advances in neural information processing systems, 2015, pp. 919   927.

[24] r. socher, j. pennington, e. h. huang, a. y. ng, and c. d. manning,    semi-supervised recursive autoencoders
for predicting sentiment distributions,    in proceedings of the conference on empirical methods in natural language
processing. association for computational linguistics, 2011, pp. 151   161.

[25] x. wang, y. liu, c. sun, b. wang, and x. wang,    predicting polarities of tweets by composing id27s with

[26] d. tang, f. wei, n. yang, m. zhou, t. liu, and b. qin,    learning sentiment-speci   c id27 for twitter

long short-term memory.    in acl (1), 2015, pp. 1343   1353.

sentiment classi   cation.    in acl (1), 2014, pp. 1555   1565.

[27] i. labutov and h. lipson,    re-embedding words.    in acl (2), 2013, pp. 489   493.
[28] s. upadhyay, k.-w. chang, m. taddy, a. kalai, and j. zou,    beyond bilingual: multi-sense id27s using

multilingual context,    arxiv preprint arxiv:1706.08160, 2017.

[29] y. kim, y. jernite, d. sontag, and a. m. rush,    character-aware neural language models.    in aaai, 2016, pp. 2741   2749.
[30] c. n. dos santos and m. gatti,    deep convolutional neural networks for id31 of short texts.    in coling,

[31] c. n. d. santos and v. guimaraes,    boosting id39 with neural character embeddings,    arxiv preprint

[32] c. d. santos and b. zadrozny,    learning character-level representations for part-of-speech tagging,    in proceedings of

the 31st international conference on machine learning (icml-14), 2014, pp. 1818   1826.

[33] y. ma, e. cambria, and s. gao,    label embedding for zero-shot    ne-grained named entity typing,    in coling, osaka,

[34] x. chen, l. xu, z. liu, m. sun, and h. luan,    joint learning of character and id27s,    in twenty-fourth

international joint conference on arti   cial intelligence, 2015.

[35] x. zheng, h. chen, and t. xu,    deep learning for chinese id40 and id52.    in emnlp, 2013, pp.

647   657.

level,    in flairs, 2017, pp. 347   352.

[36] h. peng, e. cambria, and x. zou,    radical-based hierarchical embeddings for chinese id31 at sentence

[37] p. bojanowski, e. grave, a. joulin, and t. mikolov,    enriching word vectors with subword information,    arxiv preprint

2014, pp. 69   78.

arxiv:1505.05008, 2015.

2016, pp. 171   180.

27

arxiv:1607.04606, 2016.

arxiv:1707.06556, 2017.

arxiv:1707.06961, 2017.

[38] a. herbelot and m. baroni,    high-risk learning: acquiring new word vectors from tiny data,    arxiv preprint

[39] y. pinter, r. guthrie, and j. eisenstein,    mimicking id27s using subword id56s,    arxiv preprint

[40] l. lucy and j. gauthier,    are distributional representations ready for the real world? evaluating word vectors for grounded

perceptual meaning,    arxiv preprint arxiv:1705.11168, 2017.

[41] m. e. peters, m. neumann, m. iyyer, m. gardner, c. clark, k. lee, and l. zettlemoyer,    deep contextualized word

representations,    arxiv preprint arxiv:1802.05365, 2018.

[42] a. mousa and b. schuller,    contextual bidirectional long short-term memory recurrent neural network language models:
a generative approach to id31,    in proceedings of the 15th conference of the european chapter of the
association for computational linguistics: volume 1, long papers, vol. 1, 2017, pp. 1023   1032.

[43] a. m. dai and q. v. le,    semi-supervised sequence learning,    in advances in neural information processing systems,

2015, pp. 3079   3087.

[44] a. radford, k. narasimhan, t. salimans, and i. sutskever,    improving language understanding by generative pre-
training,    url https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/language-unsupervised/language
understanding paper. pdf, 2018.

[45] j. devlin, m.-w. chang, k. lee, and k. toutanova,    bert: pre-training of deep bidirectional transformers for language

understanding,    arxiv preprint arxiv:1810.04805, 2018.

[46] a. krizhevsky, i. sutskever, and g. e. hinton,    id163 classi   cation with deep convolutional neural networks,    in

advances in neural information processing systems, 2012, pp. 1097   1105.

[47] a. sharif razavian, h. azizpour, j. sullivan, and s. carlsson,    id98 features off-the-shelf: an astounding baseline for
recognition,    in proceedings of the ieee conference on id161 and pattern recognition workshops, 2014,
pp. 806   813.

[48] y. jia, e. shelhamer, j. donahue, s. karayev, j. long, r. girshick, s. guadarrama, and t. darrell,    caffe: convolutional
architecture for fast feature embedding,    in proceedings of the 22nd acm international conference on multimedia. acm,
2014, pp. 675   678.

[49] n. kalchbrenner, e. grefenstette, and p. blunsom,    a convolutional neural network for modelling sentences,   
proceedings of the 52nd annual meeting of the association for computational linguistics, june 2014. [online].
available: http://goo.gl/esqcuc

[50] y. kim,    convolutional neural networks for sentence classi   cation,    arxiv preprint arxiv:1408.5882, 2014.
[51] y. zhang and b. wallace,    a sensitivity analysis of (and practitioners    guide to) convolutional neural networks for

sentence classi   cation,    arxiv preprint arxiv:1510.03820, 2015.

[52] s. poria, e. cambria, and a. gelbukh,    aspect extraction for opinion mining with a deep convolutional neural network,   

knowledge-based systems, vol. 108, pp. 42   49, 2016.

[53] a. kirillov, d. schlesinger, w. forkel, a. zelenin, s. zheng, p. torr, and c. rother,    ef   cient likelihood learning of a

generic id98-crf model for semantic segmentation,    arxiv preprint arxiv:1511.05067, 2015.

[54] a. waibel, t. hanazawa, g. hinton, k. shikano, and k. j. lang,    phoneme recognition using time-delay neural networks,   

ieee transactions on acoustics, speech, and signal processing, vol. 37, no. 3, pp. 328   339, 1989.

[55] a. mukherjee and b. liu,    aspect extraction through semi-supervised modeling,    in proceedings of the 50th annual
meeting of the association for computational linguistics: long papers-volume 1. association for computational
linguistics, 2012, pp. 339   348.

[56] s. ruder, p. ghaffari, and j. g. breslin,    insight-1 at semeval-2016 task 5: deep learning for multilingual aspect-based

id31,    arxiv preprint arxiv:1609.02748, 2016.

[57] p. wang, j. xu, b. xu, c.-l. liu, h. zhang, f. wang, and h. hao,    semantic id91 and convolutional neural

network for short text categorization.    in acl (2), 2015, pp. 352   357.

[58] s. poria, e. cambria, d. hazarika, and p. vij,    a deeper look into sarcastic tweets using deep convolutional neural

networks,    in coling, 2016, pp. 1601   1612.

[59] m. denil, a. demiraj, n. kalchbrenner, p. blunsom, and n. de freitas,    modelling, visualising and summarising
documents with a single convolutional neural network,    26th international conference on computational linguistics,
pp. 1601   1612, 2014.

[60] b. hu, z. lu, h. li, and q. chen,    convolutional neural network architectures for matching natural language sentences,   

in advances in neural information processing systems, 2014, pp. 2042   2050.

[61] y. shen, x. he, j. gao, l. deng, and g. mesnil,    a latent semantic model with convolutional-pooling structure for
information retrieval,    in proceedings of the 23rd acm international conference on conference on information and
knowledge management. acm, 2014, pp. 101   110.

[62] w.-t. yih, x. he, and c. meek,    id29 for single-relation id53.    in acl (2). citeseer, 2014,

pp. 643   648.

28

[63] l. dong, f. wei, m. zhou, and k. xu,    id53 over freebase with multi-column convolutional neural

[64] a. severyn and a. moschitti,    modeling relational information in question-answer pairs with convolutional neural

[65] y. chen, l. xu, k. liu, d. zeng, j. zhao et al.,    event extraction via dynamic multi-pooling convolutional neural

networks.    in acl (1), 2015, pp. 260   269.

networks,    arxiv preprint arxiv:1604.01178, 2016.

networks.    in acl (1), 2015, pp. 167   176.

[66] o. abdel-hamid, a.-r. mohamed, h. jiang, l. deng, g. penn, and d. yu,    convolutional neural networks for speech
recognition,    ieee/acm transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533   1545, 2014.
[67] d. palaz, r. collobert et al.,    analysis of id98-based id103 system using raw speech as input,    idiap, tech.

[68] z. tu, b. hu, z. lu, and h. li,    context-dependent translation selection using convolutional neural network,    arxiv

rep., 2015.

preprint arxiv:1503.02357, 2015.

[69] j. l. elman,    finding structure in time,    cognitive science, vol. 14, no. 2, pp. 179   211, 1990.
[70] t. mikolov, s. kombrink, l. burget, j.   cernock`y, and s. khudanpur,    extensions of recurrent neural network language
ieee, 2011,

model,    in acoustics, speech and signal processing (icassp), 2011 ieee international conference on.
pp. 5528   5531.

[71] i. sutskever, j. martens, and g. e. hinton,    generating text with recurrent neural networks,    in proceedings of the 28th

international conference on machine learning (icml-11), 2011, pp. 1017   1024.

[72] s. liu, n. yang, m. li, and m. zhou,    a recursive recurrent neural network for id151,   

proceedings of the 52nd annual meeting of the association for computational linguistics, pp. 1491   1500, 2014.

[73] m. auli, m. galley, c. quirk, and g. zweig,    joint language and translation modeling with recurrent neural networks.   

in emnlp, 2013, pp. 1044   1054.

[74] i. sutskever, o. vinyals, and q. v. le,    sequence to sequence learning with neural networks,    in advances in neural

information processing systems, 2014, pp. 3104   3112.

[75] t. robinson, m. hochberg, and s. renals,    the use of recurrent neural networks in continuous id103,    in

automatic speech and speaker recognition. springer, 1996, pp. 233   258.

[76] a. graves, a.-r. mohamed, and g. hinton,    id103 with deep recurrent neural networks,    in acoustics,

speech and signal processing (icassp), 2013 ieee international conference on.

ieee, 2013, pp. 6645   6649.

[77] a. graves and n. jaitly,    towards end-to-end id103 with recurrent neural networks,    in proceedings of the

31st international conference on machine learning (icml-14), 2014, pp. 1764   1772.

[78] h. sak, a. senior, and f. beaufays,    long short-term memory based recurrent neural network architectures for large

vocabulary id103,    arxiv preprint arxiv:1402.1128, 2014.

[79] a. karpathy and l. fei-fei,    deep visual-semantic alignments for generating image descriptions,    in proceedings of the

ieee conference on id161 and pattern recognition, 2015, pp. 3128   3137.

[80] d. tang, b. qin, and t. liu,    document modeling with gated recurrent neural network for sentiment classi   cation.    in

[81] j. chung, c. gulcehre, k. cho, and y. bengio,    empirical evaluation of gated recurrent neural networks on sequence

emnlp, 2015, pp. 1422   1432.

modeling,    arxiv preprint arxiv:1412.3555, 2014.

[82] k. cho, b. van merri  enboer, c. gulcehre, d. bahdanau, f. bougares, h. schwenk, and y. bengio,    learning phrase

representations using id56 encoder-decoder for id151,    arxiv preprint arxiv:1406.1078, 2014.

[83] g. chen, d. ye, e. cambria, j. chen, and z. xing,    ensemble application of convolutional and recurrent neural networks

for multi-label text categorization,    in ijid98, 2017, pp. 2377   2383.

[84] s. poria, e. cambria, d. hazarika, n. mazumder, a. zadeh, and l.-p. morency,    context-dependent id31

in user-generated videos,    in acl, 2017, pp. 873   883.

[85] a. zadeh, m. chen, s. poria, e. cambria, and l.-p. morency,    tensor fusion network for multimodal id31,   

[86] e. tong, a. zadeh, and l.-p. morency,    combating human traf   cking with deep multimodal models,    in association

in empirical methods in nlp, 2017.

for computational linguistics, 2017.

preprint arxiv:1612.08083, 2016.

preprint arxiv:1702.01923, 2017.

[87] i. chaturvedi, e. ragusa, p. gastaldo, r. zunino, and e. cambria,    id110 based extreme learning machine

for subjectivity detection,    journal of the franklin institute, 2017.

[88] y. n. dauphin, a. fan, m. auli, and d. grangier,    id38 with gated convolutional networks,    arxiv

[89] w. yin, k. kann, m. yu, and h. sch  utze,    comparative study of id98 and id56 for natural language processing,    arxiv

[90] y. lecun, y. bengio, and g. hinton,    deep learning,    nature, vol. 521, no. 7553, pp. 436   444, 2015.
[91] s. hochreiter and j. schmidhuber,    long short-term memory,    neural computation, vol. 9, no. 8, pp. 1735   1780, 1997.
[92] f. a. gers, j. schmidhuber, and f. cummins,    learning to forget: continual prediction with lstm,    9th international

conference on arti   cial neural networks, pp. 850   855, 1999.

29

[93] g. lample, m. ballesteros, s. subramanian, k. kawakami, and c. dyer,    neural architectures for named entity

recognition,    arxiv preprint arxiv:1603.01360, 2016.

[94] a. graves,    generating sequences with recurrent neural networks,    arxiv preprint arxiv:1308.0850, 2013.
[95] m. sundermeyer, h. ney, and r. schl  uter,    from feedforward to recurrent lstm neural networks for id38,   

ieee/acm transactions on audio, speech and language processing (taslp), vol. 23, no. 3, pp. 517   529, 2015.

[96] m. sundermeyer, t. alkhouli, j. wuebker, and h. ney,    translation modeling with bidirectional recurrent neural

networks.    in emnlp, 2014, pp. 14   25.

[97] r. lowe, n. pow, i. serban, and j. pineau,    the ubuntu dialogue corpus: a large dataset for research in unstructured

multi-turn dialogue systems,    arxiv preprint arxiv:1506.08909, 2015.

[98] o. vinyals, a. toshev, s. bengio, and d. erhan,    show and tell: a neural image caption generator,    in proceedings of

the ieee conference on id161 and pattern recognition, 2015, pp. 3156   3164.

[99] o. vinyals and q. le,    a neural conversational model,    arxiv preprint arxiv:1506.05869, 2015.
[100] j. li, m. galley, c. brockett, g. p. spithourakis, j. gao, and b. dolan,    a persona-based neural conversation model,   

arxiv preprint arxiv:1603.06155, 2016.

[101] m. malinowski, m. rohrbach, and m. fritz,    ask your neurons: a neural-based approach to answering questions about

images,    in proceedings of the ieee international conference on id161, 2015, pp. 1   9.

[102] a. kumar, o. irsoy, j. su, j. bradbury, r. english, b. pierce, p. ondruska, i. gulrajani, and r. socher,    ask me anything:

dynamic memory networks for natural language processing,    corr, abs/1506.07285, 2015.

[103] d. bahdanau, k. cho, and y. bengio,    id4 by jointly learning to align and translate,    arxiv

[104] a. m. rush, s. chopra, and j. weston,    a neural attention model for abstractive sentence summarization,    arxiv preprint

preprint arxiv:1409.0473, 2014.

arxiv:1509.00685, 2015.

[105] k. xu, j. ba, r. kiros, k. cho, a. courville, r. salakhudinov, r. zemel, and y. bengio,    show, attend and tell: neural
image id134 with visual attention,    in international conference on machine learning, 2015, pp. 2048   2057.
[106] o. vinyals,   . kaiser, t. koo, s. petrov, i. sutskever, and g. hinton,    grammar as a foreign language,    in advances in

neural information processing systems, 2015, pp. 2773   2781.

[107] o. vinyals, m. fortunato, and n. jaitly,    id193,    in advances in neural information processing systems,

[108] r. paulus, c. xiong, and r. socher,    a deep reinforced model for abstractive summarization,    arxiv preprint

[109] y. wang, m. huang, x. zhu, and l. zhao,    attention-based lstm for aspect-level sentiment classi   cation.    in emnlp,

[110] y. ma, h. peng, and e. cambria,    targeted aspect-based id31 via embedding commonsense knowledge

[111] d. tang, b. qin, and t. liu,    aspect level sentiment classi   cation with deep memory network,    arxiv preprint

[112] j. weston, s. chopra, and a. bordes,    memory networks,    arxiv preprint arxiv:1410.3916, 2014.
[113] a. vaswani, n. shazeer, n. parmar, and j. uszkoreit,    attention is all you need,    arxiv preprint arxiv:1706.03762,

[114] k. he, x. zhang, s. ren, and j. sun,    deep residual learning for image recognition,    in proceedings of the ieee

conference on id161 and pattern recognition, 2016, pp. 770   778.

[115] j. l. ba, j. r. kiros, and g. e. hinton,    layer id172,    arxiv preprint arxiv:1607.06450, 2016.
[116] r. socher, b. huval, c. d. manning, and a. y. ng,    semantic compositionality through recursive matrix-vector spaces,   
in proceedings of the 2012 joint conference on empirical methods in natural language processing and computational
natural language learning. association for computational linguistics, 2012, pp. 1201   1211.

[117] b. taskar, c. guestrin, and d. koller,    max-margin markov networks,    in advances in neural information processing

[118] s. r. bowman, c. potts, and c. d. manning,    id56s can learn logical semantics,    arxiv preprint

systems, 2004, pp. 25   32.

arxiv:1406.1827, 2014.

[119] k. s. tai, r. socher, and c. d. manning,    improved semantic representations from tree-structured long short-term

memory networks,    arxiv preprint arxiv:1503.00075, 2015.

[120] s. bengio, o. vinyals, n. jaitly, and n. shazeer,    scheduled sampling for sequence prediction with recurrent neural

networks,    in advances in neural information processing systems, 2015, pp. 1171   1179.

[121] m. ranzato, s. chopra, m. auli, and w. zaremba,    sequence level training with recurrent neural networks,    arxiv

preprint arxiv:1511.06732, 2015.

arxiv preprint arxiv:1606.01541, 2016.

[122] j. li, w. monroe, a. ritter, m. galley, j. gao, and d. jurafsky,    deep id23 for dialogue generation,   

[123] r. j. williams,    simple statistical gradient-following algorithms for connectionist id23,    machine

2015, pp. 2692   2700.

arxiv:1705.04304, 2017.

2016, pp. 606   615.

into an attentive lstm,    in aaai, 2018.

arxiv:1605.08900, 2016.

2017.

30

learning, vol. 8, no. 3-4, pp. 229   256, 1992.

[124] s. young, m. ga  si  c, s. keizer, f. mairesse, j. schatzmann, b. thomson, and k. yu,    the hidden information state
model: a practical framework for pomdp-based spoken dialogue management,    computer speech & language, vol. 24,
no. 2, pp. 150   174, 2010.

[125] s. young, m. ga  si  c, b. thomson, and j. d. williams,    pomdp-based statistical spoken id71: a review,   

proceedings of the ieee, vol. 101, no. 5, pp. 1160   1179, 2013.

[126] p.-h. su, v. david, d. kim, t.-h. wen, and s. young,    learning from real users: rating dialogue success with neural
networks for id23 in spoken dialogue systems,    in in proceedings of interspeech. citeseer, 2015, pp.
2007   2011.

[127] p.-h. su, m. gasic, n. mrksic, l. rojas-barahona, s. ultes, d. vandyke, t.-h. wen, and s. young,    on-line active

reward learning for policy optimisation in spoken dialogue systems,    arxiv preprint arxiv:1605.07669, 2016.

[128] i. goodfellow, j. pouget-abadie, m. mirza, b. xu, d. warde-farley, s. ozair, a. courville, and y. bengio,    generative

adversarial nets,    in advances in neural information processing systems, 2014, pp. 2672   2680.

[129] j. li, w. monroe, t. shi, a. ritter, and d. jurafsky,    adversarial learning for neural dialogue generation,    arxiv preprint

[130] r. kiros, y. zhu, r. r. salakhutdinov, r. zemel, r. urtasun, a. torralba, and s. fidler,    skip-thought vectors,    in

advances in neural information processing systems, 2015, pp. 3294   3302.

[131] d. e. rumelhart, g. e. hinton, and r. j. williams,    learning internal representations by error propagation,    dtic

arxiv:1701.06547, 2017.

document, tech. rep., 1985.

[132] d. p. kingma and m. welling,    auto-encoding id58,    arxiv preprint arxiv:1312.6114, 2013.
[133] s. r. bowman, l. vilnis, o. vinyals, a. m. dai, r. jozefowicz, and s. bengio,    generating sentences from a continuous

space,    arxiv preprint arxiv:1511.06349, 2015.

[134] y. zhang, z. gan, and l. carin,    generating text via adversarial training,    in nips workshop on adversarial training,

2016.

arxiv:1703.00955, 2017.

[135] z. hu, z. yang, x. liang, r. salakhutdinov, and e. p. xing,    controllable text generation,    arxiv preprint

[136] l. yu, w. zhang, j. wang, and y. yu,    seqgan: sequence generative adversarial nets with policy gradient,    in thirty-first

aaai conference on arti   cial intelligence, 2017.

[137] s. rajeswar, s. subramanian, f. dutil, c. pal, and a. courville,    adversarial generation of natural language,    arxiv

preprint arxiv:1705.10929, 2017.

systems, 2015, pp. 2440   2448.

[138] s. sukhbaatar, j. weston, r. fergus et al.,    end-to-end memory networks,    in advances in neural information processing

[139] c. xiong, s. merity, and r. socher,    dynamic memory networks for visual and textual id53,    arxiv, vol.

[140] j. gim  enez and l. marquez,    fast and accurate part-of-speech tagging: the id166 approach revisited,    recent advances

in natural language processing iii, pp. 153   162, 2004.

[141] z. huang, w. xu, and k. yu,    bidirectional lstm-crf models for sequence tagging,    arxiv preprint arxiv:1508.01991,

[142] d. andor, c. alberti, d. weiss, a. severyn, a. presta, k. ganchev, s. petrov, and m. collins,    globally normalized

transition-based neural networks,    arxiv preprint arxiv:1603.06042, 2016.

[143] d. chen and c. d. manning,    a fast and accurate dependency parser using neural networks.    in emnlp, 2014, pp.

1603, 2016.

2015.

740   750.

[144] c. dyer, m. ballesteros, w. ling, a. matthews, and n. a. smith,    transition-based id33 with stack long

short-term memory,    arxiv preprint arxiv:1505.08075, 2015.

[145] h. zhou, y. zhang, c. cheng, s. huang, x. dai, and j. chen,    a neural probabilistic structured-prediction method for

transition-based natural language processing,    journal of arti   cial intelligence research, vol. 58, pp. 703   729, 2017.

[146] m. zhu, y. zhang, w. chen, m. zhang, and j. zhu,    fast and accurate shift-reduce constituent parsing.    in acl (1),

[147] d. weiss, c. alberti, m. collins, and s. petrov,    structured training for neural network transition-based parsing,    arxiv

2013, pp. 434   443.

preprint arxiv:1506.06158, 2015.

[148] s. petrov, l. barrett, r. thibaux, and d. klein,    learning accurate, compact, and interpretable tree annotation,    in
proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the
association for computational linguistics. association for computational linguistics, 2006, pp. 433   440.

[149] a. passos, v. kumar, and a. mccallum,    lexicon infused phrase embeddings for named entity resolution,    arxiv preprint

arxiv:1404.5367, 2014.

2015.

[150] j. p. chiu and e. nichols,    id39 with bidirectional lstm-id98s,    arxiv preprint arxiv:1511.08308,

[151] g. luo, x. huang, c.-y. lin, and z. nie,    joint id39 and disambiguation,    in proc. emnlp, 2015,

31

pp. 879   880.

pp. 1127   1137.

[152] e. strubell, p. verga, d. belanger, and a. mccallum,    fast and accurate sequence labeling with iterated dilated

convolutions,    arxiv preprint arxiv:1702.02098, 2017.

[153] j. zhou and w. xu,    end-to-end learning of id14 using recurrent neural networks.    in acl (1), 2015,

[154] o. t  ackstr  om, k. ganchev, and d. das,    ef   cient id136 and structured learning for id14,   

transactions of the association for computational linguistics, vol. 3, pp. 29   41, 2015.

[155] l. he, k. lee, m. lewis, and l. zettlemoyer,    deep id14: what works and what   s next,    in proceedings

of the annual meeting of the association for computational linguistics, 2017.

[156] r. k. srivastava, k. greff, and j. schmidhuber,    training very deep networks,    in advances in neural information

processing systems, 2015, pp. 2377   2385.

[157] b. pang and l. lee,    seeing stars: exploiting class relationships for sentiment categorization with respect to rating
scales,    in proceedings of the 43rd annual meeting on association for computational linguistics. association for
computational linguistics, 2005, pp. 115   124.

[158] a. zadeh, r. zellers, e. pincus, and l.-p. morency,    multimodal sentiment intensity analysis in videos: facial gestures

and verbal messages,    ieee intelligent systems, vol. 31, no. 6, pp. 82   88, 2016.

[159] l.-c. yu, j. wang, k. r. lai, and x. zhang,    re   ning id27s for id31,    in proceedings of the

2017 conference on empirical methods in natural language processing, 2017, pp. 545   550.

[160] q. le and t. mikolov,    distributed representations of sentences and documents,    in proceedings of the 31st international

conference on machine learning (icml-14), 2014, pp. 1188   1196.

[161] p. koehn, f. j. och, and d. marcu,    statistical phrase-based translation,    in proceedings of the 2003 conference of the
north american chapter of the association for computational linguistics on human language technology-volume 1.
association for computational linguistics, 2003, pp. 48   54.

[162] y. wu, m. schuster, z. chen, q. v. le, m. norouzi, w. macherey, m. krikun, y. cao, q. gao, k. macherey et al.,
   google   s id4 system: bridging the gap between human and machine translation,    arxiv preprint
arxiv:1609.08144, 2016.

[163] j. gehring, m. auli, d. grangier, d. yarats, and y. n. dauphin,    convolutional sequence to sequence learning,    arxiv

[164] a. fader, l. s. zettlemoyer, and o. etzioni,    paraphrase-driven learning for open id53.    in acl (1), 2013,

preprint arxiv:1705.03122, 2017.

pp. 1608   1618.

[165] j. weston, a. bordes, s. chopra, a. m. rush, b. van merri  enboer, a. joulin, and t. mikolov,    towards ai-complete

id53: a set of prerequisite toy tasks,    arxiv preprint arxiv:1502.05698, 2015.

[166] p. rajpurkar, j. zhang, k. lopyrev, and p. liang,    squad: 100,000+ questions for machine comprehension of text,   

arxiv preprint arxiv:1606.05250, 2016.

[167] a. bordes, j. weston, and n. usunier,    open id53 with weakly supervised embedding models,    in joint

european conference on machine learning and knowledge discovery in databases. springer, 2014, pp. 165   180.

[168] d. chen, a. fisch, j. weston, and a. bordes,    reading wikipedia to answer open-domain questions,    arxiv preprint

arxiv:1704.00051, 2017.

[169] y. shen, p.-s. huang, j. gao, and w. chen,    reasonet: learning to stop reading in machine comprehension,    in
proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining. acm,
2017, pp. 1047   1055.

[170] a. w. yu, d. dohan, m.-t. luong, r. zhao, k. chen, m. norouzi, and q. v. le,    qanet: combining local convolution

with global self-attention for reading comprehension,    arxiv preprint arxiv:1804.09541, 2018.

[171] k. papineni, s. roukos, t. ward, and w.-j. zhu,    id7: a method for automatic evaluation of machine translation,    in
proceedings of the 40th annual meeting on association for computational linguistics. association for computational
linguistics, 2002, pp. 311   318.

[172] a. ritter, c. cherry, and w. b. dolan,    data-driven response generation in social media,    in proceedings of the conference
on empirical methods in natural language processing. association for computational linguistics, 2011, pp. 583   593.
[173] a. sordoni, m. galley, m. auli, c. brockett, y. ji, m. mitchell, j.-y. nie, j. gao, and b. dolan,    a neural network

approach to context-sensitive generation of conversational responses,    arxiv preprint arxiv:1506.06714, 2015.

[174] j. li, m. galley, c. brockett, j. gao, and b. dolan,    a diversity-promoting objective function for neural conversation

[175] x. zhou, d. dong, h. wu, s. zhao, d. yu, h. tian, x. liu, and r. yan,    multi-view response selection for human-

models,    arxiv preprint arxiv:1510.03055, 2015.

computer conversation.    in emnlp, 2016, pp. 372   381.

[176] i. v. serban, a. sordoni, y. bengio, a. c. courville, and j. pineau,    building end-to-end dialogue systems using

generative hierarchical neural network models.    in aaai, 2016, pp. 3776   3784.

[177] j. dodge, a. gane, x. zhang, a. bordes, s. chopra, a. miller, a. szlam, and j. weston,    evaluating prerequisite

qualities for learning end-to-end id71,    arxiv preprint arxiv:1511.06931, 2015.

32

[178] s. r. bowman, g. angeli, c. potts, and c. d. manning,    a large annotated corpus for learning natural language

id136,    arxiv preprint arxiv:1508.05326, 2015.

[179] s. pradhan, a. moschitti, n. xue, o. uryupina, and y. zhang,    conll-2012 shared task: modeling multilingual
unrestricted coreference in ontonotes,    in joint conference on emnlp and conll-shared task. association for
computational linguistics, 2012, pp. 1   40.

[180] e. f. tjong kim sang and f. de meulder,    introduction to the conll-2003 shared task: language-independent named
entity recognition,    in proceedings of the seventh conference on natural language learning at hlt-naacl 2003-volume
4. association for computational linguistics, 2003, pp. 142   147.

[181] x. liu, y. shen, k. duh, and g. jian-feng,    stochastic answer networks for id17,    arxiv

preprint arxiv:1712.03556, 2017.

[182] c. qian, z. xiao-dan, l. zhen-hua, w. si, j. hui, and i. diana,    enhanced lstm for natural language id136,    in

[183] h. luheng, l. kenton, l. mike, and s. z. luke,    deep id14: what works and whats next,    in acl,

[184] l. kenton, h. luheng, l. mike, and s. z. luke,    end-to-end neural coreference resolution,    in emnlp, 2017.
[185] e. p. matthew, a. waleed, b. chandra, and p. russell,    semi-supervised sequence tagging with bidirectional language

[186] m. bryan, b. james, x. caiming, and s. richard,    learned in translation: contextualized word vectors,    in nips 2017,

acl, 2017.

2017.

models,    in acl, 2017.

2017.

[187] a. wang, a. singh, j. michael, f. hill, o. levy, and s. r. bowman,    glue: a multi-task benchmark and analysis

platform for natural language understanding,    arxiv preprint arxiv:1804.07461, 2018.

[188] d. cer, m. diab, e. agirre, i. lopez-gazpio, and l. specia,    semeval-2017 task 1: semantic textual similarity-

multilingual and cross-lingual focused evaluation,    arxiv preprint arxiv:1708.00055, 2017.

[189] l. bentivogli, p. clark, i. dagan, and d. giampiccolo,    the    fth pascal recognizing id123 challenge.    in

[190] t. baltru  saitis, c. ahuja, and l.-p. morency,    multimodal machine learning: a survey and taxonomy,    arxiv preprint

tac, 2009.

arxiv:1705.09406, 2017.

