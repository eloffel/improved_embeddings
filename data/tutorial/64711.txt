7
1
0
2

 

p
e
s
2
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
0
8
7
0

.

9
0
7
1
:
v
i
x
r
a

id151

draft of chapter 13: id4

philipp koehn

center for speech and language processing

department of computer science

johns hopkins university

1st public draft
august 7, 2015

2nd public draft

september 25, 2017

2

contents

13 id4
.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.
.
.

.
13.3 computation graphs .

13.1 a short history .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.2 introduction to neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.2.1 linear models .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.2.2 multiple layers .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.2.3 non-linearity .
.
13.2.4 id136 .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
13.2.5 back-propagation training . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.2.6 re   nements .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
13.3.1 neural networks as computation graphs
. . . . . . . . . . . . . . . . . .
13.3.2 gradient computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.3.3 deep learning frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
13.4.1 feed-forward neural language models
13.4.2 id27 .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.4.3 ef   cient id136 and training . . . . . . . . . . . . . . . . . . . . . . . . .
13.4.4 recurrent neural language models . . . . . . . . . . . . . . . . . . . . . .
13.4.5 long short-term memory models . . . . . . . . . . . . . . . . . . . . . . .
13.4.6 id149 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.4.7 deep models
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.5.1 encoder-decoder approach . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
13.5.2 adding an alignment model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.5.3 training .
.
13.5.4 id125 .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
13.5 neural translation models .

13.4 neural language models

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

5
5
6
7
8
9
10
11
18
23
24
25
29
31
32
35
37
39
41
43
44
47
47
48
52
54

3

4

contents

13.6 re   nements .

13.7 alternate architectures .

.

.

.

.

.

.

.

.

.

.

.

.

58
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
13.6.1 ensemble decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
13.6.2 large vocabularies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
13.6.3 using monolingual data . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
13.6.4 deep models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
13.6.5 guided alignment training . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
13.6.6 modeling coverage
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
13.6.7 adaptation .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
13.6.8 adding linguistic annotation . . . . . . . . . . . . . . . . . . . . . . . . .
80
13.6.9 multiple language pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
13.7.1 convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . .
85
13.7.2 convolutional neural networks with attention . . . . . . . . . . . . . . .
87
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.7.3 self-attention .
90
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
13.8.1 domain mismatch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
13.8.2 amount of training data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.8.3 noisy data .
.
96
13.8.4 word alignment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.8.5 id125 .
98
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
13.8.6 further readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.

.
.
.

.
.

.

.

.

13.9 additional topics .
.
bibliography .
.
author index .
.
.
.
index .

.
.
.

.
.
.

.

.

.

.

13.8 current challenges .

chapter 13

id4

a major recent development in id151 is the adoption of neural net-
works. neural network models promise better sharing of statistical evidence between similar
words and inclusion of rich context. this chapter introduces several neural network modeling
techniques and explains how they are applied to problems in machine translation

13.1 a short history

already during the last wave of neural network research in the 1980s and 1990s, machine trans-
lation was in the sight of researchers exploring these methods (waibel et al., 1991). in fact, the
models proposed by forcada and   eco (1997) and casta  o et al. (1997) are striking similar to
the current dominant id4 approaches. however, none of these models
were trained on data sizes large enough to produce reasonable results for anything but toy ex-
amples. the computational complexity involved by far exceeded the computational resources
of that era, and hence the idea was abandoned for almost two decades.

during this hibernation period, data-driven approaches such as phrase-based statistical ma-
chine translation rose from obscurity to dominance and made machine translation a useful tool
for many applications, from information gisting to increasing the productivity of professional
translators.

the modern resurrection of neural methods in machine translation started with the integra-
tion of neural language models into traditional id151 systems. the pio-
neering work by schwenk (2007) showed large improvements in public evaluation campaigns.
however, these ideas were only slowly adopted, mainly due to computational concerns. the
use of gpus for training also posed a challenge for many research groups that simply lacked
such hardware or the experience to exploit it.

moving beyond the use in language models, neural network methods crept into other com-
ponents of traditional id151, such as providing additional scores or ex-
tending translation tables (schwenk, 2012; lu et al., 2014), reordering (kanouchi et al., 2016; li

5

6

chapter 13. id4

et al., 2014) and pre-ordering models (de gispert et al., 2015), and so on. for instance, the joint
translation and language model by devlin et al. (2014) was in   uential since it showed large
quality improvements on top of a very competitive id151 system.

more ambitious efforts aimed at pure id4, abandoning existing sta-
tistical approaches completely. early steps were the use of convolutional models (kalchbrenner
and blunsom, 2013) and sequence-to-sequence models (sutskever et al., 2014; cho et al., 2014).
these were able to produce reasonable translations for short sentences, but fell apart with in-
creasing sentence length. the addition of the attention mechanism    nally yielded competitive
results (bahdanau et al., 2015; jean et al., 2015b). with a few more re   nements, such as byte
pair encoding and back-translation of target-side monolingual data, id4
became the new state of the art.

within a year or two, the entire research    eld of machine translation went neural. to give
some indication of the speed of change: at the shared task for machine translation organized
by the conference on machine translation (wmt), only one pure id4
system was submitted in 2015. it was competitive, but outperformed by traditional statistical
systems. a year later, in 2016, a id4 system won in almost all language
pairs. in 2017, almost all submissions were id4 systems.

at the time of writing, id4 research is progressing at rapid pace.
there are many directions that are and will be explored in the coming years, ranging from
core machine learning improvements such as deeper models to more linguistically informed
models. more insight into the strength and weaknesses of id4 is being
gathered and will inform future work.

there is an extensive proliferation of toolkits available for research, development, and de-
ployment of id4 systems. at the time of writing, the number of toolkits
is multiplying, rather than consolidating. so, it is quite hard and premature to make speci   c
recommendations. nevertheless, some of the promising toolkits are:

    nematus (based on theano): https://github.com/edinburghnlp/nematus
    marian (a c++ re-implementation of nematus): https://marian-id4.github.io/
    openid4 (based on torch/pytorch): http://openid4.net/
    xid4 (based on dynet): https://github.com/neulab/xid4
    sockeye (based on mxnet): https://github.com/awslabs/sockeye
    t2t (based on tensor   ow): https://github.com/tensorflow/tensor2tensor

13.2 introduction to neural networks

a neural network is a machine learning technique that takes a number of inputs and predicts
outputs. in many ways, they are not very different from other machine learning methods but
have distinct strengths.

(cid:88)

13.2. introduction to neural networks

7

figure 13.1: graphical illustration of a linear model as a network: feature values are input nodes, arrows
are weights, and the score is an output node.

13.2.1 linear models

linear models are a core element of id151. a potential translation x of
a sentence is represented by a set of features hi(x). each feature is weighted by a parameter   i
to obtain an overall score. ignoring the exponential function that we used previously to turn
the linear model into a log-linear model, the following formula sums up the model.

score(  , x) =

  j hj(x)

(13.1)

j

graphically, a linear model can be illustrated by a network, where feature values are input

nodes, arrows are weights, and the score is an output node (see figure 13.1).

most prominently, we use linear models to combine different components of a machine
translation system, such as the language model, the phrase translation model, the reordering
model, and properties such as the length of the sentence, or the accumulated jump distance
between phrase translations. training methods assign a weight value   i to each such feature
hi(x), related to their importance in contributing to scoring better translations higher. in statis-
tical machine translation, this is called tuning.

however, linear models do not allow us to de   ne more complex relationships between the
features. let us say that we    nd that for short sentences the language model is less important
than the translation model, or that average phrase translation probabilities higher than 0.1 are
similarly reasonable but any value below that is really terrible. the    rst hypothetical example
implies dependence between features and the second example implies non-linear relationship
between the feature value and its impact on the    nal score. linear models cannot handle these
cases.

a commonly cited counter-example to the use of linear models is xor, i.e., the boolean
operator     with the truth table 0    0 = 0, 1    0 = 1, 0    1 = 1, and 1    1 = 0. for a linear model
with two features (representing the inputs), it is not possible to come up with weights that give
the correct output in all cases. linear models assume that all instances, represented as points
in the feature space, are linearly separable. this is not the case with xor, and may not be the
case for type of features we use in machine translation.

8

chapter 13. id4

figure 13.2: a neural network with a hidden layer.

13.2.2 multiple layers

neural networks modify linear models in two important ways. the    rst is the use of multiple
layers. instead of computing the output value directly from the input values, a hidden layer
is introduced. it is called hidden, because we can observe inputs and outputs in training in-
stances, but not the mechanism that connects them     this use of the concept hidden is similar
to its meaning in id48.

see figure 13.2 for on illustration. the network is processed in two steps. first, a linear
combination of weighted input node is computed to produce each hidden node value. then a
linear combination of weighted hidden nodes is computed to produce each output node value.

at this point, let us introduce mathematical notations from the neural network literature.

a neural network with a hidden layer consists of

    a vector of input nodes with values (cid:126)x = (x1, x2, x3, ...xn)t
    a vector of hidden nodes with values (cid:126)h = (h1, h2, h3, ...hm)t
    a vector of output nodes with values (cid:126)y = (y1, y2, y3, ...yl)t
    a matrix of weights connecting input nodes with hidden nodes w = {wij}
    a matrix of weights connecting hidden nodes with output nodes u = {uij}

the computations in a neural network with a hidden layer, as sketched out so far, are

(cid:88)
(cid:88)

i

hj =

yk =

xiwji

hjukj

(13.2)

(13.3)

j

note that we snuck in the possibility of multiple output nodes yk, although our    gures so

far only showed one.

13.2. introduction to neural networks

9

hyperbolic tangent

logistic function

recti   ed linear unit

tanh(x) = sinh(x)

cosh(x) = ex   e   x
ex+e   x

sigmoid(x) = 1

1+e   x

relu(x) = max(0,x)

output ranges
from    1 to +1

6

-

output ranges
from 0 to +1

6

output ranges
from 0 to    

  

 

6

 

 

-

-

figure 13.3: typical id180 in neural networks.

13.2.3 non-linearity

if we carefully think about the addition of a hidden layer, we realize that we have not gained
anything so far to model input/output relationships. we can easily do away with the hidden
layer by multiplying out the weights

j

(cid:88)
(cid:88)
(cid:88)

j

i

hjukj

(cid:88)
      (cid:88)

xi

i

j

yk =

=

=

xiwjiukj

      

(13.4)

ukjwji

after computing the linear combination of weighted feature values sj = (cid:80)

hence, a salient element of neural networks is the use of a non-linear activation function.
i xiwji, we obtain

the value of a node only after applying such a function hj = f (sj).

popular choices are the hyperbolic tangent tanh(x) and the logistic function sigmoid(x).
see figure 13.3 for more details on these functions. a good way to think about these activation
functions is that they segment the range of values for the linear combination sj into

    a segment where the node is turned off (values close to 0 for tanh, or    1 for sigmoid)
    a transition segment where the node is partly turned on
    a segment where the node is turned on (values close to 1)

a different popular choice is the activation function for the recti   ed linear unit (relu). it
does not allow for negative values and    oors them at 0, but does not alter the value of positive
values. it is simpler and faster to compute than tanh(x) or sigmoid(x).

you could view each hidden node as a feature detector. for a certain con   gurations of input
node values, it is turned on, for others it is turned off. advocates of neural networks claim that

10

chapter 13. id4

figure 13.4: a simple neural network with bias nodes in input and hidden layers.

the use of hidden nodes obviates (or at least drastically reduces) the need for feature engineer-
ing: instead of manually detecting useful patterns in input values, training of the hidden nodes
discovers them automatically.

we do not have to stop at a single hidden layer. the currently fashionable name deep
learning for neural networks stems from the fact that often better performance can be achieved
by deeply stacking together layers and layers of hidden nodes.

13.2.4 id136

let us walk through neural network i
nference (i.e., how output values are computed from input
  
values) with a concrete example. consider the neural network in figure 13.4. this network has
one additional innovation that we have not presented so far: bias units. these are nodes that
always have the value 1. such bias units give the network something to work with in the case
that all input values are 0. otherwise, the weighted sum sj would be 0 no matter the weights.
let us use this neural network to process some input, say the value 1 for the    rst input
node x0 and 0 for the second input node x1. the value of the bias input node (labelled x2) is
   xed to 1. to compute the value of the    rst hidden node h0, we have to carry out the following
calculation.

(cid:32)(cid:88)

(cid:33)

h0 = sigmoid

xiw1i

i

= sigmoid (1    3 + 0    4 + 1       2)
= sigmoid (1)
= 0.73

(13.5)

the calculations for the other nodes are summarized in table 13.1. the output value in
node y0 for the input (0,1) is 0.743. if we expect binary output, we would understand this result
as the value 1, since it is over the threshold of 0.5 in the range of possible output values [0;1].

here, the output for all possible binary inputs:

115-5-2-4-23243x0x1x2h0h1h2y013.2. introduction to neural networks

11

layer node
hidden
hidden
output

h0
h1
y0

summation
1    3 + 0    4 + 1       2 = 1
1    2 + 0    3 + 1       4 =    2
0.731    5 + 0.119       5 + 1       2 = 1.060

activation

0.731
0.119
0.743

table 13.1: calculations for input (1,0) to the network in figure 13.4.

input x0

0
0
1
1

input x1 hidden h0 hidden h1 output y0
0.183     0
0.743     1
0.743     1
0.334     0

0.119
0.881
0.731
0.993

0.018
0.269
0.119
0.731

0
1
0
1

our neural network computes xor. how does it do that? if we look at the hidden nodes h0
and h1, we notice that h0 acts like the boolean or: its value is high if at least of the two input
values is 1 (h0 = 0.881, 0.731, 0.993, for the three con   gurations), it otherwise has a low value
(0.119). the other hidden node h1 acts like the boolean and     it only has a high value (0.731)
if both inputs are 1. xor is effectively implemented as the subtraction of the and from the or
hidden node.

note that the non-linearity is key here. since the value for the or node h0 is not that much
higher for the input of (1,1) opposed to a single 1 in the input (0.993 vs. 0.881 and 0.731), the
distinct high value for the and node h1 in this case (0.731) manages to push the    nal output
y0 below the threshold. this would not be possible if the values of the inputs would be simply
summed up as in linear models.

as mentioned before, recently the use of the name deep learning for neural networks has
become fashionable. it emphasizes that often higher performance can be achieved by using
networks with multiple hidden layers. our xor example hints at where this power comes
from. with a single input-output layer network it is possible to mimic basic boolean operations
such as and and or since they can be modeled with linear classi   ers. xor can be expressed as
x and y     x or y, and our neural network example implements the boolean operations and
and or in the    rst layer, and the subtraction in the second layer. for functions that require more
intricate computations, more operations may be chained together, and hence a neural network
architecture with more hidden layers may be needed. it may be possible (with suf   cient train-
ing data) to build neural networks for any computer program, if the number of hidden layers
matches the depth of the computation. there is a line of research under the banner neural
turing machines that explores what kind of architectures are needed to implement basic algo-
rithms (gemici et al., 2017). for instance, a neural network with two hidden layers is suf   cient
to implement an algorithm that sorts n-bit numbers.

13.2.5 back-propagation training

training neural networks requires the optimization of weight values so that the network pre-
dicts the correct output for a set of training examples. we repeatedly feed the input from the

12

chapter 13. id4

figure 13.5: id119 training: we compute the gradient with regard to every dimension. in
this case the gradient with respect to weight w2 smaller than the gradient with respect to the weight w1,
so we move more to the left than down (note: arrows point in negative gradient direction, pointing to
the minimum).

training examples into the network, compare the computed output of the network with the
correct output from the training example, and update the weights. typically, several passes
over the training data are carried out. each pass over the data is called an epoch.

the most common training method for neural networks is called back-propagation, since
it    rst updates the weights to the output layer, and propagates back error information to earlier
layers. whenever a training example is processed, then for each node in the network, an error
term is computed which is the basis for updating the values for incoming weights.

the formulas used to compute updated values for weights follows principles of gradient
descent training. the error for a speci   c node is understood as a function of the incoming
weights. to reduce the error given this function, we compute the gradient of the error function
with respect to each of the weights, and move against the gradient to reduce the error.

why is moving alongside the gradient a good idea? consider that we optimize multiple
dimensions at the same time. if you are looking for the lowest point in an area (maybe you are
looking for water in a desert), and the ground falls off steep to the west of you, and also slightly
south of you, then you would go in a direction that is mainly west     and only slightly south.
in other words, you go alongside the gradient. see figure 13.5 for an illustration.

in the following two sections, we will derive the formulae for updating weights for our
example network. if you are less interested in the why and more in the how, you can skip these
sections and continue reading when we summarize the update formulae on page 16.

weights to the output nodes

let us    rst review and extend our notation. at an output node yi, we    rst compute a linear
combination of weight and hidden node values.

si =

wi   jhj

(13.6)

(cid:88)

j

gradient for w1gradient for w2optimumcurrent pointcombined gradient13.2. introduction to neural networks

13

this sum si is passed through an activation function such as sigmoid to compute the output

value y.

yi = sigmoid(si)

(13.7)

we compare the computed output values yi against the target output values ti from the
training example. there are various ways to compute an error value e from these values. let
us use the l2 norm.

(13.8)

(cid:88)

i

e =

(ti     yi)2

1
2

as we stated above, our goal is to compute the gradient of the error e with respect to the
weights wk to    nd out in which direction (and how strongly) we should move the weight value.
we do this for each weight wk separately. we    rst break up the computation of the gradient
into three steps, essentially unfolding the equations 13.6 to 13.8.

de

dwi   j

=

de
dyi

dyi
dsi

dsi

dwi   j

(13.9)

let us now work through each of these three steps.
    since we de   ned the error e in terms of the output values yi, we can compute the    rst

component as follows.

de
dyi

d
dyi

1
2

=

(13.10)
    the derivative of the output value yi with respect to si (the linear combination of weight
and hidden node values) depends on the activation function. in the case of sigmoid, we
have:

(ti     yi)2 =    (ti     yi)

dyi
dsi

=

d sigmoid(si)

dsi

= sigmoid(si)(1     sigmoid(si)) = yi(1     yi)

(13.11)

to keep our treatment below as general as possible and not commit to the sigmoid as an
activation function, we will use the shorthand y(cid:48)
below. note that for any given
training example and any given differentiable activation function, this value can always
be computed.

i for dyi
dsi

    finally, we compute the derivative of si with respect to the weight wi   j, which turns out

to be quite simply the value to the hidden node hj.

ds

dwi   j

=

d

dwi   j

(cid:88)

j

wi   jhj = hj

(13.12)

where are we? in equations 13.10 to 13.12, we computed the three steps needed to compute
the gradient for the error function given the unfolded laid out in equation 13.9. putting it all
together, we have

de

dwi   j

ds

de
dyi

dyi
dsi

=
dwi   j
=    (ti     yi) y(cid:48)

i hj

(13.13)

14

chapter 13. id4

factoring in a learning rate    gives us the following update formula for weight wi   j. note
that we also remove the minus sign, since we move against the gradient towards the minimum.

   wi   j =   (ti     yi) y(cid:48)

i hj

it is useful to introduce the concept of an error term   i. note that this term is associated
with a node, while the weight updates concern weights. the error term has to be computed
only once for the node, and it can be then used for each of the incoming weights.

  i = (ti     yi) y(cid:48)

i

this reduces the update formula to:

   wi   j =      i hj

weights to the hidden nodes

(13.14)

(13.15)

the computation of the gradient and hence the update formula for hidden nodes is quite anal-
ogous. as before, we    rst de   ne the linear combination zj (previously si) of input values xk
(previously hidden values hj) weighted by weights uj   k (previously weights wi   j).

(cid:88)

zj =

uj   kxk

k

this leads to the computation of the value of the hidden node hj.

hj = sigmoid(zj)

(13.16)

(13.17)

following the principles of id119, we need to compute the derivative of the error

e with respect to the weights uj   k. we decompose this derivative as before.

de

duj   k

=

de
dhj

dhj
dzj

dzj

duj   k

(13.18)

however, the computation of de
dhj

is more complex than in the case of output nodes, since
the error is de   ned in terms of output values yi, not values for hidden nodes hj. the idea
behind back-propagation is to track how the error caused by the hidden node contributed to
the error in the next layer. applying the chain rule gives us:

(cid:88)

i

de
dhj

=

de
dyi

dyi
dsi

dsi
dhj

(13.19)

13.2. introduction to neural networks

15

we already encountered the    rst two terms de
dyi

(equation 13.10) and dyi
dsi

(equation 13.11)

previously. to recap:

(cid:88)

de
dyi

dyi
dsi

=

d
dyi

(ti     yi(cid:48))2 y(cid:48)

i

1
2

i(cid:48)
(ti     yi)2 y(cid:48)

i

d
dyi

1
2

=
=    (ti     yi) y(cid:48)
=   i

i

(cid:88)

i

dsi
dhj

=

d
dhj

(cid:88)

i

de
dhj

=

the third term in equation 13.19 is computed straightforward.

wi   jhj = wi   j

(13.21)

putting equation 13.20 and equation 13.21 together, equation 13.19 can be solved as:

  iwi   j

(13.22)

this gives rise to a quite intuitive interpretation. the error that matters at the hidden node
hj depends on the error terms   i in the subsequent nodes yi, weighted by wi   j, i.e., the impact
the hidden node hj has on the output node yi.

let us tie up the remaining loose ends. the missing pieces from equation 13.18 are the

second term

dhj
dzj

=

d sigmoid(zj)

dzj

and third term

= sigmoid(zj)(1     sigmoid(zj)) = hj(1     hj) = h(cid:48)

j

dzj

duj   k

=

d

duj   k

uj   kxk = xk

(cid:88)

k

putting equation 13.22, equation 13.23, and equation 13.24 together gives us the gradient

de

duj   k

=

=

dzj

dhj
duj   k
dzj
(  iwi   j) h(cid:48)

j xk

de
dhj

(cid:88)
(cid:88)

i

if we de   ne an error term   j for hidden nodes analogous to output nodes

  j =

(  iwi   j) h(cid:48)

j

then we have an analogous update formula

i

   uj   k =      j xk

(13.20)

(13.23)

(13.24)

(13.25)

(13.26)

(13.27)

(cid:88)

16

summary

chapter 13. id4

we train neural networks by processing training examples, one at a time, and update weights
each time. what drives weight updates is the gradient towards a smaller error. weight updates
are computed based on error terms   i associated with each non-input node in the network.

for output nodes, the error term   i is computed from the actual output yi of the node for

our current network, and the target output ti for the node.

  i = (ti     yi) y(cid:48)

i

(13.28)

for hidden nodes, the error term   j is computed via back-propagating the error term   i

from subsequent nodes connected by weights wi   j.

  j =

(  iwi   j) h(cid:48)

j

(13.29)

i

computing y(cid:48)

i and h(cid:48)

j requires the derivative of the activation function, to which the weighted

sum of incoming values is passed.

given the error terms, weights wi   j (or uj   k) from each proceeding node hj (or xk) are

updated, tempered by a learning rate   .

   wi   j =      i hj
   uj   k =      j xk

(13.30)

once weights are updated, the next training example is processed. there are typically sev-

eral passes over the training set, called epochs.

example
given the neural network in figure 13.4, let us see how the training example (1,0)     1 is pro-
cessed.

let us start with the calculation of the error term    for the output node y0. during id136
(recall table 13.1 on page 11), we computed the linear combination of weighted hidden node
values s0 = 1.060 and the node value y0 = 0.743. the target value is t0 = 1.

   = (t0     y0) y(cid:48)

0 = (1     0.743)    sigmoid(cid:48)(1.060) = 0.257    0.191 = 0.049
with this number, we can compute weight updates, such as for weight w0   0.

   w0   0 =      0 h0 =       0.049    0.731 =       0.036

(13.31)

(13.32)

since the hidden node h0 leads only to one output node y0, the calculation of its error term

  0 is not more computationally complex.

  j =

(  iui   0) h(cid:48)

0 = (      w0   0)    sigmoid(cid:48)(z0) = 0.049    5    0.197 = 0.049

(13.33)

(cid:88)

i

table 13.2 summarizes the updates for all weights.

13.2. introduction to neural networks

17

node error term

y0

h0

h1

   = (t0     y0) sigmoid(s0)
   = (1     0.743)    0.191 = 0.049

weight updates
   w0   j =       hj
   w0   0 =       0.049    0.731 = 0.036
   w0   1 =       0.049    0.119 = 0.006
   w0   2 =       0.049    1 = 0.049
   uj   i =      j xi
   u0   0 =       0.048    1 = 0.048
   u0   1 =       0.048    0 = 0
   u0   2 =       0.048    1 = 0.048
  1 = 0.049       5    0.105 =    0.026    u1   0 =          0.026    1 =    0.026
   u1   1 =          0.026    0 = 0
   u1   2 =          0.026    1 =    0.026

  j =    wi   j sigmoid(zj)
  0 = 0.049    5    0.197 = 0.048

table 13.2: weight updates (with unspeci   ed learning rate   ) for the neural network in figure 13.4
(repeated above the table) when the training example (1,0)     1 is presented.

too high learning rate

bad initialization

local optimum

figure 13.6: problems with id119 training that motivate some of the re   nements detailed in
section 13.2.6: (a) a too high learning rate may lead to too drastic parameter updates, overshooting the
optimum, (b) bad initialization may require many updates to escape a plateau, and (c) the existence of
local optima which trap training.

.731.1191.7431015-5-2-4-23243x0x1x2h0h1h2y0  error(  )  error(  )  error(  )local optimumglobal optimum18

chapter 13. id4

figure 13.7: training progress over time. the error on the training set continuously decreases. however,
on a validation set (not used for training), at some point the error increases. training is stopped at the
validation minimum before such over-   tting sets in.

13.2.6 re   nements

we conclude our introduction to neural networks with some basic re   nements and consid-
erations. to motivate some of the re   nements, consider figure 13.6. while id119
training is a    ne idea, it may run into practical problems.

    setting the learning rate too high leads to updates that overshoot the optimum. con-

versely, a too low learning rate leads to slow convergence.

    bad initialization of weights may lead to long paths of many update steps to reach the
optimum. this is especially a problem with id180 like sigmoid which only
have a short interval of signi   cant change.

    the existence of local optima lead the search to get trapped and miss the global optimum.

validation set

neural network training proceeds for several epochs, i.e., full iterations over the training data.
when to stop? when we track training progress, we see that the error on the training set
continuously decreases. however, at some point over-   tting sets in, where the training data is
memorized and not suf   ciently generalized.

we can check this with an additional set of examples, called the validation set, that is not
used during training. see figure 13.7 for an illustration. when we measure the error on the
validation set at each point of training, we see that at some point this error increases. hence,
we stop training, when the minimum on the validation set is reached.

errortraining progressvalidationminimumvalidationtraining13.2. introduction to neural networks

19

weight initialization

before training starts, weights are initialized to random values. the values are choses from a
uniform distribution. we prefer initial weights that lead to node values that are in the transition
area for the activation function, and not in the low or high shallow slope where it would take a
long time to push towards a change. for instance, for the sigmoid activation function, feeding
values in the range of, say, [   1; 1] to the activation function leads to activation values in the
range of [0.269;0.731].

for the sigmoid activation function, commonly used formula for weights to the    nal layer

of a network are

where n is the size of the previous layer. for hidden layers, we chose weights from the range

(cid:2)     1   

(cid:3)

,

1   
n

n

(cid:2)    

   

   

6

   

6

   

,

nj + nj+1

nj + nj+1

(cid:3)

(13.34)

(13.35)

where nj is the size of the previous layer, nj size of next layer.

momentum term

consider the case where a weight value is far from its optimum. even if most training exam-
ples push the weight value in the same direction, it may still take a while for each of these
small updates to accumulate until the weight reaches its optimum. a common trick is to use
a momentum term to speed up training. this momentum term mt gets updated at each time
step t (i.e., for each training example). we combine the previous value of the momentum term
mt   1 with the current raw weight update value    wt and use the resulting momentum term
value to update the weights.

for instance, with a decay rate of 0.9, the update formula changes to

mt = 0.9mt   1 +    wt
wt = wt   1        mt

(13.36)

adapting learning rate per parameter

a common training strategy is to reduce the learning rate    over time. at the beginning the
parameters are far away from optimal values and have to change a lot, but in later training
stages we are concerned with    ne tuning, and a large learning rate may cause a parameter to
bounce around an optimum.

but different parameters may be at different stages on the path to their optimal values, so a
different learning rate for each parameter may be helpful. one such method, called adagrad,
records the gradients that were computed for each parameter and accumulates their square
values over time, and uses this sum to adjust the learning rate.

20

chapter 13. id4

the adagrad update formula is based on the sum of gradients of the error e with respect
dw . we divide the learning rate    for this weight

to the weight w at all time steps t, i.e., gt = det
this accumulated sum.

   wt =

gt

(13.37)

  (cid:113)(cid:80)t

   =1 g2
  

intutively, big changes in the parameter value (corresponding to big gradients gt), lead to a

reduction of the learning rate of the weight parameter.

combining the idea of momentum term and adjusting parameter update by their accumu-
lated change is the inspiration of adam, another method to transform the raw gradient into a
parameter update.

first, there is the idea of momentum, which is computed as in equation 13.36 above.

mt =   1mt   1 + (1       1)gt

(13.38)

then, there is the idea of the squares of gradients (as in adagrad) for adjusting the learning
rate. since raw accumulation does run the risk of becoming too large and hence permanently
depressing the learning rate, adam uses exponential decay, just like for the momentum term.

vt =   2vt   1 + (1       2)g2

t

(13.39)

the hyper parameters   1 and   2 are set typically close to 1, but this also means that early in
training the values for mt and vt are close to their initialization values of 0. to adjust for that,
they are corrected for this bias.

  mt =

mt
1       t

1

,

  vt =

vt

1       t

2

(13.40)

with increasing training time steps t, this correction goes away: limt       1

1     t     1.

having these pieces in hand (learning rate   , momentum   mt, accumulated change   vt),

weight update per adam is computed as

   wt =

     
  vt +  

  mt

(13.41)

common values for the hyper parameters are   1 = 0.9,   2 = 0.999 and   = 10   8.

there are various other adaptation schemes. this is an active area of research. for instance,
the second order gradient gives some useful information about the rate of change. however, it
is often expensive to compute, so other shortcuts are taken.

13.2. introduction to neural networks

21

dropout

the parameter space in which back-propagation learning and its variants are operating is lit-
tered with local optima. the hill-climbing algorithm may just climb a mole hill and be stuck
there, instead of moving towards a climb of the highest mountain. various methods have been
proposed to get training out of these local optima. one currently popular method in neural
machine translation is called drop-out.

it sounds a bit simplistic and wacky. during training, some of the nodes of the neural
network are ignored. their values are set to 0, and their associated parameters are not updated.
these dropped-out nodes are chosen at random, and may account for as much as 10%, 20% or
even more of all the nodes. training resumes for some number of iterations without the nodes,
and then a different set of drop-out nodes are selected.

the dropped-out nodes played some useful role in the model trained up to the point where
they are ignored. after that, other nodes have to pick up the slack. the end result is a more
robust model where several nodes share similar roles.

layer id172

layer id172 addresses a problem that arises especially in the deep neural networks
that we are using in id4, where computing proceeds through a large
sequence of layers. for some training examples, average values at one layer may become very
large, which feed into the following layer, also producing large output values, and so on. this is
especially a problem with id180 that do not limit the output to a narrow interval,
such as recti   ed linear units. for other training examples the average values at the same layers
may be very small. this causes a problem for training. recall from equation 13.30, that gradient
updates are strongly effected by node values. too large node values lead to exploding gradients
and too small node values lead to diminishing gradients.

to remedy this, the idea is to normalize the values on a per-layer basis. this is done by
adding additional computational steps to the neural network. recall that a feed-forward layer
consists of the the id127 of the weight matrix w with the node values from the
previous layer (cid:126)hl   1, resulting in a weighted sum (cid:126)sl, followed by an activation function such as
sigmoid.

(13.42)

we can compute the mean and variance of the values in the weighted sum vector (cid:126)sl by

(cid:126)sl = w (cid:126)hl   1
(cid:126)hl = sigmoid( (cid:126)hl)

1
h

h(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1
h(cid:88)

i   1

sl
i

h

i   1

  l =

  l =

i       l)2
(sl

(13.43)

22

chapter 13. id4

using these values, we normalize the vector (cid:126)sl using two additional bias vectors (cid:126)g and (cid:126)b

(13.44)
where    is element-wise multiplication and the difference subtracts the scalar average from each
vector element.

(cid:126)  sl =

(cid:126)g

  l    ((cid:126)sl       l) + (cid:126)b

the formula    rst normalizes the values in (cid:126)sl by shifting them against their average value,
hence ensuring that their average afterwards is 0. the resulting vector is then divided by
the variance   l. the additional bias vectors give some    exibility, they may be shared across
multiple layers of the same type, such as multiple time steps in a recurrent neural network (we
will introduce these in section 13.4.4 on page 39).

mini batches

each training example yields a set of weight updates    wi. we may    rst process all the training
examples and only afterwards apply all the updates. but neural networks have the advantage
that they can immediately learn from each training example. a training method that updates
the model with each training example is called online learning. the online learning variant of
id119 training is called stochastic id119.

online learning generally takes fewer passes over the training set (called epochs) for con-
vergence. however, since training constantly changes the weights, it is hard to parallelize.
so, instead, we may want to process the training data in batches, accumulate the weight up-
dates, and then apply them collectively. these smaller sets of training examples are called
mini batches to distinguish this approach from batch training where the entire training set is
considered one batch.

there are other variations to organize the processing of the training set, typically motivated
by restrictions of parallel processing. if we process the training data in mini batches, then we
can parallelize the computation of weight update values    w, but have to synchronize their
summation and application to the weights. if we want to distribute training over a number of
machines, it is computationally more convenient to break up the training data in equally sized
parts, perform online learning for each of the parts (optionally using smaller mini batches),
and then average the weights. surprisingly, breaking up training this way, often leads to better
results than straightforward linear processing.

finally, a scheme called hogwild runs several training threads that immediately update
weights, even though other threads still use the weight values to compute gradients. while
this is clearly violates the safe guards typically taken in parallel programming, it does not hurt
in practical experience.

vector and matrix operations

we can express the calculations needed for handling neural networks as vector and matrix
operations.

13.3. computation graphs

23

    forward computation: (cid:126)s = w (cid:126)h
    activation function: (cid:126)y = sigmoid((cid:126)h)
    error term: (cid:126)   = ((cid:126)t     (cid:126)y) sigmoid   ((cid:126)s)
    propagation of error term: (cid:126)  i = w (cid:126)  i+1    sigmoid   ((cid:126)s)
    weight updates:    w =    (cid:126)   (cid:126)ht

executing these operations is computationally expensive. if our layers have, say, 200 nodes,
then the matrix operation w(cid:126)h requires 200    200 = 40, 000 multiplications. such matrix oper-
ations are also common in another highly used area of computer science: graphics processing.
when rendering images on the screen, the geometric properties of 3-dimensional objects have
to be processed to generate the color values of the 2-dimensional image on the screen. since
there is high demand for fast graphics processing, for instance for the use in realistic looking
computer games, specialized hardware has become commonplace: graphics processing units
(gpus).

these processors have a massive number of cores (for example, the nvidia gtx 1080ti
gpu provides 3584 thread processors) but a rather lightweight instruction set. gpus provide
instructions that are applied to many data points at once, which is exactly what is needed out
the vector space computations listed above. programming for gpus is supported by various
libraries, such as cuda for c++, and has become an essential part of developing large scale
neural network applications.

the general term for scalars, vectors, and matrices is tensors. a tensor may also have more
dimensions: a sequence of matrices can be packed into a 3-dimensional tensor. such large
objects are actually frequently used in today   s neural network toolkits.

further readings a good introduction to modern neural network research is the textbook    deep
learning    (goodfellow et al., 2016). there is also book on neural network methods applied to the natural
language processing in general (goldberg, 2017).

a number of key techniques that have been recently developed have entered the standard reper-
toire of id4 research. training is made more robust by methods such as drop-out
(srivastava et al., 2014), where during training intervals a number of nodes are randomly masked. to
avoid exploding or vanishing gradients during back-propagation over several layers, gradients are typ-
ically clipped (pascanu et al., 2013). layer id172 (lei ba et al., 2016) has similar motivations, by
ensuring that node values are within reasonable bounds.

an active topic of research are optimization methods that adjust the learning rate of id119
training. popular methods are adagrad (duchi et al., 2011), adadelta (zeiler, 2012), and currently adam
(kingma and ba, 2015).

13.3 computation graphs

for our example neural network from section 13.2.5, we painstakingly worked out derivates
for gradient computations needed by id119 training. after all this hard work, it may

24

(cid:20)1.0
(cid:21)

0.0

x

(cid:20)3
(cid:21)

2

chapter 13. id4

(cid:20)3 4

(cid:21)

2 3

w1

(cid:21)

prod

(cid:20) 1
(cid:20).731
(cid:21)

   2

.119

sum

sigmoid

(cid:2)3.06(cid:3)

prod

(cid:20)   2
(cid:21)

   4

b1

(cid:2)5    5(cid:3)

w2

(cid:2)1.06(cid:3)
(cid:2).743(cid:3)

sum

sigmoid

(cid:2)   2(cid:3)

b2

figure 13.8: two layer feed-forward neural network as a computation graph, consisting of the input
value x, weight parameters w1, w2, b1, b2, and computation nodes (product, sum, sigmoid). to the right
of each parameter node, its value is shown. to the left of input and computation nodes, we show how
the input (1, 0)t is processed by the graph.

come as surprise that you will likely never have to do this again. it can be done automatically,
even for arbitrarily complex neural network architectures. there are a number of toolkits that
allow you to de   ne the network and it will take care of the rest. in this section, we will take a
close look at how this works.

13.3.1 neural networks as computation graphs

first, we will take a different look at the networks we are building. we previously represented
neural networks as graphs consisting of nodes and their connections (recall figure 13.4 on
page 10), or by mathematical equations such as

h = sigmoid(w1x + b1)
y = sigmoid(w2h + b2)

(13.45)

the equations above describe the feed-forward neural network that we use as our running
example. we now represent this math in form of a computation graph. see figure 13.8 for
an illustration of the computation graph for our network. the graph contains as nodes the
parameters of the models (the weight matrices w1, w2 and bias vectors b1, b2), the input x and
the mathematical operations that are carried out between them (product, sum, and sigmoid).
next to each parameter, we show their values.

13.3. computation graphs

25

neural networks, viewed as computation graphs, are any arbitrary connected operations
between an input and any number of parameters. some of these operations may have little to
do with any inspiration from neurons in the brain, so we are stretching the term neural networks
quite a bit here. the graph does not have to have a nice tree structure as in our example,
but may be any acyclical directed graph, i.e., anything goes as long there is a straightforward
processing direction and no cycles. another way to view such a graph is as a fancy way to
visualize a sequence of function calls that take as arguments the input, parameters, previously
computed values, or any combination thereof, but have no recursion or loops.

processing an input with the neural network requires placing the input value into the node
x and carrying out the computations. in the    gure, we show this with the input vector (1, 0)t .
the resulting numbers should look familiar since they are the same as when previously worked
through this example in section 13.2.4.

before we move on, let us take stock of what each computation node in the graph has to

accomplish. it consists of the following:

    a function that executes its computation operation
    links to input nodes
    when processing an example, the computed value

we will add two more items to each node in the following section.

13.3.2 gradient computations

so far, we showed how the computation graph can be used process an input value. now we
will examine how it can be used to vastly simply model training. model training requires an
error function and the computation of gradients to derive update rules for parameters.

the    rst of these is quite straightforward. to compute the error, we need to add another
computation at the end of the computation graph. this computation takes the computed out-
put value y and the given correct output value t from the training data and produces an error
2 (t    y)2. from the view of training, the result of
value. a typical error function is the l2 norm 1
the execution of the computation graph is an error value.

now, for the more dif   cult part     devis-
ing update rules for the parameters. look-
ing at the computation graph, model updates
originate from the error values and propagate
back to the model parameter. hence, we call
the computations needed to compute the up-
date values also the backward pass through
the graph, opposed to the forward pass that
computed output and error.

calculus refresher
in calculus, the chain rule is a formula for
computing the derivative of the composi-
tion of two or more functions. that is, if f
and g are functions, then the chain rule ex-
presses the derivative of their composition
f    g (the function which maps x to f (g(x)))
in terms of the derivatives of f and g and
the product of functions as follows:
(f     g)(cid:48) = (f(cid:48)     g)    g(cid:48).
this can be written more explicitly in terms
of the variable. let f = f    g, or equiva-
lently, f (x) = f (g(x)) for all x. then one

26

chapter 13. id4

consider the chain of operations that con-
nect the weight matrix w2 to the error com-
putation.

e = l2(y, t)
y = sigmoid(s)
s = sum(p, b2)
p = prod(h, w2)

(13.46)

where h are the values of the hidden layer
nodes, resulting from earlier computations.

to compute the update rule for the pa-
rameter matrix w2, we view the error as a
function of these parameters and take the
derivative with respect to them, in our case
dl2(w2)
. recall that when we computed this
derivate we    rst broke it up into steps using
the chain rule. we now do the same here.

dw2

dl2(w2)

dw2

=

dl2(sigmoid(sum(prod(h, w2), b2)), t)

dw2

(13.47)

=

dl2(y, t)

dsigmoid(s)

dsum(p, b2)

dprod(h, w2)

dy

ds

dp

dw2

note that for the purpose for computing an update rule for w2, we treat all the other vari-
ables in this computation (the target value t, the bias vector b2, the hidden node values h) as
constants. this breaks up the derivative of the error with respect to the parameters w2 into a
chain of derivatives along the line of the nodes of the computation graph.

hence, all we have to do for gradient computations is to come up with derivatives for each

node in the computation graph. in our example these are

dl2(y, t)

dy

=

2 (t     y)2
d 1

dy

= t     y

dsigmoid(s)

ds

dsum(p, b2)

dp

dprod(h, w2)

dw2

= sigmoid(s)(1     sigmoid(s))

=

=

dp + b2

dp
dw2h
dw2

= 1

= h

(13.48)

if we want to compute the gradient update for a parameter such as w2, we compute values

in a backward pass, starting from the error term y. see figure 13.9 for an illustration.

to give more detail on the computation of the gradients in the backward pass, starting at

the bottom of the graph:

13.3. computation graphs

27

(cid:20)1.0
(cid:21)

0.0

x

(cid:20)3
(cid:21)

2

prod

,

b1

w1

      

2 3

(cid:21)

(cid:21)

   2

   .0258

   .0258

0
   .0258 0

   .116
sum

0
   .0258 0
,

(cid:21)
(cid:20)3 4
(cid:20) 0484
(cid:21)
(cid:20) .0484
(cid:20)   .0935
(cid:21)
(cid:20) 1
(cid:21)
(cid:20) .0484
(cid:21)
(cid:20) .0484
(cid:21)
(cid:20) .0484
(cid:20).731
(cid:21)
(cid:20) .246
(cid:2)1.06(cid:3)
(cid:2).743(cid:3)
(cid:2).0331(cid:3)

(cid:2)3.06(cid:3)

   .0258

sigmoid

   .246

prod

(cid:21)

.119

w2

(cid:21)
(cid:20)   2

   4

(cid:21)

(cid:20) .0484

   .0258

      

(cid:2)   2(cid:3)       (cid:2).0492(cid:3)

(cid:2)5    5(cid:3)       (cid:2).0360 .00587(cid:3)
,(cid:2).0360 .00587(cid:3)
(cid:2).0492(cid:3),(cid:2).0492(cid:3)
(cid:2).191(cid:3)   (cid:2).257(cid:3) =(cid:2).0492(cid:3)
(cid:2)1.0(cid:3)
(cid:2).257(cid:3)

b2

t

l2

sum

sigmoid

figure 13.9: computation graph with gradients computed in the backward pass for the training example
(0, 1)t     1.0. gradients are computed with respect to the input of the nodes, so some nodes that have
two inputs also have two gradients. see text for details on the computations of the values.

28

chapter 13. id4

    for the l2 node, we use the formula
dl2(y, t)

dy

2 (t     y)2
d 1

dy

=

= t     y

(13.49)

the given target output value given as training data is t = 1, while we computed y =
0.743 in the forward pass. hence, the gradient for the l2 norm is 1     0.743 = 0.257. note
that we are using values computed in the forward pass for these gradient computations.

    for the lower sigmoid node, we use the formula

dsigmoid(s)

ds

= sigmoid(s)(1     sigmoid(s))

(13.50)

recall that the formula for the sigmoid is sigmoid(s) = 1
1+e   s . plugging in the value for
s = 1.06 computed in the forward pass into this formula gives us 0.191. the chain rule
requires us to multiply this with the value that we just computed for the l2 node, i.e.,
0.257, which gives us 0.191    0.257 = 0.0492.

    for the lower sum node, we simply copy the previous value, since the derivate is 1:

dsum(p, b2)

dp

=

dp + b2

dp

= 1

(13.51)

note that there are two gradients associated with the sum node. one with respect to the
output of the prod node, and one with the b2 parameter. in both cases, the derivative is 1,
so both values are the same. hence, the gradient in both cases is 0.0492.

    for the lower prod node, we use the formula
dprod(h, w2)

dw2

=

dw2h
dw2

= h

(13.52)

so far, we dealt with scalar values. here we encounter vectors for the    rst time: the value
of the hidden nodes h = (0.731, 0.119)t . the chain rule requires us to multiply this with
the previously computed scalar 0.0492:

(cid:16)(cid:34)

(cid:35)

0.731
0.119

   0.0492

(cid:17)t

(cid:104)

(cid:105)

=

0.0360 0.00587

as for the sum node, there are two inputs and hence two gradients. the other gradient is
with respect to the output of the upper sigmoid node.

dprod(h, w2)

dh

=

dw2h

dh

= w2

similarly to above, we compute

(w2    0.0492)t =

(cid:16)(cid:104)

(cid:105)    0.0492
(cid:17)t

=

5    5

(13.53)

(cid:34)

(cid:35)

0.246
   0.246

13.3. computation graphs

29

having all the gradients in place, we can now read of the relevant values for weight up-
dates. these are the gradients associated with trainable parameters. for the w2 weight matrix,
this is the second gradient of the prod node. so the new value for w2 at time step t + 1 is

(cid:104)

(cid:105)       

(cid:104)

(cid:105)

5 5

0.0360 0.00587

(13.54)

w t+1

2 = w t

2       

dprod(x, w t
2)

dw t
2

=

the remaining computations are carried out in very similar fashion, since they form simply

another layer of the feed-forward neural network.

our example did not include one special case: the output of a computation may be used
multiple times in subsequent steps of a computation graphs. so, there are multiple output
nodes that feed back gradients in the back-propagation pass. in this case, we add up the gradi-
ents from these descendent steps to factor in their added impact.

let us take a second look at what a node in a computation graph comprises:
    a function that computes its value
    links to input nodes (to obtain argument values)
    when processing an example in the forward pass, the computed value
    a function that executes its gradient computation
    links to children nodes (to obtain downstream gradient values)
    when processing an example in the forward pass, the computed gradient

from an object oriented programming view, a node in a computation graph provides a
forward and backward function for value and gradient computations. as instantiated in an
computation graph, it is connected with speci   c inputs and outputs, and is also aware of the
dimensions of its variables its value and gradient. during forward and backward pass, these
variables are    lled in.

13.3.3 deep learning frameworks

in the next sections, we will encounter various network architectures. what all of these share,
however, are the need for vector and matrix operations, as well as the computation of deriva-
tives to obtain weight update formulas. it would be quite tedious to write almost identical
code to deal with each these variants. hence, a number of frameworks have emerged to sup-
port developing neural network methods for any chosen problem. at the time of writing, the
most prominent ones are theano1 (a python library that dymanically generates and compiles
c++ code and is build on numpy), torch2 (a machine learning library and a script language
based on the lua programming language), pytorch3 (the python variant of torch), dynet4 (a

1http://deeplearning.net/software/theano/
2http://torch.ch/
3http://pytorch.org/
4http://dynet.readthedocs.io/

30

chapter 13. id4

c++ implementation by natural language processing researchers that can be used as a library
in c++ or python), and tensor   ow5 (a more recent entry to the genre from google).

these frameworks are less geared towards ready-to-use neural network architectures, but
provide ef   cient implementations of the vector space operations and computation of deriva-
tives, with seaid113ss support of gpus. our example from section 13.2 can be implemented in
a few lines of python code, as we will show in this section, using the example of theano (other
frameworks are quite similar).

you can execute the following commands on the python command line interface if you    rst

installed theano (pip install theano).

> import numpy
> import theano
> import theano.tensor as t

the mapping of the input layer x to the hidden layer h uses a weight matrix w, a bias vector
b, and a mapping function which consists of the linear combination t.dot and the sigmoid
activation function.

> x = t.dmatrix()
> w = theano.shared(value=numpy.array([[3.0,2.0],[4.0,3.0]]))
> b = theano.shared(value=numpy.array([-2.0,-4.0]))
> h = t.nnet.sigmoid(t.dot(x,w)+b)

note that we de   ne x as a matrix. this allows us to process several training examples at
once (a sequence of vectors). a good way to think about these de   nitions of x and h is in term of
a functional programming language. they symbolically de   ne operations. to actually de   ne a
function that can be called, the theano method function is used.

> h_function = theano.function([x], h)
> h_function([[1,0]])
array([[ 0.73105858, 0.11920292]])

this example call to h_function computes the values for the hidden nodes (compare to the

numbers in table 13.1 on page 11).

the mapping from the hidden layer h to the output layer y is de   ned in the same fashion.

w2 = theano.shared(value=numpy.array([5.0,-5.0] ))
b2 = theano.shared(-2.0)
y_pred = t.nnet.sigmoid(t.dot(h,w2)+b2)

again, we can de   ne a callable function to test the full network.

> predict = theano.function([x], y_pred)
> predict([[1,0]])
array([[ 0.7425526]])

5http://www.tensorflow.org/

13.4. neural language models

31

model training requires the de   nition of a cost function (we use the l2 norm). to formulate
it, we    rst need to de   ne the variable for the correct output. the overall cost is computed as
average over all training examples.
> y = t.dvector()
> l2 = (y-y_pred)**2
> cost = l2.mean()

id119 training requires the computation of the derivative of the cost function
with respect to the model parameters (i.e., the values in the weight matrices w and w2 and the
bias vectors b and b2. a great bene   t of using theano is that it computes the derivatives for
you. the following is also an example of a function with multiple inputs and multiple outputs.
> gw, gb, gw2, gb2 = t.grad(cost, [w,b,w2,b2])

we have now all we need to de   ne training. the function updates the model parameters

and returns the current predictions and cost. it uses a learning rate of 0.1.
> train = theano.function(inputs=[x,y],outputs=[y_pred,cost],

updates=((w, w-0.1*gw), (b, b-0.1*gb),

(w2, w2-0.1*gw2), (b2, b2-0.1*gb2)))

let us de   ne the training data.

> data_x = numpy.array([[0,0],[0,1],[1,0],[1,1]])
> data_y = numpy.array([0,1,1,0])
> predict(data_x)
array([ 0.18333462, 0.7425526 , 0.7425526 , 0.33430961])
> train(data_x,data_y)
[array([ 0.18333462, 0.7425526 , 0.7425526 , 0.33430961]),
array(0.06948320612438118)]

the training function returns the prediction and cost before the updates.

if we call the

training function again, then the predictions and cost have changed for the better.
> train(data_x,data_y)
[array([ 0.18353091, 0.74260499, 0.74321824, 0.33324929]),
array(0.06923193686092949)]

typically, we would loop over the training function until convergence. as discussed above,
we may also break up the training data into mini-batches and train on one mini-batch at a time.

13.4 neural language models

neural networks are a very powerful method to model id155 distributions
with multiple inputs p(a|b, c, d). they are robust to unseen data points     say, an unobserved
(a,b,c,d) in the training data. using traditional statistical estimation methods, we may address
such a sparse data problem with back-off and id91, which require insight into the prob-
lem (what part of the conditioning context to drop    rst?) and arbitrary choices (how many
clusters?).

32

chapter 13. id4

figure 13.10: sketch of a neural language model: we predict a word wi based on its preceding words.

id165 language models which reduce the id203 of a sentence to the product of word
probabilities in the context of a few previous words     say, p(wi|wi   4, wi   3, wi   2, wi   1). such
models are a prime example for a id155 distribution with a rich conditioning
context for which we often lack data points and would like to cluster information. in statistical
language models, complex discounting and back-off schemes are used to balance rich evidence
from lower order models     say, the bigram model p(wi|wi   1)     with the sparse estimates from
high order models. now, we turn to neural networks for help.

13.4.1 feed-forward neural language models

figure 13.10 gives a basic sketch of a 5-gram neural network language model. network nodes
representing the context words have connections to a hidden layer, which connects to the out-
put layer for the predicted word.

representing words we are immediately faced with a dif   cult question: how do we rep-
resent words? nodes in a neural network carry real-numbered values, but words are discrete
items out of a very large vocabulary. we cannot simply use token ids, since the neural network
will assume that token 124,321 is very similar to token 124,322     while in practice these num-
bers are completely arbitrary. the same arguments applies to the idea of using bit encoding
for token ids. the words (1, 1, 1, 1, 0, 0, 0, 0)t and (1, 1, 1, 1, 0, 0, 0, 1)t have very similar encod-
ings but may have nothing to do with each other. while the idea of using such bit vectors is
occasionally explored, it does not appear to have any bene   ts over what we consider next.

instead, we will represent each word with a high-dimensional vector, one dimension per
word in the vocabulary, and the value 1 for the dimension that matches the word, and 0 for the
rest. the type of vectors are called one hot vector. for instance:

    dog = (0, 0, 0, 0, 1, 0, 0, 0, 0, ...)t
    cat = (0, 0, 0, 0, 0, 0, 0, 1, 0, ...)t
    eat = (0, 1, 0, 0, 0, 0, 0, 0, 0, ...)t

word 1word 2word 3word 4word 5hidden layer13.4. neural language models

33

figure 13.11: full architecture of a feed-forward neural network language model. context words
(wi   4, wi   3, wi   2, wi   1) are represented in a one-hot vector, then projected into continuous space as
id27s (using the same weight matrix c for all words). the predicted word is computed
as a one-hot vector via a hidden layer.

these are very large vectors, and we will continue to wrestle with the impact of this choice
to represent words. one stopgap is to limit the vocabulary to the most frequent, say, 20,000
words, and pool all the other words in an other token. we could also use word classes (either
automatic clusters or linguistically motivated classes such as part-of-speech tags) to reduce the
dimensionality of the vectors. we will revisit the problem of large vocabularies later.

to pool evidence between words, we introduce another layer between the input layer and
the hidden layer. in this layer, each context word is individually projected into a lower dimen-
sional space. we use the same weight matrix for each of the context words, thus generating a
continuous space representation for each word, independent of its position in the conditioning
context. this representation is commonly referred to as id27.

words that occur in similar contexts should have similar id27s. for instance,

if the training data for a language model frequently contains the id165s

    but the cute dog jumped
    but the cute cat jumped
    child hugged the cat tightly
    child hugged the dog tightly
    like to watch cat videos
    like to watch dog videos

then the language model would bene   t from the knowledge that dog and cat occur in similar
contexts and hence are somewhat interchangeable. if we like to predict from a context where
dog occurs but we have seen this context only with the word cat, then we would still like to treat
this as positive evidence. id27s enable generalizing between words (id91)
and hence having robust predictions in unseen contexts (back-off).

word 1word 2word 3word 4word 5hidden layercccc34

chapter 13. id4

neural network architecture see figure 13.11 for a visualization of the architecture the fully
   edged feed forward neural network language model, consisting of the context words as one-
hot-vector input layer, the id27 layer, the hidden layer and predicted output word
layer.

the context words are    rst encoded as one-hot vectors. these are then passed through the
embedding matrix c, resulting in a vector of    oating point numbers, the id27.
this embedding vector has typically in the order of 500 or 1000 nodes. note that we use the
same embedding matrix c for all the context words.

also note that mathematically there is not all that much going on here. since the input to
the multiplication to the matrix c is a one hot vector, most of the input values to the matrix
multiplication are zeros. so, practically, we are selecting the one column in the matrix that
corresponds to the input word id. hence, there is no use for an activation function here. in a
way, the embedding matrix a lookup table c(wj) for id27s, indexed by the word
id wj.

c(wj) = c wj

(13.55)

mapping to the hidden layer in the model requires concatenation of all context word em-
beddings c(wj) as input to a typical feed-forward layer, say, using tanh as activation function.

(cid:16)

(cid:88)

(cid:17)

h = tanh

bh +

hjc(wj)

(13.56)

j

the output layer is interpreted as a id203 distribution over words. as before,    rst the

linear combination si of weights wij and hidden node values hj is computed for each node i.

s = w h

(13.57)

to ensure that it is indeed a proper id203 distribution, we use the softmax activation

function to ensure that all values add up to one.

pi = softmax(si, (cid:126)s) =

esi(cid:80)

j esj

(13.58)

what we described here is close to the neural probabilistic language model proposed by
bengio et al. (2003). this model had one more twist, it added direct connections of the context
id27s to the output word. so, equation 13.57 is replaced by

s = w h +

u c(wj)

(13.59)

j

their paper reports that having such direct connections from context words to output
words speeds up training, although does not ultimately improve performance. we will en-
counter the idea of short-cutting hidden layers again a bit later when we discuss deeper models
with more hidden layers. they are also called residual connections, skip connections, or even
highway connections.

(cid:88)

13.4. neural language models

35

training we train the parameters of a neural language model (id27 matrix, weight
matrices, bias vectors) by processing all the id165s in the training corpus. for each id165,
we feed the context words into the network and match the network   s output against the one-
hot vector of the correct word to be predicted. weights are updated using back-propagation
(we will go into details in the next section).

language models are commonly evaluated by perplexity, which is related to the id203
given to proper english text. a language model that likes proper english is a good language
model. hence, the training objective for language models is to increase the likelihood of the
training data.

during training, given a context x = (wn   4, wn   3, wn   2, wn   1), we have the correct value

for the 1-hot vector (cid:126)y. for each training example (x, (cid:126)y), likelihood is de   ned as

l(x, (cid:126)y; w ) =    (cid:88)

yk log pk

(13.60)

note that only one value yk is 1, the others are 0. so this really comes down to the id203
pk given to the correct word k. de   ning likelihood this way allows us to update all weights,
also the one that lead to the wrong output words.

k

13.4.2 id27

before we move on, it is worth re   ecting the role of id27s in neural machine trans-
lation and many other natural language processing tasks. we introduced them here as compact
encoding of words in relatively high-dimensional space, say 500 or 1000    oating point num-
bers. in the    eld of natural language processing, at the time of this writing, id27s
have acquired the reputation of almost magical quality.

consider the role they play in the neural language language that we just described. they

represent context words to enable prediction the next word in a sequence.

recall part of our earlier example:
    but the cute dog jumped
    but the cute cat jumped

since dog and cat occur in similar contexts, their in   uence on predicting the word jumped
should be similar. it should be different from words such as dress which is unlikely to trigger
the completion jumped. the idea that words that occur in similar contexts are semantically
similar is a powerful idea in lexical semantics.

at this point in the argument, researchers love to cite john rupert firth:

you shall know a word by the company it keeps.

or, as ludwig wittgenstein put it a bit more broadly:

36

chapter 13. id4

figure 13.12: id27s projected into 2d. semantically similar words occur close to each other.

the meaning of a word is its use.

meaning and semantics are quite dif   cult concepts with largely unresolved de   nition. the
idea of distributional lexical semantics is to de   ne word meaning by their distributional prop-
erties, i.e., in which contexts they occur. words that occur in similar contexts (dog and cat)
should have similar representations. in vector space models, such as the id27s
that we use here, similarity can be measured by a distance function, e.g., the cosine distance    
the angle between the vectors.

if we project the high-dimensional id27s down to two dimensions, we can
visualize id27s as shown in figure 13.12. in this    gure, words that are similar
(drama, theater, festival) are clustered together.

but why stop there? we would like to have semantic representations so we can carry out

semantic id136 such as

    queen = king + (woman     man)
    queens = queen + (kings     king)

indeed there is some evidence that id27 allow just that (mikolov et al., 2013).
however, we better stop here and just note that id27s are a crucial tool in neural
machine translation.

13.4. neural language models

37

13.4.3 ef   cient id136 and training

training a neural language model is computationally expensive. for billion word corpora, even
with the use of gpus, training takes several days with modern compute clusters. even using
a neural language model as a scoring component in id151 decoding
requires a lot of computation. we could restrict its use only to re-ranking n-best lists or lattices,
or consider more ef   cient methods for id136 and training.

caching for id136 however, with a few considerations, it is actually possible to use this
neural language model within the decoder.

    id27s are    xed for the words, so do not actually need to carry out the map-

ping from one-hot vectors to id27s, but just store them beforehand.

    the computation between embeddings and the hidden layer can be also partly carried
out of   ine. note that each word can occur in one of the 4 slots for conditioning context
(assuming a 5-gram language model). for each of the slots, we can pre-compute the
id127 of id27 vector and the corresponding submatrix of
weights. so, at run time, we only have to sum up these pre-computations at the hidden
layer and apply the activation function.

    computing the value for each output node is insanely expensive, since there are as many
output nodes as vocabulary items. however, we are interested only in the score for a
given word that was produced by the translation model. if we only compute its node
value, we have a score that we can use.

the last point requires a longer discussion. if we compute the node value only for the word
that we want to score with the language model, we are missing an important step. to obtain a
proper id203, we need to normalize it, which requires the computation of the values for
all the other nodes.

we could simply ignore this problem and use the scores at face value. more likely words
given a context will get higher scores than less likely words, and that is the main objective. but
since we place no constraints on the scores, we may work with models where some contexts
give high scores to many words, while some contexts do not give preference for any.

it would be great, if the node values in the    nal layer were already normalized probabilities.
there are methods to enforce this during training. let us    rst discuss training in detail, and
then move to these methods in section 13.4.3.

noise contrastive estimation we discussed earlier the problem that computing probabili-
ties with a neural language model is very expensive due to the need to normalize the output
node values yi using the softmax function. this requires computing values for all output nodes,
even if we are only interested in the score for a particular id165. to overcome the need for

38

chapter 13. id4

this explicit id172 step, we would like to train a model that already has yi values that
are normalized.

one way is to include the constraint that the id172 factor z(x) = (cid:80)

j esj is close
to 1 in the objective function. so, instead of the just the simple likelihood objective, we may
include the l2 norm of the log of this factor. note that if log z(x) (cid:39) 0, then z(x) (cid:39) 1.

yk log pk        log2 z(x)

(13.61)

l(x, (cid:126)y; w ) =    (cid:88)

k

another way to train a self-normalizing model is called noise contrastive estimation. the
main idea is to optimize the model so that it can separate correct training examples from arti-
   cially created noise examples. this method needs less computation during training, since it
does not require the computation of all output node values.

formally, we are trying to learn the model distribution pm((cid:126)y|x; w ). given a noise distribu-
tion pn((cid:126)y|x)     in our case of id38 a unigram model pn((cid:126)y) is a good choice     we
   rst generate a set of noise examples un in addition to the correct training examples ut. if both
sets have the same size |un| = |ut|, then the id203 that a given example (x; (cid:126)y)     un     ut is
predicted to be a correct training example is
p(correct|x, (cid:126)y) =

(13.62)
the objective of noise contrastive estimation is to maximize p(correct|x, (cid:126)y) for correct train-
ing examples (x; (cid:126)y)     ut and to minimize it for noise examples (x; (cid:126)y)     un. using log-
likelihood, we de   ne the objective function as
log p(correct|x, (cid:126)y) +

log (1     p(correct|x, (cid:126)y))

pm((cid:126)y|x; w ) + pn((cid:126)y|x)

pm((cid:126)y|x; w )

(cid:88)

(cid:88)

(13.63)

l =

1
2|ut|

(x;(cid:126)y)   ut

1
2|un|

(x;(cid:126)y)   un

rable values. if pm((cid:126)y|x; w ) would generally overshoot     i.e.,(cid:80)

returning the the original goal of a self-normalizing model,    rst note that the noise distri-
bution pn((cid:126)y|x) is normalized. hence, the model distribution is encouraged to produce compa-
(cid:126)y pm((cid:126)y|x; w ) > 1 then it would
also give too high values for noise examples. conversely, generally undershooting would give
too low values to correct translation examples.

training is faster, since we only need to compute the output node value for the given train-
ing and noise examples     there is no need to compute the other values, since we do not nor-
malize with the softmax function.

given the de   nition of the training objective l, we have now a complete computation graph
that we can implement using standard deep learning toolkits, as we have done before. these
toolkits compute the gradients dl
dw for all parameters w and use them for parameter updates
via id119 training (or its variants).

it may not be immediately obvious why optimizing towards classifying correct against
noise examples gives rise to a model that also predicts the correct probabilities for id165s.
but this is a variant of methods that are common in id151 in the tuning
phase. mira (margin infused relaxation algorithm) and pro (pairwise ranked optimization)
follow the same principle.

13.4. neural language models

39

figure 13.13: recurrent neural language models: after predicting word 2 in the context of following
word 1, we re-use this hidden layer (alongside the correct word 2) to predict word 3. again, the hidden
layer of this prediction is re-used for the prediction of word 4.

13.4.4 recurrent neural language models

the feed-forward neural language model that we described above is able to use longer con-
texts than traditional statistical back-off models, since it has more    exible means to deal with
unknown contexts. namely, the use of id27s to make use of similar words, and
the robust handling of unseen words in any context position. hence, it is possible to condition
on much larger contexts than traditional statistical models. in fact, large models, say, 20-gram
models, have been reported to be used.

alternatively, instead of using a    xed context word window, recurrent neural networks
may condition on context sequences of any length. the trick is to re-use the hidden layer when
predicting word wn as additional input to predict word wn   1.

see figure 13.13 for an illustration. initially, the model does not look any different from the
feed-forward neural language model that we discussed so far. the inputs to the network is the
   rst word of the sentence w1 and a second set of neurons which at this point indicate the start
of the sentence. the id27 of w1 and the start-of-sentence neurons    rst map into a
hidden layer h1, which is then used to predict the output word w2.

this model uses the same architecture as before: words (input and output) are represented
with one-hot vectors; id27s and the hidden layer use, say, 500 real valued neurons.
we use a sigmoid activation function at the hidden layer and the softmax function at the output
layer.

things get interesting when we move to predicting the third word w3 in the sequence. one
input is the directly preceding (and now known) word w2, as before. however, the neurons
in the network that we used to represent start-of-sentence are now    lled with values from
the hidden layer of the previous prediction of word w2. in a way, these neurons encode the

word 1word 2ec1h1word 2word 3ech2h1copy valuesword 3word 4ech3h2copy values40

chapter 13. id4

figure 13.14: back-propagation through time: by unfolding the recurrent neural network over a    xed
number of prediction steps (here: 3), we can derive update formulas based on the training objective of
predicting all output words and back-propagation of the error via id119.

previous sentence context. they are enriched at each step with information about a new input
word and are hence conditioned on the full history of the sentence. so, even the last word of
the sentence is conditioned in part on the    rst word of the sentence. moreover, the model is
simpler: it has less weights than a 3-gram feed-forward neural language model.

how do we train such a model with arbitrarily long contexts?
one idea: at the initial stage (predicting the second word from the    rst), we have the same
architecture and hence the same training procedure as for feed-forward neural networks. we
assess the error at the output layer and propagate updates back to the input layer. we could
process every training example this way     essentially by treating the hidden layer from the
previous training example as    xed input the current example. however, this way, we never
provide feedback to the representation of prior history in the hidden layer.

the back-propagation through time training procedure (see figure 13.14) unfolds the re-
current neural network over a    xed number of steps, by going back over, say, 5 word predic-
tions. note that, despite limiting the unfolding to 5 time steps, the network is still able to learn
dependencies over longer distances.

back-propagation through time can be either applied for each training example (here called
time step), but this is computationally quite expensive. each time computations have to be
carried out over several steps. instead, we can compute and apply weight updates in mini-
batches (recall section 13.2.6). first, we process a larger number of training examples (say,
10-20, or the entire sentence), and then update the weights.

given modern compute power, fully unfolding the recurrent neural network has become
more common. while recurrent neural networks have in theory arbitrary length, given a spe-
ci   c training example, its size is actually known and    xed, so we can fully construct the com-
putation graph for each given training example, de   ne the error as the sum of word prediction
errors, and then carry out back-propagation over the entire sentence. this does require that we
can quickly build computation graphs     so-called dynamic computation graphs     which is
currently supported by some toolkits better than others.

word 1word 2ehhword 2word 3ehword 3word 4eh13.4. neural language models

41

13.4.5 long short-term memory models

consider the following step during word prediction in a sequential language model:

after much economic progress over the years, the country     has

the directly preceding word country will be the most informative for the prediction of the
word has, all the previous words are much less relevant. in general, the importance of words
decays with distance. the hidden state in the recurrent neural network will always be updated
with the most recent word, and its memory of older words is likely to diminish over time.

but sometimes, more distant words are much more important, as the following example

shows:

the country which has made much economic progress over the years still     has

in this example, the in   ection of the verb have depends on the subject country which is

separated by a long subordinate clause.

recurrent neural networks allow modeling of arbitrarily long sequences. their architecture

is very simple. but this simplicity causes a number of problems.

    the hidden layer plays double duty as memory of the network and as continuous space

representation used to predict output words.

    while we may sometimes want to pay more attention to the directly previous word, and
sometimes pay more attention to the longer context, there is no clear mechanism to con-
trol that.

    if we train the model on long sequences, then any update needs to back propagate to the
beginning of the sentence. however, propagating through so many steps raises concerns
that the impact of recent information at any step drowns out older information.6

the rather confusingly named long short-term memory (lstm) neural network architec-
ture addresses these issues. its design is quite elaborate, although it is not very dif   cult to use
in practice.

a core distinction is that the basic building block of id137, the so-called cell,
contains an explicit memory state. the memory state in the cell is motivated by digital memory
cells in ordinary computers. digital memory cells offer operations to read, write, and reset.
while a digital memory cell may store just a single bit, a lstm cell stores a real number.

furthermore, the read/write/reset operations in a lstm cell are regulated with a real num-

bered parameter, which are called gates (see figure 13.15).

6note that there is a corresponding exploding gradient problem, where over long distance gradient values
become too large. this is typically suppressed by clipping gradients, i.e., limiting them to a maximum value set as
a hyper parameter.

42

chapter 13. id4

figure 13.15: a cell in a lstm neural network. as recurrent neural networks, it receives input from the
preceeding layer (x) and the hidden layer values from the previous time step t    1. the memory state m
is updated from the input state i and the previous time   s value of the memory state mt   1. various gates
channel information    ow in the cell towards the output value o.

    the input gate parameter regulates how much new input changes the memory state.
    the forget gate parameter regulates how much of the prior memory state is retained (or

forgotten).

    the output gate parameter regulates how strongly the memory state is passed on to the

next layer.

formally, marking the input, memory, and output values with the time step t, we de   ne the

   ow of information within a cell as follows.

memoryt = gateinput    inputt + gateforget    memoryt   1
outputt = gateoutput    memoryt

(13.64)

the hidden node value ht passed on to the next layer is the application of an activation

function f to the output value.

ht = f (outputt)

(13.65)

an lstm layer consists of a vector of lstm cells, just as traditional layers consist of a
vector of nodes. the input to lstm layer is computed in the same way as the input to a
recurrent neural network node. given the node values for the prior layer xt and the values for
the hidden layer from the previous time step ht   1, the input value is the typical combination of
id127 with weights w x and w h and an activation function g.

(cid:16)

w xxt + w hht   1(cid:17)

inputt = g

(13.66)

input gate  output gate  forget gate  ximo         hm   lstm layer time t-1next layerylstm layer time tpreceding layer13.4. neural language models

43

but how are the gate parameters set? they actually play a fairly important role. in par-
ticular contexts, we would like to give preference to recent input (gateinput (cid:39) 1), rather re-
tain past memory (gateforget (cid:39) 1), or pay less attention to the cell at the current point in time
(gateoutput (cid:39) 0). hence, this decision has to be informed by a broad view of the context.

how do we compute a value from such a complex conditioning context? well, we treat it
like a node in a neural network. for each gate a     (input, forget, output) we de   ne matrices
w xa, w ha, and w ma to compute the gate parameter value by the multiplication of weights and
node values in the previous layer xt, the hidden layer ht   1 at the previous time step, and the
memory states at the previous time step memoryt   1, followed by an activation function h.

(cid:16)

w xaxt + w haht   1 + w mamemoryt   1(cid:17)

gatea = h

(13.67)

lstm are trained the same way as recurrent neural networks, using back-propagation
through time or fully unrolling the network. while the operations within a lstm cell are more
complex than in a recurrent neural network, all the operations are still based on matrix mul-
tiplications and differentiable id180. hence, we can compute gradients for the
objective function with respect to all parameters of the model and compute update functions.

13.4.6 id149

lstm cells add a large number of additional parameters. for each gate alone, multiple weight
matrices are added. more parameters lead to longer training times and risk over   tting. as
a simpler alternative, id149 (gru) have been proposed and used in neural
translation models. at the time of writing, lstm cells seem to make a comeback in neural
machine translation, but both are still commonly used.

see figure 13.16 for an illustration for gru cells. there is no separate memory state, just
a hidden state that serves both purposes. also, there are only two gates. these gates are
predicted as before from the input and the previous state.

updatet = g(wupdate inputt + uupdate statet   1 + biasupdate)

resett = g(wreset

inputt + ureset

statet   1 + biasreset)

(13.68)

the    rst gate is used in the combination of the input and previous state. this is combination
is identical to traditional recurrent neural network, except that the previous states impact is
scaled by the reset gate. since the gate   s value is between 0 and 1, this may give preference to
the current input.

combinationt = f (w inputt + u (resett     statet   1))

(13.69)

then, the update gate is used for a interpolation of the previous state and the just computed
combination. this is done as a weighted sum, where the update gate balances between the two.

44

chapter 13. id4

figure 13.16: gated recurrent unit (gru): a simpli   cation of long short term memory (lstm) cells.

statet =(1     updatet)     statet   1 +

updatet

    combinationt) + bias

(13.70)

in one extreme case, the update gate is 0, and the previous state is passed through directly.
in another extreme case, the update gate is 1, and the new state is mainly determined from the
input, with as much impact from the previous state as the reset gate allows.

it may seem a bit redundant to have two operations with a gate each that combine prior
state and input. however, these play different roles. the    rst operation yielding combinationt
(equation 13.69) is a classic recurrent neural network component that allows more complex
computations in the combination of input and output. the second operation yielding the
new hidden state and the output of the unit (equation 13.70) allows for bypassing of the in-
put, enabling long-distant memory that simply passes through information and, during back-
propagation, passes through the gradient, thus enabling long-distance dependencies.

13.4.7 deep models

the currently fashionable name deep learning for the latest wave of neural network research
has a real motivation. large gains have been seen in tasks such as vision and id103
due to stacking multiple hidden layers together.

more layers allow for more complex computations, just as having sequences of traditional
computation components (boolean gates) allows for more complex computations such as addi-
tion and multiplication of numbers. while this has been generally recognized for a long time,
modern hardware    nally enabled to train such deep neural networks on real world problems.

update gate  reset gate  xx   hh   gru layer time t-1next layerygru layer time tpreceding layer13.4. neural language models

45

figure 13.17: deep recurrent neural networks. the input is passed through a few hidden layers before
an output prediction is made. in deep stacked models, the hidden layers are also connected horizontally,
i.e., a layer   s values at time step t depends on its value at time step t     1 as well as the previous layer
at time step t. in deep transitional models, the layers at any time step t are sequentially connected and
   rst hidden layer is also informed by the last layer at time step t     1.

and we learned from experiments in vision and speech that having a handful, and even dozens
of layers does give increasingly better quality.

how does the idea of deep neural networks apply to the sequence prediction tasks common
in language? there are several options. figure 13.17 gives two examples. in shallow neural
networks, the input is passed to a single hidden layer, from which the output is predicted.
now, a sequence of hidden layers is used. these hidden layers ht,i may be deeply stacked, so
that each layer acts like the hidden layer in the shallow recurrent neural network. its state is
conditioned on its value at the previous time step ht   1,i and the value of previous layer in the
sequence ht,i   1.

ht,1 = f1(ht   1,1, xt)
ht,i = fi(ht   1,i, ht,i   1)
yt = fi+1(ht,i )

   rst layer
for i > 1
prediction from last layer i

(13.71)

or, the hidden layers may be directly connected in deep transitional networks, where the
   rst hidden layer ht,1 is informed by the last hidden layer at the previous time step ht   1,i, but
all other hidden layers are not connected to values from previous time steps.

ht,1 = f1(ht   1,i , xt)
ht,i = fi(ht,i   1)
yt = fi+1(ht,i )

   rst layer
for i > 1
prediction from last layer i

(13.72)

inputhiddenlayeroutputinputhiddenlayer 2outputhiddenlayer 1hiddenlayer 3shallowdeep stackedinputhiddenlayer 2outputhiddenlayer 1hiddenlayer 3deep transition46

chapter 13. id4

in all these equations, the function fi may be a feedforward layer (id127

plus activation function), an lstm cell or a gru cell.

experiments with using neural language models in traditional statistical machine transla-

tion have shown bene   ts with 3   4 hidden layers (luong et al., 2015a).

while modern hardware allows training of deep models, they do stretch computational
resources to their practical limit. not only are there more computations in the neural network,
convergence of training is typically slower. adding skip connections (linking the input directly
to the output or the    nal hidden layer) sometimes speeds up training, but we still talking about
a several times longer training times than shallow networks.

further readings the    rst vanguard of neural network research tackled language models. a
prominent reference for neural language model is bengio et al. (2003), who implement an id165 lan-
guage model as a feed-forward neural network with the history words as input and the predicted word
as output. schwenk et al. (2006) introduce such language models to machine translation (also called
   continuous space language models"), and use them in re-ranking, similar to the earlier work in speech
recognition. schwenk (2007) propose a number of speed-ups. they made their implementation avail-
able as a open source toolkit (schwenk, 2010), which also supports training on a graphical processing
unit (gpu) (schwenk et al., 2012).

by    rst id91 words into classes and encoding words as pair of class and word-in-class bits,
baltescu et al. (2014) reduce the computational complexity suf   ciently to allow integration of the neural
network language model into the decoder. another way to reduce computational complexity to enable
decoder integration is the use of noise contrastive estimation by vaswani et al. (2013), which roughly
self-normalizes the output scores of the model during training, hence removing the need to compute
the values for all possible output words. baltescu and blunsom (2015) compare the two techniques -
class-based word encoding with normalized scores vs. noise-contrastive estimation without normalized
scores - and show that the letter gives better performance with much higher speed.

as another way to allow straightforward decoder integration, wang et al. (2013) convert a contin-
uous space language model for a short list of 8192 words into a traditional id165 language model in
arpa (srilm) format. wang et al. (2014) present a method to merge (or    grow") a continuous space
language model with a traditional id165 language model, to take advantage of both better estimate for
the words in the short list and the full coverage from the traditional model.

finch et al. (2012) use a recurrent neural network language model to rescore n-best lists for a translit-
eration system. sundermeyer et al. (2013) compare feed-forward with long short-term neural network
language models, a variant of recurrent neural network language models, showing better performance
for the latter in a id103 re-ranking task. mikolov (2012) reports signi   cant improvements
with reranking n-best lists of machine translation systems with a recurrent neural network language
model.

neural language model are not deep learning models in the sense that they use a lot of hidden
layers. luong et al. (2015a) show that having 3-4 hidden layers improves over having just the typical 1
layer.

language models in id4: traditional id151 models
have a straightforward mechanism to integrate additional knowledge sources, such as a large out of
domain language model. it is harder for end-to-end id4. g  l  ehre et al. (2015)
add a language model trained on additional monolingual data to this model, in form of a recurrently

13.5. neural translation models

47

figure 13.18: sequence-to-sequence encoder-decoder model: extending the language model, we con-
catenate the english input sentence the house is big with the german output sentence das haus ist gro  .
the    rst dark green box (after processing the end-of-sentence token </s>) contains the embedding of
the entire input sentence .

neural network that runs in parallel. they compare the use of the language model in re-ranking (or, re-
scoring) against deeper integration where a gated unit regulates the relative contribution of the language
model and the translation model when predicting a word.

13.5 neural translation models

we are    nally prepared to look at actual translation models. we have already done most of
the work, however, since the most commonly used architecture for id4
is a straightforward extension of neural language models with one re   nement, an alignment
model.

13.5.1 encoder-decoder approach

our    rst stab at a neural translation model is a straightforward extension of the language
model. recall the idea of a recurrent neural network to model language as a sequential process.
given all previous words, such a model predicts the next word. when we reach the end of the
sentence, we now proceed to predict the translation of the sentence, one word at a time.

see figure 13.18 for an illustration. to train such a model, we simply concatenate the input
and output sentences and use the same method as to train a language model. for decoding, we
feed in the input sentence, and then go through the predictions of the model until it predicts an
end of sentence token.

how does such a network work? once processing reaches the end of the input sentence
(having predicted the end of sentence marker </s>), the hidden state encodes its meaning. in
other words, the vector holding the values of the nodes of this    nal hidden layer is the input
sentence embedding. this is the encoder phase of the model. then this hidden state is used
to produce the translation in the decoder phase.

<s>thethehousehouseisbig.isbig.</s>given wordembeddinghidden statepredicted word</s>dasdashaushausistgro  .istgro  .</s>48

chapter 13. id4

clearly, we are asking a lot from the hidden state in the recurrent neural network here.
it
during encoder phase, it needs to incorporate all information about the input sentence.
cannot forget the    rst words towards the end of the sentence. during the decoder phase, not
only does it need to have enough information to predict each next word, there also needs to be
some accounting for what part of the input sentence has been already translated, and what still
needs to be covered.

in practice, the proposed models works reasonable well for short sentences (up to, say,
10   15 words), but fails for long sentences. some minor re   nements to this model have been
proposed, such using the sentence embedding state as input to all hidden states of the decoder
phase of the model. this makes the decoder structurally different from the encoder and reduces
some of the load from the hidden state during decoding, since it does not need to remember
anymore the input. another idea is to reverse the order of the output sentence, so that the last
words of the input sentences are close to the last words of the output sentence.

however, in the following section, we will embark on a more signi   cant improvement of

the model, by explicitly modelling alignment of output words to input words.

13.5.2 adding an alignment model

at the time of writing, the state of the art in id4 is a sequence-to-
sequence encoder-decoder model with attention. that is a mouthful, but it is essentially the
model we just described in the previous section, with a explicitly alignment mechanism. in the
deep learning world, this alignment is called attention, we are using the words alignment and
attention interchangeable here.

since the attention mechanism does add a bit of complexity to the model, we are now slowly
building up to it, by    rst taking a look at the encoder, then the decoder, and    nally the attention
mechanism.

encoder

the task of the encoder is to provide a representation of the input sentence. the input sentence
is a sequence of words, for which we    rst consult the embedding matrix. then, as in the basic
language model described previously, we process these words with a recurrent neural network.
this results in hidden states that encode each word with its left context, i.e., all the preceding
words. to also get the right context, we also build a recurrent neural network that runs right-
to-left, or more precisely, from the end of the sentence to the beginning.

figure 13.19 illustrates the model. having two recurrent neural networks running in two
directions is called a bidirectional recurrent neural network. mathematically, the encoder
consists of the embedding lookup for each input word xj, and the mapping that steps through
the hidden states

      
hj and

      
hj

      
hj = f (
      
hj = f (

         
hj+1,   e xj)
         
hj   1,   e xj)

(13.73)
(13.74)

13.5. neural translation models

49

it consists of two recurrent
figure 13.19: id4 model, part 1: input encoder.
neural networks, running right to left and left to right (bidrectional recurrent neural network). the
encoder states are the combination of the two hidden states of the recurrent neural networks.

in the equation above, we used a generic function f for a cell in the recurrent neural net-
work. this function may be a typical feed-forward neural network layer     such as f (x) =
tanh(ax + b)     or the more complex id149 (grus) or long short term memory
cells (lstms). the original paper proposing this approached used grus, but lately lstms
have become more popular.

note that we could train these models by adding a step that predicts the next word in the
sequence, but we are actually training it in the context of the full machine translation model.
limiting the description to the decoder, its output is a sequence of word representations that
concatenate the two hidden states (

      
hj).

      
hj,

decoder

the decoder is also a recurrent neural network. it takes some representation of the input con-
text (more on that in the next section on the attention mechanism) and the previous hidden
state and output word prediction, and generates a new hidden decoder state and a new output
word prediction. see figure 13.20 for an illustration.

mathematically, we start with the recurrent neural network that maintains a sequence of
hidden states si which are computed from the previous hidden state si   1, the embedding of
the previous output word eyi   1, and the input context ci (which we still have to de   ne).

si = f (si   1, eyi   1, ci)

(13.75)

again, there are several choices for the function f that combines these inputs to generate
the next hidden state: linear transforms with activation function, grus, lstms, etc. typically,
the choice here matches the encoder. so, if we use lstms for the encoder, then we also use
lstms for the decoder.

from the hidden state. we now predict the output word. this prediction takes the form of
a id203 distribution over the entire output vocabulary. if we have a vocabulary of, say,
50,000 words, then the prediction is a 50,000 dimensional vector, each element corresponding
to the id203 predicted for one word in the vocabulary.

input wordembeddingsleft-to-rightrecurrent nnright-to-leftrecurrent nn50

chapter 13. id4

figure 13.20: id4 model, part 2: output decoder. given the context from the
input sentence, and the embedding of the previously selected word, new decoder states and word pre-
dictions are computed.

the prediction vector ti is conditioned on the decoder hidden state si   1 and, again, the

embedding of the previous output word eyi   1 and the input context ci.

ti = softmax(cid:0)w (u si   1 + v eyi   1 + cci)(cid:1)

(13.76)

note that we repeat the conditioning on eyi   1 since we use the hidden state si   1 and not s1.
this separates the encoder state progression from si   1 to si from the prediction of the output
word ti.

the softmax is used to convert the raw vector into a id203 distribution, where the sum
of all values is 1. typically, the highest value in the vector indicates the output word token yi.
its id27 eyi   1 informs the next time step of the recurrent neural network.

during training, the correct output word yi is known, so training proceeds with that word.
the training objective is to give as much id203 mass as possible to the correct output
word. the cost function that drives training is hence the negative log of the id203 given
to the correct word translation.

cost =    log ti[yi]

(13.77)

ideally, we want to give the correct word the id203 1, which would mean a negative
log id203 of 0, but typically it is a lower id203, hence a higher cost. note that the
cost function is tied to individual words, the overall sentence cost is the sum of all word costs.

during id136 on a new test sentence, we typically chose the word yi with the highest
value in ti use its embedding eyi for the next steps. but we will also explore id125 strate-
gies where the next likely words are selected as yi, creating a different conditioning context for
the next words. more on that later.

contextstateti-1tiwordpredictionyi-1eyi-1selected wordyieyiembeddingsisi-1cici-113.5. neural translation models

51

figure 13.21: id4 model, part 3: attention model. associations are computed
between the last hidden state of the decoder and the word representations (encoder states). these asso-
ciations are used to compute a weighted sum of encoder states.

attention mechanism

      
hj,

we currently have two loose ends. the decoder gave us a sequence of word representations
      
hj) and the decoder expects a context ci at each step i. we now describe the attention
hj = (
mechanism that ties these ends together.

the attention mechamism is hard to visualize using our typical neural network graphs,
but figure 13.21 gives at least an idea what the input and output relations are. the attention
      
hj) and the previous hidden
mechanism is informed by all input word representations (
state of the decoder si   1, and it produces a context state ci.

      
hj,

the motivation is that we want to compute an association between the decoder state (which
contains information where we are in the output sentence production) and each input word.
based on how strong this association is, or in other words how relevant each particular input
word is to produce the next output word, we want to weight the impact of its word represen-
tation.

mathematically, we    rst compute this association with a feedforward layer (using weight

vectors wa, ua and bias value ba)

a(si   1, hj) = wat si   1 + uat hj + ba

(13.78)

the output of this computation is a scalar value, indicating how important input word j is to
produce output word i.

we normalize this attention value, so that the attention values across all input words j add

up to one, using the softmax.

  ij =

(cid:80)

exp(a(si   1, hj))
k exp(a(si   1, hk))

(13.79)

now we use the normalized attention value to weigh the contribution of the input word

representation hj to the context vector ci and we are done.

ci =

  ijhj

(13.80)

(cid:88)

j

encoder statesattentioninput contexthidden stateoutput words52

chapter 13. id4

simply adding up word representation vectors (weighted or not) may at    rst seem an odd
and simplistic thing to do. but it is very common practice in deep learning for natural language
processing. researchers have no qualms about using sentence embeddings that are simply the
sum of id27s and other such schemes.

13.5.3 training

with the complete model in hand, we can now take a closer look at training. one challenge is
that the number of steps in the decoder and the number of steps in the encoder varies with each
training example. sentence pairs consist of sentences of different length, so we cannot have the
same computation graph for each training example but instead have to dynamically create the
computation graph for each of them. this technique is called unrolling the recurrent neural
networks, and we already discussed it with regard to language models (recall section 13.4.4).
the fully unrolled computation graph for a short sentence pair is shown in figure 13.22.
note a couple of things. the error computed from this one sentence pair is the sum of the
errors computed for each word. when proceeding to the next word prediction, we use the
correct word as conditioning context for the decoder hidden state and the word prediction.
hence, the training objective is based on the id203 mass given to the correct word, given
a perfect context. there have been some attempts to use different training objectives, such as
the id7 score, but they have not yet been shown to be superior.

practical training of id4 models requires gpus which are well suited
to the high degree of parallelism inherent in these deep learning models (just think of the many
id127s). to increase parallelism even more, we process several sentence pairs
(say, 100) at once. this implies that we increase the dimensionality of all the state tensors.

to given an example. we represent each input word in speci   c sentence pair with a vector
hj. since we already have a sequence of input words, these are lined up in a matrix. when we
process a batch of sentence pairs, we again line up these matrices into a 3-dimensional tensor.
similarly, to give another example, the decoder hidden state si is a vector for each output
word. since we process a batch of sentences, we line up their hidden states into a matrix. note
that in this case it is not helpful to line up the states for all the output words, since the states
are computed sequentially.

recall the    rst computation of the attention mechanism

a(si   1, hj) = w asi   1 + u ahj + ba

(13.81)

we can pass this computation to the gpu with a matrix of encoder states si   1 and a 3-
dimensional tensor of input encodings hj, resulting in a matrix of attention values (one dimen-
sion for the sentence pairs, one dimension for the input words). due to the massive re-use of
values in w a, u a, and ba as well as the inherent parallelism of this computation, gpus can
show their true power.

you may feel that we just created a glaring contradiction. first, we argued that we have to
process one training example at a time, since sentence pairs typically have different length, and

13.5. neural translation models

53

figure 13.22: fully unrolled computation graph for training example with 7 input tokens <s> the house
is big </s> and 6 output tokens das haus is gro  </s>. the cost function (error) is computed for each
output word individually, and summed up across the sentence. when walking through the deocder
states, the correct previous output words are used as conditioning context.

input wordembeddingsleft-to-rightrecurrent nnright-to-leftrecurrent nnattentioninput contexthidden stateoutput wordpredictionsgiven output wordserroroutput wordembedding<s>thehouseisbig.</s><s>dashausistgro  .</s>54

chapter 13. id4

   

figure 13.23: to make better use of parallelism in gpus, we process a batch of training examples (sen-
tence pairs) at a time. converting a batch of training examples into a set of mini batches that have similar
length. this wastes less computation on    ller words (light blue).

hence computation graphs have different size. then, we argued for batching, say, 100 sentence
pairs together to better exploit parallelism. these are indeed con   icting goals.

see figure 13.23. when batching training examples together, we have to consider the maxi-
mum sizes for input and output sentences in a batch and unroll the computation graph to these
maximum sizes. for shorter sentences, we    ll the remaining gaps with non-words and keep
track of where the valid data is with a mask. this means, for instance, that we have to ensure
that no attention is given to words beyond the length of the input sentence, and no errors and
gradient updates are computed from output words beyond the length of the output sentence.
to avoid wasted computations on gaps, a nice trick is to sort the sentence pairs in the batch

by length and break it up into mini-batches of similar length.7

to summarize, training consists of the following steps
    shuf   e the training corpus (to avoid undue biases due to temporal or topical order)
    break up the corpus into maxi-batches
    break up each maxi-batch into mini-batches
    process each mini-batch, gather gradients
    apply all gradients for a maxi-batch to update the parameters
typically, training id4 models takes about 5   15 epochs (passes through

entire training corpus). a common stopping criteria is to check progress of the model on a val-
idation set (that is not part of the training data) and halt when the error on the validation set
does not improve. training longer would not lead to any further improvements and may even
degrade performance due to over   tting.

13.5.4 id125

translating with neural translation models proceeds one step at a time. at each step, we predict
one output word. in our model, we    rst compute a id203 distribution over all words.

7there is a bit of confusion of the technical terms here. sometimes, the entire training corpus is called a batch, as
used in the contrast between batch updating and online updating. in that context, smaller batches with a subset of
the are called mini-batches (recall section 13.2.6 on page 22). here, we use the term batch (or maxi-batch) for such a
subset, and mini-batch for a subset of the subset.

13.5. neural translation models

55

figure 13.24: elementary decoding step: the model predicts a word prediction id203 distribution.
we select the most likely word (the). its embedding is part of the conditioning context for the next word
prediction (and decoder state).

we then pick the most likely word and move to the next prediction step. since the model is
conditioned on the previous output word (recall equation 13.75), we use its id27
in the conditioning context for the next step.

see figure 13.24 for an illustration. at each time step, we obtain a id203 distribution
over words.
in practice, this distribution is most often quite spiked, only few words     or
maybe even just one word     amass almost all of the id203. in the example, the word the
received the highest id203, so we pick it as the output word.

a real example of how a id4 model translates a german sentence
into english is shown in figure 13.25. the model tends to give most, if not almost all, proba-
bility mass to the top choice, but the sentence translation also indicates word choice ambiguity,
such as believe (68.4%) vs. think (28.6%) or different (41.5%) vs. various (22.7%). there is also
ambiguity about grammatical structure, such as if the sentence should start with the discourse
connective but (42.1%) or the subject i (20.4%).

this process suggests that we perform 1-best greedy search. this makes us vulnerable to the
so-called garden-path problem. sometimes we follow a sequence of words and realize too late
that we made a mistake early on. in that case, the best sequence consists of less probable words
initially which are redeemed by subsequent words in the context of the full output. consider
the case of having to produce an idiomatic phrase that is non-compositional. the    rst words of
these phrases may be really odd word choices by themselves (e.g., piece of cake for easy). only
once the full phrase is formed, their choice is redeemed.

note that we are faced with the same problem in traditional id151
models     arguable even more so there since we rely on sparser contexts when making pre-
dictions for the next words. decoding algorithms for these models keep a list of the n-best
candidate hypotheses, expand them and keep the n-best expanded hypotheses. we can do the
same for neural translation models.

si-1cicontextstateti-1tiwordpredictionyi-1eyi-1selected wordyieyiembeddingci-1thecatthisoffishtheredogthese  yieyisi56

chapter 13. id4

input sentence
ich glaube aber auch , er ist clever genug um seine aussagen vage genug zu halten , so dass sie auf verschiedene
art und weise interpretiert werden k  nnen .

output word predictions

best
but
i
also
believe
he
is
clever
enough
to
keep
his
statements
vague
enough
so
they
can
be
interpreted
in
different
ways
.
</s>

about (1.2%), for (1.1%), in (1.0%), of (0.3%), around (0.1%), ...

alternatives
however (25.3%), i (20.4%), yet (1.9%), and (0.8%), nor (0.8%), ...
also (6.0%), , (4.7%), it (1.2%), in (0.7%), nor (0.5%), he (0.4%), ...
think (4.2%), do (3.1%), believe (2.9%), , (0.8%), too (0.5%), ...
think (28.6%), feel (1.6%), do (0.8%), ...
that (6.7%), it (2.2%), him (0.2%), ...
   s (24.4%), has (0.3%), was (0.1%), ...
smart (0.6%), ...

(42.1%)
(80.4%)
(85.2%)
(68.4%)
(90.4%)
(74.7%)
(99.1%)
(99.9%)
(95.5%)
(69.8%) maintain (4.5%), hold (4.4%), be (4.2%), have (1.1%), make (1.0%), ...
its (2.1%), statements (1.5%), what (1.0%), out (0.6%), the (0.6%), ...
(86.2%)
testimony (1.5%), messages (0.7%), comments (0.6%), ...
(91.9%)
v@@ (1.2%), in (0.6%), ambiguous (0.3%), ...
(96.2%)
and (0.2%), ...
(98.9%)
, (44.3%), to (1.2%), in (0.6%), and (0.5%), just (0.2%), that (0.2%), ...
(51.1%)
that (35.3%), it (2.5%), can (1.6%), you (0.8%), we (0.4%), to (0.3%), ...
(55.2%)
(93.2%) may (2.7%), could (1.6%), are (0.8%), will (0.6%), might (0.5%), ...
(98.4%)
(99.1%)
(96.5%)
(41.5%)
(99.3%) way (0.2%), manner (0.2%), ...
(99.2%)
(100.0%)

have (0.3%), interpret (0.2%), get (0.2%), ...
interpre@@ (0.1%), constru@@ (0.1%), ...
on (0.9%), differently (0.5%), as (0.3%), to (0.2%), for (0.2%), by (0.1%), ...
a (25.2%), various (22.7%), several (3.6%), ways (2.4%), some (1.7%), ...

</s> (0.2%), , (0.1%), ...

figure 13.25: word predictions of the id4 model. frequently, most of the proba-
bility mass is given to the top choice, but semantically related words may rank high, e.g., believe (68.4%)
vs. think (28.6%). the subword units interpre@@ are explain in section 13.6.2 on page 61.

13.5. neural translation models

57

figure 13.26: id125 in id4. after committing to a short list of speci   c
output words (the beam), new word predictions are made for each. these differ since the committed
output word is part of the conditioning context to make predictions.

when predicting the    rst word of the output sentence, we keep a beam of the top n most
likely word choices. they are scored by their id203. then, we use each of these words
in the beam in the conditioning context for the next word. due to this conditioning, we make
different word predictions for each. we now multiply the score for the partial translation (at
this point just the id203 for the    rst word), and the probabilities from its word predictions.
we select the highest scoring word pairs for the next beam. see figure 13.26 for an illustration.

this process continues. at each time step, we accumulate word translation probabilities,
giving us scores for each hypothesis. a sentence translation is complete, when the end of
sentence token is produced. at this point, we remove the completed hypothesis from the beam
and reduce beam size by 1. search terminates, when no hypotheses are left in the beam.

search produces a graph of hypotheses, as shown in figure 13.27. it starts with the start of
sentence symbol <s> and its paths terminate with the end of sentence symbol </s>. given the
compete graph, the resulting translations can be obtained by following the back-pointers. the
complete hypothesis (i.e., one that ended with a </s> symbol) with the highest score points to
the best translation.

when choosing among the best paths, we score each with the product of its word prediction
probabilities.
in practice, we get better results when we normalize the score by the output
length of a translation, i.e., divide by the number of words. we carry out this id172
after search is completed. during search, all translations in a beam have the same length, so
the id172 would make no difference.

note that in traditional id151, we were able to combine hypothe-
ses if they share the same conditioning context for future feature functions. this not possible
anymore for recurrent neural networks since we condition on the entire output word sequence
from the beginning. as a consequence, the search graph is generally less diverse than search

thisthe  yithecatthisoffishtheredogthesethesecatcatcatsdogcats58

chapter 13. id4

figure 13.27: search graph for id125 decoding in neural translation models. at each time step,
the n = 6 best partial translations (called hypotheses) are selected. an output sentence is complete
when the end of sentence token </s> is predicted. we reduce the beam after that and terminate when
n full sentence translations are completed. following the back-pointers from the end of sentence tokens
allows us to read them off. empty boxes represent hypotheses that are not part of any complete path.

graphs in id151 models. it is really just a search tree where the number
of complete paths is the same as the size of the beam.

further readings the attention model has its roots in a sequence-to-sequence model. cho et al.
(2014) use recurrent neural networks for the approach. sutskever et al. (2014) use a lstm (long short-
term memory) network and reverse the order of the source sentence before decoding.

the seminal work by bahdanau et al. (2015) adds an alignment model (so called    attention mech-
anism") to link generated output words to source words, which includes conditioning on the hidden
state that produced the preceding target word. source words are represented by the two hidden states
of recurrent neural networks that process the source sentence left-to-right and right-to-left. luong et al.
(2015b) propose variants to the attention mechanism (which they call    global" attention model) and also
a hard-constraint attention model (   local" attention model) which is restricted to a gaussian distribution
around a speci   c input word.

to explicitly model the trade-off between source context (the input words) and target context (the
already produced target words), tu et al. (2016a) introduce an interpolation weight (called    context
gate") that scales the impact of the (a) source context state and (b) the previous hidden state and the last
word when predicting the next hidden state in the decoder.

tu et al. (2017) augment the attention model with a reconstruction step. the generated output is
translated back into the input language and the training objective is extended to not only include the
likelihood of the target sentence but also the likelihood to the reconstructed input sentence.

13.6 re   nements

the previous section gave a comprehensive description of the currently most commonly used
basic neural translation model architecture. it performs fairly well out of the box for many

<s></s></s></s></s></s></s>13.6. refinements

59

checkpoint ensemble

multi-run ensemble

figure 13.28: two methods to generate alternative systems for ensembling: checkpoint ensembling uses
model dumps from various stages of the training process, while multi-run ensembling starts indepen-
dent training runs with different initial weights and order of the training data.

language pairs. since its conception, a number of re   nements have been proposed. we will
describe them in this section.

some of the re   nements are fairly general, some target particular use cases or data condi-
tions. to given one example, the best performing system at the recent wmt 2017 evaluation
campaign used ensemble decoding (section 13.6.1), byte pair encoding to address large vo-
cabularies (section 13.6.2), added synthetic data derived from monolingual target side data
(section 13.6.3) , and used deeper models (section 13.6.4).

13.6.1 ensemble decoding

a common technique in machine learning is to not just build one system for your problem,
but multiple ones and then combine them. this is called an ensemble of systems. it is such a
successful strategy that various methods have been proposed to systematically build alterna-
tive systems, for instance by using different features or different subsets of the data. for neural
networks, one straightforward way is to use different initializations or stop at different points
in the training process.

why does it work? the intuitive argument is that each system makes different mistakes.
when two systems agree, then they are more likely both right, rather than both make the same
mistake. one can also see the general principle at play in human behavior, such as setting up
committees to make decisions or the democratic voting in elections.

applying ensemble methods to our case of id4, we have to address

two sub-problems: (1) generating alternate systems, and (2) combining their output.

generating alternative systems

see figure 13.28 for an illustration of two methods for the    rst sub-problem, generating alter-
native system. when training a neural translation model, we iterate through the training data
until some stopping criteria is met. this is typically a lack of improvements of the cost function
applied to a validation set (measured in cross-id178), or the translation performance on that
validation set (measured in id7).

during training, we dump out the model at    xed intervals (say, every 10,000 iteration of
batch processing). once training is completed, we can look back at the performance of the

60

chapter 13. id4

figure 13.29: combining predictions from a ensemble of models: each model independently predicts a
id203 distribution over output words, which are averaged into a combined distribution.

model these different stages. we then pick the, say, 4 models with the best performance (typ-
ically translation quality measured in id7). this is called checkpoint ensembling since we
select the models at different checkpoints in the training process.

multi-run ensembling requires building systems in completely different training runs. as
mentioned before, this can be accomplished by using different random initialization of weights,
which leads training to seek out different local optima. we also randomly shuf   e the training
data, so using different random order will also lead to different training outcomes.

multi-run ensembling usually works a good deal better, but it is also computationally much
more expensive. note that multi-run ensembling can also build on checkpoint ensembling.
instead of combining the end points of training, we    rst apply checkpoint ensembling to each
run, and then combine those ensembles.

combine system output

neural translation models allow the combination of several systems fairly deeply. recall that
the model    rst predicts a id203 distribution over possible output words, and then com-
mits to one of the words. this is where we combine the different trained models. each model
predicts a id203 distribution and we then combine their predictions. the combination is
done by simple averaging over the distributions. the averaged distribution is then the basis
for selecting an output word.

see figure 13.29 for an illustration. there may be some bene   t to weighing the different
systems differently, although in our way of generating them, they will all have very similar
quality, so this is not typically done.

.54the.01cat.11this.00of.00fish.03there.00dog.05these.52.02.12.00.01.03.00.09model 1model 2.12.33.06.01.15.00.05.09model 3.29.03.14.08.00.07.20.00model 4.37.10.08.02.07.03.00model average.0613.6. refinements

61

reranking with right-to-left decoding

one more tweak on the idea of ensembling: instead of building multiple systems with different
random initialization, we can also build one set of system as before, and then a second set of
system where we reverse the order of the output sentences. the second set of systems are
called right-to-left systems, although arguably this is not a good name since it makes no sense
for languages such as arabic or hebrew where the normal writing order is right to left.

the deep integration we described just above does not work anymore for the combination
of left-to-right and right-to-left systems, since they produce output in different order. so, we
have to resort to reranking. this involves several steps:

    use an ensemble of left-to-right systems to generate an n-best list of candidate transla-

tions for each input sentence.

    score each candidate translation with the individual left-to-right and right-to-left sys-

tems.

    combine the scores (simple average) of the different models for each candidate, select the

candidate with the best score for each input sentence.

scoring a given candidate translation with a right-to-left system does require the require
forced decoding, a special mode of running id136 on an input sentence, but predicting a
given output sentence. this mode is actually much closer to training (where also an output
translation is given) the regular id136.

13.6.2 large vocabularies

zipf   s law tells us that words in a language are very unevenly distribution. so, there is always a
large tail of rare words. new words come into the language all the time (e.g., retweeting, website,
woke), and we also have to deal with a very large inventory of names, including company names
(e.g., ebay, yahoo, microsoft).

on the other hand, neural methods are not well equipped to deal with such large vocabu-
laries. the ideal representations for neural networks are continuous space vectors. this is why
we    rst convert discrete objects such as words into such id27s.

however, ultimately the discrete nature of words shows up. on the input side, we need
to train an embedding matrix that maps each word into its embedding. on the output side
we predict a id203 distribution over all output words. the latter is generally the bigger
concern, since the amount of computation involved is linear with the size of the vocabulary,
making this a very large matrix operation.

hence, neural translation models typically restrict the vocabulary to, say, 20,000 to 80,000
words. in initial work on id4, only the most frequent words were used,
and all others represented by a unknown or other tag. the translation of these rare words was
handled with a back-off dictionary.

62

chapter 13. id4

obama receives net@@ any@@ ahu
the relationship between obama and net@@ any@@ ahu is not exactly friendly . the two wanted
to talk about the implementation of the international agreement and about teheran    s destabil@@
ising activities in the middle east . the meeting was also planned to cover the con   ict with the
palestinians and the disputed two state solution . relations between obama and net@@ any@@
ahu have been stra@@ ined for years . washington critic@@ ises the continuous building of
settlements in israel and acc@@ uses net@@ any@@ ahu of a lack of initiative in the peace
process . the relationship between the two has further deteriorated because of the deal that obama
negotiated on iran    s atomic programme . in march , at the invitation of the republic@@ ans
, net@@ any@@ ahu made a controversial speech to the us congress , which was partly seen
as an aff@@ ront to obama . the speech had not been agreed with obama , who had rejected a
meeting with reference to the election that was at that time im@@ pending in israel .

figure 13.30: byte pair encoding (bpe) applied to english (model used 49,500 bpe operations). word
splits are indicated with @@. note that the data is also tokenized and true-cased.

the more common approach today is to break up rare words into subword units. this
may seem a bit crude but is actually very similar to standard approaches in statistical machine
translation to handle compounds (recall website     web + site) and morphology (unfollow    
un + follow, convolutions     convolution + s).
it is even a decent approach to the problem of
id68 of names which are traditionally handled by a sub-modular letter translation
component.

a popular method to create an inventory of subword units and legitimate words is byte
pair encoding. this method is trained on the parallel corpus. first, the words in the corpus
are split into characters (marking original spaces with a special space character). then, the
most frequent pair of characters is merged (in english, this may be t and h into th). this step
is repeated for a    xed given number of times. each of these steps increases the vocabulary by
one, beyond the original inventory of single characters.

the example mirrors quite well the behavior of the algorithm on real-world data sets. it
starts with grouping together with frequent letter combinations (e+r, t+h, c+h) and then joins
frequent words (the, in, of). at the end of this process, the most frequent words will emerge
as single tokens, while rare words consist of still un-merged subwords. see figure 13.30 for
an example, where subword units are indicated with two    at" symbols (@@). after 49,500 byte
pair encoding operations, the vast majority of words are intact, while rarer words are broken up
(e.g., critic@@ ises, destabil@@ ising). sometimes, the split seem to be morphologically motivated
(e.g., im@@ pending), but mostly they are not (e.g., stra@@ ined). note also the decomposition of
the relatively rare name net@@ any@@ ahu.

further readings a signi   cant limitation of id4 models is the computa-
tional burden to support very large vocabularies. to avoid this, typically the vocabulary is reduced

13.6. refinements

63

to a shortlist of, say, 20,000 words, and the remaining tokens are replaced with the unknown word to-
ken    unk". to translate such an unknown word, luong et al. (2015c); jean et al. (2015a) resort to a
separate dictionary. arthur et al. (2016) argue that neural translation models are worse for rare words
and interpolate a traditional probabilistic bilingual dictionary with the prediction of the neural machine
translation model. they use the attention mechanism to link each target word to a distribution of source
words and weigh the word translations accordingly.

source words such as names and numbers may also be directly copied into the target. gulcehre et al.
(2016) use a so-called switching network to predict either a traditional translation operation or a copying
operation aided by a softmax layer over the source sentence. they preprocess the training data to change
some target words into word positions of copied source words. similarly, gu et al. (2016) augment the
word prediction step of the neural translation model to either translate a word or copy a source word.
they observe that the attention mechanism is mostly driven by semantics and the language model in
the case of word translation, but by location in case of copying.

to speed up training, mi et al. (2016) use traditional id151 word and phrase

translation models to    lter the target vocabulary for mini batches.

sennrich et al. (2016d) split up all words to sub-word units, using character id165 models and a

segmentation based on the byte pair encoding compression algorithm.

13.6.3 using monolingual data

a key feature of id151 system are language models, trained on very
large monolingual data set. the larger the language models, the higher translation quality.
language models trained on up to a trillion words crawled from the general web have been
used. so, it is a surprise that the basic neural translation model does not use any additional
monolingual data, its language model aspect (the conditioning of the previous hidden decoder
state and the previous output) is trained jointly with the translation model aspect (the condi-
tioning on the input context).

two main ideas have been proposed to improve neural translation models with monolin-
gual data. one is to transform additional monolingual translation into parallel data by syn-
thesizing the missing half of the data, and the other is to integrate a language model as a
component into the neural network architecture.

back translation

language models improve    uency of the output. using larger amounts of monolingual data
in the target language give the machine more evidence what are common sequences of words
and what are not.

we cannot use monolingual target side data in our neural translation model training, since
it is missing the source side. so, one idea is to just synthesize this data by back translation. see
figure 13.31 for an illustration of the steps involved.

    train a reverse system that translates from the intended target language into the source
language. we typically use the same id4 setup for this as for
our    nal system, just with source and target    ipped. but we may use any system, even
traditional phrase-based systems.

64

chapter 13. id4

figure 13.31: creating synthetic parallel data from target-side monolingual data: (1) train a system in
reverse order, (2) use it to translate target-side monolingual data into the source language, (3) combine
the generated synthetic parallel data with the true parallel data in    nal system building.

    use the reverse system to translate target side monolingual data, creating a synthetic

parallel corpus.

    combine the generated synthetic parallel data with the true parallel data when building

the    nal system.

there is an open question on how much synthetic parallel data should be used in relation
to the amount of existing true parallel data. typically, there are magnitudes more monolingual
data available, but we also do not want to drown out the actual real data. successful applica-
tions of this idea used equal amounts of synthetic and true data. we may also generate much
more synthetic parallel data, but then ensure during training that we process equal amounts of
each by over-sampling the true parallel data.

adding a language model

the other idea is to train a language model as a separate component of the neural translation
model. g  l  ehre et al. (2015)    rst train the large language model as a recurrent neural net-
work on all available data, including the target side of the parallel corpus. then, they add this
language model to the neural translation model. since both language model and translation
model predict output words, the natural point to connect the two models is joining them at that
output prediction node in the network by concatenating their conditioning contexts.

we expand equation 13.75 to add the hidden state of the neural language model slm

to the
, the source context ci and the previous english

i

hidden state of the neural translation model stm
word ei   1.

i

ei = g(ci, stm

i

, slm

i

, ei   1)

(13.82)

when training the combined model, we leave the parameters of the large neural language
model unchanged, and update only the parameters of the translation model and the combina-
tion layer. the concern is that otherwise the output side of the parallel corpus would overwrite

reverse system   nal system13.6. refinements

65

figure 13.32: round trip training: in addition to training two models f   e and e   f, as done traditionally
on parallel data, we also optimize both models to convert a sentence f into e and then restore it back
into f, using on monolingual data in f. we may add a corresponding round trip starting with e.

the memory of the large monolingual corpus. in other words, the language model would over-
   t to the parallel training data and be less general.

one    nal question remains: how much weight should be given to the translation model
and how much weight should be given to the language model? the above equation considers
them in all instances the same way. but there may be output words for which the translation
model is more relevant (e.g., the translation of content words with distinct meaning) and output
words where the language model is more relevant (e.g., the introduction of relevant function
words for    uency).

the balance of the translation model and the language model can be achieved with the type
of gated units that we encountered in our discussion of the long short-term memory neural
network architecture (section 13.4.5). such a gated unit may be predicted solely from the lan-
guage model state slm
and then used as a factor that is multiplied with that language model
state before it is used in the prediction of equation 13.82.

i

gatelm
i = f (slm
i )
i    slm
i = gatelm
  slm
,   slm
ei = g(ci, stm

i

i

i

, ei   1)

(13.83)

round trip training

looking at the backtranslation idea from a strict machine learning perspective, we can see two
learning objectives. there is the objective to learn the transformations given by the parallel
data, as done traditionally. then, there is the goal to learn how to convert an output lamguage
sentence into the input language, and then back into the output language with the objective to
match the traditional sentence. a good machine translation model should be able to preserve
the meaning of the output language sentence when mapped into the input language and back.
see figure 13.32 for an illustration. there are two machine translation models. one that
translates sentences in the language direction f   e, the other in the opposite direction e   f.
these two systems may be trained with traditional means, using a parallel corpus. we can
also round trip a sentence f    rst through the f   e and then back through e   f

in this scenario, there are two objectives for model training.

mtf   emtf   eef66

chapter 13. id4

    the translation e    of the given monolingual sentence f should be a valid sentence in the

language e, as measured with a language model lme(e   ).

    the reconstruction of the translation e    back into the original language f should be easy,

as measured with the translation model mte   f (f|e   )

these two objectives can be used to update model parameters in both translation models
mtf   e and mte   f .

typical model update is driven by correct predictions of each word.

in this round-trip
scenario, the translation e    has to be computed    rst, before we can do the usual training of
model mte   f with the given sentence pair (e   ,f). to make better use of the training data, a
n-best list of translations e   1, ..., e   n is computed and model updates are computed for each of
them.

we can also update the model mtf   e with monolingual data in language f by scaling up-
dates by the language model cost lme(e   i) and the forward translation cost mtf   e(e   i|f) for
each of the translations e   i in the n-best list.

to use monolingual data in language e, training is done in the reverse round trip direction.

for details of this idea, refer to xia et al. (2016).

further readings sennrich et al. (2016c) back-translate the monolingual data into the input lan-
guage and use the obtained synthetic parallel corpus as additional training data. xia et al. (2016) use
monolingual data in a dual learning setup. machine translation engines are trained in both directions,
and in addition to regular model training from parallel data, monolingual data is translated in a round
trip (e to f to e) and evaluated with a language model for language f and reconstruction match back to e
as cost function to drive id119 updates to the model.

13.6.4 deep models

learning the lessons from other research    elds such as vision or id103, recent
work in machine translation has also looked at deeper models. simply put, this involves
adding more intermediate layers into the baseline architecture.

the core components of id4 are the encoder that takes input words
and converts them into a sequence of contextualized representations and the decoder that gen-
erates a output sequence of words. both are recurrent neural networks.

recall that we already discussed how to build deeper recurrent neural networks for lan-
guage modelling (refer back to section 13.4.7 on page 44). we now extend these ideas to the
recurrent neural networks in the encoder and the decoder.

what all these recurrent neural networks have in common is that they process an input
sequence into an output sequence, and at each time step t information from a new input xt is
combined with the hidden state from the previous time step ht   1 to predict a new hidden state
ht. from that hidden state additional predictions may be made (output words yt in the case of
the decoder, the next word in the sequence in the case of language models), or the hidden state
is used otherwise (via the attention mechanism in case of the encoder).

13.6. refinements

67

figure 13.33: deep decoder: instead of a single recurrent neural network (id56) layer for the decoder
state, in a deep model, it consists of several layers. the illustrations shows a combination of a deep
transition and stacked id56s. it omits the word prediction, word selection and output id27
steps which are identical to the original architecture, shown in figure 13.20 on page 50.

decoder see figure 13.33 for part of the decoder in id4, using a par-
ticular deeper architecture. we see that instead of a single hidden state ht for a given time step
t, we now have a sequence of hidden states ht,1, ht,2, ..., ht,i for a given time step t.

there are various options how the hidden states may be connected. previously, in sec-
tion 13.4.7 we presented two ideas. (1) in stacked recurrent neural networks where a hidden
state ht,i is conditioned on the hidden state from a previous layer ht,i   1 and the hidden state
at the same depth from a previous time step ht   1,i. (2) in deep transition recurrent neural net-
works, the    rst hidden state ht,1 is conditioned on the last hidden state from the previous time
step ht   1,i and the input, while the other hidden layers ht,i (i > 1) are just conditioned on the
previous previous layer ht,i   1.

figure 13.33 combines these two ideas. some layers are both stacked (conditioned on the
previous time step ht   1,i and previous layer ht,i   1), while others are deep transitions (condi-
tioned only on the previous layer ht,i   1.

mathematically, we can break this out into the stacked layers ht,i:

ht,1 = f1(xt, ht   1,1)
ht,i = fi(ht,i   1, ht   1,i)

for i > 1

and the deep transition layers vi,i,j.

vt,i,1 = gi,1(int,i, ht   1,i)
vt,i,j = gi,j(vt,i,j   1)
ht,i = vt,i,j

int,i is either xt or ht,i   1
for j > 1

(13.84)

(13.85)

contextdecoder state: stack 1, transition 1decoder state: stack 1, transition 2decoder state: stack 2, transition 1decoder state: stack 2, transition 268

chapter 13. id4

figure 13.34: deep alternating encoder: combination of the idea of a bidirectional recurrent neural
network previously proposed for id4 (recall figure 13.19 on page 49) and the
stacked recurrent neural network (recall figure 13.19 on page 49). this architecture may be further
extended with the idea of deep transitions, as shown for the decoder (previous figure 13.33).

the function fi(ht,i   1, ht   1,i) is computed as a sequence of function calls gi,j. each of the
functions gi,j may be implemented as feed-forward neural network layer (id127
plus activation function), long-short term memory cell (lstm), or gated recurrent unit (gru).
on either case, each function gi,j has its own set of trainable model parameters.

encoder deep recurrent neural networks for the encoder may draw in the same ideas as the
decoder, with one addition: in the baseline neural translation model, we used bidirectional
recurrent neural networks to condition on both left and right context. we want to do the same
for any deep version of the encoder.

figure 13.34 shows one idea how this could be done, called alternating recurrent neural
network. it looks basically like a stacked recurrent neural network, with one twist: the hidden
states at each layer ht,i are alternately conditioned on the hidden state from the previous time
step ht   1,i or the next time step ht+1,i.

mathematically, we formulate this as even numbered hidden states ht,2i being conditioned
on the left context ht   1,2i and odd numbered hidden states ht,2i+1 conditioned on the right
context ht+1,2i.

ht,1 = f (xt, ht   1,1)
ht,2i = f (ht,2i   1, ht   1,2i)
ht,2i+1 = f (ht,2i, ht+1,2i+1)

(13.86)

as before in the encoder, we can extend this idea by having deep transitions.
note that deep models are typically augmented with direct connections from the input to
the output. in the case of the encoder, this may mean a direct connection from the embedding
to the    nal encoder layer, or connections at each layer that pass the input directly to the output.

input id27encoder layer 1: l2rencoder layer 2: r2lencoder layer 3: l2rencoder layer 4: r2l13.6. refinements

69

s
n
o
i
t
a
l
e
r

n
e
e
w
t
e
b

56
89

a
m
a
b
o

16

72

26
96

die
beziehungen
zwischen
obama
und
netanjahu
sind
seit
jahren
angespannt
.

u
h
a
y
n
a
t
e

n

d
n
a

d
e
n

i
a
r
t
s

e
v
a
h

n
e
e
b

s
r
a
e
y

r
o
f

.

79

98

54 10
98

42

11

11

14

38
22

84
23

49

figure 13.35: alignment vs. attention: in this example, alignment points from traditional word align-
ment methods are shown as squares, and attention states as shaded boxes depending on the alignment
value (shown as percentage). they generally match up well, but note for instance that the prediction of
the output auxiliary verb sind pays attention to the entire verb group have been strained.

such residual connections help with training. in early stages, the deep architecture can be
skipped. only when a basic functioning model has been acquired, the deep architecture can
be exploited to enrich it. we typically see the bene   ts of residual connections in early training
stages (faster initial reduction of model perplexity), and less so as improvement in the    nal
converged model.

further readings recent work has shown good results with 4 stacks and 2 deep transitions each
for encoder and decoder, as well as alternating networks for the encoder (miceli barone et al., 2017).
there are a large number of variations (including the use of skip connections, the choice of lstm vs.
gru, number of layers of any type) that still need to be explored empirical for various data conditions.

13.6.5 guided alignment training

the attention mechanism in id4 models is motivated by the need to
align output words to input words. figure 13.35 shows an example of attention weights given
to english input words for each german output word during the translation of a sentence.

the attention values typically match up pretty well with word alignment used in tradi-
tional id151, obtained with tools such as giza++ or fast-align which
implement variants of the ibm models.

there are several good uses for word alignments beyond their intrinsic value of improving
the quality of translations. for instance in the next section, we will look at using the attention

70

chapter 13. id4

mechanism to explicitly track coverage of the input. we may also want to override preferences
of the id4 model with pre-speci   ed translations of certain terminology
or expressions such as numbers, dates, or measurements that are better handled by rule-based
components; this requires to know when the neural model is about to translate a speci   c source
word. but also the end user may be interested in alignment information, such as translators
using machine translation in a computer aided translation tool may want to check where an
output word originates from.

hence, instead of trusting the attention mechanism to implicitly acquire the role as word
alignmer, we may enforce this role. the idea is to provide not just the parallel corpus as train-
ing data, but also pre-computed word alignments using traditional means. such additional
information may even bene   t training of models to converge faster or overcome data sparsity
under low resource conditions.

a straightforward way to add such given word alignment to the training process is to not
change the model at all, but to just modify the training objective. typically, the goal of training
id4 models is to generate the correct output words. we can add to this
goal to also match the given word alignment.

aij input words j and output words i in a way that (cid:80)
each output word:(cid:80)

formally, we assume to have access to an alignment matrix a that speci   es alignment points
j aij = 1, i.e., each output word   s
alignment scores add up to 1. the model estimates attention scores   ij that also add up to 1 for
j   ij = 1 (recall equation 13.79 on page 51). the mismatch between given
alignment scores aij and computed attention scores   ij can be measured in several ways, such
as cross id178

or mean squared error

costce =     1
i

costmse =     1
i

aij log   ij

(aij       ij)2

(13.87)

(13.88)

i(cid:88)
i(cid:88)

i=1

j(cid:88)
j(cid:88)

j=1

i=1

j=1

this cost is added to the training objective and may be weighted.

further readings chen et al. (2016b); liu et al. (2016) add supervised word alignment information
(obtained with traditional statistical word alignment methods) to training. they augment the objective
function to also optimize matching of the attention mechanism to the given alignments.

13.6.6 modeling coverage

one impressive aspect of id4 models is how well they are able to trans-
late the entire input sentence, even when a lot of reordering is involved. but this as aspect is
not perfect, occasionally the model translates some input words multiple times, and sometimes
it misses to translate them.

13.6. refinements

71

m

g
n

r
e
d
r
o

n

i

o
t

e
v
l
o
s

e
h
t

e
l
b
o
r
p

e
h
, t

l
a
i
c
o
"s

i
s
u
o
h

e
c
n
a
i
l
l
"a

s
t
s
e
g
g
u
s

h
s
e
r
af

t
r
a
t
s

.

um
das
problem
zu
l  sen
,
schl  gt
das
unternehmen
der
gesellschaft
f  r
soziale
bildung
vor
.

37

33

13

84
1080

63
81

12

40

7118
86
84
80
45
40

12
10

30
80

10

41
4410
89

10

40
37

43

7

46 161 108 89

62 112 392 121 110 130 26 132 22

6

6

1113
19

figure 13.36: example for over-generation and under-generation: the input tokens around social housing
are attended too much, leading to hallucinated output words (das unternehmen, english: the company),
while the end of the sentence a fresh start is not attended and untranslated.

see figure 13.36 for an example. the translation has two    aws related to mis-allocation of
attention. the beginning of the phrase    social housing" alliance receives too much attention,
resulting in a faulty translation with hallucinated words: das unternehmen der gesellschaft f  r
soziale bildung, or the company of the society for social education. at the end of the input sentence,
the phrase a fresh start does not receive any attention and is hence untranslated in the output.

hence, an obvious idea is to more strictly model coverage. given the attention model, a
reasonable way to de   ne coverage is by adding up the attention states. in a complete sentence
translation, we roughly expect that each input word receives a similar amount of attention. if
some input words never receive attention or too much attention that signals a problem with
the translation.

enforcing coverage during id136 we may restrict the enforcing of proper coverage to
the decoder. when considering multiple hypothesis in id125, then we should discour-
age the ones that pay too much attention to some input words. and, once hypotheses are
completed, we can penalize those that paid only little attention to some of the input. there are
various ways to come up with scoring functions for over-generation and under-generation.

72

chapter 13. id4

coverage(j) =

(cid:88)

i

(cid:88)
(cid:16)
(cid:16)

k

  i,k

(cid:88)
(cid:88)

j

(cid:17)

(cid:17)

over-generation = max

0,

coverage(j)     1

(13.89)

under-generation = min

1,

coverage(j)

j

the use of multiple scoring functions in the decoder is common practice in traditional statis-
tical machine translation. for now, it is not in id4. a challenge is to give
proper weight to the different scoring functions. if there are only two or three weights, these
can be optimized with grid search over possible values. for more weights, we may borrow
methods such as mert or mira from id151.

coverage models the vector that accumulates coverage of input words may be directly used
to inform the attention model. previously, the attention given to a speci   c input word j was
conditioned on the previous state of the decoder si   1 and the representation of the input word
hj. now, we also add as conditioning context the accumulated attention given to the word
(compare to equation 13.78 on page 51).

a(si   1, hj) = w asi   1 + u ahj + v acoverage(j) + ba

(13.90)

coverage tracking may also integrated into the training objective. taking a page from the
guided alignment training (recall the previous section 13.6.5), we augment the training objec-
tive function with a coverage penalty with some weight   .

(cid:88)

(cid:88)

log

p (yi|x) +   

(1     coverage(j))2

(13.91)

i

j

note that in general, it is problematic to add such additional functions to the learning ob-

jective, since it does distract from the main goal of producing good translations.

fertility so far, we described coverage as the need to cover all input words roughly evenly.
however, even the earliest id151 models considered the fertility of
words, i.e., the number of output words that are generated from each input word. consider
the english do not construction: most other language do not require an equivalent of do when
negating a verb. meanwhile, other words are translated into multiple output words. for in-
stance, the german nat  rlich may be translated as of course, thus generating 2 output words.

we may augment models of coverage by adding a fertility components that predicts the
number of output words for each input words. here one example for a model that predicts the
fertility   j for each input word, and uses it to normalize the coverage statistics.

  j = n   (wjhj)

(cid:88)

(cid:88)

i

k

coverage(j) =

1
  j

  i,k

(13.92)

13.6. refinements

73

fertility   j is predicted with a neural network layer that is conditioned on the input word
representation hj and uses a sigmoid activation function (thus resulting in values from 0 to 1),
which is scaled to a pre-de   ned maximum fertility of n.

feature engineering versus machine learning the work on modeling coverage in neural
machine translation models is a nice example to contrast between the engineering approach
and the belief in generic machine learning techniques. from an engineering perspective, a
good way to improve a system is to analyze its performance,    nd weak points and consider
changes to overcome them. here, we notice over-generation and under-generation with respect
to the input, and add components to the model to overcome this problem. on the other hand,
proper coverage is one of the features of a good translation that machine learning should be
able to get from the training data. if it is not able to do that, it may need deeper models, more
robust estimation techniques, ways to    ght over-   tting or under-   tting, or other adjustments
to give it just the right amount of power needed for the problem.

it is hard to carry out the analysis needed to make generic machine learning adjustments,
given the complexity of a task like machine translation. still, the argument for deep learning is
that it does not require feature engineering, such as adding coverage models. it remains to be
seen how id4 evolves over the next years, and if it moves more into a
engineering or machine learning direction.

further readings to better model coverage, tu et al. (2016b) add coverage states for each input
word by either (a) summing up attention values, scaled by a fertility value predicted from the input
word in context, or (b) learning a coverage update function as a feed-forward neural network layer.
this coverage state is added as additional conditioning context for the prediction of the attention state.
feng et al. (2016) condition the prediction of the attention state also on the previous context state and also
introduce a coverage state (initialized with the sum of input source embeddings) that aims to subtract
covered words at each step. similarly, meng et al. (2016) separate hidden states that keep track of source
coverage and hidden states that keep track of produced output. cohn et al. (2016) add a number of
biases to model coverage, fertility, and alignment inspired by traditional id151
models. they condition the prediction of the attention state on absolute word positions, the attention
state of the previous output word in a limited window, and coverage (added attention state values) over
a limited window. they also add a fertility model and add coverage in the training objective.

13.6.7 adaptation

text may differ by style, topic, degree of formality, and so on. a common problem in the
practical development of machine translation systems is that most of the available training data
is different from the data relevant to a chosen use case. for instance, if your goal is to translate
chat room dialogs, you will realize that there is very little translated chat room data available.
there are massive quantities of of   cial publications from international organizations, random
translations crawled from the web, and maybe somewhat relevant movie subtitle translations.

74

chapter 13. id4

figure 13.37: online training of id4 models allows a straightforward domain
adaptation method: having a general domain translation system trained on general-purpose data, a
handful of additional training epochs on in-domain data allows for a domain-adapted system.

this problem is generally framed as a problem of id20. in the simplest form,
you have one set of data relevant to your use case     the in-domain data     and another set
that is less relevant     the out-of-domain data.

in traditional id151, a vast number of methods for domain adap-
tation have been proposed. models may be interpolated, we may back-off from in-domain
to out-of-domain models, we may over-sample in-domain data during training or sub-sample
out-of-domain data, etc.

for id4, a fairly straightforward method is currently the most pop-
ular (see figure 13.37). this method divides training up into two stages. first, we train the
model on all available data until convergence. then, we run a few more iterations of training
on the in-domain data only and stop training when performance on the in-domain validate set
peaks. this way, the    nal model bene   ts from all the training data, but is still specialized to the
in-domain data.

practical experience with this method shows that the second in-domain training stage may
converge very quickly. the amount if in-domain data is typically relatively small, and only a
handful of training epochs are needed.

another, less commonly used method draws on the idea of ensemble decoding (section 13.6.1).

if we train separate models on different sets of data, we may combine their predictions, just as
we did for ensemble decoding. in this case, we do want to choose weights for each model,
although how to choose these weights is not a trivial task. if there is just an in-domain and
out-of-domain model, however, this may be simply done by line search over possible values.

let us now look at a few special cases that arise in practical use.

subsample in-domain data from large collections a common problem is that the amount
of available in-domain data is very small, so just training on this data, even in a secondary
adaptation stage, risks over   tting     very good performance on the seen data but poor perfor-
mance on everything else.

adapted systemgeneral systemgeneral training datain-domain training datainitial trainingadaptation13.6. refinements

75

large random collections of parallel text often contain data that closely matches the in-
domain data. so, we may want to extract this in-domain data from the large collections of
mainly out-of-domain data. the general idea behind a variety of methods is to build two
detectors: one in-domain detector trained on in-domain data, and one out-of-domain detector
trained on out-of-domain data. we then score each sentence pair in the out-of-domain data
with both detectors and select sentence pairs that are preferred (or judged relatively relevant)
by the in-domain detector.

the classic detectors are language models trained on the source and target side of the in-
domain and out-of-domain data, resulting in a total of 4 language models: the source side
f , the target side in-domain model lmin
in-domain model lmin
e , the source side out-of-domain
model lmout
, and the target side out-of-domain model lmout
. any given sentence pair from
e
the out-of-domain data is then scored based on these models:

f

(cid:16)

(cid:17)

(cid:16)

relevancee,f =

lmin

e (e)     lmout

e (e)

+

lmin

f (f )     lmout

f (f )

(13.93)

(cid:17)

we may use traditional id165 language models or neural recurrent language models.
some work suggests to replace open class words (nouns, verbs, adjectives, adverbs) with
part-of-speech tags or word clusters. more sophisticated models not only consider domain-
relevance but noisiness of the training data (e.g., misaligned or mistranslated data). we may
even use in-domain and out-of-domain neural translation models to score sentence pairs in-
stead of source and target side sentences in isolation.

the subsampled data may be used in several ways. we may only train on this data to build

our domain-speci   c system. or, we use it in a secondary adaptation stage as outlined above.

only monolingual in-domain data what if we have no parallel data in the domain of our
use case? two main ideas have been explored. firstly, we may still use the monolingual data,
may it be in the source or target language or both, for subsampling parallel data from a large
pile of general data, as outline above.

another idea is to use existing parallel data to train an out-of-domain model, then back-
translate out-of-domain data (recall section 13.6.3) to generate a synthetic in-domain corpus,
and then use this data to adapt the initial model. in traditional id151,
much adaptation success has been achieved with just interpolating the language model, and
this idea is the neural translation equivalent to that.

multiple domains sometimes, we have multiple collections of data that are clearly identi   ed
by domain     typically categories such as information technology, medical, law, etc. we can
use the techniques described above to build specialized translation models for each of these
domains.

for a given test sentence, we then select the appropriate model. if we do not know the
domain of the test sentence, we    rst have to build a classi   er that allows us to automatically

76

chapter 13. id4

make this determination. the classi   er may be based on the methods for domain detectors
described above. given the decision of the classi   er, we then select the most appropriate model.

but we do not have to commit to a single domain. the classi   er may instead provide a
distribution of relevance of the speci   c domain models (say, 50% domain a, 30% domain b,
20% domain c) which are then used as weights in an ensemble of domain-speci   c models.

the domain classi   cation may done based on a whole document instead of each individual

sentence, which brings in more context to make a more robust decision.

as a    nal remark, it is hard to give conclusive advice on how to handle adaptation chal-
lenges, since it is such a broad topic. the style of the text may be more relevant than its content.
data may differ narrowly (e.g., of   cial publications from the united nations vs. of   cial an-
nouncements from the european union) or dramatically (e.g., chat room dialogs vs. published
laws). the amounts of in-domain and out-of-domain data differs. the data may be cleanly
separated by domain or just come in a massive disorganized pile. some of the data may be of
higher translation quality than other, which may be polluted by noise such as mistranslations,
misalignments, or even generated by some other machine translation system.

further readings there is often a domain mismatch between the bulk (or even all) of the training
data for a translation and its test data during deployment. there is rich literature in traditional statistical
machine translation on this topic. a common approach for neural models is to    rst train on all available
training data, and then run a few iterations on in-domain data only (luong and manning, 2015), as
already pioneered in neural language model adaption (ter-sarkisov et al., 2015). servan et al. (2016)
demonstrate the effectiveness of this adaptation method with small in-domain sets consisting of as little
as 500 sentence pairs.

chu et al. (2017) argue that given small amount of in-domain data leads to over   tting and suggest
to mix in-domain and out-of-domain data during adaption. freitag and al-onaizan (2016) identify
the same problem and suggest to use an ensemble of baseline models and adapted models to avoid
over   tting. peris et al. (2017) consider alternative training methods for the adaptation phase but do
not    nd consistently better results than the traditional id119 training. inspired by domain
adaptation work in id151 on sub-sampling and sentence weighting, chen et al.
(2017) build an in-domain vs. out-of-domain classi   er for sentence pairs in the training data, and then
use its prediction score to reduce the learning rate for sentence pairs that are out of domain.

farajian et al. (2017) show that traditional id151 outperforms neural ma-
chine translation when training general-purpose machine translation systems on a collection data, and
then tested on niche domains. the adaptation technique allows id4 to catch up.

a multi-domain model may be trained and informed at run-time about the domain of the input
sentence. kobus et al. (2016) apply an idea initially proposed by sennrich et al. (2016a) - to augment
input sentences for register with a politeness feature token - to the id20 problem. they
add a domain token to each training and test sentence. chen et al. (2016b) report better results over
the token approach to adapt to topics by encoding the given topic membership of each sentence as an
additional input vector to the conditioning context of word prediction layer.

13.6. refinements

77

13.6.8 adding linguistic annotation

one of the big debates in machine translation research is the question if the key to progress is to
develop better, relatively generic, machine learning methods that implicitly learn the important
features of language, or to use linguistic insight to augment data and models.

recent work in id151 has demonstrated the bene   ts of linguisti-
cally motivated models. the best id151 systems in major evaluation
campaigns for language pairs such as chinese   english and german   english are syntax-based.
while they translate sentences, they also build up the syntactic structure of the output sentence.
there have been serious efforts to move towards deeper semantics in machine translation.

the turn towards id4 was at    rst hard swing back towards better
machine learning while ignoring much linguistic insights. id4 views
translation as a generic sequence to sequence task, which just happens to involve sequences of
words in different languages. methods such as byte pair encoding or character-based transla-
tion models even put the value of the concept of a word as a basic unit into doubt.

however, recently there have been also attempts to add linguistic annotation into neural
translation models, and steps towards more linguistically motivated models. we will take a
look at successful efforts to integrate (1) linguistic annotation to the input sentence, (2) linguistic
annotation to the output sentence, and (3) build linguistically structured models.

linguistic annotation of the input one of the great bene   ts of neural networks is their abil-
ity to cope with rich context. in the id4 models we presented, each
word prediction is conditioned on the entire input sentence and all previously generated out-
put words. even if, as it is typically the case, a speci   c input sequence and partially generated
output sequence has never been observed before during training, the neural model is able to
generalize the training data and draw from relevant knowledge. in traditional statistical mod-
els, this required carefully chosen independence assumptions and back-off schemes.

so, adding more information to the conditioning context in neural translation models can
be accommodated rather straightforwardly. first, what information would be like to add? the
typical linguistic treasure chest contains part-of-speech tags, lemmas, morphological properties
of words, syntactic phrase structure, syntactic dependencies, and maybe even some semantic
annotation.

all of these can be formatted as annotations to individual input words. sometimes, this
requires a bit more work, such as syntactic and semantic annotation that spans multiple words.
see figure 13.38 for an example. to just walk through the linguistic annotation of the word girl
in the sentence:

    part of speech is nn, a noun.
    lemma is girl, the same as the surface form. the lemma differs for watched / watch.
    morphology is singular.

78

chapter 13. id4

words
part of speech
lemma
morphology
noun phrase
verb phrase
synt. dependency
depend. relation
semantic role
semantic type

the
det
the
-

begin

other

girl
det
-
-

girl
nn
girl
sing.
cont

other
watched

subj

actor

watched

attentively

adv
watch
past

other

begin

-
-
-

vfin

attentive

-

other

cont
watched

adv

manner

-

the
det
the
-

begin

cont
   re   ies

det
-
-

beautiful

jj

beautiful
plural

cont

cont
   re   ies

adj

mod

-

   re   ies
nns
   re   y

cont

cont
watched

obj

patient

animate

human

view

figure 13.38: linguistic annotation of a sentence, formatted as word-level factored representation

    the word is the continuation (cont) of the noun phrase that started with the.
    the word is not part of a verb phrase (other).
    its syntactic head is watched.
    the dependency relationship to the head is subject (subj).
    its semantic role is actor.
    there are many schemes of semantic types. for instance girl could be classi   ed as hu-

man.

note how phrasal annotations are handled. the    rst noun phrase is the girl. it is common
to use an annotation scheme that tags individual words in a phrasal annation as begin and
continuation (or intermediate), while labelling words outside such phrases as other.

how do we encode the word-level factored representation? recall that words are initially
represented as 1-hot vectors. we can encode each factor in the factored representation as a 1-
hot vector. the concatenation of these vectors is then used as input to the id27.
note that mathematically this means, that each factor of the representation is mapped to a
embedding vector, and the    nal id27 is the sum of the factor embeddings.

since the input to the id4 system is still a sequence of word embed-
dings, we do not have to change anything in the architecture of the id4
model. we just provide richer input representations and hope that the model is able to learn
how to take advantage of it.

coming back to the debate about linguistics versus machine learning. all the linguistic an-
notation proposed here can arguable be learned automatically as part of the id27s
(or contextualized id27s in the hidden encoder states). this may or may not be
true. but it does provide additional knowledge that comes from the tools that produce the an-
notation and that is particularly relevant if there is not enough training data to automatically
induce it. also, why make the job harder for the machine learning algorithm than needed? in
other words, why force the machine learning to discover features that can be readily provided?

13.6. refinements

79

sentence
syntax tree

the girl watched attentively the beautiful    re   ies

s

np

vp

det
the

nn
girl

vfin
watched

advp

adv

attentively

np

jj

beautiful

det
the

nns
   re   ies

linearized

(s (np (det the ) (nn girl ) ) (vp (vfin watched ) (advp (adv attentively
) ) (np (det the ) (jj beautiful ) (nns    re   ies ) ) ) )

figure 13.39: linearization of phrase structure grammar tree into a sequence of words     e.g., girl,
watched     and tags     e.g., (s, (np, )

ultimately, these questions will be resolved empirically by demonstrating what actually works
in speci   c data conditions.

linguistic annotation of the output what we have done for input words could be done also
for output words. instead of discussing the    ne points about what adjustments need to made
(e.g., separate softmax for each output factor), let us take a look at another annotation scheme
for the output that has been successfully applied to id4.

most syntax-based id151 models have focused on adding syntax
to the output side. traditional id165 language models are good at promoting    uency among
neighboring words, they are not powerful enough to ensure overall grammaticality of each
output sentence. by designing models that also produce and evaluate the syntactic parse struc-
ture for each output sentence, syntax-based models give the means to promote grammatically
correct output.

the word-level annotation of phrase structure syntax suggested in figure 13.38 is rather
crude. the nature of language is recursive, and annotating nested phrases cannot be easily
handled with a begin/cont/other scheme.
instead, typically tree structures are used to
represent syntax.

see figure 13.39 for an example. it shows the phrase structure syntactic parse tree for our
example sentence the girl watched attentively the beautiful    re   ies. generating a tree structures is
generally a quite different process than generating a sequence. it is typically built recursively
bottom-up with algorithms such as chart parsing.

however, we can linearize the parse tree into a sequence of words and structural tokens
that indicate the beginning     e.g.,    (np"     and end     closing parenthesis    )"     of syntactic
phrases. so, forcing syntactic parse tree annotations into our sequence-to-sequence neural ma-
chine translation model may be done by encoding the parse structure with additional output
tokens. to be perfectly clear, the idea is to produce as the output of the neural translation sys-
tem not just a sequence of words, but a sequence of a mix of output words and special tokens.

80

chapter 13. id4

the hope is that forcing the id4 model to produce syntactic structure
(even in a linearized form) encourages it to produce syntactically well-formed output. there is
some evidence to support this hope, despite the simplicity of the approach.

linguistically structured models the    eld of syntactic parsing has not been left untouched
by the recent wave of neural networks. the previous section suggests that syntactic parsing
may be done as simply as framing it as a sequence to sequence with additional output tokens.

however, the best-performing syntactic parsers use model structures that take the recur-
sive nature of language to heart. they are either inspired by convolutional networks and build
parse trees bottom-up, or are neural versions of left-to-right push-down automata that main-
tain a stack of opened phrases that any new word may extend or close, or be pushed down the
stack to start a new phrase.

there is some early work on integrating syntactic parsing and machine translation into a
uni   ed framework but no consensus on best practices has emerged yet. at the time of writing,
this is clearly still a challenge for future work.

further readings wu et al. (2012) propose to use factored representations of words (using lemma,
stem, and part of speech), with each factor encoded in a one-hot vector, in the input to a recurrent
neural network language model. sennrich and haddow (2016) use such representations in the input
and output of id4 models, demonstrating better translation quality.

13.6.9 multiple language pairs

there are more than two languages in the world. and we also have training data for many
language pairs, sometimes it is highly overlapping (e.g., european parliament proceedings in
24 languages), sometimes it is unique (e.g. canadian hansards in french and english). for
some language pairs, a lot of training data is available (e.g., french   english). but for most
language pairs, there is only very little, including commercially interesting language pairs such
as chinese   german or japanese   spanish.

there is a long history of moving beyond speci   c languages and encode meaning language-
independent, sometimes called interlingua. in machine translation, the idea is to map the input
language    rst into an interlingua, and then map the interlingua into the output language. in
such a system, we have to build just one mapping step into and one step out of the interlingua
for each language. then we can translate between it and all the other languages for which we
have done the same.

researchers in deep learning often do not hesitate to claim that intermediate states in neural
translation models encode semantics or meaning. so, can we train a id4
system that accepts text in any language as input and translates it into any other language?

13.6. refinements

81

multiple input languages let us say, we have two parallel corpora, one for german   english,
and one for french   english. we can train a id4 model on both corpora
at the same time by simply concatenating them. the input vocabulary contains both german
and french words. any input sentence will be quickly recognized as being either german or
french, due to the sentence context, disambiguating words such as du (you in german, of in
french).

the combined model trained on both data sets has one advantage over two separate mod-
els. it is exposed to both english sides of the parallel corpora and hence can learn a better
language model. there may be also be general bene   ts to having diversity in the data, leading
to more robust models.

multiple output languages we can do the same trick for the output language, by concate-
nating, say, a french   english and a french   spanish corpus. but given a french input sentence
during id136, how would the system know which output language to generate? a crude
but effective way to signal this to the model is by adding a tag like [spanish] as    rst token of
the input sentence.

[english] n   y a-t-il pas ici deux poids, deux mesures?

    is this not a case of double standards?

[spanish] n   y a-t-il pas ici deux poids, deux mesures?

      no puede verse con toda claridad que estamos utilizando un doble rasero?

if we train a system on the three corpora mentioned (german   english, french   english, and
french   spanish) we can also use it translate a sentence from german to spanish     without
having ever presented a sentence pair as training data to the system.

[spanish] messen wir hier nicht mit zweierlei ma  ?

      no puede verse con toda claridad que estamos utilizando un doble rasero?

for this to work, there has to be some representation of the meaning of the input sentence
that is not tied to the input language and the output language. surprisingly, experiments show
that this actually does work, somewhat. to achieve good quality, however, some parallel data
in the desired language pair is needed, but much less than for a standalone model (johnson
et al., 2016).

figure 13.40 summarizes this idea. a single id4 is trained on vari-
ous parallel corpora in turn, resulting in a system that may translate between any seen input
and output language. it is likely that increasingly deeper models (recall section 13.6.4) may
better serve as multi-language translators, since their deeper layer compute more abstract rep-
resentations of language.

the idea of marking the output language with a token such as [spanish] has been explored
more widely in the context of systems for a single language pair. such tokens may represent
the domain of the input sentence (kobus et al., 2016), or the required level of politeness of the
output sentence (sennrich et al., 2016a).

82

chapter 13. id4

figure 13.40: multi-language machine translation system trained on one language pair at a time, rotating
through many of them. after training on french   english, french   spanish, and german   english, it is
even able to translate from german to spanish.

sharing components instead of just throwing data at a generic id4
model, we may want to more carefully consider which components may be shared among
language-pair-speci   c models. the idea is to train one model per language pair, but some of
the components are identical in these unique models.

    the encoder may be shared in models that have the same input language.
    the decoder may be shared in models that have the same output language.
    the attention mechanism may be shared in all models for all language pairs.

sharing components means is that the same parameter values (weight matrices, etc.) are used
in these separate models. updates to them when training a model for one language pair then
also changes them for in the model for the other language pairs. there is no need to mark the
output language, since each model is trained for a speci   c language pair.

the idea of shared training of components can also be pushed further to exploit mono-
lingual data. the encoder may be trained on monolingual input language data, but we will
need to add a training objective (e.g., language model cross-id178). also, the decoder may
be trained in isolation with monolingual language model data. however, since there are no
context states available, these have to be blanked out, which may lead it to learn to ignore the
input sentence and function only as a target side language model.

further readings johnson et al. (2016) explore how well a single canonical neural translation model
is able to learn from multiple to multiple languages, by simultaneously training on on parallel corpora
for several language pairs. they show small bene   ts for several input languages with the same output
languages, mixed results for translating into multiple output languages (indicated by an additional
input language token). the most interesting result is the ability for such a model to translate in language
directions for which no parallel corpus is provided, thus demonstrating that some interlingual meaning
representation is learned, although less well than using traditional pivot methods.

firat et al. (2016) support multi-language input and output by training language-speci   c encoders

and decoders and a shared attention mechanism.

englishfrenchspanishgermaid413.7. alternate architectures

83

figure 13.41: encoding a sentence with a convolutional neural network. by always using two convolu-
tional layers, the size of the convolutions differ (here k2 and k3). decoding reverses this process.

13.7 alternate architectures

most of neural network research has focused on the use of recurrent neural networks with
attention. but this is by no means the only architecture for neural networks. arguable, a disad-
vantage of using recurrent neural networks on the input side is that it requires a long sequential
process that consumes each input word in one step. this also prohibits the ability to parallelize
the processing of all words at once, thus limiting the use of the capabilities of gpus.

there have been a few alternate suggestions for the architecture of neural machine transla-
tion models. we will brie   y present some of them in this section. it remains to be seen, if they
are a curiosity or conquer the    eld.

13.7.1 convolutional neural networks

the    rst end-to-end id4 model of the modern era (kalchbrenner and
blunsom, 2013) was actually not based on recurrent neural networks, but based on convolu-
tional neural networks. these had been shown to be very successful in image processing, thus
looking for other applications was a natural next step.

see figure 13.41 for an illustration of a convolutional network that encodes an input sen-
tence. the basic building block of these networks is a convolution. it merges the representation
of i input words into a single representation by using a matrix ki. applying the convolution
to every sequence of input words reduces the length of the sentence representation by i     1.
repeating this process leads to a sentence representation in a single vector.

the illustration shows an architecture with two convolutional ki layers, followed by a    nal
li layer that merges the sequence of phrasal representations into a single sentence representa-
tion. the size of the convolutional kernels ki and li depends on the length of the sentences.
the example shows a 6-word sentence and a sequence of k2, k3, and l3 layers. for longer
sentences, bigger kernels are needed.

the hierarchical process of building up a sentence representation bottom-up is well grounded

in linguistic insight in the recursive nature of language. it is similar to chart parsing, except that
we are not committing to a single hierarchical structure. on the other hand, we are asking an

input wordembeddingsk2 layerk3 layerl3 layer84

chapter 13. id4

figure 13.42: re   nement of the convolutional neural network model. convolutions do not result in a
single sentence embedding but a sequence. the encoder is also informed by a recurrent neural network
(connections from output id27s to    nal decoding layer.

awful lot from the resulting sentence embedding to represents the meaning of an entire sen-
tence of arbitrary length.

generating the output sentence translation reverses the bottom-up process. one problem
for the decoder is to decide the length of the output sentence. one option to address this
problem is to add a model that predicts output length from input length. this then leads to the
selection of the size of the reverse convolution matrices.

see figure 13.42 for an illustration of a variation of this idea. the shown architecture always
uses a k2 and a k3 convolutional layer, resulting in a sequence of phrasal representations, not
a single sentence embedding. there is an explicit mapping step from phrasal representations
of input words to phrasal representations of output words, called transfer layer.

the decoder of the model includes a recurrent neural network on the output side. sneaking
in a recurrent neural network here does undermine a bit the argument about better paralleliza-
tion. however, the claim still holds true for encoding the input, and a sequential language
model is just a too powerful tool to disregard.

while the just-described convolutional id4 model helped to set the
scene for neural network approaches for machine translation, it could not be demonstrated to
achieve competitive results compared to traditional approaches. the compression of the sen-
tence representation into a single vector is especially a problem for long sentences. however,
the model was used successfully in reranking candidate translations generated by traditional
id151 systems.

input wordembeddingsk2 encoding layerk2 encoding layertransfer layerk3 decoding layerk2 decoding layerselected wordoutput wordembedding13.7. alternate architectures

85

figure 13.43: encoder using stacked convolutional layers. any number of layers may be used.

13.7.2 convolutional neural networks with attention

gehring et al. (2017) propose an architecture for neural networks that combines the ideas of
convolutional neural networks and the attention mechanism. it is essentially the sequence-to-
sequence attention that we described as the canonical id4 approach, but
with the recurrent neural networks replaced by convolutional layers.

we introduced convolutions in the previous section. the idea is to combine a short sequence
of neighboring words into a single representation. to look at it in another way, a convolution
encodes a word with its left and right context, in a limited window. let us now describe in
more detail what this means for the encoder and the decoder in the neural model.

encoder see figure 13.43 for an illustration of the convolutional layers used in the encoder.
for each input word, the state at each layer is informed by the corresponding state in the pre-
vious layer and its two neighbors. note that these convolutional layers do not shorten the
sequence, because we have a convolution centered around each word, using padding (vectors
with zero values) for word positions that are out of bounds.

mathematically, we start with the input id27s exj and progress through a

sequence of layer encodings hd,j at different depth d until a maximum depth d.

h0,j = e xj
hd,j = f (hd   1,j   k, ..., hd   1,j+k)

for d > 0, d     d

(13.94)

the function f is a feed-forward layer, with a residual connection from the corresponding

previous layer state hd   1,j.

note that even with a few convolutional layers, the    nal representation of a word hd,j may
only be informed by partial sentence context     in contrast to the bi-directional recurrent neural
networks in the canonical model. however, relevant context words in the input sentence that
help with disambiguation may be outside this window.

on the other hand, there are signi   cant computational advantages to this idea. all words at
one depth can be processed in parallel, even combined into one massive tensor operation that
can be ef   ciently parallelized on a gpu.

input wordembeddingsconvolutionlayer 1convolutionlayer 2convolutionlayer 300000086

chapter 13. id4

figure 13.44: decoder in convolutional neural network with attention. the decoder state is computed
as a sequence of convolutional layers (here: 2) over the already predicted output words. each convolu-
tional state is also informed by the input context computed from the input sentence and attention.

decoder the decoder in the canonical model also has at its core a recurrent neural network.
recall its state progression de   ned in equation 13.75 on page 49:

si = f (si   1, eyi   1, ci)

(13.95)

where si is the encoder state, eyi   1 the embedding of the previous output word, and ci the
input context.

the convolutional version of this does not have recurrent decoder states, i.e., the compu-
tation does not depend on the previous state si   1, but is conditioned on the sequence of the   
most recent previous words.

si = f (eyi     , ..., eyi   1, ci)

(13.96)

furthermore, these decoder convolutions may be stacked, just as the encoder convolutional

layers.

s1,i = f (eyi     , ..., eyi   1, ci)
sd,i = f (sd   1,i        1, ..., sd   1,i, ci)

for d > 0, d       d

(13.97)

see figure 13.44 for an illustration of these equations. the main difference between the
canonical id4 model and this architecture is the conditioning of the
states of the decoder. they are computed in a sequence of convolutional layers, and also always
the input context.

attention the attention mechanism is essentially unchanged from the canonical neural trans-
lation model. recall that is is based on an association a(si   1, hj) between the word represen-
tations computed by the encoder hj and the previous state of the decoder si   1 (refer back to
equation 13.78 on page 51).

0input contextdecoder convolution 1output wordpredictionsoutput wordembeddingdecoder convolution 2selected word13.7. alternate architectures

87

since we still have such encoder and decoder states (hd,j and s   d,i   1), we use the same
here. these association scores are normalized and used to compute a weighted sum of the
input id27s (i.e., the encoder states hd,j). a re   nement is that the encoder state
hd,j and the input id27 xj is combined via addition when computing the context
vector. this is the usual trick of using residual connections to assist training with deep neural
networks.

13.7.3 self-attention

the critique of the use of recurrent neural networks is that they require a lengthy walk-through,
word by word, of the entire input sentence, which is time-consuming and limits parallelization.
the previous sections replaced the recurrent neural networks in our canonical model with con-
volutions. however, these have a limited context window to enrich representations of words.
what we would like is some architectural component that allows us to use wide context and
can be highly parallelized. what could that be?

in fact, we already encountered it: the attention mechanism. it considers associations be-
tween every input word and any output word, and uses it to build a vector representation of
the entire input sequence. the idea behind self-attention is to extend this idea to the encoder.
instead of computing the association between an input and an output word, self-attention com-
putes the association between any input word and any other input word. one way to view it is
that this mechanism re   nes the representation of each input word by enriching it with context
words that help to disambiguate it.

computing self-attention vaswani et al. (2017) de   ne self attention for a sequence of vectors
hj (of size |h|), packed into a matrix h, as

self-attention(h) = softmax

h

(13.98)

(cid:17)

(cid:16) hh t(cid:112)|h|

let us look at this equation in detail. the association between every word representation
hj any other context word hk is done via the dot product between the packed matrix h and its
transpose h t , resulting in a vector of raw association values hh t . the values in this vector are
   rst scaled by the size of the word representation vectors |h|, and then by the softmax, so that
their values add up to 1. the resulting vector of normalized association values is then used to
weigh the context words.

another way to put equation 13.98 without the matrix h notation but using word repre-

sentation vectors hj:

ajk =

  jk =

self-attention(hj) =

k

1
|h| hjht
(cid:80)
(cid:88)
exp(ajk)
   exp(aj  )
  j  hk

k

(cid:17)

(cid:16) hh t(cid:112)|h|

raw association

normalized association (softmax)

(13.99)

weighted sum

88

chapter 13. id4

self-attention layer the self-attention step described above is only one step in the self-
attention layer used to encode the input sentence. there are four more steps that follow it.

    we combine self-attention with residual connections that pass the word representation

through directly

    next up is a layer id172 step (described in section 13.2.6 on page 21).

self-attention(hj) + hj

(13.100)

  hj = layer-id172(self-attention(hj) + hj)

(13.101)

    a standard feed-forward step with relu activation function is applied.

relu(w   hj + b)

(13.102)

    this is also augmented with residual connections and layer id172.

layer-id172(relu(w   hj + b) +   hj)

(13.103)

taking a page from deep models, we now stack several such layers (say, d = 6) on top of

each other.

h0,j = exj
hd,j = self-attention-layer(hd   1,j)

start with input id27
for d > 0, d     d

(13.104)

the deep modeling is the reason behind the residual connections in the self-attention layer
    such residual connections help with training since they allow a shortcut to the input which
may be utilized in early stages of training, before it can take advantage of the more complex
interdependencies that deep models enable. the layer id172 step is one standard train-
ing trick that also helps especially with deep models.

attention in the decoder self-attention is also used in the decoder, now between output
words. the decoder also has more traditional attention. in total there are 3 sub layers.

    self attention: output words are initially encoded by id27s si = eyi. we per-
form exactly the same self-attention computation as described in equation 13.98. how-
ever, the association of a word si is limited to words sk with k     i, i.e., just the previously
produced output words. let us denote the result of this sub layer for output word i as   si
    attention: the attention mechanism in this model follows very closely self-attention. the
only difference is that, previously, we compute self attention between the hidden states h
and themselves. now, we compute attention between the decoder states   s and the    nal
encoder states h.

attention(   s, h) = softmax

h

(13.105)

(cid:17)

(cid:16)   sh t(cid:112)|h|

13.7. alternate architectures

89

figure 13.45: attention-based machine translation model: the input is encoded with several layers of
self-attention. the decoder computes attention-based representations of the input in several layers,
initialized with the previous id27s.

using the same more detailed exposition as above for self-attention:

aik =

  ik =

attention(  si) =

k

1
|h|   siht
(cid:80)
(cid:88)
exp(aik)
   exp(ai  )
  j  hk

k

(cid:17)

(cid:16)   sh t(cid:112)|h|

raw association

normalized association (softmax)

(13.106)

weighted sum

this attention computation is augmented by adding in residual connections, layer nor-
malization, and an additional relu layer, just like the self-attention layer described above.

input word representations(cid:80)

it is worth noting that, the output of the attention computation is a weighted sum over
k   j  hk. to this, we add the (self-attended) representation
of the decoder state   si via a residual connection. this allows skipping over the deep
layers, thus speeding up training.

    feed-forward layer: this sub layer is identical to the encoder, i.e., relu(ws  si + bs)

each of the sub-layers is followed by the add-and-norm step of    rst using residual connec-

tions and then layer id172 (as noted in the description of the attention sub layer).

the entire model is shown in figure 13.45,

input wordembeddingsdecoderlayer 2output wordpredictionselected output wordoutput wordembeddingself attentionlayer 1self attentionlayer 1decoderlayer 190

chapter 13. id4

further readings kalchbrenner and blunsom (2013) build a comprehensive machine translation
model by    rst encoding the source sentence with a convolutional neural network, and then generate
the target sentence by reversing the process. a re   nement of this was proposed by gehring et al. (2017)
who use multiple convolutional layers in the encoder and the decoder that do not reduce the length of
the encoded sequence but incorporate wider context with each layer.

vaswani et al. (2017) replace the recurrent neural networks used in attentional sequence-to-sequence
models with multiple self-attention layers, both for the encoder as well as the decoder. there are a
number of additional re   nements of this model: so-called multi-head attention, encoding of sentence
positions of words, etc.

13.8 current challenges

id4 has emerged as the most promising machine translation approach
in recent years, showing superior performance on public benchmarks (bojar et al., 2016) and
rapid adoption in deployments by, e.g., google (wu et al., 2016), systran (crego et al., 2016),
and wipo (junczys-dowmunt et al., 2016). but there have also been reports of poor perfor-
mance, such as the systems built under low-resource conditions in the darpa lorelei pro-
gram.8

here, we examine a number of challenges to id4 and give empirical
results on how well the technology currently holds up, compared to traditional statistical ma-
chine translation. we show that, despite its recent successes, id4 still
has to overcome various challenges, most notably performance out-of-domain and under low
resource conditions.

what a lot of the problems have in common is that the neural translation models do not
show robust behavior when confronted with conditions that differ signi   cantly from training
conditions     may it be due to limited exposure to training data, unusual input in case of out-
of-domain test sentences, or unlikely initial word choices in id125. the solution to these
problems may hence lie in a more general approach of training that steps outside optimizing
single word predictions given perfectly matching prior sequences.

another challenge that we do not examine empirically: id4 sys-
tems are much less interpretable. the answer to the question of why the training data leads
these systems to decide on speci   c word choices during decoding is buried in large matrices
of real-numbered values. there is a clear need to develop better analytics for neural machine
translation.

we use common toolkits for id4 (nematus) and traditional phrase-
based id151 (moses) with common data sets, drawn from wmt and
opus. unless noted otherwise, we use default settings, such as id125 and single model
decoding. the training data is processed with byte-pair encoding (sennrich et al., 2016c) into
subwords to    t a 50,000 word vocabulary limit.

8https://www.nist.gov/itl/iad/mig/lorehlt16- evaluations

13.8. current challenges

91

our id151 systems are trained using moses9 (koehn et al., 2007).
we build phrase-based systems using standard features that are commonly used in recent sys-
tem submissions to wmt (williams et al., 2016; ding et al., 2016). while we consider here
only phrase-based systems, we note that there are other id151 ap-
proaches such as hierarchical phrase-based models (chiang, 2007) and syntax-based models
(galley et al., 2004, 2006) that have been shown to give superior performance for language
pairs such as chinese   english and german   english.

we carry out our experiments on english   spanish and german   english. for these lan-
guage pairs, large training data sets are available. we use datasets from the shared translation
task organized alongside the conference on machine translation (wmt)10. for the domain
experiments, we use the opus corpus11 (tiedemann, 2012).

except for the domain experiments, we use the wmt test sets composed of news stories,
which are characterized by a broad range of topic, formal language, relatively long sentences
(about 30 words on average), and high standards for grammar, orthography, and style.

13.8.1 domain mismatch

a known challenge in translation is that in different domains,12 words have different trans-
lations and meaning is expressed in different styles. hence, a crucial step in developing ma-
chine translation systems targeted at a speci   c use case is id20. we expect that
methods for id20 will be developed for id4. a currently
popular approach is to train a general domain system, followed by training on in-domain data
for a few epochs (luong and manning, 2015; freitag and al-onaizan, 2016).

often, large amounts of training data are only available out of domain, but we still seek to
have robust performance. to test how well id4 and statistical machine
translation hold up, we trained    ve different systems using different corpora obtained from
opus (tiedemann, 2012). an additional system was trained on all the training data. statistics
about corpus sizes are shown in table 13.3. note that these domains are quite distant from each
other, much more so than, say, europarl, ted talks, news commentary, and global voices.

we trained both id151 and id4 systems for
all domains. all systems were trained for german-english, with tuning and test sets sub-
sampled from the data (these were not used in training). a common byte-pair encoding is
used for all training runs.

see figure 13.46 for results. while the in-domain neural and id151
systems are similar (id4 is better for it and subtitles, statistical machine
translation is better for law, medical, and koran), the out-of-domain performance for the neu-
ral machine translation systems is worse in almost all cases, sometimes dramatically so. for

9http://www.stat.org/moses/
10http://www.statmt.org/wmt17/
11http://opus.lingfil.uu.se/
12we use the customary de   nition of domain in machine translation: a domain is de   ned by a corpus from a

speci   c source, and may differ from other domains in topic, genre, style, level of formality, etc.

92

chapter 13. id4

corpus
law (acquis)
medical (emea)
it
koran (tanzil)
subtitles

words sentences w/s
25.3
12.9
9.0
20.5
8.2

715,372
1,104,752
337,817
480,421
13,873,398

18,128,173
14,301,472
3,041,677
9,848,539
114,371,754

table 13.3: corpora used to train domain-speci   c systems, taken from the opus repository. it corpora
are gnome, kde, php, ubuntu, and openof   ce.

system    

law

medical

it

koran

subtitles

all data

30.5

32.8

45.1

42.2

35.3

44.7

17.9

17.9

26.4

20.8

law

31.1

34.4

12.1

18.2

3.5

6.9

1.3

2.2

2.8

6.0

medical

3.9

10.2

39.4

43.5

2.0

8.5

0.6

2.0

1.4

5.8

it

koran

1.9

0.4

3.7

1.8

6.5

0.0

5.3

2.1

42.1

39.8

1.8

1.6

0.0

2.3

15.9

18.8

3.9

1.0

4.7

5.5

subtitles

7.0

9.9

9.3

17.8

9.2

13.6

9.0

8.4

25.9

22.1

figure 13.46: quality of systems (id7), when trained on one domain (rows) and tested on another
domain (columns). comparably, id4 systems (left bars) show more degraded
performance out of domain.

13.8. current challenges

93

source
reference
all

law

medical

it

koran

subtitles

schaue um dich herum.
look around you.
id4: look around you.
smt: look around you.
id4: sughum gravecorn.
smt: in order to implement dich schaue .
id4: emea / mb / 049 / 01-en-final work progamme for 2002
smt: schaue by dich around .
id4: switches to paused.
smt: to schaue by itself . \t \t
id4: take heed of your own souls.
smt: and you see.
id4: look around you.
smt: look around you .

figure 13.47: examples for the translation of a sentence from the subtitles corpus, when translated
with systems trained on different corpora. performance out-of-domain is dramatically worse for neural
machine translation.

instance the medical system leads to a id7 score of 3.9 (id4) vs. 10.2
(id151) on the law test set.

figure 13.47 displays an example. when translating the sentence schaue um dich herum. (ref-
erence: look around you.) from the subtitles corpus, we see mostly non-sensical and completely
unrelated output from the id4 system. for instance, the translation
from the it system is switches to paused.

note that the output of the id4 system is often quite    uent (e.g.,
take heed of your own souls.) but completely unrelated to the input, while the statistical machine
translation output betrays its dif   culties with coping with the out-of-domain input by leaving
some words untranslated (e.g., schaue by dich around.). this is of particular concern when mt
is used for information gisting     the user will be mislead by hallucinated content in the neural
machine translation output.

13.8.2 amount of training data

a well-known property of statistical systems is that increasing amounts of training data lead
to better results. in id151 systems, we have previously observed that
doubling the amount of training data gives a    xed increase in id7 scores. this holds true for
both parallel and monolingual data (turchi et al., 2008; irvine and callison-burch, 2013).

how do the data needs of id151 and id4
compare? id4 promises both to generalize better (exploiting word sim-
ilarity in embeddings) and condition on larger context (entire input and all prior output words).

94

chapter 13. id4

30

20

10

21.8

16.4

1.6

0

id7 scores with varying amounts of training data

31.1

30.4
28.6

30.3
30.1

27.8

29.2
27.4

26.1

29.6

29.2

26.9

28.6

25.7

24.7

27.9

23.5

22.4

24.9

19.6

26.9

22.2

26.2

21.2

18.2

14.7

11.9

23.4

18.1

7.2

phrase-based with big lm

phrase-based

neural

106

107

108

corpus size (english words)

figure 13.48: id7 scores for english-spanish systems trained on 0.4 million to 385.7 million words of
parallel data. quality for id4 starts much lower, outperforms statistical machine
translation at about 15 million words, and even beats a id151 system with a big
2 billion word in-domain language model under high-resource conditions.

13.8. current challenges

95

ratio

1

1024

1

512

1

256

1

128

1
64
1

words

source: a republican strategy to counter the re-election of obama
0.4 million un   rgano de coordinaci  n para el anuncio de libre determinaci  n
0.8 million
1.5 million
3.0 million una estrategia republicana para la eliminaci  n de la reelecci  n de obama
6.0 million

lista de una estrategia para luchar contra la elecci  n de hojas de ohio
explosi  n realiza una estrategia divisiva de luchar contra las elecciones de autor

estrategia siria para contrarrestar la reelecci  n del obama .

32 + 12.0 million una estrategia republicana para contrarrestar la reelecci  n de obama

figure 13.49: translations of the    rst sentence of the test set using id4 system
trained on varying amounts of training data. under low resource conditions, id4
produces    uent output unrelated to the input.

we built english-spanish systems on wmt data,13 about 385.7 million english words paired
with spanish. to obtain a learning curve, we used 1
2, and all of the data. for statis-
tical machine translation, the language model was trained on the spanish part of each subset,
respectively. in addition to a neural and id151 system trained on each
subset, we also used all additionally provided monolingual data for a big language model in
contrastive id151 systems.

512, ..., 1

1024, 1

results are shown in figure 13.48. id4 exhibits a much steeper
1
learning curve, starting with abysmal results (id7 score of 1.6 vs. 16.4 for
1024 of the data),
outperforming id151 25.7 vs. 24.7 with 1
16 of the data (24.1 million
words), and even beating the id151 system with a big language model
with the full data set (31.1 for id4, 28.4 for statistical machine transla-
tion, 30.4 for statistical with a big language model).

the contrast between the neural and id151 learning curves is quite
striking. while id4 is able to exploit increasing amounts of training data
more effectively, it is unable to get off the ground with training corpus sizes of a few million
words or less.

to illustrate this, see figure 13.49. with 1

unrelated to the input, some key words are properly translated with 1
(estrategia for strategy, elecci  n or elecciones for election), and starting with 1
become respectable.

1024 of the training data, the output is completely
256 of the data
64 the translations

512 and 1

13.8.3 noisy data

id151 is fairly robust to noisy data. the quality of systems holds up
fairly well, even if large parts of the training data are corrupted in various ways, such as mis-
aligned sentences, content in wrong languages, badly translated sentences, etc. statistical ma-
chine translation models are built on id203 distributions estimated from many occur-
rences of words and phrases. any unsystematic noise in the training only affects the tail end of
the distribution.

13spanish was last represented in 2013, we used data from http://statmt.org/wmt13/translation-task.html

96

chapter 13. id4

ratio shuf   ed
smt (id7)
id4 (id7)

0%
32.7
35.4

10%

20%

50%

32.7 (   0.0)
34.8 (   0.6)

32.6 (   0.1)
32.1 (   3.3)

32.0 (   0.7)
30.1 (   5.3)

table 13.4: impact of noise in the training data, with parts of the training corpus shuf   ed to contain
mis-aligned sentence pairs. id4 degrades severely, while statistical machine
translation holds up fairly well.

is this still the case for id4? chen et al. (2016a) considered one kind
of noise: misaligned sentence pairs in an experiments with a large english   french parallel
corpus. they shuf   e the target side of part of the training corpus, so that these sentence pairs
are mis-aligned.

table 13.4 shows the result. id151 systems hold up fairly well.
even with 50% of the data perturbed, the quality only drops from 32.7 to 32.0 id7 points,
about what is to be expected with half the valid training data. however, the neural machine
translation system degrades severely, from 35.4 to 30.1 id7 points, a drop of 5.3 points, com-
pared to the 0.7 point drop for statistical systems.

a possible explanation for this poor behavior of id4 models is that
its prediction has to    nd a good balance between language model and input context as the main
driver. when training observes increasing ratios of training example, for which the input sen-
tence is a meaningless distraction, it may generally learn to rely more on the output language
model aspect, hence hallucinating    uent by inadequate output.

13.8.4 word alignment

the key contribution of the attention model in id4 (bahdanau et al.,
2015) was the imposition of an alignment of the output words to the input words. this takes
the shape of a id203 distribution over the input words which is used to weigh them in a
bag-of-words representation of the input sentence.

arguably, this attention model does not functionally play the role of a word alignment
between the source in the target, at least not in the same way as its analog in statistical machine
translation. while in both cases, alignment is a latent variable that is used to obtain id203
distributions over words or phrases, arguably the attention model has a broader role. for
instance, when translating a verb, attention may also be paid to its subject and object since these
may disambiguate it. to further complicate matters, the word representations are products of
bidirectional gated recurrent neural networks that have the effect that each word representation
is informed by the entire sentence context.

but there is a clear need for an alignment mechanism between source and target words.
for instance, prior work used the alignments provided by the attention model to interpolate
word translation decisions with traditional probabilistic dictionaries (arthur et al., 2016), for
the introduction of coverage and fertility models (tu et al., 2016b), etc.

13.8. current challenges

s
n
o
i
t
a
l
e
r

n
e
e
w
t
e
b

56
89

a
m
a
b
o

16

72

26
96

die
beziehungen
zwischen
obama
und
netanjahu
sind
seit
jahren
angespannt
.

u
h
a
y
n
a
t
e
n

d
n
a

d
e
n
i
a
r
t
s

e
v
a
h

n
e
e
b

s
r
a
e
y

r
o
f

.

the
relationship
between
obama
and
netanyahu
has
been
stretched
for
years

49

. 11

79

98

54 10
98

42

11

11

14

38
22

84
23

97

t
n
n
a
p
s
e
g

n
e
r
h
a
j

17

.

s
i

n
t
l
  
h
r
e
v

n
e
h
c
s
i
w
z

a
m
a
b
o

u
h
a
y
n
a
t
e
n

d
n
u

s
a
d

t
i
e
s

t
s
i

47

81

72

87

93

95

38
21

16
14

38

19

33
90
32

26
54
77
12

17

(a) desired alignment

(b) mismatched alignment

figure 13.50: word alignment for english   german: comparing the attention model states (green boxes
with id203 in percent if over 10) with alignments obtained from fast-align (blue outlines).

but is the attention model in fact the proper means? to examine this, we compare the
soft alignment matrix (the sequence of attention vectors) with word alignments obtained by
traditional word alignment methods. we use incremental fast-align (dyer et al., 2013) to align
the input and output of the neural machine system.

see figure 13.50a for an illustration. we compare the word attention states (green boxes)
with the word alignments obtained with fast align (blue outlines). for most words, these match
up pretty well. both attention states and fast-align alignment points are a bit fuzzy around the
function words have-been/sind.

however, the attention model may settle on alignments that do not correspond with our in-
tuition or alignment points obtained with fast-align. see figure 13.50b for the reverse language
direction, german   english. all the alignment points appear to be off by one position. we are
not aware of any intuitive explanation for this divergent behavior     the translation quality is
high for both systems.

we measure how well the soft alignment (attention model) of the neural machine transla-

tion system match the alignments of fast-align with two metrics:

    a match score that checks for each output if the aligned input word according to fast-align

is indeed the input word that received the highest attention id203, and

    a id203 mass score that sums up the id203 mass given to each alignment point

obtained from fast-align.

98

chapter 13. id4

language pair
german   english
english   german
czech   english
english   czech
russian   english
english   russian

match
prob.
14.9% 16.0%
77.2% 63.2%
78.0% 63.3%
76.1% 59.7%
72.5% 65.0%
73.4% 64.1%

table 13.5: scores indicating overlap between attention probabilities and alignments obtained with fast-
align.

in these scores, we have to handle byte pair encoding and many-to-many alignments14

in out experiment, we use the id4 models provided by edinburgh15
(sennrich et al., 2016b). we run fast-align on the same parallel data sets to obtain alignment
models and used them to align the input and output of the id4 system.
table 13.5 shows alignment scores for the systems. the results suggest that, while drastic, the
divergence for german   english is an outlier. we note, however, that we have seen such large
a divergence also under different data conditions.

note that the attention model may produce better word alignments by guided alignment
training (chen et al., 2016b; liu et al., 2016) where supervised word alignments (such as the
ones produced by fast-align) are provided to model training.

13.8.5 id125

the task of decoding is to    nd the full sentence translation with the highest id203. in sta-
tistical machine translation, this problem has been addressed with heuristic search techniques
that explore a subset of the space of possible translation. a common feature of these search
techniques is a beam size parameter that limits the number of partial translations maintained
per input word.

there is typically a straightforward relationship between this beam size parameter and the
model score of resulting translations and also their quality score (e.g., id7). while there are
diminishing returns for increasing the beam parameter, typically improvements in these scores
can be expected with larger beams.

decoding in neural translation models can be set up in similar fashion. when predicting
the next output word, we may not only commit to the highest scoring word prediction but

14(1) id4 operates on subwords, but fast-align is run on full words. (2) if an input word
is split into subwords by byte pair encoding, then we add their attention scores. (3) if an output word is split into
subwords, then we take the average of their attention vectors. (4) the match scores and id203 mass scores are
computed as average over output word-level scores. (5) if an output word has no fast-align alignment point, it is
ignored in this computation. (6) if an output word is fast-aligned to multiple input words, then (6a) for the match
score: count it as correct if the n aligned words among the top n highest scoring words according to attention and
(6b) for the id203 mass score: add up their attention scores.

15https://github.com/rsennrich/wmt16-scripts

13.8. current challenges

czech   english

30.830.930.930.930.9 30.9

30.6

30.4 30.5 30.430.3
30.4

30

29.8

29.4

unnormalized
normalized

28.5

31

30
29.7
29.7

u
e
l
b

29

30.7

30.3

29.9

23.2
23.1

24

23

22

22

u
e
l
b

21

20

english-czech

23.9 2424.124.124 23.8
23.6 23.8

24.1

24.2 24 23.9

23.5

99

23.6

23.2

22.7

unnormalized
normalized

19.9

1

2

4

8 12 20 30 50 100 200

500 1,000

1

2

4

8 12 20 30 50 100 200

500 1,000

beam size

german   english

37

36.6
36.4

u
e
l
b

36
35.7
35.7

35

37.537.537.637.637.6 37.6 37.6

37.6 37.6

37.2
36.9 36.936.836.736.6

36.3

36.1

35.7

unnormalized
normalized

34.6

beam size

english   german

28.929 29.129.129.2 29.2 29.2

29.1

28.6
28.4 28.428.528.528.528.4

28.7

28.1

27.6

26.7

unnormalized
normalized

29

28

u
e
l
b

27
26.8
26.8

28
27.9

1

2

4

8 12 20 30 50 100 200

500 1 000

1

2

4

8 12 20 30 50 100 200

500 1 000

beam size

romanian   english

17.3

16.916.916.9

16.6 16.616.7

16.4

16.4 16.5 16.416.416.416.416.3 16.2

unnormalized
normalized

beam size

english   romanian

26
25.4 25.5 25.6 25.6
25.4

25.6 25.6 25.725.625.725.625.7 25.6 25.6
25.6

25.825.825.825.8 25.8

16
15.9

15.6

25

u
e
l
b

15.3

24

unnormalized
normalized

25.6 25.6

24.7

24

17

u
e
l
b

16
15.8
15.8

15

1

2

4

8 12 20 30 50 100 200

500 1 000

1

2

4

8 12 20 30 50 100 200

500 1 000

beam size

russian   english

27.727.827.827.727.7 27.6

27.1

26.6

26.4

25.9

25.5

unnormalized
normalized

24.1

27.5

26.9

27

26.9
26.6

u
e
l
b

26
25.5
25.5

25

beam size

english   russian

23

u
e
l
b

22

21

22.3
22.1

21.8
21.7

22.622.5
22.422.522.422.422.4 22.3

22.222.2

21.9

21.4

22.1

21.8

21.3

20.7

unnormalized
normalized

19.9
19.9
20

25.9

24.8

1

2

4

8 12 20 30 50 100 200

500 1 000

1

2

4

8 12 20 30 50 100 200

500 1 000

beam size

beam size

figure 13.51: translation quality with varying beam sizes. for large beams, quality decreases, especially
when not normalizing scores by sentence length.

100

chapter 13. id4

also maintain the next best scoring words in a list of partial translations. we record with each
partial translation the word translation probabilities (obtained from the softmax), extend each
partial translation with subsequent word predictions and accumulate these scores. since the
number of partial translation explodes exponentially with each new output word, we prune
them down to a beam of highest scoring partial translations.

as in traditional id151 decoding, increasing the beam size allows
us to explore a larger set of the space of possible translation and hence    nd translations with
better model scores.

however, as figure 13.51 illustrates, increasing the beam size does not consistently improve
translation quality. in fact, in almost all cases, worse translations are found beyond an optimal
beam size setting (we are using again edinburgh   s wmt 2016 systems). the optimal beam size
varies from 4 (e.g., czech   english) to around 30 (english   romanian).

normalizing sentence level model scores by length of the output alleviates the problem
somewhat and also leads to better optimal quality in most cases (5 of the 8 language pairs
investigated). optimal beam sizes are in the range of 30   50 in almost all cases, but quality still
drops with larger beams. the main cause of deteriorating quality are shorter translations under
wider beams.

13.8.6 further readings

other studies have looked at the comparable performance of neural and statistical machine
translation systems. bentivogli et al. (2016) considered different linguistic categories for english   
german and toral and s  nchez-cartagena (2017) compared different broad aspects such as
   uency and reordering for nine language directions.

13.9 additional topics

especially early work on neural networks for machine translation was aimed at building neural compo-
nents to be used in traditional id151 systems.

translation models by including aligned source words in the conditioning context, devlin et al.
(2014) enrich a feed-forward neural network language model with source context zhang et al. (2015)
add a sentence embedding to the conditional context of this model, which are learned using a variant
of convolutional neural networks and mapping them across languages. meng et al. (2015) use a more
complex convolutional neural network to encode the input sentence that uses gated layers and also
incorporates information about the output context.

reordering models lexicalized reordering models struggle with sparse data problems when con-
ditioned on rich context. li et al. (2014) show that a neural reordering model can be conditioned on
current and previous phrase pair (encoded with a id56 auto-encoder) to make the
same classi   cation decisions for orientation type.

13.9. additional topics

101

pre-ordering instead of handing reordering within the decoding process, we may pre-order the input
sentence into output word order. de gispert et al. (2015) use an input dependency tree to learn a model
that swaps children nodes and implement it using a feed-forward neural network. miceli barone and
attardi (2015) formulate a top-down left-to-right walk through the dependency tree and make reorder-
ing decisions at any node. they model this process with a recurrent neural network that includes past
decisions in the conditioning context.

id165 translation models an alternative view of the phrase based translation model is to break up
phrase translations into minimal translation units, and employing a id165 model over these units to
condition each minimal translation units on the previous ones. schwenk et al. (2007) treat each minimal
translation unit as an atomic symbol and train a neural language model over it. alternatively, (hu et al.,
2014) represent the minimal translation units as bag of words, (wu et al., 2014) break them even further
into single input words, single output words, or single input-output word pairs, and yu and zhu (2015)
use phrase embeddings leaned with an auto-encoder.

102

chapter 13. id4

bibliography

philip arthur, graham neubig, and satoshi nakamura. 2016.

incorporating discrete translation lex-
in proceedings of the 2016 conference on empirical methods in
icons into id4.
natural language processing. association for computational linguistics, austin, texas, pages 1557   
1567. https://aclweb.org/anthology/d16-1162.

dzmitry bahdanau, kyunghyun cho, and yoshua bengio. 2015. id4 by jointly

learning to align and translate. in iclr. http://arxiv.org/pdf/1409.0473v6.pdf.

paul baltescu and phil blunsom. 2015. pragmatic neural language modelling in machine translation. in
proceedings of the 2015 conference of the north american chapter of the association for computational lin-
guistics: human language technologies. association for computational linguistics, denver, colorado,
pages 820   829. http://www.aclweb.org/anthology/n15-1083.

paul baltescu, phil blunsom, and hieu hoang. 2014.

framework for machine translation.
http://ufal.mff.cuni.cz/pbml/102/art-baltescu-blunsom-hoang.pdf.

language modelling
the prague bulletin of mathematical linguistics 102:81   92.

oxlm: a neural

yoshua bengio, r  jean ducharme, pascal vincent, and christian jauvin. 2003. a neural probabilistic

language model. journal of machine learning research (3):1137   1155.

luisa bentivogli, arianna bisazza, mauro cettolo, and marcello federico. 2016. neural versus phrase-
based machine translation quality: a case study. in proceedings of the 2016 conference on empirical meth-
ods in natural language processing. association for computational linguistics, austin, texas, pages
257   267. https://aclweb.org/anthology/d16-1025.

ond  rej bojar, rajen chatterjee, christian federmann, yvette graham, barry haddow, matthias
huck, antonio jimeno yepes, philipp koehn, varvara logacheva, christof monz, matteo ne-
gri, aurelie neveol, mariana neves, martin popel, matt post, raphael rubino, carolina scar-
ton, lucia specia, marco turchi, karin verspoor, and marcos zampieri. 2016.
findings of
in proceedings of the first conference on ma-
the 2016 conference on machine translation.
chine translation. association for computational linguistics, berlin, germany, pages 131   198.
http://www.aclweb.org/anthology/w/w16/w16-2301.

m. asunci  n casta  o, francisco casacuberta, and enrique vidal. 1997. machine translation using
neural networks and    nite-state models. pages 160   167. http://www.mt-archive.info/tmi-1997-
castano.pdf.

boxing chen, colin cherry, george foster, and samuel larkin. 2017. cost weighting for neu-
the first workshop on neu-
ral machine translation id20.
ral machine translation. association for computational linguistics, vancouver, pages 40   46.
http://www.aclweb.org/anthology/w17-3205.

in proceedings of

boxing chen, roland kuhn, george foster, colin cherry, and fei huang. 2016a. bilingual methods for

103

104

bibliography

adaptive training data selection for machine translation. in annual meeting of the association for ma-
chine translation in the americas (amta). http://www-labs.iro.umontreal.ca/ foster/papers/biid98-
data-sel-amta16.pdf.

wenhu chen, evgeny matusov, shahram khadivi, and jan-thorsten peter. 2016b.

guided
corr abs/1607.01628.

alignment
http://arxiv.org/abs/1607.01628.

training for

topic-aware id4.

david chiang. 2007.

hierarchical phrase-based translation.

computational linguistics 33(2).

http://www.aclweb.org/anthology-new/j/j07/j07-2003.pdf.

kyunghyun cho, bart van merrienboer, dzmitry bahdanau, and yoshua bengio. 2014. on the properties
of id4: encoder   decoder approaches. in proceedings of ssst-8, eighth workshop
on syntax, semantics and structure in statistical translation. association for computational linguistics,
doha, qatar, pages 103   111. http://www.aclweb.org/anthology/w14-4012.

chenhui chu, raj dabre, and sadao kurohashi. 2017. an empirical comparison of id20
methods for id4. in proceedings of the 55th annual meeting of the association for
computational linguistics (volume 2: short papers). association for computational linguistics, vancou-
ver, canada, pages 385   391. http://aclweb.org/anthology/p17-2061.

trevor cohn, cong duy vu hoang, ekaterina vymolova, kaisheng yao, chris dyer, and gholamreza
haffari. 2016. incorporating structural alignment biases into an attentional neural translation model.
in proceedings of the 2016 conference of the north american chapter of the association for computational
linguistics: human language technologies. association for computational linguistics, san diego, cal-
ifornia, pages 876   885. http://www.aclweb.org/anthology/n16-1102.

josep maria crego, jungi kim, guillaume klein, anabel rebollo, kathy yang, jean senellart, egor
akhanov, patrice brunelle, aurelien coquard, yongchao deng, satoshi enoue, chiyo geiss, joshua
johanson, ardas khalsa, raoum khiari, byeongil ko, catherine kobus, jean lorieux, leidiana mar-
tins, dang-chuan nguyen, alexandra priori, thomas riccardi, natalia segal, christophe servan,
cyril tiquet, bo wang, jin yang, dakun zhang, jing zhou, and peter zoldan. 2016. systran   s pure
id4 systems. corr abs/1610.05540. http://arxiv.org/abs/1610.05540.

adri   de gispert, gonzalo iglesias, and bill byrne. 2015. fast and accurate preordering for smt using
neural networks. in proceedings of the 2015 conference of the north american chapter of the association for
computational linguistics: human language technologies. association for computational linguistics,
denver, colorado, pages 1012   1017. http://www.aclweb.org/anthology/n15-1105.

fast and robust neural network joint models for id151.

jacob devlin, rabih zbib, zhongqiang huang, thomas lamar, richard schwartz, and john makhoul.
2014.
in
proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1:
long papers). association for computational linguistics, baltimore, maryland, pages 1370   1380.
http://www.aclweb.org/anthology/p14-1129.

shuoyang ding, kevin duh, huda khayrallah, philipp koehn, and matt post. 2016.

the jhu
the first conference on ma-
machine translation systems for wmt 2016.
chine translation. association for computational linguistics, berlin, germany, pages 272   280.
http://www.aclweb.org/anthology/w/w16/w16-2310.

in proceedings of

john duchi, elad hazan, and yoram singer. 2011. adaptive subgradient methods for online learning

and stochastic optimization. journal of machine learning research 12(jul):2121   2159.

chris dyer, victor chahuneau, and noah a. smith. 2013. a simple, fast, and effective reparameteriza-
tion of ibm model 2. in proceedings of the 2013 conference of the north american chapter of the association
for computational linguistics: human language technologies. association for computational linguistics,
atlanta, georgia, pages 644   648. http://www.aclweb.org/anthology/n13-1073.

bibliography

105

m. amin farajian, marco turchi, matteo negri, nicola bertoldi, and marcello federico. 2017.
in proceedings of
neural vs. phrase-based machine translation in a multi-domain scenario.
the 15th conference of the european chapter of the association for computational linguistics: vol-
ume 2, short papers. association for computational linguistics, valencia, spain, pages 280   284.
http://www.aclweb.org/anthology/e17-2045.

shi feng, shujie liu, nan yang, mu li, ming zhou, and kenny q. zhu. 2016.

improving attention
in proceedings of coling
modeling with implicit distortion and fertility for machine translation.
2016, the 26th international conference on computational linguistics: technical papers. the coling 2016
organizing committee, osaka, japan, pages 3082   3092. http://aclweb.org/anthology/c16-1290.

andrew finch, paul dixon, and eiichiro sumita. 2012. rescoring a phrase-based machine transliter-
in proceedings of the 4th named en-
ation system with recurrent neural network language models.
tity workshop (news) 2012. association for computational linguistics, jeju, korea, pages 47   51.
http://www.aclweb.org/anthology/w12-4406.

orhan firat, kyunghyun cho, and yoshua bengio. 2016. multi-way, multilingual neural ma-
in proceedings of the 2016 conference
chine translation with a shared attention mechanism.
of the north american chapter of the association for computational linguistics: human language
technologies. association for computational linguistics, san diego, california, pages 866   875.
http://www.aclweb.org/anthology/n16-1101.

mikel l forcada and ram  n p   eco. 1997. recursive hetero-associative memories for translation. in

biological and arti   cial computation: from neuroscience to technology, springer, pages 453   462.

markus freitag and yaser al-onaizan. 2016. fast id20 for id4.

technical report. https://arxiv.org/pdf/1612.06897v1.pdf.

michel galley, jonathan graehl, kevin knight, daniel marcu, steve deneefe, wei wang, and ignacio
in pro-
thayer. 2006. scalable id136 and training of context-rich syntactic translation models.
ceedings of the 21st international conference on computational linguistics and 44th annual meeting of the
association for computational linguistics. association for computational linguistics, sydney, australia,
pages 961   968. http://www.aclweb.org/anthology/p/p06/p06-1121.

michel galley, mark hopkins, kevin knight, and daniel marcu. 2004. what   s in a translation
in proceedings of the joint conference on human language technologies and the annual
rule?
meeting of the north american chapter of the association of computational linguistics (hlt-naacl).
http://www.aclweb.org/anthology/n04-1035.pdf.

jonas gehring, michael auli, david grangier, denis yarats, and yann n. dauphin. 2017. convolutional

sequence to sequence learning. corr abs/1705.03122. http://arxiv.org/abs/1705.03122.

mevlana gemici, chia-chun hung, adam santoro, greg wayne, shakir mohamed, danilo jimenez
rezende, david amos, and timothy p. lillicrap. 2017. generative temporal models with memory.
corr abs/1702.04649. http://arxiv.org/abs/1702.04649.

yoav goldberg. 2017.

neural network methods for natural language processing, volume 37 of
synthesis lectures on human language technologies. morgan & claypool, san rafael, ca.
https://doi.org/10.2200/s00762ed1v01y201703hlt037.

ian goodfellow, yoshua bengio, and aaron courville. 2016. deep learning. mit press. http://www.

deeplearningbook.org.

jiatao gu, zhengdong lu, hang li, and victor o.k. li. 2016.

incorporating copying mechanism in
sequence-to-sequence learning. in proceedings of the 54th annual meeting of the association for computa-
tional linguistics (volume 1: long papers). association for computational linguistics, berlin, germany,
pages 1631   1640. http://www.aclweb.org/anthology/p16-1154.

106

bibliography

caglar gulcehre, sungjin ahn, ramesh nallapati, bowen zhou, and yoshua bengio. 2016. pointing the
unknown words. in proceedings of the 54th annual meeting of the association for computational linguistics
(volume 1: long papers). association for computational linguistics, berlin, germany, pages 140   149.
http://www.aclweb.org/anthology/p16-1014.

  aglar g  l  ehre, orhan firat, kelvin xu, kyunghyun cho, lo  c barrault, huei-chi lin, fethi bougares,
holger schwenk, and yoshua bengio. 2015. on using monolingual corpora in neural machine trans-
lation. corr abs/1503.03535. http://arxiv.org/abs/1503.03535.

yuening hu, michael auli, qin gao, and jianfeng gao. 2014. minimum translation modeling with
recurrent neural networks. in proceedings of the 14th conference of the european chapter of the association
for computational linguistics. association for computational linguistics, gothenburg, sweden, pages
20   29. http://www.aclweb.org/anthology/e14-1003.

ann irvine and chris callison-burch. 2013. combining bilingual and comparable corpora for
the eighth workshop on statistical ma-
low resource machine translation.
chine translation. association for computational linguistics, so   a, bulgaria, pages 262   270.
http://www.aclweb.org/anthology/w13-2233.

in proceedings of

s  bastien jean, kyunghyun cho, roland memisevic, and yoshua bengio. 2015a. on using very large
in proceedings of the 53rd annual meeting of the
target vocabulary for id4.
association for computational linguistics and the 7th international joint conference on natural language
processing (volume 1: long papers). association for computational linguistics, beijing, china, pages
1   10. http://www.aclweb.org/anthology/p15-1001.

s  bastien jean, orhan firat, kyunghyun cho, roland memisevic, and yoshua bengio. 2015b. mon-
treal id4 systems for wmt    c     c   n     d  c15. in proceedings of the tenth workshop
on id151. association for computational linguistics, lisbon, portugal, pages
134   140. http://aclweb.org/anthology/w15-3014.

melvin johnson, mike schuster, quoc v. le, maxim krikun, yonghui wu, zhifeng chen, nikhil thorat,
fernanda b. vi  gas, martin wattenberg, greg corrado, macduff hughes, and jeffrey dean. 2016.
google   s multilingual id4 system: enabling zero-shot translation. corr
abs/1611.04558. http://arxiv.org/abs/1611.04558.

marcin junczys-dowmunt, tomasz dwojak,
chine translation ready for deployment?
in proceedings
http://workshop2016.iwslt.org/downloads/iwslt_2016_paper_4.pdf.

and hieu hoang. 2016.
is neural ma-
a case study on 30 translation directions.
on spoken language translation (iwslt).

international workshop

the

of

nal kalchbrenner and phil blunsom. 2013. recurrent continuous translation models. in proceedings of
the 2013 conference on empirical methods in natural language processing. association for computational
linguistics, seattle, washington, usa, pages 1700   1709. http://www.aclweb.org/anthology/d13-
1176.

shin kanouchi, katsuhito sudoh, and mamoru komachi. 2016. neural reordering model considering
phrase translation and word alignment for phrase-based translation. in proceedings of the 3rd workshop
on asian translation (wat2016). the coling 2016 organizing committee, osaka, japan, pages 94   
103. http://aclweb.org/anthology/w16-4607.

diederik p. kingma and jimmy ba. 2015.

adam: a method for stochastic optimization

https://arxiv.org/pdf/1412.6980.pdf.

catherine kobus, josep crego, and jean senellart. 2016. domain control for id4.

technical report. https://arxiv.org/pdf/1612.06140v1.pdf.

philipp koehn, hieu hoang, alexandra birch, chris callison-burch, marcello federico, nicola bertoldi,

bibliography

107

brooke cowan, wade shen, christine moran, richard zens, christopher j. dyer, ond  rej bojar,
alexandra constantin, and evan herbst. 2007. moses: open source toolkit for statistical machine
in proceedings of the 45th annual meeting of the association for computational linguistics
translation.
companion volume proceedings of the demo and poster sessions. association for computational linguis-
tics, prague, czech republic, pages 177   180. http://www.aclweb.org/anthology/p/p07/p07-2045.

j. lei ba, j. r. kiros, and g. e. hinton. 2016. layer id172. arxiv e-prints .
peng li, yang liu, maosong sun, tatsuya izuha, and dakun zhang. 2014. a neural reordering model
for phrase-based translation. in proceedings of coling 2014, the 25th international conference on com-
putational linguistics: technical papers. dublin city university and association for computational lin-
guistics, dublin, ireland, pages 1897   1907. http://www.aclweb.org/anthology/c14-1179.

lemao liu, masao utiyama, andrew finch, and eiichiro sumita. 2016. id4 with
supervised attention. in proceedings of coling 2016, the 26th international conference on computational
linguistics: technical papers. the coling 2016 organizing committee, osaka, japan, pages 3093   
3102. http://aclweb.org/anthology/c16-1291.

shixiang lu, zhenbiao chen, and bo xu. 2014. learning new semi-supervised deep auto-encoder fea-
tures for id151. in proceedings of the 52nd annual meeting of the association
for computational linguistics (volume 1: long papers). association for computational linguistics, bal-
timore, maryland, pages 122   132. http://www.aclweb.org/anthology/p14-1012.

minh-thang luong and christopher manning. 2015. stanford id4 systems for
spoken language domains. in proceedings of the international workshop on spoken language translation
(iwslt). pages 76   79. http://www.mt-archive.info/15/iwslt-2015-luong.pdf.

thang luong, michael kayser, and christopher d. manning. 2015a. deep neural language mod-
in proceedings of the nineteenth conference on computational natu-
els for machine translation.
ral language learning. association for computational linguistics, beijing, china, pages 305   309.
http://www.aclweb.org/anthology/k15-1031.

thang luong, hieu pham, and christopher d. manning. 2015b. effective approaches to attention-
based id4. in proceedings of the 2015 conference on empirical methods in natural
language processing. association for computational linguistics, lisbon, portugal, pages 1412   1421.
http://aclweb.org/anthology/d15-1166.

thang luong, ilya sutskever, quoc le, oriol vinyals, and wojciech zaremba. 2015c. addressing the
in proceedings of the 53rd annual meeting of the
rare word problem in id4.
association for computational linguistics and the 7th international joint conference on natural language
processing (volume 1: long papers). association for computational linguistics, beijing, china, pages
11   19. http://www.aclweb.org/anthology/p15-1002.

fandong meng, zhengdong lu, hang li, and qun liu. 2016. interactive attention for neural machine
in proceedings of coling 2016, the 26th international conference on computational lin-
translation.
guistics: technical papers. the coling 2016 organizing committee, osaka, japan, pages 2174   2185.
http://aclweb.org/anthology/c16-1205.

fandong meng, zhengdong lu, mingxuan wang, hang li, wenbin jiang, and qun liu. 2015. encoding
source language with convolutional neural network for machine translation. in proceedings of the 53rd
annual meeting of the association for computational linguistics and the 7th international joint conference
on natural language processing (volume 1: long papers). association for computational linguistics,
beijing, china, pages 20   30. http://www.aclweb.org/anthology/p15-1003.

haitao mi, zhiguo wang, and abe ittycheriah. 2016. vocabulary manipulation for neural machine
in proceedings of the 54th annual meeting of the association for computational linguistics
translation.
(volume 2: short papers). association for computational linguistics, berlin, germany, pages 124   129.

108

bibliography

http://anthology.aclweb.org/p16-2021.

antonio valerio miceli barone and giuseppe attardi. 2015. non-projective dependency-based pre-
reordering with recurrent neural network for machine translation. in proceedings of the 53rd annual
meeting of the association for computational linguistics and the 7th international joint conference on nat-
ural language processing (volume 1: long papers). association for computational linguistics, beijing,
china, pages 846   856. http://www.aclweb.org/anthology/p15-1082.

antonio valerio miceli barone, jind  rich helcl, rico sennrich, barry haddow, and alexandra birch. 2017.
deep architectures for id4. in proceedings of the second conference on machine
translation, volume 1: research papers. association for computational linguistics, copenhagen, den-
mark, pages 99   107. http://www.aclweb.org/anthology/w17-4710.

tomas mikolov. 2012. statistical language models based on neural networks. ph.d. thesis, brno university

of technology. http://www.   t.vutbr.cz/ imikolov/id56lm/thesis.pdf.

tomas mikolov, wen-tau yih, and geoffrey zweig. 2013. linguistic regularities in continuous space
in proceedings of the 2013 conference of the north american chapter of the as-
word representations.
sociation for computational linguistics: human language technologies. association for computational
linguistics, atlanta, georgia, pages 746   751. http://www.aclweb.org/anthology/n13-1090.

razvan pascanu, tomas mikolov, and yoshua bengio. 2013. on the dif   culty of training recurrent neural
networks. in proceedings of the 30th international conference on machine learning, icml. pages 1310   
1318. http://proceedings.mlr.press/v28/pascanu13.pdf.

  lvaro peris, luis cebri  n, and francisco casacuberta. 2017. online learning for neural machine trans-

lation post-editing. corr abs/1706.03196. http://arxiv.org/abs/1706.03196.

holger schwenk. 2007. continuous space language models. computer speech and language 3(21):492   518.

https://wiki.inf.ed.ac.uk/twiki/pub/cstr/listensemester2_2009_10/sdarticle.pdf.

holger schwenk. 2010.

continuous-space language models for id151.
the prague bulletin of mathematical linguistics 93:137   146. http://ufal.mff.cuni.cz/pbml/93/art-
schwenk.pdf.

holger schwenk. 2012. continuous space translation models for phrase-based statistical machine trans-
lation. in proceedings of coling 2012: posters. the coling 2012 organizing committee, mumbai,
india, pages 1071   1080. http://www.aclweb.org/anthology/c12-2104.

holger schwenk, marta ruiz costa-jussa, and jose a. r. fonollosa. 2007.

smooth bilingual n-
in proceedings of the 2007 joint conference on empirical methods in natural lan-
gram translation.
guage processing and computational natural language learning (emnlp-conll). pages 430   438.
http://www.aclweb.org/anthology/d/d07/d07-1045.

holger schwenk, daniel dechelotte, and jean-luc gauvain. 2006. continuous space language
in proceedings of the coling/acl 2006 main confer-
models for id151.
ence poster sessions. association for computational linguistics, sydney, australia, pages 723   730.
http://www.aclweb.org/anthology/p/p06/p06-2093.

holger schwenk, anthony rousseau, and mohammed attik. 2012. large, pruned or continuous
in proceedings of the naacl-
space language models on a gpu for id151.
hlt 2012 workshop: will we ever really replace the id165 model? on the future of language
modeling for hlt. association for computational linguistics, montr  al, canada, pages 11   19.
http://www.aclweb.org/anthology/w12-2702.

rico sennrich and barry haddow. 2016. linguistic input features improve id4.
in proceedings of the first conference on machine translation. association for computational linguistics,
berlin, germany, pages 83   91. http://www.aclweb.org/anthology/w/w16/w16-2209.

bibliography

109

rico sennrich, barry haddow, and alexandra birch. 2016a. controlling politeness in neural machine
translation via side constraints. in proceedings of the 2016 conference of the north american chapter of the
association for computational linguistics: human language technologies. association for computational
linguistics, san diego, california, pages 35   40. http://www.aclweb.org/anthology/n16-1005.

rico sennrich, barry haddow,

edinburgh neural ma-
the first conference on machine
chine translation systems for wmt 16.
translation. association for computational linguistics, berlin, germany, pages 371   376.
http://www.aclweb.org/anthology/w/w16/w16-2323.

and alexandra birch. 2016b.

in proceedings of

rico sennrich, barry haddow, and alexandra birch. 2016c. improving id4 mod-
els with monolingual data. in proceedings of the 54th annual meeting of the association for computational
linguistics (volume 1: long papers). association for computational linguistics, berlin, germany, pages
86   96. http://www.aclweb.org/anthology/p16-1009.

rico sennrich, barry haddow, and alexandra birch. 2016d. id4 of rare words
with subword units. in proceedings of the 54th annual meeting of the association for computational lin-
guistics (volume 1: long papers). association for computational linguistics, berlin, germany, pages
1715   1725. http://www.aclweb.org/anthology/p16-1162.

christophe servan,

josep maria crego, and jean senellart. 2016.

a post-training id20 for id4.
http://arxiv.org/abs/1612.06141.

domain specialization:
corr abs/1612.06141.

nitish srivastava, geoffrey hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdinov. 2014.
journal of machine learning

dropout: a simple way to prevent neural networks from over   tting.
research 15:1929   1958. http://jmlr.org/papers/v15/srivastava14a.html.

martin sundermeyer, ilya oparin, jean-luc gauvain, ben freiberg, ralf schl

and hermann ney.

"uter,
ral network language models.
and
bridge.eu/downloads/_comparison_of_feedforward_and_recurrent_neural_network_language_models.pdf.

feedforward and recurrent neu-
on acoustics, speech,
http://www.eu-

in ieee international conference
8430   8434.

processing. vancouver,

comparison of

canada,

signal

pages

2013.

ilya sutskever, oriol vinyals, and quoc v. v le. 2014. sequence to sequence learning with neural
networks.
in z. ghahramani, m. welling, c. cortes, n.d. lawrence, and k.q. weinberger, edi-
tors, advances in neural information processing systems 27, curran associates, inc., pages 3104   3112.
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.

alex ter-sarkisov, holger schwenk, fethi bougares, and lo  c barrault. 2015. incremental adaptation
strategies for neural network language models. in proceedings of the 3rd workshop on continuous vec-
tor space models and their compositionality. association for computational linguistics, beijing, china,
pages 48   56. http://www.aclweb.org/anthology/w15-4006.

j  rg tiedemann. 2012.

parallel data,

tools and interfaces in opus.

in nicoletta calzolari,
khalid choukri, thierry declerck, mehmet u  gur do  gan, bente maegaard, joseph mariani, jan
odijk, and stelios piperidis, editors, proceedings of the eighth international conference on lan-
guage resources and evaluation (lrec-2012). european language resources association (elra),
istanbul, turkey, pages 2214   2218. acl anthology identi   er: l12-1246.
http://www.lrec-
conf.org/proceedings/lrec2012/pdf/463_paper.pdf.

antonio toral and v  ctor m. s  nchez-cartagena. 2017.

a multifaceted evaluation of neu-
in proceedings of the
ral versus phrase-based machine translation for 9 language directions.
15th conference of the european chapter of the association for computational linguistics: volume
1, long papers. association for computational linguistics, valencia, spain, pages 1063   1073.
http://www.aclweb.org/anthology/e17-1100.

110

bibliography

zhaopeng tu, yang liu, zhengdong lu, xiaohua liu, and hang li. 2016a. context gates for neural

machine translation. corr abs/1608.06043. http://arxiv.org/abs/1608.06043.

zhaopeng tu, yang liu, lifeng shang, xiaohua liu, and hang li. 2017. neural machine trans-
in proceedings of the 31st aaai conference on arti   cial intelligence.

lation with reconstruction.
http://arxiv.org/abs/1611.01874.

zhaopeng tu, zhengdong lu, yang liu, xiaohua liu, and hang li. 2016b. modeling coverage for
id4. in proceedings of the 54th annual meeting of the association for computational
linguistics (volume 1: long papers). association for computational linguistics, berlin, germany, pages
76   85. http://www.aclweb.org/anthology/p16-1008.

marco turchi, tijl de bie, and nello cristianini. 2008. learning performance of a machine transla-
tion system: a statistical and computational analysis. in proceedings of the third workshop on statis-
tical machine translation. association for computational linguistics, columbus, ohio, pages 35   43.
http://www.aclweb.org/anthology/w/w08/w08-0305.

ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n. gomez,
lukasz kaiser, and illia polosukhin. 2017. attention is all you need. corr abs/1706.03762.
http://arxiv.org/abs/1706.03762.

ashish vaswani, yinggong zhao, victoria fossum, and david chiang. 2013. decoding with large-scale
neural language models improves translation. in proceedings of the 2013 conference on empirical meth-
ods in natural language processing. association for computational linguistics, seattle, washington,
usa, pages 1387   1392. http://www.aclweb.org/anthology/d13-1140.

a. waibel, a. n. jain, a. e. mcnair, h. saito, a.g. hauptmann, and j. tebelskis. 1991. janus: a speech-
to-speech translation system using connectionist and symbolic processing strategies. in proceedings of
the 1991 international conference on acoustics, speech and signal processing (icassp). pages 793   796.

rui wang, masao utiyama, isao goto, eiichro sumita, hai zhao, and bao-liang lu. 2013. con-
verting continuous-space language models into id165 language models for statistical machine
in proceedings of the 2013 conference on empirical methods in natural language pro-
translation.
cessing. association for computational linguistics, seattle, washington, usa, pages 845   850.
http://www.aclweb.org/anthology/d13-1082.

rui wang, hai zhao, bao-liang lu, masao utiyama, and eiichiro sumita. 2014. neural network based
bilingual language model growing for id151. in proceedings of the 2014 con-
ference on empirical methods in natural language processing (emnlp). association for computational
linguistics, doha, qatar, pages 189   195. http://www.aclweb.org/anthology/d14-1023.

philip williams, rico sennrich, maria nadejde, matthias huck, barry haddow, and ond  rej bojar. 2016.
edinburgh   s id151 systems for wmt16. in proceedings of the first conference
on machine translation. association for computational linguistics, berlin, germany, pages 399   410.
http://www.aclweb.org/anthology/w/w16/w16-2327.

haiyang wu, daxiang dong, xiaoguang hu, dianhai yu, wei he, hua wu, haifeng wang, and
ting liu. 2014.
improve id151 with context-sensitive bilingual seman-
in proceedings of the 2014 conference on empirical methods in natural lan-
tic embedding model.
guage processing (emnlp). association for computational linguistics, doha, qatar, pages 142   146.
http://www.aclweb.org/anthology/d14-1015.

yonghui wu, mike schuster, zhifeng chen, quoc v. le, mohammad norouzi, wolfgang macherey,
maxim krikun, yuan cao, qin gao, klaus macherey, jeff klingner, apurva shah, melvin johnson,
xiaobing liu, lukasz kaiser, stephan gouws, yoshikiyo kato, taku kudo, hideto kazawa, keith
stevens, george kurian, nishant patil, wei wang, cliff young, jason smith, jason riesa, alex rud-
nick, oriol vinyals, greg corrado, macduff hughes, and jeffrey dean. 2016. google   s neural machine

bibliography

111

translation system: bridging the gap between human and machine translation. corr abs/1609.08144.
http://arxiv.org/abs/1609.08144.pdf.

youzheng wu, hitoshi yamamoto, xugang lu, shigeki matsuda, chiori hori, and hideki kashioka.
in pro-
2012. factored recurrent neural network language model in ted lecture transcription.
ceedings of the seventh international workshop on spoken language translation (iwslt). pages 222   228.
http://www.mt-archive.info/iwslt-2012-wu.pdf.

yingce xia, di he, tao qin, liwei wang, nenghai yu, tie-yan liu, and wei-ying ma. 2016. dual learning

for machine translation. corr abs/1611.00179. https://arxiv.org/pdf/1611.00179.pdf.

heng yu and xuan zhu. 2015. recurrent neural network based rule sequence model for statis-
in proceedings of the 53rd annual meeting of the association for com-
tical machine translation.
putational linguistics and the 7th international joint conference on natural language processing (vol-
ume 2: short papers). association for computational linguistics, beijing, china, pages 132   138.
http://www.aclweb.org/anthology/p15-2022.

matthew d. zeiler. 2012. adadelta: an adaptive learning rate method. corr abs/1212.5701.

http://arxiv.org/abs/1212.5701.

jiajun zhang, dakun zhang, and jie hao. 2015. local translation prediction with global sentence rep-
in proceedings of the twenty-fourth international joint conference on arti   cial intelligence

resentation.
(ijcai). pages 1398   1404. http://ijcai.org/papers15/papers/ijcai15-201.pdf.

112

bibliography

author index

ahn, sungjin 61
akhanov, egor 88
al-onaizan, yaser 75, 89
amos, david 11
arthur, philip 61, 95
attardi, giuseppe 99
attik, mohammed 45
auli, michael 83, 88, 99

ba, jimmy 23
bahdanau, dzmitry 6, 57, 94
baltescu, paul 45
barrault, lo  c 75
bengio, yoshua 6, 23, 33, 45, 57, 61, 63, 81, 94
bentivogli, luisa 98
bertoldi, nicola 75, 89
birch, alexandra 62, 65, 68, 75, 81, 89, 96
bisazza, arianna 98
blunsom, phil 6, 45, 82, 88
bojar, ond  rej 88, 89
bougares, fethi 45, 63, 75
brunelle, patrice 88
byrne, bill 5, 99

callison-burch, chris 89, 91
cao, yuan 88
casacuberta, francisco 5, 75
casta  o, m. asunci  n 5
cebri  n, luis 75
cettolo, mauro 98
chahuneau, victor 95

chatterjee, rajen 88
chen, boxing 75, 94
chen, wenhu 69, 75, 96
chen, zhenbiao 5
chen, zhifeng 80, 81, 88
cherry, colin 75, 94
chiang, david 45, 89
cho, kyunghyun 6, 45, 57, 61, 63, 81, 94
chu, chenhui 75
cohn, trevor 72
constantin, alexandra 89
coquard, aurelien 88
corrado, greg 80, 81, 88
costa-jussa, marta ruiz 99
courville, aaron 23
cowan, brooke 89
crego, josep 75, 81
crego, josep maria 75, 88
cristianini, nello 91

dabre, raj 75
dauphin, yann n. 83, 88
de bie, tijl 91
de gispert, adri   5, 99
dean, jeffrey 80, 81, 88
dechelotte, daniel 45
deneefe, steve 89
deng, yongchao 88
devlin, jacob 6, 98
ding, shuoyang 89
dixon, paul 45
dong, daxiang 99

113

author index

114

ducharme, r  jean 33, 45
duchi, john 23
duh, kevin 89
dwojak, tomasz 88
dyer, chris 72, 95
dyer, christopher j. 89

enoue, satoshi 88

farajian, m. amin 75
federico, marcello 75, 89, 98
federmann, christian 88
feng, shi 72
finch, andrew 45, 69, 96
firat, orhan 6, 45, 63, 81
fonollosa, jose a. r. 99
forcada, mikel l 5
fossum, victoria 45
foster, george 75, 94
freiberg, ben 45
freitag, markus 75, 89

galley, michel 89
gao, jianfeng 99
gao, qin 88, 99
gauvain, jean-luc 45
gehring, jonas 83, 88
geiss, chiyo 88
gemici, mevlana 11
goldberg, yoav 23
gomez, aidan n. 86, 88
goodfellow, ian 23
goto, isao 45
gouws, stephan 88
graehl, jonathan 89
graham, yvette 88
grangier, david 83, 88
gu, jiatao 62
gulcehre, caglar 61

haddow, barry 62, 65, 68, 75, 79, 81, 88, 89, 96
haffari, gholamreza 72

hao, jie 98
hauptmann, a.g. 5
hazan, elad 23
he, di 65
he, wei 99
helcl, jind  rich 68
herbst, evan 89
hinton, g. e. 23
hinton, geoffrey 23
hoang, cong duy vu 72
hoang, hieu 45, 88, 89
hopkins, mark 89
hori, chiori 79
hu, xiaoguang 99
hu, yuening 99
huang, fei 94
huang, zhongqiang 6, 98
huck, matthias 88, 89
hughes, macduff 80, 81, 88
hung, chia-chun 11

iglesias, gonzalo 5, 99
irvine, ann 91
ittycheriah, abe 62
izuha, tatsuya 5, 99

jain, a. n. 5
jauvin, christian 33, 45
jean, s  bastien 6, 61
jiang, wenbin 98
jimeno yepes, antonio 88
johanson, joshua 88
johnson, melvin 80, 81, 88
jones, llion 86, 88
junczys-dowmunt, marcin 88

kaiser, lukasz 86, 88
kalchbrenner, nal 6, 82, 88
kanouchi, shin 5
kashioka, hideki 79
kato, yoshikiyo 88
kayser, michael 44, 45
kazawa, hideto 88
khadivi, shahram 69, 75, 96

author index

khalsa, ardas 88
khayrallah, huda 89
khiari, raoum 88
kim, jungi 88
kingma, diederik p. 23
kiros, j. r. 23
klein, guillaume 88
klingner, jeff 88
knight, kevin 89
ko, byeongil 88
kobus, catherine 75, 81, 88
koehn, philipp 88, 89
komachi, mamoru 5
krikun, maxim 80, 81, 88
krizhevsky, alex 23
kudo, taku 88
kuhn, roland 94
kurian, george 88
kurohashi, sadao 75

lamar, thomas 6, 98
larkin, samuel 75
le, quoc 61
le, quoc v. 80, 81, 88
le, quoc v. v 6, 57
lei ba, j. 23
li, hang 57, 62, 72, 95, 98
li, mu 72
li, peng 5, 99
li, victor o.k. 62
lillicrap, timothy p. 11
lin, huei-chi 45, 63
liu, lemao 69, 96
liu, qun 72, 98
liu, shujie 72
liu, tie-yan 65
liu, ting 99
liu, xiaobing 88
liu, xiaohua 57, 72, 95
liu, yang 5, 57, 72, 95, 99
logacheva, varvara 88

115

lorieux, jean 88
lu, bao-liang 45
lu, shixiang 5
lu, xugang 79
lu, zhengdong 57, 62, 72, 95, 98
luong, minh-thang 75, 89
luong, thang 44, 45, 57, 61

ma, wei-ying 65
macherey, klaus 88
macherey, wolfgang 88
makhoul, john 6, 98
manning, christopher 75, 89
manning, christopher d. 44, 45, 57
marcu, daniel 89
martins, leidiana 88
matsuda, shigeki 79
matusov, evgeny 69, 75, 96
mcnair, a. e. 5
memisevic, roland 6, 61
meng, fandong 72, 98
mi, haitao 62
miceli barone, antonio valerio 68, 99
mikolov, tomas 23, 35, 45
mohamed, shakir 11
monz, christof 88
moran, christine 89

nadejde, maria 89
nakamura, satoshi 61, 95
nallapati, ramesh 61
  eco, ram  n p 5
negri, matteo 75, 88
neubig, graham 61, 95
neveol, aurelie 88
neves, mariana 88
ney, hermann 45
nguyen, dang-chuan 88
norouzi, mohammad 88

oparin, ilya 45

parmar, niki 86, 88
pascanu, razvan 23

author index

116

patil, nishant 88
peris,   lvaro 75
peter, jan-thorsten 69, 75, 96
pham, hieu 57
polosukhin, illia 86, 88
popel, martin 88
post, matt 88, 89
priori, alexandra 88

qin, tao 65

rebollo, anabel 88
rezende, danilo jimenez 11
riccardi, thomas 88
riesa, jason 88
rousseau, anthony 45
rubino, raphael 88
rudnick, alex 88

saito, h. 5
salakhutdinov, ruslan 23
s  nchez-cartagena, v  ctor m. 98
santoro, adam 11
scarton, carolina 88
schl
"uter, ralf 45
schuster, mike 80, 81, 88
schwartz, richard 6, 98
schwenk, holger 5, 45, 63, 75, 99
segal, natalia 88
senellart, jean 75, 81, 88
sennrich, rico 62, 65, 68, 75, 79, 81, 89, 96
servan, christophe 75, 88
shah, apurva 88
shang, lifeng 57
shazeer, noam 86, 88
shen, wade 89
singer, yoram 23
smith, jason 88
smith, noah a. 95
specia, lucia 88
srivastava, nitish 23

stevens, keith 88
sudoh, katsuhito 5
sumita, eiichiro 45, 69, 96
sumita, eiichro 45
sun, maosong 5, 99
sundermeyer, martin 45
sutskever, ilya 6, 23, 57, 61

tebelskis, j. 5
ter-sarkisov, alex 75
thayer, ignacio 89
thorat, nikhil 80, 81
tiedemann, j  rg 89
tiquet, cyril 88
toral, antonio 98
tu, zhaopeng 57, 72, 95
turchi, marco 75, 88, 91

uszkoreit, jakob 86, 88
utiyama, masao 45, 69, 96

van merrienboer, bart 6, 57
vaswani, ashish 45, 86, 88
verspoor, karin 88
vidal, enrique 5
vi  gas, fernanda b. 80, 81
vincent, pascal 33, 45
vinyals, oriol 6, 57, 61, 88
vymolova, ekaterina 72

waibel, a. 5
wang, bo 88
wang, haifeng 99
wang, liwei 65
wang, mingxuan 98
wang, rui 45
wang, wei 88, 89
wang, zhiguo 62
wattenberg, martin 80, 81
wayne, greg 11
williams, philip 89
wu, haiyang 99
wu, hua 99
wu, yonghui 80, 81, 88

author index

wu, youzheng 79

xia, yingce 65
xu, bo 5
xu, kelvin 45, 63

yamamoto, hitoshi 79
yang, jin 88
yang, kathy 88
yang, nan 72
yao, kaisheng 72
yarats, denis 83, 88
yih, wen-tau 35
young, cliff 88
yu, dianhai 99
yu, heng 99
yu, nenghai 65

117

zampieri, marcos 88
zaremba, wojciech 61
zbib, rabih 6, 98
zeiler, matthew d. 23
zens, richard 89
zhang, dakun 5, 88, 98, 99
zhang, jiajun 98
zhao, hai 45
zhao, yinggong 45
zhou, bowen 61
zhou, jing 88
zhou, ming 72
zhu, kenny q. 72
zhu, xuan 99
zoldan, peter 88
zweig, geoffrey 35

