ccf adl 
beijing 

april 8,  2017 

deep learning for natural language processing 

                                           

hang li 

noah   s ark lab 

huawei technologies 

outline of lecture 

    introduction 
    basics of dl for nlp 
    state of the art of dl for nlp 
    previous work at noah   s ark lab 
    recent progress at noah   s ark lab 
    advantages and disadvantages 
    summary 

ultimate goal: natural language 

understanding 

natural language dialogue 

text comprehension 

natural language understanding 

    two definitions: 

    representation-based: if system creates proper 

internal representation, then we say it    understands    
language 

    behavior-based: if system properly follows 

instruction in natural language, then we say it 
   understands    language, e.g.,    bring me a cup of tea    

    we take the latter definition 
 

 
 

five characteristics of human language 

    incompletely regular (both regular and 

idiosyncratic) 

    compositional (or recursive) 
    metaphorical 
    associated with knowledge 
    interactive  

 

natural language understanding by 

computer is extremely difficult 

    it is still not clear whether it is possible to realize 

human language ability on computer 

    on modern computer 

    the incomplete regularity and compositionality 

characteristics imply complex combinatorial 
computation 

    the metaphor, knowledge, and interaction 

characteristics imply exhaustive computation 

    big question: can we invent new computer closer to 

human brain? 
 

reason of challenge 

    a computer system must be constructed 

based on math 

    open question: whether it is possible to 

process natural language as humans, using 
math models 

    natural language processing is believed to 

be ai complete 

simplified problem formulation  

- eg., id53 

analysis 

understanding 

id136 

retrieval 

decision 

generation 

analysis 

retrieval 

generation 

id53, including 
search, can be practically 
performed, because it is 
simplified 

data-driven approach may work 

    hybrid is most realistic and effective for 

natural language processing, and ai 
    machine learning based 
    human-knowledge incorporated 
    human brain inspired 

    big data and deep learning provides new 

opportunity 

 
 

ai loop 

system 

algorit
hm 

users 

advancement in ai, 
including nlp can be made 
through the closed loop 

data 

fundamental problems of statistical 

natural language processing 

    classification: assigning a label to a string 
 
    matching:  matching two strings 

 

    translation:  transforming one string to another 

 

    id170: mapping string to structure 
 
 
    markov decision process: deciding next state given 

previous state and action 

ts         rts,cs   'ss   dfundamental problems of statistical 

natural language processing 

    classification 

    text classification 
    id31 

    matching 
    search  
    id53 
    dialogue (single turn) 

    translation 

    machine translation 
    id103 
    hand writing recognition 
    dialogue (single turn) 

    id170 

    named entity extraction 
    id52 
    sentence parsing 
    id29 
    markov decision 

process 
    dialogue (multi turn, 

task dependent) 

 

lower bound of user need  
vs upper bound of technology 

lower bound of user need 

pushing upper bound of technology 

upper bound of technology 

deep learning for natural language 

processing (dl for nlp) 

    state-of-art performances in 

    classification 
    matching 
    translation 
    id170 

    particularly, id4 

outperforms id151  

references 

1.                                                                      

2017      2    

2.                                                       2016      5    
3.                ai                                                   

2016      11    

4.                                                             2014    
 

outline of lecture 

    introduction 
    basics of dl for nlp 
    state of the art of dl for nlp 
    previous work at noah   s ark lab 
    recent progress at noah   s ark lab 
    advantages and disadvantages 
    summary 

basics of dl for nlp 

    id27 
    convolutional neural network (id98) 
    recurrent neural network (id56) 
    sequence-to-sequence learning 

id27 

id27 

    motivation:  representing words with low-dimensional 

real-valued vectors, utilizing them as input to deep 
learning methods, vs one-hot vectors 

    method: sgns (skip-gram with negative sampling) 
    tool: id97 
    input: words and their contexts in documents 
    output: embeddings of words 
    assumption: similar words occur in similar contexts 
    interpretation: factorization of mutual information matrix 
    advantage: compact representations (usually 100~ 

dimensions) 

 

skip-gram with negative sampling 

 (mikolov et al., 2013) 

    input:  occurrences between words and contexts 

 
 
 
 
 
 

1 

2 

5 

3 

2 

1 

1 

    id203 model:   

1w2w3w1c2c3c4c5cmcwecwcwdp                                 11)(),|1(   cwecwcwdp                                 11)(),|0(   skip-gram with negative sampling 

    word vector and context vector: lower dimensional 

(parameter ) vectors  

    goal: learning of the id203 model from data 
    take co-occurrence data as positive examples 
    negative sampling:  randomly sample k unobserved pairs   
                as negative examples 
    objective function in learning 

 

 
    algorithm: stochastic id119   

),(ncw)(log)(log),(#~npcwccwkcwcwln                                          ecw      ,interpretation as id105 

 (levy & goldberg 2014) 

    pointwise mutual information matrix 
 

-.5 

1 

3 

1.5 

2 

1 

-0.5 

)()(),(logcpwpcwp1w2w3w1c2c3c4c5cminterpretation as id105 

-.5 

1 

3 

1.5 

2 

1 

-0.5 

id105, 
equivalent to sgns 

7 

1 

0.5 

2.2 

1.5 

1 

3 

1 

id27 

1w2w3w1t2t3ttwcm   w1w2w3w1c2c3c4c5cmconvolutional neural network 

convolutional neural network 

    motivation:  representing sequence of words and 

utilizing the representation in deep learning methods  

    input: sequence of id27s, denoting 

sequence of words (e.g., sentence) 

    output: representation of input sequence 
    learning of model: stochastic id119 
    advantage: robust extraction of id165 features; can 

be used as part of deep model for sequence 
processing (e.g., sentence classification) 
 
 

convolutional neural network (id98)  

(kim 2014, blunsom et al. 2014, hu et al., 2014) 

the cat sat on the mat 

concatenation 

the cat  

sat on 

the cat sat  

       

sat on  

the mat 

on the mat 

    shared parameters on same level 
    fixed length, zero padding 

max pooling 

the cat  

cat sat  

cat sat  

sat on  

sat on 

on the 

on the 

the mat 

the cat sat  

cat sat on 

sat on the 

on the mat 

convolution 

the      cat      sat      on       the      mat 

example: image convolution 

filter 

dark pixel value = 1,  light pixel value = 0 
dot in filter = 1, others = 0 

0 

3 

0 

1 

leow wee kheng 

example: image convolution 

feature map 

0  0  0  0  0 

0  0  1  1  0 

0  1  3  2  0 

0  1  3  1  0 

0  1  1  0  0 

convolution operation 

    scanning image with filter having 3*3 cells, among them 3 are dot cells 
    counting number of dark pixels overlapping with dot cells at each 
position 
    creating feature map (matrix), each element represents similarity 
between filer pattern and pixel pattern at one position 
    equivalent to extracting feature using the filter 
    translation-invariant 

convolution 

  

  

  

  

    

+1 

filter     feature map 
    neuron 

convolution 

equivalent to id165 
feature extraction at 
each position 

tthititiiiliflflflilflliflflixxxzizlizlfbwlifzffbzwz],,[ location  for   vectors wordedcancatenat frominput  is 1layer  from location for neuron  ofinput  is function sigmoid is layer in    typeofneuron  of parameters are ,layer in  location for    typeofneuron  ofoutput  is ,,2,1    )(11)0()0()1(),(),(),(),()1(),(),(                                             )1(   liz),(flb),(flw),(flizmax pooling 

  

  

max pooling 

  

  

  

  

  

equivalent to id165 
feature selection 

lifzzlifzzzzfliflifliflifliflilayer in  location for    typeof pooling ofinput  are ,layer in  location for    typeof pooling ofoutput  is ),max(),1(2),1(12),(),1(2),1(12),(                     sentence classification  

using convolutional neural network 

       

  

  

  

  

  

  

  

concatenation 

max pooling 

convolution 

)(id98)max(soft)(xzbwzxfy            )(lzxyrecurrent neural network 

recurrent neural network 

    motivation:  representing sequence of words and utilizing 

the representation in deep learning methods  

    input: sequence of id27s, denoting sequence 

of words (e.g., sentence) 

    output: sequence of internal representations (hidden 

states) 

    variants:  lstm and gru, to deal with long distance 

dependency 

    learning of model: stochastic id119 
    advantage:  handling arbitrarily long sequence; can be 

used as part of deep model for sequence processing (e.g., 
id38) 
 

recurrent neural network (id56) 

 (mikolov et al. 2010) 

the cat sat on the mat 

the                           cat                        sat                                      .                       mat 

),(1tttxhfh      tx1   thtxth1   threcurrent neural network 

    

    

    

+1 

)tanh(),(11hxtxthtttbxwhwxhfh                  1   thtxthlong term short memory (lstm) 

(hochreiter & schmidhuber, 1997) 

    a memory (vector) to store 
svalues of previous state 
    input gate, output gate, and 
kforget gate to control  
    gate: element-wise product 
kwith vector of values in [0,1] 

m 

input gate 

output gate 

forget gate 

)tanh()tanh()()()(11111ttttttttgtgxtghtotoxtohtftfxtfhtitixtihtcohid18icbxwhwgbxwhwobxwhwfbxwhwi                                                                              ),(1ttxh   ),(1ttxh   ),(1ttxh   ),(1ttxh   thtitotftgtcgated recurrent unit (gru) 

(cho et al., 2014) 

    a memory (vector) to store 
svalues of previous state 
    reset gate and update gate to 
kcontrol 

reset gate 

update gate 

tttttgtgxttghtztzxtzhtrtrxtrhtgzhzhbxwhrwgbxwhwzbxwhwr                                                         )1())(tanh()()(1111      txth1   th1   th),(1   tthxtrtgtz),(1   tthxrecurrent neural network language model 

model 

objective of learning 

    input one sequence and output another 
    in training, input sequence is same as output sequence 

)max(soft)|()tanh(111bwhxxxppbxwhwhtttthxtxtht                                    tttpt1  log1thtx1   th1   txtx1   txsequence to sequence learning 

translation: sequence to sequence learning 

    

    

    

    hierarchical lstm 
    different lstm 
smodels for encoder 
sand decoder 
    reverse order of 
swords in source 
ssentence 
 

    sutskever et al. 2014 

1x1   txtxtx1h1   ththth1s1   tsts'ts1y1   tyty'ty),(),,(),(),|(111111                        ttdtttetttttsyfshxfhsygyyypx   translation: id56 encoder-decoder 

    

    

    

    

    context vector 
represents source 
sentence 
     gru is used 

    cho et al. 2014 

1x1   txtxtx1h1   ththth1s1   tsts'ts1y1   tyty'tyc),(),,(),,,(),|(111111                           ttetttdtttttthxfhcsyfshccsygyyypx   translation: attention mechanism 

    

    

    

    

    context vector 
prepresents attention 
    corresponds to 
palignment relation 
    encoder: bidirectional 
pid56 

bahdanau, et al. 2014 

1x1   txtxtx1h1   ththth1c1   tctc'tc1s1   tsts'ts1y1   tyty'ty),(),,(),,(),|(111111                        ttettttdtttttthxfhcsyfscsygyyypx   ),(11jttjjtjtjthsqhc                     outline of lecture 

    introduction 
    basics of dl for nlp 
    state of the art of dl for nlp 
    previous work at noah   s ark lab 
    recent progress at noah   s ark lab 
    advantages and disadvantages 
    summary 

state of the art of dl for nlp 

    classification 
    matching 
    translation 
    id170 

 

    references can be found at  hang li, 

zhengdong lu,  sigir 2016 tutorial 

classification 

    examples of tasks 

    search: query 

classification, document 
classification 

    id53: 

question classification, 
answer classification 

    approaches 

    world level model 
    character level model 
    hierarchical model (for 

document classification) 

class 

deep neural network  

sentence  

sentence classification: word level model 

sentence  
representation 

score 

classifier 

neural network 

sentence  

classifier: 
    softmax 
neural network: 
    convolutional neural network, 
    deep neural network 
input: 
    continuous id27,  
    discrete id27 
(one-hot) 

    kim 2014 
    blunsom et al. 2014 
    johnson & zhang 2015 
    iyyer et al.  2015 

document classification: character level model 

score 

neural network 2 

document  
representation 

neural network 1 

neural network 1: 
    deep convolutional neural         
    network  
neural network 2: 
    3-layer fully-connected    
    neural  network 
input: 
    character embedding 
data: 
    large scale training dataset 
class: 
     semantic topics 

sentence  

    zhang et al.  2016 

document classification: hierarchical model 

score 

classifier 

document 
representation 

sentence  
representations 

second layer 

network 

    

first layer 
network 

document  

classifier: 
    softmax 
first layer network: 
    recurrent neural  
    network (lstm, gru) 
second layer network: 
    recurrent neural  
    network (lstm, gru) 
attention: 
    can be employed    
    between two layers 

    tang et al.  2015 
    lai et al. 2015 
    yang et al. 2016 

matching 

    examples of tasks 

    search:  query-document 

(title) matching, similar 
query finding 

    id53: 

question answer matching 

    approaches 

    projection to latent space 
    one dimensional 

matching 

    two dimensional 

matching 

    tree matching 

score 

deep neural network  

question  

answer 

matching:  projection to latent space 

    natural extension of 
vector space model 

question  
representation 

latent space 

dot product (score) 

answer  
representation 

neural network 

neural network 

question  

answer  

neural networks: 
    convolutional neural network 
    deep neural network 
    recurrent neural network 

    huang et al. 2013 
    shen et al. 2014 
    severyn & moschitti 2015 

matching: one dimensional matching 

score 

neural network 2 

question  
representation 

answer 
representation 

neural network 1 

neural network 1 

question  

answer 

neural network 1: 
    convolutional neural network 
neural network 2: 
    deep neural network, tensor network 

    hu et al.  2014 
    qiu & huang 2015 

matching: two dimensional matching 

score 

neural network 2 

neural network 1 

neural network 1: 
    convolutional  
    neural network 
neural network 2: 
    deep neural  
   network 

question-answer matching  
representation 

question  

answer  

    hu et al.  2014 
    pang et al. 2016 
    wan et al. 2016 

matching: tree matching 

score 

dnn with sparse 
input layer 

neural network 

tree matching patterns 

 tree matching 
representation 

matching pattern 

identification 

e.g.  q=   how is x[city]   , 
a=   the food in x[city] 
is great    

question 
parse tree 

parser 

question  

answer parse 
tree 

parser 

answer 

    wang et al.  2015 

translation 

    examples of tasks 

    machine translation: 

sentence level translation 

    id53: 

answer generation from 
question 

    search: similar query 

generation 
    approaches 

    sequence-to-sequence 

learning 

    id56 encoder-decoder 
    attention mechanism 

answer 

deep neural network  

question  

translation: sequence-to-sequence learning 

(same for id56 encoder-decoder) 

target sentence 

 internal representation 

decoder 

encoder 

encoder: 
    recurrent neural  
    network 
decoder: 
    recurrent neural  
    network 

source sentence 

    sutskever et al. 2014 
    cho et al. 2014 

translation: sequence-to-sequence learning 

target sentence 

 internal representation 

decoder 

encoder 

encoder: 
    recurrent neural 
network 
decoder: 
    recurrent neural 
network 

 attention 
mechanism 

source sentence 

bahdanau, et al. 2014 

id170 

    examples of tasks 

named entities, etc 

    search: named entity 

recognition in query and 
document 

    id53: 

id39 
in question and answer 

    approaches 

    id98 
    sequence-to-sequence 

learning 

    neural network based 

parsing 

deep neural network  

sentence  

id170:  id98  

label at each position 

classifier 

sentence  
representation 

neural network 

classifier at each position: 
    softmax 
neural network: 
    convolutional neural  
    network 
 
     

sentence  

    collobert et al. 2011 

id170: sequence-to-

sequence learning  

linearized parse tree 

neural network 

sentence  

neural network: 
   sequence-to-sequence learning  
   model  
training data: 
   pairs of sentence and linearized  
   parse tree 
 
 
     

e.g., 
john has a dog .       (s (np nnp)np  (vp vbz (np dt nn)np )vp . )s 

    vinyals et al. 2015 

id170: neural network 

parse tree 

parser 

sentence  

based parsing  
parser: 
   transition-based dependency  
   parser, constituency parser,  
   crf parser 
neural network: 
   deep neural networks 
training data: 
   pairs of sentence and parse  
   tree 
 
 
     

neural 
network 

    chen & manning, 2014 
    durrett & klein, 2015 
    zhou et al., 2015 
    andor et al.,  2016 

outline of lecture 

    introduction 
    basics 
    state of the art 
    previous work at noah   s ark lab 
    recent progress at noah   s ark lab 
    advantages and disadvantages 
    summary 

id53 
- deepmatch id98 

demo 

retrieval based id53 system 

retrieved 
questions and  
answers 

 

matching 

 

retrieval 

matched 
answers 

ranked 
answers 

 

ranking 

best  
answer 

question 

online 

offline 

index of 

questions and 

answers 

 

matching 
models 

 

ranking 
model 

deep match model id98  

    represent and match two sentences simultaneously 
    two dimensional model 
    state of art model for matching in question 

answering 

sentence x 

1d   
convolution 

max-
pooling 

2d   
convolution 

mlp 

more 2d  
convolution &  
pooling 

 

y

 

e
c
n
e
t
n
e
s

    

matching  
degree 

65 

id162 

- multimodal id98 

demo 

multimodal id98 

 mlp 

    one convolutional neural 
network represents image 
    one convolutional neural 
network represents text 
    multi layer id88 
conducts matching 

id98 

       

  

  

  

  
  
  

  
  

a      dog    is  catching  a     ball 

experimental results 

    experiment 

    trained with 30k flickr data 
    outperforming other state-of-the-art models 

natural language dialogue 

- neural responding machine 

demo 

neural responding machine 

                                                                                                          

    using both local and global 
attention mechanisms 

    

    

    

    

                                                                                                    

1x1   txtxtx1h1   ththth1c1   tctc'tc1s1   tsts'ts1y1   tyty'tycexperimental results 

    experiment 

    trained with 4.4 million weibo data (chinese) 
    95% of responses are natural,  76% of responses are 

appropriate as replies 

message 

response 

occupy central is finally over 

will lujiazui (finance district 
in shanghai) be the next? 

i want to buy a samsung phone 

let us support our national 
brand. 

id4 

demo 

id4 

a        cat         is        sitting            on             the         mat 

    

    

attention 

    

    

                                                                                                

decoder 

    using coverage vectors 
to avoid over-translation 
and under-translation 
    using context gates to 
dynamically control the 
impact of attention 

encoder 

1x1   txtxtx1h1   ththth1c1   tctc'tc1s1   tsts'ts1y1   tyty'tycoverage vector 
a        cat         is        sitting            on             the         mat 

    

    

attention 

decoder 

    using coverage vectors 
to avoid over-translation 
and under-translation 

    

    

                                                                                                

encoder 

1x1   txtxtx1h1   ththth1c1   tctc'tc1s1   tsts'ts1y1   tyty'tyexperimental results 

    experiment 

    trained with 1.25 million ldc data (chinese-english) 

33  

32  

31  

30  

29  

28  

27  

26  

31.7  

30.6  

30.1  

28.3  

id4 

+coverage 

+context 
gate 

id7 

c-e 

translation 

                                        

id4 

there will be some terrorist attacks. 

+both 

some terrorist attacks will become 
more and more intense. 

experimental result 

    google id4 system works better, apparently due to its larger 

training data and more powerful computing architecture 

    google id4 system also employs coverage mechanism 

42 

40 

38 

36 

34 

32 

30 

28 

40.0 

36.8 

33.5 

google translator 
a web translator 
noah id4 system 

outline of lecture 

    introduction 
    basics of dl for nlp 
    state of the art of dl for nlp 
    previous work at noah   s ark lab 
    recent progress at noah   s ark lab 
    advantages and disadvantages 
    summary 

id53 

- neural enquirer 

id53 from relational database 

q: how many people participated in the 
mmgame in beijing? 
a:  4,200 
sql: select #_participants, where city=beijing 
 
q:  when was the latest game hosted? 
a:  2012 
sql: argmax(city, year) 

year 

2000 

learning system 

2004 

2008 

2012 

relational database 

city 

#_days 

#_medals 

sydney 

athens 

beijing 

london 

20 

35 

30 

40 

2,000 

1,500 

2,500 

2,300 

q: which city hosted the 
mmlongest olympic game 
mmbefore the game in beijing? 

question 

answering system 

a:  athens 

neural enquirer 

    query encoder:  encoding query 
    table encoder: encoding entries in table 
    five executors: executing query against table 

conducting 
matching 
between 
question and 
database entries 
multiple times 

query encoder and table encoder 

query encoder 

table encoder 

query representation 

entry representation 

table representation 

id56 

dnn 

    

query 

field  value 

    creating query embedding using id56 
    creating table embedding for each entry using dnn 

executors 

    five layers, except last layer, each layer has reader, 
mannotator, and memory  
    reader fetches important representation for each row, 
me.g.,  city=beijing 
    annotator encodes result representation for each row, 
me.g.,  row where city=beijing 

experimental results 

    experiment 

    olympic database 
    trained with 25k and 100k synthetic data 
    accuracy:  84% on 25k data,  91% on 100k data 
    significantly better than sempre (semantic parser) 
    criticism: data is synthetic 

25k data 

end-to-end  step-by-step 

100k data 

end-to-end  step-by-step 

semantic 
parser 

semantic 
parser 

65.2% 

84.0% 

96.4% 

na 

90.6% 

99.9% 

id53 

- genqa 

id53 from id13 
q: how tall is yao ming? 
a:  he is 2.29m tall and is visible from space. 
(yao ming, height, 2.29m) 
 
q:  which country was beethoven from? 
a:  he was born in what is now germany. 
(ludwig van beethoven, place of birth, germany) 

id13 

learning system 

(yao-ming, spouse, ye-li) 
(yao-ming, born, shanghai) 
(yao-ming,  height, 2.29m) 
        
(ludwig van beethoven, place of 
birth, germany) 
        
 

answer is generated 

q: how tall is liu xiang? 

id53 

a: he is 1.89m tall 

system 

genqa 

he is 2.29m tall 

generator 

enquirer 

short term memory 

interpreter 

how tall is yao ming? 

long term 
memory 

(knowledge base) 

   

   

interpreter:  creates 
representation of 
question using id56 
enquirer:  retrieves top k 
triples with highest 
matching scores using 
id98 model 

attention 

model 

    generator: generates 

answer based on 
question and retrieved 
triples using attention-
based id56 

    attention model:  

controls generation of 
answer 

key idea:   
    generation of answer based on question and retrieved 
kresult 
    combination of neural processing and symbolic 
rprocessing 

enquirer: retrieval and matching 

question and embedding 

matching 

(how, tall, is, liu, xiang) 

retrieved top k triples 
and embeddings 

< liu xiang, height, 1.90m> 
 
< yao ming, height, 2.26m> 
        
 
<liu xiang, birth place, shanghai> 

    retaining both symbolic representations and vector representations 
    using question words to retrieve top k triples 
    calculating matching scores between question and triples using 
cid98 model 
    finding best matched triples 

generator: answer generation 

he 
    

    

    

is 

tall 

2.29m 

    

< yao ming, height, 2.29m> 

how 

tall 

is 

yao 

ming 

? 

    generating answer using attention mechanism 
    at each position, a variable decides whether to  
ggenerate  a word  or use the object of top triple 

2s3s3c03   z13   z3z3y2yo3yexperimental results 

    experiment 

    trained with 720k question-answer pairs  (chinese) 

associated with 1.1m triples in knowledge-base, data is 
noisy 

    accuracy = 52% 
    data is still noisy 

 

question 
who wrote the romance of the 
three kingdoms? 

answer 
luo guanzhong in ming 
dynasty 

correct 

how old is stefanie sun this 
year? 

thirty-two, he was born 
on july 23, 1978 

wrong 

when will shrek forever after 
be released? 

release date: 
dreamworks pictures 

wrong 

natural language dialogue 

- copynet 

single turn dialogue with generating and 

copying mechanism 

model 

my name is harry potter 

dialogue system  

hi, harry potter 

dialogue system can not only generate response, but 
also copy from given message 

architecture: copynet 

hi 

,  

harry 

potter 

<eos>  

hi 

,  

harry 

decoding 

encoding 

my    name   is   harry   potter 

information 
to be decoded 

harry 

attentive 
read 
(generate) 

potter 

generate/copy 

harry 

selective 
read 
(copy) 

encoded 
information 

copynet can either generate word based on attentive read, 
or copy word based on selective read 

characteristics: copynet 

    decoder can both generate and copy 
    mixture model of generating and copying 
    attentive read: find suitable word to influence 

generation of  word in target sequence 

    selective read: find location of word to be copied 

from source sequence 

    model is fully differentiable 
    training:  maximum likelihood of target sequence 

given source sequence 
 
 

experimental results 

    experiment 

    summarization of short text in chinese 
    trained with 2.4m text-summary pairs 
    tested with 9.3k text-summary pairs 

model 

id8-1 

id8-2 

id8-l 

id56 -c 

id56 -w 

copynet -c 

copynet -w 

29.9 

26.8 

34.4 

35.0 

17.4 

16.1 

21.6 

22.3 

27.2 

24.1 

31.3 

32.0 

outline of lecture 

    introduction 
    basics 
    state of the art 
    previous work at noah   s ark lab 
    recent progress at noah   s ark lab 
    advantages and disadvantages 
    summary 

advantages and disadvantages of dl 

    strength 

    weakness 

    good at pattern recognition 

    not good at id136 and 

problems  

decision problems 

    data-driven, performance is 

    data-hungry and thus is not 

high in many tasks 

suitable when data size is small 

    end-to-end training, little or 

    difficult to handle tail 

no domain knowledge is 
needed in system 
construction 

    representation learning,  

possible in cross modal 
processing 

    gradient-based learning, 

learning algorithm is simple 

    powerful for supervised 

learning setting 

phenomena 

    model is usually a black box and 

is difficult to understand 

    computational cost of learning 

is high 

    unsupervised learning methods 

are needed 

    still lack of theoretical 

foundation 

end-to-end learning 

generation-based dialogue 

target sentence 

decoder 

 internal representation 

 attention mechanism 

encoder 

source sentence 

representation learning 

symbolic matching models 

q 

s 

d 

vector space model, 
bm25, language model for ir 

q 

d 

linear projection into latent space 
wu et al. 2012 

neural matching models 

q 

s 

m 

multimodal match model (id98), 
ma et al. 2015 

neural matching models 
are natural extension of 
symbolic matching models 

challenge in tail 

natural language processing problems are 

non-parametric 

how to deal with the long tail is challenging issue 

xinhua news data 

 

e
z
i
s
 
y
r
a
u
b
a
c
o
v

l

1400000 

1200000 

1000000 

800000 

600000 

400000 

200000 

0 

vocabulary size increases when data size increases 

data from xiao chen 

theoretical analysis 

generalization ability of deep learning 

    in practice, usually both training errors and test 

errors are small, i.e., no over-fitting 

    neural networks can    memorize    training instances 
    on the other hand, neural networks can over-fit (i.e., 

test errors are large although training errors are 
small), if random noise is injected into training data 

    number of parameters is larger than number of 

training instances 

    many open questions about the learning ability of 

deep learning 

id136 and decision 

comparison between single-turn qa and 

multi-turn qa by humans 

single-turn qa 

multi-turn qa 

    q: how tall is yao ming? 
    a: he is 2.29m tall. 

    q: how tall is yao ming? 
    a: he is 2.29m tall. 
    q: who is taller, yao ming or 

liu xiang? 

    a: he is taller, and i think 

that liu xiang is only 1.89m 
tall. 

    single turn qa is only related to fact retrieval and answer generation. 
    multi-turn qa  needs fact retrieval and answer generation, as well as 
other mental processing. more modules in human brain are involved. 

deep learning and multi-turn 

dialogue 

    dl may not be enough for natural language dialogue 
    key is dialogue management, including dialogue 

control and dialogue modeling 

    involvement of multiple    modules   , each having 

multiple    states    

    recent work tries to use id23 
    there are many open questions 

outline of lecture 

    introduction 
    basics of dl for nlp 
    state of the art of dl for nlp 
    previous work at noah   s ark lab 
    recent progress at noah   s ark lab 
    advantages and disadvantages 
    summary 

summary 

    deep learning brings high performance in 

fundamental language processing problems, 
particularly translation 

     basic models: id27, id98, id56, 

sequence to sequence learning 

    deep neural network models are state of the art for 

id53, id162, generation 
based dialogue, machine translation 

    recent progress, combination of symbolic and 

neural processing 

    advantages and limitations 

references 

1.

2.

3.

4.

zhaopeng tu, yang liu, zhengdong lu, xiaohua liu, and hang li. context gates for neural machine 
translation. transactions of the association for computational linguistics (tacl 2017). 
xing wang, zhengdong lu, zhaopeng tu, hang li, deyi xiong, and min zhang. id4 
advised by id151. the 31st aaai conference on artificial intelligence (aaai 2017), 
3330-3336, 2017. 
zhaopeng tu, yang liu, lifeng shang, xiaohua liu, and hang li. id4 with 
reconstruction. the 31st aaai conference on artificial intelligence (aaai 2017), 3097-3103. 2017 
fandong meng, zhengdong lu, hang li, qun liu. interactive attention for id4. 
proceedings of  the 26th international conference on computational linguistics (coling'16), 2174-
2185, 2016. 

5. mingxuan wang, zhengdong lu, hang li, qun liu. memory-enhanced decoder for neural machine 

6.

7.

8.

9.

translation. proceedings of the 2016 conference on empirical methods in natural language processing 
(emnlp'16), 278-286, 2016.  
pengcheng yin, zhengdong lu, hang li, ben kao, neural enquirer: learning to query tables in natural 
language. ieee data engineering bulletin 39(3): 63-73, 2016.  
jiatao gu, zhengdong lu, hang li, victor o. k. li. incorporating copying mechanism in sequence-to-
sequence learning. proceedings of the 54th annual meeting of association for computational linguistics 
(acl   16), 2016.  
zhaopeng tu, zhengdong lu, yang liu, xiaohua liu and hang li. modeling coverage for neural machine 
translation. proceedings of the 54th annual meeting of association for computational linguistics (acl   16), 
2016.  
jun yin, xin jiang, zhengdong lu, lifeng shang, hang li, xiaoming li. neural generative question 
answering. proceedings of the 25th international joint conference on artificial intelligence (ijcai   16), 
2972-2978, 2016.  

references 

10. pengcheng yin, zhengdong lu, hang li, ben kao. neural enquirer: learning to query tables with natural 

language. proceedings of the 25th international joint conference on artificial intelligence (ijcai   16), 2308-
2314, 2016.  

11. lin ma, zhengdong lu, hang li. learning to answer questions from image using convolutional neural 

network. proceedings of the thirtieth aaai conference (aaai   16), 2016.  

12. lin ma, zhengdong lu, lifeng shang, hang li, multimodal convolutional neural networks for matching 

image and sentence. proceedings of 2015 ieee international conference on id161 (iccv), 2623-
2631, 2015.  

13. baotian hu, zhaopeng tu, zhengdong lu, hang li, qingcai chen. context-dependent translation selection 

using convolutional neural network. proceedings of the 53th annual meeting of association for 
computational linguistics and the 7th international joint conference on natural language processing 
(acl-ijcnlp'15), 536-541, 2015.  

14. lifeng shang, zhengdong lu, hang li. neural responding machine for short text conversation. 

proceedings of the 53th annual meeting of association for computational linguistics and the 7th 
international joint conference on natural language processing (acl-ijcnlp'15), 1577-1586, 2015.  

15. mingxuan wang, zhengdong lu, hang li, wenbin jiang, qun liu. genid98: a convolutional architecture for 

word sequence prediction. proceedings of the 53th annual meeting of association for computational 
linguistics and the 7th international joint conference on natural language processing (acl-ijcnlp'15), 
1567-1576, 2015.  

16. fandong meng, zhengdong lu, mingxuan wang, hang li, wenbin jiang, qun liu. encoding source 

language with convolutional neural network for machine translation. proceedings of the 53th annual 
meeting of association for computational linguistics and the 7th international joint conference on 
natural language processing (acl-ijcnlp'15), 20-30, 2015.  

references 

17. mingxuan wang, zhengdong lu, hang li, qun liu. syntax-based deep matching of short texts. proceedings 

of the 24th international joint conference on artificial intelligence (ijcai'15), 1354-1361, 2015.  

18. baotian hu, zhengdong lu, hang li, qingcai chen. convolutional neural network architectures for 

matching natural language sentences. proceedings of advances in neural information processing systems 
27 (nips'14), 2042-2050, 2014.  

19. zhengdong lu, hang li. a deep architecture for matching short texts. proceedings of neural information 

processing systems 26 (nips'13), 1367-1375, 2013.  

thank you! 

