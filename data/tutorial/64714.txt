machine learning: generative 

and discriminative models

sargur n. srihari

srihari@cedar.buffalo.edu

machine learning course: 

http://www.cedar.buffalo.edu/~srihari/cse574/index.html

machine learning

outline of presentation

srihari

1.  what is machine learning?

ml applications, ml as search

2.  generative and discriminative taxonomy
3.  generative-discriminative pairs

classifiers: na  ve bayes and id28
sequential data: id48s and crfs

4.  performance comparison in  sequential applications

nlp: table extraction, id52, id66, 
handwritten word recognition, document analysis

5.  advantages, disadvantages
6.  summary
7.  references

2

machine learning

srihari

1. machine learning

     programming computers to use 
example data or past experience
     well-posed learning problems

    a computer program is said to learn from 

experience e 

    with respect to class of tasks t and performance 

measure p, 

    if its performance at tasks t, as measured by p, 

improves with experience e.

3

machine learning

srihari

problems too difficult to program by hand
     learning to drive an 
autonomous vehicle
     train computer-controlled 
vehicles to steer correctly

     drive at 70 mph for 90 

miles on public highways

     associate steering 

commands with image 
sequences

task t: driving on public, 4-lane highway using vision sensors
perform measure p: average distance traveled before error 

(as judged by human overseer)

training  e: sequence of images and steering commands recorded 

4

while observing a human driver

machine learning

srihari

example problem: 

handwritten digit recognition

wide variability of same numeral

     handcrafted rules will 

result in large no of 
rules and exceptions

     better to have a 

machine that learns 
from a large training 
set

5

machine learning

srihari

other applications of machine learning

     recognizing spoken words

     speaker-specific strategies for recognizing phonemes and words from speech 
     neural networks and methods for learning id48s for customizing to individual 

speakers, vocabularies and microphone characteristics

     search engines

     information extraction from text

     data mining

     very large databases to learn general regularities implicit in data
     classify celestial objects from image data

     decision tree  for objects in sky survey: 3 terabytes

6

machine learning

srihari

ml as searching hypotheses space
     very large space of possible hypotheses to fit:

     observed data and
     any prior knowledge held by the observer

method
concept learning boolean 

hypothesis space

id90

expressions
all possible trees

neural networks weight space

7

machine learning

srihari

ml methodologies are increasingly 

statistical

     rule-based id109 being replaced by 

probabilistic generative models

     example: autonomous agents in ai

     eliza : natural language rules to emulate therapy session
     manual specification of models, theories are increasingly 

difficult

     greater availability of data and computational power 

to migrate away from rule-based and manually 
specified models to probabilistic data-driven modes

8

machine learning

srihari

the statistical ml approach

1.  data collection

large sample of data of how humans perform the task

2.  model selection

settle on a parametric statistical model of the process

3.  parameter estimation

calculate parameter values by inspecting the data

using learned model perform:
4.  search

find optimal solution to given problem

9

machine learning

srihari

2. generative and discriminative 

models: an analogy

     the task is to determine the language that 

someone is speaking 
     generative approach:

     is to learn each language and determine as to 

which language the speech belongs to

     discriminative approach:

     is determine the linguistic differences without 

learning any language    a much easier task!

10

machine learning

taxonomy of ml models

srihari

     generative methods

     model class-conditional pdfs and prior probabilities
        generative    since sampling can generate synthetic data points
     popular models

     gaussians, na  ve bayes, mixtures of multinomials
     mixtures of gaussians, mixtures of experts, id48 (id48)
     sigmoidal belief networks, id110s, markov random fields

     discriminative methods

     directly estimate posterior probabilities 
     no attempt to model underlying id203 distributions
     focus computational resources on given task    better performance
     popular models

     id28, id166s
     traditional neural networks, nearest neighbor
     id49 (crf)

11

generative models (graphical)

parent node
selects between
components

quick medical 
reference -dt

diagnosing
diseases from 
symptoms

markov random
field

machine learning

srihari

successes of generative methods

     nlp

     traditional rule-based or boolean logic systems (eg 
dialog and lexis-nexis) are giving way to statistical 
approaches (markov models and stochastic context free 
grammars)

     medical diagnosis

     qmr knowledge base, initially a heuristic expert 

systems for reasoning about diseases and symptoms has 
been augmented with decision theoretic formulation

     genomics and bioinformatics

     sequences represented as generative id48s

13

machine learning

discriminative classifier: id166

srihari

nonlinear decision boundary

(x1, x2)  (cid:198) (x1,  x2,  x1x2)            

linear boundary
in high-dimensional
space

14

machine learning

support vector machines

srihari

     support vectors are those 

nearest patterns at 
distance b from 
hyperplane

     id166 finds hyperplane with 

maximum distance from 
nearest training patterns

     for full description of 

id166s see

http://www.cedar.buffalo.edu/ 
~srihari/cse555/id166s.pdf

three support vectors are  
shown as solid dots

15

machine learning

srihari

3. generative-discriminative pairs

     na  ve bayes and id28 form 

a generative-discriminative pair for 
classification

     their relationship mirrors that between 

id48s and linear-chain crfs for 
sequential data

16

machine learning

srihari

graphical model relationship

i

e
v
t
a
r
e
n
e
g

na  ve bayes classifier

y

sequence

x

x1

p(y,x)

xm

y

x

y1

x1

hidden markov model

condition

condition

p(y/x)

sequence

yn

xn

p(y,x)

p(y/x)

id28

conditional random field

17

i

i

e
v
t
a
n
m
r
c
s
d

i

i

machine learning

srihari

generative classifier: bayes
     given variables x =(x1 ,..,xm ) and class variable y
     joint pdf is  p(x,y)

     called generative model since we can generate more samples 

artificially

     given a full joint pdf we can 

     marginalize

p y
( )

(x,

y

)

p

=    
x
p
y
(x,
p
(x)

=

)

     condition
     by conditioning the joint pdf we form a classifier 

p y
(

| x)

     computational problem:

     if x is binary then we need 2m values
     if 100 samples are needed to estimate a given id203, 

m=10, and there are two classes then we need 2048 samples

18

machine learning

srihari

na  ve bayes classifier

     goal is to predict single class variable y 

given a vector of features x=(x1 ,..,xm )

     assume that once class labels are known 

the features are independent

     joint id203 model has the form

p y
( , x)

m

=    
p y
( )

m

1
=

p x
(
m

|

y

)

     need to estimate only m probabilities

     factor graph obtained by defining factors 

  (y)=p(y),   m (y,xm )=p(xm ,y)

19

machine learning

srihari

discriminative classifier: id28

     feature vector x
     two-class classification: class variable 

  (a)

y has values c1 and c2

     a posteriori id203 p(c1 |x) written 

as
p(c1 |x) =f(x) =    (wtx)  where

   =

a
( )

1
1 exp(
+

   

a

)

     it is known as id28 in 

statistics
     although it is a model for classification 

rather than for regression

logistic sigmoid

a

properties:
a. symmetry

  (-a)=1-  (a)

b. inverse

a=ln(  /1-  )
known as logit.
also known as 
log odds since 
it is the ratio
ln[p(c1 |x)/p(c2 |x)]

c. derivative

20
d  /da=  (1-  )

machine learning

srihari

id28 versus 
generative bayes classifier

     posterior id203 of class variable y is

p c
(
1

| x)

=

              =

)

p

p

(x |
c p c
(x |
(
1
1
1
1 exp(
+

=

   

a

)

c p c
(
)
1
1
c p c
)
(x |
(
+

)
p

)

2

  

a
a
( )   where   

)

2

=

ln

p
(x |
p
(x |

c p c
(
1
1
c p c
(

)
)

)
)

2

2

     in a generative model we estimate the class- 
conditionals (which are used to determine a)

     in the discriminative approach we directly estimate a 

as a linear function of x i.e.,  a = wtx

21

machine learning

srihari

id28 parameters

     for m-dimensional feature space logistic 
regression has m parameters w=(w1 ,..,wm )

     by contrast, generative approach

     by fitting gaussian class-conditional densities will 

result in 2m parameters for means, m(m+1)/2 
parameters for shared covariance matrix, and one for 
class prior p(c1 )

     which can be reduced to o(m) parameters by assuming 

independence via na  ve bayes

22

machine learning

srihari

multi-class id28
p x c p c
(
)
(
k
p x c p c
(
(

p c
(

| x)

)

)

)

|

k

k

     case of k>2 classes

j

j

|

=    
j
a
exp(
)
k
   
a
exp(

j

)

j

              =

     normalized exponential also known as softmax since if 

     known as normalized exponential

where ak =ln p(x|ck )p(ck )
ak >>aj then p(ck |x)=1 and p(cj |x)=0
ak =wk

tx

     in id28 we assume activations given by 

23

machine learning

srihari

graphical model for id28

     multiclass id28 can be 
 where

written as

p y
(

exp

| x)

=

+

x

k

1
(x)

z

   
  
   
y
   

  
yj

   

j

1
=

   
   
   

j

z

(x) = 

   

y

exp

   
  
   
y
   

+

k

   

j

1
=

  
yj

x

j

   
   
   

     rather than using one weight per class we 

can define feature functions that are 
nonzero only for a single class

p y
(

| x)

=

1
(x)

z

exp

   
   
   

k

   

k

1
=

  
k

f

k

y
( , x)

   
   
   

     this notation mirrors the usual notation 

for crfs

24

machine learning

srihari

4. sequence models

     classifiers predict only a single class variable
     id114 are best to model many 

variables that are interdependent

     given sequence of observations x={xn }n=1
n
     underlying sequence of states y={yn }n=1
n

25

machine learning

srihari

generative model: id48

     x is observed data sequence to be 

labeled, 
y is the random variable over the 
label sequences

     id48 is a distribution that models 

p(y, x)

y1

x1

y2

x2

yn

xn

yn

xn

     joint distribution is

p(

n

=    y x
,

)

n

1
=

p y
(

n

|

y

n

1
   

)

p

(

x

n

|

y

n

)

     highly structured network indicates 

conditional independences,
     past states independent of future states
     conditional independence of observed 

given its state.

26

machine learning

srihari

discriminative model for sequential data

     crf models the conditional 

distribution p(y/x)

     crf is a random field globally 
conditioned on the observation 
x

     the conditional distribution 
p(y|x) that follows from the 
joint distribution p(y,x) can be 
rewritten as a markov random 
field

yn

yn

y1

y2

x

27

machine learning

srihari

markov random field (mrf)

     also called undirected graphical model
     joint distribution of set of variables x is defined by an 

undirected graph as

p

(x)

1
=    
z

c

  
c

(x )
c

where c is a maximal clique 

(each node connected to every other node),

xc is the set of variables in that clique, 
  c is a potential function (or local or compatibility function)
such that   c (xc ) > 0, typically   c (xc ) = exp{-e(xc )}, and
is the partition function for id172

  
c

(x )
c

z

=        

x

c

a specific one

     model refers to a family of distributions and field refers to 

28

machine learning

srihari

mrf with input-output variables
     x is a set of input variables that are observed

    element of x is denoted x

     y is a set of output variables that we predict

    element of y is denoted y

     a are subsets of x u y

    elements of a that are in a ^ x are denoted xa
    element of a that are in a ^ y are denoted ya

     then undirected graphical model has the form

p

(x,y)

=

1
z

   

a

  

a

(x , y ) where z=

a

a

     

   

x,y

a

(x , y )
a

a

a

29

machine learning

srihari

mrf local function

     assume each local function has the form

  

(x , y )
a

a

a

=

exp

f  

am am

(x , y ) 

a

a

   
   
   

   

m

   
   
   

where   a is a parameter vector, fa are feature 
functions and  m=1,..m are feature subscripts

30

machine learning

from id48 to crf

srihari

n

n

n

n

n

|

n

(

)

)

1
   

1
=

(

p

x

p

     in an id48
y
p y
|
(

p(

   
   
   

1
z

exp

y x

       

=    y x
,

y
)
     can be rewritten as
1
} {
     further rewritten as
x
,

(
     which gives us

f
  
m m

y y
,
n

1
  
ij
{

y, x

   

exp

n i j s
   

1
z

   
   
   

=

=

p

)

1
   

1
=

(

)

m

,

m

=

n

y

i

n

,

)

n

   
   
   

p

(

y | x

)

=

p y x
( , )
p y x
', )
(

=

   

'

y

   
     note that z cancels out

   

1
=

m

y

'

   
   
   
exp

m

   
m
1
=
   
   
   

m

exp

f
  
m m

(

y y
,
n

n

1
   

,

x

n

f
  
m m

(

y y
,
n

n

1
   

,

indicator function:
1{x = x   } takes value 1when 
x=x    and  0 otherwise

  
oi

1
{

1
} {

y

n

=

i

x
n

=

o

}

   
   
   

parameters of 
the distribution:
  ={  ij ,  oi }

+

=

j

}

n

1
   

         

i s o o
       

n

y

feature functions have 
the form fm (yn ,yn-1 ,xn ):
need one feature for each
state transition (i,j)
fij (y,y   ,x)=1{y=i} 1{y   =j} and 
one for each state- 
observation pair
fio (y,y   ,x)=1{y=i} 1{x=o}

31

)

   
   
   

)

   
   
   
x

n

machine learning

srihari

crf definition

     a linear chain crf is a distribution p(y|x) 

that takes the form

p

(

y x

|

)

=

1
(x)

z

exp

   
   
   

m

   

m

1
=

f
  
m m

(

y y
,
n

n

1
   

,

x

n

)

   
   
   

     where z(x) is an instance specific 

id172 function
(

(x)

exp

f
  
m m

z

=

m

   

   
   
   

   

m

1
=

y

y y
,
n

,

x

n

)

n

1
   

   
   
   

32

machine learning

functional models

srihari

na  ve bayes classifier

y

i

e
v
t
a
r
e
n
e
g

x

x1
p(y,

x

)

xm
y
|
)

p x
(
m

m

=    
p(y)

m

1
=

y

x

hidden markov model

y1

yn

yn

x1
=    y x
p(
,

)

n

n

1
=

xn

p y
(

n

|

y

n

1
   

)

p

(

x

n

|

xn
y

)

n

exp

   
   
   
exp

m

   
m
1
=
m
   
   
   

m

   

1
=

f
  
m m

x
y
( , )

f
  
m m

(

y

   
   
   
x
', )

p

(

y x

|

)

=

   

y

'

   
   
   

exp

   
   
   
exp

m

   
m
1
=
m
   
   
   

m

   

1
=

f
  
m m

(

y y
n

,

n

1
   

,

x

n

)

f
  
m m

(

y

n

',

y

n

1
   

',

   
   
   
x

n

)

   
   
   

p y
(

|

x

)

=

   

y

'

id28

conditional random field

33

i

i

e
v
t
a
n
m
r
c
s
d

i

i

machine learning

srihari

nlp: id52

for a sequence of words w = {w1 ,w2 ,..wn }  find syntactic 
labels  s for each word:

w =  the  quick   brown   fox        jumped   over   the      lazy     dog
s  =  det  verb     adj    noun-s   verb-p   prep  det     adj    noun-s

model error
id48 5.69%
5.55%
crf

baseline is already 90%

    tag every word with its most  frequent tag
    tag unknown words as nouns

per-word error rates for id52 on 
the id32

34

machine learning

srihari

table extraction

to label lines of text document:
whether part of table and its role in table.

finding tables and extracting information is necessary 
component of data mining, question-answering and ir 
tasks.

id48
89.7%

crf
99.9%

35

machine learning

srihari

id66

     precursor to full parsing or information extraction
     identifies non-recursive cores of various phrase types in text

     input: words in a sentence annotated automatically with pos tags
     task: label each word with a label indicating 

     word is outside a chunk (o), starts a chunk (b), continues a chunk (i)

np chunks

crfs beat all reported single-model np chunking results on standard 
evaluation dataset

36

machine learning

handwritten word recognition

srihari

given word image and lexicon, find most probable 
lexical entry
algorithm outline

    oversegment image 

segment combinations are potential characters

    given y =  a word in lexicon, s = grouping of segments, 

x = input word image features

    find word in lexicon and segment grouping that maximizes
p(y,s | x),
crf model

yp
(

,x|

)
  

y

e
);x,
(
    
   =
e
);x,'
    

y

y

(

'

)
    

y
;x,(

=

m

   

j

1
=

   
      
   

yja
,(

j

;x,

s
  

+

   

i
ekj
),(
   

yykj
,
,(

,

j

,x,

t
  

)

k

   
      
   

where yi   (a-z,a-z,0-9},    : model parameters
association potential (state term)
yja
,(

yj
,(

)x,

;x,

s
  

=

(

)

)

f

s

s
   
  
ij

j

i

j

   

i

interaction potential
yykji
f
,
,(
(

;x,

t
  

=

)

,

j

k

   

i

t

i

ykj
,(
,

,

j

y

k

)x,

t
   
  
ijk

)

i

i

n
o
s
c
e
r
p

i

i

n
o
s
c
e
r
p

1

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

0

sdp
crf

crf

segment-dp

37

80

100

120

20

40

60

word recognition rank

wr rank

machine learning

srihari

document analysis (labeling 

regions) error rates
crf

neural 
network
2.35%

naive 
bayes
11.54%

1.64%

5.19%

20.90%

25.04%

machine 
printed text
handwritten 
text
noise

10.20%

15.00%

total

4.25%

7.04%

12.23%

12.58%

38

machine learning

5. advantage of crf over other models

srihari

     other generative models

    relax assuming conditional independence of observed data 

given the labels

    can contain arbitrary feature functions

     each feature function can use entire input data sequence. id203 of 

label at observed data segment may depend on any past or future data 
segments.

     other discriminative models

     avoid limitation of other discriminative markov models 

biased towards states with few successor states.

     single exponential model for  joint id203 of  entire 

sequence of labels given observed sequence.

     each factor depends only on previous label, and not future 

labels. p(y | x) = product of factors, one for each label. 

39

machine learning

srihari

disadvantages of discriminative 

classifiers
     lack elegance of generative
    priors, structure, uncertainty

     alternative notions of penalty functions, 

id173, id81s

     feel like black-boxes

    relationships between variables are not explicit 

and visualizable

40

machine learning

srihari

bridging generative and discriminative

     can performance of id166s be combined 
elegantly with flexible bayesian statistics?

     maximum id178 discrimination marries 

both methods
    solve over a distribution of parameters (a 

distribution over solutions)

41

machine learning

srihari

6. summary 

     machine learning algorithms have great practical value in a 

variety of application domains
     a well-defined learning problem requires a well-specified task, 

performance metric, and source of experience

     generative and discriminative methods are two-broad 

approaches: 
     former involve modeling, latter directly solve classification

     generative and discriminative method pairs

     na  ve bayes and id28 are a corresponding pair for 

classification

     id48 and crf are a corresponding pair for sequential data

     crf performs better in language related tasks
     generative models are more elegant, have explanatory power
42

machine learning

srihari

7. references

1. 
2. 

3. 

4. 

5. 

6. 

7. 

t. mitchell, machine learning, mcgraw-hill, 1997
c. bishop, pattern recognition and machine learning, springer, 
2006
t. jebarra, machine learning: discriminative and generative, 
kluwer, 2004
r.o. duda, p.e. hart and d. stork, pattern classification, 2nd ed, 
wiley 2002
c. sutton and a. mccallum, an introduction to conditional 
random fields for relational learning
s. shetty, h. srinivasan and s. n. srihari, handwritten word 
recognition using crfs, icdar 2007
s. shetty, h.srinivasan and s. n. srihari, segmentation and 
labeling of documents using crfs, spie-drr 2007

43

