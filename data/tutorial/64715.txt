embeddings, nn, deep learning, 
id65     in nlp 

 

horacio rodr  guez 

talp  

embeddings  

 1 

outline 

introduction 

   
    approaches to semantics 
    semantic spaces 
    nn models for nlp 
    embeddings 
    embedding of words 
    embedding of more complex units 
    deep learning 
    applications 
    conclusions 

 

embeddings  

 2 

introduction 

    embedding 

    representing meaning 

    words meaning 
    more complex units meaning 

    phrases, id165s, clauses, sentences, documents 

    approaches to semantics 

    obtaining meaning  

    id29 
    srl 
    lexical semantics 
 

embeddings  

 3 

introduction 

    what to read 

       embedding methods for natural language processing   , [bordes, 

weston, 2014] tutorial at emnlp.  

       statistical language models based on neural networks   , [mikolov, 

2012], google,  

    yoav goldberg  (2015)    a primer on neural network models for 

natural language processing    

    aur  lien bellet (2013)    tutorial on metric learning    
    richard socher   s  tutorial on deep learning using theano 

    http://deeplearning.net/tutorial/ 
 
 

embeddings  

 4 

introduction 

    what to read 

       deep learning   , geoffrey hinton tutorial on ipam 2012.  
       deep learning, id114, energy-based models, 

id170    from yann lecun.  

       deep learning for nlp (without magic)    tutorial of socher and 

manning in naacl2013 

       survey on embeddings, working notes   , horacio rodr  guez, 2016 

    https://drive.google.com/open?id=0b9kpaxxcrpn5ttlxevdrmvpjce0 

    http://deeplearning.net/reading-list/tutorials/ 

 

 

embeddings  

 5 

introduction 

    what to read 
    tutorial talp 
    deep learning for speech and language  

https://telecombcn-dl.github.io/2017-dlsl/ 

 

embeddings  

 6 

introduction 

my initial guessing  

embeddings  

 7 

approaches to semantics 

    id152 

    semantic complex entities can be built from its 

simpler constituents 
    ted briscoe (2011) introduction to formal semantics for 

natural language 

    gennaro chierchia and sally mcconnell-ginet. (2000) 
meaning and grammar - an introduction to semantics 
(second edition). the mit press, 2000. 

embeddings  

 8 

approaches to semantics 

    id65 

 

distributional hypothesis: the meaning of a word can be 
obtained from the company it has. 
 
    nice introduction in turney & pantel, 2010. 
    m. baroni and a. lenci. 2010. distributional memory: a 

general framework for corpus-based semantics.  

    m. baroni and r. zamparelli. 2010. nouns are vectors, 

adjectives are matrices: representing adjective-noun 
constructions in semantic space.  

    william blacoe and mirella lapata. 2012.  a 

comparison of vector-based representations for 
semantic composition.  
 
 
 

embeddings  

 9 

semantic spaces 

    semantic spaces 

 

    most recent effort towards solving this problem concern latent 

factor models because they tend to scale better and to be more 
robust w.r.t. the heterogeneity of multi-relational data. 
 

    these models represent entities with latent factors (usually low-

dimensional vectors or embeddings) and relationships as 
operators destined to combine them.  
 

    operators and latent factors are trained to fit the data using 

reconstruction, id91 or ranking costs. 
 

embeddings  

 10 

semantic spaces 

    ways of organizing the semantic entities 

    id65 
    vectors, matrices, tensors 
    sometimes different representations depending on pos 

    units to be represented 

    words, 
    senses 
    phrases 
    sentences 
    relations 

    rdf triples 

embeddings  

 11 

semantic spaces 

embeddings  

 12 

semantic spaces 

embeddings  

 13 

semantic spaces 

embeddings  

 14 

semantic spaces 

embeddings  

 15 

semantic spaces 

embeddings  

 16 

semantic spaces 

    popular similarities: 

 

embeddings  

 17 

semantic spaces 

    metric learning: 

 

embeddings  

 18 

semantic spaces 

    metric learning: 

 

embeddings  

 19 

semantic spaces 

    basic formulation: 

 
 
 
 
 
 
 
 

look at bellet tutorial for many other alternatives 

embeddings  

 20 

semantic spaces 

    metric learning implementations in python: 

    https://pypi.python.org/pypi/metric-learn 
    large margin nearest neighbor (lmnn) 
    information theoretic metric learning (itml) 
    sparse determinant metric learning (sdml) 
    least squares metric learning (lsml) 
    neighborhood components analysis (nca) 
    local fisher discriminant analysis (lfda) 
    relative components analysis (rca) 

 
 
 
 

embeddings  

 21 

semantic spaces 

    vector space model (vsm) 

 

    similarity of documents 
    term    document matrix 

    similarity of words 

    word     context matrix 
    similarity of relations 
    pair     pattern matrix 

embeddings  

 22 

semantic spaces 

    reducing the size of semantic spaces 

    id91 of similar words 

    point-wise mutual information (pmi) 
    2nd order pmi 

    id84 

    id45, lsi (lsa, pca,    ) 

    deerwester et al, 1988 

    plsi 

    hofman, 1999 

    id44, lda 

    blei et al, 2003 

    embeddings 

 
 

embeddings  

 23 

lsi 

    spectral theorem 

    conditions for a matrix a to be diagonalizable 
    spectral  decomposition of a 

    diagonalization  

    if a is normal (and thus hermitic  and thus if real 

symmetric) then  a decomposition  exists: 
    a = u    ut 
       is diagonal, being its entries the eigenvalues of a 
    u is unitary being its  columns the eigenvectors of a 

    singular values decomposition (svd) 

 
 
 

embeddings  

 24 

lsi 

    for a square matrix a: 
    ax = b 

 
 

a 

 
x 
 

    

 
b 
 

ax =   x  
  where x is a vector (eigenvector), and    a 

scalar (eigenvalue) 

 
 
 

 

embeddings  

 25 

lsi 

    svd decomposes: 

    a = u    vt 

       is diagonal, being its entries the eigenvalues of a 
    u,v left and right singular vector matrices 
    u: matrix of eigenvectors of y=aat 
    v: matrix of eigenvectors of y=ata 
       matrix m x n 
    u matrix m x r 
    v matrix n x r 
       matrix r x r, r     min (m,n) 

 
 
 

 
 

embeddings  

 26 

lsi 

    svd decomposes exactly: 

    a = u    vt 

    for id84 we take only the k 

highest eigenvalues (and the corresponding 
eigenvectors), i.e. uk   k vk 
    k << r 
    set k that minimizes the frobenius norm (l2) of a-a   . 

    applying uk  kvk

 

t we obtain a    that approximates a 

 
 

 
 

embeddings  

 27 

lsi 

    computing distances: 

    supose a is a term x document matrix 
    in a 

    comparing two terms: dot product of two rows in a 
    comparing two documents: dot product of two columns in a 
    comparing a term and a document: entry in a 

    in a    

    comparing two terms: dot product of two rows in uk   k 
    comparing two documents: dot product of two rows in vk   k 
    comparing a term and a document: entry in a    

 

 

 

 
 

embeddings  

 28 

plsi 

dd d 

t 

w 

embeddings  

 29 

lda 

    id44: 

    in the lda model, the topic mixture proportions for 

each document are drawn from some distribution, i.e. 
a dirichlet prior. 
    dirichlet is a multivariate generalization of beta distribution. 

    plsi can be considered a lda under an uniform 

dirichlet prior distribution 

    the distribution is based on multinomials.  that is, k-

tuples of non-negative numbers that sum to one. 
 

 

 

embeddings  

 30 

lda 

the beta distribution is a family of continuous id203 distributions defined on the 
interval [0, 1] parametrized by two positive shape parameters, 
 denoted by    and   , that appear as exponents of the random variable and control the 
shape of the distribution. 
 

embeddings  

 31 

lda 

the dirichlet distribution, often denoted dir  (    ), is a family of continuous 
multivariate id203 distributions parameterized by a vector    of positive reals. it is 
a multivariate generalization of the beta distribution 

embeddings  

 32 

lda 

    lda model 

 
 
 

 

    dir(  ) per document topic distribution 
    dir(  ) per word topic distribution 
      i topic distribution for document i 
    zij topic for the jth word in document i according to 

    multinomial(  i) 
    wij specific word 
  

embeddings  

 33 

nn models for nlp 

    feed-forward neural networks (ffnm) 
    convolutional nn (id98) 
    recurrent nn (id56) 
    recursive nn (reid98) 
    long short time models (lstm) 

embeddings  

 34 

nn models for nlp 
    feed-forward neural networks 

    general structure for an nlp classification system 
    1 extract a set of core linguistic features f1,    , fk 
that are relevant for predicting the output class. 

    2. for each feature fi of interest, retrieve the 

corresponding vector v(fi). 

    3. combine the vectors (either by concatenation, 
summation or a combination of both into an input 
vector x. 

    4. feed x into a non-linear classifier (feed-forward 

neural network). 

embeddings  

 35 

nn models for nlp 
    feed-forward neural networks 
    collobert & weston, bengio & schwenk 
    dense vectors vs. one-hot representations 
    variable number of features: continuous bag of 

words 
    cbow 

 
 
 

    weighted cbow 

 

    fully connected (affine) layer  

embeddings  

 36 

nn models for nlp 

embeddings  

 37 

nn models for nlp 

n = |v| 
n  context size 

embeddings  

 38 

nn models for nlp 

    feed-forward neural networks 

    simple id88 

 
 
 
 

    1-layer multi layer id88 (mlp1) 

 
 
 
 
 

 

embeddings  

 39 

nn models for nlp 

    feed-forward neural networks 

    2-layer mlp 

 
 
 
 
 
 
 

 
 
 
 

 

 

embeddings  

 40 

nn models for nlp 

    non-linearities 

    sigmoid 

 

    tanh 

 
 
 

    hard tanh 

 
 

    rectifier (relu) 

 
 

 

embeddings  

 41 

nn models for nlp 

    output transformations 

    softmax 

 
 
 

 

embeddings  

 42 

nn models for nlp 

    embedding layers 
    c(.) is a function from core features to an input vector. 
   

it is common for c to extract the embedding vector associated with 
each feature, and concatenate them 

 
 

 

embeddings  

 43 

nn models for nlp 

    embedding layers 
    another common choice for c is to sum the embedding vectors (this 
assumes the embedding vectors all share the same dimensionality) 

 

 

embeddings  

 44 

nn models for nlp 

    embedding layers 
    sometimes id27s v(fi) result from an    embedding layer" 

or    lookup layer". consider a vocabulary of |v| words, each 
embedded as a d dimensional vector. the collection of vectors can 
then be thought of as a |v| x d embedding matrix e in which each 
row corresponds to an embedded feature.  

embeddings  

 45 

nn models for nlp 

    id168s 

    hinge (binary) 

 

    hinge (multiclass) 

 

    log loss 

 

    categorical cross-id178 loss 

 

    ranking losses 

embeddings  

 46 

nn models for nlp 

    learning 

embeddings  

 47 

nn models for nlp 

    basic convolution & pooling 

    predictions based on ordered sets of items (e.g. the 
sequence of words in a sentence, the sequence of 
sentences in a document and so on).  

    apply a non-linear (learned) function over each 
instantiation of a k-word sliding window over the 
sentence. this function (also called filter) transforms a 
window of k words into a d dimensional vector that 
captures important properties of the words 

embeddings  

 48 

nn models for nlp 

embeddings  

 49 

nn models for nlp 

    basic convolution & pooling 

embeddings  

 50 

nn models for nlp 

embeddings  

 51 

nn models for nlp 

    basic convolution & pooling 

    the m vectors are then combined using a max pooling 

layer, resulting in a single dconv dimensional vector c. 
 
 
 
 

    we can split the vectors pi into distinct groups, apply 

the pooling separately on each group, and then 
concatenate the resulting dconv vectors. 

embeddings  

 52 

nn models for nlp 

    why not cycles?  

    recurrent nn (id56) 

    + more expressive models 
     - more costly to learn 

 

 

embeddings  

 53 

nn models for nlp 

    recurrent nn (id56) 

    representing arbitrarily sized structured inputs 

(e.g.sentences) in a  fixed-size vector, while paying 
attention to the structured properties of the input. 

 

embeddings  

 54 

nn models for nlp 

    recurrent nn (id56) 

embeddings  

 55 

nn models for nlp 

    simple id56 architecture 

    the state at position i is a linear combination of the 

input at position i and the previous state, passed 
through a non-linear activation (commonly tanh or 
relu). the output at position i is the same as the 
hidden state in that position.  

embeddings  

 56 

nn models for nlp 

    recurrent nn (id56) 

same parameterization 

embeddings  

 57 

nn models for nlp 

   10k-200k 

id56 

without w bigram nn lm 
s(t) = f(uw(t) + ws(t-1)) 
f  sigmoid or tanh 
y(t) = g(vs(t)) 
g softmax 

   50-1000 

embeddings  

 58 

nn models for nlp 

factorization of the output layer, 
c(t) is the class layer. 
assignment word to class 

embeddings  

 59 

nn models for nlp 

    recurrent nn (id56) 

same parameterization 

embeddings  

 60 

nn models for nlp 

    multi-layer (stacked) id56s 

embeddings  

 61 

nn models for nlp 

    bidirectional-id56 (bi-id56) 

embeddings  

 62 

nn models for nlp 

    long short-term models (lstm) 

embeddings  

 63 

nn models for nlp 

    introducing additional dynamic memory 

    stack-augmented id56 

    joulin, mikolov, 2015 

    other structured linear memories 
    queues, deques, double linked lists 
    grefenstette at al, 2015 
    id63s 

    graves et al, 2014 
    memory networks 

    weston et al, 2015, sukhbaatar et al, 2015 

 

embeddings  

 64 

nn models for nlp 

stack-augmented id56  (grefenstette et al, 2015) 

 

embeddings  

 65 

nn models for nlp 

stack-augmented id56  (grefenstette et al, 2015) 

 

embeddings  

 66 

nn models for nlp 

id63s nmm  (graves et al, 2014) 

 

embeddings  

 67 

nn models for nlp 

    mn (sukhbaatar et al, 2015) 

embeddings  

 68 

nn models for nlp 

    mn (sukhbaatar et al, 2015) 

embeddings  

 69 

nn models for nlp 

    long short-term models (lstm) 

embeddings  

 70 

embeddings 

    what is an embedding? 

    a mapping of an element of a textual space (a word, a phrase, a 

sentence, an entity, a relation, a predicate with or without 
arguments, an rdf triple, an image , etc.)  into an element of a, 
frequently low dimensional, vectorial space.  

    within nlp, initial embedding models were first applied to words, 
basically for building accurate language models (e.g. asr, mt).   

    lexical embeddings can serve as useful representations for 

words for a variety of nlp tasks, but learning embeddings for 
phrases or other complex linguistic units, can be challenging but 
needed for many nlp tasks. 

    while separate embeddings are learned for each word, this is 

infeasible for every phrase. |v|     105 

 

embeddings  

 71 

embeddings of words 

    id27s 

    a word representation is a mathematical object 
associated with each word, often a vector. each 
dimension   s value corresponds to a feature and might 
even have a semantic or grammatical interpretation, 
i.e. each dimension represents a syntactic or semantic 
property of the word, so we can call it a word feature. 

embeddings  

 72 

embeddings of words 

    id27s 

    conventionally, supervised lexicalized nlp 

approaches take a word and convert it to a symbolic 
id, which is then transformed into a feature vector 
using a one-hot representation. the feature vector 
has the same length as the size of the vocabulary, 
and only one dimension is on. however, the one-hot 
representation of a word suffers from data sparsity. 
for facing this problem several approaches have been 
followed.  

x  = [0 0 0     1     0 0 0] 

embeddings  

 73 

embeddings of words 

    learning id27s 

into a vector of reals.  

    the simpler id27 system consists of mapping a word token 

     w       w         n, where n is the dimension of the embedding space.  

    [mikolov et al, 2013] id97 
    [pennington et al, 2014] consider that the two main model families for 

learning word vectors are:  

    global id105 methods, such as lsi, lda, and their derivates 

(e.g. plsi, slda) 

    local context window methods, such as the skip-gram or the cbow 

models of [mikolov et al. 2013]. id97 

    [pennington et al, 2014] global log-biid75 model, glove. 

embeddings  

 74 

embeddings of words 

    learning id27s 

    the skip-gram and continuous bag-of-words (cbow) models of 

mikolov et al. (2013) propose a simple single-layer architecture 
based on the inner product between two word vectors. 

    mnih and kavukcuoglu (2013) also proposed closely-related 

vector log-bilinear models, vlbl and ivlbl 

    levy et al. (2014) proposed explicit id27s based on a 

ppmi metric. 

    all these systems use unsupervised learning 
    an interesting system, using semi-supervised learning 

approach, is [turian et al, 2010]. this system combines different 
word representations and use also id91 techniques. 

embeddings  

 75 

embeddings of words 

    learning id27s 

    a word and its context is a positive training example, a 

random word in that same context gives a negative 
training example. 

    each word is associated with a n-dimensional vector 

(context, id165, sentence,    ) 

    id27 matrix (word + context) 

     l         nx|v| 

embeddings  

 76 

embeddings of words 

    shallow (word level) nlp 

tasks 
    id52, ner 
    c&w, senna 

    similar to word vector learning 

but adding a softmax 
(supervised) layer 

    training with id26 

embeddings  

 77 

embeddings of words 

    some examples downloadble 

    glove, [pennington et al, 2014],  
    http://nlp.stanford.edu/projects/glove/ 

    [turian et al, 2010] 

    http://metaoptimize.com/projects/wordreprs/ 

    senna, [collobert, weston, 2008 

    http://ml.nec-labs.com/senna/ 

    hpca, [lebret, collobert, 2014] 

    http://www.lebret.ch/words/ 

    hlbl, [mnih, hinton, 2007] 

    http://www.cnts.ua.ac.be/conll2003/ner/ 

    id97, [mikolov et al, 2013] 

    code.google.com/p/id97 
 

 
 

 

embeddings  

 78 

embeddings of phrases 

    phrase embeddings 

    usually approached following two lines: 

    using pre-defined composition operators (mitchell and lapata, 

2008), e.g., component-wise sum/multiplication, we learn 
composition functions that rely on phrase structure and context.  

    using matrices/tensors as transformations (socher et 

al., 2011), (socher et al., 2013).  
    a popular system is socher   s use of id56, which recursively 
computes phrase embeddings based on the binary sub-tree 
associated with the phrase with matrix transformations. for a 
phrase p = (w1,w2). the model then computes the phrase 
embedding ep as:  

embeddings  

 79 

embeddings of phrases 

    neural sentence models 

    problem of variable sentence length. 
    represent the variable length sentence as a fixed-length vector. 
these models generally consist of a projection layer that maps 
words, subword units or id165s to high dimensional 
embeddings (often trained beforehand with unsupervised 
methods); the latter are then combined with the different 
architectures of neural networks, such as neural bag-of-words 
(nbow), recurrent neural network (id56), recursive neural 
network (reid98), convolutional neural network (id98) and so on. 
 

embeddings  

 80 

embeddings of phrases 

    convolutional sentence model  

    [hu et al, 2015] is a new convolutional architecture for modeling 

sentences. it takes as input the embedding of words (often 
trained beforehand with unsupervised methods) in the sentence 
aligned sequentially, and summarize the meaning of a sentence 
through layers of convolution and pooling, until reaching a fixed 
length vectorial representation in the final layer.  

    this is an interesting example of deep learning architecture. 

 
 

embeddings  

 81 

embeddings of phrases 
hu et al   s overall architecture of the convolutional sentence model 

non linear function (tanh) 

embedding of the sentence 

embeddings  

 82 

embeddings of more complex units 

    some examples downloadble 

    phrases: fct, [yu,dredze, 2015] ,  

    https://github.com/gorov/fct_phrasesim_tacl 

    entities, triples, questions: [bordes et al, 2014] 
    words, entities, triples, wn synsets, [bordes et al, 2012] 
    multi-relational data, entities, relationships: transe, [bordes et 

al, 2013] 

    structured embeddings of kbs, wn, freebase:  [bordes et al, 

2011] 

    sentences: fcm, [yu et al, 2014]  

    https://github.com/gorov/fcm_nips_workshop/ 

    sentences: lrfcm, [yu et al, 2015]  

    https://github.com/gorov/ere_re 

 

 

 
 

embeddings  

 83 

embeddings of more complex units 

    recursive autoencoders (rae) 

    from socher et al, 2011, used for paraphrase detection (see later) 
    a rae is a multilayer id56 with input = output 
    it learns feature representations for each node in the tree such that the 
word vectors underneath each node can be recursively reconstructed.  

    reconstruction =  decoder(encoder(input)) 

 

 
 

a = tanh(wx+b) 
 
x    = tanh(wta+c) 
 
cost of reconstruction 
 = ||x        x||2 

 

 

 

embeddings  

 84 

embeddings of more complex units 

    stacking rae 

    rae can be stacked to form highly non-linear representations 

 

 

 

 
 

 

embeddings  

 85 

embeddings of more complex units 

embeddings  

 86 

deep learning 

    deep learning motivation 
    from socher & manning, 2013 
    frequently ml  uses handcrafted  representations as features, in a way 

that ml task consists of optimizing weights. 

    most practical nlp ml methods use supervised approaches 

        need of labeled training data 
    but almost all data is unlabeled 

    representation learning attempts to automatically learn good features or 

representations 

    deep learning attempts to learn multiple levels of representation of 

increasing complexity/abstraction 

    at least as pre-training or in the most superficial layers unsupervised 

learning is used in deep learning  

 

 
 

 

 

 

embeddings  

 87 

deep learning 

    deep learning basics 

    from hinton, 2012 
    greedy learning 

    because of overfitting many parameters 

        id173 
    diffusion of gradient 

    learning multilayer generative models of unlabelled data by 

learning one layer of features at a time 

    hinton   s layer-based pre-training 

 

 
 

 

 

 

embeddings  

 88 

deep learning 

    deep learning basics 

    it is easy to generate an  unbiased example at the  

leaf nodes, so we can see what kinds of data the 
network believes in. 

    it is hard to infer the posterior distribution over all  possible 

configurations of hidden causes.  

    it is hard to even get  a  sample from the posterior.  
    we need a way of learning one layer at a time that takes  into 
account  the fact that we will be learning more  hidden layers 
later.  

    possibility: learning feature extraction layers and classifier layer 

into two independent steps 

 

 

 

 

embeddings  

 89 

deep learning 

    energy-based learning 

    from lecun, 2012 
    measures the compatibility between an observed variable x and 
a variable to be predicted y through an energy function e(y,x).  

     search for the y that  minimizes the energy within a set 

 

 

 

 
 

 

embeddings  

 90 

deep learning 

energy-based learning 

embeddings  

 91 

deep learning 

    energy-based learning 

    family of energy functions 
    training set 
    id168al / loss 

function 

 

 

embeddings  

 92 

deep learning 

id168s 

embeddings  

 93 

deep learning 

    deep learning results in nlp tasks 

    good results in id38 & machine translation 

    mikolov   s id97 

    good results in shallow (linear) nlp tasks (id52, ner, 

co-reference, srl,    ) 

    c&w, senna 

    state of the art results in structure building tasks (not better) 

    parsing 
    relational learning 
    id29 

    good results in a wide range of applications 

    q&a 
    paraphrase detection 
    kb population (el, sf) 
        

 

 

 

embeddings  

 94 

deep learning 

embeddings  

 95 

deep learning 

z = wx+b 
a=f(z) 

the neural activations a can be used to 
compute some function, for instance score 
in lm. 
 
score(x) = uta     r 

embeddings  

 96 

deep learning 

    alternatives 

    id56 
    lstm 
    id21 

    yosinski et al 2014 
    learn a nn nb from an existing data set db, then transfer into nt  

 

 

 
 

 

embeddings  

 97 

 

 

 

 

applications 

    qa 

    bordes 2014   s approach is based on converting questions to 

(uninterpretable) embeddings which require no pre-defined 
grammars or lexicons and can query any kb independent of its 
schema.  

    he focuses on answering simple factual questions on a broad 

range of topics, more specifically, those for which single kb 
triples stand for both the question and an answer. 

    automatically generating questions from kb triples and 

treating this as training data 

    supplementing this with a data set of question collaboratively 

marked as paraphrases but with no associated answers. 
 

 

 

 
 

embeddings  

 98 

applications 

patterns for generating questions from reverb triples 

embeddings  

 99 

applications 

    qa 

    embedding reverb 
    the model ends up learning vector embeddings of symbols, 

either for entities or relationships from reverb, or for each word 
of the vocabulary. the embeddings are used for scoring the 
similarities of a question q and a triple t, i.e. learning the function 
s(q,t).   

    it consists of projecting questions, treated as a bag of words (and 

possibly id165s as well), on the one hand, and triples on the 
other hand, into a shared embedding space and then computing 
a similarity measure (the dot product in this paper) between both 
projections.  
 

 

 
 

 

embeddings  

 100 

applications 

    qa 

    scoring function: s(q, t) = f(q)tg(t) 

    f(  ) a function mapping words from questions into rk, f(q) = 

vt  (q).  

will be learned,. 

    v is the matrix of rnv  k containing all id27s v that 

      (q) is the (sparse) binary representation of q (    {0, 1}nv) 

indicating absence or presence of words.  

    similarly, g(  ) is a function mapping entities and relationships 

from kb triples into rk, g(t) = wt  (t).  

 

 

 

 

 
 

 

embeddings  

 101 

applications 

    qa 

    scoring function: s(q, t) = f(q)tg(t) 

    w is the matrix of rne  k containing all entity and relationship 

embeddings w, that will also be learned.  

      (t) is the (sparse) binary representation of t (    {0, 1}ne) 

indicating absence or presence of entities and relationships.  

    an entity does not have the same embedding when appearing 

in the left-hand or in the right-hand side of a triple.  

 

 

 

 

 
 

 

embeddings  

 102 

applications 

    rae for paraphrase detection 

    from socher et al, 2011 
    rae learns feature representations for each node in the tree such that 

the word vectors underneath each node can be recursively 
reconstructed.  

    these feature representations are used to compute a similarity matrix 

that compares both the single words as well as all nonterminal node 
features in both sentences. 

     in order to keep as much of the resulting global information of this 
comparison as possible and deal with the arbitrary length of the two 
sentences,  a new dynamic pooling layer which outputs a fixed-size 
representation. any classifier such as a softmax classifier can then be 
used to classify whether the two sentences are paraphrases or not. 

 

 
 

 

 

 

embeddings  

 103 

applications 

paraphrase detection with rae 

embeddings  

 104 

applications 

paraphrase detection with rae 

embeddings  

 105 

applications 

paraphrase detection with rae 

representing  a sentence as an ordered list of these vectors (x1,    ,  xm) 
this word representation is better suited for raes than the binary 
number  
representations used in previous related models. 
a tree is given for each sentence by a parser.  
 

embeddings  

 106 

applications 

paraphrase detection with rae 

the binary parse tree for this input is in the form of branching 
triplets of parents with children: (p     c1c2).  
each child can be either an input word vector xi or a nonterminal 
 node in the tree.  
for both examples in last slide, we have the following triplets: 
((y1     x2x3), (y2     x1y1)),    x,  y     rn.  

embeddings  

 107 

applications 

paraphrase detection with rae 

compute the parent representations. 
p = y1 is computed from the children (c1, c2) = (x2, x3) by one standard 
neural network  layer: p = f(we[c1; c2] + b),  
where [c1; c2] is simply the concatenation of the two children, 
f an element-wise activation function and we     rnx2n  (the encoding 
matrix).  
how well this n-dimensional vector represents its direct children? 
decode their vectors in a reconstruction layer and then compute the 
euclidean distance between the original input and its reconstruction. 
 

embeddings  

 108 

applications 

parsing using matrix  
vector id56,  
socher et al, 2011 

embeddings  

 109 

applications 

parsing using matrix  
vector id56,  
socher et al, 2011 

embeddings  

 110 

conclusions 

    embeddings 

    good for words, lm, mt 

    billion of words for learning models 
    probably better than lsi; lda,     
    combining unsupervised learning with task-dependent supervised layers 
    not so good for composition of words into more complex units 

    convolution and pooling seem to be rather na  ve approaches for dealing with 

word order and relevance 

    socher   s approaches seem to go in the good direction 

    including  additional information beyond words: pos, parse, synsets,     

    nice to embed kb 

    freebase, dbpedia, bioportal,     
    other rdf (why not owl) modeled kb 

 

 

 

 
 

embeddings  

 111 

conclusions 

    deep learning 

    good results in many nlp tasks 
    good learning capabilities 

    big models 
    efficient use of computer resources, gpu,     

    difficult to interpret 
    magic, miracle ??? 
    can we get conclusions from a successful model ?? 

    greedy learning of layers is ok?? 
    how many layers ?? 
    how many neurons in each layer ?? 
    how about not nn-based models (deep id114,    ) ?? 

 

 
 

 
 

embeddings  

 112 

