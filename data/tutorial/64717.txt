learning to do

supervised learning: given data, 
predict labels

unsupervised learning: given data, 
learn about that data

id23: given data, 
choose action to maximize expected 
long-term reward 

environment

reward

action

state

agent

episode: sequence of states and actions

transition function:

we want to find a policy 

      which maximizes 

policy learning

value learning

expected future rewards starting at 
state si, choosing action ai, and then 
following policy   

maximum expected future rewards starting at state si, 
choosing action ai , and then following an optimal policy   *

the max future reward for taking action at is the 
current reward plus the next step   s max future 

reward from taking the best next action a   

but    how large is          ?

# states: ~296   60   60
# actions: 3
# q values: ~2111

1957 - 2013 

:(

enter the deep

mnih et al, 2013

features for 
estimating q

    whether the paddle can reach 

the ball

    # remaining blocks
    how are the blocks spatially 

arranged?

enter the deep

mnih et al, 2013

define approximate q* function

and choose    to minimize

we need to balance 

exploration and exploitation 

 

 

% improvement over professional player 
mnih et al, 2013

try it out!

http://selfdrivingcars.mit.edu/deeptraffic/

( kudos to lex fridman & the 6.s094 team! )

the problems with id24

    restrictive assumptions

    handles long horizons poorly

    requires a simulator

lstm id56s!

image 
(state)

q-value of 

action

conv

fc

lstm 

the problems with id24

    restrictive assumptions

    handles long horizons poorly

    requires a simulator

 

the problems with id24

    restrictive assumptions

    handles long horizons poorly

    requires a simulator

 

levine et al, 2016

 

 

