deep generative models

ishaan gulrajani

mit 6.s191 | intro to deep learning | iap 2017

learning to generate

images

mit 6.s191 | intro to deep learning | iap 2017

anh et al. 2016

learning to generate

images 
speech

mit 6.s191 | intro to deep learning | iap 2017

van der oord et al. 2016

learning to generate

images 

speech 
handwriting

mit 6.s191 | intro to deep learning | iap 2017

graves 2013

learning to generate

images 

speech 

handwriting 
language

mit 6.s191 | intro to deep learning | iap 2017

vinyals et al. 2015

idea: learn to understand data through generation

mit 6.s191 | intro to deep learning | iap 2017

generative models for classification

mit 6.s191 | intro to deep learning | iap 2017

generative models for classification

mit 6.s191 | intro to deep learning | iap 2017

generative models for representation learning

chen et al. 2016

s  nderby et al. 2016

generative models for simulation, planning, reasoning

oh et al. 2015

setup

discriminative model: given n examples                                      
learn 

generative model: given n examples       , recover 

maximum-likelihood objective:  

generation: sampling from 

attempt 1: learn          directly 

attempt 1: learn          directly 
problem: we need to enforce that 

for most models (i.e. neural networks) this integral is 
intractable.

autoregressive models

factorize dimension-wise: 

build a    next-step prediction    model 

if x is discrete, network outputs a id203 for each possible value 

if x is continuous, network outputs parameters of a simple distribution 
(e.g. gaussian mean and variance)    or just discretize! 

generation: sample one step at a time, conditioned on all previous steps 

id56s for autoregressive id38

. . . 

<start>

yesterday

x1

it
x1

was
x2

pixelid56 (van der oord et al. 2016)
autoregressive id56 
over pixels in an image 

models pixels as 
discrete-valued (256-
way softmax at each 
step)

solution 1: autoregressive models

autoregressive models are powerful density estimators, but: 

sequential generation can be slow 

doesn   t closely reflect the    true    generating process 

tends to emphasize details over global data 

not very good for learning representations

autoencoders for representation learning

classifier loss

inputs

autoencoders for representation learning

classifier loss

inputs

autoencoders for representation learning

encoder

decoder

latent representation

inputs

reconstruction

autoencoders for representation learning

encoder

decoder

latent representation

bottleneck layer

inputs

reconstruction

autoencoders for representation learning

reconstruction loss forces hidden layer to represent 
information about the input 

bottleneck hidden layer forces network to learn a 
compressed latent representation

idea: compression as implicit generative modeling

mit 6.s191 | intro to deep learning | iap 2017

id5 (vaes)

generative extension of autoencoders which allow sampling and 
estimating probabilities 

   latent variables    with fixed prior distribution 

probabilistic encoder and decoder: 

trained to maximize a lower bound on log-id203: 

tom white 2016

problems with vaes

encoder and decoder   s output distributions are typically 
limited (diagonal-covariance gaussian or similar) 

this prevents the model from capturing fine details and leads 
to blurry generations 

andrej karpathy 2015

problems with vaes

encoder and decoder   s output distributions are typically 
limited (diagonal-covariance gaussian or similar) 

this prevents the model from capturing fine details and leads 
to blurry generations 
solution: use autoregressive networks in encoder and 
decoder

chris olah 2016

we have many orders of magnitude more data than labels; 
unsupervised learning is important. 

generating implausible scenes from captions

mansimov et al. 2015

mit 6.s191 | intro to deep learning | iap 2017

