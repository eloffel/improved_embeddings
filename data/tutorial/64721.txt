multimodal learning

victoria dean

mit 6.s191 | intro to deep learning | iap 2017

talk outline

    what is multimodal learning and what are the challenges?
    flickr example: joint learning of images and tags
    image captioning: generating sentences from images
    soundnet: learning sound representation from videos

talk outline

    what is multimodal learning and what are the challenges?
    flickr example: joint learning of images and tags
    image captioning: generating sentences from images
    soundnet: learning sound representation from videos

deep learning success in single modalities

mit 6.s191 | intro to deep learning | iap 2017

deep learning success in single modalities

mit 6.s191 | intro to deep learning | iap 2017

deep learning success in single modalities

mit 6.s191 | intro to deep learning | iap 2017

what is multimodal learning?
    in general, learning that involves multiple modalities
    this can manifest itself in different ways:

    input is one modality, output is another
    multiple modalities are learned jointly
    one modality assists in the learning of another
    ...

data is usually a collection of modalities

    multimedia web content

mit 6.s191 | intro to deep learning | iap 2017

data is usually a collection of modalities

    multimedia web content

    product id126s

mit 6.s191 | intro to deep learning | iap 2017

data is usually a collection of modalities

    multimedia web content

    product id126s

    robotics

mit 6.s191 | intro to deep learning | iap 2017

why is multimodal learning hard?

    different representations

images

real-valued,
dense

mit 6.s191 | intro to deep learning | iap 2017

text

discrete,
sparse

why is multimodal learning hard?

    different representations

    noisy and missing data

mit 6.s191 | intro to deep learning | iap 2017

how can we solve these problems?
    combine separate models for single modalities at a higher 

level

    pre-train models on single-modality data

    how do we combine these models? embeddings!

mit 6.s191 | intro to deep learning | iap 2017

pretraining

    initialize with the weights from another 

network (instead of random)

    even if the task is different, low-level 

features will still be useful, such as edge 
and shape filters for images

    example: take the first 5 convolutional 

layers from a network trained on the 
id163 classification task

mit 6.s191 | intro to deep learning | iap 2017

embeddings

    a way to represent data

    in deep learning, this is usually a high-dimensional vector

    a neural network can take a piece of data and create a 

corresponding vector in an embedding space

    a neural network can take a embedding vector as an input

    example: id27s

mit 6.s191 | intro to deep learning | iap 2017

id27s

    a id27: word     high-dimensional vector

    interesting properties

mit 6.s191 | intro to deep learning | iap 2017

embeddings

    we can use embeddings to switch between modalities!

    in sequence modeling, we saw a sentence embedding to switch 

between languages for translation

    similarly, we can have embeddings for images, sound, etc. that 

allow us to transfer meaning and concepts across modalities 

mit 6.s191 | intro to deep learning | iap 2017

talk outline

    what is multimodal learning and what are the challenges?
    flickr example: joint learning of images and tags
    image captioning: generating sentences from images
    soundnet: learning sound representation from videos

flickr tagging: task

images

text

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging: task

images

text

    1 million images from flickr
    25,000 have tags

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging: model

pretrain unimodal models and combine them at a higher level

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging: model

pretrain unimodal models and combine them at a higher level

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging: model

pretrain unimodal models and combine them at a higher level

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging: example outputs

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging: example outputs

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging: visualization

mit 6.s191 | intro to deep learning | iap 2017

flickr tagging

mit 6.s191 | intro to deep learning | iap 2017

talk outline

    what is multimodal learning and what are the challenges?
    flickr example: joint learning of images and tags
    image captioning: generating sentences from images
    soundnet: learning sound representation from videos

example: image captioning

mit 6.s191 | intro to deep learning | iap 2017

example: image captioning

 a

young

 girl

 asleep

w

__

 a

young

 girl

mit 6.s191 | intro to deep learning | iap 2017

human: a young girl asleep on 
the sofa cuddling a stuffed 
bear.

computer: a close up of a child 
holding a stuffed animal.

computer: a baby is asleep 
next to a teddy bear.

mit 6.s191 | intro to deep learning | iap 2017

human: a close up of two 
bananas with bottles in the 
background.

computer: a bunch of bananas 
and a bottle of wine.

mit 6.s191 | intro to deep learning | iap 2017

human: a view of inside of a car 
where a cat is laying down.

computer: a cat sitting on top 
of a black car.

mit 6.s191 | intro to deep learning | iap 2017

human: a green monster kite 
soaring in a sunny sky.

computer: a man flying through 
the air while riding a 
snowboard.

mit 6.s191 | intro to deep learning | iap 2017

caption model for neural storytelling

we were barely able to catch the breeze at 
the beach, and it felt as if someone stepped 
out of my mind. she was in love with him for 
the first time in months, so she had no 
intention of escaping. the sun had risen 
from the ocean, making her feel more alive 
than normal. she's beautiful, but the truth is 
that i don't know what to do. the sun was 
just starting to fade away, leaving people 
scattered around the atlantic ocean. i   d 
seen the men in his life, who guided me at 
the beach once more.
jamie kiros, github.com/ryankiros/neural-storyteller

mit 6.s191 | intro to deep learning | iap 2017

talk outline

    what is multimodal learning and what are the challenges?
    flickr example: joint learning of images and tags
    image captioning: generating sentences from images
    soundnet: learning sound representation from videos

soundnet

    idea: learn a sound representation from unlabeled video
    we have good vision models that can provide information about unlabeled 

videos

    can we train a network that takes sound as an input and learns object and 

scene information?

    this sound representation could then be used for sound classification tasks

mit 6.s191 | intro to deep learning | iap 2017

soundnet training

mit 6.s191 | intro to deep learning | iap 2017

soundnet training

loss for the sound id98:

mit 6.s191 | intro to deep learning | iap 2017

soundnet training

loss for the sound id98:

x is the raw waveform

y is the rgb frames 

g(y) is the object or 
scene distribution

f(x;(cid:7578)) is the output from 
the sound id98

mit 6.s191 | intro to deep learning | iap 2017

soundnet visualization

mit 6.s191 | intro to deep learning | iap 2017

soundnet visualization

what audio inputs 
evoke the maximum 
output from this 
neuron? 

mit 6.s191 | intro to deep learning | iap 2017

soundnet: visualization of hidden units

https://projects.csail.mit.edu/soundnet/

mit 6.s191 | intro to deep learning | iap 2017

conclusion

    multimodal tasks are hard

    differences in data representation
    noisy and missing data

mit 6.s191 | intro to deep learning | iap 2017

conclusion

    multimodal tasks are hard

    differences in data representation
    noisy and missing data

    what types of models work well?

    composition of unimodal models
    pretraining unimodally

mit 6.s191 | intro to deep learning | iap 2017

conclusion

    multimodal tasks are hard

    differences in data representation
    noisy and missing data

    what types of models work well?

    composition of unimodal models
    pretraining unimodally

    examples of multimodal tasks

    model two modalities jointly (flickr tagging)
    generate one modality from another (image captioning)
    use one modality as labels for the other (soundnet)

mit 6.s191 | intro to deep learning | iap 2017

