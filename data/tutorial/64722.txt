id4

id4

graham neubig

2016-12-6

 

 

1

id4

watashi wa cmu de kouen wo shiteimasu

i

am giving a

talk

at cmu

(end)

 

 

2

id4

...

p(e1=i|f) = 0.96

p(e1=talk|f) = 0.03
p(e1=it|f) = 0.01
p(e2=am|f, e1) = 0.9
p(e2=was|f, e1) = 0.09
p(e3=giving|f, e1,2) = 0.4 p(e3= talking|f, e1,2) = 0.3

estimate the id203 of next word
f =    watashi wa kouen wo shiteimasu   
e1=
e2=
e3=
e4=
e5=
e6=

p(e3=presenting|f, e1,2) = 0.03 ...
p(e4=my|f, e1,3) = 0.15
...

p(e5=talk|f, e1,4) = 0.4
p(e5=presentation|f, e1,4) = 0.3
p(e6=(end)|f, e1,5) = 0.8

p(e5=lecture|f, e1,4) = 0.15
p(e5=discourse|f, e1,4) = 0.1
p(e6=now|f, e1,5) = 0.1

p(e4=a|f, e1,3) = 0.8

...

...

...

 

 

i
am
giving

a
talk
(end)

3

in other words, translation can be

formulated as:

id4

a id203 model

i +1

p(e   f )=   i=1
a translation algorithm

p(ei   f ,e1

i   1)

i = 0
while ei is not equal to    (end)   :

i     i+1

  ei     argmaxe p(ei|f, e1,i-1)

 

we learn the probabilities with

neural networks!

 

4

id4

why is this exciting?

    amazing results:

within three years of invention, outperforming models
developed over the past 15 years, and deployed in
commercial systems

    incredibly simple implementation:

traditional machine translation (e.g. 6k lines of python)
id4 (e.g. 280 lines of python)

    machine translation as machine learning:

easy to apply new machine techniques directly

 

 

5

id4

predicting probabilities

 

 

6

id4

translation model     language model
translation model id203

p(e   f )=   i=1

i +1

p(ei   f ,e1

i   1)

forget the input f

language model id203

i +1

p(e)=   i=1

p(ei   e1

i   1)

problem: how to predict next word

p(ei   e1

i   1)

 

 

7

id4

predicting by counting

    calculate word strings in corpus, take fraction

p(wi   w 1   w i   1)=

c(w1   wi)
c(w 1   w i   1)

i live in pittsburgh . </s>
i am a graduate student . </s>
my home is in michigan . </s>

p(live | <s> i) = c(<s> i live)/c(<s> i) = 1 / 2 = 0.5
p(am | <s> i) = c(<s> i am)/c(<s> i) = 1 / 2 = 0.5

 

 

8

id4

problems with counting

    weak when counts are low:

training:

i live in pittsburgh . </s>
i am a graduate student . </s>
my home is in michigan . </s>

<s> i live in michigan . </s>

test:

p(michigan|<s> i live in) = 0/1 = 0

p(w=<s> i live in michigan . </s>) = 0

 

    solutions: restricting length, smoothing

 

9

id4

log-linear language model [chen+ 00]
    based on the previous words, give all words a score s

previous words:    giving a   

a
the
talk
gift
hat
   

b =

3.0
2.5
-0.2
0.1
1.2
   

w1,a = 

-6.0
-5.1
0.2
0.1
0.6
   

w2,giving = 

-0.2
-0.3
1.0
2.0
-1.2
   

s = 

-3.2
-2.9
1.0
2.2
0.6
   

words
we're
 

predicting

how likely
are they?

how likely
given the
previous
 

word is    a   ?

how likely
given two

words before
is    giving   ?

total
score

10

id4

log-linear language model [chen+ 00]
    convert scores into probabilities by taking exponent

and normalizing (called the softmax function)

i   1
p(ei=x   ei   n+1

i   1
p(ei   ei   n+1
-3.2
-2.9
1.0
2.2
0.6
   

s = 

a
the
talk
gift
hat
   

 

i   1

)

)

i   1

)=

es(ei= x   ei   n +1
   ~x es(ei=~x   ei   n+1
i   1
)=softmax(s(ei   ei   n+1
0.002
0.003
0.329
0.444
0.090

softmax

p = 

 

   

))

11

id4

learning log linear models

    often learn using stochastic id119 (sgd)
    basic idea: given a training example, find the
direction that we should move parameters w to
improve id203 of word ei

  =

d
d w

i   1
p(ei   ei   n+1

)

(gradient of
the id203)

    move the parameters in that direction

 

w    w +    

 

12

id4

problem with linear models:

cannot deal with feature combinations
cows eat  steak       low
farmers eat steak       high
cows eat  hay          high
farmers eat hay       low

    cannot express by just adding features. what do we do?

    remember scores for each combination of words

steak
hay
   

w2,1,farmers,eat = 

2.0
-2.1
   

w2,1,cows,eat = 

-1.2
2.9
   

explosion in number of parameters, memory usage

    neural nets!

 

 

13

id4

neural networks

 

 

14

id4

problem: can't learn feature

combinations

    cannot achieve high accuracy on non-linear functions

x

o

o

x

 

 

15

solving non-linear classification

id4

    create two classifiers

  0(x1) = {-1, 1}

  0[1]

x

o

  0(x3) = {-1, -1}

  0(x2) = {1, 1}
o

  0[0]

x
  0(x4) = {1, -1}

 

w0,0
1
1

-1
b0,0
w0,1
-1
-1

-1
b0,1

  0[0]
  0[1]
1

  0[0]
  0[1]
1

 

step

  1[0]

step

  1[1]

16

id4

example

    these classifiers map to a new space

  0(x1) = {-1, 1}
  2

x

  0(x2) = {1, 1}
o

  1

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

 

1
1
-1

-1
-1
-1

  1(x3) = {-1, 1}

o

x

  1(x1) = {-1, -1}
  1(x4) = {-1, -1}

  1[1]

  1[0]

o

  1(x2) = {1, -1}

  1[0]

 

  1[1]

17

id4

example

    in the new space, the examples are linearly separable!

  0(x1) = {-1, 1}
  0[1]

x

  0(x2) = {1, 1}
o

  0[0]

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

1
1
1

  2[0] = y

1
1
-1

-1
-1
-1

  1[0]

  1[1]

 

  1(x3) = {-1, 1}

o

  1[1]

  1(x1) = {-1, -1} x
  1(x4) = {-1, -1}

 

  1[0]

o   1(x2) = {1, -1}

18

id4

example

tanh

  1[0]

1

tanh

  2[0]

1

tanh

  1[1]

1 1

 

19

    the final net
  0[0]
  0[1]
1

  0[0]
  0[1]
1

1
1

-1

-1
-1

-1

 

id4

id38 with

neural nets

 

 

20

overview of log linear language model

id4

ei-1
ei-2

w1
w2
b

soft
max

pi

1

pi=softmax(b+   k=1

w kei   k)
ei-1 and ei-2 are vectors where the element corresponding to
the word is 1:

n   1

the

talk

gift

giving

a

ei-1 = {1,      0,      0,      0,      0,      ...}
ei-2 = {0,      0,      0,      0,      1,      ...}

 

w1, w2 are weight matrices b is a weight vector

 

21

id4

neural network language model
    add a    hidden layer    that calculates representations

w1
w2
b

ei-1
ei-2

1

tanh

wh
hi

soft
max

pi

n   1

hi=tanh(b+   k=1
pi=softmax (w h hi)

w k ei   k)

tanh     

1

0
-4 -3 -2 -1 0
-1

 

 

1

2

3

4

22

id4

what can we do with neural nets?
    learn shared    features    of the context
    example: what do livestock eat?

    {cows, horses, sheep, goats} {eat, consume, ingest}   

eat
consume
ingest
cows
horses
sheep
goats
   

w2[1]=

-1
-1
-1
1
1
1
1
   

w1[1]=

1
1
1
-1
-1
-1
-1
   

cows eat     tanh(1)

b[1]=-1

men eat     tanh(-1)

cows find     tanh(-1)

    if both are true, positive number, otherwise negative
    simple features must remember all 4x3 combinations!
23

 

 

neural network language model

[nakamura+ 90, bengio+ 06]

id4

<s>    <s>     this     is        a      pen    </s>

    convert each word into word representation, 

considering word similarity

    convert the context into low-dimensional hidden

layer, considering contextual similarity

 

 

24

id4

learning neural networks:

back propagation

    calculate the direction the last layer needs to go in
    pass this information backwards through the network

back-propagation
  h

  p

tanh

wh
hi

soft
max

pi

compare with true
answer and calculate
gradient

w1
w2
b

ei-1
ei-2

1

 

 

25

id4

recurrent neural nets

 

 

26

id4

recurrent neural nets (id56)

    pass output of the hidden layer from the last time step

back to the hidden layer in the next time step

wr

tanh

w1
w2
b

ei-1
ei-2

1

wh
hi

soft
max

pi

    why?: can remember long distance dependencies
    example:

he doesn't have very much confidence in himself
she doesn't have very much confidence in herself

 

 

27

id4

id56s as sequence models

y1

y2

y3

y4

net

net

net

net

x1

x2

x3

x4

 

 

28

id4

recurrent neural network language model

[mikolov+ 10]

<s>    <s>     this     is        a      pen    </s>

    greatly improves accuracy of machine translation,

id103, etc.

 

 

29

calculating gradients for id56s

id4

y1

y2

  o,1

  

  o,2

  

  

net

net

net

x1

x2

x3

    first, calculate values forward
    propagate errors backward
 

 

y3

  o,3

y4   o,4

  

net

x4

30

the vanishing gradient problem

id4

y1

miniscule

  

y2

tiny

  

small

  

y3

y4   o,4

medium

net

net

net

x1

x2

x3

  

net

x4

 

 

31

id4

long short-term memory

[hochreiter+ 97]

    based on a linear function that preserves previous

values

    gating structure to control information flow

 

 

32

what can a recurrent network learn?

[karpathy+ 15]

id4

 

 

33

id4

encoder-decoder
translation model
[kalchbrenner+ 13, sutskever+ 14]

 

 

34

id4

recurrent nn encoder-decoder model

[sutskever+ 14]
kore

this     is      a    pen  </s>

wa

pen

desu

kore  wa   pen  desu </s>

    in other words, exactly like id56 language model,

but first    reads    the input sentence
p(ei   f 1

j)=   i=1

p(e1

i   f 1

i +1
 

 

j ,e1

i   1)

35

id4

example of generation

this

is

a

pen </s>

kore

wa

pen

desu

kore wa pen desu

</s>

read the input

 

write the output
j ,e1
argmaxei
p(ei   f 1

i   1)

36

 

so, how well does it work?

id4

method

phrase-based baseline
encoder-decoder
encoder-decoder w/ tricks

id7
33.30
26.17
34.81

[sutskever et al. 2014]

answer: competitive with strong phrase-based
traditional systems! (with a little work...)

 

 

37

id4

trick 1: id125

    greedy search: select one-best at every time step

    problem: locally optimal decisions not globally optimal

ref:
   an animal   

<s>

<s>

a

a

animal

dog




    id125: maintain several hypotheses every step

<s>

a

an

the

 

animal

dog

animal

 

38

id4

trick 2: ensembling

    average two models together (in regular or log space)

model 1

model 2

model 1+2

a
the
talk
gift
hat
   

p1 = 

0.002
0.003
0.329
0.444
0.090

   

p2 = 

0.030
0.021
0.410
0.065
0.440

   

pe = 

0.016
0.012
0.370
0.260
0.265

   

    why does this work?

    errors tend to be uncorrelated
    errors tend to be less confident

 

 

39

id4

small example on japanese-english

    trained on 116k short, conversational sentences

moses pbmt
encoder-decoder

id7
38.6
39.0

ribes
80.3
82.9

 

 

40

does it stand up to manual inspection?

id4

answer: yes, to some extent

input:                                                             
true:
pbmt:
encdec:the bathtub has overflowed .

the hot water overflowed from the bathtub .
the hot water up the bathtub . 

input:                                                    
true:
pbmt:
encdec: i 'd like some coffee with cream .

i 'll have some coffee with cream , please .
cream of coffee , please . 

 

 

41

id4

but, there are problems.

giving up:

input:
true:
pbmt:
encdec: you have to have a chance .

                                                        
you 'll have to have a cast .
i have a           . 

repeating:

input:
true:
pbmt:
encdec: which foundation is my favorite foundation with a foundation ?

                                                                     
which foundation comes close to my natural skin color ?
which foundation near my natural skin color ? 

 

 

42

id4

attentional models

 

 

43

problem: encoder-decoder models have

trouble with longer sentences

id4

pbmt

id56

[pouget-abadie+ 2014]

 

 

44

id4

attentional nets [bahdanau+ 15]
    while translating, decide which word to    focus    on

 

 

45

id4

looking carefully at 
(one step of) attention

this

is

a

pen

a1

  1
*

 

a2

a3

a4

softmax
  3
+ *
  2
+ *

  4
+ *

=

 

* w + b
      softmax(   ) 
    = p(e1 | f)

46

id4

exciting results!

    iwslt 2015: 
best results on de-en
    wmt 2016: 
best results on most language pairs
    wat 2016: 
best results on most language pairs
(naist/cmu model 1st on ja-en)

 

 

47

what has gotten better?

largely grammar [bentivogli+ 16]

id4

 

 

48

id4

other things to think about

 

 

49

id4

what is our training criterion?

    we train our models for likelihood
    evaluate our models based on quality of the generated

sentences (id7)

    how do we directly optimize for translation quality?

    id23 [ranzato+16]
    minimum risk training [shen+16]
    id125 optimization [wiseman+16]

 

 

50

id4

how do we handle rare words?

    neural mt has trouble with large vocabularies

    speed: takes time to do a big softmax
    accuracy: fail on less common training examples

    solutions:

    sampling-based training methods [mnih+12]
    translate using subword units [sennrich+16]
    translate using characters [chung+16]
    incorporate translation lexicons [arthur+16]

 

 

51

id4

can we train multi-lingual models?

    multi-lingual data abounds, and we would like to use it
    methods:

    train individual encoders/decoders for each language,

but share training [firat+16]

    train a single encoder/decoder for all languages

[johnson+16]

    transfer models from one language to another

[zoph+16]

 

 

52

id4

what else can we do?

    conversation [sordoni+ 15, vinyals+ 15]
input: utterance, output: next utterance

    executing programs [zaremba+ 14]

input: program, output: computation result

    and many others!

 

 

53

id4

conclusion/tutorials/papers

 

 

54

id4

conclusion

    neural mt is exciting!

 

 

55

id4

tutorials

    my neural mt tips tutorial:

https://github.com/neubig/id4-tips
    kyunghyun cho's dl4mt tutorial:

http://github.com/nyu-dl/dl4mt-tutorial

    thang luong, kyunghyun cho, and chris manning's

tutorial at acl 2016:
https://sites.google.com/site/acl16id4/
    rico sennrich's tutorial at amta 2016:

http://statmt.org/mtma16/uploads/mtma16-neural.pdf

 

 

56

id4

references

    p. arthur, g. neubig, and s. nakamura. incorporating discrete translation lexicons into id4.

    d. bahdanau, k. cho, and y. bengio. id4 by jointly learning to align and translate. in proc.

in proc. emnlp, 2016.

iclr, 2015.

    y. bengio, h. schwenk, j.-s. sen   ecal, f. morin, and j.-l. gauvain. neural probabilistic language models. in

innovations in machine learning, volume 194, pages 137   186. 2006.

    s. f. chen and r. rosenfeld. a survey of smoothing techniques for me models. speech and audio processing,

    j. chung, k. cho, and y. bengio. a character-level decoder without explicit segmentation for neural machine

ieee transactions on, 8(1):37   50, jan 2000.

translation. arxiv preprint arxiv:1603.06147,

    2016.
    o. firat, k. cho, and y. bengio. multi-way, multilingual id4 with a shared attention

mechanism. in proc. naacl, pages 866   875, 2016.

    s. hochreiter and j. schmidhuber. long short-term memory. neural computation, 9(8):1735   1780, 1997.
    s. jean, k. cho, r. memisevic, and y. bengio. on using very large target vocabulary for id4.

in proc. acl, pages 1   10, 2015.

    m. johnson, m. schuster, q. v. le, m. krikun, y. wu, z. chen, n. thorat, f. vi   egas, m. wattenberg, g. corrado, et

al. google   s multilingual id4 system: enabling zero-shot translation. arxiv preprint
arxiv:1611.04558, 2016.

    n. kalchbrenner and p. blunsom. recurrent continuous translation models. in proc. emnlp, pages 1700   1709,

seattle, washington, usa, 2013. association for computational linguistics.

    m.-t. luong, i. sutskever, q. le, o. vinyals, and w. zaremba. addressing the rare word problem in neural machine

    t. luong, r. socher, and c. manning. better word representations with id56s for morphology.

translation. in proc. acl, pages 11   19, 2015.

in proc. conll, pages 104   113, 2013.

 

 

57

id4

references

    t. mikolov, m. karafi   at, l. burget, j. cernocky`, and s. khudanpur. recurrent neural network based language

model. in proc. interspeech, pages 1045   1048,

    2010.
    a. mnih and y. w. teh. a fast and simple algorithm for training neural probabilistic language models. arxiv preprint

    m. nakamura, k. maruyama, t. kawabata, and k. shikano. neural network approach to word category prediction

arxiv:1206.6426, 2012.

for english texts. in proc. coling, 1990.

iclr, 2016.

pages 1715   1725, 2016.

    m. ranzato, s. chopra, m. auli, and w. zaremba. sequence level training with recurrent neural networks. proc.

    r. sennrich, b. haddow, and a. birch. id4 of rare words with subword units. in proc. acl,

    s. shen, y. cheng, z. he, w. he, h. wu, m. sun, and y. liu. minimum risk training for id4.

in proc. acl, pages 1683   1692, 2016.

    r. socher, c. c. lin, c. manning, and a. y. ng. parsing natural scenes and natural language with recursive neural

networks. pages 129   136, 2011.
i. sutskever, o. vinyals, and q. v. le. sequence to sequence learning with neural networks. in proc. nips, pages
3104   3112, 2014.

    a. vaswani, y. zhao, v. fossum, and d. chiang. decoding with large-scale neural language models improves

   

translation. in proc. emnlp, pages 1387   1392, 2013.

    s. wiseman and a. m. rush. sequence-to-sequence learning as beam-search optimization. in proc. emnlp,

    b. zoph, d. yuret, j. may, and k. knight. id21 for low-resource id4. in proc.

pages 1296   1306, 2016.

emnlp, pages 1568   1575, 2016.

 

 

58

