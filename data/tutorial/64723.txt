introduction

to

id4

fabienne cap

what is id4?

   neural machine trans-
lation is the approach of
modeling the entire mt
process via one big
arti   cial neural network.   
(manning, 2016)

fabienne cap

introductiontoid4

id4 example

... repeated from introduction lecture:

fabienne cap

introductiontoid4

taken from (baldahau et al. 2014)

id4 timeline

1987 early encoder-decoder, with vocabulary size 30-40

(allen, 1987)

...
2013 pure neural mt system presented

(kalchbrenner & blunsom, 2013)

2014 competitive encoder-decoder for large-scale mt

(bahdanau et al., 2015, luong et al., 2014)

2015 id4 systems in shared tasks

performs well in wmt, state-of-the-art at iwslt

2016 id4 systems top most language pairs in wmt
2016 commercial deployments of id4 launched

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

overview

id4 vs. smt

a typical id4 system

shortcomings of id4 and proposed solutions

outlook

fabienne cap

introductiontoid4

overview

id4 vs. smt

a typical id4 system

shortcomings of id4 and proposed solutions

outlook

fabienne cap

introductiontoid4

id4 vs. smt: architecture

review: smt architecture

many components     pipeline architecture
context-independent translations
idependent models (tm, lm, rm)
learn feature weights using minimum error rate training

fabienne cap

introductiontoid4

translationtranslationmodelsourcemodeladaptedreferenceupdateconvergedscoringtargetn   bestn   gram extractionscoringtestingtuningtrainingsourcetargetword alignmenttargetlanguagemodelsourcereferencetargetid4 vs. smt: architecture

review: smt architecture

many components     pipeline architecture
context-independent translations
idependent models (tm, lm, rm)
learn feature weights using minimum error rate training

fabienne cap

introductiontoid4

translationtranslationmodelsourcemodeladaptedreferenceupdateconvergedscoringtargetn   bestn   gram extractionscoringtestingtuningtrainingsourcetargetword alignmenttargetlanguagemodelsourcereferencetargetid4 vs. smt: architecture

review: smt architecture

many components     pipeline architecture
context-independent translations
idependent models (tm, lm, rm)
learn feature weights using minimum error rate training

fabienne cap

introductiontoid4

translationtranslationmodelsourcemodeladaptedreferenceupdateconvergedscoringtargetn   bestn   gram extractionscoringtestingtuningtrainingsourcetargetword alignmenttargetlanguagemodelsourcereferencetargetid4 vs. smt: architecture

review: smt architecture

many components     pipeline architecture
context-independent translations
idependent models (tm, lm, rm)
learn feature weights using minimum error rate training

fabienne cap

introductiontoid4

translationtranslationmodelsourcemodeladaptedreferenceupdateconvergedscoringtargetn   bestn   gram extractionscoringtestingtuningtrainingsourcetargetword alignmenttargetlanguagemodelsourcereferencetargetid4 vs. smt: architecture

review: smt architecture

many components     pipeline architecture
context-independent translations
idependent models (tm, lm, rm)
learn feature weights using minimum error rate training

fabienne cap

introductiontoid4

translationtranslationmodelsourcemodeladaptedreferenceupdateconvergedscoringtargetn   bestn   gram extractionscoringtestingtuningtrainingsourcetargetword alignmenttargetlanguagemodelsourcereferencetargetid4 vs. smt: architecture

a typical neural encoder-decoder architecture

everything in one large model

all parameters optimised globally

no explicit division into tm, lm rm

based on acl id4 tutorial 2016

fabienne cap

introductiontoid4

id4 vs. smt: architecture

a typical neural encoder-decoder architecture

everything in one large model

all parameters optimised globally

no explicit division into tm, lm rm

based on acl id4 tutorial 2016

fabienne cap

introductiontoid4

id4 vs. smt: architecture

a typical neural encoder-decoder architecture

everything in one large model

all parameters optimised globally

no explicit division into tm, lm rm

based on acl id4 tutorial 2016

fabienne cap

introductiontoid4

id4 vs. smt: architecture

a typical neural encoder-decoder architecture

everything in one large model

all parameters optimised globally

no explicit division into tm, lm rm

based on acl id4 tutorial 2016

fabienne cap

introductiontoid4

smt vs. id4: word alignment

fabienne cap

introductiontoid4

nat  rlichofflagswithfunhassheldonthatassumewecoursehatflaggenanspasssheldondassausdavongehenwirsmt vs. id4: word alignment

smt: word alignments

fabienne cap

introductiontoid4

nat  rlichofflagswithfunhassheldonthatassumewecoursehatflaggenanspasssheldondassausdavongehenwirsmt vs. id4: word alignment

smt: word alignments

id4: id12

taken from (bahdanau et al., 2015)

fabienne cap

introductiontoid4

nat  rlichofflagswithfunhassheldonthatassumewecoursehatflaggenanspasssheldondassausdavongehenwirsmt vs. id4: context

smt:

phrase table weights gave
a context-independent
translation score

use language models
(lm) to ensure target
language    uency

based on acl id4 tutorial 2016

fabienne cap

introductiontoid4

smt vs. id4: context

smt:

id4:

phrase table weights gave
a context-independent
translation score

use language models
(lm) to ensure target
language    uency

all translations are
context-dependent!
generate a translation
with an lm also
conditioned on the source
language

based on acl id4 tutorial 2016

fabienne cap

introductiontoid4

id4 vs. smt: performance

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

overview

id4 vs. smt

a typical id4 system

shortcomings of id4 and proposed solutions

outlook

fabienne cap

introductiontoid4

overview

id4 vs. smt

a typical id4 system

shortcomings of id4 and proposed solutions

outlook

fabienne cap

introductiontoid4

a typical id4 system

id4 is nothing but a particular variety of smt:
p(e|f) is learned using neural networks

the translation is formulated as a prediction problem, similar
to a language model but trained on both source and target
sequences

encoder-decoder models:
use a recurrent neural network (id56) to read a source
language sequence and predict a target language sequence

based on https://github.com/neubig/id4-tips and acl 2016 tutorial on id4

fabienne cap

introductiontoid4

a typical id4 system

id4 is nothing but a particular variety of smt:
p(e|f) is learned using neural networks

the translation is formulated as a prediction problem, similar
to a language model but trained on both source and target
sequences

encoder-decoder models:
use a recurrent neural network (id56) to read a source
language sequence and predict a target language sequence

based on https://github.com/neubig/id4-tips and acl 2016 tutorial on id4

fabienne cap

introductiontoid4

a typical id4 system

id4 is nothing but a particular variety of smt:
p(e|f) is learned using neural networks

the translation is formulated as a prediction problem, similar
to a language model but trained on both source and target
sequences

encoder-decoder models:
use a recurrent neural network (id56) to read a source
language sequence and predict a target language sequence

based on https://github.com/neubig/id4-tips and acl 2016 tutorial on id4

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

a)    rst hidden state of decoder = last hidden state of encoder

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

a)    rst hidden state of decoder = last hidden state of encoder
b) predict target word probabilities:
id203 estim = softmax (gi   1; parameters)

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

a)    rst hidden state of decoder = last hidden state of encoder
b) predict target word probabilities:
id203 estim = softmax (gi   1; parameters)
c) pick most probable word:
target wordi = argmaxk (id203 estimi [k])

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

a)    rst hidden state of decoder = last hidden state of encoder
b) predict target word probabilities:
id203 estim = softmax (gi   1; parameters)
c) pick most probable word:
target wordi = argmaxk (id203 estimi [k])
d) update the hidden state with this predicted value:
target embi = wordrep(target wordi , parameters)
gi = id56(gi   1, target embi   1, parameters)

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typical id4 system: encoder-decoder model

taken from the eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typical id4 system: encoder-decoder model

taken from the acl 2016 tutorial on id4

fabienne cap

introductiontoid4

a typical id4 system: encoder-decoder model

taken from the acl 2016 tutorial on id4

fabienne cap

introductiontoid4

a typical id4 system: encoder-decoder model

fabienne cap

introductiontoid4

taken from (baldahau et al. 2014)

a typical id4 system: encoder-decoder model

taken from (baldahau et al. 2014)

fabienne cap

introductiontoid4

a typical id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

a)    rst hidden state of decoder = last hidden state of encoder
b) predict target word probabilities:
id203 estim = softmax (gi   1; parameters)
c) pick most probable word:
target wordi = argmaxk (id203 estimi [k])
d) update the hidden state with this predicted value:
target embi = wordrep(target wordi , parameters)
gi = id56(gi   1, target embi   1, parameters)

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typical id4 system: encoder-decoder model

1) encode source sentence: convert into    xed-length vector

source embj = wordrep(source wordj , parameters)

2) map to hidden state using an id56:

hiddenj = id56(hj   1, source embj , parameters)

3) decode:

a)    rst hidden state of decoder = last hidden state of encoder
b) predict target word probabilities:
id203 estim = softmax (gi   1; parameters)
c) pick most probable word:
target wordi = argmaxk (id203 estimi [k])
d) update the hidden state with this predicted value:
target embi = wordrep(target wordi , parameters)
gi = id56(gi   1, target embi   1, parameters)

this is a training loop!

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

training id4 models with maximum likelihood

the parameters (  ) of the models need to be learned
standard way of doing this: maximise the log lilkelihood of the
training data:

  (cid:48) = argmax  ((cid:80)
  (cid:48) = argmin  (   (cid:80)

e ,f logp(e|f ;   ))
e ,f logp(e|f ;   )

equivalent: minimise the negative log likelihood:

minimisation using stochastic id119 (sgd):
calculate gradient of the negative log id203:
          logp(e|f ;   )
then update the parameters based on an update rule:
       update (  ,          logp(e|f ;   )
substract the gradient of the negative log likelihood multiplied
by a learning rate   
sgd update (  ,          logp(e|f ;   ),   ) :=
                        logp(e|f ;   )

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

training id4 models with maximum likelihood

the parameters (  ) of the models need to be learned
standard way of doing this: maximise the log lilkelihood of the
training data:

  (cid:48) = argmax  ((cid:80)
  (cid:48) = argmin  (   (cid:80)

e ,f logp(e|f ;   ))
e ,f logp(e|f ;   )

equivalent: minimise the negative log likelihood:

minimisation using stochastic id119 (sgd):
calculate gradient of the negative log id203:
          logp(e|f ;   )
then update the parameters based on an update rule:
       update (  ,          logp(e|f ;   )
substract the gradient of the negative log likelihood multiplied
by a learning rate   
sgd update (  ,          logp(e|f ;   ),   ) :=
                        logp(e|f ;   )

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

training id4 models with maximum likelihood

the parameters (  ) of the models need to be learned
standard way of doing this: maximise the log lilkelihood of the
training data:

  (cid:48) = argmax  ((cid:80)
  (cid:48) = argmin  (   (cid:80)

e ,f logp(e|f ;   ))
e ,f logp(e|f ;   )

equivalent: minimise the negative log likelihood:

minimisation using stochastic id119 (sgd):
calculate gradient of the negative log id203:
          logp(e|f ;   )
then update the parameters based on an update rule:
       update (  ,          logp(e|f ;   )
substract the gradient of the negative log likelihood multiplied
by a learning rate   
sgd update (  ,          logp(e|f ;   ),   ) :=
                        logp(e|f ;   )

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

training id4 models with maximum likelihood

the parameters (  ) of the models need to be learned
standard way of doing this: maximise the log lilkelihood of the
training data:

  (cid:48) = argmax  ((cid:80)
  (cid:48) = argmin  (   (cid:80)

e ,f logp(e|f ;   ))
e ,f logp(e|f ;   )

equivalent: minimise the negative log likelihood:

minimisation using stochastic id119 (sgd):
calculate gradient of the negative log id203:
          logp(e|f ;   )
then update the parameters based on an update rule:
       update (  ,          logp(e|f ;   )
substract the gradient of the negative log likelihood multiplied
by a learning rate   
sgd update (  ,          logp(e|f ;   ),   ) :=
                        logp(e|f ;   )

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

training id4 models with maximum likelihood

the parameters (  ) of the models need to be learned
standard way of doing this: maximise the log lilkelihood of the
training data:

  (cid:48) = argmax  ((cid:80)
  (cid:48) = argmin  (   (cid:80)

e ,f logp(e|f ;   ))
e ,f logp(e|f ;   )

equivalent: minimise the negative log likelihood:

minimisation using stochastic id119 (sgd):
calculate gradient of the negative log id203:
          logp(e|f ;   )
then update the parameters based on an update rule:
       update (  ,          logp(e|f ;   )
substract the gradient of the negative log likelihood multiplied
by a learning rate   
sgd update (  ,          logp(e|f ;   ),   ) :=
                        logp(e|f ;   )

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

training id4 models with maximum likelihood

the parameters (  ) of the models need to be learned
standard way of doing this: maximise the log lilkelihood of the
training data:

  (cid:48) = argmax  ((cid:80)
  (cid:48) = argmin  (   (cid:80)

e ,f logp(e|f ;   ))
e ,f logp(e|f ;   )

equivalent: minimise the negative log likelihood:

minimisation using stochastic id119 (sgd):
calculate gradient of the negative log id203:
          logp(e|f ;   )
then update the parameters based on an update rule:
       update (  ,          logp(e|f ;   )
substract the gradient of the negative log likelihood multiplied
by a learning rate   
sgd update (  ,          logp(e|f ;   ),   ) :=
                        logp(e|f ;   )

based on https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

excursion: id119

general concept in machine learning:
minimise the distance between what has been predicted and
what should have been predicted.

fabienne cap

introductiontoid4

excursion: id119

general concept in machine learning:
minimise the distance between what has been predicted and
what should have been predicted.

andrew ng   s coursera course on machine learning:
https://www.youtube.com/watch?v=ln0plndpgn4&t=121s

fabienne cap

introductiontoid4

a typical id4 system: mini-batching

problem:
id119 for each single sentence takes time!
solution:
update the gradients for multiple sentences at the same time.
this is called mini-batching

based on eacl 2017 tutorial on practical id4,
https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typical id4 system: mini-batching

problem:
id119 for each single sentence takes time!
solution:
update the gradients for multiple sentences at the same time.
this is called mini-batching

based on eacl 2017 tutorial on practical id4,
https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typical id4 system: mini-batching

problem:
id119 for each single sentence takes time!
solution:
update the gradients for multiple sentences at the same time.
this is called mini-batching

but: needs same vector length

based on eacl 2017 tutorial on practical id4,
https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typical id4 system: mini-batching

problem:
id119 for each single sentence takes time!
solution:
update the gradients for multiple sentences at the same time.
this is called mini-batching

but: needs same vector length

padding, masking:    ll with    0   
    works! but: wasteful

based on eacl 2017 tutorial on practical id4,
https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typical id4 system: mini-batching

problem:
id119 for each single sentence takes time!
solution:
update the gradients for multiple sentences at the same time.
this is called mini-batching

but: needs same vector length

padding, masking:    ll with    0   
    works! but: wasteful

better:
sort wrt. to sentence length

based on eacl 2017 tutorial on practical id4,
https://github.com/neubig/id4-tips

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

taken from eacl 2017 tutorial on practical id4
fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

summary vector is an information bottleneck
problem: sentence length! fixed sized representation
degrades as sentence lenght increases (cho et al. 2014)

reversing source sequence brings some improvement
(sutskever et al. 2014)
solution: attention

compute context vectors
as weighted average of source hidden states
weights computed by feed-forward network
with softmax activation

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

summary vector is an information bottleneck
problem: sentence length! fixed sized representation
degrades as sentence lenght increases (cho et al. 2014)

reversing source sequence brings some improvement
(sutskever et al. 2014)
solution: attention

compute context vectors
as weighted average of source hidden states
weights computed by feed-forward network
with softmax activation

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

summary vector is an information bottleneck
problem: sentence length! fixed sized representation
degrades as sentence lenght increases (cho et al. 2014)

reversing source sequence brings some improvement
(sutskever et al. 2014)
solution: attention

compute context vectors
as weighted average of source hidden states
weights computed by feed-forward network
with softmax activation

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

summary vector is an information bottleneck
problem: sentence length! fixed sized representation
degrades as sentence lenght increases (cho et al. 2014)

reversing source sequence brings some improvement
(sutskever et al. 2014)
solution: attention

compute context vectors
as weighted average of source hidden states
weights computed by feed-forward network
with softmax activation

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

summary vector is an information bottleneck
problem: sentence length! fixed sized representation
degrades as sentence lenght increases (cho et al. 2014)

reversing source sequence brings some improvement
(sutskever et al. 2014)
solution: attention

compute context vectors
as weighted average of source hidden states
weights computed by feed-forward network
with softmax activation

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

summary vector is an information bottleneck
problem: sentence length! fixed sized representation
degrades as sentence lenght increases (cho et al. 2014)

reversing source sequence brings some improvement
(sutskever et al. 2014)
solution: attention

compute context vectors
as weighted average of source hidden states
weights computed by feed-forward network
with softmax activation

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

taken from eacl 2017 tutorial on practical id4

fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

side e   ect: we obtain alignments between the source and
target sentence
but: no guarantee that it corresponds to alignment!
information can also    ow along recurrent connections.

taken from eacl 2017 tutorial on practical id4
fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

side e   ect: we obtain alignments between the source and
target sentence
but: no guarantee that it corresponds to alignment!
information can also    ow along recurrent connections.

taken from eacl 2017 tutorial on practical id4
fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

side e   ect: we obtain alignments between the source and
target sentence
but: no guarantee that it corresponds to alignment!
information can also    ow along recurrent connections.

taken from eacl 2017 tutorial on practical id4
fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

side e   ect: we obtain alignments between the source and
target sentence
but: no guarantee that it corresponds to alignment!
information can also    ow along recurrent connections.

taken from eacl 2017 tutorial on practical id4
fabienne cap

introductiontoid4

a typcial id4 system: attention mechanism

side e   ect: we obtain alignments between the source and
target sentence
but: no guarantee that it corresponds to alignment!
information can also    ow along recurrent connections.

taken from eacl 2017 tutorial on practical id4
fabienne cap

introductiontoid4

overview

id4 vs. smt

a typical id4 system

shortcomings of id4 and proposed solutions

outlook

fabienne cap

introductiontoid4

overview

id4 vs. smt

a typical id4 system

shortcomings of id4 and proposed solutions

outlook

fabienne cap

introductiontoid4

id4 example

... repeated from introduction lecture:

fabienne cap

introductiontoid4

end of today

this is the end. hold your breath and count to ten.

fabienne cap

introductiontoid4

