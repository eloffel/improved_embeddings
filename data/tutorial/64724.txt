practical id4

rico sennrich, barry haddow

institute for language, cognition and computation

university of edinburgh

april 4, 2017

(last updated: april 28, 2017)

sennrich, haddow

practical id4

1 / 109

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

1 / 109

id4 timeline

1987 early encoder-decoder, with vocabulary size 30-40 [allen, 1987]

...

2013 pure neural mt system presented [kalchbrenner and blunsom, 2013]
2014 competitive encoder-decoder for large-scale mt

[bahdanau et al., 2015, luong et al., 2014]

2015 id4 systems in shared tasks     perform well in wmt,

state-of-the-art at iwslt

2016 id4 systems top most language pairs in wmt
2016 commercial deployments of id4 launched

sennrich, haddow

practical id4

2 / 109

id4 now state-of-the-art

system
uedin-id4
metamind
uedin-syntax
nyu-umontreal
online-b
kit/limsi
cambridge
online-a
promt-rule
kit
jhu-syntax
jhu-pbmt
uedin-pbmt
online-f
online-g

id7
34.2
32.3
30.6
30.8
29.4
29.1
30.6
29.9
23.4
29.0
26.6
28.3
28.4
19.3
23.8

of   cial rank

1
2
3
4

5-10
5-10
5-10
5-10
5-10
6-10
11-12
11-12
13-14
13-15
14-15

wmt16 en   de

system
uedin-id4
online-b
online-a
uedin-syntax
kit
uedin-pbmt
jhu-pbmt
online-g
jhu-syntax
online-f

id7
38.6
35.0
32.8
34.4
33.9
35.1
34.5
30.1
31.0
20.2

of   cial rank

1
2-5
2-5
2-5
2-6
5-7
6-7
8
9
10

wmt16 de   en

sennrich, haddow

practical id4

3 / 109

id4 now state-of-the-art

system
uedin-id4
metamind
uedin-syntax
nyu-umontreal
online-b
kit/limsi
cambridge
online-a
promt-rule
kit
jhu-syntax
jhu-pbmt
uedin-pbmt
online-f
online-g

id7
34.2
32.3
30.6
30.8
29.4
29.1
30.6
29.9
23.4
29.0
26.6
28.3
28.4
19.3
23.8

of   cial rank

1
2
3
4

5-10
5-10
5-10
5-10
5-10
6-10
11-12
11-12
13-14
13-15
14-15

system
uedin-id4
online-b
online-a
uedin-syntax
kit
uedin-pbmt
jhu-pbmt
online-g
jhu-syntax
online-f

id7
38.6
35.0
32.8
34.4
33.9
35.1
34.5
30.1
31.0
20.2

of   cial rank

1
2-5
2-5
2-5
2-6
5-7
6-7
8
9
10

wmt16 de   en

wmt16 en   de

pure id4

sennrich, haddow

practical id4

3 / 109

id4 now state-of-the-art

system
uedin-id4
metamind
uedin-syntax
nyu-umontreal
online-b
kit/limsi
cambridge
online-a
promt-rule
kit
jhu-syntax
jhu-pbmt
uedin-pbmt
online-f
online-g

id7
34.2
32.3
30.6
30.8
29.4
29.1
30.6
29.9
23.4
29.0
26.6
28.3
28.4
19.3
23.8

of   cial rank

1
2
3
4

5-10
5-10
5-10
5-10
5-10
6-10
11-12
11-12
13-14
13-15
14-15

wmt16 en   de

system
uedin-id4
online-b
online-a
uedin-syntax
kit
uedin-pbmt
jhu-pbmt
online-g
jhu-syntax
online-f

id7
38.6
35.0
32.8
34.4
33.9
35.1
34.5
30.1
31.0
20.2

of   cial rank

1
2-5
2-5
2-5
2-6
5-7
6-7
8
9
10

wmt16 de   en

pure id4
id4 component

sennrich, haddow

practical id4

3 / 109

id4 now state-of-the-art

uedin-id4
nyu-umontreal
jhu-pbmt
cu-chimera
cu-tamchyna
uedin-cu-syntax
online-b
online-a
cu-tectomt
cu-mergedtrees

25.8
23.6
23.6
21.0
20.8
20.9
22.7
19.5
14.7
8.2

1
2
3
4-5
4-5
6-7
6-7
15
16
18

wmt16 en   cs

online-b
uedin-id4
uedin-pbmt
uedin-syntax
online-a
jhu-pbmt
limsi

39.2
33.9
35.2
33.6
30.8
32.2
31.0

1-2
1-2
3
4-5
4-6
5-7
6-7

wmt16 ro   en

uedin-id4
jhu-pbmt
online-b
pjatk
online-a
cu-mergedtrees

31.4
30.4
28.6
28.3
25.7
13.3

1
2
3

8-10
11
12

wmt16 cs   en

uedin-id4
qt21-himl-syscomb
kit
uedin-pbmt
online-b
uedin-lmu-hiero
rwth-syscomb
limsi
lmu-cuni
jhu-pbmt
usfd-rescoring
online-a

28.1
28.9
25.8
26.8
25.4
25.9
27.1
23.9
24.3
23.5
23.1
19.2

1-2
1-2
3-7
3-7
3-7
3-7
3-7
8-10
8-10
8-11
10-12
11-12

wmt16 en   ro

sennrich, haddow

practical id4

3 / 109

id4 now state-of-the-art

promt-rule
amu-uedin
online-b
uedin-id4
online-g
nyu-umontreal
jhu-pbmt
limsi
online-a
afrl-mitll-phr
afrl-mitll-verb
online-f

22.3
25.3
23.8
26.0
26.2
23.1
24.0
23.6
20.2
23.5
20.9
8.6

1
2-4
2-5
2-5
3-5
6
7-8
7-10
8-10
9-10
11
12

wmt16 en   ru

amu-uedin
online-g
nrc
online-b
uedin-id4
online-a
afrl-mitll-phr
afrl-mitll-contrast
promt-rule
online-f

29.1
28.7
29.1
28.1
28.0
25.7
27.6
27.0
20.4
13.5

1-2
1-3
2-4
3-5
4-5
6-7
6-7
8-9
8-9
10

wmt16 ru   en

uedin-pbmt
online-g
online-b
uh-opus
promt-smt
uh-factored
uedin-syntax
online-a
jhu-pbmt

23.4
20.6
23.6
23.1
20.3
19.3
20.4
19.0
19.1

1-4
1-4
1-4
1-4
5
6-7
6-7
8
9

wmt16 fi   en

online-g

abumatra-id4

online-b

abumatran-combo

uh-opus

nyu-umontreal
abumatran-pbsmt

online-a
jhu-pbmt

uh-factored

aalto

jhu-hltcoe

uut

15.4
17.2
14.4
17.4
16.3
15.1
14.6
13.0
13.8
12.8
11.6
11.9
11.6

1-3
1-4
1-4
3-5
4-5
6-8
6-8
6-8
9-10
9-12
10-13
10-13
11-13

wmt16 en   fi

sennrich, haddow

practical id4

3 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

id4 in production

sennrich, haddow

practical id4

4 / 109

course goals

at the end of this tutorial, you will

have a basic theoretical understanding of models/algorithms in id4
understand strengths and weaknesses of id4
know techniques that help to build state-of-the-art id4 systems
know practical tips for various problems you may encounter:

training and decoding ef   ciency
id20
ways to further improve translation quality
...

no hands-on coding/training in tutorial, but helpful resources are provided

sennrich, haddow

practical id4

5 / 109

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

6 / 109

what is a neural network?

a complex non-linear function which:

is built from simpler units (neurons, nodes, gates, . . . )
maps vectors/matrices to vectors/matrices
is parameterised by vectors/matrices

sennrich, haddow

practical id4

7 / 109

what is a neural network?

a complex non-linear function which:

is built from simpler units (neurons, nodes, gates, . . . )
maps vectors/matrices to vectors/matrices
is parameterised by vectors/matrices

why is this useful?
very expressive
can represent (e.g.) parameterised id203 distributions
evaluation and parameter estimation can be built up from components

sennrich, haddow

practical id4

7 / 109

a simple neural network classi   er

x1

x2

x3

...

xn

g(w    x + b)

y

y > 0

y <= 0

x is a vector input, y is a scalar output
w and b are the parameters (b is a bias term)
g is a non-linear activation function

sennrich, haddow

practical id4

8 / 109

why non-linearity?

functions like xor cannot be separated by a linear function

xor

truth table

output

x1
0
0
1
1

x2
0
1
0
1

0
1
1
0

x1

x2

1

0.5

0.5

1

a

b

c

1

-2

1

d

y

(neurons arranged in layers, and    re if input is     1)

sennrich, haddow

practical id4

9 / 109

why non-linearity?

functions like xor cannot be separated by a linear function

xor

truth table

output

x1
0
0
1
1

x2
0
1
0
1

0
1
1
0

x1

x2

1

0.5

0.5

1

a

b

c

1

-2

1

d

y

(neurons arranged in layers, and    re if input is     1)

sennrich, haddow

practical id4

9 / 109

why non-linearity?

functions like xor cannot be separated by a linear function

xor

truth table

output

x1
0
0
1
1

x2
0
1
0
1

0
1
1
0

x1

x2

1

0.5

0.5

1

a

b

c

1

-2

1

d

y

(neurons arranged in layers, and    re if input is     1)

sennrich, haddow

practical id4

9 / 109

why non-linearity?

functions like xor cannot be separated by a linear function

xor

truth table

output

x1
0
0
1
1

x2
0
1
0
1

0
1
1
0

x1

x2

1

0.5

0.5

1

a

b

c

1

-2

1

d

y

(neurons arranged in layers, and    re if input is     1)

sennrich, haddow

practical id4

9 / 109

id180

desirable:

differentiable (for gradient-based training)
monotonic (for better training stability)
non-linear (for better expressivity)

identity (linear)
sigmoid
tanh
recti   ed linear unit (relu)

y

3.0

2.0

1.0

   3.0    2.0    1.0

1.0

2.0

3.0

x

   1.0

sennrich, haddow

practical id4

10 / 109

more complex architectures
convolutional

recurrent

[kalchbrenner et al., 2014]

http://karpathy.github.io/2015/05/21/id56-effectiveness/

sennrich, haddow

practical id4

11 / 109

andrej karpathy

torwi   rdofawordinthesentence:s=      w1...ws      (2)toaddresstheproblemofvaryingsentencelengths,themax-tdnntakesthemaximumofeachrowintheresultingmatrixcyieldingavectorofdvalues:cmax=         max(c1,:)...max(cd,:)         (3)theaimistocapturethemostrelevantfeature,i.e.theonewiththehighestvalue,foreachofthedrowsoftheresultingmatrixc.the   xed-sizedvectorcmaxisthenusedasinputtoafullycon-nectedlayerforclassi   cation.themax-tdnnmodelhasmanydesirableproperties.itissensitivetotheorderofthewordsinthesentenceanditdoesnotdependonexternallanguage-speci   cfeaturessuchasdependencyorconstituencyparsetrees.italsogiveslargelyuni-formimportancetothesignalcomingfromeachofthewordsinthesentence,withtheexceptionofwordsatthemarginsthatareconsideredfewertimesinthecomputationofthenarrowconvolu-tion.butthemodelalsohassomelimitingas-pects.therangeofthefeaturedetectorsislim-itedtothespanmoftheweights.increasingmorstackingmultipleconvolutionallayersofthenar-rowtypemakestherangeofthefeaturedetectorslarger;atthesametimeitalsoexacerbatesthene-glectofthemarginsofthesentenceandincreasestheminimumsizesoftheinputsentencerequiredbytheconvolution.forthisreasonhigher-orderandlong-rangefeaturedetectorscannotbeeasilyincorporatedintothemodel.themaxpoolingop-erationhassomedisadvantagestoo.itcannotdis-tinguishwhetherarelevantfeatureinoneoftherowsoccursjustoneormultipletimesanditfor-getstheorderinwhichthefeaturesoccur.moregenerally,thepoolingfactorbywhichthesignalofthematrixisreducedatoncecorrespondstos   m+1;evenformoderatevaluesofsthepool-ingfactorcanbeexcessive.theaimofthenextsectionistoaddresstheselimitationswhilepre-servingtheadvantages.3convolutionalneuralnetworkswithdynamick-maxpoolingwemodelsentencesusingaconvolutionalarchi-tecturethatalternateswideconvolutionallayersk-max pooling(k=3)fully connected layerfoldingwideconvolution(m=2)dynamick-max pooling (k= f(s) =5) projectedsentence matrix(s=7)wideconvolution(m=3) the cat sat on the red matfigure3:adid98forthesevenwordinputsen-tence.wordembeddingshavesized=4.thenetworkhastwoconvolutionallayerswithtwofeaturemapseach.thewidthsofthe   ltersatthetwolayersarerespectively3and2.the(dynamic)k-maxpoolinglayershavevalueskof5and3.withdynamicpoolinglayersgivenbydynamick-maxpooling.inthenetworkthewidthofafeaturemapatanintermediatelayervariesdependingonthelengthoftheinputsentence;theresultingar-chitectureisthedynamicconvolutionalneuralnetwork.figure3representsadid98.wepro-ceedtodescribethenetworkindetail.3.1wideconvolutiongivenaninputsentence,toobtainthe   rstlayerofthedid98wetaketheembeddingwi   rdforeachwordinthesentenceandconstructthesen-tencematrixs   rd  sasineq.2.thevaluesintheembeddingswiareparametersthatareop-timisedduringtraining.aconvolutionallayerinthenetworkisobtainedbyconvolvingamatrixofweightsm   rd  mwiththematrixofactivationsatthelayerbelow.forexample,thesecondlayerisobtainedbyapplyingaconvolutiontothesen-tencematrixsitself.dimensiondand   lterwidthmarehyper-parametersofthenetwork.welettheoperationsbewideone-dimensionalconvolutionsasdescribedinsect.2.2.theresultingmatrixchasdimensionsd  (s+m   1).658training of neural networks

parameter estimation

use id119
requires labelled training data . . .
. . . and differentiable objective function

network structure enables ef   cient computation

forward pass to compute network output
backpropogation, i.e. backward pass using chain rule, to calculate
gradient

normally train stochastically using mini-batches

sennrich, haddow

practical id4

12 / 109

practical considerations

hyperparameters:

number and size of layers
minibatch size
learning rate
...

initialisation of weight matrices
stopping criterion
id173 (dropout)
bias units (always-on input)

sennrich, haddow

practical id4

13 / 109

toolkits for neural networks

what does a toolkit provide

multi-dimensional matrices (tensors)
automatic differentiation
ef   cient gpu routines for tensor operations

http://torch.ch/

tensorflow https://www.tensorflow.org/

http://deeplearning.net/software/theano/

there are many more!

sennrich, haddow

practical id4

14 / 109

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

15 / 109

language model

chain rule and markov assumption

a sentence t of length n is a sequence w1, . . . , wn

p(t ) = p(w1, . . . , wn)

=

n(cid:89)
    n(cid:89)

i=1

i=1

p(wi|w0, . . . , wi   1)
p(wi|wi   k, . . . , wi   1)

(chain rule)

(markov assumption: id165 model)

sennrich, haddow

practical id4

16 / 109

id165 language model with feedforward neural network

[vaswani et al., 2013]

id165 nnlm [bengio et al., 2003]

input: context of n-1 previous words
output: id203 distribution for next word
linear embedding layer with shared weights
one or several hidden layers

sennrich, haddow

practical id4

17 / 109

representing words as vectors

one-hot encoding

example vocabulary:    man,    runs   ,    the   ,    .   
input/output for p(runs|the man):

x0 =            

0
0
1
0

            

x1 =            

1
0
0
0

            

ytrue =            

0
1
0
0

            

size of input/output vector: vocabulary size
embedding layer is lower-dimensional and dense

smaller weight matrices
network learns to group similar words to similar point in vector space

sennrich, haddow

practical id4

18 / 109

softmax activation function

softmax function

p(y = j|x) =

exj

(cid:80)k exk

softmax function normalizes output vector to id203 distribution
    computational cost linear to vocabulary size (!)
ideally: id203 1 for correct word; 0 for rest
sgd with softmax output minimizes cross-id178 (and hence
perplexity) of neural network

sennrich, haddow

practical id4

19 / 109

feedforward neural language model: math

[vaswani et al., 2013]

h1 =   w1(ex1, ex2)
y = softmax(w2h1)

sennrich, haddow

practical id4

20 / 109

feedforward neural language model in smt

ffnlm

can be integrated as a feature in the log-linear smt model
[schwenk et al., 2006]
costly due to id127s and softmax
solutions:

n-best reranking
variants of softmax (hierarchical softmax, self-id172 [nce])
shallow networks; premultiplication of hidden layer

scales well to many input words
    models with source context [devlin et al., 2014]

sennrich, haddow

practical id4

21 / 109

recurrent neural network language model (id56lm)

id56lm [mikolov et al., 2010]

motivation: condition on arbitrarily long context
    no markov assumption
we read in one word at a time, and update hidden state incrementally
hidden state is initialized as empty vector at time step 0
parameters:

embedding matrix e
feedforward matrices w1, w2
recurrent matrix u

hi =(cid:40)0,

tanh(w1exi + u hi   1)

yi = softmax(w2hi   1)

, if i = 0
, if i > 0

sennrich, haddow

practical id4

22 / 109

id56 variants

gated units

alternative to plain id56
sigmoid layers    act as    gates    that control    ow of information
allows passing of information over long time
    avoids vanishing gradient problem
strong empirical results
popular variants:

long short term memory (lstm) (shown)
gated recurrent unit (gru)

sennrich, haddow

practical id4

23 / 109
christopher olah http://colah.github.io/posts/2015-08-understanding-lstms/

id56 variants

gated units

alternative to plain id56
sigmoid layers    act as    gates    that control    ow of information
allows passing of information over long time
    avoids vanishing gradient problem
strong empirical results
popular variants:

long short term memory (lstm) (shown)
gated recurrent unit (gru)

sennrich, haddow

practical id4

23 / 109
christopher olah http://colah.github.io/posts/2015-08-understanding-lstms/

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

24 / 109

modelling translation

suppose that we have:

a source sentence s of length m (x1, . . . , xm)
a target sentence t of length n (y1, . . . , yn)

we can express translation as a probabilistic model

t     = arg max

t

p(t|s)

expanding using the chain rule gives

p(t|s) = p(y1, . . . , yn|x1, . . . , xm)

=

n(cid:89)i=1

p(yi|y1, . . . , yi   1, x1, . . . , xm)

sennrich, haddow

practical id4

25 / 109

differences between translation and language model

target-side language model:

p(t ) =

n(cid:89)i=1

p(yi|y1, . . . , yi   1)

translation model:

p(t|s) =

n(cid:89)i=1

p(yi|y1, . . . , yi   1, x1, . . . , xm)

we could just treat sentence pair as one long sequence, but:

we do not care about p(s)
we may want different vocabulary, network architecture for source text

sennrich, haddow

practical id4

26 / 109

differences between translation and language model

target-side language model:

p(t ) =

n(cid:89)i=1

p(yi|y1, . . . , yi   1)

translation model:

p(t|s) =

n(cid:89)i=1

p(yi|y1, . . . , yi   1, x1, . . . , xm)

we could just treat sentence pair as one long sequence, but:

we do not care about p(s)
we may want different vocabulary, network architecture for source text

    use separate id56s for source and target.

sennrich, haddow

practical id4

26 / 109

encoder-decoder for translation

of

y1

s1

course

john

has

fun

y2

s2

y3

s3

y4

s4

y5

s5

sennrich, haddow

practical id4

27 / 109

encoder-decoder for translation

decoder

encoder

of

y1

s1

h1

x1

nat  rlich

course

john

has

fun

y5

s5

y2

s2

h2

x2

hat

y3

s3

h3

x3

y4

s4

h4

x4

john

spa  

sennrich, haddow

practical id4

27 / 109

summary vector

last encoder hidden-state    summarises    source sentence
with multilingual training, we can potentially learn
language-independent meaning representation

sennrich, haddow

practical id4

28 / 109

[sutskever et al., 2014]

summary vector as information bottleneck

problem: sentence length

fixed sized representation degrades as sentence length increases
reversing source brings some improvement [sutskever et al., 2014]

[cho et al., 2014]

sennrich, haddow

practical id4

29 / 109

summary vector as information bottleneck

problem: sentence length

fixed sized representation degrades as sentence length increases
reversing source brings some improvement [sutskever et al., 2014]

[cho et al., 2014]

solution: attention

compute context vector as weighted average of source hidden states
weights computed by feed-forward network with softmax activation

sennrich, haddow

practical id4

29 / 109

encoder-decoder with attention

decoder

encoder

of

y1

s1

h1

x1

nat  rlich

course

john

has

fun

y5

s5

y2

s2

0.7

h2

x2

hat

y3

s3

+

0.1

0.1

0.1

h3

x3

john

y4

s4

h4

x4

spa  

sennrich, haddow

practical id4

30 / 109

encoder-decoder with attention

decoder

encoder

of

y1

s1

h1

x1

nat  rlich

course

john

has

fun

y5

s5

y2

s2

0.6

h2

x2

hat

y3

s3

+

0.2

0.1

0.1

h3

x3

john

y4

s4

h4

x4

spa  

sennrich, haddow

practical id4

30 / 109

encoder-decoder with attention

decoder

encoder

of

y1

s1

h1

x1

nat  rlich

course

john

has

fun

y5

s5

y2

s2

0.1

h2

x2

hat

y3

s3

+

0.1

0.1

0.7

h3

x3

john

y4

s4

h4

x4

spa  

sennrich, haddow

practical id4

30 / 109

encoder-decoder with attention

decoder

encoder

of

y1

s1

h1

x1

nat  rlich

course

john

has

fun

y5

s5

y2

s2

0.1

h2

x2

hat

y3

s3

+

0.7

0.1

0.1

h3

x3

john

y4

s4

h4

x4

spa  

sennrich, haddow

practical id4

30 / 109

encoder-decoder with attention

decoder

encoder

of

y1

s1

h1

x1

nat  rlich

course

john

has

fun

y5

s5

y2

s2

0.1

h2

x2

hat

y3

s3

+

0.1

0.7

0.1

h3

x3

john

y4

s4

h4

x4

spa  

sennrich, haddow

practical id4

30 / 109

attentional encoder-decoder: maths

simpli   cations of model by [bahdanau et al., 2015] (for illustration)

plain id56 instead of gru
simpler output layer
we do not show bias terms
decoder follows look, update, generate strategy [sennrich et al., 2017]
details in https://github.com/amuid4/amuid4/blob/master/contrib/notebooks/dl4mt.ipynb

notation

w , u, e, c, v are weight matrices (of different dimensionality)

e one-hot to embedding (e.g. 50000    512)
w embedding to hidden (e.g. 512    1024)
u hidden to hidden (e.g. 1024    1024)
c context (2x hidden) to hidden (e.g. 2048    1024)
vo hidden to one-hot (e.g. 1024    50000)

separate weight matrices for encoder and decoder (e.g. ex and ey)
input x of length tx; output y of length ty

sennrich, haddow

practical id4

31 / 109

attentional encoder-decoder: maths

encoder

      h j =(cid:40)0,
      h j =(cid:40)0,

tanh(      w xexxj +       u xhj   1)
tanh(      w xexxj +       u xhj+1)

hj = (      h j,      h j)

, if j = 0
, if j > 0
, if j = tx + 1
, if j     tx

sennrich, haddow

practical id4

32 / 109

attentional encoder-decoder: maths

decoder

si =(cid:40)tanh(ws      h i),

tanh(wyeyyi   1 + uysi   1 + cci)

ti = tanh(uosi + woeyyi   1 + coci)
yi = softmax(voti)

, if i = 0
, if i > 0

attention model

eij = v(cid:62)a tanh(wasi   1 + uahj)
  ij = softmax(eij)

ci =

tx(cid:88)j=1

  ijhj

sennrich, haddow

practical id4

33 / 109

attention model

attention model

side effect: we obtain alignment between source and target sentence
information can also    ow along recurrent connections, so there is no
guarantee that attention corresponds to alignment
applications:

visualisation
replace unknown words with back-off dictionary [jean et al., 2015]
...

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

sennrich, haddow

practical id4

34 / 109

attention model

attention model also works with images:

sennrich, haddow

practical id4

[cho et al., 2015]

35 / 109

attention model

[cho et al., 2015]

sennrich, haddow

practical id4

36 / 109

application of encoder-decoder model

scoring (a translation)
p(la, croissance,   conomique, s   est, ralentie, ces, derni  res, ann  es, . |
economic, growth, has, slowed, down, in, recent, year, .) = ?

decoding ( a source sentence)
generate the most probable translation of a source sentence

y    = argmaxy p(y|economic, growth, has, slowed, down, in, recent, year, .)

sennrich, haddow

practical id4

37 / 109

decoding

exact search

generate every possible sentence t in target language
compute score p(t|s) for each
pick best one

intractable: |vocab|n translations for output length n
    we need approximative search strategy

sennrich, haddow

practical id4

38 / 109

decoding

approximative search/1: greedy search
at each time step, compute id203
distribution p (yi|s, y<i)
select yi according to some heuristic:
sampling: sample from p (yi|s, y<i)
greedy search: pick argmaxy p(yi|s, y<i)

continue until we generate <eos>

ef   cient, but suboptimal

sennrich, haddow

practical id4

39 / 109

!0.9280.175<eos>0.9990.175hello0.9460.056world0.9570.1000decoding

approximative search/2: beam
search

maintain list of k hypotheses
(beam)
at each time step, expand each
hypothesis k: p(yk
i |s, yk
<i)
select k hypotheses with
highest total id203:

(cid:89)i

p(yk

i |s, yk
<i)

k = 3

relatively ef   cient . . . beam expansion parallelisable
currently default search strategy in id4
small beam (k     10) offers good speed-quality trade-off

sennrich, haddow

practical id4

40 / 109

hello0.9460.056world0.9570.100world0.0104.632.0.0303.609!0.9280.175...0.0144.384<eos>0.9993.609world0.6845.299hi0.0074.920<eos>0.9944.390hey0.0065.107<eos>0.9990.1750ensembles

at each timestep, combine the id203 distribution of m different
ensemble components.
combine operator: typically average (log-)id203

log p (yi|s, y<i) = (cid:80)m

m=1 log pm(yi|s, y<i)

m

requirements:

same output vocabulary
same factorization of y

internal network architecture may be different
source representations may be different
(extreme example: ensemble-like model with different source
languages [junczys-dowmunt and grundkiewicz, 2016])

sennrich, haddow

practical id4

41 / 109

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

42 / 109

innovations in edinburgh   s wmt16 systems

basic encoder-decoder-with-attention, plus:

1 subword models to allow translation of rare/unknown words

3 combination of left-to-right and right-to-left models

    since networks have small,    xed vocabulary
2 back-translated monolingual data as additional training data
    allows us to make use of extensive monolingual resources
    reduces    label-bias    problem
    improves generalisation performance with small training data

4 bayesian dropout

sennrich, haddow

practical id4

43 / 109

subwords for id4: motivation

mt is an open-vocabulary problem

compounding and other productive morphological processes

they charge a carry-on bag fee.
sie erheben eine hand|gep  ck|geb  hr.

names

obama(english; german)
           (russian)
    (o-ba-ma) (japanese)

technical terms, numbers, etc.

... but neural mt architectures have small and    xed vocabulary

sennrich, haddow

practical id4

44 / 109

subword units

segmentation algorithms: wishlist

open-vocabulary id4: encode all words through small vocabulary
encoding generalizes to unseen words
small text size
good translation quality

our experiments

after preliminary experiments, we use:

character id165s (with shortlist of unsegmented words)
segmentation via byte pair encoding

sennrich, haddow

practical id4

45 / 109

byte pair encoding for id40

bottom-up character merging

starting point: character-level representation
    computationally expensive
compress representation based on id205
    byte pair encoding [gage, 1994]
repeatedly replace most frequent symbol pair (   a   ,   b   ) with    ab   
hyperparameter: when to stop
    controls vocabulary size

word
   l o w </w>   
   l o w e r </w>   
   n e w e s t </w>   
   w i d e s t </w>   

freq
5
2
6
3

vocabulary:
l o w </w> e r n s t i d

sennrich, haddow

practical id4

46 / 109

byte pair encoding for id40

bottom-up character merging

starting point: character-level representation
    computationally expensive
compress representation based on id205
    byte pair encoding [gage, 1994]
repeatedly replace most frequent symbol pair (   a   ,   b   ) with    ab   
hyperparameter: when to stop
    controls vocabulary size

word
   l o w </w>   
   l o w e r </w>   
   n e w es t </w>   
   w i d es t </w>   

freq
5
2
6
3

vocabulary:
l o w </w> e r n s t i d
es

sennrich, haddow

practical id4

46 / 109

byte pair encoding for id40

bottom-up character merging

starting point: character-level representation
    computationally expensive
compress representation based on id205
    byte pair encoding [gage, 1994]
repeatedly replace most frequent symbol pair (   a   ,   b   ) with    ab   
hyperparameter: when to stop
    controls vocabulary size

word
   l o w </w>   
   l o w e r </w>   
   n e w est </w>   
   w i d est </w>   

freq
5
2
6
3

vocabulary:
l o w </w> e r n s t i d
es est

sennrich, haddow

practical id4

46 / 109

byte pair encoding for id40

bottom-up character merging

starting point: character-level representation
    computationally expensive
compress representation based on id205
    byte pair encoding [gage, 1994]
repeatedly replace most frequent symbol pair (   a   ,   b   ) with    ab   
hyperparameter: when to stop
    controls vocabulary size

word
   l o w </w>   
   l o w e r </w>   
   n e w est</w>   
   w i d est</w>   

freq
5
2
6
3

vocabulary:
l o w </w> e r n s t i d
es est est</w>

sennrich, haddow

practical id4

46 / 109

byte pair encoding for id40

bottom-up character merging

starting point: character-level representation
    computationally expensive
compress representation based on id205
    byte pair encoding [gage, 1994]
repeatedly replace most frequent symbol pair (   a   ,   b   ) with    ab   
hyperparameter: when to stop
    controls vocabulary size

word
   lo w </w>   
   lo w e r </w>   
   n e w est</w>   
   w i d est</w>   

freq
5
2
6
3

vocabulary:
l o w </w> e r n s t i d
es est est</w> lo

sennrich, haddow

practical id4

46 / 109

byte pair encoding for id40

bottom-up character merging

starting point: character-level representation
    computationally expensive
compress representation based on id205
    byte pair encoding [gage, 1994]
repeatedly replace most frequent symbol pair (   a   ,   b   ) with    ab   
hyperparameter: when to stop
    controls vocabulary size

word
   low </w>   
   low e r </w>   
   n e w est</w>   
   w i d est</w>   

freq
5
2
6
3

vocabulary:
l o w </w> e r n s t i d
es est est</w> lo low

sennrich, haddow

practical id4

46 / 109

byte pair encoding for id40

why bpe?

open-vocabulary:
operations learned on training set can be applied to unknown words
compression of frequent character sequences improves ef   ciency
    trade-off between text length and vocabulary size

   l o w e s t </w>   

e s
es t
est </w>     est</w>
l o
lo w

    es
    est
    lo
    low

sennrich, haddow

practical id4

47 / 109

byte pair encoding for id40

why bpe?

open-vocabulary:
operations learned on training set can be applied to unknown words
compression of frequent character sequences improves ef   ciency
    trade-off between text length and vocabulary size

   l o w es t </w>   

e s
es t
est </w>     est</w>
l o
lo w

    es
    est
    lo
    low

sennrich, haddow

practical id4

47 / 109

byte pair encoding for id40

why bpe?

open-vocabulary:
operations learned on training set can be applied to unknown words
compression of frequent character sequences improves ef   ciency
    trade-off between text length and vocabulary size

   l o w est </w>   

e s
es t
est </w>     est</w>
l o
lo w

    es
    est
    lo
    low

sennrich, haddow

practical id4

47 / 109

byte pair encoding for id40

why bpe?

open-vocabulary:
operations learned on training set can be applied to unknown words
compression of frequent character sequences improves ef   ciency
    trade-off between text length and vocabulary size

   l o w est</w>   

e s
es t
est </w>     est</w>
l o
lo w

    es
    est
    lo
    low

sennrich, haddow

practical id4

47 / 109

byte pair encoding for id40

why bpe?

open-vocabulary:
operations learned on training set can be applied to unknown words
compression of frequent character sequences improves ef   ciency
    trade-off between text length and vocabulary size

   lo w est</w>   

e s
es t
est </w>     est</w>
l o
lo w

    es
    est
    lo
    low

sennrich, haddow

practical id4

47 / 109

byte pair encoding for id40

why bpe?

open-vocabulary:
operations learned on training set can be applied to unknown words
compression of frequent character sequences improves ef   ciency
    trade-off between text length and vocabulary size

   low est</w>   

e s
es t
est </w>     est</w>
l o
lo w

    es
    est
    lo
    low

sennrich, haddow

practical id4

47 / 109

evaluation: data and methods

data

wmt 15 english   german and english   russian

model

attentional encoder   decoder neural network
parameters and settings as in [bahdanau et al, 2014]

sennrich, haddow

practical id4

48 / 109

subword id4: translation quality

24.4

22.8

22.8

22.0

24.3

20.9

20.4

19.1

20.0

u
e
l
b

10.0

0.0

en-de

en-ru

smt [sennrich and haddow, 2015, haddow et al., 2015]
word-level id4 (with back-off) [jean et al., 2015]
subword-level id4: character bigrams
subword-level id4: bpe

sennrich, haddow

practical id4

49 / 109

subword id4: translation quality

1
f
m
a
r
g
n
u

i

1

0.8

0.6

0.4

0.2

0
100

id4 results en-ru

50 000 500 000

subword-level id4: bpe
subword-level id4: char bigrams
word-level (with back-off)
word-level (no back-off)

101

102

105
training set frequency rank

104

103

106

sennrich, haddow

practical id4

50 / 109

examples

system
source
reference
word-level (with back-off)
character bigrams
bpe
source
reference
word-level (with back-off)
character bigrams
bpe

sentence
health research institutes
gesundheitsforschungsinstitute
forschungsinstitute
fo|rs|ch|un|gs|in|st|it|ut|io|ne|n
gesundheits|forsch|ungsin|stitute
rak   sk
                 (rak   ska)
rak   sk     unk     rak   sk
ra|kf|is|k         |    |    |   (ra|kf|is|k)
rak|f|isk           |  |         (rak|f|iska)

sennrich, haddow

practical id4

51 / 109

bpe in wmt16 systems

used joint bpe

just concatenate source and target, then train
named-entities are split consistently

learn 89,500 merge operations
use iso-9 id68 for russian:

transliterate russian corpus into latin script
learn bpe operations on concatenation of english and transliterated
russian corpus
transliterate bpe operations into cyrillic
for russian, apply both cyrillic and latin bpe operations
    concatenate bpe    les

set vocabulary size according to bpe vocabulary

code available: https://github.com/rsennrich/subword-id4

sennrich, haddow

practical id4

52 / 109

monolingual data in id4

why monolingual data for phrase-based smt?

more training data  
relax independence assumptions  
more appropriate training data (id20)  

why monolingual data for id4?

more training data  
relax independence assumptions  
more appropriate training data (id20)  

sennrich, haddow

practical id4

53 / 109

monolingual data in id4

encoder-decoder already conditions on

previous target words

no architecture change required to learn

from monolingual data

sennrich, haddow

practical id4

54 / 109

monolingual training instances

output prediction

p(yi) is a function of hidden state si, previous output yi   1, and source
context vector ci
only difference to monolingual id56: ci

problem
we have no source context ci for monolingual training instances

sennrich, haddow

practical id4

55 / 109

monolingual training instances

output prediction

p(yi) is a function of hidden state si, previous output yi   1, and source
context vector ci
only difference to monolingual id56: ci

problem
we have no source context ci for monolingual training instances

solutions

two methods to deal with missing source context:

empty/dummy source context ci
    danger of unlearning conditioning on source
produce synthetic source sentence via back-translation
    get approximation of ci

sennrich, haddow

practical id4

55 / 109

monolingual training instances

dummy source

1-1 mix of parallel and monolingual training instances
randomly sample from monolingual data each epoch
freeze encoder/attention layers for monolingual training instances

synthetic source

1-1 mix of parallel and monolingual training instances
randomly sample from back-translated data
training does not distinguish between real and synthetic parallel data

sennrich, haddow

practical id4

56 / 109

evaluation: wmt 15 english   german

24.4
24.4
24.4

23.6
23.6

24.6

26.5

30.0

20.0

u
e
l
b

10.0

0.0

syntax-based

parallel

+monolingual

+synthetic

(id4 systems are ensemble of 4)

sennrich, haddow

practical id4

57 / 109

evaluation: wmt 15 german   english

30.0 29.3
29.3
29.3

26.7
26.7

30.4

31.6

20.0

u
e
l
b

10.0

0.0

pbsmt

parallel

+synthetic

+synth-ens4

sennrich, haddow

practical id4

58 / 109

why is monolingual data helpful?

id20 effect
reduces over-   tting
improves    uency

(see [sennrich et al., 2016] for more analysis.)

sennrich, haddow

practical id4

59 / 109

left-to-right / right-to-left reranking

target history is strong signal for next prediction

history is reliable at training time, but not at test time
low-id178 output words lead to poor translation
similar to label bias problem

reranking with reverse model can help

1 train two models, one has target reversed
2 generate n-best lists with one model
3 rescore lists with second model
4 rerank using combined scores

consistent increase (0.5     1) in id7

sennrich, haddow

practical id4

60 / 109

bayesian dropout

[gal, 2015]

dropout (randomly zeroing activations in training) prevents over   tting
follow [gal, 2015] and repeat mask across timesteps
necessary for english   romanian (0.6m sentences)
masks of 0.1-0.2 provide gain of 4-5 id7

sennrich, haddow

practical id4

61 / 109

checkpoint ensembling

training

p(e|f )

=

p1(e|f )

  

p2(e|f )

  

p3(e|f )

  

p4(e|f )

ensembling improves performance and stability
checkpoint ensembling much cheaper than independent runs

sennrich, haddow

practical id4

62 / 109

putting it all together: wmt16 results

40.0

30.0

u
e
l
b

20.0

10.0

0.0

en   cs

en   de

en   ro

en   ru

cs   en

de   en

ro   en

ru   en

parallel data

+synthetic data

+ensemble

+r2l reranking

sennrich, haddow

practical id4

63 / 109

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

64 / 109

comparison between phrase-based and neural mt

human analysis of id4 (reranking) [neubig et al., 2015]

id4 is more grammatical

word order
insertion/deletion of function words
morphological agreement

minor degradation in lexical choice?

sennrich, haddow

practical id4

65 / 109

comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, haddow

practical id4

66 / 109

comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, haddow

practical id4

66 / 109

comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, haddow

practical id4

66 / 109

comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, haddow

practical id4

66 / 109

adequacy vs. fluency in wmt16 evaluation

adequacy

+1%

fluency
+13%

100

80

60

75.4

75.8

72.2

70.8

73.9

71.2

72.8

71.1

100

80

64.6

60

78.7

77.5

68.4

71.9

66.7

67.8

74.3

cs   en de   en ro   en ru   en

cs   en de   en ro   en ru   en

online-b uedin-id4

online-b uedin-id4

figure : wmt16 direct assessment results

sennrich, haddow

practical id4

67 / 109

human evaluation in tramooc

comparison of id4 and pbsmt for en   {de,el,pt,ru}
direct assessment:
id4 obtains higher    uency judgment than pbsmt: +10%
id4 only obtains small improvement in adequacy judgment: +1%

post-editing:

id4 reduces technical effort (keystrokes): -13%
small reduction in post-editing time: -4%

    id4 errors more dif   cult to identify

error annotation

category
in   ectional morphology
word order
omission
addition
mistranslation
"no issue"

smt id4 difference
2274
1098
421
314
1593
449

-21%
-37%
-14%
-16%
-3%
+75%

1799
691
362
265
1552
788

sennrich, haddow

practical id4

68 / 109

assessing mt quality with contrastive translation pairs

questions

how well does id4 perform for speci   c linguistic phenomena?
example: is grammaticality affected by choice of subword unit?

method [sennrich, 2017]

compare id203 of human reference translation with contrastive
translation that introduces a speci   c type of error
    id4 model should prefer reference
errors related to:

morphosyntactic agreement
discontiguous units of meaning
polarity
id68

sennrich, haddow

practical id4

69 / 109

contrastive translation pairs: example

english
german (correct)
german (contrastive)

[...] that the plan will be approved
[...], dass der plan verabschiedet wird
* [...], dass der plan verabschiedet werden

subject-verb agreement

sennrich, haddow

practical id4

70 / 109

assessing mt quality with contrastive translation pairs

results

wmt16 id4 system detects agreement errors with high accuracy    
96.6   98.7%.
character-level system [lee et al., 2016] better than bpe-to-bpe
system at id68, but worse at morphosyntactic agreement
difference higher for agreement over long distances

)
t
n
e
m
e
e
r
g
a
b
r
e
v
-
t
c
e
b
u
s
(

j

y
c
a
r
u
c
c
a

1

0.9

0.8

0.7

0.6

0.5

0

bpe-to-bpe
char-to-char

4

8

distance

12

    16
16

sennrich, haddow

practical id4

71 / 109

id4 vs. pbmt: an extended test [junczys-dowmunt et al., 2016a]

experimental setup

training and test drawn from un corpus

multi-parallel, 11m lines
arabic, chinese, english, french, russian, spanish

use only parallel data, evaluate with id7 on 4000 sentences

sennrich, haddow

practical id4

72 / 109

why is neural mt output more grammatical?

phrase-based smt

log-linear combination of many    weak    features
data sparsenesss triggers back-off to smaller units
strong independence assumptions

neural mt

end-to-end trained model
generalization via continuous space representation
output conditioned on full source text and target history

sennrich, haddow

practical id4

73 / 109

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

74 / 109

resource usage

we all want our experiments to    nish faster . . .
what in   uences training speed/memory usage?

number of model parameters, especially vocabulary size
size of training instance (max. length    batch size)
hardware and library versions

decoding speed

less important for id4 researchers
standard nematus model     use amuid4 (hand-crafted gpu code)

sennrich, haddow

practical id4

75 / 109

hardware/library choice

hardware
cpu (xeon e5-2680)
gpu (titan x pascal)
gpu (titan x pascal)
gpu (titan x pascal)
gpu (titan black)
gpu (titan x)
gpu (gtx 1080)
gpu (tesla m60)
gpu (titan x pascal)

theano cudnn gpuarray sentence/s
2.5
0.8.2
83
0.8.2
138
0.8.2
0.9b
171
109
0.9b
0.9b
110
177
0.9b
110
0.9b
0.9rc3
227

no
no
5.10
5.10
5.10
5.10
5.10
5.10
5.10

no
no
no
no
no
no
no
no
yes

sennrich, haddow

practical id4

76 / 109

hyperparameters: ef   ciency

hyperparameters affect peak gpu memory and speed
gpu memory is often the bottleneck in id4 training
memory consumption affected by

number of model parameters
size of training instance (length    batchsize)
we show some pro   le output for guidance:
nematus (   test_train.sh   )
nvidia gtx 1080 gpu

layer size

vocabulary

batchsize maxlen

gpu memory

speed

embed

256
256
256
256
512
512
512

hidden
512
512
512
1024
1024
1024
1024

source
30000
30000
60000
60000
60000
60000
60000

target
30000
60000
60000
60000
60000
60000
60000

40
40
40
40
40
80
80

50
50
50
50
50
50
80

(peak)
1.2 gb
2.1 gb
2.3 gb
2.7 gb
3.6 gb
4.9 gb
6.6 gb

sennrich, haddow

practical id4

sents/s

174
148
145
95
79
110
87

words/s
4080
3470
3410
2220
1850
2600
2570

77 / 109

hyperparameters: ef   ciency

hyperparameters affect peak gpu memory and speed
gpu memory is often the bottleneck in id4 training
memory consumption affected by

number of model parameters
size of training instance (length    batchsize)
we show some pro   le output for guidance:
nematus (   test_train.sh   )
nvidia gtx 1080 gpu

layer size

vocabulary

batchsize maxlen

gpu memory

speed

embed

256
256
256
256
512
512
512

hidden
512
512
512
1024
1024
1024
1024

source
30000
30000
60000
60000
60000
60000
60000

target
30000
60000
60000
60000
60000
60000
60000

40
40
40
40
40
80
80

50
50
50
50
50
50
80

(peak)
1.2 gb
2.1 gb
2.3 gb
2.7 gb
3.6 gb
4.9 gb
6.6 gb

sennrich, haddow

practical id4

sents/s

174
148
145
95
79
110
87

words/s
4080
3470
3410
2220
1850
2600
2570

77 / 109

hyperparameters: ef   ciency

hyperparameters affect peak gpu memory and speed
gpu memory is often the bottleneck in id4 training
memory consumption affected by

number of model parameters
size of training instance (length    batchsize)
we show some pro   le output for guidance:
nematus (   test_train.sh   )
nvidia gtx 1080 gpu

layer size

vocabulary

batchsize maxlen

gpu memory

speed

embed

256
256
256
256
512
512
512

hidden
512
512
512
1024
1024
1024
1024

source
30000
30000
60000
60000
60000
60000
60000

target
30000
60000
60000
60000
60000
60000
60000

40
40
40
40
40
80
80

50
50
50
50
50
50
80

(peak)
1.2 gb
2.1 gb
2.3 gb
2.7 gb
3.6 gb
4.9 gb
6.6 gb

sennrich, haddow

practical id4

sents/s

174
148
145
95
79
110
87

words/s
4080
3470
3410
2220
1850
2600
2570

77 / 109

hyperparameters: ef   ciency

hyperparameters affect peak gpu memory and speed
gpu memory is often the bottleneck in id4 training
memory consumption affected by

number of model parameters
size of training instance (length    batchsize)
we show some pro   le output for guidance:
nematus (   test_train.sh   )
nvidia gtx 1080 gpu

layer size

vocabulary

batchsize maxlen

gpu memory

speed

embed

256
256
256
256
512
512
512

hidden
512
512
512
1024
1024
1024
1024

source
30000
30000
60000
60000
60000
60000
60000

target
30000
60000
60000
60000
60000
60000
60000

40
40
40
40
40
80
80

50
50
50
50
50
50
80

(peak)
1.2 gb
2.1 gb
2.3 gb
2.7 gb
3.6 gb
4.9 gb
6.6 gb

sennrich, haddow

practical id4

sents/s

174
148
145
95
79
110
87

words/s
4080
3470
3410
2220
1850
2600
2570

77 / 109

hyperparameters: ef   ciency

hyperparameters affect peak gpu memory and speed
gpu memory is often the bottleneck in id4 training
memory consumption affected by

number of model parameters
size of training instance (length    batchsize)
we show some pro   le output for guidance:
nematus (   test_train.sh   )
nvidia gtx 1080 gpu

layer size

vocabulary

batchsize maxlen

gpu memory

speed

embed

256
256
256
256
512
512
512

hidden
512
512
512
1024
1024
1024
1024

source
30000
30000
60000
60000
60000
60000
60000

target
30000
60000
60000
60000
60000
60000
60000

40
40
40
40
40
80
80

50
50
50
50
50
50
80

(peak)
1.2 gb
2.1 gb
2.3 gb
2.7 gb
3.6 gb
4.9 gb
6.6 gb

sennrich, haddow

practical id4

sents/s

174
148
145
95
79
110
87

words/s
4080
3470
3410
2220
1850
2600
2570

77 / 109

hyperparameters: ef   ciency

hyperparameters affect peak gpu memory and speed
gpu memory is often the bottleneck in id4 training
memory consumption affected by

number of model parameters
size of training instance (length    batchsize)
we show some pro   le output for guidance:
nematus (   test_train.sh   )
nvidia gtx 1080 gpu

layer size

vocabulary

batchsize maxlen

gpu memory

speed

embed

256
256
256
256
512
512
512

hidden
512
512
512
1024
1024
1024
1024

source
30000
30000
60000
60000
60000
60000
60000

target
30000
60000
60000
60000
60000
60000
60000

40
40
40
40
40
80
80

50
50
50
50
50
50
80

(peak)
1.2 gb
2.1 gb
2.3 gb
2.7 gb
3.6 gb
4.9 gb
6.6 gb

sennrich, haddow

practical id4

sents/s

174
148
145
95
79
110
87

words/s
4080
3470
3410
2220
1850
2600
2570

77 / 109

hyperparameters: ef   ciency

hyperparameters affect peak gpu memory and speed
gpu memory is often the bottleneck in id4 training
memory consumption affected by

number of model parameters
size of training instance (length    batchsize)
we show some pro   le output for guidance:
nematus (   test_train.sh   )
nvidia gtx 1080 gpu

layer size

vocabulary

batchsize maxlen

gpu memory

speed

embed

256
256
256
256
512
512
512

hidden
512
512
512
1024
1024
1024
1024

source
30000
30000
60000
60000
60000
60000
60000

target
30000
60000
60000
60000
60000
60000
60000

40
40
40
40
40
80
80

50
50
50
50
50
50
80

(peak)
1.2 gb
2.1 gb
2.3 gb
2.7 gb
3.6 gb
4.9 gb
6.6 gb

sennrich, haddow

practical id4

sents/s

174
148
145
95
79
110
87

words/s
4080
3470
3410
2220
1850
2600
2570

77 / 109

training: minibatches

why minibatches?

parallelization (gpus!) is more ef   ciently with larger matrices
easy way to increase matrix size: batch up training instances
other advantage: stabilizes updates

how do we deal with difference in sentence length in batch?
standard solution: pad sentence with special tokens

sennrich, haddow

practical id4

[johansen et al., 2016]

78 / 109

layerno.unitsinputalphabetsize(x)300embeddingsizes256charid56(forward)400charid56(backward)400attention300chardecoder400targetalphabetsize(t)300table1.hyperparametervaluesusedfortrainingthechar-to-charmodel.where  srcand  trgrepresentthenumberofclassesinthesourceandtargetlanguages,respec-tively.layerno.unitsinputalphabetsize(x)300embeddingsizes256charid56(forward)400spacesid56(forward)400spacesid56(backward)400attention300chardecoder400targetalphabetsize(t)300table2.hyperparametervaluesusedfortrainingthechar2word-to-charmodel.where  srcand  trgrepre-sentthenumberofclassesinthesourceandtargetlanguages,respectively.timesteps,whichcanresultinalotofwastedresources[han-nunetal.,2014](see   gure4).trainingtranslationmodelsisfurthercomplicatedbythefactthatsourceandtargetsen-tences,whilecorrelated,mayhavedifferentlengths,anditisnecessarytoconsiderbothwhenconstructingbatchesinordertoutilizecomputationpowerandramoptimally.tocircumventthisissue,westarteachepochbyshuf   ingallsamplesinthedatasetandsortingthemwithastablesort-ingalgorithmaccordingtoboththesourceandtargetsentencelengths.thisensuresthatanytwosamplesinthedatasetthathavealmostthesamesourceandtargetsentencelengthsarelocatedclosetoeachotherinthesortedlistwhiletheexactorderofsamplesvariesbetweenepochs.topackabatchwesimplystartedaddingsamplesfromthesortedsamplelisttothebatch,untilwereachedthemaximaltotalallowedcharac-terthreshold(whichwesetto50,000)forthefullbatchwithpaddingafterwhichwewouldstartonanewbatch.finallyallthebatchesarefedinrandomordertothemodelfortrain-inguntilallsampleshavebeentrainedon,andanewepochbegins.figure5illustrateswhatsuchdynamicbatchesmightlooklike.fig.4.aregularbatchwithrandomsamples.fig.5.ourdynamicbatchesofvariablebatchsizeandse-quencelength.5.3.results5.3.1.quantitativethequantitativeresultsofourmodelsareillustratedinta-ble3.noticethatthechar2word-to-charmodelout-performsthechar-to-charmodelonalldatasets(average1.28id7performanceincrease).thiscouldbeanindica-tionthateitherhavinghierarchical,word-like,representationsontheencoderorsimplythefactthattheencoderwassigni   -cantlysmaller,helpsinid4whenusingacharacterdecoderwithattention.training: minibatches

speed-ups

sort sentences of same length together [sutskever et al., 2014]
adjust batch size depending on length [johansen et al., 2016]

sennrich, haddow

practical id4

79 / 109

layerno.unitsinputalphabetsize(x)300embeddingsizes256charid56(forward)400charid56(backward)400attention300chardecoder400targetalphabetsize(t)300table1.hyperparametervaluesusedfortrainingthechar-to-charmodel.where  srcand  trgrepresentthenumberofclassesinthesourceandtargetlanguages,respec-tively.layerno.unitsinputalphabetsize(x)300embeddingsizes256charid56(forward)400spacesid56(forward)400spacesid56(backward)400attention300chardecoder400targetalphabetsize(t)300table2.hyperparametervaluesusedfortrainingthechar2word-to-charmodel.where  srcand  trgrepre-sentthenumberofclassesinthesourceandtargetlanguages,respectively.timesteps,whichcanresultinalotofwastedresources[han-nunetal.,2014](see   gure4).trainingtranslationmodelsisfurthercomplicatedbythefactthatsourceandtargetsen-tences,whilecorrelated,mayhavedifferentlengths,anditisnecessarytoconsiderbothwhenconstructingbatchesinordertoutilizecomputationpowerandramoptimally.tocircumventthisissue,westarteachepochbyshuf   ingallsamplesinthedatasetandsortingthemwithastablesort-ingalgorithmaccordingtoboththesourceandtargetsentencelengths.thisensuresthatanytwosamplesinthedatasetthathavealmostthesamesourceandtargetsentencelengthsarelocatedclosetoeachotherinthesortedlistwhiletheexactorderofsamplesvariesbetweenepochs.topackabatchwesimplystartedaddingsamplesfromthesortedsamplelisttothebatch,untilwereachedthemaximaltotalallowedcharac-terthreshold(whichwesetto50,000)forthefullbatchwithpaddingafterwhichwewouldstartonanewbatch.finallyallthebatchesarefedinrandomordertothemodelfortrain-inguntilallsampleshavebeentrainedon,andanewepochbegins.figure5illustrateswhatsuchdynamicbatchesmightlooklike.fig.4.aregularbatchwithrandomsamples.fig.5.ourdynamicbatchesofvariablebatchsizeandse-quencelength.5.3.results5.3.1.quantitativethequantitativeresultsofourmodelsareillustratedinta-ble3.noticethatthechar2word-to-charmodelout-performsthechar-to-charmodelonalldatasets(average1.28id7performanceincrease).thiscouldbeanindica-tionthateitherhavinghierarchical,word-like,representationsontheencoderorsimplythefactthattheencoderwassigni   -cantlysmaller,helpsinid4whenusingacharacterdecoderwithattention.out-of-memory: what to do

little effect on quality:
reduce batch size
remove long sentences (also in validation!)
tie embedding layer and output layer in decoder [press and wolf, 2017]
(   --tie_decoder_embeddings    in nematus)
model parallelism: different parts of model on different gpu

unknown (or negative) effect on quality:

reduce layer size
reduce target vocabulary

sennrich, haddow

practical id4

80 / 109

training and convergence

y
p
o
r
t
n
e
-
s
s
o
r
c

120

100

80

60

cross-id178 (per sentence)
cross-id178 (per sentence)
id7

0

10

20

30

training time (training instances   105)

id7 more unstable than cross-id178
useful convergence criteria: id7 early stopping

20

15

u
e
l
b

10

5

sennrich, haddow

practical id4

81 / 109

decoding ef   ciency

how to make decoding fast?

small beam size is often suf   cient
greedy decoding can be competitive in quality
    especially with knowledge distillation [kim and rush, 2016]
filter output vocabulary [jean et al., 2015, l   hostis et al., 2016]
based on which words commonly co-occur with source words
process multiple sentences in batch [wu et al., 2016]
low-precision arithmetic [wu et al., 2016]
(requires suitable hardware)

nb: amun supports batching, vocabulary    ltering

sennrich, haddow

practical id4

82 / 109

decoding speed: nematus vs. amun

single gpu, single model, titan x (pascal)

sennrich, haddow

practical id4

83 / 109

improving translation quality

there are many possible ways of improving the basic system

1

improve corpus preparation

2 id20
3 obtain appropriate synthetic data
4 hybrid of id4 and traditional smt
5 add extra linguistic information
6 minimum risk training
7 deep models
8 hyperparameter exploration

sennrich, haddow

practical id4

84 / 109

corpus preparation

cleaning

tokenisation

case normalisation

subid40

sennrich, haddow

practical id4

85 / 109

punctuation/encoding/spelling normalisation
language identi   cation
removing non-parallel segments

corpus preparation

cleaning

tokenisation

case normalisation

subid40

sennrich, haddow

practical id4

85 / 109

corpus preparation

cleaning

tokenisation

punctuation/encoding/spelling normalisation
language identi   cation
removing non-parallel segments

case normalisation

lowercasing
truecasing (convert to most frequent)
headlines etc.

subid40

sennrich, haddow

practical id4

85 / 109

corpus preparation

cleaning

tokenisation

punctuation/encoding/spelling normalisation
language identi   cation
removing non-parallel segments

case normalisation

lowercasing
truecasing (convert to most frequent)
headlines etc.

subid40

statistical
linguistically motivated

sennrich, haddow

practical id4

85 / 109

effect of noise in training data

[chen et al., 2016] add noise to wmt en-fr training data
arti   cial noise: permute order of target sentences
conclusion: id4 is more sensitive to (some types of) noise than smt

results from presentation of [chen et al., 2016] at amta 2016

sennrich, haddow

practical id4

86 / 109

   phrase-based smt is robust to noise (goutte et al., 2012)   performance is hardly affected when the misalignment rate is below 30%, and introducing 50% alignment error brings performance down less than 1 id7 point.   but does this also hold for id4?   no!   wmt en2fr task (12m training pairs), on newstest14 test21noise reduction is more important to id4id20 with continued training

sgd is sensitive to order of training instances
best practice:

   rst train on all available data
continue training on in-domain data

large id7 improvements reported with minutes of training time
[sennrich et al., 2016, luong and manning, 2015, crego et al., 2016]

u
e
l
b

40.0

30.0

20.0

10.0

0.0

fine-tuning in iwslt (en-de)

30.4

26.5

25.9

25.5

23.5

28.4

baseline

finetuned

tst2013

tst2014

tst2015

generic system (    8m sentences), fine-tune with ted (    200k )

sennrich, haddow

practical id4

87 / 109

continued training with synthetic data

what if we have monolingual in-domain training data?
we compare    ne-tuning with:

200 000 sentence pairs in-domain
200 000 target-language sentences in-domain, plus automatic
back-translation

system
wmt data
   ne-tuned on in-domain (parallel)
   ne-tuned on in-domain (synthetic)

id7 (tst2015)

25.5
28.4
26.7

english   german translation performance on iwslt test set (ted talks).

    parallel in-domain data is better, but id20 with
monolingual data is possible
    wmt16 results (using large synthetic news corpora)

sennrich, haddow

practical id4

[sennrich et al., 2016]

88 / 109

continued training with synthetic data

problem
how to create synthetic data from source-language in-domain data?

sennrich, haddow

practical id4

89 / 109

continued training with synthetic data

problem
how to create synthetic data from source-language in-domain data?

solution

1 gather source-language in-domain data.
2 translate to target language
3 use this translated data to select from commoncrawl corpus
4 back-translate selected data to create synthetic data

sennrich, haddow

practical id4

89 / 109

continued training with synthetic data

setup

language pairs: english     czech, german, polish and romanian
domains: two healthcare websites (nhs 24 and cochrane)
baselines: data drawn from wmt releases and opus
fine-tuning:

use crawls of full websites as selection    seed   
continue training with 50-50 synthetic/parallel mix

u
e
l
b

40.0

30.0

20.0

10.0

0.0

cochrane

37.6

38.5

34.4

31.5

33.4

30.2

19.1

15.5

en   cs

en   de
baseline

en   pl

en   ro
finetuned

u
e
l
b

40.0

30.0

20.0

10.0

0.0

nhs 24

31.6

32.9

28.6

29.7

26.7

23.1

24.2

19.5

en   cs

en   de
baseline

en   pl

en   ro
finetuned

sennrich, haddow

practical id4

90 / 109

continued training with synthetic data:
sample learning curve

english   polish, select using cochrane
main training on general domain,    netune on 50-50 mix

sennrich, haddow

practical id4

91 / 109

050100150200iterations (x 10000)05101520id7 on development setmainfine-tunenematus domain interpolation

general

in-domain

mini-batches

use domain interpolation to mix general and in-domain

--use_domain_interpolation
--domain_interpolation_indomain_datasets
--domain_interpolation_(min|max)
--domain_interpolation_inc

sennrich, haddow

practical id4

92 / 109

id4 hybrid models

model combination (ensembling) is well established
several ways to combine id4 with pbmt / syntax-based mt:

re-ranking output of traditional smt with id4 [neubig et al., 2015]
incorporating id4 as feature function in pbmt
[junczys-dowmunt et al., 2016b]
rescoring hiero lattices with id4 [stahlberg et al., 2016]

reduces chance of    bizarre    id4 outputs

sennrich, haddow

practical id4

93 / 109

id4 hybrid models: case study

id4 as feature function in pbmt [junczys-dowmunt et al., 2016b]
    results depend on relative performance of pbmt and id4

26.0

25.9

22.8

27.5

28.1

29.9

30.0

u
e
l
b

20.0

10.0

0.0

english   russian
phrase-based smt

russian   english
neural mt

hybrid

sennrich, haddow

practical id4

94 / 109

why linguistic features?

disambiguate words by pos

english
german
closeverb
schlie  en
closeadj
nah
closenoun ende

source
reference
baseline id4

we thought a win like this might be closeadj.
wir dachten, dass ein solcher sieg nah sein k  nnte.
*wir dachten, ein sieg wie dieser k  nnte schlie  en.

sennrich, haddow

practical id4

95 / 109

why linguistic features?

better generalization; combat data sparsity

word form
liegen (lie)
liegst (lie)
lag (lay)
l  ge (lay)

sennrich, haddow

practical id4

96 / 109

why linguistic features?

better generalization; combat data sparsity

word form lemma
liegen (lie)
liegst (lie)
lag (lay)
l  ge (lay)

liegen (lie)
liegen (lie)
liegen (lie)
liegen (lie)

morph. features
(3.p.pl. present)
(2.p.sg. present)
(3.p.sg. past)
(3.p.sg. subjunctive ii)

sennrich, haddow

practical id4

96 / 109

id4: multiple input features

use separate embeddings for each feature, then concatenate

baseline: only word feature

e(close) =            

0.5
0.2
0.3
0.1

            

|f| input features

e1(close) =      

0.4
0.1

0.2       e2(adj) =(cid:2)0.1(cid:3) e1(close) (cid:107) e2(adj) =            

0.4
0.1
0.2
0.1

            

sennrich, haddow

practical id4

97 / 109

experiments

features

lemmas
morphological features
pos tags
dependency labels
bpe tags

data

wmt16 training/test data
english   german and english   romanian

sennrich, haddow

practical id4

98 / 109

results: id7    

40.0

33.1

33.2

32.9

31.4

30.0

27.8

28.4

38.5

37.5

29.2

28.2

24.8

23.8

u
e
l
b

20.0

10.0

0.0

english   german

german   english

english   romanian

baseline
all features
baseline (+synthetic data)
all features (+synthetic data)

sennrich, haddow

practical id4

99 / 109

minimum risk training [shen et al., 2016]

the standard id4 training objective is cross-id178
    maximise id203 of training data
in traditional smt, we usually tune for id7
can train id4 to minimise expected loss

s(cid:88)s=1

e

p(y|x(s))(cid:104)   (y, y(s))(cid:105)

(id168:     ; training pair: (x(s), y(s)) )
run mrt after training with cross-id178 loss
approximate expectation with sum over samples

sennrich, haddow

practical id4

100 / 109

minimum risk training in nematus

recipe:

train initial model with standard cross-id178 training
continue training with    --objective mrt   

sensitive to hyperparameters
    use small learning rate with sgd
mixed results:

improvements over some baselines (en   ro parallel)
no improvement so far over others (en   de with synthetic data)

sennrich, haddow

practical id4

101 / 109

deep models

deep architecture by [zhou et al., 2016]

deep recurrent architectures [zhou et al., 2016, wu et al., 2016]
[zhou et al., 2016] report +4 id7 from 16 id56 layers:
(9 encoder; 7 decoder)
important trick: residual connections
challenges: ef   ciency; memory limitations

sennrich, haddow

practical id4

102 / 109

+iit.rrrrit.irrrrrrrrrrrr               k = 2 ...encoding vectorsje.rrr                  predictedwords1e      2e      3e      4e      1e      2e      3e      4e      1e      2e      3e      4e      1e      2e      3e      4e      2d3d4d411iia         ia<s>r1dfcenjoyenjoyl'app-r  cieffffffffffffffffffffencoderinterfacedecoderie      ie      rf-f connectionf-f connectionf-f connection1c2c3c4c'iek = 1... k  = 1               figure2:thenetwork.itincludesthreepartsfroid113fttoright:encoderpart(p-e),interface(p-i)anddecoderpart(p-d).weonlyshowthetopologyofdeep-attasanexample.   f   and   r   blockscorrespondtothefeed-forwardpartandthesubsequentlstmcomputation.thef-fconnectionsaredenotedbydashedredlines.connectionscanacceleratethemodelconvergenceandwhileimprovingtheperformance.asimilarideawasalsousedin(heetal.,2016;zhouandxu,2015).encoder:thelstmlayersarestackedfollowingeq.5.wecallthistypeofencoderinterleavedbi-directionalencoder.inaddition,therearetwosim-ilarcolumns(a1anda2)intheencoderpart.eachcolumnconsistsofnestackedlstmlayers.thereisnoconnectionbetweenthetwocolumns.the   rstlayersofthetwocolumnsprocessthewordrepre-sentationsofthesourcesequenceindifferentdirec-tions.atthelastlstmlayers,therearetwogroupsofvectorsrepresentingthesourcesequence.thegroupsizeisthesameasthelengthoftheinputse-quence.interface:priorencoder-decodermodelsandatten-tionmodelsaredifferentintheirmethodofextract-ingtherepresentationsofthesourcesequences.inourwork,asaconsequenceoftheintroducedf-fconnections,wehave4outputvectors(hnetandfnetofbothcolumns).therepresentationsaremodi   edforbothdeep-edanddeep-att.fordeep-ed,etisstaticandconsistsoffourparts.1:thelasttimestepoutputhnemofthe   rstcolumn.2:max-operationmax(  )ofhnetatalltimestepsofthesecondcolumn,denotedbymax(hne,a2t).max(  )denotesobtainingthemaximalvalueforeachdimensionovert.3:max(fne,a1t).4:max(fne,a2t).themax-operationandlasttimestepstateextractionprovidecompli-mentaryinformationbutdonotaffecttheperfor-mancemuch.etisusedasthe   nalrepresentationct.fordeep-att,wedonotneedtheabovetwoop-erations.weonlyconcatenatethe4outputvectorsateachtimesteptoobtainet,andasoftattentionmechanism(bahdanauetal.,2015)isusedtocalcu-latethe   nalrepresentationctfromet.etissumma-rizedas:deep-ed:et[hne,a1m,max(hne,a2t),max(fne,a1t),max(fne,a2t)]deep-att:et[hne,a1t,hne,a2t,fne,a1t,fne,a2t](7)notethatthevectordimensionalityoffisfourtimeslargerthanthatofh(seeeq.4).ctissummarizedas:deep-ed:ct=et,(const)deep-att:ct=mxt0=1  t,t0wpet0(8)  t,t0isthenormalizedattentionweightcomputedby:  t,t0=exp(a(wpet0,h1,dect   1))pt00exp(a(wpet00,h1,dect   1))(9)h1,dect   1isthe   rsthiddenlayeroutputinthedecodingpart.a(  )isanalignmentmodeldescribedin(bah-danauetal.,2015).fordeep-att,inordertore-ducethememorycost,welinearlyproject(withwp)hyperparameter exploration

massive exploration of id4 architectures
[britz et al., 2017]

spent 250,000 hours gpu time exploring hyperparameters
conclusions:

small gain from increasing embedding size
lstm better than gru
2-4 layer bidirectional encoder better
4-layer decoder gives some advantage
additive better than multiplicative attention
large beams not helpful (best = 10)

id7 variance across runs small (   0.2-0.3)

sennrich, haddow

practical id4

103 / 109

practical id4

1

introduction

2 neural networks     basics
3 language models using neural networks
4 attention-based id4 model
5 edinburgh   s wmt16 system
6 analysis: why does id4 work so well?
7 building and improving id4 systems
8 resources, further reading and wrap-up

sennrich, haddow

practical id4

104 / 109

getting started: do it yourself

sample    les and instructions for training id4 model
https://github.com/rsennrich/wmt16-scripts
pre-trained models to test decoding (and for further experiments)
http://statmt.org/rsennrich/wmt16_systems/
lab on installing/using nematus:
http://www.statmt.org/eacl2017/practical-id4-lab.pdf

sennrich, haddow

practical id4

105 / 109

(a small selection of) resources

id4 tools

nematus (theano) https://github.com/rsennrich/nematus
openid4 (torch) https://github.com/openid4/openid4
id4.matlab https://github.com/lmthang/id4.matlab
neural monkey (tensor   ow) https://github.com/ufal/neuralmonkey
lamtram (dynet) https://github.com/neubig/lamtram
...and many more https://github.com/jonsafari/id4-list

sennrich, haddow

practical id4

106 / 109

further reading

secondary literature

lecture notes by kyunghyun cho: [cho, 2015]
chapter on neural network models in    id151   
by philipp koehn http://mt-class.org/jhu/assets/papers/neural-network-models.pdf
tutorial on sequence-to-sequence models by graham neubig

https://arxiv.org/abs/1703.01619

sennrich, haddow

practical id4

107 / 109

acknowledgments
this project has received funding from the european union   s
horizon 2020 research and innovation programme under
grant agreements 645452 (qt21) and 644402 (himl).

sennrich, haddow

practical id4

108 / 109

questions

thank you!

sennrich, haddow

practical id4

109 / 109

bibliography i

allen, r. (1987).
several studies on natural language and back-propagation.
in ieee first international conference on neural networks, pages 335   341, san diego, california, usa.

bahdanau, d., cho, k., and bengio, y. (2015).
id4 by jointly learning to align and translate.
in proceedings of the international conference on learning representations (iclr).

bengio, y., ducharme, r., vincent, p., and janvin, c. (2003).
a neural probabilistic language model.
j. mach. learn. res., 3:1137   1155.

bentivogli, l., bisazza, a., cettolo, m., and federico, m. (2016).
neural versus phrase-based machine translation quality: a case study.
in emnlp 2016.

britz, d., goldie, a., luong, t., and le, q. (2017).
massive exploration of id4 architectures.
arxiv e-prints.

chen, b., kuhn, r., foster, g., cherry, c., and huang, f. (2016).
bilingual methods for adaptive training data selection for machine translation.
in proceedings of amta.

cho, k. (2015).
natural language understanding with distributed representation.
corr, abs/1511.07916.

sennrich, haddow

practical id4

110 / 109

bibliography ii

cho, k., courville, a., and bengio, y. (2015).
describing multimedia content using attention-based encoder-decoder networks.

cho, k., van merrienboer, b., bahdanau, d., and bengio, y. (2014).
on the properties of id4: encoder-decoder approaches.
arxiv e-prints.

crego, j., kim, j., klein, g., rebollo, a., yang, k., senellart, j., akhanov, e., brunelle, p., coquard, a., deng, y., enoue, s.,
geiss, c., johanson, j., khalsa, a., khiari, r., ko, b., kobus, c., lorieux, j., martins, l., nguyen, d.-c., priori, a., riccardi, t.,
segal, n., servan, c., tiquet, c., wang, b., yang, j., zhang, d., zhou, j., and zoldan, p. (2016).
systran   s pure id4 systems.
arxiv e-prints.

devlin, j., zbib, r., huang, z., lamar, t., schwartz, r., and makhoul, j. (2014).
fast and robust neural network joint models for id151.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers), pages
1370   1380, baltimore, maryland. association for computational linguistics.

gage, p. (1994).
a new algorithm for data compression.
c users j., 12(2):23   38.

gal, y. (2015).
a theoretically grounded application of dropout in recurrent neural networks.
arxiv e-prints.

ha, t.-l., niehues, j., cho, e., mediani, m., and waibel, a. (2015).
the kit translation systems for iwslt 2015.
in proceedings of the international workshop on spoken language translation (iwslt), pages 62   69.

sennrich, haddow

practical id4

111 / 109

bibliography iii

haddow, b., huck, m., birch, a., bogoychev, n., and koehn, p. (2015).
the edinburgh/jhu phrase-based machine translation systems for wmt 2015.
in proceedings of the tenth workshop on id151, pages 126   133, lisbon, portugal. association for
computational linguistics.

jean, s., cho, k., memisevic, r., and bengio, y. (2015).
on using very large target vocabulary for id4.
in
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers),
pages 1   10, beijing, china. association for computational linguistics.

johansen, a. r., hansen, j. m., obeid, e. k., s  nderby, c. k., and winther, o. (2016).
id4 with characters and hierarchical encoding.
corr, abs/1610.06550.

junczys-dowmunt, m., dwojak, t., and hoang, h. (2016a).
is id4 ready for deployment? a case study on 30 translation directions.
in proceedings of iwslt.

junczys-dowmunt, m., dwojak, t., and sennrich, r. (2016b).
the amu-uedin submission to the wmt16 news translation task: attention-based id4 models as feature functions in
phrase-based smt.
in proceedings of the first conference on machine translation, volume 2: shared task papers, pages 316   322, berlin,
germany. association for computational linguistics.

junczys-dowmunt, m. and grundkiewicz, r. (2016).
log-linear combinations of monolingual and bilingual id4 models for automatic post-editing.
in proceedings of the first conference on machine translation, pages 751   758, berlin, germany. association for computational
linguistics.

sennrich, haddow

practical id4

112 / 109

bibliography iv

kalchbrenner, n. and blunsom, p. (2013).
recurrent continuous translation models.
in proceedings of the 2013 conference on empirical methods in natural language processing, seattle. association for
computational linguistics.

kalchbrenner, n., grefenstette, e., and blunsom, p. (2014).
a convolutional neural network for modelling sentences.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 1: long papers).

kim, y. and rush, a. m. (2016).
sequence-level knowledge distillation.
corr, abs/1606.07947.

lee, j., cho, k., and hofmann, t. (2016).
fully character-level id4 without explicit segmentation.
arxiv e-prints.

l   hostis, g., grangier, d., and auli, m. (2016).
vocabulary selection strategies for id4.
arxiv e-prints.

luong, m.-t. and manning, c. d. (2015).
stanford id4 systems for spoken language domains.
in proceedings of the international workshop on spoken language translation 2015, da nang, vietnam.

luong, t., sutskever, i., le, q. v., vinyals, o., and zaremba, w. (2014).
addressing the rare word problem in id4.
corr, abs/1410.8206.

sennrich, haddow

practical id4

113 / 109

bibliography v

mikolov, t., kara     t, m., burget, l., cernock  , j., and khudanpur, s. (2010).
recurrent neural network based language model.
in
interspeech 2010, 11th annual conference of the international speech communication association, makuhari, chiba, japan, september 26-30, 2010,
pages 1045   1048.

neubig, g., morishita, m., and nakamura, s. (2015).
neural reranking improves subjective quality of machine translation: naist at wat2015.
in proceedings of the 2nd workshop on asian translation (wat2015), pages 35   41, kyoto, japan.

press, o. and wolf, l. (2017).
using the output embedding to improve language models.
in proceedings of the 15th conference of the european chapter of the association for computational linguistics (eacl),
valencia, spain.

schwenk, h., dechelotte, d., and gauvain, j.-l. (2006).
continuous space language models for id151.
in proceedings of the coling/acl 2006 main conference poster sessions, pages 723   730, sydney, australia.

sennrich, r. (2017).
how grammatical is character-level id4? assessing mt quality with contrastive translation pairs.
in proceedings of the 15th conference of the european chapter of the association for computational linguistics (eacl),
valencia, spain.

sennrich, r., firat, o., cho, k., birch, a., haddow, b., hitschler, j., junczys-dowmunt, m., l  ubli, s., barone, a. v. m., and
n  adejde, m. (2017).
nematus: a toolkit for id4.
in proceedings of eacl (demo session).

sennrich, haddow

practical id4

114 / 109

bibliography vi

sennrich, r. and haddow, b. (2015).
a joint dependency model of morphological and syntactic structure for id151.
in proceedings of the 2015 conference on empirical methods in natural language processing, pages 2081   2087, lisbon,
portugal. association for computational linguistics.

sennrich, r., haddow, b., and birch, a. (2016).
improving id4 models with monolingual data.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
86   96, berlin, germany. association for computational linguistics.

shen, s., cheng, y., he, z., he, w., wu, h., sun, m., and liu, y. (2016).
minimum risk training for id4.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers).

stahlberg, f., hasler, e., waite, a., and byrne, b. (2016).
syntactically guided id4.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: short papers), pages
299   305, berlin, germany. association for computational linguistics.

sutskever, i., vinyals, o., and le, q. v. (2014).
sequence to sequence learning with neural networks.
in
advances in neural information processing systems 27: annual conference on neural information processing systems 2014,
pages 3104   3112, montreal, quebec, canada.

vaswani, a., zhao, y., fossum, v., and chiang, d. (2013).
decoding with large-scale neural language models improves translation.
in proceedings of the 2013 conference on empirical methods in natural language processing, emnlp 2013, pages
1387   1392, seattle, washington, usa.

sennrich, haddow

practical id4

115 / 109

bibliography vii

wu, y., schuster, m., chen, z., le, q. v., norouzi, m., macherey, w., krikun, m., cao, y., gao, q., macherey, k., klingner, j.,
shah, a., johnson, m., liu, x., kaiser,   ., gouws, s., kato, y., kudo, t., kazawa, h., stevens, k., kurian, g., patil, n., wang, w.,
young, c., smith, j., riesa, j., rudnick, a., vinyals, o., corrado, g., hughes, m., and dean, j. (2016).
google   s id4 system: bridging the gap between human and machine translation.
arxiv e-prints.

zhou, j., cao, y., wang, x., li, p., and xu, w. (2016).
deep recurrent models with fast-forward connections for id4.
transactions of the association of computational linguistics     volume 4, issue 1, pages 371   383.

sennrich, haddow

practical id4

116 / 109

