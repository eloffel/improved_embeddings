deep learning for id161

lex fridman

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

id161 is machine learning

supervised 
learning

unsupervised 

learning

semi-supervised

learning

reinforcement

learning

id161

references: [81]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

images are numbers

    regression: the output variable takes continuous values

    classification: the output variable takes class labels
    underneath it may still produce continuous values such as 

id203 of belonging to a particular class.

references: [89]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

human vision seems easy         why: data

visual perception: 540 millions years of data

bipedal movement: 230+ million years of data
abstract thought: 100 thousand years of data

   encoded in the large, highly evolved sensory and motor portions of the human brain is a 
billion years of experience about the nature of the world and how to survive in it.   
abstract thought, though, is a new trick, perhaps less than 100 thousand years old. we have 
not yet mastered it. it is not all that intrinsically difficult; it just seems so when we do it.   
- hans moravec, mind children (1988)

hans moravec (cmu)

rodney brooks (mit)

marvin minsky (mit)

references: [6, 7, 11]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

human vision
its structure is instructive and inspiring!

thalamocortical system simulation: 8 million cortical neurons +  2 billion synapses:

references: [118]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

visual cortex

(its structure is instructive and inspiring)

reference: https://www.youtube.com/watch?v=_33k1zttoow

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

id161 is hard

references: [66, 69, 89]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

image classification pipeline

references: [81, 89]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

famous id161 datasets

mnist: handwritten digits

id163: id138 hierarchy

cifar-10(0): tiny images

places: natural scenes

references: [90, 91, 92, 93]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

let   s build an image classifier for cifar-10

references: [89, 91]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

let   s build an image classifier for cifar-10

accuracy
random: 10%
our image-diff (with l1): 38.6%
our image-diff (with l2): 35.4%

references: [89, 91]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

k-nearest neighbors: generalizing the image-diff classifier

tuning (hyper)parameters:

references: [89]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

k-nearest neighbors: generalizing the image-diff classifier

accuracy
random: 10%
training and testing on the same data: 35.4%
7-nearest neighbors: ~30%
human: ~94%
   
convolutional neural networks: ~95%

references: [89, 94]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

reminder: weighing the evidence

e
c
n
e
d
i
v
e

d
e
c
i
s
i
o
n
s

references: [78]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

reminder:    learning    is optimization of a function

ground truth for    6   :

   loss    function:

references: [63, 80]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

classify and image of a number

input:
(28x28)

network:

references: [80]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

convolutional neural networks

regular neural network (fully connected):

convolutional neural network:

each layer takes a 3d volume, produces 3d volume with some 
smooth function that may or may not have parameters.

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

convolutional neural networks: layers

   

input [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and 
with three color channels r,g,b.

    conv layer will compute the output of neurons that are connected to local regions in the input, each computing 

a dot product between their weights and a small region they are connected to in the input volume. this may 
result in volume such as [32x32x12] if we decided to use 12 filters.

    relu layer will apply an elementwise activation function, such as the max(0,x) thresholding at zero. this leaves 

the size of the volume unchanged ([32x32x12]).

    pool layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in 

volume such as [16x16x12].

   

fc (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of 
the 10 numbers correspond to a class score, such as among the 10 categories of cifar-10. as with ordinary 
neural networks and as the name implies, each neuron in this layer will be connected to all the numbers in the 
previous volume.

layers highlighted in blue 
have learnable parameters.

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

dealing with images: local connectivity

same neuron. just more focused (narrow    receptive field   ).

the parameters on a each filter are spatially    shared   

(if a feature is useful in one place, it   s useful elsewhere)

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

convnets: spatial arrangement of output volume

    depth: number of filters

    stride: filter step size (when we    slide    it)
    padding: zero-pad the input

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

convolution

references: [124]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

convolution

references: [124]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

convolution: representation learning

references: [124]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

convnets: pooling

references: [95]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

same architecture, many applications

this part might look different for:
    different image classification domains
    image captioning with recurrent neural networks
    image object localization with bounding box
    image segmentation with fully convolutional networks
    image segmentation with deconvolution layers

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

object recognition

case study: id163

references: [4]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

what is id163?

    id163: dataset of 14+ million images (21,841 categories)

    links to images not images

    let   s take the high level category of fruit as an example:

    total 188,000 images of fruit 
    there are 1206 granny smith apples:

references: [90]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

what is id163?

dataset

    id163: dataset of 14+ million images 

competition

    ilsvrc: id163 large scale visual 

recognition challenge

networks

    alexnet (2012)
    zfnet (2013)
    vggnet (2014)

    googlenet (2014)
    resnet (2015)
    cuimage (2016)

references: [90]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

ilsvrc challenge evaluation for classification

    top 5 error rate:

    you get 5 guesses to get the correct label

    ~20% reduction in accuracy for top 1 vs top 5

    example: in 2012 alexnet achieved 

    human annotation is a binary task:    apple    or    not apple   

references: [123]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

    alexnet (2012): first id98 (15.4%)

    8 layers
    61 million parameters

    zfnet (2013): 15.4% to 11.2%

    8 layers
    more filters. denser stride.

    vggnet (2014): 11.2% to 7.3%

    beautifully uniform:

3x3 conv, stride 1, pad 1, 2x2 max pool

    16 layers
    138 million parameters

    googlenet (2014): 11.2% to 6.7%

inception modules

   
    22 layers
    5 million parameters

(throw away fully connected layers)

    resnet (2015): 6.7% to 3.57%

    more layers = better performance
    152 layers

    cuimage (2016): 3.57% to 2.99%

    ensemble of 6 models

references: [90]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

    alexnet (2012): first id98 (15.4%)

    8 layers
    61 million parameters

    zfnet (2013): 15.4% to 11.2%

    8 layers
    more filters. denser stride.

    vggnet (2014): 11.2% to 7.3%

    beautifully uniform:

3x3 conv, stride 1, pad 1, 2x2 max pool

    16 layers
    138 million parameters

    googlenet (2014): 11.2% to 6.7%

inception modules

   
    22 layers
    5 million parameters

(throw away fully connected layers)

    resnet (2015): 6.7% to 3.57%

    more layers = better performance
    152 layers

    cuimage (2016): 3.57% to 2.99%

    ensemble of 6 models

krizhevsky et al. "id163 classification with deep convolutional neural 
networks." advances in neural information processing systems. 2012.

references: [4]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

    alexnet (2012): first id98 (15.4%)

    8 layers
    61 million parameters

    zfnet (2013): 15.4% to 11.2%

    8 layers
    more filters. denser stride.

    vggnet (2014): 11.2% to 7.3%

    beautifully uniform:

3x3 conv, stride 1, pad 1, 2x2 max pool

    16 layers
    138 million parameters

    googlenet (2014): 11.2% to 6.7%

inception modules

   
    22 layers
    5 million parameters

(throw away fully connected layers)

    resnet (2015): 6.7% to 3.57%

    more layers = better performance
    152 layers

    cuimage (2016): 3.57% to 2.99%

    ensemble of 6 models

simonyan et al. "very deep convolutional networks 
for large-scale image recognition." 2014.

references: [128]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

    alexnet (2012): first id98 (15.4%)

    8 layers
    61 million parameters

    zfnet (2013): 15.4% to 11.2%

    8 layers
    more filters. denser stride.

    vggnet (2014): 11.2% to 7.3%

    beautifully uniform:

3x3 conv, stride 1, pad 1, 2x2 max pool

    16 layers
    138 million parameters

    googlenet (2014): 11.2% to 6.7%

inception modules

   
    22 layers
    5 million parameters

(throw away fully connected layers)

    resnet (2015): 6.7% to 3.57%

    more layers = better performance
    152 layers

    cuimage (2016): 3.57% to 2.99%

    ensemble of 6 models

szegedy et al. "going deeper with convolutions." proceedings of the 
ieee conference on id161 and pattern recognition. 2015.

references: [129]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

    alexnet (2012): first id98 (15.4%)

    8 layers
    61 million parameters

    zfnet (2013): 15.4% to 11.2%

    8 layers
    more filters. denser stride.

    vggnet (2014): 11.2% to 7.3%

    beautifully uniform:

3x3 conv, stride 1, pad 1, 2x2 max pool

    16 layers
    138 million parameters

    googlenet (2014): 11.2% to 6.7%

inception modules

   
    22 layers
    5 million parameters

(throw away fully connected layers)

    resnet (2015): 6.7% to 3.57%

    more layers = better performance
    152 layers

    cuimage (2016): 3.57% to 2.99%

    ensemble of 6 models

he, kaiming, et al. "deep residual learning for image recognition." proceedings 
of the ieee conference on id161 and pattern recognition. 2016.

references: [130]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

same architecture, many applications

this part might look different for:
    different image classification domains
    image captioning with recurrent neural networks
    image object localization with bounding box
    image segmentation with fully convolutional networks
    image segmentation with deconvolution layers

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

segmentation

original

ground truth

fcn-8

references: [96]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

id164

references: [97]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

applications: image id134

references: [43     fang et al. 2015]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

applications: image id53

ren et al. "exploring models and data for image 
id53." 2015.

code: https://github.com/renmengye/imageqa-public

references: [40]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

applications: video description generation

venugopalan et al.
"sequence to sequence-video to text."  2015.

code: https://vsubhashini.github.io/s2vt.html

references: [41, 42]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

applications: modeling attention steering

jimmy ba, volodymyr mnih, and koray
kavukcuoglu. "multiple object recognition 
with visual attention." (2014).

references: [35, 36]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

application: audio classification

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

driving scene segmentation

references: [127]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

end-to-end learning of the driving task

references: http://cars.mit.edu/deeptesla

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

id161 for intelligent systems

references: [120]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

open problem: robustness

>99.6% confidence in the wrong answer

nguyen et al. "deep neural networks are easily fooled: high confidence predictions for unrecognizable images." 2015.

references: [67]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

open problem: robustness

fooled by a little distortion

szegedy et al. "intriguing properties of neural networks." 2013.

references: [68]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

object category recognition

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

object category recognition

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

object category recognition

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

object category recognition

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

object category recognition

references: [121]

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

references

all references cited in this presentation are listed in the 
following google sheets file: 

https://goo.gl/9xhp2t

course 6.s191:
intro to deep learning

lex fridman:
fridman@mit.edu

january
2017

