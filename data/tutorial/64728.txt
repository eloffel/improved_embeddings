understanding id3

balaji lakshminarayanan

joint work with:

shakir mohamed, mihaela rosca, ivo danihelka, david warde-farley, 
liam fedus, ian goodfellow, andrew dai & others

understanding gans  

balaji lakshminarayanan

problem statement
learn a generative model:

goal: given samples x1 ... xn  from true distribution p*(x), find   
p  (x) is not available -> can   t maximize density directly
however, we can sample from p  (x) efficiently

understanding gans  

balaji lakshminarayanan

high level overview of gans 

goodfellow et al. 2014

discriminator: train a classifier to distinguish between 
the two distributions using samples

generator: train to generate samples that fool the 
discriminator

minimax game alternates between training discriminator 
and generator

- nash equilibrium corresponds to minima of jensen 

shannon divergence

- need a bunch of tricks to stabilize training in practice

understanding gans  

balaji lakshminarayanan

     

gans: hope or hype?

https://github.com/junyanz/cyclegan/blob/master/imgs/horse2zebra.gif

https://github.com/hindupuravinash/the-gan-zoo

https://github.com/tkarras/progressive_growing_of_gans

understanding gans  

balaji lakshminarayanan

how do gans relate to other ideas in 
probabilistic machine learning? 

learning in implicit generative models

shakir mohamed* and balaji lakshminarayanan*

understanding gans  

balaji lakshminarayanan

implicit models
stochastic procedure that 
generates data

prescribed models

provide knowledge of the id203 
of observations & specify a 
conditional log-likelihood function.

examples: stochastic simulators of 
complex physical systems (climate, 
ecology, high-energy physics etc) 

understanding gans  

balaji lakshminarayanan

learning by comparison

we compare the estimated 

distribution to the true distribution 

using samples.

understanding gans  

balaji lakshminarayanan

learning by comparison

comparison

use a hypothesis test or comparison to 
build an auxiliary model to indicate how 
data simulated from the model differs 

from observed data. 

learning generator

adjust model parameters to better 
match the data distribution using the 
comparison.

understanding gans  

balaji lakshminarayanan

high-level idea

define a joint id168 l(  ,   ) and alternate between:

comparison loss (   discriminator   ): arg min   l(  ,   )
generative loss: arg min      l(  ,   )
global optimum is q   = p* and 
- density ratio r   = 1  or
- density difference r   = 0

how do we compare distributions?

understanding gans  

balaji lakshminarayanan

density ratios and classification

density 
ratio

combine data

assign labels

bayes    
rule

real data

simulated data

sugiyama et al, 2012

understanding gans  

balaji lakshminarayanan

density ratios and classification

density ratio

bayes    substitution

class id203

computing a density ratio is equivalent to class id203 estimation.

understanding gans  

balaji lakshminarayanan

class id203 estimation

other id168s for training classifier, e.g. brier score leads to ls-gan

related: unsupervised as supervised learning, classifier abc 

divergence minimization (f-gan)

minimize a lower bound on f-divergence between p* and q     
choices of f recover kl(p*||q) (maximum likelihood), kl(q||p*) and js(p*||q)

can use different f-divergences for learning ratio vs learning generator

density ratio estimation

optimize a bregman divergence between r* and r   
special cases include least squares importance fitting (lsif)

ratio loss ends up being identical to that of f-divergence

moment-matching

used by 

- generative moment matching networks
- training generative neural networks via maximum mean discrepancy 

optimization

connects to optimal transport literature (e.g. wasserstein gan)

summary of the approaches 

class id203 estimation

density ratio matching

- build a classifier to distinguish 

real from fake samples. 

- original gan solution.

- directly minimise the expected 
error between the true ratio and 
an estimate of it.

divergence minimisation

moment matching

- minimise a generalised divergence 
between the true density p* and the 
product r(x)q(x).
f-gan approach.

-

- match the moments between p* 

and r(x)q(x)

- mmd, optimal transport, etc.

understanding gans  

balaji lakshminarayanan

how do we learn generator?
in gans, the generator is differentiable

- generator loss is of the following form e.g. f-divergence d_f = e_q [f(r)]
- can apply re-parametrization trick

understanding gans  

balaji lakshminarayanan

choice of f-divergence

density ratio estimation literature 
has investigated choices of f

however, that   s only half of the 
puzzle. we need non-zero gradients 
for d_f = e_q [f(r)] to learn generator

r<<1 early on in training

-
- non-saturating alternative loss

we also need additional constraints 
on the discriminator

summary: learning in implicit generative models

unifying view* of gans that connects to literature on 
                       
- density ratio estimation

-     but they don   t focus on learning generator 

- approximate bayesian computation (abc) and likelihood-free id136

low dimensional, better understanding of theory

-
- bayesian id136 over parameters
- simulators are usually not differentiable (can we approximate them?)

motivates new id168s: can decouple generator loss from discriminator loss

gan-like ideas can be used in other places where density ratio appears

-

understanding gans  

balaji lakshminarayanan

comparing gans to maximum likelihood 
training using real-nvp

comparison of maximum likelihood and gan-based training of real nvps

ivo danihelka, balaji lakshminarayanan, benigno uria, daan wierstra and peter dayan 

understanding gans  

balaji lakshminarayanan

generative models and algorithms

model

id136

algorithm

prescribed models
directed latent variable 
models, dlgm, state 

space

maximum marginal 

likelihood

variational id136

vae

lower bound on 

likelihood

implicit models
generator nets, 

normalising flows, sdes, 
mechanistic simulations

hypothesis test
likelihood ratio and 

bayes risk

gan

understanding gans  

balaji lakshminarayanan

generative models and algorithms

model

id136

algorithm

prescribed models
directed latent variable 
models, dlgm, state 

space

maximum marginal 

likelihood

variational id136

vae

lower bound on 

likelihood

implicit models
generator nets, 

normalising flows, sdes, 
mechanistic simulations

hypothesis test
likelihood ratio and 

bayes risk

gan

understanding gans  

balaji lakshminarayanan

comparing id136 algorithms for a fixed model
generator is real nvp (dinh et al., 2016)

1. train by maximum likelihood (id113).
2. train a generator by wasserstein gan.
3. compare.

complementary to    on the quantitative analysis of 
decoder-based models    by wu et al., 2017

understanding gans  

balaji lakshminarayanan

wasserstein gan  
for general distributions:

considering all 1-lipschitz function
 (i.e., functions with bounded derivatives).

bounded by:
a) weight clipping (wasserstein gan;    wgan   ).
b) gradient penalty (improved training;    wgan-gp   )

f(x) is a    critic   .
the critic should give
high value to real samples and 
low value to generated samples.

idea: use an independent wasserstein critic to evaluate generators

bits/dim for nvp

dataset: 
celeba 32x32.

valid
train

understanding gans  

balaji lakshminarayanan

wasserstein distance for nvps

understanding gans  

balaji lakshminarayanan

wasserstein distance minimized by wgan

understanding gans  

balaji lakshminarayanan

id113 vs. wgan training

understanding gans  

balaji lakshminarayanan

id113 vs. wgan training (shallower generator)

understanding gans  

balaji lakshminarayanan

bits/dim for nvps trained by wgan

valid
train

worse than 
uniform pdf with 
8 bits/dim.

understanding gans  

balaji lakshminarayanan

log scale

summary

- wasserstein distance can compare models.
- wasserstein distance can be approximated by training a critic.
- training by wgan leads to nicer samples but significantly worse 

log-probabilities.

- latent codes from wgan training are non-gaussian 

understanding gans  

balaji lakshminarayanan

how do we combine vaes and gans 
to get the best of both worlds?

variational approaches for auto-encoding id3 

mihaela rosca*, balaji lakshminarayanan*, david warde-farley and shakir mohamed

understanding gans  

balaji lakshminarayanan

motivating problem: mode collapse

    mog toy example from    unrolled gan    paper
    vaes have other problems, but do not suffer from mode-collapse

    can we add auto-encoder to gans?

adding auto-encoder to gans

penalty to match 
posterior codes
to prior

implicit encoder

reconstruction loss

understanding gans  

balaji lakshminarayanan

how does it relate to evidence lower bound (elbo) 

in vaes?

penalty to match 
posterior codes
to prior

implicit encoder

reconstruction loss

understanding gans  

balaji lakshminarayanan

recap: density ratio trick

estimate the ratio of two distributions only from samples, by building a binary 
classifier to distinguish between them.

 

understanding gans  

balaji lakshminarayanan

            revisiting elbo in variational auto-encoders

understanding gans  

balaji lakshminarayanan

            revisiting elbo in variational auto-encoders

encoder can be implicit! 

more flexible distributions

understanding gans  

balaji lakshminarayanan

putting it all
together

understanding gans  

balaji lakshminarayanan

            combining vaes and gans

likelihood: reconstruction vs    synthetic likelihood    term

-
- kl: analytical vs    code discriminator   
- can recover various hybrids of vaes and gans

understanding gans  

balaji lakshminarayanan

            evaluating different variants

our vae-gan hybrid is competitive with state-of-the-art gans 

understanding gans  

balaji lakshminarayanan

cifar10 - inception score

classifier trained on id163

classifier trained on cifar10

improved techniques for training gans t. salimans, i. goodfellow, w. zaremba, v. cheung, a. radford, x. chen

understanding gans  

balaji lakshminarayanan

celeba - sample diversity

)

i

m
s
s
s
m

-

 
-
 

1
(

understanding gans  

balaji lakshminarayanan

summary: vaes and gans

    variational id136
    reconstructions
    encoder network
    the posterior latents 

match the prior

vaes

alphagan

gans

    implicit decoder
    can use implicit 

encoder: 
discriminators used to 
match distributions

understanding gans  

balaji lakshminarayanan

bridging the gap between theory & 
practice

many paths to equilibrium: gans do not need to decrease a divergence at every step 

william fedus*, mihaela rosca*, balaji lakshminarayanan, andrew dai, shakir mohamed & ian goodfellow

understanding gans  

balaji lakshminarayanan

differences between gan theory and practice
                  
lots of new gan variants have been proposed (e.g. wasserstein gan)

id168s & regularizers motivated by new theory

-
- significant difference between theory and practice

how do we bridge this gap?

- synthetic datasets where theory predicts failure
- add new regularizers to original non-saturating gan

understanding gans  

balaji lakshminarayanan

non-saturating gan

understanding gans  

balaji lakshminarayanan

gradient penalties for discriminators

understanding gans  

balaji lakshminarayanan

comparisons on synthetic 
dataset where jensen 
shannon divergence fails

- gradient penalties lead 

to better performance

understanding gans  

balaji lakshminarayanan

results on real datasets

understanding gans  

balaji lakshminarayanan

results on real datasets

understanding gans  

balaji lakshminarayanan

summary
                  
some surprising findings:

- gradient penalties stabilize (non-wasserstein) gans as well
- think not just about the ideal id168 but also the optimization

   in theory, there is no difference between theory and practice. in practice, there is.   

- better ablation experiments will help bridge this gap and move us closer to the 

holy grail

understanding gans  

balaji lakshminarayanan

other interesting research directions

understanding gans  

balaji lakshminarayanan

overloading gans and    adversarial training   

originally formulated as a minimax game between a discriminator and generator

recent insights:

- density ratio trick: discriminator estimates a density ratio. can replace 

density ratios and f-divergences in message passing with discriminators.

-

implicit/adversarial variational id136: implicit models can be used for 
flexible variational id136 (require only samples, no need for densities)
- adversarial loss: discriminator provides a mechanism to    learn    what is 

realistic, this is better than using a (gaussian) likelihood to train generator. 

confidential & proprietary

gans for imitation learning
use a separate network (discriminator) to     learn    what is realistic
adversarial imitation learning: rl reward comes from a discriminator

learning human behaviors from motion capture by adversarial imitation
josh merel, yuval tassa, dhruva tb, sriram srinivasan, jay lemmon, ziyu wang, greg wayne, nicolas heess

understanding gans  

balaji lakshminarayanan

lots of other exciting research
    research

    using ideas from convergence of nash equilibria
    connections to rl (actor-critic methods)
    control theory (e.g. numerics of gans)

    applications

    class-conditional generation, 
   
   
   
    id20

text-to-image generation
image-to-image translation
single image super-resolution

and many more ...

summary

ways to stabilize gan training

- combine with auto-encoder
- gradient penalties

tools developed in gan literature are intriguing even if you don   t care about gans

implicit variational approximations

- density ratio trick is useful in other areas (e.g. message passing)
-
- learn a realistic id168 than use a loss of convenience
- how do we handle non-differentiable simulators?

-

search using differentiable approximations?

understanding gans  

balaji lakshminarayanan

                 

thanks!

learning in implicit generative models, shakir mohamed* and balaji lakshminarayanan*

variational approaches for auto-encoding id3, mihaela rosca*, balaji 
lakshminarayanan*, david warde-farley and shakir mohamed

comparison of maximum likelihood and gan-based training of real nvps, ivo danihelka, balaji 
lakshminarayanan, benigno uria, daan wierstra and peter dayan

many paths to equilibrium: gans do not need to decrease a divergence at every step, william fedus*, 
mihaela rosca*, balaji lakshminarayanan, andrew dai, shakir mohamed and ian goodfellow

slide credits: mihaela rosca, shakir mohamed, ivo danihelka, david warde-farley, danilo rezende

papers available on my webpage http://www.gatsby.ucl.ac.uk/~balaji/

understanding gans  

balaji lakshminarayanan

