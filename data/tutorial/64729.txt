   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    essentials of machine learning algorithms (with
   python and r codes) comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]business analytics [94]essentials of machine learning
   algorithms (with python and r codes)

   [95]business analytics[96]python[97]r

essentials of machine learning algorithms (with python and r codes)

   [98]sunil ray, september 9, 2017

   note: this article was originally published on aug 10, 2015 and updated
   on sept 9th, 2017

introduction

     google   s self-driving cars and robots get a lot of press, but the
     company   s real future is in machine learning, the technology that
     enables computers to get smarter and more personal.

          eric schmidt (google chairman)

   we are probably living in the most defining period of human history.
   the period when computing moved from large mainframes to pcs to cloud.
   but what makes it defining is not what has happened, but what is coming
   our way in years to come.

   what makes this period exciting and enthralling for someone like me is
   the democratization of the various tools and techniques, which followed
   the boost in computing. welcome to the world of [99]data science!

   today, as a data scientist, i can build data crunching machines with
   complex algorithms for a few dollars per hour. but reaching here wasn   t
   easy! i had my dark days and nights.

   are you a beginner looking for a place to start your data science
   journey? presenting two comprehensive courses, full of knowledge and
   data science learning, curated just for you to learn data science
   (using python) from scratch:
     * [100]introduction to data science
     * [101]certified program: data science for beginners (with
       interviews)


who can benefit the most from this guide?

what i am giving out today is probably the most valuable guide, i have ever
created.

   the idea behind creating this guide is to simplify the journey of
   aspiring data scientists and machine learning enthusiasts across the
   world. through this guide, i will enable you to work on machine
   learning problems and gain from experience. i am providing a high level
   understanding about various machine learning algorithms along with r &
   python codes to run them. these should be sufficient to get your hands
   dirty.

   [102]machine learning algorithms, supervised, unsupervised

   i have deliberately skipped the statistics behind these techniques, as
   you don   t need to understand them at the start. so, if you are looking
   for statistical understanding of these algorithms, you should look
   elsewhere. but, if you are looking to equip yourself to start building
   machine learning project, you are in for a treat.


broadly, there are 3 types of machine learning algorithms..

1. supervised learning

   how it works: this algorithm consist of a target / outcome variable (or
   dependent variable) which is to be predicted from a given set of
   predictors (independent variables). using these set of variables,
   we generate a function that map inputs to desired outputs. the training
   process continues until the model achieves a desired level of accuracy
   on the training data. examples of supervised learning: regression,
   [103]decision tree, [104]id79, knn, id28 etc.


2. unsupervised learning

   how it works: in this algorithm, we do not have any target or outcome
   variable to predict / estimate.  it is used for id91 population
   in different groups, which is widely used for segmenting customers in
   different groups for specific intervention. examples of unsupervised
   learning: apriori algorithm, id116.


3. id23:

   how it works:  using this algorithm, the machine is trained to make
   specific decisions. it works this way: the machine is exposed to an
   environment where it trains itself continually using trial and error.
   this machine learns from past experience and tries to capture the best
   possible knowledge to make accurate business decisions. example of
   id23: markov decision process

list of common machine learning algorithms

   here is the list of commonly used machine learning algorithms. these
   algorithms can be applied to almost any data problem:
    1. id75
    2. id28
    3. decision tree
    4. id166
    5. naive bayes
    6. knn
    7. id116
    8. id79
    9. id84 algorithms
   10. gradient boosting algorithms
         1. gbm
         2. xgboost
         3. lightgbm
         4. catboost

1. id75

   it is used to estimate real values (cost of houses, number of calls,
   total sales etc.) based on continuous variable(s). here, we establish
   relationship between independent and dependent variables by fitting a
   best line. this best fit line is known as regression line and
   represented by a linear equation y= a *x + b.

   the best way to understand id75 is to relive this
   experience of childhood. let us say, you ask a child in fifth grade to
   arrange people in his class by increasing order of weight, without
   asking them their weights! what do you think the child will do? he /
   she would likely look (visually analyze) at the height and build of
   people and arrange them using a combination of these visible
   parameters. this is id75 in real life! the child has
   actually figured out that height and build would be correlated to the
   weight by a relationship, which looks like the equation above.

   in this equation:
     * y     dependent variable
     * a     slope
     * x     independent variable
     * b     intercept

   these coefficients a and b are derived based on minimizing the sum of
   squared difference of distance between data points and regression line.

   look at the below example. here we have identified the best fit line
   having linear equation y=0.2811x+13.9. now using this equation, we can
   find the weight, knowing the height of a person.

   [105]linear_regression

   id75 is of mainly two types: simple id75 and
   multiple id75. simple id75 is characterized
   by one independent variable. and, multiple id75(as the
   name suggests) is characterized by multiple (more than 1) independent
   variables. while finding best fit line, you can fit a polynomial or
   curviid75. and these are known as polynomial or
   curviid75.

   python code
#import library
#import other necessary libraries like pandas, numpy...
from sklearn import linear_model
#load train and test datasets
#identify feature and response variable(s) and values must be numeric and numpy
arrays
x_train=input_variables_values_training_datasets
y_train=target_variables_values_training_datasets
x_test=input_variables_values_test_datasets
# create id75 object
linear = linear_model.linearregression()
# train the model using the training sets and check score
linear.fit(x_train, y_train)
linear.score(x_train, y_train)
#equation coefficient and intercept
print('coefficient: \n', linear.coef_)
print('intercept: \n', linear.intercept_)
#predict output
predicted= linear.predict(x_test)

   r code
#load train and test datasets
#identify feature and response variable(s) and values must be numeric and numpy
arrays
x_train <- input_variables_values_training_datasets
y_train <- target_variables_values_training_datasets
x_test <- input_variables_values_test_datasets
x <- cbind(x_train,y_train)
# train the model using the training sets and check score
linear <- lm(y_train ~ ., data = x)
summary(linear)
#predict output
predicted= predict(linear,x_test)


2. id28

   don   t get confused by its name! it is a classification not a regression
   algorithm. it is used to estimate discrete values ( binary values like
   0/1, yes/no, true/false ) based on given set of independent
   variable(s). in simple words, it predicts the id203 of occurrence
   of an event by fitting data to a [106]logit function. hence, it is also
   known as logit regression. since, it predicts the id203,
   its output values lies between 0 and 1 (as expected).

   again, let us try and understand this through a simple example.

   let   s say your friend gives you a puzzle to solve. there are only 2
   outcome scenarios     either you solve it or you don   t. now imagine, that
   you are being given wide range of puzzles / quizzes in an attempt to
   understand which subjects you are good at. the outcome to this study
   would be something like this     if you are given a trignometry based
   tenth grade problem, you are 70% likely to solve it. on the other hand,
   if it is grade fifth history question, the id203 of getting an
   answer is only 30%. this is what id28 provides you.

   coming to the math, the log odds of the outcome is modeled as a linear
   combination of the predictor variables.
odds= p/ (1-p) = id203 of event occurrence / id203 of not event occu
rrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1x1+b2x2+b3x3....+bkxk

   above, p is the id203 of presence of the characteristic of
   interest. it chooses parameters that maximize the likelihood of
   observing the sample values rather than that minimize the sum of
   squared errors (like in ordinary regression).

   now, you may ask, why take a log? for the sake of simplicity, let   s
   just say that this is one of the best mathematical way to replicate a
   step function. i can go in more details, but that will beat the purpose
   of this article.

   [107]logistic_regression python code
#import library
from sklearn.linear_model import logisticregression
#assumed you have, x (predictor) and y (target) for training data set and x_test
(predictor) of test_dataset
# create id28 object
model = logisticregression()
# train the model using the training sets and check score
model.fit(x, y)
model.score(x, y)
#equation coefficient and intercept
print('coefficient: \n', model.coef_)
print('intercept: \n', model.intercept_)
#predict output
predicted= model.predict(x_test)

   r code
x <- cbind(x_train,y_train)
# train the model using the training sets and check score
logistic <- glm(y_train ~ ., data = x,family='binomial')
summary(logistic)
#predict output
predicted= predict(logistic,x_test)


furthermore..

   there are many different steps that could be tried in order to improve
   the model:
     * including interaction terms
     * removing features
     * [108]id173 techniques
     * using a non-linear model


3. decision tree

   this is one of my favorite algorithm and i use it quite frequently.
   it is a type of supervised learning algorithm that is mostly used
   for classification problems. surprisingly, it works for
   both categorical and continuous dependent variables. in this algorithm,
   we split the population into two or more homogeneous sets. this is done
   based on most significant attributes/ independent variables to make as
   distinct groups as possible. for more details, you can
   read: [109]decision tree simplified.

   [110]ikbzk

   source: [111]statsexchange

   in the image above, you can see that population is classified into four
   different groups based on multiple attributes to identify    if they will
   play or not   . to split the population into different heterogeneous
   groups, it uses various techniques like gini, information gain,
   chi-square, id178.

   the best way to understand how decision tree works, is to play jezzball
       a classic game from microsoft (image below). essentially, you have a
   room with moving walls and you need to create walls such that maximum
   area gets cleared off with out the balls.

   [112]download

   so, every time you split the room with a wall, you are trying to create
   2 different populations with in the same room. id90 work in
   very similar fashion by dividing a population in as different groups as
   possible.

   more: [113]simplified version of decision tree algorithms

python code

#import library
#import other necessary libraries like pandas, numpy...
from sklearn import tree
#assumed you have, x (predictor) and y (target) for training data set and x_test
(predictor) of test_dataset
# create tree object
model = tree.decisiontreeclassifier(criterion='gini') # for classification, here
 you can change the algorithm as gini or id178 (information gain) by default i
t is gini
# model = tree.decisiontreeregressor() for regression
# train the model using the training sets and check score
model.fit(x, y)
model.score(x, y)
#predict output
predicted= model.predict(x_test)

   r code
library(rpart)
x <- cbind(x_train,y_train)
# grow tree
fit <- rpart(y_train ~ ., data = x,method="class")
summary(fit)
#predict output
predicted= predict(fit,x_test)


4. id166 (support vector machine)

   it is a classification method. in this algorithm, we plot each data
   item as a point in n-dimensional space (where n is number of features
   you have) with the value of each feature being the value of a
   particular coordinate.

   for example, if we only had two features like height and hair length of
   an individual, we   d first plot these two variables in two dimensional
   space where each point has two co-ordinates (these co-ordinates are
   known as support vectors)

   [114]id1661

   now, we will find some line that splits the data between the two
   differently classified groups of data. this will be the line such that
   the distances from the closest point in each of the two groups will be
   farthest away.

   [115]id1662

   in the example shown above, the line which splits the data into two
   differently classified groups is the black line, since the two closest
   points are the farthest apart from the line. this line is our
   classifier. then, depending on where the testing data lands on either
   side of the line, that   s what class we can classify the new data as.

   more: [116]simplified version of support vector machine

   think of this algorithm as playing jezzball in n-dimensional space. the
   tweaks in the game are:
     * you can draw lines / planes at any angles (rather than just
       horizontal or vertical as in classic game)
     * the objective of the game is to segregate balls of different colors
       in different rooms.
     * and the balls are not moving.


python code

#import library
from sklearn import id166
#assumed you have, x (predictor) and y (target) for training data set and x_test
(predictor) of test_dataset
# create id166 classification object
model = id166.svc() # there is various option associated with it, this is simple f
or classification. you can refer [117]link, for mo# re detail.
# train the model using the training sets and check score
model.fit(x, y)
model.score(x, y)
#predict output
predicted= model.predict(x_test)

   r code
library(e1071)
x <- cbind(x_train,y_train)
# fitting model
fit <-id166(y_train ~ ., data = x)
summary(fit)
#predict output
predicted= predict(fit,x_test)


5. naive bayes

   it is a classification technique based on [118]bayes    theorem with an
   assumption of independence between predictors. in simple terms, a naive
   bayes classifier assumes that the presence of a particular feature in a
   class is unrelated to the presence of any other feature. for example, a
   fruit may be considered to be an apple if it is red, round, and about 3
   inches in diameter. even if these features depend on each other or upon
   the existence of the other features, a naive bayes classifier would
   consider all of these properties to independently contribute to the
   id203 that this fruit is an apple.

   naive bayesian model is easy to build and particularly useful for very
   large data sets. along with simplicity, naive bayes is known
   to outperform even highly sophisticated classification methods.

   id47 provides a way of calculating posterior id203
   p(c|x) from p(c), p(x) and p(x|c). look at the equation below:
   [119]bayes_rule

   here,
     * p(c|x) is the posterior id203 of class (target) given
       predictor (attribute).
     * p(c) is the prior id203 of class.
     * p(x|c) is the likelihood which is the id203 of predictor
       given class.
     * p(x) is the prior id203 of predictor.

   example: let   s understand it using an example. below i have a training
   data set of weather and corresponding target variable    play   . now, we
   need to classify whether players will play or not based on weather
   condition. let   s follow the below steps to perform it.

   step 1: convert the data set to frequency table

   step 2: create likelihood table by finding the probabilities like
   overcast id203 = 0.29 and id203 of playing is 0.64.

   [120]bayes_4

   step 3: now, use naive bayesian equation to calculate the posterior
   id203 for each class. the class with the highest posterior
   id203 is the outcome of prediction.

   problem: players will pay if weather is sunny, is this statement is
   correct?

   we can solve it using above discussed method, so p(yes | sunny) = p(
   sunny | yes) * p(yes) / p (sunny)

   here we have p (sunny |yes) = 3/9 = 0.33, p(sunny) = 5/14 = 0.36, p(
   yes)= 9/14 = 0.64

   now, p (yes | sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher
   id203.

   naive bayes uses a similar method to predict the id203 of
   different class based on various attributes. this algorithm is mostly
   used in text classification and with problems having multiple classes.

python code

#import library
from sklearn.naive_bayes import gaussiannb
#assumed you have, x (predictor) and y (target) for training data set and x_test
(predictor) of test_dataset
# create id166 classification object model = gaussiannb() # there is other distrib
ution for multinomial classes like bernoulli naive bayes, [121]refer link
# train the model using the training sets and check score
model.fit(x, y)
#predict output
predicted= model.predict(x_test)

   r code
library(e1071)
x <- cbind(x_train,y_train)
# fitting model
fit <-naivebayes(y_train ~ ., data = x)
summary(fit)
#predict output
predicted= predict(fit,x_test)


6. knn (k- nearest neighbors)

   it can be used for both classification and regression problems.
   however, it is more widely used in classification problems in the
   industry. k nearest neighbors is a simple algorithm that stores all
   available cases and classifies new cases by a majority vote of its k
   neighbors. the case being assigned to the class is most common amongst
   its k nearest neighbors measured by a distance function.

   these distance functions can be euclidean, manhattan, minkowski and
   hamming distance. first three functions are used for continuous
   function and fourth one (hamming) for categorical variables. if k = 1,
   then the case is simply assigned to the class of its nearest neighbor.
   at times, choosing k turns out to be a challenge while performing knn
   modeling.

   more: [122]introduction to k-nearest neighbors : simplified.

   [123]knn

   knn can easily be mapped to our real lives. if you want to learn
   about a person, of whom you have no information, you might like to find
   out about his close friends and the circles he moves in and gain access
   to his/her information!

   things to consider before selecting knn:
     * knn is computationally expensive
     * variables should be normalized else higher range variables can bias
       it
     * works on pre-processing stage more before going for knn like
       outlier, noise removal

python code

#import library
from sklearn.neighbors import kneighborsclassifier
#assumed you have, x (predictor) and y (target) for training data set and x_test
(predictor) of test_dataset
# create kneighbors classifier object model
kneighborsclassifier(n_neighbors=6) # default value for n_neighbors is 5
# train the model using the training sets and check score
model.fit(x, y)
#predict output
predicted= model.predict(x_test)

   r code
library(knn)
x <- cbind(x_train,y_train)
# fitting model
fit <-knn(y_train ~ ., data = x,k=5)
summary(fit)
#predict output
predicted= predict(fit,x_test)


7. id116

   it is a type of unsupervised algorithm which  solves the id91
   problem. its procedure follows a simple and easy  way to classify a
   given data set through a certain number of  clusters (assume k
   clusters). data points inside a cluster are homogeneous and
   heterogeneous to peer groups.

   remember figuring out shapes from ink blots? id116 is somewhat
   similar this activity. you look at the shape and spread to decipher how
   many different clusters / population are present!

   [124]splatter_ink_blot_texture_by_maki_tak-d5p6zph

   how id116 forms cluster:
    1. id116 picks k number of points for each cluster known as
       centroids.
    2. each data point forms a cluster with the closest centroids i.e. k
       clusters.
    3. finds the centroid of each cluster based on existing cluster
       members. here we have new centroids.
    4. as we have new centroids, repeat step 2 and 3. find the closest
       distance for each data point from new centroids and get associated
       with new k-clusters. repeat this process until convergence occurs
       i.e. centroids does not change.

   how to determine value of k:

   in id116, we have clusters and each cluster has its own centroid. sum
   of square of difference between centroid and the data points within a
   cluster constitutes within sum of square value for that cluster. also,
   when the sum of square values for all the clusters are added, it
   becomes total within sum of square value for the cluster solution.

   we know that as the number of cluster increases, this value keeps on
   decreasing but if you plot the result you may see that the sum of
   squared distance decreases sharply up to some value of k, and then much
   more slowly after that. here, we can find the optimum number of
   cluster.

   [125]kmenas

python code

#import library
from sklearn.cluster import kmeans
#assumed you have, x (attributes) for training data set and x_test(attributes) o
f test_dataset
# create kneighbors classifier object model
k_means = kmeans(n_clusters=3, random_state=0)
# train the model using the training sets and check score
model.fit(x)
#predict output
predicted= model.predict(x_test)

   r code
library(cluster)
fit <- kmeans(x, 3) # 5 cluster solution


8. id79

   id79 is a trademark term for an ensemble of id90. in
   id79, we   ve collection of id90 (so known as
      forest   ). to classify a new object based on attributes, each tree
   gives a classification and we say the tree    votes    for that class. the
   forest chooses the classification having the most votes (over all the
   trees in the forest).

   each tree is planted & grown as follows:
    1. if the number of cases in the training set is n, then sample of n
       cases is taken at random but with replacement. this sample will be
       the training set for growing the tree.
    2. if there are m input variables, a number m<<m is specified such
       that at each node, m variables are selected at random out of the m
       and the best split on these m is used to split the node. the value
       of m is held constant during the forest growing.
    3. each tree is grown to the largest extent possible. there is no
       pruning.

   for more details on this algorithm, comparing with decision tree and
   tuning model parameters, i would suggest you to read these articles:
    1. [126]introduction to id79     simplified
    2. [127]comparing a cart model to id79 (part 1)
    3. [128]comparing a id79 to a cart model (part 2)
    4. [129]tuning the parameters of your id79 model

   python
#import library
from sklearn.ensemble import randomforestclassifier
#assumed you have, x (predictor) and y (target) for training data set and x_test
(predictor) of test_dataset
# create id79 object
model= randomforestclassifier()
# train the model using the training sets and check score
model.fit(x, y)
#predict output
predicted= model.predict(x_test)

   r code
library(randomforest)
x <- cbind(x_train,y_train)
# fitting model
fit <- randomforest(species ~ ., x,ntree=500)
summary(fit)
#predict output
predicted= predict(fit,x_test)


9. id84 algorithms

   in the last 4-5 years, there has been an exponential increase in data
   capturing at every possible stages. corporates/ government agencies/
   research organisations are not only coming with new sources but also
   they are capturing data in great detail.

   for example: e-commerce companies are capturing more details about
   customer like their demographics, web crawling history, what they like
   or dislike, purchase history, feedback and many others to give them
   personalized attention more than your nearest grocery shopkeeper.

   as a data scientist, the data we are offered also consist of many
   features, this sounds good for building good robust model but there is
   a challenge. how   d you identify highly significant variable(s) out 1000
   or 2000? in such cases, id84 algorithm helps us
   along with various other algorithms like decision tree, id79,
   pca, factor analysis, identify based on correlation matrix, missing
   value ratio and others.

   to know more about this algorithms, you can read    [130]beginners guide
   to learn dimension reduction techniques   .

python  code

#import library
from sklearn import decomposition
#assumed you have training and test data set as train and test
# create pca obeject pca= decomposition.pca(n_components=k) #default value of k
=min(n_sample, n_features)
# for factor analysis
#fa= decomposition.factoranalysis()
# reduced the dimension of training dataset using pca
train_reduced = pca.fit_transform(train)
#reduced the dimension of test dataset
test_reduced = pca.transform(test)
#for more detail on this, please refer  [131]this link.

r code

library(stats)
pca <- princomp(train, cor = true)
train_reduced  <- predict(pca,train)
test_reduced  <- predict(pca,test)


10. gradient boosting algorithms

10.1. gbm

   gbm is a boosting algorithm used when we deal with plenty of data to
   make a prediction with high prediction power. boosting is actually an
   ensemble of learning algorithms which combines the prediction of
   several base estimators in order to improve robustness over a single
   estimator. it combines multiple weak or average predictors to a build
   strong predictor. these boosting algorithms always work well in data
   science competitions like kaggle, av hackathon, crowdanalytix.

   more: [132]know about boosting algorithms in detail

python code

#import library
from sklearn.ensemble import gradientboostingclassifier
#assumed you have, x (predictor) and y (target) for training data set and x_test
(predictor) of test_dataset
# create gradient boosting classifier object
model= gradientboostingclassifier(n_estimators=100, learning_rate=1.0, max_depth
=1, random_state=0)
# train the model using the training sets and check score
model.fit(x, y)
#predict output
predicted= model.predict(x_test)

r code

library(caret)
x <- cbind(x_train,y_train)
# fitting model
fitcontrol <- traincontrol( method = "repeatedcv", number = 4, repeats = 4)
fit <- train(y ~ ., data = x, method = "gbm", trcontrol = fitcontrol,verbose = f
alse)
predicted= predict(fit,x_test,type= "prob")[,2]

   gradientboostingclassifier and id79 are two different boosting
   tree classifier and often people ask about the [133]difference between
   these two algorithms.

10.2. xgboost

   another classic gradient boosting algorithm that   s known to be the
   decisive choice between winning and losing in some kaggle competitions.

   the xgboost has an immensely high predictive power which makes it the
   best choice for accuracy in events as it possesses both linear model
   and the tree learning algorithm, making the algorithm almost 10x faster
   than existing gradient booster techniques.

   the support includes various objective functions, including regression,
   classification and ranking.

   one of the most interesting things about the xgboost is that it is also
   called a regularized boosting technique. this helps to reduce overfit
   modelling and has a massive support for a range of languages such as
   scala, java, r, python, julia and c++.

   supports distributed and widespread training on many machines that
   encompass gce, aws, azure and yarn clusters. xgboost can also be
   integrated with spark, flink and other cloud dataflow systems with a
   built in cross validation at each iteration of the boosting process.

   to learn more about xgboost and parameter tuning,
   visit [134]https://www.analyticsvidhya.com/blog/2016/03/complete-guide-
   parameter-tuning-xgboost-with-codes-python/.

   python code:
from xgboost import xgbclassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
x = dataset[:,0:10]
y = dataset[:,10:]
seed = 1

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random
_state=seed)

model = xgbclassifier()

model.fit(x_train, y_train)

#make predictions for test data
y_pred = model.predict(x_test)

   r code:
require(caret)

x <- cbind(x_train,y_train)

# fitting model

traincontrol <- traincontrol( method = "repeatedcv", number = 10, repeats = 4)

model<- train(y ~ ., data = x, method = "xgblinear", trcontrol = traincontrol,ve
rbose = false)

or

model<- train(y ~ ., data = x, method = "xgbtree", trcontrol = traincontrol,verb
ose = false)

predicted <- predict(model, x_test)


10.3. lightgbm

   lightgbm is a gradient boosting framework that uses tree based learning
   algorithms. it is designed to be distributed and efficient with the
   following advantages:
     * faster training speed and higher efficiency
     * lower memory usage
     * better accuracy
     * parallel and gpu learning supported
     * capable of handling large-scale data

   the framework is a fast and high-performance gradient boosting one
   based on decision tree algorithms, used for ranking, classification and
   many other machine learning tasks. it was developed under the
   distributed machine learning toolkit project of microsoft.

   since the lightgbm is based on decision tree algorithms, it splits the
   tree leaf wise with the best fit whereas other boosting algorithms
   split the tree depth wise or level wise rather than leaf-wise. so when
   growing on the same leaf in light gbm, the leaf-wise algorithm can
   reduce more loss than the level-wise algorithm and hence results in
   much better accuracy which can rarely be achieved by any of the
   existing boosting algorithms.

   also, it is surprisingly very fast, hence the word    light   .

   refer to the article to know more about lightgbm:
   [135]https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes
   -the-crown-light-gbm-vs-xgboost/

   python code:
data = np.random.rand(500, 10) # 500 entities, each contains 10 features
label = np.random.randint(2, size=500) # binary target

train_data = lgb.dataset(data, label=label)
test_data = train_data.create_valid('test.id166')

param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}
param['metric'] = 'auc'

num_round = 10
bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])

bst.save_model('model.txt')

# 7 entities, each contains 10 features
data = np.random.rand(7, 10)
ypred = bst.predict(data)


   r code:
library(rlightgbm)
data(example.binary)
#parameters

num_iterations <- 100
config <- list(objective = "binary",  metric="binary_logloss,auc", learning_rate
 = 0.1, num_leaves = 63, tree_learner = "serial", feature_fraction = 0.8, baggin
g_freq = 5, id112_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_le
af = 5.0)

#create data handle and booster
handle.data <- lgbm.data.create(x)

lgbm.data.setfield(handle.data, "label", y)

handle.booster <- lgbm.booster.create(handle.data, lapply(config, as.character))

#train for num_iterations iterations and eval every 5 steps

lgbm.booster.train(handle.booster, num_iterations, 5)

#predict
pred <- lgbm.booster.predict(handle.booster, x.test)

#test accuracy
sum(y.test == (y.pred > 0.5)) / length(y.test)

#save model (can be loaded again via lgbm.booster.load(filename))
lgbm.booster.save(handle.booster, filename = "/tmp/model.txt")

   if you   re familiar with the caret package in r, this is another way of
   implementing the lightgbm.
require(caret)
require(rlightgbm)
data(iris)

model <-caretmodel.lgbm()

fit <- train(species ~ ., data = iris, method=model, verbosity = 0)
print(fit)
y.pred <- predict(fit, iris[,1:4])

library(matrix)
model.sparse <- caretmodel.lgbm.sparse()

#generate a sparse matrix
mat <- matrix(as.matrix(iris[,1:4]), sparse = t)
fit <- train(data.frame(idx = 1:nrow(iris)), iris$species, method = model.sparse
, matrix = mat, verbosity = 0)
print(fit)


10.4. catboost

   catboost is a recently open-sourced machine learning algorithm from
   yandex. it can easily integrate with deep learning frameworks like
   google   s tensorflow and apple   s core ml.

   the best part about catboost is that it does not require extensive data
   training like other ml models, and can work on a variety of data
   formats; not undermining how robust it can be.

   make sure you handle missing data well before you proceed with the
   implementation.

   catboost can automatically deal with categorical variables without
   showing the type conversion error, which helps you to focus on tuning
   your model better rather than sorting out trivial errors.

   learn more about catboost from this article:
   [136]https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-ca
   tegorical-data/

   python code:
import pandas as pd
import numpy as np

from catboost import catboostregressor

#read training and testing files
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

#imputing missing values for both train and test
train.fillna(-999, inplace=true)
test.fillna(-999,inplace=true)

#creating a training set for modeling and validation set to check model performa
nce
x = train.drop(['item_outlet_sales'], axis=1)
y = train.item_outlet_sales

from sklearn.model_selection import train_test_split

x_train, x_validation, y_train, y_validation = train_test_split(x, y, train_size
=0.7, random_state=1234)
categorical_features_indices = np.where(x.dtypes != np.float)[0]

#importing library and building model
from catboost import catboostregressormodel=catboostregressor(iterations=50, dep
th=3, learning_rate=0.1, loss_function='rmse')

model.fit(x_train, y_train,cat_features=categorical_features_indices,eval_set=(x
_validation, y_validation),plot=true)

submission = pd.dataframe()

submission['item_identifier'] = test['item_identifier']
submission['outlet_identifier'] = test['outlet_identifier']
submission['item_outlet_sales'] = model.predict(test)

   r code:
set.seed(1)

require(titanic)

require(caret)

require(catboost)

tt <- titanic::titanic_train[complete.cases(titanic::titanic_train),]

data <- as.data.frame(as.matrix(tt), stringsasfactors = true)

drop_columns = c("passengerid", "survived", "name", "ticket", "cabin")

x <- data[,!(names(data) %in% drop_columns)]y <- data[,c("survived")]

fit_control <- traincontrol(method = "cv", number = 4,classprobs = true)

grid <- expand.grid(depth = c(4, 6, 8),learning_rate = 0.1,iterations = 100, l2_
leaf_reg = 1e-3,            rsm = 0.95, border_count = 64)

report <- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = tr
ue, preproc = null,tunegrid = grid, trcontrol = fit_control)

print(report)

importance <- varimp(report, scale = false)

print(importance)


projects

   now, its time to take the plunge and actually play with some other real
   datasets. so are you ready to take on the challenge? accelerate your
   data science journey with the following practice problems:
   [137]practice problem: food demand forecasting challenge predict the
   demand of meals for a meal delivery company
   [138]practice problem: hr analytics challenge identify the employees
   most likely to get promoted
   [139]practice problem: predict number of upvotes predict number of
   upvotes on a query asked at an online question & answer platform


end notes

   by now, i am sure, you would have an idea of commonly used machine
   learning algorithms. my sole intention behind writing this article and
   providing the codes in r and python is to get you started right
   away. if you are keen to master machine learning, start right away.
   take up problems, develop a physical understanding of the process,
   apply these codes and see the fun!

   did you find this article useful ? share your views and opinions in the
   comments section below.

if you like what you just read & want to continue your analytics
learning, [140]subscribe to our emails, [141]follow us on twitter or like
our [142]facebook page.

   you can also read this article on analytics vidhya's android app
   [143]get it on google play

share this:

     * [144]click to share on linkedin (opens in new window)
     * [145]click to share on facebook (opens in new window)
     * [146]click to share on twitter (opens in new window)
     * [147]click to share on pocket (opens in new window)
     * [148]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [149]c4.5, [150]cart, [151]decision tree, [152]gbm,
   [153]id116, [154]knn, [155]linear-regression, [156]logistic
   regression, [157]machine learning, [158]naive bayes, [159]neural
   network, [160]id79, [161]reinforcement, [162]supervised
   learning, [163]unsupervised
   next article

exclusive interview with pankaj kulshreshtha, ceo, scienaptic systems

   previous article

building machine learning model is fun using orange

[164]sunil ray

   i am a business analytics and intelligence professional with deep
   experience in the indian insurance industry. i have worked for various
   multi-national insurance companies in last 7 years.

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [165]discussion portal to get your queries resolved

76 comments

     * kuber says:
       [166]august 10, 2015 at 11:59 pm
       awesowe compilation!! thank you.
     * karthikeyan says:
       [167]august 11, 2015 at 3:13 am
       thank you very much, a very useful and excellent compilation. i
       have already bookmarked this page.
     * hemanth says:
       [168]august 11, 2015 at 4:50 am
       straight, informative and effective!!
       thank you
     * venugopal says:
       [169]august 11, 2015 at 6:05 am
       good summary airticle
     * dr venugopala rao says:
       [170]august 11, 2015 at 6:27 am
       super compilation   
     * kishor basyal says:
       [171]august 11, 2015 at 7:30 am
       wonderful! really helpful
     * brian thomas says:
       [172]august 11, 2015 at 9:24 am
       very nicely done! thanks for this.
     * tesfaye says:
       [173]august 11, 2015 at 10:30 am
       thank you! well presented article.
     * tesfaye says:
       [174]august 11, 2015 at 10:31 am
       thank you! well presented.
     * huzefa says:
       [175]august 11, 2015 at 3:53 pm
       hello,
       superb information in just one blog. can anyone help me to run the
       codes in r what should be replaced with    ~    symbol in codes? help
       is appreciated
     * huzefa says:
       [176]august 11, 2015 at 3:54 pm
       hello,
       superb information in just one blog. can anyone help me to run the
       codes in r what should be replaced with    ~    symbol in codes? help
       is appreciated .
          + ashuthoshgowda says:
            [177]october 29, 2016 at 12:06 am
               ~    is used to select the variables that you   ll be using for a
            particular model.
               label~.,        uses all your attributes
               label~att1 + att2,        uses only att1 and att2 to create the
            model
     * sudipta basak says:
       [178]august 12, 2015 at 3:35 am
       enjoyed the simplicity. thanks for the effort.
     * [179]im_utm says:
       [180]august 12, 2015 at 2:37 pm
       great article    helps a lot, as naive in machine learning.
     * sunil ray says:
       [181]august 14, 2015 at 7:36 am
       hi all,
       thanks for the comment    
     * dalila says:
       [182]august 14, 2015 at 1:35 pm
       very good summary.
       thank!
       one simple point. the reason for taking the log(p/(1-p)) in
       id28 is to make the equation linear, i.e., easy to
       solve.
          + sunil ray says:
            [183]august 21, 2015 at 5:21 am
            thanks dalila        
          + borun chowdhury says:
            [184]april 21, 2016 at 8:48 am
            that   s not the reason for taking the log. the underlying
            assumption in id28 is that the id203 is
            governed by a step function whose argument is linear in the
            attributes. first of all the assumption of linearity or
            otherwise introduces bias. however, id28 being
            a parametric model some bias is inevitable. the reason to
            choose a linear relationship is not because its easy to solve
            but because a higher order polynomial introduces higher bias
            and one would not like to do so without good reason.
            now coming to the choice of log, it is just a convention.
            basically, once we have decided to go with a linear model, in
            the case of one attribute we model the id203 by
            p(x) = f( ax+b)
            such that p(-infinity)=0 and p(infinity)=0. it so happens that
            this is satisfied by
            p(x) = exp(ax+b)/ (1 + exp(ax+b))
            which can be re-written as
            log(p(x)/(1-p(x)) = a x+ b
            while i am at it, it may be useful to talk about another
            point. one should ask is why we don   t use least square method.
            the reason is that a yes/no choice is a bernoulli random
            variable and thus we estimate the id203 according to
            maximum likelihood wrt bernoulli process. for linear
            regression the assumption is that the residuals around the
               true    function are distributed according to a normal
            distribution and the maximum likelihood estimate for a normal
            distribution amounts to the least square method. so deep down
            id75 and id28 both use maximum
            likelihood estimates. its just that they are max likelihoods
            according to different distributions.
     * statis says:
       [185]august 19, 2015 at 12:14 am
       nice summary!
       @huzefa: you shouldn   t replace the    ~    in the r code, it basically
       means    as a function of   . you can also keep the    .    right after, it
       stands for    all other variables in the dataset provided   . if you
       want to be explicit, you can write y ~ x1 + x2 +     where x1, x2 ..
       are the names of the columns of your data.frame or data.table.
       further note on formula specification: by default r adds an
       intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove
       it via y ~ 0 + x. interactions are specified with either * (which
       also adds the two variables) or : (which only adds the interaction
       term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2.
       hope this helps!
     * chris says:
       [186]august 26, 2015 at 1:01 am
       you did a wonderful job! this is really helpful. thanks!
     * [187]glenn nelson says:
       [188]september 10, 2015 at 7:48 pm
       i took the stanford-coursera ml class, but have not used it, and i
       found this to be an incredibly useful summary. i appreciate the
       real-world analogues, such as your mention of jezzball. and showing
       the brief code snips is terrific.
     * [189]shankar pandala says:
       [190]september 15, 2015 at 12:09 pm
       this is very easy and helpful than any other courses i have
       completed.
       simple. clear. to the point.
     * markpratley says:
       [191]september 26, 2015 at 9:29 am
       you sir are a gentleman and a scholar!
     * whystatistics says:
       [192]september 29, 2015 at 10:25 am
       hi sunil,
       this is really superb tutorial along with good examples and codes
       which is surely much helpful. just, can you add neural network here
       in simple terms with example and code.
     * sayan putatunda says:
       [193]november 1, 2015 at 7:00 am
       errata:- fit <- kmeans(x, 3) # 5 cluster solution
       it`s a 3 cluster solution.
     * baha says:
       [194]november 27, 2015 at 1:13 pm
       well done, thank you!
     * benjamin says:
       [195]december 5, 2015 at 7:00 pm
       this is a great resource overall and surely the product of a lot of
       work.
       just a note as i go through this, your comment on logistic
       regression not actually being regression is in fact wrong. it maps
       outputs to a continuous variable bound between 0 and 1 that we
       regard as id203. it makes classification easy but that is
       still an extra step that requires the choice of a threshold which
       is not the main aim of id28. as a matter of fact it
       falls under the umbrella of generalized libear models as the glm r
       package hints it in your code example.
       i thought this was interesting to note so as not to forget that
       id28 output is richer than 0 or 1.
       thanks for the great article overall.
     * [196]ashish yelkar says:
       [197]january 6, 2016 at 6:28 am
       very nice.!!
     * bansari shah says:
       [198]january 14, 2016 at 6:27 am
       thank you.. reallu helpful article
     * ayushgg92 says:
       [199]january 22, 2016 at 9:54 am
       i wanted to know if i can use rattle instead of writing the r code
       explicitly
     * debasis says:
       [200]january 22, 2016 at 10:35 am
       thank you. very nice and useful article..
     * debashis says:
       [201]february 16, 2016 at 8:19 am
       this is such a wonderful article.
     * anthony says:
       [202]february 16, 2016 at 8:39 am
       informative and easy to follow. i   ve recently started following
       several pages like this one and this is the best material ive seen
       yet.
     * akhil says:
       [203]february 17, 2016 at 3:55 am
       one of the best content ever read regarding algorithms.
     * swathi says:
       [204]february 17, 2016 at 12:02 pm
       thank you so much for this article
     * n  colas robles says:
       [205]february 18, 2016 at 5:51 am
       cool stuff!
       i just can   t get the necessary libraries   
     * wizzerd says:
       [206]february 26, 2016 at 12:09 pm
       looks sgood article. do i need any data to do the examples?
     * col. dan sulzinger says:
       [207]march 1, 2016 at 1:21 am
       good article.
     * pansy says:
       [208]march 8, 2016 at 3:04 pm
       i have to thank you for this informative summary. really useful!
     * [209]j says:
       [210]march 10, 2016 at 8:54 pm
       somewhat irresponsible article since it does not mention any
       measure of performance and only gives cooking recipes without
       understanding what algorithm does what and the stats behind it.
       cooking recipes like these are the ones that place people in drew
       conway   s danger zone
       ([211]https://www.quora.com/in-the-data-science-venn-diagram-why-is
       -the-common-region-of-hacking-skills-and-substantive-expertise-cons
       idered-as-danger-zone), thus making programmers the worst data
       analysts (let alone scientists, that requires another mindset
       completely). i highly recommend anyone wishing to enter into this
       brave new world not to jump into statistical learning without
       proper statistical background. otherwise you could end up like
       google, target, telefonica, or google (again) and become a poster
       boy for    the big flops of big data   .
          + george says:
            [212]june 17, 2016 at 1:34 pm
            do you have a better article?please share   
     * [213]robin white says:
       [214]march 15, 2016 at 11:38 pm
       great article. it really summarize some of the most important
       topics on machine learning.
       but as asked above i would like to present thedevmasters.com as a
       company with a really good course to learn more depth about machine
       learning with great professors and a sense of community that is
       always helping itself to continue learning even after the course
       ends.
     * salman ahmed says:
       [215]march 19, 2016 at 8:29 am
       awesome , recommended this article to all my friends
     * borun chowdhury says:
       [216]april 21, 2016 at 8:13 am
       very succinct description of some important algorithms. thanks. i   d
       like to point out a mistake in the id166 section. you say    where each
       point has two co-ordinates (these co-ordinates are known as support
       vectors)   . this is not correct, the coordinates are just features.
       its the points lying on the margin that are called the    support
       vectors   . these are the points that    support    the margin i.e.
       define it (as opposed to a weighted average of all points for
       instance.)
     * isaac says:
       [217]may 24, 2016 at 2:29 am
       thank you for this wonderful article   it   s proven helpful.
     * [218]payal gour says:
       [219]june 11, 2016 at 5:52 am
       thank you very much, a very useful and excellent compilation.
     * nd says:
       [220]june 17, 2016 at 7:39 am
       very good information interms of initial knowledge
       note one warning, many methods can be fitted into a particular
       problem, but result might not be what you wish.
       hence you must always compare models, understand residuals profile
       and how prediction really predicts.
       in that sense, analysis of data is never ending.
       in r, use summary, plot and check for assumptions validity .
     * dung dinh says:
       [221]june 17, 2016 at 10:24 am
       the amazing article. i   m new in data analysis. it   s very useful and
       easy to understand.
       thanks,
     * sabarikannan says:
       [222]june 30, 2016 at 5:35 am
       this is really good article, also if you would have explain about
       anomaly dection algorithm that will really helpful for everyone to
       know , what and where to apply in machine learning   .
     * ankita srivastava says:
       [223]july 5, 2016 at 8:07 am
       a very very helpful tutorial. thanks a lot you guys.
     * haiyan says:
       [224]july 7, 2016 at 7:41 am
       the amazing article
     * namala santosh kumar says:
       [225]july 15, 2016 at 2:33 am
       analytics vidhya     i am loving it
     * mohammed abdul kaleem says:
       [226]july 16, 2016 at 8:21 am
       good article.
       thank you for explaining with python.
     * jacques gouimenou says:
       [227]august 14, 2016 at 1:57 pm
       very useful compilation. thanks!
     * baseer says:
       [228]august 18, 2016 at 5:11 am
       very precise quick tutorial for those who want to gain insight of
       machine learning
     * vishwas says:
       [229]august 24, 2016 at 4:59 pm
       great
     * ali kazim says:
       [230]august 28, 2016 at 11:41 pm
       very useful and informative. thanks for sharing it.
     * js says:
       [231]september 3, 2016 at 7:42 pm
       superb!
     * denis says:
       [232]september 4, 2016 at 10:53 am
       great summary,
       thank you
     * sanjiv says:
       [233]september 8, 2016 at 4:29 am
       great article. it would have become even better if you had some
       test data with each code snippet. add metrics and hyper parameter
       tunning for each of these models
     * faizan says:
       [234]september 13, 2016 at 9:17 am
       thanks for the    jezzball    example. you made my day!
     * satya swarup dani says:
       [235]september 27, 2016 at 10:41 am
       nicely complied. every explanation is crystal clear and very easy
       to digest. thanks for sharing knowledge.
     * emerson moizes says:
       [236]october 5, 2016 at 10:58 am
       perfect! it   s exactly what i was looking for!
       thanks for the explanation and thanks for sharing your knowledge
       with us!
     * valery says:
       [237]october 11, 2016 at 8:16 am
       very useful summary. thank you.
     * malini ramamurthy says:
       [238]october 14, 2016 at 5:53 am
       very informative article. for a person new to machine learning,
       this article gives a good starting point.
     * [239]shubham says:
       [240]october 21, 2016 at 7:23 pm
       good article.
       thank you for explaining with python.
     * [241]anand rai says:
       [242]october 21, 2016 at 7:23 pm
       good article.
       thank you for explaining with python.
           kareermatrix
     * ramesh says:
       [243]october 23, 2016 at 12:35 pm
       hi friends, i   m new person to these machine learning algorithms. i
       have some questions.?
       1) we have so many ml algorithms. but how can we choose the
       algorithms which one is suitable for my data set?
       2) how does these algorithms works.?
       3) why only these particular algorithms.? why not others.?
     * indra says:
       [244]november 3, 2016 at 11:55 pm
       nice article.. thanks for your effort
     * ravi rathore says:
       [245]november 8, 2016 at 1:44 pm
       hello
       i have to implement machine learning algorithms in python so could
       you help me in this.
       any body provide me the proper code for any algorithm.
     * raghav says:
       [246]november 10, 2016 at 8:09 pm
       all programe has error named
       error in model.frame.default(formula = as.list(y_train) ~ ., data =
       x, :
       invalid type (list) for variable    as.list(y_train)   
       what is that ?
          + gaurav says:
            [247]december 8, 2016 at 1:54 pm
            i think that    y_train    is data frame and it cannot be
            converted directly to list with    as.list    command. try this
            instead
            y_train <- as.list(as.data.frame(t(y_train)))
            see if this works for you.
     * suman says:
       [248]december 9, 2016 at 6:43 am
       very nice summary!
       can you tell how to get machine learning problems for practice?
          + nss says:
            [249]january 3, 2017 at 6:20 am
            analytics vidhya has some practice datasets. check analytics
            vidhya hackathon. also uci machine learning repository is a
            phenomenal place. google it and enjoy.
     * yb says:
       [250]december 26, 2016 at 2:46 pm
       do you have r codes based on caret?
          + nss says:
            [251]january 3, 2017 at 6:21 am
            yes.

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [252]srk       3924
   2    [2.jpg?date=2019-04-06] [253]mark12    3510
   3    [3.jpg?date=2019-04-06] [254]nilabha   3261
   4    [4.jpg?date=2019-04-06] [255]nitish007 3237
   5    [5.jpg?date=2019-04-06] [256]tezdhar   3082
   [257]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [258]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [259]understanding support vector machine algorithm from examples
       (along with code)
     * [260]essentials of machine learning algorithms (with python and r
       codes)
     * [261]a complete tutorial to learn data science with python from
       scratch
     * [262]7 types of regression techniques you should know!
     * [263]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [264]a simple introduction to anova (with applications in excel)
     * [265]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [266]top 5 machine learning github repositories and reddit discussions
   from march 2019

[267]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [268]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[269]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [270]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[271]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [272]16 opencv functions to start your id161 journey (with
   python code)

[273]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [274][ds-finhack.jpg]

   [275][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [276]about us
     * [277]our team
     * [278]career
     * [279]contact us
     * [280]write for us

   [281]about us
   [282]   
   [283]our team
   [284]   
   [285]careers
   [286]   
   [287]contact us

data scientists

     * [288]blog
     * [289]hackathon
     * [290]discussions
     * [291]apply jobs
     * [292]leaderboard

companies

     * [293]post jobs
     * [294]trainings
     * [295]hiring hackathons
     * [296]advertising
     * [297]reach us

   don't have an account? [298]sign up here.

join our community :

   [299]46336 [300]followers
   [301]20224 [302]followers
   [303]followers
   [304]7513 [305]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [306]privacy policy
     * [307]terms of use
     * [308]refund policy

   don't have an account? [309]sign up here

   iframe: [310]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [311](button) join now

   subscribe!

   iframe: [312]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [313](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/business-analytics/
  94. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  95. https://www.analyticsvidhya.com/blog/category/business-analytics/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/category/r/
  98. https://www.analyticsvidhya.com/blog/author/sunil-ray/
  99. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog&utm_medium=essentialmlalgorithmsarticle
 100. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog&utm_medium=essentialmlalgorithmsarticle
 101. https://courses.analyticsvidhya.com/bundles/data-science-beginners-with-interview
 102. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/09/machine-.png
 103. https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/
 104. https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/
 105. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/linear_regression.png
 106. https://en.wikipedia.org/wiki/logistic_function
 107. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/logistic_regression.png
 108. https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-id173/
 109. https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/
 110. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/ikbzk.png
 111. http://stats.stackexchange.com/
 112. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/download.jpg
 113. https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/
 114. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/id1661.png
 115. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/id1662.png
 116. https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/
 117. http://scikit-learn.org/stable/modules/id166.html
 118. https://en.wikipedia.org/wiki/bayes'_theorem
 119. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/bayes_rule.png
 120. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/bayes_41.png
 121. http://scikit-learn.org/stable/modules/naive_bayes.html
 122. http://introductiontok-nearestneighbors:simplified/
 123. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/knn.png
 124. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/splatter_ink_blot_texture_by_maki_tak-d5p6zph.jpg
 125. https://www.analyticsvidhya.com/wp-content/uploads/2015/08/kmenas.png
 126. https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/
 127. https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/
 128. https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/
 129. https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/
 130. https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/
 131. http://scikit-learn.org/stable/modules/decomposition.html#decompositions
 132. https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/
 133. http://discuss.analyticsvidhya.com/t/what-is-the-fundamental-difference-between-randomforest-and-gradient-boosting-algorithms/2341
 134. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
 135. https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/
 136. https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/
 137. https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=common-machine-learning-algorithms&utm_medium=blog
 138. https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=common-machine-learning-algorithms&utm_medium=blog
 139. https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=common-machine-learning-algorithms&utm_medium=blog
 140. http://feedburner.google.com/fb/a/mailverify?uri=analyticsvidhya
 141. http://twitter.com/analyticsvidhya
 142. http://facebook.com/analyticsvidhya
 143. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 144. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?share=linkedin
 145. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?share=facebook
 146. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?share=twitter
 147. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?share=pocket
 148. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?share=reddit
 149. https://www.analyticsvidhya.com/blog/tag/c4-5/
 150. https://www.analyticsvidhya.com/blog/tag/cart/
 151. https://www.analyticsvidhya.com/blog/tag/decision-tree/
 152. https://www.analyticsvidhya.com/blog/tag/gbm/
 153. https://www.analyticsvidhya.com/blog/tag/id116/
 154. https://www.analyticsvidhya.com/blog/tag/knn/
 155. https://www.analyticsvidhya.com/blog/tag/linear-regression/
 156. https://www.analyticsvidhya.com/blog/tag/logistic-regression/
 157. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 158. https://www.analyticsvidhya.com/blog/tag/naive-bayes/
 159. https://www.analyticsvidhya.com/blog/tag/neural-network/
 160. https://www.analyticsvidhya.com/blog/tag/random-forest/
 161. https://www.analyticsvidhya.com/blog/tag/reinforcement/
 162. https://www.analyticsvidhya.com/blog/tag/supervised-learning/
 163. https://www.analyticsvidhya.com/blog/tag/unsupervised/
 164. https://www.analyticsvidhya.com/blog/author/sunil-ray/
 165. https://discuss.analyticsvidhya.com/
 166. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92380
 167. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92394
 168. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92400
 169. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92402
 170. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92404
 171. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92410
 172. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92414
 173. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92416
 174. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92417
 175. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92436
 176. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92437
 177. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-117618
 178. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92497
 179. http://twitter.com/im_utm
 180. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92552
 181. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92706
 182. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-92744
 183. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-93236
 184. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-109762
 185. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-93085
 186. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-93518
 187. https://www.facebook.com/app_scoped_user_id/10153310687928141/
 188. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-94650
 189. https://www.facebook.com/app_scoped_user_id/10153564059874654/
 190. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-95096
 191. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-96010
 192. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-96181
 193. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-98659
 194. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-100598
 195. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-101309
 196. http://na/
 197. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-103192
 198. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-103718
 199. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-104348
 200. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-104350
 201. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-105770
 202. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-105771
 203. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-105805
 204. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-105820
 205. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-105853
 206. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-106262
 207. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-106402
 208. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-106874
 209. https://twitter.com/xuxoramos
 210. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-107024
 211. https://www.quora.com/in-the-data-science-venn-diagram-why-is-the-common-region-of-hacking-skills-and-substantive-expertise-considered-as-danger-zone
 212. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-112323
 213. http://thedevmasters.com/
 214. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-107434
 215. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-107736
 216. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-109757
 217. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-111356
 218. http://www.excelclutchbrake.com/
 219. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-112106
 220. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-112309
 221. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-112317
 222. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-112851
 223. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-113108
 224. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-113171
 225. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-113486
 226. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-113534
 227. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-114794
 228. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-114920
 229. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-115100
 230. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-115252
 231. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-115537
 232. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-115567
 233. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-115767
 234. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-116024
 235. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-116525
 236. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-116820
 237. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-117074
 238. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-117152
 239. http://www.kareermatrix.com/
 240. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-117379
 241. http://www.kareermatrix.com/
 242. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-117380
 243. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-117441
 244. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-117913
 245. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-118118
 246. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-118227
 247. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-119434
 248. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-119469
 249. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-120491
 250. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-120475
 251. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/#comment-120492
 252. https://datahack.analyticsvidhya.com/user/profile/srk
 253. https://datahack.analyticsvidhya.com/user/profile/mark12
 254. https://datahack.analyticsvidhya.com/user/profile/nilabha
 255. https://datahack.analyticsvidhya.com/user/profile/nitish007
 256. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 257. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 258. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 259. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 260. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 261. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 262. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 263. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 264. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 265. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 266. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 267. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 268. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 269. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 270. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 271. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 272. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 273. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 274. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 275. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 276. http://www.analyticsvidhya.com/about-me/
 277. https://www.analyticsvidhya.com/about-me/team/
 278. https://www.analyticsvidhya.com/career-analytics-vidhya/
 279. https://www.analyticsvidhya.com/contact/
 280. https://www.analyticsvidhya.com/about-me/write/
 281. http://www.analyticsvidhya.com/about-me/
 282. https://www.analyticsvidhya.com/about-me/team/
 283. https://www.analyticsvidhya.com/about-me/team/
 284. https://www.analyticsvidhya.com/about-me/team/
 285. https://www.analyticsvidhya.com/career-analytics-vidhya/
 286. https://www.analyticsvidhya.com/about-me/team/
 287. https://www.analyticsvidhya.com/contact/
 288. https://www.analyticsvidhya.com/blog
 289. https://datahack.analyticsvidhya.com/
 290. https://discuss.analyticsvidhya.com/
 291. https://www.analyticsvidhya.com/jobs/
 292. https://datahack.analyticsvidhya.com/users/
 293. https://www.analyticsvidhya.com/corporate/
 294. https://trainings.analyticsvidhya.com/
 295. https://datahack.analyticsvidhya.com/
 296. https://www.analyticsvidhya.com/contact/
 297. https://www.analyticsvidhya.com/contact/
 298. https://datahack.analyticsvidhya.com/signup/
 299. https://www.facebook.com/analyticsvidhya/
 300. https://www.facebook.com/analyticsvidhya/
 301. https://twitter.com/analyticsvidhya
 302. https://twitter.com/analyticsvidhya
 303. https://plus.google.com/+analyticsvidhya
 304. https://in.linkedin.com/company/analytics-vidhya
 305. https://in.linkedin.com/company/analytics-vidhya
 306. https://www.analyticsvidhya.com/privacy-policy/
 307. https://www.analyticsvidhya.com/terms/
 308. https://www.analyticsvidhya.com/refund-policy/
 309. https://id.analyticsvidhya.com/accounts/signup/
 310. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 311. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 312. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 313. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 315. https://www.facebook.com/analyticsvidhya
 316. https://twitter.com/analyticsvidhya
 317. https://plus.google.com/+analyticsvidhya/posts
 318. https://in.linkedin.com/company/analytics-vidhya
 319. https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 320. https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 321. https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 322. https://www.analyticsvidhya.com/blog/2017/09/interview-pankaj-kulshreshtha-ceo-scienaptic-systems/
 323. https://www.analyticsvidhya.com/blog/2017/09/building-machine-learning-model-fun-using-orange/
 324. https://www.analyticsvidhya.com/blog/author/sunil-ray/
 325. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 326. https://www.facebook.com/analyticsvidhya/
 327. https://twitter.com/analyticsvidhya
 328. https://plus.google.com/+analyticsvidhya
 329. https://plus.google.com/+analyticsvidhya
 330. https://in.linkedin.com/company/analytics-vidhya
 331. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 332. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 333. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 334. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 335. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 336. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 337. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 338. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 339. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 340. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 341. javascript:void(0);
 342. javascript:void(0);
 343. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 344. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 345. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 346. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 347. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 348. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 349. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 350. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 351. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 352. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f09%2fcommon-machine-learning-algorithms%2f&linkname=essentials%20of%20machine%20learning%20algorithms%20%28with%20python%20and%20r%20codes%29
 353. javascript:void(0);
 354. javascript:void(0);
