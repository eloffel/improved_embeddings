   [1]

linkedin

     * [2]sign in
     * [3]join now

   derivation of convolutional neural network from fully connected network
   step-by-step

   fully connected network vs. convolutional neural network

    derivation of convolutional neural network from fully connected network
                                  step-by-step

   published on april 9, 2018april 9, 2018     125 likes     2 comments

   [4]ahmed gad

[5]ahmed gad[6]follow

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

     * (button) like125
     * (button) comment2
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       25

   in image analysis, convolutional neural networks (id98s or convnets for
   short) are time and memory efficient than fully connected (fc)
   networks. but why? what are the advantages of convnets over fc networks
   in image analysis? how is convnet derived from fc networks? where the
   term convolution in id98s came from? these questions are to be answered
   in this article.

1. introduction

   image analysis has a number of challenges such as classification,
   id164, recognition, description, etc. if an image
   classifier, for example, is to be created, it should be able to work
   with a high accuracy even with variations such as occlusion,
   illumination changes, viewing angles, and others. the traditional
   pipeline of image classification with its main step of feature
   engineering is not suitable for working in rich environments. even
   experts in the field won   t be able to give a single or a group of
   features that are able to reach high accuracy under different
   variations. motivated by this problem, the idea of id171
   came out. the suitable features to work with images are learned
   automatically. this is the reason why id158s (anns)
   are one of the robust ways of image analysis. based on a learning
   algorithm such as id119 (gd), ann learns the image features
   automatically. the raw image is applied to the ann and ann is
   responsible for generating the features describing it.

2. image analysis using fc network

   let   s see how ann works with images and why id98 is efficient in its
   time and memory requirements w.r.t. the following 3x3 gray image in
   figure 1. the example given uses small image size and fewer number of
   neurons for simplicity.

  figure 1

   [:0]

   the inputs of the ann input layer are the image pixels. each pixel
   represents an input. because the ann works with 1d vectors, not 2d
   matrices, it is better to convert the above 2d image into a 1d vector
   as in figure 2.

  figure 2

   [:0]

   each pixel is mapped to an element in the vector. each element in the
   vector represented a neuron in ann. because the image has 3x3=9 pixels,
   then there will be 9 neurons in the input layer. representing the
   vector as row or column doesn   t matter but ann usually extends
   horizontally and each of its layers is represented as a column vector.

   after preparing the input of the ann, next is to add the hidden
   layer(s) that learns how to convert the image pixels into
   representative features. assume that there is a single hidden layer
   with 16 neurons as in figure 3.

  figure 3

   [:0]

   because the network is fully connected, this means that each neuron in
   layer i is connected to all neurons in layer i-1. as a result, each
   neuron in the hidden layer is connected to all 9 pixels in the input
   layer. in other words, each input pixel is connected to the 16 neurons
   in the hidden layer where each connection has a corresponding unique
   parameter. by connecting each pixel to all neurons in the hidden layer,
   there will be 9x16=144 parameters or weights for such tiny network as
   shown in figure 4.

  figure 4

   [:0]

3. large number of parameters

   the number of parameters in this fc network seems acceptable. but this
   number highly increases as the number of image pixels and hidden layers
   increase.

   for example, if this network has two hidden layers with a number of
   neurons of 90 and 50, then the number of parameters between the input
   layer and the first hidden layer is 9x90=810. the number of parameters
   between the two hidden layers is 90x50=4,500. the total number of
   parameters in such network is 810+4,500=5,310. this is a large number
   for such network. another case of a very small image of size 32x32
   (1,024 pixels). if the network operates with a single hidden layer of
   500 neurons, there are a total of 1,024*500=512,000 parameter (weight).
   this is a huge number for a network with just single hidden layer
   working with a small image. there must be a solution to decrease such
   number of parameters. this is where id98 has a critical role. it creates
   a very large network but with less number of parameters than fc
   networks.

4. neurons grouping

   the problem that makes the number of parameters gets very large even
   for small networks is that fc networks add a parameter between every
   two neurons in the successive layers. rather than assigning a single
   parameter between every two neurons, a single parameter may be given to
   a block or group of neurons as in figure 5. the pixel with index 0 in
   figure 3 is connected to the first 4 neurons with indices (0, 1, 2, and
   3) with 4 different weights. if the neurons are grouped in groups of 4
   as in figure 5, then all neurons inside the same group will be assigned
   a single parameter.

  figure 5

   [:0]

   as a result, the pixel with index 0 in figure 5 will be connected to
   the first 4 neurons with the same weight as in figure 6. the same
   parameter is assigned to every 4 successive neurons. as a result, the
   number of parameters is reduced by a factor of 4. each input neuron
   will have 16/4=4 parameters. the entire network will have 144/4=36
   parameters. it is 75% reduction of the parameters. this is fine but
   still, it is possible to reduce more parameters.

  figure 6

   [:0]

   figure 7 shows the unique connections from each pixel to the first
   neuron of each group. that is all missing connections are just a
   duplicate of the existing ones. hypothetically, there is a connection
   from each pixel to each neuron in each group as in figure 4 because the
   network is still fully connected.

  figure 7

   [:0]

   for making it simple, all connections are omitted except for the
   connections between all pixels to just the first neuron in the first
   group as shown in figure 8. it seems that each group is still connected
   to all 9 pixels and thus it will have 9 parameters. it is possible to
   reduce the number of pixels that such neuron is connected to.

  figure 8

   [:0]

5. pixels spatial correlation

   current configuration makes each neuron accepts all pixels. if there is
   a function f(x1,x2,x3,x4) that accepts 4 inputs, that means the
   decision is to be taken based on all of these 4 inputs. if the function
   with just 2 inputs gives the same results as using all 4 inputs, then
   we do not have to use all of these 4 inputs. the 2 inputs giving the
   required results are sufficient. this is similar to the above case.
   each neuron accepts all 9 pixels as inputs. if the same or better
   results will be returned by using fewer pixels, then we should go
   through it.

   usually, in image analysis, each pixel is highly correlated to pixels
   surrounding it (i.e. neighbors). the higher the distance between two
   pixels, the more they will be uncorrelated. for example, in the
   cameraman image shown in figure 9, a pixel inside the face is
   correlated to surrounding face pixels around it. but it is less
   correlated to far pixels such as sky or ground.

  figure 9

   [:0]

   based on such assumption, each neuron in the above example will accept
   just pixels that are spatially correlated to each other because working
   on all of them is reasonable. rather than applying all 9 pixels to each
   neuron as input, it is possible to just select 4 spatially correlated
   pixels as in figure 10. the first pixel of index 0 in the column vector
   located at (0,0) in the image will be applied as an input to the first
   neuron with its 3 most spatially correlated pixels. based on the input
   image, the 3 most spatially correlated pixels to that pixel are pixels
   with indices (0,1), (1,0), and (1,1). as a result, the neuron will
   accept just 4 pixels rather than 9. because all neurons in the same
   group share the same parameters, then the 4 neurons in each group will
   have just 4 parameters rather than 9. as a result, the total number of
   parameters will be 4x4=16. compared to the fully connected network in
   figure 4, there is a reduction of a 144-16=128 parameter (i.e. 88.89%
   reduction).

  figure 10

   [:0]

6. convolution in id98

   at this point, the question of why id98 is more time and memory
   efficient than fc network is answered. using fewer parameters allows
   the increase of a deep id98 with a huge number of layers and neurons
   which is not possible in fc network. next is to get the idea of
   convolution in id98.

   now there are just 4 weights assigned to all neurons in the same block.
   how will these 4 weights cover all 9 pixels? let   s see how this works.

   figure 11 shows the previous network in figure 10 but after adding the
   weights labels to the connections. inside the neuron, each of the 4
   input pixels is multiplied by its corresponding weight. the equation is
   shown in figure 11. the four pixels and weights would be better
   visualized as matrices as in figure 11. the previous result will be
   achieved by multiplying the weights matrix to the current set of 4
   pixels element-by-element. in practical, the size of the convolution
   mask should be odd such as 3x3. for being handled better, 2x2 mask is
   used in this example.

  figure 11

   [:0]

   moving to the next neuron of index 1, it will work with another set of
   spatially correlated pixels with the same weights used by the neuron
   with index 0. also, neurons with indices 2 and 3 will work with other
   two sets of spatially correlated pixels. this is shown in figure 12. it
   seems that first neuron in the group starts from the top-left pixel and
   choose a number of pixels surrounding to it. the last neuron in the
   group works on the bottom-right pixel and its surrounding pixels. the
   in-between neurons are adjusted to select in-between pixels. such
   behavior is identical to convolution. convolution between the set of
   weights of the group and the image. this is why id98 has the term
   convolution.

  figure 12

   [:0]

   the same procedure happens for the remaining groups of neurons. the
   first neuron of each group starts from the top-left corner and its
   surrounding pixels. the last neuron of each group works with the
   bottom-right corner and its surrounding pixels. the in-between neurons
   work on the in-between pixels.

7. references

   aghdam, hamed habibi, and elnaz jahani heravi. guide to convolutional
   neural networks: a practical application to traffic-sign detection and
   classification. springer, 2017.

   [7]ahmed gad

[8]ahmed gad

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

   [9]follow

   2 comments
   article-comment__guest-image
   [10]sign in to leave your comment
   show more comments.
     __________________________________________________________________

more from ahmed gad

   [11]27 articles
   from y=x to building a complete id158

[12]from y=x to building a complete artificial   

   march 29, 2019

   feature reduction using genetic algorithm with python

[13]feature reduction using genetic algorithm   

   january 29, 2019

   id158s optimization using genetic algorithm with
   python

[14]id158s optimization   

   january 24, 2019

     *    2019
     * [15]about
     * [16]user agreement
     * [17]privacy policy
     * [18]cookie policy
     * [19]copyright policy
     * [20]brand policy
     * [21]manage subscription
     * [22]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   5. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad&trk=author-info__follow-button
   7. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   8. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   9. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad&trk=author-info__follow-button-bottom
  10. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad&trk=article-reader_leave-comment
  11. https://www.linkedin.com/today/author/ahmedfgad
  12. https://www.linkedin.com/pulse/from-yx-building-complete-artificial-neural-network-ahmed-gad?trk=related_artice_from y=x to building a complete id158_article-card_title
  13. https://www.linkedin.com/pulse/feature-reduction-using-genetic-algorithm-ahmed-gad?trk=related_artice_feature reduction using genetic algorithm with python_article-card_title
  14. https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad?trk=related_artice_id158s optimization using genetic algorithm with python_article-card_title
  15. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  16. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  17. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  18. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  19. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  20. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  21. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  22. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
