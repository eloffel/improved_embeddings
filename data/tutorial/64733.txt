   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*bwjhc_kwpgcf6ryzmd4sdw.png]

under the hood of neural networks. part 1: fully connected.

   [16]go to the profile of andrey sakryukin
   [17]andrey sakryukin (button) blockedunblock (button) followfollowing
   apr 8, 2018

   deep learning is progressing fast, incredibly fast. one of the reasons
   for having such a big community of ai developers is that we got a
   number of really handy libraries like tensorflow, pytorch, caffe, and
   others. because of that, often implementation of a neural network does
   not require any profound knowledge in the area, which is quite cool!
   however, as the complexity of tasks grows, knowing what is actually
   going on inside can be quite useful. this knowledge can help you with
   the selection of id180, weights initializations,
   understanding of advanced concepts and many more. so in this set of
   articles, i   m going to explain the mathematics behind the id136 and
   training processes of different types of neural networks.

   this post i will devote the most basic type of neural networks:
   fully-connected networks. in spite of the fact that pure
   fully-connected networks are the simplest type of networks,
   understanding the principles of their work is useful for two reasons.
   first, it is way easier for the understanding of mathematics behind,
   compared to other types of networks. second, fully-connected layers are
   still present in most of the models.

   here i will explain two main processes in any supervised neural
   network: forward and backward passes in fully connected networks. the
   focus of this article will be on the concept called id26,
   which became a workhorse of the modern artificial intelligence.

forward pass

   forward pass is basically a set of operations which transform network
   input into the output space. during the id136 stage neural network
   relies solely on the forward pass. let   s consider a simple neural
   network with 2-hidden layers which tries to classify a binary number
   (here decimal 3) as even or odd:
   [1*ssfldzdb3vizmwjl1ohxog.png]

   here we assume that each neuron, except the neurons in the last layers,
   uses [18]relu activation function (the last layer uses [19]softmax).
   id180 are used to bring non-linearity into the system,
   which allows learning complex functions. so let   s write down the
   calculations, carried out in the first hidden layer:

   neuron 1 (top):
   [1*5jjkm82ackovz7dpqjpu4g.png]

   neuron 2 (bottom):
   [1*teppqz0ygy1txvu7n19dyq.png]

   rewriting this into a matrix form we will get:
   [1*g0pb7nuzrnzhcvyrnl4i5w.png]

   now if we represent inputs as a matrix i (in our case it is a vector,
   however if we use batch input we will have it of size number_of_samples
   by number_of_inputs), neuron weights as w and biases as b we will get:
   [1*6aoph2_n3zwjf1ypbkk5qw.png]

   which can be generalizaed for any layer of a fully connected neural
   network as:
   [1*np-nch7o6jss7vj06bf9ag.png]

   where i         is a layer number and f         is an activation function for a
   given layer. applying this formula to each layer of the network we will
   implement the forward pass and end up getting the network output. your
   result should look as following:
   [1*tccho60yza_mcfcloxs8ta.png]

backward pass

   if we do all calculations, we will end up with an output, which is
   actually incorrect (as 0.56 > 0.44 we output even as a result). so
   knowing this we want to update neuron weights and biases so that we get
   correct results. that   s exactly where id26 comes to play.
   id26 is an algorithm which calculates error gradients with
   respect to each network variable (neuron weights and biases). those
   gradients are later used in optimization algorithms, such as gradient
   descent, which updates them correspondingly. the process of weights and
   biases update is called backward pass.

   in order to start calculating error gradients, first, we have to
   calculate the error (in other words         loss) itself. we will use
   standard classification loss         cross id178. however, the loss
   function could be any differentiable mathematical expression. the
   standard choice for regression problem would be a root mean square
   error (rmse). the cross id178 loss looks as following:
   [1*d4hzb0zyskercdhhjcnrlw.png]

   where m is the number of classes, p is the vector of the network output
   and y is the vector of true labels. for our case we get:
   [1*wzzhjpnvllf8tgxmvhhzwg.png]

   now, in order to find error gradients with respect to each variable we
   will intensively use chain rule:
   [1*ct__c2iqwq_nzudpefemvg.png]

   so starting from the last layer and taking partial derivative of the
   loss with respect to neurons weights, we get:
   [1*g2gu_kfmyx0z22qkeudjqw.png]

   knowing the fact that in case of softmax activation and cross-enthropy
   loss we have (you can derive it yourself as a good exercise):
   [1*1_9tkdnp4tvx1iwtjqwvmg.png]

   now we can find gradient for the last layer as:
   [1*2bkfo_iiycgbwdtixp1tca.png]

   proceeding with the layer 2:
   [1*jkrup3cv5jum90mi6tvcxw.png]

   and the layer 1:
   [1*awabptaklohu_csplnikjq.png]

   following the same procedure for biases:
   [1*1j6po2tv_awg0vfrlxyjqw.png]
   [1*fauffjbu-hilgkb91c24xw.png]
   [1*ivtjilqtmggqfj8koiofca.png]

   now we can track a common pattern, which can be generalized as:
   [1*1qbbblujamor3bkuqt_f9q.png]

   which are the matrix equations for id26 algorithm. having
   those equations we can calculate the error gradient with respect to
   each weight/bias. to reduce the error we need to update our
   weights/biases in a direction opposite the gradient. this idea is used
   in id119 algorithm, which is defined as follows:
   [1*n1g1_tumhre0zloteqiwpg.png]

   where x is any trainable wariable (w or b), t is the current timestep
   (algorithm iteration) and    is a learning rate. now, setting    = 0.1
   (you can choose different, but keep in mind that small values assume
   longer training process, while high values lead to unstable training
   process) and using formulas for gradient calculations above, we can
   calculate one iteration of the id119 algorithm. you should
   get the following weight updates:
   [1*txepngxahonmqin_cyopmw.png]

   applying this changes and executing forward pass:
   [1*pb8rjjpmzxg8qbm3ie4d4w.png]

   we can see that performance of our network improved and now we have a
   bit higher value for the odd output compared to the previous example.
   running the id119 algorithm multiple times on different
   examples (or batches of samples) eventually will result in a properly
   trained neural network.

summary

   in this post i have explained the main parts of the fully-connected
   neural network training process: forward and backward passes. in spite
   of the simplicity of the presented concepts, understanding of
   id26 is an essential block in biulding robust neural models.
   i hope the knowledge you got from this post will help you to avoid
   pitfalls in the training process!

   don   t forget to clap if you found this article useful and stay tuned!
   in the [20]next post i will explain math of recurrent networks.

   please leave your feedback/thoughts/suggestions/corrections in the
   comments below!

   thanks for reading!

     * [21]machine learning
     * [22]id26
     * [23]neural networks
     * [24]deep learning
     * [25]artificial intelligence

   (button)
   (button)
   (button) 538 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of andrey sakryukin

[27]andrey sakryukin

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 538
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5223b7f78528
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_8fsbn9z9tmhe---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@andreysakryukin?source=post_header_lockup
  17. https://towardsdatascience.com/@andreysakryukin
  18. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  19. https://en.wikipedia.org/wiki/softmax_function
  20. https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78
  21. https://towardsdatascience.com/tagged/machine-learning?source=post
  22. https://towardsdatascience.com/tagged/id26?source=post
  23. https://towardsdatascience.com/tagged/neural-networks?source=post
  24. https://towardsdatascience.com/tagged/deep-learning?source=post
  25. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  26. https://towardsdatascience.com/@andreysakryukin?source=footer_card
  27. https://towardsdatascience.com/@andreysakryukin
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/5223b7f78528/share/twitter
  34. https://medium.com/p/5223b7f78528/share/facebook
  35. https://medium.com/p/5223b7f78528/share/twitter
  36. https://medium.com/p/5223b7f78528/share/facebook
