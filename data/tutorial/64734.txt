   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

neural network architectures

   [16]go to the profile of eugenio culurciello
   [17]eugenio culurciello (button) blockedunblock (button)
   followfollowing
   mar 23, 2017

   deep neural networks and deep learning are powerful and popular
   algorithms. and a lot of their success lays in the careful design of
   the neural network architecture.

   i wanted to revisit the history of neural network design in the last
   few years and in the context of deep learning.

   for a more in-depth analysis and comparison of all the networks
   reported here, please see our [18]recent article (and [19]updated
   post). one representative figure from this article is here:
   [1*n16lj3lskz2mimc_5cvkra.jpeg]
   top1 vs. operations, size     parameters. top-1 one-crop accuracy versus
   amount of operations required for a single forward pass. see
   also [20]here

   reporting top-1 one-crop accuracy versus amount of operations required
   for a single forward pass in multiple popular neural network
   architectures.

lenet5

   it is the year 1994, and this is one of the very first convolutional
   neural networks, and what propelled the field of deep learning. this
   pioneering work by yann lecun was named [21]lenet5 after many previous
   successful iterations since the year 1988!
   [0*v1vb9sdnsu1ezquy.jpg]

   the lenet5 architecture was fundamental, in particular the insight that
   image features are distributed across the entire image, and
   convolutions with learnable parameters are an effective way to extract
   similar features at multiple location with few parameters. at the time
   there was no gpu to help training, and even cpus were slow. therefore
   being able to save parameters and computation was a key advantage. this
   is in contrast to using each pixel as a separate input of a large
   multi-layer neural network. lenet5 explained that those should not be
   used in the first layer, because images are highly spatially
   correlated, and using individual pixel of the image as separate input
   features would not take advantage of these correlations.

   lenet5 features can be summarized as:
     * convolutional neural network use sequence of 3 layers: convolution,
       pooling, non-linearity    > this may be the key feature of deep
       learning for images since this paper!
     * use convolution to extract spatial features
     * subsample using spatial average of maps
     * non-linearity in the form of tanh or sigmoids
     * multi-layer neural network (mlp) as final classifier
     * sparse connection matrix between layers to avoid large
       computational cost

   in overall this network was the origin of much of the recent
   architectures, and a true inspiration for many people in the field.

the gap

   in the years from 1998 to 2010 neural network were in incubation. most
   people did not notice their increasing power, while many other
   researchers slowly progressed. more and more data was available because
   of the rise of cell-phone cameras and cheap digital cameras. and
   computing power was on the rise, cpus were becoming faster, and gpus
   became a general-purpose computing tool. both of these trends made
   neural network progress, albeit at a slow rate. both data and computing
   power made the tasks that neural networks tackled more and more
   interesting. and then it became clear   

dan ciresan net

   in 2010 dan claudiu ciresan and jurgen schmidhuber published one of the
   very fist implementations of [22]gpu neural nets. this implementation
   had both forward and backward implemented on a a [23]nvidia gtx 280
   graphic processor of an up to 9 layers neural network.

alexnet

   in 2012, alex krizhevsky released [24]alexnet which was a deeper and
   much wider version of the lenet and won by a large margin the difficult
   id163 competition.
   [0*vsi8jjfv_o6z34ks.png]

   alexnet scaled the insights of lenet into a much larger neural network
   that could be used to learn much more complex objects and object
   hierarchies. the contribution of this work were:
     * use of rectified linear units (relu) as non-linearities
     * use of dropout technique to selectively ignore single neurons
       during training, a way to avoid overfitting of the model
     * overlapping max pooling, avoiding the averaging effects of average
       pooling
     * use of gpus [25]nvidia gtx 580 to reduce training time

   at the time gpu offered a much larger number of cores than cpus, and
   allowed 10x faster training time, which in turn allowed to use larger
   datasets and also bigger images.

   the success of alexnet started a small revolution. convolutional neural
   network were now the workhorse of deep learning, which became the new
   name for    large neural networks that can now solve useful tasks   .

overfeat

   in december 2013 the nyu lab from yann lecun came up with [26]overfeat,
   which is a derivative of alexnet. the article also proposed learning
   bounding boxes, which later gave rise to many other papers on the same
   topic. i believe it is better to learn to segment objects rather than
   learn artificial bounding boxes.

vgg

   the [27]vgg networks from oxford were the first to use much smaller 3  3
   filters in each convolutional layers and also combined them as a
   sequence of convolutions.

   this seems to be contrary to the principles of lenet, where large
   convolutions were used to capture similar features in an image. instead
   of the 9  9 or 11  11 filters of alexnet, filters started to become
   smaller, too dangerously close to the infamous 1  1 convolutions that
   lenet wanted to avoid, at least on the first layers of the network. but
   the great advantage of vgg was the insight that multiple 3  3
   convolution in sequence can emulate the effect of larger receptive
   fields, for examples 5  5 and 7  7. these ideas will be also used in more
   recent network architectures as inception and resnet.
   [0*hreij1hjf7z4y9dd.jpg]

   the vgg networks uses multiple 3x3 convolutional layers to represent
   complex features. notice blocks 3, 4, 5 of vgg-e: 256  256 and 512  512
   3  3 filters are used multiple times in sequence to extract more complex
   features and the combination of such features. this is effectively like
   having large 512  512 classifiers with 3 layers, which are
   convolutional! this obviously amounts to a massive number of
   parameters, and also learning power. but training of these network was
   difficult, and had to be split into smaller networks with layers added
   one by one. all this because of the lack of strong ways to regularize
   the model, or to somehow restrict the massive search space promoted by
   the large amount of parameters.

   vgg used large feature sizes in many layers and thus id136 was
   quite [28]costly at run-time. reducing the number of features, as done
   in inception bottlenecks, will save some of the computational cost.

network-in-network

   [29]network-in-network (nin) had the great and simple insight of using
   1x1 convolutions to provide more combinational power to the features of
   a convolutional layers.

   the nin architecture used spatial mlp layers after each convolution, in
   order to better combine features before another layer. again one can
   think the 1x1 convolutions are against the original principles of
   lenet, but really they instead help to combine convolutional features
   in a better way, which is not possible by simply stacking more
   convolutional layers. this is different from using raw pixels as input
   to the next layer. here 1  1 convolution are used to spatially combine
   features across features maps after convolution, so they effectively
   use very few parameters, shared across all pixels of these features!
   [0*jia4pbbfsfqnedpw.jpg]

   the power of mlp can greatly increase the effectiveness of individual
   convolutional features by combining them into more complex groups. this
   idea will be later used in most recent architectures as resnet and
   inception and derivatives.

   nin also used an average pooling layer as part of the last classifier,
   another practice that will become common. this was done to average the
   response of the network to multiple are of the input image before
   classification.

googlenet and inception

   christian szegedy from google begun a quest aimed at reducing the
   computational burden of deep neural networks, and devised the
   [30]googlenet the first inception architecture.

   by now, fall 2014, deep learning models were becoming extermely useful
   in categorizing the content of images and video frames. most skeptics
   had given in that deep learning and neural nets came back to stay this
   time. given the usefulness of these techniques, the internet giants
   like google were very interested in efficient and large deployments of
   architectures on their server farms.

   christian thought a lot about ways to reduce the computational burden
   of deep neural nets while obtaining state-of-art performance (on
   id163, for example). or be able to keep the computational cost the
   same, while offering improved performance.

   he and his team came up with the inception module:
   [0*cjzdxzulmr_on1ao.jpg]

   which at a first glance is basically the parallel combination of 1  1,
   3  3, and 5  5 convolutional filters. but the great insight of the
   inception module was the use of 1  1 convolutional blocks (nin) to
   reduce the number of features before the expensive parallel blocks.
   this is commonly referred as    bottleneck   . this deserves its own
   section to explain: see    bottleneck layer    section below.

   googlenet used a stem without inception modules as initial layers, and
   an average pooling plus softmax classifier similar to nin. this
   classifier is also extremely low number of operations, compared to the
   ones of alexnet and vgg. this also contributed to a [31]very efficient
   network design.

bottleneck layer

   inspired by nin, the bottleneck layer of inception was reducing the
   number of features, and thus operations, at each layer, so the
   id136 time could be kept low. before passing data to the expensive
   convolution modules, the number of features was reduce by, say, 4
   times. this led to large savings in computational cost, and the success
   of this architecture.

   let   s examine this in detail. let   s say you have 256 features coming
   in, and 256 coming out, and let   s say the inception layer only performs
   3x3 convolutions. that is 256x256 x 3x3 convolutions that have to be
   performed (589,000s multiply-accumulate, or mac operations). that may
   be more than the computational budget we have, say, to run this layer
   in 0.5 milli-seconds on a google server. instead of doing this, we
   decide to reduce the number of features that will have to be convolved,
   say to 64 or 256/4. in this case, we first perform 256 -> 64 1  1
   convolutions, then 64 convolution on all inception branches, and then
   we use again a 1x1 convolution from 64 -> 256 features back again. the
   operations are now:
     * 256  64    1  1 = 16,000s
     * 64  64    3  3 = 36,000s
     * 64  256    1  1 = 16,000s

   for a total of about 70,000 versus the almost 600,000 we had before.
   almost 10x less operations!

   and although we are doing less operations, we are not losing generality
   in this layer. in fact the bottleneck layers have been proven to
   perform at state-of-art on the id163 dataset, for example, and will
   be also used in later architectures such as resnet.

   the reason for the success is that the input features are correlated,
   and thus redundancy can be removed by combining them appropriately with
   the 1x1 convolutions. then, after convolution with a smaller number of
   features, they can be expanded again into meaningful combination for
   the next layer.

inception v3 (and v2)

   christian and his team are very efficient researchers. in february 2015
   [32]batch-normalized inception was introduced as inception v2.
   batch-id172 computes the mean and standard-deviation of all
   feature maps at the output of a layer, and normalizes their responses
   with these values. this corresponds to    whitening    the data, and thus
   making all the neural maps have responses in the same range, and with
   zero mean. this helps training as the next layer does not have to learn
   offsets in the input data, and can focus on how to best combine
   features.

   in december 2015 they released a [33]new version of the inception
   modules and the corresponding architecture this article better explains
   the original googlenet architecture, giving a lot more detail on the
   design choices. a list of the original ideas are:
     * maximize information flow into the network, by carefully
       constructing networks that balance depth and width. before each
       pooling, increase the feature maps.
     * when depth is increased, the number of features, or width of the
       layer is also increased systematically
     * use width increase at each layer to increase the combination of
       features before next layer
     * use only 3x3 convolution, when possible, given that filter of 5x5
       and 7x7 can be decomposed with multiple 3x3. see figure:

   [0*fpt8z6-ykjzdob4e.jpg]
     * the new inception module thus becomes:

   [0*y9mkbwp1r8vamt2l.jpg]
     * filters can also be decomposed by [34]flattened convolutions into
       more complex modules:

   [0*rrv_n9rlyjnmq6jz.jpg]
     * inception modules can also decrease the size of the data by
       providing pooling while performing the inception computation. this
       is basically identical to performing a convolution with strides in
       parallel with a simple pooling layer:

   [0*rxf30_sjrsbfifcw.jpg]

   inception still uses a pooling layer plus softmax as final classifier.

resnet

   the revolution then came in december 2015, at about the same time as
   inception v3. [35]resnet have a simple ideas: feed the output of two
   successive convolutional layer and also bypass the input to the next
   layers!
   [0*0r0vs8myiqyob79l.jpg]

   this is similar to older ideas like [36]this one. but here they bypass
   two layers and are applied to large scales. bypassing after 2 layers is
   a key intuition, as bypassing a single layer did not give much
   improvements. by 2 layers can be thought as a small classifier, or a
   network-in-network!

   this is also the very first time that a network of > hundred, even 1000
   layers was trained.

   resnet with a large number of layers started to use a bottleneck layer
   similar to the inception bottleneck:
   [0*9tcufp28oqgok6be.jpg]

   this layer reduces the number of features at each layer by first using
   a 1x1 convolution with a smaller output (usually 1/4 of the input), and
   then a 3x3 layer, and then again a 1x1 convolution to a larger number
   of features. like in the case of inception modules, this allows to keep
   the computation low, while providing rich combination of features. see
      bottleneck layer    section after    googlenet and inception   .

   resnet uses a fairly simple initial layers at the input (stem): a 7x7
   conv layer followed with a pool of 2. contrast this to more complex and
   less intuitive stems as in inception v3, v4.

   resnet also uses a pooling layer plus softmax as final classifier.

   additional insights about the resnet architecture are appearing every
   day:
     * resnet can be seen as both parallel and serial modules, by just
       thinking of the inout as going to many modules in parallel, while
       the output of each modules connect in series
     * resnet can also be thought as [37]multiple ensembles of parallel or
       serial modules
     * it has been found that resnet usually operates on blocks of
       relatively low depth ~20   30 layers, which act in parallel, rather
       than serially flow the entire length of the network.
     * resnet, when the output is fed back to the input, as in id56, the
       network can be seen as a better [38]bio-plausible model of the
       cortex

inception v4

   and christian and team are at it again with a [39]new version of
   inception.

   the inception module after the stem is rather similar to inception v3:
   [0*sj7dp_-0r1vdpvzv.jpg]

   they also combined the inception module with the resnet module:
   [0*exgwbd4a0qkm2lu_.jpg]

   this time though the solution is, in my opinion, less elegant and more
   complex, but also full of less transparent heuristics. it is hard to
   understand the choices and it is also hard for the authors to justify
   them.

   in this regard the prize for a clean and simple network that can be
   easily understood and modified now goes to resnet.

squeezenet

   [40]squeezenet has been recently released. it is a re-hash of many
   concepts from resnet and inception, and show that after all, a better
   design of architecture will deliver small network sizes and parameters
   without needing complex compression algorithms.

enet

   our team set up to combine all the features of the recent architectures
   into a very efficient and light-weight network that uses very few
   parameters and computation to achieve state-of-the-art results. this
   network architecture is dubbed [41]enet, and was designed by [42]adam
   paszke. we have used it to perform pixel-wise labeling and
   scene-parsing. here are [43]some videos of enet in action. these videos
   are not part of the [44]training dataset.

   [45]the technical report on enet is available here. enet is a encoder
   plus decoder network. the encoder is a regular id98 design for
   categorization, while the decoder is a upsampling network designed to
   propagate the categories back into the original image size for
   segmentation. this worked used only neural networks, and no other
   algorithm to perform image segmentation.
   [1*rokvtiiylgmqkjiorkalva.png]

   as you can see in this figure enet has the highest accuracy per
   parameter used of any neural network out there!

   enet was designed to use the minimum number of resources possible from
   the start. as such it achieves such a small footprint that both encoder
   and decoder network together only occupies 0.7 mb with fp16 precision.
   even at this small size, enet is similar or above other pure neural
   network solutions in accuracy of segmentation.

an analysis of modules

   a systematic evaluation of id98 modules [46]has been presented. the
   found out that is advantageous to use:

       use elu non-linearity without batchnorm or relu with it.

       apply a learned colorspace transformation of rgb.

       use the linear learning rate decay policy.

       use a sum of the average and max pooling layers.

       use mini-batch size around 128 or 256. if this is too big for your
   gpu, decrease the learning rate proportionally to the batch size.

       use fully-connected layers as convolutional and average the
   predictions for the final decision.

       when investing in increasing training set size, check if a plateau
   has not been reach.     cleanliness of the data is more important then
   the size.

       if you cannot increase the input image size, reduce the stride in the
   con- sequent layers, it has roughly the same effect.

       if your network has a complex and highly optimized architecture, like
   e.g. googlenet, be careful with modifications.

xception

   [47]xception improves on the inception module and architecture with a
   simple and more elegant architecture that is as effective as resnet and
   inception v4.

   the xception module is presented here:
   [0*v7nfbszse6tli92y.jpg]

   this network can be anyone   s favorite given the simplicity and elegance
   of the architecture, presented here:
   [0*l4nlzvwleahcwnck.jpg]

   the architecture has 36 convolutional stages, making it close in
   similarity to a resnet-34. but the model and code is as simple as
   resnet and much more comprehensible than inception v4.

   a torch7 implementation of this network is available [48]here an
   implementation in keras/tf is availble [49]here.

   it is interesting to note that the recent xception architecture was
   also inspired by [50]our work on separable convolutional filters.

mobilenets

   a new m[51]obilenets architecture is also available since april 2017.
   this architecture uses separable convolutions to reduce the number of
   parameters. the separate convolution is the same as xception above. now
   the claim of the paper is that there is a great reduction in
   parameters         about 1/2 in case of facenet, as reported in the paper.
   here is the complete model architecture:
   [1*sijueehcals-k5tk-5m9kw.jpeg]
   mobilenets

   unfortunately, we have tested this network in actual application and
   found it to be abysmally slow on a batch of 1 on a titan xp gpu. look
   at a comparison here of id136 time per image:
     * resnet18 : 0.002871
     * alexnet : 0.001003
     * vgg16 : 0.001698
     * squeezenet : 0.002725
     * mobilenet : 0.033251

   clearly this is not a contender in fast id136! it may reduce the
   parameters and size of network on disk, but is not usable.

other notable architectures

   [52]fractalnet uses a recursive architecture, that was not tested on
   id163, and is a derivative or the more general resnet.

update

   for an update on comparison, please [53]see this post.

the future

   we believe that crafting neural network architectures is of paramount
   importance for the progress of the deep learning field. our group
   highly recommends reading carefully and understanding all the papers in
   this post.

   but one could now wonder why we have to spend so much time in crafting
   architectures, and why instead we do not use data to tell us what to
   use, and how to combine modules. this would be nice, but now it is work
   in progress. some initial interesting results are [54]here.

   note also that here we mostly talked about architectures for computer
   vision. similarly neural network architectures developed in other
   areas, and it is interesting to study the evolution of architectures
   for all other tasks also.

   if you are interested in a comparison of neural network architecture
   and computational performance, see [55]our recent paper.

acknowledgments

   this post was inspired by discussions with abhishek chaurasia, adam
   paszke, sangpil kim, alfredo canziani and others in our e-lab at purdue
   university.

about the author

   i have almost 20 years of experience in neural networks in both
   hardware and software (a rare combination). see about me here:
   [56]medium, [57]webpage, [58]scholar, [59]linkedin, and more   

     * [60]machine learning
     * [61]artificial intelligence
     * [62]towards data science
     * [63]neural networks
     * [64]deep learning

   (button)
   (button)
   (button) 3k claps
   (button) (button) (button) 14 (button) (button)

     (button) blockedunblock (button) followfollowing
   [65]go to the profile of eugenio culurciello

[66]eugenio culurciello

   i dream and build new technology

     (button) follow
   [67]towards data science

[68]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3k
     * (button)
     *
     *

   [69]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [70]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/156e5bad51ba
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/neural-network-architectures-156e5bad51ba&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/neural-network-architectures-156e5bad51ba&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_g5ahbqpt2dap---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@culurciello?source=post_header_lockup
  17. https://towardsdatascience.com/@culurciello
  18. https://arxiv.org/abs/1605.07678
  19. https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae
  20. https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae
  21. http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
  22. http://arxiv.org/abs/1003.0358
  23. http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-280
  24. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  25. http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-580/specifications
  26. http://arxiv.org/abs/1312.6229
  27. http://arxiv.org/abs/1409.1556
  28. http://arxiv.org/abs/1605.07678
  29. https://arxiv.org/abs/1312.4400
  30. https://arxiv.org/abs/1409.4842
  31. http://arxiv.org/abs/1605.07678
  32. http://arxiv.org/abs/1502.03167
  33. http://arxiv.org/abs/1512.00567
  34. http://arxiv.org/abs/1412.5474
  35. https://arxiv.org/abs/1512.03385
  36. http://yann.lecun.com/exdb/publis/pdf/sermanet-ijid98-11.pdf
  37. http://arxiv.org/abs/1605.06431
  38. https://arxiv.org/abs/1604.03640
  39. http://arxiv.org/abs/1602.07261
  40. http://arxiv.org/abs/1602.07360
  41. https://arxiv.org/abs/1606.02147
  42. https://apaszke.github.io/posts.html
  43. https://www.youtube.com/watch?v=3jq4fno5nco&list=plngy4gid0g9c4qivbrere_5v_b1pu-5pq
  44. https://www.cityscapes-dataset.com/
  45. https://arxiv.org/abs/1606.02147
  46. https://arxiv.org/abs/1606.02228
  47. https://arxiv.org/abs/1610.02357
  48. https://gist.github.com/culurciello/554c8e56d3bbaf7c66bf66c6089dc221
  49. https://keras.io/applications/#xception
  50. https://arxiv.org/abs/1412.5474
  51. https://arxiv.org/abs/1704.04861
  52. https://arxiv.org/abs/1605.07648
  53. https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae
  54. https://arxiv.org/abs/1606.06216
  55. http://arxiv.org/abs/1605.07678
  56. https://medium.com/@culurciello/
  57. https://e-lab.github.io/html/contact-eugenio-culurciello.html
  58. https://scholar.google.com/citations?user=segmqkiaaaaj
  59. https://www.linkedin.com/in/eugenioculurciello/
  60. https://towardsdatascience.com/tagged/machine-learning?source=post
  61. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  62. https://towardsdatascience.com/tagged/towards-data-science?source=post
  63. https://towardsdatascience.com/tagged/neural-networks?source=post
  64. https://towardsdatascience.com/tagged/deep-learning?source=post
  65. https://towardsdatascience.com/@culurciello?source=footer_card
  66. https://towardsdatascience.com/@culurciello
  67. https://towardsdatascience.com/?source=footer_card
  68. https://towardsdatascience.com/?source=footer_card
  69. https://towardsdatascience.com/
  70. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  72. https://medium.com/p/156e5bad51ba/share/twitter
  73. https://medium.com/p/156e5bad51ba/share/facebook
  74. https://medium.com/p/156e5bad51ba/share/twitter
  75. https://medium.com/p/156e5bad51ba/share/facebook
