   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

building an image caption generator with deep learning in tensorflow

   [11]go to the profile of cole murray
   [12]cole murray (button) blockedunblock (button) followfollowing
   apr 11, 2018
   [1*yzdxlqkxfqpdilveeljhpw.jpeg]
   generated caption: a reader successfully completing this tutorial

   in my [13]last tutorial, you learned how to create a facial recognition
   pipeline in tensorflow with convolutional neural networks. in this
   tutorial, you   ll learn how a convolutional neural network (id98) and
   long short term memory (lstm) can be combined to create an image
   caption generator and generate captions for your own images.

overview

     * introduction to image captioning model architecture
     * captions as a search problem
     * creating captions in tensorflow

prerequisites

     * basic understanding of convolutional neural networks
     * basic understanding of lstm
     * basic understanding of tensorflow

introduction to image captioning model architecture

combining a id98 and lstm

   in 2014, researchers from google released a paper, [14]show and tell: a
   neural image caption generator. at the time, this architecture was
   state-of-the-art on the mscoco dataset. it utilized a id98 + lstm to
   take an image as input and output a caption.
   [1*e90mi7yt9f0j6b9eadxfza.png]
   a id98-lstm image caption architecture [15]source

using a id98 for image embedding

   a convolutional neural network can be used to create a dense feature
   vector. this dense vector, also called an embedding, can be used as
   feature input into other algorithms or networks.

   for an image caption model, this embedding becomes a dense
   representation of the image and will be used as the initial state of
   the lstm.
   [1*tcegt4fuk_gx1fh8osyz1q.png]
   mapping input to embedding [16]source

lstm

   an lstm is a recurrent neural network architecture that is commonly
   used in problems with temporal dependences. it succeeds in being able
   to capture information about previous states to better inform the
   current prediction through its memory cell state.

   an lstm consists of three main components: a forget gate, input gate,
   and output gate. each of these gates is responsible for altering
   updates to the cell   s memory state.
   [1*j5w8frasmi93z81nlaui4w.png]
   an unrolled lstm [17]source

   for a deeper understanding of lstm   s, visit [18]chris olah   s post.

prediction with image as initial state

   in a sentence language model, an lstm is predicting the next word in a
   sentence. similarly, in a character language model, an lstm is trying
   to predict the next character, given the context of previously seen
   characters.
   [1*vwhr9w-zv8d20trgs7dufq.png]
   sentence and character model predictions [19]source

   in an image caption model, you will create an embedding of the image.
   this embedding will then be fed as initial state into an lstm. this
   becomes the first previous state to the language model, influencing the
   next predicted words.

   at each time-step, the lstm considers the previous cell state and
   outputs a prediction for the most probable next value in the sequence.
   this process is repeated until the end token is sampled, signaling the
   end of the caption.
   [1*y4p7n71lk38smrjvsmyhiw.png]
   sampling characters from an lstm. [20]source

captions as a search problem

   generating a caption can be viewed as a graph search problem. here, the
   nodes are words. the edges are the id203 of moving from one node
   to another. finding the optimal path involves maximizing the total
   id203 of a sentence.

   sampling and choosing the most probable next value is a [21]greedy
   approach to generating a caption. it is computationally efficient, but
   can lead to a sub-optimal result.

   given all possible words, it would not be computationally/space
   efficient to calculate all possible sentences and determine the optimal
   sentence. this rules out using a search algorithm such as depth first
   search or breadth first search to find the optimal path.
   [1*_rtqdunkvkg_yttbdadosg.png]
   [22]source

id125

   id125 is a breadth-first search algorithm that explores the most
   promising nodes. it generates all possible next paths, keeping only the
   top n best candidates at each iteration.

   as the number of nodes to expand from is fixed, this algorithm is
   space-efficient and allows more potential candidates than a best-first
   search.
   [1*lb1wseavm6t7c7a85s1dug.png]
   id125 for building a sentence. [23]source

review

   up to this point, you   ve learned about creating a model architecture to
   generate a sentence, given an image. this is done by utilizing a id98 to
   create a dense embedding and feeding this as initial state to an lstm.
   additionally, you   ve learned how to generate better sentences with beam
   search.

   in the next section, you   ll learn to generate captions from a
   pre-trained model in tensorflow.

creating captions in tensorflow

# project structure
          dockerfile
          bin
              download_model.py
          etc
              show-and-tell-2m.zip
              show-and-tell.pb
              word_counts.txt
          imgs
              trading_floor.jpg
          medium_show_and_tell_caption_generator
              __init__.py
              caption_generator.py
              id136.py
              model.py
              vocabulary.py
          requirements.txt

environment setup

   here, you   ll use docker to install tensorflow.

   docker is a container platform that simplifies deployment. it solves
   the problem of installing software dependencies onto different server
   environments. if you are new to docker, you can read more [24]here. to
   install docker, run:
curl [25]https://get.docker.com | sh

   after installing docker, you   ll create two files. a requirements.txt
   for the python dependencies and a dockerfile to create your docker
   environment.

   iframe: [26]/media/54799e87fd3b15c2e88c0fb0b2e827e1?postid=a142722e9b1f

   requirements.txt

   iframe: [27]/media/66a1ec68f84e1b0f40ddbf12acc28d8f?postid=a142722e9b1f

   dockerfile

   to build this image, run:
$ docker build -t colemurray/medium-show-and-tell-caption-generator -f dockerfil
e .
# on mbp, ~ 3mins
# image can be pulled from dockerhub below

   if you would like to avoid building from source, the image can be
   pulled from dockerhub using:
docker pull colemurray/medium-show-and-tell-caption-generator # recommended

download the model

   [1*zkdlri58_lq5ctuldgciia.png]
   show and tell id136 architecture [28]source

   below, you   ll download the model graph and pre-trained weights. these
   weights are from a training session on the [29]mscoco dataset for 2mm
   iterations.

   iframe: [30]/media/6899b82104b1f90032d002aa111c37d6?postid=a142722e9b1f

   to download, run:
docker run -e pythonpath=$pythonpath:/opt/app -v $pwd:/opt/app \
-it colemurray/medium-show-and-tell-caption-generator \
python3 /opt/app/bin/download_model.py \
--model-dir /opt/app/etc

   next, create a model class. this class is responsible for loading the
   graph, creating image embeddings, and running an id136 step on the
   model.

   iframe: [31]/media/db619cf915005dab502271d905d27aa4?postid=a142722e9b1f

   model.py

download the vocabulary

   when training an lstm, it is standard practice to tokenize the input.
   for a sentence model, this means mapping each unique word to a unique
   numeric id. this allows the model to utilize a softmax classifier for
   prediction.

   below, you   ll download the vocabulary used for the pre-trained model
   and create a class to load it into memory. here, the line number
   represents the numeric id of the token.
# file structure
# token num_of_occurrances
# on 213612
# of 202290
# the 196219
# in 182598
curl -o etc/word_counts.txt [32]https://raw.githubusercontent.com/colemurray/med
ium-show-and-tell-caption-generator/master/etc/word_counts.txt

   to store this vocabulary in memory, you   ll create a class responsible
   for mapping words to ids.

   iframe: [33]/media/c10a919e0af9eb359a261031ca17b0e3?postid=a142722e9b1f

creating a caption generator

   to generate captions, first you   ll create a caption generator. this
   caption generator utilizes id125 to improve the quality of
   sentences generated.

   at each iteration, the generator passes the previous state of the lstm
   (initial state is the image embedding) and previous sequence to
   generate the next softmax vector.

   the top n most probable candidates are kept and utilized in the next
   id136 step. this process continues until either the max sentence
   length is reached or all sentences have generated the end-of-sentence
   token.

   iframe: [34]/media/98ecd182d97cccc70ce92bfc7af9aefc?postid=a142722e9b1f

   next, you   ll load the show and tell model and use it with the above
   caption generator to create candidate sentences. these sentences will
   be printed along with their log id203.

   iframe: [35]/media/c150d15f9dd388adaafc02bb85e0ccca?postid=a142722e9b1f

results

   to generate captions, you   ll need to pass in one or more images to the
   script.
docker run -v $pwd:/opt/app \
-e pythonpath=$pythonpath:/opt/app \
-it colemurray/medium-show-and-tell-caption-generator  \
python3 /opt/app/medium_show_and_tell_caption_generator/id136.py \
--model_path /opt/app/etc/show-and-tell.pb \
--input_files /opt/app/imgs/trading_floor.jpg \
--vocab_file /opt/app/etc/word_counts.txt

   you should see output:
captions for image trading_floor.jpg:
 0) a group of people sitting at tables in a room . (p=0.000306)
 1) a group of people sitting around a table with laptops . (p=0.000140)
 2) a group of people sitting at a table with laptops . (p=0.000069)

   [1*jvtndr601ahk7ncrildhtq.jpeg]
   generated caption: a group of people sitting around a table
   with laptops

conclusion

   in this tutorial, you learned:
     * how a convolutional neural network and lstm can be combined to
       generate captions to an image
     * how to utilize the id125 algorithm to consider multiple
       captions and select the most probable sentence.

   complete code [36]here.

   next steps:
     * try with your own images
     * read the [37]show and tell paper
     * create an api to serve captions

call to action:

   if you enjoyed this tutorial, follow and recommend!

   interested in learning more about deep learning / machine learning?
   check out my other tutorials:

   - [38]building a facial recognition pipeline with deep learning in
   tensorflow

   [39]- deep learning id98   s in tensorflow with gpus

   [40]- deep learning with keras on google compute engine

   [41]- id126s with apache spark on google compute engine

   other places you can find me:

   - twitter: [42]https://twitter.com/_colemurray

     * [43]machine learning
     * [44]data science
     * [45]programming
     * [46]tech

   (button)
   (button)
   (button) 937 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [47]go to the profile of cole murray

[48]cole murray

   machine learning engineer | personalization @ amazon |
   [49]https://murraycole.com

     (button) follow
   [50]freecodecamp.org

[51]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 937
     * (button)
     *
     *

   [52]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [53]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a142722e9b1f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/building-an-image-caption-generator-with-deep-learning-in-tensorflow-a142722e9b1f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/building-an-image-caption-generator-with-deep-learning-in-tensorflow-a142722e9b1f&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_wcpeteeosoo4---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@colemurray?source=post_header_lockup
  12. https://medium.freecodecamp.org/@colemurray
  13. https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8
  14. https://arxiv.org/pdf/1411.4555.pdf
  15. https://arxiv.org/pdf/1411.4555.pdf
  16. https://www.slideshare.net/bhaskarmitra3/vectorland-brief-notes-from-using-text-embeddings-for-search
  17. https://colah.github.io/posts/2015-08-understanding-lstms/
  18. https://colah.github.io/posts/2015-08-understanding-lstms/
  19. https://www.youtube.com/watch?v=uxw6cs82uko
  20. https://www.youtube.com/watch?v=uxw6cs82uko
  21. https://en.wikipedia.org/wiki/greedy_algorithm
  22. http://blog.murraycole.com/wp-content/uploads/2018/04/sentence_explore_graph.png.png
  23. https://www.youtube.com/watch?v=uxw6cs82uko
  24. https://www.docker.com/
  25. https://get.docker.com/
  26. https://medium.freecodecamp.org/media/54799e87fd3b15c2e88c0fb0b2e827e1?postid=a142722e9b1f
  27. https://medium.freecodecamp.org/media/66a1ec68f84e1b0f40ddbf12acc28d8f?postid=a142722e9b1f
  28. http://blog.murraycole.com/wp-content/uploads/2018/04/show_and_tell_inception_expanded.png
  29. http://cocodataset.org/#home
  30. https://medium.freecodecamp.org/media/6899b82104b1f90032d002aa111c37d6?postid=a142722e9b1f
  31. https://medium.freecodecamp.org/media/db619cf915005dab502271d905d27aa4?postid=a142722e9b1f
  32. https://raw.githubusercontent.com/colemurray/medium-show-and-tell-caption-generator/master/etc/word_counts.txt
  33. https://medium.freecodecamp.org/media/c10a919e0af9eb359a261031ca17b0e3?postid=a142722e9b1f
  34. https://medium.freecodecamp.org/media/98ecd182d97cccc70ce92bfc7af9aefc?postid=a142722e9b1f
  35. https://medium.freecodecamp.org/media/c150d15f9dd388adaafc02bb85e0ccca?postid=a142722e9b1f
  36. https://github.com/colemurray/medium-show-and-tell-caption-generator
  37. https://arxiv.org/abs/1411.4555
  38. https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8
  39. https://medium.com/p/cba6efe0acc2
  40. https://medium.com/google-cloud/keras-inception-v3-on-google-compute-engine-a54918b0058
  41. https://medium.com/google-cloud/recommendation-systems-with-spark-on-google-dataproc-bbb276c0dafd
  42. https://twitter.com/_colemurray
  43. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  44. https://medium.freecodecamp.org/tagged/data-science?source=post
  45. https://medium.freecodecamp.org/tagged/programming?source=post
  46. https://medium.freecodecamp.org/tagged/tech?source=post
  47. https://medium.freecodecamp.org/@colemurray?source=footer_card
  48. https://medium.freecodecamp.org/@colemurray
  49. https://murraycole.com/
  50. https://medium.freecodecamp.org/?source=footer_card
  51. https://medium.freecodecamp.org/?source=footer_card
  52. https://medium.freecodecamp.org/
  53. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  55. https://medium.com/p/a142722e9b1f/share/twitter
  56. https://medium.com/p/a142722e9b1f/share/facebook
  57. https://medium.com/p/a142722e9b1f/share/twitter
  58. https://medium.com/p/a142722e9b1f/share/facebook
