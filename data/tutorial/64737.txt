   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning for text classification using spacy in python

   [16]go to the profile of susan li
   [17]susan li (button) blockedunblock (button) followfollowing
   apr 13, 2018
   [1*_0qqjxddraoelxx-oxeh-a.jpeg]
   photo credit: pixabay

   [18]spacy is a popular and easy-to-use natural language processing
   library in python. it provides current state-of-the-art accuracy and
   speed levels, and has an active open source community. however, since
   spacy is a relative new nlp library, and it   s not as widely adopted as
   [19]nltk. there is not yet sufficient tutorials available.

   in this post, we will demonstrate how text classification can be
   implemented using spacy without having any deep learning experience.

the data

   it s often time consuming and frustrating experience for a young
   researcher to find and select a suitable academic conference to submit
   his (or her) academic papers. we define    suitable conference   , meaning
   the conference is aligned with the researcher   s work and have a good
   academic ranking.

   using the conference proceeding data set, we are going to categorize
   research papers by conferences. let   s get started. the data set can be
   found [20]here.

explore

   take a quick peek:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import base64
import string
import re
from collections import counter
from nltk.corpus import stopwords
stopwords = stopwords.words('english')
df = pd.read_csv('research_paper.csv')
df.head()

   [1*fbphdo1-ohsmjvlh6uayfa.png]
   figure 1

   there is no missing values.
df.isnull().sum()

   title 0
   conference 0
   dtype: int64

   split the data to train and test sets:
from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.33, random_state=42)
print('research title sample:', train['title'].iloc[0])
print('conference of this paper:', train['conference'].iloc[0])
print('training data shape:', train.shape)
print('testing data shape:', test.shape)

   research title sample: cooperating with smartness: using heterogeneous
   smart antennas in ad-hoc networks.
   conference of this paper: infocom
   training data shape: (1679, 2)
   testing data shape: (828, 2)

   the dataset consists of 2507 short research paper titles, which have
   been classified into 5 categories (by conferences). the following
   figure summarizes the distribution of research papers by different
   conferences.
fig = plt.figure(figsize=(8,4))
sns.barplot(x = train['conference'].unique(), y=train['conference'].value_counts
())
plt.show()

   [1*kv9strxeowcklb1msu0b0g.png]
   figure 2

   the following is one way to do text preprocessing in spacy. after that,
   we are trying to find out the top words used in the papers that submit
   to the first and second categories (conferences)         infocom & iscas
import spacy
nlp = spacy.load('en_core_web_sm')
punctuations = string.punctuation
def cleanup_text(docs, logging=false):
    texts = []
    counter = 1
    for doc in docs:
        if counter % 1000 == 0 and logging:
            print("processed %d out of %d documents." % (counter, len(docs)))
        counter += 1
        doc = nlp(doc, disable=['parser', 'ner'])
        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-p
ron-']
        tokens = [tok for tok in tokens if tok not in stopwords and tok not in p
unctuations]
        tokens = ' '.join(tokens)
        texts.append(tokens)
    return pd.series(texts)
info_text = [text for text in train[train['conference'] == 'infocom']['title']]
is_text = [text for text in train[train['conference'] == 'iscas']['title']]
info_clean = cleanup_text(info_text)
info_clean = ' '.join(info_clean).split()
is_clean = cleanup_text(is_text)
is_clean = ' '.join(is_clean).split()
info_counts = counter(info_clean)
is_counts = counter(is_clean)
info_common_words = [word[0] for word in info_counts.most_common(20)]
info_common_counts = [word[1] for word in info_counts.most_common(20)]
fig = plt.figure(figsize=(18,6))
sns.barplot(x=info_common_words, y=info_common_counts)
plt.title('most common words used in the research papers for conference infocom'
)
plt.show()

   [1*zy72krb-ht1y_rpja4botw.png]
   figure 3
is_common_words = [word[0] for word in is_counts.most_common(20)]
is_common_counts = [word[1] for word in is_counts.most_common(20)]
fig = plt.figure(figsize=(18,6))
sns.barplot(x=is_common_words, y=is_common_counts)
plt.title('most common words used in the research papers for conference iscas')
plt.show()

   [1*7xkrt5hytxpvfriliuslpg.png]
   figure 4

   the top words in infocom are    networks    and    network   . it is obvious
   that infocom is a conference in the field of networking and closely
   related areas.

   the top words in iscas are    base    and    design   . it indicates that iscas
   is a conference about database, system design and related topics.

machine learning with spacy

from sklearn.feature_extraction.text import countvectorizer
from sklearn.base import transformermixin
from sklearn.pipeline import pipeline
from sklearn.id166 import linearsvc
from sklearn.feature_extraction.stop_words import english_stop_words
from sklearn.metrics import accuracy_score
from nltk.corpus import stopwords
import string
import re
import spacy
spacy.load('en')
from spacy.lang.en import english
parser = english()

   below is another way to clean text using spacy:
stoplist = set(stopwords.words('english') + list(english_stop_words))
symbols = " ".join(string.punctuation).split(" ") + ["-", "...", "   ", "   "]
class cleantexttransformer(transformermixin):
   def transform(self, x, **transform_params):
        return [cleantext(text) for text in x]
   def fit(self, x, y=none, **fit_params):
        return self
def get_params(self, deep=true):
        return {}

def cleantext(text):
    text = text.strip().replace("\n", " ").replace("\r", " ")
    text = text.lower()
    return text
def tokenizetext(sample):
    tokens = parser(sample)
    lemmas = []
    for tok in tokens:
        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != "-pron-" else
tok.lower_)
    tokens = lemmas
    tokens = [tok for tok in tokens if tok not in stoplist]
    tokens = [tok for tok in tokens if tok not in symbols]
    return tokens

   define a function to print out the most important features, the
   features that have the highest coefficients:
def printnmostinformative(vectorizer, clf, n):
    feature_names = vectorizer.get_feature_names()
    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))
    topclass1 = coefs_with_fns[:n]
    topclass2 = coefs_with_fns[:-(n + 1):-1]
    print("class 1 best: ")
    for feat in topclass1:
        print(feat)
    print("class 2 best: ")
    for feat in topclass2:
        print(feat)
vectorizer = countvectorizer(tokenizer=tokenizetext, ngram_range=(1,1))
clf = linearsvc()
pipe = pipeline([('cleantext', cleantexttransformer()), ('vectorizer', vectorize
r), ('clf', clf)])
# data
train1 = train['title'].tolist()
labelstrain1 = train['conference'].tolist()
test1 = test['title'].tolist()
labelstest1 = test['conference'].tolist()
# train
pipe.fit(train1, labelstrain1)
# test
preds = pipe.predict(test1)
print("accuracy:", accuracy_score(labelstest1, preds))
print("top 10 features used to predict: ")
printnmostinformative(vectorizer, clf, 10)
pipe = pipeline([('cleantext', cleantexttransformer()), ('vectorizer', vectorize
r)])
transform = pipe.fit_transform(train1, labelstrain1)
vocab = vectorizer.get_feature_names()
for i in range(len(train1)):
    s = ""
    indexintovocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]
]
    numoccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]
    for idx, num in zip(indexintovocab, numoccurences):
        s += str((vocab[idx], num))

   accuracy: 0.7463768115942029
   top 10 features used to predict:
   class 1 best:
   (-0.9286024231429632,    database   )
   (-0.8479561292796286,    chip   )
   (-0.7675978546440636,    wimax   )
   (-0.6933516302055982,    object   )
   (-0.6728543084136545,    functional   )
   (-0.6625144315722268,    multihop   )
   (-0.6410217867606485,    amplifier   )
   (-0.6396374843938725,    chaotic   )
   (-0.6175855765947755,    receiver   )
   (-0.6016682542232492,    web   )
   class 2 best:
   (1.1835964521070819,    speccast   )
   (1.0752051052570133,    manets   )
   (0.9490176624004726,    gossip   )
   (0.8468395015456092,    node   )
   (0.8433107444740003,    packet   )
   (0.8370516260734557,    schedule   )
   (0.8344139814680707,    multicast   )
   (0.8332232077559836,    queue   )
   (0.8255429594734555,    qos   )
   (0.8182435133796081,    location   )
from sklearn import metrics
print(metrics.classification_report(labelstest1, preds,
                                    target_names=df['conference'].unique()))
precision    recall  f1-score   support
       vldb       0.75      0.77      0.76       159
      iscas       0.90      0.84      0.87       299
   siggraph       0.67      0.66      0.66       106
    infocom       0.62      0.69      0.65       139
        www       0.62      0.62      0.62       125
avg / total       0.75      0.75      0.75       828

   here you have it. we now have done machine learning for text
   classification with the help of spacy.

   source code can be found on [21]github. have a learning weekend!

   reference: [22]kaggle

     * [23]machine learning
     * [24]nlp
     * [25]data science
     * [26]python
     * [27]classification

   (button)
   (button)
   (button) 792 claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of susan li

[29]susan li

   becoming an expert in ml, nlp, data story telling and encouraging
   others to do the same. sr data scientist, toronto canada.
   [30]https://www.linkedin.com/in/susanli/

     (button) follow
   [31]towards data science

[32]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 792
     * (button)
     *
     *

   [33]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [34]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b276b4051a49
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ehpr9ujpws7t---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@actsusanli?source=post_header_lockup
  17. https://towardsdatascience.com/@actsusanli
  18. https://spacy.io/
  19. https://www.nltk.org/
  20. https://raw.githubusercontent.com/susanli2016/machine-learning-with-python/master/research_paper.csv
  21. https://github.com/susanli2016/machine-learning-with-python/blob/master/machine learning spacy.ipynb
  22. https://www.kaggle.com/
  23. https://towardsdatascience.com/tagged/machine-learning?source=post
  24. https://towardsdatascience.com/tagged/nlp?source=post
  25. https://towardsdatascience.com/tagged/data-science?source=post
  26. https://towardsdatascience.com/tagged/python?source=post
  27. https://towardsdatascience.com/tagged/classification?source=post
  28. https://towardsdatascience.com/@actsusanli?source=footer_card
  29. https://towardsdatascience.com/@actsusanli
  30. https://www.linkedin.com/in/susanli/
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/b276b4051a49/share/twitter
  37. https://medium.com/p/b276b4051a49/share/facebook
  38. https://medium.com/p/b276b4051a49/share/twitter
  39. https://medium.com/p/b276b4051a49/share/facebook
