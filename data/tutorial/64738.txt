   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

the fall of id56 / lstm

   [16]go to the profile of eugenio culurciello
   [17]eugenio culurciello (button) blockedunblock (button)
   followfollowing
   apr 13, 2018
   [1*fr-q6mqnyjjr6htgltic7g.jpeg]

   we fell for recurrent neural networks (id56), long-short term memory
   (lstm), and all their variants. now it is time to drop them!

   it is the year 2014 and lstm and id56 make a great come-back from the
   dead. we all read [18]colah   s blog and [19]karpathy   s ode to id56. but
   we were all young and unexperienced. for a few years this was the way
   to solve sequence learning, sequence translation (id195), which also
   resulted in amazing results in speech to text comprehension and the
   raise of [20]siri, [21]cortana, [22]google voice assistant, [23]alexa.
   also let us not forget machine translation, which resulted in the
   ability to translate documents into different languages or [24]neural
   machine translation, but also translate [25]images into text, [26]text
   into images, and [27]captioning video, and     well you got the idea.

   then in the following years (2015   16) came [28]resnet and
   [29]attention. one could then better understand that lstm were a clever
   bypass technique. also attention showed that mlp network could be
   replaced by averaging networks influenced by a context vector. more on
   this later.

   it only took 2 more years, but today we can definitely say:

        drop your id56 and lstm, they are no good!   

   but do not take our words for it, also see evidence that attention
   based networks are used more and more by [30]google, [31]facebook,
   [32]salesforce, to name a few. all these companies have replaced id56
   and variants for attention based models, and it is just the beginning.
   id56 have the days counted in all applications, because they require
   more resources to train and run than attention-based models. see
   [33]this post for more info.

but why?

   remember id56 and lstm and derivatives use mainly sequential processing
   over time. see the horizontal arrow in the diagram below:
   [1*nkhwsoynut5xu7pyf6znhg.png]
   sequential processing in id56, from:
   [34]http://colah.github.io/posts/2015-08-understanding-lstms/

   this arrow means that long-term information has to sequentially travel
   through all cells before getting to the present processing cell. this
   means it can be easily corrupted by being multiplied many time by small
   numbers < 0. this is the cause of [35]vanishing gradients.

   to the rescue, came the lstm module, which today can be seen as
   multiple switch gates, and a bit like [36]resnet it can bypass units
   and thus remember for longer time steps. lstm thus have a way to remove
   some of the vanishing gradients problems.
   [1*j5w8frasmi93z81nlaui4w.png]
   sequential processing in lstm, from:
   [37]http://colah.github.io/posts/2015-08-understanding-lstms/

   but not all of it, as you can see from the figure above. still we have
   a sequential path from older past cells to the current one. in fact the
   path is now even more complicated, because it has additive and forget
   branches attached to it. no question lstm and gru and derivatives are
   able to learn a lot of longer term information! see results [38]here;
   but they can remember sequences of 100s, not 1000s or 10,000s or more.

   and one issue of id56 is that they are [39]not hardware friendly. let me
   explain: it takes a lot of resources we do not have to train these
   network fast. also it takes much resources to run these model in the
   cloud, and given that the demand for speech-to-text is growing rapidly,
   the cloud is not scalable. we will need to process at the edge, right
   into the amazon echo! see note below for more details.

what do you do?

   at this time (september 2018) i would seriously consider this approach
   [40]here. this is a 2d convolutional based neural network with causal
   convolution that can outperform both id56/lstm and attention based
   models like the [41]transformer.

   the [42]transformer has definitely been a great suggestion from 2017
   until the paper above. it has great advantages in training and in
   number of parameters, as we discussed [43]here.

   alternatively: if sequential processing is to be avoided, then we can
   find units that    look-ahead    or better    look-back   , since most of the
   time we deal with real-time causal data where we know the past and want
   to affect future decisions. not so in translating sentences, or
   analyzing recorded videos, for example, where we have all data and can
   reason on it more time. such look-back/ahead units are neural attention
   modules, which we previously explained [44]here.

   to the rescue, and combining multiple neural attention modules, comes
   the    hierarchical neural attention encoder   , shown in the figure below:
   [1*i5s9dojkw3qtkat0thrid86.png]
   hierarchical neural attention encoder

   a better way to look into the past is to use attention modules to
   summarize all past encoded vectors into a context vector ct.

   notice there is a hierarchy of attention modules here, very similar to
   the hierarchy of neural networks. this is also similar to [45]temporal
   convolutional network (tcn), reported in note 3 below.

   in the hierarchical neural attention encoder multiple layers of
   attention can look at a small portion of recent past, say 100 vectors,
   while layers above can look at 100 of these attention modules,
   effectively integrating the information of 100 x 100 vectors. this
   extends the ability of the hierarchical neural attention encoder to
   10,000 past vectors.

     this is the way to look back more into the past and be able to
     influence the future.

   but more importantly look at the length of the path needed to propagate
   a representation vector to the output of the network: in hierarchical
   networks it is proportional to log(n) where n are the number of
   hierarchy layers. this is in contrast to the t steps that a id56 needs
   to do, where t is the maximum length of the sequence to be remembered,
   and t >> n.

     it is easier to remember sequences if you hop 3   4 times, as opposed
     to hopping 100 times!

   this architecture is similar to a [46]id63, but lets
   the neural network decide what is read out from memory via attention.
   this means an actual neural network will decide which vectors from the
   past are important for future decisions.

   but what about storing to memory? the architecture above stores all
   previous representation in memory, unlike neural turning machines. this
   can be rather inefficient: think about storing the representation of
   every frame in a video         most times the representation vector does not
   change frame-to-frame, so we really are storing too much of the same!
   what can we do is add another unit to prevent correlated data to be
   stored. for example by not storing vectors too similar to previously
   stored ones. but this is really a hack, the best would be to be let the
   application guide what vectors should be saved or not. this is the
   focus of current research studies. stay tuned for more information.

     so in summary forget id56 and variants. use attention. attention
     really is all you need!

   tell your friends! it is very surprising to us to see so many companies
   still use id56/lstm for speech to text, many unaware that these networks
   are so inefficient and not scalable. please tell them about this post.

additional information

   about training id56/lstm: id56 and lstm are difficult to train because
   they require memory-bandwidth-bound computation, which is the worst
   nightmare for hardware designer and ultimately limits the applicability
   of neural networks solutions. in short, lstm require 4 linear layer
   (mlp layer) per cell to run at and for each sequence time-step. linear
   layers require large amounts of memory bandwidth to be computed, in
   fact they cannot use many compute unit often because the system has not
   enough memory bandwidth to feed the computational units. and it is easy
   to add more computational units, but hard to add more memory bandwidth
   (note enough lines on a chip, long wires from processors to memory,
   etc). as a result, id56/lstm and variants are not a good match for
   hardware acceleration, and we talked about this issue before [47]here
   and [48]here. a solution will be compute in memory-devices like the
   ones we work on at [49]fwdnxt.

notes

   note 1: hierarchical neural attention is similar to the ideas in
   [50]wavenet. but instead of a convolutional neural network we use
   hierarchical attention modules. also: hierarchical neural attention can
   be also bi-directional.

   note 2: id56 and lstm are memory-bandwidth limited problems ([51]see
   this for details). the processing unit(s) need as much memory bandwidth
   as the number of operations/s they can provide, making it impossible to
   fully utilize them! the external bandwidth is never going to be enough,
   and a way to slightly ameliorate the problem is to use internal fast
   caches with high bandwidth. the best way is to use techniques that do
   not require large amount of parameters to be moved back and forth from
   memory, or that can be re-used for multiple computation per byte
   transferred (high arithmetic intensity).

   note 3: here is a [52]paper comparing id98 to id56. temporal
   convolutional network (tcn)    outperform canonical recurrent networks
   such as lstms across a diverse range of tasks and datasets, while
   demonstrating longer effective memory   .

   note 4: related to this topic, is the fact that [53]we know little of
   how our human brain learns and remembers sequences.    we often learn and
   recall long sequences in smaller segments, such as a phone number 858
   534 22 30 memorized as four segments. behavioral experiments suggest
   that humans and some animals employ this strategy of breaking down
   cognitive or behavioral sequences into chunks in a wide variety of
   tasks            these chunks remind me of small convolutional or attention
   like networks on smaller sequences, that then are hierarchically strung
   together like in the hierarchical neural attention encoder and temporal
   convolutional network (tcn). [54]more studies make me think that
   working memory is similar to id56 networks that uses recurrent real
   neuron networks, and their capacity is very low. on the other hand both
   the cortex and [55]hippocampus give us the ability to remember really
   long sequences of steps (like: where did i park my car at airport 5
   days ago), suggesting that more parallel pathways may be involved to
   recall long sequences, where attention mechanism gate important chunks
   and force hops in parts of the sequence that is not relevant to the
   final goal or task.
   [1*hghzwoly6ibz7ftwkwuuvq.jpeg]

   note 5: the above evidence shows we do not read sequentially, in fact
   we interpret characters, words and sentences as a group. an
   attention-based or convolutional module perceives the sequence and
   projects a representation in our mind. we would not be misreading this
   if we processed this information sequentially! we would stop and notice
   the inconsistencies!

   note 6: a [56]recent paper trained unsupervised with
   attention/transformer and showed amazing performance in transfer
   learning. the vgg of nlp? this works is also an extension of
   [57]pioneering work by jeremy and sebastian, where an lstm with ad-hoc
   training procedures was able to learn unsupervised to predict the next
   word in a sequence of text, and then also able to transfer that
   knowledge to new tasks. a comparison of the effectiveness of lstm and
   transformer (attention based) is given [58]here and shows that
   attention is usually attention wins, and that    the lstm only
   outperforms the transformer on one dataset         mrpc.   

   note7: [59]here you can find a great explanation of the transformer
   architecture and data flow!

about the author

   i have almost 20 years of experience in neural networks in both
   hardware and software (a rare combination). see about me here:
   [60]medium, [61]webpage, [62]scholar, [63]linkedin, and more   

     * [64]machine learning
     * [65]deep learning
     * [66]lstm
     * [67]towards data science

   (button)
   (button)
   (button) 10k claps
   (button) (button) (button) 44 (button) (button)

     (button) blockedunblock (button) followfollowing
   [68]go to the profile of eugenio culurciello

[69]eugenio culurciello

   i dream and build new technology

     (button) follow
   [70]towards data science

[71]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 10k
     * (button)
     *
     *

   [72]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [73]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2d1594c74ce0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/the-fall-of-id56-lstm-2d1594c74ce0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/the-fall-of-id56-lstm-2d1594c74ce0&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_bo4bsgduhnbk---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@culurciello?source=post_header_lockup
  17. https://towardsdatascience.com/@culurciello
  18. http://colah.github.io/posts/2015-08-understanding-lstms/
  19. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  20. http://www.businessinsider.com/apples-siri-using-neural-networks-2016-8
  21. https://venturebeat.com/2014/04/03/behind-cortana-how-microsoft-aims-to-stand-out-with-its-personal-assistant/
  22. https://cloud.google.com/speech-to-text/
  23. https://www.zdnet.com/article/amazon-echo-the-four-hard-problems-amazon-had-to-solve-to-make-it-work/
  24. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  25. https://arxiv.org/abs/1502.03044
  26. https://arxiv.org/abs/1511.02793
  27. https://www.cs.utexas.edu/~vsub/naacl15_project.html
  28. https://arxiv.org/abs/1512.03385
  29. https://arxiv.org/abs/1502.03044
  30. https://arxiv.org/abs/1706.03762
  31. https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/
  32. https://einstein.ai/research/non-autoregressive-neural-machine-translation
  33. https://towardsdatascience.com/memory-attention-sequences-37456d271992
  34. http://colah.github.io/posts/2015-08-understanding-lstms/
  35. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  36. https://arxiv.org/abs/1512.03385
  37. http://colah.github.io/posts/2015-08-understanding-lstms/
  38. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  39. https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5
  40. https://arxiv.org/abs/1808.03867
  41. https://arxiv.org/abs/1706.03762
  42. https://arxiv.org/abs/1706.03762
  43. https://towardsdatascience.com/memory-attention-sequences-37456d271992
  44. https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d
  45. https://arxiv.org/abs/1803.01271
  46. https://arxiv.org/abs/1410.5401
  47. https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5
  48. https://towardsdatascience.com/memory-attention-sequences-37456d271992
  49. http://fwdnxt.com/
  50. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  51. https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5
  52. https://arxiv.org/abs/1803.01271
  53. http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004592
  54. https://www.ncbi.nlm.nih.gov/pubmed/23541152
  55. https://www.ncbi.nlm.nih.gov/pmc/articles/pmc3242327/
  56. https://blog.openai.com/language-unsupervised/
  57. https://arxiv.org/abs/1801.06146
  58. https://blog.openai.com/language-unsupervised/
  59. https://jalammar.github.io/illustrated-transformer/
  60. https://medium.com/@culurciello/
  61. https://e-lab.github.io/html/contact-eugenio-culurciello.html
  62. https://scholar.google.com/citations?user=segmqkiaaaaj
  63. https://www.linkedin.com/in/eugenioculurciello/
  64. https://towardsdatascience.com/tagged/machine-learning?source=post
  65. https://towardsdatascience.com/tagged/deep-learning?source=post
  66. https://towardsdatascience.com/tagged/lstm?source=post
  67. https://towardsdatascience.com/tagged/towards-data-science?source=post
  68. https://towardsdatascience.com/@culurciello?source=footer_card
  69. https://towardsdatascience.com/@culurciello
  70. https://towardsdatascience.com/?source=footer_card
  71. https://towardsdatascience.com/?source=footer_card
  72. https://towardsdatascience.com/
  73. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  75. https://medium.com/p/2d1594c74ce0/share/twitter
  76. https://medium.com/p/2d1594c74ce0/share/facebook
  77. https://medium.com/p/2d1594c74ce0/share/twitter
  78. https://medium.com/p/2d1594c74ce0/share/facebook
