   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]nanonets
   [7]nanonets
   [8]sign in[9]get started
     __________________________________________________________________

data augmentation | how to use deep learning when you have limited
data         part 2

   [10]go to the profile of bharath raj
   [11]bharath raj (button) blockedunblock (button) followfollowing
   apr 11, 2018

     this article is a comprehensive review of data augmentation
     techniques for deep learning, specific to images. this is part 2 of
     how to use deep learning when you have limited data. checkout part 1
     [12]here.

   [1*c8hnioqur4ojyezmc7onzq.png]

   we have all been there. you have a stellar concept that can be
   implemented using a machine learning model. feeling ebullient, you open
   your web browser and search for relevant data. chances are, you find a
   dataset that has around a few hundred images.

   you recall that most popular datasets have images in the order of tens
   of thousands (or more). you also recall someone mentioning having a
   large dataset is crucial for good performance. feeling disappointed,
   you wonder; can my    state-of-the-art    neural network perform well with
   the meagre amount of data i have?

   the answer is, yes! but before we get into the magic of making that
   happen, we need to reflect upon some basic questions.

why is there a need for a large amount of data?

   [1*1msfa5rkyp5ujgblexwy4q.png]
   number of parameters (in millions), for popular neural networks.

   when you train a machine learning model, what you   re really doing is
   tuning its parameters such that it can map a particular input (say, an
   image) to some output (a label). our optimization goal is to chase that
   sweet spot where our model   s loss is low, which happens when your
   parameters are tuned in the right way.

     state of the art neural networks typically have parameters in the
     order of millions!

   naturally, if you have a lot of parameters, you would need to show your
   machine learning model a proportional amount of examples, to get good
   performance. also, the number of parameters you need is proportional to
   the complexity of the task your model has to perform.

how do i get more data, if i don   t have    more data   ?

   you don   t need to hunt for novel new images that can be added to your
   dataset. why? because, neural networks aren   t smart to begin with. for
   instance, a poorly trained neural network would think that these three
   tennis balls shown below, are distinct, unique images.
   [1*l07htrw7zuhgt4oyemldig.jpeg]
   the same tennis ball, but translated.

   so, to get more data, we just need to make minor alterations to our
   existing dataset. minor changes such as flips or translations or
   rotations. our neural network would think these are distinct images
   anyway.
   [1*djnlec7yf93k4pjrjl55pa.png]
   data augmentation in play

   a convolutional neural network that can robustly classify objects even
   if its placed in different orientations is said to have the property
   called invariance. more specifically, a id98 can be invariant to
   translation, viewpoint, size or illumination (or a combination of the
   above).

   this essentially is the premise of data augmentation. in the real world
   scenario, we may have a dataset of images taken in a limited set of
   conditions. but, our target application may exist in a variety of
   conditions, such as different orientation, location, scale, brightness
   etc. we account for these situations by training our neural network
   with additional synthetically modified data.

can augmentation help even if i have lots of data?

   yes. it can help to increase the amount of relevant data in your
   dataset. this is related to the way with which neural networks learn.
   let me illustrate it with an example.
   [1*mvvwi7arkflql1poau3www.png]
   the two classes in our hypothetical dataset. the one in the left
   represents brand a (ford), and the one in the right represents brand b
   (chevrolet).

   imagine that you have a dataset, consisting of two brands of cars, as
   shown above. let   s assume that all cars of brand a are aligned exactly
   like the picture in the left (i.e. all cars are facing left) .
   likewise, all cars of brand b are aligned exactly like the picture in
   the right (i.e. facing right) . now, you feed this dataset to your
      state-of-the-art    neural network, and you hope to get impressive
   results once it   s trained.
   [1*fns3my0dvd67ri77bk1fcg.jpeg]
   a ford car (brand a), but facing right.

   let   s say it   s done training, and you feed the image above, which is a
   brand a car. but your neural network outputs that it   s a brand b car!
   you   re confused. didn   t you just get a 95% accuracy on your dataset
   using your    state-of-the-art    neural network? i   m not exaggerating,
   [13]similar incidents and goof-ups have occurred in the past.

   why does this happen? it happens because that   s how most machine
   learning algorithms work. it finds the most obvious features that
   distinguishes one class from another. here, the feature was that all
   cars of brand a were facing left, and all cars of brand b are facing
   right.

     your neural network is only as good as the data you feed it.

   how do we prevent this happening? we have to reduce the amount of
   irrelevant features in the dataset. for our car model classifier above,
   a simple solution would be to add pictures of cars of both classes,
   facing the other direction to our original dataset. better yet, you can
   just flip the images in the existing dataset horizontally such that
   they face the other side! now, on training the neural network on this
   new dataset, you get the performance that you intended to get.

     by performing augmentation, can prevent your neural network from
     learning irrelevant patterns, essentially boosting overall
     performance.
     __________________________________________________________________

getting started

   before we dive into the various augmentation techniques, there   s one
   issue that we must consider beforehand.

where do we augment data in our ml pipeline?

   the answer may seem quite obvious; we do augmentation before we feed
   the data to the model right? yes, but you have two options here. one
   option is to perform all the necessary transformations beforehand,
   essentially increasing the size of your dataset. the other option is to
   perform these transformations on a mini-batch, just before feeding it
   to your machine learning model.

   the first option is known as offline augmentation. this method is
   preferred for relatively smaller datasets, as you would end up
   increasing the size of the dataset by a factor equal to the number of
   transformations you perform (for example, by flipping all my images, i
   would increase the size of my dataset by a factor of 2).

   the second option is known as online augmentation, or augmentation on
   the fly. this method is preferred for larger datasets, as you can   t
   afford the explosive increase in size. instead, you would perform
   transformations on the mini-batches that you would feed to your model.
   some machine learning frameworks have support for online augmentation,
   which can be accelerated on the gpu.

popular augmentation techniques

   in this section, we present some basic but powerful augmentation
   techniques that are popularly used. before we explore these techniques,
   for simplicity, let us make one assumption. the assumption is that, we
   don   t need to consider what lies beyond the image   s boundary. we   ll use
   the below techniques such that our assumption is valid.

   what would happen if we use a technique that forces us to guess what
   lies beyond an image   s boundary? in this case, we need to interpolate
   some information. we   ll discuss this in detail after we cover the types
   of augmentation.

   for each of these techniques, we also specify the factor by which the
   size of your dataset would get increased (aka. data augmentation
   factor).

1. flip

   you can flip images horizontally and vertically. some frameworks do not
   provide function for vertical flips. but, a vertical flip is equivalent
   to rotating an image by 180 degrees and then performing a horizontal
   flip. below are examples for images that are flipped.
   [1*-beh1nnqlm_wj-0pcwuktw.jpeg]
   from the left, we have the original image, followed by the image
   flipped horizontally, and then the image flipped vertically.

   you can perform flips by using any of the following commands, from your
   favorite packages. data augmentation factor = 2 to 4x
# numpy.'img' = a single image.
flip_1 = np.fliplr(img)
# tensorflow. 'x' = a placeholder for an image.
shape = [height, width, channels]
x = tf.placeholder(dtype = tf.float32, shape = shape)
flip_2 = tf.image.flip_up_down(x)
flip_3 = tf.image.flip_left_right(x)
flip_4 = tf.image.random_flip_up_down(x)
flip_5 = tf.image.random_flip_left_right(x)

2. rotation

   one key thing to note about this operation is that image dimensions may
   not be preserved after rotation. if your image is a square, rotating it
   at right angles will preserve the image size. if it   s a rectangle,
   rotating it by 180 degrees would preserve the size. rotating the image
   by finer angles will also change the final image size. we   ll see how we
   can deal with this issue in the next section. below are examples of
   square images rotated at right angles.
   [1*i_f6ankj3yggkcnxqxya4a.jpeg]
   the images are rotated by 90 degrees clockwise with respect to the
   previous one, as we move from left to right.

   you can perform rotations by using any of the following commands, from
   your favorite packages. data augmentation factor = 2 to 4x
# placeholders: 'x' = a single image, 'y' = a batch of images
# 'k' denotes the number of 90 degree anticlockwise rotations
shape = [height, width, channels]
x = tf.placeholder(dtype = tf.float32, shape = shape)
rot_90 = tf.image.rot90(img, k=1)
rot_180 = tf.image.rot90(img, k=2)
# to rotate in any angle. in the example below, 'angles' is in radians
shape = [batch, height, width, 3]
y = tf.placeholder(dtype = tf.float32, shape = shape)
rot_tf_180 = tf.contrib.image.rotate(y, angles=3.1415)
# scikit-image. 'angle' = degrees. 'img' = input image
# for details about 'mode', checkout the interpolation section below.
rot = skimage.transform.rotate(img, angle=45, mode='reflect')

3. scale

   the image can be scaled outward or inward. while scaling outward, the
   final image size will be larger than the original image size. most
   image frameworks cut out a section from the new image, with size equal
   to the original image. we   ll deal with scaling inward in the next
   section, as it reduces the image size, forcing us to make assumptions
   about what lies beyond the boundary. below are examples or images being
   scaled.
   [1*inltn7gwm-m69guwfzpoaq.jpeg]
   from the left, we have the original image, the image scaled outward by
   10%, and the image scaled outward by 20%

   you can perform scaling by using the following commands, using
   scikit-image. data augmentation factor = arbitrary.
# scikit image. 'img' = input image, 'scale' = scale factor
# for details about 'mode', checkout the interpolation section below.
scale_out = skimage.transform.rescale(img, scale=2.0, mode='constant')
scale_in = skimage.transform.rescale(img, scale=0.5, mode='constant')
# don't forget to crop the images back to the original size (for
# scale_out)

4. crop

   unlike scaling, we just randomly sample a section from the original
   image. we then resize this section to the original image size. this
   method is popularly known as random cropping. below are examples of
   random cropping. if you look closely, you can notice the difference
   between this method and scaling.
   [1*ypuimialtg_9kaqwltrxjq.jpeg]
   from the left, we have the original image, a square section cropped
   from the top-left, and then a square section cropped from the
   bottom-right. the cropped sections were resized to the original
   image size.

   you can perform random crops by using any the following command for
   tensorflow. data augmentation factor = arbitrary.
# tensorflow. 'x' = a placeholder for an image.
original_size = [height, width, channels]
x = tf.placeholder(dtype = tf.float32, shape = original_size)
# use the following commands to perform random crops
crop_size = [new_height, new_width, channels]
seed = np.random.randint(1234)
x = tf.random_crop(x, size = crop_size, seed = seed)
output = tf.images.resize_images(x, size = original_size)

5. translation

   translation just involves moving the image along the x or y direction
   (or both). in the following example, we assume that the image has a
   black background beyond its boundary, and are translated appropriately.
   this method of augmentation is very useful as most objects can be
   located at almost anywhere in the image. this forces your convolutional
   neural network to look everywhere.
   [1*l07htrw7zuhgt4oyemldig.jpeg]
   from the left, we have the original image, the image translated to the
   right, and the image translated upwards.

   you can perform translations in tensorflow by using the following
   commands. data augmentation factor = arbitrary.
# pad_left, pad_right, pad_top, pad_bottom denote the pixel
# displacement. set one of them to the desired value and rest to 0
shape = [batch, height, width, channels]
x = tf.placeholder(dtype = tf.float32, shape = shape)
# we use two functions to get our desired augmentation
x = tf.image.pad_to_bounding_box(x, pad_top, pad_left, height + pad_bottom + pad
_top, width + pad_right + pad_left)
output = tf.image.crop_to_bounding_box(x, pad_bottom, pad_right, height, width)

6. gaussian noise

   over-fitting usually happens when your neural network tries to learn
   high frequency features (patterns that occur a lot) that may not be
   useful. gaussian noise, which has zero mean, essentially has data
   points in all frequencies, effectively distorting the high frequency
   features. this also means that lower frequency components (usually,
   your intended data) are also distorted, but your neural network can
   learn to look past that. adding just the right amount of noise can
   enhance the learning capability.

   a toned down version of this is the salt and pepper noise, which
   presents itself as random black and white pixels spread through the
   image. this is similar to the effect produced by adding gaussian noise
   to an image, but may have a lower information distortion level.
   [1*cx24opsnowgg7uluhkigna.png]
   from the left, we have the original image, image with added gaussian
   noise, image with added salt and pepper noise

   you can add gaussian noise to your image by using the following
   command, on tensorflow. data augmentation factor = 2x.
#tensorflow. 'x' = a placeholder for an image.
shape = [height, width, channels]
x = tf.placeholder(dtype = tf.float32, shape = shape)
# adding gaussian noise
noise = tf.random_normal(shape=tf.shape(x), mean=0.0, stddev=1.0,
dtype=tf.float32)
output = tf.add(x, noise)

advanced augmentation techniques

   real world, natural data can still exist in a variety of conditions
   that cannot be accounted for by the above simple methods. for instance,
   let us take the task of identifying the landscape in photograph. the
   landscape could be anything: freezing tundras, grasslands, forests and
   so on. sounds like a pretty straight forward classification task right?
   you   d be right, except for one thing. we are overlooking a crucial
   feature in the photographs that would affect the performance         the
   season in which the photograph was taken.

   if our neural network does not understand the fact that certain
   landscapes can exist in a variety of conditions (snow, damp, bright
   etc.), it may spuriously label frozen lakeshores as glaciers or wet
   fields as swamps.

   one way to mitigate this situation is to add more pictures such that we
   account for all the seasonal changes. but that is an arduous task.
   extending our data augmentation concept, imagine how cool it would be
   to generate effects such as different seasons artificially?

conditional gans to the rescue!

   without going into gory detail, conditional gans can transform an image
   from one domain to an image to another domain. if you think it sounds
   too vague, it   s not; that   s [14]literally how powerful this neural
   network is! below is an example of conditional gans used to transform
   photographs of summer sceneries to winter sceneries.
   [1*ezbfyk7snyvynh4bbd7zyq.jpeg]
   changing seasons using a cyclegan (source:
   [15]https://junyanz.github.io/cyclegan/)

   the above method is robust, but computationally intensive. a cheaper
   alternative would be something called neural style transfer. it grabs
   the texture/ambiance/appearance of one image (aka, the    style   ) and
   mixes it with the content of another. using this powerful technique, we
   produce an effect similar to that of our conditional gan (in fact, this
   method was introduced before cgans were invented!).

   the only downside of this method is that, the output tends to looks
   more artistic rather than realistic. however, there are certain
   advancements such as deep photo style transfer, shown below, that have
   impressive results.
   [1*ssitecgr4sjcnlboa4aspg.jpeg]
   deep photo style transfer. notice how we could generate the effect we
   desire on our dataset. (source: [16]https://arxiv.org/abs/1703.07511)

   we have not explored these techniques in great depth as we are not
   concerned with their inner working. we can use existing trained models,
   along with the magic of id21, to use it for augmentation.
     __________________________________________________________________

a brief note on interpolation

   what if you wanted to translate an image that doesn   t have a black
   background? what if you wanted to scale inward? or rotate in finer
   angles? after we perform these transformations, we need to preserve our
   original image size. since our image does not have any information
   about things outside it   s boundary, we need to make some assumptions.
   usually, the space beyond the image   s boundary is assumed to be the
   constant 0 at every point. hence, when you do these transformations,
   you get a black region where the image is not defined.
   [1*z8_8gq5zga_9peatfyx2gq.jpeg]
   from the left, an image rotated by 45 degrees anticlockwise, an image
   translated to the right, and an image scaled inward.

   but is that the right assumption? in the real world scenario, it   s
   mostly a no. image processing and ml frameworks have some standard ways
   with which you can decide on how to fill the unknown space. they are
   defined as follows.
   [1*rg4yjyvdu28lzwkwkwc55w.jpeg]
   from the left, we have the constant, edge, reflect, symmetric and
   wrap modes.

1. constant

   the simplest interpolation method is to fill the unknown region with
   some constant value. this may not work for natural images, but can work
   for images taken in a monochromatic background

2. edge

   the edge values of the image are extended after the boundary. this
   method can work for mild translations.

3. reflect

   the image pixel values are reflected along the image boundary. this
   method is useful for continuous or natural backgrounds containing
   trees, mountains etc.

4. symmetric

   this method is similar to reflect, except for the fact that, at the
   boundary of reflection, a copy of the edge pixels are made. normally,
   reflect and symmetric can be used interchangeably, but differences will
   be visible while dealing with very small images or patterns.

5. wrap

   the image is just repeated beyond its boundary, as if it   s being tiled.
   this method is not as popularly used as the rest as it does not make
   sense for a lot of scenarios.

   besides these, you can design your own methods for dealing with
   undefined space, but usually these methods would just do fine for most
   classification problems.
     __________________________________________________________________

so, if i use all of these techniques, my ml algorithm would be robust right?

   if you use it in the right way, then yes! what is the right way you
   ask? well, sometimes not all augmentation techniques make sense for a
   dataset. consider our car example again. below are some of the ways by
   which you can modify the image.
   [1*vw3kgpp_w0wn6k3gyvlvha.jpeg]
   the first image (from the left) is the original, the second one is
   flipped horizontally, the third one is rotated by 180 degrees, and the
   last one is rotated by 90 degrees (clockwise).

   sure, they are pictures of the same car, but your target application
   may never see cars presented in these orientations.

   for instance, if you   re just going to classify random cars on the road,
   only the second image would make sense to be on the dataset. but, if
   you own an insurance company that deals with car accidents, and you
   want to identify models of upside-down, broken cars as well, the third
   image makes sense. the last image may not make sense for both the above
   scenarios.

   the point is, while using augmentation techniques, we have to make sure
   to not increase irrelevant data.
     __________________________________________________________________

is it really worth the effort?

   you   re probably expecting some results to motivate you to walk the
   extra mile. fair enough; i   ve got that covered too. let me prove that
   augmentation really works, using a toy example. you can replicate this
   experiment to verify.

   let   s create two neural networks to classify data to one among four
   classes: cat, lion, tiger or a leopard. the catch is, one will not use
   data augmentation, whereas the other will. you can download the dataset
   from here [17]link.

   if you   ve checked out the dataset, you   ll notice that there   s only 50
   images per class for both training and testing. clearly, we can   t use
   augmentation for one of the classifiers. to make the odds more fair, we
   use [18]id21 to give the models a better chance with the
   scarce amount of data.
   [1*o8wwddn6m922ey3a8tsmla.png]
   the four classes in our dataset.

   for the one without augmentation, let   s use a vgg19 network. i   ve
   written a tensorflow implementation [19]here, which is based on
   [20]this implementation. once you   ve cloned [21]my repo, you can get
   the dataset from [22]here, and vgg19.npy (used for id21)
   from [23]here. you can now run the model to verify the performance.

   i would agree though, writing extra code for data augmentation is
   indeed a bit of an effort. so, to build our second model, i turned to
   [24]nanonets. they internally use id21 and data
   augmentation to provide the best results using minimal data. all you
   need to do is upload the data on their [25]website, and wait until it   s
   trained in their servers (usually around 30 minutes). what do you know,
   it   s perfect for our comparison experiment.

   once it   s done training, you can request calls to their api to
   calculate the test accuracy. checkout out my repo for a sample code
   snippet(don   t forget to insert your model   s id in the code snippet).
results
vgg19 (no augmentation)- 76% test accuracy (highest)
nanonets (with augmentation) - 94.5% test accuracy

   impressive isn   t it. it is a fact that most models perform well with
   more data. so to provide a concrete proof, i   ve mentioned the table
   below. it shows the error rate of popular neural networks on the cifar
   10 (c10) and cifar 100 (c100) datasets. c10+ and c100+ columns are the
   error rates with data augmentation.
   [1*stjfentmuxp-t70yy_jwvg.png]
   error rates of popular neural networks on the cifar 10 and cifar 100
   datasets. (source: [26]densenet)
     __________________________________________________________________

   thank you for reading this article! hit that clap button if you did!
   hope it shed some light about data augmentation. if you have any
   questions, you could hit me up on [27]social media or send me an email
   (bharathrajn98@gmail.com).

     about nanonets: nanonets is building apis to simplify deep learning
     for developers. visit us at [28]https://www.nanonets.com for more)

   thanks to [29]sarthak jain.
     * [30]machine learning
     * [31]deep learning
     * [32]artificial intelligence
     * [33]ai
     * [34]neural networks

   (button)
   (button)
   (button) 5k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of bharath raj

[36]bharath raj

   undergrad | id161 and ai enthusiast | hungry |
   [37]https://thatbrguy.github.io

     (button) follow
   [38]nanonets

[39]nanonets

   nanonets: machine learning api

     * (button)
       (button) 5k
     * (button)
     *
     *

   [40]nanonets
   never miss a story from nanonets, when you sign up for medium.
   [41]learn more
   never miss a story from nanonets
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c26971dc8ced
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/nanonets?source=avatar-lo_yu7juwrcqs2h-81ecf7ff5ef4
   7. https://medium.com/nanonets?source=logo-lo_yu7juwrcqs2h---81ecf7ff5ef4
   8. https://medium.com/m/signin?redirect=https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@thatbrguy?source=post_header_lockup
  11. https://medium.com/@thatbrguy
  12. https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab
  13. https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai
  14. https://junyanz.github.io/cyclegan/
  15. https://junyanz.github.io/cyclegan/
  16. https://arxiv.org/abs/1703.07511
  17. https://drive.google.com/drive/folders/1gpipbqbq_ak1z_4yaj7t6yrqddyybbaq?usp=sharing
  18. https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab
  19. https://github.com/thatbrguy/vgg19-transfer-learn
  20. https://github.com/machrisaa/tensorflow-vgg
  21. https://github.com/thatbrguy/vgg19-transfer-learn
  22. https://drive.google.com/drive/folders/1gpipbqbq_ak1z_4yaj7t6yrqddyybbaq?usp=sharing
  23. https://mega.nz/#!xz8gls6j!mane91nd_wyfz_8mvkusa2yca7q-1ehid122-q1fxovvs
  24. https://nanonets.com/?utm_source=medium&utm_campaign=data augmentation/
  25. https://nanonets.com/#demo
  26. https://arxiv.org/pdf/1608.06993.pdf
  27. https://thatbrguy.github.io/
  28. https://nanonets.com/?utm_source=medium&utm_campaign=data augmentation/
  29. https://medium.com/@sarthakjain?source=post_page
  30. https://medium.com/tag/machine-learning?source=post
  31. https://medium.com/tag/deep-learning?source=post
  32. https://medium.com/tag/artificial-intelligence?source=post
  33. https://medium.com/tag/ai?source=post
  34. https://medium.com/tag/neural-networks?source=post
  35. https://medium.com/@thatbrguy?source=footer_card
  36. https://medium.com/@thatbrguy
  37. https://thatbrguy.github.io/
  38. https://medium.com/nanonets?source=footer_card
  39. https://medium.com/nanonets?source=footer_card
  40. https://medium.com/nanonets
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://medium.com/p/c26971dc8ced/share/twitter
  44. https://medium.com/p/c26971dc8ced/share/facebook
  45. https://medium.com/p/c26971dc8ced/share/twitter
  46. https://medium.com/p/c26971dc8ced/share/facebook
