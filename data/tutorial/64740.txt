   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]tensorflow
     * [9]announcements
     * [10]keras
     * [11]tensorflow.js
     * [12]mobile
     * [13]responsible ai
     * [14]all stories
     * [15]tensorflow.org
     __________________________________________________________________

introducing tensorflow id203

posted by: josh dillon, software engineer; [16]mike shwe, product manager;
and [17]dustin tran, research scientist         on behalf of the tensorflow
id203 team

   go to the profile of tensorflow
   [18]tensorflow (button) blockedunblock (button) followfollowing
   apr 11, 2018

   at the 2018 tensorflow developer summit, we [19]announced tensorflow
   id203: a probabilistic programming toolbox for machine learning
   researchers and practitioners to quickly and reliably build
   sophisticated models that leverage state-of-the-art hardware. you
   should use tensorflow id203 if:
     * you want to build a generative model of data, reasoning about its
       hidden processes.
     * you need to quantify the uncertainty in your predictions, as
       opposed to predicting a single value.
     * your training set has a large number of features relative to the
       number of data points.
     * your data is structured         for example, with groups, space, graphs,
       or language semantics         and you   d like to capture this structure
       with prior information.
     * you have an inverse problem         see this [20]tfds   18 talk for
       reconstructing fusion plasmas from measurements.

   tensorflow id203 gives you the tools to solve these problems. in
   addition, it inherits the strengths of tensorflow such as automatic
   differentiation and the ability to scale performance across a variety
   of platforms: cpus, gpus, and tpus.

what   s in tensorflow id203?

   our stack of probabilistic ml tools provides modular abstractions for
   probabilistic reasoning and statistical analysis in the tensorflow
   ecosystem.
   [0*19bjhsj-2dzq7ffh.]
   an overview of tensorflow id203. the probabilistic programming
   toolbox provides benefits for users ranging from data scientists and
   statisticians to all tensorflow users.

   layer 0: tensorflow. numerical operations. in particular, the
   linearoperator class enables matrix-free implementations that can
   exploit special structure (diagonal, low-rank, etc.) for efficient
   computation. it is built and maintained by the tensorflow id203
   team and is now part of [21]tf.linalg in core tf.

   layer 1: statistical building blocks
     * distributions ([22]tf.contrib.distributions, [23]tf.distributions):
       a large collection of id203 distributions and related
       statistics with batch and [24]broadcasting semantics.
     * bijectors ([25]tf.contrib.distributions.bijectors): reversible and
       composable transformations of random variables. bijectors provide a
       rich class of transformed distributions, from classical examples
       like the [26]log-normal distribution to sophisticated deep learning
       models such as [27]masked autoregressive flows.

   (see the [28]tensorflow distributions whitepaper for more information.)

   layer 2: model building
     * edward2 ([29]tfp.edward2): a probabilistic programming language for
       specifying flexible probabilistic models as programs.
     * probabilistic layers ([30]tfp.layers): neural network layers with
       uncertainty over the functions they represent, extending tensorflow
       layers.
     * trainable distributions ([31]tfp.trainable_distributions):
       id203 distributions parameterized by a single tensor, making
       it easy to build neural nets that output id203 distributions.

   layer 3: probabilistic id136
     * id115 ([32]tfp.mcmc): algorithms for
       approximating integrals via sampling. includes [33]hamiltonian
       monte carlo, random-walk metropolis-hastings, and the ability to
       build custom transition kernels.
     * variational id136 ([34]tfp.vi): algorithms for approximating
       integrals via optimization.
     * optimizers ([35]tfp.optimizer): stochastic optimization methods,
       extending tensorflow optimizers. includes [36]stochastic gradient
       langevin dynamics.
     * monte carlo ([37]tfp.monte_carlo): tools for computing monte carlo
       expectations.

   layer 4: pre-made models and id136 (analogous to [38]tensorflow   s
   pre-made estimators)
     * bayesian structural time series (coming soon): high-level interface
       for fitting time-series models (i.e., similar to r   s bsts package).
     * generalized linear mixed models (coming soon): high-level interface
       for fitting mixed-effects regression models (i.e., similar to r   s
       lme4 package).

   the tensorflow id203 team is committed to supporting users and
   contributors with cutting-edge features, continuous code updates, and
   bug fixes. we   ll continue to add end-to-end examples and tutorials.

let   s see some examples!

linear mixed effects models with edward2

   a linear mixed effects model is a simple approach for modeling
   structured relationships in data. also known as a hierarchical linear
   model, it shares statistical strength across groups of data points in
   order to improve id136s about any individual one.

   as demonstration, consider the insteval data set from the popular
   [39]lme4 package in r, which consists of university courses and their
   evaluation ratings. using tensorflow id203, we specify the model
   as an edward2 probabilistic program (tfp.edward2), [40]which extends
   edward. the program below reifies the model in terms of its generative
   process.
import tensorflow as tf
from tensorflow_id203 import edward2 as ed
def model(features):
  # set up fixed effects and other parameters.
  intercept = tf.get_variable("intercept", [])
  service_effects = tf.get_variable("service_effects", [])
  student_stddev_unconstrained = tf.get_variable(
      "student_stddev_pre", [])
  instructor_stddev_unconstrained = tf.get_variable(
      "instructor_stddev_pre", [])
  # set up random effects.
  student_effects = ed.multivariatenormaldiag(
      loc=tf.zeros(num_students),
      scale_identity_multiplier=tf.exp(
          student_stddev_unconstrained),
      name="student_effects")
  instructor_effects = ed.multivariatenormaldiag(
      loc=tf.zeros(num_instructors),
      scale_identity_multiplier=tf.exp(
          instructor_stddev_unconstrained),
      name="instructor_effects")
  # set up likelihood given fixed and random effects.
  ratings = ed.normal(
      loc=(service_effects * features["service"] +
           tf.gather(student_effects, features["students"]) +
           tf.gather(instructor_effects, features["instructors"]) +
           intercept),
      scale=1.,
      name="ratings")
return ratings

   the model takes as input a features dictionary of    service   ,
      students   , and    instructors   ; they are vectors where each element
   describes an individual course. the model regresses on these inputs,
   posits latent random variables, and returns a distribution over the
   courses    evaluation ratings. tensorflow session runs on this output
   will return a generation of the ratings.

   check out the [41]   linear mixed effects models    tutorial for details on
   how we train the model using the [42]tfp.mcmc.hamiltonianmontecarlo
   algorithm, and how we explore and interpret the model using posterior
   predictions.

gaussian copulas with tfp bijectors

   a [43]copula is a multivariate id203 distribution for which the
   marginal id203 distribution of each variable is uniform. to build
   a copula using tfp intrinsics, one can use bijectors and
   transformeddistribution. these abstractions enable easy creation of
   complex distributions, for example:
import tensorflow_id203 as tfp
tfd = tfp.distributions
tfb = tfp.distributions.bijectors
# example: log-normal distribution
log_normal = tfd.transformeddistribution(
    distribution=tfd.normal(loc=0., scale=1.),
    bijector=tfb.exp())
# example: kumaraswamy distribution
kumaraswamy = tfd.transformeddistribution(
    distribution=tfd.uniform(low=0., high=1.),
    bijector=tfb.kumaraswamy(
        concentration1=2.,
        concentration0=2.))
# example: masked autoregressive flow
# [44]https://arxiv.org/abs/1705.07057
shift_and_log_scale_fn = tfb.masked_autoregressive_default_template(
    hidden_layers=[512, 512],
    event_shape=[28*28])
maf = tfd.transformeddistribution(
    distribution=tfd.normal(loc=0., scale=1.),
    bijector=tfb.maskedautoregressiveflow(
        shift_and_log_scale_fn=shift_and_log_scale_fn))

   the [45]   gaussian copula    creates a few custom bijectors and then shows
   how to easily build several different copulas. for more background on
   distributions, see [46]   understanding tensorflow distributions shapes.   
   it describes how to manage shapes for sampling, batch training, and
   modeling events.

variational autoencoder with tfp utilities

   a variational autoencoder is a machine learning model which uses one
   learned system to represent data in some low-dimensional space and a
   second learned system to restore the low-dimensional representation to
   what would have otherwise been the input. because tf supports automatic
   differentiation, black-box variational id136 is a breeze! example:
import tensorflow as tf
import tensorflow_id203 as tfp
# assumes user supplies `likelihood`, `prior`, `surrogate_posterior`
# functions and that each returns a
# tf.distribution.distribution-like object.
elbo_loss = tfp.vi.monte_carlo_csiszar_f_divergence(
    f=tfp.vi.kl_reverse,  # equivalent to "evidence lower bound"
    p_log_prob=lambda z: likelihood(z).log_prob(x) + prior().log_prob(z),
    q=surrogate_posterior(x),
    num_draws=1)
train = tf.train.adamoptimizer(
    learning_rate=0.01).minimize(elbo_loss)

   to see more details, check out our [47]variational autoencoder example!

bayesian neural networks with tfp probabilistic layers

   a bayesian neural network is a neural network with a prior distribution
   over its weights and biases. it provides improved uncertainty about its
   predictions via these priors. a bayesian neural network can also be
   interpreted as an infinite ensemble of neural networks: the id203
   assigned to each neural network configuration is according to the
   prior.

   as demonstration, consider the cifar-10 dataset which has features
   (images of shape 32 x 32 x 3) and labels (values from 0 to 9). to fit
   the neural network, we   ll use [48]variational id136, which is a
   suite of methods to approximate the neural network   s posterior
   distribution over weights and biases. namely, we use the recently
   published [49]flipout estimator in the tensorflow probabilistic layers
   module (tfp.layers).
import tensorflow as tf
import tensorflow_id203 as tfp
model = tf.keras.sequential([
    tf.keras.layers.reshape([32, 32, 3]),
    tfp.layers.convolution2dflipout(
        64, kernel_size=5, padding='same', activation=tf.nn.relu),
    tf.keras.layers.maxpooling2d(pool_size=[2, 2],
                                 strides=[2, 2],
                                 padding='same'),
    tf.keras.layers.reshape([16 * 16 * 64]),
    tfp.layers.denseflipout(10)
])
logits = model(features)
neg_log_likelihood = tf.nn.softmax_cross_id178_with_logits(
    labels=labels, logits=logits)
kl = sum(model.get_losses_for(inputs=none))
loss = neg_log_likelihood + kl
train_op = tf.train.adamoptimizer().minimize(loss)

   the model object composes neural net layers on an input tensor, and it
   performs stochastic forward passes with respect to probabilistic
   convolutional layer and probabilistic densely-connected layer. the
   function returns an output tensor with shape given by the batch size
   and 10 values. each row of this tensor represents the logits
   (unconstrained id203 values) that each data point belongs to one
   of the 10 classes.

   for training, we build the id168, which comprises two terms:
   the expected negative log-likelihood and the kl divergence. we
   approximate the expected negative log-likelihood via monte carlo. the
   kl divergence is added via regularizer terms which are arguments to the
   layers.

   tfp.layers can also be used with [50]eager execution using the
   tf.keras.model class.
class mnistmodel(tf.keras.model):
  def __init__(self):
    super(mnistmodel, self).__init__()
    self.dense1 = tfp.layers.denseflipout(units=10)
    self.dense2 = tfp.layers.denseflipout(units=10)
  def call(self, input):
    """run the model."""
    result = self.dense1(input)
    result = self.dense2(result)
    # reuse variables from dense2 layer
    result = self.dense2(result)
    return result
model = mnistmodel()

getting started

   to get started with probabilistic machine learning in tensorflow, just
   run:
pip install --user --upgrade tfp-nightly

   for all the code and details, check out
   [51]github.com/tensorflow/id203. we   re excited to collaborate
   with you via github, whether you   re a user or contributor!

     * [52]machine learning
     * [53]tensorflow
     * [54]id203
     * [55]tfdevsummit

   (button)
   (button)
   (button) 2.9k claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of tensorflow

[56]tensorflow

   medium member since jan 2018

   tensorflow is a fast, flexible, and scalable open-source machine
   learning library for research and production.

     (button) follow
   [57]tensorflow

[58]tensorflow

   tensorflow is a fast, flexible, and scalable open-source machine
   learning library for research and production.

     * (button)
       (button) 2.9k
     * (button)
     *
     *

   [59]tensorflow
   never miss a story from tensorflow, when you sign up for medium.
   [60]learn more
   never miss a story from tensorflow
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/dca4c304e245
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/tensorflow/introducing-tensorflow-id203-dca4c304e245&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/tensorflow/introducing-tensorflow-id203-dca4c304e245&source=--------------------------nav_reg&operation=register
   8. https://medium.com/tensorflow?source=logo-lo_2ej2g6qfzhbr---dca47aab201b
   9. https://medium.com/tensorflow/tagged/announcements
  10. https://medium.com/tensorflow/tagged/keras
  11. https://medium.com/tensorflow/tagged/javascript
  12. https://medium.com/tensorflow/tagged/mobile
  13. https://medium.com/tensorflow/tagged/responsible-ai
  14. https://medium.com/tensorflow/archive
  15. http://tensorflow.org/
  16. http://twitter.com/mikeshwe
  17. http://dustintran.com/
  18. https://medium.com/@tensorflow
  19. https://medium.com/tensorflow/highlights-from-tensorflow-developer-summit-2018-cd86615714b2
  20. https://www.youtube.com/watch?v=bb1_zlrjo1c
  21. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/linalg
  22. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distributions/python/ops
  23. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/distributions
  24. https://docs.scipy.org/doc/numpy-1.14.0/user/basics.broadcasting.html
  25. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distributions/python/ops/bijectors
  26. https://en.wikipedia.org/wiki/log-normal_distribution
  27. https://arxiv.org/abs/1705.07057
  28. https://arxiv.org/abs/1711.10604
  29. https://github.com/tensorflow/id203/tree/master/tensorflow_id203/python/edward2
  30. https://github.com/tensorflow/id203/tree/master/tensorflow_id203/python/layers
  31. https://github.com/tensorflow/id203/blob/master/tensorflow_id203/python/trainable_distributions.py
  32. https://github.com/tensorflow/id203/tree/master/tensorflow_id203/python/mcmc
  33. https://en.wikipedia.org/wiki/hamiltonian_monte_carlo
  34. https://github.com/tensorflow/id203/tree/master/tensorflow_id203/python/vi
  35. https://github.com/tensorflow/id203/tree/master/tensorflow_id203/python/optimizer
  36. http://www.icml-2011.org/papers/398_icmlpaper.pdf
  37. https://github.com/tensorflow/id203/blob/master/tensorflow_id203/python/monte_carlo.py
  38. https://www.tensorflow.org/programmers_guide/estimators
  39. https://cran.r-project.org/package=lme4
  40. http://edwardlib.org/
  41. https://github.com/tensorflow/id203/blob/master/tensorflow_id203/examples/jupyter_notebooks/linear_mixed_effects_models.ipynb
  42. https://github.com/tensorflow/id203/blob/master/tensorflow_id203/python/mcmc/hmc.py
  43. https://en.wikipedia.org/wiki/copula_(id203_theory)
  44. https://arxiv.org/abs/1705.07057
  45. https://github.com/tensorflow/id203/blob/master/tensorflow_id203/examples/jupyter_notebooks/gaussian_copula.ipynb
  46. https://github.com/tensorflow/id203/blob/master/tensorflow_id203/examples/jupyter_notebooks/understanding_tensorflow_distributions_shapes.ipynb
  47. https://github.com/tensorflow/id203/tree/master/tensorflow_id203/examples/vae
  48. https://en.wikipedia.org/wiki/variational_bayesian_methods
  49. https://arxiv.org/abs/1803.04386
  50. https://www.tensorflow.org/programmers_guide/eager
  51. https://github.com/tensorflow/id203
  52. https://medium.com/tag/machine-learning?source=post
  53. https://medium.com/tag/tensorflow?source=post
  54. https://medium.com/tag/id203?source=post
  55. https://medium.com/tag/tfdevsummit?source=post
  56. https://medium.com/@tensorflow
  57. https://medium.com/tensorflow?source=footer_card
  58. https://medium.com/tensorflow?source=footer_card
  59. https://medium.com/tensorflow
  60. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  62. https://medium.com/@tensorflow?source=post_header_lockup
  63. https://medium.com/p/dca4c304e245/share/twitter
  64. https://medium.com/p/dca4c304e245/share/facebook
  65. https://medium.com/@tensorflow?source=footer_card
  66. https://medium.com/p/dca4c304e245/share/twitter
  67. https://medium.com/p/dca4c304e245/share/facebook
