chapter 8 classical models of neural networks



8.1 the network of id88s

   the first successful model of neural network in the history of
   neurocomputing was the network of id88s. the architectural
   dynamics of this network specifies a fixed architecture of a
   one-layered network n-m at the beginning. this means that the network
   consists of n input neurons, each being an input for all m output
   neurons as it is depicted in figure 8.1. denote by x[1]    x[n] the real
   states of input neurons, x=(x[1]   x[n])       r^n is the network input, and
   denote by  y[1]    y[m ]the binary states of output neurons,
   y=(y[1]   y[m])    {0, l}^m is the network output. furthermore, w[ij]
   represents a real synaptic weight associated with the connection
   leading from the i^th input neuron (i=1,.., n) to the j^th output
   neuron (j=1, ... , m), and w[j0]=-h[j ]is a bias (a threshold h[j ]with
   the opposite sign) of the j^th output neuron corresponding to a formal
   unit input x[0]=1.

                               [image001.jpg]

        figure 8.1 : the architecture of the network of id88s.


   the computational dynamics of the network of id88s determines how
   the network function is computed. in this case, the real states of
   neurons in the input layer are assigned to the network input and the
   output neurons compute their binary states determining the network
   output in the same way as the formal neuron does (see eq.7.3)). this
   means that every id88 computes its excitation level first as the
   corresponding affine combination of inputs:

                   [[image002.gif] ]    where   [[image003.gif]
   ]
                                     (8.1)

   the coefficients w=(w[10],   ,w[1n],   ,w[m][0],   ,w[mn]) create the network
   configuration. then the id88 state is determined from its
   excitation level by applying an activation function

       s :    r    {0,1}
                                                                     (8.2)

   such as the hard limiter. this means, that the function

       y(w) :    r    {0,1}^m
                                                                    (8.3)

   of the network of id88s, which depends on the configuration w, is
   given by the following equation:

   [[image004.gif] ]    [[image003.gif] ] where  [[image005.gif]
   ]
                 (8.4)


8.2 a id88 as a pattern classifier


   a single id88 can be used to classify input patterns into two
   classes. this classification is characterized in terms of hyperplanes
   splitting up the input data space. a linear combination of weights for
   which the augmented activation potential (excitation level with bias
   included) is zero, describes a decision surface which partitions the
   input space into two regions. the decision surface is a hyperplane
   specified as:

   w.x=0,  x[0]=1  or    [[image006.gif]
   ]
                   (8.5)

   the weighted sum (including bias term) of a hidden unit clearly defines
   an n-1 dimensional hyperplane in n-dimensional space. these hyperplanes
   are seen as decision boundaries, many of which together can carve out
   complex regions necessary for classification of complex data.

                               [image007.gif]

        figure 8.2 : geometrical interpretation of a neuron function.


   the geometrical interpretation depicted in figure 8.2 may help its to
   better understand the function of a single neuron. the n input values
   of a neuron may be interpreted as coordinates of a point in the
   n-dimensional euclidean input space. in this space, the equation of a
   hyperplane has the following form:

   [[image008.gif]
   ]

   (8.6)

   this hyperplane disjoins the input space into two halfspaces. the
   coordinates [[image009.gif] ]of points that are situated in one
   halfspace, meet the following inequality:

   [[image010.gif]
   ]

   (8.7)

   the points [[image011.gif] ]from the second, remaining halfspace
   fulfill the dual inequality with the opposite relation symbol:

   [[image012.gif]
   ]

   (8.8)

   hence, the synaptic weights w[0],...,w[1] (including bias) of a neuron
   may be understood as coefficients of this hyperplane. clearly, such a
   neuron classifies the points in the input space (the coordinates of
   these points represent the neuron inputs) to which from two halfspaces
   determined by the hyperplane they belong, the neuron realizes the
   dichotomy of input space. more exactly, the neuron is active (the
   neuron state is y=1) if its inputs meet eq. 8.7 or eq. 8.8, they
   represent the coordinates of a point that lies in the first halfspace
   or on the hyperplane. in the case that this point is located in the
   second halfspace, the inputs of neuron fulfill the eq. 8.9 and the
   neuron is passive (the neuron state is y=0). the input patterns that
   can be classified by a single id88 into two distinct classes are
   called linearly separable patterns.


8.3 vectors

   in order to generalise to higher dimensions, and to relate vectors to
   the ideas of pattern space, it is convenient to describe vectors with
   respect to a cartesian coordinate system. that is, we give the
   projected lengths of the vector onto two perpendicular axes (see figure
   8.3).

                               [image013.jpg]

               figure 8.3 : vector depicted in cartesian axes.


   the vector is now described by the pair of numbers (v[1], v[2]). these
   numbers are its components in the chosen coordinate system. since they
   completely determine the vector, we may think of the vector itself as a
   pair of component values and write v=(v[1], v[2]). the vector is now an
   ordered list of numbers. notice the ordering is important, since (1,3)
   is in a different direction from (3,1). this definition immediately
   generalises to more than 2-dimensions. an n-dimensional vector is
   simply an ordered list of n numbers, v=(v[1], v[2],...v[n]). the weight
   vector w=(w[1], w[2],... , w[n]) and the input vector x=(x[1],
   x[2]   x[n]) are depicted similarly.

8.3.1  the length of a vector

   for our 2-d prototype, the length of a vector is just its length in the
   plane. in terms of its components, this is given by pythagoras   s
   theorem.


               [[image014.gif]
   ]
                                          (8.9)


   in n-dimensions, the length is defined by the natural extension of
   eq.8.9


               [[image015.gif]
   ]
                                             (8.10)



8.3.2 comparing vectors - the inner product


   consider the vectors v=(1, 1) and w=(0, 2) shown below:

                               [image016.gif]

                      figure 8.4 : two vectors at 45^o.


   the angle between them is 45^o. define the inner product v.w of the two
   vectors by

               [[image017.gif]
   ]
                                     (8.11)

   the form on the right-hand side should be familiar; it is just the same
   as that, we have used to define the activation of a id88.
   substituting the component values gives v.w=2. we will now try to give
   this a geometric interpretation.

   first we note that [[image018.gif] ]and [[image019.gif] ]. next, we
   observe that the cosine of the angle between the vectors is
   [[image020.gif] ].

                               [image021.gif]

                       figure 8.5 : diagram of cos 45.


   therefore[ [image022.gif] ]. it may be shown that this is a general
   result; that is, if the angle between two vectors v and w is
   [[image023.gif] ] then the definition of dot product given in eq.8.11
   is equivalent to

               [[image024.gif]
   ]
                                      (8.12)

   in n-dimensions the inner product is defined by the natural extension
   of eq. 8.11

               [[image025.gif]
   ]
                                           (8.13)

   although, we cannot now draw the vectors (for n > 3) the geometric
   insight we obtained for 2-d may be carried over since the behaviour
   must be the same (the definition is the same). we therefore now examine
   the significance of the inner product in two dimensions. essentially
   the inner product tells us how well    aligned    two vectors are. to see
   this, let v[w] be the component of v along the direction of w, or the
   projection of v along w.


                               [image026.gif]

                       figure 8.6 : vector projection.


   the projection is given by [[image027.gif] ] which, by eq. 8.12 gives
   us [[image028.gif] ]


   if [[image023.gif] ] is small then [[image029.gif] ] is close to its
   maximal value of one. the inner product, and hence the projection v[w],
   will be close to its maximal value; the vectors    line up    quite well.
   if [[image023.gif] ]=90^  , the cosine term is zero and so too is the
   inner product. the projection is zero because the vectors are at right
   angles; they are said to be orthogonal. if 90 < [[image023.gif] ] <
   270, the cosine is negative and so too, therefore, is the projection;
   its magnitude may be large but the negative sign indicates that the two
   vectors point into opposite half-planes.


8.3.3 inner products and id88s

   using the ideas developed above we may express the action of a
   id88 in terms of the weight and input vectors. the activation
   potential may now be expressed as

               [[image030.gif]
   ]
                                                     (8.14)

   the vector equivalent to [[image006.gif] ]now becomes

               [[image031.gif]
   ]
                                                (8.15)

   if w and w[0]x[0] are constant then this implies the projection x[w],
   of x along w is constant since

               [[image032.gif]
   ]
                                                (8.16)

   in 2-d, therefore, the equation specifies all x which lie on a line
   perpendicular to w.


                               [image033.gif]

                         figure 8.7 : w.x= constant


   there are now two cases

   1.   if [[image034.gif] ], then x must lie beyond the dotted line

                               [image035.gif]

                         figure 8.8 : w.x > w[0]x[0]


   using [[image028.gif] ] we have that [[image034.gif] ]implies
   [[image036.gif] ] that is, the activation potential is greater than the
   threshold


   2.      conversely, if [[image037.gif] ]then x must lie below the
   dotted line

                   [image038.gif]     or    [image039.gif]

                         figure 8.9 : w.x < w[0]x[0]


   using [[image028.gif] ] we have that [[image040.gif] ]implies
   [[image041.gif] ]; that is, the activation potential is less than the
   threshold

   in 3-d the line becomes a plane intersecting the cube and in n-d a
   hyperplane intersecting the n-dimensional hypercube or n-cube.

   note that the weight vector, w, is orthogonal to the decision plane in
   the augmented input space because

       w.x =
   0
                                                       (8.17)

   that is inner product of the weight and the augmented input vectors is
   zero. note that the weight vector is also orthogonal to the decision
   plane

             w.x + w[0] x[0 ]= 0

   (8.18)

   in the proper, (n-1) dimensional input space, where w=(w[1], w[2],... ,
   w[n]) and x=(x[1], x[2]   x[n]).


8.4 selection of weights for the id88

   in order for the id88 to perform a desired task, its weights must
   be properly selected. in general, two basic methods can be employed to
   select a suitable weight vector.

             by off-line calculation of weights: if the problem is
   relatively simple, it is often possible to calculate the weight vector
   from the specification of the problem.

             by learning procedure: the weight vector is determined from a
   given (training) set of input-output vectors (exemplars) in such a way
   to achieve the best classification of the training vectors.

   once the weights are selected, the id88 performs its desired
   task.


8.4.1 selection of weights by off-line calculations

   consider the problem of building a and gate using the id88. in
   this case, the desired input-output relation is specified by the truth
   table and related plot given in figure 8.10.

                               [image042.jpg]

      figure 8.10 : the truth table of and gate and its implementation.

   the input patterns (vectors) belong to two classes and are marked in
   the input space. the decision plane is the straight line described by
   te following equation:

             x[1 ]+ x[2 ]-1.5=0

   (8.19)

   in order for the weight vector to point in the direction of y=1
   patterns we select it as w=[1 1 -1.5]  for each input vector  the
   output signal is now calculated as

             [[image043.gif] ] ; [[image044.gif]
   ]
        (8.20)

   the single id88 network is depicted in figure 8.11.

                               [image045.gif]

            figure 8.11 : single id88 network of and gate.


8.4.2 the id88 learning law

   learning is a recursive procedure of modifying weights from a given set
   of input-output patterns. for a single id88, the objective of the
   learning procedure is to find a decision plane, which separates two
   classes of given input-output training vectors. once the learning is
   finalised, every input vector will be classified into an appropriate
   class. a single id88 can classify only the linearly separable
   patterns. the id88 learning procedure is an example of a
   supervised error-correcting learning law.

   in an adaptive mode the desired function of the network of id88s
   is given by a training set:

             [[image046.gif]
   ]                                                            (8.21)

   where x[k] is a real input of the k^th training pattern, and d[k] is
   the corresponding desired binary output (given by a teacher). the aim
   of adaptation is to find a configuration w such that for every input
   x[k] (k=1,...,p) from training set t, the network responds with the
   desired output d[k] in computational mode, it holds:

               y(w, x[k]) = d[k]     k=1,...,p , and
   w=(w[10],   ,w[1n],   ,w[m][0],   ,w[mn])                          (8.22)

   we can assume that an untrained id88 can generate an incorrect
   output y  d[k ]due to incorrect values of weights. we can think about
   the actual output y as an estimate of the correct, desired output d[k].

   the id88 learning law also known as the id88 convergence
   procedure can be described as follows.

   1. at the beginning of adaptation, at time 0, the weights of
   configuration w^(0) are initialized randomly close to zero,

   [[image047.gif] ], where  [[image048.gif] ], and  ( j=1,...,m,
   i=0,...,n ).                                        (8.23)

   2. the network of id88s has discrete adaptive dynamics. at each
   adaptation time step t=1, 2, 3,... one pattern from the training set is
   presented to the network which attempts to learn it. the actual output
   y[j], is compared with the desired output, d[kj], and the error e[kj]
   is calculated as

   [[image049.gif] ]
                                                              (8.24)

   [[image050.gif] ],       or         [[image051.gif]
   ]                                                                (8.25)

   note that

   [[image052.gif] ], then  [[image053.gif] ]
                                         (8.26)

   3. at each step, the weights are adapted as follows

   [[image054.gif]
   ]
                                       (8.27)

   where 0<[ [image055.gif] ]<1 is the learning rate (gain), which
   controls the adaptation rate. since we do not want to make too drastic
   a change as this might upset previous learning, we add a fraction to
   weight vector to produce a new weight vector. the gain must be adjusted
   to ensure the convergence of the algorithm. the total expression can be
   rewritten as

   [[image056.gif]
   ]
                 (8.28)

   the expression [[image057.gif] ] in eq.8.28 is the discrepancy between
   the actual j^th network output for the k^th pattern input and the
   corresponding desired output of this pattern. hence, this determines
   the error of the j^th network output with respect to the k^th training
   pattern. clearly, if this error is zero, the underlying weights are not
   modified. otherwise, this error may be either 1 or -1 since only the
   binary outputs are considered. the inventor of the id88,
   rosenblatt showed that the adaptive dynamics eq.8.28 guarantees that
   the network finds its configuration (providing that it exists) in the
   adaptive mode after a finite number of steps, for which it classifies
   all training patterns correctly (the network error with respect to the
   training set is zero), and thus the condition eq.8.22 is met.

   the order of patterns during the learning process is prescribed by the
   so-called training strattegy and it can be, for example, arranged on
   the analogy of human learning. one student reads through a textbook
   several times to prepare for an examination, another one learns
   everything properly during the first reading, and as the case may be,
   at the end both revise the parts which are not correctly answered by
   them. usually, the adaptation of the network of id88s is
   performed in the so-called training epoch in which all patterns from
   the training set are systematically presented (some of them even
   several times over). for example, at time      t = (c-l) p + k (where 1
      k    p) corresponding to the c^th training epoch the network learns
   the k^th training pattern.

   considering the network of id88s is capable of computing only a
   restricted class of functions, the significance of this model is rather
   theoretical. in addition, the above-outlined convergence theorem for
   the adaptive mode does not guarantee the learning efficiency, which has
   been confirmed by time-consuming experiments. the generalization
   capability of this model is also not revolutionary because the network
   of id88s can only be used in cases when the classified objects
   are separable by hyperplanes in the input space (special tasks in
   pattern recognition, etc.). however, this simple model is a basis of
   more complex paradigms like a general feedforward network with the
   id26-learning algorithm.

   in a modified version of the id88 learning rule, in addition to
   misclassification, the input vector x must be located far enough from
   the decision surface, or, alternatively, the net activation potential
   (augmented) [[image058.gif] ]must exceed a preset margin,
   [[image059.gif] ]. the reason for adding such a condition is to avoid
   an erroneous classification decision in a situation when
   [[image058.gif] ]is very small, and due to presence of noise, the sign
   of [[image058.gif] ]cannot be reliably established.


                      [image060.gif]     [image061.jpg]

       figure 8.12 :  tabular and graphical illustration of id88
                               learning rule.



8.5 example

   using the id88 training algorithm, we may use a id88 to
   classify two linearly separable classes a and b. examples from these
   classes may have been obtained, for example, by capturing images in a
   framestore; there may have been two classes of faces, or we want to
   separate handwritten characters into numerals and letters.


                               [image062.gif]

                     figure 8.13 : a / b classification.


   the network can now be used for calssification task: it can decide
   whether an input pattern belongs to one of two classes. if the total
   input is positive, the pattern will be assigned to class +1, if the
   total input is negative, the sample will be assigned to class    1. the
   separation between the two classes in this case is a straight line
   given by the equation:

   [[image063.gif]
   ]
                                        (8.29)

   where q denotes threshold. the single layer network represents a linear
   discriminant function. a geometrical representation of the linear
   threshold neural network is given in eq.8.29 can be written as

   [[image064.gif]
   ]
                                           (8.30)

   and we see that the weights determine the slope of the line and the
   bias determines the offset, i.e. how far the line is from the origin.
   note that also weights can be plotted in the input space: the weight
   vector is always perpendicular to the discriminant function.


                               [image065.gif]

    figure 8.14 :  geometric representation of the discriminant function
                              and the weights.


   suppose now there is four classes a, b, c, d, and that they are
   separable by two planes in pattern space.

                               [image066.jpg]

              figure 8.15 :  pattern space for a, b, c, and d.


   that is the two classes (a, b) (c, d) are linearly separable, as too
   are the classes (a, d) and (b, c). we may now train two units to
   perform these two classifications. figure 8.16 gives a table encoding
   the original 4 classes.


                               [image067.jpg]

               figure 8.16 : input coding for a, b, c, and d.


   one neuron can solve only very simple tasks. the exclusive disjunction
   xor is a typical example of a logical function that cannot be
   implemented by one neuron. for instance, consider only two binary
   inputs (whose values are taken from the set {0, 1}) and one binary
   output whose value is 1 if and only if the value of exactly one input
   is 1 (i.e. xor(0, 0) 0, xor(1, 0) 1, xor(0, 1) 1, xor(1, 1) 0).

                               [image068.jpg]

          figure 8.17 : geometrical representation of xor function.


   it follows from figure 8.17 where all possible inputs are depicted in
   the input space and labeled with the corresponding outputs that there
   is no straight line to separate the points corresponding to output
   value 1 from the points associated with output value 0. this implies
   that the neurons need to be connected into a network in order to solve
   more complicated tasks, as it is the case of the human nervous system.

   the above-mentioned principles will be illustrated through an example
   of a neural network whose computational dynamics will be described in
   more detail and also its geometrical interpretation will be outlined. a
   fixed architecture of a multilayered neural network is considered.
   further, suppose that each connection in this network is labeled with
   synaptic weights, i.e. a configuration of the multilayered network is
   given. at the beginning, the states of input layer neurons are assigned
   to a generally real network input and the remaining (hidden and output)
   neurons are passive. the computation further proceeds at discrete time
   steps. at time 1, the states of neurons from the first (hidden) layer
   are updated according to eq.7.3. this means that a neuron from this
   layer collects its inputs from the input neurons, computes its
   excitation level as a weighted sum of these inputs and its state
   (output) determines from the sign of this sum by applying the transfer
   function (hard limiter). then at time 2, the states of neurons from the
   second (hidden) layer are updated again according to eq. 7.3. in this
   case, however, the outputs of neurons from the first layer represent
   the inputs for neurons in the second layer. similarly, at time 3 the
   states of neurons in the third layer are updated, etc. thus, the
   computation proceeds in the direction from the input layer to the
   output one so that at each time step all neurons from the respective
   layer are updated in parallel based on inputs collected from the
   preceding layer. finally, the states of neurons in the output layer are
   determined which form the network output, and the computation of
   multilayered neural network terminates.

   the geometrical interpretation of neuron function will be generalized
   for the function of three-layered neural network. clearly, the neurons
   in the first (hidden) layer disjoin the network input space by
   corresponding hyperplanes into various halfspaces. hence, the number of
   these halfspaces equals the number of neurons in the first layer. then,
   the neurons in the second layer can, classify the intersections of some
   of these halfspaces, i.e. they can represent convex regions in the
   input space. this means that a neuron from the second layer is active
   if and only if the network input corresponds to a point in the input
   space that is located simultaneously in all halfspaces, which are
   classified by selected neurons from the first layer. although the
   remaining neurons from the first layer are formally connected to this
   neuron in the topology of multilayered network, however, the
   corresponding weights are zero and hence, they do not influence the
   underlying neuron. in this case, the neurons in the second layer
   realize logical conjunction and of relevant inputs; i.e. each of them
   is active if and only if all its inputs are 1. in figure 8.18, the
   partition of input space into four halfspaces p[1], p[2], p[3], p[4 ]by
   four neurons from the first layer is depicted (compare with the example
   of multilayered architecture 3-4-3-2 in figure 7.9. three convex
   regions are marked here that are the intersections of halfspaces
   k[1]=p[1]   p[2]   p[3], k[2]=p[2]   p[3]   p[4], and k[3]=p[1]   p[4]. they
   correspond to three neurons from the second layer, each of them being
   active if and only if the network input is from the respective region.

   the partition of the input space into convex regions can be exploited
   for the pattern recognition of more characters where each character is
   associated with one convex region. sometimes the part of the input
   space corresponding to a particular image cannot be closed into a
   convex region. however, a non-convex area can be obtained by a union of
   convex regions, which is implemented by a neuron from the third
   (output) layer. this means that the output neuron is active if and only
   if the network input represents a point in the input space that is
   located in at least one of the selected convex regions, which are
   classified by neurons from the second layer. in this case, the neurons
   in the output layer realize logical disjunction (or) of relevant
   inputs; i.e. each of them is active if and only if at least one of its
   inputs is 1. for example, in figure 8.18 the output neuron is active if
   and only if the network input belongs to the region k[1 ]or k[2 ](i.e.
   k[1 ]u k[2]). it is clear that the non-convex regions can generally be
   classified by three-layered neural networks.


                               [image069.gif]

             figure 8.18 : example of convex region separation.


   in the preceding considerations the fact that the logical conjunction
   and and disjunction or, respectively, are computable by one neuron with
   the computational dynamics (eq. 7.3), has been exploited. in figure
   8.19, the neuron implementation of these functions is depicted. the
   unit weights (excluding bias) ensure the weighted sum of actual binary
   inputs (taken from the set {0,1}) to equal the number of 1   s in the
   input. the threshold (the bias with the opposite sign) n for and and 1
   for or function causes the neuron to be active if and only if this
   number is at least n or 1, respectively. of course, the neurons in the
   upper layers of a multilayered network may generally compute other
   functions than only a logical conjunction or disjunction. therefore,
   the class of functions computable by multilayered neural networks is
   richer than it has been assumed in our example.

                               [image070.jpg]

             figure 8.19 : neuron implementation of and and or.

   in addition, the geometrical interpretation of the multilayered neural
   network will be illustrated by the above-mentioned important example of
   logical function, the exclusive disjunction (xor). this function is not
   computable by one neuron with the computational dynamics (eq.7.3).

                               [image071.jpg]

       figure 8.20 : geometrical interpretation of  xor computation by
                            two-layered network.


   as it can be seen in figure 8.20, the (two-dimensional) network inputs
   for which the output value of xor function is 1, can be closed by the
   intersection of two halfspaces (half-planes) p[1], p[2 ]bounded by
   hyperplanes (straight lines), into a convex region. therefore, the xor
   function can be implemented by the two-layered neural network 2-2-1
   (with one hidden layer), which is depicted in figure 8.21.


                               [image072.jpg]

           figure 8.21 : two layered network for  xor computation.



   the equation  of a line whose coordinates of two points are known is
   given as;

         [[image073.gif]
   ]
                                           (8.31)

   thus can be solved as;

         [[image074.gif]
   ]
           (8.32)

   in the above example the two coordinates of the first hyperplane can be
   found as [[image075.gif] ] and [[image076.gif] ]. using these values in
   eq.8.7 will yield;

         [[image077.gif]
   ]
                                                (8.33)

   which is the equation of the first hyperplane. the equation of the
   other hyperplane is treated similary which is

   [        [image078.gif]
   ]
                                                (8.34)

   note that any point below eq.8.31 yields the result of y=0 since
   [[image079.gif] ] (see figure 8.22 a). to give way to the network
   implementation, this result should be avoided. thus the equation is
   multiplied by    1 so that any point below the hyperplane p[2] yields y=1
   (see figure 8.22 b).


                [image080.jpg]                 [image081.jpg]

                                     (a)
                                     (b)

      figure 8.22 : the effect of the multiplication of the equation of
                             hyperplane by    1.


   also note that the following correlations are true;

   [[image082.gif] ]    [[image008.gif] ]

   [[image083.gif] ]    [[image012.gif] ]

   [[image084.gif] ]    [[image010.gif] ]


                               [image085.gif]

      figure 8.23 : block diagram of xor gate implemented with matlab.




references


   [1]        sima j. (1998). introduction to neural networks, technical
   report no. v 755, institute of computer science, academy of sciences
   of  the  czech republic

   [2]        kr  se b., and van der smagt p. (1996). an introduction to
   neural networks. (8^th ed.) university of amsterdam press, university
   of amsterdam.

   [3]        gurney k. (1997). an introduction to neural networks. (1^st
   ed.) ucl press, london ec4a 3de, uk.

   [4]        paplinski a.p. neural nets. lecture notes, dept. of computer
   sciences and software eng., manash universtiy, clayton-australia
