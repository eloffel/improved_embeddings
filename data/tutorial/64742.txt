   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]acta schola automata polonica
     * [9]about
     __________________________________________________________________

visualizing id137. part i.

australian sign language model visualization.

   go to the profile of piotr tempczyk
   [10]piotr tempczyk (button) blockedunblock (button) followfollowing
   mar 17, 2018

   written by [11]piotr tempczyk and [12]miros  aw barto  d.
   [1*2fsogfhcox1ofb_3vi-y8g.jpeg]

abstract

   long-short term memory networks are state-of-the-art tools for long
   sequence modeling. however, there is a problem with understanding what
   they have learned and investigating why they are making particular
   mistakes. many articles and papers do it for convolutional neural
   networks, but for lstm we do not have many tools to look inside and
   debug them.

   in this article we try to partially fill this gap. we visualize lstm
   network activations from australian sign language (auslan) sign
   classifying model. we do this by training a denoising autoencoder on
   lstm layer activations. we use dense autoencoders to project
   100-dimensional vector of lstm activations to 2- and 3-dimensions.
   thanks to that we can explore activations space visually to some
   extent. we analyze this low dimensional space and try to find out how
   this id84 can be helpful for finding relations
   between examples in the dataset.

auslan sign classifier

   this article is an extension of miroslav bartold   s engineering thesis
   (barto  d, 2017). the dataset used in this thesis comes from (kadous,
   2002). the dataset consist of 95 auslan signs, captured using a glove
   with high-quality position trackers. however, because there was a
   problem with data files for one of the signs, we were left with 94
   classes. each sign was performed 27 times by a native signer, and each
   time step was encoded using 22 numbers (11 per hand). the longest
   sequence in the dataset had a length of 137, but because long sequences
   were rare, we kept those with length up to 90, and padded shorter ones
   with zeros at the beginning. the dataset and its detailed description
   can be found [13]here.

   in his thesis, miroslav tested several classifiers, all based on the
   lstm architecture. classification accuracy was around 96%. for people
   unfamiliar with the subject, there is a very good explanation of lstm
   networks on [14]christopher olah   s blog.

   in this research we focused on a single architecture with one hidden
   layer of 100 lstm units. last classifying layer had 94 neurons. the
   input were 22-dimensional sequences of 90 time steps. we used the keras
   functional api, and the networks architecture is presented in figure 1.
   [1*14b6s4gi1quckz2k-2hu6a.png]
   figure 1. model architecture

   the lambda element shown in figure 1 was used to extract the last
   activation from a full sequence of activations (since we passed
   return_sequences=true to the lstm). for implementation details we refer
   the reader to our [15]repository.

first attempt to understand the internals of the lstm network

   inspired by [16](karpathy, 2015) we wanted to localize some neurons
   responsible for sub-gestures easily recognizable by humans (and shared
   between different signs), like making a fist or drawing a circle with a
   hand. this approach has failed, because of five main reasons:
     * the signal from position trackers is insufficient to fully
       reconstruct the motion of hands,
     * the representation of gesture is very different in the space of
       trackers and in reality,
     * we have only videos of gestures from [17]http://www.auslan.org.au,
       and not videos of the actual executions of the signs in the
       dataset,
     * words in the dataset and on the videos on
       [18]http://www.auslan.org.au can origin from different dialects. so
       it could be similar to comparing words like    underground    and
          subway   ,
     * 100 neurons and 94 signs is a very large space to comprehend by a
       person.

   therefore, we focused only on visualization techniques in hope that
   they will help us to reveal some mysteries of the lstm cell and the
   dataset.

denoising autoencoders

   in order to visualize lstm output activation sequences for all gestures
   we will try to project 100-dimensional vectors representing activations
   at each time step to 2- or 3-dimensional vectors using denoising
   autoencoders. our autoencoders are composed of 5 fully connected
   layers, with the 3rd layer as a bottleneck with a linear activation
   function. if you are unfamiliar with the topic you can read more about
   autoencoders [19]here.

   the linear activation function turned out to be the best activation for
   the purpose of legible plots. for all tested id180 all
   example paths (the term will be explained in next section) start near
   the (0,0) point on the plot. for non-antisymmetric functions ([20]relu
   and [21]sigmoid) all example paths were in the upper right quarter of
   the coordinate system. for antisymmetric functions, like [22]tanh and
   linear (identity function), all paths were more or less evenly
   distributed in all quarters. however the [23]tanh function squashed
   some paths near -1 and 1 (which made the plot too fuzzy), whereas the
   linear function did not. if you are interested in visualizations for
   other id180 you can find the code in the [24]repository.

   in figure 2 we present the architecture of the 2d autoencoder. the 3d
   autoencoder was almost identical except it had 3 neurons in the 3rd
   dense layer.

   autoencoders were trained on vectors of lstm cell output activations
   for all single time steps of each gesture realization. these vectors of
   activation were shuffled and some redundant activation vectors were
   removed. by a redundant activation vector we mean those from the
   beginning and at the end of each gesture, where the activations
   remained approximately constant.
   [1*i0hu8jvhnlw6yk11zphfqq.png]
   figure 2. autoencoder architecture

   noise in the autoencoders was added to the input vectors and it was
   draw from a normal distribution with mean 0 and standard deviation 0.1.
   the network was trained with the adam optimizer and the mean-squared
   error was minimized.

visualization

   by feeding a sequence of lstm cell activations corresponding to a
   single gesture to the autoencoder we obtain the activations on the
   bottleneck. we refer to this lower-dimensional bottleneck activations
   sequence as an example path.

   near the last step of some examples we present the name of the sign it
   represents.

   in figure 3 we present example paths visualization for training set.
   [1*rna3bpfjlaro1t0qqifkma.png]
   figure 3. lstm activations visualization

   each point in the visualization represents each 2d activation from the
   autoencoder for a single time step and for one example. the color scale
   represents the time step (from 0 to 90) in each sign execution and
   black lines are connecting points from a single example path. each
   point before visualization was transformed by the function lambda x:
   numpy.sign(x) * numpy.log1p(numpy.abs(x)). this transformation allowed
   us to look more closely at the beginning of each path.

   in figure 4 we present activations for the last steps for each training
   example. this is the 2d projection of input to the classification
   layer.
   [1*xqd41pmecnq3dwav3k3s1w.png]
   figure 4. last activations from lstm layer

   it is quite surprising that all paths look very smooth and are
   localized in their parts of space because all activations for each time
   step and example were shuffled before training the autoencoder. spatial
   structure from figure 4 explains why our last classifying layer reaches
   good accuracy on such a small training set (near 2000 examples).

   for those who are interested in exploring this 2d space, we have
   rendered a much bigger version of figure 2 [25]here.

   in figure 5 we present lstm activations visualization in 3d. for the
   sake of clarity we presented only points. for data analysis purposes we
   focus only on 2d visualizations in the second part of this article.
   [1*s0txjfyqp3cqocrfeli3xa.png]
   figure 5. 3d visualization of lstm activations

analysis

   visualizations look really nice, but is there something more meaningful
   in this? does the closeness of some paths mean, that these signs are
   more similar?

   let us take a look at this space when we take into account partition on
   right-handed and both-handed signs (we did not observe only left-handed
   signs). this partition was made based on statistics of variability on
   each hand tracker signals. more details in our [26]repository.

   for the sake of clarity we plot in figure 6 only paths without points.
   right-handed signs are marked in cyan, and both-handed in magenta. we
   can see clearly, that both types of signs occupy other parts of the
   space and are not mixing with each other very often.
   [1*rf_a7sneeatrwltpjhykwq.png]
   figure 6. activation paths categorized by hand usage

   now let us take a look at the pair [27]drink-[28]danger (names of signs
   are linking to films on [29]auslan signbank). these are the two cyan
   gestures occupying the middle-right mostly magenta part of figure 6. in
   our data these two gestures are one-handed, but on the film from auslan
   signbank [30]danger is obviously two-handed.

   this might be caused by mislabeling. notice that the word [31]dangerous
   is indeed one-handed, and also similar to [32]drink (at least in his
   first part of the gesture). we therefore concluded that the label
   danger should actually be dangerous. we plot these two gestures in
   figure 7.
   [1*hirbiaa2eugayrdsbqleag.png]
   figure 7. lstm activations for drink and danger

   [33]who and [34]soon signs seem similar in figure 8. the glove has only
   one bend tracker and the finger bend measurements are not very exact
   (as written in [35]data description). this is why these two gestures
   can look more similar in figure 8 than on the videos.
   [1*tm9nemfyj1bvdoh22aakrq.png]
   figure 8. lstm activations for who and soon

   [36]crazy and [37]think sign example paths occupy the same space in
   figure 9. however [38]think seems to be a main part of slightly longer
   [39]crazy gesture. when we look at videos on auslan signbank, we see
   that this relation is true and the [40]crazy sign looks like [41]think
   sign plus palm spread.
   [1*6jlv79elx_mpvgwxa7dkkw.png]
   figure 9. lstm activations for think and crazy

   although when we look at [42]you sign in figure 10 we can see that this
   sign goes perpendicular to other gestures like [43]crazy, [44]think,
   [45]sorry (and many other not shown here). when we look on videos on
   signbank, we cant see anything similar between these signs and [46]you.
   [1*mvmapocatxxy5rvofxnqdq.png]
   figure 10. lstm activations for think, crazy, sorry and you

   we have to remember that each lstm cell state has its own memory of the
   past, it is fed by the input sequence at each time step and there could
   be a difference in time moments when paths occupy the same space.
   therefore there are more variables that determine the shape of the path
   than we take into account in this analysis. this is probably the reason
   why we can observe crossings of some example paths without observing
   any visual similarity between them.

   some of the close relations suggested by this visualization turn out to
   be false. some of them are changing between retraining of the
   autoencoder or after retraining the lstm model. some of them do not,
   and they are more likely to represent real similarities. for example
   [47]god and [48]science sometimes share similar paths in 2d space and
   sometimes are far away from each other.

misclassified examples

   at the end let us look at the misclassified examples. in figures 11, 12
   and 13 we visualized them for the training, validation and test set
   respectively. blue label above misclassified example is a true class.
   below there is a label chosen by model, marked in red.

   for training examples there are only three misclassified examples and
   two of them ([49]hurt-[50]all and [51]thank-[52]hot) are very close in
   2d space. [53]thank-[54]hot are also similar on the videos.
   [55]hurt-[56]all are not.
   [1*za1-n1mu0snjzq3z9_dxvw.png]
   figure 11. misclassified examples for training set

   as expected, in validation and test set there are more misclassified
   examples, but the mistakes are made more often for gestures that are
   near in the projected space.
   [1*f1kd_hktbpvgyartgsav2q.png]
   figure 12. misclassified examples for validation set
   [1*csqognngvvp2fsgotx28fq.png]
   figure 13. misclassified examples for test set

prediction visualization

   at the end we generated a film with the visualization of activations
   development during prediction.

   iframe: [57]/media/ca4f48811cc80bf604e5a7147e7b16f2?postid=f1d3fa6aace7

   figure 14. lstm prediction visualization

conclusions

   we projected 100-dimensional vectors of activations to a
   low-dimensional space. the projection looks interesting and seems to
   preserve many, but not all, relations between signs. these relations
   seem to be similar to the relations we perceive while watching the
   gestures in real life, but without the actual videos matching the
   recorded gestures, we could not determine this beyond any doubt.

   these tools can be used to some extent to look into lstm representation
   structure and can be a better tool for finding relations between
   examples than using a raw input.
   [1*fryikusnm4m7ifmw_9ig_w.png]

references

     * kadous, m. w.,    temporal classification: extending the
       classification paradigm to multivariate time series   , phd thesis
       (draft), school of computer science and engineering, university of
       new south wales, 2002
     * karpathy, a., johnson, j. and fei-fei, l.,    visualizing and
       understanding recurrent networks   , arxiv preprint arxiv:1506.02078,
       2015
     * barto  d, m.,    wykorzystanie sieci lstm do rozpoznania znak  w j  zyka
       migowego   , engineering thesis, polish-japanese institute of
       information technology, 2017

   [1*fryikusnm4m7ifmw_9ig_w.png]

   if you enjoyed this post, please hit the clap button below and follow
   our publication for more interesting articles about ml & ai.

   thanks to [58]maciek dziubi  ski, [59]maciej   liwowski, and [60]bartosz
   topolski.
     * [61]lstm
     * [62]machine learning
     * [63]artificial intelligence
     * [64]recurrent neural network
     * [65]data visualization

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of piotr tempczyk

[66]piotr tempczyk

   medium member since jan 2018

   ai and ml researcher

     (button) follow
   [67]acta schola automata polonica

[68]acta schola automata polonica

   scientific blog about artificial intelligence and machine learning

     * (button)
       (button) 1k
     * (button)
     *
     *

   [69]acta schola automata polonica
   never miss a story from acta schola automata polonica, when you sign up
   for medium. [70]learn more
   never miss a story from acta schola automata polonica
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f1d3fa6aace7
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/asap-report/visualizing-lstm-networks-part-i-f1d3fa6aace7&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/asap-report/visualizing-lstm-networks-part-i-f1d3fa6aace7&source=--------------------------nav_reg&operation=register
   8. https://medium.com/asap-report?source=logo-lo_kzkieyz3rgih---726ba2a3e48
   9. https://medium.com/asap-report/about
  10. https://medium.com/@piotr.tempczyk
  11. https://medium.com/@piotr.tempczyk
  12. https://medium.com/@bartoldmir
  13. https://archive.ics.uci.edu/ml/datasets/australian+sign+language+signs+(high+quality)
  14. http://colah.github.io/posts/2015-08-understanding-lstms/
  15. https://github.com/asap-report/lstm-visualisation
  16. https://arxiv.org/abs/1506.02078
  17. http://www.auslan.org.au/
  18. http://www.auslan.org.au/
  19. http://ufldl.stanford.edu/tutorial/unsupervised/autoencoders/
  20. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  21. https://en.wikipedia.org/wiki/sigmoid_function
  22. https://en.wikipedia.org/wiki/hyperbolic_function#hyperbolic_tangent
  23. https://en.wikipedia.org/wiki/hyperbolic_function#hyperbolic_tangent
  24. https://github.com/asap-report/lstm-visualisation
  25. https://image.ibb.co/fk867c/lstm2d_big.png
  26. https://github.com/asap-report/lstm-visualisation
  27. http://www.auslan.org.au/dictionary/words/drink-1.html
  28. http://www.auslan.org.au/dictionary/words/danger-1.html
  29. http://www.auslan.org.au/
  30. http://www.auslan.org.au/dictionary/words/danger-1.html
  31. http://www.auslan.org.au/dictionary/words/dangerous-1.html
  32. http://www.auslan.org.au/dictionary/words/drink-1.html
  33. http://www.auslan.org.au/dictionary/words/who-1.html
  34. http://www.auslan.org.au/dictionary/words/soon-1.html
  35. https://archive.ics.uci.edu/ml/datasets/australian+sign+language+signs+(high+quality)
  36. http://www.auslan.org.au/dictionary/words/crazy-1.html
  37. http://www.auslan.org.au/dictionary/words/think-1.html
  38. http://www.auslan.org.au/dictionary/words/think-1.html
  39. http://www.auslan.org.au/dictionary/words/crazy-1.html
  40. http://www.auslan.org.au/dictionary/words/crazy-1.html
  41. http://www.auslan.org.au/dictionary/words/think-1.html
  42. http://www.auslan.org.au/dictionary/words/you-1.html
  43. http://www.auslan.org.au/dictionary/words/crazy-1.html
  44. http://www.auslan.org.au/dictionary/words/think-1.html
  45. http://www.auslan.org.au/dictionary/words/sorry-1.html
  46. http://www.auslan.org.au/dictionary/words/you-1.html
  47. http://www.auslan.org.au/dictionary/words/god-1.html
  48. http://www.auslan.org.au/dictionary/words/science-1.html
  49. http://www.auslan.org.au/dictionary/words/hurt-1.html
  50. http://www.auslan.org.au/dictionary/words/all-1.html
  51. http://www.auslan.org.au/dictionary/words/thank-1.html
  52. http://www.auslan.org.au/dictionary/words/hot-1.html
  53. http://www.auslan.org.au/dictionary/words/thank-1.html
  54. http://www.auslan.org.au/dictionary/words/hot-1.html
  55. http://www.auslan.org.au/dictionary/words/hurt-1.html
  56. http://www.auslan.org.au/dictionary/words/all-1.html
  57. https://medium.com/media/ca4f48811cc80bf604e5a7147e7b16f2?postid=f1d3fa6aace7
  58. https://medium.com/@ponadto?source=post_page
  59. https://medium.com/@sliwy?source=post_page
  60. https://medium.com/@bartek.topolski?source=post_page
  61. https://medium.com/tag/lstm?source=post
  62. https://medium.com/tag/machine-learning?source=post
  63. https://medium.com/tag/artificial-intelligence?source=post
  64. https://medium.com/tag/recurrent-neural-network?source=post
  65. https://medium.com/tag/data-visualization?source=post
  66. https://medium.com/@piotr.tempczyk
  67. https://medium.com/asap-report?source=footer_card
  68. https://medium.com/asap-report?source=footer_card
  69. https://medium.com/asap-report
  70. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  72. https://medium.com/@piotr.tempczyk?source=post_header_lockup
  73. https://medium.com/p/f1d3fa6aace7/share/twitter
  74. https://medium.com/p/f1d3fa6aace7/share/facebook
  75. https://medium.com/@piotr.tempczyk?source=footer_card
  76. https://medium.com/p/f1d3fa6aace7/share/twitter
  77. https://medium.com/p/f1d3fa6aace7/share/facebook
