   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

deep generative models

   [9]go to the profile of prakash pandey
   [10]prakash pandey (button) blockedunblock (button) followfollowing
   jan 23, 2018

   a generative model is a powerful way of learning any kind of data
   distribution using unsupervised learning and it has achieved tremendous
   success in just few years. all types of generative models aim at
   learning the true data distribution of the training set so as to
   generate new data points from this distribution with some variations.
   but it is not always possible to learn the exact distribution of our
   data either implicitly or explicitly and so we try to model a
   distribution which is as similar as possible to the true data
   distribution. for this, we can leverage the power of neural networks to
   learn a function which can approximate the mapping from the model
   distribution to the true distribution.

   two of the most commonly used and efficient approaches are variational
   autoencoders (vae) and id3 (gan). vae aims
   at maximizing the lower bound of the data log-likelihood and gan aims
   at achieving an equilibrium between generator and discriminator. in
   this blogpost, i will be explaining the working of vae and gans and the
   intuition behind them.

variational autoencoder

   i am assuming that the reader is already familiar with the working of a
   vanilla autoencoder. we know that we can use an autoencoder to encode
   an input image to a much smaller dimensional representation which can
   store latent information about the input data distribution. but in a
   vanilla autoencoder, the encoded vector can only be mapped to the
   corresponding input using a decoder. it certainly can   t be used to
   generate similar images with some variability.

   to achieve this, the model needs to learn the id203 distribution
   of the training data. vae is one of the most popular approach to learn
   the complicated data distribution such as images using neural networks
   in an unsupervised fashion. it is a probabilistic graphical model
   rooted in bayesian id136 i.e., the model aims to learn the
   underlying id203 distribution of the training data so that it
   could easily sample new data from that learned distribution. the idea
   is to learn a low-dimensional latent representation of the training
   data called latent variables (variables which are not directly observed
   but are rather inferred through a mathematical model) which we assume
   to have generated our actual training data. these latent variables can
   store useful information about the type of output the model needs to
   generate. the id203 distribution of latent variables z is denoted
   by p(z). a gaussian distribution is selected as a prior to learn the
   distribution p(z) so as to easily sample new data points during
   id136 time. now the primary objective is to model the data with
   some parameters which maximizes the likelihood of training data x. in
   short, we are assuming that a low-dimensional latent vector has
   generated our data x (x     x) and we can map this latent vector to data
   x using a deterministic function f(z;  ) parameterized by    which we
   need to evaluate (see fig. 1). under this generative process, our aim
   is to maximize the id203 of each data in x which is given as,

   p  (x) =    p  (x, z)dz =    p  (x|z)p  (z)dz (1)

   here, f(z;  )has been replaced by a distribution p  (x|z).
   [0*hogimukya0jn7szo.]
   fig. 1. latent vector mapped to data distribution using parameter   

   the intuition behind this id113 is that if the
   model can generate training samples from these latent variables then it
   can also generate similar samples with some variations. in other words,
   if we sample a large number of latent variables from p(z) and generate
   x from these variables then the generated x should match the data
   distribution p_data(x).

   now we have two questions which we need to answer. how to capture the
   distribution of latent variables and how to integrate equation 1 over
   all the dimensions of z?

   obviously it is a tedious task to manually specify the relevant
   information we would like to encode in latent vector to generate the
   output image. rather we rely on neural networks to compute z just with
   an assumption that this latent vector can be well approximated as a
   normal distribution so as to sample easily at id136 time. if we
   have a normal distribution of z in n dimensional space then it is
   always possible to generate any kind of distribution using a
   sufficiently complicated function and the inverse of this function can
   be used to learn the latent variables itself.

   in equation 1, integration is carried over all the dimensions of z and
   is therefore intractable. however, it can be calculated using methods
   of monte-carlo integration which is something not easy to implement. so
   we follow an another approach to approximately maximize p  (x) in
   equation 1. the idea of vae is to infer p(z) using p(z|x) which we
   don   t know. we infer p(z|x) using a method called variational id136
   which is basically an optimization problem in bayesian statistics. we
   first model p(z|x) using simpler distribution q(z|x) which is easy to
   find and we try to minimize the difference between p(z|x) and q(z|x)
   using kl-divergence metric approach so that our hypothesis is close to
   the true distribution. this is followed by a lot of mathematical
   equations which i will not be explaining here but you can find it in
   this [11]tutorial. but i must say that those equations are not very
   difficult to understand once you get the intuition behind vae.

   the final objective function of vae is :-
   [0*6ua0ntjdwpvdtwlr.]

   the above equation has a very nice interpretation. the term q(z|x) is
   basically our encoder net, z is our encoded representation of data x(x
       x) and p(x|z) is our decoder net. so in the above equation our goal
   is to maximize the log-likelihood of our data distribution under some
   error given by d_kl[q(z|x) || p(z|x)]. it can easily be seen that vae
   is trying to minimize the lower bound of log(p(x)) since p(z|x) is not
   tractable but the kl-divergence term is always greater than or equal to
   zero. this is same as maximizing e[logp(x|z)] and minimizing
   d_kl[q(z|x) || p(z|x)]. we know that maximizing e[logp(x|z)] is a
   id113 and is modeled using a decoder net. as i
   said earlier that we want our latent representation to be close to
   gaussian and hence we assume p(z) as n(0, 1). following this
   assumption, q(z|x) should also be close to this distribution. if we
   assume that it is a gaussian with parameters   (x) and   (x), the error
   due to the difference between these two distributions i.e., p(z) and
   q(z|x) given by kl-divergence results in a closed form solution given
   below.
   [0*ipt1kc_w3fbkj1tb.]

   considering we are optimizing the lower variational bound, our
   optimization function is :

   log(p(x|z))     d_kl[q(z|x)   p(z)], where the solution of the second term
   is shown above.

   hence, our id168 will contain two terms. first one is
   reconstruction loss of the input to output and the second loss is
   kl-divergence term. now we can train the network using id26
   algorithm. but there is a problem and that is the first term doesn   t
   only depend on the parameters of p but also on the parameters of q but
   this dependency doesn   t appear in the above equation. so how to
   backpropagate through the layer where we are sampling z randomly from
   the distribution q(z|x) or n[  (x),   (x)] so that p can decode.
   gradients can   t flow through random nodes. we use reparameterization
   trick to make the network differentiable. we sample from n(  (x),   (x))
   by first sampling        n(0, i), then computing z=  (x) +   1/2(x)     .

   this has been very beautifully shown in the fig. 2 ? . it should be
   noted that the feedforward step is identical for both of these networks
   (left & right) but gradients can only backpropagate through right
   network.
   [0*lrw8dl56nzjipo88.]
   fig.2. reparameterization trick used to backpropagate through
   random nodes

   at id136 time, we can simply sample z from n(0, 1) and feed it to
   decoder net to generate new data point. since we are optimizing the
   lower variational bound, the quality of the generated image is somewhat
   poor as compared to state-of-the art techniques like generative
   adversarial networks.

   the best thing of vae is that it learns both the generative model and
   an id136 model. although both vae and gans are very exciting
   approaches to learn the underlying data distribution using unsupervised
   learning but gans yield better results as compared to vae. in vae, we
   optimize the lower variational bound whereas in gan, there is no such
   assumption. in fact, gans don   t deal with any explicit id203
   density estimation. the failure of vae in generating sharp images
   implies that the model is not able to learn the true posterior
   distribution. vae and gan mainly differ in the way of training. let   s
   now dive into id3.

id3

   yann lecun says that adversarial training is the coolest thing since
   sliced bread. seeing the popularity of id3
   and the quality of the results they produce, i think most of us would
   agree with him. adversarial training has completely changed the way we
   teach the neural networks to do a specific task. generative adversarial
   networks don   t work with any explicit density estimation like
   id5. instead, it is based on game theory approach
   with an objective to find nash equilibrium between the two networks,
   generator and discriminator. the idea is to sample from a simple
   distribution like gaussian and then learn to transform this noise to
   data distribution using universal function approximators such as neural
   networks.

   this is achieved by adversarial training of these two networks. a
   generator model g learns to capture the data distribution and a
   discriminator model d estimates the id203 that a sample came from
   the data distribution rather than model distribution. basically the
   task of the generator is to generate natural looking images and the
   task of the discriminator is to decide whether the image is fake or
   real. this can be thought of as a mini-max two player game where the
   performance of both the networks improves over time. in this game, the
   generator tries to fool the discriminator by generating real images as
   far as possible and the generator tries to not get fooled by the
   discriminator by improving its discriminative capability. below image
   shows the basic architecture of gan.
   [0*aug_twhbhwz8lgkl.]
   fig.3. building block of generative adversarial network

   we define a prior on input noise variables p(z) and then the generator
   maps this to data distribution using a complex differentiable function
   with parameters   g. in addition to this, we have another network called
   discriminator which takes in input x and using another differentiable
   function with parameters   d outputs a single scalar value denoting the
   id203 that x comes from the true data distribution p_data(x). the
   objective function of the gan is defined as
   [0*2y5ph-zpufargz43.]

   in the above equation, if the input to the discriminator comes from
   true data distribution then d(x) should output 1 to maximize the above
   objective function w.r.t d whereas if the image has been generated from
   the generator then d(g(z)) should output 1 to minimize the objective
   function w.r.t g. the latter basically implies that g should generate
   such realistic images which can fool d. we maximize the above function
   w.r.t parameters of discriminator using gradient ascent and minimize
   the same w.r.t parameters of generator using id119. but
   there is a problem in optimizing generator objective. at the start of
   the game when the generator hasn   t learned anything, the gradient is
   usually very small and when it is doing very well, the gradients are
   very high (see fig. 4). but we want the opposite behavior. we therefore
   maximize e[log(d(g(z))] rather than minimizing e[log(1-d(g(z))]
   [0*su1x5wfvapx1zzs9.]
   fig.4. cost for the generator as a function of discriminator response
   on the generated image

   the training process consists of simultaneous application of stochastic
   id119 on discriminator and generator. while training, we
   alternate between k steps of optimizing d and one step of optimizing g
   on the mini-batch. the process of training stops when the discriminator
   is unable to distinguish   g and   data i.e. d(x,   _d) =    or when   g =
     _data.

   one of the earliest model on gan employing convolutional neural network
   was dcgan which stands for deep convolutional generative adversarial
   networks. this network takes as input 100 random numbers drawn from a
   uniform distribution and outputs an image of desired shape. the network
   consists of many convolutional, deconvolutional and fully connected
   layers. the network uses many deconvolutional layers to map the input
   noise to the desired output image. batch id172 is used to
   stabilize the training of the network. relu activation is used in
   generator for all layers except the output layer which uses tanh layer
   and leaky relu is used for all layers in the discriminator. this
   network was trained using mini-batch stochastic id119 and
   adam optimizer was used to accelerate training with tuned
   hyperparameters. the results of the paper were quite interesting. the
   authors showed that the generators have interesting vector arithmetic
   properties using which we can manipulate images in the way we want.
   [0*cyg4ng00jlbid1978i.]
   fig.5. generator of dcgan
   [0*a3lmordmfbj1qed7.]
   fig.6. discriminator of dcgan

   one of the most widely used variation of gans is conditional gan which
   is constructed by simply adding conditional vector along with the noise
   vector (see fig. 7). prior to cgan, we were generating images randomly
   from random samples of noise z. what if we want to generate an image
   with some desired features. is there any way to provide this extra
   information to the model anyhow about what type of image we want to
   generate? the answer is yes and conditional gan is the way to do that.
   by conditioning the model on additional information which is provided
   to both generator and discriminator, it is possible to direct the data
   generation process. conditional gans are used in a variety of tasks
   such as text to image generation, image to image translation, automated
   image tagging etc. a unified structure of both the networks has been
   shown in the diagram below.

   one of the cool thing about gans is that they can be trained even with
   small training data. indeed the results of gans are promising but the
   training procedure is not trivial especially setting up the
   hyperparameters of the network. moreover, gans are difficult to
   optimize as they don   t converge easily. of course there are some tips
   and tricks to hack gans but they may not always help. you can find some
   of these tips [12]here. also, we don   t have any criteria for the
   quantitative evaluation of the results except to check whether the
   generated images are perceptually realistic or not.

conclusion

   deep learning models are really achieving human level performance in
   supervised learning but the same is not true for unsupervised learning.
   nevertheless, deep learning scientists are working hard to improve the
   performance of unsupervised models. in this blogpost, we saw how two of
   the most famous unsupervised learning frameworks of generative models
   actually work. we got to know the problems in id5
   and why adversarial networks are better at producing realistic images.
   but there are problems with gans such as stabilizing their training
   which is still an active area of research. however gans are really
   powerful and currently they are being used in a variety of tasks such
   as high quality image (see this [13]video) and video generation, text
   to image translation, image enhancement, reconstruction of 3d models of
   objects from images, music generation, cancer drug discovery etc.
   besides this, many deep learning researchers are also working to unify
   these two models and to get the best of both these models. seeing the
   increasing rate of advancement of deep learning, i believe that gans
   will open many closed doors of artificial intelligence such as
   semi-supervised learning and id23. in the next few
   years, generative models is going to be very helpful for graphics
   designing, designing of attractive user-interfaces etc. it may also be
   possible to generate natural language texts using generative
   adversarial networks.

   references:-

   [14]https://arxiv.org/pdf/1606.05908.pdf

   [15]https://arxiv.org/pdf/1406.2661.pdf

   [16]https://wiseodd.github.io/techblog/2016/12/10/variational-autoencod
   er/

     * [17]artificial intelligence
     * [18]deep learning
     * [19]machine learning
     * [20]generative model
     * [21]autoencoder

   (button)
   (button)
   (button) 223 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of prakash pandey

[23]prakash pandey

   deep learning enthusiast

     * (button)
       (button) 223
     * (button)
     *
     *

   [24]go to the profile of prakash pandey
   never miss a story from prakash pandey, when you sign up for medium.
   [25]learn more
   never miss a story from prakash pandey
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e0f149995b7c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@prakashpandey9/deep-generative-models-e0f149995b7c&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@prakashpandey9/deep-generative-models-e0f149995b7c&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@prakashpandey9?source=post_header_lockup
  10. https://medium.com/@prakashpandey9
  11. https://arxiv.org/pdf/1606.05908.pdf
  12. https://github.com/soumith/ganhacks
  13. https://youtu.be/xoxxpcy5gr4
  14. https://arxiv.org/pdf/1606.05908.pdf
  15. https://arxiv.org/pdf/1406.2661.pdf
  16. https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/
  17. https://medium.com/tag/artificial-intelligence?source=post
  18. https://medium.com/tag/deep-learning?source=post
  19. https://medium.com/tag/machine-learning?source=post
  20. https://medium.com/tag/generative-model?source=post
  21. https://medium.com/tag/autoencoder?source=post
  22. https://medium.com/@prakashpandey9?source=footer_card
  23. https://medium.com/@prakashpandey9
  24. https://medium.com/@prakashpandey9
  25. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  27. https://medium.com/p/e0f149995b7c/share/twitter
  28. https://medium.com/p/e0f149995b7c/share/facebook
  29. https://medium.com/p/e0f149995b7c/share/twitter
  30. https://medium.com/p/e0f149995b7c/share/facebook
