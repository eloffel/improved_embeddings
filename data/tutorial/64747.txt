   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

multinomial naive bayes classifier for text analysis (python)

   [16]go to the profile of syed sadat nazrul
   [17]syed sadat nazrul (button) blockedunblock (button) followfollowing
   apr 9, 2018
   [1*ppg7yzwsyto1u1ow370sva.jpeg]

   one of the most popular applications of machine learning is the
   analysis of categorical data, specifically text data. issue is that,
   there are a ton of tutorials out there for numeric data but very little
   for texts. considering how most of my past blogs on machine learning
   were based on scikit-learn, i decided to have some fun with this one by
   implementing the whole thing on my own.

   in this blog, i will cover how you can implement a multinomial naive
   bayes classifier for the [18]20 newsgroups dataset. the 20 newsgroups
   dataset comprises around 18000 newsgroups posts on 20 topics split in
   two subsets: one for training (or development) and the other one for
   testing (or for performance evaluation). the split between the train
   and test set is based upon a messages posted before and after a
   specific date.

libraries

   first, let us import the libraries needed for writing the
   implementation:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import operator

class distribution

   first, we calculate the fraction of documents in each class:
   [1*j74sr42zoygo0jzmxc7efw.gif]
#training label
train_label = open('20news-bydate/matlab/train.label')
#pi is the fraction of each class
pi = {}
#set a class index for each document as key
for i in range(1,21):
    pi[i] = 0

#extract values from training labels
lines = train_label.readlines()
#get total number of documents
total = len(lines)
#count the occurence of each class
for line in lines:
    val = int(line.split()[0])
    pi[val] += 1
#divide the count of each class by total documents
for key in pi:
    pi[key] /= total

print("id203 of each class:")
print("\n".join("{}: {}".format(k, v) for k, v in pi.items()))

   [1*jq7bb1cikckfq82xycm7gw.png]

id203 distribution over vocabulary

   let   s first create the pandas dataframe
#training data
train_data = open('20news-bydate/matlab/train.data')
df = pd.read_csv(train_data, delimiter=' ', names=['docidx', 'wordidx', 'count']
)
#training label
label = []
train_label = open('/home/sadat/downloads/hw2_210/20news-bydate/matlab/train.lab
el')
lines = train_label.readlines()
for line in lines:
    label.append(int(line.split()[0]))
#increase label length to match docidx
docidx = df['docidx'].values
i = 0
new_label = []
for index in range(len(docidx)-1):
    new_label.append(label[i])
    if docidx[index] != docidx[index+1]:
        i += 1
new_label.append(label[i]) #for-loop ignores last value
#add label column
df['classidx'] = new_label
df.head()

   [1*yw_yr5nyfqhd8hvs516mgq.png]

id203 of each word per class

   for calculating our id203, we will find the average of each word
   for a given class.

   for class j and word i, the average is given by:
   [1*hlazt-2ugftuzmqbfbeuzq.png]

   however, since some words will have 0 counts, we will perform a laplace
   smoothing with low   :
   [1*l0rgaxzgrsaxkkfoinabuw.png]

   where v is an array of all the words in the vocabulary
#alpha value for smoothing
a = 0.001
#calculate id203 of each word based on class
pb_ij = df.groupby(['classidx','wordidx'])
pb_j = df.groupby(['classidx'])
pr =  (pb_ij['count'].sum() + a) / (pb_j['count'].sum() + 16689)
#unstack series
pr = pr.unstack()
#replace nan or columns with 0 as word count with a/(count+|v|+1)
for c in range(1,21):
    pr.loc[c,:] = pr.loc[c,:].fillna(a/(pb_j['count'].sum()[c] + 16689))
#convert to dictionary for greater speed
pr_dict = pr.to_dict()
pr

   [1*5rfv9c70use7jv1oguonaa.png]

stop words

   stop words are words that show up a lot in every document (e.g.
   prepositions and pronouns).
#common stop words from online
stop_words = [
"a", "about", "above", "across", "after", "afterwards",
"again", "all", "almost", "alone", "along", "already", "also",
"although", "always", "am", "among", "amongst", "amoungst", "amount", "an", "and
", "another", "any", "anyhow", "anyone", "anything", "anyway", "anywhere", "are"
, "as", "at", "be", "became", "because", "become","becomes", "becoming", "been",
 "before", "behind", "being", "beside", "besides", "between", "beyond", "both",
"but", "by","can", "cannot", "cant", "could", "couldnt", "de", "describe", "do",
 "done", "each", "eg", "either", "else", "enough", "etc", "even", "ever", "every
", "everyone", "everything", "everywhere", "except", "few", "find","for","found"
, "four", "from", "further", "get", "give", "go", "had", "has", "hasnt", "have",
 "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "her
s", "herself", "him", "himself", "his", "how", "however", "i", "ie", "if", "in",
 "indeed", "is", "it", "its", "itself", "keep", "least", "less", "ltd", "made",
"many", "may", "me", "meanwhile", "might", "mine", "more", "moreover", "most", "
mostly", "much", "must", "my", "myself", "name", "namely", "neither", "never", "
nevertheless", "next","no", "nobody", "none", "noone", "nor", "not", "nothing",
"now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or
", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "o
wn", "part","perhaps", "please", "put", "rather", "re", "same", "see", "seem", "
seemed", "seeming", "seems", "she", "should","since", "sincere","so", "some", "s
omehow", "someone", "something", "sometime", "sometimes", "somewhere", "still",
"such", "take","than", "that", "the", "their", "them", "themselves", "then", "th
ence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "t
hese", "they",
"this", "those", "though", "through", "throughout",
"thru", "thus", "to", "together", "too", "toward", "towards",
"under", "until", "up", "upon", "us",
"very", "was", "we", "well", "were", "what", "whatever", "when",
"whence", "whenever", "where", "whereafter", "whereas", "whereby",
"wherein", "whereupon", "wherever", "whether", "which", "while",
"who", "whoever", "whom", "whose", "why", "will", "with",
"within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourse
lves"
]

   now, let   s create the vocabulary dataframe
vocab = open('vocabulary.txt')
vocab_df = pd.read_csv(vocab, names = ['word'])
vocab_df = vocab_df.reset_index()
vocab_df['index'] = vocab_df['index'].apply(lambda x: x+1) vocab_df.head()

   [1*jwebdwrtfk0ukwux6qif5w.png]

   getting the counts of each word in the vocabulary and setting stop
   words to 0:
#index of all words
tot_list = set(vocab_df['index'])
#index of good words
vocab_df = vocab_df[~vocab_df['word'].isin(stop_words)]
good_list = vocab_df['index'].tolist()
good_list = set(good_list)
#index of stop words
bad_list = tot_list - good_list
#set all stop words to 0
for bad in bad_list:
    for j in range(1,21):
        pr_dict[j][bad] = a/(pb_j['count'].sum()[j] + 16689)

   multinomial naive bayes classifier

   combining id203 distribution of p with fraction of documents
   belonging to each class.

   for class j, word i at a word frequency of f:
   [1*aakwbojonsfg2txzsyr9sq.gif]

   in order to avoid underflow, we will use the sum of logs:
   [1*ncv5t9tzwevvw24di5iahw.gif]
   [1*innuwwwf-2psqo93vaohiw.gif]

   one issue is that, if a word appears again, the id203 of it
   appearing again goes up. in order to smooth this, we take the log of
   the frequency:
   [1*hqbjr5m3salsouze1cpsyw.gif]

   also, in order to take stop words into account, we will add a inverse
   document frequency (idf)weight on each word:
   [1*qbeboi9mcjplkbg6ylw9sg.gif]
   [1*4xdlolxubdsn9jix_odjva.gif]

   even though the stop words have already been set to 0 for this specific
   use case, the idf implementation is being added to generalize the
   function.
#calculate idf
tot = len(df['docidx'].unique())
pb_ij = df.groupby(['wordidx'])
idf = np.log(tot/pb_ij['docidx'].count())
idf_dict = idf.to_dict()
def mnb(df, smooth = false, idf = false):
    '''
    multinomial naive bayes classifier
    :param df [pandas dataframe]: dataframe of data
    :param smooth [bool]: apply smoothing if true
    :param idf [bool]: apply inverse document frequency if true
    :return predict [list]: predicted class id
    '''
    #using dictionaries for greater speed
    df_dict = df.to_dict()
    new_dict = {}
    prediction = []

    #new_dict = {docidx : {wordidx: count},....}
    for idx in range(len(df_dict['docidx'])):
        docidx = df_dict['docidx'][idx]
        wordidx = df_dict['wordidx'][idx]
        count = df_dict['count'][idx]
        try:
            new_dict[docidx][wordidx] = count
        except:
            new_dict[df_dict['docidx'][idx]] = {}
            new_dict[docidx][wordidx] = count
    #calculating the scores for each doc
    for docidx in range(1, len(new_dict)+1):
        score_dict = {}
        #creating a id203 row for each class
        for classidx in range(1,21):
            score_dict[classidx] = 1
            #for each word:
            for wordidx in new_dict[docidx]:
                #check for frequency smoothing
                #log(1+f)*log(pr(i|j))
                if smooth:
                    try:
                        id203=pr_dict[wordidx][classidx]
                        power = np.log(1+ new_dict[docidx][wordidx])
                        #check for idf
                        if idf:
                            score_dict[classidx]+=(
                               power*np.log(
                               id203*idf_dict[wordidx]))
                        else:
                            score_dict[classidx]+=power*np.log(
                                                   id203)
                    except:
                        #missing v will have log(1+0)*log(a/16689)=0
                        score_dict[classidx] += 0
                #f*log(pr(i|j))
                else:
                    try:
                        id203 = pr_dict[wordidx][classidx]
                        power = new_dict[docidx][wordidx]
                        score_dict[classidx]+=power*np.log(
                                           id203)
                        #check for idf
                        if idf:
                            score_dict[classidx]+= power*np.log(
                                   id203*idf_dict[wordidx])
                    except:
                        #missing v will have 0*log(a/16689) = 0
                        score_dict[classidx] += 0
            #multiply final with pi
            score_dict[classidx] +=  np.log(pi[classidx])

        #get class with max probabilty for the given docidx
        max_score = max(score_dict, key=score_dict.get)
        prediction.append(max_score)

    return prediction

   comparing the effects of smoothing and idf:
regular_predict = mnb(df, smooth=false, idf=false)
smooth_predict  = mnb(df, smooth=true, idf=false)
tfidf_predict   = mnb(df, smooth=false, idf=true)
all_predict     = mnb(df, smooth=true, idf=true)
#get list of labels
train_label = pd.read_csv('20news-bydate/matlab/train.label',
                          names=['t'])
train_label= train_label['t'].tolist()
total = len(train_label)
models = [regular_predict, smooth_predict,
          tfidf_predict, all_predict]
strings = ['regular', 'smooth', 'idf', 'both']

for m,s in zip(models,strings):
    val = 0
    for i,j in zip(m, train_label):
        if i != j:
            val +=1
        else:
            pass
    print(s,"error:\t\t",val/total * 100, "%")

   [1*75pwhrercque9ib4hk852a.png]

   as we can see, idf has little effect as we removed the stop words.
   smoothing, however, makes the model more accurate.

   hence, our optimal model is:
   [1*hqbjr5m3salsouze1cpsyw.gif]

test data

   now that we have out model, let   s use it to predict our test data.
#get test data
test_data = open('20news-bydate/matlab/test.data')
df = pd.read_csv(test_data, delimiter=' ', names=['docidx', 'wordidx', 'count'])
#get list of labels
test_label = pd.read_csv('/home/sadat/downloads/hw2_210/20news-bydate/matlab/tes
t.label', names=['t'])
test_label= test_label['t'].tolist()
#mnb calculation
predict = mnb(df, smooth = true, idf = false)
total = len(test_label)
val = 0
for i,j in zip(predict, test_label):
    if i == j:
        val +=1
    else:
        pass
print("error:\t",(1-(val/total)) * 100, "%")

   [1*zr50vs2nbuxhm78vhqlplq.png]

     * [19]machine learning
     * [20]text analytics
     * [21]bayesian machine learning
     * [22]naive bayes
     * [23]python

   (button)
   (button)
   (button) 501 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of syed sadat nazrul

[25]syed sadat nazrul

   using machine learning to catch cyber and financial criminals by day    
   and writing cool blogs by night.

     (button) follow
   [26]towards data science

[27]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 501
     * (button)
     *
     *

   [28]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [29]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8dd6825ece67
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_mdaivmn4gd6j---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@sadatnazrul?source=post_header_lockup
  17. https://towardsdatascience.com/@sadatnazrul
  18. http://qwone.com/~jason/20newsgroups/
  19. https://towardsdatascience.com/tagged/machine-learning?source=post
  20. https://towardsdatascience.com/tagged/text-analytics?source=post
  21. https://towardsdatascience.com/tagged/bayesian-machine-learning?source=post
  22. https://towardsdatascience.com/tagged/naive-bayes?source=post
  23. https://towardsdatascience.com/tagged/python?source=post
  24. https://towardsdatascience.com/@sadatnazrul?source=footer_card
  25. https://towardsdatascience.com/@sadatnazrul
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/8dd6825ece67/share/twitter
  32. https://medium.com/p/8dd6825ece67/share/facebook
  33. https://medium.com/p/8dd6825ece67/share/twitter
  34. https://medium.com/p/8dd6825ece67/share/facebook
