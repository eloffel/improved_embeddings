   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]intro to artificial intelligence
     * [9]artificial intelliegence
     * [10]machine learning
     * [11]self-driving
     * [12]technology
     __________________________________________________________________

entity extraction using deep learning based on [13]guillaume genthial work
on ner

   [14]go to the profile of dhanoop karunakaran
   [15]dhanoop karunakaran (button) blockedunblock (button)
   followfollowing
   apr 1, 2018
   [1*m3uyigshem1u1h70gwfgmw.png]

introduction

   entity extraction from text is a major natural language processing
   (nlp) task. as the recent advancement in the deep learning(dl) enable
   us to use them for nlp tasks and producing huge differences in accuracy
   compared to traditional methods.

   i have attempted to extract the information from article using both
   deep learning and traditional methods. result was amazing as dl method
   got accuracy of 85% over 65% from legacy methods.

   the aim of the project is to tag each words of the articles into 4
   categories: organisation, person, miscellaneous, and other. then find
   the organisation and names most prominent in the article. the deep
   learning model tag each word into above 4 categories. then a rule based
   approach to filter the unwanted tagging and finding most prominent
   names and organisation.

   the code base is taken from [16]guillaume genthial   s repo and all the
   credit goes to him for the work on ner. also [17]guillaume genthial
   blog has some low level detail on his ner project.

   thanks to [18]guillaume genthial   s blog about sequence tagging work
   which is the backbone of this project.

high level architecture of the model

   [1*2nlvz-elogzw941on-buba.png]
   architecture

   this is the high level architecture of model that tag the words into
   each categories. i would like to explain each component of the model to
   give you high level understanding about the componets. in general,
   components are divided into three sections(based on [19]guillaume
   genthial   s blog):
     * word representation: the first thing we can do is load some
       pre-trained id27s ([20]glove). also, we are extracting
       some meaning from the characters.
     * contextual word representation: for each word in its context, we
       need to get a meaningful representation using lstm
     * decoding: once we have a vector representing each word, we can use
       it to make a prediction.

hot encoding(words to numbers)

   deep learning models accept only numerical data as input rather than
   text. in order to use deep learning model for wide range of
   applications which are not numerical based, then input data needs to be
   converted into numerical form. this process is called hot encoding.

   here is the small example code how it   s done:
word_counts = counter(words)
sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=true)
int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}
vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}

   similartly we have to get all the character used in the input data and
   convert them to vectors for character embedding.

id27 & character embedding

   a id27 is a learned representation for text where words that
   have the same meaning have a similar representation. id27 is
   usually done using neural networks where words or phrases from the
   vocabulary are mapped to vectors of real numbers.

   however, generating word vectors for datasets can be computationally
   expensive(see my github [21]repo if you would like to play with it).
   the easy way to work around this is to use pretrained id27s,
   such as [22]the glove vectors collected by researchers at stanford nlp.
   [1*krs0lzmf_sxr_hifj28diq.png]

   character embedding is a vector representation of characters where it
   can derive word vectors. the main use of this embedding is a lot of
   entities don   t even have a pretrained word vector, so the word vector
   can be calculated from character vectors.there is a good online
   [23]resource available to know about the character embedding details.

lstm

   [1*i-tubklmwvi-gj2o2u7haw@2x.png]
   nn vs id56

   recurrent neural networks(id56) are a type of id158
   designed to recognize patterns in sequences of data, such as text,
   genomes, handwriting, the spoken word, or numerical times series data
   emanating from sensors, stock markets and government agencies. it can
   understand the contextual meaning of the text.
   [1*uwlur9hclp7unyh8nzmnia.jpeg]
   id56 neurons

   lstm is a type of recurrent neural network which can store more
   contextual information than simple recurrent neural network. major
   difference between simple id56 and lstm lies in the architecture of each
   neuron.

   for each word in its context, we need to get a meaningful
   representation using lstm.

   if you want to read more about lstm and id56 in general, below mentioned
   article is useful:
    1. [24]http://karpathy.github.io/2015/05/21/id56-effectiveness
    2. [25]https://deeplearning4j.org/lstm.html

conditional random field(crf)

   we can use softmax as the final decoding step to predict the tags. when
   we use softmax, it give the id203 of the word being in any of the
   classifications. but that method makes local choices. in other words,
   even if we capture some information from the context, the tagging
   decision is still local. we don   t make use of the neighbouring tagging
   decisions using softmax activation function.. for instance, in new
   york, the fact that we are tagging york as a location should help us to
   decide that new corresponds to the beginning of a location.

   in crfs, our input data is sequential, and we have to take previous
   context into account when making predictions on a data point. we make
   use of linear-chain crf in this project. in linear-chain crf, features
   to depend on only the current and previous labels, rather than
   arbitrary labels throughout the sentence.

   to model this behavior, we will use feature functions, that will have
   multiple input values, which are going to be:
     * a sentence s
     * the position i of a word in the sentence
     * the label li of the current word
     * the label li   1 of the previous word

   next, assign each feature function fj a weight   j . given a sentence s,
   we can now score a labeling l of s by adding up the weighted features
   over all words in the sentence:
   [1*mkf7bqrtcldpqwfh-olyaw.png]

   example feature functions based on id52
     * f1(s,i,li,li   1)=1 if li= adverb and the ith word ends in    -ly   ; 0
       otherwise. if the weight   1 associated with this feature is large
       and positive, then this feature is essentially saying that we
       prefer labelings where words ending in -ly get labeled as adverb.
     * f2(s,i,li,li   1)=1 if i=1, li= verb, and the sentence ends in a
       question mark; 0 otherwise. if the weight   2 associated with this
       feature is large and positive, then labelings that assign verb to
       the first word in a question (e.g.,    is this a sentence beginning
       with a verb?   ) are preferred.
     * f3(s,i,li,li   1)=1 if li   1= adjective and li= noun; 0 otherwise. a
       positive weight for this feature means that adjectives tend to be
       followed by nouns.
     * f4(s,i,li,li   1)=1 if li   1= preposition and li= preposition. a
       negative weight   4 for this function would mean that prepositions
       don   t tend to follow prepositions, so we should avoid labelings
       where this happens.

   finally, we can transform these scores into probabilities p(l|s)
   between 0 and 1 by exponentiating and normalizing:
   [1*zpjng1wzcdhgm7t65pypzq.png]

   to sum up, to build a conditional random field, you just define a bunch
   of feature functions (which can depend on the entire sentence, a
   current position, and nearby labels), assign them weights, and add them
   all together, transforming at the end to a id203 if necessary.
   basically, we need to do 2 things(based on [26]guillaume genthial   s
   blog):
    1. find the sequence of tags with the best score.
    2. compute a id203 distribution over all the sequence of tags

   luckily, tensorflow provided the library to do the crf that makes easy
   for us to implement.
log_likelihood, transition_params=tf.contrib.crf.crf_log_likelihood(
scores, labels, sequence_lengths)

   (the above is code is taken from [27]guillaume genthial   s github repo)

   crf reads:
    1. [28]http://blog.echen.me/2012/01/03/introduction-to-conditional-ran
       dom-fields/
    2. [29]http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.p
       df

how model works

   for each word, we want to build a vector that will capture the meaning
   and relevant features for our task. we   re gonna build this vector as a
   concatenation of the id27s from glove and a vector containing
   features extracted from the character level. one option is to use some
   kind of neural network to make this extraction automatically for us. in
   this post, we   re gonna use a bi-lstm at the character level,

   we hot-encode all the words in conll dataset which has an entry in
   glove id27s. as mentioned, nn accepts only vectors, not text,
   so we have to convert them to vectors. the conll dataset contains words
   and it   s corresponding tags. after the hot encoding, both of them
   converted to vectors.

   code for hot encoding words and it   s tags:
with open(self.filename) as f:
    words, tags = [], []
    for line in f:
        line = line.strip()
        if (len(line) == 0 or line.startswith("-docstart-")):
            if len(words) != 0:
                niter += 1
                if self.max_iter is not none and niter > self.max_iter:
                    break
                yield words, tags
                words, tags = [], []
        else:
            ls = line.split(' ')
            word, tag = ls[0],ls[-1]
            if self.processing_word is not none:
                word = self.processing_word(word)
            if self.processing_tag is not none:
                tag = self.processing_tag(tag)
            words += [word]
            tags += [tag]

   (the above is code is taken from [30]guillaume genthial   s github repo)

   code for pulling the vectors for words , tags, and characters:
if vocab_chars is not none and chars == true:
    char_ids = []
    for char in word:
        # ignore chars out of vocabulary
        if char in vocab_chars:
            char_ids += [vocab_chars[char]]
if lowercase:
    word = word.lower()
if word.isdigit():
    word = num
if vocab_words is not none:
    if word in vocab_words:
        word = vocab_words[word]
    else:
        if allow_unk:
            word = vocab_words[unk]
        else:
            print(word)
            print(vocab_words)
if vocab_chars is not none and chars == true:
    return char_ids, word
else:
    return word

   (the above is code is taken from [31]guillaume genthial   s github repo)

   now, let   s use tensorflow built-in functions to load the word
   embeddings. assume that embeddings is a numpy array with our glove
   embeddings, such that embeddings[i] gives the vector of the i-th word.
l = tf.variable(embeddings, dtype=tf.float32, trainable=false)
pretrained_embeddings = tf.nn.embedding_lookup(l, word_ids)

   (the above is code is taken from [32]guillaume genthial   s github repo)

   now, we can build the id27s from the characters. here, we
   don   t have any pretrained character embeddings.
_char_embeddings = tf.get_variable(
        name="_char_embeddings",
        dtype=tf.float32,
        shape=[self.config.nchars, self.config.dim_char])
char_embeddings = tf.nn.embedding_lookup(_char_embeddings,
        self.char_ids_tensor, name="char_embeddings")
s = tf.shape(char_embeddings)
char_embeddings = tf.reshape(char_embeddings,
        shape=[s[0]*s[1], s[-2], self.config.dim_char])
word_lengths = tf.reshape(self.word_lengths_tensor, shape=[s[0]*s[1]])
cell_fw = tf.contrib.id56.lstmcell(self.config.hidden_size_char,
        state_is_tuple=true)
cell_bw = tf.contrib.id56.lstmcell(self.config.hidden_size_char,
        state_is_tuple=true)
_output = tf.nn.bidirectional_dynamic_id56(
        cell_fw, cell_bw, char_embeddings,
        sequence_length=word_lengths, dtype=tf.float32)

   (the above is code is taken from [33]guillaume genthial   s github repo)

   once we have our word representation, we simply run a bi-lstm over the
   sequence of word vectors and obtain another sequence of vectors.
cell_fw = tf.contrib.id56.lstmcell(self.config.hidden_size_lstm)
cell_bw = tf.contrib.id56.lstmcell(self.config.hidden_size_lstm)
(output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_id56(
    cell_fw, cell_bw, self.word_embeddings,
    sequence_length=self.sequence_lengths_tensor, dtype=tf.float32)
output = tf.concat([output_fw, output_bw], axis=-1)
output = tf.nn.dropout(output, self.dropout_tensor)

   (the above is code is taken from [34]guillaume genthial   s github repo)

   at this stage, each word is associated to a vector that captures
   information from the meaning of the word, its characters and its
   context. let   s use it to make a final prediction. we can use a fully
   connected neural network to get a vector where each entry corresponds
   to a score for each tag.
w = tf.get_variable("w", dtype=tf.float32,
                    shape=[2*self.config.hidden_size_lstm, self.config.ntags])

b = tf.get_variable("b", shape=[self.config.ntags],
        dtype=tf.float32, initializer=tf.zeros_initializer())
nsteps = tf.shape(output)[1]
output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])
pred = tf.matmul(output, w) + b
self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])

   (the above is code is taken from [35]guillaume genthial   s github repo)

   finally we use crf method to find the tag of each words. implementing a
   crf only takes one-line! the following code computes the loss and also
   returns the trans_params that will be useful for prediction.
log_likelihood, _trans_params = tf.contrib.crf.crf_log_likelihood(
self.logits, self.labels_tensor, self.sequence_lengths_tensor)
self.trans_params = _trans_params
self.loss = tf.reduce_mean(-log_likelihood)

   (the above is code is taken from [36]guillaume genthial   s github repo)

   and then, we can define our train operator as
optimizer = tf.train.adamoptimizer(self.lr_tensor)
self.train_op = optimizer.minimize(self.loss)

   (the above is code is taken from [37]guillaume genthial   s github repo)

   once we define the model, run the model using dataset over few epoch to
   get the trained model.

how to use the trained model

   tensorflow provides ability to save the model weights so that it can be
   restored later. whenever we run the prediction step, model weights are
   loaded so that it doesn   t need to train again.
def save_session(self):
    """saves session = weights"""
    if not os.path.exists(self.config.dir_model):
        os.makedirs(self.config.dir_model)
    self.saver.save(self.sess, self.config.dir_model)
def restore_session(self, dir_model):
    self.saver.restore(self.sess, dir_model)

   (the above is code is taken from [38]guillaume genthial   s github repo)

   each article is feed into the model where it split into words, then go
   through the series process mentioned above to get the output. final
   output from the model classify each word into 4 categories:
   organisation, person, miscellaneous, and other. as it has not 100%
   accuracy going through rule based approach to filter the result further
   to extract the names and organisation correctly which is most prominent
   in the article.

   the article is based on guillaume genthial   s code base for the entity
   extraction, please visit the his [39]github repo for the code in
   action. there is a slight changes based on the my project intention
   which can be found [40]here.

   if you like my write up, follow me on github, linkedin, and/or medium
   profile.

     * [41]machine learning
     * [42]deep learning
     * [43]python
     * [44]tech
     * [45]programming

   (button)
   (button)
   (button) 866 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [46]go to the profile of dhanoop karunakaran

[47]dhanoop karunakaran

   software engineer, deep learning & machine learning engineer, self
   driving cars nanodegree holder@ udacity

     (button) follow
   [48]intro to artificial intelligence

[49]intro to artificial intelligence

   let's learn ai together

     * (button)
       (button) 866
     * (button)
     *
     *

   [50]intro to artificial intelligence
   never miss a story from intro to artificial intelligence, when you sign
   up for medium. [51]learn more
   never miss a story from intro to artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8014acac6bb8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/intro-to-artificial-intelligence/entity-extraction-using-deep-learning-8014acac6bb8&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/intro-to-artificial-intelligence/entity-extraction-using-deep-learning-8014acac6bb8&source=--------------------------nav_reg&operation=register
   8. https://medium.com/intro-to-artificial-intelligence?source=logo-lo_1nlaxl7hatxn---29b90ea3a2db
   9. https://medium.com/intro-to-artificial-intelligence/tagged/artificial-intelligence
  10. https://medium.com/intro-to-artificial-intelligence/tagged/machine-learning
  11. https://medium.com/intro-to-artificial-intelligence/tagged/self-driving-cars
  12. https://medium.com/intro-to-artificial-intelligence/tagged/technology
  13. https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html
  14. https://medium.com/@dhanoopkarunakaran?source=post_header_lockup
  15. https://medium.com/@dhanoopkarunakaran
  16. https://github.com/guillaumegenthial/sequence_tagging
  17. https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html
  18. https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html
  19. https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html
  20. https://nlp.stanford.edu/projects/glove/
  21. https://github.com/dkarunakaran/dlnd-other/tree/master/embeddings
  22. https://nlp.stanford.edu/projects/glove/
  23. http://minimaxir.com/2017/04/char-embeddings/
  24. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  25. https://deeplearning4j.org/lstm.html
  26. https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html
  27. https://github.com/guillaumegenthial/sequence_tagging
  28. http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/
  29. http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf
  30. https://github.com/guillaumegenthial/sequence_tagging
  31. https://github.com/guillaumegenthial/sequence_tagging
  32. https://github.com/guillaumegenthial/sequence_tagging
  33. https://github.com/guillaumegenthial/sequence_tagging
  34. https://github.com/guillaumegenthial/sequence_tagging
  35. https://github.com/guillaumegenthial/sequence_tagging
  36. https://github.com/guillaumegenthial/sequence_tagging
  37. https://github.com/guillaumegenthial/sequence_tagging
  38. https://github.com/guillaumegenthial/sequence_tagging
  39. https://github.com/guillaumegenthial/sequence_tagging
  40. https://github.com/dkarunakaran/entity_recoginition_deep_learning
  41. https://medium.com/tag/machine-learning?source=post
  42. https://medium.com/tag/deep-learning?source=post
  43. https://medium.com/tag/python?source=post
  44. https://medium.com/tag/tech?source=post
  45. https://medium.com/tag/programming?source=post
  46. https://medium.com/@dhanoopkarunakaran?source=footer_card
  47. https://medium.com/@dhanoopkarunakaran
  48. https://medium.com/intro-to-artificial-intelligence?source=footer_card
  49. https://medium.com/intro-to-artificial-intelligence?source=footer_card
  50. https://medium.com/intro-to-artificial-intelligence
  51. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  53. https://medium.com/p/8014acac6bb8/share/twitter
  54. https://medium.com/p/8014acac6bb8/share/facebook
  55. https://medium.com/p/8014acac6bb8/share/twitter
  56. https://medium.com/p/8014acac6bb8/share/facebook
