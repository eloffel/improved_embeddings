   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

diving deeper into id23 with id24

   [11]go to the profile of thomas simonini
   [12]thomas simonini (button) blockedunblock (button) followfollowing
   apr 10, 2018
   [1*syfg8ahktvnmv_vlrk0c0a.png]

     this article is part of deep id23 course with
     tensorflow        . check the syllabus [13]here.

   today we   ll learn about id24. id24 is a value-based
   id23 algorithm.

   this article is the second part of a free series of blog post about
   deep id23. for more information and more resources,
   check out the [14]syllabus of the course. see [15]the first article
   here.

   in this article you   ll learn:
     * what id24 is
     * how to implement it with numpy

the big picture: the knight and the princess

   [1*h7b4evx3b-sv5ovhh8nrnw.png]

   let   s say you   re a knight and you need to save the princess trapped in
   the castle shown on the map above.

   you can move one tile at a time. the enemy can   t, but land on the same
   tile as the enemy, and you will die. your goal is to go the castle by
   the fastest route possible. this can be evaluated using a    points
   scoring    system.
     * you lose -1 at each step (losing points at each step helps our
       agent to be fast).
     * if you touch an enemy, you lose -100 points, and the episode ends.
     * if you are in the castle you win, you get +100 points.

   the question is: how do you create an agent that will be able to do
   that?

   here   s a first strategy. let say our agent tries to go to each tile,
   and then colors each tile. green for    safe,    and red if not.
   [1*imhk8jfkt6udruwm8rvoha.png]
   the same map, but colored in to show which tiles are safe to visit.

   then, we can tell our agent to take only green tiles.

   but the problem is that it   s not really helpful. we don   t know the best
   tile to take when green tiles are adjacent each other. so our agent can
   fall into an infinite loop by trying to find the castle!

introducing the q-table

   here   s a second strategy: create a table where we   ll calculate the
   maximum expected future reward, for each action at each state.

   thanks to that, we   ll know what   s the best action to take for each
   state.

   each state (tile) allows four possible actions. these are moving left,
   right, up, or down.
   [1*kwu9timqawzciooj3plyca.png]
   0 are impossible moves (if you   re in top left hand corner you can   t go
   left or up!)

   in terms of computation, we can transform this grid into a table.

   this is called a q-table (   q    for    quality    of the action). the columns
   will be the four actions (left, right, up, down). the rows will be the
   states. the value of each cell will be the maximum expected future
   reward for that given state and action.
   [1*fbjmzvxbydx2-qoxzhnzfq.png]

   each q-table score will be the maximum expected future reward that i   ll
   get if i take that action at that state with the best policy given.

   why do we say    with the policy given?    it   s because we don   t implement
   a policy. instead, we just improve our q-table to always choose the
   best action.

   think of this q-table as a game    cheat sheet.    thanks to that, we know
   for each state (each line in the q-table) what   s the best action to
   take, by finding the highest score in that line.

   yeah! we solved the castle problem! but wait    how do we calculate the
   values for each element of the q table?

   to learn each value of this q-table, we   ll use the id24
   algorithm.

id24 algorithm: learning the action value function

   the action value function (or    q-function   ) takes two inputs:    state   
   and    action.    it returns the expected future reward of that action at
   that state.
   [1*6iqzimifk1oeivwmlu1esw.png]

   we can see this q function as a reader that scrolls through the q-table
   to find the line associated with our state, and the column associated
   with our action. it returns the q value from the matching cell. this is
   the    expected future reward.   
   [1*yklmxnrdxleidbv6aszuig.png]

   but before we explore the environment, the q-table gives the same
   arbitrary fixed value (most of the time 0). as we explore the
   environment, the q-table will give us a better and better approximation
   by iteratively updating q(s,a) using the bellman equation (see below!).

the id24 algorithm process

   [1*qeoqeqwyyps1p8yuwyajvq.png]
   [0*vokuagu68-cduncy.]
   the id24 algorithm   s pseudo-code

   step 1: initialize q-values
   we build a q-table, with m cols (m= number of actions), and n rows (n =
   number of states). we initialize the values at 0.
   [1*ut7-8vva-twc40_yaeqz7q.png]

   step 2: for life (or until learning is stopped)
   steps 3 to 5 will be repeated until we reached a maximum number of
   episodes (specified by the user) or until we manually stop the
   training.

   step 3: choose an action
   choose an action a in the current state s based on the current q-value
   estimates.

   but   what action can we take in the beginning, if every q-value equals
   zero?

   that   s where the exploration/exploitation trade-off that we spoke about
   in [16]the last article will be important.

   the idea is that in the beginning, we   ll use the epsilon greedy
   strategy:
     * we specify an exploration rate    epsilon,    which we set to 1 in the
       beginning. this is the rate of steps that we   ll do randomly. in the
       beginning, this rate must be at its highest value, because we don   t
       know anything about the values in q-table. this means we need to do
       a lot of exploration, by randomly choosing our actions.
     * we generate a random number. if this number > epsilon, then we will
       do    exploitation    (this means we use what we already know to select
       the best action at each step). else, we   ll do exploration.
     * the idea is that we must have a big epsilon at the beginning of the
       training of the q-function. then, reduce it progressively as the
       agent becomes more confident at estimating q-values.

   [1*9stlebor62fudsorwxyjrg.png]

   steps 4   5: evaluate!
   take the action a and observe the outcome state s    and reward r. now
   update the function q(s,a).

   we take the action a that we chose in step 3, and then performing this
   action returns us a new state s    and a reward r (as we saw in the
   id23 process in [17]the first article).

   then, to update q(s,a) we use the bellman equation:
   [1*jmcvwhhbzcxdc-irby9jtw.png]

   the idea here is to update our q(state, action) like this:
new q value =
   current q value +
   lr * [reward + discount_rate * (highest q value between possible actions from
 the new state s    )     current q value ]

   let   s take an example:
   [1*-3msnoxnipuicgruwvz9ng.png]
     * one cheese = +1
     * two cheese = +2
     * big pile of cheese = +10 (end of the episode)
     * if you eat rat poison =-10 (end of the episode)

   step 1: we init our q-table
   [1*uyb4uchcwfa2sylik9hnaq.png]
   the initialized q-table

   step 2: choose an action
   from the starting position, you can choose between going right or down.
   because we have a big epsilon rate (since we don   t know anything about
   the environment yet), we choose randomly. for example    move right.
   [1*iyjum__mnp-as7m5ktduya.png]
   [1*vy6vfj3rnbmi9spshouf8a.png]
   we move at random (for instance, right)

   we found a piece of cheese (+1), and we can now update the q-value of
   being at start and going right. we do this by using the bellman
   equation.

   steps 4   5: update the q-function
   [1*jmcvwhhbzcxdc-irby9jtw.png]
   [1*wzi7y0s26kw3fqtzx8hz8a.png]
     * first, we calculate the change in q value   q(start, right)
     * then we add the initial q value to the   q(start, right) multiplied
       by a learning rate.

   think of the learning rate as a way of how quickly a network abandons
   the former value for the new. if the learning rate is 1, the new
   estimate will be the new q-value.
   [1*iahknvqbregjj2jwn7fleq.png]
   the updated q-table

   good! we   ve just updated our first q value. now we need to do that
   again and again until the learning is stopped.

implement a id24 algorithm

     we made a video where we implement a id24 agent that learns to
     play taxi-v2 with numpy.

   iframe: [18]/media/0b8309ba8b5433560dba9f5ba4602484?postid=c18d0db58efe

   now that we know how it works, we   ll implement the id24 algorithm
   step by step. each part of the code is explained directly in the
   jupyter notebook below.

   you can access it in the [19]deep id23 course repo.

   or you can access it directly on google colaboratory:
   [20]q* learning with frozen lake
   colab.research.google.com

a recap   

     * id24 is a value-based id23 algorithm that
       is used to find the optimal action-selection policy using a q
       function.
     * it evaluates which action to take based on an action-value function
       that determines the value of being in a certain state and taking a
       certain action at that state.
     * goal: maximize the value function q (expected future reward given a
       state and action).
     * q table helps us to find the best action for each state.
     * to maximize the expected reward by selecting the best of all
       possible actions.
     * the q come from quality of a certain action in a certain state.
     * function q(state, action)     returns expected future reward of that
       action at that state.
     * this function can be estimated using id24, which iteratively
       updates q(s,a) using the bellman equation
     * before we explore the environment: q table gives the same arbitrary
       fixed value     but as we explore the environment     q gives us a
       better and better approximation.

   that   s all! don   t forget to implement each part of the code by
   yourself         it   s really important to try to modify the code i gave you.

   try to add epochs, change the learning rate, and use a harder
   environment (such as frozen-lake with 8x8 tiles). have fun!

   next time we   ll work on deep id24, one of the biggest
   breakthroughs in deep id23 in 2015. and we   ll train
   an agent that that plays doom and kills enemies!
   [1*q4xjhlc0iaoznnk5613psq.gif]
   doom!

   iframe: [21]/media/0c073161eb43580eaf8ce4eb75324965?postid=c18d0db58efe

   if you liked my article, please click the      below as many time as you
   liked the article so other people will see this here on medium. and
   don   t forget to follow me!

   if you have any thoughts, comments, questions, feel free to comment
   below or send me an email: hello@simoninithomas.com, or tweet me
   [22]@thomassimonini.
   [23][1*_yn1fzvefdmlobiysstizg.png]
   [24][1*md-f5vn1swyvhrzabvsu_w.png]
   [25][1*pqiptt-cdi8uwosxufn2dq.png]

   iframe: [26]/media/abe75436fcf1f39ac48a98a7ac598cc3?postid=c18d0db58efe

   keep learning, stay awesome!
     __________________________________________________________________

deep id23 course with tensorflow        

        [27]syllabus

        [28]video version

   part 1: [29]an introduction to id23

   part 2: [30]diving deeper into id23 with id24

   part 3: [31]an introduction to deep id24: let   s play doom

   part 3+: [32]improvements in deep id24: dueling double id25,
   prioritized experience replay, and fixed q-targets

   part 4: [33]an introduction to policy gradients with doom and cartpole

   part 5: [34]an intro to advantage actor critic methods: let   s play
   sonic the hedgehog!

   part 6: [35]proximal policy optimization (ppo) with sonic the hedgehog
   2 and 3

   part 7: [36]curiosity-driven learning made easy part i

     * [37]machine learning
     * [38]deep learning
     * [39]artificial intelligence
     * [40]tech
     * [41]programming

   (button)
   (button)
   (button) 11.7k claps
   (button) (button) (button) 41 (button) (button)

     (button) blockedunblock (button) followfollowing
   [42]go to the profile of thomas simonini

[43]thomas simonini

   deep learning engineer / founder of deep id23 course
   [44]https://bit.ly/2mx2mne

     (button) follow
   [45]freecodecamp.org

[46]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 11.7k
     * (button)
     *
     *

   [47]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [48]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c18d0db58efe
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_tn6vyxal7fjm---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@thomassimonini?source=post_header_lockup
  12. https://medium.freecodecamp.org/@thomassimonini
  13. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  14. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  15. https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419
  16. https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419
  17. https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419
  18. https://medium.freecodecamp.org/media/0b8309ba8b5433560dba9f5ba4602484?postid=c18d0db58efe
  19. https://github.com/simoninithomas/deep_reinforcement_learning_course/tree/master/id24/frozenlake
  20. https://colab.research.google.com/drive/17im0vx848vywfww3du-l-fcn3y1vhcgx
  21. https://medium.freecodecamp.org/media/0c073161eb43580eaf8ce4eb75324965?postid=c18d0db58efe
  22. https://twitter.com/thomassimonini
  23. https://twitter.com/thomassimonini
  24. https://www.youtube.com/channel/uc8xusf1ed9af8x8j19ha5og?view_as=subscriber
  25. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  26. https://medium.freecodecamp.org/media/abe75436fcf1f39ac48a98a7ac598cc3?postid=c18d0db58efe
  27. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  28. https://www.youtube.com/channel/uc8xusf1ed9af8x8j19ha5og?view_as=subscriber
  29. https://medium.com/p/4339519de419/edit
  30. https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe
  31. https://medium.freecodecamp.org/an-introduction-to-deep-id24-lets-play-doom-54d02d8017d8
  32. https://medium.freecodecamp.org/improvements-in-deep-id24-dueling-double-id25-prioritized-experience-replay-and-fixed-58b130cc5682
  33. https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f
  34. https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d
  35. https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e
  36. https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359
  37. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  38. https://medium.freecodecamp.org/tagged/deep-learning?source=post
  39. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  40. https://medium.freecodecamp.org/tagged/tech?source=post
  41. https://medium.freecodecamp.org/tagged/programming?source=post
  42. https://medium.freecodecamp.org/@thomassimonini?source=footer_card
  43. https://medium.freecodecamp.org/@thomassimonini
  44. https://bit.ly/2mx2mne
  45. https://medium.freecodecamp.org/?source=footer_card
  46. https://medium.freecodecamp.org/?source=footer_card
  47. https://medium.freecodecamp.org/
  48. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  50. https://colab.research.google.com/drive/17im0vx848vywfww3du-l-fcn3y1vhcgx
  51. https://medium.com/p/c18d0db58efe/share/twitter
  52. https://medium.com/p/c18d0db58efe/share/facebook
  53. https://medium.com/p/c18d0db58efe/share/twitter
  54. https://medium.com/p/c18d0db58efe/share/facebook
