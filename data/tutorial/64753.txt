   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

the deep learning(.ai) dictionary

ever struggle to recall what adam, relu or yolo mean? look no further and
check out every term you need to master deep learning.

   [16]go to the profile of jan zawadzki
   [17]jan zawadzki (button) blockedunblock (button) followfollowing
   apr 6, 2018

   surviving in the [18]coursera deep learning world means understanding
   and navigating through the jungle of technical terms. you   re not    % sure
   what adagrad, dropout, or xavier initialization mean? use this guide as
   a reference to freshen up your memory when you stumble upon a term that
   you safely parked in a dusty corner in the back of your mind.
   [1*dhaiqyd3mx2zbe7ecjlssa.gif]

   this dictionary aims to briefly explain the most important terms of the
   coursera deep learning specialization from andrew ng   s deeplearning.ai.
   it contains short explanations of the terms, accompanied by links to
   follow-up posts, images, and original papers. the post aims to be
   equally useful for deep learning beginners and practitioners.

   let   s open the encyclopedia of deep learning.     
   [1*apxmqkpdom3rtme7mksn-a.jpeg]
     __________________________________________________________________

   [19]activation function    used to create a non-linear transformation of
   the input. the inputs are multiplied by weights and added to a bias
   term. popular id180 include relu, tanh or sigmoid.
   [1*em576ipqrqpl21_ucknvha.jpeg]
   source: [20]https://bit.ly/2gbeocg

   [21]adam optimization         can be used instead of stochastic gradient
   descent optimization methods to iteratively adjust network weights.
   adam is computationally efficient, works well with large data sets, and
   requires little hyperparameter tuning, according to the [22]inventors.
   adam uses an adaptive learning rate   , instead of a predefined and
   fixed learning rate. adam is currently the default optimization
   algorithm in deep learning models.

   [23]adaptive gradient algorithm         adagrad is a id119
   optimization algorithm that features an adjustable learning rate for
   every parameter. adagrad adjusts the parameters on frequently updated
   parameters in smaller steps than for less frequently updated
   parameters. it thus fares well on very sparse data sets, e.g. for
   adapting id27s in natural language processing tasks. read the
   paper [24]here.
   [1*q0lk6b6gzvssqsdn-20zja.png]

   [25]average pooling         averages the results of a convolutional
   operation. it is often used to shrink the size of an input. average
   pooling was primarily used in older convolutional neural networks
   architectures, while recent architectures favor maximum pooling.

   [26]alexnet         a popular id98 architecture with eight layers. it is a
   more extensive network architecture than lenet and takes longer to
   train. alexnet won the 2012 id163 image classification challenge.
   read the paper [27]here.
   [1*fhurcqxjgvm9cnvmn9imla.png]
   source: [28]https://goo.gl/bvxbhl

   [29]id26    the general framework used to adjust network
   weights to minimize the id168 of a neural network. the
   algorithm travels backward through the network and adjusts the weights
   through a form of id119 of each activation function.
   [1*z__ebffssbqi90ox56eemg.gif]
   id26 travels back through the network and adjusts
   the weights.

   [30]batch id119         regular id119 optimization
   algorithm. performs parameter updates for the entire training set. the
   algorithm needs to calculate the gradients of the whole training set
   before completing a step of parameter updates. thus, batch gradient can
   be very slow for large training sets.

   [31]batch id172         normalizes the values in a neural network
   layer to values between 0 and 1. this helps train the neural network
   faster.

   [32]bias    occurs when the model does not achieve a high accuracy on the
   training set. it is also called underfitting. when a model has a high
   bias, it will generally not yield high accuracy on the test set.
   [1*3o5pvkz95nzjssyamsfdla.png]
   source: [33]https://goo.gl/htksqs

   [34]classification         when the target variable belongs to a distinct
   class, not a continuous variable. image classification, fraud detection
   or natural language processing are examples of deep learning
   classification tasks.

   [35]convolution         a mathematical operation which multiplies an input
   with a filter. convolutions are the foundation of convolutional neural
   networks, which excel at identifying edges and objects in images.
   [1*z-q6m52pzvju4a_83kvpyq.gif]

   [36]cost function         defines the difference between the calculated
   output and what it should be. cost functions are one of the key
   ingredients of learning in deep neural networks, as they form the basis
   for parameter updates. the network compares the outcome of its forward
   propagation with the ground-truth and adjusts the network weights
   accordingly to minimize the cost function. the root mean squared error
   is a simple example of a cost function.

   [37]deep neural network         a neural network with many hidden layers,
   usually more than five. it is not defined how many layers minimum a
   deep neural network has to have. deep neural networks are a powerful
   form of machine learning algorithms which are used to determine credit
   risk, steer self-driving cars and detect new planets in the universe.
   [1*fh_japo_rob4pzzdab8y-q.png]
   derivative of a function. source: [38]https://goo.gl/hqkdeg

   [39]derivative         the derivative is the slope of a function at a
   specific point. derivatives are calculated to let the id119
   algorithm adjust weight parameters towards the local minimum.

   [40]dropout         a id173 technique which randomly eliminates
   nodes and its connections in deep neural networks. dropout reduces
   overfitting and enables faster training of deep neural networks. each
   parameter update cycle, different nodes are dropped during training.
   this forces neighboring nodes to avoid relying on each other too much
   and figuring out the correct representation themselves. it also
   improves the performance of certain classification tasks. read the
   paper [41]here.
   [1*qtg_lmhdhotceb4lzo124q.png]
   source: [42]https://goo.gl/oby4l5

   [43]end-to-end learning         an algorithm is able to solve the entire task
   by itself. additional human intervention, like model switching or new
   data labeling, is not necessary. for example, [44]end-to-end driving
   means that the neural network figures out how to adjust the steering
   command just by evaluating images.

   [45]epoch    encompasses a single forward and backward pass through the
   training set for every example. a single epoch touches every training
   example in an iteration.

   [46]forward propagation         a forward pass in deep neural networks. the
   input travels through the id180 of the hidden layers
   until it produces a result at the end. forward propagation is also used
   to predict the result of an input example after the weights have been
   properly trained.

   [47]fully-connected layer         a fully-connected layer transforms an input
   with its weights and passes the result to the following layer. this
   layer has access to all inputs or activations from the previous layer.

   [48]gated recurrent unit    a gated recurrent unit (gru) conducts
   multiple transformations on the given input. it is mostly used in
   natural language processing tasks. grus prevent the vanishing gradients
   problem in id56s, similar to lstms. in contrast to lstms, grus don   t use
   a memory unit and are thus more computationally efficient while
   achieving a similar performance. read the paper [49]here.
   [1*1m6t4gmkbzf3w8itfxrr-w.png]
   no forget gate, in contrast to lstm. source: [50]https://goo.gl/duptdv

   [51]human-level performance         the best possible performance of a group
   of human experts. algorithms can exceed human-level performance.
   valuable metric to compare and improve neural network against.

   [52]hyperparameters         determine performance of your neural network.
   examples of hyperparameters are, e.g. learning rate, iterations of
   id119, number of hidden layers, or the activation function.
   not to be confused with parameters or weights, which the dnn learns
   itself.

   [53]id163         collection of thousands of images and their annotated
   classes. very useful resource for image classification tasks.
   [1*s1whbmr7wln4dviztljq5w.gif]

   [54]iteration         total number of forward and backward passes of a neural
   network. every batch counts as one pass. if your training set has 5
   batches and trains 2 epochs, then it will run 10 iterations.

   [55]id119         helps neural network decide how to adjust
   parameters to minimize the cost function. repeatedly adjust parameters
   until the global minimum is found. [56]this post contains a
   well-explained, holistic overview of different id119
   optimization methods.
   [1*jze7skobk6wivkcmto1jdw.png]
   source: [57]https://bit.ly/2jnoelr
   [1*wdpvmeqq3ew57f-rg_vdhw.png]

   [58]layer         a set of id180 which transform the input.
   neural networks use multiple hidden layers to create output. you
   generally distinguish between the input, hidden, and output layers.

   [59]learning rate decay         a concept to adjust the learning rate during
   training. allows for flexible learning rate adjustments. in deep
   learning, the learning rate typically decays the longer the network is
   trained.
   [1*sglpe-xjcsnektt95f g.png]
   max pooling.

   [60]maximum pooling         only selects the maximum values of a specific
   input area. it is often used in convolutional neural networks to reduce
   the size of the input.

   [61]long short-term memory         a special form of id56 which is able to
   learn the context of an input. while regular id56s suffer from vanishing
   gradients when corresponding inputs are located far away from each
   other, lstms can learn these long-term dependencies. read the paper
   [62]here.
   [1*u1wu_6kvzln8wls6hncdsw.png]
   input and output of an lstm unit. source: [63]https://bit.ly/2glkymf

   [64]mini-batch id119    an optimization algorithm which runs
   id119 on smaller subsets of the training data. the method
   enables parallelization as different workers separately iterate through
   different mini-batches. for every mini-batch, compute the cost and
   update the weights of the mini-batch. it   s an efficient combination of
   batch and stochastic id119.
   [1*4er2lw2htwsgd1wkzc8xoq.jpeg]
   source: [65]https://bit.ly/2iz7uob

   [66]momentum         a id119 optimization algorithm to smooth the
   oscillations of stochastic id119 methods. momentum
   calculates the average direction of the direction of the previously
   taken steps and adjusts the parameter update in this direction. imagine
   a ball rolling downhill and using this momentum when adjusting to roll
   left or right. the ball rolling downhill is an analogy to gradient
   descent finding the local minimum.

   [67]neural network         a machine learning model which transforms inputs.
   a vanilla neural network has an input, hidden, and output layer. neural
   networks have become the tool of choice for finding complex patterns in
   data.

   [68]non-max suppression         algorithm used as a part of yolo. it helps
   detect the correct bounding box of an object by eliminating overlapping
   bounding boxes with a lower confidence of identifying the object. read
   the paper [69]here.
   [1*mi6ipsd0gsdtcvkubzzi9q.jpeg]
   source:[70]https://bit.ly/2h303sf

   [71]recurrent neural networks         id56s allow the neural network to
   understand the context in speech, text or music. the id56 allows
   information to loop through the network, thus persisting important
   features of the input between earlier and later layers.
   [1*jlufn2ag9vvhp0i7kfvlqw.png]
   source: [72]https://goo.gl/nr7hf8

   [73]relu    a rectified linear unit, is a simple linear transformation
   unit where the output is zero if the input is less than zero and the
   output is equal to the input otherwise. relu is the activation function
   of choice because it allows neural networks to train faster and it
   prevents information loss.

   [74]regression    form of statistical learning where the output variable
   is a continuous instead of a categorical value. while classification
   assigns a class to the input variable, regression assigns a value that
   has an infinite number of possible values, typically a number. examples
   are the prediction of house prices or customer age.

   [75]root mean squared propagation         rmsprop is an extension of the
   stochastic id119 optimization method. the algorithm features
   a learning rate for every parameter, but not a learning rate for the
   entire training set. rmsprop adjusts the learning rate based on how
   quickly the parameters changed in previous iterations. read the paper
   here.

   [76]parameters         weights of a dnn which transform the input before
   applying the activation function. each layer has its own set of
   parameters. the parameters are adjusted through id26 to
   minimize the id168.
   [1*g1mlj3pjvekca8rorv--4w.png]
   weights of a neural network

   [77]softmax         an extension of the id28 function which
   calculates the id203 of the input belonging to every one of the
   existing classes. softmax is often used in the final layer of a dnn.
   the class with the highest id203 is chosen as the predicted
   class. it is well-suited for classification tasks with more than two
   output classes.
   [1*a-ikzvisite1lucv1loira.png]
   source: [78]https://bit.ly/2hdwzhl

   [79]stochastic id119         an optimization algorithm which
   performs a parameter update for every single training example. the
   algorithm converges usually much faster than batch id119,
   which performs a parameter update after calculating the gradients for
   the entire training set.

   [80]supervised learning         form of deep learning where an output label
   exists for every input example. the labels are used to compare the
   output of a dnn to the ground-truth values and minimize the cost
   function. other forms of deep learning tasks are semi-supervised
   training and unsupervised training.

   [81]id21         a technique to use the parameters from one
   neural network for a different task without retraining the entire
   network. use weights from a previously trained network and remove
   output layer. replace the last layer with your own softmax or logistic
   layer and train network again. works because lower layers often detect
   similar things like edges which are useful for other image
   classification tasks.

   [82]unsupervised learning         a form of machine learning where the output
   class is not known. gans or variational auto encoders are used in
   unsupervised deep learning tasks.

   [83]validation set         the validation set is used to find the optimal
   hyperparameters of a deep neural network. generally, the dnn is trained
   with different combinations of hyperparameters are tested on the
   validation set. the best performing set of hyperparameters is then
   applied to make the final prediction on the test set. pay attention to
   balancing the validation set. if lots of data is available, use as much
   as 99% for the training, 0.5% for the validation and 0.5% the test set.

   [84]vanishing gradients         the problem arises when training very deep
   neural networks. in id26, weights are adjusted based on
   their gradient, or derivative. in deep neural networks, the gradients
   of the earlier layers can become so vanishingly small, that the weights
   are not updated at all. the relu activation function is suited to
   address this problem because it doesn   t squash the input as much as
   other functions. read the paper [85]here.

   [86]variance    occurs when the dnn overfits to the training data. the
   dnn fails to distinguish noise from pattern and models every variance
   in the training data. a model with high variance usually fails to
   accurately generalize to new data.

   [87]vector         a combination of values that are passed as inputs into an
   activation layer of a dnn.
   [1*uohicgs-gl0r1kpgs8o56w.png]

   [88]vgg-16         a popular network architecture for id98s. it simplifies the
   architecture of alexnet and has a total of 16 layers. there are many
   pretrained vgg models which can be applied to novel use cases through
   id21. read the paper [89]here.

   [90]xavier initialization         xavier initialization assigns the start
   weights in the first hidden layer so that the input signals reach deep
   into the neural network. it scales the weights based on the number of
   neurons and outputs. this way, it prevents the signal from either
   becoming too small or too large later in the network.

   [91]yolo         you only look once, is an algorithm to identify objects in
   an image. convolutions are used to determine the id203 of an
   object being in a part of an image. non-max suppression and anchor
   boxes are then used to correctly locate the objects. read the paper
   [92]here.
     __________________________________________________________________

   i hope this dictionary helped you get a clearer understanding of the
   terms used in the deep learning world. keep this guide handy when
   taking the coursera deep learning specialization to quickly look up
   terms and concepts.
     __________________________________________________________________

   if you think this post was helpful, don   t forget to show your      through
                  and follow me on [93]medium and [94]linkedin to hear more about
   deep learning, online courses, id101, and life. also,
   [95]check [96]these [97]posts about the deep learning specialization
   out. please comment to share your opinion. cheers!     

     * [98]machine learning
     * [99]deep learning
     * [100]dictionary
     * [101]neural networks
     * [102]andrew ng

   (button)
   (button)
   (button) 2.9k claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [103]go to the profile of jan zawadzki

[104]jan zawadzki

   infinitely curious data scientist [105]@volkswagen group services |
   deeplearning.ai ambassador | learn & share

     (button) follow
   [106]towards data science

[107]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.9k
     * (button)
     *
     *

   [108]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [109]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ade421df39e4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/the-deep-learning-ai-dictionary-ade421df39e4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/the-deep-learning-ai-dictionary-ade421df39e4&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_rjqygml8geqi---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@janzawadzki?source=post_header_lockup
  17. https://towardsdatascience.com/@janzawadzki
  18. https://www.coursera.org/courses?languages=en&query=deep+learning
  19. https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
  20. https://bit.ly/2gbeocg
  21. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
  22. https://arxiv.org/abs/1412.6980
  23. https://www.youtube.com/watch?v=8ngkbjfbwjg
  24. http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf
  25. http://ufldl.stanford.edu/tutorial/supervised/pooling/
  26. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  27. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  28. https://goo.gl/bvxbhl
  29. http://neuralnetworksanddeeplearning.com/chap2.html
  30. https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1
  31. https://towardsdatascience.com/batch-id172-in-neural-networks-1ac91516821c
  32. https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted
  33. https://goo.gl/htksqs
  34. https://medium.com/fuzz/machine-learning-classification-models-3040f71e2529
  35. http://colah.github.io/posts/2014-07-understanding-convolutions/
  36. https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220
  37. https://deeplearning4j.org/neuralnet-overview
  38. https://goo.gl/hqkdeg
  39. https://stackoverflow.com/questions/14829785/why-derivative-of-a-function-is-used-to-calculate-local-minimum-instead-of-the-a
  40. https://machinelearningmastery.com/dropout-id173-deep-learning-models-keras/
  41. http://jmlr.org/papers/v15/srivastava14a.html
  42. https://goo.gl/oby4l5
  43. https://www.quora.com/what-is-end-to-end-learning-in-machine-learning?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
  44. https://arxiv.org/abs/1604.07316
  45. http://neuralnetworksanddeeplearning.com/chap1.html
  46. https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426
  47. http://cs231n.github.io/convolutional-networks/#fc
  48. https://www.quora.com/are-gru-gated-recurrent-unit-a-special-case-of-lstm
  49. https://arxiv.org/abs/1406.1078
  50. https://goo.gl/duptdv
  51. http://datalya.com/blog/2017/machine-learning-versus-human-level-performance
  52. https://www.quora.com/what-are-hyperparameters-in-machine-learning
  53. http://www.image-net.org/
  54. https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks
  55. https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html
  56. http://ruder.io/optimizing-gradient-descent/
  57. https://bit.ly/2jnoelr
  58. http://ufldl.stanford.edu/tutorial/supervised/multilayerneuralnetworks/
  59. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1
  60. http://ufldl.stanford.edu/tutorial/supervised/pooling/
  61. http://colah.github.io/posts/2015-08-understanding-lstms/
  62. http://www.bioinf.jku.at/publications/older/2604.pdf
  63. https://bit.ly/2glkymf
  64. https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/
  65. https://bit.ly/2iz7uob
  66. http://www.cs.bham.ac.uk/~jxb/nn/l8.pdf
  67. https://www.youtube.com/watch?v=aircaruvnkk
  68. https://www.youtube.com/watch?v=snymimfnkuy
  69. https://arxiv.org/abs/1705.02950
  70. https://bit.ly/2h303sf
  71. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  72. https://goo.gl/nr7hf8
  73. https://github.com/kulbear/deep-learning-nano-foundation/wiki/relu-and-softmax-activation-functions
  74. https://machinelearningmastery.com/linear-regression-for-machine-learning/
  75. https://www.quora.com/what-is-an-intuitive-explanation-of-rmsprop
  76. https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/
  77. http://ufldl.stanford.edu/tutorial/supervised/softmaxregression/
  78. https://bit.ly/2hdwzhl
  79. http://ufldl.stanford.edu/tutorial/supervised/optimizationstochasticgradientdescent/
  80. https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab
  81. https://machinelearningmastery.com/transfer-learning-for-deep-learning/
  82. https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294
  83. http://www.fast.ai/2017/11/13/validation-sets/
  84. http://neuralnetworksanddeeplearning.com/chap5.html
  85. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321
  86. https://elitedatascience.com/overfitting-in-machine-learning
  87. https://stackoverflow.com/questions/38379905/what-is-vector-in-terms-of-machine-learning
  88. https://www.quora.com/what-is-the-vgg-neural-network
  89. https://arxiv.org/pdf/1409.1556.pdf
  90. http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization
  91. https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006
  92. https://arxiv.org/pdf/1612.08242v1.pdf
  93. https://medium.com/@janzawadzki
  94. https://www.linkedin.com/in/jan-zawadzki/
  95. https://medium.com/machine-learning-world/netflix-or-coursera-how-to-finish-andrew-ngs-1st-deep-learning-course-in-7-days-6fa293ee83d8
  96. https://towardsdatascience.com/https-medium-com-janzawadzki-applying-andrew-ngs-1st-deep-neural-network-on-the-titanic-survival-data-set-b77edbc83816
  97. https://towardsdatascience.com/structuring-your-machine-learning-project-course-summary-in-1-picture-and-22-nuggets-of-wisdom-95b051a6c9dd
  98. https://towardsdatascience.com/tagged/machine-learning?source=post
  99. https://towardsdatascience.com/tagged/deep-learning?source=post
 100. https://towardsdatascience.com/tagged/dictionary?source=post
 101. https://towardsdatascience.com/tagged/neural-networks?source=post
 102. https://towardsdatascience.com/tagged/andrew-ng?source=post
 103. https://towardsdatascience.com/@janzawadzki?source=footer_card
 104. https://towardsdatascience.com/@janzawadzki
 105. http://twitter.com/volkswagen
 106. https://towardsdatascience.com/?source=footer_card
 107. https://towardsdatascience.com/?source=footer_card
 108. https://towardsdatascience.com/
 109. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 111. https://medium.com/p/ade421df39e4/share/twitter
 112. https://medium.com/p/ade421df39e4/share/facebook
 113. https://medium.com/p/ade421df39e4/share/twitter
 114. https://medium.com/p/ade421df39e4/share/facebook
