   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]building lang.ai
     * [9]company culture
     * [10]machine learning
     * [11]development
     * [12]about us
     __________________________________________________________________

understanding what is behind id31 (part i)

build your first sentiment classifier in 3 steps

   [13]go to the profile of enrique fueyo
   [14]enrique fueyo (button) blockedunblock (button) followfollowing
   mar 27, 2018
   [0*btx0olni9meryzcc.]
   photo by [15]jerry kiesewetter on [16]unsplash

   this article doesn   t explain the state of the art of id31
   but the fundamentals of how a computer can learn to infer the polarity
   of a given document and use it as an excuse to introduce different
   concepts used in nlp. thus, no deep technical background is needed.

introduction

   id31, sometimes called opinion mining or polarity
   detection, refers to the set of ai algorithms and techniques used to
   extract the polarity of a given document: whether the document is
   positive, negative or neutral.

   typically this polarity is represented as either a set of classes (ex.
   {positive, neutral, negative}) or a real number that represents the
   id203 that the document is positive or negative. some other
   implementations use more classes or grades between positive, negative
   and neutral (0   5 stars, 0   10 grade).

   historically, it is considered that id31 started in early
   2000   s with the articles published by [17]bo pang and lillian lee and
   by [18]peter turney.

machine learning

   machine learning represents a branch of ai that covers the algorithms
   that are able to grasp some knowledge from data (training) and build a
   model or make data-driven predictions.

   this algorithms can also be classified as:
     * supervised learning is used to denote the algorithms that, at
       training time, are presented with the input sample and its expected
       output;
     * unsupervised learning algorithms have access to the input samples
       but not to the desired outcomes.
     * in between, we can also find semi-supervised algorithms where the
       training set is composed of both: raw input samples and pairs of
       input and output samples.

   in this article we will focus on supervised id31, where
   we have a training corpus of multiple sentences labelled as either
   positive or negative. for simplicity we will omit the neutral label so
   we just have to solve a binary classification problem.

the    bag of words    model

   this model is a simplification that removes all the information
   regarding the order of the words and only pays attention to what words
   are included in the document. optionally their frequency is also taken
   into account: the number of times each word occurs in the document.

   as any simplification, it reduces the complexity of the task and makes
   it more tractable at the expense of losing some information that might
   be useful so, depending on the task you are going to perform, it may be
   helpful or not.

   the information we give up here is grammar or syntactic information as
   we have lost the order of the words (they all are mixed inside the
   bag!). however, for document classification tasks like sentiment
   analysis or topic extraction, where the outcome is typically influenced
   by a few key words that hold the semantic information, this shouldn   t
   be a huge problem specially in short and informal texts.

   as an example, for the id159 there won   t be any difference
   between the sentence    alice loves bob    and    bob loves alice   .     

our first id31 classifier

step 1: analyzing our corpus

   as mentioned, we need a corpus to train the classifier with. typically,
   your corpus will consist on a set of thousands, tens of thousands or
   more sentences labelled in one of the classes (positive-negative or
   positive-neutral-negative, 0   5 stars   ). our corpus will be much
   smaller, and to avoid any bias we will use the same number of sentences
   for each class (three sentences)

   analysis for our corpus:
     * you like that movie         positive
     * we enjoyed the meal         positive
     * i like apples         positive
     * i hate wars         negative
     * you hate apples         negative
     * that movie was terrible         negative

step 2: creating our vocabulary

   then, we have to create our vocabulary (the words the classifier has
   been trained on) from all the words that appeared in the corpus
   sentences and count how many sentences, in each class, contain each
   word.

   vocabulary: {you, like, that, movie, we, enjoyed, the, meal, i, apples,
   hate, wars, was, terrible}

   this table below is based on our limited corpus but the idea is that,
   as long as the corpus is large enough, neutral words will tend to
   appear evenly in both positive and negative sentences (for instance
   movie) and it should prevent words like was to be so negative (0/3 vs
   1/3).
+--------+---------+----------+
|        | positive| negative |
+--------+---------+----------+
| i      | 1       | 1        |
| enjoyed| 1       | 0        |
| like   | 2       | 0        |
| hate   | 0       | 1        |
| was    | 0       | 1        |
| that   | 1       | 1        |
| movie  | 1       | 1        |
| ...    | .       | .        |
+--------+---------+----------+

   positive words will tend to occur more in positive sentences as well as
   negative words with negative sentences.

step 3: predicting the sentiment

   now, for every new text, we can try to predict its sentiment given the
   words it contains. to accomplish it, we will calculate a positive score
   and a negative score by multiplying the positive and negative scores
   for each word from the table above; then we just have to check which
   score is higher.

   for numerical stability we will add a small (i.e. smoothing) constant
   number k to all the values in the table above to avoid multiplying by
   zero if a word hasn   t occur in any specific class. we will use k=0.01.

   let   s analyze the following example:

      i enjoyed that movie   
   positive = i(1.01) * enjoyed(1.01) * that(1.01) * movie(1.01) = 1.04
   negative = i(1.01) * enjoyed(0.01) * that(1.01) * movie(1.01) = 0.01

conclusion

   based on the previous result we can conclude that the sentence    i
   enjoyed that movie    is positive since the positive value (1.04) is
   bigger than the negative one (0.01).

the na  ve-bayes classifier

   [1*m9je9yibm57dkbu49ssola.jpeg]
   simple statement of bayes    theorem         photo by mattbuck

   we have used a simplified version of the [19]na  ve-bayes classifier, a
   supervised machine learning algorithms that applies [20]bayes    theorem
   supposing the independence between all the features (an individual
   property or characteristic being observed), that   s why it is called
   na  ve.

   the independence between words assumption is a huge assumption in
   language since the occurrence of a word is highly affected by the
   surrounding words in the sentence, i.e. they are not independent as we
   have just supposed. even though this simplification doesn   t hold true,
   this model still works for multiple tasks (especially for short and
   simple texts).

wrap up

   apart from the baseline we have described here, there are some
   modifications we can do to achieve better results and overcome some of
   the problems this simple model has. we will discuss a few of them in
   the next article.

   update: [21]part 2 has been published.
     __________________________________________________________________

   check the other articles in our [22]building lang.ai publication. we
   write about [23]machine learning, [24]software development, and our
   [25]company culture.

     * [26]machine learning
     * [27]id31
     * [28]ai
     * [29]artificial intelligence

   (button)
   (button)
   (button) 83 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of enrique fueyo

[31]enrique fueyo

   cto @ lang.ai

     (button) follow
   [32]building lang.ai

[33]building lang.ai

   stories from our team

     * (button)
       (button) 83
     * (button)
     *
     *

   [34]building lang.ai
   never miss a story from building lang.ai, when you sign up for medium.
   [35]learn more
   never miss a story from building lang.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://building.lang.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/eaf1bcb43d2d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-i-eaf1bcb43d2d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-i-eaf1bcb43d2d&source=--------------------------nav_reg&operation=register
   8. https://building.lang.ai/?source=logo-lo_ahpczoweefj5---46335322f971
   9. https://building.lang.ai/tagged/company-culture
  10. https://building.lang.ai/tagged/machine-learning
  11. https://building.lang.ai/tagged/development
  12. https://lang.ai/company
  13. https://building.lang.ai/@efueyo?source=post_header_lockup
  14. https://building.lang.ai/@efueyo
  15. https://unsplash.com/@jerryinocmd?utm_source=medium&utm_medium=referral
  16. https://unsplash.com/?utm_source=medium&utm_medium=referral
  17. https://www.cs.cornell.edu/home/llee/papers/sentiment.pdf
  18. http://nparc.nrc-cnrc.gc.ca/eng/view/accepted/?id=4bb7a0c8-9d9b-4ded-bcf6-fdf64ee28ccc
  19. https://en.wikipedia.org/wiki/naive_bayes_classifier
  20. https://en.wikipedia.org/wiki/bayes'_theorem
  21. https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-ii-9307926d1435
  22. http://building.lang.ai/
  23. https://building.lang.ai/tagged/machine-learning
  24. https://building.lang.ai/tagged/development
  25. https://building.lang.ai/tagged/company-culture
  26. https://building.lang.ai/tagged/machine-learning?source=post
  27. https://building.lang.ai/tagged/sentiment-analysis?source=post
  28. https://building.lang.ai/tagged/ai?source=post
  29. https://building.lang.ai/tagged/artificial-intelligence?source=post
  30. https://building.lang.ai/@efueyo?source=footer_card
  31. https://building.lang.ai/@efueyo
  32. https://building.lang.ai/?source=footer_card
  33. https://building.lang.ai/?source=footer_card
  34. https://building.lang.ai/
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/eaf1bcb43d2d/share/twitter
  38. https://medium.com/p/eaf1bcb43d2d/share/facebook
  39. https://medium.com/p/eaf1bcb43d2d/share/twitter
  40. https://medium.com/p/eaf1bcb43d2d/share/facebook
