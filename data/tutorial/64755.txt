   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]building lang.ai
     * [9]company culture
     * [10]machine learning
     * [11]development
     * [12]about us
     __________________________________________________________________

understanding what is behind id31 (part ii)

fine-tuning our sentiment classifier

   [13]go to the profile of enrique fueyo
   [14]enrique fueyo (button) blockedunblock (button) followfollowing
   apr 5, 2018

   hint! check part i first, where we introduced a simple algorithm to
   analyze the sentiment of a given document. in this article we will talk
   about different modifications that might help us improve the
   performance of our classifier.
   [15]understanding what is behind id31 (part i)
   build your first sentiment classifier in 3 stepsbuilding.lang.ai
     __________________________________________________________________

   [0*c3nmcjgvf0da8w6a.]
   photo by [16]hannes wolf on [17]unsplash

fine-tuning i: id84

   to create a good classifier with the model described in [18]part i, we
   need a big and properly labelled corpus in order to compute a
   comprehensive word-sentiment occurrence table. in the training corpus,
   there should be statistically enough examples of each word in different
   contexts so the occurrences computed in the table can leverage a good
   approximation of their real probabilities (frequencies).

   there are several techniques aimed to reduce the dimensionality of the
   problem to make it more manageable. basically they consist on reducing
   the table size (number of different words or tokens in our vocabulary)
   so the requirements about the size of the training corpus are lowered.

   if we pay attention only to the words as they appear in the text (what
   is called the form) we will learn different weights for like, likes and
   liked and we will need a much bigger corpus to learn accurate weights
   for all words. we could also collapse all these words into the single
   word token like (which is called the lemma) so we combine everything we
   have learnt from the different forms. this process is called
   lemmatization.

   if you don   t have an available lemmatizer for your language there is
   another approach, called [19]id30, which tries to obtain the root
   of a word by removing some morphological parts. for example, a stemmer
   algorithm will trim adorable, adores, adore and adorably to    ador   ,
   thus all words are treated as the same (only one row in the vocabulary
   table) . the most well known stemmer algorithm is the [20]porter
   stemmer.

   however, it is important to note that this process makes the model
   simpler (we have to deal with fewer different words) but it   s not
   guaranteed to help in all situations; it will mostly depend on your use
   case.

keep in mind

   consider the case where you want to detect the sentiment of product
   reviews and you decide to use lemmatization. a    breaking product    is
   very different from a    broken product   , however your classifier will
   treat both cases equally: break + product.

fine-tuning ii: negations

   now we will face a much more complex problem for id31:
   negations. for multiple nlp algorithms, negation has always been a
   difficult situation to deal with because, with just a single tiny word,
   we can completely invert the meaning of a whole sentence.

   let   s see the following example:

     i really do like everytime that we all gather to have lunch together
     around the table at your grandparent   s house.

     i really don   t like everytime that we all gather to have lunch
     together arround the table at your grandparent   s house.

   these two sentences are almost identical except for one word (1 out of
   20) but they have completely opposite meanings.

   even worse, we can also assume that both tokens, do and don   t, will
   tend to appear evenly in either positive and negative documents so they
   won   t help us to calculate the sentiment scores.

   a quick trick would be to transform the documents before the training
   step (aka preprocessing) as it follows:

   if we find negations like don   t or won   t, which are followed by a verb,
   we can remove them and prepend the verb with the special flag not_.
   then, a phrase like    i don   t love apples    would be transformed to    i
   not_love apples    and the word not_love would be different to the word
   love (different rows in the positive-negative table).

   hopefully this trick will help us to build a table like:
+----------+--------------+--------------+
|          | positive     | negative    |
+----------+--------------+--------------+
| ...      | ...          | ...          |
| love     | large number | small number |
| not_love | small number | large number |
| ...      | ...          | ...          |
+----------+--------------+--------------+

   for other negations, like    the movie isn   t good enough    the negation
   should be applied to the following adjective (not_good).

keep in mind

   once you want to analyze new documents, you will have to apply exactly
   the same preprocessing you have applied to the documents in the
   training set.

fine-tuning iii: id165 models

   so far we have been working with single words (or tokens, in case we
   have performed some id30) which, combined with the bag of words
   model, implied the assumption of independent words. one way to reduce
   the effect of this simplification by adding some context to the words
   is to use id165s.

   an id165 is a set of n consecutive words and we can use them as the
   building blocks of our model: the rows for the table need to compute.
   in fact, we have been using the id165 model for the specific case of n
   equals one (n=1) which is also called unigrams (for n=2 they are called
   bigrams, for n=3 trigrams, four-grams and so on   ). when dealing with
   id165s, special tokens to denote the beginning and end of a sentence
   are sometimes used.

   for instance, given the sentence    we enjoyed the meal   , the id165s
   would be:

     bigrams:
     (_,we) (we, enjoyed) (enjoyed, the) (the, meal) (meal, _)

     trigrams:
     (_, _, we) (_, we, enjoyed) (we, enjoyed, the) (enjoyed, the, meal)
     (the, meal, _) (meal, _, _)

   ideally, this could also help with negations, even without the
   previously explained preprocessing, since you could end up with bigrams
   like (don   t, love). we may also mix multiple tricks if we find that
   they work for our use case. for example, with lemmatization + negations
   + bigram model, we would get the following:

     he doesn   t really like ice-creams
     (_, he) (he. really) (really, not_like) (not_like, ice-cream)
     (ice-cream,_)

   the larger the id165s is, the more context you add, but your
   vocabulary (the number of different word combinations) grows too, which
   requires you to have a larger dataset to train on. for instance, if you
   have v words in your training set, your table will need to have v rows
   for the unigram model, roughly v   rows for the bigram model, v   for the
   trigrams, etc.

combining id165s

   we can also combine multiple id165s models, which is called backoff:
   for a given document we may use trigrams when found in the training
   table. otherwise, if a trigram is not found, we then try to use the
   bigrams or directly fallback to use unigrams.

   normally, when you have to backoff to a lower-order id165 model, we
   discount the weights as a way to give more confidence to the high-order
   id165s compared to the low-order id165s. this scaling factor can be
   constant (e.g. 0.8 for bigrams and 0.6 for unigrams) or it might depend
   on the specific id165s that we had to backoff.

wrap up

   apart from these tricks you may consider implementing other models that
   have an increased complexity. we may cover some of these models in a
   future article. stay tuned by following this publication.
     __________________________________________________________________

   check the other articles in our [21]building lang.ai publication. we
   write about [22]machine learning, [23]software development, and our
   [24]company culture.

     * [25]machine learning
     * [26]id31
     * [27]artificial intelligence
     * [28]nlp

   (button)
   (button)
   (button) 84 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of enrique fueyo

[30]enrique fueyo

   cto @ lang.ai

     (button) follow
   [31]building lang.ai

[32]building lang.ai

   stories from our team

     * (button)
       (button) 84
     * (button)
     *
     *

   [33]building lang.ai
   never miss a story from building lang.ai, when you sign up for medium.
   [34]learn more
   never miss a story from building lang.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://building.lang.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9307926d1435
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-ii-9307926d1435&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-ii-9307926d1435&source=--------------------------nav_reg&operation=register
   8. https://building.lang.ai/?source=logo-lo_sfllv7lbghql---46335322f971
   9. https://building.lang.ai/tagged/company-culture
  10. https://building.lang.ai/tagged/machine-learning
  11. https://building.lang.ai/tagged/development
  12. https://lang.ai/company
  13. https://building.lang.ai/@efueyo?source=post_header_lockup
  14. https://building.lang.ai/@efueyo
  15. https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-i-eaf1bcb43d2d
  16. https://unsplash.com/@hannes_wolf?utm_source=medium&utm_medium=referral
  17. https://unsplash.com/?utm_source=medium&utm_medium=referral
  18. https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-i-eaf1bcb43d2d
  19. https://en.wikipedia.org/wiki/id30
  20. https://tartarus.org/martin/porterstemmer/def.txt
  21. http://building.lang.ai/
  22. https://building.lang.ai/tagged/machine-learning
  23. https://building.lang.ai/tagged/development
  24. https://building.lang.ai/tagged/company-culture
  25. https://building.lang.ai/tagged/machine-learning?source=post
  26. https://building.lang.ai/tagged/sentiment-analysis?source=post
  27. https://building.lang.ai/tagged/artificial-intelligence?source=post
  28. https://building.lang.ai/tagged/nlp?source=post
  29. https://building.lang.ai/@efueyo?source=footer_card
  30. https://building.lang.ai/@efueyo
  31. https://building.lang.ai/?source=footer_card
  32. https://building.lang.ai/?source=footer_card
  33. https://building.lang.ai/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://building.lang.ai/understanding-what-is-behind-sentiment-analysis-part-i-eaf1bcb43d2d
  37. https://medium.com/p/9307926d1435/share/twitter
  38. https://medium.com/p/9307926d1435/share/facebook
  39. https://medium.com/p/9307926d1435/share/twitter
  40. https://medium.com/p/9307926d1435/share/facebook
