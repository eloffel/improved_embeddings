   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

an introduction to deep id24: let   s play doom

   [11]go to the profile of thomas simonini
   [12]thomas simonini (button) blockedunblock (button) followfollowing
   apr 11, 2018
   [1*htkkecdpwboud813vu_kjg.png]

     this article is part of deep id23 course with
     tensorflow        . check the syllabus [13]here.

   [14]last time, we learned about id24: an algorithm which produces
   a q-table that an agent uses to find the best action to take given a
   state.

   but as we   ll see, producing and updating a q-table can become
   ineffective in big state space environments.

   this article is the third part of a series of blog post about deep
   id23. for more information and more resources, check
   out the [15]syllabus of the course.

   today, we   ll create a deep q neural network. instead of using a
   q-table, we   ll implement a neural network that takes a state and
   approximates q-values for each action based on that state.

   thanks to this model, we   ll be able to create an agent that learns to
   play [16]doom!
   [1*q4xjhlc0iaoznnk5613psq.gif]
   our id25 agent

   in this article you   ll learn:
     * what is deep id24 (dql)?
     * what are the best strategies to use with dql?
     * how to handle the temporal limitation problem
     * why we use experience replay
     * what are the mathematics behind dql
     * how to implement it in tensorflow

adding    deep    to id24

   in the [17]last article, we created an agent that plays frozen lake
   thanks to the id24 algorithm.

   we implemented the id24 function to create and update a q-table.
   think of this as a    cheat-sheet    to help us to find the maximum
   expected future reward of an action, given a current state. this was a
   good strategy         however, this is not scalable.

   imagine what we   re going to do today. we   ll create an agent that learns
   to play doom. doom is a big environment with a gigantic state space
   (millions of different states). creating and updating a q-table for
   that environment would not be efficient at all.

   the best idea in this case is to create a [18]neural network that will
   approximate, given a state, the different q-values for each action.
   [1*w5guxedz9ivryqm_mluxoq.png]

how does deep id24 work?

   this will be the architecture of our deep id24:
   [1*lgleewhrvsuegpbun8_ktg.png]

   this can seem complex, but i   ll explain the architecture step by step.

   our deep q neural network takes a stack of four frames as an input.
   these pass through its network, and output a vector of q-values for
   each action possible in the given state. we need to take the biggest
   q-value of this vector to find our best action.

   in the beginning, the agent does really badly. but over time, it begins
   to associate frames (states) with best actions to do.

preprocessing part

   [1*qggnc_0bkqetpqmuftrc6a.png]

   preprocessing is an important step. we want to reduce the complexity of
   our states to reduce the computation time needed for training.

   first, we can grayscale each of our states. color does not add
   important information (in our case, we just need to find the enemy and
   kill him, and we don   t need color to find him). this is an important
   saving, since we reduce our three colors channels (rgb) to 1
   (grayscale).

   then, we crop the frame. in our example, seeing the roof is not really
   useful.

   then we reduce the size of the frame, and we we stack four sub-frames
   together.

the problem of temporal limitation

   [19]arthur juliani gives an awesome explanation about this topic in
   [20]his article. he has a clever idea: using [21]lstm neural networks
   for handling the problem.

   however, i think it   s better for beginners to use stacked frames.

   the first question that you can ask is why we stack frames together?

   we stack frames together because it helps us to handle the problem of
   temporal limitation.

   let   s take an example, in the game of pong. when you see this frame:
   [1*0lwyobh4p-jqjk19q6qyig.png]

   can you tell me where the ball is going?

   no, because one frame is not enough to have a sense of motion!

   but what if i add three more frames? here you can see that the ball is
   going to the right.
   [1*mooqjuikr_fvv2weevpr8a.png]

   that   s the same thing for our doom agent. if we give him only one frame
   at a time, it has no idea of motion. and how can it make a correct
   decision, if it can   t determine where and how fast objects are moving?

using convolution networks

   the frames are processed by three convolution layers. these layers
   allow you to exploit spatial relationships in images. but also, because
   frames are stacked together, you can exploit some spatial properties
   across those frames.

   if you   re not familiar with convolution, please read this [22]excellent
   intuitive article by [23]adam geitgey.

   each convolution layer will use elu as an activation function. elu has
   been proven to be a good [24]activation function for convolution
   layers.

   we use one fully connected layer with elu activation function and one
   output layer (a fully connected layer with a linear activation
   function) that produces the q-value estimation for each action.

experience replay: making more efficient use of observed experience

   experience replay will help us to handle two things:
     * avoid forgetting previous experiences.
     * reduce correlations between experiences.

   i will explain these two concepts.

   this part and the illustrations were inspired by the great explanation
   in the deep id24 chapter in the deep learning foundations
   nanodegree by [25]udacity.

avoid forgetting previous experiences

   we have a big problem: the variability of the weights, because there is
   high correlation between actions and states.

   remember in the first article ([26]introduction to reinforcement
   learning), we spoke about the id23 process:
   [1*akyfroemmkkybqjovlt2jq.png]

   at each time step, we receive a tuple (state, action, reward,
   new_state). we learn from it (we feed the tuple in our neural network),
   and then throw this experience.

   our problem is that we give sequential samples from interactions with
   the environment to our neural network. and it tends to forget the
   previous experiences as it overwrites with new experiences.

   for instance, if we are in the first level and then the second (which
   is totally different), our agent can forget how to behave in the first
   level.
   [1*p4lfgkliollqbkwyz_jid86.png]
   by learning how to play on water level, our agent will forget how to
   behave on the first level

   as a consequence, it can be more efficient to make use of previous
   experience, by learning with it multiple times.

   our solution: create a    replay buffer.    this stores experience tuples
   while interacting with the environment, and then we sample a small
   batch of tuple to feed our neural network.

   think of the replay buffer as a folder where every sheet is an
   experience tuple. you feed it by interacting with the environment. and
   then you take some random sheet to feed the neural network
   [1*rft8mbbkuspzdolp_wfzfa.png]

   this prevents the network from only learning about what it has
   immediately done.

reducing correlation between experiences

   we have another problem         we know that every action affects the next
   state. this outputs a sequence of experience tuples which can be highly
   correlated.

   if we train the network in sequential order, we risk our agent being
   influenced by the effect of this correlation.

   by sampling from the replay buffer at random, we can break this
   correlation. this prevents action values from oscillating or diverging
   catastrophically.

   it will be easier to understand that with an example. let   s say we play
   a first-person shooter, where a monster can appear on the left or on
   the right. the goal of our agent is to shoot the monster. it has two
   guns and two actions: shoot left or shoot right.
   [1*ixrnqjjca-wilzoe0zpojq.png]
   the table represents the q-values approximations

   we learn with ordered experience. say we know that if we shoot a
   monster, the id203 that the next monster comes from the same
   direction is 70%. in our case, this is the correlation between our
   experiences tuples.

   let   s begin the training. our agent sees the monster on the right, and
   shoots it using the right gun. this is correct!

   then the next monster also comes from the right (with 70% id203),
   and the agent will shoot with the right gun. again, this is good!

   and so on and on   
   [1*eg4hoqjjstvq9fhdfpqrkq.png]
   red gun is the action taken

   the problem is, this approach increases the value of using the right
   gun through the entire state space.
   [1*hhntnnrnfwovegkqdtrlna.png]
   we can see that the q-value for monster being at left and shooting with
   right gun is positive (even if it   s not rational)

   and if our agent doesn   t see a lot of left examples (since only 30%
   will probably come from the left), our agent will only finish by
   choosing right regardless of where the monster comes from. this is not
   rational at all.
   [1*ptdjxvig6ghn5glv_myczw.png]
   even if the monster comes to the left, our agent will shoot with the
   right gun

   we have two parallel strategies to handle this problem.

   first, we must stop learning while interacting with the environment. we
   should try different things and play a little randomly to explore the
   state space. we can save these experiences in the replay buffer.

   then, we can recall these experiences and learn from them. after that,
   go back to play with updated value function.

   as a consequence, we will have a better set of examples. we will be
   able to generalize patterns from across these examples, recalling them
   in whatever order.

   this helps avoid being fixated on one region of the state space. this
   prevents reinforcing the same action over and over.

   this approach can be seen as a form of supervised learning.

   we   ll see in future articles that we can also use    prioritized
   experience replay.    this lets us present rare or    important    tuples to
   the neural network more frequently.

our deep id24 algorithm

   first a little bit of mathematics:

   [27]remember that we update our q value for a given state and action
   using the bellman equation:
   [1*js8r4aq2zzoilk0mmp_ocg.png]

   in our case, we want to update our neural nets weights to reduce the
   error.

   the error (or td error) is calculated by taking the difference between
   our q_target (maximum possible value from the next state) and q_value
   (our current prediction of the q-value)
   [1*zplt-1wtwu_7bgmzcbfjbq.png]
initialize doom environment e
initialize replay memory m with capacity n (= finite capacity)
initialize the id25 weights w
for episode in max_episode:
    s = environment state
    for steps in max_steps:
         choose action a from state s using epsilon greedy.
         take action a, get r (reward) and s' (next state)
         store experience tuple <s, a, r, s'> in m
         s = s' (state = new_state)

         get random minibatch of exp tuples from m
         set q_target = reward(s,a) +    maxq(s')
         update w =    (q_target - q_value) *     w q_value

   there are two processes that are happening in this algorithm:
     * we sample the environment where we perform actions and store the
       observed experiences tuples in a replay memory.
     * select the small batch of tuple random and learn from it using a
       id119 update step.

let   s implement our deep q neural network

     we made a video where we implement a deep id24 agent with
     tensorflow that learns to play atari space invaders            .

   iframe: [28]/media/5ca312476022e3a71d10caf83004a9aa?postid=54d02d8017d8

   now that we know how it works, we   ll implement our deep q neural
   network step by step. each step and each part of the code is explained
   directly in the jupyter notebook linked below.

   you can access it in the [29]deep id23 course repo.

   iframe: [30]/media/e08468c8c26f372d35b964de2b8308b5?postid=54d02d8017d8

   that   s all! you   ve just created an agent that learns to play doom.
   awesome!

   don   t forget to implement each part of the code by yourself. it   s
   really important to try to modify the code i gave you. try to add
   epochs, change the architecture, add fixed q-values, change the
   learning rate, use a harder environment (such as health gathering)   and
   so on.have fun!

   in the next article, i will discuss the last improvements in deep
   id24:
     * fixed q-values
     * prioritized experience replay
     * double id25
     * dueling networks

   but next time we   ll work on policy gradients by training an agent that
   plays doom, and we   ll try to survive in an hostile environment by
   collecting health.
   [1*dnez6gx3fp4dclj59xrnfq.gif]

   iframe: [31]/media/0c073161eb43580eaf8ce4eb75324965?postid=54d02d8017d8

   if you liked my article, please click the      below as many time as you
   liked the article so other people will see this here on medium. and
   don   t forget to follow me!

   if you have any thoughts, comments, questions, feel free to comment
   below or send me an email: hello@simoninithomas.com, or tweet me
   [32]@thomassimonini.
   [33][1*_yn1fzvefdmlobiysstizg.png]
   [34][1*md-f5vn1swyvhrzabvsu_w.png]
   [35][1*pqiptt-cdi8uwosxufn2dq.png]

   iframe: [36]/media/abe75436fcf1f39ac48a98a7ac598cc3?postid=54d02d8017d8

   keep learning, stay awesome!
     __________________________________________________________________

deep id23 course with tensorflow        

        [37]syllabus

        [38]video version

   part 1: [39]an introduction to id23

   part 2: [40]diving deeper into id23 with id24

   part 3: [41]an introduction to deep id24: let   s play doom

   part 3+: [42]improvements in deep id24: dueling double id25,
   prioritized experience replay, and fixed q-targets

   part 4: [43]an introduction to policy gradients with doom and cartpole

   part 5: [44]an intro to advantage actor critic methods: let   s play
   sonic the hedgehog!

   part 6: [45]proximal policy optimization (ppo) with sonic the hedgehog
   2 and 3

   part 7: [46]curiosity-driven learning made easy part i

     * [47]machine learning
     * [48]deep learning
     * [49]artificial intelligence
     * [50]programming
     * [51]tech

   (button)
   (button)
   (button) 7.9k claps
   (button) (button) (button) 36 (button) (button)

     (button) blockedunblock (button) followfollowing
   [52]go to the profile of thomas simonini

[53]thomas simonini

   deep learning engineer / founder of deep id23 course
   [54]https://bit.ly/2mx2mne

     (button) follow
   [55]freecodecamp.org

[56]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 7.9k
     * (button)
     *
     *

   [57]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [58]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/54d02d8017d8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/an-introduction-to-deep-id24-lets-play-doom-54d02d8017d8&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/an-introduction-to-deep-id24-lets-play-doom-54d02d8017d8&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_a4syi2lkgsis---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@thomassimonini?source=post_header_lockup
  12. https://medium.freecodecamp.org/@thomassimonini
  13. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  14. https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe
  15. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  16. https://en.wikipedia.org/wiki/doom_(1993_video_game)
  17. https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe
  18. http://neuralnetworksanddeeplearning.com/
  19. https://medium.com/@awjuliani
  20. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2
  21. http://colah.github.io/posts/2015-08-understanding-lstms/
  22. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  23. https://medium.com/@ageitgey
  24. https://arxiv.org/pdf/1511.07289.pdf
  25. https://eu.udacity.com/
  26. https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419
  27. https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe
  28. https://medium.freecodecamp.org/media/5ca312476022e3a71d10caf83004a9aa?postid=54d02d8017d8
  29. https://github.com/simoninithomas/deep_reinforcement_learning_course/tree/master/id25/doom
  30. https://medium.freecodecamp.org/media/e08468c8c26f372d35b964de2b8308b5?postid=54d02d8017d8
  31. https://medium.freecodecamp.org/media/0c073161eb43580eaf8ce4eb75324965?postid=54d02d8017d8
  32. https://twitter.com/thomassimonini
  33. https://twitter.com/thomassimonini
  34. https://www.youtube.com/channel/uc8xusf1ed9af8x8j19ha5og?view_as=subscriber
  35. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  36. https://medium.freecodecamp.org/media/abe75436fcf1f39ac48a98a7ac598cc3?postid=54d02d8017d8
  37. https://simoninithomas.github.io/deep_reinforcement_learning_course/
  38. https://www.youtube.com/channel/uc8xusf1ed9af8x8j19ha5og?view_as=subscriber
  39. https://medium.com/p/4339519de419/edit
  40. https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-id24-c18d0db58efe
  41. https://medium.freecodecamp.org/an-introduction-to-deep-id24-lets-play-doom-54d02d8017d8
  42. https://medium.freecodecamp.org/improvements-in-deep-id24-dueling-double-id25-prioritized-experience-replay-and-fixed-58b130cc5682
  43. https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f
  44. https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d
  45. https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e
  46. https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359
  47. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  48. https://medium.freecodecamp.org/tagged/deep-learning?source=post
  49. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  50. https://medium.freecodecamp.org/tagged/programming?source=post
  51. https://medium.freecodecamp.org/tagged/tech?source=post
  52. https://medium.freecodecamp.org/@thomassimonini?source=footer_card
  53. https://medium.freecodecamp.org/@thomassimonini
  54. https://bit.ly/2mx2mne
  55. https://medium.freecodecamp.org/?source=footer_card
  56. https://medium.freecodecamp.org/?source=footer_card
  57. https://medium.freecodecamp.org/
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. https://medium.com/p/54d02d8017d8/share/twitter
  61. https://medium.com/p/54d02d8017d8/share/facebook
  62. https://medium.com/p/54d02d8017d8/share/twitter
  63. https://medium.com/p/54d02d8017d8/share/facebook
