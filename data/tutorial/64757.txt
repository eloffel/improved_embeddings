   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*uw8vvfjtcjrbpu9p0q0iog.jpeg]

introduction to bayesian id75

an explanation of the bayesian approach to linear modeling

   [16]go to the profile of will koehrsen
   [17]will koehrsen (button) blockedunblock (button) followfollowing
   apr 13, 2018

   the [18]bayesian vs frequentist debate is one of those academic
   arguments that i find more interesting to watch than engage in. rather
   than enthusiastically jump in on one side, i think it   s more productive
   to learn both methods of [19]statistical id136 and apply them where
   appropriate. in that line of thinking, recently, i have been working to
   learn and apply bayesian id136 methods to supplement the
   frequentist statistics covered in my grad classes.

   one of my first areas of focus in applied bayesian id136 was
   bayesian linear modeling. the most important part of the learning
   process might just be explaining an idea to others, and this post is my
   attempt to introduce the concept of bayesian id75. we   ll
   do a brief review of the frequentist approach to id75,
   introduce the bayesian interpretation, and look at some results applied
   to a simple dataset. i kept the code out of this article, but it can be
   found on [20]github in a jupyter notebook.

recap of frequentist id75

   the frequentist view of id75 is probably the one you are
   familiar with from school: the model assumes that the response variable
   (y) is a linear combination of weights multiplied by a set of predictor
   variables (x). the full formula also includes an error term to account
   for random sampling noise. for example, if we have two predictors, the
   equation is:
   [1*umoui8s8awqieozeuoq0rq.png]

   y is the response variable (also called the dependent variable),      s
   are the weights (known as the model parameters), x   s are the values of
   the predictor variables, and    is an [21]error term representing random
   sampling noise or the effect of variables not included in the model.

   id75 is a simple model which makes it easily
   interpretable:   _0 is the intercept term and the other weights,      s,
   show the effect on the response of increasing a predictor variable. for
   example, if   _1 is 1.2, then for every unit increase in x_1,the
   response will increase by 1.2.

   we can generalize the linear model to any number of predictors using
   matrix equations. adding a constant term of 1 to the predictor matrix
   to account for the intercept, we can write the matrix formula as:
   [1*mkaaql--30i1a4y3zfvelw.png]

   the goal of learning a linear model from training data is to find the
   coefficients,   , that best explain the data. in frequentist linear
   regression, the best explanation is taken to mean the coefficients,   ,
   that minimize the residual sum of squares (rss). rss is the total of
   the squared differences between the known values (y) and the predicted
   model outputs (  , pronounced y-hat indicating an estimate). the
   residual sum of squares is a function of the model parameters:
   [1*gc1jko6knbj_r7qsssn38w.png]

   the summation is taken over the n data points in the training set. we
   won   t go into the details here ([22]check out this reference for the
   derivation), but this equation has a closed form solution for the model
   parameters,   , that minimize the error. this is known as the
   [23]maximum likelihood estimate of    because it is the value that is
   the most probable given the inputs, x, and outputs, y. the closed form
   solution expressed in matrix form is:
   [1*vdmgx4ts7irs4ejdfudqjq.png]

   (again, we have to put the    hat    on    because it represents an estimate
   for the model parameters.) don   t let the matrix math scare you off!
   thanks to libraries like [24]scikit-learn in python, we generally don   t
   have to calculate this by hand (although it is good practice to code a
   id75). this method of fitting the model parameters by
   minimizing the rss is called [25]ordinary least squares (ols).

   what we obtain from frequentist id75 is a single estimate
   for the model parameters based only on the training data. our model is
   completely informed by the data: in this view, everything that we need
   to know for our model is encoded in the training data we have
   available.

   once we have   -hat, we can estimate the output value of any new data
   point by applying our model equation:
   [1*oqc4nygnco_8ow-a3x9rla.png]

   as an example of ols, we can perform a id75 on real-world
   data which has duration and calories burned for 15000 exercise
   observations. below is the data and ols model obtained by solving the
   above matrix equation for the model parameters:
   [1*6rjr9xogzdlbwumexk-ctq.png]

   with ols, we get a single estimate of the model parameters, in this
   case, the intercept and slope of the line.we can write the equation
   produced by ols:
calories = -21.83 + 7.17 * duration

   from the slope, we can say that every additional minute of exercise
   results in 7.17 additional calories burned. the intercept in this case
   is not as helpful, because it tells us that if we exercise for 0
   minutes, we will burn -21.86 calories! this is just an artifact of the
   ols fitting procedure, which finds the line that minimizes the error on
   the training data regardless of whether it physically makes sense.

   if we have a new datapoint, say an exercise duration of 15.5 minutes,
   we can plug it into the equation to get a point estimate of calories
   burned:

   calories = -21.83 + 7.17 * 15.5 = 89.2

   ordinary least squares gives us a single point estimate for the output,
   which we can interpret as the most likely estimate given the data.
   however, if we have a small dataset we might like to express our
   estimate as a distribution of possible values. this is where bayesian
   id75 comes in.

bayesian id75

   in the bayesian viewpoint, we formulate id75 using
   id203 distributions rather than point estimates. the response, y,
   is not estimated as a single value, but is assumed to be drawn from a
   id203 distribution. the model for bayesian id75 with
   the response sampled from a normal distribution is:
   [1*jnludqc9nwqkt3t9hrgqiw.png]

   the output, y is generated from a normal (gaussian) distribution
   characterized by a mean and variance. the mean for id75 is
   the transpose of the weight matrix multiplied by the predictor matrix.
   the variance is the square of the standard deviation    (multiplied by
   the identity matrix because this is a multi-dimensional formulation of
   the model).

   the aim of bayesian id75 is not to find the single    best   
   value of the model parameters, but rather to determine the posterior
   distribution for the model parameters. not only is the response
   generated from a id203 distribution, but the model parameters are
   assumed to come from a distribution as well. the posterior id203
   of the model parameters is conditional upon the training inputs and
   outputs:
   [1*jnxtbqdzzfcafc1pxvzuqq.png]

   here, p(  |y, x) is the posterior id203 distribution of the model
   parameters given the inputs and outputs. this is equal to the
   likelihood of the data, p(y|  , x), multiplied by the prior id203
   of the parameters and divided by a id172 constant. this is a
   simple expression of id47, the fundamental underpinning of
   bayesian id136:
   [1*uo2k-_mzasjwf_w0xzuywg.png]

   let   s stop and think about what this means. in contrast to ols, we have
   a posterior distribution for the model parameters that is proportional
   to the likelihood of the data multiplied by the prior id203 of
   the parameters. here we can observe the two primary benefits of
   bayesian id75.
    1. priors: if we have domain knowledge, or a guess for what the model
       parameters should be, we can include them in our model, unlike in
       the frequentist approach which assumes everything there is to know
       about the parameters comes from the data. if we don   t have any
       estimates ahead of time, we can use [26]non-informative priors for
       the parameters such as a normal distribution.
    2. posterior: the result of performing bayesian id75 is a
       distribution of possible model parameters based on the data and the
       prior. this allows us to quantify our uncertainty about the model:
       if we have fewer data points, the posterior distribution will be
       more spread out.

   as the amount of data points increases, the likelihood washes out the
   prior, and in the case of infinite data, the outputs for the parameters
   converge to the values obtained from ols.

   the formulation of model parameters as distributions encapsulates the
   bayesian worldview: we start out with an initial estimate, our prior,
   and as we gather more evidence, our model becomes less wrong. bayesian
   reasoning is a natural extension of our intuition. often, we have an
   initial hypothesis, and as we collect data that either supports or
   disproves our ideas, we change our model of the world (ideally this is
   how we would reason)!

implementing bayesian id75

   in practice, evaluating the posterior distribution for the model
   parameters is intractable for continuous variables, so we use sampling
   methods to draw samples from the [27]posterior in order to approximate
   the posterior. the technique of drawing random samples from a
   distribution to approximate the distribution is one application of
   [28]monte carlo methods. there are a number of algorithms for monte
   carlo sampling, with the most common being [29]variants of markov chain
   monte carlo (see this [30]post for an application in python).

bayesian linear modeling application

   i   ll skip the code for this post (see the notebook for the
   implementation in pymc3) but the basic procedure for implementing
   bayesian id75 is: specify priors for the model parameters
   (i used normal distributions in this example), creating a model mapping
   the training inputs to the training outputs, and then have a markov
   chain monte carlo (mcmc) algorithm draw samples from the posterior
   distribution for the model parameters. the end result will be posterior
   distributions for the parameters. we can inspect these distributions to
   get a sense of what is occurring.

   the first plots show the approximations of the posterior distributions
   of model parameters. these are the result of 1000 steps of mcmc,
   meaning the algorithm drew 1000 steps from the posterior distribution.
   [1*igkqvaktk2ccrhkezowagq.png]
   [1*zeahpqkqdkc8xeak64hcqw.png]

   if we compare the mean values for the slope and intercept to those
   obtained from ols (the intercept from ols was -21.83 and the slope was
   7.17), we see that they are very similar. however, while we can use the
   mean as a single point estimate, we also have a range of possible
   values for the model parameters. as the number of data points
   increases, this range will shrink and converge one a single value
   representing greater confidence in the model parameters. (in bayesian
   id136 a range for a variable is called a credible interval and
   which has a [31]slightly different interpretation from a confidence
   interval in frequentist id136).

   when we want show the linear fit from a bayesian model, instead of
   showing only estimate, we can draw a range of lines, with each one
   representing a different estimate of the model parameters. as the
   number of datapoints increases, the lines begin to overlap because
   there is less uncertainty in the model parameters.

   in order to demonstrate the effect of the number of datapoints in the
   model, i used two models, the first, with the resulting fits shown on
   the left, used 500 datapoints and the one on the right used 15000
   datapoints. each graph shows 100 possible models drawn from the model
   parameter posteriors.
   [1*8ba09thsc_cy5leijm8oea.png]
   [1*c8er-v648on7nb11emqwlg.png]
   bayesian id75 model results with 500 (left) and 15000
   observations (right)

   there is much more variation in the fits when using fewer data points,
   which represents a greater uncertainty in the model. with all of the
   data points, the ols and bayesian fits are nearly identical because the
   priors are washed out by the likelihoods from the data.

   when predicting the output for a single datapoint using our bayesian
   linear model, we also do not get a single value but a distribution.
   following is the id203 density plot for the number of calories
   burned exercising for 15.5 minutes. the red vertical line indicates the
   point estimate from ols.
   [1*vkbwqdqfz_crz1c2ew9dxa.png]
   posterior id203 density of calories burned from bayesian model

   we see that the id203 of the number of calories burned peaks
   around 89.3, but the full estimate is a range of possible values.

conclusions

   instead of taking sides in the bayesian vs frequentist debate (or any
   argument), it is more constructive to learn both approaches. that way,
   we can apply them in the right situation.

   in problems where we have limited data or have some prior knowledge
   that we want to use in our model, the bayesian id75
   approach can both incorporate prior information and show our
   uncertainty. bayesian id75 reflects the bayesian
   framework: we form an initial estimate and improve our estimate as we
   gather more data. the bayesian viewpoint is an intuitive way of looking
   at the world and [32]bayesian id136 can be a useful alternative to
   its frequentist counterpart. data science is not about taking sides,
   but about figuring out the best tool for the job, and having more
   techniques in your repertoire only makes you more effective!

   as always, i welcome feedback and constructive criticism. i can be
   reached on twitter [33]@koehrsen_will.

   sources
    1. [34]https://www.quantstart.com/articles/bayesian-linear-regression-
       models-with-pymc3
    2. [35]http://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/
    3. [36]https://wiseodd.github.io/techblog/2017/01/05/bayesian-regressi
       on/
    4. [37]pymc3 introduction

     * [38]data science
     * [39]education
     * [40]statistics
     * [41]learning
     * [42]towards data science

   (button)
   (button)
   (button) 5.8k claps
   (button) (button) (button) 18 (button) (button)

     (button) blockedunblock (button) followfollowing
   [43]go to the profile of will koehrsen

[44]will koehrsen

   data scientist at cortex intel, data science communicator

     (button) follow
   [45]towards data science

[46]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 5.8k
     * (button)
     *
     *

   [47]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [48]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e66e60791ea7
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_yrpfjo26gj70---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@williamkoehrsen?source=post_header_lockup
  17. https://towardsdatascience.com/@williamkoehrsen
  18. http://noahpinionblog.blogspot.com/2013/01/bayesian-vs-frequentist-is-there-any.html
  19. https://en.wikipedia.org/wiki/statistical_id136
  20. https://github.com/willkoehrsen/data-analysis/blob/master/bayesian_lr/bayesian id75 demonstration.ipynb
  21. https://stats.stackexchange.com/questions/129055/understanding-the-error-term
  22. http://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf
  23. https://www.quantstart.com/articles/maximum-likelihood-estimation-for-linear-regression
  24. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.linearregression.html
  25. https://en.wikipedia.org/wiki/ordinary_least_squares
  26. https://stats.stackexchange.com/questions/27813/what-is-the-point-of-non-informative-priors
  27. https://stats.stackexchange.com/questions/307882/why-is-it-necessary-to-sample-from-the-posterior-distribution-if-we-already-know
  28. https://en.wikipedia.org/wiki/monte_carlo_method#applied_statistics
  29. https://s3.amazonaws.com/academia.edu.documents/39690347/750.pdf?awsaccesskeyid=akiaiwowyygz2y53ul3a&expires=1523672050&signature=t6fjd5fhdmfjbaltl1/uqggoz9y=&response-content-disposition=inline; filename=markov_chain_monte_carlo_algorithms_for.pdf
  30. https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98
  31. https://en.wikipedia.org/wiki/credible_interval
  32. https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-id203-and-statistics-spring-2014/readings/mit18_05s14_reading20.pdf
  33. http://twitter.com/@koehrsen_will
  34. https://www.quantstart.com/articles/bayesian-linear-regression-models-with-pymc3
  35. http://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/
  36. https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/
  37. https://pdfs.semanticscholar.org/8085/b60ce1771647f11ccc4728397275b502f359.pdf
  38. https://towardsdatascience.com/tagged/data-science?source=post
  39. https://towardsdatascience.com/tagged/education?source=post
  40. https://towardsdatascience.com/tagged/statistics?source=post
  41. https://towardsdatascience.com/tagged/learning?source=post
  42. https://towardsdatascience.com/tagged/towards-data-science?source=post
  43. https://towardsdatascience.com/@williamkoehrsen?source=footer_card
  44. https://towardsdatascience.com/@williamkoehrsen
  45. https://towardsdatascience.com/?source=footer_card
  46. https://towardsdatascience.com/?source=footer_card
  47. https://towardsdatascience.com/
  48. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  50. https://medium.com/p/e66e60791ea7/share/twitter
  51. https://medium.com/p/e66e60791ea7/share/facebook
  52. https://medium.com/p/e66e60791ea7/share/twitter
  53. https://medium.com/p/e66e60791ea7/share/facebook
