   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id4         using id195 with keras

translation from english to french using encoder-decoder model

   [16]go to the profile of ravindra kompella
   [17]ravindra kompella (button) blockedunblock (button) followfollowing
   jan 8, 2018
   [1*u_gg8uj0zh5j2pu_ojkbpa.jpeg]

   this article is motivated by this [18]keras example and [19]this paper
   on encoder-decoder network. the idea is to gain intuitive and detailed
   understanding from this example. my own implementation of this example
   referenced in this story is provided at my [20]github link.

   before we start, it may help to go through [21]my other post on lstm
   that helps in understanding the fundamentals of lstms specifically in
   this context.

   below is the detailed network architecture used for training the
   id195 encoder         decoder network. we will refer this figure through
   out.
   [1*1i2ttjckmhlq-r73ern4zq.png]
   fig a    encoder-decoder training architecture for id4         image
   copyright@[22]ravindra kompella

   firstly we will go about training the network. then we will look at the
   id136 models on how to translate a given english sentence to
   french. id136 model (used for predicting on the input sequence) has
   a slightly different decoder architecture and we will discuss that in
   detail when we come there.

   training the network    

   so how does the training data look?
     * we have 10,000 english sentences and corresponding 10,000
       translated french sentences. so our nb_samples = 10000.

   overall plan for training    
    1. create one-hot character embeddings for english and french
       sentences. these will be the inputs to the encoder and the decoder.
       the french one-hot character embeds will also be used as target
       data for id168.
    2. feed character by character embeds into the encoder till the end of
       the english sentence sequence.
    3. obtain the final encoder states (hidden and cell states) and feed
       them into the decoder as its initial state.
    4. decoder will have 3 inputs at every time step         2 decoder states
       and the french character embeds fed to it character by character.
    5. at every step of the decoder, the output of the decoder is sent to
       softmax layer that is compared with the target data.

   detailed flow for training the network with code    

   refer to snippet 1         note that we have appended    \t    for start of the
   french sentence and    \n    to signify end of the french sentence. these
   appended french sentences will be used as inputs to decoder. all the
   english characters and french characters are collected in separate
   sets. these sets are converted to character level dictionaries (useful
   for retrieving the index and character values later).
   [1*5ctpdizhis-osfi-xszkqq.png]
   snippet 1

   refer to snippet 2    prepare the embeds for encoder input, decoder input
   and the target data embeds. we will create one-hot encoding for each
   character in english and french separately. these are called as
   tokenized_eng_sentences and tokenized_fra_sentences in the code
   snippet. these will be the inputs to encoder and decoder respectively.
   note that the target_data french character embeds that we compare at
   the softmax layer output are offset by (t+1) compared to the decoder
   input embeds (because there is no start tag in target data         refer to
   the above architecture diagram for more clarity). hence the target_data
   in the below code snippet is accordingly offset ( note k-1 in second
   dimension of the target_data array below)
   [1*-ujfgal2wc3rvnbchi0wxa.png]
   snippet 2.

   refer to snippet 2         as we noted in [23]my other post on lstm, the
   embeds (tokenized_eng_sentences and tokenized_fra_sentences and
   target_data) are 3d arrays. the first dimension corresponds to
   nb_samples ( =10,000 in this case). the second dimension corresponds to
   the maximum length of english / french sentence and the third dimension
   corresponds to total number of english / french characters.

   refer to snippet 3: we will input character by character (of-course,
   their corresponding one hot embeds) into the encoder network. for the
   encoder_lstm, we had set return_state = true . we did not do
   return_sequences = true (and by default this is set to false). this
   would mean that we obtain only the final encoded cell state and the
   encoded hidden state at the end of the input sequence and not the
   intermediate states at every time step. these will be the final encoded
   states that are used to initialize the state of the decoder.
   [1*6rduajw7575awz8wpsrmpw.png]
   snippet 3    encoder model for training

   refer to snippet 3         also note that the input shape has been specified
   as (none, len(eng_chars)). this means the encoder lstm can dynamically
   unroll that many timesteps as the number of characters till it reaches
   the end of sequence for that sentence.

   refer to snippet 4         inputs to the decoder will be the french character
   embeds (contained in tokenized_fra_sentences array) one by one at each
   time step along with the previous state values. the previous states for
   the first step of the decoder will be initialized with the final
   encoder states that we collected earlier in snippet 3. for this reason,
   note that the initial_state=encoder_states has been set in the below
   code snippet. from the subsequent step on wards the state inputs to
   decoder will be its cell state and its hidden state.
   [1*jartb9j6thvxoi0-zfrmwq.png]
   snippet 4         decoder model for training

   also from the above code snippet, notice that the decoder is setup with
   return_sequences = true along with return_state = true. so we obtain
   decoder output and the two decoder states at every timestep. while
   return_state = true has been declared here, we are not going to use the
   decoder states while training the model. the reason for its presence is
   that they will be used while building the decoder id136 model (that
   we will see later). the decoder output is passed through the softmax
   layer that will learn to classify the correct french character.

   refer to snippet 5         the id168 is categorical cross id178
   that is obtained by comparing the predicted values from softmax layer
   with the target_data (one-hot french character embeds).

   now the model is ready for training. train the entire network for the
   specified number of epochs.
   [1*7plhqnrrurbzt21kqxmmeq.png]
   snippet 5         y = target_data ( containing one hot french character
   embeds)

   testing (id136 mode)    

   below is the architecture used for id136 models    the id136
   model will leverage all the network parameters learnt during training
   but we define them separately because the inputs and outputs during
   id136 are different from what they were during training the
   network.

   from the below figure, observe that there are no changes on the encoder
   side of the network. so we feed the new english sentence (one hot
   character embedded) vector as input sequence to the encoder model and
   obtain the final encoding states.
   [1*nyptruttvd9xujwl-cvl3q.png]
   fig b    encoder-decoder id136 model architecture for id4    image
   copyright @[24]ravindra kompella

   contrast this figure b with figure a on the decoder side. the major
   changes can be seen are as below    
     * at the first time step, the decoder has 3 inputs         the start tag
          \t    and the two encoder states. we input the first character as
          \t    ( its one hot embed vector) into the first time step of the
       decoder.
     * then the decoder outputs the first predicted character (assume it
       is    v   ).
     * observe how the blue lines are getting connected back into the
       decoder input for the next time step. so this predicted character
          v    will be fed as an input to the decoder at the next timestep.
     * also note that we only obtain the one hot embed vector of the
       predicted character using the np.argmax function on the output of
       the softmax layer at each timestep. so we do a reverse dictionary
       lookup on the index to obtain the actual character    v   .
     * from next time step on wards the decoder still has 3 inputs but
       different from the first time step . they being         one hot encode of
       previous predicted character, previous decoder cell state and the
       previous decoder hidden state

   given the above understanding, now lets look at the code    

   refer to snippet 6         the encoder id136 model is quite
   straightforward. this is going to output only the encoder_states.
   [1*-txzxjtxzcsmvojemxudca.png]
   snippet 6         encoder id136 model

   refer to snippet 7         the decoder model is more elaborate. note that we
   create separate    input    for decoder hidden state and decoder cell
   state. this is because we are going to feed these states at every time
   step (other than the first time step         recall that at the first time
   step we feed only the encoder states) into the decoder and the decoder
   id136 model is a separate standalone model. both the encoder and
   decoder will be called recursively for each character that is to be
   generated in the translated sequence.
   [1*lcdbhwmg2ybnzrc_e3e8dw.png]
   snippet 7         decoder id136 model

   refer to snippet 8    we get the encoder states into states_val variable.
   on the first call inside the while loop, these hidden and cell states
   from the encoder will be used to initialize the decoder_model_inf that
   are provided as input to the model directly. once we predict the
   character using softmax, we now input this predicted character ( using
   the target_seq 3d array for one-hot embed of the predicted character)
   along with the updated states_val (updated from the previous decoder
   states) for the next iteration of the while loop. note that we reset
   our target_seq before we create a one-hot embed of the predicted
   character every time in the while loop.
   [1*dyhkywndliyskr9tgyebjg.png]
   snippet 8         function to recursively call the decoder for predicting the
   translated character sequence.

   that   s it! now we have a trained model that can translate english
   sentences to french! below are the results obtained after training the
   network for 25 epochs.
   [1*rr1ujta-5gpjn37cuay57q.png]
   results obtained using some sample training data

   if you plan to use any of the above architecture diagram figures,
   please feel free to do so and request you to mention my name in the
   image credit.

   please show your applause by holding the clapping icon, if you find any
   useful takeaway from this article.

     * [25]machine learning
     * [26]artificial intelligence
     * [27]nlp
     * [28]language learning

   (button)
   (button)
   (button) 874 claps
   (button) (button) (button) 19 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of ravindra kompella

[30]ravindra kompella

   ai, ml, id161@endovigilant, app dev@
   [31]https://itunes.apple.com/pg/developer/ravindra-kompella/ 93234511
   , [32]https://github.com/kmsravindra

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 874
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c23540453c74
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/neural-machine-translation-using-id195-with-keras-c23540453c74&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/neural-machine-translation-using-id195-with-keras-c23540453c74&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_dgmjjrid113jmn---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@kmsravindra123?source=post_header_lockup
  17. https://towardsdatascience.com/@kmsravindra123
  18. https://github.com/keras-team/keras/blob/master/examples/lstm_id195.py
  19. https://arxiv.org/abs/1409.3215
  20. https://goo.gl/bebhgq
  21. https://medium.com/@kmsravindra123/lstm-nuggets-for-practical-applications-5beef5252092
  22. https://medium.com/@kmsravindra123
  23. https://medium.com/@kmsravindra123/lstm-nuggets-for-practical-applications-5beef5252092
  24. https://medium.com/@kmsravindra123
  25. https://towardsdatascience.com/tagged/machine-learning?source=post
  26. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  27. https://towardsdatascience.com/tagged/nlp?source=post
  28. https://towardsdatascience.com/tagged/language-learning?source=post
  29. https://towardsdatascience.com/@kmsravindra123?source=footer_card
  30. https://towardsdatascience.com/@kmsravindra123
  31. https://itunes.apple.com/pg/developer/ravindra-kompella/ 93234511
  32. https://github.com/kmsravindra
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/c23540453c74/share/twitter
  39. https://medium.com/p/c23540453c74/share/facebook
  40. https://medium.com/p/c23540453c74/share/twitter
  41. https://medium.com/p/c23540453c74/share/facebook
