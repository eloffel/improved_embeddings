   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    an overview of id173 techniques in deep
   learning (with python code) comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]an overview of id173 techniques
   in deep learning (with python code)

   [95]deep learning[96]python

an overview of id173 techniques in deep learning (with python code)

   [97]shubham jain, april 19, 2018

introduction

   one of the most common problem data science professionals face is to
   avoid overfitting. have you come across a situation where your model
   performed exceptionally well on train data, but was not able to predict
   test data. or you were on the top of a competition in public
   leaderboard, only to fall hundreds of places in the final rankings?
   well     this is the article for you!

   avoiding overfitting can single-handedly improve our model   s
   performance.

   in this article, we will understand the concept of overfitting and how
   id173 helps in overcoming the same problem. we will then look
   at a few different id173 techniques and take a case study in
   python to further solidify these concepts.

   note: this article assumes that you have basic knowledge of neural
   networks and its implementation in keras. if not, you can refer to the
   below articles first:
     * [98]fundamentals of deep learning     starting with artificial neural
       network
     * [99]tutorial: optimizing neural networks using keras (with image
       recognition case study)


table of contents

    1. what is id173?
    2. how does id173 help in reducing overfitting?
    3. different id173 techniques in deep learning
          + l2 and l1 id173
          + dropout
          + data augmentation
          + early stopping
    4. case study on mnist data using keras

what is id173?

   before we deep dive into the topic, take a look at this image:

   have you seen this image before? as we move towards the right in this
   image, our model tries to learn too well the details and the noise from
   the training data, which ultimately results in poor performance on the
   unseen data.

   in other words, while going towards the right, the complexity of the
   model increases such that the training error reduces but the testing
   error doesn   t. this is shown in the image below.

   source: slideplayer

   if you   ve built a neural network before, you know how complex they are.
   this makes them more prone to overfitting.

   id173 is a technique which makes slight modifications to the
   learning algorithm such that the model generalizes better. this in turn
   improves the model   s performance on the unseen data as well.


how does id173 help reduce overfitting?

   let   s consider a neural network which is overfitting on the training
   data as shown in the image below.

   if you have studied the concept of id173 in machine learning,
   you will have a fair idea that id173 penalizes the
   coefficients. in deep learning, it actually penalizes the weight
   matrices of the nodes.

   assume that our id173 coefficient is so high that some of the
   weight matrices are nearly equal to zero.

   this will result in a much simpler linear network and slight
   underfitting of the training data.

   such a large value of the id173 coefficient is not that
   useful. we need to optimize the value of id173 coefficient in
   order to obtain a well-fitted model as shown in the image below.


different id173 techniques in deep learning

   now that we have an understanding of how id173 helps in
   reducing overfitting, we   ll learn a few different techniques in order
   to apply id173 in deep learning.


l2 & l1 id173

   l1 and l2 are the most common types of id173. these update the
   general cost function by adding another term known as the
   id173 term.

   cost function = loss (say, binary cross id178) + id173 term

   due to the addition of this id173 term, the values of weight
   matrices decrease because it assumes that a neural network with smaller
   weight matrices leads to simpler models. therefore, it will also reduce
   overfitting to quite an extent.

   however, this id173 term differs in l1 and l2.

   in l2, we have:

   here, lambda is the id173 parameter. it is the hyperparameter
   whose value is optimized for better results. l2 id173 is also
   known as weight decay as it forces the weights to decay towards zero
   (but not exactly zero).

   in l1, we have:

   in this, we penalize the absolute value of the weights. unlike l2, the
   weights may be reduced to zero here. hence, it is very useful when we
   are trying to compress our model. otherwise, we usually prefer l2 over
   it.

   in keras, we can directly apply id173 to any layer using the
   [100]regularizers.

   below is the sample code to apply l2 id173 to a dense layer.
from keras import regularizers
model.add(dense(64, input_dim=64,
                kernel_regularizer=regularizers.l2(0.01)

   note: here the value 0.01 is the value of id173 parameter,
   i.e., lambda, which we need to optimize further. we can optimize it
   using the [101]grid-search method.

   similarly, we can also apply l1 id173. we will look at this in
   more detail in a case study later in this article.


dropout

   this is the one of the most interesting types of id173
   techniques. it also produces very good results and is consequently the
   most frequently used id173 technique in the field of deep
   learning.

   to understand dropout, let   s say our neural network structure is akin
   to the one shown below:

   so what does dropout do? at every iteration, it randomly selects some
   nodes and removes them along with all of their incoming and outgoing
   connections as shown below.

   so each iteration has a different set of nodes and this results in a
   different set of outputs. it can also be thought of as an ensemble
   technique in machine learning.

   ensemble models usually perform better than a single model as they
   capture more randomness. similarly, dropout also performs better than a
   normal neural network model.

   this id203 of choosing how many nodes should be dropped is the
   hyperparameter of the dropout function. as seen in the image above,
   dropout can be applied to both the hidden layers as well as the input
   layers.

   source: chatbotslife

   due to these reasons, dropout is usually preferred when we have a large
   neural network structure in order to introduce more randomness.

   in keras, we can implement dropout using the [102]keras core layer.
   below is the python code for it:
from keras.layers.core import dropout

model = sequential([
 dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu
'),
 dropout(0.25),

dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='soft
max'),
 ])

   as you can see, we have defined 0.25 as the id203 of dropping. we
   can tune it further for better results using the grid search method.

data augmentation

   the simplest way to reduce overfitting is to increase the size of the
   training data. in machine learning, we were not able to increase the
   size of training data as the labeled data was too costly.

   but, now let   s consider we are dealing with images. in this case, there
   are a few ways of increasing the size of the training data     rotating
   the image, flipping, scaling, shifting, etc. in the below image, some
   transformation has been done on the handwritten digits dataset.

   this technique is known as data augmentation. this usually provides a
   big leap in improving the accuracy of the model. it can be considered
   as a mandatory trick in order to improve our predictions.

   in keras, we can perform all of these transformations using
   [103]imagedatagenerator. it has a big list of arguments which you you
   can use to pre-process your training data.

   below is the sample code to implement it.
from keras.preprocessing.image import imagedatagenerator
datagen = imagedatagenerator(horizontal flip=true)
datagen.fit(train)

early stopping

   early stopping is a kind of cross-validation strategy where we keep one
   part of the training set as the validation set. when we see that the
   performance on the validation set is getting worse, we immediately stop
   the training on the model. this is known as early stopping.

   in the above image, we will stop training at the dotted line since
   after that our model will start overfitting on the training data.

   in keras, we can apply early stopping using the [104]callbacks
   function. below is the sample code for it.
from keras.callbacks import earlystopping

earlystopping(monitor='val_err', patience=5)

   here, monitor denotes the quantity that needs to be monitored and
      val_err    denotes the validation error.

   patience denotes the number of epochs with no further improvement after
   which the training will be stopped. for better understanding, let   s
   take a look at the above image again. after the dotted line, each epoch
   will result in a higher value of validation error. therefore, 5 epochs
   after the dotted line (since our patience is equal to 5), our model
   will stop because no further improvement is seen.

   note: it may be possible that after 5 epochs (this is the value defined
   for patience in general), the model starts improving again and the
   validation error starts decreasing as well. therefore, we need to take
   extra care while tuning this hyperparameter.


a case study on mnist data with keras

   by this point, you should have a theoretical understanding of the
   different techniques we have gone through. we will now apply this
   knowledge to our deep learning practice problem     [105]identify the
   digits. once you have downloaded the dataset, start following the below
   code! first, we   ll import some of the basic libraries.
%pylab inline

import numpy as np
import pandas as pd
from scipy.misc import imread
from sklearn.metrics import accuracy_score

from matplotlib import pyplot

import tensorflow as tf
import keras

# to stop potential randomness
seed = 128
rng = np.random.randomstate(seed)

   now, let   s load the dataset.
root_dir = os.path.abspath('/users/shubhamjain/downloads/av/identify the digits/
')
data_dir = os.path.join(root_dir, 'data')
sub_dir = os.path.join(root_dir, 'sub')

## reading train file only
train = pd.read_csv(os.path.join(data_dir, 'train', 'train.csv'))
train.head()

   take a look at some of our images now.

   img_name = rng.choice(train.filename)
filepath = os.path.join(data_dir, 'train', 'images', 'train', img_name)

img = imread(filepath, flatten=true)

pylab.imshow(img, cmap='gray')
pylab.axis('off')
pylab.show()

#storing images in numpy arrays
temp = []
for img_name in train.filename:
 image_path = os.path.join(data_dir, 'train', 'images', 'train', img_name)
 img = imread(image_path, flatten=true)
 img = img.astype('float32')
 temp.append(img)

x_train = np.stack(temp)

x_train /= 255.0
x_train = x_train.reshape(-1, 784).astype('float32')

y_train = keras.utils.np_utils.to_categorical(train.label.values)

   create a validation dataset, in order to optimize our model for better
   scores. we will go with a 70:30 train and validation dataset ratio.
split_size = int(x_train.shape[0]*0.7)

x_train, x_test = x_train[:split_size], x_train[split_size:]
y_train, y_test = y_train[:split_size], y_train[split_size:]

   first, let   s start with building a simple neural network with 5 hidden
   layers, each having 500 nodes.
# import keras modules
from keras.models import sequential
from keras.layers import dense


# define vars
input_num_units = 784
hidden1_num_units = 500
hidden2_num_units = 500
hidden3_num_units = 500
hidden4_num_units = 500
hidden5_num_units = 500
output_num_units = 10

epochs = 10
batch_size = 128

model = sequential([
 dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu
'),
 dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='re
lu'),
 dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='re
lu'),
 dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='re
lu'),
 dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='re
lu'),

dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='soft
max'),
 ])

   note that we are just running it for 10 epochs. let   s quickly check the
   performance of our model.
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['accur
acy'])

trained_model_5d = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch
_size, validation_data=(x_test, y_test))


   now, let   s try the l2 regularizer over it and check whether it gives
   better results than a simple neural network model.
from keras import regularizers

model = sequential([
 dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu
',
 kernel_regularizer=regularizers.l2(0.0001)),
 dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l2(0.0001)),
 dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l2(0.0001)),
 dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l2(0.0001)),
 dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l2(0.0001)),

dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='soft
max'),
 ])
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['accur
acy'])

trained_model_5d = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch
_size, validation_data=(x_test, y_test))


   note that the value of lambda is equal to 0.0001. great! we just
   obtained an accuracy which is greater than our previous nn model.

   now, let   s try the l1 id173 technique.
## l1

model = sequential([
 dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu
',
 kernel_regularizer=regularizers.l1(0.0001)),
 dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l1(0.0001)),
 dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l1(0.0001)),
 dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l1(0.0001)),
 dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='re
lu',
 kernel_regularizer=regularizers.l1(0.0001)),

dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='soft
max'),
 ])
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['accur
acy'])
trained_model_5d = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch
_size, validation_data=(x_test, y_test))

   this doesn   t show any improvement over the previous model. let   s jump
   to the dropout technique.
## dropout

from keras.layers.core import dropout
model = sequential([
 dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu
'),
 dropout(0.25),
 dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='re
lu'),
 dropout(0.25),
 dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='re
lu'),
 dropout(0.25),
 dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='re
lu'),
 dropout(0.25),
 dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='re
lu'),
 dropout(0.25),

dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='soft
max'),
 ])
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['accur
acy'])
trained_model_5d = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch
_size, validation_data=(x_test, y_test))

   not bad! dropout also gives us a little improvement over our simple nn
   model.

   now, let   s try data augmentation.
from keras.preprocessing.image import imagedatagenerator
datagen = imagedatagenerator(zca_whitening=true)
# loading data
train = pd.read_csv(os.path.join(data_dir, 'train', 'train.csv'))
temp = []
for img_name in train.filename:
 image_path = os.path.join(data_dir, 'train', 'images', 'train', img_name)
 img = imread(image_path, flatten=true)
 img = img.astype('float32')
 temp.append(img)

x_train = np.stack(temp)

x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)

x_train = x_train.astype('float32')


   now, fit the training data in order to augment.
# fit parameters from data
datagen.fit(x_train)

   here, i have used zca_whitening as the argument, which highlights the
   outline of each digit as shown in the image below.

## splitting
y_train = keras.utils.np_utils.to_categorical(train.label.values)
split_size = int(x_train.shape[0]*0.7)

x_train, x_test = x_train[:split_size], x_train[split_size:]
y_train, y_test = y_train[:split_size], y_train[split_size:]
## reshaping
x_train=np.reshape(x_train,(x_train.shape[0],-1))/255
x_test=np.reshape(x_test,(x_test.shape[0],-1))/255
## structure using dropout
from keras.layers.core import dropout
model = sequential([
 dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu
'),
 dropout(0.25),
 dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='re
lu'),
 dropout(0.25),
 dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='re
lu'),
 dropout(0.25),
 dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='re
lu'),
 dropout(0.25),
 dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='re
lu'),
 dropout(0.25),

dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='soft
max'),
 ])
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['accur
acy'])
trained_model_5d = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch
_size, validation_data=(x_test, y_test))

   wow! we got a big leap in the accuracy score. and the good thing is
   that it works every time. we just need to select a proper argument
   depending upon the images we have in our dataset.

   now, let   s try our final technique     early stopping.
from keras.callbacks import earlystopping
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['accur
acy'])
trained_model_5d = model.fit(x_train, y_train, nb_epoch=epochs, batch_size=batch
_size, validation_data=(x_test, y_test)
 , callbacks = [earlystopping(monitor='val_acc', patience=2)])

   you can see that our model stops after only 5 iterations as the
   validation accuracy was not improving. it gives good results in cases
   where we run it for a larger value of epochs. you can say that it   s a
   technique to optimize the value of the number of epochs.


end notes

   i hope that now you have an understanding of id173 and the
   different techniques  required to implement it in deep learning models.
   i highly recommend applying it whenever you are dealing with a deep
   learning task. it will help you expand your horizons and gain a better
   understanding of the topic.

   did you find this article helpful? please share your opinions/thoughts
   in the comments section below.

[106]learn, [107]engage [108], compete and [109]get hired!

   you can also read this article on analytics vidhya's android app
   [110]get it on google play

share this:

     * [111]click to share on linkedin (opens in new window)
     * [112]click to share on facebook (opens in new window)
     * [113]click to share on twitter (opens in new window)
     * [114]click to share on pocket (opens in new window)
     * [115]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [116]data augmentation, [117]dropout, [118]early stopping,
   [119]improving accuracy of deep learning models, [120]l1
   id173, [121]l2 id173, [122]mnist data,
   [123]optimizing neural networks, [124]id173,
   [125]id173 in deep learning
   next article

cars.com is using machine learning to predict the sales of cars

   previous article

an introduction to id207 and network analysis (with python codes)

[126]shubham jain

   i am currently pursing my b.tech in ceramic engineering from iit
   (b.h.u) varanasi. i am an aspiring data scientist and a ml enthusiast.
   i am really passionate about changing the world by using artificial
   intelligence.
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [127]discussion portal to get your queries resolved

2 comments

     * pramod says:
       [128]april 28, 2018 at 9:15 am
       thanks a lot bro. immensely helpful.
       [129]reply
     * chetan says:
       [130]may 1, 2018 at 11:55 pm
       could we achieve the same validation accuracy with much lesser
       architecture ?
       if so, then our hyperparameters (especially the l2 lambda) must be
       quite high (say 0.05). isn   t it ?
       or, is there any other way of combating this ??
       [131]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [132]srk       3924
   2    [2.jpg?date=2019-04-06] [133]mark12    3510
   3    [3.jpg?date=2019-04-06] [134]nilabha   3261
   4    [4.jpg?date=2019-04-06] [135]nitish007 3237
   5    [5.jpg?date=2019-04-06] [136]tezdhar   3082
   [137]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [138]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [139]understanding support vector machine algorithm from examples
       (along with code)
     * [140]essentials of machine learning algorithms (with python and r
       codes)
     * [141]a complete tutorial to learn data science with python from
       scratch
     * [142]7 types of regression techniques you should know!
     * [143]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [144]a simple introduction to anova (with applications in excel)
     * [145]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [146]top 5 machine learning github repositories and reddit discussions
   from march 2019

[147]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [148]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[149]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [150]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[151]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [152]16 opencv functions to start your id161 journey (with
   python code)

[153]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [154][ds-finhack.jpg]

   [155][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [156]about us
     * [157]our team
     * [158]career
     * [159]contact us
     * [160]write for us

   [161]about us
   [162]   
   [163]our team
   [164]   
   [165]careers
   [166]   
   [167]contact us

data scientists

     * [168]blog
     * [169]hackathon
     * [170]discussions
     * [171]apply jobs
     * [172]leaderboard

companies

     * [173]post jobs
     * [174]trainings
     * [175]hiring hackathons
     * [176]advertising
     * [177]reach us

   don't have an account? [178]sign up here.

join our community :

   [179]46336 [180]followers
   [181]20224 [182]followers
   [183]followers
   [184]7513 [185]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [186]privacy policy
     * [187]terms of use
     * [188]refund policy

   don't have an account? [189]sign up here

   iframe: [190]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [191](button) join now

   subscribe!

   iframe: [192]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [193](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/author/shubham-jain/
  98. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  99. https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/
 100. https://keras.io/regularizers/
 101. http://scikit-learn.org/stable/modules/grid_search.html
 102. https://keras.io/layers/core/#dropout
 103. https://keras.io/preprocessing/image/
 104. https://keras.io/callbacks/
 105. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/
 106. https://www.analyticsvidhya.com/blog
 107. http://discuss.analyticsvidhya.com/
 108. https://datahack.analyticsvidhya.com/
 109. https://www.analyticsvidhya.com/jobs/#/user/
 110. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 111. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/?share=linkedin
 112. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/?share=facebook
 113. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/?share=twitter
 114. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/?share=pocket
 115. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/?share=reddit
 116. https://www.analyticsvidhya.com/blog/tag/data-augmentation/
 117. https://www.analyticsvidhya.com/blog/tag/dropout/
 118. https://www.analyticsvidhya.com/blog/tag/early-stopping/
 119. https://www.analyticsvidhya.com/blog/tag/improving-accuracy-of-deep-learning-models/
 120. https://www.analyticsvidhya.com/blog/tag/l1-id173/
 121. https://www.analyticsvidhya.com/blog/tag/l2-id173/
 122. https://www.analyticsvidhya.com/blog/tag/mnist-data/
 123. https://www.analyticsvidhya.com/blog/tag/optimizing-neural-networks/
 124. https://www.analyticsvidhya.com/blog/tag/id173/
 125. https://www.analyticsvidhya.com/blog/tag/id173-in-deep-learning/
 126. https://www.analyticsvidhya.com/blog/author/shubham-jain/
 127. https://discuss.analyticsvidhya.com/
 128. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/#comment-152893
 129. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/#comment-152893
 130. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/#comment-152971
 131. https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-id173-techniques/#comment-152971
 132. https://datahack.analyticsvidhya.com/user/profile/srk
 133. https://datahack.analyticsvidhya.com/user/profile/mark12
 134. https://datahack.analyticsvidhya.com/user/profile/nilabha
 135. https://datahack.analyticsvidhya.com/user/profile/nitish007
 136. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 137. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 138. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 139. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 140. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 141. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 142. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 143. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 144. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 145. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 146. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 147. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 148. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 149. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 150. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 151. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 152. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 153. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 154. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 155. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 156. http://www.analyticsvidhya.com/about-me/
 157. https://www.analyticsvidhya.com/about-me/team/
 158. https://www.analyticsvidhya.com/career-analytics-vidhya/
 159. https://www.analyticsvidhya.com/contact/
 160. https://www.analyticsvidhya.com/about-me/write/
 161. http://www.analyticsvidhya.com/about-me/
 162. https://www.analyticsvidhya.com/about-me/team/
 163. https://www.analyticsvidhya.com/about-me/team/
 164. https://www.analyticsvidhya.com/about-me/team/
 165. https://www.analyticsvidhya.com/career-analytics-vidhya/
 166. https://www.analyticsvidhya.com/about-me/team/
 167. https://www.analyticsvidhya.com/contact/
 168. https://www.analyticsvidhya.com/blog
 169. https://datahack.analyticsvidhya.com/
 170. https://discuss.analyticsvidhya.com/
 171. https://www.analyticsvidhya.com/jobs/
 172. https://datahack.analyticsvidhya.com/users/
 173. https://www.analyticsvidhya.com/corporate/
 174. https://trainings.analyticsvidhya.com/
 175. https://datahack.analyticsvidhya.com/
 176. https://www.analyticsvidhya.com/contact/
 177. https://www.analyticsvidhya.com/contact/
 178. https://datahack.analyticsvidhya.com/signup/
 179. https://www.facebook.com/analyticsvidhya/
 180. https://www.facebook.com/analyticsvidhya/
 181. https://twitter.com/analyticsvidhya
 182. https://twitter.com/analyticsvidhya
 183. https://plus.google.com/+analyticsvidhya
 184. https://in.linkedin.com/company/analytics-vidhya
 185. https://in.linkedin.com/company/analytics-vidhya
 186. https://www.analyticsvidhya.com/privacy-policy/
 187. https://www.analyticsvidhya.com/terms/
 188. https://www.analyticsvidhya.com/refund-policy/
 189. https://id.analyticsvidhya.com/accounts/signup/
 190. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 191. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 192. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 193. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 195. https://www.facebook.com/analyticsvidhya
 196. https://twitter.com/analyticsvidhya
 197. https://plus.google.com/+analyticsvidhya/posts
 198. https://in.linkedin.com/company/analytics-vidhya
 199. https://www.analyticsvidhya.com/blog/2018/04/cars-com-is-using-machine-learning-to-predict-the-sales-of-cars/
 200. https://www.analyticsvidhya.com/blog/2018/04/introduction-to-graph-theory-network-analysis-python-codes/
 201. https://www.analyticsvidhya.com/blog/author/shubham-jain/
 202. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#d6a5bea3b4beb7bbf8bcb7bfb8f8b5b3a4e7e296bfa2b4bea3f8b7b5f8bfb8
 203. https://www.linkedin.com/in/shubham-jain-25a104108/
 204. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 205. https://www.facebook.com/analyticsvidhya/
 206. https://twitter.com/analyticsvidhya
 207. https://plus.google.com/+analyticsvidhya
 208. https://plus.google.com/+analyticsvidhya
 209. https://in.linkedin.com/company/analytics-vidhya
 210. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 211. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 212. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 213. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 214. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 215. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 216. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 217. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 218. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 219. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 220. javascript:void(0);
 221. javascript:void(0);
 222. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 223. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 224. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 225. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 226. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 227. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 228. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 229. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 230. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 231. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2018%2f04%2ffundamentals-deep-learning-id173-techniques%2f&linkname=an%20overview%20of%20id173%20techniques%20in%20deep%20learning%20%28with%20python%20code%29
 232. javascript:void(0);
 233. javascript:void(0);
