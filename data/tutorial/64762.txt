   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    web scraping in python using scrapy (with
   multiple examples) comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]business intelligence [94]web scraping in python using
   scrapy (with multiple examples)

   [95]business intelligence[96]python[97]web analytics

web scraping in python using scrapy (with multiple examples)

   [98]mohd sanad zaki rizvi, july 25, 2017

introduction

   the explosion of the internet has been a boon for [99]data science
   enthusiasts. the variety and quantity of data that is available today
   through the internet is like a treasure trove of secrets and mysteries
   waiting to be solved. for example, you are planning to travel     how
   about scraping a few travel recommendation sites, pull out comments
   about various do to things and see which property is getting a lot of
   positive responses from the users! the list of use cases is endless.

   yet, there is no fixed methodology to extract such data and much of it
   is unstructured and full of noise.

   such conditions make web scraping a necessary technique for a data
   scientist   s toolkit. as it is rightfully said,

     any content that can be viewed on a webpage can be scraped. period.

   with the same spirit, you will be building different kinds of web
   scraping systems using [100]python in this article and will learn some
   of the challenges and ways to tackle them.

   by end of this article, you would know a framework to scrape the web
   and would have scrapped multiple websites     let   s go!


table of contents

    1. overview of scrapy
    2. write your first web scraping code with scrapy
         1. set up your system
         2. scraping reddit: fast experimenting with scrapy shell
         3. writing custom scrapy spiders
    3. case studies using scrapy
         1. scraping an e-commerce site
         2. scraping techcrunch: create your own rss feed reader


1. overview of scrapy

   scrapy is a [101]python framework for large scale web scraping. it
   gives you all the tools you need to efficiently extract data from
   websites, process them as you want, and store them in your preferred
   structure and format.

   as diverse the internet is, there is no    one size fits all    approach in
   extracting data from websites. many a time ad hoc approaches are taken
   and if you start writing code for every little task you perform, you
   will eventually end up creating your own scraping framework. scrapy is
   that framework.

     with scrapy you don   t need to reinvent the wheel.

   note: there are no specific prerequisites of this article, a basic
   knowledge of html and css is preferred. if you still think you need a
   refresher, do a quick read of [102]this article.


2. write your first web scraping code with scrapy

   we will first quickly take a look at how to setup your system for web
   scraping and then see how we can build a simple web scraping system for
   extracting data from reddit website.


2.1 set up your system

   scrapy supports both versions of python 2 and 3. if you   re using
   anaconda, you can install the package from the conda-forge channel,
   which has up-to-date packages for linux, windows and os x.

   to install scrapy using conda, run:
conda install -c conda-forge scrapy

   alternatively, if you   re on linux or mac osx, you can directly install
   scrapy by:
pip install scrapy

   note: this article will follow python 2 with scrapy.


2.2 scraping reddit: fast experimenting with scrapy shell

   recently there was a season launch of a prominent tv series (gots7) and
   the social media was on fire, people all around were posting memes,
   theories, their reactions etc. i had just learnt scrapy and was
   wondering if it can be used to catch a glimpse of people   s reactions?


scrapy shell

   i love the python shell, it helps me    try out    things before i can
   implement them in detail. similarly, scrapy provides a shell of its own
   that you can use to experiment. to start the scrapy shell in your
   command line type:
scrapy shell

   woah! scrapy wrote a bunch of stuff. for now, you don   t need to worry
   about it. in order to get information from reddit (about got) you will
   have to first run a crawler on it. a crawler is a program that browses
   web sites and downloads content. sometimes crawlers are also referred
   as spiders.


about reddit

   [103]reddit is a discussion forum website. it allows users to create
      subreddits     for a single topic of discussion. it supports all the
   features that conventional discussion portals have like creating a
   post, voting, replying to post, including images and links etc. reddit
   also ranks the post based on their votes using a ranking algorithm of
   its own.

   a crawler needs a starting point to start crawling(downloading) content
   from. let   s see, on googling    game of thrones reddit    i found that
   reddit has a sub-reddit exclusively for game of thrones at
   [104]https://www.reddit.com/r/gameofthrones/ this will be the crawler   s
   start url.

   to run the crawler in the shell type:
fetch("[105]https://www.reddit.com/r/gameofthrones/")

   when you crawl something with scrapy it returns a    response    object
   that contains the downloaded information. let   s see what the crawler
   has downloaded:
view(response)

   this command will open the downloaded page in your default browser.

   wow that looks exactly like the website, the crawler has successfully
   downloaded the entire web page.

   let   s see how does the raw content looks like:
print response.text

   that   s a lot of content but not all of it is relevant. let   s create
   list of things that need to be extracted :
     * title of each post
     * number of votes it has
     * number of comments
     * time of post creation


extracting title of posts

   scrapy provides ways to extract information from html based on css
   selectors like class, id etc. let   s find the css selector for title,
   right click on any post   s title and select    inspect    or    inspect
   element   :

   this will open the the developer tools in your browser:

   as it can be seen,  the css class    title    is applied to all <p> tags
   that have titles. this will helpful in filtering out titles from rest
   of the content in the response object:
response.css(".title::text").extract()

   here response.css(..) is a function that helps extract content based on
   css selector passed to it. the    .    is used with the title because it   s
   a css . also you need to use ::text to tell your scraper to extract
   only text content of the matching elements. this is done because scrapy
   directly returns the matching element along with the html code. look at
   the following two examples:

   notice how    ::text    helped us filter and extract only the text content.


extracting vote counts for each post

   now this one is tricky, on inspecting, you get three scores:

   the    score    class is applied to all the three so it can   t be used as a
   unique selector is required. on further inspection, it can be seen that
   the selector that uniquely matches the vote count that we need is the
   one that contains both    score    and    unvoted   .

   when more than two selectors are required to identify an element, we
   use them both. also since both are css classes we have to use    .    with
   their names. let   s try it out first by extracting the first element
   that matches:
response.css(".score.unvoted").extract_first()

   see that the number of votes of the first post is correctly displayed.
   note that on reddit, the votes score is dynamic based on the number of
   upvotes and downvotes, so it   ll be changing in real time. we will add
      ::text    to our selector so that we only get the vote value and not the
   complete vote element. to fetch all the votes:
response.css(".score.unvoted::text").extract()

   note: scrapy has two functions to extract the content extract() and
   extract_first().


dealing with relative time stamps: extracting time of post creation

   on inspecting the post it is clear that the    time    element contains the
   time of the post.

   there is a catch here though, this is only the relative time(16 hours
   ago etc.) of the post. this doesn   t give any information about the date
   or time zone the time is in. in case we want to do some analytics, we
   won   t be able to know by which date do we have to calculate    16 hours
   ago   . let   s inspect the time element a little more:

   the    title    attribute of time has both the date and the time in utc.
   let   s extract this instead:
response.css("time::attr(title)").extract()

   the .attr(attributename) is used to get the value of the specified
   attribute of the matching element.


 extracting number of comments:

   i leave this as a practice assignment for you. if you have any issues,
   you can post them here: [106]https://discuss.analyticsvidhya.com/ and
   the community will help you out      .

   so far:
     * response     an object that the scrapy crawler returns. this object
       contains all the information about the downloaded content.
     * response.css(..)     matches the element with the given css
       selectors.
     * extract_first(..)     extracts the    first    element that matches the
       given criteria.
     * extract(..)     extracts    all    the elements that match the given
       criteria.


   note: css selectors are a very important concept as far as web scraping
   is considered, you can read more about it [107]here and how to [108]use
   css selectors with scrapy.


2.3 writing custom spiders

   as mentioned above, a spider is a program that downloads content from
   web sites or a given url. when extracting data on a larger scale, you
   would need to write custom spiders for different websites since there
   is no    one size fits all    approach in web scraping owing to diversity
   in website designs. you also would need to write code to convert the
   extracted data to a structured format and store it in a reusable format
   like csv, json, excel etc. that   s a lot of code to write, luckily
   scrapy comes with most of these functionality built in.


creating a scrapy project

   let   s exit the scrapy shell first and create a new scrapy project:
scrapy startproject ourfirstscraper

   this will create a folder    ourfirstscraper    with the following
   structure:

   for now, the two most important files are:
     * settings.py     this file contains the settings you set for your
       project, you   ll be dealing a lot with it.
     * spiders/     this folder is where all your custom spiders will be
       stored. every time you ask scrapy to run a spider, it will look for
       it in this folder.


creating a spider

   let   s change directory into our first scraper and create a basic spider
      redditbot    :
scrapy genspider redditbot [109]www.reddit.com/r/gameofthrones/

   this will create a new spider    redditbot.py    in your spiders/ folder
   with a basic template:

   few things to note here:
     * name : name of the spider, in this case it is    redditbot   . naming
       spiders properly becomes a huge relief when you have to maintain
       hundreds of spiders.
     * allowed_domains : an optional list of strings containing domains
       that this spider is allowed to crawl. requests for urls not
       belonging to the domain names specified in this list won   t be
       followed.
     * parse(self, response) : this function is called whenever the
       crawler successfully crawls a url. remember the response object
       from earlier? this is the same response object that is passed to
       the parse(..).

   after every successful crawl the parse(..) method is called and so
   that   s where you write your extraction logic. let   s add the earlier
   logic wrote earlier to extract titles, time, votes etc. in the parse
   function:
def parse(self, response):
        #extracting the content using css selectors
        titles = response.css('.title.may-blank::text').extract()
        votes = response.css('.score.unvoted::text').extract()
        times = response.css('time::attr(title)').extract()
        comments = response.css('.comments::text').extract()

        #give the extracted content row wise
        for item in zip(titles,votes,times,comments):
            #create a dictionary to store the scraped info
            scraped_info = {
                'title' : item[0],
                'vote' : item[1],
                'created_at' : item[2],
                'comments' : item[3],
            }

            #yield or give the scraped info to scrapy
            yield scraped_info


   note: here yield scraped_info does all the magic. this line returns the
   scraped info(the dictionary of votes, titles, etc.) to scrapy which in
   turn processes it and stores it.

   save the file redditbot.py and head back to shell. run the spider with
   the following command:
scrapy crawl redditbot

   scrapy would print a lot of stuff on the command line. let   s focus on
   the data.

   notice that all the data is downloaded and extracted in a dictionary
   like object that meticulously has the votes, title, created_at and
   comments.


exporting scraped data as a csv

   getting all the data on the command line is nice but as a data
   scientist, it is preferable to have data in certain formats like csv,
   excel, json etc. that can be imported into programs. scrapy provides
   this nifty little functionality where you can export the downloaded
   content in various formats. many of the popular formats are already
   supported.

   open the settings.py file and add the following code to it:
#export as csv feed
feed_format = "csv"
feed_uri = "reddit.csv"

   and run the spider :
scrapy crawl redditbot

   this will now export all scraped data in a file reddit.csv. let   s see
   how the csv looks:

   what happened here:
     * feed_format : the format in which you want the data to be exported.
       supported formats are: json, json lines, xml and csv.
     * feed_uri : the location of the exported file.

   there are a plethora of forms that scrapy support for exporting feed if
   you want to dig deeper you can check [110]here and [111]using css
   selectors in scrapy.

   now that you have successfully created a system that crawls web content
   from a link, scrapes(extracts) selective data from it and saves it in
   an appropriate structured format let   s take the game a notch higher and
   learn more about web scraping.


3. case studies using scrapy

   let   s now look at a few case studies to get more experience of scrapy
   as a tool and its various functionalities.


scraping an e-commerce site

   the advent of internet and smartphones has been an impetus to the
   e-commerce industry. with millions of customers and billions of dollars
   at stake, the market has started seeing the multitude of players. which
   in turn has led to rise of e-commerce aggregator platforms which
   collect and show you the information regarding your products from
   across multiple portals? for example when planning to buy a smartphone
   and you would want to see the prices at different platforms at a single
   place. what does it take to build such an aggregator platform? here   s
   my small take on building an e-commerce site scraper.

   as a test site, you will scrape shopclues for 4g-smartphones

   let   s first generate a basic spider:
scrapy genspider shopclues www.shopclues.com/mobiles-featured-store-4g-smartphon
e.html

   this is how the shop clues web page looks like:

   the following information needs to be extracted from the page:
     * product name
     * product price
     * product discount
     * product image


extracting image urls of the product

   on careful inspection, it can be seen that the attribute    data-img    of
   the <img> tag can be used to extract image urls:
response.css("img::attr(data-img)").extract()


extracting product name from <img> tags

   notice that the    title    attribute of the <img> tag contains the
   product   s full name:

response.css("img::attr(title)").extract()

   similarly, selectors for price(   .p_price   ) and
   discount(   .prd_discount   ).


how to download product images?

   scrapy provides reusable images pipelines for downloading files
   attached to a particular item (for example, when you scrape products
   and also want to download their images locally).

   the images pipeline has a few extra functions for processing images. it
   can:
     * convert all downloaded images to a common format (jpg) and mode
       (rgb)
     * thumbnail generation
     * check images width/height to make sure they meet a minimum
       constraint

   in order to use the images pipeline  to download images, it needs to be
   enabled in the settings.py file. add the following lines to the file :
item_pipelines = {
  'scrapy.pipelines.images.imagespipeline': 1
}
images_store = 'tmp/images/'

   you are basically telling scrapy to use the    images pipeline    and the
   location for the images should be in the folder    tmp/images/. the final
   spider would now be:
import scrapy

class shopcluesspider(scrapy.spider):
   #name of spider
   name = 'shopclues'

   #list of allowed domains
   allowed_domains = ['www.shopclues.com/mobiles-featured-store-4g-smartphone.ht
ml']
   #starting url
   start_urls = ['http://www.shopclues.com/mobiles-featured-store-4g-smartphone.
html/']
   #location of csv file
   custom_settings = {
       'feed_uri' : 'tmp/shopclues.csv'
   }


   def parse(self, response):
       #extract product information
       titles = response.css('img::attr(title)').extract()
       images = response.css('img::attr(data-img)').extract()
       prices = response.css('.p_price::text').extract()
       discounts = response.css('.prd_discount::text').extract()


       for item in zip(titles,prices,images,discounts):
           scraped_info = {
               'title' : item[0],
               'price' : item[1],
               'image_urls' : [item[2])], #set's the url for scrapy to download
images
               'discount' : item[3]
           }

           yield scraped_info

   a few things to note here:
     * custom_settings : this is used to set settings of an individual
       spider. remember that settings.py is for the whole project so here
       you tell scrapy that the output of this spider should be stored in
       a csv  file    shopclues.csv    that is to be stored in the    tmp   
       folder.
     * scraped_info[   image_urls   ]  : this is the field that scrapy checks
       for the image   s link. if you set this field with a list of urls, ,
       scrapy will automatically download and store those images for you.

   on running the spider the output can be read from    tmp/shopclues.csv   :

   you also get the images downloaded. check the folder    tmp/images/full   
   and you will see the images:

   also, notice that scrapy automatically adds the download path of the
   image on your system in the csv:

   there you have your own little e-commerce aggregator     

   if you want to dig in you can read more about scrapy   s images pipeline
   [112]here


scraping techcrunch: creating your own rss feed reader

   techcrunch is one of my favourite blogs that i follow to stay abreast
   with news about startups and latest technology products. just like many
   blogs nowadays techcrunch gives its own rss feed here :
   [113]https://techcrunch.com/feed/ . one of scrapy   s features is its
   ability to handle xml data with ease and in this part, you are going to
   extract data from techcrunch   s rss feed.

   create a basic spider:
scrapy genspider techcrunch [114]techcrunch.com/feed/

   let   s have a look at the xml, the marked portion is data of interest:

   here are some observations from the page:
     * each article is present between <item></item> tags and there are 20
       such items(articles).
     * the title of the post is in <title></title> tags.
     * link to the article can be found in <link> tags.
     * <pubdate> contains the date of publishing.
     * the author name is enclosed between funny looking <dc:creator>
       tags.


overview of xpath and xml

   xpath is a syntax that is used to define xml documents. it can be used
   to traverse through an xml document. note that xpath   s follows a
   hierarchy.


extracting title of post

   let   s extract the title of the first post. similar to response.css(..)
   , the function response.xpath(..) in scrapy to deal with xpath. the
   following code should do it:
response.xpath("//item/title").extract_first()


   output :
u'<title xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="htt
p://wellformedweb.org/commentapi/" xmlns:dc
="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/atom" xml
ns:sy="http://purl.org/rss/1.0/modules/syndication/"
xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:georss="http://www.ge
orss.org/georss" xmlns:geo="http://www.w3.org/2003/
01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/">why the future of
 deep learning depends on finding good data</title>'

   wow! that   s a lot of content, but only the text content of the title is
   of interest. let   s filter it out:
response.xpath("//item/title/text()").extract_first()

   output :
u'why the future of deep learning depends on finding good data'

   this is much better. notice that text() here is equivalent of ::text
   from css selectors. also look at the xpath //item/title/text() here you
   are basically saying find the element    item    and extract the    text   
   content of its sub element    title   .

   similarly, the xpaths for link, pubdate as :
     * link     //item/link/text()
     * date of publishing     //item/pubdate/text()


extracting author name: dealing with namespaces in xml

   notice the <creator> tags:

   the tag itself has some text    dc:    because of which it can   t be
   extracted using xpath and the author name itself is crowded with
      ![cdata..    irrelevant text. these are just xml namespaces and you
   don   t want to have anything to do with them so we   ll ask scrapy to
   remove the namespace:
response.selector.remove_namespaces()

   now when you try extracting the author name , it will work :
response.xpath("//item/creator/text()").extract_first()

   output : u   ophir tanz,cambron carter   

   the complete spider for techcrunch would be:
import scrapy

class techcrunchspider(scrapy.spider):
    #name of the spider
    name = 'techcrunch'

    #list of allowed domains
    allowed_domains = ['techcrunch.com/feed/']

    #starting url for scraping
    start_urls = ['http://techcrunch.com/feed/']

    #setting the location of the output csv file
    custom_settings = {
        'feed_uri' : 'tmp/techcrunch.csv'
    }

    def parse(self, response):
        #remove xml namespaces
        response.selector.remove_namespaces()

        #extract article information
        titles = response.xpath('//item/title/text()').extract()
        authors = response.xpath('//item/creator/text()').extract()
        dates = response.xpath('//item/pubdate/text()').extract()
        links = response.xpath('//item/link/text()').extract()

        for item in zip(titles,authors,dates,links):
            scraped_info = {
                'title' : item[0],
                'author' : item[1],
                'publish_date' : item[2],
                'link' : item[3]
            }

            yield scraped_info

   let   s run the spider:
scrapy crawl techcrunch

   and there you have your own rss reader :)!


end notes

   in this article, we have just scratched the surface of scrapy   s
   potential as a web scraping tool. nevertheless, if you have experience
   with any other tools for scraping it would have been evident by now
   that in efficiency and practical application, scrapy wins hands down.
   all the code used in this article is [115]available on github. also,
   check out some of the interesting projects built with scrapy:
     * [116]scraping the steam game store with scrapy
     * [117]internet archaeology: scraping time series data from
       archive.org
     * [118]filtering startup news with machine learning and scrapy
     * [119]advanced web scraping: bypassing    403 forbidden,    captchas,
       and more

[120]learn, [121]engage, [122]compete & [123]get hired

   you can also read this article on analytics vidhya's android app
   [124]get it on google play

share this:

     * [125]click to share on linkedin (opens in new window)
     * [126]click to share on facebook (opens in new window)
     * [127]click to share on twitter (opens in new window)
     * [128]click to share on pocket (opens in new window)
     * [129]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [130]advanced web scraping, [131]automated web crawling,
   [132]python, [133]python3, [134]rss feed reader, [135]rss scraper,
   [136]scraping an e-commerce website, [137]scraping reddit,
   [138]scraping techcrunch, [139]scrapy, [140]spiders, [141]web crawlers,
   [142]web data extraction, [143]web harvesting, [144]web scraping,
   [145]web spiders
   next article

tableau for beginners     data visualisation made easy

   previous article

data scientist     ahmedabad (4-9 years of experience)

[146]mohd sanad zaki rizvi

   a computer science graduate, i have previously worked as a research
   assistant at the university of southern california(usc-ict) where i
   employed nlp and ml to make better virtual stem mentors. my research
   interests include using ai and its allied fields of nlp and computer
   vision for tackling real-world problems.
     *
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [147]discussion portal to get your queries resolved

93 comments

     * mayank srivastava says:
       [148]july 25, 2017 at 10:38 am
       by far the simplest and the best explaination about scrapy. thanks
       !!
       [149]reply
          + [150]mohd sanad zaki rizvi says:
            [151]july 25, 2017 at 11:11 am
            thanks for your comment, mayank!     
            [152]reply
     * karthikeyan palanisamy says:
       [153]july 25, 2017 at 10:47 am
       hi mohammed,
       a very detailed article on scraping. could you please let me know
       how does scrapy differs from beautifulsoup?
       [154]reply
          + [155]mohd sanad zaki rizvi says:
            [156]july 25, 2017 at 11:05 am
            hey karthikeyan,
            beautifulsoup is a library that    parses    html or xml content.
            in other words, it reads your html file and helps extract
            content from it.
            scrapy is a full blown web scraping framework. that means, it
            already has the functionality that beautifulsoup provides
            along with that it offers much more.
            when you are developing a web scraping system, you would need
            a way to send requests to the websites (probably using
            requests or urllib) , you would need a way to send multiple
            requests at once(multiprocessing/asynchronous) so that you can
            download content faster. you would also need a way to export
            your downloaded content in various required formats, if you
            are working on large scale projects, you would require
            deploying your scraping code across distributed systems.
            scrapy provides you with all of that and much more in built.
            and yeah, you can use beautifulsoup with scrapy if you prefer.
            hope this helps,
            sanad     
            [157]reply
               o karthikeyan palanisamy says:
                 [158]july 25, 2017 at 11:10 am
                 thanks sanad for a very detailed answer.
                 [159]reply
     * ankit says:
       [160]july 25, 2017 at 11:13 am
       hi sanad,
       i am currently started using scrapy but two roadblocks i have first
       in our domain we need to crawl pdf pages which scrapy doesn   t
       provide and after googling i found couple of paid ways which we
       don   t prefer, second how we write junit for any scrapy code to do
       unit testing is there any framework for this?
       please help me out on this.
       thanks
       ankit
       [161]reply
          + [162]mohd sanad zaki rizvi says:
            [163]july 25, 2017 at 3:27 pm
            hey ankit,
            1. i   m not sure what do you mean by crawling pdf pages? if you
            are trying to scrape websites for pdf files, it again depends
            on what you are trying to achieve. you can probably use scrapy
            to extract link of target pdfs and urllib2 or requests to
            fetch the pdf files. and then you can use something like
            pdfminer( [164]https://pypi.python.org/pypi/pdfminer/) to
            parse pdf and extract information.
            2. regarding writing unit tests for scrapy code, it provides
            an integrated way to unit test spiders, check out spiders
            contracts :
            [165]https://doc.scrapy.org/en/latest/topics/contracts.html
            [166]reply
               o ankit says:
                 [167]july 25, 2017 at 3:35 pm
                 hi sanad,
                 thanks for your response ya my use case is to scrape pdf
                 data, i   ll go through the provided links and then let see
                     
                 thanks
                 [168]reply
               o amit says:
                 [169]july 25, 2017 at 3:50 pm
                 hi rizvi,
                 thank you very much for respoding to ankit   s (my
                 colleague
                 ) query.the issue is not in extracting text from pdf but
                 in extracting the relevant info of the structure of the
                 pdf(tables etc).in other words,no info on any way to
                 identify the data as tabular or its structure in pdf
                 document.what we are trying to do is to extract specific
                 info (for eg specific column data from a table in pdf
                 document).that   s where most of the open source libraries
                 falter.reason looks to be more about the way pdf has been
                 encoded .
                 hope,the query is clear .in case,you need additional
                 info,pls let me know.any help in this regard well be
                 highly appreciated.primarily ,we are looking for python
                 apis.even if open source java libraries can do the
                 same,we can invoke the same from python code.
                 [170]reply
     * isindor richie says:
       [171]july 25, 2017 at 12:57 pm
       hello   thanks for an explanatory tutorial   how can i start the scrapy
       server from jupiter notebook?
       [172]reply
          + [173]mohd sanad zaki rizvi says:
            [174]july 25, 2017 at 1:08 pm
            hey isindor,
            i usually don   t run scrapy server from jupyter notebook. i run
            it from the command line to export data in csvs and then
            import those csvs using pandas in notebook.
            but, you can execute any terminal command in jupyter notebook
            using    !    sign before the command. something like this:
            !scrapy crawl redditbot
            hope this helps.
            sanad
            [175]reply
               o isindor richie says:
                 [176]july 25, 2017 at 9:11 pm
                 yeah. thanks!
                 [177]reply
     * [178]charles says:
       [179]july 25, 2017 at 2:49 pm
       great article but i   m a little surprised it didn   t touch on the
       challenges of using scrapy when trying to scrape javascript heavy
       websites.
       most of the sites that i work with now require also using splash to
       render the javascript. as such i   ve also started looking at the
       selenium and webdriver option.
       at first, i tried very hard to limit myself to only scrapy and
       splash but after a month working on a complicated site, i   m really
       wishing i would have changed approaches much earlier. i   ve done
       more in a few days with selenium using the page object pattern than
       in weeks of scrapy and splash development.
       [180]reply
          + [181]mohd sanad zaki rizvi says:
            [182]july 25, 2017 at 3:02 pm
            hey charles,
            true that with the advent of javascript based front end
            frameworks and libraries, it is becoming difficult to scrape
            websites as such. we would have to use selenium and webdriver
            to aid in the part where we require user action like clicking
            a popup or filling a form. it   s not rare to see scrapy applied
            in conjunction with selenium in projects.
            yet, we have to remind ourselves that that   s not the problem
            scrapy is meant to solve. you could argue web scraping is a
            domain of its own with sub domains, one such sub domain being
            dealing with dynamic/javascript heavy websites.
            this article   s goal was supposed to get a beginner started
            with web scraping especially with the use of scrapy. it would
            have been overkill to try to cover all aspects of advanced web
            scraping.
            hope this helps,
            sanad
            [183]reply
               o isindor richie says:
                 [184]july 25, 2017 at 9:10 pm
                 i look forward to a tutorial covering scraping js heavy
                 sites..
                 thanks.
                 [185]reply
               o [186]charles says:
                 [187]july 25, 2017 at 9:59 pm
                 hello sanad,
                 thank you for your reply.
                 please don   t take my comment as anything but
                 constructive.
                 the article and work you are providing are wonderful.
                 please keep it up.
                 i   m relatively new to scrapy myself as i only started
                 with it a few months ago.
                 i was just trying to add a note for anyone new to it that
                 there is a potential gotcha if you   re working with a
                 website that heavily utilizes javascript.
                 great stuff! please keep it coming.
                 [188]reply
                    # [189]mohd sanad zaki rizvi says:
                      [190]july 25, 2017 at 11:33 pm
                      hey charles,
                      thanks for your feedback     
                      regards,
                      sanad
                      [191]reply
     * ram says:
       [192]july 25, 2017 at 7:24 pm
       hi sanad,
       i have an issue with starting the scrapy shell. when i am typing
       scrapy shell in the command terminal/ ! scrapy shell in jupyter
       notebook it is showing
          scrapy is not recognized as internal or external command, operable
       program or batch file   
       any suggestions on how to overcome this issue and proceed further
       thanks.
       [193]reply
     * anurag kumar says:
       [194]july 25, 2017 at 10:45 pm
       can scrapy scrape data that is inside the iframe?
       //copy pasting of xpath of website isnt working
       [195]reply
          + [196]mohd sanad zaki rizvi says:
            [197]july 26, 2017 at 1:20 pm
            hey anurag,
            an iframe is used when you want to embed a web page within
            another web page. what is actually happening under the hood is
            the element is showing the content of a given url. more here    
            [198]https://www.w3schools.com/tags/tag_iframe.asp
            what you will do in this case is extract all such urls that
            iframe is displaying using scrapy and then create another
            request for those urls and give them to scrapy. it will then
            handle things similarly. if i am not wrong, this answer will
            help you     [199]https://stackoverflow.com/a/24302223
            also, learn more about scrapy requests here    
            [200]https://doc.scrapy.org/en/latest/topics/request-response.
            html
            i would suggest you to first understand the basics of scrapy
            well.
            hope this helps     
            sanad
            [201]reply
     * angeline shalini says:
       [202]july 26, 2017 at 10:04 am
       very nice article, i am beginner in webscraping, have been using
       beautiful soup. i am excited to try out the examples using scrapy.
       great job with the explanation. what are some of the
       websites/people/blogs that i can follow to better understand
       webscraping and also get the latest info?
       [203]reply
          + [204]mohd sanad zaki rizvi says:
            [205]july 26, 2017 at 10:16 am
            hey angeline,
            check out the resources given in the end note. those are some
            really good blogs/people to follow to keep updated with
            scrapy.
            thanks.
            sanad
            [206]reply
     * [207]lwebzem says:
       [208]july 26, 2017 at 4:50 pm
       this is great, i tried to use it from the shell for the same url
       that is in the example with python 3 and win 10 but i got error as
       below. can you suggest something as i am new to scrapy.
       thanks
       in [4]: print (response.text)
                                                                                  
       unicodeencodeerror traceback (most recent call last)
       in ()
          -> 1 print (response.text)
       c:\users\owner\anaconda3\lib\encodings\cp437.py in encode(self,
       input, final)
       17 class incrementalencoder(codecs.incrementalencoder):
       18 def encode(self, input, final=false):
          > 19 return
       codecs.charmap_encode(input,self.errors,encoding_map)[0]
       20
       21 class incrementaldecoder(codecs.incrementaldecoder):
       unicodeencodeerror:    charmap    codec can   t encode character    \u2022   
       in position
       1047: character maps to
       [209]reply
          + [210]mohd sanad zaki rizvi says:
            [211]july 26, 2017 at 5:00 pm
            hey,
            i was not able to reproduce the error.
            [212]reply
          + ashutosh baunthiyal says:
            [213]august 18, 2017 at 11:13 am
            hi lwebzem,
            use this    
            print (response.text.encode(   utf-8   ))
            [214]reply
     * pulkit verma says:
       [215]july 26, 2017 at 6:23 pm
       during running command scrapy genspider techcrunch
       techchrunch.com/feed/ , i encountered an error which related to
       permission i.e permission denied :   .\\techchrunch.py   . how can i
       resolve this error, i am using python 3 and anaconda in windows.
       [216]reply
          + [217]mohd sanad zaki rizvi says:
            [218]august 2, 2017 at 1:01 am
            hey pulkit ,
            i think you don   t have the permission to write to your disk.
            it doesn   t seem to be a scrapy issue.
            [219]reply
     * ram says:
       [220]july 26, 2017 at 7:52 pm
       hi sanad,
       i am not able to open scrapy shell. an error    scrapy is not a
       recognized external or internal command or batch file    is coming
       when i am typing scrapy shell in the terminal.
       please help
       thanks
       [221]reply
          + [222]mohd sanad zaki rizvi says:
            [223]july 27, 2017 at 9:16 am
            you have to first install scrapy with the following command:
            pip install scrapy
            [224]reply
     * saurabh says:
       [225]july 27, 2017 at 2:30 am
       hi sanad,
       very nice article. thanks.
       i have one question regarding scrapping within web site request
       limit . how do we control number of request sent to website , so it
       doesn   t hold the web traffic and also is in limit of not getting
       blocked. what is the limit for number of request sent based on your
       experience.
       thanks
       [226]reply
          + [227]mohd sanad zaki rizvi says:
            [228]july 27, 2017 at 9:15 am
            hey saurabh,
            1. the settings.py file has many parameters that you can use
            to tune your scraping code. like the maximum number of
            concurrent requests sent to a site, maximum depth of crawl
            etc. check this    
            [229]https://doc.scrapy.org/en/latest/topics/settings.html#top
            ics-settings-ref
            2. scrapy also has this feature called    autothrottle   . it
            automatically controls the number of requests and crawling
            speed based on the server response time to avoid getting
            blocked and prevent putting a load on the server. here    
            [230]https://doc.scrapy.org/en/latest/topics/autothrottle.html
            #topics-autothrottle
            thanks
            sanad     
            [231]reply
     * krish c a says:
       [232]july 27, 2017 at 5:46 pm
       great article and explained the flow in step-by-step manner, so
       simple that even python beginners can also give a try and see the
       code working.
       [233]reply
          + [234]mohd sanad zaki rizvi says:
            [235]july 27, 2017 at 5:58 pm
            thank you for your comment!     
            [236]reply
     * machine learning library says:
       [237]july 28, 2017 at 8:44 am
       [238]https://mllib.wordpress.com/2017/07/27/web-scraping-in-python-
       using-scrapy-with-multiple-examples/
       [239]reply
     * vrana95 says:
       [240]july 30, 2017 at 10:15 pm
       hi sanad,
       really nice article. thanks for putting it up.
       [241]reply
          + [242]mohd sanad zaki rizvi says:
            [243]july 31, 2017 at 2:23 pm
            hey,
            thanks for the feedback     
            [244]reply
     * [245]kabir says:
       [246]august 1, 2017 at 2:42 pm
       i am getting error in below line
       scrapy startproject ourfirstscraper
       [247]http://www.reddit.com/r/gameofthrones/
       error
       file       , line 1
       scrapy startproject ourfirstscraper
       [248]http://www.reddit.com/r/gameofthrones
       syntax error : invalid syntax
       [249]reply
          + [250]mohd sanad zaki rizvi says:
            [251]august 8, 2017 at 3:23 pm
            hey kabir,
            there was a typo in this line which has been fixed. please
            check again.
            sanad     
            [252]reply
     * yash says:
       [253]august 1, 2017 at 6:54 pm
       hi sanad,
       i   m getting this error
       attributeerror traceback (most recent call last)
       in ()
          -> 1 response.css(   img::attr(data-img)   ).extract()
       attributeerror:    nonetype    object has no attribute    css   
       [254]reply
          + [255]mohd sanad zaki rizvi says:
            [256]august 7, 2017 at 8:21 pm
            hey yash,
            this basically means that your    response    object is empty or
            not properly made. there would be some error in preceding
            lines of code.
            sanad
            [257]reply
     * dhawal modi says:
       [258]august 5, 2017 at 8:29 pm
       great artice! i also have a doubt that if i want the spider to also
       go through the other webpages   .like in the redditbot if i want to
       scrape the the pages after the first page   how do i iterate the
       links i have specified in    allowed_domains    list??
       [259]reply
          + [260]mohd sanad zaki rizvi says:
            [261]august 5, 2017 at 8:40 pm
            hey dhawal,
            in order to make your scraper go to the next pages, you would
            need the link to the next page. check out this tutorial    
            [262]https://doc.scrapy.org/en/latest/intro/tutorial.html#foll
            owing-links
            hope this helps,
            sanad     
            [263]reply
               o dhawal modi says:
                 [264]august 5, 2017 at 11:15 pm
                 found it in the docs, thanks!
                 [265]reply
     * murali says:
       [266]august 8, 2017 at 7:53 am
       hi sanad,
       thanks for the nice tutorial. when i was trying to pull data from
       shopclues, i am getting only one record in the output csv file. not
       sure what is the issue with my code. i am pasting the code and the
       generated log. can you please advise?
       import scrapy
       class myshopcluesspider(scrapy.spider):
       name =    myshopclues   
       allowed_domains =
       [   www.shopclues.com/mobiles-featured-store-4g-smartphone.html   ]
       start_urls =
       [   http://www.shopclues.com/mobiles-featured-store-4g-smartphone.htm
       l/   ]
       custom_settings={
          feed_uri   :   tmp/shopclues.csv   
       }
       def parse(self, response):
       images=response.css(   img::attr(data-img)   ).extract()
       titles=response.css(   img::attr(title)   ).extract()
       prices=response.css(   .p_price::text   ).extract()
       discounts=response.css(   .prd_discount::text   ).extract()
       for item in zip(titles,prices,images,discounts):
       scraped_info={
          title   :item[0],
          price   :item[1],
          image_urls   :[item[2]],
          discount   :item[3]
       }
       yield scraped_info
       (c:\users\mupago\appdata\local\conda\conda\envs\my_root)
       c:\users\mupago\www.shopclues.com\shopclues\spiders>scrapy crawl
       myshopclues
       2017-08-07 22:17:13 [scrapy.utils.log] info: scrapy 1.3.3 started
       (bot: shopclues)
       2017-08-07 22:17:13 [scrapy.utils.log] info: overridden settings:
       {   bot_name   :    shopclues   ,    feed_format   :    csv   ,    newspider_module   :
          shopclues.spiders   ,    robotstxt_obey   : true,    spider_modules   :
       [   shopclues.spiders   ]}
       2017-08-07 22:17:13 [scrapy.middleware] info: enabled
       extensions:[   scrapy.extensions.corestats.corestats   ,
          scrapy.extensions.telnet.telnetconsole   ,
          scrapy.extensions.feedexport.feedexporter   ,
          scrapy.extensions.logstats.logstats   ]
       2017-08-07 22:17:13 [scrapy.middleware] info: enabled downloader
       middlewares:
       [   scrapy.downloadermiddlewares.robotstxt.robotstxtmiddleware   ,
          scrapy.downloadermiddlewares.httpauth.httpauthmiddleware   ,
          scrapy.downloadermiddlewares.downloadtimeout.downloadtimeoutmiddle
       ware   ,
          scrapy.downloadermiddlewares.defaultheaders.defaultheadersmiddlewa
       re   ,
          scrapy.downloadermiddlewares.useragent.useragentmiddleware   ,
          scrapy.downloadermiddlewares.retry.retrymiddleware   ,
          scrapy.downloadermiddlewares.redirect.metarefreshmiddleware   ,
          scrapy.downloadermiddlewares.httpcompression.httpcompressionmiddle
       ware   ,
          scrapy.downloadermiddlewares.redirect.redirectmiddleware   ,
          scrapy.downloadermiddlewares.cookies.cookiesmiddleware   ,
          scrapy.downloadermiddlewares.stats.downloaderstats   ]
       2017-08-07 22:17:13 [scrapy.middleware] info: enabled spider
       middlewares:
       [   scrapy.spidermiddlewares.httperror.httperrormiddleware   ,
          scrapy.spidermiddlewares.offsite.offsitemiddleware   ,
          scrapy.spidermiddlewares.referer.referermiddleware   ,
          scrapy.spidermiddlewares.urllength.urllengthmiddleware   ,
          scrapy.spidermiddlewares.depth.depthmiddleware   ]
       2017-08-07 22:17:13 [scrapy.middleware] info: enabled item
       pipelines:
       [   scrapy.pipelines.images.imagespipeline   ]
       2017-08-07 22:17:13 [scrapy.core.engine] info: spider opened
       2017-08-07 22:17:13 [scrapy.extensions.logstats] info: crawled 0
       pages (at 0 pages/min), scraped 0 items (at 0 items/min)
       2017-08-07 22:17:13 [scrapy.extensions.telnet] debug: telnet
       console listening on 127.0.0.1:6024
       2017-08-07 22:17:15 [scrapy.core.engine] debug: crawled (200)
       (referer: none)
       2017-08-07 22:17:15 [scrapy.core.engine] debug: crawled (200)
       (referer: none)
       2017-08-07 22:17:15 [scrapy.pipelines.files] debug: file
       (uptodate): downloadedimage from referred in
       2017-08-07 22:17:15 [scrapy.core.scraper] debug: scraped from
       {   title   :    swipe konnect neo 4g, black [4g volte, quad core,
       android v6.0 marshmallow, 5mp camera] (black)   ,    price   :    rs.3099   ,
          image_urls   :
       [   https://cdn.shopclues.com/images/thumbnails/79033/200/200/1241223
       07101876702konnectneo4gmainimage15000260381501218627.jpg   ],
          discount   :    28% off   ,    images   : [{   url   :
          https://cdn.shopclues.com/images/thumbnails/79033/200/200/12412230
       7101876702konnectneo4gmainimage15000260381501218627.jpg   ,    path   :
          full/d03603c774c1a790d1e813e73743e60f1db3bd16.jpg   ,    checksum   :
          217ee1803456f4b83294c302d41cc9e7   }]}
       2017-08-07 22:17:15 [scrapy.core.engine] info: closing spider
       (finished)
       2017-08-07 22:17:15 [scrapy.extensions.feedexport] info: stored csv
       feed (1 items) in: tmp/shopclues.csv
       2017-08-07 22:17:15 [scrapy.statscollectors] info: dumping scrapy
       stats:{   downloader/request_bytes   : 482,
          downloader/request_count   : 2,
          downloader/request_method_count/get   : 2,
          downloader/response_bytes   : 32121,
          downloader/response_count   : 2,
          downloader/response_status_count/200   : 2,
          file_count   : 1,
          file_status_count/uptodate   : 1,
          finish_reason   :    finished   ,
          finish_time   : datetime.datetime(2017, 8, 8, 2, 17, 15, 400925),
          item_scraped_count   : 1,
          log_count/debug   : 5,
          log_count/info   : 8,
          response_received_count   : 2,
          scheduler/dequeued   : 1,
          scheduler/dequeued/memory   : 1,
          scheduler/enqueued   : 1,
          scheduler/enqueued/memory   : 1,
          start_time   : datetime.datetime(2017, 8, 8, 2, 17, 13, 850770)}
       2017-08-07 22:17:15 [scrapy.core.engine] info: spider closed
       (finished)
       [267]reply
          + [268]mohd sanad zaki rizvi says:
            [269]august 8, 2017 at 3:15 pm
            hey ,
            i checked your code but couldn   t find anything that stands out
            as such. i reused my code from here
            [270]https://github.com/mohdsanadzakirizvi/web-scraping-magic-
            with-scrapy-and-python
            and it works perfectly fine. hope this helps
            sanad
            [271]reply
               o al t. says:
                 [272]august 18, 2017 at 11:17 am
                 i have tried to replicate the first tutorial by scraping
                 from several other sites, and each time my spider only
                 yields the first row    the code is almost identical to the
                 got example. any advice is appreciated.
                 #yellowbot.py
                 # -*- coding: utf-8 -*-
                 import scrapy
                 class yellowbotspider(scrapy.spider):
                 name =    yellowbot   
                 allowed_domains = [   www.yellowpages.com   ]
                 start_urls =
                 [   https://www.yellowpages.com/search?search_terms=coffee+
                 shops&geo_location_terms=portland%2c+or   ]
                 def parse(self, response):
                 #extracting zee content using css selectors
                 name = response.css(   .business-name::text   ).extract()
                 street_address =
                 response.css(   .street-address::text   ).extract()
                 phone =
                 response.css(   .phones.phone.primary::text   ).extract()
                 for item in zip(name, street_address, phone):
                 #create a dictionary to store scraped info
                 scraped_info = {
                    shop_name    : item[0],
                    street_address    : item[1],
                    phone_number    : item[2],
                 }
                 yield scraped_info
                 [273]reply
                    # [274]mohd sanad zaki rizvi says:
                      [275]august 18, 2017 at 11:39 am
                      hey,
                      there can be many reasons for that. it is hard to
                      figure out without much info. could you please post
                      your code along with screenshots of your terminal
                      output and create a new question here    
                      [276]https://discuss.analyticsvidhya.com/
                      that way even the community would be able to help
                      you!
                      thanks
                      sanad
                      [277]reply
          + fernando says:
            [278]august 26, 2017 at 8:54 am
            this line need be inner    for    section:    yield scraped_info   .
            (sorry for my poor english)
            [279]reply
     * avilash says:
       [280]august 8, 2017 at 2:08 pm
       getting invalid syntax error when i try to run the spider.
       was able to see the response and text responses individually,
       >>>scrapy crawl smartpricebot
       file       , line 1
       scrapy crawl smartpricebot
       ^
       syntaxerror: invalid syntax
       dont know why must be missing something. please help resolve this
       [281]reply
          + [282]mohd sanad zaki rizvi says:
            [283]august 8, 2017 at 3:27 pm
            hey avilash,
            you are trying to run the spider from within the python or
            scrapy shell. this command works when you are in your regular
            terminal(command line). as i have mentioned in my article,
            exit the scrapy shell first and then try it.
            sanad     
            [284]reply
               o avilash says:
                 [285]august 8, 2017 at 11:32 pm
                 thanks, buddy, it is a very helpful article.
                 doubt:
                 also, if there are no unique attributes on any particular
                 page, can we have any start and stop points or use regex
                 to restrict the crawl to a specific area of a page
                 also if you can address pagination and scroll down    load
                 more    pages it would be great help.
                 [286]reply
     * [287]chang ka yuen says:
       [288]august 8, 2017 at 8:24 pm
       i read thousands of articles and watch millions of video tutorial
       to learn scrapy, but i   m still not able to run a project
       successfully, all my spiders stuck in the half way, or comeback
       with empty data. after i read your article, i finally can built a
       project which is work, really thanks a lot.
       by the way, can you please give another scrapy tutorial regarding
       how to schedule the scrapy task? thanks once again.
       [289]reply
     * [290]chang ka yuen says:
       [291]august 8, 2017 at 8:25 pm
       i read thousands of articles and watch millions of video tutorial
       to learn scrapy, but i   m still not able to run a project
       successfully, all my spiders stuck in the half way, or comeback
       with empty data. after i read your article, i finally can built a
       project which is work, really thanks a lot.
       by the way, can you please give another scrapy tutorial regarding
       how to schedule the scrapy task, and how to overwrite a csv file?
       thanks once again.
       [292]reply
          + [293]mohd sanad zaki rizvi says:
            [294]august 8, 2017 at 8:30 pm
            hey chang,
            thanks for the comment, means a lot!
            i   ll see what i can do.     
            regards
            sanad
            [295]reply
     * azam says:
       [296]august 9, 2017 at 9:43 am
       how do you handle if item[   discount   ] = 0 . i want to skip any 0 or
       empty value from scraped data in csv.
       [297]reply
     * declan says:
       [298]august 11, 2017 at 1:15 am
       great job on the scraping walkthroughs
       is there a way to scrape multiple websites for a keyword and
       extract associated info ? kind of similar to what google does but
       returning some additional variables related to the keyword ?
       [299]reply
          + [300]mohd sanad zaki rizvi says:
            [301]august 26, 2017 at 5:20 pm
            hey declan,
            thanks for the feedback!     
            [302]reply
     * fernando says:
       [303]august 26, 2017 at 8:58 am
       great tutorial, the examples are very easy for learning and works
       fine, greetings from chile
       [304]reply
          + [305]mohd sanad zaki rizvi says:
            [306]august 26, 2017 at 5:21 pm
            thanks for the appreciation fernando!     
            [307]reply
     * ajay says:
       [308]august 27, 2017 at 10:26 am
       great article !
       i am new to scrapy and this information helped me a lot.
       can we scrap data of websites which have log in criteria?
       that is,
       if i need data from a website for multiple users,each user has
       unique id,
       the log in criteria is entering id.
       now i want to scrap data of user and display same in o/p.
       can i do this?
       and can we log in to website through code without redirecting to
       that website??
       [309]reply
          + [310]mohd sanad zaki rizvi says:
            [311]august 31, 2017 at 2:18 pm
            hey ajay,
            thanks for appreciating!     
            yes, look here-
            [312]https://doc.scrapy.org/en/latest/topics/request-response.
            html#topics-request-response-ref-request-userlogin
            hope this helps,
            sanad     
            [313]reply
          + [314]mohd sanad zaki rizvi says:
            [315]october 9, 2017 at 10:48 pm
            hey ajay,
            check this out
            [316]https://blog.scrapinghub.com/2012/10/26/filling-login-for
            ms-automatically/
            [317]reply
     * sandeep says:
       [318]september 1, 2017 at 2:27 pm
       hi,
       i am planning to use scrapy for one of the bank site to crawl all
       the pages/api and parse the response like    cache-control   ,
          content-length    with their values along with respected url as an
       automated way. is this possible with scrapy?
       [319]reply
          + [320]mohd sanad zaki rizvi says:
            [321]october 9, 2017 at 10:46 pm
            hey sandeep,
            yes you can     
            hope this helps,
            sanad
            [322]reply
     * odin says:
       [323]october 6, 2017 at 9:47 pm
       hi sanad, thank you for this great post, very illuminating. full
       disclosure total beginner here(to scrapy)
       i am tasked with extracting links from a bunch of websites (about
       50) and i was wondering if it is possible with scrapy and if it is
       could you give me a brief guide on how, or direct me somewhere i
       can get help on the same.
       [324]reply
          + [325]mohd sanad zaki rizvi says:
            [326]october 9, 2017 at 10:43 pm
            hey odin, thank you for your feedback..! different kinds of
            site require different methodologies, as you can probably see
            from the diverse case studies given in the article.
            [327]reply
     * [328]pgjosh says:
       [329]october 9, 2017 at 9:06 am
       hello author,
       great article. i just read it. i will try it later. please answer
       my queries if i stuck     
       thank you.
       [330]reply
          + [331]mohd sanad zaki rizvi says:
            [332]october 9, 2017 at 10:44 pm
            hey pgjosh,
            sure , anytime     
            sanad
            [333]reply
     * jonas says:
       [334]october 9, 2017 at 4:38 pm
       hi mohd! great tutorial, very thorough. this is what i have been
       looking for, for my big data project.
       i   m new to both python, scraping, crawling and all that but this
       looks like something i could get started with right away.
       could you give some hints on how to get both the posts data and the
       comments connected to that post? or maybe a link where one can find
       more help on this?
       thanks again, it   s highly appreciated!
       /jonas
       [335]reply
          + [336]mohd sanad zaki rizvi says:
            [337]october 9, 2017 at 10:50 pm
            hey jonas,
            check this out
            [338]https://doc.scrapy.org/en/latest/topics/request-response.
            html
            [339]reply
     * [340]vincent says:
       [341]october 18, 2017 at 1:06 am
       reposting because i posted in the wrong place.
       how would i use the save scrapy items and integrate it in my
       project so it will display the items on the website page?
       [342]reply
          + [343]mohd sanad zaki rizvi says:
            [344]november 27, 2017 at 10:24 am
            hey vincent,
            store the items in a database. in your website, fetch the
            contents from the above database .
            [345]reply
     * lukasz says:
       [346]november 6, 2017 at 9:38 pm
       great tutorial +mohd sanad zaki rizvi. i wonder how to make such a
       scraper and put it on a website. i mean to make a paid tool.
       somebody login pays and then use the tool. could you point me out
       where i can find such a tutorial because i   m searching for it and
       can   t find it? greets.
       [347]reply
          + [348]mohd sanad zaki rizvi says:
            [349]november 27, 2017 at 10:23 am
            hey lukasz,
            you would probably rent a cloud machine and run your scraper
            on that and it will store the scraped content in a database.
            whenever someone wants to access the scraped content they
            would visit your website that will fetch the content from the
            above database.
            hope this helps,
            sanad     
            [350]reply
               o lukasz says:
                 [351]november 27, 2017 at 1:15 pm
                 thanks     
                 [352]reply
     * [353]dvrow says:
       [354]november 10, 2017 at 12:05 am
       super useful, thank you! helped me get an overview of the whole
       process.
       [355]reply
     * baba says:
       [356]november 10, 2017 at 3:43 pm
       hello there!
       i want to know to how crawl javascript pages or other which
       source-code does not contains actuall items shown in the website.
       is it possible to crawl javascript webpages on canopy-python 3.5+?
       thanks
       [357]reply
          + [358]mohd sanad zaki rizvi says:
            [359]november 27, 2017 at 10:21 am
            hey baba,
            for scraping js rendered pages you will have to use another
            framework like selenium in conjunction with scrapy. check out
            how selenium works here    
            [360]https://medium.com/@hoppy/how-to-test-or-scrape-javascrip
            t-rendered-websites-with-python-selenium-a-beginner-step-by-c1
            37892216aa
            regards,
            sanad     
            [361]reply
               o baba says:
                 [362]november 27, 2017 at 10:36 am
                 thanks sanad for ur reply   . i got it
                 [363]reply
     * khushboo bansal says:
       [364]november 20, 2017 at 11:33 am
       hi sanad
       i want to get information regarding the startups and incubation
       pages that various universities have on their websites. so what i
       have in my head is that i will have to first crawl various pages to
       get the urls of the various universities and then again deploy the
       crawler on each of those urls to get the url of the page where the
       data regarding startups and incubation centers is present. am i
       thinking in the right direction?
       thanks
       khushboo
       [365]reply
          + [366]mohd sanad zaki rizvi says:
            [367]november 27, 2017 at 10:17 am
            hey khushboo,
            yes, looks alright. there is a feature in scrapy that lets you
            follow the links you have extracted and scrape content from
            them too. check out
            [368]https://doc.scrapy.org/en/latest/intro/tutorial.html#foll
            owing-links
            sanad     
            [369]reply
     * deana nanson says:
       [370]december 11, 2017 at 9:14 pm
       i   ve been reviewing online more than 7 hours today to make web
       scraping in python using scrapy, yet
       i never found any interesting article like yours. it is pretty
       worth enough for me.
       in my view, if all site owners and bloggers made good content as
       you did, the net
       will be much more useful than ever before.
       [371]reply
     * [372]mich says:
       [373]december 15, 2017 at 10:17 am
       great tutorial, the examples are very easy for learning and works
       fine.
       [374]reply
          + [375]mohd sanad zaki rizvi says:
            [376]december 15, 2017 at 10:37 am
            hey mich,
            glad to know it helped you, keep posting your
            doubts/suggestions here!
            sanad     
            [377]reply
     * jai says:
       [378]december 20, 2017 at 12:59 pm
       this is a very simplest and most useful post related to scrapy for
       a beginner. thanks for posting. before this scrapy was a mystery
       for me.
       [379]reply
     * adrian says:
       [380]december 27, 2017 at 4:38 pm
       it   s very good. the     print response.text    dont work for me and
       searching i found thah replace the response.text for response.body
       work very good.
       thanks for the examples.
       [381]reply
     * amar hunter says:
       [382]december 30, 2017 at 7:11 pm
       hi, i have seen that you replying to every question. and due to
       that i like this blog.
       i want to extract information from whole website including all
       hyperlinks it has attached.
       can i do that with scrapy or not.
       thank you in advance.
       [383]reply
          + [384]mohd sanad zaki rizvi says:
            [385]december 30, 2017 at 7:42 pm
            i don   t see any problem with that. though i   d like you to
            check out these links after doing the blog    
            1. crawl a website using its sitemap    
            [386]https://doc.scrapy.org/en/latest/topics/spiders.html#site
            mapspider
            2. scrapy   s link extractor    
            [387]https://doc.scrapy.org/en/latest/topics/link-extractors.h
            tml
            let me know how did it go!
            sanad     
            [388]reply
     * christopher mcmahon says:
       [389]february 21, 2018 at 9:12 pm
       hey there! thanks for the tutorial! i wanted to show something that
       didn   t initially but i got to work by reading through the rest of
       the tutorial.
       in the tutorial where it first says to edit your settings.py file
       to :
       # export as csv feed
       feed_uri =    reddit.csv   
       feed_format =    csv   
       this didn   t work for me. i   m not sure where the files were/are
       being stored or downloaded too but it was not the current folder
       where the spider resides. later in the tutorial there is another
       spider example where a customer setting is specified just before
       the parse function and after the urls, i edited it to see if i
       could get it to work and it did-
       custom_settings = {
          feed_uri    :    tmp/reddit.csv   
       }
       i just wanted to point that out. if anyone can provide any insight
       as to where my initial downloaded csvs have gone to, i   d definitely
       appreciate it.
       thanks again for the tutorial! i   ve done it two or three times now.
       [390]reply
     * himanshu says:
       [391]march 7, 2018 at 8:36 pm
       awsm tutorial man but i have a doubt . how we can download the .mkv
       file format through scrapy .
       [392]reply
     * [393]madani says:
       [394]may 14, 2018 at 4:57 pm
       clean and crystal article, thanks
       scrapy is the best framework for scraping
       [395]reply
          + mohd sanad zaki rizvi says:
            [396]august 13, 2018 at 11:46 pm
            madani,
            i   m glad you liked it and find it useful. thanks for the
            feedback!
            sanad
            [397]reply
     * iota says:
       [398]august 13, 2018 at 10:18 pm
       this is why anyone can learn machine learning. you are using
       publicly available datasets, or scraping data from the web via
       python libraries like scrapy, everyone has access to quality data
       sets.
       [399]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [400]srk       3924
   2    [2.jpg?date=2019-04-06] [401]mark12    3510
   3    [3.jpg?date=2019-04-06] [402]nilabha   3261
   4    [4.jpg?date=2019-04-06] [403]nitish007 3237
   5    [5.jpg?date=2019-04-06] [404]tezdhar   3082
   [405]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [406]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [407]understanding support vector machine algorithm from examples
       (along with code)
     * [408]essentials of machine learning algorithms (with python and r
       codes)
     * [409]a complete tutorial to learn data science with python from
       scratch
     * [410]7 types of regression techniques you should know!
     * [411]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [412]a simple introduction to anova (with applications in excel)
     * [413]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [414]top 5 machine learning github repositories and reddit discussions
   from march 2019

[415]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [416]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[417]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [418]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[419]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [420]16 opencv functions to start your id161 journey (with
   python code)

[421]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [422][ds-finhack.jpg]

   [423][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [424]about us
     * [425]our team
     * [426]career
     * [427]contact us
     * [428]write for us

   [429]about us
   [430]   
   [431]our team
   [432]   
   [433]careers
   [434]   
   [435]contact us

data scientists

     * [436]blog
     * [437]hackathon
     * [438]discussions
     * [439]apply jobs
     * [440]leaderboard

companies

     * [441]post jobs
     * [442]trainings
     * [443]hiring hackathons
     * [444]advertising
     * [445]reach us

   don't have an account? [446]sign up here.

join our community :

   [447]46336 [448]followers
   [449]20224 [450]followers
   [451]followers
   [452]7513 [453]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [454]privacy policy
     * [455]terms of use
     * [456]refund policy

   don't have an account? [457]sign up here

   iframe: [458]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [459](button) join now

   subscribe!

   iframe: [460]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [461](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/business-intelligence/
  94. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
  95. https://www.analyticsvidhya.com/blog/category/business-intelligence/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/category/web-analytics-2/
  98. https://www.analyticsvidhya.com/blog/author/mohdsanadzakirizvigmail-com/
  99. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=webscrapinginpythonarticle
 100. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=webscrapinginpythonarticle
 101. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=webscrapinginpythonarticle
 102. https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/
 103. https://www.reddit.com/
 104. https://www.reddit.com/r/gameofthrones/
 105. https://www.reddit.com/r/gameofthrones/
 106. https://discuss.analyticsvidhya.com/
 107. https://www.w3schools.com/cssref/css_selectors.asp
 108. https://doc.scrapy.org/en/latest/topics/selectors.html
 109. http://www.reddit.com/r/gameofthrones/
 110. https://doc.scrapy.org/en/latest/topics/feed-exports.html
 111. https://doc.scrapy.org/en/latest/topics/selectors.html#using-selectors
 112. https://doc.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.images
 113. https://techcrunch.com/feed/
 114. https://techcrunch.com/feed/
 115. https://github.com/mohdsanadzakirizvi/web-scraping-magic-with-scrapy-and-python
 116. https://blog.scrapinghub.com/2017/07/07/scraping-the-steam-game-store-with-scrapy/
 117. http://sangaline.com/post/wayback-machine-scraper/
 118. https://monkeylearn.com/blog/filtering-startup-news-machine-learning/
 119. http://sangaline.com/post/advanced-web-scraping-tutorial/
 120. https://www.analyticsvidhya.com/blog/
 121. https://discuss.analyticsvidhya.com/
 122. https://datahack.analyticsvidhya.com/contest/all/
 123. http://analyticsvidhya.com/jobs
 124. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 125. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?share=linkedin
 126. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?share=facebook
 127. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?share=twitter
 128. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?share=pocket
 129. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?share=reddit
 130. https://www.analyticsvidhya.com/blog/tag/advanced-web-scraping/
 131. https://www.analyticsvidhya.com/blog/tag/automated-web-crawling/
 132. https://www.analyticsvidhya.com/blog/tag/python/
 133. https://www.analyticsvidhya.com/blog/tag/python3/
 134. https://www.analyticsvidhya.com/blog/tag/rss-feed-reader/
 135. https://www.analyticsvidhya.com/blog/tag/rss-scraper/
 136. https://www.analyticsvidhya.com/blog/tag/scraping-an-e-commerce-website/
 137. https://www.analyticsvidhya.com/blog/tag/scraping-reddit/
 138. https://www.analyticsvidhya.com/blog/tag/scraping-techcrunch/
 139. https://www.analyticsvidhya.com/blog/tag/scrapy/
 140. https://www.analyticsvidhya.com/blog/tag/spiders/
 141. https://www.analyticsvidhya.com/blog/tag/web-crawlers/
 142. https://www.analyticsvidhya.com/blog/tag/web-data-extraction/
 143. https://www.analyticsvidhya.com/blog/tag/web-harvesting/
 144. https://www.analyticsvidhya.com/blog/tag/web-scraping/
 145. https://www.analyticsvidhya.com/blog/tag/web-spiders/
 146. https://www.analyticsvidhya.com/blog/author/mohdsanadzakirizvigmail-com/
 147. https://discuss.analyticsvidhya.com/
 148. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132800
 149. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132800
 150. http://simplysanad.com/blog/
 151. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132805
 152. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132805
 153. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132801
 154. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132801
 155. http://simplysanad.com/blog/
 156. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132802
 157. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132802
 158. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132804
 159. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132804
 160. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132806
 161. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132806
 162. http://simplysanad.com/blog/
 163. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132824
 164. https://pypi.python.org/pypi/pdfminer/
 165. https://doc.scrapy.org/en/latest/topics/contracts.html
 166. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132824
 167. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132827
 168. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132827
 169. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132829
 170. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132829
 171. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132810
 172. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132810
 173. http://simplysanad.com/blog/
 174. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132814
 175. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132814
 176. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132855
 177. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132855
 178. https://charlesgreen.org/
 179. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132819
 180. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132819
 181. http://simplysanad.com/blog/
 182. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132821
 183. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132821
 184. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132854
 185. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132854
 186. https://charlesgreen.org/
 187. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132862
 188. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132862
 189. http://simplysanad.com/blog/
 190. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132869
 191. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132869
 192. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132845
 193. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132845
 194. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132865
 195. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132865
 196. http://simplysanad.com/blog/
 197. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132921
 198. https://www.w3schools.com/tags/tag_iframe.asp
 199. https://stackoverflow.com/a/24302223
 200. https://doc.scrapy.org/en/latest/topics/request-response.html
 201. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132921
 202. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132906
 203. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132906
 204. http://simplysanad.com/blog/
 205. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132908
 206. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132908
 207. http://intelligentonlinetools.com/blog/
 208. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132935
 209. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132935
 210. http://simplysanad.com/blog/
 211. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132936
 212. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132936
 213. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134597
 214. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134597
 215. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132944
 216. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132944
 217. http://simplysanad.com/blog/
 218. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133412
 219. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133412
 220. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132950
 221. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132950
 222. http://simplysanad.com/blog/
 223. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132979
 224. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132979
 225. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132966
 226. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132966
 227. http://simplysanad.com/blog/
 228. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132978
 229. https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings-ref
 230. https://doc.scrapy.org/en/latest/topics/autothrottle.html#topics-autothrottle
 231. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-132978
 232. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133018
 233. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133018
 234. http://simplysanad.com/blog/
 235. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133021
 236. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133021
 237. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133071
 238. https://mllib.wordpress.com/2017/07/27/web-scraping-in-python-using-scrapy-with-multiple-examples/
 239. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133071
 240. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133248
 241. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133248
 242. http://simplysanad.com/blog/
 243. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133290
 244. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133290
 245. http://---/
 246. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133371
 247. http://www.reddit.com/r/gameofthrones/
 248. http://www.reddit.com/r/gameofthrones
 249. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133371
 250. http://simplysanad.com/blog/
 251. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133882
 252. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133882
 253. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133386
 254. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133386
 255. http://simplysanad.com/blog/
 256. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133832
 257. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133832
 258. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133731
 259. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133731
 260. http://simplysanad.com/blog/
 261. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133732
 262. https://doc.scrapy.org/en/latest/intro/tutorial.html#following-links
 263. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133732
 264. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133741
 265. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133741
 266. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133860
 267. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133860
 268. http://simplysanad.com/blog/
 269. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133881
 270. https://github.com/mohdsanadzakirizvi/web-scraping-magic-with-scrapy-and-python
 271. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133881
 272. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134598
 273. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134598
 274. http://simplysanad.com/blog/
 275. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134600
 276. https://discuss.analyticsvidhya.com/
 277. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134600
 278. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135145
 279. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135145
 280. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133879
 281. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133879
 282. http://simplysanad.com/blog/
 283. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133883
 284. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133883
 285. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133914
 286. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133914
 287. http://none/
 288. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133898
 289. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133898
 290. http://none/
 291. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133899
 292. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133899
 293. http://simplysanad.com/blog/
 294. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133900
 295. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133900
 296. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133951
 297. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-133951
 298. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134060
 299. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-134060
 300. http://simplysanad.com/blog/
 301. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135176
 302. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135176
 303. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135146
 304. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135146
 305. http://simplysanad.com/blog/
 306. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135177
 307. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135177
 308. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135215
 309. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135215
 310. http://simplysanad.com/blog/
 311. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135799
 312. https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-userlogin
 313. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135799
 314. http://simplysanad.com/blog/
 315. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139154
 316. https://blog.scrapinghub.com/2012/10/26/filling-login-forms-automatically/
 317. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139154
 318. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135960
 319. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-135960
 320. http://simplysanad.com/blog/
 321. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139153
 322. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139153
 323. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-138905
 324. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-138905
 325. http://simplysanad.com/blog/
 326. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139151
 327. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139151
 328. http://none/
 329. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139088
 330. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139088
 331. http://simplysanad.com/blog/
 332. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139152
 333. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139152
 334. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139128
 335. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139128
 336. http://simplysanad.com/blog/
 337. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139155
 338. https://doc.scrapy.org/en/latest/topics/request-response.html
 339. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139155
 340. http://a practice test website/
 341. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139931
 342. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-139931
 343. http://simplysanad.com/blog/
 344. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145010
 345. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145010
 346. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-142450
 347. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-142450
 348. http://simplysanad.com/blog/
 349. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145009
 350. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145009
 351. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145051
 352. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145051
 353. http://www.davidrowthorn.net/
 354. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-142930
 355. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-142930
 356. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-143010
 357. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-143010
 358. http://simplysanad.com/blog/
 359. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145008
 360. https://medium.com/@hoppy/how-to-test-or-scrape-javascript-rendered-websites-with-python-selenium-a-beginner-step-by-c137892216aa
 361. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145008
 362. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145015
 363. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145015
 364. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-144189
 365. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-144189
 366. http://simplysanad.com/blog/
 367. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145007
 368. https://doc.scrapy.org/en/latest/intro/tutorial.html#following-links
 369. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-145007
 370. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-147024
 371. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-147024
 372. http://technotask.ru/
 373. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-147777
 374. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-147777
 375. http://simplysanad.com/blog/
 376. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-147780
 377. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-147780
 378. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-148795
 379. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-148795
 380. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-149680
 381. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-149680
 382. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-150104
 383. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-150104
 384. http://simplysanad.com/blog/
 385. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-150106
 386. https://doc.scrapy.org/en/latest/topics/spiders.html#sitemapspider
 387. https://doc.scrapy.org/en/latest/topics/link-extractors.html
 388. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-150106
 389. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-151526
 390. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-151526
 391. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-151748
 392. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-151748
 393. http://http/
 394. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-153247
 395. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-153247
 396. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-154563
 397. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-154563
 398. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-154561
 399. https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/#comment-154561
 400. https://datahack.analyticsvidhya.com/user/profile/srk
 401. https://datahack.analyticsvidhya.com/user/profile/mark12
 402. https://datahack.analyticsvidhya.com/user/profile/nilabha
 403. https://datahack.analyticsvidhya.com/user/profile/nitish007
 404. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 405. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 406. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 407. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 408. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 409. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 410. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 411. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 412. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 413. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 414. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 415. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 416. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 417. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 418. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 419. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 420. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 421. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 422. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 423. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 424. http://www.analyticsvidhya.com/about-me/
 425. https://www.analyticsvidhya.com/about-me/team/
 426. https://www.analyticsvidhya.com/career-analytics-vidhya/
 427. https://www.analyticsvidhya.com/contact/
 428. https://www.analyticsvidhya.com/about-me/write/
 429. http://www.analyticsvidhya.com/about-me/
 430. https://www.analyticsvidhya.com/about-me/team/
 431. https://www.analyticsvidhya.com/about-me/team/
 432. https://www.analyticsvidhya.com/about-me/team/
 433. https://www.analyticsvidhya.com/career-analytics-vidhya/
 434. https://www.analyticsvidhya.com/about-me/team/
 435. https://www.analyticsvidhya.com/contact/
 436. https://www.analyticsvidhya.com/blog
 437. https://datahack.analyticsvidhya.com/
 438. https://discuss.analyticsvidhya.com/
 439. https://www.analyticsvidhya.com/jobs/
 440. https://datahack.analyticsvidhya.com/users/
 441. https://www.analyticsvidhya.com/corporate/
 442. https://trainings.analyticsvidhya.com/
 443. https://datahack.analyticsvidhya.com/
 444. https://www.analyticsvidhya.com/contact/
 445. https://www.analyticsvidhya.com/contact/
 446. https://datahack.analyticsvidhya.com/signup/
 447. https://www.facebook.com/analyticsvidhya/
 448. https://www.facebook.com/analyticsvidhya/
 449. https://twitter.com/analyticsvidhya
 450. https://twitter.com/analyticsvidhya
 451. https://plus.google.com/+analyticsvidhya
 452. https://in.linkedin.com/company/analytics-vidhya
 453. https://in.linkedin.com/company/analytics-vidhya
 454. https://www.analyticsvidhya.com/privacy-policy/
 455. https://www.analyticsvidhya.com/terms/
 456. https://www.analyticsvidhya.com/refund-policy/
 457. https://id.analyticsvidhya.com/accounts/signup/
 458. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 459. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 460. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 461. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 463. https://www.facebook.com/analyticsvidhya
 464. https://twitter.com/analyticsvidhya
 465. https://plus.google.com/+analyticsvidhya/posts
 466. https://in.linkedin.com/company/analytics-vidhya
 467. https://www.analyticsvidhya.com/blog/2017/07/data-visualisation-made-easy/
 468. https://www.analyticsvidhya.com/blog/2017/07/data-scientist-ahmedabad-4-9-years-of-experience/
 469. https://www.analyticsvidhya.com/blog/author/mohdsanadzakirizvigmail-com/
 470. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#0c616364687f6d626d68766d67657e65767a654c6b616d6560226f6361
 471. https://www.linkedin.com/in/mohdsanad
 472. https://github.com/mohdsanadzakirizvi/
 473. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 474. https://www.facebook.com/analyticsvidhya/
 475. https://twitter.com/analyticsvidhya
 476. https://plus.google.com/+analyticsvidhya
 477. https://plus.google.com/+analyticsvidhya
 478. https://in.linkedin.com/company/analytics-vidhya
 479. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 480. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 481. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 482. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 483. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 484. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 485. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 486. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 487. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 488. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 489. javascript:void(0);
 490. javascript:void(0);
 491. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 492. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 493. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 494. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 495. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 496. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 497. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 498. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 499. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 500. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f07%2fweb-scraping-in-python-using-scrapy%2f&linkname=web%20scraping%20in%20python%20using%20scrapy%20%28with%20multiple%20examples%29
 501. javascript:void(0);
 502. javascript:void(0);
