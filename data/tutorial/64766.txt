   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

multi label text classification with scikit-learn

   [16]go to the profile of susan li
   [17]susan li (button) blockedunblock (button) followfollowing
   apr 21, 2018
   [1*irols5mmuuubkh9yst-ulq.jpeg]
   photo credit: pexels

   [18]multi-class classification means a classification task with more
   than two classes; each label are mutually exclusive. the classification
   makes the assumption that each sample is assigned to one and only one
   label.

   on the other hand, [19]multi-label classification assigns to each
   sample a set of target labels. this can be thought as predicting
   properties of a data-point that are not mutually exclusive, such as tim
   horton are often categorized as both bakery and coffee shop.
   multi-label text classification has many real world applications such
   as categorizing businesses on yelp or classifying movies into one or
   more genre(s).

problem formulation

   anyone who has been the target of abuse or harassment online will know
   that it doesn   t go away when you log off or switch off your phone.
   researchers at google are working on tools to study toxic comments
   online. in this post, we will build a multi-label model that   s capable
   of detecting different types of toxicity like severe toxic, threats,
   obscenity, insults, and so on. we will be using supervised classifiers
   and text representations. a toxic comment might be about any of toxic,
   severe toxic, obscene, threat, insult or identity hate at the same time
   or none of the above. the data set can be found at [20]kaggle.

   (disclaimer from the data source: the dataset contains text that may be
   considered profane, vulgar, or offensive.)

exploring

%matplotlib inline
import re
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import tfidfvectorizer
from sklearn.naive_bayes import multinomialnb
from sklearn.metrics import accuracy_score
from sklearn.multiclass import onevsrestclassifier
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from sklearn.id166 import linearsvc
from sklearn.linear_model import logisticregression
from sklearn.pipeline import pipeline
import seaborn as sns
df = pd.read_csv("train 2.csv", encoding = "iso-8859-1")
df.head()

   number of comments in each category
df_toxic = df.drop(['id', 'comment_text'], axis=1)
counts = []
categories = list(df_toxic.columns.values)
for i in categories:
    counts.append((i, df_toxic[i].sum()))
df_stats = pd.dataframe(counts, columns=['category', 'number_of_comments'])
df_stats

   [1*4lsnqh9mffax8pawmv5yow.png]
   figure 1
df_stats.plot(x='category', y='number_of_comments', kind='bar', legend=false, gr
id=true, figsize=(8, 5))
plt.title("number of comments per category")
plt.ylabel('# of occurrences', fontsize=12)
plt.xlabel('category', fontsize=12)

   [1*-7gj7qzpfjnzpuqtt8ieta.png]
   figure 2

multi-label

   how many comments have multi labels?
rowsums = df.iloc[:,2:].sum(axis=1)
x=rowsums.value_counts()
#plot
plt.figure(figsize=(8,5))
ax = sns.barplot(x.index, x.values)
plt.title("multiple categories per comment")
plt.ylabel('# of occurrences', fontsize=12)
plt.xlabel('# of categories', fontsize=12)

   [1*zn54e8q9d36klf1amtgoxg.png]
   figure 3

   vast majority of the comment text are not labeled.
print('percentage of comments that are not labelled:')
print(len(df[(df['toxic']==0) & (df['severe_toxic']==0) & (df['obscene']==0) & (
df['threat']== 0) & (df['insult']==0) & (df['identity_hate']==0)]) / len(df))

   percentage of comments that are not labelled:
   0.8983211235124177

   the distribution of the number of words in comment texts.
lens = df.comment_text.str.len()
lens.hist(bins = np.arange(0,5000,50))

   [1*p1g3zpfbrmwen1ylvi3zfq.png]
   figure 4

   most of the comment text length are within 500 characters, with some
   outliers up to 5,000 characters long.

   there is no missing comment in comment text column.
print('number of missing comments in comment text:')
df['comment_text'].isnull().sum()

   number of missing comments in comment text:

   0

   have a peek the first comment, the text needs to be cleaned.
df['comment_text'][0]

      explanation\rwhy the edits made under my username hardcore metallica
   fan were reverted? they weren   t vandalisms, just closure on some gas
   after i voted at new york dolls fac. and please don   t remove the
   template from the talk page since i   m retired now.89.205.38.27   

id174

   create a function to clean the text
def clean_text(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub('\w', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = text.strip(' ')
    return text

   clean up comment_text column:
df['comment_text'] = df['comment_text'].map(lambda com : clean_text(com))
df['comment_text'][0]

      explanation why the edits made under my username hardcore metallica
   fan were reverted they were not vandalisms just closure on some gas
   after i voted at new york dolls fac and please do not remove the
   template from the talk page since i am retired now 89 205 38 27   

   much better!

   split the data to train and test sets:
categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_
hate']
train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=true
)
x_train = train.comment_text
x_test = test.comment_text
print(x_train.shape)
print(x_test.shape)

   (106912,)
   (52659,)

classifiers training

pipeline

   scikit-learn provides a pipeline utility to help automate machine
   learning workflows. pipelines are very common in machine learning
   systems, since there is a lot of data to manipulate and many data
   transformations to apply. so we will utilize pipeline to train every
   classifier.

onevsrest multi-label strategy

   the multi-label algorithm accepts a binary mask over multiple labels.
   the result for each prediction will be an array of 0s and 1s marking
   which class labels apply to each row input sample.

naive bayes

   onevsrest strategy can be used for multi-label learning, where a
   classifier is used to predict multiple labels for instance. naive bayes
   supports multi-class, but we are in a multi-label scenario, therefore,
   we wrap naive bayes in the onevsrestclassifier.
# define a pipeline combining a text feature extractor with multi lable classifi
er
nb_pipeline = pipeline([
                ('tfidf', tfidfvectorizer(stop_words=stop_words)),
                ('clf', onevsrestclassifier(multinomialnb(
                    fit_prior=true, class_prior=none))),
            ])
for category in categories:
    print('... processing {}'.format(category))
    # train the model using x_dtm & y
    nb_pipeline.fit(x_train, train[category])
    # compute the testing accuracy
    prediction = nb_pipeline.predict(x_test)
    print('test accuracy is {}'.format(accuracy_score(test[category], prediction
)))

       processing toxic
   test accuracy is 0.9191401279933155
       processing severe_toxic
   test accuracy is 0.9900112041626312
       processing obscene
   test accuracy is 0.9514802787747584
       processing threat
   test accuracy is 0.9971135038644866
       processing insult
   test accuracy is 0.9517271501547694
       processing identity_hate
   test accuracy is 0.9910556600011394

linearsvc

svc_pipeline = pipeline([
                ('tfidf', tfidfvectorizer(stop_words=stop_words)),
                ('clf', onevsrestclassifier(linearsvc(), n_jobs=1)),
            ])
for category in categories:
    print('... processing {}'.format(category))
    # train the model using x_dtm & y
    svc_pipeline.fit(x_train, train[category])
    # compute the testing accuracy
    prediction = svc_pipeline.predict(x_test)
    print('test accuracy is {}'.format(accuracy_score(test[category], prediction
)))

       processing toxic
   test accuracy is 0.9599498661197516
       processing severe_toxic
   test accuracy is 0.9906948479842003
       processing obscene
   test accuracy is 0.9789019920621356
       processing threat
   test accuracy is 0.9974173455629617
       processing insult
   test accuracy is 0.9712299891756395
       processing identity_hate
   test accuracy is 0.9919861752027194

id28

logreg_pipeline = pipeline([
                ('tfidf', tfidfvectorizer(stop_words=stop_words)),
                ('clf', onevsrestclassifier(logisticregression(solver='sag'), n_
jobs=1)),
            ])
for category in categories:
    print('... processing {}'.format(category))
    # train the model using x_dtm & y
    logreg_pipeline.fit(x_train, train[category])
    # compute the testing accuracy
    prediction = logreg_pipeline.predict(x_test)
    print('test accuracy is {}'.format(accuracy_score(test[category], prediction
)))

       processing toxic
   test accuracy is 0.9548415275641391
       processing severe_toxic
   test accuracy is 0.9910556600011394
       processing obscene
   test accuracy is 0.9761104464573956
       processing threat
   test accuracy is 0.9973793653506523
       processing insult
   test accuracy is 0.9687612753755294
       processing identity_hate
   test accuracy is 0.991758293928863

   the three classifiers produced similar results. we have created a
   strong baseline for the toxic comment multi-label text classification
   problem.

   the full code for this post can be found on [21]github. i look forward
   to hearing any feedback or comment.

     * [22]machine learning
     * [23]nlp
     * [24]data science
     * [25]scikit learn
     * [26]python

   (button)
   (button)
   (button) 2k claps
   (button) (button) (button) 18 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of susan li

[28]susan li

   becoming an expert in ml, nlp, data story telling and encouraging
   others to do the same. sr data scientist, toronto canada.
   [29]https://www.linkedin.com/in/susanli/

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2k
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/30714b7819c5
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_4ryj8k0gm64u---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@actsusanli?source=post_header_lockup
  17. https://towardsdatascience.com/@actsusanli
  18. https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f
  19. https://en.wikipedia.org/wiki/multi-label_classification
  20. https://www.kaggle.com/jhoward/nb-id166-strong-linear-baseline/data
  21. https://github.com/susanli2016/machine-learning-with-python/blob/master/multi label text classification.ipynb
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/nlp?source=post
  24. https://towardsdatascience.com/tagged/data-science?source=post
  25. https://towardsdatascience.com/tagged/scikit-learn?source=post
  26. https://towardsdatascience.com/tagged/python?source=post
  27. https://towardsdatascience.com/@actsusanli?source=footer_card
  28. https://towardsdatascience.com/@actsusanli
  29. https://www.linkedin.com/in/susanli/
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/30714b7819c5/share/twitter
  36. https://medium.com/p/30714b7819c5/share/facebook
  37. https://medium.com/p/30714b7819c5/share/twitter
  38. https://medium.com/p/30714b7819c5/share/facebook
