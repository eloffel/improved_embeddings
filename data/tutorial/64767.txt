   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

coding neural network         parameters    initialization

   [16]go to the profile of imad dabbura
   [17]imad dabbura (button) blockedunblock (button) followfollowing
   apr 19, 2018
   [0*lfeja5fdskne0a_q.png]

   optimization, in machine learning/deep learning contexts, is the
   process of changing the model   s parameters to improve its performance.
   in other words, it   s the process of finding the best parameters in the
   predefined hypothesis space to get the best possible performance. there
   are three kinds of optimization algorithms:
     * optimization algorithm that is not iterative and simply solves for
       one point.
     * optimization algorithm that is iterative in nature and converges to
       acceptable solution regardless of the parameters initialization
       such as id119 applied to id28.
     * optimization algorithm that is iterative in nature and applied to a
       set of problems that have non-convex id168s such as neural
       networks. therefore, parameters    initialization plays a critical
       role in speeding up convergence and achieving lower error rates.

   in this post, we   ll look at three different cases of parameters   
   initialization and see how this affects the error rate:
    1. initialize all parameters to zero.
    2. initialize parameters to random values from standard normal
       distribution or uniform distribution and multiply it by a scalar
       such as 10.
    3. initialize parameters based on:

     * xavier recommendation.
     * kaiming he recommendation.

   we   ll be using functions we wrote in [18]   coding neural
   network         forward propagation and id26    post to initialize
   parameters, compute forward propagation and back-propagation as well as
   the cross-id178 cost.

   to illustrate the above cases, we   ll use the cats vs dogs dataset which
   consists of 50 images for cats and 50 images for dogs. each image is
   150 pixels x 150 pixels on rgb color scale. therefore, we would have
   67,500 features where each column in the input matrix would be one
   image which means our input data would have 67,500 x 100 dimension.

   let   s first load the data and show a sample of two images before we
   start the helper functions.

   iframe: [19]/media/4c3df18aac8cb1534267ae7703d3cfd2?postid=f7c2d770e874

   [0*kl3kynxatsggmxuw.png]
   figure 1: sample images

   we   ll write now all the helper functions that will help us initialize
   parameters based on different methods as well as writing l-layer model
   that we   ll be using to train our neural network.

   iframe: [20]/media/0025e6381037900c79774e52b6c2048e?postid=f7c2d770e874

   iframe: [21]/media/065a1c79a8c5d1fa7e7d351a2e3a57de?postid=f7c2d770e874

initializing all parameters to zero

   here, we   ll initialize all weight matrices and biases to zeros and see
   how this would affect the error rate as well as the learning
   parameters.
# train nn with zeros initialization parameters
layers_dims = [x.shape[0], 5, 5, 1]
parameters = model(x, y, layers_dims, hidden_layers_activation_fn="tanh", initia
lization_method="zeros") accuracy(x, parameters, y,"tanh")
the cost after 100 iterations is: 0.6931471805599453
the cost after 200 iterations is: 0.6931471805599453
the cost after 300 iterations is: 0.6931471805599453
the cost after 400 iterations is: 0.6931471805599453
the cost after 500 iterations is: 0.6931471805599453
the cost after 600 iterations is: 0.6931471805599453
the cost after 700 iterations is: 0.6931471805599453
the cost after 800 iterations is: 0.6931471805599453
the cost after 900 iterations is: 0.6931471805599453
the cost after 1000 iterations is: 0.6931471805599453
the accuracy rate is: 50.00%.

   [0*f1-5n730cjtpc0kt.png]
   figure 2: cost curve using zero intialization method

   as the cost curve shows, the neural network didn   t learn anything! that
   is because of symmetry between all neurons which leads to all neurons
   have the same update on every iteration. therefore, regardless of how
   many iterations we run the optimization algorithms, all the neurons
   would still get the same update and no learning would happen. as a
   result, we must break symmetry when initializing parameters so that the
   model would start learning on each update of the id119.

initializing parameters with big random values

   there is no big difference if the random values are initialized from
   standard normal distribution or uniform distribution so we   ll use
   standard normal distribution in our examples. also, we   ll multiply the
   random values by a big number such as 10 to show that initializing
   parameters to big values may cause our optimization to have higher
   error rates (and even diverge in some cases). let   s now train our
   neural network where all weight matrices have been intitialized using
   the following formula: np.random.randn() * 10
# train nn with random initialization parameters
layers_dims = [x.shape[0], 5, 5, 1]
parameters = model(x, y, layers_dims, hidden_layers_activation_fn="tanh", initia
lization_method="random") accuracy(x, parameters, y,"tanh")
the cost after 100 iterations is: 1.2413142077549013
the cost after 200 iterations is: 1.1258751902393416
the cost after 300 iterations is: 1.0989052435267657
the cost after 400 iterations is: 1.0840966471282327
the cost after 500 iterations is: 1.0706953292105978
the cost after 600 iterations is: 1.0574847320236294
the cost after 700 iterations is: 1.0443168708889223
the cost after 800 iterations is: 1.031157857251139
the cost after 900 iterations is: 1.0179838815204902
the cost after 1000 iterations is: 1.004767088515343
the accuracy rate is: 55.00%.

   [0*zmky_2ncqivlayzx.png]
   figure 3: cost curve using random initialization method

   random initialization here is helping but still the id168 has
   high value and may take long time to converge and achieve a
   significantly low value.

initializing parameters based on he and xavier recommendations

   we   ll explore two initialization methods:
     * kaiming he method is best applied when activation function applied
       on hidden layers is rectified linear unit (relu). so that the
       weight on each hidden layer would have the following variance:
       var(w^l )= 2/n^(l-1). we can achieve this by multiplying the random
       values from standard normal distribution by

   [1*icxsqj1lzk92yjtskoe4wg.png]
     * xavier method is best applied when activation function applied on
       hidden layers is hyperbolic tangent so that the weight on each
       hidden layer would have the following variance: var(w^l )=
       1/n^(l-1). we can achieve this by multiplying the random values
       from standard normal distribution by

   [1*mumqfci5xmjhxhxka7mjrg.png]

   we   ll train the network using both methods and look at the results.
# train nn where all parameters were initialized based on he recommendation
layers_dims = [x.shape[0], 5, 5, 1]
parameters = model(x, y, layers_dims, hidden_layers_activation_fn="tanh", initia
lization_method="he")
accuracy(x, parameters, y,"tanh")
the cost after 100 iterations is: 0.6300611704834093
the cost after 200 iterations is: 0.49092836452522753
the cost after 300 iterations is: 0.46579423512433943
the cost after 400 iterations is: 0.6516254192289226
the cost after 500 iterations is: 0.32487779301799485
the cost after 600 iterations is: 0.4631461605716059
the cost after 700 iterations is: 0.8050310690163623
the cost after 800 iterations is: 0.31739195517372376
the cost after 900 iterations is: 0.3094592175030812
the cost after 1000 iterations is: 0.19934509244449203
the accuracy rate is: 99.00%.

   [0*tu_r_1h3fwqk4xze.png]
   figure 4: cost curve using he initialization method
# train nn where all parameters were initialized based on xavier recommendation
layers_dims = [x.shape[0], 5, 5, 1]
parameters = model(x, y, layers_dims, hidden_layers_activation_fn="tanh", initia
lization_method="xavier") accuracy(x, parameters, y,"tanh")
accuracy(x, parameters, y, "tanh")
the cost after 100 iterations is: 0.6351961521800779
the cost after 200 iterations is: 0.548973489787121
the cost after 300 iterations is: 0.47982386652748565
the cost after 400 iterations is: 0.32811768889968684
the cost after 500 iterations is: 0.2793453045790634
the cost after 600 iterations is: 0.3258507563809604
the cost after 700 iterations is: 0.2873032724176074
the cost after 800 iterations is: 0.0924974839405706
the cost after 900 iterations is: 0.07418011931058155
the cost after 1000 iterations is: 0.06204402572328295
the accuracy rate is: 99.00%.

   [0*3u8legg1wneltmp9.png]
   figure 5: cost curve using xavier initialization method

   as shown from applying the four methods, parameters    initial values
   play a huge role in achieving low cost values as well as converging and
   achieve lower training error rates. the same would apply to test error
   rate if we had test data.

conclusion

   deep learning frameworks make it easier to choose between different
   initialization methods without worrying about implementing it
   ourselves. nonetheless, it   s important to understand the critical role
   initial values of the parameters in the overall performance of the
   network. below are some key takeaways:
     * well chosen initialization values of parameters leads to:

    1. speed up convergence of id119.
    2. increase the likelihood of id119 to find lower training
       and generalization error rates.

     * because we   re dealing with iterative optimization algorithms with
       non-convex id168, different initializations lead to
       different results.
     * random initialization is used to break symmetry and make sure
       different hidden units can learn different things.
     * don   t initialize to values that are too large.
     * kaiming he (he) initialization works well for neural networks with
       relu activation function.
     * xavier initialization works well for neural networks with
       hyperbolic tangent activation function.

   the source code that created this post can be found [22]here.
     __________________________________________________________________

   originally published at [23]imaddabbura.github.io on april 20, 2018.

     * [24]machine learning
     * [25]ai
     * [26]artificial intelligence
     * [27]deep learning
     * [28]data science

   (button)
   (button)
   (button) 305 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of imad dabbura

[30]imad dabbura

   data scientist

     (button) follow
   [31]towards data science

[32]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 305
     * (button)
     *
     *

   [33]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [34]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f7c2d770e874
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/coding-neural-network-parameters-initialization-f7c2d770e874&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/coding-neural-network-parameters-initialization-f7c2d770e874&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_b33r08nbldcb---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@imadphd?source=post_header_lockup
  17. https://towardsdatascience.com/@imadphd
  18. https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76
  19. https://towardsdatascience.com/media/4c3df18aac8cb1534267ae7703d3cfd2?postid=f7c2d770e874
  20. https://towardsdatascience.com/media/0025e6381037900c79774e52b6c2048e?postid=f7c2d770e874
  21. https://towardsdatascience.com/media/065a1c79a8c5d1fa7e7d351a2e3a57de?postid=f7c2d770e874
  22. https://github.com/imaddabbura/blog-posts/blob/master/notebooks/coding-neural-network-parameters-initialization.ipynb
  23. https://imaddabbura.github.io/post/coding_neural_network_parameters_initialization/
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/ai?source=post
  26. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  27. https://towardsdatascience.com/tagged/deep-learning?source=post
  28. https://towardsdatascience.com/tagged/data-science?source=post
  29. https://towardsdatascience.com/@imadphd?source=footer_card
  30. https://towardsdatascience.com/@imadphd
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/f7c2d770e874/share/twitter
  37. https://medium.com/p/f7c2d770e874/share/facebook
  38. https://medium.com/p/f7c2d770e874/share/twitter
  39. https://medium.com/p/f7c2d770e874/share/facebook
