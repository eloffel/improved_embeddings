   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

id3 (gans) in 50 lines of code (pytorch)

   [9]go to the profile of dev nag
   [10]dev nag (button) blockedunblock (button) followfollowing
   feb 10, 2017

   tl;dr: gans are simpler to set up than you think
   [1*az5-3wdndyyc2u0aq7rhig.png]

   in 2014, ian goodfellow and his colleagues at the university of
   montreal published a [11]stunning paper introducing the world to gans,
   or id3. through an innovative combination
   of computational graphs and game theory they showed that, given enough
   modeling power, two models fighting against each other would be able to
   co-train through plain old id26.

   the models play two distinct (literally, adversarial) roles. given some
   real data set r, g is the generator, trying to create fake data that
   looks just like the genuine data, while d is the discriminator, getting
   data from either the real set or g and labeling the difference.
   goodfellow   s metaphor (and a fine one it is) was that g was like a team
   of forgers trying to match real paintings with their output, while d
   was the team of detectives trying to tell the difference. (except that
   in this case, the forgers g never get to see the original data         only
   the judgments of d. they   re like blind forgers.)
   [1*-gfsbymy9ojuqj-a3gtfeg.png]

   in the ideal case, both d and g would get better over time until g had
   essentially become a    master forger    of the genuine article and d was
   at a loss,    unable to differentiate between the two distributions.   

   in practice, what goodfellow had shown was that g would be able to
   perform a form of unsupervised learning on the original dataset,
   finding some way of representing that data in a (possibly) much
   lower-dimensional manner. and as yann lecun famously stated,
   [12]unsupervised learning is the    cake    of true ai.
     __________________________________________________________________

   this powerful technique seems like it must require a metric ton of code
   just to get started, right? nope. using [13]pytorch, we can actually
   create a very simple gan in under 50 lines of code. there are really
   only 5 components to think about:
     * r: the original, genuine data set
     * i: the random noise that goes into the generator as a source of
       id178
     * g: the generator which tries to copy/mimic the original data set
     * d: the discriminator which tries to tell apart g   s output from r
     * the actual    training    loop where we teach g to trick d and d to
       beware g.

   1.) r: in our case, we   ll start with the simplest possible r         a bell
   curve. this function takes a mean and a standard deviation and returns
   a function which provides the right shape of sample data from a
   gaussian with those parameters. in our sample code, we   ll use a mean of
   4.0 and a standard deviation of 1.25.
   [1*xsue-nhsjozk9lfi3rayuw.png]

   2.) i: the input into the generator is also random, but to make our job
   a little bit harder, let   s use a uniform distribution rather than a
   normal one. this means that our model g can   t simply shift/scale the
   input to copy r, but has to reshape the data in a non-linear way.
   [1*wuhevnk25v3zxqzucwfdag.png]

   3.) g: the generator is a standard feedforward graph         two hidden
   layers, three linear maps. we   re using a hyperbolic tangent activation
   function    cuz we   re old-school like that. g is going to get the
   uniformly distributed data samples from i and somehow mimic the
   normally distributed samples from r         without ever seeing r.
   [1*zwdlje92gogco2ickgz3ta.png]

   4.) d: the discriminator code is very similar to g   s generator code; a
   feedforward graph with two hidden layers and three linear maps. the
   activation function here is a sigmoid         nothing fancy, people. it   s
   going to get samples from either r or g and will output a single scalar
   between 0 and 1, interpreted as    fake    vs.    real   . in other words, this
   is about as milquetoast as a neural net can get.
   [1*k92baysiin49q2stuwnvtw.png]

   5.) finally, the training loop alternates between two modes: first
   training d on real data vs. fake data, with accurate labels (think of
   this as [14]police academy); and then training g to fool d, with
   inaccurate labels (this is more like those preparation montages from
   [15]ocean   s eleven). it   s a fight between good and evil, people.
   [1*gnhl1t1dr4yxcti1b5u03a.png]

   even if you haven   t seen pytorch before, you can probably tell what   s
   going on. in the first (green) section, we push both types of data
   through d and apply a differentiable criterion to d   s guesses vs. the
   actual labels. that pushing is the    forward    step; we then call
      backward()    explicitly in order to calculate gradients, which are then
   used to update d   s parameters in the d_optimizer step() call. g is used
   but isn   t trained here.

   then in the last (red) section, we do the same thing for g         note that
   we also run g   s output through d (we   re essentially giving the forger a
   detective to practice on) but we do not optimize or change d at this
   step. we don   t want the detective d to learn the wrong labels. hence,
   we only call g_optimizer.step().

   and   that   s all. there   s some other boilerplate code but the
   gan-specific stuff is just those 5 components, nothing else.
     __________________________________________________________________

   after a few thousand rounds of this forbidden dance between d and g,
   what do we get? the discriminator d gets good very quickly (while g
   slowly moves up), but once it gets to a certain level of power, g has a
   worthy adversary and begins to improve. really improve.

   over 5,000 training rounds, training d 20 times and then g 20 times in
   each round, the mean of g   s output overshoots 4.0 but then comes back
   in a fairly stable, correct range (left). likewise, the standard
   deviation initially drops in the wrong direction but then rises up to
   the desired 1.25 range (right), matching r.
   [1*2qm33rqwbkvf3g1vg2hnvg.png]

   ok, so the basic stats match r, eventually. how about the higher
   moments? does the shape of the distribution look right? after all, you
   could certainly have a uniform distribution with a mean of 4.0 and a
   standard deviation of 1.25, but that wouldn   t really match r. let   s
   look at the final distribution emitted by g:
   [1*qddj7cglg2thkqwfs0q1lg.png]

   not bad. the right tail is a bit fatter than the left, but the skew and
   kurtosis are, shall we say, evocative of the original gaussian.

   g recovers the original distribution r nearly perfectly         and d is left
   cowering in the corner, mumbling to itself, unable to tell fact from
   fiction. this is precisely the behavior we want (see [16]figure 1 in
   goodfellow). from fewer than 50 lines of code.

   now, a word of warning: gans can be picky. and fragile. and when they
   get into weird states, they often don   t come out without a bit of
   coaxing. running my sample code ten times (over 5,000 rounds each)
   showed the following ten distributions:
   [1*emrya3l8hkzra6zhkq07pq.png]

   eight of the ten runs result in pretty good final
   distributions         resembling gaussians with means of 4 and standard
   deviations in the right ballpark. but two of the runs don   t         in one
   case (run #5), there   s a concave distribution with a mean around 6.0
   and in the last run (#10), there   s a narrow peak at -11! as you start
   to apply gans across pretty much any context, you   ll see this
   phenomenon         gans are not nearly as stable as, say, the average
   supervised learning workflow. but when they work, they can seem almost
   magical.

   goodfellow would go on to publish many other papers on gans, including
   a [17]2016 gem describing some practical improvements, including the
   minibatch discrimination method adapted here. and [18]here   s a 2-hour
   tutorial he presented at nips 2016. for tensorflow users, here   s a
   parallel [19]post from aylien on gans.

   ok. enough talk. [20]go look at the code.

     * [21]machine learning
     * [22]artificial intelligence
     * [23]deep learning
     * [24]unsupervised learning
     * [25]machine intelligence

   (button)
   (button)
   (button) 4k claps
   (button) (button) (button) 32 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of dev nag

[27]dev nag

   ai/ml & startup strategy. founder/cto @ wavefront (acq. by vmware).
   former google, paypal engineer. stanford math.

     * (button)
       (button) 4k
     * (button)
     *
     *

   [28]go to the profile of dev nag
   never miss a story from dev nag, when you sign up for medium. [29]learn
   more
   never miss a story from dev nag
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e81b79659e3f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@devnag?source=post_header_lockup
  10. https://medium.com/@devnag
  11. https://arxiv.org/pdf/1406.2661.pdf
  12. https://www.facebook.com/yann.lecun/posts/10153426023477143
  13. http://pytorch.org/
  14. https://en.wikipedia.org/wiki/police_academy_(film)
  15. https://en.wikipedia.org/wiki/ocean's_eleven
  16. https://arxiv.org/pdf/1406.2661.pdf
  17. https://arxiv.org/pdf/1606.03498.pdf
  18. https://channel9.msdn.com/events/neural-information-processing-systems-conference/neural-information-processing-systems-conference-nips-2016/generative-adversarial-networks
  19. http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/
  20. https://github.com/devnag/pytorch-generative-adversarial-networks
  21. https://medium.com/tag/machine-learning?source=post
  22. https://medium.com/tag/artificial-intelligence?source=post
  23. https://medium.com/tag/deep-learning?source=post
  24. https://medium.com/tag/unsupervised-learning?source=post
  25. https://medium.com/tag/machine-intelligence?source=post
  26. https://medium.com/@devnag?source=footer_card
  27. https://medium.com/@devnag
  28. https://medium.com/@devnag
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/e81b79659e3f/share/twitter
  32. https://medium.com/p/e81b79659e3f/share/facebook
  33. https://medium.com/p/e81b79659e3f/share/twitter
  34. https://medium.com/p/e81b79659e3f/share/facebook
