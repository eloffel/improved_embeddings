   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]tensorflow
     * [9]announcements
     * [10]keras
     * [11]tensorflow.js
     * [12]mobile
     * [13]responsible ai
     * [14]all stories
     * [15]tensorflow.org
     __________________________________________________________________

classifying text with tensorflow estimators

   [16]go to the profile of julian eisenschlos
   [17]julian eisenschlos (button) blockedunblock (button) followfollowing
   mar 7, 2018

   posted by sebastian ruder and julian eisenschlos, google developer
   experts

   hello there! throughout this post we will show you how to classify text
   using estimators in tensorflow. here   s the outline of what we   ll cover:
     * loading data using datasets.
     * building baselines using pre-canned estimators.
     * using id27s.
     * building custom estimators with convolution and lstm layers.
     * loading pre-trained word vectors.
     * evaluating and comparing models using tensorboard.
     __________________________________________________________________

   welcome to part 4 of a blog series that introduces tensorflow datasets
   and estimators. you don   t need to read all of the previous material,
   but take a look if you want to refresh any of the following concepts.
   [18]part 1 focused on pre-made estimators, [19]part 2 discussed feature
   columns, and [20]part 3 how to create custom estimators.

   here in part 4, we will build on top of all the above to tackle a
   different family of problems in natural language processing (nlp). in
   particular, this article demonstrates how to solve a text
   classification task using custom tensorflow estimators, embeddings, and
   the [21]tf.layers module. along the way, we   ll learn about id97 and
   id21 as a technique to bootstrap model performance when
   labeled data is a scarce resource.

   we will show you relevant code snippets. [22]here   s the complete
   jupyter notebook that you can run locally or on [23]google
   colaboratory. the plain .py source file is also available [24]here.
   note that the code was written to demonstrate how estimators work
   functionally and was not optimized for maximum performance.

the task

   the dataset we will be using is the imdb [25]large movie review
   dataset, which consists of 25,000 highly polar movie reviews for
   training, and 25,000 for testing. we will use this dataset to train a
   binary classification model, able to predict whether a review is
   positive or negative.

   for illustration, here   s a piece of a negative review (with 2 stars) in
   the dataset:

     now, i love italian horror films. the cheesier they are, the better.
     however, this is not cheesy italian. this is week-old spaghetti
     sauce with rotting meatballs. it is amateur hour on every level.
     there is no suspense, no horror, with just a few drops of blood
     scattered around to remind you that you are in fact watching a
     horror film.

   keras provides a convenient handler for importing the dataset which is
   also available as a serialized numpy array .npz file to download
   [26]here. for text classification, it is standard to limit the size of
   the vocabulary to prevent the dataset from becoming too sparse and high
   dimensional, causing potential overfitting. for this reason, each
   review consists of a series of word indexes that go from 4 (the most
   frequent word in the dataset: the) to 4999, which corresponds to
   orange. index 1 represents the beginning of the sentence and the index
   2 is assigned to all unknown (also known as out-of-vocabulary or oov)
   tokens. these indexes have been obtained by pre-processing the text
   data in a pipeline that cleans, normalizes and tokenizes each sentence
   first and then builds a dictionary indexing each of the tokens by
   frequency.

   after we   ve loaded the data in memory we pad each of the sentences with
   0 to a fixed size (here: 200) so that we have two 2-dimensional
   25000  200 arrays for training and testing respectively.

   iframe: [27]/media/f82c1a98d20995fc6b3f1c8e9fdf8958?postid=a99603033fbe

input functions

   the estimator framework uses input functions to split the data pipeline
   from the model itself. several helper methods are available to create
   them, whether your data is in a .csv file, or in a pandas.dataframe,
   whether it fits in memory or not. in our case, we can use
   dataset.from_tensor_slices for both the train and test sets.

   iframe: [28]/media/c60e3c72417457a0630676216a979d5a?postid=a99603033fbe

   we shuffle the training data and do not predefine the number of epochs
   we want to train, while we only need one epoch of the test data for
   evaluation. we also add an additional "len" key that captures the
   length of the original, unpadded sequence, which we will use later.

   datasets can work with out-of-memory sources (not needed in this case)
   by streaming them record by record, and the shuffle method uses a
   buffer_size to continuously sample from fixed sized set without loading
   the entire thing into memory.

building a baseline

   it   s good practice to start any machine learning project trying basic
   baselines. the simpler the better as having a simple and robust
   baseline is key to understanding exactly how much we are gaining in
   terms of performance by adding extra complexity. it may very well be
   the case that a simple solution is good enough for our requirements.

   with that in mind, let us start by trying out one of the simplest
   models for text classification. that would be a sparse linear model
   that gives a weight to each token and adds up all of the results,
   regardless of the order. as this model does not care about the order of
   words in a sentence, we normally refer to it as a bag-of-words
   approach. let   s see how we can implement this model using an estimator.

   we start out by defining the feature column that is used as input to
   our classifier. as we have seen in [29]part 2,
   categorical_column_with_identity is the right choice for this
   pre-processed text input. if we were feeding raw text tokens other
   feature_columns could do a lot of the pre-processing for us. we can now
   use the pre-made linearclassifier.

   iframe: [30]/media/d795bf0cefdf06bf3d7b960c3da62c72?postid=a99603033fbe

   finally, we create a simple function that trains the classifier and
   additionally creates a precision-recall curve. as we do not aim to
   maximize performance in this blog post, we only train our models for
   25,000 steps.

   iframe: [31]/media/708371f7d4348142146c039732a84291?postid=a99603033fbe

   one of the benefits of choosing a simple model is that it is much more
   interpretable. the more complex a model, the harder it is to inspect
   and the more it tends to work like a black box. in this example, we can
   load the weights from our model   s last checkpoint and take a look at
   what tokens correspond to the biggest weights in absolute value. the
   results look like what we would expect.

   iframe: [32]/media/2260e3f91540dc986b1bbb409d2d2d2c?postid=a99603033fbe

   [0*tcgmsiaq9ptb9lml.png]

   as we can see, tokens with the most positive weight such as
      refreshing    are clearly associated with positive sentiment, while
   tokens that have a large negative weight unarguably evoke negative
   emotions. a simple but powerful modification that one can do to improve
   this model is weighting the tokens by their [33]tf-idf scores.

embeddings

   the next step of complexity we can add are id27s. embeddings
   are a dense low-dimensional representation of sparse high-dimensional
   data. this allows our model to learn a more meaningful representation
   of each token, rather than just an index. while an individual dimension
   is not meaningful, the low-dimensional space         when learned from a
   large enough corpus         has been shown to capture relations such as
   tense, plural, gender, thematic relatedness, and many more. we can add
   id27s by converting our existing feature column into an
   embedding_column. the representation seen by the model is the mean of
   the embeddings for each token (see the combiner argument in the
   [34]docs). we can plug in the embedded features into a pre-canned
   dnnclassifier.

   a note for the keen observer: an embedding_column is just an efficient
   way of applying a fully connected layer to the sparse binary feature
   vector of tokens, which is multiplied by a constant depending of the
   chosen combiner. a direct consequence of this is that it wouldn   t make
   sense to use an embedding_column directly in a linearclassifier because
   two consecutive linear layers without non-linearities in between add no
   prediction power to the model, unless of course the embeddings are
   pre-trained.

   iframe: [35]/media/d4b1fb8d5217506078407a25c43c0cec?postid=a99603033fbe

   we can use tensorboard to visualize our 50-dimensional word vectors
   projected into r   using [36]id167. we expect similar words to be close
   to each other. this can be a useful way to inspect our model weights
   and find unexpected behaviors.
   [0*lxce4spyhtev4lvh.gif]

convolutions

   at this point one possible approach would be to go deeper, further
   adding more fully connected layers and playing around with layer sizes
   and training functions. however, by doing that we would add extra
   complexity and ignore important structure in our sentences. words do
   not live in a vacuum and meaning is compositional, formed by words and
   its neighbors.

   convolutions are one way to take advantage of this structure, similar
   to how we can model salient clusters of pixels for [37]image
   classification. the intuition is that certain sequences of words, or
   id165s, usually have the same meaning regardless of their overall
   position in the sentence. introducing a structural prior via the
   convolution operation allows us to model the interaction between
   neighboring words and consequently gives us a better way to represent
   such meaning.

   the following image shows how a filter matrix f of shape d  m slides
   across each 3-gram window of tokens to build a new feature map.
   afterwards a pooling layer is usually applied to combine adjacent
   results.
   [1*tsw55mivzhwb-gra21q7zw.png]
   source: [38]learning to rank short text pairs with convolutional deep
   neural networks by severyn et al. [2015]

   let us look at the full model architecture. the use of dropout layers
   is a id173 technique that makes the model less likely to
   overfit.
   [1*zwj1g4hem-px1i54j_9gaw.png]

   as seen in previous blog posts, the tf.estimator framework provides a
   high-level api for training machine learning models, defining train(),
   evaluate() and predict() operations, handling checkpointing, loading,
   initializing, serving, building the graph and the session out of the
   box. there is a small family of pre-made estimators, like the ones we
   used earlier, but it   s most likely that you will need to [39]build your
   own.

   writing a custom estimator means writing a model_fn(features, labels,
   mode, params) that returns an estimatorspec. the first step will be
   mapping the features into our embedding layer:

   iframe: [40]/media/5ea0dadcf3bc5421378bbbf2b2426439?postid=a99603033fbe

   then we use tf.layers to process each output sequentially.

   iframe: [41]/media/8b1e1b974f46c135acc15777dc1db4a2?postid=a99603033fbe

   finally, we will use a head to simplify the writing of our last part of
   the model_fn. the head already knows how to compute predictions, loss,
   train_op, metrics and export outputs, and can be reused across models.
   this is also used in the pre-made estimators and provides us with the
   benefit of a uniform evaluation function across all of our models. we
   will use binary_classification_head, which is a head for single label
   binary classification that uses sigmoid_cross_id178_with_logits as
   the id168 under the hood.

   iframe: [42]/media/8a7e1e297a6b98e34014267c406e5373?postid=a99603033fbe

   running this model is just as easy as before:

   iframe: [43]/media/2a528965a76f35f38af8e4b226214ab3?postid=a99603033fbe

id137

   using the estimator api and the same model head, we can also create a
   classifier that uses a long short-term memory (lstm) cell instead of
   convolutions. recurrent models such as this are some of the most
   successful building blocks for nlp applications. an lstm processes the
   entire document sequentially, recursing over the sequence with its cell
   while storing the current state of the sequence in its memory.

   one of the drawbacks of recurrent models compared to id98s is that,
   because of the nature of recursion, models turn out deeper and more
   complex, which usually produces slower training time and worse
   convergence. lstms (and id56s in general) can suffer convergence issues
   like vanishing or exploding gradients, that said, with sufficient
   tuning they can obtain state-of-the-art results for many problems. as a
   rule of thumb id98s are good at feature extraction, while id56s excel at
   tasks that depend on the meaning of the whole sentence, like question
   answering or machine translation.

   each cell processes one token embedding at a time and updates its
   internal state based on a differentiable computation that depends on
   both the embedding vector x at time t    and the previous state h at time
   t   1   . in order to get a better understanding of how lstms work, you can
   refer to chris olah   s [44]blog post.
   [0*_lp7l6lhnbheterr.png]
   source: [45]understanding id137 by chris olah

   the complete lstm model can be expressed by the following simple
   flowchart:
   [1*y3w54qfx7d2p0i4fkhuddq.png]

   in the beginning of this post, we padded all documents up to 200
   tokens, which is necessary to build a proper tensor. however, when a
   document contains fewer than 200 words, we don   t want the lstm to
   continue processing padding tokens as it does not add information and
   degrades performance. for this reason, we additionally want to provide
   our network with the length of the original sequence before it was
   padded. internally, the model then copies the last state through to the
   sequence   s end. we can do this by using the "len" feature in our input
   functions. we can now use the same logic as above and simply replace
   the convolutional, pooling, and flatten layers with our lstm cell.

   iframe: [46]/media/de14c14c68ca157fe85040ddd85289bf?postid=a99603033fbe

pre-trained vectors

   most of the models that we have shown before rely on id27s as
   a first layer. so far, we have initialized this embedding layer
   randomly. however, [47]much [48]previous [49]work has shown that using
   embeddings pre-trained on a large unlabeled corpus as initialization is
   beneficial, particularly when training on only a small number of
   labeled examples. the most popular pre-trained embedding is
   [50]id97. leveraging knowledge from unlabeled data via pre-trained
   embeddings is an instance of [51]id21.

   to this end, we will show you how to use them in an estimator. we will
   use the pre-trained vectors from another popular model, [52]glove.

   iframe: [53]/media/d5baa867b3d0faaefda81d95f9d8d359?postid=a99603033fbe

   after loading the vectors into memory from a file we store them as a
   numpy.array using the same indexes as our vocabulary. the created array
   is of shape (5000, 50). at every row index, it contains the
   50-dimensional vector representing the word at the same index in our
   vocabulary.

   iframe: [54]/media/c39da8a327d73aa59f69d5e267e67910?postid=a99603033fbe

   finally, we can use a custom initializer function and pass it in the
   params object to our id98_model_fn , without any modifications.

   iframe: [55]/media/cfdf3f4099f903ea8e0d5aefdb1bbdef?postid=a99603033fbe

running tensorboard

   now we can launch tensorboard and see how the different models we   ve
   trained compare against each other in terms of training time and
   performance.

   in a terminal, we run
> tensorboard --logdir={model_dir}

   we can visualize many metrics collected while training and testing,
   including the id168 values of each model at each training step,
   and the precision-recall curves. this is of course most useful to
   select which model works best for our use-case as well as how to choose
   classification thresholds.
   [1*_tl-on9ivzdcwyvgz4lf3a.png]
   [1*acyqhb1znyzq3ydtiy7wpg.png]
   precision recall curves on the test data for each of our models on the
   left and training loss across steps on the right

getting predictions

   to obtain predictions on new sentences we can use the predict method in
   the estimator instances, which will load the latest checkpoint for each
   model and evaluate on the unseen examples. but before passing the data
   into the model we have to clean up, tokenize and map each token to the
   corresponding index as we see below.

   iframe: [56]/media/e63398f87ce09f0f8a9461ec3e111460?postid=a99603033fbe

   it is worth noting that the checkpoint itself is not sufficient to make
   predictions; the actual code used to build the estimator is necessary
   as well in order to map the saved weights to the corresponding tensors.
   it   s a good practice to associate saved checkpoints with the branch of
   code with which they were created.

   if you are interested in exporting the models to disk in a fully
   recoverable way, you might want to look into the [57]savedmodel class,
   which is especially useful for serving your model through an api using
   [58]tensorflow serving.

   in this blog post, we explored how to use estimators for text
   classification, in particular for the imdb reviews dataset. we trained
   and visualized our own embeddings, as well as loaded pre-trained ones.
   we started from a simple baseline and made our way to convolutional
   neural networks and lstms.

   for more details, be sure to check out:
     * a [59]jupyter notebook that can run locally, or on
       [60]colaboratory.
     * the complete [61]source code for this blog post.
     * the tensorflow [62]embedding guide.
     * the tensorflow [63]vector representation of words tutorial.
     * the nltk [64]processing raw text chapter on how to design langage
       pipelines.

   in a following post, we will show how to build a model using eager
   execution, work with out-of-memory datasets, train in cloud ml, and
   deploy with tensorflow serving.
     __________________________________________________________________

   thanks for reading! if you like you can find us online at [65]ruder.io
   and [66]@eisenjulian. send our way all your feedback and questions.
   [67]julian eisenschlos (@eisenjulian) | twitter
   the latest tweets from julian eisenschlos (@eisenjulian). math, nlp,
   deep learning * co-founder @botmaker_io *   twitter.com
   [68]sebastian ruder (@seb_ruder) | twitter
   the latest tweets from sebastian ruder (@seb_ruder). nlp, deep learning
   phd student @insight_centre * research   twitter.com

   thanks to [69]sebastian ruder and [70]tensorflow.
     * [71]machine learning
     * [72]tensorflow
     * [73]nlp
     * [74]tutorial
     * [75]community

   (button)
   (button)
   (button) 228 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [76]go to the profile of julian eisenschlos

[77]julian eisenschlos

   mathematician and machine learning engineer focusing on data-fusion and
   nlp. currently chief scientist at botmaker. previously software
   engineer at facebook.

     (button) follow
   [78]tensorflow

[79]tensorflow

   tensorflow is a fast, flexible, and scalable open-source machine
   learning library for research and production.

     * (button)
       (button) 228
     * (button)
     *
     *

   [80]tensorflow
   never miss a story from tensorflow, when you sign up for medium.
   [81]learn more
   never miss a story from tensorflow
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a99603033fbe
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/tensorflow/classifying-text-with-tensorflow-estimators-a99603033fbe&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/tensorflow/classifying-text-with-tensorflow-estimators-a99603033fbe&source=--------------------------nav_reg&operation=register
   8. https://medium.com/tensorflow?source=logo-lo_vvgg5edejndb---dca47aab201b
   9. https://medium.com/tensorflow/tagged/announcements
  10. https://medium.com/tensorflow/tagged/keras
  11. https://medium.com/tensorflow/tagged/javascript
  12. https://medium.com/tensorflow/tagged/mobile
  13. https://medium.com/tensorflow/tagged/responsible-ai
  14. https://medium.com/tensorflow/archive
  15. http://tensorflow.org/
  16. https://medium.com/@eisenjulian?source=post_header_lockup
  17. https://medium.com/@eisenjulian
  18. https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html
  19. https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html
  20. https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html
  21. https://www.tensorflow.org/api_docs/python/tf/layers
  22. https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb
  23. https://goo.gl/fxscra
  24. https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py
  25. http://ai.stanford.edu/~amaas/data/sentiment/
  26. https://s3.amazonaws.com/text-datasets/imdb.npz
  27. https://medium.com/media/f82c1a98d20995fc6b3f1c8e9fdf8958?postid=a99603033fbe
  28. https://medium.com/media/c60e3c72417457a0630676216a979d5a?postid=a99603033fbe
  29. https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html
  30. https://medium.com/media/d795bf0cefdf06bf3d7b960c3da62c72?postid=a99603033fbe
  31. https://medium.com/media/708371f7d4348142146c039732a84291?postid=a99603033fbe
  32. https://medium.com/media/2260e3f91540dc986b1bbb409d2d2d2c?postid=a99603033fbe
  33. https://en.wikipedia.org/wiki/tf   idf
  34. https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column
  35. https://medium.com/media/d4b1fb8d5217506078407a25c43c0cec?postid=a99603033fbe
  36. https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding
  37. https://www.tensorflow.org/tutorials/layers
  38. https://www.semanticscholar.org/paper/learning-to-rank-short-text-pairs-with-convolution-severyn-moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9
  39. https://www.tensorflow.org/extend/estimators
  40. https://medium.com/media/5ea0dadcf3bc5421378bbbf2b2426439?postid=a99603033fbe
  41. https://medium.com/media/8b1e1b974f46c135acc15777dc1db4a2?postid=a99603033fbe
  42. https://medium.com/media/8a7e1e297a6b98e34014267c406e5373?postid=a99603033fbe
  43. https://medium.com/media/2a528965a76f35f38af8e4b226214ab3?postid=a99603033fbe
  44. https://colah.github.io/posts/2015-08-understanding-lstms/
  45. https://colah.github.io/posts/2015-08-understanding-lstms/
  46. https://medium.com/media/de14c14c68ca157fe85040ddd85289bf?postid=a99603033fbe
  47. https://arxiv.org/abs/1607.01759
  48. https://arxiv.org/abs/1301.3781
  49. https://arxiv.org/abs/1103.0398
  50. https://www.tensorflow.org/tutorials/id97
  51. http://ruder.io/transfer-learning/
  52. https://nlp.stanford.edu/projects/glove/
  53. https://medium.com/media/d5baa867b3d0faaefda81d95f9d8d359?postid=a99603033fbe
  54. https://medium.com/media/c39da8a327d73aa59f69d5e267e67910?postid=a99603033fbe
  55. https://medium.com/media/cfdf3f4099f903ea8e0d5aefdb1bbdef?postid=a99603033fbe
  56. https://medium.com/media/e63398f87ce09f0f8a9461ec3e111460?postid=a99603033fbe
  57. https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators
  58. https://github.com/tensorflow/serving
  59. https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb
  60. https://goo.gl/fxscra
  61. https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py
  62. https://www.tensorflow.org/programmers_guide/embedding
  63. https://www.tensorflow.org/tutorials/id97
  64. http://www.nltk.org/book/ch03.html
  65. http://ruder.io/
  66. https://twitter.com/eisenjulian
  67. https://twitter.com/eisenjulian
  68. https://twitter.com/seb_ruder
  69. https://medium.com/@sebastianruder?source=post_page
  70. https://medium.com/@tensorflow?source=post_page
  71. https://medium.com/tag/machine-learning?source=post
  72. https://medium.com/tag/tensorflow?source=post
  73. https://medium.com/tag/nlp?source=post
  74. https://medium.com/tag/tutorial?source=post
  75. https://medium.com/tag/community?source=post
  76. https://medium.com/@eisenjulian?source=footer_card
  77. https://medium.com/@eisenjulian
  78. https://medium.com/tensorflow?source=footer_card
  79. https://medium.com/tensorflow?source=footer_card
  80. https://medium.com/tensorflow
  81. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  83. https://twitter.com/eisenjulian
  84. https://twitter.com/seb_ruder
  85. https://medium.com/p/a99603033fbe/share/twitter
  86. https://medium.com/p/a99603033fbe/share/facebook
  87. https://medium.com/p/a99603033fbe/share/twitter
  88. https://medium.com/p/a99603033fbe/share/facebook
