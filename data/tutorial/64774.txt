   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

what a disentangled net we weave: representation learning in vaes (pt. 1)

   [16]go to the profile of cody marie wild
   [17]cody marie wild (button) blockedunblock (button) followfollowing
   apr 15, 2018

   it   s a truth universally acknowledged: that data not in possession of
   labels must be in want of unsupervised learning.

   glibness aside, it   s commonly understood that supervised learning has
   meaningful downsides: labels are costly, noisy, and direct your problem
   towards the achievement of a somewhat artificial goal, rather than
   simply learning meaningful contours of the data in a more neutral way.
   however, they do give us something very valuable in the context of
   learning: a straightforward objective to maximize. all modern neural
   network systems are built off of id119, which modifies
   parameter values in order to to optimize an outcome. in the context of
   unsupervised learning, the question of what exactly you want the model
   to optimize itself for is a little less clear.

   one common strategy for unsupervised learning is that of generative
   models, the idea of which is: you should give a model the task of
   producing samples from a given distribution, because performing well at
   that task require the model to implicitly learn about that
   distribution. this intuitively makes sense, if you think about it human
   terms: if we wanted to test that, for example, that a child understood
   the category    houses   , we might continually ask them to draw a house,
   and if they were able to do so many times without constantly drawing
   the same house, or drawing things         like toasters    that don   t fit the
   category, we might reasonably assume that they understand what criteria
   make something essentially house-like.

   the two most common methods used for generative modeling are generative
   adversarial networks (gans) and variational auto-encoders (vaes).
   [1*dscpggrc-mjnvozwxo2wvg.jpeg]

   gans work by taking in noise (z) and using a generator neural network
   to transform that z into a facsimile version (x-hat) of the data. that
   data is then compared to a real example (x) by the discriminator, and
   the generator learns to create fake images that the discriminator is
   more likely to classify as real. at first glance, it may not be obvious
   what the role of z is in all this. why do we need this noise as our
   input, if it   s not adding any informative value? the answer is that it
   allows us to sample. a crucial aspect of generative modeling is that we
   don   t simply want a model that can generate one example from the
   distribution in question, but one where we can make repeated draws and
   get a different output each time. since the generator is just made up
   of matrices, satisfying this criteria of sampling requires that at
   least part of the network setup be stochastic. in a gan, as we traverse
   the space of our z noise, we get different draws from the distribution,
   based on how the x produced by the generator when it   s given each z
   sample as input. also, because we decide and control what distribution
   z we use, it   s easy to sample after the model is trained: we simply
   sample z values from that same chosen distribution, and know that, in
   expectation, the sampled z we get will be from a region where the
   generator has learned to produce realistic outputs using that z.

a variational autoencoder crash course

   vaes         which will be the focus of the rest of this post         have a
   similar structure, though different in two primary regards.
    1. instead of just mapping from a randomly-drawn z into an image, vaes
       take a specific x as input, learn a encoder that maps q(z|x) from
       that x to a compressed code z. that code, which hopefully contains
       the information necessary to reconstruct that x, is run through a
       decoder, p(x|z), to predict that specific x back out again.
    2. instead of using a neural network to generate your loss between the
       true and generated/reconstructed x, vaes typically   use a
       pixel-wise id168 that measures the pixel distance between
       the reconstructed and original x.

   for the purposes of this post, the first of these is the most salient
   difference: the fact that, in a vae, you don   t just care about
   producing something that looks like it came from the data distribution
   as a whole, you care about reproducing the specific image (or,
   generically, observation from your data distribution) you were given as
   input. therefore, your z distribution, which for gans is just a useful
   source of randomness, typically needs to encode information that can be
   used for reconstruction of that specific image. the theory is: since z
   is a lower dimensional vector, the network is forced to learn a
   compressed and informative representation of the input image.

   however, we now come back to the criterion we outlined earlier with
   gans: the need to be able to sample from the model after we   ve trained
   it. and, in order to do that sampling effectively, you need to be able
   to sample a given z after training, and have high confidence that that
   region of z-space corresponds to realistic outputs.

   one way of doing this, and the way generally used by vaes, is to
   incentivize the network to push the z values it encodes out of x         that
   is, q(z|x)         to be close to some prior, p(z), which is typically an
   dimensionwise-independent multivariate gaussian. this is also referred
   to as an    isotropic    gaussian. this sounds jargon-y and complex, but in
   visual terms, it   s really not. in two dimensions, a gaussian with each
   dimension   s variance equal to one, and no covariance between dimensions
   just looks like a circle, centered at 0. in higher dimensions, it   s a
   sphere (or hypersphere).

   these two elements combine into the following objective function:
   [1*gjbpznrktfi9v2mmeswogw.png]

   in this objective, the first term corresponds to the reconstruction
   loss (also called data likelihood loss) and conceptually maps to    how
   good is my model at generating things that are similar to the data
   distribution   . the second term is the kl divergence between the q(z|x)
   values your network encodes for each x , and a prior distribution p(z).
   since kl divergence is lowest when the two distributions are
   equivalent, this term is pushing towards z values that are more
   concentrated in the space of the prior multivariate gaussian. this term
   is typically referred to as the id173 term.
   [1*4i9v5mops5hk548fv5qvoa.png]

   however, vaes in particular aren   t solely or even primarily used as
   generative models; their main utility is as representation learners. by
   this, we mean that vaes are often used for their ability to create an
   lower-dimensional coding distribution (the aforementioned z)
   that         modelers hope         has been forced to learn meaningful concepts
   within the data, due to the bottleneck and reconstruction structure.
   the logic of this is essentially: if you have a limited bandwidth in
   which to transmit information, you would prioritize using it to
   describe the most salient aspects of the data you   re trying to
   transmit.

   this is valuable because it   s broadly understood that a lot of the
   value of deep networks is in their capacity as learned feature
   extractors: systems that can take in a high-dimensional input, and
   generate more semantically meaningful features out of it. if an
   unsupervised system could achieve a similar goal, of creating a highly
   informative, compressed feature representation by using a lot of
   labeled data, then there   s good reason to believe a high-performing
   model can be trained using those features, and only a small amount of
   labeled data.

   this all sounds great: a simple, probabilistically-grounded solution to
   our unsupervised learning woes. but, not so fast.

   despite the promise of this technique, in practice, there are two main
   difficulties that researchers trying to use vaes as representation
   learners have faced: entangled codes, and ignored codes. this blog post
   will address betavae, which solves for the first potential pitfall, and
   part 2 will focus on infovae, which responds to the second  .

   (a few notes: i   m using    code    here and throughout the post as
   shorthand for the low-dimensional z representation learned by the
   encoder. also, as in much of modern ml, the most common use case for
   vaes in papers is with image data, so i   ll occasionally refer to things
   like    pixelwise loss    , that are specifically relevant to image data. i
   leave those in because i think they provide useful intuition, but to
   allay confusion and possible overfitting: the ideas of vae apply to
   domains beyond images)

defining disentanglement

   in the real world that our models seek to represent, there are some
   factors of variation that can be modified independently, and others
   that can   t be (or, for practical purposes, never are). a trivial
   example of this is: if you   re modeling pictures of people, then
   someone   s clothing is independent of their height, whereas the length
   of their left leg is strongly dependent on the length of their right
   leg. the goal of disentangled features can be most easily understood as
   wanting to use each dimension of your latent z code to encode one and
   only one of these underlying independent factors of variation. using
   the example from above, a disentangled representation would represent
   someone   s height and clothing as separate dimensions of the z code.
   [1*qvyqn1pexwcd8vnovr0pfg.png]

   the goal of disentanglement has a few different motivations. in a
   practical sense, imagine you were learning a generative model to create
   pictures of people, with the ultimate goal of generative a bunch of
   fake people to be in the background of a video game scene. you might
   want to be able to tell the model,    i want to generate someone who
   looks like this person, but is taller   . if you   ve learned a z dimension
   that independently encodes a person   s height, then you can modify that,
   keeping everything else the same. if you instead encoded height and
   gender in a shared dimension, changing the height while keeping all
   other aspects of the person constant wouldn   t be possible, since
   modifying the internal dimension for height would also modify gender.

   from a more information theoretic perspective, a disentangled
   representation is useful because when you capture the most meaningful
   or salient ways that observations differ from one another, those axes
   of difference will often be valuable for a variety of supervised task.
   when this is true, it allows you to use less data and a less complex
   model to perform a given supervised task, when you use this
   disentangled representation as input.

   as a motivating example of what more and less entangled codes look
   like, take a look at the picture below. the lefthand grid provides
   examples of less independent codings, where the righthand grid shows
   more independent ones.
   [1*izbn9uqjlccx_zd9jfeapa.png]

   in each of the rows above, one of the dimensions of a z code is being
   varied as you move from left to right, and then a decoder is being used
   to turn that z code into an image. this model was trained off of a very
   simple dataset consisting only of a white dot, located somewhere on a
   black background. there are only two independently-modifiable
   parameters here: horizontal direction, and vertical direction. the
   righthand grid has learned z values that map directly to these factors:
   the first dimension interpolates between the very bottom and very top,
   at about the same horizontal location, and the second dimension
   interpolates between the far right and far left, at about the same
   vertical location. the latter two dimensions are uninformative.

   by contrast, most of the factors learned by the lefthand model combine
   changes in vertical and horizontal position. the first factor moves up
   and to the left, the second, only down, and the last, down and to the
   right. also interestingly, in the last factor, you see a strong
   discontinuity between the 7th and the 8th images, where the ball
   suddenly    jumps    far down to the right. even though there are only two
   independent dimensions underlying this data, the lefthand grid has
   (inefficiently) spread its representation out over 4 dimensions.

   all in all: the two main ways in which the representations in the
   righthand grid are superior is that they are smooth, and that they
   represent independent axes.

want some beta?

   the lefthand grid         the inefficient, and entangled one         was learned by
   a (normal) vae, and the righthand grid         the disentangled one         was
   learned by a [18]beta vae. the beta vae is really just a name for a
   very simple concept: taking the    id173    or    prior enforcement   
   term in the vae loss, and cranking up how much of a constraint that
   term is by putting a stronger weight on it in the objective.
   [1*gjbpznrktfi9v2mmeswogw.png]
   [1*blvai_agkcugz_jfc88r7a.png]
   you   re not imagining things; this paper   s many pages of math boiled
   down to adding a coefficient to a term in the objective, and turning
   that coefficient to values greater than 1

   the intuition between why the difference in these two equations
   translates into the difference between the two grids isn   t immediately
   obvious, but there are some valuable nuggets of understanding if you
   dig deep enough.

   to start with the basics of what   s happening: the kl divergence term
   quantifies different is the conditional/encoded distribution of z from
   a prior. it   s then penalizing the network when it produces z
   distributions that look substantially different from a multivariate
   gaussian prior, where each dimension is n(0, 1) and the dimensions are
   statistically independent (that is to say: the correlation between the
   dimensions is 0). in the following discussion, there are a few
   important features of z to remember:
     * since this is a variational autoencoder, the encoder network
       doesn   t just produce a deterministic value of z that is passed
       forward, it encodes the mean and variance of a z distribution; the
       network samples from that distribution, and passes the sample
       forward. so when we say we   re enforcing a multivariate gaussian
       prior, that means we   re pushing those encoded means to be closer to
       0, the encoded standard deviations to be closer to 1, and the
       correlation between dimensions to be closer to 0.
     * if you look at the equation above, we   re applying the constraint to
       the conditional distributions, or, the distributions encoded from
       each individual x. so, when the prior pushes the mean values for
       these conditional distributions closer to 0, that means that the
       mean values are constrained in terms of how much they can differ
       between different x observations.

   for this to make sense as a useful constraint, let   s think about what
   the z code has to do, and what its options are for doing it. from the
   point of view of the reconstruction loss, the job of the z distribution
   is to pass forward information that will communicate, as unambiguously
   as possible, the features of the x that it was encoded from. when the
   weight on the id173 loss is turned way up, it has three main
   effects on the z vector that   s learned.
    1. smoothness: it incentivizes the reconstructions to change smoothly
       as you vary values of z, rather than being jumpy or discontinuous
    2. parsimony: it incentivizes the network to compress information
       about x into as few z dimensions as possible
    3. axis-alignment: it incentivizes to align the main axes of data
       variability with the dimensions of the z vector

   altogether, these incentives push the network to compress information
   about x into a small number of smoothly varying, independent
   dimensions.

keeping it smooth

   [1*xhqeutfpnnf6beit0e-r8a.png]
   graphic from:    understanding disentangling in b-vae   .

   one potential solution to the problem of reconstruction loss is to push
   the means farther apart, and push the standard deviations lower, to
   reduce the chance of there being confusion between the z values
   generated from different observations

   however, this behavior is exactly what the id173 term is
   pushing against: it would prefer means that are closer to one another,
   and standard deviations that are closer to 1. as the image above shows,
   in this regime, if the network encodes x2 with a wide distribution,
   then it   s quite possible that x-tilde will be sampled, which is
   actually likelier under x1 than it is under x2. so the network is
   liable to reconstruct x2 when it was meant to reconstruct x1.

   this strong incentive for what the authors call    data locality    has the
   effect of pushing the network to encode factors that smoothly vary
   across the data.

use your dimensions wisely

   [1*izbn9uqjlccx_zd9jfeapa.png]

   let   s return for a moment to the white-blobs comparison from earlier.
   recall that one of the biggest differences between the two coding
   schemes was how many dimensions the network used to encode what was,
   underneath, two dimensions of generative factors. the reason why we see
   this effect is that under beta-vae (with beta >> 1), the model pays a
   price for each dimension it uses to encode information. that   s because
   the only way for a dimension to incur no cost is for it to be
   equivalent to the global prior of n(0, 1), for all input values of x.
   when a dimension is invariant across all values of x, then, by
   definition, it doesn   t contain any information about x. whenever the
   model diverges from this no-information state, and starts actually
   encoding z with different means as a function of x, that imposes a cost
   in the objective function.

   this drive for dimensional efficiency means that the model wants to
   encode only the most informative axes of variation. i like to imagine
   the vanilla vae scenario as analogous to the using four basis
   vectors         in the id202 sense         in a space that   s only actually
   two dimensions. it   s extraneous, but when there   s little or no penalty
   for the model, that waste isn   t made salient to the model. by contrast,
   under the betavae, the model is incentivized to reduce the number of
   basis vectors, keeping only those valuable enough in describing the
   space to be worthwhile.

   obviously, the white blob case where there are literally exactly two
   axes of generation is an oversimplification. with real data, it   s
   almost never going to be the case that there   s some lower dimensional
   space that captures all of the meaningful variation. but, it might be
   the case that you have two axes that capture almost all of the
   variation, and a third axis that captures only a small remainder. in
   that case, the network might still choose to not represent that third
   dimension, because it   s not adding enough explanatory power to be worth
   paying the cost of representation.

keeping ourselves aligned

   the last component of disentanglement, axis-alignment, comes from the
   assumption that, if there really are underlying generative factors,
   then different factors will provide different amounts of explanatory
   power. for example, imagine if we had a version of the white circles
   dataset, but in addition to the circle changing position, it also
   changed radius. now, it   s certainly true that the size of the circle is
   an independent factor, and that we might want our model to capture it.
   but, it   s also true that the reconstruction loss you   ll incur from
   having the circle in a totally incorrect place is far higher than the
   reconstruction loss you   d lose from just reconstructing the wrong size
   circle, since many pixels will overlap between the differently-sized
   circles.

   since this dimension provides less useful information, it   s not worth
   as much cost. and, remember, it   s costly under the id173 term
   to move conditional means to informative positions, or reduce
   conditional variance to minimize the likelihood of confusion. so, where
   the network might be willing to pay the price of having sharp and
   unambiguous positional information, the z values for size may be closer
   to the prior: with a wider standard deviation and a less spread-out set
   of means, meaning that only very big and very small circles can be
   distinguished from one another. this desire, to align the capacity of
   our latent dimensions in accordance to their informative value, is a
   natural outcome of the constraint we   ve applied.

   however, because the gaussian is isotropic, with no covariance between
   dimensions, we can only learn different variances for different factors
   if we align those different factors. so, because the network is
   incentivized to scale variance in accordance with the informativeness
   of a factor, and we expect distinct generative factors to be the ones
   that have the most distinct levels of informativeness (due to
   representing distinct generative processes), it   s incentivized to align
   its major generative factors with the dimensions of z.

reconstructing knowledge

   on a fundamental level, the approach of the beta vae is not a
   difference in kind from a vanilla vae, but a difference in emphasis. in
   the most simplified framing: when you turn beta up to high values, it   s
   just a much more regularized vae. a lot of the theory of vaes already
   revolves around forcing compression by applying an information
   bottleneck. betavae says that, to generate properly disentangled
   factors, that bottleneck needs to be even stronger.

   one benefit of this fundamental similarity between the methods is that
   a lot of the intuitions we can get out of betavae         of how the latent
   space is shaped under an extreme version of the id173
   constraint         also help us better understand how typical vaes work, and
   what kinds of representations we can expect them to create. after all,
   a lot of hope and weight is resting on models like these to help lead
   the charger of generative representation learning, and so it   s
   important to remember: they won   t pick up the kinds of features we hope
   they will, they   ll only pick up the kinds of features we incentivize
   them to find.

   (a lot of the intuition i reframed above comes from [19]this paper,
   released by the authors of the original beta vae approach, which
   explicitly tries to provide explanations for why their method works)
    1. this is not true all of the time, since there are papers like
       [20]this one that stick discriminators on the end of vaes to do a
       more    feature-wise    id168, but the    squared elementwise
       difference    is a common enough standard that i   ll stick to
       referring to it in this post
    2. it was going to be all one piece, but mid-drafting i realized it
       was leaning in the direction of 6000 words, and decided
       post-division was the wisest choice.

     * [21]machine learning
     * [22]generative model
     * [23]unsupervised learning
     * [24]artificial intelligence
     * [25]towards data science

   (button)
   (button)
   (button) 1.6k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of cody marie wild

[27]cody marie wild

   machine learning data scientist; lover of cats, languages, and elegant
   systems; professional curious person.

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.6k
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9e5dbc205bd1
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-a-disentangled-net-we-weave-representation-learning-in-vaes-pt-1-9e5dbc205bd1&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-a-disentangled-net-we-weave-representation-learning-in-vaes-pt-1-9e5dbc205bd1&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_jdtur9x8jwl4---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@cody.marie.wild?source=post_header_lockup
  17. https://towardsdatascience.com/@cody.marie.wild
  18. https://openreview.net/pdf?id=sy2fzu9gl
  19. https://arxiv.org/pdf/1804.03599.pdf
  20. https://arxiv.org/pdf/1512.09300.pdf
  21. https://towardsdatascience.com/tagged/machine-learning?source=post
  22. https://towardsdatascience.com/tagged/generative-model?source=post
  23. https://towardsdatascience.com/tagged/unsupervised-learning?source=post
  24. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  25. https://towardsdatascience.com/tagged/towards-data-science?source=post
  26. https://towardsdatascience.com/@cody.marie.wild?source=footer_card
  27. https://towardsdatascience.com/@cody.marie.wild
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/9e5dbc205bd1/share/twitter
  34. https://medium.com/p/9e5dbc205bd1/share/facebook
  35. https://medium.com/p/9e5dbc205bd1/share/twitter
  36. https://medium.com/p/9e5dbc205bd1/share/facebook
