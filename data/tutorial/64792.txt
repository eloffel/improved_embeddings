   #[1]machine learning explained    feed [2]machine learning explained   
   comments feed [3]overfitting isn   t simple: overfitting re-explained
   with priors, biases, and no free lunch [4]techniques for effectively
   learning [5]alternate [6]alternate

   [7]skip to content

   [8]machine learning explained

   deep learning, python, data wrangling and other machine learning
   related topics explained for practitioners
   (button) menu

     * [9]about this blog
     * [10]github

paper dissected:    glove: global vectors for word representation    explained

   pre-trained id27s are a staple in deep learning for nlp. the
   pioneer of id27s in mainstream deep learning is the renowned
   id97. glove is another commonly used method of obtaining
   pre-trained embeddings. unfortunately, there are very few practitioners
   that seem to actually understand glove; many just consider it    another
   id97   . in reality, glove is a much more principled approach to word
   embeddings that provides deep insights into id27s in general.

   glove is a must-read paper for any self-respecting nlp engineer that
   can seem intimidating at first glance. this post aims to dissect and
   explain the paper for engineers and highlight the differences and
   similarities between glove and id97 .


tl;dr

     * glove aims to achieve two goals:
          + (1) create word vectors that capture meaning in vector space
          + (2) takes advantage of global count statistics instead of only
            local information
     * unlike id97     which learns by streaming sentences     glove
       learns based on a co-occurrence matrix and trains word vectors so
       their differences predict co-occurrence ratios
     * glove weights the loss based on word frequency
     * somewhat surprisingly, id97 and glove turn out to be extremely
       similar, despite starting off from entirely different starting
       points

motivation and brief review of id27s

   the problem with id97

   glove was published after id97 so the natural question to ask
   is: why is id97 not enough?

   in case you are not familiar, here   s a quick explanation of id97.
   id97 trains id27s by optimizing a id168 with
   id119, just like any other deep learning model. in id97,
   the id168 is computed by measuring how well a certain word can
   predict its surroundings words. for instance, take the sentence

      the cat sat on the mat   .

   suppose the context window is size 2, and the word we are focusing on
   is    sat   . the context words are    the   ,    cat   ,    on    and    the   . id97
   tries to either predict the word in focus from the context words (this
   is called the cbow model) or the context words using the word in focus
   (this is called the skip-gram model).

   [id97_diagrams.png?resize=530%2c308&#038;ssl=1]

   i won   t go into the details here since there are plenty of explanations
   online. the important thing to note is that id97 only takes local
   contexts into account. it does not take advantage of global count
   statistics. for example,    the    and    cat    might be used together often,
   but id97 does not know if this is because    the    is a common word or
   if this is because the words    the    and    cat    have a strong linkage
   (well actually, it indirectly does, but this is a topic i   ll cover
   later on in this post). this is the motivation for using global count
   statistics.

   the problem with global count statistic based methods

   before id97, using matrices that contained global count information
   was one of the major ways of converting words to vectors (yes, the idea
   of word vectors/embeddings existed far before id97). for instance,
   latent semantic analysis (lsa) computed id27s by decomposing
   term-document matrices using singular value decomposition. though these
   methods take advantage of global information, the obtained vectors do
   not show the same behavior as those obtained by id97. for instance,
   in id97, word analogies can be expressed in terms of simple
   (vector) arithmetic such as in the case of    king     man + woman =
   queen   .

   [linear-relationships.png?w=840&#038;ssl=1]

   this behavior is desirable because it shows that the vectors capture
   dimensions of meaning. some dimensions of the vectors might capture
   whether the word is male or female, present tense or past tense, plural
   or singular, etc.. this means that downstream models can easily extract
   the meaning from these vectors, which is what we really want to achieve
   when training word vectors.


   glove aims to take the best of both worlds: take global information
   into account while learning dimensions of meaning. from this initial
   goal, glove builds up a principled method of training word vectors.

building glove, step by step

   data preparation

   in id97, the vectors were learned so that they could achieve the
   task of predicting surrounding words in a sentence.  during training,
   id97 streams the sentences and computes the loss for each batch of
   words.

   in glove, the authors take a more principled approach. the first step
   is to build a co-occurrence matrix. glove also takes local context into
   account by computing the co-occurrence matrix using a fixed window size
   (words are deemed to co-occur when they appear together within a fixed
   window). for instance, the sentence

      the cat sat on the mat   

   with a window size of  2 would be converted to the co-occurrence matrix
       the cat sat on mat
   the 2   1   2   1  1
   cat 1   1   1   1  0
   sat 2   1   1   1  0
   on  1   1   1   1  1
   mat 1   0   0   1  1

   notice how the matrix is symmetric: this is because when the word    cat   
   appears in the context of    sat   , the opposite (the word    sat    appearing
   in the context of   cat   ) also happens.

   what should we predict?

   now, the question is how to connect the vectors with the statistics
   computed above. the underlying principle behind glove can be stated as
   follows: the co-occurrence ratios between two words in a context are
   strongly connected to meaning.

   this sounds difficult but the idea is really simple. take the words
      ice    and    steam   , for instance. ice and steam differ in their state
   but are the same in that they are both forms of water. therefore, we
   would expect words related to water (like    water    and    wet   ) to appear
   equally in the context of    ice    and    steam   . in contrast, words like
      cold    and    solid    would probably appear near    ice    but would not
   appear near    steam   .

   the following table shows the actual statistics that show this
   intuition well:

   the probabilities shown here are basically just counts of how often the
   word k appears when the words    ice    and    steam    are in the context,
   where k refers to the words    solid   ,    gas   ,    water   , and    fashion   . as
   you can see, words that are related to the nature of    ice    and    steam   
   (   solid    and    gas    respectively) occur far more often with their
   corresponding words that the non-corresponding word. in contrast, words
   like    water    and    fashion    which are not particularly related to either
   have a id203 ratio near 1. note that the id203 ratios can
   be computed easily using the co-occurrence matrix.


   from here on, we   ll need a bit of mathematical notation to make the
   explanation easier. we   ll use x to refer to the co-occurrence matrix
   and x_{ij} to refer to the i, j th element in x which is equal to the
   number of times word j appears in the context of word i . we   ll also
   define x_i = \sum_l{x_{il}} to refer to the total number of words that
   have appeared in the context of i . other notation will be defined as
   we go along.

   deriving the glove equation

   (this subsection is not directly necessary to understand how glove and
   id97 differ, etc. so feel free to skip it if you are not
   interested.)

   now, it seems clear that we should be aiming to predict the
   co-occurrence ratios using the word vectors. for now, we   ll express the
   relation between the ratios with the following equation:

   f(w_i, w_j, \tilde{w_k}) \approx \frac{p_{ij}}{p_{jk}}

   here, p_{ij}  refers to the id203 of the word j appearing in the
   context of i , and can be computed as

   p_{ij} = \frac{\textrm{number of times j appeared in context of
   i}}{\textrm{number of words that appeared in context of i}} =
   \frac{x_{ij}}{x_i} .

   f is some unknown function that takes the embeddings for the words i,
   k,  j as input. notice that there are two kinds of embeddings: input
   and output (expressed as w and \tilde{w} ) for the context and focus
   words. this is a relatively minor detail but is important to keep in
   mind.

   now, the question becomes what f should be.  if you recall, one of the
   goals of glove was to create vectors with meaningful dimensions that
   expressed meaning using simple arithmetic (addition and
   subtraction). we should choose f so that the vectors it leads to meet
   this property.

   since we want simple arithmetic between the vectors to have meaning,
   it   s only natural that we make the input to the function f also be the
   result of arithmetic between vectors. the simplest way to do this is by
   making the input to f the difference between the vectors we   re
   comparing:

   f(w_i - w_j, \tilde{w_k}) \approx \frac{p_{ij}}{p_{jk}}

   now, we want create a linear relation between w_i - w_j and \tilde{w_k}
   . this can be accomplished by using the dot product:

   f(\textrm{dot}(w_i - w_j, \tilde{w_k}) \approx\frac{p_{ij}}{p_{jk}}

   now, we   ll use two tricks to determine f and simplify this equation:
    1. by taking the log of the id203 ratios, we can convert the
       ratio into a subtraction between probabilities
    2. by adding a bias term for each word, we can capture the fact that
       some words just occur more often than others.

   these two tricks give us the following equation:

   \textrm{dot}(w_i - w_j, \tilde{w_k}) + b_i - b_j = \log(p_{ik}) -
   \log(p{jk})

   we can convert this equation into an equation over a single entry in
   the co-occurrence matrix.

   \textrm{dot}(w_i, \tilde{w_k}) + b_i = \log(p_{ik}) = \log(x_{ik}) -
   \log(x_i)

   by absorbing the final term on the right-hand side into the bias term,
   and adding an output bias for symmetry, we get

   \textrm{dot}(w_i, \tilde{w_k}) + b_i + \tilde{b}_k = \log(x_{ik})

   and here we are. this is the core equation behind glove.


   weighting occurrences

   there is one problem with the equation above: it weights all
   co-occurrences equally. unfortunately, not all co-occurrences have the
   same quality of information. co-occurrences that are infrequent will
   tend to be noisy and unreliable, so we want to weight frequent
   co-occurrences more heavily. on the other hand, we don   t want
   co-occurrences like    it is    dominating the loss, so we don   t want to
   weight too heavily based on frequency.

   through experimentation, the authors of the paper found the following
   weighting function to perform relatively well:

   \textrm{weight}(x) = \textrm{min}(1, (x / x_{max})^\frac{3}{4})

   the function becomes easier to imagine when it is plotted:

   essentially, the function gradually increases with x but does not
   become any larger than 1. using this function, the loss becomes

   \sum_{ij}{\textrm{weight}(x_{ij})(\textrm{dot}(w_i, \tilde{w_j}) + b_i
   + \tilde{b_k} - \log(x_{ij}))^2)}


comparison with id97

   though glove and id97 use completely different methods for
   optimization, they are actually surprisingly mathematically similar.
   this is because, although id97 does not explicitly decompose a
   co-occurrence matrix, it implicitly optimizes over one by streaming
   over the sentences.

   id97 optimizes the log likelihood of seeing words in the same
   context windows together, where the id203 of seeing word j in the
   context of i is estimated as

   q_{ij} = \textrm{softmax}(\textrm{dot}(w_i, \tilde{w_j}))

   the id168 can be expressed as

   -\sum_{i \in \textrm{corpus} ; j \in \textrm{context}(i) } \log{q_{ij}}

   the number of times \log{q_{ij}} is added to the loss is equivalent to
   the number of times the pair i, j appears in the corpus. therefore,
   this loss is equivalent to

   \sum_{i, j} x_{ij} \log{q_{ij}}

   now, if we recall, the actual id203 of word j appearing in the
   context of i is

   p_{ij} = x_{ij} / x_{i}

   meaning we can group over the context word i and get the following
   equation:

   \sum_{i}{ x_i\sum_{j}\frac{x_{ij}}{x_i}\log{q_{ij} }} = \sum_{i}{
   x_i\sum_{j}p_{ij}\log{q_{ij} }}

   notice how the term \sum_{j}p_{ij}\log{q_{ij}} takes the form of the
   cross-id178 loss. in other words, the loss of id97 is actually
   just a frequency weighted sum over the cross-id178 between the
   predicted word distribution and the actual word distribution in the
   context of the word i . the weighting factor x_i comes from the fact
   that we stream across all the data equally, meaning a word that appears
   n times will contribute to the loss n times. there is no inherent
   reason to stream across all the data equally though. in fact, the
   id97 paper actually argues that filtering the data and reducing
   updates from frequent context words improves performance. this means
   that the weighting factor can be an arbitrary function of frequency,
   leading to the id168

   \sum_{ij}{ \textrm{weight}(x_{ij})p_{ij}\log{q_{ij} }}

   notice how this only differs from the glove equation we derived above
   in terms of the loss measure between the actual observed frequencies
   and the predicted frequencies. whereas glove uses the log mean squared
   error between the predicted (unnormalized) probabilities and the actual
   observed (unnormalized) probabilities, id97 uses the cross-id178
   loss over the normalized probabilities.

   the glove paper argues that log mean squared error is better than
   cross-id178 because cross id178 tends to put too much weight on the
   long tails. in reality, this is probably a subject that needs more
   analysis. the essential thing to know is that id97 and glove are
   actually virtually mathematically the same, despite having entirely
   different derivations!


computational complexity

   if id97 and glove are mathematically similar, the natural thing to
   ask is if one is faster than the other.

   glove requires computing the co-occurrence matrix, but since both
   methods need to build the vocabulary before optimization and the
   co-occurrence matrix can be built during this process, this is not the
   defining factor in the computational complexity.

   instead, the computational complexity of glove is proportionate to the
   number of non-zero elements of x . in contrast, the computational
   complexity of id97 is proportionate to the size of the corpus |c| .

   though i won   t go into the details, the frequency of a word pair can be
   approximated based on its frequency rank r_{ij} (the ranking of the
   word pair when all words pairs are sorted by frequency):

   x_{ij} \approx \frac{k}{r_{ij}^\alpha}

   using this, we can approximate the number of non-zero elements in x as

   \mathcal{o}(|c|) if \alpha < 1 else \mathcal{o}(|c|^{\frac{1}{\alpha}})
   .

   essentially, if some words occur very frequently it   s faster to
   optimize over the statistics rather than to iterate over all the entire
   corpus repeatedly.


results

   though the glove paper presents many experimental results, i   ll focus
   the results it presents in terms of comparison with id97 here.

   the authors show that glove consistently produces better embeddings
   faster than id97. this makes sense, given how glove is much more
   principled in its approach to id27s.

   unfortunately, empirical results published after the glove paper seem
   to favor the conclusion that glove and id97 perform roughly the
   same for many downstream tasks. furthermore, most people use
   pre-trained id27s so the training time is not much of an
   advantage. therefore, glove should be one choice for pre-trained word
   embeddings, but should not be the only choice.


conclusion and further readings

   glove is not just    another id97   ; it is a more systematic approach
   to id27s that sheds light on the behavior of id97 and
   embeddings in general. i hope this post has given the reader some more
   insight into this outstanding and deep paper.

   here are some further readings for those interested:

   [11]the original glove paper

   [12]the id97 paper


   other papers dissected:

   [13]attention is all you need

   [14]deep image prior

   [15]quasi-recurrent neural networks

share this:

     * [16]click to share on twitter (opens in new window)
     * [17]click to share on facebook (opens in new window)
     *

like this:

   like loading...

related

   author [18]keitakuritaposted on [19]april 29, 2018may 4, 2018categories
   [20]deep learning, [21]nlp, [22]paper

post navigation

   [23]previous previous post: overfitting isn   t simple: overfitting
   re-explained with priors, biases, and no free lunch
   [24]next next post: techniques for effectively learning

top posts & pages

     * [25]an in-depth tutorial to allennlp (from basics to elmo and bert)
     * [26]weight id172 and layer id172 explained
       (id172 in deep learning part 2)
     * [27]paper dissected: "attention is all you need" explained
     * [28]lightgbm and xgboost explained
     * [29]a practical introduction to nmf (nonnegative matrix
       factorization)

subscribe to blog via email

   find anything useful? ;)
   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

categories

     * [30]id161 (2)
     * [31]deep learning (22)
     * [32]fromscratch (1)
     * [33]jupyter (2)
     * [34]kaggle (1)
     * [35]machine learning (13)
     * [36]nlp (11)
     * [37]paper (10)
     * [38]python (1)
     * [39]skills (1)
     * [40]software (1)
     * [41]software engineering (2)
     * [42]uncategorized (3)

archives

     * [43]april 2019
     * [44]february 2019
     * [45]january 2019
     * [46]november 2018
     * [47]september 2018
     * [48]august 2018
     * [49]june 2018
     * [50]may 2018
     * [51]april 2018
     * [52]march 2018
     * [53]february 2018
     * [54]january 2018
     * [55]december 2017

     * [56]about this blog
     * [57]github

   [58]machine learning explained [59]proudly powered by wordpress

   iframe: [60]likes-master

   %d bloggers like this:

references

   1. http://id113xplained.com/feed/
   2. http://id113xplained.com/comments/feed/
   3. http://id113xplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/
   4. http://id113xplained.com/2018/05/04/techniques-for-effectively-learning/
   5. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/
   6. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/&format=xml
   7. http://id113xplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/#content
   8. http://id113xplained.com/
   9. http://id113xplained.com/about-this-blog/
  10. https://github.com/keitakurita
  11. https://www.aclweb.org/anthology/d14-1162
  12. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  13. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  14. http://id113xplained.com/2018/01/18/paper-dissected-deep-image-prior-explained/
  15. http://id113xplained.com/2018/04/09/paper-dissected-quasi-recurrent-neural-networks-explained/
  16. http://id113xplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/?share=twitter
  17. http://id113xplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/?share=facebook
  18. http://id113xplained.com/author/admin/
  19. http://id113xplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/
  20. http://id113xplained.com/category/machine-learning/deep-learning/
  21. http://id113xplained.com/category/nlp/
  22. http://id113xplained.com/category/paper/
  23. http://id113xplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/
  24. http://id113xplained.com/2018/05/04/techniques-for-effectively-learning/
  25. http://id113xplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/
  26. http://id113xplained.com/2018/01/13/weight-id172-and-layer-id172-explained-id172-in-deep-learning-part-2/
  27. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  28. http://id113xplained.com/2018/01/05/lightgbm-and-xgboost-explained/
  29. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  30. http://id113xplained.com/category/computer-vision/
  31. http://id113xplained.com/category/machine-learning/deep-learning/
  32. http://id113xplained.com/category/fromscratch/
  33. http://id113xplained.com/category/jupyter/
  34. http://id113xplained.com/category/kaggle/
  35. http://id113xplained.com/category/machine-learning/
  36. http://id113xplained.com/category/nlp/
  37. http://id113xplained.com/category/paper/
  38. http://id113xplained.com/category/python/
  39. http://id113xplained.com/category/skills/
  40. http://id113xplained.com/category/software/
  41. http://id113xplained.com/category/software-engineering/
  42. http://id113xplained.com/category/uncategorized/
  43. http://id113xplained.com/2019/04/
  44. http://id113xplained.com/2019/02/
  45. http://id113xplained.com/2019/01/
  46. http://id113xplained.com/2018/11/
  47. http://id113xplained.com/2018/09/
  48. http://id113xplained.com/2018/08/
  49. http://id113xplained.com/2018/06/
  50. http://id113xplained.com/2018/05/
  51. http://id113xplained.com/2018/04/
  52. http://id113xplained.com/2018/03/
  53. http://id113xplained.com/2018/02/
  54. http://id113xplained.com/2018/01/
  55. http://id113xplained.com/2017/12/
  56. http://id113xplained.com/about-this-blog/
  57. https://github.com/keitakurita
  58. http://id113xplained.com/
  59. https://wordpress.org/
  60. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
