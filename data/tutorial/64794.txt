   #[1]machine learning explained    feed [2]machine learning explained   
   comments feed [3]creating custom magic commands in jupyter [4]an
   overview of sentence embedding methods [5]alternate [6]alternate

   [7]skip to content

   [8]machine learning explained

   deep learning, python, data wrangling and other machine learning
   related topics explained for practitioners
   (button) menu

     * [9]about this blog
     * [10]github

an intuitive explanation of id5 (vaes part 1)

   id5 (vaes) are a popular and widely used method.
   though there are many papers and tutorials on vaes, many tend to be far
   too in-depth or mathematical to be accessible to those without a strong
   foundation in id203 and machine learning.

   in this post, i attempt to offer a gentler introduction to vaes that
   omits equations as much as possible in favor of intuitive explanations.
   i do not mean to say that understanding the math is meaningless;
   instead, i will try to provide an intuitive understanding that will
   assist readers in actually tackling more advanced readings and papers.

1. what are vaes for?

1.1 autoencoders

   before diving into    variational    autoencoders, i want to familiarize
   the reader with the notion of an autoencoder (in case you are already
   familiar, please jump to section 1.2).

   basically, an autoencoder is a method of obtaining a low-dimensional
   representation of some higher level input, such as images. the idea is
   very simple: you train a neural network to take the image as input and
   return it as the output. this would be trivial if the model had enough
   capacity to store the entire image. autoencoders teach the network to
   compress the information by forcing an    encoder network    to output a
   low dimensional representation z , which is then consumed by a    decoder
   network    to output the original image. this framework is illustrated
   below:

   autoencoder.png

1.2 the motivation behind vaes

   autoencoders are great, but wouldn   t it be great if we could feed
   encodings to the decoder that were not generated from the images in the
   source dataset? if we could, we could create new, reasonable images
   simply by sending a bunch of vectors to the decoder and observing the
   output.

   mousou

   what prevents us from using autoencoders in this manner? simply put, we
   have no idea if the vectors we are passing to the decoder are valid.
   chances are, if we give a random vector to the decoder, we will just
   get a bunch of random pixels back. this is because the decoder assumes
   that the encodings adhere to some unknown distribution, and does not
   account for encodings outside of it.

   this is the problem that vaes try to solve. essentially, vaes attempt
   to make sure that encodings that come from some known id203
   distribution can be decoded to produce reasonable outputs, even if they
   are not encodings of actual images.

2. details of the vae model

   the approach of vaes is a seemingly simple extension of the
   autoencoder: instead of forcing the encoder to produce a single
   encoding, it forces the encoder to produce a id203 distribution
   (in practice, a gaussian distribution) over encodings. the decoder will
   then sample an encoding from that id203 distribution, and try to
   reconstruct the original input.

   vae

   this forces the decoder to produce reasonable outputs over a range of
   different encodings. since a gaussian distribution can be parametrized
   by its mean and covariance matrix, we have the encoder output a mean
   vector  \mu and a covariance matrix  \sigma (restricted to a diagonal
   matrix for simplicity).

   an astute reader would realize that this approach has a problem: there
   is nothing stopping the encoder from outputting an extremely narrow
   distribution, effectively a single value. obviously, this is desirable
   from the perspective of the model, since the decoder would no longer
   have to care about covering a wide range of inputs. this defeats the
   purpose of the vae thought.

   in order to combat this, the vae introduces a loss other than the
   reconstruction loss: the kl divergence between the distribution
   produced by the encoder and a unit gaussian distribution (mean 0,
   covariance matrix is the identity matrix). the kl divergence is a way
   of measuring the distance between two distributions. adding the kl
   divergence term to the loss forces the model to make a trade-off
   between ease of reproduction and covering the unit gaussian. this
   allows us to safely create new images from encodings sampled from the
   unit gaussian.

3. optimization and the reparameterization trick

   autoencoders are simple to train since you simply have to backpropagate
   the reconstruction loss across the weights of the network.

   vaes are not as simple to optimize though. the key problem is that the
   sampling operation is not differentiable. this means we cannot
   propagate the gradients from the reconstruction error to the encoder.

   no_backprop

   normally we would have to resort to more complicated optimization
   techniques like reinforce, but luckily we are able to resolve this
   problem through the reparameterization trick.

   the idea behind this trick is to isolate the sampling from the
   parameter estimation (mean and variance). first, we sample \epsilon
   from a unit gaussian distribution.  thanks to the properties of the
   gaussian distribution, we can make the sample to adhere to a gaussian
   distribution with mean \mu and covariance matrix \sigma by transforming
   it as follows:

   z = \mu + \sigma^{\frac{1}{2}} \epsilon

   vae_complete.png

   this removes the sampling operation from the flow of gradients, so we
   can now train the model through simple id26.

   this ease of training along with the generational model it produces are
   what make vaes so powerful and popular.

4. conclusion and further readings

   hopefully, this post served to foster an intuitive understanding of
   vaes. although i deliberately left equations out of this post, the math
   is actually quite interesting. i will be covering the mathematics
   behind vaes in a separate, future post. for those who want more
   in-depth understanding now, i refer them to the following resources:

   [11]tutorial on id5

   [12]auto-encoding id58

   if you found this article insightful and want to learn more, i have
   explained the mathematics behind vaes in more detail in [13]this post.

share this:

     * [14]click to share on twitter (opens in new window)
     * [15]click to share on facebook (opens in new window)
     *

like this:

   like loading...

related

   author [16]keitakuritaposted on [17]december 28, 2017march 2,
   2018categories [18]deep learning, [19]machine learning

post navigation

   [20]previous previous post: creating custom magic commands in jupyter
   [21]next next post: an overview of sentence embedding methods

top posts & pages

     * [22]an in-depth tutorial to allennlp (from basics to elmo and bert)
     * [23]weight id172 and layer id172 explained
       (id172 in deep learning part 2)
     * [24]paper dissected: "attention is all you need" explained
     * [25]lightgbm and xgboost explained
     * [26]a practical introduction to nmf (nonnegative matrix
       factorization)

subscribe to blog via email

   find anything useful? ;)
   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

categories

     * [27]id161 (2)
     * [28]deep learning (22)
     * [29]fromscratch (1)
     * [30]jupyter (2)
     * [31]kaggle (1)
     * [32]machine learning (13)
     * [33]nlp (11)
     * [34]paper (10)
     * [35]python (1)
     * [36]skills (1)
     * [37]software (1)
     * [38]software engineering (2)
     * [39]uncategorized (3)

archives

     * [40]april 2019
     * [41]february 2019
     * [42]january 2019
     * [43]november 2018
     * [44]september 2018
     * [45]august 2018
     * [46]june 2018
     * [47]may 2018
     * [48]april 2018
     * [49]march 2018
     * [50]february 2018
     * [51]january 2018
     * [52]december 2017

     * [53]about this blog
     * [54]github

   [55]machine learning explained [56]proudly powered by wordpress

   iframe: [57]likes-master

   %d bloggers like this:

references

   1. http://id113xplained.com/feed/
   2. http://id113xplained.com/comments/feed/
   3. http://id113xplained.com/2017/12/28/creating-custom-magic-commands-in-jupyter/
   4. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/
   5. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/
   6. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/&format=xml
   7. http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/#content
   8. http://id113xplained.com/
   9. http://id113xplained.com/about-this-blog/
  10. https://github.com/keitakurita
  11. https://arxiv.org/abs/1606.05908
  12. https://arxiv.org/abs/1312.6114
  13. http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/
  14. http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/?share=twitter
  15. http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/?share=facebook
  16. http://id113xplained.com/author/admin/
  17. http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/
  18. http://id113xplained.com/category/machine-learning/deep-learning/
  19. http://id113xplained.com/category/machine-learning/
  20. http://id113xplained.com/2017/12/28/creating-custom-magic-commands-in-jupyter/
  21. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/
  22. http://id113xplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/
  23. http://id113xplained.com/2018/01/13/weight-id172-and-layer-id172-explained-id172-in-deep-learning-part-2/
  24. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  25. http://id113xplained.com/2018/01/05/lightgbm-and-xgboost-explained/
  26. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  27. http://id113xplained.com/category/computer-vision/
  28. http://id113xplained.com/category/machine-learning/deep-learning/
  29. http://id113xplained.com/category/fromscratch/
  30. http://id113xplained.com/category/jupyter/
  31. http://id113xplained.com/category/kaggle/
  32. http://id113xplained.com/category/machine-learning/
  33. http://id113xplained.com/category/nlp/
  34. http://id113xplained.com/category/paper/
  35. http://id113xplained.com/category/python/
  36. http://id113xplained.com/category/skills/
  37. http://id113xplained.com/category/software/
  38. http://id113xplained.com/category/software-engineering/
  39. http://id113xplained.com/category/uncategorized/
  40. http://id113xplained.com/2019/04/
  41. http://id113xplained.com/2019/02/
  42. http://id113xplained.com/2019/01/
  43. http://id113xplained.com/2018/11/
  44. http://id113xplained.com/2018/09/
  45. http://id113xplained.com/2018/08/
  46. http://id113xplained.com/2018/06/
  47. http://id113xplained.com/2018/05/
  48. http://id113xplained.com/2018/04/
  49. http://id113xplained.com/2018/03/
  50. http://id113xplained.com/2018/02/
  51. http://id113xplained.com/2018/01/
  52. http://id113xplained.com/2017/12/
  53. http://id113xplained.com/about-this-blog/
  54. https://github.com/keitakurita
  55. http://id113xplained.com/
  56. https://wordpress.org/
  57. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
