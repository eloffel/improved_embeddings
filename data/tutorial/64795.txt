   #[1]machine learning explained    feed [2]machine learning explained   
   comments feed [3]a practical introduction to nmf (nonnegative matrix
   factorization) [4]paper dissected:    attention is all you need   
   explained [5]alternate [6]alternate

   [7]skip to content

   [8]machine learning explained

   deep learning, python, data wrangling and other machine learning
   related topics explained for practitioners
   (button) menu

     * [9]about this blog
     * [10]github

an introduction to the math of id5 (vaes part 2)

   in an [11]earlier post, i gave an intuitive (but informal) explanation
   of vaes. though simple intuition would be sufficient to get a vae
   working, vaes are only one among numerous methods that use a similar
   mode of thought.

   in order to understand vaes in depth and to be able to comprehend other
   related methods, a mathematical understanding can be very helpful. this
   post builds upon an intuitive understanding and attempts to deepen it
   using the math that underpins vaes.

1. the objective

   most machine learning models optimize some objective with regards to
   some parameters. the first step towards understanding vaes, therefore,
   is to understand the objective and what it intuitively means.

1.1 the ideal objective

   in regular supervised learning, we attempt to optimize parameters
   \theta to maximize the likelihood of obtaining the data:

   p(x, y | \theta)

   where x is the data and y are the labels. generally, the model is a
   parameterized id203 distribution that takes in inputs and outputs
   class probabilities (in the case of classification) or an estimate of
   some value (in the case of regression).

   in the case of autoencoders, we do not have any labels. what then, is
   the objective?

   recall that in the case of the vae, we want to learn a model that can
   generate outputs that resemble the inputs it received during training
   when it is given latent codes sampled from the unit gaussian
   \mathcal{n}(0, i) . we will denote the codes as latent variable z . in
   probabilistic terms, a    reasonable sample    is a sample that has a high
   id203 of belonging to the same distribution as the data x
   . therefore, our objective is to maximize p(x | z) for z that are
   sampled from p(z) = \mathcal{n}(0, i) .

   formally, we can write this objective as:

   p(x | \theta) = \int p(x|z, \theta)p(z) dz

   in practice, we maximize the log-likelihood of the data (also known as
   the evidence)

   \log(p(x | \theta))

   this is our ideal objective that we want to maximize. for the sake of
   brevity, i will leave the parameter \theta out of the equations from
   here on.

   having formulated our objective, we now turn to optimization. the
   problem is, the above objective     as we will demonstrate     is extremely
   difficult to optimize directly. therefore, we make a few    adjustments   
   to the objective. these    adjustments    are at the heart of vaes, as well
   as the field of variational id136 as a whole.


1.2 the real objective

   an astute reader may be a bit confused by my explanation above: in the
   previous post, i explained that the encoder produced samples of z
   according to the distribution

   \mathcal{n}(\mu, \sigma)

   where \mu and \sigma were dependent on x . where is this distribution
   in the above equation?

   before explaining this, let   s examine why directly optimizing $latex
   \int p(x|z, \theta)p(z) dz  $ does not work in practice.

   when we try to optimize $latex \int p(x|z, \theta)p(z) dz $, we are
   optimizing e_{z \sim p(z)}[p(x|z, \theta)] . since this is impossible
   to compute analytically, we optimize this is by sampling z from $ latex
   p(z) $ and computing $latex p(x|z, \theta) $, then performing gradient
   descent.

   the problem is that for each data point x , a great majority of z \sim
   p(z) is unlikely to produce anything close to it. this makes learning
   extremely inefficient since we will need to sample a large number of z
   to obtain any meaningful output for x .

   what if, instead of sampling from the prior p(z) , we sampled z from
   the posterior distribution  p(z | x) for each data point x ? this would
   allow us to concentrate on samples that are likely to produce x and
   avoid wasting samples. we would need to slightly change the objective
   though.

   using bayes rule, we can prove that

   \log p(x) = \log p(x|z) + \log p(z) - \log p(z|x)

   e_{z \sim p(z|x)}[ \log p(x) ] = \log p(x) = e_{z \sim p(z|x)}[ \log
   p(x|z) + \log p(z) - \log p(z|x) ]

   \log p(x) = e_{z \sim p(z|x)}[\log p(x|z)] - k_{dl}(p(z | x) ||  p(z))

   where k_{dl} is the kl-divergence.

   from this equation, we see that the only modification we need to make
   is to add a kl-divergence term between the prior and the posterior, and
   we can optimize the likelihood of x by sampling from the posterior.

   the only problem is, the posterior is impossible to calculate as well,
   so we are essentially stuck. remember, in the vae, p(x|z) is an
   extremely complicated function computed by a neural network. we have no
   way of finding the true posterior.

   this is where  \mathcal{n}(\mu, \sigma) comes in. instead of sampling
   from p(z|x) , we sample from an approximate distribution q(z|x)
   = \mathcal{n}(\mu, \sigma) , which is parameterized by a neural
   network.

   using a similar procedure as the above, we can prove that

   \log p(x) - k_{dl}(q(z | x) ||  p(z | x)) = e_{z \sim q(z|x)}[\log
   p(x|z)] - k_{dl}(q(z | x) ||  p(z))

   this is almost entirely the same as the above objective, except for the
   kl-divergence term in the left-hand side.

   this equation is the centerpiece of the mathematics behind vaes. let   s
   look into it a bit further.


2. dissecting the objective

   \log p(x) - k_{dl}(q(z | x) ||  p(z | x)) = e_{z \sim q(z|x)}[\log
   p(x|z)] - k_{dl}(q(z | x) ||  p(z))

   let   s look at the right-hand side. the right-hand side is composed of
   two terms. the first term

   e_{z \sim q(z|x)}[\log p(x|z)]

   measures how likely the sampled z are to generate the data. this
   basically pulls the model in the direction of explaining the data as
   well as it can.

   the second term

   k_{dl}(q(z | x) ||  p(z))

   measures how far q(z | x) deviates from the prior. this essentially
   acts as a kind of id173 term, stopping the model from
   outputting extremely narrow distributions to easily explain the data.

   the right-hand side is commonly known as the evidence lower bound
   (elbo). this is because it sets a lower bound on the evidence (the
   log-likelihood of the data). to understand further, let   s look at the
   left-hand side.

   the left-hand side of the equation is almost the log evidence. the only
   difference is the kl-term. recall that the kl-divergence becomes zero
   if and only if the two distributions are exactly the same. this means
   that if q(z|x) is sufficiently close to p(z|x) , then this term is
   exactly the log likelihood. otherwise, the kl-divergence is always
   positive, so the right-hand side effectively acts as a lower bound on
   the log-evidence.

   therefore, if q(z|x) is sufficiently flexible so that it can
   approximate p(z|x) , then we can optimize logp(x) by optimizing the
   right-hand side equation.

   thankfully, q(z|x) is parametrized by a neural network so we can expect
   it to be sufficiently flexible. since we can sample from q(z | x) and
   the kl-divergence is analytically computable so the right-hand side can
   be optimized relatively easily in the case of vaes.

   this is the theoretical backbone of vaes and why they work. the encoder
   is     in a way- an auxiliary tool that enables quicker training so that
   we can obtain a good decoder. by having the encoder approximate the
   posterior, we virtually optimize the evidence function while retaining
   a realistic degree of trainability.

   incidentally, this equation and the elbo often appear when we are
   trying to train probabilistic models that produce distribution rather
   than single estimates. the above equation is independent of the
   underlying models and id203 distributions, which is one of the
   reasons why it is so powerful.

3. conclusion

   hopefully, this post served to give you a better understanding of vaes
   and the mathematics behind them. variational methods are applied in a
   wide variety of areas, so the above reasoning is worth understanding in
   depth.

share this:

     * [12]click to share on twitter (opens in new window)
     * [13]click to share on facebook (opens in new window)
     *

like this:

   like loading...

related

   author [14]keitakuritaposted on [15]december 28, 2017march 2,
   2018categories [16]deep learning

post navigation

   [17]previous previous post: a practical introduction to nmf
   (nonnegative id105)
   [18]next next post: paper dissected:    attention is all you need   
   explained

top posts & pages

     * [19]an in-depth tutorial to allennlp (from basics to elmo and bert)
     * [20]weight id172 and layer id172 explained
       (id172 in deep learning part 2)
     * [21]lightgbm and xgboost explained
     * [22]paper dissected: "attention is all you need" explained
     * [23]a practical introduction to nmf (nonnegative matrix
       factorization)

subscribe to blog via email

   find anything useful? ;)
   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

categories

     * [24]id161 (2)
     * [25]deep learning (22)
     * [26]fromscratch (1)
     * [27]jupyter (2)
     * [28]kaggle (1)
     * [29]machine learning (13)
     * [30]nlp (11)
     * [31]paper (10)
     * [32]python (1)
     * [33]skills (1)
     * [34]software (1)
     * [35]software engineering (2)
     * [36]uncategorized (3)

archives

     * [37]april 2019
     * [38]february 2019
     * [39]january 2019
     * [40]november 2018
     * [41]september 2018
     * [42]august 2018
     * [43]june 2018
     * [44]may 2018
     * [45]april 2018
     * [46]march 2018
     * [47]february 2018
     * [48]january 2018
     * [49]december 2017

     * [50]about this blog
     * [51]github

   [52]machine learning explained [53]proudly powered by wordpress

   iframe: [54]likes-master

   %d bloggers like this:

references

   1. http://id113xplained.com/feed/
   2. http://id113xplained.com/comments/feed/
   3. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
   4. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
   5. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/
   6. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/&format=xml
   7. http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/#content
   8. http://id113xplained.com/
   9. http://id113xplained.com/about-this-blog/
  10. https://github.com/keitakurita
  11. http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/
  12. http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/?share=twitter
  13. http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/?share=facebook
  14. http://id113xplained.com/author/admin/
  15. http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/
  16. http://id113xplained.com/category/machine-learning/deep-learning/
  17. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  18. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  19. http://id113xplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/
  20. http://id113xplained.com/2018/01/13/weight-id172-and-layer-id172-explained-id172-in-deep-learning-part-2/
  21. http://id113xplained.com/2018/01/05/lightgbm-and-xgboost-explained/
  22. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  23. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  24. http://id113xplained.com/category/computer-vision/
  25. http://id113xplained.com/category/machine-learning/deep-learning/
  26. http://id113xplained.com/category/fromscratch/
  27. http://id113xplained.com/category/jupyter/
  28. http://id113xplained.com/category/kaggle/
  29. http://id113xplained.com/category/machine-learning/
  30. http://id113xplained.com/category/nlp/
  31. http://id113xplained.com/category/paper/
  32. http://id113xplained.com/category/python/
  33. http://id113xplained.com/category/skills/
  34. http://id113xplained.com/category/software/
  35. http://id113xplained.com/category/software-engineering/
  36. http://id113xplained.com/category/uncategorized/
  37. http://id113xplained.com/2019/04/
  38. http://id113xplained.com/2019/02/
  39. http://id113xplained.com/2019/01/
  40. http://id113xplained.com/2018/11/
  41. http://id113xplained.com/2018/09/
  42. http://id113xplained.com/2018/08/
  43. http://id113xplained.com/2018/06/
  44. http://id113xplained.com/2018/05/
  45. http://id113xplained.com/2018/04/
  46. http://id113xplained.com/2018/03/
  47. http://id113xplained.com/2018/02/
  48. http://id113xplained.com/2018/01/
  49. http://id113xplained.com/2017/12/
  50. http://id113xplained.com/about-this-blog/
  51. https://github.com/keitakurita
  52. http://id113xplained.com/
  53. https://wordpress.org/
  54. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
