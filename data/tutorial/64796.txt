   #[1]machine learning explained    feed [2]machine learning explained   
   comments feed [3]an intuitive explanation of id5
   (vaes part 1) [4]a practical introduction to nmf (nonnegative matrix
   factorization) [5]alternate [6]alternate

   [7]skip to content

   [8]machine learning explained

   deep learning, python, data wrangling and other machine learning
   related topics explained for practitioners
   (button) menu

     * [9]about this blog
     * [10]github

an overview of sentence embedding methods

   id27s/vectors are a powerful method that has greatly assisted
   neural network based nlp methods. as of now, id97 and glove tend to
   be used as the standard method for obtaining id27s (although
   there are other methods out there).

   compared to this, the task of obtaining sentence embeddings remains
   more elusive. although sentence embeddings can often be obtained in a
   supervised way by training a model on top of id27s, there are
   often circumstances where we want to obtain sentence embeddings in an
   unsupervised manner. for instance, we may want to conduct paraphrase
   identification or create a system for retrieving similar sentences
   efficiently, where we do not have explicit supervision data.

   in this blog post, i aim to present an overview of some important
   unsupervised sentence embedding methods. this is by no means
   comprehensive; there are many more methods out there, and if there are
   any important ones that i have missed, it would be great if you could
   leave comments so that i can learn and improve this post     

1. topic models

   although often overshadowed by neural network based methods, topic
   modeling is a surprisingly powerful and interpretable framework for
   embedding sentences.

   id96 basically aims to decompose a set of documents/sentences
   into several distinct topics based on the co-occurrence of words. for
   instance, in news corpuses, words corresponding to politics, science,
   and economics will tend to coexist within the same document. topic
   modeling represents a document as a weighted sum of topics, which can
   be, in turn, used as a sentence embedding.

   nmf and lda are both popular methods for id96. for those that
   are interested in the details, i have written a post on nmf [11]here.

   advantages

   id96 is easily interpretable and efficient to calculate. the
   legitimacy of the modeling can be examined by the obtained topics.
   id96 is also easy to implement, with nmf being implemented in
   [12]sklearn, and lda being implemented in [13]gensim.

   disadvantages

   id96 does not take the order of the words in a sentence into
   account. it does not capture the meaning of the sentence either. for
   instance, the sentences

      i hate this movie because of its usage of cgi and acting   

   and

      i love this movie because of its usage of cgi and acting   

   would probably be represented as similar vectors, since their
   bag-of-words representations are extremely similar.


2. paragraph vectors (doc2vec)

   introduced in [14]this paper, paragraph vectors (often referred to as
   doc2vec) are an extension of id97.  id97 attempts to predict a
   word(s) in a sentence from its surrounding words (or predict
   surrounding words from a single word, but this difference is not
   important in this post). paragraph vectors extend this by training
   paragraph vectors as auxiliary information during this prediction task.

   the details are shown in the figure below.

   doc2vec

   each paragraph (or sentence/document) is associated with a vector.
   during training of the id97 model, the paragraph vector is either
   averaged or concatenated with the context vector (composed of the word
   vectors of the surrounding words in the sentence), and used in the
   prediction.

   intuitively, paragraph vectors encode the additional information
   necessary to predict a word from its context that is not clear just
   from the context words themselves. for example, the following sentence

      i have a pet _____   

   could contain a multitude of words, including    cat   ,    dog   , and
      snake   . the paragraph vector would assist prediction by encoding which
   word enters the blank.

   as a word of caution, this is an explanation of the distributed-memory
   model. in the original paper, the distributed-bow model is also
   introduced, but for the sake of brevity, i will omit it in this post.

   advantages

   in the original paper, paragraph vectors are explained to have the
   following advantages:

   (1) they inherit the semantics of word vectors

   for instance, the word    powerful    is closer to    strong    in the semantic
   space compared to    paris   . paragraph vectors can utilize these learned
   representations.

   (2) they take word order into account

   though paragraph vectors do not use a sequential model like an id56,
   they take the word order into account in a small context, similar to
   how an id165 model with a large n would. this is important since
   id165 models preserve a lot of information of the paragraph, including
   word order. compared to id165 models, paragraph vectors are
   significantly lower dimension and less prone to overfitting when used.

   paragraph vectors are implemented in [15]gensim as well, making them
   very easy to use. they are also relatively quick to train as well.

   disadvantages

   what kind of information is actually captured in the paragraph vectors
   is unclear and difficult to interpret, since the prediction task
   encodes information both in word vectors and paragraph vectors. the
   quality of the vectors is also highly dependent on the quality of the
   word vectors.


3. skip-thought vectors

   introduced in [16]this paper, skip-thought vectors are     in a way    
   id97 for sentences. where id97 attempts to predict surrounding
   words from certain words in a sentence, skip-thought vector extends
   this idea to sentences: it predicts surrounding sentences from a given
   sentence.

   the model is summarized in the following figure:

   skip-thought

   skip-thought vectors use the encoder-decoder model to first encode a
   sentence into a vector, then decode that representation into the
   surrounding sentences.
   despite being deceptively simple, skip-thought vectors have achieved
   outstanding results in a wide variety of tasks including sentiment
   analysis and paraphrase detection.

   advantages

   skip-thought vectors not only take word order into account but also
   take the ordering of sentences into account. this allows it to encode
   rich information into the embedding. unsurprisingly, skip-thought
   vectors often show the best performance among other unsupervised
   methods on a wide range of tasks.

   disadvantages

   unlike the other methods, skip-thought vectors require the sentences to
   be ordered in a semantically meaningful way. this makes this method
   difficult to use for domains such as social media text, where each
   snippet of text exists in isolation. by virtue of it being a deep
   neural network model, it is also much slower to train than the other
   methods.


4. fastsent

   skip-thought vectors are slow to train. fastsent attempts to remedy
   this inefficiency while expanding on the core idea of skip-thought:
   that predicting surrounding sentences is a powerful way to obtain
   distributed representations.

   formally, fastsent represents sentences as the simple sum of its word
   embeddings, making training efficient. the id27s are learned
   so that the inner product between the sentence embedding and the word
   embeddings of surrounding sentences is maximized.

   advantages

   fastsent shares the advantages and disadvantages of skip-thought in its
   limitation of usage and the kind of information it encodes. it is
   vastly more efficient than skip-thought though, which is its main
   advantage.

   disadvantages

   fastsent sacrifices word order for the sake of efficiency, which can be
   a large disadvantage depending on the use-case.


5. weighted sum of word vectors

   [17]this paper showed that an    embarrassingly simple    baseline for
   sentence embedding can work well: a weighted sum of word vectors.

   in this method, each word vector is weighted by the factor \frac{a}{a +
   p(w)} where a is a hyperparameter and p(w) is the (estimated) word
   frequency. this is similar to tf-idf weighting, where more frequent
   terms are weighted down.

   the paper gives an interesting theoretical justification to this
   formulation, based on a model of sentences where each word in a
   sentence is generated by a    random-walk    of the discourse (sentence)
   vector.

   advantages

   due to its simplicity, this method is very easy to implement by hand
   and fast to compute. it is also widely applicable and can be used for
   both short and long sentences from a wide variety of domains including
   social media.

   disadvantages

   being a relatively new method, the weighted sum of word vectors has not
   been extensively used and tested compared to older methods like
   skip-thought, making its performance difficult to predict. word order
   and surrounding sentences are ignored as well, limiting the information
   that is encoded.


6. other methods and further readings

   there are too many sentence embedding methods to cover in this post, so
   i mainly focused on methods that were unique to natural language.

   similarly to images or any other domain, there are various sentence
   embedding methods that are universally applicable, such as denoising
   autoencoders and variational auto encoders.


   for further information on methods explained or omitted above, i will
   refer the reader to the following resources:

   [18]cmu neural nets for nlp 2017 lesson 6: using/evaluating sentence
   representations

   [19]learning distributed representations of sentences from unlabeled
   data

share this:

     * [20]click to share on twitter (opens in new window)
     * [21]click to share on facebook (opens in new window)
     *

like this:

   like loading...

related

   author [22]keitakuritaposted on [23]december 28, 2017march 2,
   2018categories [24]deep learning, [25]nlp

post navigation

   [26]previous previous post: an intuitive explanation of variational
   autoencoders (vaes part 1)
   [27]next next post: a practical introduction to nmf (nonnegative matrix
   factorization)

top posts & pages

     * [28]an in-depth tutorial to allennlp (from basics to elmo and bert)
     * [29]weight id172 and layer id172 explained
       (id172 in deep learning part 2)
     * [30]paper dissected: "attention is all you need" explained
     * [31]lightgbm and xgboost explained
     * [32]a practical introduction to nmf (nonnegative matrix
       factorization)

subscribe to blog via email

   find anything useful? ;)
   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

categories

     * [33]id161 (2)
     * [34]deep learning (22)
     * [35]fromscratch (1)
     * [36]jupyter (2)
     * [37]kaggle (1)
     * [38]machine learning (13)
     * [39]nlp (11)
     * [40]paper (10)
     * [41]python (1)
     * [42]skills (1)
     * [43]software (1)
     * [44]software engineering (2)
     * [45]uncategorized (3)

archives

     * [46]april 2019
     * [47]february 2019
     * [48]january 2019
     * [49]november 2018
     * [50]september 2018
     * [51]august 2018
     * [52]june 2018
     * [53]may 2018
     * [54]april 2018
     * [55]march 2018
     * [56]february 2018
     * [57]january 2018
     * [58]december 2017

     * [59]about this blog
     * [60]github

   [61]machine learning explained [62]proudly powered by wordpress

   iframe: [63]likes-master

   %d bloggers like this:

references

   1. http://id113xplained.com/feed/
   2. http://id113xplained.com/comments/feed/
   3. http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/
   4. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
   5. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/
   6. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/&format=xml
   7. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/#content
   8. http://id113xplained.com/
   9. http://id113xplained.com/about-this-blog/
  10. https://github.com/keitakurita
  11. http://keitakurita.com/2017/11/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  12. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.nmf.html
  13. https://radimrehurek.com/gensim/models/ldamodel.html
  14. https://arxiv.org/pdf/1405.4053.pdf
  15. https://radimrehurek.com/gensim/models/doc2vec.html
  16. https://arxiv.org/pdf/1506.06726.pdf
  17. https://openreview.net/pdf?id=syk00v5xx
  18. https://www.youtube.com/watch?v=7j20mc6j_qu
  19. http://www.aclweb.org/anthology/n16-1162
  20. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/?share=twitter
  21. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/?share=facebook
  22. http://id113xplained.com/author/admin/
  23. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/
  24. http://id113xplained.com/category/machine-learning/deep-learning/
  25. http://id113xplained.com/category/nlp/
  26. http://id113xplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1/
  27. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  28. http://id113xplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/
  29. http://id113xplained.com/2018/01/13/weight-id172-and-layer-id172-explained-id172-in-deep-learning-part-2/
  30. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  31. http://id113xplained.com/2018/01/05/lightgbm-and-xgboost-explained/
  32. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  33. http://id113xplained.com/category/computer-vision/
  34. http://id113xplained.com/category/machine-learning/deep-learning/
  35. http://id113xplained.com/category/fromscratch/
  36. http://id113xplained.com/category/jupyter/
  37. http://id113xplained.com/category/kaggle/
  38. http://id113xplained.com/category/machine-learning/
  39. http://id113xplained.com/category/nlp/
  40. http://id113xplained.com/category/paper/
  41. http://id113xplained.com/category/python/
  42. http://id113xplained.com/category/skills/
  43. http://id113xplained.com/category/software/
  44. http://id113xplained.com/category/software-engineering/
  45. http://id113xplained.com/category/uncategorized/
  46. http://id113xplained.com/2019/04/
  47. http://id113xplained.com/2019/02/
  48. http://id113xplained.com/2019/01/
  49. http://id113xplained.com/2018/11/
  50. http://id113xplained.com/2018/09/
  51. http://id113xplained.com/2018/08/
  52. http://id113xplained.com/2018/06/
  53. http://id113xplained.com/2018/05/
  54. http://id113xplained.com/2018/04/
  55. http://id113xplained.com/2018/03/
  56. http://id113xplained.com/2018/02/
  57. http://id113xplained.com/2018/01/
  58. http://id113xplained.com/2017/12/
  59. http://id113xplained.com/about-this-blog/
  60. https://github.com/keitakurita
  61. http://id113xplained.com/
  62. https://wordpress.org/
  63. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
