   #[1]machine learning explained    feed [2]machine learning explained   
   comments feed [3]an overview of sentence embedding methods [4]an
   introduction to the math of id5 (vaes part 2)
   [5]alternate [6]alternate

   [7]skip to content

   [8]machine learning explained

   deep learning, python, data wrangling and other machine learning
   related topics explained for practitioners
   (button) menu

     * [9]about this blog
     * [10]github

a practical introduction to nmf (nonnegative id105)

   with the rise of complex models like deep learning, we often forget
   simpler, yet powerful machine learning methods that can be equally
   powerful. nmf (nonnegative id105) is one effective
   machine learning technique that i feel does not receive enough
   attention. nmf has a wide range of uses, from id96 to signal
   processing.

   this post aims to be a practical introduction to nmf. instead of
   delving into the mathematical proofs, i will attempt to provide the
   minimal intuition and knowledge necessary to use nmf in practice and
   interpret the results. i   ll also show example code for id96
   using nmf at the end of this post.

1. what is nmf, and what is the intuition behind it?

   nmf (nonnegative id105)  is a id105
   method where we constrain the matrices to be nonnegative. in order to
   understand nmf, we should clarify the underlying intuition between
   id105.

   suppose we factorize a matrix x into two matrices w and h so that x
   \approx wh

   there is no guarantee that we can recover the original matrix, so we
   will approximate it as best as we can.

   now, suppose that x   is composed of m rows x_1, x_2, ... x_m ,  w is
   composed of k rows w_1, w_2, ... w_k  ,  h is composed of m rows h_1,
   h_2, ... h_m . each row in  x can be considered a data point. for
   instance, in the case of decomposing images, each row in  x   is a
   single image, and each column represents some feature.

   mf_equations

   take the i-th row in x , x_i . if you think about the equation, you
   will find that x_i can be written as

   the meaning of this equation becomes clearer when we visualize it:

   mf_concept

   basically, we can interpret x_i to be a weighted sum of some components
   (or bases if you are more familiar with id202), where each row
   in  h is a component, and each row in w   contains the weights of each
   component.

   note that in this explanation, we treat each row of the input matrix x
   to be a single data point. in other explanations, each column might be
   considered a data point, in which case each column in w becomes a
   component and each column in  h   becomes a set of weights.

   in practice, we introduce various conditions on the components, so that
   they can be interpreted in a meaningful manner. in the case of nmf, we
   constrict the underlying components and weights to be non-negative.
   essentially, nmf decomposes each data point into an overlay of certain
   components.

2. when do we use nmf?

   nmf is suited for tasks where the underlying factors can be interpreted
   as non-negative. this is best demonstrated by examples, so let   s look
   at two examples below.

   example 1: faces

   a different, commonly used factorization method is pca (principle
   components analysis).

   all you currently need to know about pca is that it creates factors
   that can be both positive and negative.

   let   s see what happens when we try decomposing various faces. in the
   following image, we show the components (bases) we calculated with pca
   on the left and the weights corresponding to a single image on the
   right. red represents negative values.

   pca_faces

   if you look at the components, you see that they don   t make much sense.
   it   s also difficult to interpret what it means for a face to have a
      negative    component.

   now, let   s see what happens when we use nmf.

   nmf_faces

   now, the components seem to have some meaning. some of them look like
   parts of a nose or parts of an eye. we can consider each face to be an
   overlay of several components. this means we can interpret the
   decomposition of a face as having a certain weight of a certain nose
   type, a certain amount of some eye type, etc.


   example 2: id96

   imagine if you wanted to decompose a term-document matrix, where each
   column represented a document, and each element in the document
   represented the weight of a certain word (the weight might be the raw
   count or the tf-idf weighted count or some other encoding scheme; those
   details are not important here).

   what happens when we decompose this into two matrices? imagine if the
   documents came from news articles. the word    eat    would be likely to
   appear in food-related articles, and therefore co-occur with words like
      tasty    and    food   . therefore, these words would probably be grouped
   together into a    food    component vector, and each article would have a
   certain weight of the    food    topic.

   therefore, an nmf decomposition of the term-document matrix would yield
   components that could be considered    topics   , and decompose each
   document into a weighted sum of topics. this is called id96
   and is an important application of nmf.

   note that this interpretation would not be possible with other
   decomposition methods. we cannot interpret what it means to have a
      negative    weight of the food topic. this is another example where the
   underlying components (topics) and their weights should be
   non-negative.

   another interesting property of nmf is that it naturally produces
   sparse representations. this makes sense in the case of id96:
   documents generally do not contain a large number of topics.


3. how do we conduct nmf?

   in order to conduct nmf we formalize an objective function and
   iteratively optimize it. nmf is an np-hard problem in general, so we
   will aim for a good local minima.

   let   s look at the objective function.

   although there are some variants, a generally used measures of distance
   is the frobenius norm (the sum of element-wise squared errors).
   formalizing this, we obtain the following objective:

   minimize \, \| x - wh \|_{f}^{2} \,  w.r.t.  \, w, h \, s.t. \, w, h
   \geq 0

   next, we will introduce a couple of optimization methods.

   i. multiplicative update

   a commonly used method of optimization is the multiplicative update
   method.

   in this method, w and h   are each updated iteratively according to the
   following rule:

   h \leftarrow h \odot \frac{w^tx}{w^twh}

   w \leftarrow w \odot \frac{xh^t}{whh^t}

   where the matrix not being updated is kept constant.

   the objective functions can be proved to be non-increasing under this
   rule. a less formal, but more intuitive explanation of this method is
   to interpret this as rescaled id119.

   rescaled id119 can be written as follows:

   h \leftarrow h - \eta \odot [w^twh - w^tx]

   w \leftarrow w - \zeta \odot [whh^t - xh^t]

   if we set \eta to be \frac{h}{w^twh}  and \zeta   to be \frac{w}{whh^t}
   , then we obtain the multiplicative update rule.

   ii. hierarchical alternating least squares

   though less commonly used, in [11]this paper, hierarchical alternating
   least squares (hals) is shown to be a faster method for computing nmf.

   hals updates one column of w at a time, using the property that the
   columns of w do not interact with each other. it computes the
   closed-form optimal solution for each column, keeping all other columns
   constant.

   the rule can be expressed as follows:

   w[:,i] \leftarrow max(0, \frac{xh_i^t - \sum_{k \neq
   i}w[:,i]h_kh_i^t}{\| h_i \|^2})

   where w[:,i]   is the i-th column of  w  and $latex  h_i $ is the i-th
   row of h .


4. other commonly used tricks

   i. initialization

   any iterative optimization scheme is sensitive to its initialization.
   though random initialization often performs sufficiently well, other
   methods of initialization also exist.

   one such method uses svd to compute a rough estimate of the matrices w
     and h . if the non-negativity condition did not exist, taking the top
   k singular values and their corresponding vectors would construct the
   best rank k estimate, measured by the frobenius norm. since w   and  h
   must be non-negative, we must slightly modify the vectors we use.
   detailed discussion is beyond the scope of this blog post, so please
   refer to [12]this paper for the details.

   ii. id173

   we discussed the unregularized objective above, but we may want to
   impose stronger sparsity constraints or prevent the weights from
   becoming too large. to solve these problems, we can introduce l_1
   and  l_2 id173 losses on the weights of the matrices.


5. using nmf in practice

   nmf is implemented in scikit-learn, making it very easy to use. if you
   understood the above explanation, you will see that it is not that
   difficult to implement by hand either.

   the following is some example code of nmf for id96 on a
   term-document matrix, using scikit-learn. we use the built-in 20
   newsgroups dataset and see what topics are generated.



import numpy as np

from sklearn.datasets import fetch_20newsgroups

from sklearn.feature_extraction.text import tfidfvectorizer

from sklearn.decomposition import nmf

data= fetch_20newsgroups(remove=('headers', 'footers', 'quotes')).data

# convert the text to a tf-idf weighted term-document matrix

vectorizer = tfidfvectorizer(max_features=2000, min_df=10, stop_words='english')

x = vectorizer.fit_transform(data)

idx_to_word = np.array(vectorizer.get_feature_names())

# apply nmf

nmf = nmf(n_components=20, solver="mu")

w = nmf.fit_transform(x)

h = nmf.components_

# print the topics

for i, topic in enumerate(h):

    print("topic {}: {}".format(i + 1, ",".join([str(x) for x in idx_to_word[top
ic.argsort()[-10:]]])))



   aside from the time for downloading the dataset, the code executes in
   less than 10 seconds on my macbook pro.

   here are the results:

   topic 1: don,make,say,really,way,did,right,good,time,people
   topic 2:
   pc,appreciated,information,program,mail,looking,need,software,help,use
   topic 3:
   lord,church,christian,christians,believe,faith,christ,bible,jesus,god
   topic 4:
   nsa,algorithm,public,escrow,government,keys,clipper,encryption,chip,key
   topic 5: hd,cd,floppy,controller,ide,hard,disk,drives,scsi,drive
   topic 6: 50,20,price,condition,offer,shipping,10,new,sale,00
   topic 7: using,ms,program,problem,running,files,window,dos,file,windows
   topic 8: teams,win,hockey,play,season,players,year,games,team,game
   topic 9: ftp,pub,cc,university,cs,soon,banks,gordon,pitt,edu
   topic 10: new,oil,speed,good,miles,dealer,engine,bike,cars,car
   topic 11: ram,color,bus,driver,vga,cards,drivers,monitor,video,card
   topic 12: land,state,war,peace,arabs,jewish,arab,jews,israeli,israel
   topic 13:
   appreciate,address,windows,looking,info,tell,hi,mail,advance,thanks
   topic 14:
   understand,agree,moral,david,pretty,need,know,people,don,think
   topic 15: mean,ll,oh,looks,look,new,wondering,sounds,like,just
   topic 16: ftp,mean,info,file,read,let,anybody,don,does,know
   topic 17: sun,hp,address,email,bob,internet,article,dave,list,com
   topic 18: sci,earth,moon,station,gov,orbit,launch,shuttle,nasa,space
   topic 19:
   soviet,azerbaijan,said,genocide,turks,turkey,armenia,turkish,armenians,
   armenian
   topic 20: noticed,months,recently,list,tried,people,heard,seen,got,ve

   the originally intended topics are:
   comp.graphics

   comp.os.ms-windows.misc

   comp.sys.ibm.pc.hardware

   comp.sys.mac.hardware

   comp.windows.x
   rec.autos

   rec.motorcycles

   rec.sport.baseball

   rec.sport.hockey
   sci.crypt

   sci.electronics

   sci.med

   sci.space
   misc.forsale      talk.politics.misc

   talk.politics.guns

   talk.politics.mideast
   talk.religion.misc

   alt.atheism

   soc.religion.christian


   seems relatively reasonable for such simple code.


6. conclusion

   i hope this blog post has informed you with the basics of nmf and has
   convinced you that nmf is a quick but surprisingly powerful machine
   learning method.


7. further readings

   for more details about the mathematics involved and related methods,
   check out the readings below:


   [13]scikit-learn user guide


   papers

   [14]algorithms for non-negative id105

   [15]the why and how of nonnegative id105

share this:

     * [16]click to share on twitter (opens in new window)
     * [17]click to share on facebook (opens in new window)
     *

like this:

   like loading...

related

   author [18]keitakuritaposted on [19]december 28, 2017march 2,
   2018categories [20]machine learning, [21]nlp

post navigation

   [22]previous previous post: an overview of sentence embedding methods
   [23]next next post: an introduction to the math of variational
   autoencoders (vaes part 2)

top posts & pages

     * [24]an in-depth tutorial to allennlp (from basics to elmo and bert)
     * [25]weight id172 and layer id172 explained
       (id172 in deep learning part 2)
     * [26]paper dissected: "attention is all you need" explained
     * [27]lightgbm and xgboost explained
     * [28]a practical introduction to nmf (nonnegative matrix
       factorization)

subscribe to blog via email

   find anything useful? ;)
   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

categories

     * [29]id161 (2)
     * [30]deep learning (22)
     * [31]fromscratch (1)
     * [32]jupyter (2)
     * [33]kaggle (1)
     * [34]machine learning (13)
     * [35]nlp (11)
     * [36]paper (10)
     * [37]python (1)
     * [38]skills (1)
     * [39]software (1)
     * [40]software engineering (2)
     * [41]uncategorized (3)

archives

     * [42]april 2019
     * [43]february 2019
     * [44]january 2019
     * [45]november 2018
     * [46]september 2018
     * [47]august 2018
     * [48]june 2018
     * [49]may 2018
     * [50]april 2018
     * [51]march 2018
     * [52]february 2018
     * [53]january 2018
     * [54]december 2017

     * [55]about this blog
     * [56]github

   [57]machine learning explained [58]proudly powered by wordpress

   iframe: [59]likes-master

   %d bloggers like this:

references

   1. http://id113xplained.com/feed/
   2. http://id113xplained.com/comments/feed/
   3. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/
   4. http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/
   5. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
   6. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/&format=xml
   7. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/#content
   8. http://id113xplained.com/
   9. http://id113xplained.com/about-this-blog/
  10. https://github.com/keitakurita
  11. https://arxiv.org/abs/1401.5226
  12. http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/papers/hpclab020107.pdf
  13. http://scikit-learn.org/stable/modules/decomposition.html#nmf
  14. https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf
  15. https://arxiv.org/pdf/1401.5226.pdf
  16. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/?share=twitter
  17. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/?share=facebook
  18. http://id113xplained.com/author/admin/
  19. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  20. http://id113xplained.com/category/machine-learning/
  21. http://id113xplained.com/category/nlp/
  22. http://id113xplained.com/2017/12/28/an-overview-of-sentence-embedding-methods/
  23. http://id113xplained.com/2017/12/28/an-introduction-to-the-math-of-variational-autoencoders-vaes-part-2/
  24. http://id113xplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/
  25. http://id113xplained.com/2018/01/13/weight-id172-and-layer-id172-explained-id172-in-deep-learning-part-2/
  26. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  27. http://id113xplained.com/2018/01/05/lightgbm-and-xgboost-explained/
  28. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  29. http://id113xplained.com/category/computer-vision/
  30. http://id113xplained.com/category/machine-learning/deep-learning/
  31. http://id113xplained.com/category/fromscratch/
  32. http://id113xplained.com/category/jupyter/
  33. http://id113xplained.com/category/kaggle/
  34. http://id113xplained.com/category/machine-learning/
  35. http://id113xplained.com/category/nlp/
  36. http://id113xplained.com/category/paper/
  37. http://id113xplained.com/category/python/
  38. http://id113xplained.com/category/skills/
  39. http://id113xplained.com/category/software/
  40. http://id113xplained.com/category/software-engineering/
  41. http://id113xplained.com/category/uncategorized/
  42. http://id113xplained.com/2019/04/
  43. http://id113xplained.com/2019/02/
  44. http://id113xplained.com/2019/01/
  45. http://id113xplained.com/2018/11/
  46. http://id113xplained.com/2018/09/
  47. http://id113xplained.com/2018/08/
  48. http://id113xplained.com/2018/06/
  49. http://id113xplained.com/2018/05/
  50. http://id113xplained.com/2018/04/
  51. http://id113xplained.com/2018/03/
  52. http://id113xplained.com/2018/02/
  53. http://id113xplained.com/2018/01/
  54. http://id113xplained.com/2017/12/
  55. http://id113xplained.com/about-this-blog/
  56. https://github.com/keitakurita
  57. http://id113xplained.com/
  58. https://wordpress.org/
  59. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
