   #[1]machine learning explained    feed [2]machine learning explained   
   comments feed [3]creating custom magic commands in jupyter [4]alternate
   [5]alternate

   [6]skip to content

   [7]machine learning explained

   deep learning, python, data wrangling and other machine learning
   related topics explained for practitioners
   (button) menu

     * [8]about this blog
     * [9]github

how to write a web crawler in python (with examples!)

    machine learning requires a large amount of data. in some cases, other
   people might have already created great open datasets that we can use.
   however, sometimes we need to make out own datasets.
   one way to gather lots of data efficiently is by using a crawler.
   crawlers traverse the internet and accumulate useful data. python has a
   rich ecosystem of crawling related libraries. this post does not aim to
   introduce those libraries, but rather aims to inform the reader of how
   crawling works through implementing a simple crawler from scratch.
   i will introduce an example of a custom crawler that i wrote myself to
   crawl twitter in a future article, so if you are interested in actual
   applications of self-implemented crawlers, please take a look     

1. how do crawlers work?

   the workings of a crawler are very simple. below is a step by step
   explanation of what kind of actions take place behind crawling.


1.1 networks, requests, and the internet.

   when you look at a page on the internet through a browser like firefox
   or google chrome, you are getting the contents of the page from a
   remote server (of course the results might be cached, and there are all
   sorts of small details that might differ, but bear with me). you get
   these pages by sending a    request    to the server. the server returns
   the corresponding results by sending a    response   .
   there are various kinds of    requests   , but for the sake of simplicity i
   will only cover get requests in this article.
   a get request is basically the kind of request that happens when you
   access a url through a browser. the way a remote server knows that the
   request being sent to them is directed at them, and what resource to
   send back, is by looking at the url of the request. for example, a url
   like
   [10]http://www.hoge.com/foo
   is sent to    [11]www.hoge.com   , and requests the resource corresponding
   to    foo   . the server at [12]www.hoge.com thus returns the resource to
   the referrer (the sender of the request).
   the most important takeaway from this section is that browsing through
   pages is nothing more than simply sending requests and receiving
   responses.  anything that can be accessed on the internet can be
   acquired (theoretically) through this method.
   having clarified this, now we can understand the workings of a crawler.


1.2 extracting information

   basically, a crawler does roughly the same thing that a browser does:
   it sends a request, and acquires the response.
   the difference between a crawler and a browser, is that a browser
   visualizes the response for the user, whereas a crawler extracts useful
   information from the response.
   the question is, how exactly do you extract the necessary information
   from the response?
   this actually differs from case to case, but generally, you will have
   to use a html parser.
   web pages are (mostly) written in html. a detailed explanation of html
   and parsing it is outside the scope of this blog post, but i will give
   a brief explanation that will suffice for the purposes of understanding
   the basics of crawling. html, for those who are not familiar with it,
   stands for hyper text markup language, and is a language for expressing
   the contents of the page in a a structural manner. the structure of the
   page is expressed by enclosing information between tags, like below.
   hello world!
   tags can have several attributes, such as ids and classes. tags can
   also be nested. for example:
   this is a link
   represents an a tag within a div tag with the class    hoge   .
   the pages you crawl will (hopefully) have some common underlying
   structure, and you will be exploiting that to extract the necessary
   information. the underlying structure will differ for each set of pages
   and the type of information. therefore, before starting to crawl, you
   must investigate the structure of the pages you are trying to extract
   information from.
   the way you can investigate is by using the web inspector on your
   browser. in chrome, you can do this by opening the devtools, or by
   right clicking on any element on the page and selecting    inspect   . this
   will open up a tool that allows you to examine the html of the page at
   hand.


1.3 getting the next url to crawl

   just extracting information from one page is not very useful. most of
   the time, you will want to crawl multiple pages. however, it is often
   difficult or tedious to list up all the pages you want to crawl in
   advance. this is why crawlers will often extract the next url to crawl
   from the html of the page.
   the next url you want to access will often be embedded in the response
   you get. for example, if you are crawling search results, the link to
   the next set of search results will often appear at the bottom of the
   page.
   by dynamically extracting the next url to crawl, you can keep on
   crawling until you exhaust search results, without having to worry
   about terminating, how many search results there are, etc.


2. implementation

   having the above explained, implementing the crawler should be, in
   principle, easy.
   all you have to do is to manage the following 3 tasks:
        1. get the response from a url in the list of urls to crawl
        2. extract information from the url
        3. update the list of urls to crawl
   1 and 2 will require more specialized libraries. the libraries i would
   recommend are:
   1: requests ([13]http://requests-docs-ja.readthedocs.io/en/latest/)
   2: beautiful soup
   ([14]https://www.crummy.com/software/beautifulsoup/bs4/doc/)
   the following is a sample crawler that crawls wikipedia and prints out
   the list of search results for a given url.
from bs4 import beautifulsoup
import requests
from collections import deque

def parse_html(html):
    soup = beautifulsoup(html)
    title_element = soup.find("div", attrs={"class": "mw-search-result-heading"}
).find("a")
    title = title_element.attrs["title"] # in this crawler, we will collect the
title element
    links = soup.find_all("a", attrs={"class": "mw-nextlink"}) # get the next li
nks to crawl
    return title, [x.attrs["href"] for x in links if "href" in x.attrs]

def crawl(url_list):
    urls_to_crawl = deque(url_list)
    output_data = []
    while len(urls_to_crawl) > 0:
        url = urls_to_crawl.pop()
        url_contents = requests.get(url).text
        data, links = parse_html(url_contents)
        output_data.append(data)
        urls_to_crawl.extend(links)
    return output_data

   the crawl function is where all the work takes place. the list of urls
   is managed as a collections.deque object. urls are inserted and
   extracted from this object. the crawler uses the requests library to
   send a request to a url, fetches the response, and handles the response
   in the handle_response function.
   the handle_response function extracts the result titles in the first
   for loop and the links to the following pages in the second for loop.
   the way that the parse_html function extracts the titles is by using
   their common structure: a div tag with the
   class    mw-search-result-heading    that enclose an a tag with the title
   as the title attribute. the links to the following pages are extracted
   similarly: they are all a tags with the class    mw-nextlink    and the url
   of the next page as the href attribute.
   all newly found links are pushed to the queue, and crawling continues.

3. improvements

   the above is the basic structure of any crawler. if you want to use
   your crawler more extensively though, you might want to make a few
   improvements:

1. error handling

   when you crawl multiple pages, chances are, you are going to encounter
   some dysfunctional or nonexistent pages. you will want to make sure you
   handle errors such as connection errors or servers that never respond
   appropriately.

2. more detailed finish conditions

   often times, you only need to crawl n results, and any further results
   are unnecessary. you will want the option to terminate your crawler
   based on the number of items you have acquired.

3. multiprocessing

   this is more optional than the above two. however, when the number of
   urls to crawl is large, and the extraction process is long,
   multiprocessing can be necessary to obtain the results you want in a
   reasonable amount of time.
   i hope you this post has made you see how simple crawling actually is,
   and how implementing your own crawler is relatively simple.
   thank you for reading this post, and happy crawling!

share this:

     * [15]click to share on twitter (opens in new window)
     * [16]click to share on facebook (opens in new window)
     *

like this:

   like loading...

related

   author [17]keitakuritaposted on [18]december 28, 2017march 2,
   2018categories [19]python, [20]software engineering

post navigation

   [21]next next post: creating custom magic commands in jupyter

top posts & pages

     * [22]an in-depth tutorial to allennlp (from basics to elmo and bert)
     * [23]weight id172 and layer id172 explained
       (id172 in deep learning part 2)
     * [24]lightgbm and xgboost explained
     * [25]paper dissected: "attention is all you need" explained
     * [26]a practical introduction to nmf (nonnegative matrix
       factorization)

subscribe to blog via email

   find anything useful? ;)
   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

categories

     * [27]id161 (2)
     * [28]deep learning (22)
     * [29]fromscratch (1)
     * [30]jupyter (2)
     * [31]kaggle (1)
     * [32]machine learning (13)
     * [33]nlp (11)
     * [34]paper (10)
     * [35]python (1)
     * [36]skills (1)
     * [37]software (1)
     * [38]software engineering (2)
     * [39]uncategorized (3)

archives

     * [40]april 2019
     * [41]february 2019
     * [42]january 2019
     * [43]november 2018
     * [44]september 2018
     * [45]august 2018
     * [46]june 2018
     * [47]may 2018
     * [48]april 2018
     * [49]march 2018
     * [50]february 2018
     * [51]january 2018
     * [52]december 2017

     * [53]about this blog
     * [54]github

   [55]machine learning explained [56]proudly powered by wordpress

   iframe: [57]likes-master

   %d bloggers like this:

references

   1. http://id113xplained.com/feed/
   2. http://id113xplained.com/comments/feed/
   3. http://id113xplained.com/2017/12/28/creating-custom-magic-commands-in-jupyter/
   4. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/how-to-write-a-web-crawler-in-python-with-examples/
   5. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2017/12/28/how-to-write-a-web-crawler-in-python-with-examples/&format=xml
   6. http://id113xplained.com/2017/12/28/how-to-write-a-web-crawler-in-python-with-examples/#content
   7. http://id113xplained.com/
   8. http://id113xplained.com/about-this-blog/
   9. https://github.com/keitakurita
  10. http://www.hoge.com/foo
  11. http://www.hoge.com/
  12. http://www.hoge.com/
  13. http://requests-docs-ja.readthedocs.io/en/latest/
  14. https://www.crummy.com/software/beautifulsoup/bs4/doc/)
  15. http://id113xplained.com/2017/12/28/how-to-write-a-web-crawler-in-python-with-examples/?share=twitter
  16. http://id113xplained.com/2017/12/28/how-to-write-a-web-crawler-in-python-with-examples/?share=facebook
  17. http://id113xplained.com/author/admin/
  18. http://id113xplained.com/2017/12/28/how-to-write-a-web-crawler-in-python-with-examples/
  19. http://id113xplained.com/category/python/
  20. http://id113xplained.com/category/software-engineering/
  21. http://id113xplained.com/2017/12/28/creating-custom-magic-commands-in-jupyter/
  22. http://id113xplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/
  23. http://id113xplained.com/2018/01/13/weight-id172-and-layer-id172-explained-id172-in-deep-learning-part-2/
  24. http://id113xplained.com/2018/01/05/lightgbm-and-xgboost-explained/
  25. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  26. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  27. http://id113xplained.com/category/computer-vision/
  28. http://id113xplained.com/category/machine-learning/deep-learning/
  29. http://id113xplained.com/category/fromscratch/
  30. http://id113xplained.com/category/jupyter/
  31. http://id113xplained.com/category/kaggle/
  32. http://id113xplained.com/category/machine-learning/
  33. http://id113xplained.com/category/nlp/
  34. http://id113xplained.com/category/paper/
  35. http://id113xplained.com/category/python/
  36. http://id113xplained.com/category/skills/
  37. http://id113xplained.com/category/software/
  38. http://id113xplained.com/category/software-engineering/
  39. http://id113xplained.com/category/uncategorized/
  40. http://id113xplained.com/2019/04/
  41. http://id113xplained.com/2019/02/
  42. http://id113xplained.com/2019/01/
  43. http://id113xplained.com/2018/11/
  44. http://id113xplained.com/2018/09/
  45. http://id113xplained.com/2018/08/
  46. http://id113xplained.com/2018/06/
  47. http://id113xplained.com/2018/05/
  48. http://id113xplained.com/2018/04/
  49. http://id113xplained.com/2018/03/
  50. http://id113xplained.com/2018/02/
  51. http://id113xplained.com/2018/01/
  52. http://id113xplained.com/2017/12/
  53. http://id113xplained.com/about-this-blog/
  54. https://github.com/keitakurita
  55. http://id113xplained.com/
  56. https://wordpress.org/
  57. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
