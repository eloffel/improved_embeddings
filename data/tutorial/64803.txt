   #[1]publisher [2]listendata - atom [3]listendata - rss [4]listendata -
   atom

   [5]menu
     * [6]about
     * [7]index
     * [8]write for us

   [9]listendata

   [10]menu
     * [11]home
     * [12]sas
          + [13]tutorials
          + [14]sas certification
          + [15]interview questions
          + [16]resumes
     * [17]r
     * [18]python
     * [19]data science
     * [20]sql
     * [21]excel
          + [22]functions
          + [23]advanced
          + [24]dashboard / charts
          + [25]vba / macros
          + [26]statistical analysis
          + [27]mathematical analysis
          + [28]resumes
     * [29]spss
     * [30]resources
     * [31]infographics

   search...___________ go
   [32]home    [33]data science    [34]python    python for data science :
   learn in 3 days

python for data science : learn in 3 days

   [35]30 comments [36]data science, [37]python
   this tutorial helps you to learn data science with python with
   examples. python is an open source language and it is widely used as a
   high-level programming language for general-purpose programming. it has
   gained high popularity in data science world. as data science domain is
   rising these days, ibm recently predicted demand for data science
   professionals would rise by more than 25% by 2020. in the pypl
   popularity of programming language index, python scored second rank
   with a 14 percent share. in advanced analytics and predictive analytics
   market, it is ranked among top 3 programming languages for advanced
   analytics.

                      data science with python tutorial

   table of contents
    1. getting started with python
          + [38]python 2.7 vs. 3.6
          + [39]python for data science : introduction
          + [40]how to install python?
          + [41]spyder shortcut keys
          + [42]basic programs in python
          + [43]comparison, logical and assignment operators
    2. data structures and conditional statements
          + [44]python data structures
          + [45]python conditional statements
    3. python libraries
          + [46]list of popular packages (comparison with r)
          + [47]popular python commands
          + [48]how to import a package
    4. data manipulation using pandas
          + [49]pandas data structures - series and dataframe
          + [50]important pandas functions (vs. r functions)
          + [51]examples - data analysis with pandas
    5. data science with python
          + [52]id28
          + [53]decision tree
          + [54]id79
          + [55]grid search - hyper parameter tuning
          + [56]cross validation
          + [57]preprocessing steps

   python 2.7 vs 3.6
   google yields thousands of articles on this topic. some bloggers
   opposed and some in favor of 2.7. if you filter your search criteria
   and look for only recent articles (late 2016 onwards), you would see
   majority of bloggers are in favor of python 3.6. see the following
   reasons to support python 3.6.
   1. the official end date for the python 2.7 is year 2020. afterward
   there would be no support from community. it does not make any sense to
   learn 2.7 if you learn it today.
   2. python 3.6 supports 95% of top 360 python packages and almost 100%
   of top packages for data science.
   what's new in python 3.6
   it is cleaner and faster. it is a language for the future. it fixed
   major issues with versions of python 2 series. python 3 was first
   released in year 2008. it has been 9 years releasing robust versions of
   python 3 series.
   key takeaway

     you should go for python 3.6. in terms of learning python, there are
     no major differences in python 2.7 and 3.6. it is not too difficult
     to move from python 3 to python 2 with a few adjustments. your focus
     should go on learning python as a language.

python for data science : introduction

   python is widely used and very popular for a variety of software
   engineering tasks such as website development, cloud-architecture,
   back-end etc. it is equally popular in data science world. in advanced
   analytics world, there has been several debates on r vs. python. there
   are some areas such as number of libraries for statistical analysis,
   where r wins over python but python is catching up very fast. with
   popularity of big data and data science, python has become first
   programming language of data scientists.
   there are several reasons to learn python. some of them are as follows
   -
    1. python runs well in automating various steps of a predictive
       model.
    2. python has awesome robust libraries for machine learning, natural
       language processing, deep learning, big data and artificial
       intelligence.
    3. python wins over r when it comes to deploying machine learning
       models in production.
    4. it can be easily integrated with big data frameworks such as spark
       and hadoop.
    5. python has a great online community support.

   do you know these sites are developed in python?
    1. youtube
    2. instagram
    3. reddit
    4. dropbox
    5. disqus

   how to install python
   there are two ways to download and install python
    1. [58]download anaconda. it comes with python software along with
       preinstalled popular libraries.
    2. download [59]python from its official website. you have to manually
       install libraries.

recommended : go for first option and download anaconda. it saves a lot of
time in learning and coding python

   coding environments
   anaconda comes with two popular ide :
    1. jupyter (ipython) notebook
    2. spyder

   spyder. it is like rstudio for python. it gives an environment wherein
   writing python code is user-friendly. if you are a sas user, you can
   think of it as sas enterprise guide / sas studio. it comes with a
   syntax editor where you can write programs. it has a console to check
   each and every line of code. under the 'variable explorer', you can
   access your created data files and function. i highly recommend spyder!

                             [60][spyder.png]
                     spyder - python coding environment

   jupyter (ipython) notebook
   jupyter is equivalent to markdown in r. it is useful when you need to
   present your work to others or when you need to create step by step
   project report as it can combine code, output, words, and graphics.
   spyder shortcut keys
   the following is a list of some useful spyder shortcut keys which makes
   you more productive.
    1. press f5 to run the entire script
    2. press f9 to run selection or line
    3. press ctrl + 1 to comment / uncomment
    4. go to front of function and then press ctrl + i to see
       documentation of the function
    5. run %reset -f to clean workspace
    6. ctrl + left click on object to see source code
    7. ctrl+enter executes the current cell.
    8. shift+enter executes the current cell and advances the cursor to
       the next cell

   list of arithmetic operators with examples
   arithmetic operators      operation            example
   +                    addition            10 + 2 = 12
                          subtraction         10     2 = 8
   *                    multiplication      10 * 2 = 20
   /                    division            10 / 2 = 5.0
   %                    modulus (remainder) 10 % 3 = 1
   **                   power               10 ** 2 = 100
   //                   floor               17 // 3 = 5
   (x + (d-1)) // d     ceiling             (17 +(3-1)) // 3 = 6
   basic programs
   example 1

     #basics
     x = 10
     y = 3
     print("10 divided by 3 is", x/y)
     print("remainder after 10 divided by 3 is", x%y)

   result :
   10 divided by 3 is 3.33
   remainder after 10 divided by 3 is 1
   example 2

     x = 100
     x > 80 and x <=95
     x > 35 or x < 60

x > 80 and x <=95
out[45]: false
x > 35 or x < 60
out[46]: true



   comparison & logical operators description example
   > greater than 5 > 3 returns true
   < less than 5 < 3 returns false
   >= greater than or equal to 5 >= 3 returns true
   <= less than or equal to 5 <= 3 return false
   == equal to 5 == 3 returns false
   != not equal to 5 != 3 returns true
   and check both the conditions x > 18 and x <=35
   or if atleast one condition hold true x > 35 or x < 60
   not opposite of condition not(x>7)
   assignment operators
   it is used to assign a value to the declared variable. for e.g. x += 25
   means x = x +25.

     x = 100
     y = 10
     x += y
     print(x)

print(x)
110

   in this case, x+=y implies x=x+y which is x = 100 + 10.

     similarly, you can use x-=y, x*=y and x /=y

   python data structure
   in every programming language, it is important to understand the data
   structures. following are some data structures used in python.
   1. list
   it is a sequence of multiple values. it allows us to store different
   types of data such as integer, float, string etc. see the examples of
   list below. first one is an integer list containing only integer.
   second one is string list containing only string values. third one is
   mixed list containing integer, string and float values.
    1. x = [1, 2, 3, 4, 5]
    2. y = [   a   ,    o   ,    g   ,    m   ]
    3. z = [   a   , 4, 5.1,    m   ]

   get list item
   we can extract list item using indexes. index starts from 0 and end
   with (number of elements-1).

     x = [1, 2, 3, 4, 5]
     x[0]
     x[1]
     x[4]
     x[-1]
     x[-2]

x[0]
out[68]: 1

x[1]
out[69]: 2

x[4]
out[70]: 5

x[-1]
out[71]: 5

x[-2]
out[72]: 4

   x[0] picks first element from list. negative sign tells python to
   search list item from right to left. x[-1] selects the last element
   from list.
   you can select multiple elements from a list using the following method

     x[:3] returns [1, 2, 3]

   2. tuple
   a tuple is similar to a list in the sense that it is a sequence of
   elements. the difference between list and tuple are as follows -
    1. a tuple cannot be changed once constructed whereas list can be
       modified.
    2. a tuple is created by placing comma-separated values inside
       parentheses ( ). whereas, list is created inside square brackets [
       ]

   examples

     k = (1,2,3)
     state = ('delhi','maharashtra','karnataka')

   perform for loop on tuple

     for i in state:
         print(i)

delhi
maharashtra
karnataka


   [61]detailed tutorial : python data structures
   functions
   like print(), you can create your own custom function. it is also
   called user-defined functions. it helps you in automating the
   repetitive task and calling reusable code in easier way.
   rules to define a function
    1. function starts with def keyword followed by function name and ( )
    2. function body starts with a colon (:) and is indented
    3. the keyword return ends a function  and give value of previous
       expression.

     def sum_fun(a, b):
         result = a + b
         return result

     z = sum_fun(10, 15)

   result : z = 25
   suppose you want python to assume 0 as default value if no value is
   specified for parameter b.

     def sum_fun(a, b=0):
         result = a + b
         return result
     z = sum_fun(10)

   in the above function, b is set to be 0 if no value is provided for
   parameter b. it does not mean no other value than 0 can be set here. it
   can also be used as z = sum_fun(10, 15)
   conditional statements (if else)
   conditional statements are commonly used in coding. it is if else
   statements. it can be read like : " if a condition holds true, then
   execute something. else execute something else"
   note : the if and else statements ends with a colon :
   example

     k = 27
     if k%5 == 0:
       print('multiple of 5')
     else:
       print('not a multiple of 5')

   result : not a multiple of 5
   popular python packages for data analysis & visualization
   some of the leading packages in python along with equivalent libraries
   in r are as follows-
    1. pandas. for data manipulation and data wrangling. a collections of
       functions to understand and explore data. it is counterpart of
       dplyr and reshape2 packages in r.
    2. numpy. for numerical computing. it's a package for efficient array
       computations. it allows us to do some operations on an entire
       column or table in one line. it is roughly approximate to rcpp
       package in r which eliminates the limitation of slow speed in r.
       [62]numpy tutorial
    3. scipy.  for mathematical and scientific functions such
       as integration, interpolation, signal processing, id202,
       statistics, etc. it is built on numpy.
    4. scikit-learn. a collection of machine learning algorithms. it is
       built on numpy and scipy. it can perform all the techniques that
       can be done in r using glm, knn, randomforest, rpart, e1071
       packages.
    5. matplotlib. for data visualization. it's a leading package for
       graphics in python. it is equivalent to ggplot2 package in r.
    6. statsmodels. for statistical and predictive modeling. it includes
       various functions to explore data and generate descriptive and
       predictive analytics. it allows users to run descriptive
       statistics, methods to impute missing values, statistical tests and
       take table output to html format.
    7. pandasql.  it allows sql users to write sql queries in python. it
       is very helpful for people who loves writing sql queries to
       manipulate data. it is equivalent to sqldf package in r.

   maximum of the above packages are already preinstalled in spyder.

   comparison of python and r packages by data mining task
   task python package r package
   ide rodeo / spyder rstudio
   data manipulation pandas dplyr and reshape2
   machine learning scikit-learn glm, knn, randomforest, rpart, e1071
   data visualization ggplot + seaborn + bokeh ggplot2
   character functions built-in functions stringr
   reproducibility jupyter knitr
   sql queries pandasql sqldf
   working with dates datetime lubridate
   web scraping beautifulsoup rvest
   popular python commands
   the commands below would help you to install and update new and
   existing packages. let's say, you want to install / uninstall pandas
   package.
   install package
   !pip install pandas
   uninstall package
   !pip uninstall pandas
   show information about installed package
   !pip show pandas
   list of installed packages
   !pip list
   upgrade a package
   !pip install --upgrade pandas

   how to import a package
   there are multiple ways to import a package in python. it is important
   to understand the difference between these styles.
   1. import pandas as pd
   it imports the package pandas under the alias pd. a function
   dataframe in package pandas is then submitted with pd.dataframe.
   2. import pandas
   it imports the package without using alias but here the function
   dataframe is submitted with full package name pandas.dataframe
   3. from pandas import *
   it imports the whole package and the function dataframe is executed
   simply by typing dataframe. it sometimes creates confusion when same
   function name exists in more than one package.
   pandas data structures : series and dataframe
   in pandas package, there are two data structures - series and
   dataframe. these structures are explained below in detail -
    1. series is a one-dimensional array. you can access individual
       elements of a series using position. it's similar to vector in r.

   in the example below, we are generating 5 random values.

     import pandas as pd
     import numpy as np
     s1 = pd.series(np.random.randn(5))
     s1

0   -2.412015
1   -0.451752
2    1.174207
3    0.766348
4   -0.361815
dtype: float64

   extract first and second value
   you can get a particular element of a series using index value. see the
   examples below -
   s1[0]
-2.412015

   s1[1]
-0.451752

   s1[:3]
0   -2.412015
1   -0.451752
2    1.174207

   2. dataframe
   it is equivalent to data.frame in r. it is a 2-dimensional data
   structure that can store data of different data types such as
   characters, integers, floating point values, factors. those who are
   well-conversant with ms excel, they can think of data frame as excel
   spreadsheet.
   comparison of data type in python and pandas
   the following table shows how python and pandas package stores data.
                 data type                 pandas   standard python
   for character variable                object     string
   for categorical variable              category   -
   for numeric variable without decimals int64      int
   numeric characters with decimals      float64    float
   for date time variables               datetime64 -
   important pandas functions
   the table below shows comparison of pandas functions with r functions
   for various data wrangling and manipulation tasks. it would help you to
   memorize pandas functions. it's a very handy information for
   programmers who are new to python. it includes solutions for most of
   the frequently used data exploration tasks.
   functions r python (pandas package)
   installing a package install.packages('name') !pip install name
   loading a package library(name) import name as other_name
   checking working directory getwd() import os
   os.getcwd()
   setting working directory setwd() os.chdir()
   list files in a directory dir() os.listdir()
   remove an object rm('name') del object
   select variables select(df, x1, x2) df[['x1', 'x2']]
   drop variables select(df, -(x1:x2)) df.drop(['x1', 'x2'], axis = 1)
   filter data filter(df, x1 >= 100) df.query('x1 >= 100')
   structure of a dataframe str(df) df.info()
   summarize dataframe summary(df) df.describe()
   get row names of dataframe "df" rownames(df) df.index
   get column names colnames(df) df.columns
   view top n rows head(df,n) df.head(n)
   view bottom n rows tail(df,n) df.tail(n)
   get dimension of data frame dim(df) df.shape
   get number of rows nrow(df) df.shape[0]
   get number of columns ncol(df) df.shape[1]
   length of data frame length(df) len(df)
   get random 3 rows from dataframe sample_n(df, 3) df.sample(n=3)
   get random 10% rows sample_frac(df, 0.1) df.sample(frac=0.1)
   check missing values is.na(df$x) pd.isnull(df.x)
   sorting arrange(df, x1, x2) df.sort_values(['x1', 'x2'])
   rename variables rename(df, newvar = x1) df.rename(columns={'x1':
   'newvar'})
   data manipulation with pandas - examples
   1. import required packages
   you can import required packages using import statement. in the syntax
   below, we are asking python to import numpy and pandas package. the
   'as' is used to alias package name.

     import numpy as np
     import pandas as pd

   2. build dataframe
   we can build dataframe using dataframe() function of pandas package.

     mydata = {'productcode': ['aa', 'aa', 'aa', 'bb', 'bb', 'bb'],
             'sales': [1010, 1025.2, 1404.2, 1251.7, 1160, 1604.8],
             'cost' : [1020, 1625.2, 1204, 1003.7, 1020, 1124]}
     df = pd.dataframe(mydata)

    in this dataframe, we have three variables - productcode, sales, cost.

                             [63][df_python.png]
                               sample dataframe

   to import data from csv file
   you can use read_csv() function from pandas package to get data into
   python from csv file.

     mydata= pd.read_csv("c:\\users\\deepanshu\\documents\\file1.csv")

   make sure you use double backslash when specifying path of csv file.
   alternatively, you can use forward slash to mention file path inside
   read_csv() function.
   detailed tutorial : [64]import data in python
   3. to see number of rows and columns
   you can run the command below to find out number of rows and columns.

     df.shape

    result : (6, 3). it means 6 rows and 3 columns.
   4. to view first 3 rows
   the df.head(n) function can be used to check out first some n rows.

     df.head(3)

     cost productcode   sales
0  1020.0          aa  1010.0
1  1625.2          aa  1025.2
2  1204.0          aa  1404.2

   5. select or drop variables
   to keep a single variable, you can write in any of the following three
   methods -

     df.productcode
     df["productcode"]
     df.loc[: , "productcode"]

   to select variable by column position, you can use df.iloc function. in
   the example below, we are selecting second column. column index starts
   from 0. hence, 1 refers to second column.

     df.iloc[: , 1]

   we can keep multiple variables by specifying desired variables inside [
   ]. also, we can make use of df.loc() function.

     df[["productcode", "cost"]]
     df.loc[ : , ["productcode", "cost"]]

   drop variable
   we can remove variables by using df.drop() function. see the example
   below -

     df2 = df.drop(['sales'], axis = 1)

   6. to summarize data frame
   to summarize or explore data, you can submit the command below.

     df.describe()

              cost       sales
count     6.000000     6.00000
mean   1166.150000  1242.65000
std     237.926793   230.46669
min    1003.700000  1010.00000
25%    1020.000000  1058.90000
50%    1072.000000  1205.85000
75%    1184.000000  1366.07500
max    1625.200000  1604.80000

   to summarise all the character variables, you can use the following
   script.

     df.describe(include=['o'])

   similarly, you can use df.describe(include=['float64']) to view summary
   of all the numeric variables with decimals.
   to select only a particular variable, you can write the following code
   -

     df.productcode.describe()
     or
     df["productcode"].describe()

count      6
unique     2
top       bb
freq       3
name: productcode, dtype: object

   7. to calculate summary statistics
   we can manually find out summary statistics such as count, mean, median
   by using commands below

     df.sales.mean()
     df.sales.median()
     df.sales.count()
     df.sales.min()
     df.sales.max()

   8. filter data
   suppose you are asked to apply condition - productcode is equal to "aa"
   and sales greater than or equal to 1250.

     df1 = df[(df.productcode == "aa") & (df.sales >= 1250)]

   it can also be written like :

     df1 = df.query('(productcode == "aa") & (sales >= 1250)')

   in the second query, we do not need to specify dataframe along with
   variable name.
   9. sort data
   in the code below, we are arrange data in ascending order by sales.

     df.sort_values(['sales'])

   10.  group by : summary by grouping variable
   like sql group by, you want to summarize continuous variable by
   classification variable. in this case, we are calculating average sale
   and cost by product code.

     df.groupby(df.productcode).mean()

                    cost        sales
productcode
aa           1283.066667  1146.466667
bb           1049.233333  1338.833333

   instead of summarising for multiple variable, you can run it for a
   single variable i.e. sales. submit the following script.

     df["sales"].groupby(df.productcode).mean()

   11. define categorical variable
   let's create a classification variable - id which contains only 3
   unique values - 1/2/3.

     df0 = pd.dataframe({'id': [1, 1, 2, 3, 1, 2, 2]})

   let's define as a categorical variable.
   we can use astype() function to make id as a categorical variable.

     df0.id = df0["id"].astype('category')

   summarize this classification variable to check descriptive statistics.

     df0.describe()

       id
count    7
unique   3
top      2
freq     3

   frequency distribution
   you can calculate frequency distribution of a categorical variable. it
   is one of the method to explore a categorical variable.

     df['productcode'].value_counts()

bb    3
aa    3

   12. generate histogram
   histogram is one of the method to check distribution of a continuous
   variable. in the figure shown below, there are two values for variable
   'sales' in range 1000-1100. in the remaining intervals, there is only a
   single value. in this case, there are only 5 values. if you have a
   large dataset, you can plot histogram to identify outliers in a
   continuous variable.

     df['sales'].hist()

                         [65][histogram_python.png]
                                  histogram

   13. boxplot
   boxplot is a method to visualize continuous or numeric variable. it
   shows minimum, q1, q2, q3, iqr, maximum value in a single graph.

     df.boxplot(column='sales')

                              [66][boxplot.png]
                                   boxplot

   detailed tutorial : [67]data analysis with pandas tutorial
   data science using python - examples
   in this section, we cover how to perform data mining and machine
   learning algorithms with python. sklearn is the most frequently used
   library for running data mining and machine learning algorithms. we
   will also cover statsmodels library for regression techniques.
   statsmodels library generates formattable output which can be used
   further in project report and presentation.
   1. install the required libraries
   import the following libraries before reading or exploring data

     #import required libraries
     import pandas as pd
     import statsmodels.api as sm
     import numpy as np

   2. download and import data into python
   with the use of python library, we can easily get data from web into
   python.

     # read data from web
     df = pd.read_csv("https://stats.idre.ucla.edu/stat/data/binary.csv")

variables type description
gre continuous graduate record exam score
gpa continuous grade point average
rank categorical prestige of the undergraduate institution
admit binary admission in graduate school

   the binary variable admit is a target variable.
   3. explore data
   let's explore data. we'll answer the following questions -
    1. how many rows and columns in the data file?
    2. what are the distribution of variables?
    3. check if any outlier(s)
    4. if outlier(s), treat them
    5. check if any missing value(s)
    6. impute missing values (if any)

     # see no. of rows and columns
     df.shape

   result : 400 rows and 4 columns
   in the code below, we rename the variable rank to 'position' as rank is
   already a function in python.

     # rename rank column
     df = df.rename(columns={'rank': 'position'})

   summarize and plot all the columns.

     # summarize
     df.describe()
     # plot all of the columns
     df.hist()

   categorical variable analysis
   it is important to check the frequency distribution of categorical
   variable. it helps to answer the question whether data is skewed.

     # summarize
     df.position.value_counts(ascending=true)

1     61
4     67
3    121
2    151

   generating crosstab
   by looking at cross tabulation report, we can check whether we have
   enough number of events against each unique values of categorical
   variable.

     pd.crosstab(df['admit'], df['position'])

position   1   2   3   4
admit
0         28  97  93  55
1         33  54  28  12

   number of missing values
   we can write a simple loop to figure out the number of blank values in
   all variables in a dataset.

     for i in list(df.columns) :
         k = sum(pd.isnull(df[i]))
         print(i, k)

   in this case, there are no missing values in the dataset.
   4. id28 model
   id28 is a special type of regression where target
   variable is categorical in nature and independent variables be discrete
   or continuous. in this post, we will demonstrate only binary logistic
   regression which takes only binary values in target variable. unlike
   id75, id28 model returns id203 of
   target variable.it assumes binomial distribution of dependent variable.
   in other words, it belongs to binomial family.
   in python, we can write r-style model formula y ~ x1 + x2 + x3 using
   patsy and statsmodels libraries. in the formula, we need to define
   variable 'position' as a categorical variable by mentioning it inside
   capital c(). you can also define reference category using reference=
   option.

     #reference category
     from patsy import dmatrices, treatment
     y, x = dmatrices('admit ~ gre + gpa + c(position,
     treatment(reference=4))', df, return_type = 'dataframe')

   it returns two datasets - x and y. the dataset 'y' contains variable
   admit which is a target variable. the other dataset 'x' contains
   intercept (constant value), dummy variables for treatment, gre and gpa.
   since 4 is set as a reference category, it will be 0 against all the
   three dummy variables. see sample below -
p  p_1 p_2 p_3
3  0 0 1
3  0 0 1
1  1 0 0
4  0 0 0
4  0 0 0
2  0 1 0

   split data into two parts
   80% of data goes to training dataset which is used for building model
   and 20% goes to test dataset which would be used for validating the
   model.

     from sklearn.model_selection import train_test_split
     x_train, x_test, y_train, y_test = train_test_split(x, y,
     test_size=0.2, random_state=0)

   build id28 model
   by default, the regression without formula style does not include
   intercept. to include it, we already have added intercept in x_train
   which would be used as a predictor.

   #fit logit model

   logit = sm.logit(y_train, x_train)

   result = logit.fit()

   #summary of id28 model

   result.summary()

   result.params

                          logit regression results
==============================================================================
dep. variable:                  admit   no. observations:                  320
model:                          logit   df residuals:                      315
method:                           id113   df model:                            4
date:                sat, 20 may 2017   pseudo r-squ.:                 0.03399
time:                        19:57:24   log-likelihood:                -193.49
converged:                       true   ll-null:                       -200.30
                                        llr p-value:                  0.008627
================================================================================
=======
                      coef    std err          z       p|z|      [95.0% conf. in
t.]
--------------------------------------------------------------------------------
-------
c(position)[t.1]     1.4933      0.440      3.392      0.001         0.630     2
.356
c(position)[t.2]     0.6771      0.373      1.813      0.070        -0.055     1
.409
c(position)[t.3]     0.1071      0.410      0.261      0.794        -0.696     0
.910
gre                  0.0005      0.001      0.442      0.659        -0.002     0
.003
gpa                  0.4613      0.214     -2.152      0.031        -0.881    -0
.041
================================================================================
======

   confusion matrix and odd ratio
   odd ratio is exponential value of parameter estimates.

     #confusion matrix
     result.pred_table()
     #odd ratio
     np.exp(result.params)

   prediction on test data
   in this step, we take estimates of logit model which was built on
   training data and then later apply it into test data.

     #prediction on test data
     y_pred = result.predict(x_test)

   calculate area under curve (roc)

     # auc on test data
     false_positive_rate, true_positive_rate, thresholds =
     roc_curve(y_test, y_pred)
     auc(false_positive_rate, true_positive_rate)

   result : auc = 0.6763
   calculate accuracy score

     accuracy_score([ 1 if p > 0.5 else 0 for p in y_pred ], y_test)

   decision tree model
   id90 can have a target variable continuous or categorical.
   when it is continuous, it is called regression tree. and when it is
   categorical, it is called classification tree. it selects a variable at
   each step that best splits the set of values. there are several
   algorithms to find best split. some of them are gini, id178, c4.5,
   chi-square. there are several advantages of decision tree. it is simple
   to use and easy to understand. it requires a very few data preparation
   steps. it can handle mixed data - both categorical and continuous
   variables. in terms of speed, it is a very fast algorithm.
#drop intercept from predictors for tree algorithms
x_train = x_train.drop(['intercept'], axis = 1)
x_test = x_test.drop(['intercept'], axis = 1)

#decision tree
from sklearn.tree import decisiontreeclassifier
model_tree = decisiontreeclassifier(max_depth=7)

#fit the model:
model_tree.fit(x_train,y_train)

#make predictions on test set
predictions_tree = model_tree.predict_proba(x_test)

#auc
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predicti
ons_tree[:,1])
auc(false_positive_rate, true_positive_rate)

   result : auc = 0.664
   important note

     feature engineering plays an important role in building predictive
     models. in the above case, we have not performed variable selection.
     we can also select best parameters by using grid search fine tuning
     technique.

   id79 model
   decision tree has limitation of overfitting which implies it does not
   generalize pattern. it is very sensitive to a small change in training
   data. to overcome this problem, id79 comes into picture. it
   grows a large number of trees on randomised data. it selects random
   number of variables to grow each tree. it is more robust algorithm than
   decision tree. it is one of the most popular machine learning
   algorithm. it is commonly used in data science competitions. it is
   always ranked in top 5 algorithms. it has become a part of every data
   science toolkit.
#id79
from sklearn.ensemble import randomforestclassifier
model_rf = randomforestclassifier(n_estimators=100, max_depth=7)

#fit the model:
target = y_train['admit']
model_rf.fit(x_train,target)

#make predictions on test set
predictions_rf = model_rf.predict_proba(x_test)

#auc
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predicti
ons_rf[:,1])
auc(false_positive_rate, true_positive_rate)

#variable importance
importances = pd.series(model_rf.feature_importances_, index=x_train.columns).so
rt_values(ascending=false)
print(importances)
importances.plot.bar()

   result : auc = 0.6974
   grid search - hyper parameters tuning
   the sklearn library makes hyper-parameters tuning very easy. it is a
   strategy to select the best parameters for an algorithm. in
   scikit-learn they are passed as arguments to the constructor of the
   estimator classes. for example, max_features in randomforest. alpha for
   lasso.
from sklearn.model_selection import gridsearchcv
rf = randomforestclassifier()
target = y_train['admit']

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_features': ['sqrt', 3, 4]
}

cv_rfc = gridsearchcv(estimator=rf , param_grid=param_grid, cv= 5, scoring='roc_
auc')
cv_rfc.fit(x_train,target)

#parameters with scores
cv_rfc.grid_scores_

#best parameters
cv_rfc.best_params_
cv_rfc.best_estimator_

#make predictions on test set
predictions_rf = cv_rfc.predict_proba(x_test)

#auc
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predicti
ons_rf[:,1])
auc(false_positive_rate, true_positive_rate)

   cross validation

     # cross validation
     from sklearn.linear_model import logisticregression
     from sklearn.model_selection import
     cross_val_predict,cross_val_score
     target = y['admit']
     prediction_logit = cross_val_predict(logisticregression(), x,
     target, cv=10, method='predict_proba')
     #auc
     cross_val_score(logisticregression(fit_intercept = false), x,
     target, cv=10, scoring='roc_auc')

   data mining : preprocessing steps
   1.  the machine learning package sklearn requires all categorical
   variables in numeric form. hence, we need to convert all
   character/categorical variables to be numeric. this can be accomplished
   using the following script. in sklearn,  there is already a function
   for this step.
from sklearn.preprocessing import labelencoder
def converttonumeric(df):
    cols = list(df.select_dtypes(include=['category','object']))
    le = labelencoder()
    for i in cols:
        try:
            df[i] = le.fit_transform(df[i])
        except:
            print('error in variable :'+i)
    return df

converttonumeric(df)

                              [68][encode.png]
                                  encoding

   2. create dummy variables
   suppose you want to convert categorical variables into dummy variables.
   it is different to the previous example as it creates dummy variables
   instead of convert it in numeric form.

     productcode_dummy = pd.get_dummies(df["productcode"])
     df2 = pd.concat([df, productcode_dummy], axis=1)

   the output looks like below -
   aa  bb
0   1   0
1   1   0
2   1   0
3   0   1
4   0   1
5   0   1

   create k-1 categories
   to avoid multi-collinearity, you can set one of the category as
   reference category and leave it while creating dummy variables. in the
   script below, we are leaving first category.

     productcode_dummy = pd.get_dummies(df["productcode"],
     prefix='pcode', drop_first=true)
     df2 = pd.concat([df, productcode_dummy], axis=1)

   3. impute missing values
   imputing missing values is an important step of predictive modeling. in
   many algorithms, if missing values are not filled, it removes complete
   row. if data contains a lot of missing values, it can lead to huge data
   loss. there are multiple ways to impute missing values. some of the
   common techniques - to replace missing value with mean/median/zero. it
   makes sense to replace missing value with 0 when 0 signifies
   meaningful. for example, whether customer holds a credit card product.
   fill missing values of a particular variable

     # fill missing values with 0
     df['var1'] = df['var1'].fillna(0)
     # fill missing values with mean
     df['var1'] = df['var1'].fillna(df['var1'].mean())

   apply imputation to the whole dataset

     from sklearn.preprocessing import imputer
     # set an imputer object
     mean_imputer = imputer(missing_values='nan', strategy='mean',
     axis=0)
     # train the imputor
     mean_imputer = mean_imputer.fit(df)
     # apply imputation
     df_new = mean_imputer.transform(df.values)

   4. outlier treatment
   there are many ways to handle or treat outliers (or extreme values).
   some of the methods are as follows -
    1. cap extreme values at 95th / 99th percentile depending on
       distribution
    2. apply log transformation of variables. see below the implementation
       of log transformation in python.

     import numpy as np
     df['var1'] = np.log(df['var1'])

   5. standardization
   in some algorithms, it is required to standardize variables before
   running the actual algorithm. standardization refers to the process of
   making mean of variable zero and unit variance (standard deviation).

     #load dataset
     dataset = load_boston()
     predictors = dataset.data
     target = dataset.target
     df = pd.dataframe(predictors, columns = dataset.feature_names)
     #apply standardization
     from sklearn.preprocessing import standardscaler
     k = standardscaler()
     df2 = k.fit_transform(df)

   next steps
   practice, practice and practice. download free public data sets from
   kaggle / ucla websites and try to play around with data and generate
   insights from it with pandas package and build statistical models using
   sklearn package. i hope you would find this tutorial helpful. i tried
   to cover all the important topics which beginner must know about
   python. once completion of this tutorial, you can flaunt you know how
   to program it in python and you can implement machine learning
   algorithms using sklearn package.
   [listendata-logo.png]

   love this post? spread the word

   get free email updates : enter your email add submit

   *please confirm your email address by clicking on the link sent to your
                                   email*

   related posts:

   30 responses to "python for data science : learn in 3 days"
    1. anonymous[69]23 may 2017 at 07:31
       hi, excelent tutorial!!! i'm mostly a user of r but want to learn
       python. the thing is i work a lot with spatial data: spatial
       relationships (spdep), interpolation (kriging with gstat or
       multilevel b-splines with mba) etc.; and then machine learning
       methods with the data that comes from spatial features.
       i understand that the ml cappabilities are already in pythoon but
       i'm worried about the spatial workflow, can you give me some
       insights on this?
       thanks,
       great blog!
       reply[70]delete
       replies
         1. [71]louis feoncy[72]27 april 2018 at 09:10
            the very best people are always very helpful to you personally
            for solving up your all kind of data source management
            problems and form this [73]you can try this out
            https://activewizards.com/ certainly get the best data
            scientist, which are professional in their work of information
            handling and they may easily solve your all type of data
            management problems in short time.
            [74]delete
            replies
                 reply
            reply
    2. [75]pablo moreno[76]23 may 2017 at 10:36
       thanks for developing this. for first time after few attempts, i
       can start working with python!
       reply[77]delete
       replies
         1. [78]deepanshu bhalla[79]24 may 2017 at 08:26
            glad you found it helpful. cheers!
            [80]delete
            replies
                 reply
         2. [81]kapil sharma[82]14 february 2018 at 01:01
            hi deepanshu. can i have your contact number please. i want to
            talk regarding the courses.
            [83]delete
            replies
                 reply
            reply
    3. [84]goussu mgoussu[85]29 may 2017 at 07:57
       thanks. some things come late in the tutorial (like the np loading)
       but it is a good overview.
       reply[86]delete
       replies
            reply
    4. [87]unknown[88]7 june 2017 at 15:12
       excelent! i appreciate the comparison between r and python
       commands! very useful!
       reply[89]delete
       replies
            reply
    5. [90]sayan putatunda[91]11 june 2017 at 01:53
       very well written article!
       reply[92]delete
       replies
            reply
    6. [93]young joon oh[94]12 june 2017 at 11:15
       hi.
       i am using pythin 3.6.
       y, x = dmatrices("admit ~ gre + gpa + c(position,
       treatment(reference=4))", df, return_type = 'dataframe')
       this code generate this error
       c, including its class classregistry, has been deprecated since
       sympy
       1.0. it will be last supported in sympy version 1.0. use direct
       imports from the defining module instead. see
       https://github.com/sympy/sympy/issues/9371 for more info.
       .
       .
       .
       typeerror: 'bool' object is not callable
       how can i handle this ?
       thank you
       reply[95]delete
       replies
            reply
    7. anonymous[96]13 june 2017 at 00:50
       very useful tutorial, lucidly presented
       reply[97]delete
       replies
            reply
    8. anonymous[98]13 june 2017 at 10:41
       blatant copy paste from
       http://www.datasciencecentral.com/profiles/blogs/learn-python-in-3-
       days-step-by-step-guide
       reply[99]delete
       replies
         1. [100]deepanshu bhalla[101]13 june 2017 at 10:44
            haha. did you see the author of the post on
            datasciencecentral? read the first line - guest blog by
            deepanshu bhalla.
            [102]delete
            replies
                 reply
         2. [103]hugo[104]16 june 2017 at 00:20
            well, on the bright side, the poster was looking out for you
            even if s/he didn't realize it. :-)
            [105]delete
            replies
                 reply
            reply
    9. [106]p@#r[107]4 august 2017 at 21:58
                                                     
       reply[108]delete
       replies
            reply
   10. [109]ghulam rasool[110]27 september 2017 at 01:15
       thanks for an amazing introduction to python.
       reply[111]delete
       replies
            reply
   11. anonymous[112]13 november 2017 at 03:26
       thank you for this interesting tutorial.
       reply[113]delete
       replies
            reply
   12. [114]gagan gupta[115]27 november 2017 at 12:54
       nicely written.. thanks
       reply[116]delete
       replies
            reply
   13. [117]peter myers[118]4 december 2017 at 12:59
       thank you for the tutorial. bookmarked this so i can learn to use
       what you find essential when using the pandas package.
       reply[119]delete
       replies
            reply
   14. [120]venu gaddam[121]21 december 2017 at 09:23
       excellent resources to get hands on quick with python
       reply[122]delete
       replies
            reply
   15. [123]alberta rose[124]28 may 2018 at 02:42
       this comment has been removed by a blog administrator.
       reply[125]delete
       replies
            reply
   16. [126]nicole kristen[127]22 june 2018 at 03:58
       hey very nice blog!!
       i enjoy reading through your article post, i wanted to write a
       little comment to support you and wish you a good continuation. all
       the best for all your blogging efforts. the way of explanation
       about the comparison between [128]r and python is nice.
       appreciate the recommendation! let me try it out.
       keep working ,great job!
       reply[129]delete
       replies
            reply
   17. [130]grace a. king[131]25 june 2018 at 07:55
       very useful tutorial, lucidly presented
       reply[132]delete
       replies
            reply
   18. [133]gerald[134]26 june 2018 at 10:41
       i didn   t know about that jupyter thing, thank you for information.
       spyder shortcut keys are quite useful too, but i think what most
       important in programming is to know the top. for example, the
       [135]top programming languages 2018 can help you to know all the
       possibilities and the most convenient keys. i use the website to
       know what works now and came with some new ideas. if you can be
       really fast you won   t succeed.
       reply[136]delete
       replies
            reply
   19. [137]ankit singh[138]18 august 2018 at 03:48
       hi while i am running
       import pandas as pd
       s1 = pd.series(np.random.randn(5))
       s1
       its gives out an error as "np is not defined".
       can you please rectify?
       reply[139]delete
       replies
         1. [140]deepanshu bhalla[141]18 august 2018 at 09:35
            you also need to submit "import numpy as np" before
            pd.series()
            [142]delete
            replies
                 reply
         2. [143]trishan jaiswal[144]18 august 2018 at 13:19
            hey there, you have to import numpy as well
            [145]delete
            replies
                 reply
            reply
   20. [146]nahi_malum[147]1 december 2018 at 04:24
       ne pulse ultra for beginners
       reply[148]delete
       replies
            reply
   21. [149]hkr supports[150]18 march 2019 at 23:49
       very useful tutorial, lucidly presented
       [151]best it technical support
       reply[152]delete
       replies
            reply
   22. [153]it canvass[154]19 march 2019 at 23:33
       hello,
       nice article    very useful
       thanks for sharing the information.
       [155]servicenow training online
       reply[156]delete
       replies
            reply
   23. [157]lifeline[158]20 march 2019 at 07:23
       sir..my doubt is that "do we need to worry about removal of
       variables based on multicollinearity or the sklearn will take care
       of it automatically"
       reply[159]delete
       replies
            reply

   add comment
   load more...

   [160]next     [161]    prev

   [162]newer post [163]older post [164]home

   subscribe to: [165]post comments (atom)

   [166]follow us on facebook
   join us with 5000+ subscribers
   subscribe to free updates
   enter your email..._ subscribe

   copyright 2018 [167]listendata

references

   visible links
   1. https://plus.google.com/+listendata
   2. https://www.listendata.com/feeds/posts/default
   3. https://www.listendata.com/feeds/posts/default?alt=rss
   4. https://www.listendata.com/feeds/2811903644401203682/comments/default
   5. https://www.listendata.com/2017/05/python-data-science.html
   6. https://www.listendata.com/p/about-listen-data.html
   7. https://www.listendata.com/p/site-map.html
   8. https://www.listendata.com/p/write-for-us.html
   9. https://www.listendata.com/
  10. https://www.listendata.com/2017/05/python-data-science.html
  11. https://www.listendata.com/
  12. https://www.listendata.com/p/sas-tutorials.html
  13. https://www.listendata.com/p/sas-tutorials.html
  14. https://www.listendata.com/search/label/sas certification
  15. https://www.listendata.com/2013/09/sas-interview-questions.html
  16. https://www.listendata.com/p/resume-templates.html
  17. https://www.listendata.com/p/r-programming-tutorials.html
  18. https://www.listendata.com/search/label/python
  19. https://www.listendata.com/p/statistics-tutorials.html
  20. https://www.listendata.com/p/proc-sql.html
  21. https://www.listendata.com/p/excel-tutorials.html
  22. https://www.listendata.com/p/excel-for-beginners.html
  23. https://www.listendata.com/p/advanced-excel-tutorials.html
  24. https://www.listendata.com/search/label/excel charts
  25. https://www.listendata.com/p/excel-vba-tutorials.html
  26. https://www.listendata.com/p/statistical-analysis-using-excel.html
  27. https://www.listendata.com/p/mathematics-with-excel.html
  28. https://www.listendata.com/2014/05/mis-developer-senior-reporting-analyst.html
  29. https://www.listendata.com/p/spss-tutorials.html
  30. https://www.listendata.com/search/label/analytics
  31. https://www.listendata.com/search/label/infographics
  32. https://www.listendata.com/
  33. https://www.listendata.com/search/label/data science?&max-results=8
  34. https://www.listendata.com/search/label/python?&max-results=8
  35. https://www.listendata.com/2017/05/python-data-science.html#comment-form
  36. https://www.listendata.com/search/label/data science
  37. https://www.listendata.com/search/label/python
  38. http://www.listendata.com/2017/05/python-data-science.html#comparison
  39. http://www.listendata.com/2017/05/python-data-science.html#python_datascience
  40. http://www.listendata.com/2017/05/python-data-science.html#install_python
  41. http://www.listendata.com/2017/05/python-data-science.html#spyder_shortcut
  42. http://www.listendata.com/2017/05/python-data-science.html#python_basic
  43. http://www.listendata.com/2017/05/python-data-science.html#operators
  44. http://www.listendata.com/2017/05/python-data-science.html#datastructure
  45. http://www.listendata.com/2017/05/python-data-science.html#ifelse_python
  46. http://www.listendata.com/2017/05/python-data-science.html#python_packages
  47. http://www.listendata.com/2017/05/python-data-science.html#python_commands
  48. http://www.listendata.com/2017/05/python-data-science.html#import_package
  49. http://www.listendata.com/2017/05/python-data-science.html#pandas_datastructure
  50. http://www.listendata.com/2017/05/python-data-science.html#pandas_functions
  51. http://www.listendata.com/2017/05/python-data-science.html#pandas_examples
  52. http://www.listendata.com/2017/05/python-data-science.html#logistic_regression
  53. http://www.listendata.com/2017/05/python-data-science.html#decision_tree
  54. http://www.listendata.com/2017/05/python-data-science.html#random_forest
  55. http://www.listendata.com/2017/05/python-data-science.html#hyperparameter
  56. http://www.listendata.com/2017/05/python-data-science.html#cross_validation
  57. http://www.listendata.com/2017/05/python-data-science.html#preprocessing_steps
  58. https://www.continuum.io/downloads
  59. https://www.python.org/downloads/
  60. https://3.bp.blogspot.com/--7ugod3th2s/wsht5ten9ci/aaaaaaaagqs/qagri0kvnvctvrkdbbfj_xknky5kjfdlaclcb/s1600/spyder.png
  61. http://www.listendata.com/2017/06/python-data-structures.html
  62. http://www.listendata.com/2017/12/numpy-tutorial.html
  63. https://2.bp.blogspot.com/-pxz3hvzj3tg/wr2kilbtpai/aaaaaaaagpy/3l7pay8rhpq5ewgxelddagdwqvzccugsqclcb/s1600/df_python.png
  64. http://www.listendata.com/2017/02/import-data-in-python.html
  65. https://3.bp.blogspot.com/-ioygqfx8hwo/wr3gj--xc0i/aaaaaaaagpo/tqlc0mdvzqwbrc1xhticgid-1c5pgouxaclcb/s1600/histogram_python.png
  66. https://1.bp.blogspot.com/-hfndd5ho9ak/wr3kb-0f2ti/aaaaaaaagps/mpnfwzbf0raaavy2buvkbajep4rg8zlnaclcb/s1600/boxplot.png
  67. http://www.listendata.com/2017/12/python-pandas-tutorial.html
  68. https://4.bp.blogspot.com/-il5164xijag/wsbaxicw9mi/aaaaaaaagqq/ulw0s40iesogid166pdgupiwrpuhbqtm6cgclcb/s1600/encode.png
  69. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1495549903985#c1416041353514064341
  70. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=1416041353514064341
  71. https://www.blogger.com/profile/16452824907046530716
  72. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1524845443664#c4290438809231192220
  73. https://activewizards.com/
  74. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=4290438809231192220
  75. https://www.blogger.com/profile/17027164851086207979
  76. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1495561005443#c135092820871304633
  77. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=135092820871304633
  78. https://www.blogger.com/profile/09802839558125192674
  79. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1495639598596#c2731053409747561631
  80. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=2731053409747561631
  81. https://www.blogger.com/profile/15614763716983065890
  82. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1518598895677#c2144046505536028154
  83. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=2144046505536028154
  84. https://www.blogger.com/profile/00229320646105398804
  85. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1496069835457#c5876710129749039570
  86. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=5876710129749039570
  87. https://www.blogger.com/profile/17154113144668639057
  88. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1496873543989#c2718064954310023235
  89. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=2718064954310023235
  90. https://www.blogger.com/profile/02051644185426430663
  91. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1497171193948#c7320738253783011162
  92. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=7320738253783011162
  93. https://www.blogger.com/profile/12456338375976299549
  94. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1497291345661#c2170961671375998114
  95. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=2170961671375998114
  96. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1497340244859#c8716874327027911696
  97. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=8716874327027911696
  98. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1497375683475#c7391674703159035330
  99. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=7391674703159035330
 100. https://www.blogger.com/profile/09802839558125192674
 101. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1497375891824#c2051646864304005778
 102. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=2051646864304005778
 103. https://www.blogger.com/profile/09385582189890337553
 104. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1497597648126#c1423278623572639465
 105. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=1423278623572639465
 106. https://www.blogger.com/profile/01015721718687112670
 107. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1501909093786#c4904298876236018171
 108. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=4904298876236018171
 109. https://www.blogger.com/profile/02042705538530175003
 110. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1506500101085#c5489223836874257146
 111. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=5489223836874257146
 112. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1510572396409#c3278683870304433325
 113. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=3278683870304433325
 114. https://www.blogger.com/profile/14006666632445199642
 115. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1511816054331#c1742978393399433053
 116. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=1742978393399433053
 117. http://www.courageousdata.com/
 118. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1512421195182#c3786741896500618518
 119. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=3786741896500618518
 120. https://www.blogger.com/profile/06995036905838658831
 121. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1513877027228#c6834813628755934154
 122. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=6834813628755934154
 123. https://www.blogger.com/profile/09147270509551447919
 124. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1527500557363#c5055853297972519562
 125. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=5055853297972519562
 126. https://www.blogger.com/profile/14350573733361300097
 127. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1529665138260#c419917276170342953
 128. https://mindmajix.com/python-vs-sas-vs-r/
 129. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=419917276170342953
 130. https://www.blogger.com/profile/15266636111837841901
 131. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1529938536696#c1225395422914179167
 132. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=1225395422914179167
 133. https://www.blogger.com/profile/05012779954349378731
 134. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1530034908943#c5862991586238702510
 135. https://litslink.com/top-programming-languages-used-by-coders-in-2018
 136. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=5862991586238702510
 137. https://www.blogger.com/profile/02229864074863255316
 138. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1534589311246#c2629525111926762111
 139. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=2629525111926762111
 140. https://www.blogger.com/profile/09802839558125192674
 141. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1534610158132#c6578552710655117431
 142. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=6578552710655117431
 143. https://www.blogger.com/profile/00432502176387329655
 144. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1534623585025#c8434464184031699395
 145. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=8434464184031699395
 146. https://www.blogger.com/profile/13388862155266808244
 147. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1543667067656#c3416489896391303399
 148. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=3416489896391303399
 149. https://www.blogger.com/profile/03079988379785753987
 150. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1552978198135#c5030148082399845207
 151. https://hkrsupports.com/
 152. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=5030148082399845207
 153. https://www.blogger.com/profile/14350125401382785870
 154. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1553063606688#c8524754966025027572
 155. http://itcanvass.com/
 156. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=8524754966025027572
 157. https://www.blogger.com/profile/01406801845730094670
 158. https://www.listendata.com/2017/05/python-data-science.html?showcomment=1553091839583#c586762338971608056
 159. https://www.blogger.com/delete-comment.g?blogid=7958828565254404797&postid=586762338971608056
 160. https://www.listendata.com/2017/04/number-of-observations-in-sas-data.html
 161. https://www.listendata.com/2017/05/feature-selection-boruta-package.html
 162. https://www.listendata.com/2017/05/feature-selection-boruta-package.html
 163. https://www.listendata.com/2017/04/number-of-observations-in-sas-data.html
 164. https://www.listendata.com/
 165. https://www.listendata.com/feeds/2811903644401203682/comments/default
 166. https://www.facebook.com/listendata/
 167. https://www.listendata.com/

   hidden links:
 169. https://2.bp.blogspot.com/-yzgskcb-puo/wpaaxuacl7i/aaaaaaaagve/f5kjhbq2ndgnxiviiyhwpod6hqgiygy-wclcbgas/s1600/data%2bscience%2bpython.png
 170. https://www.blogger.com/null
 171. https://www.blogger.com/null
 172. https://www.blogger.com/null
 173. https://www.blogger.com/null
 174. https://www.blogger.com/null
 175. https://www.blogger.com/null
 176. https://www.blogger.com/null
 177. http://www.listendata.com/2017/06/python-data-structures.html
 178. https://www.blogger.com/null
 179. https://www.blogger.com/null
 180. https://www.blogger.com/null
 181. https://www.blogger.com/null
 182. https://www.blogger.com/null
 183. https://www.blogger.com/null
 184. https://www.blogger.com/null
 185. https://www.blogger.com/null
 186. https://www.blogger.com/null
 187. https://www.blogger.com/null
 188. https://www.blogger.com/null
 189. https://www.blogger.com/null
 190. https://www.blogger.com/null
 191. https://www.blogger.com/comment-iframe.g?blogid=7958828565254404797&postid=2811903644401203682
 192. https://www.blogger.com/rearrange?blogid=7958828565254404797&widgettype=html&widgetid=html1&action=editwidget&sectionid=sidebar
 193. https://www.blogger.com/rearrange?blogid=7958828565254404797&widgettype=html&widgetid=html5&action=editwidget&sectionid=sidebar
