   #[1]machine learning explained    feed [2]machine learning explained   
   comments feed [3]learning rate tuning in deep learning: a practical
   guide [4]a comprehensive introduction to torchtext (practical torchtext
   part 1) [5]alternate [6]alternate

   [7]skip to content

   [8]machine learning explained

   deep learning, python, data wrangling and other machine learning
   related topics explained for practitioners
   (button) menu

     * [9]about this blog
     * [10]github

an intuitive introduction to the hessian for deep learning practitioners

   the hessian is one of those words that strikes fear into people who
   aren   t immersed in optimization.  encountering it in papers can
   immediately drive away the unconfident reader.

   in my opinion, most articles concerning the hessian are far too dry and
   don   t give the reader an intuitive understanding of what the hessian
   means and what effects it has. this is a shame, since the hessian is
   one of the most useful tools for analyzing and understanding the
   optimization of neural networks, and there is a huge body of literature
   that uses it to derive many impressive results (including the effects
   of momentum and id172).

   this post attempts to provide a gentle and intuitive introduction to
   the hessian and its connections to deep learning. i will try to explain
   things in a geometric sense whenever possible and explain any math that
   is necessary along the way. at the end of this post, hopefully, you
   won   t be afraid of that dreaded h^{-1} symbol anymore     

1. basic intuition behind the hessian

   before going into the hessian, we need to have a clear picture of what
   a gradient is.

   here   s the formal definition: the gradient is the rate of change of
   some function (in deep learning, this is generally the id168)
   in various directions.

   intuitively, the derivative of the loss is basically how much the loss
   changes if you change the input by a very small amount. in
   one-dimensional space, the gradient is simply the slope of the function
   at that point.

   the above diagram has only one variable (the x-axis), but many
   functions have multiple inputs. geometrically, each input can be
   considered a different direction in high-dimensional space. you don   t
   need to imagine 12 dimensions to visualize this; just imagine a
   function with two inputs. for such functions, there is a different rate
   of change for each direction.
   a two-input function plotted in a three-dimensional space. the arrows
   at the bottom represent the gradient.

   the gradient is simply a collection of the derivative of the function
   for each direction.

   each element of the gradient is simply the slope of the function in
   each direction x_1, x_2, ... x_n . simple, right?

   next, we need to understand second-order derivatives. the second-order
   derivative is simply the derivative of the derivative. in other words,
   it is the rate of change of the slope. this will make more intuitive
   sense when you look at the following diagram for a one-dimensional
   function:

   the red arrows represent the derivative/slope at each point. in the
   left diagram, the slope changes a lot, while it is more stable in the
   right diagram. as you can see, the rate of change of the slope
   corresponds to how    curved    each id168 is. the sharper the
   curve, the more rapidly the slope changes.

   like we said earlier, in high-dimensional space, there is a different
   rate of change = slope for each direction. in addition to this, there
   is a different rate of change of the slope in each direction as well,
   meaning that the number of second-order derivatives is d^2 , where d is
   the dimensionality of the input. this is why second-order derivatives
   are expressed by the hessian matrix (or hessian), which is defined by

          {\mathbf h}={\begin{bmatrix}{\dfrac {\partial ^{2}f}{\partial
          x_{1}^{2}}}&{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial
          x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial
          x_{1}\,\partial x_{n}}}\\[2.2ex]{\dfrac {\partial
          ^{2}f}{\partial x_{2}\,\partial x_{1}}}&{\dfrac {\partial
          ^{2}f}{\partial x_{2}^{2}}}&\cdots &{\dfrac {\partial
          ^{2}f}{\partial x_{2}\,\partial x_{n}}}\\[2.2ex]\vdots &\vdots
          &\ddots &\vdots \\[2.2ex]{\dfrac {\partial ^{2}f}{\partial
          x_{n}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial
          x_{n}\,\partial x_{2}}}&\cdots &{\dfrac {\partial
          ^{2}f}{\partial x_{n}^{2}}}\end{bmatrix}}.

   each row represents the change of the gradient in a certain direction.
   alternatively, you could consider each column to be the gradient of one
   element of the gradient (take a moment to chew on that). the hessian is
   often expressed as h in papers, so whenever you see the symbol h ,
   chances are, the paper is using some analysis related to the hessian.

   though high-dimensional spaces often defy our intuitions, one intuition
   that doesn   t change in the one-dimensional and high-dimensional case is
   this: the second-order derivatives represent the curvature of the
   function. the difference is that the curvature will differ according to
   the direction, which presents a whole new set of problems.

   this is where using one dimension to intuitively understand the hessian
   meets its limit. to understand how the hessian is related to
   optimization of deep neural networks, we need a geometric
   understanding, which requires one more dimension (don   t worry, we won   t
   need any more      ).


2. connecting geometric intuition to analysis: eigenvectors and eigenvalues
of the hessian

   neural networks are far too complex to analyze, so we will build our
   intuition using the following, simple problem: minimizing a quadratic
   form. quadratic forms are nothing to be afraid of; a two-dimensional
   quadratic form is simply any function that can be expressed like:

   \frac{1}{2}a_{11}x_1^2 + a_{12}x_1x_2 +\frac{1}{2}a_{22}x_2^2 + b_1x_1
   + b_2x_2 + c

   when we write this in matrix form, we get the following equation

   \frac{1}{2}x^tax - b^tx + c

   where a is symmetric (its i -th row and i -th column are the same)
   [11]^1. note that this can be extended easily to arbitrarily large
   dimensions, and we are just focusing on the two-dimensional case for
   clarity.

   for this function, the gradient is ax - b and the hessian is a .  if
   you know matrix calculus, this is easy to derive. otherwise, you could
   try deriving the derivative for each element in x.  the optimal
   solution is where the gradient ax - b becomes 0. therefore, it is x =
   a^{-1}b. [12]^2

   the quadratic form is key to understanding the hessian and appears all
   over the place, so is worth looking into in depth. le   s try building an
   image of the hessian in two-dimensional space. the key question is:
   what does a quadratic form look like? as it turns out, this largely
   depends on the hessian. depending on the hessian, here are some
   possible shapes:


   as you can see, the function curves in different ways according to the
   hessian. since the hessian represents the curvature, this makes perfect
   sense. the next question then, is how exactly the curvature is
   determined by the hessian.

   this is where we turn to     arguably     the most important part of this
   blog post: the relationship between the shape of the quadratic form and
   the hessian   s eigenvalues. the eigenvalues and eigenvectors of the
   hessian are going to be the bridge between various equations and
   analysis surrounding the hessian and our intuition.

   in case you   re a bit rusty on id202, the eigenvectors of
   matrix m are vectors that do not change direction when multiplied with
   m , and the eigenvalues represent the change in length of the
   eigenvector when multiplied with m . in other words

   mv_i = \lambda_iv_i

   where v_i is an eigenvector and \lambda_i is an eigenvalue. each
   eigenvector has a single eigenvalue as a pair. eigenvectors and
   eigenvalues are very important tools for analyzing matrices, and often
   tell us a lot about the nature of the matrix.

   in the case of the hessian, the eigenvectors and eigenvalues have the
   following important properties:
    1. each eigenvector represents a direction where the curvature is
       independent of the other directions
    2. the curvature in the direction of the eigenvector is determined by
       the eigenvalue. if the eigenvalue is larger, there is a larger
       curvature, and if it positive, the curvature will be positive, and
       vice-versa.

   let   s dissect these properties one by one.

   remember the definition of the hessian? (i   ll repost it here so you
   don   t have to scroll up).

   {\mathbf h}={\begin{bmatrix}{\dfrac {\partial ^{2}f}{\partial
   x_{1}^{2}}}&{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial
   x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial
   x_{n}}}\\[2.2ex]{\dfrac {\partial ^{2}f}{\partial x_{2}\,\partial
   x_{1}}}&{\dfrac {\partial ^{2}f}{\partial x_{2}^{2}}}&\cdots &{\dfrac
   {\partial ^{2}f}{\partial x_{2}\,\partial x_{n}}}\\[2.2ex]\vdots
   &\vdots &\ddots &\vdots \\[2.2ex]{\dfrac {\partial ^{2}f}{\partial
   x_{n}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial
   x_{n}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial
   x_{n}^{2}}}\end{bmatrix}}.

   let   s look at the first element in the second row,
   \frac{\partial^2f}{\partial x_2 \partial x_2} . this is the rate of
   change in direction x_2 of the gradient in direction x_1 . this is
   already confusing and hard to imagine. wouldn   t it be great if the
   gradient only changed in the direction it pointed in?

   this is where the eigenvectors come to our rescue. for the
   eigenvectors, the gradient only changes in the direction it points in.
   in other words, all the non-diagonal elements of the hessian are 0 for
   the eigenvectors. this becomes even clearer when we plot the contours
   of the id168:

   a contour is a set of points where the id168 is equal. in our
   problem, the contours are nice ellipses. here, the eigenvectors are the
   axes of the ellipses.

   next, we turn to the properties of the eigenvalues. the eigenvalues
   represent the curvature in the direction of each eigenvector. in the
   contours above, the length of the axis is determined by the eigenvalue
   in its direction. remember, larger eigenvalue = larger curvature =
   larger rate of change, so the axes in the direction of large
   eigenvalues are actually shorter.

   returning to the 3-d plot of the various id168s, we can now
   deduce the eigenvalues simply by looking at the shape. id168
   (a) is curving upwards in two directions, so has two positive
   eigenvalues, whereas the eigenvalues are both negative for (b). (d) has
   one negative and one positive eigenvalue. (c) is a bit special: one of
   its eigenvalues is 0. this is why the loss is flat across one axis.

   this intuition is at the core of understanding the hessian, so is worth
   going over several times until you get a clear image. once you   re
   ready, let   s turn to the most exciting part of this post: the relation
   between the hessian and optimization.


3. the relation between the hessian and optimization

   the hessian determines the curve of the id168, so probably has
   some relation to how id119 would proceed. the key intuition
   is the relation between the eigenvalue, the eigenvector, and the speed
   and direction of convergence when using id119. long story
   short, the larger the eigenvalue, the faster the convergence from the
   direction of its corresponding eigenvector.

   mathematically, this can be (casually) derived as follows:

   1. a id119 step with step size \alpha can be expressed as
   follows:

   x^{(t+1)} = x^t - \alpha (ax^{(t)} -b)

   2. if x^{(t)} = v_i (the i -th eigenvector),

   x^{(t+1)} = (1 - \alpha\lambda_i)^tx^0

   where x_0 is the initial vector.

   3. any vector x can be expressed as a weighted sum of eigenvectors.
   therefore, for each component,  the convergence rate is 1 -
   \alpha\lambda_i .

   intuitively, this is because the curve is steeper in directions with
   larger eigenvalues. imagine a ball rolling down the loss curve. going
   back to our one-dimensional example,

   you can see that the stronger the curve, the quicker the ball reaches
   the minima, where the gradient is zero. you could also imagine how fast
   the gradient is converging to zero. let   s take the gradient of the two
   id168s above and plot them.

   in the left diagram, the gradient changes quickly so crosses 0 quickly
   as well. in the right diagram, the gradient slowly changes, so does not
   reach 0 within the diagram.

   an astute reader will probably have realized, but there is no guarantee
   that the convergence rate  1 - \alpha\lambda_i is between 0 and 1. in
   fact, if we choose the wrong step size \alpha , the parameters could
   start diverging. this is similar to how rolling a ball too quickly down
   the loss curve would cause it to fly off into the distance.

   the reason why a wrong step size could cause problems is that if the
   gradient changes rapidly, the gradient a few steps away would probably
   be completely different from its current value. we don   t necessarily
   mind the gradient becoming slightly different. what concerns us is
   whether the sign of the gradient is the same. if we take a step that is
   too large, we could end up actually increasing the loss. the ideal step
   size for each eigenvector component is when x converges to 0 in one
   step. therefore, it is \alpha = \frac{1}{\lambda_i} .

   if all the eigenvalues are the same, choosing the step-size is trivial.
   the problem is when the eigenvalues are very different. in this case,
   we have to be careful not to allow any of the components to diverge:
   this means that the step-size is effectively bounded by the largest
   curvature = largest eigenvalue. on the other hand, the speed of
   convergence is determined by the smallest eigenvalue. this is why the
   ratio between the largest and smallest eigenvalues in the hessian is a
   very important value and has its own name: the    condition number   . the
   problem of the condition number being very large is called
      ill-conditioning   , and is a prevalent problem in many areas of
   optimization. let   s see its relation to deep learning.


4. the hessian and deep learning

   now, the id168s in neural networks are nowhere near as simple
   as the toy problem we analyzed. however, when we look at small, local
   areas of the id168, we can approximate the id168 as a
   quadratic form. this is done using a technique called taylor expansion,
   which looks like:

   there   s no need to be intimidated by this equation anymore; all it   s
   saying is that we are approximating a function using a quadratic form,
   which you should have an intuitive image of already.

   when we look at the optimization of neural networks from this
   perspective, a large portion of the problems in deep learning can be
   attributed to ill-conditioning: some parameters have huge curvature
   while some have smaller curvature. here is a brief overview of how some
   staple methods in deep learning can be considered as methods of
   mitigating this problem:

   normalizing the inputs

   it turns out that when the inputs are not normalized, the mean of the
   inputs often makes the largest eigenvalue of the hessian even larger.
   normalizing the inputs reduces the largest curvature, and makes the
   hessian    better conditioned   .

   incidentally, this is similar to how batch id172 makes training
   easier. i   ve covered this topic in [13]another blog post so if you   re
   interested, please take a look.

   momentum

   instead of changing the hessian itself, momentum is a way of reducing
   the step size in directions of larger curvature and increasing it in
   directions of smaller curvature. intuitively, when the parameters go
   back and forth, the curvature is probably high, so momentum
   automatically reduces the learning rate for that directions. [14]this
   fantastic blog post has a very in-depth discussion of this topic, so i
   would highly recommend it if you   re interested.


5. conclusion and further readings

   this post attempted to be an intuitive introduction to the hessian. the
   important things to remember are:
     * second-order derivatives represent the curvature
     * the eigenvalues of the hessian represents the curvature of the loss
       function is in the direction of the corresponding eigenvector.
       stronger curves mean a faster change in the gradient.
     * the eigenvalues also represent how fast the parameters converge
       from the direction of the corresponding eigenvector.
     * directions with large eigenvalues need smaller step-sizes because
       they converge much faster, and will diverge if the step-size is too
       large. directions with smaller eigenvalues need larger step-sizes
       because otherwise, they converge far too slowly.
     * the ratio between the largest and smallest eigenvalues is,
       therefore, very important, which is why it has its own name: the
       condition number
     * many of the techniques in deep learning are (implicitly) trying to
       tackle the problem of a large condition number

   there is a lot of great advanced content out there relating to using
   second-order methods to understand deep learning better. equipped with
   the intuition i   ve provided in this blog post, you should be able to
   read through them relatively easily. below, i will list a few resources
   that i found to be very informative and interesting:

   papers and blog posts

   [15]no more pesky learning rates

   [16]an explanation of why momentum really works from the perspective of
   second-order optimization

   [17]an explanation of the conjugate gradient method

   books

   [18]the deep learning book

   [19]neural networks: tricks of the trade (lecture notes in computer
   science)

share this:

     * [20]click to share on twitter (opens in new window)
     * [21]click to share on facebook (opens in new window)
     *

    1. in case you   re wondering, even if a is not symmetric, we can derive
       an equivalent quadratic form using a symmetric matrix. therefore,
       constraining a to be symmetric is not a problem
    2. this assumes that a has an inverse matrix. technically this does
       not always hold, but for simplicity, i   ve omitted cases where a has
       no inverse matrix.

like this:

   like loading...

related

   author [22]keitakuritaposted on [23]february 2, 2018march 2,
   2018categories [24]deep learning

post navigation

   [25]previous previous post: learning rate tuning in deep learning: a
   practical guide
   [26]next next post: a comprehensive introduction to torchtext
   (practical torchtext part 1)

top posts & pages

     * [27]an in-depth tutorial to allennlp (from basics to elmo and bert)
     * [28]weight id172 and layer id172 explained
       (id172 in deep learning part 2)
     * [29]paper dissected: "attention is all you need" explained
     * [30]lightgbm and xgboost explained
     * [31]a practical introduction to nmf (nonnegative matrix
       factorization)

subscribe to blog via email

   find anything useful? ;)
   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

categories

     * [32]id161 (2)
     * [33]deep learning (22)
     * [34]fromscratch (1)
     * [35]jupyter (2)
     * [36]kaggle (1)
     * [37]machine learning (13)
     * [38]nlp (11)
     * [39]paper (10)
     * [40]python (1)
     * [41]skills (1)
     * [42]software (1)
     * [43]software engineering (2)
     * [44]uncategorized (3)

archives

     * [45]april 2019
     * [46]february 2019
     * [47]january 2019
     * [48]november 2018
     * [49]september 2018
     * [50]august 2018
     * [51]june 2018
     * [52]may 2018
     * [53]april 2018
     * [54]march 2018
     * [55]february 2018
     * [56]january 2018
     * [57]december 2017

     * [58]about this blog
     * [59]github

   [60]machine learning explained [61]proudly powered by wordpress

   iframe: [62]likes-master

   %d bloggers like this:

references

   visible links
   1. http://id113xplained.com/feed/
   2. http://id113xplained.com/comments/feed/
   3. http://id113xplained.com/2018/01/29/learning-rate-tuning-in-deep-learning-a-practical-guide/
   4. http://id113xplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/
   5. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/
   6. http://id113xplained.com/wp-json/oembed/1.0/embed?url=http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/&format=xml
   7. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/#content
   8. http://id113xplained.com/
   9. http://id113xplained.com/about-this-blog/
  10. https://github.com/keitakurita
  11. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/#easy-footnote-bottom-1-250
  12. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/#easy-footnote-bottom-2-250
  13. http://id113xplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-id172-really-works-id172-in-deep-learning-part-1/
  14. https://distill.pub/2017/momentum/
  15. https://arxiv.org/pdf/1206.1106.pdf
  16. https://distill.pub/2017/momentum/
  17. https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf
  18. http://www.deeplearningbook.org/
  19. https://www.amazon.com/gp/product/364235288x/ref=as_li_tl?ie=utf8&camp=1789&creative=9325&creativeasin=364235288x&linkcode=as2&tag=keitakurita-20&linkid=c4906ad7bf947b9ade55ab6903ac59b6
  20. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/?share=twitter
  21. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/?share=facebook
  22. http://id113xplained.com/author/admin/
  23. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/
  24. http://id113xplained.com/category/machine-learning/deep-learning/
  25. http://id113xplained.com/2018/01/29/learning-rate-tuning-in-deep-learning-a-practical-guide/
  26. http://id113xplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/
  27. http://id113xplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/
  28. http://id113xplained.com/2018/01/13/weight-id172-and-layer-id172-explained-id172-in-deep-learning-part-2/
  29. http://id113xplained.com/2017/12/29/attention-is-all-you-need-explained/
  30. http://id113xplained.com/2018/01/05/lightgbm-and-xgboost-explained/
  31. http://id113xplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/
  32. http://id113xplained.com/category/computer-vision/
  33. http://id113xplained.com/category/machine-learning/deep-learning/
  34. http://id113xplained.com/category/fromscratch/
  35. http://id113xplained.com/category/jupyter/
  36. http://id113xplained.com/category/kaggle/
  37. http://id113xplained.com/category/machine-learning/
  38. http://id113xplained.com/category/nlp/
  39. http://id113xplained.com/category/paper/
  40. http://id113xplained.com/category/python/
  41. http://id113xplained.com/category/skills/
  42. http://id113xplained.com/category/software/
  43. http://id113xplained.com/category/software-engineering/
  44. http://id113xplained.com/category/uncategorized/
  45. http://id113xplained.com/2019/04/
  46. http://id113xplained.com/2019/02/
  47. http://id113xplained.com/2019/01/
  48. http://id113xplained.com/2018/11/
  49. http://id113xplained.com/2018/09/
  50. http://id113xplained.com/2018/08/
  51. http://id113xplained.com/2018/06/
  52. http://id113xplained.com/2018/05/
  53. http://id113xplained.com/2018/04/
  54. http://id113xplained.com/2018/03/
  55. http://id113xplained.com/2018/02/
  56. http://id113xplained.com/2018/01/
  57. http://id113xplained.com/2017/12/
  58. http://id113xplained.com/about-this-blog/
  59. https://github.com/keitakurita
  60. http://id113xplained.com/
  61. https://wordpress.org/
  62. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914

   hidden links:
  64. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/#easy-footnote-1-250
  65. http://id113xplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/#easy-footnote-2-250
