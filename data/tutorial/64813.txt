   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    machine
   learning workflows in python from scratch part 2: id116 id91
   comments feed [5]kdnuggets    news 17:n22, jun 7: 7 steps to mastering
   data preparation with python; why does deep learning not have a local
   minimum? [6]how hr managers use data science to manage talent for their
   companies

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]jun    [29]tutorials,
   overviews    machine learning workflows in python from scratch part 2:
   id116 id91 ( [30]17:n23 )

machine learning workflows in python from scratch part 2: id116 id91

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 234
   tags: [33]id91, [34]id116, [35]machine learning, [36]python,
   [37]workflow

   the second post in this series of tutorials for implementing machine
   learning workflows in python from scratch covers implementing the
   id116 id91 algorithm.
     __________________________________________________________________

   by [38]matthew mayo, kdnuggets.

   in the first part of this series, we started off rather slowly but
   deliberately. [39]the previous post laid out our goals, and started off
   with some basic building blocks for our machine learning workflows and
   pipelines we will eventually get to. if you have not yet read the first
   installment in this series, i suggest that you do so before moving on.

   this time around we pick up steam, and will be doing so with an
   implementation of the id116 id91 algorithm. we will discuss
   specific aspects of id116 as they come up while coding, but if you
   are interested in a superficial overview of what the algorithm is
   about, as well as how it relates to other id91 methods, you could
   [40]check this out.

                             ml workflows header
          the id116 id91 algorithm in python. from scratch.

   the only real prerequisites moving forward are the [41]dataset.py
   module we created in the first post, along with the original
   [42]iris.csv file, so make sure you have both of those handy.


the id116 id91 algorithm


   id116 is a simple, yet often effective, approach to id91. k
   points are randomly chosen as cluster centers, or centroids, and all
   training instances are plotted and added to the closest cluster. after
   all instances have been added to clusters, the centroids, representing
   the mean of the instances of each cluster are re-calculated, with these
   re-calculated centroids becoming the new centers of their respective
   clusters.

   at this point, all cluster membership is reset, and all instances of
   the training set are re-plotted and -added to their closest, possibly
   re-centered, cluster. this iterative process continues until there is
   no change to the centroids or their membership, and the clusters are
   considered settled.

   as far as approaches to id91 go, id116 is about as simple as
   they come -- its conceptual simplicity is elegant, almost poetic. it's
   also a proven workhorse, lending to its staying power, often producing
   useful results.

   what were going to do, in a nutshell, is code up a simple id116
   implementation that will group similar things together and keep
   dissimilar things apart, at least theoretically. simple enough. keep in
   mind that "similar" here is reduced to meaning "relatively closely
   co-located in euclidean space," or something very non-philosophical
   like that.

   we will need a few functions here. thinking about the steps involved in
   the algorithm can help us solidify what those might be:
     * set initial centroids
     * measure distances between data instances and centroids
     * add data instances as members of closest centroid
     * re-calculate centroids
     * if necessary, re-measure, re-cluster, re-calculate

   that's the algorithm in a nutshell. but before we get there, a
   (temporary) step backward.


data preparation... again


   while writing this post, it eventually occurred to me that an important
   part of our data preparation workflow was missing. before we convert
   our pandas dataframe to a numpy ndarray (matrix), we will want to make
   sure our numeric values are actually numeric values, and not strings
   masquerading as numeric values. since we read our data from a csv file
   last time, even our numeric values were being stored as strings
   (apparent at the bottom of the previous post by the fact that the
   numbers are surrounded by single quotes -- e.g. '5.7').

   so, the best way to deal with this is to create another function and
   add it to our dataset.py file, which will convert strings to their
   numeric value representations (we already have a function for
   converting class names to numeric values, and keeping track of these
   changes). the function went through 3 specific iterations as i played
   with it, namely those which accepted: 1) a dataset and the name of a
   single attribute, the corresponding column of which all values should
   be converted from strings to floats; 2) a dataset and a list of
   attribute names...; and 3) a dataset and either a single attribute as a
   string, or a list of attributes as a *gasp* list.

   the final iteration (the third, more flexible, option) is the one shown
   below. let's add it to the dataset.py module from last time.

   ok, with that out of the way, we will be able to load a dataset, clean
   it, and create a (fully numeric) matrix representation, which can then
   be fed into our id116 id91 algorithm, once we have one.
   speaking of which...


initializing centroids


   the first thing our algorithm needs to do is to create a set of k
   initial centroids. there are a variety of ways to approach this, but we
   will start with the most basic: random initialization. we need a
   function that accepts a dataset and an integer k, and which will return
   an ndarray of that number of randomly-generated centroids.

   you may have already imagined how random initialization could go awry.
   as a quick example, think about 5 distinct, tightly clustered classes
   in 2 dimensional space, with a poorly initialized set of centroids, as
   shown below.

                             poor initialization
               obviously non-optimal centroid initialization.

   even without the mathematics to support the intuition, it's clear that
   these centroids are not optimally placed. however, one of id116'
   strong attributes is its ability to recover from such initialization,
   with successive rounds of id91 moving toward optimal placement by
   minimizing the mean distance between cluster instance members and
   cluster centroids.

   while this can happen remarkably quickly, poor initialization with
   sufficient amounts of high-dimensionality data can lead to a greater
   number of id91 iterations. the initial data space survey for
   random placement can also, itself, become lengthy. and so alternative
   methods for centroid initialization are available, some of which we may
   look at in the future.

   a quick note about testing our code: doing so as we go with heavily
   contrived scenarios seems more trouble than it's worth, so we will hold
   out until the end to see how we did in total.


measuring distances


   with a dataset in hand and a collection of initialized centroids, we
   will eventually have to perform a lot of measurement calculations. in
   fact, for each id91 iteration we will have to measure from each
   data point to each centroid, in order to know which cluster an instance
   belongs to (perhaps temporarily). so let's write a measurement function
   for euclidean space. we will use [43]scipy here for the heavy lifting;
   while coding a distance measurement is not very difficult, [44]scipy
   includes a function optimized for such calculations on vectors, which,
   conveniently, is exactly what we will be doing.

   let's wrap this functionality in our own function, in case we want to
   later change or experiment with how we calculate distance.

   with the ability to initialize centroids and to make our measurements,
   we can now write a function to control the logic of our id91
   algorithm, and perform the few additional required steps.


id91 instances


   before we look at the code, here is a simple overview of the process
   our algorithm will be following, taking into account the few functions
   from above, as well as below.

                               id116 diagram
                  our id116 id91 algorithm process.

   the code below is well-commented, but let's walk through a few of the
   main points.

   first, our function accepts a dataset as a numpy ndarray, as well as
   the number of clusters we want to use in the id91 process. it
   returns several things:
     * the resulting centroids after id91 is finished
     * the cluster assignments after id91
     * the number of iterations which were required by id91
       algorithm
     * the original centroids

   an ndarray to store instance cluster assignments and their errors are
   created, and then centroids are initialized, with a copy held to return
   later.

   a while loop then continues until there has been no change to cluster
   assignments, meaning that convergence has been reached -- note that no
   additional mechanism to limit the number of iterations exists at this
   point. instances to centroid distances are calculated, with the minimum
   distance tracked, which is used to determine cluster assignment. the
   closest centroid is then recorded, along with the actual distance
   between the 2 entities (the error), and a check is performed to see if
   the cluster assignment for the particular instances has changed.

   after this has been performed for each instance, the centroid locations
   are updated, simply by using the mean values of the member instances as
   centroid coordinates. the number of iterations are also recorded. a
   check as to whether convergence has been reached kicks control out of
   the while loop once appropriate, and the items outlined above are
   returned.

   i want to point out that some of this code, as well as additional
   inspiration, comes from the book "[45]machine learning in action"
   (mlia), by peter harrington. i bought this book when it was first
   released, and it has proven invaluable for reasons related to the
   criticism the book often receives, most of which focuses on either not
   enough theory and/or issues with code. in my humble opinion, however,
   these are both assets. if you are like me, in that you came to the book
   with theoretical understanding, and are willing and able to fight
   through code which may need tuning, or which is simply enough that you
   can provide your own changes, mlia can be a useful resource to anyone
   looking to make their first foray into coding machine learning
   algorithms.


testing our id116 id91 algorithm


   there are a few typical tasks which we are going to forego for this
   post and will revisit later, but i wanted to point them out here.

   first, when id91, especially with attributes of varying scales,
   it is generally a good idea to at least consider the idea of scaling or
   otherwise normalizing the data, to ensure that a single attribute (or
   collection of attributes) of a vastly greater scale then the others
   does not end up accounting for more than it should. if we have 3
   attributes, with the first 2 in the range [0, 1] and the third in the
   range [0, 100], it's easy to see that the third variable will dominate
   the measurements and subsequent cluster membership allocations.

   second, when id91 (as with much of machine learning), we can
   split a dataset into training and testing sets, allowing us to train a
   model on one subset of data, and then test the model on the other
   (separate) set of data. while this is not always part of the goal of a
   given id91 task, as we may simply want to build a id91
   model and not be concerned with using it to classify subsequent
   instances, it often can be. we will proceed with our test code below
   without taking either of these into consideration at this point, but
   may double back in a subsequent post.

   before moving ahead, make sure you have added the dataset-related
   function outlined further above to the existing [46]dataset.py module,
   and have created a [47]kmeans.py module to house all of the relevant
   functions.

   let's try our code:

number of iterations: 5

final centroids:
[[ 6.62244898  2.98367347  5.57346939  2.03265306  2.        ]
 [ 5.006       3.418       1.464       0.244       0.        ]
 [ 5.91568627  2.76470588  4.26470588  1.33333333  1.01960784]]

cluster membership and error of first 10 instances:
[[ 1.        0.021592]
 [ 1.        0.191992]
 [ 1.        0.169992]
 [ 1.        0.269192]
 [ 1.        0.039192]
 [ 1.        0.467592]
 [ 1.        0.172392]
 [ 1.        0.003592]
 [ 1.        0.641592]
 [ 1.        0.134392]]

original centroids:
[[ 4.38924071  3.94546253  5.49200482  0.40216215  1.95277771]
 [ 5.43873792  3.58653594  2.73064731  0.79820023  0.97661014]
 [ 4.62570586  2.46497863  3.14311939  2.4121321   0.43495676]]

   looks good! this particular test of our code resulted in the above, but
   you will find that subsequent iterations return different results -- at
   least, different numbers of iterations and sets of original centroids.


looking ahead


   though we have not yet evaluated our id91 results, we will stop
   here for now... but, on that note, i bet you can guess what is in store
   for the next post. next time we will focus on a few more
   id91-related activities. we have an algorithm which we can use to
   build models, but there needs to be some mechanisms for evaluating and
   visualizing their results. this is what we will get to next.

   thinking further ahead, i then plan to turn our attention to
   classification using the k-nearest neighbors algorithm, as well a
   number of classification-related tasks. once again, i hope you have
   found this helpful enough to check the next installment.


   related:
     * [48]machine learning workflows in python from scratch part 1: data
       preparation
     * [49]toward increased id116 id91 efficiency with the naive
       sharding centroid initialization method
     * [50]id116 & other id91 algorithms: a quick intro with
       python
     __________________________________________________________________

   [51][prv.gif] previous post
   [52]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [53]another 10 free must-read books for machine learning and data
       science
    2. [54]9 must-have skills you need to become a data scientist, updated
    3. [55]who is a typical data scientist in 2019?
    4. [56]the pareto principle for data scientists
    5. [57]what no one will tell you about data science job applications
    6. [58]19 inspiring women in ai, big data, data science, machine
       learning
    7. [59]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [60]id158s optimization using genetic algorithm
       with python
    2. [61]who is a typical data scientist in 2019?
    3. [62]8 reasons why you should get a microsoft azure certification
    4. [63]the pareto principle for data scientists
    5. [64]r vs python for data visualization
    6. [65]how to work in data science, ai, big data
    7. [66]the deep learning toolset     an overview

[67]latest news

     * [68]download your datax guide to ai in marketing
     * [69]kdnuggets offer: save 20% on strata in london
     * [70]training a champion: building deep neural nets for big ...
     * [71]building a recommender system
     * [72]predict age and gender using convolutional neural netwo...
     * [73]top tweets, mar 27     apr 02: here is a great ex...

more recent stories

     * [74]top tweets, mar 27     apr 02: here is a great explanat...
     * [75]odsc east is selling out; odsc india announced
     * [76]accelerate ai and data science career expo, may 4, 2019
     * [77]grow your data career at datasciencego, san diego, sep 27-29
     * [78]getting started with nlp using the pytorch framework
     * [79]how to diy your data science education
     * [80]top 8 data science use cases in gaming
     * [81]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [82]make better data-driven business decisions
     * [83]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [84]two predictive analytics world events in europe this fall
     * [85]7 qualities your big data visualization tools absolutely must
       ...
     * [86]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [87]which face is real?
     * [88]yeshiva university: program director / tenure track faculty
       me...
     * [89]top 10 coding mistakes made by data scientists
     * [90]uber   s case study at paw industry 4.0: machine learning ...
     * [91]xai     a data scientist   s mouthpiece
     * [92]what does gpt-2 think about the ai arms race?
     * [93]openclassrooms: data freelance online course creator
       [telecomm...

   [94]kdnuggets home    [95]news    [96]2017    [97]jun    [98]tutorials,
   overviews    machine learning workflows in python from scratch part 2:
   id116 id91 ( [99]17:n23 )
      2019 kdnuggets. [100]about kdnuggets.  [101]privacy policy.
   [102]terms of service

   [103]subscribe to kdnuggets news
   [104][tw_c48.png] [105]facebook [106]linkedin
   x
   [envelope.png] [107]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/06/machine-learning-workflows-python-scratch-part-2.html/feed
   5. https://www.kdnuggets.com/2017/n22.html
   6. https://www.kdnuggets.com/2017/06/hr-managers-data-science-manage-talent.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/06/index.html
  29. https://www.kdnuggets.com/2017/06/tutorials.html
  30. https://www.kdnuggets.com/2017/n23.html
  31. https://www.kdnuggets.com/2017/n22.html
  32. https://www.kdnuggets.com/2017/06/hr-managers-data-science-manage-talent.html
  33. https://www.kdnuggets.com/tag/id91
  34. https://www.kdnuggets.com/tag/id116
  35. https://www.kdnuggets.com/tag/machine-learning
  36. https://www.kdnuggets.com/tag/python
  37. https://www.kdnuggets.com/tag/workflow
  38. https://www.kdnuggets.com/author/matt-mayo
  39. https://www.kdnuggets.com/2017/05/machine-learning-workflows-python-scratch-part-1.html
  40. https://www.kdnuggets.com/2016/09/comparing-id91-techniques-concise-technical-overview.html
  41. https://gist.github.com/mmmayo13/9859a457760db10ec4842be3aa1a2334
  42. https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv
  43. https://www.scipy.org/
  44. https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.euclidean.html
  45. https://www.amazon.com/machine-learning-action-peter-harrington/dp/1617290181/
  46. https://gist.github.com/mmmayo13/935684dd226ef05f7d291e8cf5ed873a
  47. https://gist.github.com/mmmayo13/956937ec1fc695163b8e052b55c09208
  48. https://www.kdnuggets.com/2017/05/machine-learning-workflows-python-scratch-part-1.html
  49. https://www.kdnuggets.com/2017/03/naive-sharding-centroid-initialization-method.html
  50. https://www.kdnuggets.com/2017/03/id116-id91-algorithms-intro-python.html
  51. https://www.kdnuggets.com/2017/n22.html
  52. https://www.kdnuggets.com/2017/06/hr-managers-data-science-manage-talent.html
  53. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  54. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  55. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  56. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  57. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  58. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  59. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  60. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  61. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  62. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  63. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  64. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  65. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  66. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  67. https://www.kdnuggets.com/news/index.html
  68. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  69. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  70. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  71. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  72. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  73. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  74. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  75. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  76. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  77. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  78. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  79. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  80. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  81. https://www.kdnuggets.com/2019/n13.html
  82. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  83. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  84. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  85. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  86. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  87. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  88. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  89. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
  90. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
  91. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
  92. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
  93. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
  94. https://www.kdnuggets.com/
  95. https://www.kdnuggets.com/news/index.html
  96. https://www.kdnuggets.com/2017/index.html
  97. https://www.kdnuggets.com/2017/06/index.html
  98. https://www.kdnuggets.com/2017/06/tutorials.html
  99. https://www.kdnuggets.com/2017/n23.html
 100. https://www.kdnuggets.com/about/index.html
 101. https://www.kdnuggets.com/news/privacy-policy.html
 102. https://www.kdnuggets.com/terms-of-service.html
 103. https://www.kdnuggets.com/news/subscribe.html
 104. https://twitter.com/kdnuggets
 105. https://facebook.com/kdnuggets
 106. https://www.linkedin.com/groups/54257
 107. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 109. https://www.kdnuggets.com/
 110. https://www.kdnuggets.com/
