   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

data2vis: automatic generation of data visualizations using
sequence-to-sequence recurrent neural networks

   [16]go to the profile of victor dibia
   [17]victor dibia (button) blockedunblock (button) followfollowing
   apr 11, 2018
   [1*mipc69nbqmu-ibemscl-ia.jpeg]
   a collection of random examples generated by the model given datasets
   from the [18]rdataset collection..

   we formulate data visualization as a sequence to sequence translation
   problem.

   tldr; we train a model that can take in a dataset as input and
   generates a set of plausible visualization as output.

     we have an early paper draft describing the work on [19]arxiv,
     [20]short video, [21]source code and [22]demo. feedback, discussion
     ([23]@vykthur, [24]@serravis) and comments welcome!

   [1*foohqv-qkf6aeqf8jftpbg.gif]
   [25]demo web app. we provide a web interface where a user can paste
   data (or load a random dataset) and get set of generated visualizations
   (using id125).

   want to try training a similar model? we have posted source code used
   in the experiment.
   [26]victordibia/data2vis
   data2vis: automatic generation of data visualizations using sequence to
   sequence recurrent neural networks    github.com

   this work is done jointly with a [27]colleague (cagatay demiralp) and
   started from a conversation we had after a paper discussion meeting. we
   had read some papers where various forms of generative and sequence
   models were used to create a wide range of stuff         from generating
   images (gans), music, source code, image captions to generating
   questions and answers about images (vqa) etc. despite the quirks that
   can sometimes be associated with these models (one eyed cats, music
   that ultimately lacks that natural feel etc), they all demonstrate a
   promise of value when trained and deployed at scale.

   and so we were curious about the possibility applying advances in
   demonstrated within these deep learning models to the task of creating
   visualizations. this is valuable as visualization authoring can be a
   time consuming process (selecting what tool to use, which fields to
   visualize, which transforms to apply and which interactions to
   support). if we can get a model to do some of these things, we felt
   that could improve the visualization authoring process. the first
   challenge was related to finding ways to formulate the problem such
   that it becomes amenable to deep learning. how can we teach a model to
   understand and then generate a visualization? how do we curate our
   training dataset? do we assemble nice images of visualizations + data
   pairs and train on those?

   luckily, similar problems related to improving visualization authoring
   have been considered by researchers in the past. one line of
   interesting work is around efforts to create declarative languages that
   succinctly describe visualization and provide the    right    trade off
   between expressivity and speed in authoring visualizations. with these
   languages or grammars, you write some compact text specification and
   some engine does the hardwork of turning that text into actual
   visualizations. a good example of such a declarative language is
   [28]vega-lite created by the awesome researchers over at [29]university
   of washington interactive data lab. for our experiments, we represent
   visualizations using vega-lite grammar specifications (json).
   [1*kifwkmbqgkkgsr4-ka8y5w.png]
   a [30]vega-lite visualization (left), specified using the vega-lite
   grammar (right).

what we did

   if we then think of visualizations as text specifications, the problem
   is simplified to generating a bunch of these specifications, given some
   input data. and so our next task was to explore models that learn the
   space of a visualization specification format and perhaps can
      hallucinate    some new visualizations? we began our initial experiments
   following notes from [31]karpathy   s blog         we generated visualization
   specifications (data + spec), concatenated them and attempted to train
   an id56 that would generate something plausible.

   from this first experimentations, we learned quite a bit about how well
   character id56s (lstms) perform with respect to learning the structure
   of data contained within nested structures like a vega-lite
   specification. we found that while character based id56s work well for
   the problem (compared to word-based id56s), their limitations in
   modeling long sequences and memory requirement for long term sequences
   made them challenging to train (unintelligible tokens after a few
   characters, limited learning progress even after thousands of steps,
   millions of parameters without preprocessing). also, character id56s did
   not provide a concrete mapping between our input and output pairs for
   training. these challenges motivated us to think of other ways to
   formulate the problem    approaches that learn the structure of our
   output space, and generates new data given some input. essentially
   translating from source data specification to target visualization
   specification.

sequence to sequence models

   sequence to sequence models [1,2,3] are capable of doing just
   that         they take in some input (e.g. text in one language) and generate
   an output (e.g. same text in a different language). they have been
   applied with wide success to problems such as language translation,
   text summarization, image captioning. variations of sequence to
   sequence models that utilize encoder decoder architectures have also
   been explored for translating between formal [32]programming languages,
   [33]learning domain specific programs and general [34]program
   synthesis.
   [1*ykh0ufmbebpy-t4wycie3a.jpeg]
   we train a sequence to sequence model based on the work of [35]britz et
   al 2017. training data consists of a source token (a single row from a
   data set) and a target token (a valid vegalite visualization
   specification) pair.

data and training

   first we assembled a training dataset (source and target pairs) based
   on examples of valid vega-lite visualizations. a total of 215k pair
   samples based on 11 distinct datasets and 4300 vega-lite examples were
   used. please see the [36]paper for more details on how the data is
   sampled and pre-processed.

   we train a sequence to sequence model using the [37]architecture and
   sample [38]code provided by [39]britz et al 2017. the model is an
   encoder-decoder architecture with attention mechanism (please see the
   [40]britz paper for more details on the architecture). we also perform
   some simple id172 on the dataset to simplify training. we
   replace string and numeric field names using a short notation            str   
   and    num    in the source sequence (dataset). next, a similar backward
   transformation (post processing) is replicated in the target sequence
   to maintain consistency in field names. these transformations help
   scaffold the learning process by reducing the vocabulary size, and
   prevents the lstm from learning field names (learning field names in
   the training set is not useful to our generation problem). in turn we
   are able to reduce the overall source and target sequence length,
   reduce training time and reduce the number of hidden layers which the
   model needs to converge.

id125 decoding

   to explore a variety of generated visualizations, we use id125
   decoding algorithm. as opposed to outputting the most likely (highest
   id203) translation of an input sequence, id125 expands all
   possible next steps during generation and keeps the k most likely,
   where k is a user specified parameter known as the beam width. unlike
   conventional language translation systems where id125 is applied
   mainly to improve translation quality by maximizing conditional
   probabilities of generated sequences, we also explore id125 as a
   way to generate a diverse set of candidate visualizations by outputting
   all parallel beam results. with id125, we observe the model
   generates diverse plots, exploring combinations of chart types and the
   use of multiple variables.

early results

   [1*vklk9rtlsmudsreuyelu3g.jpeg]
   examples where the model has learned to generate univariate plots that
   summarize fields selected from the dataset.
   [1*wmc-ggbrygzhqzxil0j29g.jpeg]
   [1*3rufuivmkbo34h3ran2eoa.jpeg]
   examples of generated visualizations where the model has learned common
   selection patterns and leverages concepts such as responses (yes, no)
   and sex (male, female)

   to evaluate the model, we use the [41]rdataset repository (cleaned and
   converted to a valid json format) which was not included in our
   training. the range of valid univariate and multivariate visualizations
   produced suggests the model captures aspects of the visualization
   generation process. as training progresses, the model incrementally
   learns the vocabulary and syntax for valid vega-lite specifications,
   learning to use quotes, brackets, symbols and keywords. the model also
   appears to have learned to use the right type of variable
   specifications in the vega-lite grammar (e.g. it correctly assigns a
   string type for text fields and a quantitative for numeric fields).

   qualitative results also suggest the use of appropriate transformations
   (bins, aggregate) on appropriate fields (e.g. means are performed on
   numeric fields). the model also learns about common data selection
   patterns that occur within visualizations and their combination with
   other variables to create bivariate plots. for example, as experts
   create visualizations, it is common to group data by geography
   (country, state, sex), characteristics of individuals (citizenship
   status, marital status, sex) etc. early results suggests that our model
   begins to learn these patterns and apply them in its generation of
   visualizations. for example, it learns to subset data using common
   ordinal fields such as responses (yes/no), sex (male/female) etc and
   plots these values against other fields. finally, in all cases, the
   model generates a perfectly valid json file and valid vega-lite
   specification with some minor failure cases.
   [1*xngv0wbm1n3qim7bxl119w.jpeg]
   a collection of random examples generated (beam width k = 15) by the
   model given some test dataset. these examples demonstrate initial
   results on the model   s capabilities in generating plausible
   visualizations and its current limitations (cases where the model
   generates non-existent fields or uses transforms that result in
   invalid charts).

   as observed with many deep learning models, there are promising results
   and also quirks or failure cases (e.g. 5 legged dogs, 3 eyed cats etc).
   in our case, the model sometimes generates specifications using
   non-existent (phantom variables) or applies transformations that are
   invalid during runtime leading to empty plots. see the figure above
   (plots marked in orange). our intuition is that these challenges can be
   addressed by expanding our relatively small dataset.

why automate visualizations with deep learning?

   there a few reasons why we think automated generation of visualizations
   is interesting:

making visualization authoring easier.

   providing users who have little or no programming experience with the
   ability to rapidly create expressive data visualizations empowers them
   and brings data visualization into their personal workflow. for
   experts, models like data2vis also hold potential to    seed    the data
   visualization process, reducing the amount of time spent specifying
   visualization language syntax and supporting iteration over
   visualization possibilities. this motivation resonates with other
   research efforts that explore deep learning approaches for source code
   generation and translation ([42]ucberkeley, [43]microsoft research
   etc).

the promise of scale

   in theory, as we assemble more diverse and complex training examples
   (and a better architecture perhaps), datavis should learn more complex
   visualization strategies, increasing its utility. this holds promise to
   create a model that may one day achieve human-level (or superhuman
   level) expertise in data visualization much like we have observed in
   other domains such as image recognition, gaming, medical imaging etc.

going beyond    heuristics   

   existing approaches to visualization generation tend to be based on
   rules and heuristics that generate univariate summary plots or plots
   based on combinations of rules. in such situations, [44]jeff dean
   identifies opportunities to improve such systems using machine learning
   (ml) and encourages the    learn    and    meta learn    everything approach.
   he also highlights challenges with [45]heuristics         they don   t adapt to
   patterns of usage and may fail to take valuable context into
   consideration. we also agree that ml and ai approaches offer
   opportunity to outperform rule-based methods by learning from existing
   training examples.

what   s next

   this project is still very much work-in-progress - there are
   limitations and future work we hope to address. in addition to
   collecting more data and running experiments to improve data2vis, some
   future work includes:

extending data2vis to generate multiple plausible visualizations

   data2vis is currently implemented as a sequence-to-sequence translation
   model and outputs a single visualization spec for a given dataset. now,
   wont it be awesome to train a model that generates multiple valid
   visualizations for a given dataset? we think it would.

targeting additional grammars

   perhaps training models that can map input data to multiple different
   visualization specification languages (e.g. vega-lite, ggplot2, d3
   etc.). or translating visualizations from one language to others. this
   could help make visualizations more accessible by enabling
   visualization specification reuse across languages, platforms and
   systems.

natural language and visualization specification

   how about text2vis? - models that generate visualizations using natural
   language text in addition to input data. think a person exploring data
   in front of a large screen and asking for visualizations based on
   specific fields, transforms or interactions?

conclusion

   this article discusses some ideas and early results around automatic
   generation of visualizations using sequence to sequence models.
   hopefully, this work serves as a baseline for future work in automated
   generation of visualizations using deep learning approaches. feel free
   to read the [46]paper for more details, and share your thoughts too
   ([47]@vykthur, [48]@serravis)! we hope to share code and trained models
   sometime soon.

acknowledgement

   this work was enabled by the contributions of many individuals. thanks
   to the authors of the [49]vega-lite,[50]voyager library and for sharing
   example data [4] used for our experiments. many thanks to the authors
   of the [51]tensorflow [52]id195 model implementation and the
   [53]tensorflow library team         their work enabled us to learn about
   sequence models and rapidly prototype our experiments will little
   previous experience.

references

   [1] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. 2014. neural
   machine translation by jointly learning to align and translate. (sep
   2014).

   [2] ilya sutskever, oriol vinyals, and quoc v. le. 2014. sequence to
   sequence learning with neural networks

   [3] denny britz, anna goldie, thang luong, and quoc le. 2017. massive
   exploration of id4 architectures

   [4] jorge poco and jeffrey heer. 2017. reverse-engineering
   visualizations: recovering visual encodings from chart images. computer
   graphics forum (proc. eurovis) (2017).

   thanks to [54]  a  atay demiralp.
     * [55]machine learning
     * [56]generative model
     * [57]data visualization
     * [58]recurrent neural network
     * [59]towards data science

   (button)
   (button)
   (button) 422 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [60]go to the profile of victor dibia

[61]victor dibia

   hci researcher with interests in creating usable ai. ml researcher at
   cloudera fast forward labs, google dev expert for ml.

     (button) follow
   [62]towards data science

[63]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 422
     * (button)
     *
     *

   [64]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [65]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5da8e9d3e43e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/data2vis-automatic-generation-of-data-visualizations-using-sequence-to-sequence-recurrent-neural-5da8e9d3e43e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/data2vis-automatic-generation-of-data-visualizations-using-sequence-to-sequence-recurrent-neural-5da8e9d3e43e&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_zoowdl8zmtrt---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@victor.dibia?source=post_header_lockup
  17. https://towardsdatascience.com/@victor.dibia
  18. https://github.com/vincentarelbundock/rdatasets
  19. https://arxiv.org/abs/1804.03126
  20. https://vimeo.com/264192643
  21. https://github.com/victordibia/data2vis
  22. http://hci.stanford.edu/~cagatay/data2vis/
  23. https://twitter.com/vykthur
  24. https://twitter.com/serravis
  25. http://hci.stanford.edu/~cagatay/data2vis/
  26. https://github.com/victordibia/data2vis
  27. https://hci.stanford.edu/~cagatay/
  28. https://idl.cs.washington.edu/papers/vega-lite/
  29. https://idl.cs.washington.edu/
  30. https://vega.github.io/vega-lite/
  31. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  32. https://arxiv.org/abs/1802.03691
  33. https://www.microsoft.com/en-us/research/blog/deep-learning-program-synthesis/
  34. https://sunblaze-ucb.github.io/program-synthesis/index.html
  35. https://arxiv.org/abs/1703.03906
  36. https://arxiv.org/abs/1804.03126
  37. https://arxiv.org/abs/1703.03906
  38. https://github.com/google/id195
  39. https://arxiv.org/abs/1703.03906
  40. https://arxiv.org/abs/1703.03906
  41. https://github.com/vincentarelbundock/rdatasets
  42. https://sunblaze-ucb.github.io/program-synthesis/index.html
  43. https://www.microsoft.com/en-us/research/blog/deep-learning-program-synthesis/
  44. http://learningsys.org/nips17/assets/slides/dean-nips17.pdf
  45. http://learningsys.org/nips17/assets/slides/dean-nips17.pdf
  46. https://arxiv.org/abs/1804.03126
  47. https://twitter.com/vykthur
  48. https://twitter.com/serravis
  49. https://idl.cs.washington.edu/papers/vega-lite/
  50. https://idl.cs.washington.edu/papers/voyager/
  51. https://medium.com/@tensorflow
  52. https://github.com/google/id195
  53. https://medium.com/@tensorflow
  54. https://medium.com/@cagatay.demiralp?source=post_page
  55. https://towardsdatascience.com/tagged/machine-learning?source=post
  56. https://towardsdatascience.com/tagged/generative-model?source=post
  57. https://towardsdatascience.com/tagged/data-visualization?source=post
  58. https://towardsdatascience.com/tagged/recurrent-neural-network?source=post
  59. https://towardsdatascience.com/tagged/towards-data-science?source=post
  60. https://towardsdatascience.com/@victor.dibia?source=footer_card
  61. https://towardsdatascience.com/@victor.dibia
  62. https://towardsdatascience.com/?source=footer_card
  63. https://towardsdatascience.com/?source=footer_card
  64. https://towardsdatascience.com/
  65. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  67. https://github.com/victordibia/data2vis
  68. https://medium.com/p/5da8e9d3e43e/share/twitter
  69. https://medium.com/p/5da8e9d3e43e/share/facebook
  70. https://medium.com/p/5da8e9d3e43e/share/twitter
  71. https://medium.com/p/5da8e9d3e43e/share/facebook
