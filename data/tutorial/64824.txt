   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]intuition machine
     * [9]patterns
     * [10]methodology
     * [11]strategy
     * [12]deep learning playbook
     __________________________________________________________________

taxonomy of methods for deep meta learning

   go to the profile of carlos e. perez
   [13]carlos e. perez (button) blockedunblock (button) followfollowing
   mar 5, 2017
   [1*zzjj6kvn-d7heoyfba1qwa.jpeg]
   credit: [14]https://unsplash.com/search/search?photo=ice__bo2vws

   let   s talk about meta-learning because this is one confusing topic. i
   wrote a previous post about [15]deconstructing meta-learning which
   explored    learning to learn   . i realized thought that there is another
   kind of meta-learning that practitioners are more familiar with. this
   kind of meta-learning can be understood as algorithms the search and
   select different dl architectures. hyper-parameter optimization is an
   instance of this, however there are another more elaborate algorithms
   that follow the same prescription of searching for architectures.

   hyper-parameter optimization is a common technique used in machine
   learning. daniel saltiel has a short post on    [16]state of
   hyperparameter selection    that covers grid search, random search
   ([17]bengio et. al)and gaussian processes as a way to sample out
   different machine learning models. these are standard techniques and is
   exploited by facebook in their [18]fblearner platform:

     many machine learning algorithms have numerous hyperparameters that
     can be optimized. at facebook   s scale, a 1 percent improvement in
     accuracy for many models can have a meaningful impact on people   s
     experiences. so with flow, we built support for large-scale
     parameter sweeps and other automl features that leverage idle cycles
     to further improve these models.

   [1*9r3u-7exq5rlzp4itffwbw.png]
   hyper-parameter optimization generates networks selecting weight
   initialization and learning algorithm parameters.

   there are many hyperparameters that one can chose from in the regime of
   deep learning architectures. a recent paper,    [19]evolving deep neural
   networks    provides a comprehensive list of global parameters that are
   typically used in the conventional search approaches (i.e. learning
   rate) as well as more hyperparameters that involve more details about
   the architecture of the deep learning network.
   [1*ibxyuc9yndgf7gu4pzkwdq.png]

   in the research, what is explored is an algorithm, codeepneat, for
   optimizing deep learning architectures through evolution. the claim is
   that their evolution inspired approach is, five times to thirty times
   speedup over state-of-the-art bayesian optimization algorithms on a
   variety of deep-learning problems. the approach used is based on idea
   called the successivehalving algorithm. the algorithm uniformly
   allocates a budget to a set of hyperparameter configurations, it
   evaluates the performance of all configurations, then throws out the
   poorest performing half, and then repeat until one configurations
   remains.
   [1*_ni0oeih0_dhikmvkh83jg.png]
   codeepneat generates networks selecting weight initialization, learning
   hyperparameters and layers.

   two recent papers that were submitted to iclr 2017 explore the use of
   id23 to learn new kinds of deep learning
   architectures (   [20]designing neural network architectures using
   id23    and    [21]neural architecture search with
   id23   ).

   the first paper describes the use of reinforcement id24 to
   discover id98 architectures, you can find some of their generated id98s
   in caffe here:[22]https://bowenbaker.github.io/metaqnn/ . these are the
   different parameters that are sampled by the metaqnn algorithm:
   [1*5j71nwg9cgdncrswmv3enw.png]
   [1*qpszaoc4wajkwmh5kx-kfa.png]
   metaqnn uses rl to generate networks selecting layers and connections
   of a network.

   the second paper (neural architecture search) employs uses
   id23 (rl) to train a an architecture generator lstm
   to build a language that describes new dl architectures. the lstm is
   trained via a policy gradient method to maximize the generation of new
   architectures. the research explores this method to generate
   convolutional architectures (id98) as well as generating recurrent
   architectures (id56). for id98 generation the following parameters were
   used:
   [1*m8bs7xtzpvko8wjy5ixslw.png]

   and for the id56 generation the following was used:
   [1*p6nwdjyuf_3xjxs6s-54xg.png]

   the trained generator id56 is a two-layer lstm, this id56 generates an
   architecture that is trained for 50 epochs. the reward used for
   updating the generator id56 is the maximum validation accuracy of the
   last 5 epochs cubed (see paper). after the the id56 trains 12,800
   architectures (google has this luxury), then select the candidate
   architecture that achieves the best accuracy in validation. run the
   candidate architecture is then run through hyperparameter optimization
   (i.e. grid search) to find the best performing instance.
   [1*ekivkhesn5qe39pthxwjdw.png]
   neural architecture search         rl trains lstm to select layers and
   connections of a id98 or lstm.

   here is an example of lstm cells that were generated by this system:
   [1*vr0el8tsrd9d6ordphxyzq.png]

   clearly incomprehensible by most humans. mind blowing and
   state-of-the-art.

   an even more recent paper (   [23]large-scale evolution of image
   classifiers   ) also from google brain employs an evolutionary algorithm
   using mutation operators that are inspired by    rules of thumb    coming
   from various dl papers. the evolution algorithm uses repeated pairwise
   competitions of random individuals, and select from the pair the better
   performing individual (i..e. tournament selection). the set of mutation
   operators that the authors used are as follows:
     * alter-learning-rate .
     * identity (effectively means    keep training   ).
     * reset-weights (sampled as in he et al. (2015), for example).
     * insert-convolution (inserts a convolution at a random location in
       the    convolutional backbone   . the inserted convolution has 3    3
       filters, strides of 1 or 2 at random, number of channels same as
       input. may apply batch-id172 and relu activation or none at
       random).
     * remove-convolution.
     * alter-stride (only powers of 2 are allowed).
     * alter-number-of-channels (of random conv.).
     * filter-size (horizontal or vertical at random, on random
       convolution, odd values only).
     * insert-one-to-one (inserts a one-to-one/identity connection,
       analogous to insert-convolution mutation).
     * add-skip (identity between random layers).
     * remove-skip (removes random skip).

   this    neuro-evolution    approach has some interesting result
   conclusions. specifically that it is capable of    constructing large,
   accurate networks    and    the process described, once started, needs no
   participation from the experimenter.    the impression by some about this
   paper is that it could have only be executed by the folks at google
   with    computation at unprecedented levels   . clearly this is a brute
   force approach that can be refined in a neural network were trained to
   emit these rules rather than through random evolution. a very
   interesting discovery in this network is that some of the better
   performing networks simply stacked convolution networks on top of each
   other without non-linearities. this defies convention, however there
   may in fact be some justification to it! (note: [24]another paper with
   a similar genetic algorithm)

   in neural architecture search, an lstm was trained to generate
   architectures. we can think of meta-learning in a general sense as
   being the    machines that generate other machines   . [25]hyper networks
   also fall into the category of machines generating other machines.
   hyper networks are dl architectures that learn how to generate the
   weights of another dl architecture. the claimed utility of the approach
   relates to it being a kind of generalization of weight sharing. unlike
   the other methods mentioned above, both hyper networks and learning to
   learn, a neural network replaces the entire functionality rather than
   specifying hyper-parameters:
   [1*mdrpruqlvkibmit5x5seug.png]
   hyper network         learns a neural network that generated the weights of
   another network.
   [1*vj1ddxovxzvbaiugcil1fa.png]
   learning to learn         replaced sgd with a trained lstm

   note that id21 is similar to both of these in that another
   network provides the weight initialization. in addition, these are only
   for single instances and not generative. perhaps one could use a
   generative technique (i.e. vae, gans etc) to generate optimal dl
   architectures.

   in all the above approaches, the method employs different search
   mechanisms (i.e. grid, gaussian processes, evolution, id24,
   policy gradients) to discover (among the many generated architectures)
   better configurations. the key idea to emphasize is that
   hyper-parameters can be generalized into a form of a language. a domain
   specific language (dsl) with the above hyper-parameters as its
   vocabulary can serve as a general basis for generating new
   architectures.

   it is instructive to understand that dsls are a meta-model. there is
   however an even more abstract level, that is the meta meta-model. i
   wrote about this in    [26]the meta model and meta meta-model of deep
   learning   . the key question of that post was that it was obvious as to
   the best vocabulary for the meta meta-model. the meta meta-model
   specifies the kind of hyper-parameters available at the meta-model. we
   can clearly have a language that specifies down to the description of
   every neuron. however, we want to strike an effective balance of
   abstraction. there is also an entire spectrum as to what is mutable and
   trainable.

   the delineation of what happens in hyper-parameter optimization and
   what happens while training a dnn is actually somewhat arbitrary. we
   actually see this in the newer architectures like [27]deepmind pathnet,
   also a architecture search method, where hyper-parametrization and
   training are all in the same network (or fabric ). pathnet re-uses
   lower layers (effectively exploring initialization schemes) and
   explores multiple ways to connect a network.
   [1*sc8ie-fohnpsvc25yr9hig.png]
   pathnet selects new connections (not layers) while using a combination
   of sgd and ga.

   we have a glimpse of a dsl driven architecture in my previous post
   about    [28]a language driven approach to deep learning training    where
   a prescription that is quite general is presented. specifically, (1)
   define a dsl, (2) generate data from the dsl, (3) train a dl network
   from the samples then (4) use the network to guide a search algorithm
   to find better solutions. in that post, i described the dsl as being
   something that can be applied to any domain. in this article, it is
   clear that the same program induction inspired approach can be used to
   searching for deep learning architectures.

   here   s are summary of the different deep meta learning methods
   discussed here.
   [1*emfw8hfcinwrnknpmjxszw.png]
   taxonomy of meta-learning methods

   the variety clearly corresponds to the kind of meta-data that is
   manipulated to generate simulated architectures. in summary, current
   meta-learning capabilities involve either support for search for
   architectures or networks inside networks. the former is an established
   technique to automatically explore better architectures and the latter
   performs well in automatically fine-tuning algorithms.
   [1*j9kar_3vwdjk8twhtmxc0g.png]
   exploit deep learning: [29]the deep learning ai playbook

     * [30]machine learning
     * [31]artificial intelligence
     * [32]deep learning
     * [33]design patterns

   (button)
   (button)
   (button) 389 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of carlos e. perez

[34]carlos e. perez

   medium member since feb 2018

   author of artificial intuition and the deep learning playbook    
   linkedin.com/in/ceperez

     (button) follow
   [35]intuition machine

[36]intuition machine

   deep learning patterns, methodology and strategy

     * (button)
       (button) 389
     * (button)
     *
     *

   [37]intuition machine
   never miss a story from intuition machine, when you sign up for medium.
   [38]learn more
   never miss a story from intuition machine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c88ae0afb6c8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/machines-that-search-for-deep-learning-architectures-c88ae0afb6c8&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/machines-that-search-for-deep-learning-architectures-c88ae0afb6c8&source=--------------------------nav_reg&operation=register
   8. https://medium.com/intuitionmachine?source=logo-lo_qyv4uqsenlhd---d777623c68cf
   9. https://medium.com/intuitionmachine/tagged/design-patterns
  10. https://medium.com/intuitionmachine/tagged/agile-methodology
  11. https://medium.com/intuitionmachine/tagged/strategy
  12. https://deeplearningplaybook.com/
  13. https://medium.com/@intuitmachine
  14. https://unsplash.com/search/search?photo=ice__bo2vws
  15. https://medium.com/intuitionmachine/deconstructing-deep-meta-learning-77f08d7e5a97#.bped3fm0w
  16. https://startup.ml/blog/hyperparam
  17. http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a
  18. https://code.facebook.com/posts/1072626246134461/introducing-fblearner-flow-facebook-s-ai-backbone/
  19. https://arxiv.org/pdf/1703.00548v1.pdf
  20. https://arxiv.org/abs/1611.02167
  21. http://openreview.net/pdf?id=r1ue8hcxg
  22. https://bowenbaker.github.io/metaqnn/
  23. https://arxiv.org/pdf/1703.01041.pdf
  24. https://arxiv.org/pdf/1703.01513.pdf
  25. http://blog.otoro.net/2016/09/28/hyper-networks/
  26. https://medium.com/intuitionmachine/the-meta-model-and-meta-meta-model-of-deep-learning-10062f0bf74c#.xbefkxot6
  27. https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.wls0s68ed
  28. https://medium.com/intuitionmachine/a-generalized-id64-framework-for-deep-learning-architectures-970075bad781#.8yyzkgvh1
  29. https://deeplearningplaybook.com/
  30. https://medium.com/tag/machine-learning?source=post
  31. https://medium.com/tag/artificial-intelligence?source=post
  32. https://medium.com/tag/deep-learning?source=post
  33. https://medium.com/tag/design-patterns?source=post
  34. https://medium.com/@intuitmachine
  35. https://medium.com/intuitionmachine?source=footer_card
  36. https://medium.com/intuitionmachine?source=footer_card
  37. https://medium.com/intuitionmachine
  38. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  40. https://medium.com/@intuitmachine?source=post_header_lockup
  41. https://medium.com/p/c88ae0afb6c8/share/twitter
  42. https://medium.com/p/c88ae0afb6c8/share/facebook
  43. https://medium.com/@intuitmachine?source=footer_card
  44. https://medium.com/p/c88ae0afb6c8/share/twitter
  45. https://medium.com/p/c88ae0afb6c8/share/facebook
