   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

id108 to improve machine learning results

how ensemble methods work: id112, boosting and stacking

   [16]go to the profile of vadim smolyakov
   [17]vadim smolyakov (button) blockedunblock (button) followfollowing
   aug 22, 2017
   [18][1*jdqusr0c9xkln4vqpc3fzq.png]
   [1*-xbxuogb5j0irqib9druba.jpeg]

   id108 helps improve machine learning results by combining
   several models. this approach allows the production of better
   predictive performance compared to a single model. that is why ensemble
   methods placed first in many prestigious machine learning competitions,
   such as the netflix competition, kdd 2009, and kaggle.

   the [19]statsbot team wanted to give you the advantage of this approach
   and asked a data scientist, vadim smolyakov, to dive into three basic
   id108 techniques.
     __________________________________________________________________

   ensemble methods are meta-algorithms that combine several machine
   learning techniques into one predictive model in order to decrease
   variance (id112), bias (boosting), or improve predictions (stacking).

   ensemble methods can be divided into two groups:
     * sequential ensemble methods where the base learners are generated
       sequentially (e.g. adaboost).
       the basic motivation of sequential methods is to exploit the
       dependence between the base learners. the overall performance can
       be boosted by weighing previously mislabeled examples with higher
       weight.
     * parallel ensemble methods where the base learners are generated in
       parallel (e.g. id79).
       the basic motivation of parallel methods is to exploit independence
       between the base learners since the error can be reduced
       dramatically by averaging.

   most ensemble methods use a single base learning algorithm to produce
   homogeneous base learners, i.e. learners of the same type, leading to
   homogeneous ensembles.

   there are also some methods that use heterogeneous learners, i.e.
   learners of different types, leading to heterogeneous ensembles. in
   order for ensemble methods to be more accurate than any of its
   individual members, the base learners have to be as accurate as
   possible and as diverse as possible.

id112

   id112 stands for bootstrap aggregation. one way to reduce the
   variance of an estimate is to average together multiple estimates. for
   example, we can train m different trees on different subsets of the
   data (chosen randomly with replacement) and compute the ensemble:
   [1*vlsqxganq-cudci_lyh3ya.png]

   id112 uses bootstrap sampling to obtain the data subsets for training
   the base learners. for aggregating the outputs of base learners,
   id112 uses voting for classification and averaging for regression.

   we can study id112 in the context of classification on the iris
   dataset. we can choose two base estimators: a decision tree and a id92
   classifier. figure 1 shows the learned decision boundary of the base
   estimators as well as their id112 ensembles applied to the iris
   dataset.

   accuracy: 0.63 (+/- 0.02) [decision tree]
   accuracy: 0.70 (+/- 0.02) [id92]
   accuracy: 0.64 (+/- 0.01) [id112 tree]
   accuracy: 0.59 (+/- 0.07) [id112 id92]
   [0*_qr1_tdjtpchtmde.]

   the decision tree shows the axes    parallel boundaries, while the k=1
   nearest neighbors fit closely to the data points. the id112 ensembles
   were trained using 10 base estimators with 0.8 subsampling of training
   data and 0.8 subsampling of features.

   the decision tree id112 ensemble achieved higher accuracy in
   comparison to the id92 id112 ensemble. id92 are less sensitive to
   perturbation on training samples and therefore they are called stable
   learners.

     combining stable learners is less advantageous since the ensemble
     will not help improve generalization performance.

   the figure also shows how the test accuracy improves with the size of
   the ensemble. based on cross-validation results, we can see the
   accuracy increases until approximately 10 base estimators and then
   plateaus afterwards. thus, adding base estimators beyond 10 only
   increases computational complexity without accuracy gains for the iris
   dataset.

   we can also see the learning curves for the id112 tree ensemble.
   notice an average error of 0.3 on the training data and a u-shaped
   error curve for the testing data. the smallest gap between training and
   test errors occurs at around 80% of the training set size.

     a commonly used class of ensemble algorithms are forests of
     randomized trees.

   in id79s, each tree in the ensemble is built from a sample
   drawn with replacement (i.e. a bootstrap sample) from the training set.
   in addition, instead of using all the features, a random subset of
   features is selected, further randomizing the tree.

   as a result, the bias of the forest increases slightly, but due to the
   averaging of less correlated trees, its variance decreases, resulting
   in an overall better model.
   [0*ugzcqfxlc-97vr10.]

   in an extremely randomized trees algorithm randomness goes one step
   further: the splitting thresholds are randomized. instead of looking
   for the most discriminative threshold, thresholds are drawn at random
   for each candidate feature and the best of these randomly-generated
   thresholds is picked as the splitting rule. this usually allows
   reduction of the variance of the model a bit more, at the expense of a
   slightly greater increase in bias.

boosting

   boosting refers to a family of algorithms that are able to convert weak
   learners to strong learners. the main principle of boosting is to fit a
   sequence of weak learners    models that are only slightly better than
   random guessing, such as small id90    to weighted versions of
   the data. more weight is given to examples that were misclassified by
   earlier rounds.

   the predictions are then combined through a weighted majority vote
   (classification) or a weighted sum (regression) to produce the final
   prediction. the principal difference between boosting and the committee
   methods, such as id112, is that base learners are trained in sequence
   on a weighted version of the data.

   the algorithm below describes the most widely used form of boosting
   algorithm called adaboost, which stands for adaptive boosting.
   [0*mmyd6wgrep-oboki.]

   we see that the first base classifier y1(x) is trained using weighting
   coefficients that are all equal. in subsequent boosting rounds, the
   weighting coefficients are increased for data points that are
   misclassified and decreased for data points that are correctly
   classified.

   the quantity epsilon represents a weighted error rate of each of the
   base classifiers. therefore, the weighting coefficients alpha give
   greater weight to the more accurate classifiers.
   [0*yu6i_z6uwcqlhpua.]

   the adaboost algorithm is illustrated in the figure above. each base
   learner consists of a decision tree with depth 1, thus classifying the
   data based on a feature threshold that partitions the space into two
   regions separated by a linear decision surface that is parallel to one
   of the axes. the figure also shows how the test accuracy improves with
   the size of the ensemble and the learning curves for training and
   testing data.

   gradient tree boosting is a generalization of boosting to arbitrary
   differentiable id168s. it can be used for both regression and
   classification problems. gradient boosting builds the model in a
   sequential way.
   [1*ncol0wpk85jg1k5qek-6ig.jpeg]

   at each stage the decision tree hm(x) is chosen to minimize a loss
   function l given the current model fm-1(x):
   [1*ogvgucu2qpzbk_gonoxudq.jpeg]

   the algorithms for regression and classification differ in the type of
   id168 used.

stacking

   stacking is an id108 technique that combines multiple
   classification or regression models via a meta-classifier or a
   meta-regressor. the base level models are trained based on a complete
   training set, then the meta-model is trained on the outputs of the base
   level model as features.

   the base level often consists of different learning algorithms and
   therefore stacking ensembles are often heterogeneous. the algorithm
   below summarizes stacking.
   [0*gxmz7sixhyvzgce_.]
   [0*68zdjt_8rz953y5u.]

   the following accuracy is visualized in the top right plot of the
   figure above:

   accuracy: 0.91 (+/- 0.01) [knn]
   accuracy: 0.91 (+/- 0.06) [id79]
   accuracy: 0.92 (+/- 0.03) [naive bayes]
   accuracy: 0.95 (+/- 0.03) [stacking classifier]

   the stacking ensemble is illustrated in the figure above. it consists
   of id92, id79, and naive bayes base classifiers whose
   predictions are combined by id28 as a meta-classifier.
   we can see the blending of decision boundaries achieved by the stacking
   classifier. the figure also shows that stacking achieves higher
   accuracy than individual classifiers and based on learning curves, it
   shows no signs of overfitting.

   stacking is a commonly used technique for winning the kaggle data
   science competition. for example, the first place for the otto group
   product classification challenge was won by a stacking ensemble of over
   30 models whose output was used as features for three meta-classifiers:
   xgboost, neural network, and adaboost. see the following [20]link for
   details.

code

   in order to view the code used to generate all figures, have a look at
   the following [21]ipython notebook.

conclusion

   in addition to the methods studied in this article, it is common to use
   ensembles in deep learning by training diverse and accurate
   classifiers. diversity can be achieved by varying architectures,
   hyper-parameter settings, and training techniques.

   ensemble methods have been very successful in setting record
   performance on challenging datasets and are among the top winners of
   kaggle data science competitions.

recommended reading

     * zhi-hua zhou,    ensemble methods: foundations and algorithms   , crc
       press, 2012
     * l. kuncheva,    combining pattern classifiers: methods and
       algorithms   , wiley, 2004
     * [22]kaggle ensembling guide
     * [23]scikit learn ensemble guide
     * [24]s. rachka, mlxtend library
     * [25]kaggle winning ensemble

   iframe: [26]/media/02231cd5403151a2a463e36cc3b88462?postid=d1dcd548e936

you   d also like:

   [27]support vector machines tutorial
   learning id166s from examplesblog.statsbot.co
   [28]google analytics audit checklist and tools
   auditing a google analytics setup like a problog.statsbot.co
   [29]id3 (gans): engine and applications
   how generative adversarial nets are used to make our life
   betterblog.statsbot.co

     * [30]machine learning
     * [31]ensemble
     * [32]id108
     * [33]data science
     * [34]id79

   (button)
   (button)
   (button) 2k claps
   (button) (button) (button) 9 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of vadim smolyakov

[36]vadim smolyakov

   passionate about data science and machine learning
   [37]https://github.com/vsmolyakov

     (button) follow
   [38]stats and bots

[39]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 2k
     * (button)
     *
     *

   [40]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [41]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d1dcd548e936
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/ensemble-learning-d1dcd548e936&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/ensemble-learning-d1dcd548e936&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_nmrihfur5otq---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@vsmolyakov?source=post_header_lockup
  17. https://blog.statsbot.co/@vsmolyakov
  18. https://cube.dev/
  19. http://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=ensemble
  20. https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335
  21. https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/ensemble_methods.ipynb
  22. https://mlwave.com/kaggle-ensembling-guide/
  23. http://scikit-learn.org/stable/modules/ensemble.html
  24. http://rasbt.github.io/mlxtend/
  25. https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335
  26. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=d1dcd548e936
  27. https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93
  28. https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a
  29. https://blog.statsbot.co/generative-adversarial-networks-gans-engine-and-applications-f96291965b47
  30. https://blog.statsbot.co/tagged/machine-learning?source=post
  31. https://blog.statsbot.co/tagged/ensemble?source=post
  32. https://blog.statsbot.co/tagged/ensemble-learning?source=post
  33. https://blog.statsbot.co/tagged/data-science?source=post
  34. https://blog.statsbot.co/tagged/random-forest?source=post
  35. https://blog.statsbot.co/@vsmolyakov?source=footer_card
  36. https://blog.statsbot.co/@vsmolyakov
  37. https://github.com/vsmolyakov
  38. https://blog.statsbot.co/?source=footer_card
  39. https://blog.statsbot.co/?source=footer_card
  40. https://blog.statsbot.co/
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93
  44. https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a
  45. https://blog.statsbot.co/generative-adversarial-networks-gans-engine-and-applications-f96291965b47
  46. https://medium.com/p/d1dcd548e936/share/twitter
  47. https://medium.com/p/d1dcd548e936/share/facebook
  48. https://medium.com/p/d1dcd548e936/share/twitter
  49. https://medium.com/p/d1dcd548e936/share/facebook
