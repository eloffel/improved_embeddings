   #[1]machinelearning-blog.com    feed [2]machinelearning-blog.com   
   comments feed [3]machinelearning-blog.com    id21 comments
   feed [4]pros and cons of neural networks [5]the id28
   algorithm [6]alternate [7]alternate [8]machinelearning-blog.com
   [9]wordpress.com

   [10]skip to content

   [11]search

   search for: ____________________ search

   [12]machinelearning-blog.com

   tutorials and explanations about applied machine learning.

   (button) menu
     * [13]home
     * [14]about
     * [15]contact

   [16]open search

[17]id21

   id21 is the reuse of a pre-trained model on a new problem.
   it is currently very popular in the field of deep learning because it
   enables you to train deep neural networks with comparatively little
   data. this is very useful since most real-world problems typically do
   not have millions of labeled data points to train such complex models.
   this blog post is intended to give you an overview of what transfer
   learning is, how it works, why you should use it and when you can use
   it. it will introduce you to the different approaches of transfer
   learning and provide you with some resources on already pre-trained
   models.

table of contents:

     * what is it?
     * how it works
     * why it is used?
     * when you should use it
     * approaches to id21
          + training a model to reuse it
          + using a pre-trained model
          + feature extraction
     * summary


what is it?

   in id21, the knowledge of an already trained machine
   learning model is applied to a different but related problem. for
   example, if you trained a simple classifier to predict whether an image
   contains a backpack, you could use the knowledge that the model gained
   during its training to recognize other objects like sunglasses.

   with id21, we basically try to exploit what has been
   learned in one task to improve generalization in another. we transfer
   the weights that a network has learned at task a to a new task b.

   1_dvc-cgzanelnsi4w_z-eva.png

   the general idea is to use knowledge, that a model has learned from a
   task where a lot of labeled training data is available, in a new task
   where we don   t have a lot of data. instead of starting the learning
   process from scratch, you start from patterns that have been learned
   from solving a related task.

   id21 is mostly used in id161 and natural
   language processing tasks like id31, because of the huge
   amount of computational power that is needed for them.

   it is not really a machine learning technique. id21 can be
   seen as a    design methodology    within machine learning like for
   example, active learning. it is also not an exclusive part or
   study-area of machine learning. nevertheless, it has become quite
   popular in the combination with neural networks, since they require
   huge amounts of data and computational power.


how it works

   for example, in id161, neural networks usually try to detect
   edges in their earlier layers, shapes in their middle layer and some
   task-specific features in the later layers. with id21, you
   use the early and middle layers and only re-train the latter layers. it
   helps us to leverage the labeled data of the task it was initially
   trained on.

   let   s go back to the example of a model trained for recognizing a
   backpack on an image, which will be used to identify sunglasses. in the
   earlier layers, the model has learned to recognize objects and because
   of that, we will only re-train the latter layers, so that it will learn
   what separates sunglasses from other objects.


   transfer_learning_nathan in id21, we try to transfer as
   much knowledge as possible from the previous task, the model was
   trained on, to the new task at hand. this knowledge can be in various
   forms depending on the problem and the data. for example, it could be
   how models are composed which would allow us to more easily identify
   novel objects.


why it is used?

   using id21 has several benefits that we will discuss in
   this section. the main advantages are basically that you save training
   time, that your neural network performs better in most cases and that
   you don   t need a lot of data.

   usually, you need a lot of data to train a neural network from scratch
   but you don   t always have access to enough data. that is where transfer
   learning comes into play because with it you can build a solid machine
   learning model with comparatively little training data because the
   model is already pre-trained. this is especially valuable in natural
   language processing (nlp) because there is mostly expert knowledge
   required to created large labeled datasets. therefore you also save a
   lot of training time, because it can sometimes take days or even weeks
   to train a deep neural network from scratch on a complex task.

   according to demis hassabis, the ceo of deepmind technologies, transfer
   is also one of the most promising techniques that could someday lead us
   to artificial general intelligence (agi):

   bildschirmfoto 2018-04-16 um 17.38.37.png


when you should use it

   as it is always the case in machine learning, it is hard to form rules
   that are generally applicable. but i will provide you with some
   guidelines.

   you would typically use id21 when (a) you don   t have
   enough labeled training data to train your network from scratch and/or
   (b) there already exists a network that is pre-trained on a similar
   task, which is usually trained on massive amounts of data. another case
   where its use would be appropriate is when task-1 and task-2 have the
   same input.

   if the original model was trained using tensorflow, you can simply
   restore it and re-train some layers for your task. note that transfer
   learning only works if the features learned from the first task are
   general, meaning that they can be useful for another related task as
   well. also, the input of the model needs to have the same size as it
   was initially trained with. if you don   t have that, you need to add a
   preprocessing step to resize your input to the needed size.


approaches to id21

   now we will discuss different approaches to id21. note
   that these have different names throughout literature but the overall
   concept is mostly the same.

1. training a model to reuse it

   imagine you want to solve task a but don   t have enough data to train a
   deep neural network. one way around this issue would be to find a
   related task b, where you have an abundance of data. then you could
   train a deep neural network on task b and use this model as starting
   point to solve your initial task a. if you have to use the whole model
   or only a few layers of it, depends heavily on the problem you are
   trying to solve.

   if you have the same input in both tasks, you could maybe just reuse
   the model and make predictions for your new input. alternatively, you
   could also just change and re-train different task-specific layers and
   the output layer.

2. using a pre-trained model

   approach 2 would be to use an already pre-trained model. there are a
   lot of these models out there, so you have to do a little bit of
   research. how many layers you reuse and how many you are training
   again, depends like i already said on your problem and it is therefore
   hard to form a general rule.

   keras, for example, provides nine pre-trained models that you can use
   for id21, prediction, feature extraction and fine-tuning.
   you can find these models and also some brief tutorial on how to use
   them [18]here.

   there are also many research institutions that released models they
   have trained. this type of id21 is most commonly used
   throughout deep learning.

3. feature extraction

   another approach is to use deep learning to discover the best
   representation of your problem, which means finding the most important
   features. this approach is also known as representation learning and
   can often result in a much better performance than can be obtained with
   hand-designed representation.

   pca_reduction

   most of the time in machine learning, features are manually
   hand-crafted by researchers and domain experts. fortunately, deep
   learning can extract features automatically. note that this does not
   mean that feature engineering and domain knowledge isn   t important
   anymore because you still have to decide which features you put into
   your network. but neural networks have the ability to learn which
   features, you have put into it, are really important and which ones
   aren   t. a representation learning algorithm can discover a good
   combination of features within a very short timeframe, even for complex
   tasks which would otherwise require a lot of human effort.

   the learned representation can then be used for other problems as well.
   you simply use the first layers to spot the right representation of
   features but you don   t use the output of the network because it is too
   task-specific. simply feed data into your network and use one of the
   intermediate layers as the output layer. this layer can then be
   interpreted as a representation of the raw data.

   this approach is mostly used in id161 because it can reduce
   the size of your dataset, which decreases computation time and makes it
   more suitable for traditional algorithms as well.


popular pre-trained models

   there are a some pre-trained machine learning models out there that
   became quite popular. one of them is the inception-v3 model, which was
   trained for the [19]id163    large visual recognition challenge   . in
   this challenge, participants had to classify images into [20]1000
   classes, like    zebra   ,    dalmatian   , and    dishwasher   .

   [21]here you can see a very good tutorial from tensorflow on how to
   retrain image classifiers.

   microsoft also offers some pre-trained models which are available for
   both r and python development, through the [22]microsoftml r
   package and the [23]microsoftml python package.

   other quite popular models are resnet and alexnet. i also encourage you
   to visit [24]pretrained.ml which is a sortable and searchable
   compilation of pre-trained deep learning models, along with demos and
   code.


summary

   in this post, you have learned what id21 is and why it
   matters. you also discovered how it is done along with some of its
   benefits. we talked about why it can reduce the size of your dataset,
   why it decreases training time and why you also need less data when you
   use it. we discussed when it is appropriate to do id21 and
   what are the different approaches to it. lastly, i provided you with a
   collection of models that are already pre-trained.


resources

   [25]https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e
   04ef8

   [26]http://ruder.io/transfer-learning/

   [27]https://www.datacamp.com/community/tutorials/transfer-learning

   [28]https://machinelearningmastery.com/transfer-learning-for-deep-learn
   ing/

   [29]hands-on machine learning with scikit-learn and tensorflow:
   concepts, tools, and techniques for building intelligent systems

   [30]deep learning (adaptive computation and machine learning)

teilen mit:

     * [31]twitter
     * [32]facebook
     *

like this:

   like loading...

related

published by niklas donges

   [33]view all posts by niklas donges
   16. april 2018

   [34]deep learning

post navigation

   [35]pros and cons of neural networks
   [36]the id28 algorithm

leave a reply [37]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [38]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [39]log out /
   [40]change )
   google photo

   you are commenting using your google account. ( [41]log out /
   [42]change )
   twitter picture

   you are commenting using your twitter account. ( [43]log out /
   [44]change )
   facebook photo

   you are commenting using your facebook account. ( [45]log out /
   [46]change )
   [47]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   ____________________

   (button) follow

archives

     * [48]february 2019 (1)
     * [49]september 2018 (2)
     * [50]august 2018 (1)
     * [51]july 2018 (1)
     * [52]april 2018 (4)
     * [53]march 2018 (3)
     * [54]february 2018 (4)
     * [55]january 2018 (4)
     * [56]december 2017 (3)
     * [57]november 2017 (5)

recent posts

     * [58]6 concepts of andrew ng   s book:    machine learning yearning   
     * [59]a brief history of asr: automatic id103
     * [60]connectionist temporal classification
     * [61]agile and non-agile project management
     * [62]introduction to nlp

     * [63]impressum
     * [64]privacy policy

   [65]wordpress.com.

   [66]up    


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [67]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [68]cookie policy

   iframe: [69]likes-master

   %d bloggers like this:

references

   visible links
   1. https://machinelearning-blog.com/feed/
   2. https://machinelearning-blog.com/comments/feed/
   3. https://machinelearning-blog.com/2018/04/16/transfer-learning/feed/
   4. https://machinelearning-blog.com/2018/04/09/neural-networks-vs-traditional-algorithms/
   5. https://machinelearning-blog.com/2018/04/23/logistic-regression-101/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://machinelearning-blog.com/2018/04/16/transfer-learning/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://machinelearning-blog.com/2018/04/16/transfer-learning/&for=wpcom-auto-discovery
   8. https://machinelearning-blog.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://machinelearning-blog.com/2018/04/16/transfer-learning/#content
  11. https://machinelearning-blog.com/2018/04/16/transfer-learning/#search-container
  12. https://machinelearning-blog.com/
  13. https://machinelearning-blog.com/
  14. https://machinelearning-blog.com/about/
  15. https://machinelearning-blog.com/kontakt/
  16. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  17. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  18. https://keras.io/applications/
  19. http://image-net.org/
  20. http://image-net.org/challenges/lsvrc/2014/browse-synsets
  21. https://www.tensorflow.org/tutorials/image_retraining
  22. https://docs.microsoft.com/en-us/machine-learning-server/r-reference/microsoftml/microsoftml-package
  23. https://docs.microsoft.com/en-us/machine-learning-server/python-reference/microsoftml/microsoftml-package
  24. http://pretrained.ml/
  25. https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8
  26. http://ruder.io/transfer-learning/
  27. https://www.datacamp.com/community/tutorials/transfer-learning
  28. https://machinelearningmastery.com/transfer-learning-for-deep-learning/
  29. https://www.amazon.de/hands-machine-learning-scikit-learn-tensorflow/dp/1491962291/ref=sr_1_sc_1?s=books-intl-de&ie=utf8&qid=1523873619&sr=1-1-spell&keywords=hands+on+ml
  30. https://www.amazon.de/deep-learning-adaptive-computation-machine/dp/0262035618/ref=sr_1_1?s=books-intl-de&ie=utf8&qid=1523873635&sr=1-1&keywords=deep+learning
  31. https://machinelearning-blog.com/2018/04/16/transfer-learning/?share=twitter
  32. https://machinelearning-blog.com/2018/04/16/transfer-learning/?share=facebook
  33. https://machinelearning-blog.com/author/niklasd96/
  34. https://machinelearning-blog.com/category/general/deep-learning/
  35. https://machinelearning-blog.com/2018/04/09/neural-networks-vs-traditional-algorithms/
  36. https://machinelearning-blog.com/2018/04/23/logistic-regression-101/
  37. https://machinelearning-blog.com/2018/04/16/transfer-learning/#respond
  38. https://gravatar.com/site/signup/
  39. javascript:highlandercomments.doexternallogout( 'wordpress' );
  40. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  41. javascript:highlandercomments.doexternallogout( 'googleplus' );
  42. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  43. javascript:highlandercomments.doexternallogout( 'twitter' );
  44. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  45. javascript:highlandercomments.doexternallogout( 'facebook' );
  46. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  47. javascript:highlandercomments.cancelexternalwindow();
  48. https://machinelearning-blog.com/2019/02/
  49. https://machinelearning-blog.com/2018/09/
  50. https://machinelearning-blog.com/2018/08/
  51. https://machinelearning-blog.com/2018/07/
  52. https://machinelearning-blog.com/2018/04/
  53. https://machinelearning-blog.com/2018/03/
  54. https://machinelearning-blog.com/2018/02/
  55. https://machinelearning-blog.com/2018/01/
  56. https://machinelearning-blog.com/2017/12/
  57. https://machinelearning-blog.com/2017/11/
  58. https://machinelearning-blog.com/2019/02/18/6-concepts-of-andrew-ngs-book-machine-learning-yearning/
  59. https://machinelearning-blog.com/2018/09/07/a-brief-history-of-asr-automatic-speech-recognition/
  60. https://machinelearning-blog.com/2018/09/05/753/
  61. https://machinelearning-blog.com/2018/08/24/process-management-agile-and-non-agile-practices/
  62. https://machinelearning-blog.com/2018/07/25/natural-language-processing/
  63. https://machinelearning-blog.com/impressum/
  64. https://machinelearning-blog.com/privacy-policy/
  65. https://wordpress.com/?ref=footer_custom_com
  66. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  67. https://machinelearning-blog.com/2018/04/16/transfer-learning/
  68. https://automattic.com/cookies
  69. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  71. https://machinelearning-blog.com/
  72. https://machinelearning-blog.com/2018/04/16/transfer-learning/#comment-form-guest
  73. https://machinelearning-blog.com/2018/04/16/transfer-learning/#comment-form-load-service:wordpress.com
  74. https://machinelearning-blog.com/2018/04/16/transfer-learning/#comment-form-load-service:twitter
  75. https://machinelearning-blog.com/2018/04/16/transfer-learning/#comment-form-load-service:facebook
