   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]buzzrobot
     * [9]about
     * [10]advisors
     * [11]subscribe
     __________________________________________________________________

machine translation without the data

   [12]go to the profile of harshvardhan gupta
   [13]harshvardhan gupta (button) blockedunblock (button) followfollowing
   jan 4, 2018

   deep learning is being aggressively used in day-to-day tasks. it
   especially excels in areas where there is a degree of    humanness   
   involved, e.g. image recognition. probably the most useful feature of
   deep networks, unlike other machine learning algorithms, is that their
   performance increases as it gets more data. so if it is possible to get
   more data, a performance increase can be expected.

   one of the tasks where deep networks excel is machine translation. they
   are currently the state-of-the-art in this task, and feasible enough
   that even [14]google translate now uses them. in machine translation,
   sentence-level parallel data is required to train the model, i.e. for
   every sentence in the source language there needs to be the translated
   language in the target language. it is not hard to imagine why this
   could be a problem. it is hard to get a large amount of data (so that
   the power of deep learning can be used) for some language pairs.

how this article is structured

     this article is based on facebook   s recent paper[15] unsupervised
     machine translation using monolingual corpora only

   this article loosely follows the structure of the paper. i added my own
   bits to explain the material to simplify it.

   this article requires some (very) basic knowledge about neural networks
   like id168s, auto-encoders, etc.

the problem with machine translation

   as mentioned briefly above, the biggest problem with using neural
   networks in machine translation is that it requires a dataset of
   sentence pairs in both languages. it is available for widely spoken
   languages like english and french, but will not be available for other
   pairs. if the language pair data was available, this problem would be a
   supervised task.

the solution

   the authors of the paper figured out how to convert this task into an
   unsupervised task. in this task, the only data required would be two
   arbitrary corpora of each of the two languages, e.g. any fiction novel
   in english vs. any fiction novel in spanish. note that the two novels
   do not have to be the same.

   in the most abstract sense, the authors found out how to learn a latent
   space that is common between both the languages.

a recap on auto-encoders

   auto-encoders are a broad class of neural networks that are used on
   unsupervised tasks. the idea is that they are made to re-create the
   same input that they have been fed. the key is that the network has a
   layer in the middle, called the bottleneck layer. this layer is
   supposed to capture all the interesting information about the input and
   throw away the useless information.
   [1*p7afcjamglwztvjw3sd-5q.jpeg]
   conceptual auto-encoder. the middle block is the bottleneck layer that
   stores the compressed representation.[16]source

   simply put, the space in which the input ( now transformed by the
   encoder) lies in the bottleneck layer is known as the latent space.

de-noising auto-encoders

   if an auto-encoder is taught to reconstruct the input exactly the way
   it was fed to it, it may simply learn to do nothing at all. in this
   case, the outputs will be perfectly reconstructed, but we won   t have
   any useful features in the bottleneck layer. to remedy this, de-noising
   auto-econders are used. first, the actual input is corrupted slightly
   by adding some noise to it. then the network is made to reconstruct the
   original image (not the noisy version). this way, the network is forced
   to learn useful features of the image by learning what the noise is
   (and what the really useful features are).
   [1*-t3m1r-r_5i-o06irbuj0g.png]
   conceptual example of a de-noising auto-encoder. the left image is
   reconstructed using the neural network to produce the image on the
   right. in this case, the green neurons together form the bottleneck
   layer. [17]source

why learn a common latent space?

   the latent space captures the features of the data (in our case, the
   data is sentences). so if it was possible to learn a space that would
   produce the same features when language a was fed to it as when
   language b is fed to it, it would be possible to have a translation
   between them. since the model has learned the right    features   ,
   encoding from language a   s encoder, and decoding using language b   s
   decoder would effectively be asking it to do a translation.

   as you may have guessed , the authors utilised denoising auto-encoders
   to learn a feature space. they also figured out how to make the auto
   encoder learn a common latent space (they call it an aligned latent
   space)in order to perform unsupervised machine translation.

denoising auto-encoders in language

   the authors used a denoising encoder to learn the features in an
   unsupervised manner. the loss defined by them is:
   [1*n96oosiace7ox2jvaztmha.jpeg]
   equation 1.0 denoising auto-encoder loss

explanation of equation 1.0

   l is the language(for this setup , there will be 2 possible
   languages) . x is the input. c(x) is just the result after adding noise
   to x. we will get to noise creating function c shortly. e() is the
   encoder, and d() is the decoder. the term at the end , with the   (x
   hat ,x) is the sum of cross id178 errors at the token level. since we
   have an input sequence , and we get an output sequence , we need to
   make sure that every token is in the right order. therefore such a loss
   is used. it can be thought of a multi label classification , where the
   ith token in the input is compared with the ith token in the output. a
   token is a single unit which cannot be broken further. in our case, it
   is a single word.

   so, equation 1.0 is the loss that will make the network minimze the
   difference between the output of it(when given a noisy input), and the
   original , untouched sentence.

the      and ~ symbol notation.

   the      is the symbol for expectation. in this context, it means , the
   distribution of the inputs need to come from the language l, and the
   average of the loss is taken. it is just a mathematical formality , and
   the actual loss during implementation (sum of cross id178) will be
   implemented as usual.

   the ~ in particular , means    comes from a id203 distribution of   .

   i wont go into details here, but you can read about this notation in
   detail in [18]chapter 8.1 in the deep learning book.

how to add noise

   unlike images , where its just possible to add floating point numbers
   to pixels to add noise, adding noise to language needs to be different.
   therefore, the authors developed their own system to create noise. they
   denote their noise function as c() . it takes in the input sentence,
   and outputs the noisy version of that sentence.

   there are two different ways to add noise.

   first, it is possible to simply drop a word from the input with a
   id203 of p_wd.

   secondly, each word can shift from its original position with this
   constraint
   [1*kvglrlxusell7tos1ze2va.jpeg]

   here,    means the shifted location of the ith token. so , equation 2.0
   means :    a token can shift from its position at most k tokens to the
   left or to the right   

   the authors used a k value of 3 , and a p_wd value of .1

cross domain training

   in order to learn to translate between two languages , there should be
   some process to map an input sentence(in language a) to an output
   sentence (in language b). the authors call this cross domain training.
   first, an input sentence (x) is sampled. then the translated output(y)
   is produced by using the model(m()) from the previous iteration.
   putting it together we have y = m(x). after that, y is corrupted using
   the same noise function c() described above ,giving c(y). the encoder
   of language a is made to encode this corrupted version, and the decoder
   of language b is made to decode the output from language a   s encoder,
   and recreate a clean version of c(y) . the models are trained using the
   same sum of cross id178 error like in equation 1.0.

learning a common latent space by adversarial training

   so far , there has been no mention of how to learn the common latent
   space. the cross domain training mentioned above may somewhat help
   learn a space that is similar, but a stronger constraint to push the
   models to learn a similar latent space is required.

   the authors used adversarial training. they used another model(called
   discriminator) that takes the output of each of the encoders, and
   predict which language that encoded sentence belongs to. then , the
   gradients from the discriminator are taken , and the encoder is also
   trained to fool the discriminator. this is conceptually no different
   than a standard gan (generative adversarial network). the discriminator
   takes in the feature vector of each time step(because id56s are used),
   and predicts which language it came from.

putting it all together

   the 3 different losses(autoencoder loss, translation loss , and
   discriminator loss) mentioned above are added together , and all the
   model weights are updated in one step.

   since this was a sequence to sequence problem , the authors used an
   lstm network, with attention, i.e. there are two lstm based
   autocoders , one for each language.

   at a high level, there are 3 main steps to training this architecture.
   it follows an iterative training procedure. the training loop would
   look somewhat like this:
    1. obtain translation using encoder of language a and decoder of
       language b
    2. train each auto-encoder to regenerate an uncorrupted sentence when
       given a corrupted sentence
    3. improve the translation by corrupting the translation obtained in
       step 1 , and recreating it. for this step the encoder of language
       a , and decoder of language b are trained together (and also
       encoder of language b and decoder of language a )

   note that even though step 2 and 3 are listed separately, the weights
   are updated for both of them together.

how to jumpstart this framework

   as mentioned above, the model uses its own translation from the
   previous iteration to improve on its translation capabilities.
   therefore, before the training loop begins, it is important to have
   some form of translation capability already. the authors used
   fasttext , to learn word level bilingual dictionary. note that this
   method is very naiive and required only to give the model a starting
   point.

   the whole framework is given in the flowchart below
   [1*5oge7blhuogqp2vtm0zaew.jpeg]
   high level working of the entire translation framework. credit:
   [19]robert aguilera

conclusion

   this was an explanation of a very new technique to perform unsupervised
   machine translation. it used multiple different losses to improve on
   individual tasks, while using adversarial training to add constraints
   to the behaviour of the architecture.

call to action

   this was my 10th medium post. if you have any feedback, or any cool
   papers you want me to cover, feel free to mention it in the comments.

   if you like article, make sure to hold that              icon for as long as you
   want to!

     * [20]machine learning
     * [21]artificial intelligence
     * [22]deep learning
     * [23]data science
     * [24]technology

   (button)
   (button)
   (button) 1.7k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of harshvardhan gupta

[26]harshvardhan gupta

   stupid and hungry

     (button) follow
   [27]buzzrobot

[28]buzzrobot

   the publication aims to cover practical aspects of ai technology along
   with interviews with notable people in the ai field.

     * (button)
       (button) 1.7k
     * (button)
     *
     *

   [29]buzzrobot
   never miss a story from buzzrobot, when you sign up for medium.
   [30]learn more
   never miss a story from buzzrobot
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://buzzrobot.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/21846fecc4c0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://buzzrobot.com/machine-translation-without-the-data-21846fecc4c0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://buzzrobot.com/machine-translation-without-the-data-21846fecc4c0&source=--------------------------nav_reg&operation=register
   8. https://buzzrobot.com/?source=logo-lo_ledszbm8cqtr---47d409e68707
   9. https://buzzrobot.com/about
  10. https://buzzrobot.com/advisors-38cd35a7a8e1
  11. https://upscri.be/3b4d4f/
  12. https://buzzrobot.com/@harshsayshi?source=post_header_lockup
  13. https://buzzrobot.com/@harshsayshi
  14. https://en.wikipedia.org/wiki/google_neural_machine_translation
  15. https://arxiv.org/abs/1711.00043
  16. https://blog.keras.io/building-autoencoders-in-keras.html
  17. http://www.birving.com/presentations/autoencoders/index.html#/
  18. http://www.deeplearningbook.org/contents/optimization.html
  19. http://robertaguileradesign.com/
  20. https://buzzrobot.com/tagged/machine-learning?source=post
  21. https://buzzrobot.com/tagged/artificial-intelligence?source=post
  22. https://buzzrobot.com/tagged/deep-learning?source=post
  23. https://buzzrobot.com/tagged/data-science?source=post
  24. https://buzzrobot.com/tagged/technology?source=post
  25. https://buzzrobot.com/@harshsayshi?source=footer_card
  26. https://buzzrobot.com/@harshsayshi
  27. https://buzzrobot.com/?source=footer_card
  28. https://buzzrobot.com/?source=footer_card
  29. https://buzzrobot.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/21846fecc4c0/share/twitter
  33. https://medium.com/p/21846fecc4c0/share/facebook
  34. https://medium.com/p/21846fecc4c0/share/twitter
  35. https://medium.com/p/21846fecc4c0/share/facebook
