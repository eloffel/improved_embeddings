   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

id21 using keras

   [9]go to the profile of prakash jay
   [10]prakash jay (button) blockedunblock (button) followfollowing
   apr 15, 2017
   [1*zcxqy5c-mwrzjlo7rypyrq.png]
   inception-v3 google research

what is id21?

   id21, is a research problem in machine learning that
   focuses on storing knowledge gained while solving one problem and
   applying it to a different but related problem.

   iframe: [11]/media/b6c52d4e837d433674b3626d69280699?postid=d804b2e04ef8

   signup for my newsletter

why id21?

     * in practice a very few people train a convolution network from
       scratch (random initialisation) because it is rare to get enough
       dataset. so, using pre-trained network weights as initialisations
       or a fixed feature extractor helps in solving most of the problems
       in hand.
     * very deep networks are expensive to train. the most complex models
       take weeks to train using hundreds of machines equipped with
       expensive gpus.
     * determining the topology/flavour/training method/hyper parameters
       for deep learning is a black art with not much theory to guide you.

my experiences:

     "don't try to be an hero" ~andrej karapathy

   most of the id161 problems i faced doesn   t have very large
   datasets(5000 images         40,000 images). even with extreme data
   augmentation strategies it is difficult to achieve decent accuracy.
   training these networks with millions of parameters generally tend to
   overfit the model. so id21 comes to our rescue.

how id21 helps ?

   when you look at what these deep learning networks learn, they try to
   detect edges in the earlier layers, shapes in the middle layer and some
   high level data specific features in the later layers. these trained
   networks are generally helpful in solving other id161
   problems. lets have a look at how to do id21 using keras
   and various cases in id21.
   [1*l8nwufrce1bt9adin7tu4a.png]
   inception v3 google research

simple implementation using keras:

   iframe: [12]/media/6f17675e8779b1b9b1c9f30c6e281780?postid=d804b2e04ef8

   keeping in mind that convnet features are more generic in early layers
   and more original-dataset-specific in later layers, here are some
   common rules of thumb for navigating the 4 major scenarios:

1. new dataset is small and similar to original dataset:

   there is a problem of over-fitting, if we try to train the entire
   network. since the data is similar to the original data, we expect
   higher-level features in the convnet to be relevant to this dataset as
   well. hence, the best idea might be to train a linear classifier on the
   id98 codes.

   so lets freeze all the vgg19 layers and train only the classifier
for layer in model.layers:
   layer.trainable = false
#now we will be training only the classifiers (fc layers)

2. new dataset is large and similar to the original dataset

   since we have more data, we can have more confidence that we won   t
   overfit if we were to try to fine-tune through the full network.
for layer in model.layers:
   layer.trainable = true
#the default is already set to true. i have mentioned it here to make things cle
ar.

   in case if you want to freeze the first few layers as these layers will
   be detecting edges and blobs, you can freeze them by using the
   following code.
for layer in model.layers[:5]:
   layer.trainable = false.
# here i am freezing the first 5 layers

3. new dataset is small but very different from the original dataset

   since the dataset is very small, we may want to extract the features
   from the earlier layer and train a classifier on top of that. this
   requires a little bit of knowledge on h5py.

   iframe: [13]/media/110ca898aecc40aea0ce316a47d439eb?postid=d804b2e04ef8

   the above code should help. it will extract the    block2_pool    features.
   in general this is not helpful as this layer has (64*64*128) features
   and training a classifier on top of it might not help us exactly. we
   can add a few fc layers and train a neural network on top of it. that
   should be straight forward.
     * add few fc layers and output layer.
     * set the weights for earlier layers and freeze them.
     * train the network.

4. new dataset is large and very different from the original dataset.

   this is straight forward. since you have large dataset, you can design
   your own network or use the existing ones.
     * train the network using random initialisations or use the
       pre-trained network weights as initialisers. the second one is
       generally preferred.
     * if you are using a different network or making small modification
       here and there for the existing network, be careful with the naming
       conventions.

references

    1. [14]cs231n.github.io/transfer-learning/
    2. [15]keras.io
    3. [16]https://github.com/fchollet/keras

   let me know your feedback and interesting things you were able to do
   with id21.

   my github profile: [17]https://github.com/prakashvanapalli

     * [18]machine learning
     * [19]deep learning
     * [20]id21
     * [21]id161

   (button)
   (button)
   (button) 3.4k claps
   (button) (button) (button) 39 (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of prakash jay

[23]prakash jay

   senior data scientist [24]@fractalanalytics

     * (button)
       (button) 3.4k
     * (button)
     *
     *

   [25]go to the profile of prakash jay
   never miss a story from prakash jay, when you sign up for medium.
   [26]learn more
   never miss a story from prakash jay
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d804b2e04ef8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@14prakash?source=post_header_lockup
  10. https://medium.com/@14prakash
  11. https://medium.com/media/b6c52d4e837d433674b3626d69280699?postid=d804b2e04ef8
  12. https://medium.com/media/6f17675e8779b1b9b1c9f30c6e281780?postid=d804b2e04ef8
  13. https://medium.com/media/110ca898aecc40aea0ce316a47d439eb?postid=d804b2e04ef8
  14. http://cs231n.github.io/transfer-learning/
  15. http://keras.io/
  16. https://github.com/fchollet/keras
  17. https://github.com/prakashvanapalli
  18. https://medium.com/tag/machine-learning?source=post
  19. https://medium.com/tag/deep-learning?source=post
  20. https://medium.com/tag/transfer-learning?source=post
  21. https://medium.com/tag/computer-vision?source=post
  22. https://medium.com/@14prakash?source=footer_card
  23. https://medium.com/@14prakash
  24. http://twitter.com/fractalanalytics
  25. https://medium.com/@14prakash
  26. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  28. https://medium.com/p/d804b2e04ef8/share/twitter
  29. https://medium.com/p/d804b2e04ef8/share/facebook
  30. https://medium.com/p/d804b2e04ef8/share/twitter
  31. https://medium.com/p/d804b2e04ef8/share/facebook
