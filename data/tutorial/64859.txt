   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

id3         a theoretical walk-through.

   [9]go to the profile of sameera ramasinghe
   [10]sameera ramasinghe (button) blockedunblock (button) followfollowing
   feb 14, 2018

   since ian goodfellow first proposed the idea of gans
   ([11]https://arxiv.org/abs/1406.2661), it has become a buzz word within
   ml community, simply because it works stunningly well (given that you
   came up with a perfect architecture). many people, specially yann
   lecun, a who is considered as one of the giants in deep learning,
   stated at some point that gans are a significant breakthrough in deep
   learning.

   one thing i have noticed is that many people who claim to be familiar
   with gans lack the theoretical foundation that lies beneath it, which
   is important.

   reader alert: this post is not for absolute noobs. i assume you have a
   basic understanding on what gans actually do in a practical point of
   view, as i will not discuss them here. i will use terms such as
   discriminator and generator without much explanation, assuming that you
   know it already. in a nutshell, gans are a set of generative models,
   which can learn to generate data (ideally) identical to a given data
   distribution. at present, it is mainly used in id161 domain
   to generate semantically meaningful images for various purposes.

   here, i will go through each important theoretical point mentioned in
   the original paper and try to explain it in simple terms along with
   derivations, whenever necessary.

   enough talk   so here we go   

   first, we   ll focus on the most important equation in the whole paper.
   [1*feqtqdfgjxyk3s5gxfjeuq.png]

   let   s carefully pay attention to each of the terms in this equation.

   d(x): the discriminator function. simply put, if you input a    x    data
   point (generated or from the original dataset) through d, it will
   output a scaler value between 0 and 1. this value is the id203
   that    x    is from the original dataset. let   s repeat. keep in mind that
   d(x) outputs the id203 that    x    is from the original dataset. not
   the other way around. ideally, we want the d(x) (at the equilibrium
   point) to output 0.5 to every data point of x distribution, whether
   it   s from the generator or from the original dataset. intuitively this
   means that the d(x) cannot distinguish between generated data and
   original data, which implies that generator is generating data
   perfectly matching with the original distribution.

   g(z): the generator function. in here, z is the noise vector, which is
   the input to the generator function. the output of g(z) is a matrix
   whose dimensions are equal to x   s. ideally, we want g(z) to output
   matrices which are indistinguishable from the original data (x)
   distribution.

   if you look closely at the equation 1, there are two loops. the
   objective of the inner loop is to maximize the right hand side
   expression as far as possible (by only tweaking d   s parameters). the
   objective of the outer loop is to minimize the right hand side
   expression as far as possible (by only tweaking g   s parameters).

   let   s see what does this mean intuitively.

   to the whole function to be maximized, the first term e(log(d(x)) needs
   to be maximized. which means d(x) needs to be maximized. if you can
   remember the log (x) plot, you will figure out that when d(x) becomes
   close to 1, e(log(d(x)) becomes close to 0. when d(x) becomes close to
   0 e(log(d(x)) becomes close to -infinity. which means that when
   maximizing the first term, the d(x) will try to output values close to
   1, for original data. which is the purpose of d(x).

   then let   s look at the second term. the maximum value of 1-log(d(g(z))
   is positive infinity, and it gets that value when d(g(z)) = 0. which
   means that when maximizing the second term, the d(g(z)) will try to
   output values close to 0, which is the purpose of d(x).

   now let   s have a look at the outer loop.

   the objective of the outer loop is to minimize the right hand side
   equation by only tweaking g parameters. which means we will only be
   changing g. now the first term of the right hand side does not depend
   on g. this means that we can ignore the first term when considering
   outer loop. when trying to minimize the second term, lowest value it
   could have is -infinity, which is achieved when d(g(z)) = 1. but d
   should output 1 only when the input is from the original data. this
   means that g(z) will have to tweak its parameters in such a way that
   the output of g(z) will have to be as close as possible to original
   data distribution.

   right   now after this analysis, you should understand that playing this
   minimax game results in making g(z) as close as possible to the
   original data distribution, make d(x) = 1 when x is from original data,
   and make d(g(z)) = 0. now this the point where most of the people stop
   reading the analysis and assume that they have understood the whole
   paper. seems pretty straightforward right?

   no   it   s not straightforward. it   s the rest of the analysis which is
   more interesting and contains the whole idea of gans (specially
   regarding the convergence of gans).

   now let   s see what is missing in the above explanation. observe that
   the inner loop of the minimax game tries to push p_g~d(g(z)) towards 0,
   while outer loop tries to push p_g~d(g(z)) towards 1. so, where would
   this game end? what would happen if the inner loop wins ? or if outer
   loop wins? is there an equilibrium point at all? if so, does that
   equilibrium point produce the optimum generator? the rest of the
   analysis is purely there to answer these questions.

   i am going to repeat the two key questions again.
    1. is there an equilibrium point to this minimax game?
    2. if there is, does that point produce the optimum generator?

   we need    yes    as the answer to both these questions. let   s see if
   that   s the case.

   by definition, e(f(x) of some function f(x) with respect to a
   id203 distribution p(x) is the average value of f(x) when x is
   drawn from p(x). then e(x) is calculated as,
   [1*lxhdmpxgzctdxggiasat-a.png]

   therefore, we can rewrite v(d,g) as,
   [1*94qlssvhwxb0zfxy0qzhfw.png]

   now comes the tricky part. [12]lotus theorem that comes with statistics
   states that if g(x) = x and one knows p(x) but not p(g(x)), e(g(x)) can
   be still found using,
   [1*pkxascororpqg8utjko4pg.png]

   now let,
   [1*bu9xygfhndnhdr6fclzdkq.png]

   we know that,
   [1*nu2fajzjxn-_vnprhov1ga.png]

   then by lotus theorem,
   [1*tal3w1wcter-xbtqzfv2ig.png]

   therefore, we can rewrite v(d,g) as,
   [1*yf1aswocwpmgi5psjb0hda.png]

   consider function,
   [1*tcub9kvbrxkvioz9d3fviq.png]

   now visualize a 3d plot,
   [1*zroytjtbewy_yldtd6uwuq.png]
   figure 1

   as you can see in the plot, different d(x) functions will give
   different f(pdata, pg) curves, for same data points of pdata and pg.
   v(g,d) is the area under the curve f(pdata,pg). so, if we can find a
   d*(x) for every (pdata,pg) point, which gives the maximum f(pdata,pg)
   value for each of those points, integrating along d*(x) curve will give
   us the highest area under f(pdata,pg) curve.

   how do we find the maximum of a function? easy. we differentiate it and
   find the locations where it is equal to zero. considering each constant
   data point (pdata,pg) and differentiating w.r.t. d(x),
   [1*cxzgjbnslc3apgu6gvjrmw.png]

   this equation is quite intuitive. it says that for given two
   distributions, pdata(original) , pg (generated), ideal discriminator
   should be able to identify the original data portion.

   so as you can see, for any data point (pdata,pg), if we choose d(x) =
   d*(x), we will get the highest possible value for f(pdata,pg). so
   integrating along that curve will give us the maximum value for v(g,d).
   note that d*(x) is not a static function. it will try to change it   s
   value towards d*(x) in each (pg, pdata) point during the maximization
   of v(g,d). practically, that   s what id119 tries to do to
   d(x) while training. trying to bring it closer to d*(x) at each (pg,
   pdata) point, by tweaking its parameters through id26. you
   should also notice that pg and pdata distributions are static during
   this procedure, as we do not change the generator (g) parameters.

   so we can rewrite the equation,
   [1*5-qs2hdypdvr0fcmrgbraq.png]

   right   now we focus on minimizing game on c(g) by tweaking g(z)
   parameters where c(g) is,
   [1*m414zojlf9vdfjrjzw4i9g.png]

   some of you might be confused now, as you do not see a g(z) in the
   equation now, and must be wondering how are we going to minimize
   equation by tweaking g(z). note that pg and pdata are both dependent on
   g(z). so g(z) is embedded in the equation. also, by tweaking g(z) we
   are changing d*(x) too. that   s why we are interested in finding out in
   the whole space of x, is there a point, which satisfies the condition
   for d*(x) (which is the goal of maximizing game), and also the minimum
   of c(g) (which is the goal of minimizing game). in other words, if
   there is such a point, then there is an equilibrium point to this
   minimax game. otherwise, the two players will play this game forever
   without an agreement. also, another important point is, does that point
   give us the optimum generator? (as you may see, i am just re-phrasing
   our two main questions)

   we ask ourselves the question,    what constraint does the optimal
   generator impose upon pg and pdata ?   . obviously the answer is pg =
   pdata. in other words, the generated data distribution needs to be
   identical to the original data distribution. so we will start from that
   point.

   let pg = pdata. then, d*(x) = 1/2, by maximizing game. then, c(g) =
   log(1/2) + log(1/2) = -log4. to see that this is also the global
   minimum point of c(g) we subtract,
   [1*b4_ho3gusotad2cg54veja.png]

   from c(g) and get,
   [1*k5ocjko8wawli2e_2k753a.png]

   i assume you have heard of kl divergence. if you haven   t, it   s a
   measure of how much a given distribution differs from a second
   distribution. the definition of kl divergence is,
   [1*esyd39iv-uc6ptpjubpsmg.png]

   by applying this on above equation we get,
   [1*0r7gfcs1hcamszhcw0gnsq.png]

   by definition, jenson- shanon divergence between two distributions is
   given as,
   [1*o4_yejkvk0yyg5l6bvmjbq.png]

   using this we get,
   [1*cd4crxx-wvsl9niepz6wcw.png]

   now, jenson- shanon divergence between two distributions is a non
   negative value, therefore we get,
   [1*uzrgqckgemc7gpeb1_atgw.png]

   therefore it is proven that the global minimum value c(g) can have is
   -log(4), and it is achieved with d*(x) = 1/2.

   so it should be clear now, that at the point of pg=pdata, both
   maximizing player and minimizing player will achieve there goals. in
   other words, that is our equilibrium point! which answers    yes    our
   first main question. and most importantly, in that equilibrium point,
   pg=pdata, which gives us the    yes    answer for the second question and
   the optimum generator. hurraah!!!

   so all done? seems yes doesn   t it? well a one small glitch remains.

   how are we going to reach this optimum point? true, the minimizing,
   maximizing game will get us there, but it   s all just words and numbers.
   what is the practical algorithm we are going to use to get there.

   what the authors have done to answer this question is that coming up
   with an algorithm proving that this algorithm will get us to the
   optimal point. let   s have a look at this algorithm.
   [1*vfcvuupzwyroeybvtw9t0g.png]
   convergence algorithm

   let   s have a quick thought on what does this global optima looks like.
   remember that d needs to maximize v(g,d) for a given g, and again g
   will try to minimize v(g,d) at the optimal d. so what does this give
   us?
   [1*gruvkiabl5ykjnwdbfggua.png]
   figure 2

   if you guessed it correctly, it will be a saddle point. something close
   to above plot. i used pg instead of g for easier understanding of the
   next part of this article. as pg is solely dependent on g, we can use
   pg instead of g. note that this is not a 100% accurate plot, as d will
   not be optimal at the same value for every given pg.

   our goal is to reach the red point in the plot. the paper then says
   something like this.

        the subderivatives of a supremum of convex functions include the
     derivative of the function at the point where the maximum is
     attained.   .

   what does this mean in plain english? let   s see.

   i will draw another plot pg vs v(g,d), making d(x) = d*(x) (optimal d)
   at every point of pg. we will reach this point by applying gradient
   descent for d, for each fixed point of pg as in the inner loop of the
   convergence algorithm (see the algorithm). this simply means, in the
   above plot, at each pg, i will pick the maximum value of v(g,d) by
   varying d(x) and plot pg vs v(g,d). this will give me a plot like
   below.
   [1*nwt-jede2goyqzsxwyylng.png]
   figure 3

   the above statement simply says, if i get the set of derivative of
   v(d*,g) w.r.t. pg, it will include the all the partial derivatives of
   v(d,g) w.r.t. pg, at the locations where v(d,g) is maximum for a given
   pg (see the plot in figure 2). this means that the derivative at the
   red dot in figure 2 is somewhere in the set of derivatives of the
   function in figure 3. and by applying id119 on g(z)
   parameters (partial differentiating v(d*,g) w.r.t. pg), we can update
   pg and reach that red dot. furthermore by previous proofs, we know at
   that global optimum, pdata = pg and d(x) = 1/2, which is exactly what
   we want.

   if this last part is not very clear to you, let   s think of it in simple
   terms. look again at figure 2.

     by applying id119 to d, for a given pg, we get to the
     optimum d for that pg. (inner loop of the convergence algorithm).

     then keeping d(x) fixed, we apply id119 to pg, and get
     closer to the red dot. (outer loop of the convergence algorithm)

     since partial derivatives of pg at optimum d points include red dot,
     given enough capacity to d and g, we will eventually reach the red
     point by using the convergence algorithm .

   thats it!! that   s the complete theoretical explanation of the
   id3 paper by goodfellow et al. before
   concluding, for the sake of completeness i will quickly go through few
   things mentioned in the paper. i intentionally kept these at last, as
   it is easier to understand them after going through the whole
   explanation. the paper contains the following figure,
   [1*ibfnxn7nwylgyz8sdhrtuq.png]
   figure 5

   green line is pg, black line is pdata and blue line is d(x). let   s go
   through each of these one-by-one. (a) is the initial situation. pg is
   distributed far from pdata, and d can roughly distinguish between them
   (low value at pg and high value at pdata). the we update d using
   gradient decent, which will give us (b). now d can distinguish between
   pg and pdata better. then in (c ), we update g, which will move pg
   closer to pdata. after few iterations, pg=pdata and d = 1/2 everywhere
   as in (d).

   paper then mentions that in practice,

     rather than training g to minimize (log(1-d(g(z)))), it   s better to
     train to g to maximize log(d(g(z))).

   how do we make sense of this theoretically?

   let   s have a look at log(d(g(z))) and log(1-d(g(z))) plots.
   [1*wgbqiezzlbq1lrucsskpvw.png]

   in practice, initially g is lousy and produce data which are clearly
   distinguishable from original data. therefore initially, d(g(z)) ~ 0.
   now look at the two plots. s1 and s2 are the gradients of the two plots
   where d(g(z)) is close to 0. as |s1| >> |s2|, log(d(g(z))) will
   initially have much stronger gradients than log(1-d(g(z))). remember
   our goal is to bring d(g(z)) to 1/2. while minimizing game tries to
   push d(g(z)) towards 1, maximizing game will try to push d(g(z))
   towards zero. if maximizing payer wins initially (which means
   separately identify generated and original data with a very high
   accuracy), d(g(z)) will never reach 1/2. therefore for minimizing
   player, it is important to have stronger gradients initially to keep up
   with the maximizing player and reach the equilibrium point.

   well   that   s all there is to it!! :) hope you enjoyed the post :d

   if there are any questions or suggestions, don   t hesitate to leave a
   comment. have a good day!!

     * [13]machine learning
     * [14]generative adversarial
     * [15]deep learning

   (button)
   (button)
   (button) 649 claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [16]go to the profile of sameera ramasinghe

[17]sameera ramasinghe

   graduate student in deep learning@anu, co-founder@conscientai

     * (button)
       (button) 649
     * (button)
     *
     *

   [18]go to the profile of sameera ramasinghe
   never miss a story from sameera ramasinghe, when you sign up for
   medium. [19]learn more
   never miss a story from sameera ramasinghe
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5889d5a8f2bb
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@samramasinghe/generative-adversarial-networks-a-theoretical-walk-through-5889d5a8f2bb&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@samramasinghe/generative-adversarial-networks-a-theoretical-walk-through-5889d5a8f2bb&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@samramasinghe?source=post_header_lockup
  10. https://medium.com/@samramasinghe
  11. https://arxiv.org/abs/1406.2661
  12. https://en.wikipedia.org/wiki/law_of_the_unconscious_statistician
  13. https://medium.com/tag/machine-learning?source=post
  14. https://medium.com/tag/generative-adversarial?source=post
  15. https://medium.com/tag/deep-learning?source=post
  16. https://medium.com/@samramasinghe?source=footer_card
  17. https://medium.com/@samramasinghe
  18. https://medium.com/@samramasinghe
  19. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  21. https://medium.com/p/5889d5a8f2bb/share/twitter
  22. https://medium.com/p/5889d5a8f2bb/share/facebook
  23. https://medium.com/p/5889d5a8f2bb/share/twitter
  24. https://medium.com/p/5889d5a8f2bb/share/facebook
