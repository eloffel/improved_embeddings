   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id23 demystified: a gentle introduction

episode 1, demystifying agent/environment interaction, and the components of
a id23 agent.

   [16]go to the profile of mohammad ashraf
   [17]mohammad ashraf (button) blockedunblock (button) followfollowing
   apr 7, 2018

   in a long blog post series starting with this episode, i   ll try to
   simplify the theory behind the science of id23 and
   its applications and cover code examples to make a solid illustration.
   [1*9jsvbirkmpbga5vcdmwdea.jpeg]
   mujoco   s humanoids

     what is id23 ?

   id23 or rl for short is the science of decision
   making or the optimal way of making decisions. when an infant plays,
   waves its arms, it has no explicit teacher, but it does have a direct
   sensorimotor connection to its environment. exercising this connection
   produces a wealth of information about cause and effect, about
   consequences of actions, and about what to do in order to achieve
   goals.
   [1*mpgk9wtnnvp3i4-9jfgd3w.png]
   fig. 1

   this is the key idea behind rl, we have an environment which represents
   the outside world to the agent and an agent that takes actions,
   receives observations from the environment that consists of a reward
   for his action and information of his new state. that reward informs
   the agent of how good or bad was the taken action, and the observation
   tells him what is his next state in the environment.

   the agent tries to figure out the the best actions to take or the
   optimal way to behave in the environment in order to carry out his task
   in the best possible way.
   [1*aamg88iumybh36oelfn6xg.gif]
   humanoid learning how to run

   this is a simulation of a humanoid that learned how to run after
   executing the sequence of acting, observing, and then acting until it
   finally figured out the best action to take at each time step to
   achieve its task, i.e. running efficiently.

     success stories.

   here is a successful example of an rl agent that learned to play
   breakout like any human being after 400 training episodes. after 600
   training episodes, the agent finds and exploits the best strategy of
   tunnelling and then hitting the ball behind the wall.

   keep in mind there is no explicit human supervisor, the agent learns by
   trial and error.

   iframe: [18]/media/002f359af31d1164c9fdfe21653dde2e?postid=36c39c11ec14

   deepmind   s id25 breakout

   another amazing success story is how[19] deepmind used rl to simulate
   locomotion behavior on [20]mujoco   s simulation models. the agent is
   given proprioception and simplified vision to perceive the environment.

   the agent learns to run, jump, crouch, and climb through relentless
   attempts of trials and learning from its errors.

   iframe: [21]/media/b3ee80bb0523a0309521595fdd969c99?postid=36c39c11ec14

   we can   t ignore the biggest event in ai community, the ultimate
   smackdown of human versus machine, where deepmind   s alphago ruthlessly
   managed to defeat [22]lee sedol, the south korean professional go
   player of 9 dan rank, 4 matches against 1 in march 2016. this guy has
   18 world championships under his belt.

   iframe: [23]/media/1b388b74424674a7e9d562239584576a?postid=36c39c11ec14

   alphago documentary official trailer

        the game of go is the holy grail of artificial intelligence.
     everything we   ve ever tried in ai, it just falls over when you try
     the game of go.   

     david silver, lead researcher for alphago
     __________________________________________________________________

     what makes rl different from other machine learning paradigms ?

   in rl there is no supervisor, only a reward signal or a real number
   that tells the agent how good or bad was his action. feedback from the
   environment might be delayed over several time steps, it   s not
   necessarily instantaneous e.g. for the task of reaching a goal in a
   grid-world, the feedback might be at the end when the agent reaches the
   goal. the agent might spend some time exploring and wandering in the
   environment until it finally reaches the goal after a while to realize
   what were the good and bad actions it has taken.

   in machine learning or supervised learning, we have a dataset that
   describes the environment to the algorithm and the right answers or
   actions to do when faced with a specific situation, and the algorithm
   tries to generalize from that data to new situations.

   in rl, the data is not i.i.d (independent and identically distributed).
   the agent might spend some time in a certain part of the environment
   and not see other parts which might be interesting to learn the optimal
   behavior. so time really matters, the agent must explore pretty much
   every part of the environment to be able to take the right actions.

   the agent influences the environment through its actions which in turn
   affect the subsequent data it receives from the environment, it   s an
   active learning process.
     __________________________________________________________________

     what are the rewards ?

   a reward rt is a scalar feedback signal that indicates how well the
   agent is doing at time step t. the agent   s job is to maximize the
   expected sum of rewards. id23 is based on the reward
   hypothesis which states that,

        all goals can be described by the maximization of expected
     cumulative rewards   .

   e.g. if we want the humanoid to learn how to walk, we can break down
   this goal into a positive reward for forward motion and negative reward
   for falling over. the agent will learn that the negative reward is
   associated with the actions the cause him to fall over, and eventually
   it   ll learn that it   s not good to take that action, instead it should
   take the other action that gets him the positive reward.

   we   ve seen that the agent must select the actions that maximize the
   future rewards, but some actions might have long term consequences.
   given that some rewards can be delayed, the agent can   t be greedy all
   the time, i.e. take action associated with maximum reward at the
   current time, it has to plan ahead (more on that later). it might be
   better to sacrifice immediate reward to gain long-term reward, or vice
   versa.
     __________________________________________________________________

     agent and environment.

   in fig. 1, the agent and the environment interact at each other over a
   sequence of discrete time steps, t = 0, 1, 2, 3,    . at each time step
   t, the agent receives some representation of the environment   s state,
   st     s, and on that basis selects an action, at     a(s).

   one time step later, in part as a consequence of its action, the agent
   receives a numerical reward, rt+1     r        , and finds itself in a new
   state, st+1.

   all the sequence of observations, actions, and rewards during the
   agent   s life time up to time step t is called the history,

   ht = s1, a1, r2,          . , st-1, at-1, rt.

   it   s what the agent has seen so far, i.e. all the observable variables
   up to time step t.

   what happens next depends on the history, the agent selects actions,
   i.e. a mapping from the history to an action. the environment emits
   next state and a reward associated with the taken action.

   naively, it seems that, this is probably the best way to encode what
   the agent has encountered so far, and the agent should use this
   information to decide what is the next action, but the history is not
   very useful since it   s enormous, and it   ll be infeasible to use this
   approach in the real world interesting problems.

   instead, we turn to the state, which is like a summary of information
   encountered so far. it is used to determine what happens next. a state
   is a function of history, st = f(ht).
     __________________________________________________________________

     state of agent and environment.

   the environment state is the information used within the environment to
   determine what happens next from the environment   s perspective, i.e.
   spit out the observation or next state and reward.

   the environment state st is the private representation of the
   environment, i.e. whatever data the environment uses to pick the next
   state and reward. the environment state is not usually visible to the
   agent, even if it   s visible, it might contain some irrelevant
   information.

   the agent state captures what happened to the agent so far, it
   summarizes what is going on and the agent uses this to pick the next
   action. the agent state is its internal representation, i.e. whatever
   information the agent uses to pick the next action.

   the state can be any function of history as we mentioned, and the agent
   decides what this function is going to be.
     __________________________________________________________________

     how to represent the agent state ?

   an information state a.k.a markov state contains all useful information
   from the history. we use that markov state to represent the agent   s
   state.

     a state st is markov if and only if ;

     p[st+1 | st] = p[st+1 | s1,    .. , st]

   this means the agent state is markov if that state contains all the
   useful information the agent has encountered so far, which in turn
   means, we can throw away all the previous states and just retain the
   agent   s current state.

   if we have the markov property, the future is independent of the past
   given the present. once the current state is known, the history may be
   thrown away, and that state is a sufficient statistic that gives us the
   same characterization of the future as if we have all the history.

   there is the notion of fully observable environments, where the agent
   directly observes the environment state and as a result, the
   observation emitted from the environment is the agent   s new state as
   well as the environment   s new state. formally this s a markov decision
   process or (mdp) (next blog post).

   on the other hand, there is the notion of partially observable
   environments, where the agent indirectly observes the environment, e.g.
   a robot with camera vision isn   t told its exact location, all that the
   agent knows is what lies in front of it. now the agent state     the
   environment state. formally, this is a partially observable markov
   decision process or (pomdp).

   in pomdp, the agent must find a way to construct its own state
   representation. as we mentioned before, we could use the complete
   history to construct the current state, but this is a naive approach.

   instead we could use agent   s belief of the environment, i.e. where the
   agent thinks it   s in the environment,
   [1*l04r5jdzn5v3dkmkai5t0w.png]

   this is a bayesian approach, where there is a id203 distribution
   over where the agent thinks it   s in the environment.

   we could use the recurrent neural network to construct the current
   state of the agent,
   [1*htsr7e1qmej6mqwzgeiemg.png]

   where there is a linear combination of the previous state multiplied by
   some weight, and the current observation multiplied by some other
   weight. we pass this linear combination through some non-linearity, and
   that gives us the current state of the agent.
     __________________________________________________________________

     components of a id23 agent.

   a id23 agent may include one or more of these
   components:

   a policy,

   it   s a id203 distribution over actions given states, i.e. the
   agent   s behavior function or how the agent picks his actions given that
   it   s in some certain state. it could be a deterministic policy that we
   want to learn from experience,
   [1*9mm5fuqmjcsqluvwqwm7aa.png]

   or a stochastic policy,
   [1*piihmdwgf2qvbq-rzucpfg.png]

   a value function,

   it   s a function that tells us how good is each state and/or action,
   i.e. how good is it to be in a particular state, and how good is it to
   take a particular action. it informs the agent of how much reward to
   expect if it takes a particular action in a particular state.

   in short, it   s a prediction of expected future rewards used to evaluate
   goodness/badness of states, therefore enabling the agent to select
   between different actions,
   [1*utcqvuv4wc2g_naqjfshjw.png]

   in some state s, and time step t, the value function informs the agent
   of the expected sum of future rewards on a given policy     , so as to
   choose the right action that maximizes that expected sum of rewards.
   the value function depends on the way the agent is behaving.

        is a discount factor, where          [0, 1]. it informs the agent of how
   much it should care about rewards now to rewards in the future.

   if (     = 0), that means the agent is short-sighted, i.e. it only cares
   about the first reward. if (     = 1), that means the agent is
   far-sighted, i.e. it cares about all future rewards.

   a model,

   a model predicts what the environment will do next. it   s the agent   s
   representation of the environment, i.e. how the agent thinks the
   environment works.

   there is a transition function p, which predicts the next state or the
   dynamics of the environment,
   [1*qgtdlewbyx92usinqg8voq.png]

   it tells us the id203 distribution over next possible successor
   states, given the current state and the action taken by the agent. we
   can try to learn these dynamics.

   there is a reward function r, which predicts the next immediate reward
   associated with the taken action, given the current state;
   [1*v_2vth91riykkca-jsq2gq.png]
     __________________________________________________________________

     categorizing id23 agents.

     * value based agent, the agent will evaluate all the states in the
       state space, and the policy will be kind of implicit, i.e. the
       value function tells the agent how good is each action in a
       particular state and the agent will choose the best one.
     * policy based agent, instead of representing the value function
       inside the agent, we explicitly represent the policy. the agent
       searches for the optimal action-value function which in turn will
       enable it to act optimally.
     * actor-critic agent, this agent is a value-based and policy-based
       agent. it   s an agent that stores both of the policy, and how much
       reward it is getting from each state.
     * model-based agent, the agent tries to build a model of how the
       environment works, and then plan to get the best possible behavior.
     * model-free agent, here the agent doesn   t try to understand the
       environment, i.e. it doesn   t try to build the dynamics. instead we
       go directly to the policy and/or value function. we just see
       experience and try to figure out a policy of how to behave
       optimally to get the most possible rewards.

   make sure you give this post 10 claps and my blog a follow if you
   enjoyed this post and want to see more!

     references.

   [24]richard sutton   s intro to id23

     * [25]id23
     * [26]artificial intelligence

   (button)
   (button)
   (button) 1.8k claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of mohammad ashraf

[28]mohammad ashraf

   a computer engineering student. geek about ai and reinforcement
   learning. twitter: [29]@mhmdelsersy, github: neo-47

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.8k
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/36c39c11ec14
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_il1scivzwh5l---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@m.elsersy96?source=post_header_lockup
  17. https://towardsdatascience.com/@m.elsersy96
  18. https://towardsdatascience.com/media/002f359af31d1164c9fdfe21653dde2e?postid=36c39c11ec14
  19. https://deepmind.com/
  20. http://www.mujoco.org/
  21. https://towardsdatascience.com/media/b3ee80bb0523a0309521595fdd969c99?postid=36c39c11ec14
  22. https://en.wikipedia.org/wiki/lee_sedol
  23. https://towardsdatascience.com/media/1b388b74424674a7e9d562239584576a?postid=36c39c11ec14
  24. http://incompleteideas.net/book/bookdraft2017nov5.pdf
  25. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  26. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  27. https://towardsdatascience.com/@m.elsersy96?source=footer_card
  28. https://towardsdatascience.com/@m.elsersy96
  29. http://twitter.com/mhmdelsersy
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/36c39c11ec14/share/twitter
  36. https://medium.com/p/36c39c11ec14/share/facebook
  37. https://medium.com/p/36c39c11ec14/share/twitter
  38. https://medium.com/p/36c39c11ec14/share/facebook
