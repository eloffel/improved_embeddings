   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id23 demystified: id100 (part 1)

episode 2, demystifying markov processes, markov reward processes, bellman
equation, and id100.

   [16]go to the profile of mohammad ashraf
   [17]mohammad ashraf (button) blockedunblock (button) followfollowing
   apr 10, 2018

   in the previous [18]blog post we talked about id23
   and its characteristics. we mentioned the process of the agent
   observing the environment output consisting of a reward and the next
   state, and then acting upon that. this whole process is a markov
   decision process or an mdp for short.

   this blog post is a bit mathy. grab your coffee and a comfortable
   chair, and just dive in.
   [1*quboz2yq5fy6ynzyvspxzw.png]
   a markov decision process

   mdps are meant to be a straightforward framing of the problem of
   learning from interaction to achieve a goal. the agent and the
   environment interact continually, the agent selecting actions and the
   environment responding to these actions and presenting new situations
   to the agent.

   formally, an mdp is used to describe an environment for reinforcement
   learning, where the environment is fully observable. almost all rl
   problems can be formalized as mdps.
     __________________________________________________________________

what is exactly a markov decision process ?

   to understand the mdp, first we have to understand the markov property.

     the markov property

   the markov property states that,

         the future is independent of the past given the present.   

   once the current state in known, the history of information encountered
   so far may be thrown away, and that state is a sufficient statistic
   that gives us the same characterization of the future as if we have all
   the history.

   in mathematical terms, a state st has the markov property, if and only
   if;

   p[st+1 | st] = p[st+1 | s1,    .. , st],

   the state captures all relevant information from history.

   for a markov state s and successor state s   , the state transition
   id203 function is defined by,
   [1*mttgdzm6xvbmdbjsrdzsia.png]

   it   s a id203 distribution over next possible successor states,
   given current state, i.e. the agent is in some state, there is a
   id203 to go to the first state, and another id203 to go to
   the second state and so on.

   we can put this transition function in the form of a matrix, where each
   row sums to 1,
   [1*gwitxdi2ioias0un7g4w-w.png]
     __________________________________________________________________

     markov process

   a markov process is a memory-less random process, i.e. a sequence of
   random states s1, s2,    .. with the markov property. a markov process or
   markov chain is a tuple (s, p) on state space s, and transition
   function p. the dynamics of the system can be defined by these two
   components s and p. when we sample from an mdp, it   s basically a
   sequence of states or as we call it an episode.
   [1*rcko3p9x0rdv3v57rfckig.png]
   student mrp from david silver class.

   here, we have different states with different successors, e.g. in class
   1 you might go to class 2 with id203 0.5 or facebook with
   id203 0.5. an episode would be for example [class 1    class 2    
   class 3     pass    sleep]. sleep is the terminal state or absorbing state
   that terminates an episode.
     __________________________________________________________________

     markov reward process

   a markov reward process or an mrp is a markov process with value
   judgment, saying how much reward accumulated through some particular
   sequence that we sampled.

   an mrp is a tuple (s, p, r,     ) where s is a finite state space, p is
   the state transition id203 function, r is a reward function
   where,

   rs =     [rt+1 | st = s],

   it says how much immediate reward we expect to get from state s at the
   moment.

   there is the notion of the return gt, which is the total discounted
   rewards from time step t. this is what we care about, the goal is to
   maximize this return,
   [1*k_gl6jq_7qdli0w_8etinq.png]

        is a discount factor, where          [0, 1]. it informs the agent of how
   much it should care about rewards now to rewards in the future. if (     =
   0), that means the agent is short-sighted, in other words, it only
   cares about the first reward. if (     = 1), that means the agent is
   far-sighted, i.e. it cares about all future rewards.what we care about
   is the total rewards that we   re going to get.

   you might be confused, why put a discounting factor ?. why not get all
   the rewards undiscounted ?. it turns out to be mathematically
   convenient to discount rewards, here we guarantee that the algorithm
   will converge, and avoid infinite returns in loopy markov processes.

   note that, although the return is a sum of an infinite number of terms,
   it is still
   finite if the reward is nonzero and constant, if      < 1. for example, if
   the reward is a constant +1, then the return is,
   [1*avbh-5zyw8z6hu5yixi30w.png]

   another reason to discount rewards is that, the agent is not certain
   about what would happen in the future, it might be better to take the
   immediate reward rather than waiting in the hope to get a larger reward
   in the future, so      defines a kind of finite horizon for what to care
   about.      implicitly encoded the animal/human cognitive model, which
   shows preferences for immediate rewards.

   bear with me, imagine you are in a situation, where someone offered you
   to get a 1000$ now, or to get 1000$ after each passing hour for 10
   hours, but with each passing hour the value of a 1000$ is decreasing by
   some factor      to the power of the passed hours. as a rational person
   trying to get the maximum possible amount of money, your choice depends
   on     .

   if (     = 0.1), with simple calculation, your return would be,

   gt = 1000 + 100 + 10+    ..,

   on the other hand, if (     = 0.9), the return would be,

   gt = 1000 + 900 + 810+       

   notice that, if the decreasing factor is near zero, you   ll care only
   about first hour or 2 hours, and stop wasting your time and just leave,
   i.e. get to the terminal state, all that comes afterwards is worthless.
   if      is near 1, you will wait the 10 hours to get the money, i.e. still
   taking actions to get rewards. so      defined a horizon for acting in the
   environment.

   sometimes, it   s possible to use undiscounted mrps (i.e.      = 1), if we
   are sure that all sequences will terminate

   as we mentioned in the previous blog post, the value function tells us
   how good is each state and/or action, i.e. how good is it to be in a
   particular state, and how good is it to take a particular action. it
   informs the agent of how much reward to expect if it takes a particular
   action in a particular state.

   the state-value function of an mrp is the expected return starting from
   state s,
   [1*qgsyixmk1bc-akbq-riosa.png]

   e.g. the return of the previously mentioned episode [class 1    class 2    
   class 3     pass    sleep] would be,
   [1*azjlv77t-axml8m31vtzha.png]

   the value of state class 1 would be = -2.25.
     __________________________________________________________________

     bellman equation

   the agent tries to get the most expected sum of rewards from every
   state it lands in. in order to achieve that we must try to get the
   optimal value function, i.e. the maximum sum of cumulative rewards.
   [19]bellman equation will help us to do so.

   using bellman equation, the value function will be decomposed into two
   part; an immediate reward, rt+1, and discounted value of the successor
   state     v(st+1),
   [1*wmzewxcoaqyi2lfkd2sn8q.png]

   we unroll the return gt,
   [1*2lkgfixczmolq22nmij-wg.png]

   then substitute the return gt+1, starting from time step t+1,
   [1*xyj-fgruihnrlm7rbujopg.png]

   finally, since the expected value function is a linear function,
   meaning that     (ax+by)= a    (x) +b    (y). the expected value of the return
   gt+1 is the value of the state st+1,
   [1*jqa-x97xissitijitzjfza.png]

   that gives us the bellman equation for mrps,
   [1*0dh3s4ijvcehnglpdoy4lw.png]

   so, for each state in the state space, the bellman equation gives us
   the value of that state,
   [1*f0fvql0pajoscnyp0ujn5a.png]

   the value of the state s is the reward we get upon leaving that state,
   plus a discounted average over next possible successor states, where
   the value of each possible successor state is multiplied by the
   id203 that we land in it.
   [1*fszkxvwzuypftorzbscika.png]
   student mrp from david silver class.

   for our example state space, the value of class 3 is the reward -2 that
   we get upon leaving that state added to the discounted average over
   next possible successor states.

   this bellman equation is a linear equation, i.e. it can be solved
   directly,
   [1*7essennztowwirdzbwrdng.png]

   the complexity of this computation is o(n  ) for n states. this direct
   solution is only possible for small mrps. for larger mrps, there are
   many iterative methods, e.g. id145, monte-carlo
   evaluation, and temporal-difference learning.
     __________________________________________________________________

     markov decision process

   an mdp is a markov reward process with decisions, it   s an environment
   in which all states are markov. this is what we want to solve. an mdp
   is a tuple (s, a, p, r,     ), where s is our state space, a is a finite
   set of actions, p is the state transition id203 function,
   [1*tu9gqs5dlz2negiw4xngca.png]

   r is the reward function,
   [1*gdiazyeby1onaq_2jhscbg.png]

   and      is a discount factor          [0, 1].
   [1*ytz8cg5yfov_kdjkz6dnjg.png]
   student mdp with actions

   remember that a policy      is a distribution over actions given states. a
   policy fully defines the behavior of an agent,
   [1*wlk7ennbkyet7egtgiahpq.png]

   mdp policies depend on the current state, not the history, i.e.
   policies are stationary at ~     (.|st),     t > 0, which means whenever the
   agent lands in a particular state, it   ll take the action that the
   policy decided before, for all different time steps. policies can be
   stochastic to allow us to do exploration in the state space.

   we can always recover a markov process or an mrp from an mdp. given an
   mdp m = (s, a, p, r,     ), and a policy     , the state sequence s1, s2,    ..
   is a markov process (s, p) on policy     .

   the state and reward sequence s1, r1, s2, r2,        is a markov reward
   process (s, p, r,     ) where,
   [1*ho8yeu6-xzs36harhlafsw.png]

   we   re going to average over all things that might happen under our
   policy     . now we have defined the transition dynamics function on
   policy      to be the average of the transition dynamics of all things
   that we might do.

   there is a id203 to take action a under policy      from state s, we
   multiply the id203 that we take that action by what would happen
   after we took that action in state s, i.e. the successor states we
   might land in.

   the same goes for the reward function,
   [1*8w65h-nzchv3gjkcy3ucoa.png]

   we average over all possible rewards associated with different possible
   actions from state s.
     __________________________________________________________________

   we already have the value function for mrps, but there was no
   decisions. now we got a policy, i.e. some way to choose to behave in a
   markov process.

        the state-value function v    (s) of an mdp is the expected return
     starting from state s, and then following policy     .   

   the value function tells us how good is it to be in state s if i   m
   following policy     , i.e. the expectations when we sample all actions
   according to policy     ,
   [1*hj9nqhgb77qewbd7dsk7ba.png]

        the action-value function q    (s, a) is the expected return starting
     from state s, taking action a, and following policy     .    

   it tells us how good is it to take a particular action from a
   particular state,
   [1*w9rvumgm9vrmgrorx2dg_q.png]
   [1*7uok31fzndiitndbjrwnqw.png]
   state-value function in student mdp

   in part 2, i   ll cover bellman expectation equation, bellman optimality
   equation, and how to get the optimal policy, optimal state-value
   function, and optimal action-value function.

   make sure you give this post 50 claps and my blog a follow if you
   enjoyed this post and want to see more!

     references

   [20]richard sutton   s intro to id23

     * [21]id23
     * [22]artificial intelligence
     * [23]markov chains

   (button)
   (button)
   (button) 3k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of mohammad ashraf

[25]mohammad ashraf

   a computer engineering student. geek about ai and reinforcement
   learning. twitter: [26]@mhmdelsersy, github: neo-47

     (button) follow
   [27]towards data science

[28]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3k
     * (button)
     *
     *

   [29]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [30]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bf00dda41690
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_yyl1dagoiene---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@m.elsersy96?source=post_header_lockup
  17. https://towardsdatascience.com/@m.elsersy96
  18. https://medium.com/@m.elsersy96/reinforcement-learning-demystified-36c39c11ec14
  19. https://en.wikipedia.org/wiki/bellman_equation
  20. http://incompleteideas.net/book/bookdraft2017nov5.pdf
  21. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  22. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  23. https://towardsdatascience.com/tagged/markov-chains?source=post
  24. https://towardsdatascience.com/@m.elsersy96?source=footer_card
  25. https://towardsdatascience.com/@m.elsersy96
  26. http://twitter.com/mhmdelsersy
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/bf00dda41690/share/twitter
  33. https://medium.com/p/bf00dda41690/share/facebook
  34. https://medium.com/p/bf00dda41690/share/twitter
  35. https://medium.com/p/bf00dda41690/share/facebook
