   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id23 demystified: id100 (part 2)

episode 3, demystifying bellman expectation equation, bellman optimality
equation, optimal policy, and optimal value function.

   [16]go to the profile of mohammad ashraf
   [17]mohammad ashraf (button) blockedunblock (button) followfollowing
   apr 20, 2018
   [1*quboz2yq5fy6ynzyvspxzw.png]
   an mdp

   in the previous blog post , [18]id23 demystified:
   id100 (part 1), i explained the markov decision
   process and bellman equation without mentioning how to get the optimal
   policy or optimal value function.

   in this blog post i   ll explain how to get the optimal behavior in an
   mdp, starting with bellman expectation equation.
     __________________________________________________________________

     bellman expectation equation

   as we mentioned before, the state-value function can be decomposed into
   immediate reward rt+1, and discounted value of successor state
       v    (st+1) on policy     ,
   [1*b9su29nqszs4nnjytn4osg.png]

   the idea is wherever you are, you take one step and you get your
   immediate reward for that step, and then you look at the value where
   you end up. the sum of those things together tells you how good it was
   to be in your original state.

   in this case, you start in state s, following policy     , but the value
   being in that state is the immediate reward you get, added to the value
   of the successor state, if you know your are going to follow the policy
        from that state on-wards.

   similarly, the action-value function can be decomposed,
   [1*ctxuxz90echbehei6_eshq.png]

   if i   m in state s, and i take action a from there, i   ll get an
   immediate reward for that action, and then i   ll look at where i end up,
   and i can ask, what is the action-value in the state i end up in under
   the action i   ll pick from that point onwards. the sum of those things
   together tells us how good it was to take that action from that
   particular state.

   since we have multiple actions from one state s, and the policy defines
   a id203 distribution over those actions, we are gonna average,
   and that is the bellman expectation equation,
   [1*n-xdxegwobbwzrx6zogi9q.png]

   from a particular state s, there are multiple actions, i   m gonna
   average over the actions that i might take. there is a id203 that
   i   m gonna take the first action and another id203 that i   m gonna
   take the second action and so on. this id203 distribution is
   defined by a policy     .

   when we reach q (the action value) for the action that we took, it
   tells us how good is it to take that action from that state. averaging
   over possible action-values tells us how good is it to be in state s.
   [1*pszje9fvyyoktdzgezelmw.png]

   after i took that action, i   m committed to it. the environment might
   blow me left or right, since there is stochasticity in the environment.
   i want to ask for each of these situations i might get blown to, how
   good is it?, what is the value of being in that situation following my
   policy onwards.

   so we are gonna average over possible things that might happen, i.e.
   possible successor states the agent might land in, meaning multiplying
   each state value on policy      we might land in by the id203 that
   we land in it. summing all those things together tells us how good it
   was to take that particular action from that particular state s.

   remember v    (s) is telling us how good is it to be in a particular
   state, and q    (s, a) is telling us how good is it to take a particular
   action from a given state.

   stitching bellman expectation equation for v    (s),
   [1*xoomvlcobyze1q-6taptma.png]

   at the root, we got the value function for state s. it tells us how
   good is it to be in that particular state. we consider all the actions
   we might take next and we consider all the things the environment might
   do to us after we took some action. for each of the things the
   environment might do, there are successor states. we might land in one
   of those states.

   we want to know how good is it to be in that state we landed in, and
   carry on with our usual policy, i.e. how much reward i   ll get for
   carrying on from that point onwards. we are gonna average over all
   those together, we   re weighting each of the first two arcs by a
   id203 the policy will select, and weighting each of the second
   level arcs by a id203 the environment will select, and this gives
   us the value of being in the root.

   stitching bellman expectation equation for q    (s),
   [1*2us_ypm-938cbnh8uma2ua.png]

   starting from an action a, we now can look ahead, considering where the
   environment might blow us in after we took that action. first we get
   the immediate reward for our action, and then we average over possible
   states we might land in, i.e. the value of each state we might land in
   multiplied by a id203 the environment will select and average
   over all those things together.

   then consider from the state we   re blown to which action might i take
   next. there is a id203 distribution over possible actions from
   there, and then we average over.
   [1*ufgiiubeud5cmwxlsy2q0w.png]
   an example mdp

   for our state space, we evaluate the state in red. the policy defines a
   uniform id203 distribution for the two possible actions, we   re
   gonna weight each of the things that might happen after taking one
   action by 0.5.

   for the action    study   , we   re gonna weight it by 0.5 multiplied by the
   immediate reward for that action, and since the state we   re gonna land
   in is the terminal state, i.e. it has zero value, the action value will
   be just the id203 of the action multiplied by the reward for that
   action.

   for the second action that we might take    pub   , we   re gonna weight the
   things that might happen after taking that action by the id203
   that we take that action.

   first, we get the immediate reward for that action +1, added to an
   average over possible successor states. there is a chance node that we
   go to some state, or some other state, or return to the state where we
   started.

   we multiply the value of each state we might land in after taking the
   action by the id203 that we land in in, which is defined by the
   environment. the sum of those things together gives us the value of
   being in our original state.

   bellman expectation equation can be expressed concisely using the
   induced mrp,
   [1*q5zd5adruha2qdf2havkkw.png]

   we can solve it directly,
   [1*0rjnfause4r-t7zya4ohjq.png]
     __________________________________________________________________

     optimal value function

     the optimal state-value function v*(s) is the maximum value function
     over all policies.

   [1*pok42uotvkxz7l3ugzjh9w.png]

   it   s the best possible solution of an mdp. of all kinds of different
   policies that we can follow in our markov chain, all we care about is
   the best of them, i.e. what is the maximum possible rewards that we can
   extract from an mdp.

     the optimal action-value function q*(s, a) is the maximum action
     value function over all policies.

   [1*vzjbo6nl1rrik-tplr8hba.png]

   for the state   action pair (s, a), this function gives the expected
   return for taking action a in state s, and thereafter following an
   optimal policy, i.e. the maximum amount of rewards you can extract
   starting in state s, and taking action a.

   if we know q*(s, a), then we   re basically done. it tells us the right
   action to take. the optimal value function specifies the best possible
   performance in the mdp. an mdp is solved when we know the optimal value
   function.
     __________________________________________________________________

     optimal policy

   what is the best possible way to behave in an mdp ?. a policy is just a
   stochastic mapping from states to actions, we want to know what is the
   best one of those things.

   we are gonna define a partial ordering over different policies,
   [1*vy1ea_smeph-qq-fz4e4ca.png]

   one policy is better than another policy if the value function for that
   policy is greater than the value function of the other policy in all
   states. for any markov decision process, there exists an optimal policy
       * that is better than or equal to all other policies,     * >     ,        .

   it   s possible to have more than one optimal policy, e.g. if there are
   two separate actions that take us to the same state, it doesn   t matter
   which one of those we take.

   remember, all optimal policies achieve optimal value function. all
   optimal policies achieve the optimal action-value function.
     __________________________________________________________________

     finding an optimal policy

   an optimal policy can be found by maximizing over q*(s, a), i.e. pick
   the action that gives you the most q*(s, a),
   [1*m8hhjwo8cmrhz1cscmfu-a.png]

   if you are in some state s, you just pick the action a with id203
   1 that maximizes q*(s, a), and that is the action that will give you
   maximum possible rewards.

   there is always a deterministic policy for any mdp. if we know q*(s,
   a), we immediately have the optimal policy.
   [1*7gia3zmwqmzrimz2gifwig.png]
   an exmaple mdp

   here, we have different actions from every state. notice that we pick
   the action that has the maximum value among different actions in a
   given state. that gives us the optimal policy in this state space.
     __________________________________________________________________

     bellman optimality equation

   the optimal value functions are recursively related by the bellman
   optimality equation. bellman optimality equation for v*,
   [1*tdhblxgakg5js70-sssrpa.png]

   what is the optimal value of being in state s ?. we consider each of
   the actions we might take, and an action will take us to one of the
   chance nodes. we look at the action value for each action.

   instead of taking average like in bellman expectation equation, we take
   the maximum of q*(s, a), and that tells us how good is it to be in that
   state s.

   bellman optimality equation for q*,
   [1*l71wmjiwjqeybxo1-bgorw.png]

   we can do one-step lookahead, but lookahead to what the environment
   might do to us, remember we don   t control what the environment might
   do.

   each of the states that we might end up in, has some optimal value, and
   we average over them. we don   t get to pick where the environment might
   blow us in. we have to average over all the things the environment
   might do to us, and that tells us how good our action is.
     __________________________________________________________________

   stitching bellman optimality equation for v*(s),
   [1*-9_cacm_a93cc-272kbmwq.png]

   this is a recursive equation that we can solve. we are looking ahead at
   the action we can take, and maximizing over those actions. we   re also
   looking ahead at the dice the environment might roll, we don   t control
   the dice, and we average over those things together.

   stitching bellman optimality equation for q*(s, a),
   [1*bftq2wrhny0tfs8evcjsfw.png]

   the same goes for optimal action value function, we are already
   committed to the action that we took, so we got the reward associated
   with that action, and then the environment might blow us to different
   states.

   we average over the optimal action values of those states we might land
   in, i.e. the best action that we can take in each of the successor
   states, and that gives us the value of the action that we took.
   [1*tvcya3s5exhjvixmx4vhrg.png]
   an exmaple mdp

   the value of this state is the maximum over the possible two actions.
   we can take the    facebook    action and get a reward -1 added to the
   optimal value of the state we are gonna land in, or take the    study   
   action and get a reward -2 added to the value of the state we are gonna
   land in.

   maximizing over those gives us the maximum expected sum of rewards we
   can get from our original state.
     __________________________________________________________________

     solving the bellman optimality equation

   bellman optimality equation is a non-linear equation, there is no
   closed form solution in general, but there are many iterative solution
   methods that we can apply, e.g. value iteration, policy iteration,
   q-leaning, and sarsa.

   in the next blog post, i   ll discuss how to solve the bellman optimality
   equation using id145, with code examples, and that will
   enable us to do planning in an mdp.
     __________________________________________________________________

     the intuition behind bellman equation

   remember in the [19]first blog post when we talked about atari games
   and how an rl agent learned to play breakout like any human being after
   400 training episodes.

   in this example the q* and v* are telling the agent what is the maximum
   amount of score it can get. imagine that you are the agent, you got the
   screen, and you ask what is the maximum score i can get from that
   screen ?.

   the principle of optimality, discussed later, tells us how to get the
   maximum score, we have to behave optimally for one step, and then
   behave optimally the remainder of the trajectory.

   now all we need to do is to figure out how to behave optimally for one
   step, and the way to behave optimally for one step is to maximize over
   those optimal value functions in the places we might end up in.

   the intuition is that, just by breaking down the trajectory into those
   two parts; the optimal decision for one step, and the optimal decision
   from now onwards, we can describe what it means to have optimal
   dynamics to the whole problem.

   make sure you give this post 50 claps, leave a response expressing your
   thoughts, and give my blog a follow if you enjoyed this post and want
   to see more!

     previous episodes

     * [20]id23: a gentle introduction
     * [21]markov decision process: (part 1)

     references

   [22]richard sutton   s intro to id23

     * [23]id23
     * [24]markov chains
     * [25]artificial intelligence

   (button)
   (button)
   (button) 867 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of mohammad ashraf

[27]mohammad ashraf

   a computer engineering student. geek about ai and reinforcement
   learning. twitter: [28]@mhmdelsersy, github: neo-47

     (button) follow
   [29]towards data science

[30]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 867
     * (button)
     *
     *

   [31]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [32]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b209e8617c5a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-2-b209e8617c5a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-2-b209e8617c5a&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_znjims58lokq---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@m.elsersy96?source=post_header_lockup
  17. https://towardsdatascience.com/@m.elsersy96
  18. https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690
  19. https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14
  20. https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14
  21. https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690
  22. http://incompleteideas.net/book/bookdraft2017nov5.pdf
  23. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  24. https://towardsdatascience.com/tagged/markov-chains?source=post
  25. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  26. https://towardsdatascience.com/@m.elsersy96?source=footer_card
  27. https://towardsdatascience.com/@m.elsersy96
  28. http://twitter.com/mhmdelsersy
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/b209e8617c5a/share/twitter
  35. https://medium.com/p/b209e8617c5a/share/facebook
  36. https://medium.com/p/b209e8617c5a/share/twitter
  37. https://medium.com/p/b209e8617c5a/share/facebook
