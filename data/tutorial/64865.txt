   #[1]github [2]recent commits to pytorch-gan:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]106
     * [35]star [36]2,759
     * [37]fork [38]687

[39]eriklindernoren/[40]pytorch-gan

   [41]code [42]issues 6 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   pytorch implementations of id3.
     * [47]179 commits
     * [48]1 branch
     * [49]0 releases
     * [50]6 contributors
     * [51]mit

    1. [52]python 99.6%
    2. [53]shell 0.4%

   (button) python shell
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/e
   [55]download zip

downloading...

   want to be notified of new releases in eriklindernoren/pytorch-gan?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@eriklindernoren
   [63]eriklindernoren [64]bicyclegan: revert to edges2shoes as default
   dataset
   latest commit [65]1f130df apr 2, 2019
   [66]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [67]assets [68]bicyclegan: sample in readme, bn etc. apr 2, 2019
   [69]data [70]figures apr 23, 2018
   [71]implementations [72]bicyclegan: revert to edges2shoes as default
   dataset apr 2, 2019
   [73].gitignore [74]mnist id172. black refactoring. mar 28, 2019
   [75]license [76]initial commit apr 21, 2018
   [77]readme.md [78]update readme.md apr 2, 2019
   [79]requirements.txt [80]added pytorch 0.4.0 to requirements may 24,
   2018

readme.md

                               [81][logo.png]

pytorch-gan

   collection of pytorch implementations of generative adversarial network
   varieties presented in research papers. model architectures will not
   always mirror the ones proposed in the papers, but i have chosen to
   focus on getting the core ideas covered instead of getting every layer
   configuration right. contributions and suggestions of gans to implement
   are very welcomed.

   see also: [82]keras-gan

table of contents

     * [83]installation
     * [84]implementations
          + [85]auxiliary classifier gan
          + [86]adversarial autoencoder
          + [87]began
          + [88]bicyclegan
          + [89]boundary-seeking gan
          + [90]conditional gan
          + [91]context-conditional gan
          + [92]context encoder
          + [93]coupled gan
          + [94]cyclegan
          + [95]deep convolutional gan
          + [96]discogan
          + [97]dragan
          + [98]dualgan
          + [99]energy-based gan
          + [100]gan
          + [101]infogan
          + [102]least squares gan
          + [103]munit
          + [104]pix2pix
          + [105]pixelda
          + [106]semi-supervised gan
          + [107]softmax gan
          + [108]stargan
          + [109]super-resolution gan
          + [110]unit
          + [111]wasserstein gan
          + [112]wasserstein gan gp
          + [113]wasserstein gan div

installation

$ git clone https://github.com/eriklindernoren/pytorch-gan
$ cd pytorch-gan/
$ sudo pip3 install -r requirements.txt

implementations

auxiliary classifier gan

   auxiliary classifier generative adversarial network

authors

   augustus odena, christopher olah, jonathon shlens

abstract

   synthesizing high resolution photorealistic images has been a
   long-standing challenge in machine learning. in this paper we introduce
   new methods for the improved training of generative adversarial
   networks (gans) for image synthesis. we construct a variant of gans
   employing label conditioning that results in 128x128 resolution image
   samples exhibiting global coherence. we expand on previous work for
   image quality assessment to provide two new analyses for assessing the
   discriminability and diversity of samples from class-conditional image
   synthesis models. these analyses demonstrate that high resolution
   samples provide class information not present in low resolution
   samples. across 1000 id163 classes, 128x128 samples are more than
   twice as discriminable as artificially resized 32x32 samples. in
   addition, 84.7% of the classes have samples exhibiting diversity
   comparable to real id163 data.

   [114][paper] [115][code]

run example

$ cd implementations/acgan/
$ python3 acgan.py

                              [116][acgan.gif]

adversarial autoencoder

   adversarial autoencoder

authors

   alireza makhzani, jonathon shlens, navdeep jaitly, ian goodfellow,
   brendan frey

abstract

   n this paper, we propose the "adversarial autoencoder" (aae), which is
   a probabilistic autoencoder that uses the recently proposed generative
   adversarial networks (gan) to perform variational id136 by matching
   the aggregated posterior of the hidden code vector of the autoencoder
   with an arbitrary prior distribution. matching the aggregated posterior
   to the prior ensures that generating from any part of prior space
   results in meaningful samples. as a result, the decoder of the
   adversarial autoencoder learns a deep generative model that maps the
   imposed prior to the data distribution. we show how the adversarial
   autoencoder can be used in applications such as semi-supervised
   classification, disentangling style and content of images, unsupervised
   id91, id84 and data visualization. we
   performed experiments on mnist, street view house numbers and toronto
   face datasets and show that adversarial autoencoders achieve
   competitive results in generative modeling and semi-supervised
   classification tasks.

   [117][paper] [118][code]

run example

$ cd implementations/aae/
$ python3 aae.py

began

   began: boundary equilibrium id3

authors

   david berthelot, thomas schumm, luke metz

abstract

   we propose a new equilibrium enforcing method paired with a loss
   derived from the wasserstein distance for training auto-encoder based
   id3. this method balances the generator and
   discriminator during training. additionally, it provides a new
   approximate convergence measure, fast and stable training and high
   visual quality. we also derive a way of controlling the trade-off
   between image diversity and visual quality. we focus on the image
   generation task, setting a new milestone in visual quality, even at
   higher resolutions. this is achieved while using a relatively simple
   model architecture and a standard training procedure.

   [119][paper] [120][code]

run example

$ cd implementations/began/
$ python3 began.py

bicyclegan

   toward multimodal image-to-image translation

authors

   jun-yan zhu, richard zhang, deepak pathak, trevor darrell, alexei a.
   efros, oliver wang, eli shechtman

abstract

   many image-to-image translation problems are ambiguous, as a single
   input image may correspond to multiple possible outputs. in this work,
   we aim to model a \emph{distribution} of possible outputs in a
   conditional generative modeling setting. the ambiguity of the mapping
   is distilled in a low-dimensional latent vector, which can be randomly
   sampled at test time. a generator learns to map the given input,
   combined with this latent code, to the output. we explicitly encourage
   the connection between output and the latent code to be invertible.
   this helps prevent a many-to-one mapping from the latent code to the
   output during training, also known as the problem of mode collapse, and
   produces more diverse results. we explore several variants of this
   approach by employing different training objectives, network
   architectures, and methods of injecting the latent code. our proposed
   method encourages bijective consistency between the latent encoding and
   output modes. we present a systematic comparison of our method and
   other variants on both perceptual realism and diversity.

   [121][paper] [122][code]

                     [123][bicyclegan_architecture.jpg]

run example

$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/bicyclegan/
$ python3 bicyclegan.py

                            [124][bicyclegan.png]

           various style translations by varying the latent code.

boundary-seeking gan

   boundary-seeking id3

authors

   r devon hjelm, athul paul jacob, tong che, adam trischler, kyunghyun
   cho, yoshua bengio

abstract

   id3 (gans) are a learning framework that
   rely on training a discriminator to estimate a measure of difference
   between a target and generated distributions. gans, as normally
   formulated, rely on the generated samples being completely
   differentiable w.r.t. the generative parameters, and thus do not work
   for discrete data. we introduce a method for training gans with
   discrete data that uses the estimated difference measure from the
   discriminator to compute importance weights for generated samples, thus
   providing a policy gradient for training the generator. the importance
   weights have a strong connection to the decision boundary of the
   discriminator, and we call our method boundary-seeking gans (bgans). we
   demonstrate the effectiveness of the proposed algorithm with discrete
   image and character-based id86. in addition, the
   boundary-seeking objective extends to continuous data, which can be
   used to improve stability of training, and we demonstrate this on
   celeba, large-scale scene understanding (lsun) bedrooms, and id163
   without conditioning.

   [125][paper] [126][code]

run example

$ cd implementations/bgan/
$ python3 bgan.py

conditional gan

   conditional generative adversarial nets

authors

   mehdi mirza, simon osindero

abstract

   generative adversarial nets [8] were recently introduced as a novel way
   to train generative models. in this work we introduce the conditional
   version of generative adversarial nets, which can be constructed by
   simply feeding the data, y, we wish to condition on to both the
   generator and discriminator. we show that this model can generate mnist
   digits conditioned on class labels. we also illustrate how this model
   could be used to learn a multi-modal model, and provide preliminary
   examples of an application to image tagging in which we demonstrate how
   this approach can generate descriptive tags which are not part of
   training labels.

   [127][paper] [128][code]

run example

$ cd implementations/cgan/
$ python3 cgan.py

                               [129][cgan.gif]

context-conditional gan

   semi-supervised learning with context-conditional generative
   adversarial networks

authors

   emily denton, sam gross, rob fergus

abstract

   we introduce a simple semi-supervised learning approach for images
   based on in-painting using an adversarial loss. images with random
   patches removed are presented to a generator whose task is to fill in
   the hole, based on the surrounding pixels. the in-painted images are
   then presented to a discriminator network that judges if they are real
   (unaltered training images) or not. this task acts as a regularizer for
   standard supervised training of the discriminator. using our approach
   we are able to directly train large vgg-style networks in a
   semi-supervised fashion. we evaluate on stl-10 and pascal datasets,
   where our approach obtains performance comparable or superior to
   existing methods.

   [130][paper] [131][code]

run example

$ cd implementations/id35an/
$ python3 id35an.py

context encoder

   context encoders: id171 by inpainting

authors

   deepak pathak, philipp krahenbuhl, jeff donahue, trevor darrell, alexei
   a. efros

abstract

   we present an unsupervised visual id171 algorithm driven by
   context-based pixel prediction. by analogy with auto-encoders, we
   propose context encoders -- a convolutional neural network trained to
   generate the contents of an arbitrary image region conditioned on its
   surroundings. in order to succeed at this task, context encoders need
   to both understand the content of the entire image, as well as produce
   a plausible hypothesis for the missing part(s). when training context
   encoders, we have experimented with both a standard pixel-wise
   reconstruction loss, as well as a reconstruction plus an adversarial
   loss. the latter produces much sharper results because it can better
   handle multiple modes in the output. we found that a context encoder
   learns a representation that captures not just appearance but also the
   semantics of visual structures. we quantitatively demonstrate the
   effectiveness of our learned features for id98 pre-training on
   classification, detection, and segmentation tasks. furthermore, context
   encoders can be used for semantic inpainting tasks, either stand-alone
   or as initialization for non-parametric methods.

   [132][paper] [133][code]

run example

$ cd implementations/context_encoder/
<follow steps at the top of context_encoder.py>
$ python3 context_encoder.py

                         [134][context_encoder.png]

     rows: masked | inpainted | original | masked | inpainted | original

coupled gan

   coupled id3

authors

   ming-yu liu, oncel tuzel

abstract

   we propose coupled generative adversarial network (cogan) for learning
   a joint distribution of multi-domain images. in contrast to the
   existing approaches, which require tuples of corresponding images in
   different domains in the training set, cogan can learn a joint
   distribution without any tuple of corresponding images. it can learn a
   joint distribution with just samples drawn from the marginal
   distributions. this is achieved by enforcing a weight-sharing
   constraint that limits the network capacity and favors a joint
   distribution solution over a product of marginal distributions one. we
   apply cogan to several joint distribution learning tasks, including
   learning a joint distribution of color and depth images, and learning a
   joint distribution of face images with different attributes. for each
   task it successfully learns the joint distribution without any tuple of
   corresponding images. we also demonstrate its applications to domain
   adaptation and image transformation.

   [135][paper] [136][code]

run example

$ cd implementations/cogan/
$ python3 cogan.py

                              [137][cogan.gif]

                     generated mnist and mnist-m images

cyclegan

   unpaired image-to-image translation using cycle-consistent adversarial
   networks

authors

   jun-yan zhu, taesung park, phillip isola, alexei a. efros

abstract

   image-to-image translation is a class of vision and graphics problems
   where the goal is to learn the mapping between an input image and an
   output image using a training set of aligned image pairs. however, for
   many tasks, paired training data will not be available. we present an
   approach for learning to translate an image from a source domain x to a
   target domain y in the absence of paired examples. our goal is to learn
   a mapping g:x   y such that the distribution of images from g(x) is
   indistinguishable from the distribution y using an adversarial loss.
   because this mapping is highly under-constrained, we couple it with an
   inverse mapping f:y   x and introduce a cycle consistency loss to push
   f(g(x))   x (and vice versa). qualitative results are presented on
   several tasks where paired training data does not exist, including
   collection style transfer, object transfiguration, season transfer,
   photo enhancement, etc. quantitative comparisons against several prior
   methods demonstrate the superiority of our approach.

   [138][paper] [139][code]

   [140][687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732
                         f6379636c6567616e2e706e67]

run example

$ cd data/
$ bash download_cyclegan_dataset.sh monet2photo
$ cd ../implementations/cyclegan/
$ python3 cyclegan.py --dataset_name monet2photo

                             [141][cyclegan.png]

                        monet to photo translations.

deep convolutional gan

   deep convolutional generative adversarial network

authors

   alec radford, luke metz, soumith chintala

abstract

   in recent years, supervised learning with convolutional networks (id98s)
   has seen huge adoption in id161 applications. comparatively,
   unsupervised learning with id98s has received less attention. in this
   work we hope to help bridge the gap between the success of id98s for
   supervised learning and unsupervised learning. we introduce a class of
   id98s called deep convolutional id3
   (dcgans), that have certain architectural constraints, and demonstrate
   that they are a strong candidate for unsupervised learning. training on
   various image datasets, we show convincing evidence that our deep
   convolutional adversarial pair learns a hierarchy of representations
   from object parts to scenes in both the generator and discriminator.
   additionally, we use the learned features for novel tasks -
   demonstrating their applicability as general image representations.

   [142][paper] [143][code]

run example

$ cd implementations/dcgan/
$ python3 dcgan.py

                              [144][dcgan.gif]

discogan

   learning to discover cross-domain relations with generative adversarial
   networks

authors

   taeksoo kim, moonsu cha, hyunsoo kim, jung kwon lee, jiwon kim

abstract

   while humans easily recognize relations between data from different
   domains without any supervision, learning to automatically discover
   them is in general very challenging and needs many ground-truth pairs
   that illustrate the relations. to avoid costly pairing, we address the
   task of discovering cross-domain relations given unpaired data. we
   propose a method based on id3 that learns
   to discover relations between different domains (discogan). using the
   discovered relations, our proposed network successfully transfers style
   from one domain to another while preserving key attributes such as
   orientation and face identity.

   [145][paper] [146][code]

   [147][687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732
            f646973636f67616e5f6172636869746563747572652e706e67]

run example

$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/discogan/
$ python3 discogan.py --dataset_name edges2shoes

                             [148][discogan.png]

    rows from top to bottom: (1) real image from domain a (2) translated
                                 image from
     domain a (3) reconstructed image from domain a (4) real image from
                                domain b (5)
    translated image from domain b (6) reconstructed image from domain b

dragan

   on convergence and stability of gans

authors

   naveen kodali, jacob abernethy, james hays, zsolt kira

abstract

   we propose studying gan training dynamics as regret minimization, which
   is in contrast to the popular view that there is consistent
   minimization of a divergence between real and generated distributions.
   we analyze the convergence of gan training from this new point of view
   to understand why mode collapse happens. we hypothesize the existence
   of undesirable local equilibria in this non-convex game to be
   responsible for mode collapse. we observe that these local equilibria
   often exhibit sharp gradients of the discriminator function around some
   real data points. we demonstrate that these degenerate local equilibria
   can be avoided with a gradient penalty scheme called dragan. we show
   that dragan enables faster training, achieves improved stability with
   fewer mode collapses, and leads to generator networks with better
   modeling performance across a variety of architectures and objective
   functions.

   [149][paper] [150][code]

run example

$ cd implementations/dragan/
$ python3 dragan.py

dualgan

   dualgan: unsupervised dual learning for image-to-image translation

authors

   zili yi, hao zhang, ping tan, minglun gong

abstract

   conditional id3 (gans) for cross-domain
   image-to-image translation have made much progress recently. depending
   on the task complexity, thousands to millions of labeled image pairs
   are needed to train a conditional gan. however, human labeling is
   expensive, even impractical, and large quantities of data may not
   always be available. inspired by dual learning from natural language
   translation, we develop a novel dual-gan mechanism, which enables image
   translators to be trained from two sets of unlabeled images from two
   domains. in our architecture, the primal gan learns to translate images
   from domain u to those in domain v, while the dual gan learns to invert
   the task. the closed loop made by the primal and dual tasks allows
   images from either domain to be translated and then reconstructed.
   hence a id168 that accounts for the reconstruction error of
   images can be used to train the translators. experiments on multiple
   image translation tasks with unlabeled data show considerable
   performance gain of dualgan over a single gan. for some tasks, dualgan
   can even achieve comparable or slightly better results than conditional
   gan trained on fully labeled data.

   [151][paper] [152][code]

run example

$ cd data/
$ bash download_pix2pix_dataset.sh facades
$ cd ../implementations/dualgan/
$ python3 dualgan.py --dataset_name facades

energy-based gan

   energy-based generative adversarial network

authors

   junbo zhao, michael mathieu, yann lecun

abstract

   we introduce the "energy-based generative adversarial network" model
   (ebgan) which views the discriminator as an energy function that
   attributes low energies to the regions near the data manifold and
   higher energies to other regions. similar to the probabilistic gans, a
   generator is seen as being trained to produce contrastive samples with
   minimal energies, while the discriminator is trained to assign high
   energies to these generated samples. viewing the discriminator as an
   energy function allows to use a wide variety of architectures and loss
   functionals in addition to the usual binary classifier with logistic
   output. among them, we show one instantiation of ebgan framework as
   using an auto-encoder architecture, with the energy being the
   reconstruction error, in place of the discriminator. we show that this
   form of ebgan exhibits more stable behavior than regular gans during
   training. we also show that a single-scale architecture can be trained
   to generate high-resolution images.

   [153][paper] [154][code]

run example

$ cd implementations/ebgan/
$ python3 ebgan.py

gan

   generative adversarial network

authors

   ian j. goodfellow, jean pouget-abadie, mehdi mirza, bing xu, david
   warde-farley, sherjil ozair, aaron courville, yoshua bengio

abstract

   we propose a new framework for estimating generative models via an
   adversarial process, in which we simultaneously train two models: a
   generative model g that captures the data distribution, and a
   discriminative model d that estimates the id203 that a sample
   came from the training data rather than g. the training procedure for g
   is to maximize the id203 of d making a mistake. this framework
   corresponds to a minimax two-player game. in the space of arbitrary
   functions g and d, a unique solution exists, with g recovering the
   training data distribution and d equal to 1/2 everywhere. in the case
   where g and d are defined by multilayer id88s, the entire system
   can be trained with id26. there is no need for any markov
   chains or unrolled approximate id136 networks during either
   training or generation of samples. experiments demonstrate the
   potential of the framework through qualitative and quantitative
   evaluation of the generated samples.

   [155][paper] [156][code]

run example

$ cd implementations/gan/
$ python3 gan.py

                               [157][gan.gif]

infogan

   infogan: interpretable representation learning by information
   maximizing generative adversarial nets

authors

   xi chen, yan duan, rein houthooft, john schulman, ilya sutskever,
   pieter abbeel

abstract

   this paper describes infogan, an information-theoretic extension to the
   generative adversarial network that is able to learn disentangled
   representations in a completely unsupervised manner. infogan is a
   generative adversarial network that also maximizes the mutual
   information between a small subset of the latent variables and the
   observation. we derive a lower bound to the mutual information
   objective that can be optimized efficiently, and show that our training
   procedure can be interpreted as a variation of the wake-sleep
   algorithm. specifically, infogan successfully disentangles writing
   styles from digit shapes on the mnist dataset, pose from lighting of 3d
   rendered images, and background digits from the central digit on the
   svhn dataset. it also discovers visual concepts that include hair
   styles, presence/absence of eyeglasses, and emotions on the celeba face
   dataset. experiments show that infogan learns interpretable
   representations that are competitive with representations learned by
   existing fully supervised methods.

   [158][paper] [159][code]

run example

$ cd implementations/infogan/
$ python3 infogan.py

                             [160][infogan.gif]

          result of varying categorical latent variable by column.

                             [161][infogan.png]

            result of varying continuous latent variable by row.

least squares gan

   least squares id3

authors

   xudong mao, qing li, haoran xie, raymond y.k. lau, zhen wang, stephen
   paul smolley

abstract

   unsupervised learning with id3 (gans) has
   proven hugely successful. regular gans hypothesize the discriminator as
   a classifier with the sigmoid cross id178 id168. however, we
   found that this id168 may lead to the vanishing gradients
   problem during the learning process. to overcome such a problem, we
   propose in this paper the least squares id3
   (lsgans) which adopt the least squares id168 for the
   discriminator. we show that minimizing the objective function of lsgan
   yields minimizing the pearson   2 divergence. there are two benefits of
   lsgans over regular gans. first, lsgans are able to generate higher
   quality images than regular gans. second, lsgans perform more stable
   during the learning process. we evaluate lsgans on five scene datasets
   and the experimental results show that the images generated by lsgans
   are of better quality than the ones generated by regular gans. we also
   conduct two comparison experiments between lsgans and regular gans to
   illustrate the stability of lsgans.

   [162][paper] [163][code]

run example

$ cd implementations/lsgan/
$ python3 lsgan.py

munit

   multimodal unsupervised image-to-image translation

authors

   xun huang, ming-yu liu, serge belongie, jan kautz

abstract

   unsupervised image-to-image translation is an important and challenging
   problem in id161. given an image in the source domain, the
   goal is to learn the conditional distribution of corresponding images
   in the target domain, without seeing any pairs of corresponding images.
   while this conditional distribution is inherently multimodal, existing
   approaches make an overly simplified assumption, modeling it as a
   deterministic one-to-one mapping. as a result, they fail to generate
   diverse outputs from a given source domain image. to address this
   limitation, we propose a multimodal unsupervised image-to-image
   translation (munit) framework. we assume that the image representation
   can be decomposed into a content code that is domain-invariant, and a
   style code that captures domain-specific properties. to translate an
   image to another domain, we recombine its content code with a random
   style code sampled from the style space of the target domain. we
   analyze the proposed framework and establish several theoretical
   results. extensive experiments with comparisons to the state-of-the-art
   approaches further demonstrates the advantage of the proposed
   framework. moreover, our framework allows users to control the style of
   translation outputs by providing an example style image. code and
   pretrained models are available at [164]this https url

   [165][paper] [166][code]

run example

$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/munit/
$ python3 munit.py --dataset_name edges2shoes

                              [167][munit.png]

                     results by varying the style code.

pix2pix

   unpaired image-to-image translation with conditional adversarial
   networks

authors

   phillip isola, jun-yan zhu, tinghui zhou, alexei a. efros

abstract

   we investigate conditional adversarial networks as a general-purpose
   solution to image-to-image translation problems. these networks not
   only learn the mapping from input image to output image, but also learn
   a id168 to train this mapping. this makes it possible to apply
   the same generic approach to problems that traditionally would require
   very different loss formulations. we demonstrate that this approach is
   effective at synthesizing photos from label maps, reconstructing
   objects from edge maps, and colorizing images, among other tasks.
   indeed, since the release of the pix2pix software associated with this
   paper, a large number of internet users (many of them artists) have
   posted their own experiments with our system, further demonstrating its
   wide applicability and ease of adoption without the need for parameter
   tweaking. as a community, we no longer hand-engineer our mapping
   functions, and this work suggests we can achieve reasonable results
   without hand-engineering our id168s either.

   [168][paper] [169][code]

   [170][687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732
             f706978327069785f6172636869746563747572652e706e67]

run example

$ cd data/
$ bash download_pix2pix_dataset.sh facades
$ cd ../implementations/pix2pix/
$ python3 pix2pix.py --dataset_name facades

                             [171][pix2pix.png]

      rows from top to bottom: (1) the condition for the generator (2)
                               generated image
    based of condition (3) the true corresponding image to the condition

pixelda

   unsupervised pixel-level id20 with generative adversarial
   networks

authors

   konstantinos bousmalis, nathan silberman, david dohan, dumitru erhan,
   dilip krishnan

abstract

   collecting well-annotated image datasets to train modern machine
   learning algorithms is prohibitively expensive for many tasks. one
   appealing alternative is rendering synthetic data where ground-truth
   annotations are generated automatically. unfortunately, models trained
   purely on rendered images often fail to generalize to real images. to
   address this shortcoming, prior work introduced unsupervised domain
   adaptation algorithms that attempt to map representations between the
   two domains or learn to extract features that are domain-invariant. in
   this work, we present a new approach that learns, in an unsupervised
   manner, a transformation in the pixel space from one domain to the
   other. our generative adversarial network (gan)-based method adapts
   source-domain images to appear as if drawn from the target domain. our
   approach not only produces plausible samples, but also outperforms the
   state-of-the-art on a number of unsupervised id20
   scenarios by large margins. finally, we demonstrate that the adaptation
   process generalizes to object classes unseen during training.

   [172][paper] [173][code]

mnist to mnist-m classification

   trains a classifier on images that have been translated from the source
   domain (mnist) to the target domain (mnist-m) using the annotations of
   the source domain images. the classification network is trained jointly
   with the generator network to optimize the generator for both providing
   a proper domain translation and also for preserving the semantics of
   the source domain image. the classification network trained on
   translated images is compared to the naive solution of training a
   classifier on mnist and evaluating it on mnist-m. the naive model
   manages a 55% classification accuracy on mnist-m while the one trained
   during id20 achieves a 95% classification accuracy.
$ cd implementations/pixelda/
$ python3 pixelda.py

   method  accuracy
   naive     55%
   pixelda   95%

                             [174][pixelda.png]

     rows from top to bottom: (1) real images from mnist (2) translated
                                 images from
            mnist to mnist-m (3) examples of images from mnist-m

semi-supervised gan

   semi-supervised generative adversarial network

authors

   augustus odena

abstract

   we extend id3 (gans) to the semi-supervised
   context by forcing the discriminator network to output class labels. we
   train a generative model g and a discriminator d on a dataset with
   inputs belonging to one of n classes. at training time, d is made to
   predict which of n+1 classes the input belongs to, where an extra class
   is added to correspond to the outputs of g. we show that this method
   can be used to create a more data-efficient classifier and that it
   allows for generating higher quality samples than a regular gan.

   [175][paper] [176][code]

run example

$ cd implementations/sgan/
$ python3 sgan.py

softmax gan

   softmax gan

authors

   min lin

abstract

   softmax gan is a novel variant of generative adversarial network (gan).
   the key idea of softmax gan is to replace the classification loss in
   the original gan with a softmax cross-id178 loss in the sample space
   of one single batch. in the adversarial learning of n real training
   samples and m generated samples, the target of discriminator training
   is to distribute all the id203 mass to the real samples, each
   with id203 1m, and distribute zero id203 to generated data.
   in the generator training phase, the target is to assign equal
   id203 to all data points in the batch, each with id203
   1m+n. while the original gan is closely related to noise contrastive
   estimation (nce), we show that softmax gan is the importance sampling
   version of gan. we futher demonstrate with experiments that this simple
   change stabilizes gan training.

   [177][paper] [178][code]

run example

$ cd implementations/softmax_gan/
$ python3 softmax_gan.py

stargan

   stargan: unified id3 for multi-domain
   image-to-image translation

authors

   yunjey choi, minje choi, munyoung kim, jung-woo ha, sunghun kim, jaegul
   choo

abstract

   recent studies have shown remarkable success in image-to-image
   translation for two domains. however, existing approaches have limited
   scalability and robustness in handling more than two domains, since
   different models should be built independently for every pair of image
   domains. to address this limitation, we propose stargan, a novel and
   scalable approach that can perform image-to-image translations for
   multiple domains using only a single model. such a unified model
   architecture of stargan allows simultaneous training of multiple
   datasets with different domains within a single network. this leads to
   stargan's superior quality of translated images compared to existing
   models as well as the novel capability of flexibly translating an input
   image to any desired target domain. we empirically demonstrate the
   effectiveness of our approach on a facial attribute transfer and a
   facial expression synthesis tasks.

   [179][paper] [180][code]

run example

$ cd implementations/stargan/
<follow steps at the top of stargan.py>
$ python3 stargan.py

                             [181][stargan.png]

    original | black hair | blonde hair | brown hair | gender flip | aged

super-resolution gan

   photo-realistic single image super-resolution using a generative
   adversarial network

authors

   christian ledig, lucas theis, ferenc huszar, jose caballero, andrew
   cunningham, alejandro acosta, andrew aitken, alykhan tejani, johannes
   totz, zehan wang, wenzhe shi

abstract

   despite the breakthroughs in accuracy and speed of single image
   super-resolution using faster and deeper convolutional neural networks,
   one central problem remains largely unsolved: how do we recover the
   finer texture details when we super-resolve at large upscaling factors?
   the behavior of optimization-based super-resolution methods is
   principally driven by the choice of the objective function. recent work
   has largely focused on minimizing the mean squared reconstruction
   error. the resulting estimates have high peak signal-to-noise ratios,
   but they are often lacking high-frequency details and are perceptually
   unsatisfying in the sense that they fail to match the fidelity expected
   at the higher resolution. in this paper, we present srgan, a generative
   adversarial network (gan) for image super-resolution (sr). to our
   knowledge, it is the first framework capable of inferring
   photo-realistic natural images for 4x upscaling factors. to achieve
   this, we propose a perceptual id168 which consists of an
   adversarial loss and a content loss. the adversarial loss pushes our
   solution to the natural image manifold using a discriminator network
   that is trained to differentiate between the super-resolved images and
   original photo-realistic images. in addition, we use a content loss
   motivated by perceptual similarity instead of similarity in pixel
   space. our deep residual network is able to recover photo-realistic
   textures from heavily downsampled images on public benchmarks. an
   extensive mean-opinion-score (mos) test shows hugely significant gains
   in perceptual quality using srgan. the mos scores obtained with srgan
   are closer to those of the original high-resolution images than to
   those obtained with any state-of-the-art method.

   [182][paper] [183][code]

   [184][687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732
                      f737570657272657367616e2e706e67]

run example

$ cd implementations/srgan/
<follow steps at the top of srgan.py>
$ python3 srgan.py

                           [185][superresgan.png]

                     nearest neighbor upsampling | srgan

unit

   unsupervised image-to-image translation networks

authors

   ming-yu liu, thomas breuel, jan kautz

abstract

   unsupervised image-to-image translation aims at learning a joint
   distribution of images in different domains by using images from the
   marginal distributions in individual domains. since there exists an
   infinite set of joint distributions that can arrive the given marginal
   distributions, one could infer nothing about the joint distribution
   from the marginal distributions without additional assumptions. to
   address the problem, we make a shared-latent space assumption and
   propose an unsupervised image-to-image translation framework based on
   coupled gans. we compare the proposed framework with competing
   approaches and present high quality image translation results on
   various challenging unsupervised image translation tasks, including
   street scene image translation, animal image translation, and face
   image translation. we also apply the proposed framework to domain
   adaptation and achieve state-of-the-art performance on benchmark
   datasets. code and additional results are available in this [186]https
   url.

   [187][paper] [188][code]

run example

$ cd data/
$ bash download_cyclegan_dataset.sh apple2orange
$ cd implementations/unit/
$ python3 unit.py --dataset_name apple2orange

wasserstein gan

   wasserstein gan

authors

   martin arjovsky, soumith chintala, l  on bottou

abstract

   we introduce a new algorithm named wgan, an alternative to traditional
   gan training. in this new model, we show that we can improve the
   stability of learning, get rid of problems like mode collapse, and
   provide meaningful learning curves useful for debugging and
   hyperparameter searches. furthermore, we show that the corresponding
   optimization problem is sound, and provide extensive theoretical work
   highlighting the deep connections to other distances between
   distributions.

   [189][paper] [190][code]

run example

$ cd implementations/wgan/
$ python3 wgan.py

wasserstein gan gp

   improved training of wasserstein gans

authors

   ishaan gulrajani, faruk ahmed, martin arjovsky, vincent dumoulin, aaron
   courville

abstract

   id3 (gans) are powerful generative models,
   but suffer from training instability. the recently proposed wasserstein
   gan (wgan) makes progress toward stable training of gans, but sometimes
   can still generate only low-quality samples or fail to converge. we
   find that these problems are often due to the use of weight clipping in
   wgan to enforce a lipschitz constraint on the critic, which can lead to
   undesired behavior. we propose an alternative to clipping weights:
   penalize the norm of gradient of the critic with respect to its input.
   our proposed method performs better than standard wgan and enables
   stable training of a wide variety of gan architectures with almost no
   hyperparameter tuning, including 101-layer resnets and language models
   over discrete data. we also achieve high quality generations on
   cifar-10 and lsun bedrooms.

   [191][paper] [192][code]

run example

$ cd implementations/wgan_gp/
$ python3 wgan_gp.py

                             [193][wgan_gp.gif]

wasserstein gan div

   wasserstein divergence for gans

authors

   jiqing wu, zhiwu huang, janine thoma, dinesh acharya, luc van gool

abstract

   in many domains of id161, id3
   (gans) have achieved great success, among which the fam- ily of
   wasserstein gans (wgans) is considered to be state-of-the-art due to
   the theoretical contributions and competitive qualitative performance.
   however, it is very challenging to approximate the k-lipschitz
   constraint required by the wasserstein-1 metric (w-met). in this paper,
   we propose a novel wasserstein divergence (w-div), which is a relaxed
   version of w-met and does not require the k-lipschitz constraint.as a
   concrete application, we introduce a wasserstein divergence objective
   for gans (wgan-div), which can faithfully approximate w-div through
   optimization. under various settings, including progressive growing
   training, we demonstrate the stability of the proposed wgan-div owing
   to its theoretical and practical advantages over wgans. also, we study
   the quantitative and visual performance of wgan-div on standard image
   synthesis benchmarks, showing the superior performance of wgan-div
   compared to the state-of-the-art methods.

   [194][paper] [195][code]

run example

$ cd implementations/wgan_div/
$ python3 wgan_div.py

                             [196][wgan_div.png]

     *    2019 github, inc.
     * [197]terms
     * [198]privacy
     * [199]security
     * [200]status
     * [201]help

     * [202]contact github
     * [203]pricing
     * [204]api
     * [205]training
     * [206]blog
     * [207]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [208]reload to refresh your
   session. you signed out in another tab or window. [209]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/eriklindernoren/pytorch-gan/commits/master.atom
   3. https://github.com/eriklindernoren/pytorch-gan#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/eriklindernoren/pytorch-gan
  32. https://github.com/join
  33. https://github.com/login?return_to=/eriklindernoren/pytorch-gan
  34. https://github.com/eriklindernoren/pytorch-gan/watchers
  35. https://github.com/login?return_to=/eriklindernoren/pytorch-gan
  36. https://github.com/eriklindernoren/pytorch-gan/stargazers
  37. https://github.com/login?return_to=/eriklindernoren/pytorch-gan
  38. https://github.com/eriklindernoren/pytorch-gan/network/members
  39. https://github.com/eriklindernoren
  40. https://github.com/eriklindernoren/pytorch-gan
  41. https://github.com/eriklindernoren/pytorch-gan
  42. https://github.com/eriklindernoren/pytorch-gan/issues
  43. https://github.com/eriklindernoren/pytorch-gan/pulls
  44. https://github.com/eriklindernoren/pytorch-gan/projects
  45. https://github.com/eriklindernoren/pytorch-gan/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/eriklindernoren/pytorch-gan/commits/master
  48. https://github.com/eriklindernoren/pytorch-gan/branches
  49. https://github.com/eriklindernoren/pytorch-gan/releases
  50. https://github.com/eriklindernoren/pytorch-gan/graphs/contributors
  51. https://github.com/eriklindernoren/pytorch-gan/blob/master/license
  52. https://github.com/eriklindernoren/pytorch-gan/search?l=python
  53. https://github.com/eriklindernoren/pytorch-gan/search?l=shell
  54. https://github.com/eriklindernoren/pytorch-gan/find/master
  55. https://github.com/eriklindernoren/pytorch-gan/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/eriklindernoren/pytorch-gan
  57. https://github.com/join?return_to=/eriklindernoren/pytorch-gan
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/eriklindernoren
  63. https://github.com/eriklindernoren/pytorch-gan/commits?author=eriklindernoren
  64. https://github.com/eriklindernoren/pytorch-gan/commit/1f130dfca726e14254e4fd78e5fb63f08931acd3
  65. https://github.com/eriklindernoren/pytorch-gan/commit/1f130dfca726e14254e4fd78e5fb63f08931acd3
  66. https://github.com/eriklindernoren/pytorch-gan/tree/1f130dfca726e14254e4fd78e5fb63f08931acd3
  67. https://github.com/eriklindernoren/pytorch-gan/tree/master/assets
  68. https://github.com/eriklindernoren/pytorch-gan/commit/fc9e5824ad7bd3094f5012dc6fc3d2348481a2f4
  69. https://github.com/eriklindernoren/pytorch-gan/tree/master/data
  70. https://github.com/eriklindernoren/pytorch-gan/commit/df342dd3ecd5270a8810bdaae30c1c6571ac31bc
  71. https://github.com/eriklindernoren/pytorch-gan/tree/master/implementations
  72. https://github.com/eriklindernoren/pytorch-gan/commit/1f130dfca726e14254e4fd78e5fb63f08931acd3
  73. https://github.com/eriklindernoren/pytorch-gan/blob/master/.gitignore
  74. https://github.com/eriklindernoren/pytorch-gan/commit/c5d6be170013db8975d96806084df573d87e5d26
  75. https://github.com/eriklindernoren/pytorch-gan/blob/master/license
  76. https://github.com/eriklindernoren/pytorch-gan/commit/a18bcc2695e95e10420570913816317b1f233438
  77. https://github.com/eriklindernoren/pytorch-gan/blob/master/readme.md
  78. https://github.com/eriklindernoren/pytorch-gan/commit/8817a7fab99d8b13cfe719c5b1da8c610664e6b2
  79. https://github.com/eriklindernoren/pytorch-gan/blob/master/requirements.txt
  80. https://github.com/eriklindernoren/pytorch-gan/commit/8400a6e4ab0eb1a69f641bc568ce56049a2112eb
  81. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/logo.png
  82. https://github.com/eriklindernoren/keras-gan
  83. https://github.com/eriklindernoren/pytorch-gan#installation
  84. https://github.com/eriklindernoren/pytorch-gan#implementations
  85. https://github.com/eriklindernoren/pytorch-gan#auxiliary-classifier-gan
  86. https://github.com/eriklindernoren/pytorch-gan#adversarial-autoencoder
  87. https://github.com/eriklindernoren/pytorch-gan#began
  88. https://github.com/eriklindernoren/pytorch-gan#bicyclegan
  89. https://github.com/eriklindernoren/pytorch-gan#boundary-seeking-gan
  90. https://github.com/eriklindernoren/pytorch-gan#conditional-gan
  91. https://github.com/eriklindernoren/pytorch-gan#context-conditional-gan
  92. https://github.com/eriklindernoren/pytorch-gan#context-encoder
  93. https://github.com/eriklindernoren/pytorch-gan#coupled-gan
  94. https://github.com/eriklindernoren/pytorch-gan#cyclegan
  95. https://github.com/eriklindernoren/pytorch-gan#deep-convolutional-gan
  96. https://github.com/eriklindernoren/pytorch-gan#discogan
  97. https://github.com/eriklindernoren/pytorch-gan#dragan
  98. https://github.com/eriklindernoren/pytorch-gan#dualgan
  99. https://github.com/eriklindernoren/pytorch-gan#energy-based-gan
 100. https://github.com/eriklindernoren/pytorch-gan#gan
 101. https://github.com/eriklindernoren/pytorch-gan#infogan
 102. https://github.com/eriklindernoren/pytorch-gan#least-squares-gan
 103. https://github.com/eriklindernoren/pytorch-gan#munit
 104. https://github.com/eriklindernoren/pytorch-gan#pix2pix
 105. https://github.com/eriklindernoren/pytorch-gan#pixelda
 106. https://github.com/eriklindernoren/pytorch-gan#semi-supervised-gan
 107. https://github.com/eriklindernoren/pytorch-gan#softmax-gan
 108. https://github.com/eriklindernoren/pytorch-gan#stargan
 109. https://github.com/eriklindernoren/pytorch-gan#super-resolution-gan
 110. https://github.com/eriklindernoren/pytorch-gan#unit
 111. https://github.com/eriklindernoren/pytorch-gan#wasserstein-gan
 112. https://github.com/eriklindernoren/pytorch-gan#wasserstein-gan-gp
 113. https://github.com/eriklindernoren/pytorch-gan#wasserstein-gan-div
 114. https://arxiv.org/abs/1610.09585
 115. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/acgan/acgan.py
 116. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/acgan.gif
 117. https://arxiv.org/abs/1511.05644
 118. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/aae/aae.py
 119. https://arxiv.org/abs/1703.10717
 120. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/began/began.py
 121. https://arxiv.org/abs/1711.11586
 122. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/bicyclegan/bicyclegan.py
 123. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/bicyclegan_architecture.jpg
 124. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/bicyclegan.png
 125. https://arxiv.org/abs/1702.08431
 126. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/bgan/bgan.py
 127. https://arxiv.org/abs/1411.1784
 128. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/cgan/cgan.py
 129. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/cgan.gif
 130. https://arxiv.org/abs/1611.06430
 131. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/id35an/id35an.py
 132. https://arxiv.org/abs/1604.07379
 133. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/context_encoder/context_encoder.py
 134. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/context_encoder.png
 135. https://arxiv.org/abs/1606.07536
 136. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/cogan/cogan.py
 137. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/cogan.gif
 138. https://arxiv.org/abs/1703.10593
 139. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/cyclegan/cyclegan.py
 140. https://camo.githubusercontent.com/c653ddc55471557b851a7059540e80799fad7e29/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6379636c6567616e2e706e67
 141. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/cyclegan.png
 142. https://arxiv.org/abs/1511.06434
 143. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/dcgan/dcgan.py
 144. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/dcgan.gif
 145. https://arxiv.org/abs/1703.05192
 146. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/discogan/discogan.py
 147. https://camo.githubusercontent.com/e004d49943b2b1264496d5db1a9bff807afa8a68/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f646973636f67616e5f6172636869746563747572652e706e67
 148. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/discogan.png
 149. https://arxiv.org/abs/1705.07215
 150. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/dragan/dragan.py
 151. https://arxiv.org/abs/1704.02510
 152. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/dualgan/dualgan.py
 153. https://arxiv.org/abs/1609.03126
 154. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/ebgan/ebgan.py
 155. https://arxiv.org/abs/1406.2661
 156. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/gan/gan.py
 157. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/gan.gif
 158. https://arxiv.org/abs/1606.03657
 159. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/infogan/infogan.py
 160. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/infogan.gif
 161. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/infogan.png
 162. https://arxiv.org/abs/1611.04076
 163. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/lsgan/lsgan.py
 164. https://github.com/nvlabs/munit
 165. https://arxiv.org/abs/1804.04732
 166. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/munit/munit.py
 167. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/munit.png
 168. https://arxiv.org/abs/1611.07004
 169. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/pix2pix/pix2pix.py
 170. https://camo.githubusercontent.com/e8c023b62678aa244f1a474bf643c66c45ef0feb/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f706978327069785f6172636869746563747572652e706e67
 171. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/pix2pix.png
 172. https://arxiv.org/abs/1612.05424
 173. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/pixelda/pixelda.py
 174. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/pixelda.png
 175. https://arxiv.org/abs/1606.01583
 176. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/sgan/sgan.py
 177. https://arxiv.org/abs/1704.06191
 178. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/softmax_gan/softmax_gan.py
 179. https://arxiv.org/abs/1711.09020
 180. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/stargan/stargan.py
 181. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/stargan.png
 182. https://arxiv.org/abs/1609.04802
 183. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/srgan/srgan.py
 184. https://camo.githubusercontent.com/07288b4b467fbf547c6757a448f8e786bf20f295/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f737570657272657367616e2e706e67
 185. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/superresgan.png
 186. https://github.com/mingyuliutw/unit
 187. https://arxiv.org/abs/1703.00848
 188. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/unit/unit.py
 189. https://arxiv.org/abs/1701.07875
 190. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/wgan/wgan.py
 191. https://arxiv.org/abs/1704.00028
 192. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/wgan_gp/wgan_gp.py
 193. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/wgan_gp.gif
 194. https://arxiv.org/abs/1712.01026
 195. https://github.com/eriklindernoren/pytorch-gan/blob/master/implementations/wgan_div/wgan_div.py
 196. https://github.com/eriklindernoren/pytorch-gan/blob/master/assets/wgan_div.png
 197. https://github.com/site/terms
 198. https://github.com/site/privacy
 199. https://github.com/security
 200. https://githubstatus.com/
 201. https://help.github.com/
 202. https://github.com/contact
 203. https://github.com/pricing
 204. https://developer.github.com/
 205. https://training.github.com/
 206. https://github.blog/
 207. https://github.com/about
 208. https://github.com/eriklindernoren/pytorch-gan
 209. https://github.com/eriklindernoren/pytorch-gan

   hidden links:
 211. https://github.com/
 212. https://github.com/eriklindernoren/pytorch-gan
 213. https://github.com/eriklindernoren/pytorch-gan
 214. https://github.com/eriklindernoren/pytorch-gan
 215. https://help.github.com/articles/which-remote-url-should-i-use
 216. https://github.com/eriklindernoren/pytorch-gan#pytorch-gan
 217. https://github.com/eriklindernoren/pytorch-gan#table-of-contents
 218. https://github.com/eriklindernoren/pytorch-gan#installation
 219. https://github.com/eriklindernoren/pytorch-gan#implementations
 220. https://github.com/eriklindernoren/pytorch-gan#auxiliary-classifier-gan
 221. https://github.com/eriklindernoren/pytorch-gan#authors
 222. https://github.com/eriklindernoren/pytorch-gan#abstract
 223. https://github.com/eriklindernoren/pytorch-gan#run-example
 224. https://github.com/eriklindernoren/pytorch-gan#adversarial-autoencoder
 225. https://github.com/eriklindernoren/pytorch-gan#authors-1
 226. https://github.com/eriklindernoren/pytorch-gan#abstract-1
 227. https://github.com/eriklindernoren/pytorch-gan#run-example-1
 228. https://github.com/eriklindernoren/pytorch-gan#began
 229. https://github.com/eriklindernoren/pytorch-gan#authors-2
 230. https://github.com/eriklindernoren/pytorch-gan#abstract-2
 231. https://github.com/eriklindernoren/pytorch-gan#run-example-2
 232. https://github.com/eriklindernoren/pytorch-gan#bicyclegan
 233. https://github.com/eriklindernoren/pytorch-gan#authors-3
 234. https://github.com/eriklindernoren/pytorch-gan#abstract-3
 235. https://github.com/eriklindernoren/pytorch-gan#run-example-3
 236. https://github.com/eriklindernoren/pytorch-gan#boundary-seeking-gan
 237. https://github.com/eriklindernoren/pytorch-gan#authors-4
 238. https://github.com/eriklindernoren/pytorch-gan#abstract-4
 239. https://github.com/eriklindernoren/pytorch-gan#run-example-4
 240. https://github.com/eriklindernoren/pytorch-gan#conditional-gan
 241. https://github.com/eriklindernoren/pytorch-gan#authors-5
 242. https://github.com/eriklindernoren/pytorch-gan#abstract-5
 243. https://github.com/eriklindernoren/pytorch-gan#run-example-5
 244. https://github.com/eriklindernoren/pytorch-gan#context-conditional-gan
 245. https://github.com/eriklindernoren/pytorch-gan#authors-6
 246. https://github.com/eriklindernoren/pytorch-gan#abstract-6
 247. https://github.com/eriklindernoren/pytorch-gan#run-example-6
 248. https://github.com/eriklindernoren/pytorch-gan#context-encoder
 249. https://github.com/eriklindernoren/pytorch-gan#authors-7
 250. https://github.com/eriklindernoren/pytorch-gan#abstract-7
 251. https://github.com/eriklindernoren/pytorch-gan#run-example-7
 252. https://github.com/eriklindernoren/pytorch-gan#coupled-gan
 253. https://github.com/eriklindernoren/pytorch-gan#authors-8
 254. https://github.com/eriklindernoren/pytorch-gan#abstract-8
 255. https://github.com/eriklindernoren/pytorch-gan#run-example-8
 256. https://github.com/eriklindernoren/pytorch-gan#cyclegan
 257. https://github.com/eriklindernoren/pytorch-gan#authors-9
 258. https://github.com/eriklindernoren/pytorch-gan#abstract-9
 259. https://github.com/eriklindernoren/pytorch-gan#run-example-9
 260. https://github.com/eriklindernoren/pytorch-gan#deep-convolutional-gan
 261. https://github.com/eriklindernoren/pytorch-gan#authors-10
 262. https://github.com/eriklindernoren/pytorch-gan#abstract-10
 263. https://github.com/eriklindernoren/pytorch-gan#run-example-10
 264. https://github.com/eriklindernoren/pytorch-gan#discogan
 265. https://github.com/eriklindernoren/pytorch-gan#authors-11
 266. https://github.com/eriklindernoren/pytorch-gan#abstract-11
 267. https://github.com/eriklindernoren/pytorch-gan#run-example-11
 268. https://github.com/eriklindernoren/pytorch-gan#dragan
 269. https://github.com/eriklindernoren/pytorch-gan#authors-12
 270. https://github.com/eriklindernoren/pytorch-gan#abstract-12
 271. https://github.com/eriklindernoren/pytorch-gan#run-example-12
 272. https://github.com/eriklindernoren/pytorch-gan#dualgan
 273. https://github.com/eriklindernoren/pytorch-gan#authors-13
 274. https://github.com/eriklindernoren/pytorch-gan#abstract-13
 275. https://github.com/eriklindernoren/pytorch-gan#run-example-13
 276. https://github.com/eriklindernoren/pytorch-gan#energy-based-gan
 277. https://github.com/eriklindernoren/pytorch-gan#authors-14
 278. https://github.com/eriklindernoren/pytorch-gan#abstract-14
 279. https://github.com/eriklindernoren/pytorch-gan#run-example-14
 280. https://github.com/eriklindernoren/pytorch-gan#gan
 281. https://github.com/eriklindernoren/pytorch-gan#authors-15
 282. https://github.com/eriklindernoren/pytorch-gan#abstract-15
 283. https://github.com/eriklindernoren/pytorch-gan#run-example-15
 284. https://github.com/eriklindernoren/pytorch-gan#infogan
 285. https://github.com/eriklindernoren/pytorch-gan#authors-16
 286. https://github.com/eriklindernoren/pytorch-gan#abstract-16
 287. https://github.com/eriklindernoren/pytorch-gan#run-example-16
 288. https://github.com/eriklindernoren/pytorch-gan#least-squares-gan
 289. https://github.com/eriklindernoren/pytorch-gan#authors-17
 290. https://github.com/eriklindernoren/pytorch-gan#abstract-17
 291. https://github.com/eriklindernoren/pytorch-gan#run-example-17
 292. https://github.com/eriklindernoren/pytorch-gan#munit
 293. https://github.com/eriklindernoren/pytorch-gan#authors-18
 294. https://github.com/eriklindernoren/pytorch-gan#abstract-18
 295. https://github.com/eriklindernoren/pytorch-gan#run-example-18
 296. https://github.com/eriklindernoren/pytorch-gan#pix2pix
 297. https://github.com/eriklindernoren/pytorch-gan#authors-19
 298. https://github.com/eriklindernoren/pytorch-gan#abstract-19
 299. https://github.com/eriklindernoren/pytorch-gan#run-example-19
 300. https://github.com/eriklindernoren/pytorch-gan#pixelda
 301. https://github.com/eriklindernoren/pytorch-gan#authors-20
 302. https://github.com/eriklindernoren/pytorch-gan#abstract-20
 303. https://github.com/eriklindernoren/pytorch-gan#mnist-to-mnist-m-classification
 304. https://github.com/eriklindernoren/pytorch-gan#semi-supervised-gan
 305. https://github.com/eriklindernoren/pytorch-gan#authors-21
 306. https://github.com/eriklindernoren/pytorch-gan#abstract-21
 307. https://github.com/eriklindernoren/pytorch-gan#run-example-20
 308. https://github.com/eriklindernoren/pytorch-gan#softmax-gan
 309. https://github.com/eriklindernoren/pytorch-gan#authors-22
 310. https://github.com/eriklindernoren/pytorch-gan#abstract-22
 311. https://github.com/eriklindernoren/pytorch-gan#run-example-21
 312. https://github.com/eriklindernoren/pytorch-gan#stargan
 313. https://github.com/eriklindernoren/pytorch-gan#authors-23
 314. https://github.com/eriklindernoren/pytorch-gan#abstract-23
 315. https://github.com/eriklindernoren/pytorch-gan#run-example-22
 316. https://github.com/eriklindernoren/pytorch-gan#super-resolution-gan
 317. https://github.com/eriklindernoren/pytorch-gan#authors-24
 318. https://github.com/eriklindernoren/pytorch-gan#abstract-24
 319. https://github.com/eriklindernoren/pytorch-gan#run-example-23
 320. https://github.com/eriklindernoren/pytorch-gan#unit
 321. https://github.com/eriklindernoren/pytorch-gan#authors-25
 322. https://github.com/eriklindernoren/pytorch-gan#abstract-25
 323. https://github.com/eriklindernoren/pytorch-gan#run-example-24
 324. https://github.com/eriklindernoren/pytorch-gan#wasserstein-gan
 325. https://github.com/eriklindernoren/pytorch-gan#authors-26
 326. https://github.com/eriklindernoren/pytorch-gan#abstract-26
 327. https://github.com/eriklindernoren/pytorch-gan#run-example-25
 328. https://github.com/eriklindernoren/pytorch-gan#wasserstein-gan-gp
 329. https://github.com/eriklindernoren/pytorch-gan#authors-27
 330. https://github.com/eriklindernoren/pytorch-gan#abstract-27
 331. https://github.com/eriklindernoren/pytorch-gan#run-example-26
 332. https://github.com/eriklindernoren/pytorch-gan#wasserstein-gan-div
 333. https://github.com/eriklindernoren/pytorch-gan#authors-28
 334. https://github.com/eriklindernoren/pytorch-gan#abstract-28
 335. https://github.com/eriklindernoren/pytorch-gan#run-example-27
 336. https://github.com/
