community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   45
   45
   aditya sharma
   march 9th, 2018
   deep learning
   +3

convolutional neural networks with tensorflow

   in this tutorial, you'll learn how to construct and implement
   convolutional neural networks (id98s) in python with the tensorflow
   framework.

   tensorflow is a famous deep learning framework. in this blog post, you
   will learn the basics of this extremely popular python library and
   understand how to implement these deep, feed-forward artificial neural
   networks with it.

   to be precise, you'll will be introduced to the following topics in
   today's tutorial:

     * you'll be first introduced to [3]tensors and how they differ from
       matrices; once you understand what tensors are then you'll be
       introduced to the [4]tensorflow framework, within this you will
       also see that how even a single line of code is implemented via a
       [5]computational graph in tensorflow, then you will learn about
       some of the the package's concepts that play a major role in you to
       do deep learning like [6]constants, variables and placeholders,
     * then, you'll will be headed to the most interesting part of this
       tutorial. that is, the implementation of [7]convolutional neural
       network: first you will try to understand the data. you'll use
       python and its libraries to load, explore and analyze your data.
       you'll also preprocess your data: you   ll learn how to visualize
       your images as a matrix, reshape your data and rescale the images
       between 0 and 1 if required.
     * with all of this done, you are ready to [8]construct the deep
       neural network model: you'll start off by defining the network
       parameters, then learn how to create wrappers to increase the
       simplicity of your code, define weights and biases, model the
       network, define loss and optimizer nodes. once you have all this in
       place you are ready for [9]training and testing your model;
     * after your model's evaluation, you'll learn more about overfitting
       and how you can overcome it by [10]adding a dropout layer. you will
       then again train the model with dropout layers inserted in the
       network, [11]evaluate the model on test set and compare the results
       of both the models; next, you'll make [12]predictions on the test
       data, convert the probabilities into class labels and plot few test
       samples that your model correctly classified and incorrectly
       classified. you will visualize the classification report which will
       have precision, recall, f-1 score of all classes present in the
       test dataset.

tensors

   in layman's terms, a tensor is a way of representing the data in deep
   learning. a tensor can be a 1-dimensional, a 2-dimensional, a
   3-dimensional array, etc. you can think of a tensor as a
   multidimensional array. in machine learning and deep learning you have
   datasets which are high dimensional, in which each dimension represents
   a different feature of that dataset.

   consider the following example of a dog versus cat classification
   problem, where the dataset you're working with has multiple variety of
   both cats and dogs images. now, in order to correctly classify a dog or
   a cat when given an image, the network has to learn discriminative
   features like color, face structure, ears, eyes, shape of the tail etc.

   these features are incorporated by the tensors.

   tensorflow id98

   tip: if you want to get to know more about tensors, check out
   datacamp's [13]tensorflow tutorial for beginners.

   but how are tensors then any different from matrices? you'll find out
   in the next section!

tensors versus matrices: differences

   a matrix is a two-dimensional grid of size $n    m$ that contains
   numbers: you can add and subtract matrices of the same size, multiply
   one matrix with another as long as the sizes are compatible $((n    m)   
   (m    p) = n    p)$, and multiply an entire matrix by a constant.

   a vector is a matrix with just one row or column (but see below).

   a tensor is often thought of as a generalized matrix. that is, it could
   be
     * a 1-d matrix, like a vector, which is actually such a tensor,
     * a 3-d matrix (something like a cube of numbers),
     * a 0-d matrix (a single number), or
     * a higher dimensional structure that is harder to visualize.

   the dimension of the tensor is called its rank.

   any rank-2 tensor can be represented as a matrix, but not every matrix
   is really a rank-2 tensor. the numerical values of a tensor   s matrix
   representation depend on what transformation rules have been applied to
   the entire system.

tensorflow: constants, variables and placeholders

   tensorflow is a framework developed by google on 9th november 2015. it
   is written in python, c++ and cuda. it supports platforms like linux,
   microsoft windows, macos, and android. tensorflow provides multiple
   api's in python, c++, java etc. the most widely used api is python and
   you will implementing a convolutional neural network using python api
   in this tutorial.

   the name tensorflow is derived from the operations, such as adding or
   multiplying, that id158s perform on
   multidimensional data arrays. these arrays are called tensors in this
   framework, which is slightly different from what you saw earlier.

   so why is there a mention of a flow when you're talking about
   operations?

   let's consider a simple equation and its diagram, represented as a
   computational graph. note: don't worry if you don't get this equation
   straight away, this is just to help you to understand how the flow
   takes place while using the tensorflow framework.
prediction = tf.nn.softmax(tf.matmul(w,x) + b)

   tensorflow python

   in tensorflow, every line of code that you write has to go through a
   computational graph. as in the above figure, you can see that first $w$
   and $x$ get multiplied and then comes $b$ which is added to the output
   of $w$ and $x$. after adding the output of $w$ and $x$ with $b$, a
   softmax function is applied and a final output is generated.

   you'll find that, when you're working with tensorflow, constants,
   variables and placeholders come handy to define the input data, class
   labels, weights and biases.
     * constants takes no input, you use them to store constant values.
       they produce a constant output that it stores.

import tensorflow as tf
a = tf.constant(2.0)
b = tf.constant(3.0)
c = a * b

   here, nodes a and b are constants that store values 2.0 and 3.0. node c
   stores the operation that multiplies the nodes a and b, respectively.
   when you initialize a session and run c, you'll see that the output
   that you get back is 6.0:
sess = tf.session()
sess.run(c)

6.0

     * placeholders allow you to feed input on the run. because of this
       flexibility, placeholders are used which allows your computational
       graph to take inputs as parameters. defining a node as a
       placeholder assures that node, that it is expected to receive a
       value later or during runtime. here, "runtime" means that the input
       is fed to the placeholder when you run your computational graph.

# creating placeholders
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)

# assigning addition operation w.r.t. a and b to node add
add = a + b

# create session object
sess = tf.session()

# executing add by passing the values [1, 3] [2, 4] for a and b respectively
output = sess.run(add, {a: [1,3], b: [2, 4]})
print('adding a and b:', output)

('adding a and b:', array([ 3.,  7.], dtype=float32))

   in this case, you have explicitly provided the data type with
   tf.float32. note that this data type is therefore a single precision,
   which is stored in 32 bits form. however, in cases where you do not do
   this, just like in the first example, tensorflow will infer the type of
   the constant/variable from the initialized value.

   python deep learning
     * variables allow you to modify the graph such that it can produce
       new outputs with respect to the same inputs. a variable allows you
       to add such parameters or node to the graph that are trainable.
       that is, the value can be modified over the period of a time.

#variables are defined by providing their initial value and type
variable = tf.variable([0.9,0.7], dtype = tf.float32)

#variable must be initialized before a graph is used for the first time.
init = tf.global_variables_initializer()
sess.run(init)

   constants are initialized when you call tf.constant, and their value
   can never change. but, variables are not initialized when you call
   tf.variable. to initialize all the variables in tensorflow, you need to
   explicitly call the global variable intializer
   global_variables_initializer(), which initializes all the existing
   variables in your tensorflow code, as you can see in the above code
   chunk.

   variables survive across multiple executions of a graph unlike normal
   tensors that are only instantiated when a graph is run and are
   immediately deleted afterwards.

   in this section, you have seen that placeholders are used for holding
   the input data and class labels, whereas variables are used for the
   purpose of weights and biases. don't worry if you have still not been
   able to develop proper intuition about how a computational graph works,
   for what placeholders and variables are usually used for in deep
   learning. you will be address all these topics later on in this
   tutorial.

convolutional neural network (id98) in tensorflow

fashion-mnist dataset

   before you go ahead and load in the data, it's good to take a look at
   what you'll exactly be working with! the [14]fashion-mnist dataset
   contains zalando's article images, with 28x28 grayscale images of
   65,000 fashion products from 10 categories, and 6,500 images per
   category. the training set has 55,000 images, and the test set has
   10,000 images. you can double check this later when you have loaded in
   your data! ;)

   fashion-mnist is similar to the mnist dataset that you might already
   know, which you use to classify handwritten digits. that means that the
   image dimensions, training and test splits are similar.

   tip: if you want to learn how to implement an multi-layer id88
   (mlp) for classification tasks with this latter dataset, [15]go to this
   tutorial, or if you want to learn about convolutional neural networks
   and its implementation in a keras framework, check out [16]this
   tutorial.

   you can find the fashion-mnist dataset [17]here. unlike the keras or
   scikit-learn packages, tensorflow has no predefined module to load the
   fashion mnist dataset, though by default it has mnist dataset. to load
   the data, you first need to download the data from the above link and
   then structure the data in a particular folder format as shown below to
   be able to work with it. otherwise, tensorflow will download and use
   the original mnist.

load the data

   you first start with importing all the required modules like numpy,
   matplotlib and most importantly tensorflow.
# import libraries
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
%matplotlib inline
import os
os.environ["cuda_visible_devices"]="0" #for training on gpu

   after importing all the modules you will now learn how you can load
   data in tensorflow, which should be pretty straightforward. the only
   thing that you should take into account is the one_hot=true argument,
   which you'll also find in the line of code below: it converts the
   categorical class labels to binary vectors.

   in one-hot encoding, you convert the categorical data into a vector of
   numbers. you do this because machine learning algorithms can't work
   with categorical data directly. instead, you generate one boolean
   column for each category or class. only one of these columns could take
   on the value 1 for each sample. that explains the term "one-hot
   encoding".

   but what does such a one-hot encoded data column look like?

   for your problem statement, the one hot encoding will be a row vector,
   and for each image, it will have a dimension of 1 x 10. it's important
   to note here that the vector consists of all zeros except for the class
   that it represents. there, you'll find a 1. for example, the ankle boot
   image that you plotted above has a label of 9, so for all the ankle
   boot images, the one hot encoding vector would be [0 0 0 0 0 0 0 0 0
   1].

   now that all of this is clear, it's time to import the data!
data = input_data.read_data_sets('data/fashion',one_hot=true)

extracting data/fashion/train-images-idx3-ubyte.gz
extracting data/fashion/train-labels-idx1-ubyte.gz
extracting data/fashion/t10k-images-idx3-ubyte.gz
extracting data/fashion/t10k-labels-idx1-ubyte.gz

   once you have the training and testing data loaded, you're all set to
   analyze the data in order to get some intuition about the dataset that
   you are going to work with for this tutorial!

analyze the data

   before you start any heavy lifting, it's always a good idea to check
   out what the images in the dataset look like. first, you can take a
   programmatical approach and check out their dimensions. also, take into
   account that if you want to explore your images, these have already
   been rescaled between 0 and 1. that means that you would not need to
   rescale the image pixels again!
# shapes of training set
print("training set (images) shape: {shape}".format(shape=data.train.images.shap
e))
print("training set (labels) shape: {shape}".format(shape=data.train.labels.shap
e))

# shapes of test set
print("test set (images) shape: {shape}".format(shape=data.test.images.shape))
print("test set (labels) shape: {shape}".format(shape=data.test.labels.shape))

training set (images) shape: (55000, 784)
training set (labels) shape: (55000, 10)
test set (images) shape: (10000, 784)
test set (labels) shape: (10000, 10)

   from the above output, you can see that the training data has a shape
   of 55000 x 784: there are 55,000 training samples each of
   784-dimensional vector. similarly, the test data has a shape of 10000 x
   784, since there are 10,000 testing samples.

   the 784 dimensional vector is nothing but a 28 x 28 dimensional matrix.
   that's why you will be reshaping each training and testing sample from
   a 784 dimensional vector to a 28 x 28 x 1 dimensional matrix in order
   to feed the samples in to the id98 model.

   for simplicity, let's create a dictionary that will have class names
   with their corresponding categorical class labels.
# create dictionary of target classes
label_dict = {
 0: 't-shirt/top',
 1: 'trouser',
 2: 'pullover',
 3: 'dress',
 4: 'coat',
 5: 'sandal',
 6: 'shirt',
 7: 'sneaker',
 8: 'bag',
 9: 'ankle boot',
}

   also, let's take a look at the images in your dataset:
plt.figure(figsize=[5,5])

# display the first image in training data
plt.subplot(121)
curr_img = np.reshape(data.train.images[0], (28,28))
curr_lbl = np.argmax(data.train.labels[0,:])
plt.imshow(curr_img, cmap='gray')
plt.title("(label: " + str(label_dict[curr_lbl]) + ")")

# display the first image in testing data
plt.subplot(122)
curr_img = np.reshape(data.test.images[0], (28,28))
curr_lbl = np.argmax(data.test.labels[0,:])
plt.imshow(curr_img, cmap='gray')
plt.title("(label: " + str(label_dict[curr_lbl]) + ")")

<matplotlib.text.text at 0x7f3d17e38cd0>

   [output_46_1_maw6b7.png]

   the output of above two plots are one of the sample images from both
   training and testing data, and these images are assigned a class label
   of 4 (coat) and 9 (ankle boot). similarly, other fashion products will
   have different labels, but similar products will have same labels. this
   means that all the 6,500 ankle boot images will have a class label of
   9.

id174

   the images are of size 28 x 28 (or a 784-dimensional vector).

   the images are already rescaled between 0 and 1 so you don't need to
   rescale them again, but to be sure let's visualize an image from
   training dataset as a matrix. along with that let's also print the
   maximum and minimum value of the matrix.
data.train.images[0]

array([0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00784314, 0.0509804 ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.00784314, 0.00392157, 0.        , 0.        ,
       0.5137255 , 0.92549026, 0.909804  , 0.87843144, 0.2901961 ,
       0.        , 0.        , 0.00392157, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00392157, 0.        ,
       0.        , 0.        , 0.41960788, 0.9176471 , 0.87843144,
       0.8470589 , 0.8980393 , 0.8980393 , 0.21568629, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.00784314, 0.        , 0.        , 0.36078432, 0.8000001 ,
       0.8352942 , 0.8431373 , 0.882353  , 0.8470589 , 0.9215687 ,
       0.80392164, 0.8941177 , 0.7019608 , 0.2509804 , 0.        ,
       0.        , 0.00784314, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.00392157, 0.        , 0.        ,
       0.75294125, 0.8980393 , 0.854902  , 0.8470589 , 0.78823537,
       0.90196085, 1.        , 0.882353  , 0.8196079 , 0.8352942 ,
       0.8431373 , 0.89019614, 0.4901961 , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.01568628, 0.        , 0.09411766, 0.909804  , 0.8078432 ,
       0.8313726 , 0.8941177 , 0.8235295 , 0.8000001 , 0.86666673,
       0.76470596, 0.85098046, 0.8470589 , 0.8078432 , 0.8470589 ,
       0.8000001 , 0.        , 0.        , 0.00784314, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.01176471, 0.        ,
       0.3921569 , 0.93725497, 0.85098046, 0.8117648 , 0.86274517,
       0.87843144, 0.83921576, 0.8431373 , 0.8313726 , 0.8588236 ,
       0.8196079 , 0.8352942 , 0.8313726 , 0.90196085, 0.15686275,
       0.        , 0.01176471, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.6745098 , 0.9333334 ,
       0.86666673, 0.882353  , 0.854902  , 0.86274517, 0.86666673,
       0.91372555, 0.87843144, 0.8235295 , 0.8431373 , 0.86666673,
       0.83921576, 0.92549026, 0.40784317, 0.        , 0.00784314,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.86274517, 0.9215687 , 0.87843144, 0.882353  ,
       0.8705883 , 0.854902  , 0.85098046, 0.7843138 , 0.8745099 ,
       0.8431373 , 0.8588236 , 0.8705883 , 0.85098046, 0.91372555,
       0.6       , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.82745105,
       0.90196085, 0.8941177 , 0.8862746 , 0.882353  , 0.86666673,
       0.8705883 , 0.85098046, 0.83921576, 0.86274517, 0.8588236 ,
       0.8470589 , 0.8588236 , 0.8980393 , 0.7843138 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.01568628, 0.8941177 , 0.8862746 , 0.90196085,
       0.882353  , 0.87843144, 0.882353  , 0.8745099 , 0.8352942 ,
       0.8588236 , 0.86666673, 0.8588236 , 0.854902  , 0.8705883 ,
       0.8862746 , 0.9176471 , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.227451  ,
       0.93725497, 0.87843144, 0.91372555, 0.882353  , 0.8745099 ,
       0.8745099 , 0.86666673, 0.83921576, 0.8745099 , 0.8588236 ,
       0.85098046, 0.854902  , 0.86274517, 0.86666673, 0.8431373 ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.37254903, 0.9568628 , 0.8705883 ,
       0.9058824 , 0.8862746 , 0.8745099 , 0.87843144, 0.87843144,
       0.85098046, 0.86274517, 0.854902  , 0.8588236 , 0.86666673,
       0.8588236 , 0.85098046, 0.89019614, 0.14901961, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.52156866, 0.9490197 , 0.8705883 , 0.9490197 , 0.89019614,
       0.87843144, 0.8862746 , 0.89019614, 0.83921576, 0.86666673,
       0.86274517, 0.8588236 , 0.8705883 , 0.909804  , 0.83921576,
       0.9215687 , 0.27450982, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.70980394, 0.9294118 ,
       0.87843144, 0.8745099 , 0.909804  , 0.8745099 , 0.882353  ,
       0.89019614, 0.85098046, 0.8745099 , 0.8588236 , 0.8588236 ,
       0.86666673, 0.8431373 , 0.8431373 , 0.92549026, 0.42352945,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.854902  , 0.91372555, 0.90196085, 0.6431373 ,
       0.94117653, 0.8745099 , 0.882353  , 0.8862746 , 0.854902  ,
       0.8745099 , 0.8470589 , 0.86666673, 0.86274517, 0.61960787,
       0.86274517, 0.8980393 , 0.62352943, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.95294124,
       0.909804  , 0.8941177 , 0.49411768, 0.9843138 , 0.87843144,
       0.882353  , 0.90196085, 0.8862746 , 0.8745099 , 0.854902  ,
       0.86274517, 0.8980393 , 0.47450984, 0.91372555, 0.8941177 ,
       0.7607844 , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.8588236 , 0.9058824 , 0.8000001 ,
       0.427451  , 1.        , 0.8588236 , 0.89019614, 0.8862746 ,
       0.7803922 , 0.882353  , 0.8745099 , 0.8431373 , 0.9450981 ,
       0.36078432, 0.8980393 , 0.882353  , 0.8352942 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.01960784,
       0.8941177 , 0.90196085, 0.7372549 , 0.4901961 , 1.        ,
       0.85098046, 0.8862746 , 0.90196085, 0.8352942 , 0.882353  ,
       0.8705883 , 0.83921576, 0.9921569 , 0.36862746, 0.8588236 ,
       0.87843144, 0.9294118 , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.12156864, 0.91372555, 0.91372555,
       0.68235296, 0.5647059 , 1.        , 0.8470589 , 0.87843144,
       0.91372555, 0.8705883 , 0.882353  , 0.87843144, 0.8431373 ,
       0.9960785 , 0.41960788, 0.8196079 , 0.8705883 , 0.8431373 ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.3529412 , 0.9058824 , 0.909804  , 0.63529414, 0.59607846,
       1.        , 0.854902  , 0.882353  , 0.91372555, 0.854902  ,
       0.8745099 , 0.87843144, 0.8352942 , 1.        , 0.43529415,
       0.7568628 , 0.87843144, 0.86666673, 0.19607845, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.6784314 , 0.9450981 ,
       0.93725497, 0.6431373 , 0.61960787, 0.9960785 , 0.86274517,
       0.882353  , 0.9176471 , 0.85098046, 0.8705883 , 0.8705883 ,
       0.8352942 , 0.9960785 , 0.45882356, 0.7843138 , 0.8941177 ,
       0.91372555, 0.65882355, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.4431373 , 0.82745105, 1.        , 0.6313726 ,
       0.6862745 , 0.9960785 , 0.8588236 , 0.8941177 , 0.9176471 ,
       0.86666673, 0.8745099 , 0.87843144, 0.8352942 , 0.9960785 ,
       0.5137255 , 0.7960785 , 0.82745105, 0.8000001 , 0.18431373,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.8196079 , 0.9215687 ,
       0.8588236 , 0.8941177 , 0.9176471 , 0.86666673, 0.87843144,
       0.8745099 , 0.8470589 , 0.9960785 , 0.5882353 , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.87843144, 0.9176471 , 0.86666673, 0.8941177 ,
       0.9176471 , 0.86666673, 0.8705883 , 0.8745099 , 0.86274517,
       0.9333334 , 0.6862745 , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.00784314, 0.        , 0.        , 0.91372555,
       0.90196085, 0.8745099 , 0.882353  , 0.909804  , 0.86274517,
       0.8705883 , 0.87843144, 0.86274517, 0.9215687 , 0.72156864,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.00392157,
       0.        , 0.        , 1.        , 0.9450981 , 0.8980393 ,
       0.9333334 , 0.93725497, 0.882353  , 0.9058824 , 0.92549026,
       0.8941177 , 0.9725491 , 0.86666673, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.37647063, 0.6745098 , 0.7686275 , 0.81568635, 0.8705883 ,
       0.85098046, 0.8196079 , 0.7843138 , 0.75294125, 0.64705884,
       0.26666668, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        ], dtype=float32)

np.max(data.train.images[0])

1.0

np.min(data.train.images[0])

0.0

   let us reshape the images so that it's of size 28 x 28 x 1, and feed
   this as an input to the network.
# reshape training and testing image
train_x = data.train.images.reshape(-1, 28, 28, 1)
test_x = data.test.images.reshape(-1,28,28,1)

train_x.shape, test_x.shape

((55000, 28, 28, 1), (10000, 28, 28, 1))

   you need not reshape the labels since they already have the correct
   dimensions, but let us put the training and testing labels in separate
   variables and also print their respective shapes just be on the safer
   side.
train_y = data.train.labels
test_y = data.test.labels

train_y.shape, test_y.shape

((55000, 10), (10000, 10))

the deep neural network

   you'll use three convolutional layers:

     the first layer will have 32-3 x 3 filters,

     the second layer will have 64-3 x 3 filters and

     the third layer will have 128-3 x 3 filters.

   in addition, there are three max-pooling layers each of size 2 x 2.

   [fashion-mnist-architecture_htbpsz.png]

   you start off with defining the training iterations training_iters, the
   learning rate learning_rate and the batch size batch_size. keep in mind
   that all these are hyperparameters and that these don't have fixed
   values, as these differ for every problem statement.

   nevertheless, here's what you usually can expect:
     * training iterations indicate the number of times you train your
       network,
     * it is a good practice to use a learning rate of 1e-3, learning rate
       is a factor that is multiplied with the weights based on which the
       weights get updated and this indeed helps in reducing the
       cost/loss/cross id178 and ultimately in converging or reaching
       the local optima. the learning rate should neither be too high or
       too low it should be a balanced rate and
     * the batch size means that your training images will be divided in a
       fixed batch size and at every batch it will take a fixed number of
       images and train them. it's recommended to use a batch size in the
       power of 2, since the number of physical processor is often a power
       of 2, using a number of virtual processor different from a power of
       2 leads to poor performance. also, taking a very large batch size
       can lead to memory errors so you have to make sure that the machine
       you run your code on has sufficient ram to handle specified batch
       size.

training_iters = 200
learning_rate = 0.001
batch_size = 128

network parameters

   next, you need to define the network parameters. firstly, you define
   the number of inputs. this is 784 since the image is initially loaded
   as a 784-dimensional vector. later, you will see that how you will
   reshape the 784-dimensional vector to a 28 x 28 x 1 matrix. secondly,
   you'll also define the number of classes, which is nothing else than
   the number of class labels.
# mnist data input (img shape: 28*28)
n_input = 28

# mnist total classes (0-9 digits)
n_classes = 10

   now is the time to use those placeholders, about which you read
   previously in this tutorial. you will define an input placeholder x,
   which will have a dimension of none x 784 and the output placeholder
   with a dimension of none x 10. to reiterate, placeholders allow you to
   do operations and build your computation graph without feeding in data.
   similarly, y will hold the label of the training images in form matrix
   which will be a none*10 matrix.
   the row dimension is none. that's because you have defined batch_size,
   which tells placeholders that they will receive this dimension at the
   time when you will feed in the data to them. since you set the batch
   size to 128, this will be the row dimension of the placeholders.
#both placeholders are of type float
x = tf.placeholder("float", [none, 28,28,1])
y = tf.placeholder("float", [none, n_classes])

creating wrappers for simplicity

   in your network architecture model, you will have multiple convolution
   and max-pooling layers. in such cases, it's always a better idea to
   define convolution and max-pooling functions, so that you can call them
   as many times you want to use them in your network.
     * in the conv2d() function you pass 4 arguments: input x, weights w,
       bias b and strides. this last argument is by default set to 1, but
       you can always play with it to see how the network performs. the
       first and last stride must always be 1, because the first is for
       the image-number and the last is for the input-channel (since the
       image is a gray-scale image which has only one channel). after
       applying the convolution, you will add bias and apply an activation
       function that is called rectified linear unit (relu).
     * the max-pooling function is simple: it has the input x and a kernel
       size k, which is set to be 2. this means that the max-pooling
       filter will be a square matrix with dimensions 2 x 2 and the stride
       by which the filter will move in is also 2.

   you will padding equal to same which ensures that while performing the
   convolution operations, the boundary pixels of the image are not left
   out, so padding equal to same will basically adds zeros at the
   boundaries of the input and allow the convolution filter to access the
   boundary pixels as well.

   similarly, in max-pooling operation padding equal to same will add
   zeros. later, when you will define the weights and the biases you will
   notice that an input of size 28 x 28 is downsampled to 4 x 4 after
   applying three max-pooling layers.
def conv2d(x, w, b, strides=1):
    # conv2d wrapper, with bias and relu activation
    x = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding='same')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)

def maxpool2d(x, k=2):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='s
ame')

   after you have defined the conv2d and maxpool2d wrappers, now you can
   now define your weights and biases variables. so, let's get started!
   but first, let's understand each weight and bias parameter step by
   step. you will create two dictionaries, one for weight and the second
   for the bias parameter.
     * if you can recall from the above figure that the first convolution
       layer has 32-3x3 filters, so the first key (wc1) in the weight
       dictionary has an argument shape that takes a tuple with 4 values:
       the first and are the filter size, while the third is the number of
       channels in the input image and the last represents the number of
       convolution filters you want in the first convolution layer. the
       first key in biases dictionary, bc1, will have 32 bias parameters.
     * similarly, the second key (wc2) of the weight dictionary has a
       shape parameter that will take a tuple with 4 values: the first and
       second again refer to the filter size, and the third represents the
       number of channels from the previous output. since you pass 32
       convolution filters on the input image, you will have 32 channels
       as an output from the first convolution layer operation. the last
       represents the number of filters you want in the second convolution
       filter. note that the second key in biases dictionary, bc2, will
       have 64 parameters.

   you will do the same for the third convolution layer.
     * now, it's important to understand the fourth key (wd1). after
       applying 3 convolution and max-pooling operations, you are
       downsampling the input image from 28 x 28 x 1 to 4 x 4 x 1 and now
       you need to flatten this downsampled output to feed this as input
       to the fully connected layer. that's why you do the multiplication
       operation $44128$, which is the output of the previous layer or
       number of channels that are outputted by the convolution layer 3.
       the second element of the tuple that you pass to shape has number
       of neurons that you want in the fully connected layer. similarly,
       in biases dictionary, the fourth key bd1 has 128 parameters.

   you will follow the same logic for the last fully connected layer, in
   which the number of neurons will be equivalent to the number of
   classes.
weights = {
    'wc1': tf.get_variable('w0', shape=(3,3,1,32), initializer=tf.contrib.layers
.xavier_initializer()),
    'wc2': tf.get_variable('w1', shape=(3,3,32,64), initializer=tf.contrib.layer
s.xavier_initializer()),
    'wc3': tf.get_variable('w2', shape=(3,3,64,128), initializer=tf.contrib.laye
rs.xavier_initializer()),
    'wd1': tf.get_variable('w3', shape=(4*4*128,128), initializer=tf.contrib.lay
ers.xavier_initializer()),
    'out': tf.get_variable('w6', shape=(128,n_classes), initializer=tf.contrib.l
ayers.xavier_initializer()),
}
biases = {
    'bc1': tf.get_variable('b0', shape=(32), initializer=tf.contrib.layers.xavie
r_initializer()),
    'bc2': tf.get_variable('b1', shape=(64), initializer=tf.contrib.layers.xavie
r_initializer()),
    'bc3': tf.get_variable('b2', shape=(128), initializer=tf.contrib.layers.xavi
er_initializer()),
    'bd1': tf.get_variable('b3', shape=(128), initializer=tf.contrib.layers.xavi
er_initializer()),
    'out': tf.get_variable('b4', shape=(10), initializer=tf.contrib.layers.xavie
r_initializer()),
}

   now, it's time to define the network architecture! unfortunately, this
   is not as simple as you do it in the keras framework!

   the conv_net() function takes 3 arguments as an input: the input x and
   the weights and biases dictionaries. again, let's go through the
   construction of the network step by step:
     * firstly, you reshape the 784-dimensional input vector to a 28 x 28
       x 1 matrix. as you had seen earlier, the images are loaded as a
       784-dimensional vector but you will feed the input to your model as
       a matrix of size 28 x 28 x 1. the -1 in the reshape() function
       means that it will infer the first dimension on its own but the
       rest of the dimension are fixed, that is, 28 x 28 x 1.
     * next, as shown in the figure of the architecture of the model, you
       will define conv1 which takes input as an image, weights wc1 and
       biases bc1. next, you apply max-pooling on the output of conv1 and
       you will basically perform a process analogous to this until conv3.
     * since your task is to classify, given an image it belongs to which
       class label. so, after you pass through all the convolution and
       max-pooling layers, you will flatten the output of conv3. next,
       you'll connect the flattened conv3 neurons with each and every
       neuron in the next layer. then you will apply activation function
       on the output of the fully connected layer fc1.
     * finally, in the last layer, you will have 10 neurons since you have
       to classify 10 labels. that means that you will connect all the
       neurons of fc1 in the output layer with 10 neurons in the last
       layer.

def conv_net(x, weights, biases):

    # here we call the conv2d function we had defined above and pass the input i
mage x, weights wc1 and bias bc1.
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    # max pooling (down-sampling), this chooses the max value from a 2*2 matrix
window and outputs a 14*14 matrix.
    conv1 = maxpool2d(conv1, k=2)

    # convolution layer
    # here we call the conv2d function we had defined above and pass the input i
mage x, weights wc2 and bias bc2.
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    # max pooling (down-sampling), this chooses the max value from a 2*2 matrix
window and outputs a 7*7 matrix.
    conv2 = maxpool2d(conv2, k=2)

    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])
    # max pooling (down-sampling), this chooses the max value from a 2*2 matrix
window and outputs a 4*4.
    conv3 = maxpool2d(conv3, k=2)


    # fully connected layer
    # reshape conv2 output to fit fully connected layer input
    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    # output, class prediction
    # finally we multiply the fully connected layer with the weights and add a b
ias term.
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out

loss and optimizer nodes

   you will start with constructing a model and call the conv_net()
   function by passing in input x, weights and biases. since this is a
   multi-class classification problem, you will use softmax activation on
   the output layer. this will give you probabilities for each class
   label. the id168 you use is cross id178.

   the reason you use cross id178 as a id168 is because the
   cross-id178 function's value is always positive, and tends toward
   zero as the neuron gets better at computing the desired output, y, for
   all training inputs, x. these are both properties you would intuitively
   expect for a cost function. it avoids the problem of learning slowing
   down which means that if the weights and biases are initialized in a
   wrong fashion even then it helps in recovering faster and does not
   hamper much the training phase.

   in tensorflow, you define both the activation and the cross id178
   id168s in one line. you pass two parameters which are the
   predicted output and the ground truth label y. you will then take the
   mean (reduce_mean) over all the batches to get a single loss/cost
   value.
   next, you define one of the most popular optimization algorithms: the
   adam optimizer. you can read more about the optimizer from [18]here and
   you specify the learning rate with explicitly stating minimize cost
   that you had calculated in the previous step.
pred = conv_net(x, weights, biases)

cost = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(logits=pred, label
s=y))

optimizer = tf.train.adamoptimizer(learning_rate=learning_rate).minimize(cost)

evaluate model node

   to test your model, let's define two more nodes: correct_prediction and
   accuracy. it will evaluate your model after every training iteration
   which will help you to keep track of the performance of your model.
   since after every iteration the model is tested on the 10,000 testing
   images, it will not have seen in the training phase.

   you can always save the graph and run the testing part later as well.
   but for now, you will test within the session.
#here you check whether the index of the maximum value of the predicted image is
 equal to the actual labelled image. and both will be a column vector.
correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))

#calculate accuracy across all the given images and average them out.
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   remember that your weights and biases are variables and that you have
   to initialize them before you can make use of them. so let's do that
   with the following line of code:
# initializing the variables
init = tf.global_variables_initializer()

training and testing the model

   when you train and test your model in tensorflow, you go through the
   following steps:
     * you start off with launching the graph. this is a class that runs
       all the tensorflow operations and launches the graph in a session.
       all the operations have to be within the indentation.
     * then, you run the session, which will execute the variables that
       were initialized in the previous step and evaluates the tensor.
     * next, you define a for loop that runs for the number of training
       iterations you had specified in the beginning. right after that,
       you'll initiate a second for loop, which is for the number of
       batches that you will have based on the batch size you chose, so
       you divide the total number of images by the batch size.
     * you will then input the images based on the batch size you pass in
       batch_x and their respective labels in batch_y.
     * now is the most important step. just like you ran the initializer
       after creating the graph, now you feed the placeholders x and y the
       actual data in a dictionary and run the session by passing the cost
       and the accuracy that you had defined earlier. it returns the loss
       (cost) and accuracy.
     * you can print the loss and training accuracy after each epoch
       (training iteration) is completed.

   after each training iteration is completed, you run only the accuracy
   by passing all the 10000 test images and labels. this will give you an
   idea of how accurately your model is performing while it is training.

   it's usually recommended to do the testing once your model is trained
   completely and validate only while it is in training phase after each
   epoch. however, let's stick with this approach for now.
with tf.session() as sess:
    sess.run(init)
    train_loss = []
    test_loss = []
    train_accuracy = []
    test_accuracy = []
    summary_writer = tf.summary.filewriter('./output', sess.graph)
    for i in range(training_iters):
        for batch in range(len(train_x)//batch_size):
            batch_x = train_x[batch*batch_size:min((batch+1)*batch_size,len(trai
n_x))]
            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(trai
n_y))]
            # run optimization op (backprop).
                # calculate batch loss and accuracy
            opt = sess.run(optimizer, feed_dict={x: batch_x,
                                                              y: batch_y})
            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,
                                                              y: batch_y})
        print("iter " + str(i) + ", loss= " + \
                      "{:.6f}".format(loss) + ", training accuracy= " + \
                      "{:.5f}".format(acc))
        print("optimization finished!")

        # calculate accuracy for all 10000 mnist test images
        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_x,y :
 test_y})
        train_loss.append(loss)
        test_loss.append(valid_loss)
        train_accuracy.append(acc)
        test_accuracy.append(test_acc)
        print("testing accuracy:","{:.5f}".format(test_acc))
    summary_writer.close()

iter 0, loss= 0.338081, training accuracy= 0.87500
optimization finished!
('testing accuracy:', '0.83890')
iter 1, loss= 0.210727, training accuracy= 0.91406
optimization finished!
('testing accuracy:', '0.87810')
iter 2, loss= 0.169724, training accuracy= 0.95312
optimization finished!
('testing accuracy:', '0.89260')
iter 3, loss= 0.154453, training accuracy= 0.93750
optimization finished!
('testing accuracy:', '0.89600')
iter 4, loss= 0.143760, training accuracy= 0.93750
optimization finished!
('testing accuracy:', '0.89610')
iter 5, loss= 0.142700, training accuracy= 0.93750
optimization finished!
('testing accuracy:', '0.89680')
iter 6, loss= 0.114542, training accuracy= 0.94531
optimization finished!
('testing accuracy:', '0.90190')
iter 7, loss= 0.104471, training accuracy= 0.94531
optimization finished!
('testing accuracy:', '0.90100')
iter 8, loss= 0.089115, training accuracy= 0.96094
optimization finished!
('testing accuracy:', '0.90360')
iter 9, loss= 0.090392, training accuracy= 0.96094
optimization finished!
('testing accuracy:', '0.90420')
iter 10, loss= 0.066802, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.89960')
iter 11, loss= 0.062734, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.89870')
iter 12, loss= 0.071126, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.88770')
iter 13, loss= 0.051628, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.89670')
iter 14, loss= 0.049411, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.90280')
iter 15, loss= 0.057557, training accuracy= 0.97656
optimization finished!
('testing accuracy:', '0.89920')
iter 16, loss= 0.053462, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.89780')
iter 17, loss= 0.042286, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.89980')
iter 18, loss= 0.017384, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.89930')
iter 19, loss= 0.017027, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.89130')
iter 20, loss= 0.032651, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.89500')
iter 21, loss= 0.032651, training accuracy= 0.98438
optimization finished!
('testing accuracy:', '0.88890')
iter 22, loss= 0.030661, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91070')
iter 23, loss= 0.010199, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90620')
iter 24, loss= 0.006742, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91330')
iter 25, loss= 0.015453, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91110')
iter 26, loss= 0.011107, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91070')
iter 27, loss= 0.012601, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90920')
iter 28, loss= 0.013160, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90470')
iter 29, loss= 0.006266, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91160')
iter 30, loss= 0.007183, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91080')
iter 31, loss= 0.006205, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91490')
iter 32, loss= 0.008915, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.90940')
iter 33, loss= 0.001174, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91470')
iter 34, loss= 0.002065, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91310')
iter 35, loss= 0.002440, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91210')
iter 36, loss= 0.001424, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91710')
iter 37, loss= 0.002666, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91510')
iter 38, loss= 0.001833, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91070')
iter 39, loss= 0.004789, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91510')
iter 40, loss= 0.003274, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91160')
iter 41, loss= 0.001958, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91270')
iter 42, loss= 0.004119, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90950')
iter 43, loss= 0.003570, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90750')
iter 44, loss= 0.008136, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91090')
iter 45, loss= 0.003319, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91210')
iter 46, loss= 0.001454, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91420')
iter 47, loss= 0.000695, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91050')
iter 48, loss= 0.002378, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91070')
iter 49, loss= 0.001328, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91520')
iter 50, loss= 0.002429, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90830')
iter 51, loss= 0.000840, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91150')
iter 52, loss= 0.002838, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91120')
iter 53, loss= 0.001496, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91040')
iter 54, loss= 0.001293, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91010')
iter 55, loss= 0.002120, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91390')
iter 56, loss= 0.000601, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91540')
iter 57, loss= 0.001594, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91200')
iter 58, loss= 0.001877, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91370')
iter 59, loss= 0.000769, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91670')
iter 60, loss= 0.002815, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91100')
iter 61, loss= 0.007895, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91280')
iter 62, loss= 0.009527, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91520')
iter 63, loss= 0.003365, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91010')
iter 64, loss= 0.001035, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91570')
iter 65, loss= 0.004112, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91210')
iter 66, loss= 0.002977, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91100')
iter 67, loss= 0.000183, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91530')
iter 68, loss= 0.000348, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91290')
iter 69, loss= 0.001012, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90890')
iter 70, loss= 0.010831, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91020')
iter 71, loss= 0.000026, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91830')
iter 72, loss= 0.000644, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91190')
iter 73, loss= 0.001083, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91410')
iter 74, loss= 0.000335, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91290')
iter 75, loss= 0.012580, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91660')
iter 76, loss= 0.000295, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91390')
iter 77, loss= 0.001756, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91280')
iter 78, loss= 0.001754, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91300')
iter 79, loss= 0.086850, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91600')
iter 80, loss= 0.002057, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91170')
iter 81, loss= 0.018806, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91100')
iter 82, loss= 0.000346, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91100')
iter 83, loss= 0.000076, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91240')
iter 84, loss= 0.000004, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91590')
iter 85, loss= 0.001539, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90970')
iter 86, loss= 0.000007, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91530')
iter 87, loss= 0.002044, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91350')
iter 88, loss= 0.000013, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91260')
iter 89, loss= 0.000201, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90670')
iter 90, loss= 0.000217, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91070')
iter 91, loss= 0.000109, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91990')
iter 92, loss= 0.000056, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91500')
iter 93, loss= 0.001038, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91120')
iter 94, loss= 0.000383, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91110')
iter 95, loss= 0.000004, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91650')
iter 96, loss= 0.000905, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91490')
iter 97, loss= 0.000080, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91370')
iter 98, loss= 0.013281, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91180')
iter 99, loss= 0.005379, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91390')
iter 100, loss= 0.001301, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91240')
iter 101, loss= 0.000181, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91610')
iter 102, loss= 0.000176, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91490')
iter 103, loss= 0.000249, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91160')
iter 104, loss= 0.000944, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.90990')
iter 105, loss= 0.000097, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91700')
iter 106, loss= 0.000887, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91350')
iter 107, loss= 0.000004, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91720')
iter 108, loss= 0.000010, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91530')
iter 109, loss= 0.000097, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91670')
iter 110, loss= 0.000072, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91780')
iter 111, loss= 0.000240, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91640')
iter 112, loss= 0.002454, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91370')
iter 113, loss= 0.000007, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91690')
iter 114, loss= 0.000013, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91340')
iter 115, loss= 0.000450, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91440')
iter 116, loss= 0.000401, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91680')
iter 117, loss= 0.000428, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91510')
iter 118, loss= 0.000002, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91680')
iter 119, loss= 0.000159, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91640')
iter 120, loss= 0.000326, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91610')
iter 121, loss= 0.000944, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91060')
iter 122, loss= 0.003092, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91160')
iter 123, loss= 0.000741, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91460')
iter 124, loss= 0.000804, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91710')
iter 125, loss= 0.000302, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91900')
iter 126, loss= 0.000462, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91470')
iter 127, loss= 0.000300, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91560')
iter 128, loss= 0.000005, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91470')
iter 129, loss= 0.000101, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91610')
iter 130, loss= 0.000666, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91530')
iter 131, loss= 0.000094, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.92080')
iter 132, loss= 0.011843, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91940')
iter 133, loss= 0.002057, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91220')
iter 134, loss= 0.000372, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91760')
iter 135, loss= 0.000468, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91360')
iter 136, loss= 0.000006, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91880')
iter 137, loss= 0.000097, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91150')
iter 138, loss= 0.000013, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91740')
iter 139, loss= 0.000171, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91520')
iter 140, loss= 0.000157, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91310')
iter 141, loss= 0.000008, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91630')
iter 142, loss= 0.000035, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91680')
iter 143, loss= 0.000693, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91560')
iter 144, loss= 0.000539, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91470')
iter 145, loss= 0.000129, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91680')
iter 146, loss= 0.000347, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91660')
iter 147, loss= 0.000241, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.92050')
iter 148, loss= 0.000007, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91640')
iter 149, loss= 0.000021, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91870')
iter 150, loss= 0.000358, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91390')
iter 151, loss= 0.000101, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91460')
iter 152, loss= 0.000044, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91400')
iter 153, loss= 0.000015, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91700')
iter 154, loss= 0.000063, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91680')
iter 155, loss= 0.000149, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91140')
iter 156, loss= 0.000277, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91470')
iter 157, loss= 0.000098, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91910')
iter 158, loss= 0.000023, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91490')
iter 159, loss= 0.000239, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91640')
iter 160, loss= 0.001147, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91570')
iter 161, loss= 0.000009, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91650')
iter 162, loss= 0.000963, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91630')
iter 163, loss= 0.000422, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91530')
iter 164, loss= 0.000007, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91360')
iter 165, loss= 0.000026, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91480')
iter 166, loss= 0.000294, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91340')
iter 167, loss= 0.000350, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91330')
iter 168, loss= 0.000917, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91990')
iter 169, loss= 0.000174, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91390')
iter 170, loss= 0.000066, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91740')
iter 171, loss= 0.000078, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91560')
iter 172, loss= 0.000020, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91540')
iter 173, loss= 0.000010, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91630')
iter 174, loss= 0.000048, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91720')
iter 175, loss= 0.008362, training accuracy= 0.99219
optimization finished!
('testing accuracy:', '0.91170')
iter 176, loss= 0.000415, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91590')
iter 177, loss= 0.000202, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91620')
iter 178, loss= 0.000279, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91560')
iter 179, loss= 0.000003, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91100')
iter 180, loss= 0.000128, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91750')
iter 181, loss= 0.000288, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91930')
iter 182, loss= 0.000138, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91680')
iter 183, loss= 0.000400, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91990')
iter 184, loss= 0.000049, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.92000')
iter 185, loss= 0.000866, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91420')
iter 186, loss= 0.000241, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91670')
iter 187, loss= 0.000004, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91610')
iter 188, loss= 0.000058, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91290')
iter 189, loss= 0.000194, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91650')
iter 190, loss= 0.000008, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91480')
iter 191, loss= 0.000010, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91790')
iter 192, loss= 0.000916, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91710')
iter 193, loss= 0.000006, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91460')
iter 194, loss= 0.000001, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91600')
iter 195, loss= 0.000046, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91460')
iter 196, loss= 0.000044, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91750')
iter 197, loss= 0.000633, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91110')
iter 198, loss= 0.000028, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91830')
iter 199, loss= 0.000206, training accuracy= 1.00000
optimization finished!
('testing accuracy:', '0.91870')

   the test accuracy looks impressive. it turns out that your classifier
   does better than the benchmark that was reported [19]here, which is an
   id166 classifier with mean accuracy of 0.897. also, the model does well
   compared to some of the deep learning models mentioned on the
   [20]github profile of the creators of fashion-mnist dataset.

   however, you saw that the model looked like it was overfitting since
   the training accuracy is more than the testing accuracy. are these
   results really all that good?

   let's put your model evaluation into perspective and plot the accuracy
   and loss plots between training and validation data:
plt.plot(range(len(train_loss)), train_loss, 'b', label='training loss')
plt.plot(range(len(train_loss)), test_loss, 'r', label='test loss')
plt.title('training and test loss')
plt.xlabel('epochs ',fontsize=16)
plt.ylabel('loss',fontsize=16)
plt.legend()
plt.figure()
plt.show()

<matplotlib.figure.figure at 0x7feac8194250>

   [output_87_0_fxlljb.png]
plt.plot(range(len(train_loss)), train_accuracy, 'b', label='training accuracy')
plt.plot(range(len(train_loss)), test_accuracy, 'r', label='test accuracy')
plt.title('training and test accuracy')
plt.xlabel('epochs ',fontsize=16)
plt.ylabel('loss',fontsize=16)
plt.legend()
plt.figure()
plt.show()

<matplotlib.figure.figure at 0x7feac80419d0>

   [output_88_0_csdpyx.png]

   from the above two plots, you can see that the test accuracy almost
   became stagnant after 50-60 epochs and rarely increased at certain
   epochs. in the beginning, the validation accuracy was linearly
   increasing with loss, but then it did not increase much.

   the validation loss shows that this is the sign of overfitting, similar
   to test accuracy it linearly decreased but after 25-30 epochs, it
   started to increase. this means that the model tried to memorize the
   data and succeeded.

   this was it for this tutorial, but there is a task for you all:
     * your task is to reduce the overfitting of the above model, by
       introducing dropout technique. for simplicity, you may like to
       follow along with the tutorial [21]convolutional neural networks in
       python with keras, even though it is in keras, but still the
       accuracy and loss heuristics are pretty much the same. so,
       following along with this tutorial will help you to add dropout
       layers in your current model. since, both of the tutorial have
       exactly similar architecture.
     * secondly, try to improve the testing accuracy, may be by deepening
       the network a bit, or adding learning rate decay for faster
       convergence, or try playing with the optimizer and so on!

go further and master deep learning with tensorflow!

   this tutorial was good start to understanding how tensorflow works
   underneath the hood along with an implementation of convolutional
   neural networks in python. if you were able to follow along easily or
   even with little more efforts, well done! try doing some experiments
   maybe with same model architecture but using different types of public
   datasets available. you could also try playing with different weight
   intializers, may be deepen the network architecture, change learning
   rate etc. and see how your network performs by changing these
   parameters. but try changing them one at a time only then you will get
   more intuition about these parameters.

   there is still a lot to cover, so why not take datacamp   s [22]deep
   learning in python course? in the meantime, also make sure to check out
   the [23]tensorflow documentation, if you haven   t done so already. you
   will find more examples and information on all functions, arguments,
   more layers, etc. it will undoubtedly be an indispensable resource when
   you   re learning how to work with neural networks in python!
   45
   45
   [24]0

   related posts
   must read
   python
   +4

[25]keras tutorial: deep learning in python

   karlijn willems
   february 4th, 2019
   python
   +4

[26]convolutional neural networks in python with keras

   aditya sharma
   december 5th, 2017

   (button)
   post a comment

   [27]subscribe to rss
   [28]about[29]terms[30]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#comments
   3. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#tensors
   4. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#tensorflow
   5. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#computational_graph
   6. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#placeholder
   7. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#id98
   8. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#dnn
   9. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#training
  10. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#dropout
  11. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#dropout_evaluate
  12. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#predictions
  13. https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  14. https://arxiv.org/abs/1708.07747
  15. https://www.datacamp.com/community/tutorials/deep-learning-python
  16. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  17. https://github.com/zalandoresearch/fashion-mnist
  18. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
  19. http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/
  20. https://github.com/zalandoresearch/fashion-mnist
  21. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#explore
  22. https://www.datacamp.com/courses/deep-learning-in-python
  23. https://www.tensorflow.org/
  24. https://www.datacamp.com/community/tutorials/id98-tensorflow-python#comments
  25. https://www.datacamp.com/community/tutorials/deep-learning-python
  26. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  27. https://www.datacamp.com/community/rss.xml
  28. https://www.datacamp.com/about
  29. https://www.datacamp.com/terms-of-use
  30. https://www.datacamp.com/privacy-policy

   hidden links:
  32. https://www.datacamp.com/
  33. https://www.datacamp.com/community
  34. https://www.datacamp.com/community/tutorials
  35. https://www.datacamp.com/community/data-science-cheatsheets
  36. https://www.datacamp.com/community/open-courses
  37. https://www.datacamp.com/community/podcast
  38. https://www.datacamp.com/community/chat
  39. https://www.datacamp.com/community/blog
  40. https://www.datacamp.com/community/tech
  41. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/id98-tensorflow-python
  42. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/id98-tensorflow-python
  43. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/id98-tensorflow-python
  44. https://www.datacamp.com/profile/adityasharma101993
  45. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/id98-tensorflow-python
  46. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/id98-tensorflow-python
  47. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/id98-tensorflow-python
  48. https://www.datacamp.com/profile/karlijn
  49. https://www.datacamp.com/profile/adityasharma101993
  50. https://www.facebook.com/pages/datacamp/726282547396228
  51. https://twitter.com/datacamp
  52. https://www.linkedin.com/company/datamind-org
  53. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
