community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   12
   12
   elias dabbas
   april 24th, 2018
   webscraping
   +1

absolute and weighted frequency of words in text

   in this tutorial, you'll learn about absolute and weighted word
   frequency in id111 and how to calculate it with defaultdict and
   pandas dataframes.

   an important set of metrics in id111 relates to the frequency of
   words (or any token) in a certain corpus of text documents. however,
   you can also use an additional set of metrics in cases where each
   document has an associated numeric value describing a certain attribute
   of the document.

   some examples:
     * tweets and their respective number of engagements.
     * urls and their pageviews and bounces.
     * movie titles and their gross revenue.
     * keywords and their impressions, clicks, and conversions.

   in this tutorial,
     * you will first go through the process of creating a simple function
       that calculates and compares the absolute and weighted occurrence
       of words in a corpus of documents. this can sometimes uncover
       hidden trends and aggregates that aren't necessarily clear by
       looking at the top ten or so values. they can often be different
       from the absolute word frequency as well.
     * then, you will see a real-life data set (movie titles and the gross
       revenue), and hope to discover hidden trends. a teaser: love will
       come up somehow!
     * you will be using python as a programming language and use the
       collections module's defaultdict data structure for the heavy
       lifting, as well as pandas dataframes to manage the final output.

absolute and weighted word frequency: introduction

   let's assume that you have two tweets and that their content and number
   of impressions (views) are as follows:
   tweet text views
   spain      800
   france     200

   it is simple to do the basic analysis and find out that your words are
   split 50:50 between 'france' and 'spain'. in many cases, this is all
   you have, and you can only measure the absolute frequency of words, and
   try to infer certain relationships. in this case, you have some data
   about each of the documents.

   the weighed frequency here, is clearly different, and the split is
   80:20. in other words, although 'spain' and 'france' both appeared once
   each in your tweets, from your readers' perspective, the former
   appeared 800 times, while the latter appeared 200 times. there's a big
   difference!

simple word frequency using defaultdict

   now consider this slightly more involved example for a similar set of
   documents:
        document      views
   france             200
   spain              180
   spain beaches      170
   france beaches     160
   spain best beaches 160

   you now loop through the documents, split them into words, and count
   the occurrences of each of the words:
from collections import defaultdict

import pandas as pd

text_list = ['france', 'spain', 'spain beaches', 'france beaches', 'spain best b
eaches']

word_freq = defaultdict(int)

for text in text_list:
    for word in text.split():
        word_freq[word] += 1

pd.dataframe.from_dict(word_freq, orient='index') \
.sort_values(0, ascending=false) \
.rename(columns={0: 'abs_freq'})

           abs_freq
    spain  3
   beaches 3
   france  2
    best   1

   in the loop above, the first line loops through text_list one by one.
   the second line (within each document) loops through the words of each
   item, split by the space character (which could have been any other
   character ('-', ',', '_', etc.)).

   when you try to assign a value to word_freq[word] there are two
   possible scenarios:
    1. the key word exists: in which case the assignment is done (adding
       one)
    2. the key word is not in word_freq, in this case defaultdict calls
       the default function that it was assigned to when it was first
       defined, which is int in this case.

   when int is called it returns zero. now the key exists, its value is
   zero, and it is ready to get assigned an additional 1 to its value.

   although the top word was 'france' in the first table, after counting
   all the words within each document we can see that 'spain' and
   'beaches' are tied for the first position. this is important in
   uncovering hidden trends, especially when the list of documents you are
   dealing with, is in the tens, or hundreds, of thousands.

weighted word frequency

   now that you have counted the occurences of each word in the corpus of
   documents, you want to see the weighted frequency. that is, you want to
   see how many times the words appeared to your readers, compared to how
   many times you used them.

   in the first table, the absolute frequency of the words was split
   evenly between 'spain' and 'france', but 'spain' had clearly much more
   weight, because its value was 800, versus 200 or 'france'.

   but what would be the weighted word frequency for the second, slightly
   more complex, table?

   let's find out!

   you can re-use some of the code that you used above, but with some
   additions:
# default value is now a list with two ints
word_freq = defaultdict(lambda: [0, 0])

# the `views` column you had in the first dataframe
num_list = [200, 180, 170, 160, 160]

# looping is now over both the text and the numbers
for text, num in zip(text_list, num_list):
    for word in text.split():
        # same as before
        word_freq[word][0] += 1
        # new line, incrementing the numeric value for each word
        word_freq[word][1] += num

columns = {0: 'abs_freq', 1: 'wtd_freq'}

abs_wtd_df = pd.dataframe.from_dict(word_freq, orient='index') \
             .rename(columns=columns) \
             .sort_values('wtd_freq', ascending=false) \
             .assign(rel_value=lambda df: df['wtd_freq'] / df['abs_freq']) \
            .round()

abs_wtd_df.style.background_gradient(low=0, high=.7, subset=['rel_value'])

           abs_freq wtd_freq rel_value
    spain  3        510      170
   beaches 3        490      163
   france  2        360      180
    best   1        160      160

   some observations:
     * although 'france' was the highest phrase overall, 'spain' and
       'beaches' seem to be more prominent when you take the weighted
       frequency.
     * rel_value is a simple division to get the value per occurrence of
       each word.
     * looking at rel_value, you also see that, even though 'france' is
       quite low on the wtd_freq metric, there seems to be potential in
       it, because the value per occurence is high. this might hint at
       increasing your content coverage of 'france' for example.

   you might also like to add some other metrics that show the percentages
   and cumulative percenatages of each type of frequency so that you can
   get a better perspective on how many words form the bulk of the total,
   if any:
abs_wtd_df.insert(1, 'abs_perc', value=abs_wtd_df['abs_freq'] / abs_wtd_df['abs_
freq'].sum())
abs_wtd_df.insert(2, 'abs_perc_cum', abs_wtd_df['abs_perc'].cumsum())
abs_wtd_df.insert(4, 'wtd_freq_perc', abs_wtd_df['wtd_freq'] / abs_wtd_df['wtd_f
req'].sum())
abs_wtd_df.insert(5, 'wtd_freq_perc_cum', abs_wtd_df['wtd_freq_perc'].cumsum())
abs_wtd_df.style.background_gradient(low=0, high=0.8)

   abs_freq abs_perc abs_perc_cum wtd_freq wtd_freq_perc wtd_freq_perc_cum
   rel_value
   spain 3 0.333333 0.333333 510 0.335526 0.335526 170
   beaches 3 0.333333 0.666667 490 0.322368 0.657895 163
   france 2 0.222222 0.888889 360 0.236842 0.894737 180
   best 1 0.111111 1 160 0.105263 1 160

   more can be analyzed, and with more data you would typically get more
   surprises.

   so how might this look in a real-world setting with some real data?

   you will take a look at movie titles, see which words are most used in
   the titles -which is the absolute frequency-, and which words are
   associated with the most revenue, or the weighted frequency.

   [3]boxoffice mojo has a list of more than 15,000 movies, together with
   their associated gross revenue and ranks. start by scraping the data
   using requests and beautifulsoup - you can already explore the
   boxoffice mojo here if you'd like:

   iframe: [4]https://boxofficemojo.herokuapp.com/

import requests
from bs4 import beautifulsoup

final_list = []

for i in range(1, 156):
    if not i%10:
        print(i)
    page = 'http://www.boxofficemojo.com/alltime/domestic.htm?page=' + str(i) +
'&p=.htm'
    resp = requests.get(page)
    soup = beautifulsoup(resp.text, 'lxml')
    # trial and error to get the exact positions
    table_data = [x.text for x in soup.select('tr td')[11:511]]
    # put every 5 values in a row
    temp_list = [table_data[i:i+5] for i in range(0, len(table_data[:-4]), 5)]
    for temp in temp_list:
        final_list.append(temp)

10
20
30
40
50
60
70
80
90
100
110
120
130
140
150

boxoffice_df = pd.dataframe.from_records(final_list)

boxoffice_df.head(10)

     0               1                2        3         4
   0 1  star wars: the force awakens bv   $936,662,225 2015
   1 2  avatar                       fox  $760,507,625 2009^
   2 3  black panther                bv   $681,084,109 2018
   3 4  titanic                      par. $659,363,944 1997^
   4 5  jurassic world               uni. $652,270,625 2015
   5 6  marvel's the avengers        bv   $623,357,910 2012
   6 7  star wars: the last jedi     bv   $620,181,382 2017
   7 8  the dark knight              wb   $534,858,444 2008^
   8 9  rogue one: a star wars story bv   $532,177,324 2016
   9 10 beauty and the beast (2017)  bv   $504,014,165 2017
boxoffice_df.tail(15)

           0                   1                    2      3    4
   15485 15486 the dark hours                    n/a      $423 2005
   15486 15487 2:22                              magn.    $422 2017
   15487 15488 state park                        atl      $421 1988
   15488 15489 the magician (2010)               reg.     $406 2010
   15489 15490 skinless                          ppf      $400 2014
   15490 15491 cinemanovels                      mont.    $398 2014
   15491 15492 hannah: buddhism's untold journey kl       $396 2016
   15492 15493 apartment 143                     magn.    $383 2012
   15493 15494 the marsh                         all.     $336 2007
   15494 15495 the chambermaid                   fm       $315 2015
   15495 15496 news from planet mars             kl       $310 2016
   15496 15497 trojan war                        wb       $309 1997
   15497 15498 lou! journal infime               distrib. $287 2015
   15498 15499 intervention                      all.     $279 2007
   15499 15500 playback                          magn.    $264 2012

   you will see that some numeric values have some special characters, ($,
   , , and ^), and some values are actually n/a. so you need to change
   those:
na_year_idx =  [i for i, x in enumerate(final_list) if x[4] == 'n/a'] # get the
indexes of the 'n/a' values
new_years = [1998, 1999, 1960, 1973]  # got them by checking online

print(*[(i, x) for i, x in enumerate(final_list) if i in na_year_idx], sep='\n')
print('new year values:', new_years)

(8003, ['8004', 'warner bros. 75th anniversary film festival', 'wb', '$741,855',
 'n/a'])
(8148, ['8149', 'hum aapke dil mein rahte hain', 'eros', '$668,678', 'n/a'])
(8197, ['8198', 'purple moon (re-issue)', 'mira.', '$640,945', 'n/a'])
(10469, ['10470', 'amarcord', 'jan.', '$125,493', 'n/a'])
new year values: [1998, 1999, 1960, 1973]

for na_year, new_year in zip(na_year_idx, new_years):
    final_list[na_year][4] = new_year
    print(final_list[na_year], new_year)

['8004', 'warner bros. 75th anniversary film festival', 'wb', '$741,855', 1998]
1998
['8149', 'hum aapke dil mein rahte hain', 'eros', '$668,678', 1999] 1999
['8198', 'purple moon (re-issue)', 'mira.', '$640,945', 1960] 1960
['10470', 'amarcord', 'jan.', '$125,493', 1973] 1973

   now you turn the list into a pandas dataframe by naming the columns
   with the appropriate names, and converting to the data types that you
   want.
import re
regex = '|'.join(['\$', ',', '\^'])

columns = ['rank', 'title', 'studio', 'lifetime_gross', 'year']

boxoffice_df = pd.dataframe({
    'rank': [int(x[0]) for x in final_list],  # convert ranks to integers
    'title': [x[1] for x in final_list],  # get titles as is
    'studio': [x[2] for x in final_list],  # get studio names as is
    'lifetime_gross': [int(re.sub(regex, '', x[3])) for x in final_list],  # rem
ove special characters and convert to integer
    'year': [int(re.sub(regex, '', str(x[4]))) for x in final_list],  # remove s
pecial characters and convert to integer
})
print('rows:', boxoffice_df.shape[0])
print('columns:', boxoffice_df.shape[1])
print('\ndata types:')
print(boxoffice_df.dtypes)
boxoffice_df.head(15)

rows: 15500
columns: 5

data types:
lifetime_gross     int64
rank               int64
studio            object
title             object
year               int64
dtype: object

      lifetime_gross rank studio                   title                   year
   0  936662225      1    bv     star wars: the force awakens              2015
   1  760507625      2    fox    avatar                                    2009
   2  681084109      3    bv     black panther                             2018
   3  659363944      4    par.   titanic                                   1997
   4  652270625      5    uni.   jurassic world                            2015
   5  623357910      6    bv     marvel's the avengers                     2012
   6  620181382      7    bv     star wars: the last jedi                  2017
   7  534858444      8    wb     the dark knight                           2008
   8  532177324      9    bv     rogue one: a star wars story              2016
   9  504014165      10   bv     beauty and the beast (2017)               2017
   10 486295561      11   bv     finding dory                              2016
   11 474544677      12   fox    star wars: episode i - the phantom menace 1999
   12 460998007      13   fox    star wars                                 1977
   13 459005868      14   bv     avengers: age of ultron                   2015
   14 448139099      15   wb     the dark knight rises                     2012

   the word 'star' is one of the top, as it appears in five of the top
   fifteen movies, and you also know that the star wars series has even
   more movies, several of them in the top as well.

   let's now utilize the code you developed and see how it works on this
   data set. there's nothing new in the code below, you simply put it all
   in one function:
def word_frequency(text_list, num_list, sep=none):
    word_freq = defaultdict(lambda: [0, 0])

    for text, num in zip(text_list, num_list):
        for word in text.split(sep=sep):
            word_freq[word][0] += 1
            word_freq[word][1] += num

    columns = {0: 'abs_freq', 1: 'wtd_freq'}

    abs_wtd_df = (pd.dataframe.from_dict(word_freq, orient='index')
                 .rename(columns=columns )
                 .sort_values('wtd_freq', ascending=false)
                 .assign(rel_value=lambda df: df['wtd_freq'] / df['abs_freq']).r
ound())

    abs_wtd_df.insert(1, 'abs_perc', value=abs_wtd_df['abs_freq'] / abs_wtd_df['
abs_freq'].sum())
    abs_wtd_df.insert(2, 'abs_perc_cum', abs_wtd_df['abs_perc'].cumsum())
    abs_wtd_df.insert(4, 'wtd_freq_perc', abs_wtd_df['wtd_freq'] / abs_wtd_df['w
td_freq'].sum())
    abs_wtd_df.insert(5, 'wtd_freq_perc_cum', abs_wtd_df['wtd_freq_perc'].cumsum
())

    return abs_wtd_df
word_frequency(boxoffice_df['title'], boxoffice_df['lifetime_gross']).head()

   abs_freq abs_perc abs_perc_cum wtd_freq wtd_freq_perc wtd_freq_perc_cum
   rel_value
   the 3055 0.068747 0.068747 67518342498 0.081167 0.081167 22100930.0
   the 1310 0.029479 0.098227 32973100860 0.039639 0.120806 25170306.0
   of 1399 0.031482 0.129709 30180592467 0.036282 0.157087 21572975.0
   and 545 0.012264 0.141973 12188113847 0.014652 0.171739 22363512.0
   2 158 0.003556 0.145529 9032673058 0.010859 0.182598 57168817.0

   unsurprisingly, the 'stop words' are the top ones, which is pretty much
   the same for most collections of documents. you also have them
   duplicated, where some are capitalized and some are not. so you have
   two clear things to take care of:
    1. remove all stop words: you can do this by adding a new parameter to
       the function, and supplying your own list of stop words.
    2. handle all words in lower case to remove duplicates

   here is a simple update to the function (new rm_words parameter, as
   well as lines 6,7, and 8):
# words will be expanded
def word_frequency(text_list, num_list, sep=none, rm_words=('the', 'and', 'a')):

    word_freq = defaultdict(lambda: [0, 0])

    for text, num in zip(text_list, num_list):
        for word in text.split(sep=sep):
            # this should take care of ignoring the word if it's in the stop wor
ds
            if word.lower() in rm_words:
                continue
            # .lower() makes sure we are not duplicating words
            word_freq[word.lower()][0] += 1
            word_freq[word.lower()][1] += num

    columns = {0: 'abs_freq', 1: 'wtd_freq'}

    abs_wtd_df = (pd.dataframe.from_dict(word_freq, orient='index')
                 .rename(columns=columns )
                 .sort_values('wtd_freq', ascending=false)
                 .assign(rel_value=lambda df: df['wtd_freq'] / df['abs_freq']).r
ound())

    abs_wtd_df.insert(1, 'abs_perc', value=abs_wtd_df['abs_freq'] / abs_wtd_df['
abs_freq'].sum())
    abs_wtd_df.insert(2, 'abs_perc_cum', abs_wtd_df['abs_perc'].cumsum())
    abs_wtd_df.insert(4, 'wtd_freq_perc', abs_wtd_df['wtd_freq'] / abs_wtd_df['w
td_freq'].sum())
    abs_wtd_df.insert(5, 'wtd_freq_perc_cum', abs_wtd_df['wtd_freq_perc'].cumsum
())

    abs_wtd_df = abs_wtd_df.reset_index().rename(columns={'index': 'word'})

    return abs_wtd_df

from collections import defaultdict
word_freq_df =  word_frequency(boxoffice_df['title'],
                               boxoffice_df['lifetime_gross'],
                               rm_words=['of','in', 'to', 'and', 'a', 'the',
                                         'for', 'on', '&', 'is', 'at', 'it',
                                         'from', 'with'])



word_freq_df.head(15).style.bar(['abs_freq', 'wtd_freq', 'rel_value'],
                                               color='#60ddff') # e6e9eb

   word abs_freq abs_perc abs_perc_cum wtd_freq wtd_freq_perc
   wtd_freq_perc_cum rel_value
   0 2 158 0.00443945 0.00443945 9032673058 0.0137634 0.0137634
   5.71688e+07
   1 star 45 0.0012644 0.00570385 5374658819 0.00818953 0.0219529
   1.19437e+08
   2 man 195 0.00547907 0.0111829 3967037854 0.0060447 0.0279976
   2.03438e+07
   3 part 41 0.00115201 0.0123349 3262579777 0.00497129 0.0329689
   7.95751e+07
   4 movie 117 0.00328744 0.0156224 3216050557 0.00490039 0.0378693
   2.74876e+07
   5 3 63 0.00177016 0.0173925 3197591193 0.00487227 0.0427415 5.07554e+07
   6 ii 67 0.00188255 0.0192751 3077712883 0.0046896 0.0474311 4.5936e+07
   7 wars: 6 0.000168587 0.0194437 2757497155 0.00420168 0.0516328
   4.59583e+08
   8 last 133 0.003737 0.0231807 2670229651 0.00406871 0.0557015
   2.00769e+07
   9 harry 27 0.00075864 0.0239393 2611329714 0.00397896 0.0596805
   9.67159e+07
   10 me 140 0.00393369 0.027873 2459330128 0.00374736 0.0634279
   1.75666e+07
   11 potter 10 0.000280978 0.028154 2394811427 0.00364905 0.0670769
   2.39481e+08
   12 black 71 0.00199494 0.0301489 2372306467 0.00361476 0.0706917
   3.34128e+07
   13 - 49 0.00137679 0.0315257 2339484878 0.00356474 0.0742564
   4.77446e+07
   14 story 107 0.00300646 0.0345322 2231437526 0.00340011 0.0776565
   2.08546e+07

   let's take a look at the same dataframe sorted based on abs_freq:
(word_freq_df.sort_values('abs_freq', ascending=false)
 .head(15)
    .style.bar(['abs_freq', 'wtd_freq', 'rel_value'],
               color='#60ddff'))

   word abs_freq abs_perc abs_perc_cum wtd_freq wtd_freq_perc
   wtd_freq_perc_cum rel_value
   26 love 211 0.00592863 0.069542 1604206885 0.00244438 0.111399
   7.60288e+06
   2 man 195 0.00547907 0.0111829 3967037854 0.0060447 0.0279976
   2.03438e+07
   24 my 191 0.00536668 0.0618994 1629540498 0.00248298 0.106478
   8.53163e+06
   15 i 168 0.00472043 0.0392526 2203439786 0.00335745 0.081014
   1.31157e+07
   0 2 158 0.00443945 0.00443945 9032673058 0.0137634 0.0137634
   5.71688e+07
   10 me 140 0.00393369 0.027873 2459330128 0.00374736 0.0634279
   1.75666e+07
   31 life 134 0.0037651 0.0764822 1534647732 0.00233839 0.123297
   1.14526e+07
   8 last 133 0.003737 0.0231807 2670229651 0.00406871 0.0557015
   2.00769e+07
   23 you 126 0.00354032 0.0565327 1713758853 0.00261131 0.103995
   1.36013e+07
   4 movie 117 0.00328744 0.0156224 3216050557 0.00490039 0.0378693
   2.74876e+07
   14 story 107 0.00300646 0.0345322 2231437526 0.00340011 0.0776565
   2.08546e+07
   33 night 104 0.00292217 0.0814555 1523917360 0.00232204 0.127946
   1.46531e+07
   19 american 93 0.00261309 0.0478505 1868854192 0.00284763 0.0932591
   2.00952e+07
   117 girl 90 0.0025288 0.151644 842771661 0.00128416 0.26663 9.36413e+06
   16 day 87 0.00244451 0.0416971 2164198760 0.00329766 0.0843116
   2.48758e+07

   now let's visualize to compare both and see the hidden trends:
import matplotlib.pyplot as plt
plt.figure(figsize=(20,8))
plt.subplot(1, 2, 1)

word_freq_df_abs = word_freq_df.sort_values('abs_freq', ascending=false).reset_i
ndex()

plt.barh(range(20),
         list(reversed(word_freq_df_abs['abs_freq'][:20])), color='#288fb7')
for i, word in enumerate(word_freq_df_abs['word'][:20]):
    plt.text(word_freq_df_abs['abs_freq'][i], 20-i-1,
             s=str(i+1) + '. ' + word + ': '  + str(word_freq_df_abs['abs_freq']
[i]),
             ha='right', va='center', fontsize=14, color='white', fontweight='bo
ld')
plt.text(0.4, -1.1, s='number of times the word was used in a movie title; out o
f 15500 movies.', fontsize=14)
plt.text(0.4, -1.8, s='data: boxofficemojo.com apr. 2018.   feedback: @eliasdabb
as', fontsize=14)


plt.vlines(range(0, 210, 10), -1, 20, colors='gray', alpha=0.1)
plt.hlines(range(0, 20, 2), 0, 210, colors='gray', alpha=0.1)
plt.yticks([])
plt.xticks([])
plt.title('words most used in movie titles', fontsize=22, fontweight='bold')

# =============
plt.subplot(1, 2, 2)
# plt.axis('off')
plt.barh(range(20),
         list(reversed(word_freq_df['wtd_freq'][:20])), color='#288fb7')
for i, word in enumerate(word_freq_df['word'][:20]):
    plt.text(word_freq_df['wtd_freq'][i], 20-i-1,
             s=str(i+1) + '. ' + word + ': ' + '$' + str(round(word_freq_df['wtd
_freq'][i] / 1000_000_000, 2)) + 'b',
             ha='right', va='center', fontsize=14, color='white', fontweight='bo
ld')
plt.text(0.4, -1.1, s='alltime boxoffice revenue of all movies whos title contai
ned the word. (top word is "2") ', fontsize=14)
plt.text(0.4, -1.8, s='data collection & methodology: http://bit.ly/word_freq',
fontsize=14)

plt.vlines(range(0, 9_500_000_000, 500_000_000), -1, 20, colors='gray', alpha=0.
1)
plt.hlines(range(0, 20, 2), 0, 10_000_000_000, colors='gray', alpha=0.1)
plt.xlim((-70_000_000, 9_500_000_000))
plt.yticks([])
plt.xticks([])
plt.title('words most associated with boxoffice revenue', fontsize=22, fontweigh
t='bold')
plt.tight_layout(pad=0.01)
plt.show()

   [output_27_0_kujy3w.png]

   it seems that in the minds of producers and writers at least, love does
   conquer all! it is the most used word in all of the movie titles. it is
   not that high when it comes to weighted frequency (box-office revenue),
   though.

   in other words, if you look at all the titles of movies, the word
   'love' would be the one you would most find. but estimating which word
   appeared the most in the eyes of the viewers (using gross revenue as a
   metric), then '2', 'star', and 'man' would be the most viewed, or
   associated with the most revenue.

   just to be clear: these are very simple calculations. when you say that
   the weighted frequency of the word 'love' is 1,604,106,767, it simply
   means that the sum of the lifetime gross of all movies who's title
   included the word 'love' was that amount.

   it's also interesting that '2' is the top word. obviously, it is not a
   word, but it's an indication that the second parts of movie series
   amount to a very large sum. so is '3', which is in the fifth position.
   note that 'part' and 'ii' are also in the top ten, confirming the same
   fact.

   'american', and 'movie', have high relative value.

   a quick note on the stop words used in this function

   usually, you would supply a more comprehensive list of stop words than
   the one here, especially if you are dealing with articles, or social
   media posts. for example the nltk package provides lists of stop words
   in several languages, and these can be downloaded and used.

   the words here were chosen after a few checks on the top movies. many
   of these are usually considered stop words, but in the case of movie
   titles, it made sense to keep some of them as they might give some
   insight. for example, the words 'i', 'me', 'you' might hint at some
   social dynamics. another reason is that movie titles are very short
   phrases, and we are trying to make as much sense as we can from them.

   you can definitely try it with your own set of words, and see slightly
   different results.

   looking back at the original list of movie titles, we see that some of
   the top words don't even appear in the top ten, and this is exactly the
   kind of insight that we are trying to uncover by using this approach.
boxoffice_df.head(10)

     lifetime_gross rank studio            title             year
   0 936662225      1    bv     star wars: the force awakens 2015
   1 760507625      2    fox    avatar                       2009
   2 681084109      3    bv     black panther                2018
   3 659363944      4    par.   titanic                      1997
   4 652270625      5    uni.   jurassic world               2015
   5 623357910      6    bv     marvel's the avengers        2012
   6 620181382      7    bv     star wars: the last jedi     2017
   7 534858444      8    wb     the dark knight              2008
   8 532177324      9    bv     rogue one: a star wars story 2016
   9 504014165      10   bv     beauty and the beast (2017)  2017

   next, i think it would make sense to further explore the top words that
   are interesting. let's filter the movies that contain '2' and see:
(boxoffice_df[boxoffice_df['title']
              .str
              .contains('2 | 2', case=false)] # spaces used to exclude words lik
e '2010'
              .head(10))

     lifetime_gross rank studio                    title                    year
 15  441226247      16   dw     shrek 2                                     2004
 30  389813101      31   bv     guardians of the galaxy vol. 2              2017
 31  381011219      32   wb     harry potter and the deathly hallows part 2 2011
 35  373585825      36   sony   spider-man 2                                2004
 38  368061265      39   uni.   despicable me 2                             2013
 64  312433331      65   par.   iron man 2                                  2010
 79  292324737      80   lg/s   the twilight saga: breaking dawn part 2     2012
 87  281723902      88   lgf    the hunger games: mockingjay - part 2       2015
 117 245852179      118  bv     toy story 2                                 1999
 146 226164286      147  nl     rush hour 2                                 2001

   let's also take a peek at the top 'star' movies:
boxoffice_df[boxoffice_df['title'].str.contains('star | star', case=false)].head
(10)

    lifetime_gross rank studio                    title                     year
 0  936662225      1    bv     star wars: the force awakens                 2015
 6  620181382      7    bv     star wars: the last jedi                     2017
 8  532177324      9    bv     rogue one: a star wars story                 2016
11  474544677      12   fox    star wars: episode i - the phantom menace    1999
12  460998007      13   fox    star wars                                    1977
33  380270577      34   fox    star wars: episode iii - revenge of the sith 2005
65  310676740      66   fox    star wars: episode ii - attack of the clones 2002
105 257730019      106  par.   star trek                                    2009
140 228778661      141  par.   star trek into darkness                      2013
301 158848340      302  par.   star trek beyond                             2016

   and, lastly, the top 'man' movies:
boxoffice_df[boxoffice_df['title'].str.contains('man | man', case=false)].head(1
0)

      lifetime_gross rank studio                   title                    year
  18  423315812      19   bv     pirates of the caribbean: dead man's chest 2006
  22  409013994      23   bv     iron man 3                                 2013
  35  373585825      36   sony   spider-man 2                               2004
  48  336530303      49   sony   spider-man 3                               2007
  53  330360194      54   wb     batman v superman: dawn of justice         2016
  59  318412101      60   par.   iron man                                   2008
  64  312433331      65   par.   iron man 2                                 2010
  82  291045518      83   wb     man of steel                               2013
  176 206852432      177  wb     batman begins                              2005
  182 202853933      183  sony   the amazing spider-man 2                   2014

next steps and improvements

   as a first step, you might try to get more words: movie titles are
   extremely short and many times don't convey the literal meaning of the
   words. for example, a godfather is supposed to be a person who
   whitnesses a child's christening, and promises to take care of that
   child (or maybe a mafioso who kills for pleasure?!).

   a further exercise might be to get more detailed descriptions, in
   addition to the movie title. for example:

     "a computer hacker learns from mysterious rebels about the true
     nature of his reality and his role in the war against its
     controllers."

   tells us much more about the movie, than 'the matrix'.

   alternatively, you could also take one of the below paths to enrich
   your analysis:
     * better statistical analysis: handling extreme values / outliers,
       using other metrics.
     * id111: grouping similar words and topics together ('happy',
       'happiness', 'happily', etc.)
     * granlular analysis: running the same function for different years /
       decades, or for certain production studios.

   you might be interested in exploring the data yourself as well as other
   data:
     * [5]boxoffice data
     * [6]gutenberg top 1,000 downloaded books, which invites you to ask
       questions such as:
          + which words are the most used in book titles?
          + which words are the most associated with book downloads?
     * [7]iphone search keywords (obtained through [8]serps)
          + what do people search for together with 'iphone'?
          + what has the most weighted frequency?

   ok, so now yo have explored the counts of words in movie titles, and
   seen the difference between the absolute and weighted frequencies, and
   how letting one out might miss a big part of the picture. you have also
   seen the limitations of this approach, and have some suggestions on how
   to improve your analysis.

   you went through the process of creating a special function that you
   can run easily to analyze any similar text data set with numbers, and
   know how this can improve your understanding of this kind of data set.

   try it out by analyzing your tweets' performance, your website's urls,
   your facebook posts, or any other similar data set you might come
   across.

   it might be easier to just [9]clone the repository with the code and
   try for yourself.

   the word_frequency function is part of the [10]advertools package,
   which you can download and try using in your work / research.

   check it out and let me know! [11]@eliasdabbas
   12
   12
   [12]0
   (button)
   post a comment

   [13]subscribe to rss
   [14]about[15]terms[16]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency#comments
   3. http://www.boxofficemojo.com/alltime/domestic.htm
   4. https://boxofficemojo.herokuapp.com/
   5. https://github.com/eliasdabbas/word_frequency/tree/master/data/boxoffice.csv
   6. https://github.com/eliasdabbas/word_frequency/tree/master/data/gutenberg.csv
   7. https://github.com/eliasdabbas/word_frequency/tree/master/data/iphone.csv
   8. https://serps.com/tools/keyword-research/
   9. https://github.com/eliasdabbas/word_frequency
  10. https://github.com/eliasdabbas/advertools
  11. https://twitter.com/eliasdabbas
  12. https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency#comments
  13. https://www.datacamp.com/community/rss.xml
  14. https://www.datacamp.com/about
  15. https://www.datacamp.com/terms-of-use
  16. https://www.datacamp.com/privacy-policy

   hidden links:
  18. https://www.datacamp.com/
  19. https://www.datacamp.com/community
  20. https://www.datacamp.com/community/tutorials
  21. https://www.datacamp.com/community/data-science-cheatsheets
  22. https://www.datacamp.com/community/open-courses
  23. https://www.datacamp.com/community/podcast
  24. https://www.datacamp.com/community/chat
  25. https://www.datacamp.com/community/blog
  26. https://www.datacamp.com/community/tech
  27. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency
  28. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency
  29. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency
  30. https://www.datacamp.com/profile/elias-1bcfbc85-6b61-466c-a28f-3084efe432d1
  31. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency
  32. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency
  33. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency
  34. https://www.facebook.com/pages/datacamp/726282547396228
  35. https://twitter.com/datacamp
  36. https://www.linkedin.com/company/datamind-org
  37. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
