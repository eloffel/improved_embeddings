   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

write an ai to win at pong from scratch with id23

   [9]go to the profile of dhruv parthasarathy
   [10]dhruv parthasarathy (button) blockedunblock (button)
   followfollowing
   sep 25, 2016

   there   s a huge difference between reading about id23
   and actually implementing it.

   in this post, you   ll implement a neural network for reinforcement
   learning and see it learn more and more as it finally becomes good
   enough to beat the computer in pong! you can play around with other
   such atari games at the [11]openai gym.

   by the end of this post, you   ll be able to do the following:
     * write a neural network from scratch.
     * implement a policy gradient with id23.
     * build an ai for pong that can beat the computer in less than 250
       lines of python.
     * use openai gym.

   iframe: [12]/media/8073d36fb99c8a9ddf3f3354cf6e6655?postid=956b57d4f6e0

   andrej karpathy   s final output

sources

   the code and the idea are all tightly based on andrej karpathy   s
   [13]blog post. the code in me_pong.py is intended to be a simpler to
   follow version of pong.py which was written by dr. karpathy.

prerequisites and background reading

   to follow along, you   ll need to know the following:
     * basic python
     * neural network design and backpropogation
     * calculus and id202

   if you want a deeper dive into the material at hand, read the [14]blog
   post on which all of this is based. this post is meant to be a simpler
   introduction to that material.

   great! let   s get started.
     __________________________________________________________________

setup

    1. open up [15]the code to follow along.
    2. follow the instructions for installing [16]openai gym. you may need
       to install cmake first.
    3. run pip install -e .[atari]
    4. go back to the root of this repository and open a new file called
       my_pong.py in your favorite editor.
    5. let   s get to problem solving. here   s the problem:

problem

   we are given the following:
     * a sequence of images (frames) representing each frame of the pong
       game.
     * an indication when we   ve won or lost the game.
     * an opponent agent that is the traditional pong computer player.
     * an agent we control that we can tell to move up or down at each
       frame.

   can we use these pieces to train our agent to beat the computer?
   moreover, can we make our solution generic enough so it can be reused
   to win in games that aren   t pong?

solution

   indeed, we can! andrej does this by building a neural network that
   takes in each image and outputs a command to our ai to move up or down.
   [1*05exqkj0noowv80snveyjg.png]
   the architecture of andrej   s solution from his blog post.

   we can break this down a bit more into the following steps:

   our neural network, based heavily on andrej   s solution, will do the
   following:
    1. take in images from the game and preprocess them (remove color,
       background, downsample etc.).
    2. use the neural network to compute a id203 of moving up.
    3. sample from that id203 distribution and tell the agent to
       move up or down.
    4. if the round is over (you missed the ball or the opponent missed
       the ball), find whether you won or lost.
    5. when the episode has finished(someone got to 21 points), pass the
       result through the id26 algorithm to compute the
       gradient for our weights.
    6. after 10 episodes have finished, sum up the gradient and move the
       weights in the direction of the gradient.
    7. repeat this process until our weights are tuned to the point where
       we can beat the computer. that   s basically it! let   s start looking
       at how our code achieves this.

   ok now that we   ve described the problem and its solution, let   s get to
   writing some code!
     __________________________________________________________________

code

   we   re now going to follow the code in [17]me_pong.py. please keep it
   open and read along! the code starts here:
def main():

initialization

   first, let   s use openai gym to make a game environment and get our very
   first image of the game.

   iframe: [18]/media/07de8c83feebd944376eba2d7b766012?postid=956b57d4f6e0

   next, we set a bunch of parameters based off of [19]andrej   s blog post.
   we aren   t going to worry about tuning them but note that you can
   probably get better performance by doing so. the parameters we will use
   are:
     * batch_size: how many rounds we play before updating the weights of
       our network.
     * gamma: the discount factor we use to discount the effect of old
       actions on the final result. (don   t worry about this yet)
     * decay_rate: parameter used in rmsprop algorithm. (don   t worry about
       this yet)
     * num_hidden_layer_neurons: how many neurons are in our hidden layer.
     * learning_rate: the rate at which we learn from our results to
       compute the new weights. a higher rate means we react more to
       results and a lower rate means we don   t react as strongly to each
       result.

   iframe: [20]/media/e49eed7e6ce0eda929c2633a4a76fa4a?postid=956b57d4f6e0

   then, we set counters, initial values, and the initial weights in our
   neural network.

   weights are stored in matrices. layer 1 of our neural network is a 200
   x 6400 matrix representing the weights for our hidden layer. for layer
   1, element w1_ij represents the weight of neuron i for input pixel j in
   layer 1.

   layer 2 is a 200 x 1 matrix representing the weights of the output of
   the hidden layer on our final output. for layer 2, element w2_i
   represents the weights we place on the activation of neuron i in the
   hidden layer.

   we initialize each layer   s weights with random numbers for now. we
   divide by the square root of the number of the dimension size to
   normalize our weights.

   iframe: [21]/media/1ea394f239754130dee30c4f91d843c7?postid=956b57d4f6e0

   next, we set up the initial parameters for rmsprop (a method for
   updating weights that we will discuss later). don   t worry too much
   about understanding what you see below. i   m mainly bringing it up here
   so we can continue to follow along the main code block.

   iframe: [22]/media/f540ae6abefbfc021b624081a17bc684?postid=956b57d4f6e0

   we   ll need to collect a bunch of observations and intermediate values
   across the episode and use those to compute the gradient at the end
   based on the result. the below sets up the arrays where we   ll collect
   all that information.

   iframe: [23]/media/289ad46c69b15c6011e341a25932469d?postid=956b57d4f6e0

   ok we   re all done with the setup! if you were following, it should look
   something like this:

   iframe: [24]/media/6832e7f9c2f731a3c424880374ca1e11?postid=956b57d4f6e0

   phew. now for the fun part!

figuring out how to move

   the crux of our algorithm is going to live in a loop where we
   continually make a move and then learn based on the results of the
   move. we   ll put everything in a while block for now but in reality you
   might set up a break condition to stop the process.

   the first step to our algorithm is processing the image of the game
   that openai gym passed us. we really don   t care about the entire image
   - just certain details. we do this below:

   iframe: [25]/media/a40c40d1e7a298b84655d64f25cd6ab4?postid=956b57d4f6e0

   let   s dive into preprocess_observations to see how we convert the image
   openai gym gives us into something we can use to train our neural
   network. the basic steps are:
    1. crop the image (we just care about the parts with information we
       care about).
    2. downsample the image.
    3. convert the image to black and white (color is not particularly
       important to us).
    4. remove the background.
    5. convert from an 80 x 80 matrix of values to 6400 x 1 matrix
       (flatten the matrix so it   s easier to use).
    6. store just the difference between the current frame and the
       previous frame if we know the previous frame (we only care about
       what   s changed).

   iframe: [26]/media/764a70f64e4799d78f78a0b0deb95015?postid=956b57d4f6e0

   now that we   ve preprocessed the observations, let   s move on to actually
   sending the observations through our neural net to generate the
   id203 of telling our ai to move up. here are the steps we   ll
   take:

   iframe: [27]/media/0aba1c66b3aa782e5e556f6e5dfd6a90?postid=956b57d4f6e0

   how exactly does apply_neural_nets take observations and weights and
   generate a id203 of going up? this is just the forward pass of
   the neural network. let   s look at the code below for more information:

   iframe: [28]/media/a57922e2aced15a23529c89fa316e3b4?postid=956b57d4f6e0

   as you can see, it   s not many steps at all! let   s go step by step:
    1. compute the unprocessed hidden layer values by simply finding the
       dot product of the weights[1] (weights of layer 1) and the
       observation_matrix. if you remember, weights[1] is a 200 x 6400
       matrix and observations_matrix is a 6400 x 1 matrix. so the dot
       product will give us a matrix of dimensions 200 x 1. we have 200
       neurons so each row represents the output of one neuron.
    2. next, we apply a non linear thresholding function on those hidden
       layer values - in this case just a simple relu. at a high level,
       this introduces the nonlinearities that makes our network capable
       of computing nonlinear functions rather than just simple linear
       ones.
    3. we use those hidden layer activation values to calculate the output
       layer values. this is done by a simple dot product of
       hidden_layer_values (200 x 1) and weights[   2   ] (1 x 200) which
       yields a single value (1 x 1).
    4. finally, we apply a sigmoid function on this output value so that
       it   s between 0 and 1 and is therefore a valid id203
       (id203 of going up).

   let   s return to the main algorithm and continue on. now that we have
   obtained a id203 of going up, we need to now record the results
   for later learning and choose an action to tell our ai to implement:

   iframe: [29]/media/6978b703a9c67848c3296a20a2fa5468?postid=956b57d4f6e0

   we choose an action by flipping an imaginary coin that lands    up    with
   id203 up_id203 and down with 1 - up_id203. if it
   lands up, we choose tell our ai to go up and if not, we tell it to go
   down. we also

   having done that, we pass the action to openai gym via
   env.step(action).

   ok we   ve covered the first half of the solution! we know what action to
   tell our ai to take. if you   ve been following along, your code should
   look like this:

   iframe: [30]/media/7fbcebda4f721a9894d359058cb81dcf?postid=956b57d4f6e0

   now that we   ve made our move, it   s time to start learning so we figure
   out the right weights in our neural network!

learning

   learning is all about seeing the result of the action (i.e. whether or
   not we won the round) and changing our weights accordingly. the first
   step to learning is asking the following question:
     * how does changing the output id203 (of going up) affect my
       result of winning the round?

   mathematically, this is just the derivative of our result with respect
   to the outputs of our final layer. if l is the value of our result to
   us and f is the function that gives us the activations of our final
   layer, this derivative is just    l/   f.

   in a binary classification context (i.e. we just have to tell the ai
   one of two actions, up or down), this derivative turns out to be
   [1*3suftaru_kjkurxsgvnlqw.png]

   note that    in the above equation represents the sigmoid function. read
   the attribute classification section [31]here for more information
   about how we get the above derivative. we simplify this further below:
   l/   f = true_label(0 or 1)     predicted_label(0 or 1)

   after one action(moving the paddle up or down), we don   t really have an
   idea of whether or not this was the right action. so we   re going to
   cheat and treat the action we end up sampling from our id203 as
   the correct action.

   our predicion for this round is going to be the id203 of going up
   we calculated. using that, we have that    l/   f can be computed by

   iframe: [32]/media/705c3d46a767cb114e2d3bebe82cfc63?postid=956b57d4f6e0

   awesome! we have the gradient per action.

   the next step is to figure out how we learn after the end of an episode
   (i.e. when we or our opponent miss the ball and someone gets a point).
   we do this by computing the policy gradient of the network at the end
   of each episode. the intuition here is that if we won the round, we   d
   like our network to generate more of the actions that led to us
   winning. alternatively, if we lose, we   re going to try and generate
   less of these actions.

   openai gym provides us the handy done variable to tell us when an
   episode finishes (i.e. we missed the ball or our opponent missed the
   ball). when we notice we are done, the first thing we do is compile all
   our observations and gradient calculations for the episode. this allows
   us to apply our learnings over all the actions in the episode.

   iframe: [33]/media/4398098c21dba187f0126cb3185d7876?postid=956b57d4f6e0

   next, we want to learn in such a way that actions taken towards the end
   of an episode more heavily influence our learning than actions taken at
   the beginning. this is called discounting.

   think about it this way - if you moved up at the first frame of the
   episode, it probably had very little impact on whether or not you win.
   however, closer to the end of the episode, your actions probably have a
   much larger effect as they determine whether or not your paddle reaches
   the ball and how your paddle hits the ball.

   we   re going to take this weighting into account by discounting our
   rewards such that rewards from earlier frames are discounted a lot more
   than rewards for later frames. after this, we   re going to finally use
   id26 to compute the gradient (i.e. the direction we need to
   move our weights to improve).

   iframe: [34]/media/bc9583215e69f68402f34ee1cca93c10?postid=956b57d4f6e0

   let   s dig in a bit into how the policy gradient for the episode is
   computed. this is one of the most important parts of reinforcement
   learning as it   s how our agent figures out how to improve over time.

   to begin with, if you haven   t already, read this [35]excerpt on
   id26 from michael nielsen   s excellent free book on deep
   learning.

   as you   ll see in that excerpt, there are four fundamental equations of
   backpropogation, a technique for computing the gradient for our
   weights.
   [1*wdbit-gyo3zon23tqpnsoa.png]
   four fundamental equations of id26. source: michael nielsen

   our goal is to find    c/   w1 (bp4), the derivative of the cost function
   with respect to the first layer   s weights, and    c/   w2, the derivative
   of the cost function with respect to the second layer   s weights. these
   gradients will help us understand what direction to move our weights in
   for the greatest improvement.

   to begin with, let   s start with    c/   w2. if a^l2 is the activations of
   the hidden layer (layer 2), we see that the formula is:
   [1*mrvwi6usnihqjgykqmpplq.png]

   indeed, this is exactly what we do here:

   iframe: [36]/media/032f47baf99e301b841a3316d4b6d6d9?postid=956b57d4f6e0

   next, we need to calculate    c/   w1. the formula for that is:
   [1*qvzrz13bqnoa0nj2w9fqiq.png]

   and we also know that a^l1 is just our observation_values.

   so all we need now is   ^l2. once we have that, we can calculate    c/   w1
   and return. we do just that below:

   iframe: [37]/media/07fa299f447ff11f14024cc8d92f3717?postid=956b57d4f6e0

   if you   ve been following along, your function should look like this:

   iframe: [38]/media/187d43bdf4685f19f05a0d4812ca9922?postid=956b57d4f6e0

   computing the gradient

   with that, we   ve finished id26 and computed our gradients!

   after we have finished batch_size episodes, we finally update our
   weights for our neural network and implement our learnings.

   iframe: [39]/media/ab342b41c0c4ce51347714f66982a81c?postid=956b57d4f6e0

   update the weights every batch episodes

   to update the weights, we simply apply rmsprop, an algorithm for
   updating weights described by sebastian reuder [40]here.
   [1*dpfmjd7nwmahq2qabhdv-w.png]

   we implement this below:

   iframe: [41]/media/485302514e980acc9bb9eac0471152fd?postid=956b57d4f6e0

   updating weights using rmsprop

   this is the step that tweaks our weights and allows us to get better
   over time.

   this is basically it! putting it altogether it should look like
   [42]this.

   you just coded a full neural network for playing pong! uncomment
   env.render() and run it for 3   4 days to see it finally beat the
   computer! you   ll need to do some pickling as done in andrej karpathy   s
   solution to be able to visualize your results when you win.

performance

   according to the blog post, this algorithm should take around 3 days of
   training on a macbook to start beating the computer.

   consider tweaking the parameters or using convolutional neural nets to
   boost the performance further.

further reading

   if you want a further primer into neural networks and reinforcement
   learning, there are some great resources to learn more (i work at
   udacity as the director of machine learning programs):
     * [43]andrej karpathy   s original blog post
     * [44]udacity   s free deep learning course by google
     * [45]udacity   s free supervised learning course by georgia tech
     * [46]michael neilsen   s deep learning book
     * [47]sutton and barto   s id23 book

   thanks to [48]eric gonzalez, [49]brynn claypoole, [50]sam collins,
   [51]tim vergenz, and [52]cameron pittman.
     * [53]machine learning
     * [54]neural networks
     * [55]id23

   (button)
   (button)
   (button) 961 claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [56]go to the profile of dhruv parthasarathy

[57]dhruv parthasarathy

   [58]@dhruvp. vp eng [59]@athelas. mit math and cs undergrad    13. mit cs
   masters    14. previously: director of ai programs @ udacity.

     * (button)
       (button) 961
     * (button)
     *
     *

   [60]go to the profile of dhruv parthasarathy
   never miss a story from dhruv parthasarathy, when you sign up for
   medium. [61]learn more
   never miss a story from dhruv parthasarathy
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/956b57d4f6e0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@dhruvp?source=post_header_lockup
  10. https://medium.com/@dhruvp
  11. https://gym.openai.com/
  12. https://medium.com/media/8073d36fb99c8a9ddf3f3354cf6e6655?postid=956b57d4f6e0
  13. http://karpathy.github.io/2016/05/31/rl/
  14. http://karpathy.github.io/2016/05/31/rl/
  15. https://github.com/dhruvp/atari-pong/blob/master/me_pong.py
  16. https://gym.openai.com/docs
  17. https://github.com/dhruvp/atari-pong/blob/master/me_pong.py
  18. https://medium.com/media/07de8c83feebd944376eba2d7b766012?postid=956b57d4f6e0
  19. http://karpathy.github.io/2016/05/31/rl/
  20. https://medium.com/media/e49eed7e6ce0eda929c2633a4a76fa4a?postid=956b57d4f6e0
  21. https://medium.com/media/1ea394f239754130dee30c4f91d843c7?postid=956b57d4f6e0
  22. https://medium.com/media/f540ae6abefbfc021b624081a17bc684?postid=956b57d4f6e0
  23. https://medium.com/media/289ad46c69b15c6011e341a25932469d?postid=956b57d4f6e0
  24. https://medium.com/media/6832e7f9c2f731a3c424880374ca1e11?postid=956b57d4f6e0
  25. https://medium.com/media/a40c40d1e7a298b84655d64f25cd6ab4?postid=956b57d4f6e0
  26. https://medium.com/media/764a70f64e4799d78f78a0b0deb95015?postid=956b57d4f6e0
  27. https://medium.com/media/0aba1c66b3aa782e5e556f6e5dfd6a90?postid=956b57d4f6e0
  28. https://medium.com/media/a57922e2aced15a23529c89fa316e3b4?postid=956b57d4f6e0
  29. https://medium.com/media/6978b703a9c67848c3296a20a2fa5468?postid=956b57d4f6e0
  30. https://medium.com/media/7fbcebda4f721a9894d359058cb81dcf?postid=956b57d4f6e0
  31. http://cs231n.github.io/neural-networks-2/#losses
  32. https://medium.com/media/705c3d46a767cb114e2d3bebe82cfc63?postid=956b57d4f6e0
  33. https://medium.com/media/4398098c21dba187f0126cb3185d7876?postid=956b57d4f6e0
  34. https://medium.com/media/bc9583215e69f68402f34ee1cca93c10?postid=956b57d4f6e0
  35. http://neuralnetworksanddeeplearning.com/chap2.html
  36. https://medium.com/media/032f47baf99e301b841a3316d4b6d6d9?postid=956b57d4f6e0
  37. https://medium.com/media/07fa299f447ff11f14024cc8d92f3717?postid=956b57d4f6e0
  38. https://medium.com/media/187d43bdf4685f19f05a0d4812ca9922?postid=956b57d4f6e0
  39. https://medium.com/media/ab342b41c0c4ce51347714f66982a81c?postid=956b57d4f6e0
  40. http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop
  41. https://medium.com/media/485302514e980acc9bb9eac0471152fd?postid=956b57d4f6e0
  42. https://github.com/dhruvp/atari-pong/blob/master/me_pong.py
  43. http://karpathy.github.io/2016/05/31/rl/
  44. https://www.udacity.com/course/deep-learning--ud730
  45. https://www.udacity.com/course/machine-learning-supervised-learning--ud675
  46. http://neuralnetworksanddeeplearning.com/
  47. https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html
  48. https://medium.com/@ericrgon?source=post_page
  49. https://medium.com/@bclaypoole?source=post_page
  50. https://medium.com/@smcllns?source=post_page
  51. https://medium.com/@timvergenz?source=post_page
  52. https://medium.com/@cameronpittman?source=post_page
  53. https://medium.com/tag/machine-learning?source=post
  54. https://medium.com/tag/neural-networks?source=post
  55. https://medium.com/tag/reinforcement-learning?source=post
  56. https://medium.com/@dhruvp?source=footer_card
  57. https://medium.com/@dhruvp
  58. http://twitter.com/dhruvp
  59. http://twitter.com/athelas
  60. https://medium.com/@dhruvp
  61. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  63. https://medium.com/p/956b57d4f6e0/share/twitter
  64. https://medium.com/p/956b57d4f6e0/share/facebook
  65. https://medium.com/p/956b57d4f6e0/share/twitter
  66. https://medium.com/p/956b57d4f6e0/share/facebook
