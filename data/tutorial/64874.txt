   [1]

linkedin

     * [2]sign in
     * [3]join now

   building convolutional neural network using numpy from scratch

   id98 conv, relu, & pooling layers results

         building convolutional neural network using numpy from scratch

   published on april 20, 2018april 20, 2018     63 likes     14 comments

   [4]ahmed gad

[5]ahmed gad[6]follow

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

     * (button) like63
     * (button) comment14
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       7

   using already existing models in ml/dl libraries might be helpful in
   some cases. but to have better control and understanding, you should
   try to implement them yourself. this article shows how a id98 is
   implemented just using numpy.

   this article is ranked top among the most shared articles in
   kdnuggets.com in the period from 23-april to 29-april 2018 and got
   their silver padge.

   iframe: [7]about:blank

   here is the top shared stories in such period.

   iframe: [8]about:blank

   convolutional neural network (id98) is the state-of-art technique for
   analyzing multidimensional signals such as images. there are different
   libraries that already implements id98 such as tensorflow and keras.
   such libraries isolates the developer from some details and just give
   an abstract api to make life easier and avoid complexity in the
   implementation. but in practice, such details might make a difference.
   sometimes, the data scientist have to go through such details to
   enhance the performance. the solution in such situation is to build
   every piece of such model your own. this gives the highest possible
   level of control over the network. also, it is recommended to implement
   such models to have better understanding over them.

   in this article, id98 is created using only numpy library. just three
   layers are created which are convolution (conv for short), relu, and
   max pooling. the major steps involved are as follows:

   1. reading the input image.

   2. preparing filters.

   3. conv layer: convolving each filter with the input image.

   4. relu layer: applying relu activation function on the feature maps
   (output of conv layer).

   5. max pooling layer: applying the pooling operation on the output of
   relu layer.

   6. stacking conv, relu, and max pooling layers.

1. reading input image

   the following code reads an already existing image from the skimage
   python library and converts it into gray.
1.  import skimage.data

2.  # reading the image

3.  img = skimage.data.chelsea()

4.  # converting the image into gray.

5.  img = skimage.color.rgb2gray(img)

   reading image is the first step because next steps depend on the input
   size. the image after being converted into gray is shown below.

   [:0]

2. preparing filters

   the following code prepares the filters bank for the first conv layer
   (l1 for short):
1.  l1_filter = numpy.zeros((2,3,3))

   a zero array is created according to the number of filters and the size
   of each filter. 2 filters of size 3x3 are created that is why the zero
   array is of size (2=num_filters, 3=num_rows_filter,
   3=num_columns_filter). size of the filter is selected to be 2d array
   without depth because the input image is gray and has no depth (i.e. 2d
   ). if the image is rgb with 3 channels, the filter size must be (3, 3,
   3=depth).

   the size of the filters bank is specified by the above zero array but
   not the actual values of the filters. it is possible to override such
   values as follows to detect vertical and horizontal edges.
1.  l1_filter[0, :, :] = numpy.array([[[-1, 0, 1],

2.                                     [-1, 0, 1],

3.                                     [-1, 0, 1]]])

4.  l1_filter[1, :, :] = numpy.array([[[1,   1,  1],

5.                                     [0,   0,  0],

6.                                     [-1, -1, -1]]])

3. conv layer

   after preparing the filters, next is to convolve the input image by
   them. the next line convolves the image with the filters bank using a
   function called conv:
1.  l1_feature_map = conv(img, l1_filter)

   such function accepts just two arguments which are the image and the
   filter bank which is implemented as below.
1.  def conv(img, conv_filter):

2.      if len(img.shape) > 2 or len(conv_filter.shape) > 3: # check if number o
f image channels matches the filter depth.

3.          if img.shape[-1] != conv_filter.shape[-1]:

4.              print("error: number of channels in both image and filter must m
atch.")

5.              sys.exit()

6.      if conv_filter.shape[1] != conv_filter.shape[2]: # check if filter dimen
sions are equal.

7.          print('error: filter must be a square matrix. i.e. number of rows an
d columns must match.')

8.          sys.exit()

9.      if conv_filter.shape[1]%2==0: # check if filter diemnsions are odd.

10.         print('error: filter must have an odd size. i.e. number of rows and
columns must be odd.')

11.         sys.exit()

12.

13.     # an empty feature map to hold the output of convolving the filter(s) wi
th the image.

14.     feature_maps = numpy.zeros((img.shape[0]-conv_filter.shape[1]+1,

15.                                 img.shape[1]-conv_filter.shape[1]+1,

16.                                 conv_filter.shape[0]))

17.

18.     # convolving the image by the filter(s).

19.     for filter_num in range(conv_filter.shape[0]):

20.         print("filter ", filter_num + 1)

21.         curr_filter = conv_filter[filter_num, :] # getting a filter from the
 bank.

22.         """

23.         checking if there are mutliple channels for the single filter.

24.         if so, then each channel will convolve the image.

25.         the result of all convolutions are summed to return a single feature
 map.

26.         """

27.         if len(curr_filter.shape) > 2:

28.             conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # array hol
ding the sum of all feature maps.

29.             for ch_num in range(1, curr_filter.shape[-1]): # convolving each
 channel with the image and summing the results.

30.                 conv_map = conv_map + conv_(img[:, :, ch_num],

31.                                   curr_filter[:, :, ch_num])

32.         else: # there is just a single channel in the filter.

33.             conv_map = conv_(img, curr_filter)

34.         feature_maps[:, :, filter_num] = conv_map # holding feature map with
 the current filter.

35.
return feature_maps # returning all feature maps.

   the function starts by ensuring that the depth of each filter is equal
   to the number of image channels. in the code below, the outer if checks
   if the channel and the filter have a depth. if a depth already exists,
   then the inner if checks their inequality. if there is no match, then
   the script will exit.
1.  if len(img.shape) > 2 or len(conv_filter.shape) > 3: # check if number of im
age channels matches the filter depth.

2.          if img.shape[-1] != conv_filter.shape[-1]:

3.              print("error: number of channels in both image and filter must m
atch.")
            sys.exit()

   moreover, the size of the filter should be odd and filter dimensions
   are equal (i.e. number of rows and columns are odd and equal). this is
   checked according to the following two if blocks. if such conditions
   don   t met, the script will exit.
1.  if conv_filter.shape[1] != conv_filter.shape[2]: # check if filter dimension
s are equal.

2.      print('error: filter must be a square matrix. i.e. number of rows and co
lumns must match.')

3.      sys.exit()

4.  if conv_filter.shape[1]%2==0: # check if filter diemnsions are odd.

5.      print('error: filter must have an odd size. i.e. number of rows and colu
mns must be odd.')

6.      sys.exit()

   not satisfying any of the conditions above is a proof that the filter
   depth is suitable with the image and convolution is ready to be
   applied. convolving the image by the filter starts by initializing an
   array to hold the outputs of convolution (i.e. feature maps) by
   specifying its size according to the following code:
1.  # an empty feature map to hold the output of convolving the filter(s) with t
he image.

2.  feature_maps = numpy.zeros((img.shape[0]-conv_filter.shape[1]+1,

3.                              img.shape[1]-conv_filter.shape[1]+1,

4.                              conv_filter.shape[0]))

   because there is no stride nor padding, the feature map size will be
   equal to (img_rows-filter_rows+1, image_columns-filter_columns+1,
   num_filters) as above in the code. note that there is an output feature
   map for every filter in the bank. that is why the number of filters in
   the filter bank (conv_filter.shape[0]) is used to specify the size as a
   third argument.
1.      # convolving the image by the filter(s).

2.      for filter_num in range(conv_filter.shape[0]):

3.          print("filter ", filter_num + 1)

4.          curr_filter = conv_filter[filter_num, :] # getting a filter from the
 bank.

5.          """

6.          checking if there are mutliple channels for the single filter.

7.          if so, then each channel will convolve the image.

8.          the result of all convolutions are summed to return a single feature
 map.

9.          """

10.         if len(curr_filter.shape) > 2:

11.             conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # array hol
ding the sum of all feature maps.

12.             for ch_num in range(1, curr_filter.shape[-1]): # convolving each
 channel with the image and summing the results.

13.                 conv_map = conv_map + conv_(img[:, :, ch_num],

14.                                   curr_filter[:, :, ch_num])

15.         else: # there is just a single channel in the filter.

16.             conv_map = conv_(img, curr_filter)

17.         feature_maps[:, :, filter_num] = conv_map # holding feature map with
 the current filter.

    return feature_maps # returning all feature maps.

   the outer loop iterates over each filter in the filter bank and returns
   it for further steps according to this line:
1.  curr_filter = conv_filter[filter_num, :] # getting a filter from the bank.

   if the image to be convolved has more than one channel, then the filter
   must has a depth equal to such number of channels. convolution in this
   case is done by convolving each image channel with its corresponding
   channel in the filter. finally, the sum of the results will be the
   output feature map. if the image has just a single channel, then
   convolution will be straight forward. determining such behavior is done
   in such if-else block:
1.  if len(curr_filter.shape) > 2:

2.       conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # array holding th
e sum of all feature map

3.       for ch_num in range(1, curr_filter.shape[-1]): # convolving each channe
l with the image and summing the results.

4.          conv_map = conv_map + conv_(img[:, :, ch_num],

5.                                    curr_filter[:, :, ch_num])

6.  else: # there is just a single channel in the filter.

7.      conv_map = conv_(img, curr_filter)

   you might notice that the convolution is applied by a function called
   conv_ which is different from the conv function. the function conv just
   accepts the input image and the filter bank but doesn   t apply
   convolution its own. it just passes each set of input-filter pairs to
   be convolved to the conv_ function. this is just for making the code
   simpler to investigate. here is the implementation of the conv_
   function:
1.  def conv_(img, conv_filter):

2.      filter_size = conv_filter.shape[1]

3.      result = numpy.zeros((img.shape))

4.      #looping through the image to apply the convolution operation.

5.      for r in numpy.uint16(numpy.arange(filter_size/2.0,

6.                            img.shape[0]-filter_size/2.0+1)):

7.          for c in numpy.uint16(numpy.arange(filter_size/2.0,

8.                                             img.shape[1]-filter_size/2.0+1)):


9.              """

10.             getting the current region to get multiplied with the filter.

11.             how to loop through the image and get the region based on

12.             the image and filer sizes is the most tricky part of convolution
.

13.             """

14.             curr_region = img[r-numpy.uint16(numpy.floor(filter_size/2.0)):r
+numpy.uint16(numpy.ceil(filter_size/2.0)),

15.                               c-numpy.uint16(numpy.floor(filter_size/2.0)):c
+numpy.uint16(numpy.ceil(filter_size/2.0))]

16.             #element-wise multipliplication between the current region and t
he filter.

17.             curr_result = curr_region * conv_filter

18.             conv_sum = numpy.sum(curr_result) #summing the result of multipl
ication.

19.             result[r, c] = conv_sum #saving the summation in the convolution
 layer feature map.

20.

21.     #clipping the outliers of the result matrix.

22.     final_result = result[numpy.uint16(filter_size/2.0):result.shape[0]-nump
y.uint16(filter_size/2.0),

23.                           numpy.uint16(filter_size/2.0):result.shape[1]-nump
y.uint16(filter_size/2.0)]

24.     return final_result

   it iterates over the image and extracts regions of equal size to the
   filter according to this line:
1.  curr_region = img[r:r+filter_size, c:c+filter_size]

   then it apply element-wise multiplication between the region and the
   filter and summing them to get a single value as the output according
   to these lines:
1.  #element-wise multipliplication between the current region and the filter.

2.  curr_result = curr_region * conv_filter

3.  conv_sum = numpy.sum(curr_result) #summing the result of multiplication.

4.  result[r, c] = conv_sum #saving the summation in the convolution layer featu
re map.

   after convolving each filter by the input, the feature maps are
   returned by the conv function. the following figure shows the feature
   maps returned by such conv layer.

   [:0]

   the output of such layer will be applied to the relu layer.

4. relu layer

   the relu layer applies the relu activation function over each feature
   map returned by the conv layer. it is called using the relu function
   according to the following line of code:
l1_feature_map_relu = relu(l1_feature_map)

   the relu function is implemented as follows:
1.  def relu(feature_map):

2.      #preparing the output of the relu activation function.

3.      relu_out = numpy.zeros(feature_map.shape)

4.      for map_num in range(feature_map.shape[-1]):

5.          for r in numpy.arange(0,feature_map.shape[0]):

6.              for c in numpy.arange(0, feature_map.shape[1]):

7.                  relu_out[r, c, map_num] = numpy.max([feature_map[r, c, map_n
um], 0])

8.      return relu_out

   it is very simple. just loop though each element in the feature map and
   return the original value in the feature map if it is larger than 0.
   otherwise, return 0. the outputs of the relu layer are shown in the
   next figure.

   [:0]

   the output of the relu layer is applied to the max pooling layer.

5. max pooling layer

   the max pooling layer accepts the output of the relu layer and applies
   the max pooling operation according to the following line:
1.  l1_feature_map_relu_pool = pooling(l1_feature_map_relu, 2, 2)

   it is implemented using the pooling function as follows:
1. def pooling(feature_map, size=2, stride=2):

2.      #preparing the output of the pooling operation.

3.      pool_out = numpy.zeros((numpy.uint16((feature_map.shape[0]-size+1)/strid
e),

4.                              numpy.uint16((feature_map.shape[1]-size+1)/strid
e),

5.                              feature_map.shape[-1]))

6.      for map_num in range(feature_map.shape[-1]):

7.          r2 = 0

8.          for r in numpy.arange(0,feature_map.shape[0]-size-1, stride):

9.              c2 = 0

10.             for c in numpy.arange(0, feature_map.shape[1]-size-1, stride):

11.                 pool_out[r2, c2, map_num] = numpy.max([feature_map[r:r+size,
  c:c+size, map_num]])

12.                 c2 = c2 + 1

13.             r2 = r2 +1

14.     return pool_out

   the function accepts three inputs which are the output of the relu
   layer, pooling mask size, and stride. it simply creates an empty array,
   as previous, that holds the output of such layer. the size of such
   array is specified according to the size and stride arguments as in
   such line:
1.  pool_out = numpy.zeros((numpy.uint16((feature_map.shape[0]-size+1)/stride),


2.                          numpy.uint16((feature_map.shape[1]-size+1)/stride),


3.                          feature_map.shape[-1]))

   then it loops through the input, channel by channel according to the
   outer loop that uses the looping variable map_num. for each channel in
   the input, max pooling operation is applied. according to the stride
   and size used, the region is clipped and the max of it is returned in
   the output array according to this line:
pool_out[r2, c2, map_num] = numpy.max([feature_map[r:r+size,  c:c+size, map_num]
])

   the outputs of such pooling layer are shown in the next figure. note
   that the size of the pooling layer output is smaller than its input
   even if they seem identical in their graphs.

   [:0]

6. stacking layers

   up to this point, the id98 architecture with conv, relu, and max pooling
   layers is complete. there might be some other layers to be stacked in
   addition to the previous ones as below.
1.  # second conv layer

2.  l2_filter = numpy.random.rand(3, 5, 5, l1_feature_map_relu_pool.shape[-1])

3.  print("\n**working with conv layer 2**")

4.  l2_feature_map = conv(l1_feature_map_relu_pool, l2_filter)

5.  print("\n**relu**")

6.  l2_feature_map_relu = relu(l2_feature_map)

7.  print("\n**pooling**")

8.  l2_feature_map_relu_pool = pooling(l2_feature_map_relu, 2, 2)

9.  print("**end of conv layer 2**\n")

   the previous conv layer uses 3 filters with their values generated
   randomly. that is why there will be 3 feature maps resulted from such
   conv layer. this is also the same for the successive relu and pooling
   layers. outputs of such layers are shown below.

   [:0]

1.  # third conv layer

2.  l3_filter = numpy.random.rand(1, 7, 7, l2_feature_map_relu_pool.shape[-1])

3.  print("\n**working with conv layer 3**")

4.  l3_feature_map = conv(l2_feature_map_relu_pool, l3_filter)

5.  print("\n**relu**")

6.  l3_feature_map_relu = relu(l3_feature_map)

7.  print("\n**pooling**")

8.  l3_feature_map_relu_pool = pooling(l3_feature_map_relu, 2, 2)

9.  print("**end of conv layer 3**\n")

   the following figure shows the outputs of the previous layers. the
   previous conv layer accepts just a single filter. that is why there is
   only one feature map as output.

   [:0]

   but remember, the output of each previous layer is the input to the
   next layer. for example, such lines accepts the previous outputs as
   their inputs.
1.  l2_feature_map = conv(l1_feature_map_relu_pool, l2_filter)

2.  l3_feature_map = conv(l2_feature_map_relu_pool, l3_filter)

7. complete code

   the complete code is available in [9]github
   ([10]https://github.com/ahmedfgad/numpyid98). the code contains the
   visualization of the outputs from each layer using the matplotlib
   library.
import skimage.data
import numpy
import matplotlib
import sys

def conv_(img, conv_filter):
    filter_size = conv_filter.shape[1]
    result = numpy.zeros((img.shape))
    #looping through the image to apply the convolution operation.
    for r in numpy.uint16(numpy.arange(filter_size/2.0,
                          img.shape[0]-filter_size/2.0+1)):
        for c in numpy.uint16(numpy.arange(filter_size/2.0,
                                           img.shape[1]-filter_size/2.0+1)):
            """
            getting the current region to get multiplied with the filter.
            how to loop through the image and get the region based on
            the image and filer sizes is the most tricky part of convolution.
            """
            curr_region = img[r-numpy.uint16(numpy.floor(filter_size/2.0)):r+num
py.uint16(numpy.ceil(filter_size/2.0)),
                              c-numpy.uint16(numpy.floor(filter_size/2.0)):c+num
py.uint16(numpy.ceil(filter_size/2.0))]
            #element-wise multipliplication between the current region and the f
ilter.
            curr_result = curr_region * conv_filter
            conv_sum = numpy.sum(curr_result) #summing the result of multiplicat
ion.
            result[r, c] = conv_sum #saving the summation in the convolution lay
er feature map.

    #clipping the outliers of the result matrix.
    final_result = result[numpy.uint16(filter_size/2.0):result.shape[0]-numpy.ui
nt16(filter_size/2.0),
                          numpy.uint16(filter_size/2.0):result.shape[1]-numpy.ui
nt16(filter_size/2.0)]
    return final_result
def conv(img, conv_filter):
    if len(img.shape) > 2 or len(conv_filter.shape) > 3: # check if number of im
age channels matches the filter depth.
        if img.shape[-1] != conv_filter.shape[-1]:
            print("error: number of channels in both image and filter must match
.")
            sys.exit()
    if conv_filter.shape[1] != conv_filter.shape[2]: # check if filter dimension
s are equal.
        print('error: filter must be a square matrix. i.e. number of rows and co
lumns must match.')
        sys.exit()
    if conv_filter.shape[1]%2==0: # check if filter diemnsions are odd.
        print('error: filter must have an odd size. i.e. number of rows and colu
mns must be odd.')
        sys.exit()

    # an empty feature map to hold the output of convolving the filter(s) with t
he image.
    feature_maps = numpy.zeros((img.shape[0]-conv_filter.shape[1]+1,
                                img.shape[1]-conv_filter.shape[1]+1,
                                conv_filter.shape[0]))

    # convolving the image by the filter(s).
    for filter_num in range(conv_filter.shape[0]):
        print("filter ", filter_num + 1)
        curr_filter = conv_filter[filter_num, :] # getting a filter from the ban
k.
        """
        checking if there are mutliple channels for the single filter.
        if so, then each channel will convolve the image.
        the result of all convolutions are summed to return a single feature map
.
        """
        if len(curr_filter.shape) > 2:
            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # array holding
 the sum of all feature maps.
            for ch_num in range(1, curr_filter.shape[-1]): # convolving each cha
nnel with the image and summing the results.
                conv_map = conv_map + conv_(img[:, :, ch_num],
                                  curr_filter[:, :, ch_num])
        else: # there is just a single channel in the filter.
            conv_map = conv_(img, curr_filter)
        feature_maps[:, :, filter_num] = conv_map # holding feature map with the
 current filter.
    return feature_maps # returning all feature maps.

def pooling(feature_map, size=2, stride=2):
    #preparing the output of the pooling operation.
    pool_out = numpy.zeros((numpy.uint16((feature_map.shape[0]-size+1)/stride),

                            numpy.uint16((feature_map.shape[1]-size+1)/stride),

                            feature_map.shape[-1]))
    for map_num in range(feature_map.shape[-1]):
        r2 = 0
        for r in numpy.arange(0,feature_map.shape[0]-size-1, stride):
            c2 = 0
            for c in numpy.arange(0, feature_map.shape[1]-size-1, stride):
                pool_out[r2, c2, map_num] = numpy.max([feature_map[r:r+size,  c:
c+size, map_num]])
                c2 = c2 + 1
            r2 = r2 +1
    return pool_out

def relu(feature_map):
    #preparing the output of the relu activation function.
    relu_out = numpy.zeros(feature_map.shape)
    for map_num in range(feature_map.shape[-1]):
        for r in numpy.arange(0,feature_map.shape[0]):
            for c in numpy.arange(0, feature_map.shape[1]):
                relu_out[r, c, map_num] = numpy.max([feature_map[r, c, map_num],
 0])
    return relu_out

# reading the image
#img = skimage.io.imread("fruits2.png")
img = skimage.data.chelsea()
# converting the image into gray.
img = skimage.color.rgb2gray(img)

# first conv layer
#l1_filter = numpy.random.rand(2,7,7)*20 # preparing the filters randomly.
l1_filter = numpy.zeros((2,3,3))
l1_filter[0, :, :] = numpy.array([[[-1, 0, 1],
                                   [-1, 0, 1],
                                   [-1, 0, 1]]])
l1_filter[1, :, :] = numpy.array([[[1,   1,  1],
                                   [0,   0,  0],
                                   [-1, -1, -1]]])

print("\n**working with conv layer 1**")
l1_feature_map = conv(img, l1_filter)
print("\n**relu**")
l1_feature_map_relu = relu(l1_feature_map)
print("\n**pooling**")
l1_feature_map_relu_pool = pooling(l1_feature_map_relu, 2, 2)
print("**end of conv layer 1**\n")

# second conv layer
l2_filter = numpy.random.rand(3, 5, 5, l1_feature_map_relu_pool.shape[-1])
print("\n**working with conv layer 2**")
l2_feature_map = conv(l1_feature_map_relu_pool, l2_filter)
print("\n**relu**")
l2_feature_map_relu = relu(l2_feature_map)
print("\n**pooling**")
l2_feature_map_relu_pool = pooling(l2_feature_map_relu, 2, 2)
print("**end of conv layer 2**\n")

# third conv layer
l3_filter = numpy.random.rand(1, 7, 7, l2_feature_map_relu_pool.shape[-1])
print("\n**working with conv layer 3**")
l3_feature_map = conv(l2_feature_map_relu_pool, l3_filter)
print("\n**relu**")
l3_feature_map_relu = relu(l3_feature_map)
print("\n**pooling**")
l3_feature_map_relu_pool = pooling(l3_feature_map_relu, 2, 2)
print("**end of conv layer 3**\n")

# graphing results
fig0, ax0 = matplotlib.pyplot.subplots(nrows=1, ncols=1)
ax0.imshow(img).set_cmap("gray")
ax0.set_title("input image")
ax0.get_xaxis().set_ticks([])
ax0.get_yaxis().set_ticks([])
matplotlib.pyplot.savefig("in_img.png", bbox_inches="tight")
matplotlib.pyplot.close(fig0)

# layer 1
fig1, ax1 = matplotlib.pyplot.subplots(nrows=3, ncols=2)
ax1[0, 0].imshow(l1_feature_map[:, :, 0]).set_cmap("gray")
ax1[0, 0].get_xaxis().set_ticks([])
ax1[0, 0].get_yaxis().set_ticks([])
ax1[0, 0].set_title("l1-map1")

ax1[0, 1].imshow(l1_feature_map[:, :, 1]).set_cmap("gray")
ax1[0, 1].get_xaxis().set_ticks([])
ax1[0, 1].get_yaxis().set_ticks([])
ax1[0, 1].set_title("l1-map2")

ax1[1, 0].imshow(l1_feature_map_relu[:, :, 0]).set_cmap("gray")
ax1[1, 0].get_xaxis().set_ticks([])
ax1[1, 0].get_yaxis().set_ticks([])
ax1[1, 0].set_title("l1-map1relu")

ax1[1, 1].imshow(l1_feature_map_relu[:, :, 1]).set_cmap("gray")
ax1[1, 1].get_xaxis().set_ticks([])
ax1[1, 1].get_yaxis().set_ticks([])
ax1[1, 1].set_title("l1-map2relu")

ax1[2, 0].imshow(l1_feature_map_relu_pool[:, :, 0]).set_cmap("gray")
ax1[2, 0].get_xaxis().set_ticks([])
ax1[2, 0].get_yaxis().set_ticks([])
ax1[2, 0].set_title("l1-map1relupool")

ax1[2, 1].imshow(l1_feature_map_relu_pool[:, :, 1]).set_cmap("gray")
ax1[2, 0].get_xaxis().set_ticks([])
ax1[2, 0].get_yaxis().set_ticks([])
ax1[2, 1].set_title("l1-map2relupool")

matplotlib.pyplot.savefig("l1.png", bbox_inches="tight")
matplotlib.pyplot.close(fig1)

# layer 2
fig2, ax2 = matplotlib.pyplot.subplots(nrows=3, ncols=3)
ax2[0, 0].imshow(l2_feature_map[:, :, 0]).set_cmap("gray")
ax2[0, 0].get_xaxis().set_ticks([])
ax2[0, 0].get_yaxis().set_ticks([])
ax2[0, 0].set_title("l2-map1")

ax2[0, 1].imshow(l2_feature_map[:, :, 1]).set_cmap("gray")
ax2[0, 1].get_xaxis().set_ticks([])
ax2[0, 1].get_yaxis().set_ticks([])
ax2[0, 1].set_title("l2-map2")

ax2[0, 2].imshow(l2_feature_map[:, :, 2]).set_cmap("gray")
ax2[0, 2].get_xaxis().set_ticks([])
ax2[0, 2].get_yaxis().set_ticks([])
ax2[0, 2].set_title("l2-map3")

ax2[1, 0].imshow(l2_feature_map_relu[:, :, 0]).set_cmap("gray")
ax2[1, 0].get_xaxis().set_ticks([])
ax2[1, 0].get_yaxis().set_ticks([])
ax2[1, 0].set_title("l2-map1relu")

ax2[1, 1].imshow(l2_feature_map_relu[:, :, 1]).set_cmap("gray")
ax2[1, 1].get_xaxis().set_ticks([])
ax2[1, 1].get_yaxis().set_ticks([])
ax2[1, 1].set_title("l2-map2relu")

ax2[1, 2].imshow(l2_feature_map_relu[:, :, 2]).set_cmap("gray")
ax2[1, 2].get_xaxis().set_ticks([])
ax2[1, 2].get_yaxis().set_ticks([])
ax2[1, 2].set_title("l2-map3relu")

ax2[2, 0].imshow(l2_feature_map_relu_pool[:, :, 0]).set_cmap("gray")
ax2[2, 0].get_xaxis().set_ticks([])
ax2[2, 0].get_yaxis().set_ticks([])
ax2[2, 0].set_title("l2-map1relupool")

ax2[2, 1].imshow(l2_feature_map_relu_pool[:, :, 1]).set_cmap("gray")
ax2[2, 1].get_xaxis().set_ticks([])
ax2[2, 1].get_yaxis().set_ticks([])
ax2[2, 1].set_title("l2-map2relupool")

ax2[2, 2].imshow(l2_feature_map_relu_pool[:, :, 2]).set_cmap("gray")
ax2[2, 2].get_xaxis().set_ticks([])
ax2[2, 2].get_yaxis().set_ticks([])
ax2[2, 2].set_title("l2-map3relupool")

matplotlib.pyplot.savefig("l2.png", bbox_inches="tight")
matplotlib.pyplot.close(fig2)

# layer 3
fig3, ax3 = matplotlib.pyplot.subplots(nrows=1, ncols=3)
ax3[0].imshow(l3_feature_map[:, :, 0]).set_cmap("gray")
ax3[0].get_xaxis().set_ticks([])
ax3[0].get_yaxis().set_ticks([])
ax3[0].set_title("l3-map1")

ax3[1].imshow(l3_feature_map_relu[:, :, 0]).set_cmap("gray")
ax3[1].get_xaxis().set_ticks([])
ax3[1].get_yaxis().set_ticks([])
ax3[1].set_title("l3-map1relu")

ax3[2].imshow(l3_feature_map_relu_pool[:, :, 0]).set_cmap("gray")
ax3[2].get_xaxis().set_ticks([])
ax3[2].get_yaxis().set_ticks([])
ax3[2].set_title("l3-map1relupool")

   [11]ahmed gad

[12]ahmed gad

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

   [13]follow

   14 comments
   article-comment__guest-image
   [14]sign in to leave your comment
   show more comments.
     __________________________________________________________________

more from ahmed gad

   [15]27 articles
   from y=x to building a complete id158

[16]from y=x to building a complete artificial   

   march 29, 2019

   feature reduction using genetic algorithm with python

[17]feature reduction using genetic algorithm   

   january 29, 2019

   id158s optimization using genetic algorithm with
   python

[18]id158s optimization   

   january 24, 2019

     *    2019
     * [19]about
     * [20]user agreement
     * [21]privacy policy
     * [22]cookie policy
     * [23]copyright policy
     * [24]brand policy
     * [25]manage subscription
     * [26]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   5. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/building-convolutional-neural-network-using-numpy-from-ahmed-gad&trk=author-info__follow-button
   7. about:blank
   8. about:blank
   9. https://github.com/ahmedfgad/numpyid98
  10. https://github.com/ahmedfgad/numpyid98
  11. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
  12. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
  13. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/building-convolutional-neural-network-using-numpy-from-ahmed-gad&trk=author-info__follow-button-bottom
  14. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/building-convolutional-neural-network-using-numpy-from-ahmed-gad&trk=article-reader_leave-comment
  15. https://www.linkedin.com/today/author/ahmedfgad
  16. https://www.linkedin.com/pulse/from-yx-building-complete-artificial-neural-network-ahmed-gad?trk=related_artice_from y=x to building a complete id158_article-card_title
  17. https://www.linkedin.com/pulse/feature-reduction-using-genetic-algorithm-ahmed-gad?trk=related_artice_feature reduction using genetic algorithm with python_article-card_title
  18. https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad?trk=related_artice_id158s optimization using genetic algorithm with python_article-card_title
  19. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  20. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  21. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  22. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  23. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  24. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  25. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  26. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
