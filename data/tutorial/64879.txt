   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

interpreting machine learning models

   [16]go to the profile of lars hulstaert
   [17]lars hulstaert (button) blockedunblock (button) followfollowing
   feb 20, 2018

   regardless of the end goal of your data science solutions, an end-user
   will always prefer solutions that are interpretable and understandable.
   moreover, as a data scientist you will always benefit from the
   interpretability of your model to validate and improve your work. in
   this blog post i attempt to explain the importance of interpretability
   in machine learning and discuss some simple actions and frameworks that
   you can experiment with yourself.
   [0*pnypj3flkzc2dr8f.png]
   [18]xkcd on machine learning

why is interpretability in machine learning important?

   in traditional statistics, we construct and verify hypotheses by
   investigating the data at large. we build models to construct rules
   that we can incorporate into our mental models of processes. a
   marketing firm for example can build a model that correlates marketing
   campaign data to finance data in order to determine what constitutes an
   effective marketing campaign.
   this is a top-down approach to data science, and interpretability is
   key as it is a cornerstone of the rules and processes that are defined.
   as correlation often does not equal causality, a solid model
   understanding is needed when it comes to making decisions and
   explaining them.

   in a bottom-up approach to data science, we delegate parts of the
   business process to machine learning models. in addition, completely
   new business ideas are enabled by machine learning . bottom-up data
   science typically corresponds to the automation of manual and laborious
   tasks. a manufacturing firm can for example put sensors on their
   machines and perform predictive maintenance. as a result, maintenance
   engineers can work more efficiently and don   t need to perform expensive
   periodic checks. model interpretability is necessary to verify the that
   what the model is doing is in line with what you expect and it allows
   to create trust with the users and ease the transition from manual to
   automated processes.
   [0*ysxc_ks40muifbbq.png]
   in a top-down process, you iteratively construct and validate a set of
   hypotheses. in a bottom-up approach, you attempt to automate a process
   by solving a problem from the bottom-up.

   as a data scientist you are often concerned with fine-tuning models to
   obtain optimal performance. data science is often framed as:    given
   data x with labels y, find the model with minimal error   . while the
   ability to train performant models is a critical skill for a data
   scientist, it is important to be able to look at the bigger picture.
   interpretability of data and machine learning models is one of those
   aspects that is critical in the practical    usefulness    of a data
   science pipeline and it ensures that the model is aligned with the
   problem you want to solve. although it is easy to lose yourself in
   experimenting with state-of-the-art techniques when building models,
   being able to properly interpret your findings is an essential part of
   the data science process.
   [0*qkaukfdmphhgajtg.jpg]
   interpreting models is necessary to verify the usefulness of the model
   predictions.

why is it essential to do an in-depth analysis of your models?

   there are several reasons to focus on model interpretability as a data
   scientist. although there is overlap between these, they capture the
   different motivations for interpretability:

     identify and mitigate bias.

   bias is potentially present in any dataset and it is up to the data
   scientist to identify and attempt to fix it. datasets can be limited in
   size and they might not be representable for the full population, or
   the data capturing process might have not accounted for potential
   biases. biases often only become apparent after thorough data analysis
   or when the relation between model predictions and the model input is
   analysed. if you want to learn more about the different types of types
   of bias exist, i highly recommend the video below. note that there is
   no single solution to resolving bias, but a critical step towards
   interpretability being aware of potential bias.

   iframe: [19]/media/b7e2a24f5613a3d48b9b6828eb504122?postid=70c30694a05f

   other examples of bias are the following:

   e.g. id97 vectors contain [20]gender biases due to the inherent
   biases that are present in the corpora they have been trained on. when
   you would train a model with these id27s, a recruiter
   searching for "technical profiles" will leave female resumes at the
   bottom of the pile.

   e.g. when you train an id164 model on a small, manually
   created dataset, it is often the case that the breadth of images is too
   limited. a wide variety of images of the objects in different
   environments, different lightning conditions and different angles is
   required in order to avoid an model that only fits to noisy and
   unimportant elements in the data.

     accounting for the context of the problem.

   in most problems, you are working with a dataset that is only a rough
   representation of the problem you are trying to solve and a machine
   learning model can typically not capture the full complexity of the
   real-life task. an interpretable model helps you to understand and
   account for the factors that are (not) included in the model and
   account for the context of the problem when taking actions based on
   model predictions.

     improving generalisation and performance.

   a high interpretability typically leads to a model that generalises
   better. interpretability is not about understanding every single detail
   of the model for all of the data points. the combination of solid data,
   model and problem understanding is necessary to have a solution that
   performs better.

     ethical and legal reasons.

   in industries like finance and healthcare it is essential to audit the
   decision process and ensure it is e.g. not discriminatory or violating
   any laws. with the rise of data and privacy protection regulation like
   gdpr, interpretability becomes even more essential. in addition, in
   medical applications or self-driving cars, a single incorrect
   prediction can have a significant impact and being able to    verify    the
   model is critical. therefore the system should be able to explain how
   it reached a given recommendation.

interpreting your models

   a common quote on model interpretability is that with an increase in
   model complexity, model interpretability goes down at least as fast.
   feature importance is a basic (and often free) approach to interpreting
   your model. even for black-box models such as deep learning, techniques
   exist to improve interpretability. finally, the lime framework will be
   discussed, which serves as a toolbox for model analysis.

   feature importance
     * generalised linear models

   generalised linear models ([21]glm   s) are all based on the following
   principle:
   if you take a linear combination of your features x with the model
   weights w, and feed the result through a squash function f, you can use
   it to predict a wide variety of response variables. most common
   applications for glm   s are regression (id75),
   classification (id28) or modelling poisson processes
   (poisson regression). the weights that are obtained after training are
   a direct proxy of feature importance and they provide very concrete
   interpretation of the model internals.

   e.g. when building a text classifier you can plot the most important
   features and verify whether the model is overfitting on noise. if the
   most important words do not correspond to your intuition (e.g. names or
   stopwords), it probably means that the model is fitting to noise in the
   dataset and it won   t perform well on new data.
   [0*g5tgstlr1mqbfnoz.png]
   an example of a neat visualisation for text interpretability purposes
   from [22]tidytextmining.
     * id79 and id166   s

   even non-linear models such as tree based models (e.g. id79)
   also allow to obtain information on the feature importance. in random
   forest, feature importance comes for free when training a model, so it
   is a great way to verify initial hypotheses and identify    what    the
   model is learning. the weights in kernel based approaches such as id166   s
   are often not a very good proxy of feature importance. the advantage of
   kernel methods is that you are able to capture non-linear relations
   between variables by projecting the features into kernel space. on the
   other hand, just looking at the weights as feature importance does not
   do justice to the feature interaction.
   [0*ztwkq69goh8-ltb5.png]
   by looking at the feature importance, you can identify what the model
   is learning. as a lot of importance in this model is put into time of
   the day, it might be worthwhile to incorporate additional time-based
   features. ([23]kaggle)
     * deep learning

   deep learning models are notorious for their un-interpretability due to
   the shear number of parameters and the complex approach to extracting
   and combining features. as this class of models is able to obtain
   state-of-the-art performance on a lot of tasks, a lot of research is
   focused on linking model predictions to the inputs.
   [1*aelhps_fmjlxtxqhbb8v4w.png]
   the amount of research on interpretable machine learning is growing
   rapidly ([24]mit).

   especially when moving towards even more complex systems that process
   text and image data, it becomes hard to interpret what the model is
   actually learning. the main focus in research is currently primarily on
   linking and correlating outputs or predictions back to the input data.
   while this is fairly easy in the context of linear model, it is still
   an unsolved problem for deep learning networks. the two main approaches
   are either gradient-based or attention-based.

   -in gradient-based methods, the gradients of the target concept
   calculated in a backward pass are used to produce a map that highlights
   the important regions in the input for predicting the target concept.
   this is typically applied in the context of id161.
   [1*dpoq3yrkcxm_30xewadyqq.png]
   [25]grad-cam, a gradient-based method is used in visual caption
   generation. based on the output caption, the method determines which
   regions in the input image were important.

   -attention-based methods are typically used with sequential data (e.g.
   text data). in addition to the normal weights of the network, attention
   weights are trained that act as    input gates   . these attention weights
   determine how much each of the different elements in the final network
   output. besides interpretability, attention within the context of the
   e.g. text-based question-answering also leads to better results as the
   network is able to    focus    its attention.
   [0*ke2487hwdjlqo7hl.png]
   in id53 with attention, it is possible to indicate which
   words in the text are most important to determine the answer on a
   question.

   lime

   [26]lime is a more general framework that aims to make the predictions
   of    any    machine learning model more interpretable.

   in order to remain model-independent, lime works by modifying the input
   to the model locally. so instead of trying to understand the entire
   model at the same time, a specific input instance is modified and the
   impact on the predictions are monitored. in the context of text
   classification, this means that some of the words are e.g. replaced, to
   determine which elements of the input impact the predictions.

   if you have any questions on interpretability in machine learning, i   ll
   be happy to read them in the comments. follow me on [27]medium or
   [28]twitter if you want to receive updates on my blog posts!

     * [29]machine learning
     * [30]deep learning
     * [31]data science
     * [32]statistics
     * [33]towards data science

   (button)
   (button)
   (button) 3.7k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of lars hulstaert

[35]lars hulstaert

   data scientist at microsoft. previously masters student at cambridge,
   engineering student in ghent. i like connecting the dots.

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.7k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/70c30694a05f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_mx1uwqaou3xm---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@lars.hulstaert?source=post_header_lockup
  17. https://towardsdatascience.com/@lars.hulstaert
  18. https://xkcd.com/1838/
  19. https://towardsdatascience.com/media/b7e2a24f5613a3d48b9b6828eb504122?postid=70c30694a05f
  20. http://wordbias.umiacs.umd.edu/
  21. https://en.wikipedia.org/wiki/generalized_linear_model
  22. https://www.tidytextmining.com/02-sentiment-analysis_files/figure-html/pipetoplot-1.png
  23. https://www.kaggle.com/general/13285
  24. http://people.csail.mit.edu/beenkim/papers/beenk_finaledv_icml2017_tutorial.pdf
  25. https://arxiv.org/pdf/1610.02391.pdf
  26. https://github.com/marcotcr/lime
  27. https://medium.com/@lars.hulstaert
  28. https://twitter.com/larshulstaert
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/deep-learning?source=post
  31. https://towardsdatascience.com/tagged/data-science?source=post
  32. https://towardsdatascience.com/tagged/statistics?source=post
  33. https://towardsdatascience.com/tagged/towards-data-science?source=post
  34. https://towardsdatascience.com/@lars.hulstaert?source=footer_card
  35. https://towardsdatascience.com/@lars.hulstaert
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/70c30694a05f/share/twitter
  42. https://medium.com/p/70c30694a05f/share/facebook
  43. https://medium.com/p/70c30694a05f/share/twitter
  44. https://medium.com/p/70c30694a05f/share/facebook
