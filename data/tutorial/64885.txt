   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]insight data
     * [9]about insight
     * [10]data science
     * [11]data engineering
     * [12]health data
     * [13]ai
     * [14]data pm
     * [15]devops
     __________________________________________________________________

how to solve 90% of nlp problems: a step-by-step guide

using machine learning to understand and leverage text.

   [16]go to the profile of emmanuel ameisen
   [17]emmanuel ameisen (button) blockedunblock (button) followfollowing
   jan 24, 2018
   [1*j8-5vgowtmsijhyhauljsw.png]
   how you can apply the 5 w   s and h to text data!

   for more content like this, follow [18]insight and [19]emmanuel on
   twitter.

text data is everywhere

   whether you are an established company or working to launch a new
   service, you can always leverage text data to validate, improve, and
   expand the functionalities of your product. the science of extracting
   meaning and learning from text data is an active topic of research
   called natural language processing (nlp).

   nlp produces [20]new and [21]exciting [22]results on a daily basis, and
   is a very large field. however, having worked with hundreds of
   companies, the insight team has seen a few key practical applications
   come up much more frequently than any other:
     * identifying different cohorts of users/customers (e.g. predicting
       churn, lifetime value, product preferences)
     * accurately detecting and extracting different categories of
       feedback (positive and negative reviews/opinions, mentions of
       particular attributes such as clothing size/fit   )
     * classifying text according to intent (e.g. request for basic help,
       urgent problem)

   while many nlp papers and tutorials exist online, we have found it hard
   to find guidelines and tips on how to approach these problems
   efficiently from the ground up.

how this article can help

   after leading hundreds of projects a year and gaining advice from top
   teams all over the united states, we wrote this post to explain how to
   build machine learning solutions to solve problems like the ones
   mentioned above. we   ll begin with the simplest method that could work,
   and then move on to more nuanced solutions, such as feature
   engineering, word vectors, and deep learning.

   after reading this article, you   ll know how to:
     * gather, prepare and inspect data
     * build simple models to start, and transition to deep learning if
       necessary
     * interpret and understand your models, to make sure you are actually
       capturing information and not noise

   we wrote this post as a step-by-step guide; it can also serve as a high
   level overview of highly effective standard approaches.
     __________________________________________________________________

   this post is accompanied by [23]an interactive notebook demonstrating
   and applying all these techniques. feel free to run the code and follow
   along!

step 1: gather your data

example data sources

   every machine learning problem starts with data, such as a list of
   emails, posts, or tweets. common sources of textual information
   include:
     * product reviews (on amazon, yelp, and various app stores)
     * user-generated content (tweets, facebook posts, stackoverflow
       questions)
     * troubleshooting (customer requests, support tickets, chat logs)

      disasters on social media    dataset

   for this post, we will use a dataset generously provided by [24]figure
   eight, called    disasters on social media   , where:

     contributors looked at over 10,000 tweets culled with a variety of
     searches like    ablaze   ,    quarantine   , and    pandemonium   , then noted
     whether the tweet referred to a disaster event (as opposed to a joke
     with the word or a movie review or something non-disastrous).

   our task will be to detect which tweets are about a disastrous event as
   opposed to an irrelevant topic such as a movie. why? a potential
   application would be to exclusively notify law enforcement officials
   about urgent emergencies while ignoring reviews of the most recent adam
   sandler film. a particular challenge with this task is that both
   classes contain the same search terms used to find the tweets, so we
   will have to use subtler differences to distinguish between them.

   in the rest of this post, we will refer to tweets that are about
   disasters as    disaster   , and tweets about anything else as
      irrelevant   .

labels

   we have labeled data and so we know which tweets belong to which
   categories. as richard socher outlines below, it is usually faster,
   simpler, and cheaper to find and label enough data to train a model on,
   rather than trying to optimize a complex unsupervised method.
   [1*cdnxya_fmxxeceq1kutfrg.png]
   richard socher   s pro-tip

step 2: clean your data

     the number one rule we follow is:    your model will only ever be as
     good as your data.   

   one of the key skills of a data scientist is knowing whether the next
   step should be working on the model or the data. a good rule of thumb
   is to look at the data first and then clean it up. a clean dataset will
   allow a model to learn meaningful features and not overfit on
   irrelevant noise.

   here is a checklist to use to clean your data: (see the [25]code for
   more details):
    1. remove all irrelevant characters such as any non alphanumeric
       characters
    2. [26]tokenize your text by separating it into individual words
    3. remove words that are not relevant, such as    @    twitter mentions or
       urls
    4. convert all characters to lowercase, in order to treat words such
       as    hello   ,    hello   , and    hello    the same
    5. consider combining misspelled or alternately spelled words to a
       single representation (e.g.    cool   /   kewl   /   cooool   )
    6. consider [27]lemmatization (reduce words such as    am   ,    are   , and
          is    to a common form such as    be   )

   after following these steps and checking for additional errors, we can
   start using the clean, labelled data to train models!

step 3: find a good data representation

   machine learning models take numerical values as input. models working
   on images, for example, take in a matrix representing the intensity of
   each pixel in each color channel.
   [1*6pw5mpaxyhybzxkc-hkf0a.png]
   a smiling face represented as a matrix of numbers.

   our dataset is a list of sentences, so in order for our algorithm to
   extract patterns from the data, we first need to find a way to
   represent it in a way that our algorithm can understand, i.e. as a list
   of numbers.

one-hot encoding (bag of words)

   a natural way to represent text for computers is to encode each
   character individually as a number ([28]ascii for example). if we were
   to feed this simple representation into a classifier, it would have to
   learn the structure of words from scratch based only on our data, which
   is impossible for most datasets. we need to use a higher level
   approach.

   for example, we can build a vocabulary of all the unique words in our
   dataset, and associate a unique index to each word in the vocabulary.
   each sentence is then represented as a list that is as long as the
   number of distinct words in our vocabulary. at each index in this list,
   we mark how many times the given word appears in our sentence. this is
   called a [29]id159, since it is a representation that
   completely ignores the order of words in our sentence. this is
   illustrated below.
   [1*oq3suk0ayc8z8i1qil5big.png]
   representing sentences as a bag of words. sentences on the left,
   representation on the right. each index in the vectors represent one
   particular word.

visualizing the embeddings

   we have around 20,000 words in our vocabulary in the    disasters of
   social media    example, which means that every sentence will be
   represented as a vector of length 20,000. the vector will contain
   mostly 0s because each sentence contains only a very small subset of
   our vocabulary.

   in order to see whether our embeddings are capturing information that
   is relevant to our problem (i.e. whether the tweets are about disasters
   or not), it is a good idea to visualize them and see if the classes
   look well separated. since vocabularies are usually very large and
   visualizing data in 20,000 dimensions is impossible, techniques like
   [30]pca will help project the data down to two dimensions. this is
   plotted below.
   [1*ikis3efujlrmvk_jeqmvzq.png]
   visualizing bag of words embeddings.

   the two classes do not look very well separated, which could be a
   feature of our embeddings or simply of our id84. in
   order to see whether the bag of words features are of any use, we can
   train a classifier based on them.

step 4: classification

   when first approaching a problem, a general best practice is to start
   with the simplest tool that could solve the job. whenever it comes to
   classifying data, a common favorite for its versatility and
   explainability is [31]id28. it is very simple to train
   and the results are interpretable as you can easily extract the most
   important coefficients from the model.

   we split our data in to a training set used to fit our model and a test
   set to see how well it generalizes to unseen data. after training, we
   get an accuracy of 75.4%. not too shabby! guessing the most frequent
   class (   irrelevant   ) would give us only 57%. however, even if 75%
   precision was good enough for our needs, we should never ship a model
   without trying to understand it.

step 5: inspection

confusion matrix

   a first step is to understand the types of errors our model makes, and
   which kind of errors are least desirable. in our example, false
   positives are classifying an irrelevant tweet as a disaster, and false
   negatives are classifying a disaster as an irrelevant tweet. if the
   priority is to react to every potential event, we would want to lower
   our false negatives. if we are constrained in resources however, we
   might prioritize a lower false positive rate to reduce false alarms. a
   good way to visualize this information is using a [32]confusion matrix,
   which compares the predictions our model makes with the true label.
   ideally, the matrix would be a diagonal line from top left to bottom
   right (our predictions match the truth perfectly).
   [1*dicbwuooqfezdwrofsz-cq.png]
   confusion matrix (green is a high proportion, blue is low)

   our classifier creates more false negatives than false positives
   (proportionally). in other words, our model   s most common error is
   inaccurately classifying disasters as irrelevant. if false positives
   represent a high cost for law enforcement, this could be a good bias
   for our classifier to have.

explaining and interpreting our model

   to validate our model and interpret its predictions, it is important to
   look at which words it is using to make decisions. if our data is
   biased, our classifier will make accurate predictions in the sample
   data, but the model would not generalize well in the real world. here
   we plot the most important words for both the disaster and irrelevant
   class. plotting word importance is simple with bag of words and
   id28, since we can just extract and rank the
   coefficients that the model used for its predictions.
   [1*uick4xwfmf8mh0ll_fujca.png]
   bag of words: word importance

   our classifier correctly picks up on some patterns (hiroshima,
   massacre), but clearly seems to be overfitting on some meaningless
   terms (heyoo, x1392). right now, our id159 is dealing with
   a huge vocabulary of different words and treating all words equally.
   however, some of these words are very frequent, and are only
   contributing noise to our predictions. next, we will try a way to
   represent sentences that can account for the frequency of words, to see
   if we can pick up more signal from our data.

step 6: accounting for vocabulary structure

tf-idf

   in order to help our model focus more on meaningful words, we can use a
   [33]tf-idf score (term frequency, inverse document frequency) on top of
   our id159. tf-idf weighs words by how rare they are in our
   dataset, discounting words that are too frequent and just add to the
   noise. here is the pca projection of our new embeddings.
   [1*wjd8-cq009lmkrvfcvaw0w.png]
   visualizing tf-idf embeddings.

   we can see above that there is a clearer distinction between the two
   colors. this should make it easier for our classifier to separate both
   groups. let   s see if this leads to better performance. training another
   id28 on our new embeddings, we get an accuracy of 76.2%.

   a very slight improvement. has our model started picking up on more
   important words? if we are getting a better result while preventing our
   model from    cheating    then we can truly consider this model an upgrade.
   [1*bdcusronh3di6r8xmzurcg.png]
   tf-idf: word importance

   the words it picked up look much more relevant! although our metrics on
   our test set only increased slightly, we have much more confidence in
   the terms our model is using, and thus would feel more comfortable
   deploying it in a system that would interact with customers.

step 7: leveraging semantics

id97

   our latest model managed to pick up on high signal words. however, it
   is very likely that if we deploy this model, we will encounter words
   that we have not seen in our training set before. the previous model
   will not be able to accurately classify these tweets, even if it has
   seen very similar words during training.

   to solve this problem, we need to capture the semantic meaning of
   words, meaning we need to understand that words like    good    and
      positive    are closer than    apricot    and    continent.    the tool we will
   use to help us capture meaning is called id97.

   using pre-trained words

   [34]id97 is a technique to find continuous embeddings for words. it
   learns from reading massive amounts of text and memorizing which words
   tend to appear in similar contexts. after being trained on enough data,
   it generates a 300-dimension vector for each word in a vocabulary, with
   words of similar meaning being closer to each other.

   the authors of the [35]paper open sourced a model that was pre-trained
   on a very large corpus which we can leverage to include some knowledge
   of semantic meaning into our model. the pre-trained vectors can be
   found in the [36]repository associated with this post.

sentence level representation

   a quick way to get a sentence embedding for our classifier is to
   average id97 scores of all words in our sentence. this is a bag of
   words approach just like before, but this time we only lose the syntax
   of our sentence, while keeping some semantic information.
   [1*tho9nkchwkcaoilvs1ehuq.png]
   id97 sentence embedding

   here is a visualization of our new embeddings using previous
   techniques:
   [1*dt66magpumpgxhfawooiea.png]
   visualizing id97 embeddings.

   the two groups of colors look even more separated here, our new
   embeddings should help our classifier find the separation between both
   classes. after training the same model a third time (a logistic
   regression), we get an accuracy score of 77.7%, our best result yet!
   time to inspect our model.

the complexity/explainability trade-off

   since our embeddings are not represented as a vector with one dimension
   per word as in our previous models, it   s harder to see which words are
   the most relevant to our classification. while we still have access to
   the coefficients of our id28, they relate to the 300
   dimensions of our embeddings rather than the indices of words.

   for such a low gain in accuracy, losing all explainability seems like a
   harsh trade-off. however, with more complex models we can leverage
   black box explainers such as [37]lime in order to get some insight into
   how our classifier works.

   lime

   lime is [38]available on github through an open-sourced package. a
   black-box explainer allows users to explain the decisions of any
   classifier on one particular example by perturbing the input (in our
   case removing words from the sentence) and seeing how the prediction
   changes.

   let   s see a couple explanations for sentences from our dataset.
   [1*w8zlzbhildswpeww_bbcaw.png]
   correct disaster words are picked up to classify as    relevant   .
   [1*44zlmsnb-heny0bxssmzfg.png]
   here, the contribution of the words to the classification seems
   less obvious.

   however, we do not have time to explore the thousands of examples in
   our dataset. what we   ll do instead is run lime on a representative
   sample of test cases and see which words keep coming up as strong
   contributors. using this approach we can get word importance scores
   like we had for previous models and validate our model   s predictions.
   [1*yxfofx_kxovglm_olzd9sw.png]
   id97: word importance

   looks like the model picks up highly relevant words implying that it
   appears to make understandable decisions. these seem like the most
   relevant words out of all previous models and therefore we   re more
   comfortable deploying in to production.

step 8: leveraging syntax using end-to-end approaches

   we   ve covered quick and efficient approaches to generate compact
   sentence embeddings. however, by omitting the order of words, we are
   discarding all of the syntactic information of our sentences. if these
   methods do not provide sufficient results, you can utilize more complex
   model that take in whole sentences as input and predict labels without
   the need to build an intermediate representation. a common way to do
   that is to treat a sentence as a sequence of individual word vectors
   using either id97 or more recent approaches such as [39]glove or
   [40]cove. this is what we will do below.
   [1*p6ybb_gmgxmosjfq-qb5xa.png]
   a highly effective end-to-end architecture ([41]source)

   [42]convolutional neural networks for sentence classification train
   very quickly and work well as an entry level deep learning
   architecture. while convolutional neural networks (id98) are mainly
   known for their performance on image data, they have been providing
   excellent results on text related tasks, and are usually much quicker
   to train than most complex nlp approaches (e.g. [43]lstms and
   [44]encoder/decoder architectures). this model preserves the order of
   words and learns valuable information on which sequences of words are
   predictive of our target classes. contrary to previous models, it can
   tell the difference between    alex eats plants    and    plants eat alex.   

   training this model does not require much more work than previous
   approaches (see [45]code for details) and gives us a model that is much
   better than the previous ones, getting 79.5% accuracy! as with the
   models above, the next step should be to explore and explain the
   predictions using the methods we described to validate that it is
   indeed the best model to deploy to users. by now, you should feel
   comfortable tackling this on your own.

final notes

   here is a quick recap of the approach we   ve successfully used:
     * start with a quick and simple model
     * explain its predictions
     * understand the kind of mistakes it is making
     * use that knowledge to inform your next step, whether that is
       working on your data, or a more complex model.

   these approaches were applied to a particular example case using models
   tailored towards understanding and leveraging short text such as
   tweets, but the ideas are widely applicable to a variety of problems. i
   hope this helped you, we   d love to hear your comments and questions!
   feel free to comment below or reach out to [46]@emmanuelameisen here or
   on [47]twitter.
     __________________________________________________________________

   want to learn applied artificial intelligence from top professionals in
   silicon valley or new york? learn more about the [48]artificial
   intelligence program.

   are you a company working in ai and would like to get involved in the
   insight ai fellows program? feel free to [49]get in touch.

     * [50]machine learning
     * [51]business
     * [52]artificial intelligence
     * [53]tutorial
     * [54]insight ai

   (button)
   (button)
   (button) 16.7k claps
   (button) (button) (button) 35 (button) (button)

     (button) blockedunblock (button) followfollowing
   [55]go to the profile of emmanuel ameisen

[56]emmanuel ameisen

   head of ai at insight data science [57]@emmanuelameisen

     (button) follow
   [58]insight data

[59]insight data

   insight fellows program - your bridge to a thriving career

     * (button)
       (button) 16.7k
     * (button)
     *
     *

   [60]insight data
   never miss a story from insight data, when you sign up for medium.
   [61]learn more
   never miss a story from insight data
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.insightdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/fda605278e4e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&source=--------------------------nav_reg&operation=register
   8. https://blog.insightdatascience.com/?source=logo-lo_e6avkym4y8rw---d02e65779d7b
   9. https://blog.insightdatascience.com/tagged/about-insight
  10. https://blog.insightdatascience.com/tagged/insight-data-science
  11. https://blog.insightdatascience.com/tagged/insight-data-engineering
  12. https://blog.insightdatascience.com/tagged/insight-health-data
  13. https://blog.insightdatascience.com/tagged/insight-ai
  14. https://blog.insightdatascience.com/tagged/insight-data-pm
  15. https://blog.insightdatascience.com/tagged/insight-devops
  16. https://blog.insightdatascience.com/@emmanuelameisen?source=post_header_lockup
  17. https://blog.insightdatascience.com/@emmanuelameisen
  18. https://twitter.com/insightdatasci
  19. https://twitter.com/emmanuelameisen
  20. https://arxiv.org/abs/1704.01444
  21. https://arxiv.org/abs/1711.00043
  22. https://arxiv.org/abs/1708.04729
  23. https://github.com/hundredblocks/concrete_nlp_tutorial/blob/master/nlp_notebook.ipynb
  24. http://www.figure-eight.com/data-for-everyone
  25. https://github.com/hundredblocks/concrete_nlp_tutorial/blob/master/nlp_notebook.ipynb
  26. https://nlp.stanford.edu/ir-book/html/htid113dition/id121-1.html
  27. https://nlp.stanford.edu/ir-book/html/htid113dition/id30-and-lemmatization-1.html
  28. https://en.wikipedia.org/wiki/ascii
  29. https://en.wikipedia.org/wiki/bag-of-words_model
  30. https://en.wikipedia.org/wiki/principal_component_analysis
  31. https://en.wikipedia.org/wiki/logistic_regression
  32. https://en.wikipedia.org/wiki/confusion_matrix
  33. https://en.wikipedia.org/wiki/tf   idf
  34. https://arxiv.org/abs/1301.3781
  35. https://arxiv.org/abs/1301.3781
  36. https://github.com/hundredblocks/concrete_nlp_tutorial
  37. https://arxiv.org/abs/1602.04938
  38. https://github.com/marcotcr/lime
  39. https://nlp.stanford.edu/projects/glove/
  40. https://arxiv.org/abs/1708.00107
  41. https://arxiv.org/abs/1408.5882
  42. https://arxiv.org/abs/1408.5882
  43. http://colah.github.io/posts/2015-08-understanding-lstms/
  44. https://www.tensorflow.org/tutorials/id195
  45. https://github.com/hundredblocks/concrete_nlp_tutorial/blob/master/nlp_notebook.ipynb
  46. https://medium.com/@emmanuelameisen
  47. https://twitter.com/emmanuelameisen
  48. http://insightdata.ai/?utm_source=nlp-notebook&utm_medium=blog&utm_content=top
  49. http://insightdatascience.com/partnerships?utm_source=nlp-notebook&utm_medium=blog&utm_content=top
  50. https://blog.insightdatascience.com/tagged/machine-learning?source=post
  51. https://blog.insightdatascience.com/tagged/business?source=post
  52. https://blog.insightdatascience.com/tagged/artificial-intelligence?source=post
  53. https://blog.insightdatascience.com/tagged/tutorial?source=post
  54. https://blog.insightdatascience.com/tagged/insight-ai?source=post
  55. https://blog.insightdatascience.com/@emmanuelameisen?source=footer_card
  56. https://blog.insightdatascience.com/@emmanuelameisen
  57. http://twitter.com/emmanuelameisen
  58. https://blog.insightdatascience.com/?source=footer_card
  59. https://blog.insightdatascience.com/?source=footer_card
  60. https://blog.insightdatascience.com/
  61. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  63. https://medium.com/p/fda605278e4e/share/twitter
  64. https://medium.com/p/fda605278e4e/share/facebook
  65. https://medium.com/p/fda605278e4e/share/twitter
  66. https://medium.com/p/fda605278e4e/share/facebook
