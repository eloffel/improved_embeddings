   #[1]github [2]recent commits to vecmap:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]24
     * [35]star [36]321
     * [37]fork [38]69

[39]artetxem/[40]vecmap

   [41]code [42]issues 7 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   a framework to learn cross-lingual id27 mappings
     * [47]21 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]gpl-3.0

    1. [52]python 93.9%
    2. [53]shell 6.1%

   (button) python shell
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/a
   [55]download zip

downloading...

   want to be notified of new releases in artetxem/vecmap?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@artetxem
   [63]artetxem [64]update references
   latest commit [65]585bf74 oct 25, 2018
   [66]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [67].gitignore
   [68]license.txt [69]initial commit sep 27, 2016
   [70]readme.md [71]update references oct 25, 2018
   [72]cupy_utils.py [73]add cuda support through cupy (optional
   dependency) jan 12, 2018
   [74]embeddings.py
   [75]eval_analogy.py
   [76]eval_similarity.py
   [77]eval_translation.py
   [78]get_data.sh
   [79]map_embeddings.py [80]release code from our acl 2018 paper jun 8,
   2018
   [81]normalize_embeddings.py

readme.md

vecmap (cross-lingual id27 mappings)

   this is an open source implementation of our framework to learn
   cross-lingual id27 mappings, described in the following
   papers:
     * mikel artetxe, gorka labaka, and eneko agirre. 2018. [82]a robust
       self-learning method for fully unsupervised cross-lingual mappings
       of id27s. in proceedings of the 56th annual meeting of
       the association for computational linguistics (volume 1: long
       papers).
     * mikel artetxe, gorka labaka, and eneko agirre. 2018.
       [83]generalizing and improving bilingual id27 mappings
       with a multi-step framework of linear transformations. in
       proceedings of the thirty-second aaai conference on artificial
       intelligence (aaai-18), pages 5012-5019.
     * mikel artetxe, gorka labaka, and eneko agirre. 2017. [84]learning
       bilingual id27s with (almost) no bilingual data. in
       proceedings of the 55th annual meeting of the association for
       computational linguistics (volume 1: long papers), pages 451-462.
     * mikel artetxe, gorka labaka, and eneko agirre. 2016. [85]learning
       principled bilingual mappings of id27s while preserving
       monolingual invariance. in proceedings of the 2016 conference on
       empirical methods in natural language processing, pages 2289-2294.

   the package includes a script to build cross-lingual id27s
   with or without parallel data as described in the papers, as well as
   evaluation tools in word translation induction, word
   similarity/relatedness and word analogy.

   if you use this software for academic research, [86]please cite the
   relevant paper(s).

requirements

     * python 3
     * numpy
     * scipy
     * cupy (optional, only required for cuda support)

usage

   in order to build your own cross-lingual id27s, you should
   first train monolingual id27s for each language using your
   favorite tool (e.g. [87]id97 or [88]fasttext) and then map them to
   a common space with our software as described below. having done that,
   you can evaluate the resulting cross-lingual embeddings using our
   included tools as discussed next.

mapping

   the mapping software offers 4 main modes with our recommended settings
   for different scenarios:
     * supervised (recommended if you have a large training dictionary):

python3 map_embeddings.py --supervised train.dict src.emb trg.emb src_mapped.emb
 trg_mapped.emb

     * semi-supervised (recommended if you have a small seed dictionary):

python3 map_embeddings.py --semi_supervised train.dict src.emb trg.emb src_mappe
d.emb trg_mapped.emb

     * identical (recommended if you have no seed dictionary but can rely
       on identical words):

python3 map_embeddings.py --identical src.emb trg.emb src_mapped.emb trg_mapped.
emb

     * unsupervised (recommended if you have no seed dictionary and do not
       want to rely on identical words):

python3 map_embeddings.py --unsupervised src.emb trg.emb src_mapped.emb trg_mapp
ed.emb

   src.emb and trg.emb refer to the input monolingual embeddings, which
   should be in the id97 text format, whereas src_mapped.emb and
   trg_mapped.emb refer to the output cross-lingual embeddings. the
   training dictionary train.dict, if any, should be given as a text file
   with one entry per line (source word + whitespace + target word).

   if you have a nvidia gpu, append the --cuda flag to the above commands
   to make things faster.

   for most users, the above settings should suffice. choosing the right
   mode should be straightforward depending on the resources available: as
   a general rule, you should prefer the mode with the highest supervision
   for the resources you have, although it is advised to try different
   variants in case of doubt.

   in addition to these recommended modes, the software also offers
   additional options to adjust different aspects of the mapping method as
   described in the papers. while most users should not need to deal with
   those, you can learn more about them by running the tool with the
   --help flag. you can either use one of the recommended modes and modify
   a few options on top of it, or do not use any recommended mode and set
   all options yourself. in fact, if you dig into the code, you will see
   that the above modes simply set recommended defaults for all the
   different options.

evaluation

   you can evaluate your mapped embeddings in bilingual lexicon extraction
   (aka dictionary induction or word translation) as follows:
python3 eval_translation.py src_mapped.emb trg_mapped.emb -d test.dict

   the above command uses standard nearest neighbor retrieval by default.
   for best results, it is recommended that you use csls retrieval
   instead:
python3 eval_translation.py src_mapped.emb trg_mapped.emb -d test.dict --retriev
al csls

   while better, csls is also significantly slower than nearest neighbor,
   so do not forget to append the --cuda flag to the above command if you
   have a nvidia gpu.

   in addition to bilingual lexicon extraction, you can also evaluate your
   mapped embeddings in cross-lingual word similarity as follows:
python3 eval_similarity.py -l --backoff 0 src_mapped.emb trg_mapped.emb -i test_
similarity.txt

   finally, we also offer an evaluation tool for monolingual word
   analogies, which mimics the one included with id97 but should run
   significantly faster:
python3 eval_analogy.py -l src_mapped.emb -i test_analogies.txt -t 30000

dataset

   you can use the following script to download the main dataset used in
   our papers, which is an extension of that of [89]dinu et al. (2014):
./get_data.sh

reproducing results

   while we always recommend to use the above settings for best results
   when working with your own embeddings, we also offer additional modes
   to replicate the systems from our different papers as follows:
     * acl 2018 (currently equivalent to the unsupervised mode):

python3 map_embeddings.py --acl2018 src.emb trg.emb src_mapped.emb trg_mapped.em
b

     * aaai 2018 (currently equivalent to the supervised mode, except for
       minor differences in re-weighting, id172 and dimensionality
       reduction):

python3 map_embeddings.py --aaai2018 train.dict src.emb trg.emb src_mapped.emb t
rg_mapped.emb

     * acl 2017 (superseded by our acl 2018 system; offers 2 modes
       depending on the initialization):

python3 map_embeddings.py --acl2017 src.emb trg.emb src_mapped.emb trg_mapped.em
b
python3 map_embeddings.py --acl2017_seed train.dict src.emb trg.emb src_mapped.e
mb trg_mapped.emb

     * emnlp 2016 (superseded by our aaai 2018 system):

python3 map_embeddings.py --emnlp2016 train.dict src.emb trg.emb src_mapped.emb
trg_mapped.emb

faq

how long does training take?

     * the supervised mode (--supervised) should run in around 2 minutes
       in either cpu or gpu.
     * the rest of recommended modes (either --semi_supervised,
       --identical or --unsupervised) should run in around 5 hours in cpu,
       or 10 minutes in gpu (titan xp or similar).

this is running much slower for me! what can i do?

    1. if you have a gpu, do not forget the --cuda flag.
    2. make sure that your numpy installation is properly linked to
       blas/lapack. this is particularly important if you are working on
       cpu, as it can have a huge impact in performance if not properly
       set up.
    3. there are different settings that affect the execution time of the
       algorithm and can thus be adjusted to make things faster: the batch
       size (--batch_size), the vocabulary cutoff (--vocabulary_cutoff),
       the stochastic dictionary induction settings (--stochastic_initial,
       --stochastic_multiplier and --stochastic_interval) and the
       convergence threshold (--threshold), among others. however, most of
       these settings will have a direct impact in the quality of the
       resulting embeddings, so you should not play with them unless you
       really know what you are doing.

prior versions of this software included nice scripts to reproduce the exact
same results reported in your papers. why are those missing now?

   as the complexity of the software (and the number of
   publications/results to reproduce) increased, maintaining those nice
   scripts became very tedious. moreover, with the inclusion of cuda
   support and fp32 precision, reproducing the exact same results on
   different platforms became inviable due to minor numerical variations
   in the underlying computations, which were magnified by self-learning
   (e.g. the exact same command is likely to produce a slightly different
   output on cpu and gpu). while the effect in the final results is
   negligible (the observed variations are around 0.1-0.2 accuracy
   points), this made it unfeasible to reproduce the exact same results in
   different platforms.

   instead of that, we now provide an [90]easy interface to run all the
   systems proposed in our different papers. we think that this might be
   even more useful than the previous approach: the most skeptical user
   should still be able to easily verify our results, while we also
   provide a simple interface to test our different systems in other
   datasets.

the ablation test in your acl 2018 paper reports 0% accuracies for removing
csls, but i am getting better results. why is that?

   after publishing the paper, we discovered a bug in the code that was
   causing those 0% accuracies. now that the bug is fixed, the effect of
   removing csls is not that dramatic, although it still has a big
   negative impact. at the same time, the effect of removing the
   bidirectional dictionary induction in that same ablation test is
   slightly smaller.

see also

   vecmap is a basic building block of [91]monoses, our unsupervised
   id151 system. you can use them in combination
   to train your own machine translation model from monolingual corpora
   alone.

publications

   if you use this software for academic research, please cite the
   relevant paper(s) as follows (in case of doubt, please cite the acl
   2018 paper, or the aaai 2018 paper if you use the supervised mode):
@inproceedings{artetxe2018acl,
  author    = {artetxe, mikel  and  labaka, gorka  and  agirre, eneko},
  title     = {a robust self-learning method for fully unsupervised cross-lingua
l mappings of id27s},
  booktitle = {proceedings of the 56th annual meeting of the association for com
putational linguistics (volume 1: long papers)},
  year      = {2018},
  pages     = {789--798}
}

@inproceedings{artetxe2018aaai,
  author    = {artetxe, mikel  and  labaka, gorka  and  agirre, eneko},
  title     = {generalizing and improving bilingual id27 mappings with
 a multi-step framework of linear transformations},
  booktitle = {proceedings of the thirty-second aaai conference on artificial in
telligence},
  year      = {2018},
  pages     = {5012--5019}
}

@inproceedings{artetxe2017acl,
  author    = {artetxe, mikel  and  labaka, gorka  and  agirre, eneko},
  title     = {learning bilingual id27s with (almost) no bilingual dat
a},
  booktitle = {proceedings of the 55th annual meeting of the association for com
putational linguistics (volume 1: long papers)},
  year      = {2017},
  pages     = {451--462}
}

@inproceedings{artetxe2016emnlp,
  author    = {artetxe, mikel  and  labaka, gorka  and  agirre, eneko},
  title     = {learning principled bilingual mappings of id27s while p
reserving monolingual invariance},
  booktitle = {proceedings of the 2016 conference on empirical methods in natura
l language processing},
  year      = {2016},
  pages     = {2289--2294}
}

license

   copyright (c) 2016-2018, mikel artetxe

   licensed under the terms of the gnu general public license, either
   version 3 or (at your option) any later version. a full copy of the
   license can be found in license.txt.

     *    2019 github, inc.
     * [92]terms
     * [93]privacy
     * [94]security
     * [95]status
     * [96]help

     * [97]contact github
     * [98]pricing
     * [99]api
     * [100]training
     * [101]blog
     * [102]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [103]reload to refresh your
   session. you signed out in another tab or window. [104]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/artetxem/vecmap/commits/master.atom
   3. https://github.com/artetxem/vecmap#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/artetxem/vecmap
  32. https://github.com/join
  33. https://github.com/login?return_to=/artetxem/vecmap
  34. https://github.com/artetxem/vecmap/watchers
  35. https://github.com/login?return_to=/artetxem/vecmap
  36. https://github.com/artetxem/vecmap/stargazers
  37. https://github.com/login?return_to=/artetxem/vecmap
  38. https://github.com/artetxem/vecmap/network/members
  39. https://github.com/artetxem
  40. https://github.com/artetxem/vecmap
  41. https://github.com/artetxem/vecmap
  42. https://github.com/artetxem/vecmap/issues
  43. https://github.com/artetxem/vecmap/pulls
  44. https://github.com/artetxem/vecmap/projects
  45. https://github.com/artetxem/vecmap/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/artetxem/vecmap/commits/master
  48. https://github.com/artetxem/vecmap/branches
  49. https://github.com/artetxem/vecmap/releases
  50. https://github.com/artetxem/vecmap/graphs/contributors
  51. https://github.com/artetxem/vecmap/blob/master/license.txt
  52. https://github.com/artetxem/vecmap/search?l=python
  53. https://github.com/artetxem/vecmap/search?l=shell
  54. https://github.com/artetxem/vecmap/find/master
  55. https://github.com/artetxem/vecmap/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/artetxem/vecmap
  57. https://github.com/join?return_to=/artetxem/vecmap
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/artetxem
  63. https://github.com/artetxem/vecmap/commits?author=artetxem
  64. https://github.com/artetxem/vecmap/commit/585bf74c6489419682eef9aebe7a8d15f0873b6c
  65. https://github.com/artetxem/vecmap/commit/585bf74c6489419682eef9aebe7a8d15f0873b6c
  66. https://github.com/artetxem/vecmap/tree/585bf74c6489419682eef9aebe7a8d15f0873b6c
  67. https://github.com/artetxem/vecmap/blob/master/.gitignore
  68. https://github.com/artetxem/vecmap/blob/master/license.txt
  69. https://github.com/artetxem/vecmap/commit/147f05e389a22d6b76de9f3a65e91fe4b4564036
  70. https://github.com/artetxem/vecmap/blob/master/readme.md
  71. https://github.com/artetxem/vecmap/commit/585bf74c6489419682eef9aebe7a8d15f0873b6c
  72. https://github.com/artetxem/vecmap/blob/master/cupy_utils.py
  73. https://github.com/artetxem/vecmap/commit/ef8a6dd88808227e9e2173eeeb5ecf8b43ce154e
  74. https://github.com/artetxem/vecmap/blob/master/embeddings.py
  75. https://github.com/artetxem/vecmap/blob/master/eval_analogy.py
  76. https://github.com/artetxem/vecmap/blob/master/eval_similarity.py
  77. https://github.com/artetxem/vecmap/blob/master/eval_translation.py
  78. https://github.com/artetxem/vecmap/blob/master/get_data.sh
  79. https://github.com/artetxem/vecmap/blob/master/map_embeddings.py
  80. https://github.com/artetxem/vecmap/commit/b560c94e3ea07e7082471ea6cf4a2e43af702059
  81. https://github.com/artetxem/vecmap/blob/master/normalize_embeddings.py
  82. https://aclweb.org/anthology/p18-1073
  83. https://www.aaai.org/ocs/index.php/aaai/aaai18/paper/view/16935/16781
  84. https://aclweb.org/anthology/p17-1042
  85. https://aclweb.org/anthology/d16-1250
  86. https://github.com/artetxem/vecmap#publications
  87. https://github.com/tmikolov/id97
  88. https://github.com/facebookresearch/fasttext
  89. http://clic.cimec.unitn.it/~georgiana.dinu/down/
  90. https://github.com/artetxem/vecmap#reproducing-results
  91. https://github.com/artetxem/monoses
  92. https://github.com/site/terms
  93. https://github.com/site/privacy
  94. https://github.com/security
  95. https://githubstatus.com/
  96. https://help.github.com/
  97. https://github.com/contact
  98. https://github.com/pricing
  99. https://developer.github.com/
 100. https://training.github.com/
 101. https://github.blog/
 102. https://github.com/about
 103. https://github.com/artetxem/vecmap
 104. https://github.com/artetxem/vecmap

   hidden links:
 106. https://github.com/
 107. https://github.com/artetxem/vecmap
 108. https://github.com/artetxem/vecmap
 109. https://github.com/artetxem/vecmap
 110. https://help.github.com/articles/which-remote-url-should-i-use
 111. https://github.com/artetxem/vecmap#vecmap-cross-lingual-word-embedding-mappings
 112. https://github.com/artetxem/vecmap#requirements
 113. https://github.com/artetxem/vecmap#usage
 114. https://github.com/artetxem/vecmap#mapping
 115. https://github.com/artetxem/vecmap#evaluation
 116. https://github.com/artetxem/vecmap#dataset
 117. https://github.com/artetxem/vecmap#reproducing-results
 118. https://github.com/artetxem/vecmap#faq
 119. https://github.com/artetxem/vecmap#how-long-does-training-take
 120. https://github.com/artetxem/vecmap#this-is-running-much-slower-for-me-what-can-i-do
 121. https://github.com/artetxem/vecmap#prior-versions-of-this-software-included-nice-scripts-to-reproduce-the-exact-same-results-reported-in-your-papers-why-are-those-missing-now
 122. https://github.com/artetxem/vecmap#the-ablation-test-in-your-acl-2018-paper-reports-0-accuracies-for-removing-csls-but-i-am-getting-better-results-why-is-that
 123. https://github.com/artetxem/vecmap#see-also
 124. https://github.com/artetxem/vecmap#publications
 125. https://github.com/artetxem/vecmap#license
 126. https://github.com/
