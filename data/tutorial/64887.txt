   #[1]publisher

   [2][cm_community_blue.png] [icon_cm_v2.png]
     * [3]find a mentor
          + [4]web programming
               o [5]javascript
               o [6]python
               o [7]php
               o [8]react
               o [9]vue.js
               o [10]angularjs
               o [11]node.js
               o [12]jquery
               o [13]rails
               o [14]django
               o [15]laravel
               o [16]html
               o [17]css
          + [18]mobile app programming
               o [19]android
               o [20]ios
               o [21]swift
               o [22]kotlin
               o [23]xcode
               o [24]ionic
               o [25]react native
          + [26]programming languages
               o [27]javascript
               o [28]python
               o [29]java
               o [30]ruby
               o [31]golang
               o [32]php
               o [33]c++
               o [34]c#
               o [35]c
          + [36]data science /engineering
               o [37]machine learning
               o [38]tensorflow
               o [39]ai
               o [40]python
               o [41]java
               o [42]r
               o [43]matlab
          + [44]database /operations
               o [45]database
               o [46]mysql
               o [47]sql
               o [48]aws
               o [49]linux
               o [50]mongodb
               o [51]heroku
               o [52]docker
               o [53]postgres
               o [54]nginx
          + [55]others
               o [56]electron
               o [57]unity 3d
               o [58]git
               o [59]stripe
               o [60]wordpress
     * [61]find a freelancer
          + [62]front-end
               o [63]javascript
               o [64]react
               o [65]angularjs
               o [66]vue.js
               o [67]web
               o [68]html/css
          + [69]back-end
               o [70]ruby on rails
               o [71]python
               o [72]php
               o [73]elixir
               o [74]node.js
               o [75]django
               o [76]laravel
               o [77]erlang
               o [78]golang
               o [79]lambda
          + [80]mobile/app
               o [81]swift
               o [82]ios
               o [83]android
               o [84]react native
               o [85]ionic
          + [86]database
               o [87]mysql
               o [88]postgres
               o [89]mongodb
               o [90]sql
          + [91]devops
               o [92]aws
               o [93]docker
               o [94]heroku
               o [95]linux
          + others
               o [96]git
               o [97]wordpress
               o [98]stripe
               o [99]web
               o [100]full stack
               o [101]software
     * learning center
       blog
       get insights on scaling, management, and product development for
       founders and engineering managers.
       community posts
       read programming tutorials, share your knowledge, and become better
       developers together.
       hot topics
          + [102]android
          + [103]angular
          + [104]ios
          + [105]javascript
          + [106]node.js
          + [107]python
          + [108]react
          + [109]blockchain
          + [110]ethereum

   ____________________
   write a post
   sign up
   [111]log in
   [112]find a mentor[113]find a freelancer[114]community
   post[115]blogsign uplog in
   [116]james le
   follow
   tech and product enthusiast

a gentle introduction to neural networks for machine learning

   published mar 19, 2018last updated sep 14, 2018
   a gentle introduction to neural networks for machine learning

why do we need machine learning?

   we need machine learning for tasks that are too complex for humans to
   code directly, i.e. tasks that are so complex that it is impractical,
   if not impossible, for us to work out all of the nuances and code for
   them explicitly. so instead, we provide a machine learning algorithm
   with a large amount of data and let it explore and search for a model
   that will work out what the programmers have set out to achieve.

   let   s look at these two examples:
     * it   s very hard to write programs that solve problems like
       recognizing a 3d object, from a novel viewpoint, in new lighting
       conditions, in a cluttered scene. we don   t know what program to
       write because we don   t know how it   s done in our brain. even if we
       had a good idea for how to do it, the program might be horrendously
       complicated.
     * it   s hard to write a program to compute the id203 that a
       credit card transaction is fraudulent. there may not be any rules
       that are both simple and reliable. we need to combine a very large
       number of weak rules. fraud is a moving target, but the program
       needs to keep changing.

   then comes the machine learning approach: instead of writing a program
   by hand for each specific task, we collect lots of examples that
   specify the correct output for a given input. a machine learning
   algorithm then takes these examples and produces a program that does
   the job. the program produced by the learning algorithm may look very
   different from a typical hand-written program     it may contain millions
   of numbers. if we do it right, the program works for new cases, as well
   as the ones we trained it on. if the data changes, the program can
   change too by training from the new data. you should note that
   conducting massive amounts of computation is now cheaper than paying
   someone to write a task-specific program.

   some examples of tasks best solved by machine learning include:
     * recognizing patterns: objects in real scenes, facial identities or
       facial expressions, and/or spoken words
     * recognizing anomalies: unusual sequences of credit card
       transactions, unusual patterns of sensor readings in a nuclear
       power plant
     * prediction: future stock prices or currency exchange rates, which
       movies a person will like

   1.jpg

   what are neural networks?

   neural networks are a class of models within the general machine
   learning literature. neural networks are a specific set of algorithms
   that have revolutionized machine learning. they are inspired by
   biological neural networks and the current so-called deep neural
   networks have proven to work quite well. neural networks are themselves
   general function approximations, which is why they can be applied to
   almost any machine learning problem about learning a complex mapping
   from the input to the output space.

   here are the three reasons you should study neural computation:
     * to understand how the brain actually works: it   s very big and very
       complicated and made of stuff that dies when you poke it, so we
       need to use computer simulations.
     * to understand a style of parallel computation inspired by neurons
       and their adaptive connections: it   s a very different style from
       sequential computation.
     * to solve practical problems by using novel learning algorithms
       inspired by the brain: learning algorithms can be very useful even
       if they are not how the brain actually works.

top 10 neural network architectures you need to know

   1 - id88s
   considered the first generation of neural networks, id88s are
   simply computational models of a single neuron. id88 was
   originally coined by frank rosenblatt ([117]   the id88: a
   probabilistic model for information storage and organization in the
   brain   ) ^[118][1]. also called feed-forward neural network, a
   id88 feeds information from the front to the back. training
   id88s usually require back-propagation, giving the network paired
   datasets of inputs and outputs. inputs are sent into the neuron,
   processed, and result in an output. the error that is back propagated
   is usually the difference between the input and the output data. if the
   network has enough hidden neurons, it can always model the relationship
   between the input and output. practically, their use is a lot more
   limited, but they are popularly combined with other networks to form
   new networks.

   2.png

   if you choose features by hand and have enough , you can do almost
   anything. for binary input vectors, we can have a separate feature unit
   for each of the exponentially many binary vectors and we can make any
   possible discrimination for binary input vectors. however, id88s
   do have limitations: once the hand-coded features have been determined,
   there are very strong limitations on what a id88 can learn.

   2 - convolutional neural networks
   in 1998, yann lecun and his collaborators developed a really good
   recognizer for handwritten digits called [119]lenet. it used
   back-propagation in a feedforward net with many hidden layers, many
   maps of replicated units in each layer, output pooling of nearby
   replicated units, a wide net that can cope with several characters at
   once, even if they overlap, and a clever way of training a complete
   system, not just a recognizer. it was later formalized under the name
   ***convolutional neural networks (id98s)***.

   3.png

   convolutional neural networks are quite different from most other
   networks. they are primarily used for image processing, but can also be
   used for other types of input, such as as audio. a typical use case for
   id98s is where you feed the network images and it classifies the data.
   id98s tend to start with an input    scanner,    which is not intended to
   parse all of the training data at once. for example, to input an image
   of 100 x 100 pixels, you wouldn   t want a layer with 10,000 nodes.
   rather, you create a scanning input layer of say, 10 x 10, and you feed
   the first 10 x 10 pixels of the image. once you   ve passed that input,
   you feed it the next 10 x 10 pixels by moving the scanner one pixel to
   the right.

   4.png

   this input data is then fed through convolutional layers instead of
   normal layers, where not all nodes are connected. each node only
   concerns itself with close neighboring cells. these convolutional
   layers also tend to shrink as they become deeper, mostly by easily
   divisible factors of the input. beside these convolutional layers, they
   also often feature pooling layers. pooling is a way to filter out
   details: a commonly found pooling technique is max pooling, where we
   take, say, 2 x 2 pixels and pass on the pixel with the most amount of
   red. if you want to dig deeper into id98s, read yann lecun   s original
   paper,    [120]gradient-based learning applied to document recognition   
   (1998) ^[121][2].

   3 - recurrent neural networks
   to understand id56s, we need to have a brief overview of sequence
   modeling. when applying machine learning to sequences, we often want to
   turn an input sequence into an output sequence that lives in a
   different domain. for example, turn a sequence of sound pressures into
   a sequence of word identities. when there is no separate target
   sequence, we can get a teaching signal by trying to predict the next
   term in the input sequence. the target output sequence is the input
   sequence with an advance of one step. this seems much more natural than
   trying to predict one pixel in an image from the other pixels, or one
   patch of an image from the rest of the image. predicting the next term
   in a sequence blurs the distinction between supervised and unsupervised
   learning. it uses methods designed for supervised learning but doesn   t
   require a separate teaching signal.

   10.1.png

   memoryless models are the standard approach to this task. in
   particular, autoregressive models can predict the next term in a
   sequence from a fixed number of previous terms using    delay taps.   
   feed-forward neural nets are generalized autoregressive models that use
   one or more layers of non-linear hidden units. however, if we give our
   generative model some hidden state, and if we give this hidden state
   its own internal dynamics, we get a much more interesting kind of model
   that can store information in its hidden state for a long time. if the
   dynamics and the way it generates outputs from its hidden state are
   noisy, we will never know its exact hidden state. the best we can do is
   infer a id203 distribution over the space of hidden state
   vectors. this id136 is only tractable for two types of hidden state
   models.

   originally introduced in jeffrey elman's    [122]finding structure in
   time    (1990) ^[123][3], recurrent neural networks (id56s) are basically
   id88s. however, unlike id88s, which are stateless, they
   have connections between passes, connections through time. id56s are
   very powerful, because they combine two properties: 1) a distributed
   hidden state that allows them to store a lot of information about the
   past efficiently and 2) non-linear dynamics that allow them to update
   their hidden state in complicated ways. with enough neurons and time,
   id56s can compute anything that your computer can compute. so what kinds
   of behavior can id56s exhibit? they can oscillate, settle to point
   attractors, and behave chaotically. they can potentially learn to
   implement lots of small programs that each capture a nugget of
   knowledge and run in parallel, interacting to produce very complicated
   effects.

   10.jpg

   one big problem with id56s is the vanishing (or exploding) gradient
   problem, where, depending on the id180 used, information
   rapidly gets lost over time. intuitively, this wouldn   t be much of a
   problem because these are just weights and not neuron states, but the
   weights through time is actually where the information from the past is
   stored. if the weight reaches a value of 0 or 1,000,000, the previous
   state won   t be very informative. id56s can, in principle, be used in
   many fields, as most forms of data that don   t actually have a timeline
   (non- audio or video) can be represented as a sequence. a picture or a
   string of text can be fed one pixel or character at a time, so time
   dependent weights are used for what came before in the sequence, not
   actually what happened x seconds before. in general, recurrent networks
   are a good choice for advancing or completing information, like
   autocompletion.

   4 - long / short term memory
   [124]hochreiter & schmidhuber (1997) ^[125][4] solved the problem of
   getting a id56 to remember things for a long time by building what is
   known as ***long-short term memory networks (lstms)***. lstms try to
   combat the vanishing/exploding gradient problem by introducing gates
   and an explicitly defined memory cell. the memory cell stores the
   previous values and holds onto it unless a "forget gate" tells the cell
   to forget those values. lstms also have an "input gate" that adds new
   stuff to the cell and an "output gate" that decides when to pass along
   the vectors from the cell to the next hidden state.

   9.png

   recall that with all id56s, the values coming in from x_train and
   h_previous are used to determine what happens in the current hidden
   state. the results of the current hidden state (h_current) are used to
   determine what happens in the next hidden state. lstms simply add a
   cell layer to make sure the transfer of hidden state information from
   one iteration to the next is reasonably high. put another way, we want
   to remember stuff from previous iterations for as long as needed, and
   the cells in lstms allow this to happen. lstms are able to learn
   complex sequences, such as hemingway   s writing or mozart   s music.

   5 - gated recurrent unit
   id149 (grus) are a slight variation on lstms. they take
   x_train and h_previous as inputs. they perform some calculations and
   then pass along h_current. in the next iteration, x_train.next and
   h_current are used for more calculations, and so on. what makes them
   different from lstms is that grus don't need the cell layer to pass
   values along. the calculations within each iteration ensure that the
   h_current values being passed along either retain a high amount of old
   information or are jump-started with a high amount of new information.

   8.png

   in most cases, grus function very similarly to lstms, with the biggest
   difference being that grus are slightly faster and easier to run (but
   also slightly less expressive). in practice, these tend to cancel each
   other out, as you need a bigger network to regain some expressiveness,
   which then in turn cancels out the performance benefits. in some cases
   where the extra expressiveness is not needed, grus can outperform
   lstms. you can read more about grus in junyoung chung   s 2014
      [126]empirical evaluation of gated recurrent neural networks on
   sequence modeling    ^[127][5].

   6 - hopfield network
   recurrent networks of non-linear units are generally very hard to
   analyze. they can behave in many different ways: settle to a stable
   state, oscillate, or follow chaotic trajectories that cannot be
   predicted far into the future. to resolve this problem, john hopfield
   introduced the hopfield net in his 1982 work    [128]neural networks and
   physical systems with emergent collective computational abilities   
   ^[129][6]. a hopfield network (hn) is a network where every neuron is
   connected to every other neuron. it is a completely entangled plate of
   spaghetti as even all the nodes function as everything. each node is
   inputted before training, then hidden during training and output
   afterwards. the networks are trained by setting the value of the
   neurons to the desired pattern, after which the weights can be
   computed. the weights do not change after this. once trained for one or
   more patterns, the network will always converge to one of the learned
   patterns because the network is only stable in those states.

   7.png

   there is another computational role for hopfield nets. instead of using
   the net to store memories, we use it to construct interpretations of
   sensory input. the input is represented by the visible units, the
   states of the hidden units, and the badness of the interpretation is
   represented by the energy.

   unfortunately, people have shown that a hopfield net is very limited in
   its capacity. a hopfield net of n units can only memorize 0.15n
   patterns because of the so-called spurious minima in its energy
   function. the idea is that since the energy function is continuous in
   the space of its weights, if two local minima are too close, they might
      fall    into each other to create a single local minima that doesn   t
   correspond to any training sample, while forgetting about the two
   samples it is supposed to memorize. this phenomenon significantly
   limits the number of samples that a hopfield net can learn.

   7 - id82
   a id82 is a type of stochastic recurrent neural network.
   it can be seen as the stochastic, generative counterpart of hopfield
   nets. it was one of the first neural networks capable of learning
   internal representations and able to represent and solve difficult
   combinatoric problems. first introduced by geoffrey hinton and terrence
   sejnowski in    [130]learning and relearning in id82s   
   (1986) ^[131][7], id82s are a lot like hopfield networks,
   but some neurons are marked as input neurons and others remain
      hidden.    the input neurons become output neurons at the end of a full
   network update. it starts with random weights and learns through
   back-propagation. compared to a hopfield net, the neurons mostly have
   binary activation patterns.

   the goal of learning for a id82 learning algorithm is to
   maximize the product of the probabilities that the id82
   assigns to the binary vectors in the training set. this is equivalent
   to maximizing the sum of the log probabilities that the boltzmann
   machine assigns to the training vectors. it is also equivalent to
   maximizing the id203 that we would obtain exactly the n training
   cases if we did the following: 1) let the network settle to its
   stationary distribution n different time with no external input and 2)
   sample the visible vector once each time.

   6.png

   an efficient mini-batch learning procedure was proposed for boltzmann
   machines by [132]salakhutdinov and hinton in 2012 ^[133][8].
     * for the positive phase, first initialize the hidden probabilities
       at 0.5, clamp a data vector on the visible units, then update all
       of the hidden units in parallel until convergence using mean field
       updates. after the net has converged, record pipj for every
       connected pair of units and average this over all data in the
       mini-batch.
     * for the negative phase: first keep a set of    fantasy particles.   
       each particle has a value that is a global configuration.
       sequentially update all of the units in each fantasy particle a few
       times. for every connected pair of units, average sisj over all of
       the fantasy particles.

   in a general id82, the stochastic updates of units need to
   be sequential. there is a special architecture that allows alternating
   parallel updates that are much more efficient (no connections within a
   layer, no skip-layer connections). this mini-batch procedure makes the
   updates of the id82 more parallel. this is called a deep
   id82 (dbm), a general id82 with a lot of
   missing connections.

   8 - id50
   back-propagation is considered the standard method in artificial neural
   networks for calculating the error contribution of each neuron after a
   batch of data is processed. however, there are some major problems
   using back-propagation. first, it requires labeled training data while
   almost all data is unlabeled. second, the learning time does not scale
   well, which means it is very slow in networks with multiple hidden
   layers. third, it can get stuck in poor local optima, so for deep nets,
   they are far from optimal.

   to overcome the limitations of back-propagation, researchers have
   considered using unsupervised learning approaches. this helps keep the
   efficiency and simplicity of using a gradient method for adjusting the
   weights, while also using to model the structure of the sensory input.
   in particular, they adjust the weights to maximize the id203 that
   a generative model would have generated the sensory input. the question
   is what kind of generative model should we learn? can it be an
   energy-based model like a id82? or a causal model made of
   idealized neurons? or a hybrid of the two?

   5.png

   yoshua bengio came up with id50 (   [134]greedy
   layer-wise training of deep networks   ) ^[135][9], which have been shown
   to be effectively trainable stack by stack. this technique is also
   known as greedy training, where greedy means making locally optimal
   solutions to get to a decent but possibly not optimal answer. a belief
   net is a directed acyclic graph composed of stochastic variables. using
   belief net, we get to observe some of the variables, and we would like
   to solve two problems: 1) the id136 problem: infer the states of
   the unobserved variables, and 2) the learning problem: adjust the
   interactions among variables to make the network more likely to
   generate the training data.

   id50 can be trained through contrastive divergence or
   back-propagation and learn to represent the data as a probabilistic
   model. once trained or converged to a stable state through unsupervised
   learning, the model can be used to generate new data. if trained with
   contrastive divergence, it can even classify existing data because the
   neurons have been taught to look for different features.

   9 - autoencoders
   autoencoders are neural networks designed for unsupervised learning,
   i.e. when the data unlabeled. as data-compression models, they can be
   used to encode a given input into a representation of smaller
   dimension. a decoder can then be used to reconstruct the input back
   from the encoded version.

   the work they do is very similar to principal component analysis, which
   is generally used to represent a given input using fewer numbers of
   dimensions than originally present. so, for example, in nlp, if you
   represent a word as a vector of 100 numbers, you could use pca to
   represent it in 10 numbers. of course, that would result in loss of
   some information, but it is a good way to represent your input if you
   can only work with a limited number of dimensions. also, it is a good
   way to visualize the data because you can easily plot the reduced
   dimensions on a 2d graph, as opposed to a 100-dimensional vector.
   autoencoders do similar work     the difference is that they can use
   non-linear transformations to encode the given vector into smaller
   dimensions (compared to pca, which is a linear transformation), so it
   can generate more complex encodings.

   11.png

   they can be used for dimension reduction, pre-training of other neural
   networks, data generation, etc. there are a couple of reasons: (1) they
   provide flexible mappings both ways, (2) the learning time is linear
   (or better) in the number of training cases, and (3) the final encoding
   model is fairly compact and fast. however, it turns out that it   s very
   difficult to optimize deep auto -encoders using back-propagation. with
   small initial weights, the back-propagated gradient dies. nowadays,
   they are rarely used in practical applications, mostly because in the
   key areas where they were once considered breakthroughs (such as
   layer-wise pre-training), vanilla supervised learning works better.
   check out [136]the original paper by bourlard and kamp, dated 1988
   ^[137][10].

   10 - generative adversarial network
   in    [138]generative adversarial nets    (2014) ^[139][11], ian goodfellow
   introduced a new breed of neural network, in which two networks work
   together. id3 (gans) consist of any two
   networks (although often a combination of feed forwards and
   convolutional neural nets), with one tasked to generate content
   (generative) and the other to judge content (discriminative). the
   discriminative model has the task of determining whether a given image
   looks natural (an image from the dataset) or artificially created. the
   generator   s task is to create natural looking images that are similar
   to the original data distribution. this can be thought of as a zero-sum
   or minimax two player game. the analogy used in the paper is that the
   generative model is like    a team of counterfeiters, trying to produce
   and use fake currency    while the discriminative model is like    the
   police, trying to detect the counterfeit currency.    the generator is
   trying to fool the discriminator while the discriminator is trying to
   not get fooled by the generator. as the models train through
   alternating optimization, both methods are improved until the point
   where the    counterfeits are indistinguishable from the genuine
   articles.   

   12.png

   [140]according to yann lecun, these networks could be the next big
   development. they are one of the few successful techniques in
   unsupervised machine learning, and are quickly revolutionizing our
   ability to perform generative tasks. over the last few years, we   ve
   come across some very impressive results. there is a lot of active
   research in the field to apply gans for language tasks, improve their
   stability and ease of training, and so on. they are already being
   applied in industry for a variety of applications, ranging from:
   interactive image editing, 3d shape estimation, drug discovery, and
   semi-supervised learning, to robotics.

conclusion

   neural networks are one of the most beautiful programming paradigms
   ever invented. in the conventional approach to programming, we tell the
   computer what to do and break big problems up into many small,
   precisely defined tasks that the computer can easily perform. in
   contrast, we don   t tell the computer how to solve our problems for a
   neural network. instead, it learns from observational data and figures
   out its own solution to the problem.

   today, deep neural networks and deep learning achieve outstanding
   performance for many important problems in id161, speech
   recognition, and natural language processing. they   re being deployed on
   a large scale by companies such as google, microsoft, and facebook. i
   hope that this post helps you learn the core concepts of neural
   networks, including modern techniques for deep learning.

   additional readings
     * [141]a visual and interactive guide to the basics of neural
       networks
     * [142]yes you should understand backprop
     * [143]convnets: a modular perspective
     * [144]understanding convolutions
     * [145]the unreasonable effectiveness of recurrent neural networks
     * [146]understanding id137
     * [147]generative models

   paper references
     __________________________________________________________________

    1. rosenblatt, frank.    the id88: a probabilistic model for
       information storage and organization in the brain.    psychological
       review 65.6 (1958): 386. [148]      
    2. lecun, yann, et al.    gradient-based learning applied to document
       recognition.    proceedings of the ieee 86.11 (1998): 2278-2324.
       [149]      
    3. elman, jeffrey l.    finding structure in time.    cognitive science
       14.2 (1990): 179-211. [150]      
    4. hochreiter, sepp, and j  rgen schmidhuber.    long short-term memory.   
       neural computation 9.8 (1997): 1735-1780. [151]      
    5. chung, junyoung, et al.    empirical evaluation of gated recurrent
       neural networks on sequence modeling.    arxiv preprint
       arxiv:1412.3555 (2014). [152]      
    6. hopfield, john j.    neural networks and physical systems with
       emergent collective computational abilities.    proceedings of the
       national academy of sciences 79.8 (1982): 2554-2558. [153]      
    7. hinton, geoffrey e., and terrence j. sejnowski.    learning and
       releaming in id82s.    parallel distributed processing:
       explorations in the microstructure of cognition 1 (1986): 282-317.
       [154]      
    8. salakhutdinov, rusland r., and hinton, geoffrey e..    deep boltzmann
       machines.    proceedings of the 20th international conference on ai
       and statistics, vol.5, pp. 448-455, clearwater beach, florida, usa,
       16-18 apr 2009. pmlr. [155]      
    9. bengio, yoshua, et al.    greedy layer-wise training of deep
       networks.    advances in neural information processing systems 19
       (2007): 153. [156]      
   10. bourlard, herv  , and yves kamp.    auto-association by multilayer
       id88s and singular value decomposition.    biological
       cybernetics 59.4-5 (1988): 291-294. [157]      
   11. goodfellow, ian, et al.    generative adversarial nets.    advances in
       neural information processing systems. 2014. [158]      

   [159]neural networks[160]deep learning[161]machine learning
   report

   enjoy this post? give james le a like if it's helpful.
   42
   [162][icon-comment.png] 2
   share
   [163]james le
   tech and product enthusiast
   ai practitoner. podcast addict. coffee fanatic. book sidekick. amateur
   writer. seasoned traveler. continuous learner. all things
   entrepreneurship.
   follow
   discover and read more posts from james le
   get started
   enjoy this post?

   leave a like and comment for james
   42
   [164][icon-comment.png] 2
   [icon-comment.png] 2replies

   ____________________________________________________________
   ____________________________________________________________
   [markdown-icon.png] [165]github flavored markdown supported
   submit
   [166]alex sherstinsky
   8 months ago

   hi! i enjoyed your comprehensive overview. i also wanted to share with
   you that my new paper demystifying id56/lstm is now available at
   [167]https://arxiv.org/abs/1808.03314     perhaps it might be of interest
   to you. the paper explains id56/lstm using an alternative pedagogical
   approach and also proposes two novel extensions to the vanilla lstm
   network. in addition, my paper dispels certain myths (e.g., the truth
   is that lstm network does not address exploding gradients, only
   vanishing gradients). of course comments are most welcome, please feel
   to share as you deem appropriate, and i am happy to connected on
   linkedin and/or discuss. thank you, and best regards. --alex
   sherstinsky
   reply
   [168]rob duwors
   10 months ago

   great article. would be great to see one on the integration of symbolic
   processing models like mathematical models in a given domain with
   neural models. for example interpretation of oil well logs using
   standard computational functions and neural models exploiting knowledge
   of regional geology.
   reply

references

   visible links
   1. https://plus.google.com/+codementor
   2. https://www.codementor.io/community
   3. https://www.codementor.io/experts
   4. https://www.codementor.io/experts/web-programming
   5. https://www.codementor.io/javascript-experts
   6. https://www.codementor.io/python-experts
   7. https://www.codementor.io/php-experts
   8. https://www.codementor.io/reactjs-experts
   9. https://www.codementor.io/vue-experts
  10. https://www.codementor.io/angularjs-experts
  11. https://www.codementor.io/nodejs-experts
  12. https://www.codementor.io/jquery-experts
  13. https://www.codementor.io/ruby-on-rails-experts
  14. https://www.codementor.io/django-experts
  15. https://www.codementor.io/laravel-experts
  16. https://www.codementor.io/html-experts
  17. https://www.codementor.io/css-experts
  18. https://www.codementor.io/experts/mobile-app-programming
  19. https://www.codementor.io/android-experts
  20. https://www.codementor.io/ios-experts
  21. https://www.codementor.io/swift-experts
  22. https://www.codementor.io/kotlin-experts
  23. https://www.codementor.io/xcode-experts
  24. https://www.codementor.io/ionic-experts
  25. https://www.codementor.io/reactnative-experts
  26. https://www.codementor.io/experts/coding
  27. https://www.codementor.io/javascript-experts
  28. https://www.codementor.io/python-experts
  29. https://www.codementor.io/java-experts
  30. https://www.codementor.io/ruby-experts
  31. https://www.codementor.io/go-experts
  32. https://www.codementor.io/php-experts
  33. https://www.codementor.io/c_plus_plus-experts
  34. https://www.codementor.io/c_sharp-experts
  35. https://www.codementor.io/c-experts
  36. https://www.codementor.io/ai-experts
  37. https://www.codementor.io/machine-learning-experts
  38. https://www.codementor.io/tensorflow-experts
  39. https://www.codementor.io/artificial-intelligence-experts
  40. https://www.codementor.io/python-experts
  41. https://www.codementor.io/java-experts
  42. https://www.codementor.io/r-experts
  43. https://www.codementor.io/matlab-experts
  44. https://www.codementor.io/experts/devops
  45. https://www.codementor.io/database-experts
  46. https://www.codementor.io/mysql-experts
  47. https://www.codementor.io/sql-experts
  48. https://www.codementor.io/amazon-web-services-experts
  49. https://www.codementor.io/linux-experts
  50. https://www.codementor.io/mongodb-experts
  51. https://www.codementor.io/heroku-experts
  52. https://www.codementor.io/docker-experts
  53. https://www.codementor.io/postgresql-experts
  54. https://www.codementor.io/nginx-experts
  55. https://www.codementor.io/experts/development-process
  56. https://www.codementor.io/electron-experts
  57. https://www.codementor.io/unity3d-experts
  58. https://www.codementor.io/git-experts
  59. https://www.codementor.io/stripe-experts
  60. https://www.codementor.io/wordpress-experts
  61. https://www.codementor.io/developers
  62. https://www.codementor.io/freelance-developers/front-end
  63. https://www.codementor.io/freelance-developers/javascript
  64. https://www.codementor.io/freelance-developers/reactjs
  65. https://www.codementor.io/freelance-developers/angularjs
  66. https://www.codementor.io/freelance-developers/vue
  67. https://www.codementor.io/freelance-developers/web
  68. https://www.codementor.io/freelance-developers/html_css
  69. https://www.codementor.io/freelance-developers/back-end
  70. https://www.codementor.io/freelance-developers/ruby-on-rails
  71. https://www.codementor.io/freelance-developers/python
  72. https://www.codementor.io/freelance-developers/php
  73. https://www.codementor.io/freelance-developers/elixir
  74. https://www.codementor.io/freelance-developers/nodejs
  75. https://www.codementor.io/freelance-developers/django
  76. https://www.codementor.io/freelance-developers/laravel
  77. https://www.codementor.io/freelance-developers/erlang
  78. https://www.codementor.io/freelance-developers/go
  79. https://www.codementor.io/freelance-developers/lambda
  80. https://www.codementor.io/freelance-developers/app
  81. https://www.codementor.io/freelance-developers/swift
  82. https://www.codementor.io/freelance-developers/ios
  83. https://www.codementor.io/freelance-developers/android
  84. https://www.codementor.io/freelance-developers/reactnative
  85. https://www.codementor.io/freelance-developers/ionic
  86. https://www.codementor.io/freelance-developers/database
  87. https://www.codementor.io/freelance-developers/mysql
  88. https://www.codementor.io/freelance-developers/postgresql
  89. https://www.codementor.io/freelance-developers/mongodb
  90. https://www.codementor.io/freelance-developers/sql
  91. https://www.codementor.io/freelance-developers/devops
  92. https://www.codementor.io/freelance-developers/amazon-web-services
  93. https://www.codementor.io/freelance-developers/docker
  94. https://www.codementor.io/freelance-developers/heroku
  95. https://www.codementor.io/freelance-developers/linux
  96. https://www.codementor.io/freelance-developers/git
  97. https://www.codementor.io/freelance-developers/wordpress
  98. https://www.codementor.io/freelance-developers/stripe
  99. https://www.codementor.io/freelance-developers/web
 100. https://www.codementor.io/freelance-developers/full-stack
 101. https://www.codementor.io/freelance-developers/software
 102. https://www.codementor.io/community/topic/android
 103. https://www.codementor.io/community/topic/angularjs
 104. https://www.codementor.io/community/topic/ios
 105. https://www.codementor.io/community/topic/javascript
 106. https://www.codementor.io/community/topic/nodejs
 107. https://www.codementor.io/community/topic/python
 108. https://www.codementor.io/community/topic/reactjs
 109. https://www.codementor.io/community/topic/blockchain
 110. https://www.codementor.io/community/topic/ethereum
 111. https://www.codementor.io/login
 112. https://www.codementor.io/experts
 113. https://www.codementor.io/developers
 114. https://www.codementor.io/community
 115. https://www.codementor.io/blog
 116. https://www.codementor.io/james_aka_yale
 117. http://www.ling.upenn.edu/courses/cogs501/rosenblatt1958.pdf
 118. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn1
 119. http://yann.lecun.com/exdb/lenet/
 120. http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf
 121. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn2
 122. https://crl.ucsd.edu/~elman/papers/fsit.pdf
 123. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn3
 124. http://www.bioinf.jku.at/publications/older/2604.pdf
 125. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn4
 126. https://arxiv.org/pdf/1412.3555v1.pdf
 127. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn5
 128. https://bi.snu.ac.kr/courses/g-ai09-2/hopfield82.pdf
 129. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn6
 130. https://papers.cnl.salk.edu/pdfs/learning and relearning in id82s 1986-3239.pdf
 131. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn7
 132. http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf
 133. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn8
 134. https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf
 135. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn9
 136. https://www.semanticscholar.org/paper/auto-association-by-multilayer-id88s-and-sin-bourlard-kamp/f5821548720901c89b3b7481f7500d7cd64e99bd
 137. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn10
 138. https://arxiv.org/pdf/1406.2661v1.pdf
 139. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fn11
 140. https://www.quora.com/what-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning
 141. http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
 142. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
 143. http://colah.github.io/posts/2014-07-conv-nets-modular/
 144. http://colah.github.io/posts/2014-07-understanding-convolutions/
 145. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 146. http://colah.github.io/posts/2015-08-understanding-lstms/
 147. https://blog.openai.com/generative-models/
 148. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref1
 149. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref2
 150. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref3
 151. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref4
 152. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref5
 153. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref6
 154. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref7
 155. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref8
 156. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref9
 157. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref10
 158. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#fnref11
 159. https://www.codementor.io/community/topic/neural-networks
 160. https://www.codementor.io/community/topic/deep-learning
 161. https://www.codementor.io/community/topic/machine-learning
 162. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#comments-hkijvz7lp
 163. https://www.codementor.io/james_aka_yale
 164. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#comments-hkijvz7lp
 165. https://guides.github.com/features/mastering-markdown/
 166. https://www.codementor.io/alexsherstinsky
 167. https://arxiv.org/abs/1808.03314
 168. https://www.codementor.io/rjd

   hidden links:
 170. https://www.codementor.io/blog
 171. https://www.codementor.io/community/trending
 172. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#why-do-we-need-machine-learning
 173. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp
 174. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#top-10-neural-network-architectures-you-need-to-know
 175. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#conclusion
 176. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#comment-mj5csp6mt
 177. https://www.codementor.io/james_aka_yale/a-gentle-introduction-to-neural-networks-for-machine-learning-hkijvz7lp#comment-khxbz034v
