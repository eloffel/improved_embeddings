   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

components of convolutional neural networks

   [16]go to the profile of john olafenwa
   [17]john olafenwa (button) blockedunblock (button) followfollowing
   feb 25, 2018
   [1*tv_6-fk-g3wds2drybrgpw.jpeg]

   recent state-of-the-art architectures have employed a number of
   additional components to complement the convolution operation. in this
   post, i would be explaining some of the most important components that
   have improved both the speed and accuracy of modern convolutional
   neural networks. i would begin by explaining the theory of each of the
   components and finalize with a practical implementation in keras.

pooling

   the first secret sauce that has made id98s very effective is pooling.
   pooling is a vector to scalar transformation that operates on each
   local region of an image, just like convolutions do, however, unlike
   convolutions, they do not have filters and do not compute dot products
   with the local region, instead they compute the average of the pixels
   in the region (average pooling) or simply picks the pixel with the
   highest intensity and discards the rest (max pooling).

   the above is a 2 x 2 pooling, it will effectively reduce the size of
   feature maps by a factor of 2.

   the very idea of pooling can seem counter-productive as it leads to
   loss of information, however it has proven to be very effective in
   practice because, it makes covnets invariant to variations in the
   presentation of an image and it also reduces the effects of background
   noise. max pooling has worked best in recent years, it is based on the
   idea that the maximum pixel in a region represents the most important
   feature in that region. often images of objects we wish to classify
   could contain a number of other objects, for example, a cat appearing
   somewhere in the picture of a car could mislead the classifier. pooling
   helps to alleviate the effects of this, and makes covnets generalize
   better.

   it also greatly reduces the computational cost of the covnet.
   generally, the size of images at each layer in the network is directly
   proportional to the computational cost (flops) of each layer. pooling
   reduces the dimensions of the image as the layers get deeper, hence, it
   helps prevent an explosion of the number of flops a network requires.
   strided convolutions are sometimes used as an alternative to pooling.

dropouts

   overfitting is a phenomenon whereby a network works well on the
   training set, but performs poorly on the test set. this is often due to
   excessive dependence on the presence of specific features in the
   training set. dropout is a technique for combating over-fitting. it
   works by randomly setting some activations to 0, essentially killing
   them. by doing this, the network is forced to explore more ways of
   classifying the images instead of over-depending on some features. this
   was one of the key elements in the alexnet.

batch id172

   a major problem with neural networks is vanishing gradients. this is a
   situation whereby the gradients become too small, hence, training
   surfers terribly. ioffe and szegedy from google brain discovered that
   this was largely due to internal covariate shift, a situation that
   arises from the change data distribution as information flows through
   the network. what they did was to device the technique known as batch
   id172. this works by normalizing every batch of the image to
   have zero mean and unit variance.

   it is usually placed before non-linearity(relu) in id98s. it greatly
   improves accuracy while incredibly speeding up the training process.

data augmentation

   the last ingredient required or modern covnets is data augmentation.
   the human vision system is excellent at adapting to image translations,
   rotations and other forms of distortions. take an image and flip it
   anyway, most people can still recognize it. however, covnets are not
   very good at handling such distortions, they could fail terribly due to
   minor translations. they key to resolving this is to randomly distort
   the training images, using horizontal flipping, vertical flipping,
   rotation, whitening, shifting and other distortions. this would enable
   covnets to learn how to handle this distortions, hence, they would be
   able to work well in the real world.

   another common technique is to subtract the mean image from every image
   and also divide by the standard deviation.

   having bored you with the explanations of what these components are and
   why they work well, i shall now explain how to implement them in keras.

   in this post, all experiments would be on cifar10, a data set of 60,000
   32 x 32 rgb images. it is divided into 50,000 training images images
   and 10, 000 test images

   to make things more modular, let   s create a simple function for each
   layer
def unit(x,filters):
    out = batchid172()(x)
    out = activation("relu")(out)
    out = conv2d(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding="s
ame")(out)
    return out

   here is the most vital aspect of our code, the unit function defines a
   simple layer that contains three layers, first is batchid172
   which i earlier explained, next we add relu activation and finally, we
   add the convolution, notice how i put relu before conv, it is a recent
   practice called    pre-activation   

   now we shall combine many of this unit layer into a single model
def minimodel(input_shape):
    images = input(input_shape)
    net = unit(images,64)
    net = unit(net,64)
    net = unit(net,64)
    net = maxpooling2d(pool_size=(2,2))(net)
    net = unit(net,128)
    net = unit(net,128)
    net = unit(net,128)
    net = maxpooling2d(pool_size=(2, 2))(net)
    net = unit(net,256)
    net = unit(net,256)
    net = unit(net,256)
    net = dropout(0.5)(net)
    net = averagepooling2d(pool_size=(8,8))(net)
    net = flatten()(net)
    net = dense(units=10,activation="softmax")(net)
    model = model(inputs=images,outputs=net)
    return model

   here we use the functional api to define our model, we begin with three
   unit cells with 64 filters each, a max pooling layer follows, reducing
   our 32 x 32 images to 16 by 16. next is 3, 128 filters units followed
   by pooling, at this point, our images become 8 x 8, finally, we have
   another 3 units with 256 channels. notice that each time we reduce our
   image dimensions by a factor of 2, we double the number of channels.

   we add dropout with a ratio of 0.5, this would randomly deactivate 50%
   of our parameters, as i earlier explained, it combats overfitting.

   next we need to load the cifar10 dataset and perform some data
   augmentation
#load the cifar10 dataset
(train_x, train_y) , (test_x, test_y) = cifar10.load_data()
#normalize the data
train_x = train_x.astype('float32') / 255
test_x = test_x.astype('float32') / 255
#subtract the mean image from both train and test set
train_x = train_x - train_x.mean()
test_x = test_x - test_x.mean()
#divide by the standard deviation
train_x = train_x / train_x.std(axis=0)
test_x = test_x / test_x.std(axis=0)

   in the code above, after loading the train and test data, we subtract
   the mean image from each image and divide by the standard deviation,
   this is a basic data augmentation technique, sometimes, we might
   subtract the mean only and skip the standard deviation part, whichever
   works best should be used.

   for more advance data augmentation, our image loading process would
   slighly change, keras has a very useful data augmentation utility that
   simplifies the whole process.

   the code below would do the trick
datagen = imagedatagenerator(rotation_range=10,
                             width_shift_range=5. / 32,
                             height_shift_range=5. / 32,
                             horizontal_flip=true)
# compute quantities required for featurewise id172
# (std, mean, and principal components if zca whitening is applied).
datagen.fit(train_x)

   in the above, first, we specify a rotation angle of 10 degrees, a shift
   of 5/32 for both height and width and finally horizontal flip, all
   these transformations would be randomly applied to the images in the
   training set. note that many more transformations exist, you can take a
   look at all parameters you can specify for the class. bear in mind that
   over use of data augmentation might be detrimental.

   next, we have to convert the labels to one-hot encoding
#encode the labels to vectors
train_y = keras.utils.to_categorical(train_y,10)
test_y = keras.utils.to_categorical(test_y,10)

   i already explained this in my previous tutorials, so i won   t be
   explaining them here again. infact, almost everything else that makes
   up the training process is exactly as in my previous tutorials, hence,
   here is the full code
#import needed classes
import keras
from keras.datasets import cifar10
from keras.layers import dense,conv2d,maxpooling2d,flatten,averagepooling2d,drop
out,batchid172,activation
from keras.models import model,input
from keras.optimizers import adam
from keras.callbacks import learningratescheduler
from keras.callbacks import modelcheckpoint
from math import ceil
import os
from keras.preprocessing.image import imagedatagenerator
def unit(x,filters):
    out = batchid172()(x)
    out = activation("relu")(out)
    out = conv2d(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding="s
ame")(out)
    return out
#define the model
def minimodel(input_shape):
    images = input(input_shape)
    net = unit(images,64)
    net = unit(net,64)
    net = unit(net,64)
    net = maxpooling2d(pool_size=(2,2))(net)
    net = unit(net,128)
    net = unit(net,128)
    net = unit(net,128)
    net = maxpooling2d(pool_size=(2, 2))(net)
    net = unit(net,256)
    net = unit(net,256)
    net = unit(net,256)
    net = dropout(0.25)(net)
    net = averagepooling2d(pool_size=(8,8))(net)
    net = flatten()(net)
    net = dense(units=10,activation="softmax")(net)
    model = model(inputs=images,outputs=net)
    return model
#load the cifar10 dataset
(train_x, train_y) , (test_x, test_y) = cifar10.load_data()
#normalize the data
train_x = train_x.astype('float32') / 255
test_x = test_x.astype('float32') / 255
#subtract the mean image from both train and test set
train_x = train_x - train_x.mean()
test_x = test_x - test_x.mean()
#divide by the standard deviation
train_x = train_x / train_x.std(axis=0)
test_x = test_x / test_x.std(axis=0)
datagen = imagedatagenerator(rotation_range=10,
                             width_shift_range=5. / 32,
                             height_shift_range=5. / 32,
                             horizontal_flip=true)
# compute quantities required for featurewise id172
# (std, mean, and principal components if zca whitening is applied).
datagen.fit(train_x)
#encode the labels to vectors
train_y = keras.utils.to_categorical(train_y,10)
test_y = keras.utils.to_categorical(test_y,10)
#define a common unit
input_shape = (32,32,3)
model = minimodel(input_shape)
#print a summary of the model
model.summary()
#specify the training components
model.compile(optimizer=adam(0.001),loss="categorical_crossid178",metrics=["ac
curacy"])
epochs = 20
steps_per_epoch = ceil(50000/128)
# fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(train_x, train_y, batch_size=128),
                    validation_data=[test_x,test_y],
                    epochs=epochs,steps_per_epoch=steps_per_epoch, verbose=1, wo
rkers=4)
#evaluate the accuracy of the test dataset
accuracy = model.evaluate(x=test_x,y=test_y,batch_size=128)
model.save("cifar10model.h5")

   a few things are different here, first
input_shape = (32,32,3)
model = minimodel(input_shape)
#print a summary of the model
model.summary()

   as i earlier explained, cifar 10 is made up of 32 x 32 rgb images,
   hence, the input shape has 3 channels. this is quite self-explanatory.

   the next line creates an instance of the model we defined and we pass
   in the input shape

   finally, the last line would print out a full summary of our network
   including the number of parameters.

   the last part that needs explaining is
epochs = 20
steps_per_epoch = ceil(50000/128)
# fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(train_x, train_y, batch_size=128),
                    validation_data=[test_x,test_y],
                    epochs=epochs,steps_per_epoch=steps_per_epoch, verbose=1, wo
rkers=4)
#evaluate the accuracy of the test dataset
accuracy = model.evaluate(x=test_x,y=test_y,batch_size=128)
model.save("cifar10model.h5")

   first we define the number of epochs to run and also, the number of
   steps per epoch, don   t get confused with the numbers
steps_per_epoch = ceil(50000/128)

   50000 here is the number of training images in total, here we are using
   a batch size of 128, that means, for each of the 20 epochs, the network
   would have to go over 50000/128 batches of images.

   next is the fit function, this is clearly different from the fit
   function i explained in my earlier tutorials.

   a second look below would help
fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(train_x, train_y, batch_size=128),
                    validation_data=[test_x,test_y],
                    epochs=epochs,steps_per_epoch=steps_per_epoch, verbose=1, wo
rkers=4)

   due to our use of the data generator class for data augmentation
   purposes, we have to use the fit_generator function, also we do not
   pass in train_x and train_y directly, instead we pass them via the flow
   function from the data generator, we also specify the batch size, next
   we state the validation data which in this case is the test data. all
   other things remain same.

   this setup yields 82% after 20 epochs.

   you can try tweaking the parameters and the network to see how much you
   can improve the accuracy. in the next tutorial i would be explaining
   some other tricks and techniques required to truly build very efficient
   id98 architectures. the purpose of this tutorial is to introduce you to
   the basic components.

   if you want to get deeper into id161. download my free ebook
      introduction to deep id161    from
   [18]https://john.specpal.science

   if you have any questions, comment below or reach to me on twitter via
   [19]@johnolafenwa

     * [20]machine learning
     * [21]id98
     * [22]deep learning
     * [23]ai
     * [24]keras

   (button)
   (button)
   (button) 126 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of john olafenwa

[26]john olafenwa

   creator of torchfusion
   ([27]https://github.com/johnolafenwa/torchfusion), co-founder and cto
   at deepquestai ([28]https://deepquestai.com), dl researcher.

     (button) follow
   [29]towards data science

[30]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 126
     * (button)
     *
     *

   [31]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [32]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6ff66296b456
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/components-of-convolutional-neural-networks-6ff66296b456&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/components-of-convolutional-neural-networks-6ff66296b456&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_g7ymzdnrx4ku---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@johnolafenwa?source=post_header_lockup
  17. https://towardsdatascience.com/@johnolafenwa
  18. https://john.specpal.science/deepvision
  19. https://twitter.com/johnolafenwa
  20. https://towardsdatascience.com/tagged/machine-learning?source=post
  21. https://towardsdatascience.com/tagged/id98?source=post
  22. https://towardsdatascience.com/tagged/deep-learning?source=post
  23. https://towardsdatascience.com/tagged/ai?source=post
  24. https://towardsdatascience.com/tagged/keras?source=post
  25. https://towardsdatascience.com/@johnolafenwa?source=footer_card
  26. https://towardsdatascience.com/@johnolafenwa
  27. https://github.com/johnolafenwa/torchfusion
  28. https://deepquestai.com/
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/6ff66296b456/share/twitter
  35. https://medium.com/p/6ff66296b456/share/facebook
  36. https://medium.com/p/6ff66296b456/share/twitter
  37. https://medium.com/p/6ff66296b456/share/facebook
