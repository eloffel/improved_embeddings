   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

node2vec: embeddings for graph data

   [16]go to the profile of elior cohen
   [17]elior cohen (button) blockedunblock (button) followfollowing
   apr 16, 2018

   hotlinks:
   original article: [18]node2vec: scalable id171 for networks,
   aditya grover and jure leskovec
   algorithm implementation         by me: [19]github repo         python3
   algorithm implementation         by the algo author: [20]github repo (by
   aditya grover)         python2
   showcase code:
   [21]https://github.com/eliorc/medium/blob/master/nod2vec-fifa17-example
   .ipynb
     __________________________________________________________________

motivation

   embeddings    a word that every data scientist has heard by now, but
   mostly in the context of nlp.
   so why do we even bother embedding stuff?
   as i see it, creating quality embeddings and feeding it into models, is
   the exact opposite of the famous say    garbage in, garbage out    .
   when you feed low quality data into your models, you put the entire
   load of learning on your model, as it will have to learn all the
   necessary conclusions that could be derived from the data.
   on the contrary, when you use quality embeddings, you already put some
   knowledge in your data and thus make the task of learning the problem
   easier for your models.
   another point to think about is information vs domain knowledge.
   for example, let   s consider id27s (id97) and bag of words
   representations.
   while both of them can have the entire information about which words
   are in a sentence, id27s also include domain knowledge like
   relationship between words and such.
   in this post, i   m going to talk about a technique called node2vec which
   aims to create embeddings for nodes in a graph (in the g(v, e, w) sense
   of the word).
   [1*ymmseis0kjfmu6npas3-gg.png]

   i will explain how it works and finally supply my own implementation
   for python 3, with some extras.

embedding process

   so how is done?
   the embedding themselves, are learnt in the same way as id97   s
   embeddings are learnt         using a skip-gram model.
   if you are familiar with the id97 skip-gram model, great, if not i
   recommend [22]this great post which explains it in great detail as from
   this point forward i assume you are familiar with it.

   the most natural way i can think about explaining node2vec is to
   explain how node2vec generates a    corpus            and if we understand
   id97 we already know how to embed a corpus.

   so how do we generate this corpus from a graph? that   s exactly the
   innovative part of node2vec and it does so in an intelligent way which
   is done using the sampling strategy.

   in order to generate our corpus from the input graph, let   s think about
   a corpus as a group of directed acyclic graphs, with a maximum out
   degree of 1. if we think about it this is a perfect representation for
   a text sentence, where each word in the sentence is a node and it
   points on the next word in the sentence.
   [1*oeujhzd3ippord7sfr1ahq.png]
   sentence in a graph representation

   in this way, we can see that id97 can already embed graphs, but a
   very specific type of them.
   most graphs though, aren   t that simple, they can be (un)directed,
   (un)weighted, (a)cyclic and are basically much more complex in
   structure than text.

   in order to solve that, node2vec uses a tweakable (by hyperparameters)
   sampling strategy, to sample these directed acyclic subgraphs. this is
   done by generating id93 from each node of the graph. quite
   simple right?

   before we delve how the sampling strategy uses the hyperparameters to
   generate these sub graphs, lets visualize the process:
   [1*3pmstioig4qc3lrqs4xrng.png]
   node2vec embedding process

sampling strategy

   by now we get the big picture and it   s time to dig deeper.
   node2vec   s sampling strategy, accepts 4 arguments:
            number of walks: number of id93 to be generated from each
   node in the graph
            walk length: how many nodes are in each random walk
            p: return hyperparameter
            q: inout hyperaprameter
   and also the standard skip-gram parameters (context window size, number
   of iterations etc.)

   the first two hyperparameters are pretty self explanatory.
   the algorithm for the random walk generation will go over each node in
   the graph and will generate <number of walks> id93, of length
   <walk length>.
   q and p, are better explained with a visualization.
   consider you are on the random walk, and have just transitioned from
   node <t> to node <v> in the following diagram (taken from the article).
   [1*44_ys2jed8b0nvdbj4tqlg.png]

   the id203 to transition from <v> to any one of his neighbors is
   <edge weight>*<  > (normalized), where <  > is depended on the
   hyperparameters.
   p controls the id203 to go back to <t> after visiting <v>.
   q controls the id203 to go explore undiscovered parts of the
   graphs.
   in a intuitive way, this is somewhat like the perplexity parameter in
   tsne, it allows you to emphasize the local/global structure of the
   graph.
   do not forget that the weight is also taken into consideration, so the
   final travel id203 is a function of:
    1. the previous node in the walk
    2. p and q
    3. edge weight

   this part is important to understand as it is the essence of node2vec.
   if you did not fully comprehend the idea behind the sampling strategy i
   strongly advise you to read this part again.

   using the sampling strategy, node2vec will generate    sentences    (the
   directed subgraphs) which are will be used for embedding just like text
   sentences are used in id97. why change something if it works right?
     __________________________________________________________________

code (showcase)

   now its time to put node2vec into action.
   you can find the entire code for this node2vec test drive [23]here.
   i am using for the example my implementation of the node2vec algorithm,
   which adds support for assigning node specific parameters (q, p,
   num_walks and walk length).

   what we are going to do, using formation of european football teams, is
   to embed the teams, players and positions of 7 different clubs.
   the data i   m going to be using is taken from the [24]fifa 17 dataset on
   kaggle.
   in fifa (by easports) each team can be represented as a graph, see
   picture below.
   [1*fzibudrfm4hjgppqfjksaw.jpeg]
   formation example from fifa17, easily interpreted as a graph

   as we can see, each position is connected to other positions and when
   playing each position is assigned a player.
   there dozens of different formations, and the connectivity between them
   differs. also there are type of positions that are in some formations
   but are non existent in others, for example the    lm    position is not
   existent in this formation but is in others.

   this is how we are going to do this:
   1. nodes will be players, team names and positions
   2. for each team, create a separate graph where each player node is
   connected to his team name node, connected to his teammates nodes and
   connected to his teammate position nodes.
   3. apply node2vec to the resulting graphs

   *notice: in order to create separate nodes for each position inside and
   between teams, i added suffixes to similar nodes and after the walk
   generation i have removed them. this is a technicality, inspect code in
   repo for better understanding

   first rows of the input data looks like this (after some permutations):
   [1*rqodeksr1u8cjrwhgnx-og.png]
   sample rows from the input data

   then we construct the graph, using the fifa17 formations.
   using my node2vec package the graph must be an instance of
   networkx.graph.
   inspecting the graph edges after this, we will get the following
for edge in graph.edges:
    print(edge)
>>> ('james_rodriguez', 'real_madrid')
>>> ('james_rodriguez', 'cm_1_real_madrid')
>>> ('james_rodriguez', 'toni_kroos')
>>> ('james_rodriguez', 'cm_2_real_madrid')
>>> ('james_rodriguez', 'luka_modric')
>>> ('lw_real_madrid', 'cm_1_real_madrid')
>>> ('lw_real_madrid', 'lb_real_madrid')
>>> ('lw_real_madrid', 'toni_kroos')
>>> ('lw_real_madrid', 'marcelo')
...

   as we can see, each player is connected to his team, the positions and
   teammates according to the formation.
   all of the suffixes attached to the positions will be returned to their
   original string after the walks are computed ( lw_real_madrid    lw).

   so now that we have the graph, we execute node2vec
# pip install node2vec
from node2vec import node2vec
# generate walks
node2vec = node2vec(graph, dimensions=20, walk_length=16, num_walks=100)
# reformat position nodes
fix_formatted_positions = lambda x: x.split('_')[0] if x in formatted_positions
else x
reformatted_walks = [list(map(fix_formatted_positions, walk)) for walk in node2v
ec.walks]
node2vec.walks = reformatted_walks
# learn embeddings
model = node2vec.fit(window=10, min_count=1)

   we give node2vec.node2vec a networkx.graph instance, and after
   using .fit() (which accepts any parameter accepted by we get a
   gensim.models.id97) we get in return a gensim.models.id97
   instance.

   first we will inspect the similarity between different nodes.
   we expect the most similar nodes to a team, would be its teammates:
for node, _ in model.most_similar('real_madrid'):
    print(node)
>>> james_rodriguez
>>> luka_modric
>>> marcelo
>>> karim_benzema
>>> cristiano_ronaldo
>>> pepe
>>> gareth_bale
>>> sergio_ramos
>>> carvajal
>>> toni_kroos

   for those who are not familiar with european football, these are all
   indeed real madrid   s players!

   next, we inspect similarities to a specific position. we would expect
   to get players playing in that position or near it at worse
# right wingers
for node, _ in model.most_similar('rw'):
    # show only players
    if len(node) > 3:
        print(node)
>>> pedro
>>> jose_callejon
>>> raheem_sterling
>>> henrikh_mkhitaryan
>>> gareth_bale
>>> dries_mertens
# goal keepers
for node, _ in model.most_similar('gk'):
    # show only players
    if len(node) > 3:
        print(node)
>>> thibaut_courtois
>>> gianluigi_buffon
>>> keylor_navas
>>> azpilicueta
>>> manuel_neuer

   in the first try (right wingers) we indeed get different right wingers
   from different clubs, again a perfect match.
   in the second try though, we get all goalkeepers except azpilicueta
   which is actually a defender         this could be due to the fact that
   goalkeepers are not very connected to the team, only to central backs
   usually.

   works pretty good right? just before we finish, lets use tsne to reduce
   dimensionality and visualize the player nodes.
   [1*o7x6p8g5_pyydooshhwdza.png]
   visualization of player nodes (tsne reduced dimensionality)

   check it out, we get beautiful clusters based on the different clubs.
     __________________________________________________________________

final words

   graph data is almost everywhere, and where its not you can usually put
   it on a graph yet the node2vec algorithm is not so popular.
   the algorithm also grants great flexibility with its hyperparameters so
   you can decide which kind of information you wish to embed, and if you
   have the the option to construct the graph yourself (and is not a
   given) your options are limitless.
   hopefully you will find use in this article and have added a new tool
   to your machine learning arsenal.

   if someone wants to contribute to my node2vec implementation, please
   contact me.

     * [25]machine learning

   (button)
   (button)
   (button) 683 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of elior cohen

[27]elior cohen

   data scientist, pythonista

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 683
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/32a866340fef
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ypcfc7ti11fk---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@eliorcohen?source=post_header_lockup
  17. https://towardsdatascience.com/@eliorcohen
  18. https://arxiv.org/pdf/1607.00653.pdf
  19. https://github.com/eliorc/node2vec
  20. https://github.com/aditya-grover/node2vec
  21. https://github.com/eliorc/medium/blob/master/nod2vec-fifa17-example.ipynb
  22. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  23. https://github.com/eliorc/medium/blob/master/nod2vec-fifa17-example.ipynb
  24. https://www.kaggle.com/artimous/complete-fifa-2017-player-dataset-global
  25. https://towardsdatascience.com/tagged/machine-learning?source=post
  26. https://towardsdatascience.com/@eliorcohen?source=footer_card
  27. https://towardsdatascience.com/@eliorcohen
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/32a866340fef/share/twitter
  34. https://medium.com/p/32a866340fef/share/facebook
  35. https://medium.com/p/32a866340fef/share/twitter
  36. https://medium.com/p/32a866340fef/share/facebook
