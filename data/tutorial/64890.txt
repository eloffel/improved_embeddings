   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

deep learning meets physics: restricted id82s part i

theory behind restricted id82s         a powerful tool for recomender
systems

   [16]go to the profile of artem oppermann
   [17]artem oppermann (button) blockedunblock (button) followfollowing
   apr 27, 2018

   this tutorial is part one of a two part series about restricted
   id82s, a powerful deep learning architecture for
   id185. in this part i introduce the theory behind
   restricted id82s. the second part consists of a step by
   step guide through a practical implementation of a model which can
   predict whether a user would like a movie or not.

     the practical part is now available [18]here.

   [1*9hzmp0nkc0okiw7ryfs3gw.png]

table of contents:

     * 0. introduction
     * 1. restricted id82s
     * 1.1 architecture
     * 1.2 an energy-based-model
     * 1.3 a probabilistic model
     * 2. id185 with restricted id82s
     * 2.1 recognizing latent factors in the data
     * 2.2 using latent factors for prediction
     * 3. training
     * 3.1 id150
     * 3.2 contrastive divergence
     __________________________________________________________________

0. introduction

   restricted id82s (rbms) are neural networks that belong to
   so called energy based models. this type of neural networks may be not
   that familiar to the reader of this article as e.g. feedforward or
   convolution neural networks. yet this kind of neural networks gained
   big popularity in recent years in the context of the [19]netflix prize
   where rbms achieved state of the art performance in collaborative
   filtering and have beaten most of the competition.
     __________________________________________________________________

1. restricted id82s

1.1 architecture

   in my opinion rbms have one of the easiest architectures of all neural
   networks. as it can be seen in fig.1. a rbm consists out of one
   input/visible layer (v1,   ,v6), one hidden layer (h1, h2) and
   corresponding biases vectors bias a and bias b. the absence of an
   output layer is apparent. but as it can be seen later an output layer
   wont be needed since the predictions are made differently as in regular
   feedforward neural networks.
   [1*hkad1ispyq2ijl9lxxkv3a.png]

1.2 an energy-based-model

   energy is a term that may not be associated with deep learning in the
   first place. rather is energy a quantitative property of physics. e.g.
   gravitational energy describes the potential energy a body with mass
   has in relation to another massive object due to gravity. yet some deep
   learning architectures use the idea of energy as a metric for
   measurement of the models quality.
   [1*0vmi7mjlpxfm7sl9fe4fqa.jpeg]
   fig. 2. gravitational energy of two body masses.

   one purpose of deep learning models is to encode dependencies between
   variables. the capturing of dependencies happen through associating of
   a scalar energy to each configuration of the variables, which serves as
   a measure of compatibility. a high energy means a bad compatibility. an
   energy based model model tries always to minimize a predefined energy
   function. the energy function for the rbms is defined as:
   [1*djggfw3ol9y420gwejxmbw.png]
   eq. 1. energy function of a restricted id82

   as it can be noticed the value of the energy function depends on the
   configurations of visible/input states, hidden states, weights and
   biases. the training of rbm consists in finding of parameters for given
   input values so that the energy reaches a minimum.

1.3 a probabilistic model

   restricted id82s are probabilistic. as opposed to
   assigning discrete values the model assigns probabilities. at each
   point in time the rbm is in a certain state. the state refers to the
   values of neurons in the visible and hidden layers v and h. the
   id203 that a certain state of v and h can be observed is given by
   the following joint distribution:
   [1*iq2tn7alaegiig4sfksgga.png]
   eq. 2. joint distribution for v and h.

   here z is called the    partition function    that is the summation over
   all possible pairs of visible and hidden vectors.

   this is the point where restricted id82s meets physics for
   the second time. the joint distribution is known in physics as the
   [20]boltzmann distribution which gives the id203 that a particle
   can be observed in the state with the energy e. as in physics we assign
   a id203 to observe a state of v and h, that depends on the
   overall energy of the model. unfortunately it is very difficult to
   calculate the joint id203 due to the huge number of possible
   combination of v and h in the partition function z. much easier is the
   calculation of the conditional probabilities of state h given the state
   v and conditional probabilities of state v given the state h:
   [1*nxzvmlmv6kdqo2k77wnfna.png]
   eq. 3. conditional probabilities for h and v.

   it should be noticed beforehand (before demonstrating this fact on
   practical example) that each neuron in a rbm can only exist in a binary
   state of 0 or 1. the most interesting factor is the id203 that a
   hidden or visible layer neuron is in the state 1         hence activated.
   given an input vector v the id203 for a single hidden neuron j
   being activated is:
   [1*yx_c_itc8acuybhhcjqy5g.png]
   eq. 4. id155 for one hidden neuron, given v.

   here is    the sigmoid function. this equation is derived by applying
   the bayes rule to eq.3 and a lot of expanding which will be not covered
   here.

   analogous the id203 that a binary state of a visible neuron i is
   set to 1 is:
   [1*6bmmnqk8h3a_bfsq5k3j-a.png]
   eq. 5. id155 for one visible neuron, given h.
     __________________________________________________________________

2. id185 with restricted id82s

2. 1 recognizing latent factors in the data

   lets assume some people were asked to rate a set of movies on a scale
   of 1   5 stars. in classical factor analysis each movie could be
   explained in terms of a set of latent factors. for example, movies like
   harry potter and fast and the furious might have strong associations
   with a latent factors of fantasy and action. on the other hand users
   who like toy story and wall-e might have strong associations with
   latent pixar factor. rbms are used to analyse and find out these
   underlying factors. after some epochs of the training phase the neural
   network has seen all ratings in the training date set of each user
   multiply times. at this time the model should have learned the
   underlying hidden factors based on users preferences and corresponding
   collaborative movie tastes of all users.

   the analysis of hidden factors is performed in a binary way. instead of
   giving the model user ratings that are continues (e.g. 1   5 stars), the
   user simply tell if they liked (rating 1) a specific movie or not
   (rating 0). the binary rating values represent the inputs for the
   input/visible layer. given the inputs the rmb then tries to discover
   latent factors in the data that can explain the movie choices. each
   hidden neuron represents one of the latent factors. given a large
   dataset consisting out of thousands of movies it is quite certain that
   a user watched and rated only a small amount of those. it is necessary
   to give yet unrated movies also a value, e.g. -1.0 so that the network
   can identify the unrated movies during training time and ignore the
   weights associated with them.

   lets consider the following example where a user likes lord of the
   rings and harry potter but does not like the matrix, fight club and
   titanic. the hobbit has not been seen yet so it gets a -1 rating. given
   these inputs the id82 may identify three hidden factors
   drama, fantasy and science fiction which correspond to the movie
   genres.
   [1*zy4c980_7mfemytii6jvtw.png]
   fig. 3. identification of latent factors.

   given the movies the rmb assigns a id203 p(h|v) (eq. 4) for each
   hidden neuron. the final binary values of the neurons are obtained by
   sampling from [21]bernoulli distribution using the id203 p.

   in this example only the hidden neuron that represents the genre
   fantasy becomes activate. given the movie ratings the restricted
   id82 recognized correctly that the user likes fantasy the
   most.

2.2 using latent factors for prediction

   after the training phase the goal is to predict a binary rating for the
   movies that had not been seen yet. given the training data of a
   specific user the network is able to identify the latent factors based
   on this users preference. since the latent factors are represented by
   the hidden neurons we can use p(v|h) (eq. 5) and sample from bernoulli
   distribution to find out which of the visible neurons now become
   active.
   [1*de0rdpu_xrqt0bmave4vqa.png]
   fig. 4. using hidden neurons for the id136.

   fig. 4 shows the new ratings after using the hidden neuron values for
   the id136. the network did identified fantasy as the preferred
   movie genre and rated the hobbit as a movie the user would like.

   in summary the process from training to the prediction phase goes as
   follows:
    1. train the network on the data of all users
    2. during id136 time take the training data of a specific user
    3. use this data to obtain the activations of hidden neurons
    4. use the hidden neuron values to get the activations of input
       neurons
    5. the new values of input neurons show the rating the user would give
       yet unseen movies
     __________________________________________________________________

3. training

   the training of the restricted id82 differs from the
   training of a regular neural networks via stochastic id119.
   the deviation of the training procedure for a rbm wont be covered here.
   instead i will give an short overview of the two main training steps
   and refer the reader of this article to check out the original paper on
   [22]restricted id82s.

3.1 id150

   the first part of the training is called id150. given an input
   vector v we are using p(h|v) (eq.4) for prediction of the hidden values
   h. knowing the hidden values we use p(v|h) (eq.5) for prediction of new
   input values v. this process is repeated k times. after k iterations we
   obtain an other input vector v_k which was recreated from original
   input values v_0.
   [1*umbnsjvsmagqkvnqka62yg.png]

3.2 contrastive divergence

   the update of the weight matrix happens during the contrastive
   divergence step. vectors v_0 and v_k are used to calculate the
   activation probabilities for hidden values h_0 and h_k (eq.4). the
   difference between the [23]outer products of those probabilities with
   input vectors v_0 and v_k results in the update matrix:
   [1*gumqmq2nfgkwmchdoysxha.png]
   eq. 6. update matrix.

   using the update matrix the new weights can be calculated with gradient
   ascent, given by:
   [1*npmdgs0itn1wbuwn5ztb2g.png]
   eq. 7. update rule for the weights.
     __________________________________________________________________

special announcement: there is an online-course coming!

   we   re close to wrapping up our long-awaited course    [24]deep learning
   for predictive analytics   . the course has an emphasis on building deep
   learning applications in the field of predictive analytics and making
   it work in a production environment. a skillset which is usually not
   covered by other online courses         but crucial for those who want to
   work in this field professionally.

   if you are interested in getting notified when the course will be
   released or to receive further details, you can subscribe to the
   newsletter below         nice!

   ps: also by subscribing you are securing one of the limited places and
   a 50% discount on release.
   [25]subscribe here
   deep learning academy email formseepurl.com
   [1*jmtwmr9r1itplvgaatevvg.png]
     __________________________________________________________________

references

    1. [26]https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf
    2. [27]https://www.cs.toronto.edu/~hinton/absps/guidetr.pdf

     * [28]artificial intelligence
     * [29]deep learning
     * [30]machine learning
     * [31]data science
     * [32]towards data science

   (button)
   (button)
   (button) 3.4k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of artem oppermann

[34]artem oppermann

   teaching deep learning for the production environment. @
   [35]www.deeplearning-academy.com. visit us and secure one of the
   limited places.

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.4k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6df5c4918c15
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_vecibj80h3yd---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@artem.oppermann?source=post_header_lockup
  17. https://towardsdatascience.com/@artem.oppermann
  18. https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-ii-4b159dce1ffb
  19. https://en.wikipedia.org/wiki/netflix_prize
  20. https://en.wikipedia.org/wiki/boltzmann_distribution
  21. https://en.wikipedia.org/wiki/bernoulli_distribution
  22. https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf
  23. https://en.wikipedia.org/wiki/outer_product
  24. https://www.deeplearning-academy.com/
  25. http://eepurl.com/dlatea
  26. https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf
  27. https://www.cs.toronto.edu/~hinton/absps/guidetr.pdf
  28. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  29. https://towardsdatascience.com/tagged/deep-learning?source=post
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/data-science?source=post
  32. https://towardsdatascience.com/tagged/towards-data-science?source=post
  33. https://towardsdatascience.com/@artem.oppermann?source=footer_card
  34. https://towardsdatascience.com/@artem.oppermann
  35. http://www.deeplearning-academy.com/
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. http://eepurl.com/dlatea
  42. https://medium.com/p/6df5c4918c15/share/twitter
  43. https://medium.com/p/6df5c4918c15/share/facebook
  44. https://medium.com/p/6df5c4918c15/share/twitter
  45. https://medium.com/p/6df5c4918c15/share/facebook
