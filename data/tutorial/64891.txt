   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   implementing deep learning methods and feature engineering for text
   data: the glove model comments feed [5]data science interview guide
   [6]bitcoin trade signals

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2018    [28]apr    [29]tutorials,
   overviews    implementing deep learning methods and feature engineering
   for text data: the glove model ( [30]18:n18 )

implementing deep learning methods and feature engineering for text data: the
glove model

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]deep learning, [34]feature engineering, [35]nlp, [36]python,
   [37]id111

   the glove model stands for global vectors which is an unsupervised
   learning model which can be used to obtain dense word vectors similar
   to id97.
     __________________________________________________________________

                                                            c [38]comments

   by [39]dipanjan sarkar, intel

     editor's note: this post is only one part of a far more thorough and
     in-depth original, [40]found here, which covers much more than what
     is included here.


the glove model


   the glove model stands for global vectors which is an unsupervised
   learning model which can be used to obtain dense word vectors similar
   to id97. however the technique is different and training is
   performed on an aggregated global word-word co-occurrence matrix,
   giving us a vector space with meaningful sub-structures. this method
   was invented in stanford by pennington et al. and i recommend you to
   read the original paper on glove, [41]   glove: global vectors for word
   representation    by pennington et al. which is an excellent read to get
   some perspective on how this model works.

   we won   t cover the implementation of the model from scratch in too much
   detail here but if you are interested in the actual code, you can check
   out the [42]official glove page. we will keep things simple here and
   try to understand the basic concepts behind the glove model. we have
   talked about count based id105 methods like lsa and
   predictive methods like id97. the paper claims that currently, both
   families suffer significant drawbacks. methods like lsa efficiently
   leverage statistical information but they do relatively poorly on the
   word analogy task like how we found out semantically similar words.
   methods like skip-gram may do better on the analogy task, but they
   poorly utilize the statistics of the corpus on a global level.

   the basic methodology of the glove model is to first create a huge
   word-context co-occurence matrix consisting of (word, context) pairs
   such that each element in this matrix represents how often a word
   occurs with the context (which can be a sequence of words). the idea
   then is to apply id105 to approximate this matrix as
   depicted in the following figure.

                       [1*untssilztkxjlg99vxxsqw.png]
           conceptual model for the glove model   s implementation

   considering the word-context (wc) matrix, word-feature (wf) matrix
   and feature-context (fc) matrix, we try to factorize wc = wf x fc, such
   that we we aim to reconstruct wc from wf and fc by multiplying them.
   for this, we typically initialize wf and fc with some random weights
   and attempt to multiply them to get wc    (an approximation of wc) and
   measure how close it is to wc. we do this multiple times
   using [43]stochastic id119 (sgd) to minimize the error.
   finally, the word-feature matrix (wf) gives us the id27s for
   each word where f can be preset to a specific number of dimensions. a
   very important point to remember is that both id97 and glove models
   are very similar in how they work. both of them aim to build a vector
   space where the position of each word is influenced by its neighboring
   words based on their context and semantics. id97 starts with local
   individual examples of word co-occurrence pairs and glove starts with
   global aggregated co-occurrence statistics across all words in the
   corpus.


applying glove features for machine learning tasks


   let   s try and leverage glove based embeddings for our document
   id91 task. the very popular [44]spacy framework comes with
   capabilities to leverage glove embeddings based on different language
   models. you can also [45]get pre-trained word vectors and load them up
   as needed using gensim or spacy. we will first install spacy and use
   the [46]en_vectors_web_lg model which consists of 300-dimensional word
   vectors trained on [47]common crawl with glove.
# use the following command to install spacy
> pip install -u spacy

or
> conda install -c conda-forge spacy

# download the following language model and store it in disk
https://github.com/explosion/spacy-models/releases/tag/en_vectors_web_lg-2.0.0

# link the same to spacy
> python -m spacy link ./spacymodels/en_vectors_web_lg-2.0.0/en_vectors_web_lg e
n_vecs

linking successful
    ./spacymodels/en_vectors_web_lg-2.0.0/en_vectors_web_lg --> ./anaconda3/lib/
site-packages/spacy/data/en_vecs

you can now load the model via spacy.load('en_vecs')

   there are automated ways to install models in spacy too, you can check
   their [48]models & languages page for more information if needed. i had
   some issues with the same so i had to manually load them up. we will
   now load up our language model using spacy.

total word vectors: 1070971

   this validates that everything is working and in order. let   s get the
   glove embeddings for each of our words now in our toy corpus.

                       [1*g5esnpyyeu_pnwzwmycbnw.png]
                glove embeddings for words in our toy corpus

   we can now use id167 to visualize these embeddings similar to what we
   did using our id97 embeddings.

                       [1*ik7wvohok7mdava6tqpoia.png]
             visualizing glove id27s on our toy corpus

   the beauty of spacy is that it will automatically provide you the
   averaged embeddings for words in each document without having to
   implement a function like we did in id97. we will leverage the same
   to get document features for our corpus and use [49]id116 id91
   to cluster our documents.

                       [1*euubxnjykkjtnqmcz4wyxa.png]
         clusters assigned based on our document features from glove

   we see consistent clusters similar to what we obtained from our
   id97 model which is good! the glove model claims to perform better
   than the id97 model in many scenarios as illustrated in the
   following graph from the [50]original paper by pennington el al.

                       [1*m84ros1ymc7azutuyjnonw.png]
                        glove vs id97 performance
     (source: [51]https://nlp.stanford.edu/pubs/glove.pdf by pennington
                                   et al.)

   the above experiments were done by training 300-dimensional vectors on
   the same 6b token corpus (wikipedia 2014 + gigaword 5) with the same
   400,000 word vocabulary and a symmetric context window of size 10 in
   case anyone is interested in the details.


   bio: [52]dipanjan sarkar is a data scientist @intel, an author, a
   mentor @springboard, a writer, and a sports and sitcom addict.

   [53]original. reposted with permission.

   related:
     * [54]text id174: a walkthrough in python
     * [55]a general approach to preprocessing text data
     * [56]a framework for approaching textual data science tasks
     __________________________________________________________________

   [57][prv.gif] previous post
   [58]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [59]another 10 free must-read books for machine learning and data
       science
    2. [60]9 must-have skills you need to become a data scientist, updated
    3. [61]who is a typical data scientist in 2019?
    4. [62]the pareto principle for data scientists
    5. [63]what no one will tell you about data science job applications
    6. [64]19 inspiring women in ai, big data, data science, machine
       learning
    7. [65]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [66]id158s optimization using genetic algorithm
       with python
    2. [67]who is a typical data scientist in 2019?
    3. [68]8 reasons why you should get a microsoft azure certification
    4. [69]the pareto principle for data scientists
    5. [70]r vs python for data visualization
    6. [71]how to work in data science, ai, big data
    7. [72]the deep learning toolset     an overview

[73]latest news

     * [74]download your datax guide to ai in marketing
     * [75]kdnuggets offer: save 20% on strata in london
     * [76]training a champion: building deep neural nets for big ...
     * [77]building a recommender system
     * [78]predict age and gender using convolutional neural netwo...
     * [79]top tweets, mar 27     apr 02: here is a great ex...

   [80]kdnuggets home    [81]news    [82]2018    [83]apr    [84]tutorials,
   overviews    implementing deep learning methods and feature engineering
   for text data: the glove model ( [85]18:n18 )
      2019 kdnuggets. [86]about kdnuggets.  [87]privacy policy. [88]terms
   of service

   [89]subscribe to kdnuggets news
   [90][tw_c48.png] [91]facebook [92]linkedin
   x

   [envelope.png] [93]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-glove.html/feed
   5. https://www.kdnuggets.com/2018/04/data-science-interview-guide.html
   6. https://www.kdnuggets.com/2018/04/bitcoin-trade-signals.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2018/index.html
  28. https://www.kdnuggets.com/2018/04/index.html
  29. https://www.kdnuggets.com/2018/04/tutorials.html
  30. https://www.kdnuggets.com/2018/n18.html
  31. https://www.kdnuggets.com/2018/04/data-science-interview-guide.html
  32. https://www.kdnuggets.com/2018/04/bitcoin-trade-signals.html
  33. https://www.kdnuggets.com/tag/deep-learning
  34. https://www.kdnuggets.com/tag/feature-engineering
  35. https://www.kdnuggets.com/tag/nlp
  36. https://www.kdnuggets.com/tag/python
  37. https://www.kdnuggets.com/tag/text-mining
  38. https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-glove.html#comments
  39. https://www.linkedin.com/in/dipanzan
  40. https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa
  41. https://nlp.stanford.edu/pubs/glove.pdf
  42. https://nlp.stanford.edu/projects/glove/
  43. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  44. https://spacy.io/
  45. https://nlp.stanford.edu/projects/glove/
  46. https://spacy.io/models/en#en_vectors_web_lg
  47. http://commoncrawl.org/
  48. https://spacy.io/usage/models
  49. https://en.wikipedia.org/wiki/id116_id91
  50. https://nlp.stanford.edu/pubs/glove.pdf
  51. https://nlp.stanford.edu/pubs/glove.pdf
  52. https://www.linkedin.com/in/dipanzan
  53. https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa
  54. https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html
  55. https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html
  56. https://www.kdnuggets.com/2017/11/framework-approaching-textual-data-tasks.html
  57. https://www.kdnuggets.com/2018/04/data-science-interview-guide.html
  58. https://www.kdnuggets.com/2018/04/bitcoin-trade-signals.html
  59. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  60. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  61. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  62. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  63. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  64. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  65. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  66. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  67. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  68. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  69. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  70. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  71. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  72. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  73. https://www.kdnuggets.com/news/index.html
  74. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  75. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  76. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  77. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  78. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  79. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  80. https://www.kdnuggets.com/
  81. https://www.kdnuggets.com/news/index.html
  82. https://www.kdnuggets.com/2018/index.html
  83. https://www.kdnuggets.com/2018/04/index.html
  84. https://www.kdnuggets.com/2018/04/tutorials.html
  85. https://www.kdnuggets.com/2018/n18.html
  86. https://www.kdnuggets.com/about/index.html
  87. https://www.kdnuggets.com/news/privacy-policy.html
  88. https://www.kdnuggets.com/terms-of-service.html
  89. https://www.kdnuggets.com/news/subscribe.html
  90. https://twitter.com/kdnuggets
  91. https://facebook.com/kdnuggets
  92. https://www.linkedin.com/groups/54257
  93. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  95. https://www.kdnuggets.com/
  96. https://www.kdnuggets.com/
