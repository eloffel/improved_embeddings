   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

td learning         richard sutton presentation at id23 summer
school, montreal 2017

   [9]go to the profile of ranko mosic
   [10]ranko mosic (button) blockedunblock (button) followfollowing
   aug 30, 2017

   these are highlights and slides from prof. sutton   s [11]lecture:
     * this is really a special time ; perhaps we are at a point where we
       understand enough of how mind works so that we can create
       intelligence by design; exact timeline is not known, but we can say
       with high id203 that it will occur in the next few decades
     * why is this happening ? moore   s law; increase in available
       computation alters everything and is profound ( we reached the
       end of moore   s law/dennard scaling , further compute power growth
       now depends on innovative processor architectures )

   [1*grkeysoodeg5xtebxlskkg.png]
     * methods that scale with computation (    weak ai    ) are the future of
       ai; as we get more computing power these methods become more
       powerful ( which hasn   t always been true )
     * if you think dl and rl are on the right part of the history, you
       might be wrong
     * supervised learning and model-free id23 are only
       weakly scalable; for example, learning value function and improving
       policy is only one object, even with a few features ( meaning it
       can   t efficiently use increasing computing power )
     * sl is weakly scalable because it requires humans to provide
       labelled data sets; you want to be able to learn from raw data

   [1*datwom8npsdznjwrxu-qeg.png]
     * what is fully scalable ? prediction learning         learning to predict
       what will happen; it is unsupervised supervised learning; we have a
       target         we just wait and see what happens         that   s our target; you
       have a target, but you don   t need a human to provide it         you just
       get it from the data; this is scalable model-free learning, and
       maybe it is the scalable model-free learning
     * prediction learning is at the heart of our our control methods, we
       learn value functions; td learning is the scalable, model-free
       method

   [1*4nifsj54ysbodovims_38w.png]
     * predicting is the key problem; dl, sl is not about predicting (
       only predicts what a label would be         it is not about predicting
       over time, about wait and see what happens, which makes all the
       difference !! )
     * td is a method of learning to predict, widely used in rl to predict
       future rewards, value functions; it is the center, the core of many
       methods ( id24, sarsa, td(  ), deep q networks, actor-critic
       ); td is not universally used         alphago, policy based methods don   t
       use it

   [1*vvniahxlmswqkukisozv7a.png]
     * td is sort of optional         it certainly feels optional, but perhaps
       you should always be using it, it is ubiquitous
     * it can be used to predict anything; it is a general method for
       prediction learning, not just for rewards
     * criteria to distinguish td from other ( possibly sl ) methods is if
       they update a guess from a guess ( which sounds dangerous         how do
       we constrain/anchor it, how do we tie it down ?)
     * td learning is learning prediction from another, later
       prediction         an estimate from an estimate, and if it is good enough

   [1*gvqhhcwdwsgmhfvbcfpcbq.png]
     * td error is the difference between two temporally successive
       predictions; if you play a game, make a move and think:   i am
       winning   , then make another move and think:   now i am losing            you
       try to learn from it; you don   t know who is going to win, but you
       try to learn from the difference in your predictions
     * where does the target come from         does it come from the end of the
       game, or from your next prediction ?
     * td-gammon         backgammon playing program from 1992 used single nn
       with one layer which predicted the id203 of winning; updated
       td error as the difference between two successive win id203
       predictions; backpropagated td error and learnt by playing against
       itself         learning from trial and error

   [1*ynrnmbc1xxerf4idclz2qg.png]
     * the question is: do we need to use td learning and when do we need
       it ? many people in rl think you can get away without it
     * it is only relevant in multi-step prediction problems         only when
       the thing predicted is multiple steps in future
     * td learning is broadly applicable as everything you want to do is a
       multi-step prediction         what the stock market index will be, for
       example; as you get more data, you make a new prediction; you make
       repeated predictions about long term outcomes         who the us will go
       to war against next         long term predictions, which don   t jump up to
       the end

   [1*pteb4olkb__k9zb6zrkcsq.png]
   [1*wv7wa7gboff95vhsmqkr1w.png]
     * can we not ignore step by step method and wait to the end of the
       game to see who wins ( use one step method instead ) ? can we learn
       one step model and compose your model ? the answer is: you can   t

   [1*_qylshqvi-efrvrdhew2lg.png]
     * in other words: can we treat multiple steps as one big step, or we
       have to treat it bit by bit ?
     * also: can we learn one step predictions as a model, then iterate
       them to get a multi-step prediction ? it is a trap to think it is
       enough to use low level model of the world as simulation of the
       world; for example to treat the world as mdp and model the
       transition probabilities, or treat the world as a engineering model
       where we just have to learn velocities, and then integrate these
       low level differential equations; short term models and iteration
       feel good because we think that if it can be done perfectly in one
       step, we think it can be done perfectly for as far as i want to
       look into the future

   [1*6gcq2x1br79k6jyhypfy0w.png]
     * two problems with one-step approach: first we can   t do it
       perfectly: we always have an approximation, and when we try to
       iterate, we will have propagation and compounding of errors (
       useless long term prediction ); secondly: it is exponentially
       complex as number of choices increases, as world is stochastic; it
       quickly becomes computationally intractable

   [1*detftprwc2utxwerss_-wg.png]
   [1*kbgeip2mq2bdwtnl43g0ig.png]
   [1*iuwufmals1sy8n7sympbeq.png]

   monte carlo method means stepping through all states and actions to a
   terminal state, finding out gt ( total reward ) and updating value of a
   state v(st).
   [1*eh0vqiyefc36gxqha4iiqw.png]

   td method: state value v(st) is updated ( backed up ) with immediate
   reward rt+1 and discounted value of the next state         v(st+1)
   [1*jiz4k7yfhwoe60keuji5bw.png]

   id145 method only works for mdps i.e. environments with
   fully known transition probabilities i.e. where model is known.

   id145 is not considering a single line through possible
   tree like mc or td; it considers all possibilities         in this example it
   considers both actions.
   [1*zznlfzemyx8a0k3seunuaq.png]

   td is the only method that both bootstraps ( your target involves a
   guess in existing prediction) and samples. mc doesn   t bootstrap         it
   looks at all returns all the way to the end, ie there are no estimates
   playing a role in return. dp bootstraps         you use your estimates, which
   gradually get better.

   mc and td are learning methods         they sample, because they don   t know
   how the world works.
   [1*6korssgiz8yhl9srfiyg1a.png]

   td(0) is one step estimate of the return, as opposed to mc   s actual
   return for the whole series of steps ( all the way to the terminal
   state ). with td(0) you can even learn without finding out the final
   outcome.

   both of these methods will converge, the only question is which is
   faster.

   note by rm :some of these tenets might be changed as new rl platforms
   become available; one example is recently announced [12]berkeley   s ray
   platform         a new project aimed at replacing spark for ml applications
   with its parallelized, asynchronous policy updates:
   [1*f9d5edeu6495kwkc4glrwq.png]
   [13]berkeley   s ray platform
   [1*vwbbcvjuak_wztcjk2bryg.png]
   [1*koq2vba9af8j3f7lxja0zq.png]

   useful tip on how to easily distinguish between on-policy and
   off-policy learning: on-policy means there is almost only one
   policy         one that you are learning about.

   on having to choose between td and mc: if you use td, you can
   parametrically vary    ( the height of your backups ) to get to any
   intermediate point between one step td and mc.
   [1*zc25iavnjwkhb_r85ktqxg.png]
   [1*edunvbcmvsivmzw66i4cgq.png]
   td expressed via feature and parameter vectors

   conclusion: prof. sutton is obviously quite bullish on td. deepmind
   results seem to confirm his views         here is an example of how well 1
   step id24 scales out on 16 threads, even when compared to a3c :
   [1*sl1awqybymlo7n2vl-3lmq.png]

   on the other hand, we can   t disregard policy gradient algorithms (
   reinforce ), as well as methods that learn approximations for both
   policy and value functions (actor-critic ) since there is also strong
   research in that direction. these algorithms have interesting
   properties and are utilized by some papers that apply policy based
   algorithms to trading and option pricing, claiming superior results
   compared to td based methods.

     * [14]machine learning
     * [15]id23
     * [16]fintech
     * [17]ai

   (button)
   (button)
   (button) 38 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [18]go to the profile of ranko mosic

[19]ranko mosic

   big data / machine learning consultant [20]ranko.mosic@gmail.com
   408-757-0053

     * (button)
       (button) 38
     * (button)
     *
     *

   [21]go to the profile of ranko mosic
   never miss a story from ranko mosic, when you sign up for medium.
   [22]learn more
   never miss a story from ranko mosic
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/3fee3238d162
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@ranko.mosic/prof-richard-sutton-at-montreal-2017-reinforcement-learning-rlss-summer-school-montreal-2017-3fee3238d162&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@ranko.mosic/prof-richard-sutton-at-montreal-2017-reinforcement-learning-rlss-summer-school-montreal-2017-3fee3238d162&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@ranko.mosic?source=post_header_lockup
  10. https://medium.com/@ranko.mosic
  11. http://videolectures.net/deeplearning2017_sutton_td_learning/
  12. https://www.oreilly.com/ideas/ray-a-distributed-execution-framework-for-emerging-ai-applications-full-keynote-post
  13. https://www.oreilly.com/ideas/ray-a-distributed-execution-framework-for-emerging-ai-applications-full-keynote-post
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/reinforcement-learning?source=post
  16. https://medium.com/tag/fintech?source=post
  17. https://medium.com/tag/ai?source=post
  18. https://medium.com/@ranko.mosic?source=footer_card
  19. https://medium.com/@ranko.mosic
  20. mailto:ranko.mosic@gmail.com
  21. https://medium.com/@ranko.mosic
  22. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  24. https://rodneybrooks.com/the-end-of-moores-law/
  25. https://medium.com/p/3fee3238d162/share/twitter
  26. https://medium.com/p/3fee3238d162/share/facebook
  27. https://medium.com/p/3fee3238d162/share/twitter
  28. https://medium.com/p/3fee3238d162/share/facebook
