   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    a
   tutorial on the expectation maximization (em) algorithm comments feed
   [5]build your analytics skills with tdwi online learning
   [6]interpretability over accuracy

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]aug    [29]tutorials,
   overviews    a tutorial on the expectation maximization (em) algorithm
   ( [30]16:n32 )

a tutorial on the expectation maximization (em) algorithm

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 52
   tags: [33]id91, [34]data science, [35]data science education,
   [36]predictive analytics, [37]statistics

   this is a short tutorial on the expectation maximization algorithm and
   how it can be used on estimating parameters for multi-variate data.
     __________________________________________________________________

   by elena sharova, [38]codefying.

   we are presented with some unlabelled data and we are told that it
   comes from a multi-variate gaussian distribution. our task is to come
   up with the hypothesis for the means and the variances of each
   distribution. for example, figure 1 shows (labelled) data drawn from
   two gaussians. we need to estimate the means and variances of the x   s
   and the y   s of the blue and red distribution. how are we going to do
   this?

   2-dimensional data

       figure 1. 2-dimensional data drawn from 2 normal distributions.

   well, we can plot the data of each dimension and estimate the means by
   looking at the plots. for example, we could use a histogram plot to
   estimate that the mean of the x   s of the blue distribution is about 0.6
   and the mean of the x   s from the red distribution is about 0.0. by
   looking at the spread of each cluster we can estimate that the variance
   of blue x   s is small, perhaps 0.0, and variance of red x   s is about
   0.025. that would be a pretty good guess since the actual distribution
   was drawn from n(0.6, 0.003) for blue and n(0.02, 0.03) for red. can we
   do better than looking at the plots? can we come up with a good
   estimates for the means and the variances in a more robust way?

   histogram

        figure 2. histogram of (unlabelled) data drawn from 2 normal
                               distributions.

   the expectation maximization (em) algorithm can be used to generate the
   best hypothesis for the distributional parameters of some multi-modal
   data. note that we say    the best    hypothesis. but what is    the best   ?
   the best hypothesis for the distributional parameters is the maximum
   likelihood hypothesis     the one that maximizes the id203 that
   this data we are looking at comes from k distributions, each with a
   mean m[k] and variance sigma[k]^2. in this tutorial we are assuming
   that we are dealing with k normal distributions.

   in a single modal normal distribution this hypothesis h is estimated
   directly from the data as:

                       estimated m = m~ = sum(x[i])/n

                                 equation 1

                 estimated sigma2= sigma2~= sum(xi- m~)^2/n

                                 equation 2

   which are simply the trusted arithmetic average and variance. in a
   multi-modal distribution we need to estimate h = [ m[1],m[2],...,m[k];
   sigma[1]^2,sigma[2]^2,...,sigma[k]^2 ]. the em algorithm is going to
   help us to do this. let   s see how.

   we begin with some initial estimate for each m[k]~ and sigma[k]^2~. we
   will have a total of k estimates for each parameter. the estimates can
   be taken from the plots we made earlier, our domain knowledge, or they
   even can be wild (but not too wild) guesses. we then proceed to take
   each data point and answer the following question     what is the
   id203 that this data point was generated from a normal
   distribution with mean m[k]~ and sigma[k]^2~? that is, we repeat this
   question for each set of our distributional parameters. in figure 1 we
   plotted data from 2 distributions. thus we need to answer these
   questions twice     what is the id203 that a data point x[i],
   i=1,...n, was drawn from n(m[1]~, sigma[1]^2~) and what is the
   id203 that it was drawn from n(m[2]~, sigma[2]^2~). by the normal
   density function we get:

    p(x[i] belongs to n(m[1]~ , sigma[1]^2~))=1/sqrt(2*pi* sigma[1]^2~) *
                    exp(-(x[i]- m[1]~)^2/(2*sigma[1]^2~))

                                 equation 3

    p(x[i] belongs to n(m[2]~ , sigma[2]^2~))=1/sqrt(2*pi* sigma[2]^2~) *
                    exp(-(x[i]- m[2]~)^2/(2*sigma[2]^2~))

                                 equation 4

   the individual probabilities only tell us half of the story because we
   still need to take into account the id203 of picking n(m[1]~,
   sigma[1]^2~) or n(m[2]~, sigma[2]^2~) to draw the data from. we now
   arrive at what is known as responsibilities of each distribution for
   each data point. in a classification task this responsibility can be
   expressed as the id203 that a data point x[i] belongs to some
   class c[k]:

      p(x[i] belongs to c[k]) = omega[k]~ * p(x[i] belongs to n(m[1]~ ,
         sigma[1]^2~)) / sum(omega[k]~ * p(x[i] belongs to n(m[1]~ ,
                               sigma[1]^2~)))

                                 equation 5

   in equation 5 we introduce a new parameter omega[k]~ which is the
   id203 of picking k   s distribution to draw the data point from.
   figure 1 indicates that each of our two clusters are equally likely to
   be picked. but like with m[k]~ and sigma[k]^2~ we do not really know
   the value for this parameter. therefore we need to guess it and it is a
   part of our hypothesis:

    h = [ m[1], m[2], ..., m[k]; sigma[1]^2, sigma[2]^2, ..., sigma[k]^2;
                   omega[1]~, omega[2~], ..., omega[k]~ ]

   you could be asking yourself where the denominator in equation 5 comes
   from. the denominator is the sum of probabilities of observing x[i] in
   each cluster weighted by that cluster   s id203. essentially, it is
   the total id203 of observing x[i] in our data.

   if we are making hard cluster assignments, we will take the maximum
   p(x[i] belongs to c[k]) and assign the data point to that cluster. we
   repeat this probabilistic assignment for each data point. in the end
   this will give us the first data    re-shuffle    into k clusters. we are
   now in a position to update the initial estimates for h to h'. these
   two steps of estimating the distributional parameters and updating them
   after probabilistic data assignments to clusters is repeated until
   convergences to h*. in summary, the two steps of the em algorithm are:
    1. e-step: perform probabilistic assignments of each data point to
       some class based on the current hypothesis h for the distributional
       class parameters;
    2. m-step: update the hypothesis h for the distributional class
       parameters based on the new data assignments.

   during the e-step we are calculating the expected value of cluster
   assignments. during the m-step we are calculating a new maximum
   likelihood for our hypothesis.

   elena sharova bio: [39]elena sharova is a data scientist, financial
   risk analyst and software developer. she holds an msc in machine
   learning and data mining from university of bristol.

   related:
     * [40]a comparison between pca and hierarchical id91
     * [41]6 crazy things deep learning and topological data analysis can
       do with your data
     * [42]data science 102: id116 id91 is not a free lunch
     __________________________________________________________________

   [43][prv.gif] previous post
   [44]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [45]another 10 free must-read books for machine learning and data
       science
    2. [46]9 must-have skills you need to become a data scientist, updated
    3. [47]who is a typical data scientist in 2019?
    4. [48]the pareto principle for data scientists
    5. [49]what no one will tell you about data science job applications
    6. [50]19 inspiring women in ai, big data, data science, machine
       learning
    7. [51]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [52]id158s optimization using genetic algorithm
       with python
    2. [53]who is a typical data scientist in 2019?
    3. [54]8 reasons why you should get a microsoft azure certification
    4. [55]the pareto principle for data scientists
    5. [56]r vs python for data visualization
    6. [57]how to work in data science, ai, big data
    7. [58]the deep learning toolset     an overview

[59]latest news

     * [60]download your datax guide to ai in marketing
     * [61]kdnuggets offer: save 20% on strata in london
     * [62]training a champion: building deep neural nets for big ...
     * [63]building a recommender system
     * [64]predict age and gender using convolutional neural netwo...
     * [65]top tweets, mar 27     apr 02: here is a great ex...

   [66]kdnuggets home    [67]news    [68]2016    [69]aug    [70]tutorials,
   overviews    a tutorial on the expectation maximization (em) algorithm
   ( [71]16:n32 )
      2019 kdnuggets. [72]about kdnuggets.  [73]privacy policy. [74]terms
   of service

   [75]subscribe to kdnuggets news
   [76][tw_c48.png] [77]facebook [78]linkedin
   x

   [envelope.png] [79]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/08/tutorial-expectation-maximization-algorithm.html/feed
   5. https://www.kdnuggets.com/2016/08/tdwi-analytics-skills-online-learning.html
   6. https://www.kdnuggets.com/2016/08/salford-interpretability-over-accuracy.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/08/index.html
  29. https://www.kdnuggets.com/2016/08/tutorials.html
  30. https://www.kdnuggets.com/2016/n32.html
  31. https://www.kdnuggets.com/2016/08/tdwi-analytics-skills-online-learning.html
  32. https://www.kdnuggets.com/2016/08/salford-interpretability-over-accuracy.html
  33. https://www.kdnuggets.com/tag/id91
  34. https://www.kdnuggets.com/tag/data-science
  35. https://www.kdnuggets.com/tag/data-science-education
  36. https://www.kdnuggets.com/tag/predictive-analytics
  37. https://www.kdnuggets.com/tag/statistics
  38. https://codefying.wordpress.com/
  39. https://codefying.wordpress.com/
  40. https://www.kdnuggets.com/2016/02/qlucore-comparison-pca-hierarchical-id91.html
  41. https://www.kdnuggets.com/2015/11/crazy-deep-learning-topological-data-analysis.html
  42. https://www.kdnuggets.com/2015/01/data-science-102-kmeans-id91-not-free-lunch.html
  43. https://www.kdnuggets.com/2016/08/tdwi-analytics-skills-online-learning.html
  44. https://www.kdnuggets.com/2016/08/salford-interpretability-over-accuracy.html
  45. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  46. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  47. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  48. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  49. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  50. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  51. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  52. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  53. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  54. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  55. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  56. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  57. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  58. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  59. https://www.kdnuggets.com/news/index.html
  60. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  61. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  62. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  63. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  64. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  65. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  66. https://www.kdnuggets.com/
  67. https://www.kdnuggets.com/news/index.html
  68. https://www.kdnuggets.com/2016/index.html
  69. https://www.kdnuggets.com/2016/08/index.html
  70. https://www.kdnuggets.com/2016/08/tutorials.html
  71. https://www.kdnuggets.com/2016/n32.html
  72. https://www.kdnuggets.com/about/index.html
  73. https://www.kdnuggets.com/news/privacy-policy.html
  74. https://www.kdnuggets.com/terms-of-service.html
  75. https://www.kdnuggets.com/news/subscribe.html
  76. https://twitter.com/kdnuggets
  77. https://facebook.com/kdnuggets
  78. https://www.linkedin.com/groups/54257
  79. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  81. https://www.kdnuggets.com/
  82. https://www.kdnuggets.com/
