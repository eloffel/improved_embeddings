   #[1]rss

   [ ]

   i am a research scientist at cogent labs.

   [2]home [3]about [4]archive [5]projects [6]tags
     * [7]tiago@nehalemlabs.net

     * [8]tmramalho
     * [9]tmramalho

      2018. all rights reserved.

[10]tiago ramalho just another ai researcher

[11]id84 101: id202, hidden variables and
generative models

   13 apr 2015

   suppose you are faced with a high dimensional dataset and want to find
   some structure in the data: often there are only a few causes, but lots
   of different data points are generated due to noise corruption. how can
   we infer these causes? here i   m going to cover the simplest method to
   do this id136: we will assume the data is generated by a linear
   transformation of the hidden causes. in this case, it is quite simple
   to recover the parameters of this transformation and therefore
   determine the hidden (or latent) variables which represent their cause.

   mathematically, let   s say the data is given by a high-dimensional (size
   n) vector $u$ which is generated by a low-dimensional (size m) vector
   $v$ via
   $$u=\sum_i^m w_i v_i + \text{noise}$$

   where $w_i$ are n-dimensional vectors which encode the transformation.
   for simplicity i will assume $u$ to have mean value 0. let   s assume the
   noise is gaussian. then the distribution for the data is
   $$p(u|v) =
   \frac{1}{\sqrt{(2\pi)^n\left|\sigma\right|}}\exp\left(-\frac{1}{2}(u-w\
   cdot v)^t \sigma^{-1}(u-w\cdot v)\right)$$

   with $\sigma$ a diagonal matrix. this means that we assume any
   correlation structure in the data arises from the transformation $w$,
   and not due to noise correlations. this model is the main feature of
   the procedure known as [12]factor analysis.

   to fit the model we need both the weights $w$ and factors $v$ which
   best fit the data (in the case of no prior information for $v$, this
   corresponds to maximising the [13]likelihood function). it is
   convenient to take the logarithm of the likelihood function:
   $$\log p(u|v) = -\frac{1}{2}(u-w\cdot v)^t \sigma^{-1}(u-w\cdot v)
   -\frac{1}{2} \log |\sigma| - \frac{n}{2} \log 2\pi$$

   so we are looking for $\arg\max_{w,v,\sigma} \log p(u|v)$. the obvious
   way to obtain the parameters is to use [14]expectation-maximisation: we
   calculate what are the factors given $w$ and $\sigma$, and then update
   those parameters given the new factors in such a way as to maximise the
   log-likelihood. iterating this process will result in convergence to a
   final set of weights and factors, which will be a local minimum of the
   likelihood.

   let   s examine how this works with a very simple example: the [15]iris
   dataset. this is a 4 dimensional dataset, with each dimension
   corresponding to a physical measurement of the sepals and petals of a
   flower. there are 3 different flowers so we can assume that each
   flower   s genome will code for different physical features. so the
   question is whether there is a    genome variable    $v$ which maps into
   the observed physical features $u$? let   s load the dataset directly
   using sklearn:
from sklearn import datasets

iris = datasets.load_iris()
x = iris.data # physical measurements
y = iris.target # actual species for visualization

   and now we can plug this directly into the factor analysis model:
import matplotlib.pyplot as plt
from sklearn.decomposition import factoranalysis

model = factoranalysis(n_components=2)
x_reduced = model.fit_transform(x)
plt.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y, cmap=plt.cm.paired)

   let   s visualize the hidden factors for the actual data points we have
   by inverting the transformation. below you can see the data with $v_1$
   in the x-axis and $v_2$ in the y-axis.

   factor analysis with 2 components.

   in fact, it seems that the first component $v_1$ manages to describe
   accurately the variation between the 3 different species. let   s try to
   do factor analysis with just one component and see what comes out (i
   added some randomness in the y-direction to visualize the points
   better).

   factor analysis with 1 component.

   with 1 component we obtain an estimate for $\sigma^{-1} = [
   0.16350346, 0.15436415, 0.00938691, 0.04101233]$ which corresponds to
   the variation of the data which is not explained by the linear
   transformation of $v$. what if we would force all diagonal elements of
   $\sigma$ to be identical? in that case, we observe that the vectors
   $w_i$ actually form an orthogonal basis (i.e. each one is perpendicular
   to the next). this in turn means that when we project the data to the
   space of the hidden variables, each $v_i$ will be uncorrelated to the
   others (which does not happen with factor analysis).

   id202 explains why this happens: after some calculation, we
   can observe that the obtained $w_i$ correspond to the principal
   eigenvectors (i.e. are associated to the largest eigenvalues) of the
   sample covariance matrix $c= \frac{1}{n}\sum_{j=1}^n (u^j - \bar{u})
   (u^j-\bar{u})^t$ with the overscripts denoting sample index. this
   method has a specific name and is called [16]probabilistic principal
   components analysis. setting $\sigma = 0$ we obtain the popular vanilla
   [17]pca.

   typically we sort the eigenvectors such that $w_1$ corresponds to the
   largest eigenvalue of $c$ meaning that the hidden variable $v_1$
   explains the most variation in the observed data. now $w_2$ must be
   perpendicular to $w_1$, which means that $v_2$ will explain the most
   remaining variation in the data, and so on. if we allow $m=n$ then it
   would be possible to completely reconstruct the data from $v$.

   pca with 2 components.

   in the iris dataset we can observe that the projected subspace using
   probabilistic pca is different to the one found in factor analysis. we
   can confirm that we found orthogonal components by printing $w_1 \cdot
   w_2$ and comparing it to factor analysis:
model = pca(n_components=2)
x_reduced = model.fit_transform(x)
print np.dot(model.components_[0], model.components_[1])
# 3.43475248243e-16

model = factoranalysis(n_components=2)
x_reduced = model.fit_transform(x)
print np.dot(model.components_[0], model.components_[1])
# 0.146832660483

   in the case of handwritten digits, the first two components in pca
   appear to capture more of the structure of the problem than the first
   two factors in factor analysis. however, since this is a strongly
   nonlinear problem, none of them can accurately separate all 10 digit
   classes.

   digits with fa digits with pca

   if we look at the log-likelihood equivalent for pca we observe a very
   deep connection: $\log p(u|v) = -\frac{1}{2}(u-w\cdot v)^2$. the $w, v$
   that come out of pca (i.e. maximise the log-likelihood) also minimize
   the least squares reconstruction error for the simplest linear
   model. from a data compression perspective, that means that we can
   compress the information contained in $u$ by saving only $w$ and $v$.
   this is worth it when we have a large number of samples of $u$, since
   $w$ is constant and each $v$ is smaller than each $u$.

   with the fit distribution in hand, we can even generate new samples of
   $u$. we just need to draw some samples of low-dimensional $v$ and pass
   it through the linear model we have. let   s see what happens when we
   apply this idea to the digits dataset, sampling $v$ from a gaussian
   approximation of the $v$ corresponding to the actual data distribution.
v = model.transform(x)
v_samples = np.random.multivariate_normal(
    np.mean(v, axis=0), np.cov(v.t), size=100)
x_samples = model.inverse_transform(v_samples)

   digits from generative distribution. given that we are applying a
   linear model to a nonlinear problem, i'm really happy to be able to
   make out a few digits!

   to finish this long post, let   s just look at what happens if we don   t
   assume a flat prior for $v$. two common options are to assume $p(v)$ is
   a gaussian or an [18]exponential distribution. then the log-likelihood
   (getting rid of constants, as usual) becomes:
   $$\log p(u|v) = -\frac{1}{2}(u-w\cdot v)^2 - \alpha v^2 - \beta |v|$$

   those two terms correspond to the often used $l_2$ and $l_1$
   [19]id173, which for linear models have the fancy names of
   ridge regression and lasso respectively (not really sure why they need
   fancy names). writing this in a more cs notation, we can say that our
   problem consists of maximising:
   $$||u-w\cdot v||^2 - \alpha ||v||^2 - \beta ||v||$$

   this problem has been given the name of [20]sparse pca, and we can find
   an implementation in scikits. let   s see what happens when we apply it
   to the digits dataset.
from sklearn.decomposition import sparsepca
from sklearn import datasets

dset = datasets.load_digits()
x = dset.data
y = dset.target

model = sparsepca(n_components=10, alpha=0)
x_reduced = model.fit_transform(x)

print np.dot(model.components_[0], model.components_[1])

n_row = 5
n_col = 2
plt.figure(figsize=(6, 15))
for i in xrange(10):
        comp = model.components_[i]
        plt.subplot(n_row, n_col, i + 1)
        vmax = max(comp.max(), -comp.min())
        plt.imshow(comp.reshape([8,8]), cmap=plt.cm.gray, interpolation='nearest
')
        plt.xticks(())
        plt.yticks(())
plt.tight_layout()
plt.savefig('digits_spca_rec.png')

   immediately we notice that we lose the orthogonality of the components
   due to the constraints. let   s see how they look like:

   10 components found by sparse pca.

   [21]here is a gist with all the code used for this post.

[22]parallel programming with opencl and python: parallel scan

   23 jun 2014

   this post will continue the subject of how to implement common
   algorithms in a parallel processor, which i started to discuss
   [23]here. today we come to the second pattern, the [24]scan. an example
   is the cumulative sum, where you iterate over an array and calculate
   the sum of all elements up to the current one. like reduce, the
   algorithm we   ll talk about is not exclusive for the sum operation, but
   for any binary associative operation (the max is another common
   example). there are two ways to do a parallel scan:  the hills steele
   scan, which needs $\log n$ steps; and the blelloch scan, requiring $2
   \log n$ steps. the blelloch scan is useful if you have more data than
   processors, because it only needs to do $\mathcal{o}(n)$ operations
   (this quantity is also referred to as the work efficiency); while the
   hillis steele scan needs $\mathcal{o}(n \log n)$ operations. so let   s
   look at how to implement both of them with opencl kernels.

hillis steele

   this is the simplest scan to implement, and is also quite simple to
   understand. it is worthwhile noting that this is an inclusive scan,
   meaning the first element of the output array o is described as a
   function of the input array a as follows:

   naturally, you could replace the summation with any appropriate
   operator. rather than describing the algorithm with words, i think a
   picture will make it much clearer than i could:

   [25]visualization of the hillis steele inclusive scan.

   visualization of the hillis steele inclusive scan. credit: [26]nvidia.

   as you can see, the key is to add the value of your neighbor $2^d$
   positions to your left to yourself, or just do nothing if such neighbor
   does not exist. this is quite simple to translate to an opencl kernel:
#define swap(a,b) {__local float *tmp=a;a=b;b=tmp;}
__kernel void scan(__global float *a,
                   __global float *r,
                   __local float *b,
                   __local float *c)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);
    uint gs = get_local_size(0);

    c[lid] = b[lid] = a[gid];
    barrier(clk_local_mem_fence);

    for(uint s = 1; s &lt; gs; s &lt;&lt;= 1) {
        if(lid &gt; (s-1)) {
            c[lid] = b[lid]+b[lid-s];
        } else {
            c[lid] = b[lid];
        }
        barrier(clk_local_mem_fence);
        swap(b,c);
    }
    r[gid] = b[lid];
}

   the for loop variable s represents the neighbor distance: it is
   multiplied by two every loop until we reach n neighbors. note the use
   of the swap macro: it swaps the b and c pointers which will always
   denote the current step (c) and the previous step (b) without needing
   to copy memory.

blelloch

   the blelloch scan is an exclusive scan, which means the sum is computed
   up to the current element but excluding it. in practice it means the
   result is the same as the inclusive scan, but shifted by one position
   to the right:

   the idea of the algorithm is to avoid repeating summations of the same
   numbers. as an example, if you look at the picture above you can see
   that to calculate $o_5$ we need to add together ${x_5, x_4, x_2+x_3,
   x_1+x_0}$. that means we are essentially repeating the calculation of
   $o_3$ for nothing. so to avoid this we   d need to come up with a non
   overlapping set of partial sums that each calculation could reuse.
   that   s what this algorithm does!

   as before, the algorithm is better explained with a picture or two:

   [27]the up sweep portion of the blelloch scan.

   the up sweep portion of the blelloch scan. here the partial sums are
   calculated. credit: [28]nvidia.

   [29]the down sweep portion of the blelloch scan.

   the down sweep portion of the blelloch scan. here the partial sums are
   used to calculate the final answer.

   you can see how the up sweep part consists of calculating partial sums,
   and the down sweep combines them in such a way that we end up with the
   correct results in the correct memory positions. let   s see how to do
   this in opencl:
__kernel void scan(__global float *a,
                   __global float *r,
                   __local float *b,
                   uint n_items)
{
    uint gid = get_global_id(0);
    uint lid = get_local_id(0);
    uint dp = 1;

    b[2*lid] = a[2*gid];
    b[2*lid+1] = a[2*gid+1];

    for(uint s = n_items&gt;&gt;1; s &gt; 0; s &gt;&gt;= 1) {
        barrier(clk_local_mem_fence);
        if(lid &lt; s) {
            uint i = dp*(2*lid+1)-1;
            uint j = dp*(2*lid+2)-1;
            b[j] += b[i];
        }

        dp &lt;&lt;= 1;
    }

    if(lid == 0) b[n_items - 1] = 0;

    for(uint s = 1; s &lt; n_items; s &lt;&lt;= 1) {
        dp &gt;&gt;= 1;
        barrier(clk_local_mem_fence);

        if(lid &lt; s) {
            uint i = dp*(2*lid+1)-1;
            uint j = dp*(2*lid+2)-1;

            float t = b[j];
            b[j] += b[i];
            b[i] = t;
        }
    }

    barrier(clk_local_mem_fence);

    r[2*gid] = b[2*lid];
    r[2*gid+1] = b[2*lid+1];
}

   it took me a little while to wrap my head around both steps of the
   algorithm, but the end code is pretty similar to the hillis steele.
   there are two loops instead of one, and the indexing is a bit tricky,
   but i think that comparing the code to the picture it should be
   straightforward. you can also find a cuda version in the nvidia
   [30]paper where i took the pictures from.

[31]parallel programming with opencl and python: parallel reduce

   16 jun 2014

   once you know how to use python to run opencl kernels on your device
   (read [32]part i and [33]part ii of this series) you need to start
   thinking about the programming patterns you will use. while many tasks
   are inherently parallel (like calculating the value of a function for n
   different values) and you can just straightforwardly run n copies on
   your processors, most interesting tasks involve dependencies in the
   data. for instance if you want to simply sum n numbers in the simplest
   possible way, the thread doing the summing needs to know about all n
   numbers, so you can only run one thread, leaving most of your cores
   unused. so what we need to come up with are clever ways to decompose
   the problem into individual parts which can be run in parallel, and
   then combine them all in the end.

   this is the strong point of [34]intro to parallel programming,
   available free. if you really want to learn about this topic in depth
   you should watch the whole course. here i will only show how i
   implemented some of the algorithms discussed there in opencl (the
   course is in cuda). i   ll discuss three algorithms: reduce, scan and
   histogram. they show how you can use some properties of mathematical
   operators to decompose a long operation into a series of many small,
   independent operations.

   reduce is the simplest of the three. let   s say you have to sum n
   numbers. on a serial computer, you   d create a temporary variable and
   add the value of each number to it in turn. it appears there is not
   much to parallelize here. the key is that addition is a binary
   associative operator. that means that $a + b + c + d = (a+b) + (c+d)$.
   so if i have two cores i can add the first two numbers on one, the last
   two on the other, and then sum those two intermediate results. you can
   convince yourself that we only need $\mathcal{o}(\log n)$ steps if we
   have enough parallel processing units; as opposed to $\mathcal{o}(n)$
   steps in the serial case. a reduction kernel in opencl is
   straightforward, assuming n is smaller than the number of cores in a
   single compute unit:
__kernel void reduce(__global float *a,
                     __global float *r,
                     __local float *b)
{
    uint gid = get_global_id(0);
    uint wid = get_group_id(0);
    uint lid = get_local_id(0);
    uint gs = get_local_size(0);

    b[lid] = a[gid];
    barrier(clk_local_mem_fence);

    for(uint s = gs/2; s &gt; 0; s &gt;&gt;= 1) {
        if(lid &lt; s) {
          b[lid] += b[lid+s];
        }
        barrier(clk_local_mem_fence);
    }
    if(lid == 0) r[wid] = b[lid];
}

   full code [35]here. first we copy the numbers into shared memory. to
   make sure we always access memory in a contiguous fashion, we then take
   the first n/2 numbers and add to them the other n/2, so now we have n/2
   numbers to add up. then we take half of those and add the next half,
   until there is only one number remaining. that   s the one we copy back
   to main memory.

   [36]reduction stages.

   reduction stages. image from [37]nvidia slides.

   if we have n bigger than the number of cores in a single unit, we need
   to call the kernel multiple times. each core will compute its final
   answer, and then we run reduce again on that array of answers until we
   have our final number. in the python code i [38]linked to, you can see
   how we enqueue the kernel twice, but with fewer threads the second
   time:
'''run kernels the first time for all n numbers, this will result
in n/n_threads numbers left to sum'''
evt = prg.reduce(queue, (n,), (n_threads,), a_buf, r_buf, loc_buf)
evt.wait()
print evt.profile.end - evt.profile.start

'''because i'd set n=n_threads*n_threads, there are n_threads numbers
left to sum, we sum those here, leaving 1 number'''
evt = prg.reduce(queue, (n_threads,), (n_threads,), r_buf, o_buf, loc_buf)
evt.wait()
print evt.profile.end - evt.profile.start

'''copy the scalar back to host memory'''
cl.enqueue_copy(queue, o, o_buf)

[39]parallel programming with opencl and python: vectors and concurrency

   25 may 2014

   [40]last time we saw how to run some simple code on the gpu. now let   s
   look at some particular aspects related to parallel programming we
   should be aware of. since gpus are massively parallel processors, you   d
   expect you could write your kernel code for a single data piece, and by
   running enough copies of the kernel you   d be maximizing your device   s
   performance. well, you   d be wrong! i   m going to focus on the three most
   obvious issues which could hamper your parallel code   s performance:
     * each of the individual cores is actually a vector processor, which
       means it can perform an operation on multiple numbers at a time.
     * at some point the individual threads might need to write to the
       same position in memory (i.e. to accumulate a value). to make sure
       the result is correct, they need to take turns doing it, which
       means they spend time waiting for each other doing nothing.
     * most code is limited by memory bandwidth, not compute performance.
       this means that the gpu can   t get the data to the processing cores
       as fast as they can actually perform the computation required.

   let   s look at the vector instructions first. let   s say you need to add
   a bunch of floating point numbers. you could have each core add one
   number at a time, but in reality you can do better. each processing
   core can actually perform that addition on multiple numbers at a time
   (currently it is common to process the equivalent of 4 floats at a
   time, or a 128 bit vector). you can determine the maximum size of a
   vector for each type by accessing the cl_device_preferred_vector_width_
   [property](http://documen.tician.de/pyopencl/runtime.html) for a
   device.

   to take advantage of this feature, opencl defines types such as float4,
   int2 etc with overloaded math operations. let   s take a look at a map
   operation implementing vectors:
__kernel void sq(__global float4 *a,
__global float *c)
{
    int gid = get_global_id(0);
    float4 pix = a[gid];
    c[gid] = .299f * pix.x + .587f * pix.y + .114f * pix.z;
}

   full code [41]here. a float4 just corresponds to a memory position with
   4 floats next to each other. the data structure has 4 fields: x,y,z,w,
   which are just pointers to each of those memory positions. so if you
   want to create an array of floats to be processed by this code, just
   pack each set of 4 floats you want to access together contiguously.
   interestingly, if you read float3 vectors from an array, opencl will
   still jump between every set of 4 floats, which means you lose one
   number and might result in nasty bugs. either you leave every 4th
   position unused, as i did here, or you start doing your own pointer
   arithmetic. a small warning: if you choose to load random memory
   positions your memory bandwidth might suffer, because it is faster to
   read 32 (or some multiple) contiguous bytes at a time (if you are
   interested in knowing more about this topic, google memory coalescing).

   this previous code was still just a map, where every computation result
   would be written to a different memory position. what if all threads
   want to write to the same memory position? opencl provides atomic
   operations, which make sure no other thread is writing to that memory
   position before doing it. let   s compare a naive summation to a memory
   position with atomic summation:
import pyopencl as cl
import numpy as np

n_threads = 100000
n = 10
a = np.zeros(10).astype(np.int32)

ctx = cl.create_some_context()
queue = cl.commandqueue(ctx, properties=cl.command_queue_properties.profiling_en
able)

mf = cl.mem_flags
a_buf = cl.buffer(ctx, mf.copy_host_ptr, hostbuf=a)

prg = cl.program(ctx, """
    __kernel void naive(__global int *a,
    int n)
    {
      int gid = get_global_id(0);
      int pos = gid % n;
      a[pos] = a[pos] + 1;
    }
    __kernel void atomics(__global int *a,
    int n)
    {
      int gid = get_global_id(0);
      int pos = gid % n;
      atomic_inc(a+pos);
    }
    """).build()

n_workers = (n_threads,)

naive_res = np.empty_like(a)
evt = prg.naive(queue, n_workers, none, a_buf, np.int32(n))
evt.wait()
print evt.profile.end - evt.profile.start
cl.enqueue_copy(queue, naive_res, a_buf)
print naive_res

a_buf = cl.buffer(ctx, mf.copy_host_ptr, hostbuf=a)
atomics_res = np.empty_like(a)
evt = prg.atomics(queue, n_workers, none, a_buf, np.int32(n))
evt.wait()
print evt.profile.end - evt.profile.start
cl.enqueue_copy(queue, atomics_res, a_buf)
print atomics_res

   the first kernel runs fast, but returns the wrong result. the second
   kernel is way slower, but is an order of magnitude slower, because
   threads had to wait for each other.
1582240
[25 25 25 25 25 25 25 25 25 25]
31392000
[10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]

   there is really no better way to do concurrent writes, so you   ll have
   to live with the slow down if you absolutely need to do them. but often
   you can restructure your algorithm in such a way as to minimize the
   number of concurrent writes you need to do, which will speed up your
   code. another way to avoid concurrency issues is to take advantage of
   the memory hierarchy in a gpu, which we   ll discuss next.

   like a cpu, there is a hierarchy of memory pools, with closer, faster
   pools being smaller and the off die,    slow memory    being relatively
   large (a few gbs as of now). in opencl, these different memory spaces
   have the following names: private memory refers to the registers in
   each core, and is initialized in the kernel code itself; shared memory
   refers to a cache in each processing unit (which is shared by all cores
   within a processing unit); and global memory refers to the off die ram
   (there are even other memory types, but let   s stick to the basics for
   now).

   [42]diagram of the memory hierarchy in a typical gpu. credit: amd.

   diagram of the memory hierarchy in a typical gpu. credit: [43]amd.

   in the previous code examples we   d always read and write from global
   memory, but every time we load something from there we will have to
   wait hundreds of clock cycles for it to be loaded. so it would be
   better if we   d load the initial data from global memory, used shared
   memory to store intermediate results and then store the final result
   back in global memory, to be read by the host. let   s look at a program
   which blurs an image by dividing it into blocks, loading each block
   into a compute unit   s shared memory and having each core blur one
   specific pixel.
__kernel void blur(__global uchar4 *c,
            __global uchar4 *res,
            __local uchar4 *c_loc,
            uint w, uint h)
{
        uint xg = get_global_id(0);
        uint yg = get_global_id(1);
        uint xl = get_local_id(0)+1;
        uint yl = get_local_id(1)+1;
        uint wm = get_local_size(0)+2;
        uint wl = get_local_size(0);
        uint hl = get_local_size(1);
        c_loc[xl+wm*yl] = c[xg+w*yg];
        uint left = clamp(xg-1, (uint)0, w);
        if(xl==1) c_loc[0+wm*yl] = c[left+w*yg];
        uint right = clamp(xg+1, (uint)0, w);
        if(xl==wl) c_loc[(wl+1)+wm*yl] = c[right+w*yg];
        uint top = clamp(yg-1, (uint)0, h);
        if(yl==1) c_loc[xl+wm*0] = c[xg+w*top];
        uint bot = clamp(yg+1, (uint)0, h);
        if(yl==hl) c_loc[xl+wm*(hl+1)] = c[xg+w*bot];
        barrier(clk_local_mem_fence);
        uchar4 blr = c_loc[xl+wm*(yl-1)]/(uchar)5 +
                  c_loc[xl-1+wm*yl]/(uchar)5 +
                  c_loc[xl+wm*yl]/(uchar)5 +
                  c_loc[xl+1+wm*yl]/(uchar)5 +
                  c_loc[xl+wm*(yl+1)]/(uchar)5;
        res[xg+w*yg] = blr;
}

   whoa that   s a lot of lines! but they are all super simple, so let   s
   look at it line by line. in the function declaration you can see
   the __local uchar4 pointer. that points to the shared memory we are
   going to use. unfortunately we cannot initialize it from the host (can
   only copy values to global memory) so we use lines 13-21 to read values
   from global memory into the local buffer, taking into account boundary
   conditions. in this code we distributed the threads in a 2d
   configuration so each thread has an id in both x and y dimensions
   (notice the argument for get_global_id and get_local_id denoting the
   dimension) so we can read off the x and y positions we want to process
   plus the block size directly in lines 6-12.

   once the block values have been copied to shared memory, we tell all
   the threads within one compute unit to wait for each other with line
   22. this makes sure that everyone has loaded their corresponding values
   into shared memory before the code continues. it   s important to do this
   because each thread will read many values in the following computation.
   line 23 is just the [44]laplace kernel which does the actual blurring.
   finally we write the value back into global memory.

   so how do we set up this code in pyopencl. i won   t reproduce the
   [45]full code here, so let   s just look at the few important lines:
n_local = (16,12)
nn_buf = cl.localmemory(4*(n_local[0]+2)*(n_local[1]+2))
n_workers = (cat.size[0], cat.size[1])

prg.blur(queue, n_workers, n_local, pix_buf, pixb_buf, nn_buf, np.uint32(cat.siz
e[0]), np.uint32(cat.size[1]))

   localmemory is how we tell pyopencl we are going to use some shared
   memory in our kernel: we need to tell it how many bytes to reserve for
   this particular buffer. in this particular case we need block width
   plus two for the boundaries multiplied by block height plus boundaries,
   times 4 bytes for the size of a uchar4. n_workers corresponds to a
   tuple with the picture   s width and height, which means we launch a
   thread for each pixel in the image, distributed in the aforementioned
   2d array. n_local corresponds to the local block size, i.e. how many
   threads will share a compute unit/shared memory. i encourage you to
   look at the full code and run it!

   next time i   ll cover some practical algorithms we can run on a gpu.

[46]parallel programming with opencl and python

   28 apr 2014

   in the next few posts i   ll cover my experiences with learning how to
   program efficient parallel programs on gpus using opencl. because the
   machine i got was a mac pro with the top of the line gpus (7 teraflops)
   i needed to use opencl, which is a bit complex and confusing at first
   glance. it also requires a lot of boilerplate code which makes it
   really hard to just jump in and start experimenting. i ultimately
   decided to use pyopencl, which allows us to do the boring boilerplate
   stuff in just a few lines of python and focus on the actual parallel
   programs (the kernels).

   first, a few pointers on what i read. a great introduction to the
   abstract concepts of parallel programming is the udacity
   course [47]introduction to parallel programming. they use c and cuda to
   illustrate the concepts, which means you can   t directly apply what you
   see there on a computer with a non nvidia gpu. to learn the opencl api
   itself, i used the book [48]opencl in action: how to accelerate
   graphics and computation. as for pyopencl, the [49]documentation is a
   great place to start. you can also find all the python code i used in
   [50]github.

   i assume you know the basics of how gpus work and what they are useful
   for. my intention is to    translate    the existing tutorials into
   pyopencl, which lets you start running code much sooner than any c
   based framework. additionally, because we are using opencl, we can run
   our simple code on most computers. to start with, let   s look at how to
   access the data structures which contain information about the
   available opencl devices on our computer:
import pyopencl as cl
plat = cl.get_platforms()
plat[0].get_devices()

   in a given computer, you can have different implementations of opencl
   (i.e. an amd driver,and an nvidia driver); these are known as
   platforms. usually you   ll only have one platform in your computer. a
   platform contains the devices it is responsible for, so by querying the
   platform data structure we can look at all the devices in our system.
   the mac pro shows the following list of available devices:
[&lt;pyopencl.device 'intel(r) xeon(r) cpu e5-2697 v2 @ 2.70ghz' on 'apple' at 0
xffffffff&gt;, &lt;pyopencl.device 'ati radeon hd - firepro d700 compute engine'
 on 'apple' at 0x1021c00&gt;, &lt;pyopencl.device 'ati radeon hd - firepro d700
compute engine' on 'apple' at 0x2021c00&gt;]

   to actually run something on these devices, we need to create a context
   to manage the queues of kernels which will be executed there. so say i
   want to run something on my first gpu. i   d create a context with:
import pyopencl as cl
plat = cl.get_platforms()
devices = plat[0].get_devices()
ctx = cl.context([devices[1]])
ctx.get_info(cl.context_info.devices)

   the final line queries the device associated with the context we just
   created:
[&lt;pyopencl.device 'ati radeon hd - firepro d700 compute engine' on 'apple' at
 0x1021c00&gt;]

   why would we need to query the devices in a context if we put the
   devices there in the first place? one reason is that you can create a
   context without bothering to look up any platforms or devices
   beforehand. just run
import pyopencl as cl
ctx = cl.create_some_context()

   and you   re done! when you run the script, a prompt will ask you for a
   specific device out of all possible devices, or you can set an
   environment variable to specify which one you want by default. in the
   following, i   ll always use this method to create a context, but if you
   want more control over which devices you choose, [51]this example is
   quite enlightening.

   now that we know how to access the devices, let   s take a look at how to
   run code there. i   ll start with a simple parallel pattern, the map. we
   are going to apply a function to each of the data points independently,
   which allows for maximal parallelism. here is the code:
import pyopencl as cl
import numpy as np

a = np.arange(32).astype(np.float32)
res = np.empty_like(a)

ctx = cl.create_some_context()
queue = cl.commandqueue(ctx)

mf = cl.mem_flags
a_buf = cl.buffer(ctx, mf.read_only | mf.copy_host_ptr, hostbuf=a)
dest_buf = cl.buffer(ctx, mf.write_only, res.nbytes)

prg = cl.program(ctx, """
    __kernel void sq(__global const float *a,
    __global float *c)
    {
      int gid = get_global_id(0);
      c[gid] = a[gid] * a[gid];
    }
    """).build()

prg.sq(queue, a.shape, none, a_buf, dest_buf)

cl.enqueue_copy(queue, res, dest_buf)

print a, res

   in line 7 we create the context, as before. then, we create a queue in
   line 8, which is what schedules the kernels to run on the device. now
   let   s skip a few lines and look at the actual opencl code, on lines
   14-21. you can see that the opencl code itself is in the c programming
   language, and is passed to the program object as a string. in a real
   project we should write this code in a .cl file separate from the
   python project and have the code be read from there, but for these
   simple examples i   ll leave the code as a string. once the program
   object is initialized with some code, we call its build method to
   compile it to a binary native to the gpu.

   you can see from the kernel   s signature that it expects to receive
   pointers to two memory locations. these point to the gpu   s main memory,
   and must be initialized before running the kernel. that   s what lines
   10-12 are for: they let pyopencl know that two blocks of memory must be
   initialized on the gpu before the kernel is run, and if necessary, what
   values should be copied to that memory (the hostbuf parameter,
   which points to the source array on the host   s main memory). the memory
   is actually only allocated / copied when the kernel actually reaches
   the top of the queue, and before it runs.

   we add the kernel to the queue in line 23, telling pyopencl which queue
   to add it to first; then how many instances of the kernel will be run
   (we want to spawn many instances to take advantage of the parallel
   nature of the gpu) and how they will be distributed (the none
   parameter, we   ll cover this in a later post);  and finally the
   parameters which should be passed (the memory locations). finally in
   line 25 we copy the values from the memory at res back to an array in
   the host memory. if we did this in c we would have needed 100+ lines of
   code by now so i   m really happy pyopencl exists.

   finally let   s look at the kernel code itself, the actual opencl code
   and see what it does:
__kernel void sq(__global const float *a, __global float *c)
{
    int gid = get_global_id(0);
    c[gid] = a[gid] * a[gid];
}

   each of the tiny processors on the gpu will run a copy of this code.
   first we access each thread   s unique global id. this will allow the
   processor to identify which piece of memory it should work on. then, it
   loads the value from the a array and squares it, storing it in the
   correct position in the c array. simple! next time we   ll look at some
   more advanced operations we can perform on data.
   [52]older newer

references

   1. https://tmramalho.github.io/atom.xml
   2. https://tmramalho.github.io/
   3. https://tmramalho.github.io/about/
   4. https://tmramalho.github.io/archive/
   5. https://tmramalho.github.io/projects/
   6. https://tmramalho.github.io/tags/
   7. mailto:tiago@nehalemlabs.net
   8. https://github.com/tmramalho
   9. https://www.twitter.com/tmramalho
  10. https://tmramalho.github.io/
  11. https://blog/2015/04/13/dimensionality-reduction-101-linear-algebra-hidden-variables-and-generative-models/
  12. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.factoranalysis.html
  13. https://en.wikipedia.org/wiki/likelihood_function
  14. https://en.wikipedia.org/wiki/expectation   maximization_algorithm
  15. https://en.wikipedia.org/wiki/iris_flower_data_set
  16. http://www.miketipping.com/papers/met-mppca.pdf
  17. https://en.wikipedia.org/wiki/principal_component_analysis
  18. https://en.wikipedia.org/wiki/exponential_distribution
  19. https://en.wikipedia.org/wiki/id173_(mathematics)
  20. http://web.stanford.edu/~hastie/papers/spc_jcgs.pdf
  21. https://gist.github.com/tmramalho/ea938b7958d2803227f5
  22. https://blog/2014/06/23/parallel-programming-with-opencl-and-python-parallel-scan/
  23. https://tmramalho.github.io/blog/2014/05/25/parallel-programming-with-opencl-and-python-parallel-reduce/
  24. https://en.wikipedia.org/wiki/prefix_sum
  25. https://tmramalho.github.io/images/2014/06/hs1.png
  26. http://developer.download.nvidia.com/compute/cuda/1.1-beta/x86_website/projects/scan/doc/scan.pdf
  27. https://tmramalho.github.io/images/2014/06/bl1.png
  28. http://developer.download.nvidia.com/compute/cuda/1.1-beta/x86_website/projects/scan/doc/scan.pdf
  29. https://tmramalho.github.io/images/2014/06/bl2.png
  30. http://developer.download.nvidia.com/compute/cuda/1.1-beta/x86_website/projects/scan/doc/scan.pdf
  31. https://blog/2014/06/16/parallel-programming-with-opencl-and-python-parallel-reduce/
  32. https://tmramalho.github.io/blog/2014/04/28/parallel-programming-with-opencl-and-python/
  33. https://tmramalho.github.io/blog/2014/05/25/parallel-programming-with-opencl-and-python-vectors-and-concurrency/
  34. https://www.udacity.com/course/cs344
  35. https://github.com/tmramalho/easy-pyopencl/blob/master/008_localreduce.py
  36. https://tmramalho.github.io/images/2014/06/reduction.png
  37. http://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
  38. https://github.com/tmramalho/easy-pyopencl/blob/master/008_localreduce.py
  39. https://blog/2014/05/25/parallel-programming-with-opencl-and-python-vectors-and-concurrency/
  40. https://tmramalho.github.io/blog/2014/04/28/parallel-programming-with-opencl-and-python/
  41. https://github.com/tmramalho/easy-pyopencl/blob/master/002_gray.py
  42. https://tmramalho.github.io/images/2014/05/fig1.png
  43. http://developer.amd.com/resources/documentation-articles/articles-whitepapers/opencl-optimization-case-study-fast-fourier-transform-part-ii/
  44. https://en.wikipedia.org/wiki/discrete_laplace_operator
  45. https://github.com/tmramalho/easy-pyopencl/blob/master/006_fasterblur.py
  46. https://blog/2014/04/28/parallel-programming-with-opencl-and-python/
  47. https://www.udacity.com/course/cs344
  48. http://www.amazon.com/opencl-action-accelerate-graphics-computation/dp/1617290173/
  49. http://documen.tician.de/pyopencl/
  50. https://github.com/tmramalho/easy-pyopencl
  51. https://github.com/pyopencl/pyopencl/blob/master/examples/benchmark.py
  52. https://tmramalho.github.io/page2
