   [1]sebastianraschka

   [2]about [3]blog [4]books [5]elsewhere [6]resources [7]publications
   [8]teaching [9]software

machine learning faq

   it is always a pleasure to engage in discussions about machine
   learning. below, i collected some of the most frequently asked
   questions that i answered via email or other social network platforms
   in hope that these are useful to others!

     the only thing to do with good advice is to pass it on. it is never
     of any use to oneself.
         oscar wilde

general questions about machine learning and    data science   

     * [10]what are machine learning and data science?
     * [11]why do you and other people sometimes implement machine
       learning algorithms from scratch?
     * [12]what learning path/discipline in data science i should focus
       on?
     * [13]at what point should one start contributing to open source?
     * [14]how important do you think having a mentor is to the learning
       process?
     * [15]where are the best online communities centered around data
       science/machine learning or python?
     * [16]how would you explain machine learning to a software engineer?
     * [17]how would your curriculum for a machine learning beginner look
       like?
     * [18]what is the definition of data science?
     * [19]how do data scientists perform model selection? is it different
       from kaggle?

questions about the machine learning field

     * [20]how are artificial intelligence and machine learning related?
     * [21]what are some real-world examples of applications of machine
       learning in the field?
     * [22]what are the different fields of study in data mining?
     * [23]what are differences in research nature between the two fields:
       machine learning & data mining?
     * [24]how do i know if the problem is solvable through machine
       learning?
     * [25]what are the origins of machine learning?
     * [26]how was classification, as a learning machine, developed?
     * [27]which machine learning algorithms can be considered as among
       the best?
     * [28]what are the broad categories of classifiers?
     * [29]what is the difference between a classifier and a model?
     * [30]what is the difference between a parametric learning algorithm
       and a nonparametric learning algorithm?
     * [31]what is the difference between a cost function and a loss
       function in machine learning?

questions about machine learning concepts and statistics

id180

     * [32]why is the relu function not differentiable at x=0?
     * [33]what is the derivative of the logistic sigmoid function?

cost functions and optimization

     * [34]what is the derivative of the mean squared error?
     * [35]how to compute gradients with id26 for arbitrary
       loss and id180?
     * [36]fitting a model via closed-form equations vs. id119
       vs stochastic id119 vs mini-batch learning     what is the
       difference?
     * [37]how do you derive the id119 rule for linear
       regression and adaline?

regression analysis

     * [38]what is the difference between pearson r and simple linear
       regression?

tree models

     * [39]how does the id79 model work? how is it different from
       id112 and boosting in ensemble models?
     * [40]what are the disadvantages of using classic decision tree
       algorithm for a large dataset?
     * [41]why are implementations of decision tree algorithms usually
       binary, and what are the advantages of the different impurity
       metrics?
     * [42]why are we growing id90 via id178 instead of the
       classification error?
     * [43]when can a id79 perform terribly?
     * [44]does id79 select a subset of features for every tree
       or every node?

model evaluation

     * [45]what is overfitting?
     * [46]how can i avoid overfitting?
     * [47]is it always better to have the largest possible number of
       folds when performing cross validation?
     * [48]when training an id166 classifier, is it better to have a large
       or small number of support vectors?
     * [49]how do i evaluate a model?
     * [50]what is the best validation metric for multi-class
       classification?
     * [51]what factors should i consider when choosing a predictive model
       technique?
     * [52]what are the best toy datasets to help visualize and understand
       classifier behavior?
     * [53]how do i select id166 kernels?
     * [54]interlude: comparing and computing performance metrics in
       cross-validation     imbalanced class problems and 3 different ways
       to compute the f1 score

id28

     * [55]what is softmax regression and how is it related to logistic
       regression?
     * [56]why is id28 considered a linear model?
     * [57]what is the probabilistic interpretation of regularized
       id28?
     * [58]does id173 in id28 always results in
       better fit and better generalization?
     * [59]what is the major difference between naive bayes and logistic
       regression?
     * [60]what exactly is the    softmax and the multinomial logistic loss   
       in the context of machine learning?
     * [61]what is the relation between id28 and neural
       networks and when to use which?
     * [62]id28: why sigmoid function?
     * [63]is there an analytical solution to id28 similar
       to the normal equation for id75?

neural networks and deep learning

     * [64]what is the difference between deep learning and usual machine
       learning?
     * [65]can you give a visual explanation for the back propagation
       algorithm for neural networks?
     * [66]why did it take so long for deep networks to be invented?
     * [67]what are some good books/papers for learning deep learning?
     * [68]why are there so many deep learning libraries?
     * [69]why do some people hate neural networks/deep learning?
     * [70]how can i know if deep learning works better for a specific
       problem than id166 or id79?
     * [71]what is wrong when my neural network   s error increases?
     * [72]how do i debug an id158 algorithm?
     * [73]what is the difference between a id88, adaline, and
       neural network model?
     * [74]what is the basic idea behind the dropout technique?
     * [75]is dropout applied before or after the non-linear activation
       function

other algorithms for supervised learning

     * [76]why is nearest neighbor a lazy algorithm?

unsupervised learning

     * [77]what are some of the issues with id91?

semi-supervised learning

     * [78]what are the advantages of semi-supervised learning over
       supervised and unsupervised learning?

ensemble methods

     * [79]is combining classifiers with stacking better than selecting
       the best one?

preprocessing, feature selection and extraction

     * [80]why do we need to re-use training parameters to transform test
       data?
     * [81]what are the different id84 methods in
       machine learning?
     * [82]what is the difference between lda and pca for dimensionality
       reduction?
     * [83]when should i apply data id172/standardization?
     * [84]does mean centering or feature scaling affect a principal
       component analysis?
     * [85]how do you attack a machine learning problem with a large
       number of features?
     * [86]what are some common approaches for dealing with missing data?
     * [87]what is the difference between filter, wrapper, and embedded
       methods for feature selection?
     * [88]should data preparation/pre-processing step be considered one
       part of feature engineering? why or why not?
     * [89]is a bag of words feature representation for text
       classification considered as a sparse matrix?

naive bayes

     * [90]why is the naive bayes classifier naive?
     * [91]what is the decision boundary for naive bayes?
     * [92]is it possible to mix different variable types in naive bayes,
       for example, binary and continues features?

other

     * [93]what is euclidean distance in terms of machine learning?
     * [94]when should one use median, as opposed to the mean or average?

programming languages and libraries for data science and machine learning

     * [95]is r used extensively today in data science?
     * [96]what is the main difference between tensorflow and
       scikit-learn?

   [97]q

      2013-2019 sebastian raschka

references

   visible links
   1. https://sebastianraschka.com/
   2. https://sebastianraschka.com/about.html
   3. https://sebastianraschka.com/blog/index.html
   4. https://sebastianraschka.com/books.html
   5. https://sebastianraschka.com/elsewhere.html
   6. https://sebastianraschka.com/resources.html
   7. http://stat.wisc.edu/~sraschka/publications/
   8. http://stat.wisc.edu/~sraschka/teaching
   9. http://stat.wisc.edu/~sraschka/software
  10. https://sebastianraschka.com/faq/docs/datascience-ml.html
  11. https://sebastianraschka.com/faq/docs/implementing-from-scratch.html
  12. https://sebastianraschka.com/faq/docs/data-science-career.html
  13. https://sebastianraschka.com/faq/docs/open-source.html
  14. https://sebastianraschka.com/faq/docs/mentor.html
  15. https://sebastianraschka.com/faq/docs/ml-python-communities.html
  16. https://sebastianraschka.com/faq/docs/ml-to-a-programmer.html
  17. https://sebastianraschka.com/faq/docs/ml-curriculum.html
  18. https://sebastianraschka.com/faq/docs/definition_data-science.html
  19. https://sebastianraschka.com/faq/docs/model-selection-in-datascience.html
  20. https://sebastianraschka.com/faq/docs/ai-and-ml.html
  21. https://sebastianraschka.com/faq/docs/ml-examples.html
  22. https://sebastianraschka.com/faq/docs/datamining-overview.html
  23. https://sebastianraschka.com/faq/docs/datamining-vs-ml.html
  24. https://sebastianraschka.com/faq/docs/ml-solvable.html
  25. https://sebastianraschka.com/faq/docs/ml-origins.html
  26. https://sebastianraschka.com/faq/docs/classifier-history.html
  27. https://sebastianraschka.com/faq/docs/best-ml-algo.html
  28. https://sebastianraschka.com/faq/docs/classifier-categories.html
  29. https://sebastianraschka.com/faq/docs/difference_classifier_model.html
  30. https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html
  31. https://sebastianraschka.com/faq/docs/cost-vs-loss.html
  32. https://sebastianraschka.com/faq/docs/relu-derivative.html
  33. https://sebastianraschka.com/faq/docs/logistic-sigmoid-derivative.html
  34. https://sebastianraschka.com/faq/docs/mse-derivative.html
  35. https://sebastianraschka.com/faq/docs/backprop-arbitrary.html
  36. https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html
  37. https://sebastianraschka.com/faq/docs/linear-gradient-derivative.html
  38. https://sebastianraschka.com/faq/docs/pearson-r-vs-linear-regr.html
  39. https://sebastianraschka.com/faq/docs/id112-boosting-rf.html
  40. https://sebastianraschka.com/faq/docs/decision-tree-disadvantages.html
  41. https://sebastianraschka.com/faq/docs/decision-tree-binary.html
  42. https://sebastianraschka.com/faq/docs/decisiontree-error-vs-id178.html
  43. https://sebastianraschka.com/faq/docs/random-forest-perform-terribly.html
  44. https://sebastianraschka.com/faq/docs/random-forest-feature-subsets.html
  45. https://sebastianraschka.com/faq/docs/overfitting.html
  46. https://sebastianraschka.com/faq/docs/avoid-overfitting.html
  47. https://sebastianraschka.com/faq/docs/number-of-kfolds.html
  48. https://sebastianraschka.com/faq/docs/num-support-vectors.html
  49. https://sebastianraschka.com/faq/docs/evaluate-a-model.html
  50. https://sebastianraschka.com/faq/docs/multiclass-metric.html
  51. https://sebastianraschka.com/faq/docs/choosing-technique.html
  52. https://sebastianraschka.com/faq/docs/clf-behavior-data.html
  53. https://sebastianraschka.com/faq/docs/select_id166_kernels.html
  54. https://sebastianraschka.com/faq/docs/computing-the-f1-score.html
  55. https://sebastianraschka.com/faq/docs/softmax_regression.html
  56. https://sebastianraschka.com/faq/docs/logistic_regression_linear.html
  57. https://sebastianraschka.com/faq/docs/probablistic-logistic-regression.html
  58. https://sebastianraschka.com/faq/docs/regularized-logistic-regression-performance.html
  59. https://sebastianraschka.com/faq/docs/naive-bayes-vs-logistic-regression.html
  60. https://sebastianraschka.com/faq/docs/softmax.html
  61. https://sebastianraschka.com/faq/docs/logisticregr-neuralnet.html
  62. https://sebastianraschka.com/faq/docs/logistic-why-sigmoid.html
  63. https://sebastianraschka.com/faq/docs/logistic-analytical.html
  64. https://sebastianraschka.com/faq/docs/difference-deep-and-normal-learning.html
  65. https://sebastianraschka.com/faq/docs/visual-id26.html
  66. https://sebastianraschka.com/faq/docs/inventing-deeplearning.html
  67. https://sebastianraschka.com/faq/docs/deep-learning-resources.html
  68. https://sebastianraschka.com/faq/docs/many-deeplearning-libs.html
  69. https://sebastianraschka.com/faq/docs/deeplearning-criticism.html
  70. https://sebastianraschka.com/faq/docs/deeplearn-vs-id166-randomforest.html
  71. https://sebastianraschka.com/faq/docs/neuralnet-error.html
  72. https://sebastianraschka.com/faq/docs/nnet-debugging-checklist.html
  73. https://sebastianraschka.com/faq/docs/diff-id88-adaline-neuralnet.html
  74. https://sebastianraschka.com/faq/docs/dropout.html
  75. https://sebastianraschka.com/faq/docs/dropout-activation.html
  76. https://sebastianraschka.com/faq/docs/lazy-knn.html
  77. https://sebastianraschka.com/faq/docs/issues-with-id91.html
  78. https://sebastianraschka.com/faq/docs/semi-vs-supervised.html
  79. https://sebastianraschka.com/faq/docs/logistic-boosting.html
  80. https://sebastianraschka.com/faq/docs/scale-training-test.html
  81. https://sebastianraschka.com/faq/docs/dimensionality-reduction.html
  82. https://sebastianraschka.com/faq/docs/lda-vs-pca.html
  83. https://sebastianraschka.com/faq/docs/when-to-standardize.html
  84. https://sebastianraschka.com/faq/docs/pca-scaling.html
  85. https://sebastianraschka.com/faq/docs/large-num-features.html
  86. https://sebastianraschka.com/faq/docs/missing-data.html
  87. https://sebastianraschka.com/faq/docs/feature_sele_categories.html
  88. https://sebastianraschka.com/faq/docs/dataprep-vs-dataengin.html
  89. https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html
  90. https://sebastianraschka.com/faq/docs/naive-naive-bayes.html
  91. https://sebastianraschka.com/faq/docs/naive-bayes-boundary.html
  92. https://sebastianraschka.com/faq/docs/naive-bayes-vartypes.html
  93. https://sebastianraschka.com/faq/docs/euclidean-distance.html
  94. https://sebastianraschka.com/faq/docs/median-vs-mean.html
  95. https://sebastianraschka.com/faq/docs/r-in-datascience.html
  96. https://sebastianraschka.com/faq/docs/tensorflow-vs-scikitlearn.html
  97. https://www.quora.com/profile/sebastian-raschka-1

   hidden links:
  99. https://sebastianraschka.com/faq/index.html
 100. https://sebastianraschka.com/email.html
 101. https://twitter.com/rasbt
 102. https://github.com/rasbt
 103. https://scholar.google.com/citations?user=x4rcc0iaaaaj&hl=enrasbt
 104. https://linkedin.com/in/sebastianraschka
