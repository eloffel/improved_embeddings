   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

advances in nlp in 2017

   [9]go to the profile of valentin malykh
   [10]valentin malykh (button) blockedunblock (button) followfollowing
   dec 22, 2017

   disclaimer: first of all i need to say that all these trends and
   advances are only my point of view, other people could have other
   perspectives.

trends

   in 2017 a.d. i could spot two main trends:
     * faster, more parallel         much of attention of the community was
       pointed at doing things faster, more parallel as a way to achieve
       speedup.
     * unsupervised         unsupervised approaches are very common in computer
       vision, but comparatively rare in natural language processing (only
       id97 could be mentioned from major ideas in recent years). this
       year is marked by increased usage of unsupervised training.

   this article is mostly about first one, for the second trend see the
   [11]consecutive article. it also features speedup, which seems to be
   inevitable nowadays anyway.

attention is all you need

   [0*plm2xpxx4tppwueq.]
   transformer architecture

   [12]this already famous paper marked the second coming of feed-forward
   networks to nlp. this work is from google and some famous researchers
   like jakob uszkoreit and   ukasz kaiser. the idea behind transformer
   architecture featured in article is simple and brilliant: let   s forget
   about recurrency and all that stuff and just try to use attention to do
   the job. and this idea has actually worked!

   but first lets remember that all current state of the art neural
   machine translation architectures work on recurrent networks. these
   networks intuitively are really suit for natural language processing
   tasks like machine translation, since they have explicit memory they
   keep during id136. this feature has the obvious bright side, but it
   also has accompanying dark side: since we keep the memory of what has
   been done before, we need to process the data only in that particular
   consecutive order. as a result the processing of the whole data is slow
   (e.g. in comparison to id98s), and this exact issue address the authors.

   hence the transformer architecture is feed-forward without any
   recurrency. here, what they use instead to do the whole job is
   attention.
   [1*waraj3qukb3yjcixgkkdrq.png]
   attention used in the paper.

   let   s first refresh the standard bahdanau   s approach to attention:
   [1*er-rbsdqjvq2l1ykdwxvgq.png]
   attention mechanism from [13]id4 by jointly
   learning to align and translate

   the idea of attention is that we need to focus on some relevant input
   in encoder to do better decoding. in the simplest case the relevance is
   defined as similarity of specific input to current output. this
   similarity in its turn could be defined as sum of some inputs with
   weights, where the weights are summing up to 1, and the biggest weight
   is corresponding to most relevant input.

   in the figure, we could see the already classic dzmitry bahdanau   s
   approach: we have one input         the hidden states of encoder (h   s) and
   some coefficient to sum this hidden states with (a   s). these
   coefficients are not preset, they are generated from some other input
   different from encoder hidden states.

   in contrast the authors of paper in question suggested so-called
   self-attention on the input data. the term    self    in the name refers to
   idea that attention is applied to the data on which is it being
   computed, in contrast to the standard approach where one uses some
   additional input to produce attention on the given input.

   furthermore this self-attention is named multi-head since it makes the
   same operation multiple times in parallel. this feature could be
   compared to convolutional filters if you   re looking for analog, i.e.
   each head has been focusing on different places in input. the other
   main feature of the attention from the paper is usage of three inputs
   (instead of two in standard approach). as you can see in the figure, at
   first we compute    sub-attention    on q (query) and k (key) and after
   that combining this with v (value) from the input. this feature refers
   us to the notion of memory, which an attention actually is.

   aside the main feature of this architecture there are two secondary,
   yet significant features:
     * positional encoding,
     * masked attention for decoder.

   positional encoding         as we remember, whole architecture of the model
   is feed-forward, so there is no notion of sequence inside the network.
   to inject the knowledge of time sequences in the network the positional
   encoding was proposed. for me the usage of trigonometric functions
   (sines & cosines), which form position of word in the document, isn   t
   that obvious in this capacity, but it is working: this embedding in
   combination with actual id27 (e.g. above mentioned id97)
   brings the sense of a word and its relative position to our network.

   masked attention         simple yet important feature: again, since there is
   no notion of sequence inside the network, we need to somehow filter the
   propositions of network for future words which are actually unavailable
   when we do the decoding. so as you may have spotted on the picture of
   attention, there is a place for mask, which figuratively speaking
   crosses out the words which position if in the future to the current
   one.

   all these features allowed this architecture to not only work, but even
   improve the state of the art in machine translation.

parallel decoder for id4

   the latter feature was unsatisfying for the authors of this [14]paper,
   written by richard socher   s group from salesforce research. so this
   masked attention for decoder was just not good enough for them in terms
   of speedup got from the parallel encoder, and they decided to take the
   next step:    why can   t we make a parallel decoder, if we already have a
   parallel encoder?    that is only my speculation, but i bet the authors
   had a similar question in their minds. and they have found a way to
   solve the issue.
   [0*lsftof0lvpqjdzgg.]
   non-autoregressive decoding

   they called it non-autoregressive decoding and the whole architecture
   non-autoregressive transformer, which means, that now not a single word
   is dependent on another one. this is an exaggeration, but not that big,
   after all. the idea here is that the encoder in this architecture
   produces so-called fertility rate for each word it sees. this fertility
   rate is used to generate the actual translation for each word, based
   only on word itself. this could be thought of like we have a standard
   alignment matrix for machine translation:
   [1*qy6rvwh7mo9omcvd76vxma.png]
   picture from [15]id4 by jointly learning to
   align and translate

   as you can see, some of the words could refer to more than one word,
   and some seem to not refer to any word in particular. thus the
   fertility rate just slices this matrix into pieces where each piece is
   for specific word in the source language.

   therefore we have the fertility, but this is not enough for the wholly
   parallel decoding. as you can see we need some more attention
   layers         positional attention (which refers us again to positional
   encoding) and inter-attention, which replaced masked attention from
   original transformer.

   unfortunately, giving such a boost in speed (up to 8x in some cases),
   the non-autoregressive decoder takes a few points of id7 in return. so
   there is room for improvement!

   in the [16]next part we   ll discuss other important works, considering
   unsupervised approaches, in the first place.

   thanks to [17]varvara logacheva.
     * [18]2017
     * [19]machine learning
     * [20]deep learning
     * [21]naturallanguageprocessing
     * [22]natural language

   (button)
   (button)
   (button) 227 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of valentin malykh

[24]valentin malykh

   ai research at deep learning lab @ mipt

     * (button)
       (button) 227
     * (button)
     *
     *

   [25]go to the profile of valentin malykh
   never miss a story from valentin malykh, when you sign up for medium.
   [26]learn more
   never miss a story from valentin malykh
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b00e927fcc57
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@madrugado/advances-in-nlp-in-2017-b00e927fcc57&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@madrugado/advances-in-nlp-in-2017-b00e927fcc57&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@madrugado?source=post_header_lockup
  10. https://medium.com/@madrugado
  11. https://medium.com/@madrugado/advances-in-nlp-in-2017-part-ii-d8da391a3f01
  12. https://arxiv.org/abs/1706.03762
  13. https://arxiv.org/abs/1409.0473
  14. https://einstein.ai/static/images/pages/research/non-autoregressive-neural-machine-translation/non-autoregressive-neural-mt.pdf
  15. http://arxiv.org/abs/1409.0473
  16. https://medium.com/@madrugado/advances-in-nlp-in-2017-part-ii-d8da391a3f01
  17. https://medium.com/@varvara.logacheva?source=post_page
  18. https://medium.com/tag/2017?source=post
  19. https://medium.com/tag/machine-learning?source=post
  20. https://medium.com/tag/deep-learning?source=post
  21. https://medium.com/tag/naturallanguageprocessing?source=post
  22. https://medium.com/tag/natural-language?source=post
  23. https://medium.com/@madrugado?source=footer_card
  24. https://medium.com/@madrugado
  25. https://medium.com/@madrugado
  26. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  28. https://medium.com/p/b00e927fcc57/share/twitter
  29. https://medium.com/p/b00e927fcc57/share/facebook
  30. https://medium.com/p/b00e927fcc57/share/twitter
  31. https://medium.com/p/b00e927fcc57/share/facebook
