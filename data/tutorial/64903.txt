   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

advances in nlp in 2017 (part ii)

   [9]go to the profile of valentin malykh
   [10]valentin malykh (button) blockedunblock (button) followfollowing
   jan 19, 2018

   the first part is devoted to the main fresh idea in our
   field         transformer. if you are interested in feed-forward networks
   disrupting id56s    monopoly, welcome to the [11]first part.

unsupervised machine translation

   the next part of this article is devoted to task which seemed
   impossible even a few years back: unsupervised mt. the papers which
   will be discussed are:
     * [12]unsupervised id4
     * [13]unsupervised machine translation using monolingual corpora only
     * [14]style-transfer from non-parallel text by cross-alignment

   the last paper seems to not belong here. but as they say         do not judge
   book by its cover. all three papers have the same main idea behind. in
   a nutshell it could be expressed like this: we have two auto-encoders
   for different text sources (i.e. different languages, or texts of
   different styles) and we just switch the decoder parts for them. how
   does it work? let   s try to sort it out.
   [1*gxqjiid122k0hxxltyz52dfg.png]
   auto-encoder (left); encoder-decoder (right)

   the auto-encoder is an encoder-decoder where decoder decodes to
   original space. this means that input and output belong to the same
   language (or style). so if we have some text, we train an encoder to
   produce a vector containing all the info needed by decoder to
   reconstruct the original sentence. in the ideal case the reconstructed
   sentence will be just the same. but in many cases it is not exactly so.
   we need to somehow measure the similarity between the source and
   reconstructed sentences. and the machine translation field has an
   answer for such a challenge. there is a standard formula which is used
   to measure the similarity, it is called id7.
     * id7         the id7 score measures how many words and ngrams (n
       consecutive words) overlap in a given translation and a reference
       translation. the most commonly used id7 version is id7-4, which
       considers words, bigrams, trigrams and 4-grams. it also uses a
       penalty for too short translations.

   as you may guess it is not differentiable, so some other way to train
   our translator is needed. for auto-encoder this could be a standard
   cross-id178, but it   s not enough for translation. let   s skip this for
   now and go on.

   ok, now we have a tool to build our auto-encoder. the next thing which
   we need to do is to train two of them: one for source language (style),
   and another one for target language. and also we also need them crossed
   over: make the decoder for the source language restore encoded target
   strings, and vice versa.
   [1*yqyh2wyocz_rsfro9lmxhq.png]
   two auto-encoders and crossover

   here is the tricky part: in auto-encoder (or any encoder-decoder) in
   the middle we have a so-called hidden representation         a vector in some
   high-dimensional space. and if we want two our auto-encoders to be
   compatible, we need their hidden representations to be in the same
   space. how it is reachable? through some additional loss for these two
   auto-encoders. this loss comes from a discriminator, which in its turn
   refers us to gans.
     * gan         the generative adversarial network. the idea of gan could be
       expressed as    a network is playing with itself and trying to
       deceive   . there are three main components in gans: generator         to
       produce representations of some input in a way that such
       representations resembles ground truth as much as possible, golden
       source         to produce ground truth, and discriminator         to tell where
       its input comes from: generator or golden source, and we    punish   
       generator if it can. vise versa, we    punish    discriminator if it
       can   t, so both generator and discriminator are getting better
       during the training.

   in our case discriminator (l_adv on the figure) has to tell where the
   input comes from    target or source language (for machine translation),
   target or source style (for style transfer task). on the figure above
   we can see two auto-encoders represented as separate blocks         encoders
   and decoders. they have a link between them in the middle, where the
   discriminator is placed. training two auto-encoders in parallel with
   such additional loss leads the model to making hidden representations
   in both encoders similar (upper part of the figure), so the rest is
   clear         just replace the original decoder with its counterpart from the
   neighbouring auto-encoder (lower part of the figure) and enjoy your
   model translating from one language to another.

   all three papers have this idea behind the scene, with specific details
   in each case. the explanation above comes mostly from [15]unsupervised
   machine translation using monolingual corpora only, so i should also
   mention the previous work of the same authors, since its results are
   used in their work in question:
     * [16]word translation without parallel data

   the idea of this work is simple and brilliant:
   [1*q0r7exngr5gssnpnxoww3w.png]
   two vector spaces are pulled on each other

   let   s say that we have id27s for two different languages.
   (suppose that we work with texts from the same domain, say news or
   fiction books.) we can assume that the vocabularies of news corpora in
   a pair of languages are close: for the majority of words in the source
   corpus we can find their translations in the target news corpus         like
   president, taxation or ecology will be definitely presented in the news
   on both languages. so why we can   t we just juxtapose these words and
   pull one vector space on another? and we actually can. even more, we
   can find a function to transform the whole space (and the dots i.e.
   vectors for words in it) to some other space, where these dots will be
   placed on dots in another space. in this work the authors show that
   this could be done in an unsupervised manner which means we have no
   need in explicit dictionary.

   the [17]style-transfer from non-parallel text by cross-alignment paper
   is placed in this section since languages could be treated just like
   different styles of text and authors mention it in the paper. also this
   work is interesting because they have their [18]code published.

controllable text generation

   this section is really close to the previous one, but still a little
   bit different. the works in question here:
     * [19]style transfer in text: exploration and evaluation
     * [20]toward controlled generation of text

   in the first paper we have a different approach to style transfer which
   is closer to controllable generation, so it is placed here instead of
   previous section where belongs its double. the idea of controllable
   generation could be illustrated by this figure:
   [1*j194mi9d2v74lkafhjc8kw.png]
   controllable text generation

   here we see again the auto-encoder on text, but with additional
   information: the hidden representation of input (here it will be sense)
   is enriched by additional feature vector. this feature vector is
   encoding some specific properties of text, e.g. sentiment or grammar
   tense.

   as you can see on the picture, we also have a discriminator in addition
   to auto-encoder. there could be even more than one discriminators if we
   have multiple properties encoded at once. so at the end we have a
   composite id168         the reconstruction loss from auto-encoder and
   an additional loss for specified text features. therefore,
   reconstruction loss here has somewhat different meaning         it represents
   only sense of a sentence, not its features we force to be explicit.

simple recurrent unit

   and the last but not least section. it is devoted again to speed of
   computation. despite the fact that in the [21]first part we discussed
   ground-breaking newcomers from fully-connected nets, all the field to
   this day works on recurrent networks. and another well-known fact is
   that id56s are much slower than id98s. or aren   t they? to answer this
   question, let   s explore this paper:
     * [22]training id56s as fast as id98s

   i think that authors of this work tried to answer the question: why
   id56s are so slow? what makes them be like that? and they found the key:
   id56s are sequential, this is in their nature. but what if we could do
   as little of sequentionality as possible? let   s say that (almost)
   everything is independent of its previous state. then we could process
   all the inputs in parallel. so our task is to throw out all unneeded
   dependencies on previous states. and that is where it comes to:
   [1*we1947u5het1prtnywzmmg.png]
   sru equations

   as you can see, only two equations depend on previous state. and this
   equations work with vectors, not the matrices. all the heavy
   mathematics could be done independently in parallel. and at the end we
   just add few multiplications to handle the sequential nature of data.
   this setup proves to be great, check it out yourself:
   [1*-3ntvi3ahcfe1dw65vdleq.png]
   sru testing results

   the simple recurrent unit (sru) speed is approaching that of id98!

conclusion

   we can see that in 2017 the field has its own disruptors, like
   transformer, and breakthroughs like unsupervised machine translation,
   and also         the for common good the fast id56s (which are faster than
   fasterid56s, it you know what i mean). i   m looking into 2018 with
   aspiration of a new breakthroughs and advances in ways i still cannot
   imagine.

   thanks to [23]varvara logacheva.
     * [24]machine learning
     * [25]naturallanguageprocessing
     * [26]natural language
     * [27]deep learning
     * [28]machine translation

   (button)
   (button)
   (button) 232 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of valentin malykh

[30]valentin malykh

   ai research at deep learning lab @ mipt

     * (button)
       (button) 232
     * (button)
     *
     *

   [31]go to the profile of valentin malykh
   never miss a story from valentin malykh, when you sign up for medium.
   [32]learn more
   never miss a story from valentin malykh
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d8da391a3f01
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@madrugado/advances-in-nlp-in-2017-part-ii-d8da391a3f01&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@madrugado/advances-in-nlp-in-2017-part-ii-d8da391a3f01&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@madrugado?source=post_header_lockup
  10. https://medium.com/@madrugado
  11. https://medium.com/@madrugado/advances-in-nlp-in-2017-b00e927fcc57
  12. https://arxiv.org/abs/1710.11041
  13. https://arxiv.org/abs/1711.00043
  14. https://arxiv.org/abs/1705.09655
  15. https://arxiv.org/abs/1711.00043
  16. https://arxiv.org/abs/1710.04087
  17. https://arxiv.org/abs/1705.09655
  18. https://github.com/shentianxiao/language-style-transfer
  19. https://arxiv.org/abs/1711.06861
  20. https://arxiv.org/abs/1703.00955
  21. https://medium.com/@madrugado/advances-in-nlp-in-2017-b00e927fcc57
  22. https://arxiv.org/abs/1709.02755
  23. https://medium.com/@varvara.logacheva?source=post_page
  24. https://medium.com/tag/machine-learning?source=post
  25. https://medium.com/tag/naturallanguageprocessing?source=post
  26. https://medium.com/tag/natural-language?source=post
  27. https://medium.com/tag/deep-learning?source=post
  28. https://medium.com/tag/machine-translation?source=post
  29. https://medium.com/@madrugado?source=footer_card
  30. https://medium.com/@madrugado
  31. https://medium.com/@madrugado
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/d8da391a3f01/share/twitter
  35. https://medium.com/p/d8da391a3f01/share/facebook
  36. https://medium.com/p/d8da391a3f01/share/twitter
  37. https://medium.com/p/d8da391a3f01/share/facebook
