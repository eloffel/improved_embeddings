                               [1]sourceforge



                                  jgibblda

         a java implementation of id44 (lda)
         using id150 for parameter estimation and id136

                     [2]http://jgibblda.sourceforge.net/

                            copyright    2008 by

       [3]xuan-hieu phan (pxhieu at gmail dot com), graduate school of
                   information sciences, tohoku university

     [4]cam-tu nguyen (ncamtu at gmail dot com), college of technology,
                     vietnam national university, hanoi
   _______________________________________________________________________

   1. [5]introduction

           1.1. [6]description

           1.2. [7]news, comments, and bug reports

           1.3. [8]license

   2.[9]how to use jgibblda from command line

           2.1. [10]download

           2.2. [11]command line & input parameters

                   2.2.1. [12]parameter estimation from scratch

                   2.2.2. [13]parameter estimation from a previously
   estimated model

                   2.2.3. [14]id136 for previously unseen (new) data

           2.3. [15]input data format

           2.4. [16]outputs

           2.5. [17]case study

   3. [18]how to program with jgibblda

   4.[19]citation, links, acknowledgements, and references

1. introduction

1.1. description

   jgibblda is a java implementation of id44 (lda)
   using id150 technique for parameter estimation and id136.
   the input and output for jgibblda are the same format as gibblda++
   ([20] http://gibbslda.sourceforge.net/). because the parameter
   id136 process require less computational time than parameter
   estimation, jgibblda focus on infering hidden/latent topic structures
   of unseen data upon the model estimated using gibblda++ . it also
   provides a convenient api to get topic structures for an array of input
   strings.

   lda was first introduced by david blei et al [[21]blei03]. there have
   been several implementations of this model in c (using variational
   methods), java, and matlab. we decided to release this implementation
   of lda in java using id150 to provide an alternative choice to
   the topic-model community.

   jgibblda is useful for the following potential application areas:

              information retrieval (analyzing semantic/latent
   topic/concept structures of large text collection for a more
   intelligent information search.

              document classification/id91, document summarization,
   and text/web data mining community in general.

              id185

              content-based image id91, object recognition, and other
   applications of id161 in general.

              other potential applications in biological data.

1.2. news, comments, and bug reports.

   - june 06, 2006:

              released version 1.0

   we highly appreciate any suggestion, comment, and bug report.

1.3. license

   jgibblda is a free software; you can redistribute it and/or modify it
   under the terms of the gnu general public license as published by the
   free software foundation.
   jgibblda is distributed in the hope that it will be useful, but without
   any warranty; without even the implied warranty of merchantability or
   fitness for a particular purpose. see the gnu general public license
   for more details.
   you should have received a copy of the gnu general public license along
   with jgibblda; if not, write to the free software foundation, inc., 59
   temple place, suite 330, boston, ma 02111-1307 usa.

2. how to use jgibblda from command line

2.1. download

   you can find and download document, source code of jgibblda at
   [22]http://sourceforge.net/projects/jgibblda

   here are some other tools developed by the same author:

              [23]flexcrfs: flexible id49

              [24]crftagger: crf english pos chunker

              [25]crfchunker: crf english phrase chunker

              [26]jtextpro: a java-based text processing toolkit

              [27]jwebpro: a java-based web processing toolkit

              [28]jvnsegmenter: a java-based vietnamese id40
   tool

2.2. command line & input parameters

   in this section, we describe how to use this tool for parameter
   estimation and id136 for new data. suppose that the current working
   directory is the home directory of jgibblda and we are in linux
   platform. the command lines for other cases are similar.

2.2.1. parameter estimation from scratch

     $ java [-mx512m] -cp bin:lib/args4j-2.0.6.jar jgibblda.lda -est
     [-alpha <double>] [-beta <double>] [-ntopics <int>] [-niters <int>]
     [-savestep <int>] [-twords <int>]    dir <string> -dfile <string>

   in which (parameters in [ ] are optional):

              -est: estimate the lda model from scratch

              -alpha <double>: the value of alpha, hyper-parameter of lda.
   the default value of alpha is 50 / k (k is the the number of topics).
   see  [[29]griffiths04] for a detailed discussion of choosing alpha and
   beta values.

              -beta <double>: the value of beta, also the hyper-parameter
   of lda. its default value is 0.1

              -ntopics <int>: the number of topics. its default value is
   100. this depends on the input dataset. see [[30]griffiths04] and
   [[31]blei03] for a more careful discussion of selecting the number of
   topics.

              -niters <int>: the number of id150 iterations. the
   default value is 2000.

              -savestep <int>: the step (counted by the number of gibbs
   sampling iterations) at which the lda model is saved to hard disk. the
   default value is 200.

              -twords <int>: the number of most likely words for each
   topic. the default value is zero. if you set this parameter a value
   larger than zero, e.g., 20, jgibblda will print out the list of top 20
   most likely words per each topic each time it save the model to hard
   disk according to the parameter savestep above.

              -dir <string>: the input training data directory

              -dfile <string>: the input training data file. see[32]
   section 2.3 for a description of input data format.

2.2.3. parameter estimation from a previously estimated model

     $ java [-mx512m]-cp bin:lib/args4j-2.0.6.jar jgibblda.lda -estc -dir
     <string> -model <string> [-niters <int>] [-savestep <int>] [-twords
     <int>]

   in which (parameters in [ ] are optional):

              -estc: continue to estimate the model from a previously
   estimated model.

              -dir <string>: the directory contain the previously estimated
   model

              -model <string>: the name of the previously estimated model.
   see [33]section 2.4 to know how jgibblda saves outputs on hard disk.

              -niters <int>: the number of id150 iterations to
   continue estimating. the default value is 2000.

              -savestep <int>: the step (counted by the number of gibbs
   sampling iterations) at which the lda model is saved to hard disk. the
   default value is 200.

              -twords <int>: the number of most likely words for each
   topic. the default value is zero. if you set this parameter a value
   larger than zero, e.g., 20, jgibblda will print out the list of top 20
   most likely words per each topic each time it save the model to hard
   disk according to the parameter savestep above.

2.2.3. id136 for previously unseen (new) data

     $ java [-mx512m] -cp bin:lib/args4j-2.0.6.jar jgibblda.lda -inf -dir
     <string> -model <string> [-niters <int>] [-twords <int>] -dfile
     <string>

   in which (parameters in [ ] are optional):

              -inf: do id136 for previously unseen (new) data using a
   previously estimated lda model.

              -dir <string>: the directory contain the previously estimated
   model

              -model <string>: the name of the previously estimated model.
   see [34]section 2.4. to know how jgibblda saves outputs on hard disk.

              -niters <int>: the number of id150 iterations for
   id136. the default value is 20.

              -twords <int>: the number of most likely words for each topic
   of the new data. the default value is zero. if you set this parameter a
   value larger than zero, e.g., 20, jgibblda will print out the list of
   top 20 most likely words per each topic after id136.

              -dfile <string>:the file containing new data. see [35]section
   2.3 for a description of input data format.

2.3. input data format

   both data for training/estimating the model and new data (i.e.,
   previously unseen data) have the same format as follows:

     [m]
     [document[1]]
     [document[2]]
     ...
     [document[m]]

   in which the first line is the total number for documents [m]. each
   line after that is one document. [document[i]] is the i^th document of
   the dataset that consists of a list of n[i] words/terms.

     [document[i]] = [word[i1]] [word[i2]] ... [word[ini]]

   in which all [word[ij]] (i=1..m, j=1..n[i]) are text strings and they
   are separated by the blank character.

   note that the terms document and word here are abstract and should not
   only be understood as normal text documents. this is because lda can be
   used to discover the underlying topic structures of any kind of
   discrete data. therefore, jgibblda is not limited to text and natural
   language processing but can also be applied to other kinds of data like
   images and biological sequences. also, keep in mind that for text/web
   data collections, we should first preprocess the data (e.g., removing
   stop words and rare words, id30, etc.) before estimating with
   jgibblda.

2.4. outputs

2.4.1. outputs of id150 estimation of jgibblda

   outputs of id150 estimation of jgibblda include the following
   files:

     <model_name>.others
     <model_name>.phi
     <model_name>.theta
     <model_name>.tassign
     <model_name>.twords

   in which:

              <model_name>: is the name of a lda model corresponding to the
   time step it was saved on the hard disk. for example, the name of the
   model was saved at the id150 iteration 400^th will be
   model-00400. similarly, the model was saved at the 1200^th iteration is
   model-01200. the model name of the last id150 iteration is
   model-final.

              <model_name>.others: this file contains some parameters of
   lda model, such as:

     alpha=?
     beta=?
     ntopics=? # i.e., number of topics
     ndocs=? # i.e., number of documents
     nwords=? # i.e., the vocabulary size
     liter=? # i.e., the id150 iteration at which the model was
     saved

              <model_name>.phi: this file contains the word-topic
   distributions, i.e., p(word[w]|topic[t]). each line is a topic, each
   column is a word in the vocabulary

              <model_name>.theta: this file contains the topic-document
   distributions, i.e., p(topic[t]|document[m]). each line is a document
   and each column is a topic.

              <model_name>.tassign: this file contains the topic
   assignments for words in training data. each line is a document that
   consists of a list of <word[ij]>:<topic of word[ij]>

              <model_file>.twords: this file contains twords most likely
   words of each topic. twords is specified in the command.

   jgibblda also saves a file called wordmap.txt that contains the maps
   between words and word's ids (integer). this is because jgibblda works
   directly with integer ids of words/terms inside instead of text
   strings.

2.4.2. outputs of id150 id136 for previously unseen data

   the outputs of jgibblda id136 are almost the same as those of the
   estimation process except that the contents of those files are of the
   new data. the <model_name> is exactly the same as the filename of the
   input (new) data.

2.5. case study

   for example, we want to estimate a lda model for a collection of
   documents stored in file called models/casestudy/newdocs.dat and then
   use that model to do id136 for new data stored in file
   models/casestudy/newdocs.dat.
   we want to estimate for 100 topics with alpha = 0.5 and beta = 0.1. we
   want to perform 1000 id150 iterations, save a model at every
   100 iterations, and each time a model is saved, print out the list of
   20 most likely words for each topic. supposing that we are now at the
   home directory of jgibblda, we will execute the following command to
   estimate lda model from scratch:

     $ java -mx512m -cp bin:lib/args4j-2.0.6.jar jgibblda.lda -est -alpha
     0.5 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 20
     -dfile models/casestudy/newdocs.dat

   now look into the models/casestudy directory, we can see the outputs as
   described in [36]section 2.4.

   now, we want to continue to perform another 800 id150
   iterations from the previously estimated model model-01000 with
   savestep = 100, twords = 30, we perform the following command:

     $ java -mx512m -cp bin:lib/args4j-2.0.6.jar -estc -dir
     models/casestudy/ -model model-01000 -niters 800 -savestep 100
     -twords 30

   now, look into the casestudy directory to see the outputs.

   now, if we want to do id136 (30 id150 iterations) for the
   new data newdocs.dat (note that the new data file is stored in the same
   directory of the lda models) using one of the previously estimated lda
   models, for example model-01800, we perform the following command:

     $ java -mx512m -cp bin:lib/args4j-2.0.6.jar -inf -dir
     models/casestudy/ -model model-01800 -niters 30 -twords 20 -dfile
     newdocs.dat

   now, look into the casestudy directory, we can see the outputs of the
   id136s:

     newdocs.dat.others
     newdocs.dat.phi
     newdocs.dat.tassign
     newdocs.dat.theta
     newdocs.dat.twords

   here are the outputs of two large-scale datasets estimated:

              [37]200 topics from wikipedia (240mb, 71,986 docs)

              [38]200 topics from ohsumed (a subset of medline abstracts,
   156mb, 233,442 abstracts)

              [39]60 topics, [40]120 topics, [41]100 topics and [42]200
   topics from vnexpress news collection (in vietnamese)

              [43]200 topics from wikipedia collection (in vietnamese)

              [44]120 topics from vnexpress news and wikipedia collection
   (in vietnamese)

3. how to program with jgibblda

3.1. initialize an id136r

   in order to id136 topic model for an unseen dataset, we first need
   an id136r. because it take long time to load an estimated model, we
   ussually initilize one instance of id136r and use it for multiple
   id136s. firstly, we need to create an instance of ldacmdoption and
   initilize it similarly as follows:

   ldacmdoption ldaoption = new ldacmdoption();
   ldaoption.inf = true;
   ldaoption.dir = "c:\\ldamodeldir";
   ldaoption.modelname = "newdocs";
   ldaoption.niters = 100;

   here, the dir variable of ldacmdoption indicates the directory
   containing the estimated topic model (for example: model generated from
   command line). the modelname is the name of the estimated topic model
   and niters is the number of gibb sampling steps for id136.

   next, we use that ldacmdoption to initilize an id136r as follows:

   id136r id136r = new id136r();
   id136r.init(option);

3.2. id136 for previously unseen data

      id136 for data from file


   ldaoption.dfile = "input-lda-data.txt";
   model newmodel = id136r.id136();

   here, dfile is the file containing input data in the same format
   described in the section 2.3


       id136 for an array of strings


   string [] test = {"politics bill clinton", "law court", "football
   match"};
   model newmodel = id136r.id136(test);

4. citation, links, acknowledgements, and references

4.1. citation

              xuan-hieu phan, le-minh nguyen, and susumu horiguchi.
   [45]learning to classify short and sparse text & web with hidden topics
   from large-scale data collections. in proc. of the 17th international
   world wide web conference ([46]www 2008), pp.91-100, april 2008,
   beijing, china.

              istvan biro, jacint szabo, and andras benczur. [47]latent
   dirichlet allocation in web spam filtering. in proc. of the fourth
   international workshop on adversarial information retrieval on the web,
   www 2008, april 2008, beijing, china.

4.2. links

   here are some pointers to other implementations of lda:

              [48]lda-c (variational methods)

              [49]matlab id96

              [50]java version of lda-c and a short java version of gibbs
   sampling for lda

              [51]lda package (using variational methods, including c and
   matlab code)

4.3. acknowledgements

   our code is based on the [52]java code of gregor heinrich and the
   theoretical description of id150 for lda in [[53]heinrich]. i
   would like to thank heinrich for sharing the code and a comprehensive
   technical report.

   we would like to thank sourceforge.net for hosting this project.

   [54]sourceforge

4.4. references

              [andrieu03] c. andrieu, n.d. freitas, a. doucet, and m.
   jordan: [55]an introduction to mcmc for machine learning, machine
   learning (2003)

              [blei03] d. blei, a. ng, and m. jordan: [56]latent dirichlet
   allocation, journal of machine learning research (2003).

              [blei07] d. blei and j. lafferty: [57]a correlated topic
   model of science, the annals of applied statistics (2007).

              [griffiths] t. griffiths: [58]id150 in the
   generative model of id44, technical report.

              [griffiths04] t. griffiths and m. steyvers: [59]finding
   scientific topics, proc. of the national academy of sciences (2004).

              [heinrich] g. heinrich: [60]parameter estimation for text
   analysis, technical report.

              [hofmann99] t. hofmann: [61]probabilistic latent semantic
   analysis, proc. of uai (1999).

              [wei06] x. wei and w.b. croft: [62]lda-based document models
   for ad-hoc retrieval, proc. of acm sigir (2006).
   _______________________________________________________________________

                          last updated june 6, 2008

references

   visible links
   1. http://sourceforge.net/
   2. http://jgibblda.sourceforge.net/
   3. http://pxhieu.googlepages.com/index.html
   4. http://www.coltech.vnu.edu.vn/httt
   5. http://jgibblda.sourceforge.net/#1._introduction_
   6. http://jgibblda.sourceforge.net/#1.1._description_
   7. http://jgibblda.sourceforge.net/#1.2._news,_comments,_and_bug_reports._
   8. http://jgibblda.sourceforge.net/#1.3._license
   9. http://jgibblda.sourceforge.net/#2._how_to_use_jgibblda_from_command_lin
  10. http://jgibblda.sourceforge.net/#2.1._download
  11. http://jgibblda.sourceforge.net/#_2.2._command_line_&_input_parameter
  12. http://jgibblda.sourceforge.net/#_2.2.1._parameter_estimation_from_sc
  13. http://jgibblda.sourceforge.net/#_2.2.3._parameter_estimation_from_a_
  14. http://jgibblda.sourceforge.net/#_2.2.3._id136_for_previously_uns
  15. http://jgibblda.sourceforge.net/#_2.3._input_data_format
  16. http://jgibblda.sourceforge.net/#2.4_outputs
  17. http://jgibblda.sourceforge.net/#_2.5._case_study
  18. http://jgibblda.sourceforge.net/#3._how_to_program_with_jgibblda
  19. http://jgibblda.sourceforge.net/#4._citation,_links,_acknowledgements,_a
  20. http://gibbslda.sourceforge.net/
  21. http://jgibblda.sourceforge.net/#blei03
  22. http://sourceforge.net/projects/jgibblda
  23. http://flexcrfs.sourceforge.net/
  24. http://crftagger.sourceforge.net/
  25. http://crfchunker.sourceforge.net/
  26. http://jtextpro.sourceforge.net/
  27. http://jwebpro.sourceforge.net/
  28. http://jvnsegmenter.sourceforge.net/
  29. http://jgibblda.sourceforge.net/#griffiths04
  30. http://jgibblda.sourceforge.net/#griffiths04
  31. http://jgibblda.sourceforge.net/#blei03
  32. http://jgibblda.sourceforge.net/#_2.3._input_data_format
  33. http://jgibblda.sourceforge.net/#2.4_outputs
  34. http://jgibblda.sourceforge.net/#2.4_outputs
  35. http://jgibblda.sourceforge.net/#2.3._input_data_format
  36. http://jgibblda.sourceforge.net/#2.4_outputs
  37. http://gibbslda.sourceforge.net/wikipedia-topics.txt
  38. http://gibbslda.sourceforge.net/ohsumed-topics.txt
  39. http://gibbslda.sourceforge.net/vnexpress-060topics.txt
  40. http://gibbslda.sourceforge.net/vnexpress-120topics.txt
  41. http://jgibblda.sourceforge.net/vnexpress-100topics.txt
  42. http://gibbslda.sourceforge.net/vnexpress-200topics.txt
  43. http://jgibblda.sourceforge.net/wiki-200topics.txt
  44. http://jgibblda.sourceforge.net/vnwiki-120topics.txt
  45. http://gibbslda.sourceforge.net/fp224-phan.pdf
  46. http://www2008.org/index.html
  47. http://airweb.cse.lehigh.edu/2008/submissions/biro_2008_latent_dirichlet_allocation_spam.pdf
  48. http://www.cs.princeton.edu/~blei/lda-c/index.html
  49. http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm
  50. http://www.arbylon.net/projects/
  51. http://chasen.org/~daiti-m/dist/lda/
  52. http://www.arbylon.net/projects/ldagibbssampler.java
  53. http://jgibblda.sourceforge.net/#heinrich
  54. http://sourceforge.net/
  55. http://citeseer.ist.psu.edu/andrieu03introduction.html
  56. http://citeseer.ist.psu.edu/blei03latent.html
  57. http://www.cs.princeton.edu/~blei/papers/bleilafferty2007.pdf
  58. http://citeseer.ist.psu.edu/613963.html
  59. http://www.pnas.org/cgi/reprint/0307752101v1.pdf
  60. http://www.arbylon.net/publications/text-est.pdf
  61. http://www.cs.brown.edu/~th/papers/hofmann-uai99.pdf
  62. http://ciir.cs.umass.edu/pubfiles/ir-464.pdf

   hidden links:
  64. http://sourceforge.net/
  65. http://sourceforge.net/
  66. http://sourceforge.net/
  67. http://sourceforge.net/
  68. http://sourceforge.net/
