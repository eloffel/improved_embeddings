   #[1]mostafa dehghani    feed [2]mostafa dehghani    comments feed
   [3]mostafa dehghani    some highlights of mila deep learning and
   id23 summer schools 2017 comments feed [4]learning to
   attend, copy, and generate for session-based query suggestion
   [5]learning useful id194s for search [6]alternate
   [7]alternate

   [8]skip to content
   {mylastname}@uva.nl science park 904, 1098 xh amsterdam

[9]mostafa dehghani

machine learning researcher

   (button)
     * [10]home
     * [11]about
     * [12]publications
     * [13]talks
     * [14]research experience

some highlights of mila deep learning and id23 summer
schools 2017

   [15]august 31, 2017september 8, 2017 [16]mostafa dehghani

   a couple weeks a go, i attended [17]milla deep learning summer school
   (dlss) from june 26th to july 1st and id23 summer
   school (rlss) from  july 3rd to 5th, 2017, organized by [18]yoshua
   bengio and [19]aaron courville. you can find information about the
   lectures [20]here. in the following, i will share some of "my"
   highlights from the summer schools.

different types of learning problems

   the first day of summer school was on general topics of machine
   learning and neural networks. [21]doina precup gave a talk which was a
   [22]gentle refreshing of the general concepts of machine learning and
   then [23]hugo larochelle covered the basics of neural networks. in the
   [24]second part of his talk, hugo started by dividing learning problems
   into different types, based on the data and settings of the problem
   during training and id136. based on his grouping, each learning
   problem can be classified to one of these categories:

     * supervised learning (like classification and regression) in which
       at training time, we have a set of pairs of <input, target> that
       are coming from an unknown distribution and at test time, we are
       given another set of data that we assume they are sampled from the
       same distribution. adapting a neural network to a supervised
       learning problem is often about adapting the output layer.
     * unsupervised learning (like distribution estimation and
       id84) in which we have a similar setup to the
       supervised learning unless we don't have targets. generally, in
       unsupervised learning, we aim at learning something about the
       distribution of the data, or we just want to learn what is the
       potential implicit manifold around which most of the data lie.
     * semi-supervised learning (like tasks for which a small amount of
       labeled data is available) in which at training time often we
       assume we don't have a lot of <input, targets> pairs, but we have a
       lot of unlabeled examples and we hope to leverage the unlabeled
       data to do a better job at the test time.
     * id72 (like object recognition in images with
       multiple objects) which we can think of it as a problem setting
       that we have input and a series of labels, each with regard to a
       different task and we would like to simultaneously solve these task
       ideally with a single neural network. the reason that we are
       interested in multitask learning is that we suspect that there are
       some types of tasks that if we solve them jointly, we will do
       better than solving them separately and this generally could be the
       effect of the knowledge that we can share between tasks by learning
       them jointly.
     * id21 (like when we just care about detecting an object
       in an image and we have caption of images in our training data)
       which is similar to multitask learning that we assume at training
       time, we are seeing multiple labels for different tasks, but there
       is one task that we are particularly interested in and we hope that
       seeing all the other labels, we can do a better job for that
       particular task (not all of them) at test time.
     * structured output prediction (like image id134 and
       machine translation) where the target is a more structured object
       like a sequence of words that are not independent.
     * id20 (like classifying sentiment in reviews of
       different products) where the distribution of our data during
       training is different from the distribution of our data during test
       time. we might have some unlabeled data during training that are
       sampled from the same distribution as our test data. the key is
       that we need to learn an unbiased representation which is
       independent of the domain of the input data. one way to do that is
       to have an adversarial setup in which we have a classifier unit in
       our model that is in charge of detecting the domain of the input
       and we try to learn a representation using which the classifier
       unit is doing as bad as possible.
     * one-shut learning (like recognizing a person based on a single
       picture of him/her) in which we assume that at training time, we
       are seeing examples that belong to a set of c classes, but at test
       time, we will be asked to classify examples that belong to classes
       we have never seen at training time. as a side information, we can
       consider that we have only one example from that class during
       training.
     * zero-shut learning (like recognizing an object based on a worded
       description of it) which is similar to one-shut learning but at
       test time, instead of having an example from a new class, we are
       going to have some feature representation of what the task is.

     fyi ad hoc retrieval=zero shot
         fernando diaz (@diazf_acm) [25]june 26, 2017

   i recommend taking a look at [26]his slides as you can easily clear up
   which type of learning problems you are dealing with and make better
   decisions to solve it.

unintuitive properties of neural networks

   in a part of hugo's talk, he spent some time talking about properties
   of neural networks which are perhaps not super intuitive. for instance,
   some times neural networks are able to achieve the level of accuracy
   for a task which is close to the level of accuracy that human can
   achieve but it turns out that they make really dumb mistakes that make
   no sense to us, for example, cases in id164 in an image
   where the source of error is not something like optical illusions which
   happens for human, but just very small differences that are not even
   visible for human.

neural networks are strangely non-convex

   another way in which neural networks are intriguing is that they are
   strangely non-convex. so when we are thinking about non-convexity, we
   think of one or two dimensions and we can imagine a nonconvex objective
   function with a lot of local optima. then considering the standard
   id119 procedure which starts from a randomly chosen point,
   we might think it is kind of hopeless and we are never going to find
   the global optimum of this nonconvex function.

   but it turns out that this intuition is not great at describing the
   complexity of neural networks. this is because training a neural
   network is not about an optimization problem in one dimension with a
   single parameter, but it is an optimization in very high dimensional
   space with a lot of parameters. in this situation, a lot of critical
   points, i.e. points corresponds to a derivative of zero are actually
   not the local minima, but look likes a saddle point. a saddle point is
   a point that there is a direction that the function is locally convex
   and essentially increasing in both directions, but another direction
   exists with opposite situation, where the function is decreasing in
   both sides:

   this is more or less a good news because essentially unless we fall
   exactly at the critical point, we fall a little bit off that point, the
   gradient points to a right direction allowing us to improve the loss.
   in other words, we can think that in a very high dimensional space, if
   we have a critical point, and we think a bout a random direction, the
   chances of the critical point is a local minimum is very small because
   it should curve up in all directions and in general there is going to
   be a direction that the function goes down (saddle point). essentially,
   assuming a rather smooth function,  the closer we are to the global
   minimum, the harder it is to have a direction that is descending in
   this high dimensional space.

neural networks work best when badly trained

   imagine we have a simple objective function with a single parameter and
   we have two global minima:

   the training function is the average loss of the training set and
   testing function is the average loss from the same neural network but
   on the test set. abstracting it, we can assume the testing function
   looks like the training function but maybe a shifted version of it. so,
   if the local minimum is very narrow, the optimal point might match up
   very badly with the value we are observing on the test set. so, there
   is this general observation that flat minima tend to be associated with
   better generalization. it has been shown that under some definition of
   flatness, when we are using large mni batch sizes, we are more likely
   to find sharp minima and with small mini batches, we have a lot of
   stochasticity which doesn't allow us to find sharp minima and we
   preferentially end up in a flat minimum.

neural networks can easily memorize

   although neural networks are state of the art in terms of their ability
   of generalization, if we train them more on a problem where they should
   not fit a lot,  assuming that they generalize well since they have less
   capacity, they will overfit. it has been shown that if we take a
   dataset and randomly assign labels to data points, the same neural
   network that is trained longer will eventually fit the data which shows
   memorization. what this fact suggests is that we cannot think of the
   learning algorithm and the architecture separately.

neural networks can be compressed

   this is related to the idea of knowledge distillation where we have a
   large neural network that we already trained and we then initialize a
   smaller neural network (e.g. with fewer hidden units) trained on the
   same dataset while it also predicts some properties of the large
   network (essentially the logits), we can more or less achieve the
   similar performance with the smaller neural network. more interestingly
   it has been shown that with a shallow neural network, we can
   approximate surprisingly well a deep neural network. however, if we try
   to train the shallow network from scratch we cannot reach the same
   level of performance and in a way, we need the guidance of the larger
   one.

bias-variance trade-off

   [27]max welling gave a talk about "marrying id114 & deep
   learning" and he started with a really nice discussion about
   bias-variance trade-off, and he said, if we want to take one thing away
   from this summer school, it should be the bias-variance trade-off, as
   it is actually the way we think about over-fitting and under-fitting,
   which is the core of machine learning!

   often time, deep learning is seen as an optimisation problem in which
   we have an objective, we have a dataset, and we should try as hard as
   we can to optimise the objective. most of the time, we train for a week
   and the learning curve is getting better and better. it is clear that
   the optimization is really important for deep learning but a learning
   problem is more than just an optimization problem, it's a statistical
   id136 problem.

   we can imagine we build a machine that throws darts at a target,  and
   the machine might be a bit wiggly due to for example an unstable
   underground, which means every time we shoot a dart, it might not hit
   the target in the same spot. also, the might be a systematic error in
   this machine, which means the mean of all the darts is off from the
   center. we call the systematic error the "bias" and the error caused by
   the wiggliness of the machine the "variance" and the ideal case is
   having a machine with low bias and low variance (top-left in the
   image).

   in terms of equations, we have  y= f(x) +\varepsilon , where  y is
   label and f(x) is the underlying function that we try to estimate and
   \varepsilon is the label noise drawn from the normal distribution. we
   are interested for example in the squared error for point x : err(x) =
   e\big[(y-\hat{f}(x))^2\big] , which is the difference between the true
   label y and our estimate  \hat{f}(x) . we can decompose this error to
   three terms:

   err(x) =\big(e[\hat{f}(x)]-f(x)\big)^2
   +e[\big(\hat{f}(x)-e\big[\hat{f}(x)]\big)^2\big] +\sigma_e

   the first term is bias as it looks at the difference between the
   expected value of our estimate (expectation taken over many datasets)
   and the true value, the next term is the variance, which is the
   variance of the estimator that only depends on the estimator, and the
   third term which is basically the noise. noise is something that we
   cannot do anything about as it is part of the characteristics of the
   problem we deal with, so we simply ignore it.

   the game here is to try to get either the bias down or variance down in
   order to get the total error down. the following plot shows the
   relation between error and its components with the model complexity:

   if we have a very simple model with a few parameters, the variance is
   going to be small, but the bias is very large. at the other side of the
   spectrum, with a complex model, we can fit basically everything, but on
   a particular dataset, we might get still a large error which means the
   variance is high. we want to sort of trade these two off and find the
   optimal point where the bias squared and variance added up is minimal.

what is the learning algorithm in the human brain that allows us to learn
about sequences?

   in one of the sessions, we had [28]yoshua bengio talking about
   recurrent neural networks and he covered a wide range of related
   problems to id56s. among them, there was a discussion that "how would
   the human brain do anything like back propagation through time?".
   actually, it does not seem very plausible that the brain would use a
   strategy anything like back propagation through time. in back
   propagation through time, we just unfold the graph, we wait until the
   end and we can compute the gradients as usual with the chain rule. but,
   it actually does not seem very plausible that the brain would use a
   strategy anything it as it requires us to store the whole sequence in
   the memory, hold it there, and look at the outcome and then compute the
   gradients in the reverse time direction. we might say that although we
   don't learn long term dependency with the length of our life, we do it
   on a daily basis, however, it still does not feel completely right.

   there is another strategy to compute the gradient, which was introduced
   in the late eighties, called real time recurrent learning, which is a
   forward evaluation of the gradient. in another word, as we are moving
   forward in time, we can compute the derivative of each of the state
   variables with respect to each of the parameters, and each time we get
   a cost, we can use that immediately to make an update and we don't need
   to wait for the end of the sequence, which is a form of online
   learning. it seems very similar to what human brain does but
   unfortunately when we take the equations to the paper, we end up with
   that the computations are not practical and wouldn't scale with the
   size of the brain, even with the size of the networks we now train for
   speech and language. there are some works in this direction proposing
   approximate gradient estimators instead of doing exact gradient
   computation, but this is still an open question that how can we do
   online update that is computationally efficient.

some of the theoretical puzzles of deep learning

   during the summer school, we had a really good session about
   "theoretical neuroscience and deep learning theory", by  [29]surya
   ganguli. in a part of this session, surya talked about some of the
   theoretical puzzles of deep learning and he divided these puzzles into
   three main topics: trainability, expressivity, and generalizability.
     * trainability: if a good network solution exists with small training
       error, how do we find it? and what makes a learning problem
       difficult?
       there are a bunch of research works attacking this question by
       focusing on human semantic cognition. they try to understand the
       infant level learning from the psychological point of view and
       build neural networks to mimic this by modeling hierarchical
       differentiation of concepts. then they investigating the ability of
       these neural networks on learning a range of simple and complex
       tasks.
     * expressivity: what kinds of functions can a deep network express
       that shallow networks cannot?
       we can investigate this part by connecting the deep neural networks
       to the theory of chaos. it turns out that chaos is a special case
       of deep learning! so it's important to know that deep learning can
       have a chaotic face and we may wish to avoid it. on the other hand,
       the chaos can be the origin of high expressivity as well.
       to understand this, we can pick a random network with l layer and n
       neurons in each layer, where the weights and biases are i.i.d
       gaussian and we can have an arbitrary nonlinearity. we can ask this
       question that "having a single point in the space (a single feature
       vector), and we consider it as input and propagate it through the
       network, does its length grow or shrink?", or more importantly, "if
       we have a pair of points in the space that are close to each other,
       as they propagate through the network, do they get more similar to
       each other over time, or do they decorrelated and become more
       different over time?".  so if the network makes two points more
       similar, we call it "ordered regime" and if the network
       fundamentally amplifying small initial differences and making them
       very different, which is the very definition of chaos: the
       sensitivity to the small changes in the initial conditions, and
       that is called the "chaotic regime".
       in a neural network, biases are independent of the input and they
       push the state of the network to the same direction, regardless of
       the input. so in case of having two points, biases tend to align
       two different input and make them similar to each other. however,
       the large weight can overwhelm the common bias and eventually it
       makes the inputs dissimilar to each other. so we can have a
       boundary between chaotic and ordered regime based on biases and
       weights. the conclusion is that deep neural network can have
       expressivity if they go to the chaotic regime in a controlled way.

     * generalizability: what principles do deep networks use to place
       id203 or make decisions in regions of input space with little
       data?
       generalizability is one of the hardest issues and it is still an
       open problem. some research works investigated and discussed the
       fact that generalizability depends on both expressivity and
       optimizability in a weird way. for instance, it has been shown that
       deep networks, when they are operated with a few data points and
       many more parameters, can memorize random labels. what is happening
       is that based on the general procedure of training, we try to find
       the best model on the training set, but in deep learning, we are
       not perfect at optimizing, so even though we can achieve a lot of
       expressivity, the space of accessible functions we can get to is
       limited and the ability of generalization is tied up not only with
       the space of function but optimization.
       also from the stability perspective, it has been shown that if we
       train our network with small mini batches, that adds noise to our
       gradient and it turns out we generalize better which is also
       related to finding flat optima during optimization, while when we
       train with large mini batches we might think that's better as we
       get a better gradient, but we generalize worse.

gans for language generation?

   [30]ian goodfellow gave a talk on id3. at
   the end of the session, i asked about ian's idea on gans for language.
   he mentioned that based on his observation, he feels that gans are not
   quite there yet. he also said "one thing i don't understand is that
   there is quite a lot of enthusiasms for gans for text in general and to
   me, that's a little bit confusing because text seems one of the places
   where gans are least likely to help us. for gans, they work if the
   output is differentiable because we need to back propagate through the
   discriminator, and then through the output or the generator and the
   generator itself."

   he pointed out that making the output of gans very stable and reliable
   is really hard even for continuous space and for cases where the output
   is discrete, it seems like tackling two difficulties at ones and we
   need to resolve both of them before we see really a success. so it
   doesn't seem to easily have cut up to where id56s text generation is.

learning to learn

   [31]nando de freitas gave a talk on learning to learn. he started by a
   discussion on the next challenges in ai and pointed out some simple
   human abilities that a one-year old kid has and no machine that we have
   ever developed and no machine that we have insight that we think we
   will be able to build in the next couple of years is capable of them.
   for instance, the ability of reasoning about objects and interacting
   with them and manipulating them with an intrinsic motivation or the
   advanced ability of the human to learn by imitation. nando argued that
   this is mostly because the human has the ability to learn from the
   beginning and human learns the ability to learn.  so learning to learn
   would be one the most obvious directions to drive the ai research
   toward.

   one of the problems that is getting popular in machine learning is
   few-shot learning, and the challenge is how a neural network can learn
   from a few data. nando argued that learning to learn can tackle this
   problem as if we have a model that is capable of learning how to use
   data at test time, it is basically able to quickly learn from a small
   dataset.

   generally, the learning to learn paradigm is about learning one
   algorithm using another algorithm and basically, we can say "learning
   to learn x by y", and one can plug in anything as x and y (like
   nips2016 deepmind paper, where x=sgd and y=sgd). as one of the
   instances of the learning to learn paradigm, nando also discussed the
   cases where a neural network controls the behavior of another neural
   network, for example as the optimizer and optimize, or a network
   generating parameters of the other network.

learning  to guess from a guess

   [32]richard s. sutton also gave a talk on id23,
   mostly focused on temporal-difference learning. richard started talking
   about scalability and said "methods that scale with computation are the
   future of ai" and scale with computation means that when we get more
   computer power, these methods become more powerful. a lot of current
   methods are weakly scalable including supervised learning that we need
   to gather a lot of data and even model-free rl is not a strong scalable
   method.

   the only strong scalable method would be "prediction learning", where
   we learn to predict what will happen.  in prediction learning, we don't
   need human labeling and we have the target just by waiting. so,
   prediction learning is the scalable model-free learning.

   richard talked continued by talking about temporal-difference learning
   (td-learning) as a method for learning to predict which is widely used
   in id23 to predict future reward or value functions.
   td-learning is the core of many methods like id24, sarsa,
   td-lambda etc. td-learning is learning a prediction from another later
   prediction, which is, in fact,  learning a guess from a guess. as an
   example, consider we have a model (a neural network) that takes a chess
   position and generates the id203 of winning. and to train that,
   we define the error simply as the id203 of winning in one
   position minus the id203 of winning in the next position and it's
   the temporal change in the prediction, then we can use a standard
   method (like back propagation) to update the parameters of the model
   with respect to the error and you can learn the model playing against
   itself and end up with a really strong model competitive with word best
   chess players.

   we are dealing with a multi-stage learning problem when we want to use
   dt-learning and one would say: can   t we just think of the multi-step as
   one big step, and then use one-step methods? the answer is no, we
   really can   t (and shouldn   t want to). because it's not scalable as you
   need to remember all the things you did and in this case, the
   computation is poorly distributed over time which makes it very costy.
   the other question would be that: can   t we just learn one-step
   predictions, and then iterate them (compose them) to produce multi-step
   predictions when needed?. the answer is again no, as first of all, we
   cannot do each step perfectly and iterating over this imperfect steps,
   we get a propagation of errors. besides, it's exponentially complex
   because as we look a head, at each step there will be many
   possibilities for action and different choices and this quickly gets
   computationally intractable. so, in case of having a multi-stage
   learning problem, td-learning would an obvious choice.

share this:

     * [33]click to share on twitter (opens in new window)
     * [34]click to share on facebook (opens in new window)
     *

related

    [35]life, [36]research experience

post navigation

   [37] learning to attend, copy, and generate for session-based query
   suggestion
   [38]learning useful id194s for search

i am mostafa   

   i am mostafa    but almost all of my friends call me    mosi   . i am a phd
   student at the university of amsterdam and trying to help machines to
   pass the turing test    [39]read more   

categories

     * [40]design
     * [41]life
     * [42]research experience
     * [43]scientific paper
     * [44]work experience

find me in:

     *
     *
     *
     *
     *
     *
     *
     *

archives

     * [ ] [45]2018 (5)
          + [46]october (1)
          + [47]july (1)
          + [48]june (1)
          + [49]february (1)
          + [50]january (1)
     * [x] [51]2017 (16)
          + [52]december (1)
          + [53]november (3)
          + [54]september (2)
          + [55]august (3)
          + [56]july (1)
          + [57]june (1)
          + [58]may (2)
          + [59]april (1)
          + [60]march (1)
          + [61]february (1)
     * [ ] [62]2016 (14)
          + [63]december (1)
          + [64]november (3)
          + [65]september (1)
          + [66]july (4)
          + [67]june (3)
          + [68]april (1)
          + [69]january (1)
     * [ ] [70]2015 (9)
          + [71]december (1)
          + [72]november (2)
          + [73]august (1)
          + [74]july (1)
          + [75]june (2)
          + [76]april (2)
     * [ ] [77]2014 (8)
          + [78]december (1)
          + [79]august (1)
          + [80]june (3)
          + [81]may (2)
          + [82]january (1)
     * [ ] [83]2013 (5)
          + [84]december (1)
          + [85]september (1)
          + [86]august (1)
          + [87]may (1)
          + [88]april (1)
     * [ ] [89]2012 (1)
          + [90]december (1)
     * [ ] [91]2011 (1)
          + [92]march (1)
     * [ ] [93]2010 (1)
          + [94]september (1)

recent postes

     *

[95]alexey's thesis cover
        october 4, 2018
     *

[96]internship at apple
        july 3, 2018

   close and accept
   privacy & cookies: this site uses cookies. by continuing to use this
   website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [97]cookie policy

   search for: ____________________ search

   [98]proudly powered by wordpress | theme: dublin by [99]justfreethemes.

references

   visible links
   1. http://mostafadehghani.com/feed/
   2. http://mostafadehghani.com/comments/feed/
   3. http://mostafadehghani.com/2017/08/31/some-highlights-of-mila-deep-learning-and-reinforcement-learning-summer-schools-2017/feed/
   4. http://mostafadehghani.com/2017/08/07/learning-to-attend-copy-and-generate-for-session-based-query-suggestion/
   5. http://mostafadehghani.com/2017/09/25/learning-useful-document-representations-for-search/
   6. http://mostafadehghani.com/wp-json/oembed/1.0/embed?url=http://mostafadehghani.com/2017/08/31/some-highlights-of-mila-deep-learning-and-reinforcement-learning-summer-schools-2017/
   7. http://mostafadehghani.com/wp-json/oembed/1.0/embed?url=http://mostafadehghani.com/2017/08/31/some-highlights-of-mila-deep-learning-and-reinforcement-learning-summer-schools-2017/&format=xml
   8. http://mostafadehghani.com/2017/08/31/some-highlights-of-mila-deep-learning-and-reinforcement-learning-summer-schools-2017/#content
   9. http://mostafadehghani.com/
  10. http://mostafadehghani.com/
  11. http://mostafadehghani.com/about/
  12. http://mostafadehghani.com/publications/
  13. http://mostafadehghani.com/talks/
  14. http://mostafadehghani.com/category/research-experience/
  15. http://mostafadehghani.com/2017/08/31/some-highlights-of-mila-deep-learning-and-reinforcement-learning-summer-schools-2017/
  16. http://mostafadehghani.com/author/mostafadehghani/
  17. https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/
  18. http://www.iro.umontreal.ca/~bengioy/yoshua_en/
  19. https://aaroncourville.wordpress.com/
  20. https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/schedule/
  21. http://cs.mcgill.ca/~dprecup/
  22. http://videolectures.net/site/normal_dl/tag=1129744/deeplearning2017_precup_machine_learning_01.pdf
  23. http://www.dmi.usherb.ca/~larocheh/index_en.html
  24. https://drive.google.com/file/d/0byukrdicdk7-uxb1r1zpx082mek/view
  25. https://twitter.com/diazf_acm/status/879481920714887168
  26. https://drive.google.com/file/d/0byukrdicdk7-uxb1r1zpx082mek/view
  27. https://staff.fnwi.uva.nl/m.welling/
  28. http://www.iro.umontreal.ca/~bengioy/yoshua_en/
  29. https://web.stanford.edu/dept/app-physics/cgi-bin/person/surya-gangulijanuary-2012/
  30. http://www.iangoodfellow.com/
  31. http://www.cs.ox.ac.uk/people/nando.defreitas/
  32. http://www.incompleteideas.net/sutton/
  33. http://mostafadehghani.com/2017/08/31/some-highlights-of-mila-deep-learning-and-reinforcement-learning-summer-schools-2017/?share=twitter
  34. http://mostafadehghani.com/2017/08/31/some-highlights-of-mila-deep-learning-and-reinforcement-learning-summer-schools-2017/?share=facebook
  35. http://mostafadehghani.com/category/life/
  36. http://mostafadehghani.com/category/research-experience/
  37. http://mostafadehghani.com/2017/08/07/learning-to-attend-copy-and-generate-for-session-based-query-suggestion/
  38. http://mostafadehghani.com/2017/09/25/learning-useful-document-representations-for-search/
  39. http://mostafadehghani.com/about/
  40. http://mostafadehghani.com/category/design/
  41. http://mostafadehghani.com/category/life/
  42. http://mostafadehghani.com/category/research-experience/
  43. http://mostafadehghani.com/category/scientific-paper/
  44. http://mostafadehghani.com/category/work-experience/
  45. http://mostafadehghani.com/2018/
  46. http://mostafadehghani.com/2018/10/
  47. http://mostafadehghani.com/2018/07/
  48. http://mostafadehghani.com/2018/06/
  49. http://mostafadehghani.com/2018/02/
  50. http://mostafadehghani.com/2018/01/
  51. http://mostafadehghani.com/2017/
  52. http://mostafadehghani.com/2017/12/
  53. http://mostafadehghani.com/2017/11/
  54. http://mostafadehghani.com/2017/09/
  55. http://mostafadehghani.com/2017/08/
  56. http://mostafadehghani.com/2017/07/
  57. http://mostafadehghani.com/2017/06/
  58. http://mostafadehghani.com/2017/05/
  59. http://mostafadehghani.com/2017/04/
  60. http://mostafadehghani.com/2017/03/
  61. http://mostafadehghani.com/2017/02/
  62. http://mostafadehghani.com/2016/
  63. http://mostafadehghani.com/2016/12/
  64. http://mostafadehghani.com/2016/11/
  65. http://mostafadehghani.com/2016/09/
  66. http://mostafadehghani.com/2016/07/
  67. http://mostafadehghani.com/2016/06/
  68. http://mostafadehghani.com/2016/04/
  69. http://mostafadehghani.com/2016/01/
  70. http://mostafadehghani.com/2015/
  71. http://mostafadehghani.com/2015/12/
  72. http://mostafadehghani.com/2015/11/
  73. http://mostafadehghani.com/2015/08/
  74. http://mostafadehghani.com/2015/07/
  75. http://mostafadehghani.com/2015/06/
  76. http://mostafadehghani.com/2015/04/
  77. http://mostafadehghani.com/2014/
  78. http://mostafadehghani.com/2014/12/
  79. http://mostafadehghani.com/2014/08/
  80. http://mostafadehghani.com/2014/06/
  81. http://mostafadehghani.com/2014/05/
  82. http://mostafadehghani.com/2014/01/
  83. http://mostafadehghani.com/2013/
  84. http://mostafadehghani.com/2013/12/
  85. http://mostafadehghani.com/2013/09/
  86. http://mostafadehghani.com/2013/08/
  87. http://mostafadehghani.com/2013/05/
  88. http://mostafadehghani.com/2013/04/
  89. http://mostafadehghani.com/2012/
  90. http://mostafadehghani.com/2012/12/
  91. http://mostafadehghani.com/2011/
  92. http://mostafadehghani.com/2011/03/
  93. http://mostafadehghani.com/2010/
  94. http://mostafadehghani.com/2010/09/
  95. http://mostafadehghani.com/2018/10/04/alexeys-thesis-cover/
  96. http://mostafadehghani.com/2018/07/03/internship-at-apple/
  97. https://automattic.com/cookies/
  98. http://wordpress.org/
  99. http://justfreethemes.com/dublin

   hidden links:
 101. http://mostafadehghani.com/about/
 102. https://twitter.com/m__dehghani
 103. https://nl.linkedin.com/in/mostafa-dehghani-982b0b57
 104. https://github.com/mostafadehghani/
 105. https://soundcloud.com/mostafa_dehghani
 106. https://www.instagram.com/mo0osina/
 107. https://www.facebook.com/dehghani.mostafa
 108. https://plus.google.com/+mostafadehghani
 109. http://mostafadehghani.com/feed/
 110. http://mostafadehghani.com/2018/07/03/internship-at-apple/
