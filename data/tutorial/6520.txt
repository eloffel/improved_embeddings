   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    a brief (and comprehensive) guide to
   stochastic id119 algorithms comments feed [4]alternate
   [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

a brief (and comprehensive) guide to stochastic id119 algorithms

   [25]10/03/201712/28/2017[26]artificial intelligence, [27]deep learning,
   [28]generic, [29]machine learning, [30]machine learning algorithms
   addenda, [31]neural networks[32]no comments

   stochastic id119 (sgd) is a very powerful technique,
   currently employed to optimize all deep learning models. however, the
   vanilla algorithm has many limitations, in particular when the system
   is ill-conditioned and could never find the global minimum. in this
   post, we   re going to analyze how it works and the most important
   variations that can speed up the convergence in deep models.

   first of all, it   s necessary to standardize the naming. in some books,
   the expression    stochastic id119    refers to an algorithm
   which operates on a batch size equal to 1, while    mini-batch gradient
   descent    is adopted when the batch size is greater than 1. in this
   context, we assume that stochastic id119 operates on batch
   sizes equal or greater than 1. in particular, if we define the loss
   function for a single sample as:

   where x is the input sample, y the label(s) and    is the parameter
   vector, we can also define the partial cost function considering a
   batch size equal to n:

   the vanilla stochastic id119 algorithm is based on a   
   update rule that must move the weights in opposite direction of the
   gradient of l (the gradient points always toward a maximum):

   this process is represented in the following figure:


      is the learning rate, while   start is the initial point and   opt is
   the global minimum we   re looking for. in a standard optimization
   problem, without particular requirements, the algorithm converges in a
   limited number of iterations. unfortunately, the reality is a little
   bit different, in particular in deep models, where the number of
   parameters is in the order of ten or one hundred million. when the
   system is relatively shallow, it   s easier to find local minima where
   the training process can stop, while in deeper models, the id203
   of a local minimum becomes smaller and, instead, saddle points become
   more and more likely.

   to understand this concept, assuming a generic vectorial function l(  ),
   the conditions for a point to be a minimum are:

   if the hessian matrix is (m x m) where m is the number of parameters,
   the second condition is equivalent to say that all m eigenvalues must
   be non-negative (or that all principal minors composed with the first n
   rows and n columns must be non-negative).

   from a probabilistic viewpoint, p(h positive semi-def.)     0 when m        ,
   therefore local minima are rare in deep models (this doesn   t mean that
   local minima are impossible, but their relative weight is lower and
   lower in deeper models). if the hessian matrix has both positive and
   negative eigenvalues (and the gradient is null), the hessian is said to
   be indefinite and the point is called saddle point. in this case, the
   point is maximum considering an orthogonal projection and a minimum for
   another one. in the following figure, there   s an example created with a
   3-dimensional cost function:

   it   s obvious that both local minima and saddle points are problematic
   and the optimization algorithm should be able to avoid them. moreover,
   there are sometimes conditions called plateaus, where l(  ) is almost
   flat in a very wide region. this drives the gradient to become close to
   zero with an increased risk to stop at a sub-optimal point. this
   example is shown in the next figure:

   for these reasons, now we   re going to discuss some common methods to
   improve the performance of a vanilla stochastic id119
   algorithm.

gradient perturbation

   a very simple approach to the problem of plateaus is adding a small
   noisy term (gaussian noise) to the gradient:

   the variance should be carefully chosen (for example, it could decay
   exponentially during the epochs). however, this method can be a very
   simple and effective solution to allow a movement even in regions where
   the gradient is close to zero.

momentum and nesterov momentum

   the previous approach was quite simple and in many cases, it can
   difficult to implement. a more robust solution is provided by
   introducing an [33]exponentially weighted moving average for the
   gradients. the idea is very intuitive: instead of considering only the
   current gradient, we can attach part of its history to the correction
   factor, so to avoid an abrupt change when the surface becomes flat. the
   momentum algorithm is, therefore:

   the first equation computes the correction factor considering the
   weight   . if    is small, the previous gradients are soon discarded. if,
   instead,        1, their effect continues for a longer time. a common
   value in many application is between 0.75 and 0.99, however, it   s
   important to consider    as a hyperparameter to adjust in every
   application. the second term performs the parameter update. in the
   following figure, there   s a vectorial representation of a momentum
   step:

   a slightly different variation is provided by the nesterov momentum.
   the difference with the base algorithm is that we first apply the
   correction with the current factor v(t) to determine the gradient and
   then compute v(t+1) and correct the parameters:

   this algorithm can improve the converge speed (the result has been
   theoretically proven by nesterov in the scope of mathematical
   optimization), however, in deep learning contexts, it doesn   t seem to
   produce excellent results. it can be implemented alone or in
   conjunction with the other algorithms that we   re going to discuss.

rmsprop

   this algorithm, proposed by g. hinton, is based on the idea to adapt
   the correction factor for each parameter, so to increase the effect on
   slowly-changing parameters and reduce it when their change magnitude is
   very large. this approach can dramatically improve the performance of a
   deep network, but it   s a little bit more expensive than momentum
   because we need to compute a speed term for each parameter:

   this term computes the exponentially weighted moving average of the
   gradient squared (element-wise). just like for momentum,    determines
   how fast the previous speeds are forgotten. the parameter update rule
   is:

      is the learning rate and    is a small constant (~ 1e-6    1e-5)
   introduced to avoid a division by zero when the speed is null. as it   s
   possible to see, each parameter is updated with a rule that is very
   similar to the vanilla stochastic id119, but the actual
   learning rate is adjusted per single parameter using the reciprocal of
   the square root of the relative speed. it   s easy to understand that
   large gradients determine large speeds and, adaptively, the
   corresponding update is smaller and vice-versa. rmsprop is a very
   powerful and flexible algorithm and it is widely used in deep
   id23, id98, and id56-based projects.

adam

   adam is an adaptive algorithm that could be considered as an extension
   of rmsprop. instead of considering the only exponentially weighted
   moving average of the gradient square, it computes also the same value
   for the gradient itself:

     1 and   2 are forgetting factors like in the other algorithms. the
   authors suggest values greater than 0.9. as both terms are moving
   estimations of the first and the second moment, they can be biased (see
   [34]this article for further information). adam provided a bias
   correction for both terms:

   the parameter update rule becomes:

   this rule can be considered as a standard rmsprop one with a momentum
   term. in fact, the correction term is made up of: (numerator) the
   moving average of the gradient and (denominator) the adaptive factor to
   modulate the magnitude of the change so to avoid different changing
   amplitudes for different parameters. the constant   , like for rmsprop,
   should be set equal to 1e-6 and it   s necessary to improve the numerical
   stability.

adagrad

   this is another adaptive algorithm based on the idea to consider the
   historical sum of the gradient square and set the correction factor of
   a parameter so to scale its value with the reciprocal of the squared
   historical sum. the concept is not very different from rmsprop and
   adam, but, in this case, we don   t use an exponentially weighted moving
   average, but the whole history. the accumulation step is:

   while the update step is exactly as rmsprop:

   adagrad shows good performances in many tasks, increasing the
   convergence speed, but it has a drawback deriving from accumulating the
   whole squared gradient history. as each term is non-negative, g         and
   the correction factor (which is the adaptive learning rate)     0.
   therefore, during the first iterations, adagrad can produce significant
   changes, but at the end of a long training process, the change rate is
   almost null.

adadelta

   adadelta is algorithm proposed by m. d. zeiler to solve the problem of
   adagrad. the idea is to consider a limited window instead of
   accumulating for the whole history. in particular, this result is
   achieved using an exponentially weighted moving average (like for
   rmsprop):

   a very interesting expedient introduced by adadelta is to normalize the
   parameter updates so to have the same unit of the parameter. in fact,
   if we consider rmsprop, we have:

   this means that an update is unitless. to avoid this problem, adadelta
   computes the exponentially weighted average of the squared updates and
   apply this update rule:

   this algorithm showed very good performances in many deep learning
   tasks and it   s less expensive than adagrad. the best value for the
   constant    should be assessed through cross-validation, however, 1e-6
   is a good default for many tasks, while    can be set greater than 0.9
   in order to avoid a predominance of old gradients and updates.

conclusions

   stochastic id119 is intrinsically a powerful method,
   however, in non-convex scenarios, its performances can be degraded. we
   have explored different algorithms (most of them are currently the
   first choice in deep learning tasks), showing their strengths and
   weaknesses. now the question is: which is the best? the answer,
   unfortunately, doesn   t exist. all of them can perform well in some
   contexts and bad in others. in general, all adaptive methods tend to
   show similar behaviors, but every problem is a separate universe and
   the only silver bullet we have is trial and error. i hope the
   exploration has been clear and any constructive comment or question is
   welcome!

references:

     * goodfellow i., bengio y., courville a., [35]deep learning, the mit
       press
     * duchi j., hazan e., singer y., [36]adaptive subgradient methods for
       online learning and stochastic optimization, journal of machine
       learning research 12 (2011) 2121-2159
     * zeiler m. d., adadelta: [37]an adaptive learning rate method,
       arxiv:1212.5701

   see also:

[38]quickprop: an almost forgotten neural training algorithm     giuseppe
bonaccorso

     standard back-propagation is probably the best neural training
     algorithm for shallow and deep networks, however, it is based on the
     chain rule of derivatives and an update in the first layers requires
     a knowledge back-propagated from the last layer.

share:

     * [39]click to share on twitter (opens in new window)
     * [40]click to share on facebook (opens in new window)
     * [41]click to share on linkedin (opens in new window)
     * [42]click to share on pocket (opens in new window)
     * [43]click to share on tumblr (opens in new window)
     * [44]click to share on reddit (opens in new window)
     * [45]click to share on pinterest (opens in new window)
     * [46]click to share on skype (opens in new window)
     * [47]click to share on whatsapp (opens in new window)
     * [48]click to share on telegram (opens in new window)
     * [49]click to email this to a friend (opens in new window)
     * [50]click to print (opens in new window)
     *

you can also be interested in these articles:

   [51]adadelta[52]adagrad[53]adam[54]algorithms[55]deep
   learning[56]machine learning[57]rmsprop[58]sgd

post navigation

   [59]a virtual jacques lacan discusses about artificial intelligence
   [60]ml algorithms addendum: passive aggressive algorithms

leave a reply [61]cancel reply

   iframe: [62]jetpack_remote_comment

follow me

     * [63]linkedin
     * [64]twitter
     * [65]facebook
     * [66]github
     * [67]instagram
     * [68]amazon
     * [69]medium
     * [70]rss

search articles

   ____________________ (button)

latest blog posts

     * [71]machine learning algorithms     second edition 08/28/2018
     * [72]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [73]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [74]a book that every data scientist should read 06/22/2018
     * [75]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [76]my tweets

   copyright    2019 [77]giuseppe bonaccorso. all rights reserved.
   [78]privacy policy - [79]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [80]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/
  26. https://www.bonaccorso.eu/category/generic/artificial-intelligence/
  27. https://www.bonaccorso.eu/category/machine-learning/deep-learning/
  28. https://www.bonaccorso.eu/category/generic/
  29. https://www.bonaccorso.eu/category/machine-learning/
  30. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
  31. https://www.bonaccorso.eu/category/machine-learning/neural-networks/
  32. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/#comments
  33. https://en.wikipedia.org/wiki/moving_average#exponential_moving_average
  34. https://en.wikipedia.org/wiki/unbiased_estimation_of_standard_deviation
  35. https://amzn.to/2fqqqlg
  36. http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf
  37. https://arxiv.org/pdf/1212.5701.pdf
  38. https://www.bonaccorso.eu/2017/09/15/quickprop-an-almost-forgotten-neural-training-algorithm/
  39. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=twitter
  40. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=facebook
  41. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=linkedin
  42. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=pocket
  43. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=tumblr
  44. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=reddit
  45. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=pinterest
  46. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=skype
  47. https://api.whatsapp.com/send?text=a brief (and comprehensive) guide to stochastic id119 algorithms https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/
  48. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=telegram
  49. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/?share=email
  50. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/#print
  51. https://www.bonaccorso.eu/tag/adadelta/
  52. https://www.bonaccorso.eu/tag/adagrad/
  53. https://www.bonaccorso.eu/tag/adam/
  54. https://www.bonaccorso.eu/tag/algorithms/
  55. https://www.bonaccorso.eu/tag/deep-learning/
  56. https://www.bonaccorso.eu/tag/machine-learning/
  57. https://www.bonaccorso.eu/tag/rmsprop/
  58. https://www.bonaccorso.eu/tag/sgd/
  59. https://www.bonaccorso.eu/2017/10/01/virtual-jacques-lacan-discusses-artificial-intelligence/
  60. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/
  61. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/#respond
  62. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=1618&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=fe80ecc7a92b624fbf51f328b0a87e3a226488d9#parent=https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/
  63. https://www.linkedin.com/in/giuseppebonaccorso/
  64. https://twitter.com/giuseppeb/
  65. https://www.facebook.com/giuseppe.bonaccorso/
  66. https://github.com/giuseppebonaccorso/
  67. https://www.instagram.com/giuseppebonaccorso/
  68. https://www.amazon.com/author/giuseppebonaccorso
  69. https://medium.com/@giuseppe.bonaccorso
  70. https://www.bonaccorso.eu/feed/
  71. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
  72. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
  73. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
  74. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
  75. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
  76. https://twitter.com/giuseppeb
  77. https://www.bonaccorso.eu/
  78. https://www.iubenda.com/privacy-policy/331721
  79. https://www.iubenda.com/privacy-policy/331721/cookie-policy
  80. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/#cancel

   hidden links:
  82. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
