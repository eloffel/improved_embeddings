   #[1]machine thoughts    feed [2]machine thoughts    comments feed
   [3]machine thoughts    comprehension based id38 comments
   feed [4]cognitive architectures [5]formalism, platonism and mentalese
   [6]alternate [7]alternate [8]machine thoughts [9]wordpress.com

   [10]machine thoughts
   [11]skip to content
     * [12]home
     * [13]about

   [14]    cognitive architectures
   [15]formalism, platonism and mentalese    

[16]comprehension based id38

   posted on [17]november 15, 2016 by [18]mcallester

   one of the holy grails of the modern deep learning community is to
   develop effective methods for unsupervised learning.  i have always
   held out hope that the semantics of english could be learned from raw
   unlabeled text.  the plausibility of a statement should affect the
   id203 of that statement. it would seem that a perfect language
   model     one approaching the true perplexity or id178 of english    
   must incorporate semantics.  for this reason i read with great interest
   the recent [19]dramatic improvement in id38 achieved
   by jozefowicz et. al. at google brain.  they report a perplexity of
   about 30 on google   s [20]one billion word benchmark.  perplexities
   below 60 have been very difficult to achieve.

   at ttic we have been working on machine comprehension as another
   direction for the acquisition of semantics. we created a [21]   who did
   what    benchmark for reading comprehension based on the [22]ldc gigaword
   corpus.  i will discuss reading comprehension a bit more below, but i
   first want to focus on the nature of news corpora.  we found that the
   gigaword corpus contained multiple articles on the same events.  we
   exploited this fact in the construction of our who-did-what dataset.  i
   was curious how the billion word benchmark handled this multiplicity.
   presumably the training data contained information about the same
   entities and events that occur in the test sentences. what is the
   id178 of a statement under the listener   s id203 distribution
   when the listener knows (the semantic content of) what the speaker is
   going to say?  to make this issue clear, here are some examples of
   training and test sentences from google   s one billion word corpus:

   train: saudi arabia on wednesday decided to drop the widely used west
   texas intermediate oil contract as the benchmark for pricing its oil.

   test: saudi arabia , the world    s largest oil exporter , in 2009
   dropped the widely used wti oil contract as the benchmark for pricing
   its oil to customers in the us.

   train: bobby salcedo was re-elected in november to a second term on the
   board of the el monte city school district.

   test: bobby salcedo was first elected to the school board in 2004 and
   was re-elected in november.

   these examples were found by running the lucene information retrieval
   system on the training data using the test sentence as the query
   (thanks to hai wang).  these examples suggest that id38 is
   related to reading comprehension.  reading comprehension is the task of
   answering a multiple choice question about a passage involving entities
   and events not present in general structured knowledge sources.
   recently several large scale reading comprehension benchmarks have been
   created with cloze-style questions     questions formed by deleting a
   word or phrase from a corpus sentence or article summary.  cloze-style
   reading comprehension benchmarks include the [23]id98/daily mail
   benchmark, the [24]children   s book test, and our who-did-what benchmark
   mentioned above.  there is also a recently constructed [25]lambada
   dataset which is intended as a id38 benchmark but which
   can be approached most effectively as a reading comprehension task
   ([26]chu et. al.).  i predict a convergence of reading comprehension
   and id38     comprehension based id38.

   for comprehension based id38 to make sense one should have
   training data describing the same entities and events as those
   occurring in the test data.  it might be good to measure the id178
   (or perplexity) of just the content words, or even just named entities.
    but one should of course avoid including the test sentences in the
   training data. it turns out that google   s billion word benchmark has a
   problem in this regard.  we found the following examples:

   train: al qaeda deputy leader ayman al-zawahri called on muslims in a
   new audiotape released monday to strike jewish and american targets in
   revenge for israel    s offensive in the gaza strip earlier this month.

   test: al-qaida deputy leader ayman al-zawahri called on muslims in a
   new audiotape released monday to strike jewish and american targets in
   revenge for israel    s recent offensive in the gaza strip.

   train: rbs shares have only recently risen past the average level paid
   by the government , but lloyds shares are still low despite a recent
   banking sector rally.

   test: rbs shares have only recently risen past the average level paid
   by the government , but lloyds shares are still languishing despite the
   recent banking sector rally .

   train: washington     aids patients should have a genetic test before
   treatment with glaxosmithkline plc    s drug ziagen to see whether they
   face a higher risk of a potentially fatal reaction , u.s. regulators
   said on thursday .

   test: washington ( reuters )     aids patients should be given a genetic
   test before treatment with glaxosmithkline plc    s drug , ziagen , to
   see if they face a higher risk of a potentially fatal reaction , u.s.
   regulators said on thursday .

   these example occur when slightly edited versions of an article appear
   on different newswires or in article revisions. although we have not
   done a careful study, it appears that something like half of the test
   sentences in google   s billion word benchmark are essentially duplicates
   of training sentences.

   in spite of the problems with the billion word benchmark, an
   appropriate benchmark for comprehension based id38 should
   be easy to construct.  for example, we could require that the test
   sentences be separated by, say, a week from any training sentence.  or
   we could simply de-duplicate the articles using a soft-match criterion
   for duplication.  the training data should consist of complete articles
   rather than shuffled sentences.  also note that new test data is
   continuously available     it is hard to overfit next week   s news.

   these are exciting times.  can we obtain a command of english in a
   machine simply by training on very large corpora of text?  i find it
   fun to be optimistic.
   advertisements

share this:

     * [27]twitter
     * [28]facebook
     *

like this:

   like loading...

related

   this entry was posted in [29]uncategorized. bookmark the [30]permalink.
   [31]    cognitive architectures
   [32]formalism, platonism and mentalese    

leave a reply [33]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [34]googleplus-sign-in

     *
     *

   [35]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [36]log out /
   [37]change )
   google photo

   you are commenting using your google account. ( [38]log out /
   [39]change )
   twitter picture

   you are commenting using your twitter account. ( [40]log out /
   [41]change )
   facebook photo

   you are commenting using your facebook account. ( [42]log out /
   [43]change )
   [44]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * recent posts
          + [45]thoughts from ttic31230: rethinking generalization.
          + [46]thoughts from ttic31230: langevin dynamics and the
            struggle for the soul of sgd.
          + [47]thoughts from ttic31230: hyperparameter conjugacy
          + [48]superintelligence and the truth
          + [49]predictive coding and mutual information
          + [50]rl and the game of mathematics
          + [51]the role of theory in deep learning
          + [52]choice as a natural kind term
          + [53]ctc and the eg algotithm: discrete latent choices without
            id23
          + [54]vae = em
          + [55]deep meaning beyond thought vectors
          + [56]the plausibility of near-term machine sentience.
          + [57]formalism, platonism and mentalese
          + [58]comprehension based id38
          + [59]cognitive architectures
          + [60]architectures and language instincts
          + [61]why we need a new foundation of mathematics
          + [62]the foundations of mathematics.
          + [63]friendly ai and the servant mission
          + [64]ai and free will: when do choices exist?
       advertisements

   [65]machine thoughts
   [66]wordpress.com.

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [67]cookie policy

   iframe: [68]likes-master

   %d bloggers like this:

references

   visible links
   1. https://machinethoughts.wordpress.com/feed/
   2. https://machinethoughts.wordpress.com/comments/feed/
   3. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/feed/
   4. https://machinethoughts.wordpress.com/2016/06/20/cognitive-architectures/
   5. https://machinethoughts.wordpress.com/2017/07/17/formalism-platonism-and-mentalese/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/&for=wpcom-auto-discovery
   8. https://machinethoughts.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://machinethoughts.wordpress.com/
  11. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/#content
  12. https://machinethoughts.wordpress.com/
  13. https://machinethoughts.wordpress.com/about/
  14. https://machinethoughts.wordpress.com/2016/06/20/cognitive-architectures/
  15. https://machinethoughts.wordpress.com/2017/07/17/formalism-platonism-and-mentalese/
  16. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  17. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  18. https://machinethoughts.wordpress.com/author/dmcallester/
  19. https://arxiv.org/pdf/1602.02410.pdf
  20. http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41880.pdf
  21. https://arxiv.org/abs/1608.05457
  22. http://www.statmt.org/lm-benchmark/
  23. https://arxiv.org/abs/1506.03340
  24. https://arxiv.org/pdf/1511.02301.pdf
  25. http://clic.cimec.unitn.it/marco/publications/lambada-acl2016.pdf
  26. https://arxiv.org/abs/1610.08431
  27. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/?share=twitter
  28. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/?share=facebook
  29. https://machinethoughts.wordpress.com/category/uncategorized/
  30. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  31. https://machinethoughts.wordpress.com/2016/06/20/cognitive-architectures/
  32. https://machinethoughts.wordpress.com/2017/07/17/formalism-platonism-and-mentalese/
  33. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/#respond
  34. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://machinethoughts.wordpress.com&color_scheme=light
  35. https://gravatar.com/site/signup/
  36. javascript:highlandercomments.doexternallogout( 'wordpress' );
  37. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  38. javascript:highlandercomments.doexternallogout( 'googleplus' );
  39. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  40. javascript:highlandercomments.doexternallogout( 'twitter' );
  41. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  42. javascript:highlandercomments.doexternallogout( 'facebook' );
  43. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  44. javascript:highlandercomments.cancelexternalwindow();
  45. https://machinethoughts.wordpress.com/2019/04/03/thoughts-from-ttic21230-rethinking-generalization/
  46. https://machinethoughts.wordpress.com/2019/03/27/thoughts-from-ttic31230-langevin-dynamics-and-the-struggle-for-the-soul-of-sgd/
  47. https://machinethoughts.wordpress.com/2019/03/20/thoughts-from-ttic31230-hyperparameter-conjugacy/
  48. https://machinethoughts.wordpress.com/2018/09/18/superintelligence-and-the-truth/
  49. https://machinethoughts.wordpress.com/2018/08/15/predictive-coding-and-mutual-information/
  50. https://machinethoughts.wordpress.com/2018/04/14/rl-and-the-game-of-mathematics/
  51. https://machinethoughts.wordpress.com/2017/12/08/the-role-of-theory-in-deep-learning/
  52. https://machinethoughts.wordpress.com/2017/11/29/choice-as-a-natural-kind-term/
  53. https://machinethoughts.wordpress.com/2017/11/02/ctc-training-latent-discrete-sequential-decisions-without-rl/
  54. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  55. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  56. https://machinethoughts.wordpress.com/2017/07/22/the-plausibility-of-near-term-machine-sentience/
  57. https://machinethoughts.wordpress.com/2017/07/17/formalism-platonism-and-mentalese/
  58. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  59. https://machinethoughts.wordpress.com/2016/06/20/cognitive-architectures/
  60. https://machinethoughts.wordpress.com/2016/06/12/architectures-and-language-instincts/
  61. https://machinethoughts.wordpress.com/2016/01/01/why-we-need-a-new-foundation-of-mathematics/
  62. https://machinethoughts.wordpress.com/2015/01/27/the-foundations-of-mathematics-2/
  63. https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/
  64. https://machinethoughts.wordpress.com/2014/08/03/ai-and-free-will-when-do-choices-exist/
  65. https://machinethoughts.wordpress.com/
  66. https://wordpress.com/?ref=footer_custom_com
  67. https://automattic.com/cookies
  68. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  70. https://machinethoughts.wordpress.com/
  71. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/#comment-form-guest
  72. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/#comment-form-load-service:wordpress.com
  73. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/#comment-form-load-service:twitter
  74. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/#comment-form-load-service:facebook
