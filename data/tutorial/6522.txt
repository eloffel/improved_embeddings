   #[1]machine thoughts    feed [2]machine thoughts    comments feed
   [3]machine thoughts    vae = em comments feed [4]deep meaning beyond
   thought vectors [5]ctc and the eg algotithm: discrete latent choices
   without id23 [6]alternate [7]alternate [8]machine
   thoughts [9]wordpress.com

   [10]machine thoughts
   [11]skip to content
     * [12]home
     * [13]about

   [14]    deep meaning beyond thought vectors
   [15]ctc and the eg algotithm: discrete latent choices without
   id23    

[16]vae = em

   posted on [17]october 2, 2017 by [18]mcallester

   i recently realized the connection between the expectation maximization
   algorithm (em) and id5 (vae).  both optimize the
   same objective function where vae performs id119 based on a
   sampling estimate of the gradient while em performs exact alternating
   maximization in models where this is possible.  it turns out that the
   alternating optimization view of em is described in section 9.4 of
   bishop   s 2006 text on machine learning and also in section 19.2 of the
   text by goodfellow, bengio and courville.   i had overlooked this
   connection between em and vaes, and it seems important, so here is a
   blog post that helps me, at least, organize my thinking.

   first some background.

   the [19]expectation maximization (em) algorithm is a time-honored
   cornerstone of machine learning.  its general formulation was given in
   a classic 1977 paper by dempster, laird and rubin, although many
   special cases had been developed prior to that.  the algorithm is used
   for estimating unknown parameters of a id203 distribution
   involving latent variables.  the algorithm is usually introduced by
   looking at the special case of modeling a collection of points by a
   mixture of gaussians.  here we are given points where the model assumes
   that each point came from an unlabeled component of the mixture (the
   component associated with the point is a latent variable). given the
   points one must estimate a component weight, a mean and a covariance
   matrix for each component of the mixture.

   screen shot 2017-10-02 at 10.06.41 am.png

   this is a non-id76 problem. em iteratively re-estimates
   the parameters with a guarantee that for each iteration, unless one is
   already at a parameter setting with zero id203 gradients (a
   stationary point), the id203 of the points under the model
   improves.  there are numerous important special cases such as learning
   the parameters of a hidden markov model or a probabilistic context free
   grammar.

   [20]id5 (vaes) ([21]tutorial) were introduced in
   2014 by kingma and welling in the context of deep learning.
   autoencoding is related to compression.  jpeg compresses an image into
   an encoded form that uses less memory but can then be decompressed to
   get the image back with some loss of resolution.

   screen shot 2017-10-02 at 10.09.13 am.png
   x \hspace{1.0in}p_\psi(z|x) \hspace{1.0in} z \hspace{1.0in}
   p_\theta(z,x)\hspace{1.0in}x'

   [kevin franz]

   information theoretically, compression is closely relate to
   distribution modeling. shannon   s source coding theorem states that for
   any id203 distribution, such as the distribution on    natural   
   images, one can (block) code the elements using a number of bits per
   element equal to the information-theoretic id178 of the distribution.
    in the above figure the average number of bits in the compressed form
   in the middle should be equal to the id178 of the id203
   distribution over images.

   vaes, however, do not directly address the compression problem.
   instead they focus on fitting the parameters \theta of the distribution
   p_\theta(z,x) so as to maximize the marginal id203 p_\theta(x) of
   a given set x of samples (such as a set of points or a set of images).
   the encoder is just a tool to make this parameter fitting more
   efficient. the problem being solved by a vae is the same as the problem
   being solved by em     fitting the parameters of a id203
   distribution to given data where the model includes latent variables
   not specified in the data.

   vae = em. both em and vaes are formulated in terms of the following
   objective function on the two distributions p_\psi(z|y) (the encoder)
   and p_\theta(y,z) (the model defining p(z) and p(y|z) ).

   \psi^*,\theta^* = \mathrm{argmax}_{\psi,\theta}\;\sum_y\;{\cal
   l}(\psi,\theta,y)

   {\cal l}(\psi,\theta,y) = e_{z \sim p_\psi(z|y)}\; \ln p_\theta(z,y) +
   h(p_\psi(z|y))\;\;\;(1)

   = \ln\;p_{\theta}(y) - kl(p_\psi(z|y),p_{\theta}(z|y))\;\;\;(2)

   here y is summed over training data. the equivalence of  (1) and (2) is
   important as they provide different outlooks on the objective.   the
   equivalence is implied by the following observation.

   e_{z \sim p_\psi(z|y)} \;\ln p_\theta(z,y)
   = e_{z \sim p_\psi(z|y)} \;\ln p_\theta(y)p_\theta(z|y)
   = e_{z \sim p_\psi(z|y)}\;\ln p_\theta(y) + \;e_{z \sim
   p_\psi(z|y)}\;\ln p_\theta(z|y)
   = \ln p_\theta(y) + \;e_{z \sim p_\psi(z|y)}\;\ln p_\theta(z|y)

   em can be defined as alternating optimization of \sum_y {\cal
   l}(\psi,\phi,y) where (2) supports the optimization of \psi (the e
   step) and (1) supports the optimization of \theta (the m step).  the em
   algorithm applies to models where both the e step and the m step can be
   solved efficiently exactly and for such models each m step is
   guaranteed to improve \sum _y \ln p_\theta(y) .  for these models this
   alternating optimization is typically far superior to any form of
   id119 on \sum_y \ln p_\theta(y) .

   vaes are used in models where the alternating optimization cannot be
   done efficiently in closed form.  a vae performs id119 on
   (1) where the gradient is estimated by sampling z from the encoder
   p_\psi(z|y) .  this typically involves a    reparameterization trick   .
   one represents p_\psi(z|y) by taking z = g_\psi(y,\eta) where \eta is a
   random variable of a fixed distribution. we can then rewrite (1) as

   {\cal l}(\psi,\theta,y) = e_{\eta}\; \ln p_\theta(g_\psi(y,\eta),y) +
   h(p_\psi(g_\psi(y,\eta)|y))\;\;\;(3)

   it is common to take p(\eta) ,  p_\psi(z|y) , p_\theta(z) and p_\theta
   (y|z) to all be high dimensional gaussians each represented by a mean
   and a (diagonal) covariance . the noise variable \eta is taken to be a
   fixed zero mean isotropic gaussian while p_\psi(z|y) , p_\theta(z) and
   p_\theta (y|z) have means and covariances computed by deep networks.
   we can now calculate the gradient of (3) for any given sample of \eta .

   it is not clear whether the term    variational auto encoder    should
   imply the use of gaussians.  goodfellow et al. seem to take the
   position that vaes are defined by id119 on (1) where the
   gradient is estimated by sampling z from p_\psi(z|y) .  this does not
   necessarily involve gaussians.  [22]a version of id48-vaes has recently
   been formulated which includes discrete latent id48 states.  however the
   observed variable y is still continuous and gaussians are still
   involved.  presumably one can formulate vaes where both y and z are
   structured discrete variables, such a language model with discrete
   latent states.  these vaes would presumably not involve gaussians.

   other approaches to optimizing {\cal l}(\psi,\theta,y) are possible.
   the id103 algorithm ctc uses an exact e step but a
   gradient m step and achieves id119 on deep networks with
   discrete latent variables but without latent variable sampling.  see
   the following blog post for a discussion of the ctc approach which we
   might call the eg (expected gradient) algorithm.
   advertisements

share this:

     * [23]twitter
     * [24]facebook
     *

like this:

   like loading...

related

   this entry was posted in [25]uncategorized. bookmark the [26]permalink.
   [27]    deep meaning beyond thought vectors
   [28]ctc and the eg algotithm: discrete latent choices without
   id23    

leave a reply [29]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [30]googleplus-sign-in

     *
     *

   [31]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [32]log out /
   [33]change )
   google photo

   you are commenting using your google account. ( [34]log out /
   [35]change )
   twitter picture

   you are commenting using your twitter account. ( [36]log out /
   [37]change )
   facebook photo

   you are commenting using your facebook account. ( [38]log out /
   [39]change )
   [40]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * recent posts
          + [41]thoughts from ttic31230: rethinking generalization.
          + [42]thoughts from ttic31230: langevin dynamics and the
            struggle for the soul of sgd.
          + [43]thoughts from ttic31230: hyperparameter conjugacy
          + [44]superintelligence and the truth
          + [45]predictive coding and mutual information
          + [46]rl and the game of mathematics
          + [47]the role of theory in deep learning
          + [48]choice as a natural kind term
          + [49]ctc and the eg algotithm: discrete latent choices without
            id23
          + [50]vae = em
          + [51]deep meaning beyond thought vectors
          + [52]the plausibility of near-term machine sentience.
          + [53]formalism, platonism and mentalese
          + [54]comprehension based id38
          + [55]cognitive architectures
          + [56]architectures and language instincts
          + [57]why we need a new foundation of mathematics
          + [58]the foundations of mathematics.
          + [59]friendly ai and the servant mission
          + [60]ai and free will: when do choices exist?
       advertisements

   [61]machine thoughts
   [62]wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [63]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [64]cookie policy

   iframe: [65]likes-master

   %d bloggers like this:

references

   visible links
   1. https://machinethoughts.wordpress.com/feed/
   2. https://machinethoughts.wordpress.com/comments/feed/
   3. https://machinethoughts.wordpress.com/2017/10/02/vae-em/feed/
   4. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
   5. https://machinethoughts.wordpress.com/2017/11/02/ctc-training-latent-discrete-sequential-decisions-without-rl/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://machinethoughts.wordpress.com/2017/10/02/vae-em/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://machinethoughts.wordpress.com/2017/10/02/vae-em/&for=wpcom-auto-discovery
   8. https://machinethoughts.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://machinethoughts.wordpress.com/
  11. https://machinethoughts.wordpress.com/2017/10/02/vae-em/#content
  12. https://machinethoughts.wordpress.com/
  13. https://machinethoughts.wordpress.com/about/
  14. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  15. https://machinethoughts.wordpress.com/2017/11/02/ctc-training-latent-discrete-sequential-decisions-without-rl/
  16. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  17. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  18. https://machinethoughts.wordpress.com/author/dmcallester/
  19. https://en.wikipedia.org/wiki/expectation   maximization_algorithm
  20. https://arxiv.org/abs/1312.6114
  21. https://arxiv.org/abs/1606.05908
  22. https://groups.uni-paderborn.de/nt/pubs/2017/interspeech_2017_ebbers_paper.pdf
  23. https://machinethoughts.wordpress.com/2017/10/02/vae-em/?share=twitter
  24. https://machinethoughts.wordpress.com/2017/10/02/vae-em/?share=facebook
  25. https://machinethoughts.wordpress.com/category/uncategorized/
  26. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  27. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  28. https://machinethoughts.wordpress.com/2017/11/02/ctc-training-latent-discrete-sequential-decisions-without-rl/
  29. https://machinethoughts.wordpress.com/2017/10/02/vae-em/#respond
  30. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://machinethoughts.wordpress.com&color_scheme=light
  31. https://gravatar.com/site/signup/
  32. javascript:highlandercomments.doexternallogout( 'wordpress' );
  33. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  34. javascript:highlandercomments.doexternallogout( 'googleplus' );
  35. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  36. javascript:highlandercomments.doexternallogout( 'twitter' );
  37. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  38. javascript:highlandercomments.doexternallogout( 'facebook' );
  39. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  40. javascript:highlandercomments.cancelexternalwindow();
  41. https://machinethoughts.wordpress.com/2019/04/03/thoughts-from-ttic21230-rethinking-generalization/
  42. https://machinethoughts.wordpress.com/2019/03/27/thoughts-from-ttic31230-langevin-dynamics-and-the-struggle-for-the-soul-of-sgd/
  43. https://machinethoughts.wordpress.com/2019/03/20/thoughts-from-ttic31230-hyperparameter-conjugacy/
  44. https://machinethoughts.wordpress.com/2018/09/18/superintelligence-and-the-truth/
  45. https://machinethoughts.wordpress.com/2018/08/15/predictive-coding-and-mutual-information/
  46. https://machinethoughts.wordpress.com/2018/04/14/rl-and-the-game-of-mathematics/
  47. https://machinethoughts.wordpress.com/2017/12/08/the-role-of-theory-in-deep-learning/
  48. https://machinethoughts.wordpress.com/2017/11/29/choice-as-a-natural-kind-term/
  49. https://machinethoughts.wordpress.com/2017/11/02/ctc-training-latent-discrete-sequential-decisions-without-rl/
  50. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  51. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  52. https://machinethoughts.wordpress.com/2017/07/22/the-plausibility-of-near-term-machine-sentience/
  53. https://machinethoughts.wordpress.com/2017/07/17/formalism-platonism-and-mentalese/
  54. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  55. https://machinethoughts.wordpress.com/2016/06/20/cognitive-architectures/
  56. https://machinethoughts.wordpress.com/2016/06/12/architectures-and-language-instincts/
  57. https://machinethoughts.wordpress.com/2016/01/01/why-we-need-a-new-foundation-of-mathematics/
  58. https://machinethoughts.wordpress.com/2015/01/27/the-foundations-of-mathematics-2/
  59. https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/
  60. https://machinethoughts.wordpress.com/2014/08/03/ai-and-free-will-when-do-choices-exist/
  61. https://machinethoughts.wordpress.com/
  62. https://wordpress.com/?ref=footer_custom_com
  63. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  64. https://automattic.com/cookies
  65. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  67. https://machinethoughts.wordpress.com/
  68. https://machinethoughts.wordpress.com/2017/10/02/vae-em/#comment-form-guest
  69. https://machinethoughts.wordpress.com/2017/10/02/vae-em/#comment-form-load-service:wordpress.com
  70. https://machinethoughts.wordpress.com/2017/10/02/vae-em/#comment-form-load-service:twitter
  71. https://machinethoughts.wordpress.com/2017/10/02/vae-em/#comment-form-load-service:facebook
