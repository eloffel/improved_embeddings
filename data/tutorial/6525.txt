   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    ml algorithms addendum: hebbian learning
   comments feed [4]alternate [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

ml algorithms addendum: hebbian learning

   [25]08/21/201709/30/2017[26]artificial intelligence, [27]computational
   neuroscience, [28]machine learning, [29]machine learning algorithms
   addenda, [30]neural networks, [31]python[32]no comments

   [33]hebbian learning is one the most famous learning theories, proposed
   by the canadian psychologist [34]donald hebb in 1949, many years before
   his results were confirmed through neuroscientific experiments.
   artificial intelligence researchers immediately understood the
   importance of his theory when applied to id158s
   and, even if more efficient algorithms have been adopted in order to
   solve complex problems, neuroscience continues finding more and more
   evidence of natural neurons whose learning process is almost perfectly
   modeled by hebb   s equations.

   hebb   s rule is very simple and can be discussed starting from a
   high-level structure of a neuron with a single output:

   we are considering a linear neuron, therefore the output y is a linear
   combination of its input values x:

   according to the hebbian theory, if both pre- and post-synaptic units
   behave in the same way (firing or remaining in the steady state), the
   corresponding synaptic weight will be reinforced. vice versa, if their
   behavior is discordant, w will be weakened. in other words, using a
   famous aphorism,    neurons that fire together, wire together   . from a
   mathematical viewpoint, this rule can expressed (in a discretized
   version) as:

   alpha is the learning rate. to better understand the implications of
   this rule, it   s useful to express it using vectors:

   c is the input correlation matrix (if the samples are zero-centered, c
   is also the covariance matrix), therefore the weight vector will be
   updated in a way that maximizes the components corresponding to the
   input maximum variance direction. in particular, considering the
   time-continuous version, if c has a dominant eigenvalue, the solution
   w(t) can be expressed as a vector with the same direction of the
   corresponding c eigenvector. in other words, hebbian learning is
   performing a pca, extracting the first principal component.

   neuron sketch even if, this approach is feasible, there   s a problem in
   this rule: it is unstable. c is a positive semidefinite matrix,
   therefore its eigenvalues are always non-negative. that means the w(t)
   will be a linear combination of eigenvectors with coefficients
   increasing exponentially with t. considering the discrete version, it   s
   easy to understand that, if x and y are greater than 1, the learning
   process will increase the absolute value of the weights indefinitely,
   producing an overflow.

   this problem can be solved by imposing the id172 of weights
   after each update (so to make them saturate to finite values), but this
   solution is biologically unlikely because each synapse needs to know
   all other weights. the best alternative approach has been proposed by
   [35]oja and the rule is named after him:

   the rule is always hebbian, but it now includes an auto-normalizing
   term (-wy  ). it   s easy to show the corresponding time-continuous
   differential equation has now negative eigenvalues and the solution
   w(t) converges. just like in the pure hebb   s rule, he vector w will
   always converge to the dominant c eigenvector, but in this case its
   norm will be a finite (small) number.

   implementing the oja   s rule in python (with tensorflow too), it   s a
   very easy task. let   s start with a random bidimensional centered
   dataset (obtained using scikit-learn make_blobs() and
   standardscaler()):


   in the following [36]gist, the covariance (correlation) matrix is
   computed, together with its eigenvectors and then the oja   s rule is
   applied to the dataset:
   view the code on [37]gist.

   as it   s possible to see, the algorithm has converged to the second
   eigenvector, whose corresponding eigenvalue is the highest.

   an extension to the oja   s rule to multi-output networks is provided by
   the sanger   s rule (also known as generalized hebbian algorithm):

   in this case, the normalizing (and decorrelating) factor is applied
   considering only the synaptic weights before the current one
   (included). using a vectorial notation, the update rule becomes:

   tril() is a function the returns the lower triangle of a square matrix.
   sanger   s rule is able to extract all principal components starting from
   the first and continuing with all output units. just like, for oja   s
   rule, in the following [38]gist, the rule is applied to the same
   dataset:
   view the code on [39]gist.

   as it   s possible to see, the weight matrix contains (as columns) the
   two principal components (roughly parallel to the eigenvectors of c).

   hebbian learning is a very powerful unsupervised approach, thanks to
   its simplicity and biological evidence. it   s easy to apply this
   methodology to different bio-inspired problems like orientation
   sensitivity. i recommend [1] for further details about these techniques
   and other neuroscientific models.

   references:
    1. dayan p., abbott l. f., [40]theoretical neuroscience: computational
       and mathematical modeling of neural systems, the mit press

   see also:

[41]hodgkin-huxley spiking neuron model in python     giuseppe bonaccorso

     the hodgkin-huxley model (published on 1952 in the journal of
     physiology [1]) is the most famous spiking neuron model (also if
     there are simpler alternatives like the    integrate-and-fire    model
     which performs quite well). it   s made up of a system of four
     ordinary differential equations that can be easily integrated using
     several different tools.

share:

     * [42]click to share on twitter (opens in new window)
     * [43]click to share on facebook (opens in new window)
     * [44]click to share on linkedin (opens in new window)
     * [45]click to share on pocket (opens in new window)
     * [46]click to share on tumblr (opens in new window)
     * [47]click to share on reddit (opens in new window)
     * [48]click to share on pinterest (opens in new window)
     * [49]click to share on skype (opens in new window)
     * [50]click to share on whatsapp (opens in new window)
     * [51]click to share on telegram (opens in new window)
     * [52]click to email this to a friend (opens in new window)
     * [53]click to print (opens in new window)
     *

you can also be interested in these articles:

   [54]hebb[55]machine learning[56]neural networks[57]neuroscience

post navigation

   [58]hodgkin-huxley spiking neuron model in python
   [59]ml algorithms addendum: instance based learning

leave a reply [60]cancel reply

   iframe: [61]jetpack_remote_comment

follow me

     * [62]linkedin
     * [63]twitter
     * [64]facebook
     * [65]github
     * [66]instagram
     * [67]amazon
     * [68]medium
     * [69]rss

search articles

   ____________________ (button)

latest blog posts

     * [70]machine learning algorithms     second edition 08/28/2018
     * [71]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [72]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [73]a book that every data scientist should read 06/22/2018
     * [74]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [75]my tweets

   copyright    2019 [76]giuseppe bonaccorso. all rights reserved.
   [77]privacy policy - [78]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [79]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/
  26. https://www.bonaccorso.eu/category/generic/artificial-intelligence/
  27. https://www.bonaccorso.eu/category/computational-neuroscience/
  28. https://www.bonaccorso.eu/category/machine-learning/
  29. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
  30. https://www.bonaccorso.eu/category/machine-learning/neural-networks/
  31. https://www.bonaccorso.eu/category/software/python/
  32. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/#comments
  33. https://en.wikipedia.org/wiki/hebbian_theory
  34. https://en.wikipedia.org/wiki/donald_hebb
  35. https://en.wikipedia.org/wiki/erkki_oja
  36. https://gist.github.com/giuseppebonaccorso/7a8c60ffdd9426399a42bdca6dda7bf5
  37. https://gist.github.com/giuseppebonaccorso/7a8c60ffdd9426399a42bdca6dda7bf5
  38. https://gist.github.com/giuseppebonaccorso/54a06fc45d4b5e2845545757f02f5f2b
  39. https://gist.github.com/giuseppebonaccorso/54a06fc45d4b5e2845545757f02f5f2b
  40. https://www.amazon.com/theoretical-neuroscience-computational-mathematical-modeling/dp/0262541858/ref=sr_1_1?s=books&ie=utf8&qid=1503323179&sr=1-1&keywords=abbott+neuroscience
  41. https://www.bonaccorso.eu/2017/08/19/hodgkin-huxley-spiking-neuron-model-python/
  42. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=twitter
  43. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=facebook
  44. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=linkedin
  45. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=pocket
  46. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=tumblr
  47. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=reddit
  48. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=pinterest
  49. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=skype
  50. https://api.whatsapp.com/send?text=ml algorithms addendum: hebbian learning https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/
  51. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=telegram
  52. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/?share=email
  53. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/#print
  54. https://www.bonaccorso.eu/tag/hebb/
  55. https://www.bonaccorso.eu/tag/machine-learning/
  56. https://www.bonaccorso.eu/tag/neural-networks/
  57. https://www.bonaccorso.eu/tag/neuroscience/
  58. https://www.bonaccorso.eu/2017/08/19/hodgkin-huxley-spiking-neuron-model-python/
  59. https://www.bonaccorso.eu/2017/08/29/ml-algorithms-addendum-instance-based-learning/
  60. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/#respond
  61. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=1167&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=289921c3196114c3fe011ed7b71ad5a510dc6eb6#parent=https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/
  62. https://www.linkedin.com/in/giuseppebonaccorso/
  63. https://twitter.com/giuseppeb/
  64. https://www.facebook.com/giuseppe.bonaccorso/
  65. https://github.com/giuseppebonaccorso/
  66. https://www.instagram.com/giuseppebonaccorso/
  67. https://www.amazon.com/author/giuseppebonaccorso
  68. https://medium.com/@giuseppe.bonaccorso
  69. https://www.bonaccorso.eu/feed/
  70. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
  71. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
  72. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
  73. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
  74. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
  75. https://twitter.com/giuseppeb
  76. https://www.bonaccorso.eu/
  77. https://www.iubenda.com/privacy-policy/331721
  78. https://www.iubenda.com/privacy-policy/331721/cookie-policy
  79. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/#cancel

   hidden links:
  81. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
