                          wikipedia multilingual open relations corpus version 1.0

1. introduction
this dataset contains open relation phrases extracted from the multilingual wiki
pedia corpus https://www.wikipedia.org/.


      2. a brief overview of the corpus


id36 (re) is the task of assigning a semantic relationship betwee
n a pair of arguments. re systems come in two flavors: closed-domain, which use
a closed set of relation phrases to specify relationships, and open domain, wher
e an arbitrary phrase can describe the relation between the arguments. the outpu
ts of re systems are used in a variety of downstream applications, including  qu
estion answering, information extraction etc. although re systems work accuratel
y for english and a few other languages where tools for syntactic analysis (pars
ers, pos taggers, named entity analyzers) are available, there is very little wo
rk on developing re systems for most of the world's languages where such analysi
s tools are not available. however, we do have translation systems between engli
sh and many other languages. faruqui and kumar (2015) describe a cross-lingual p
rojection algorithm for multilingual re by translating text from a foreign langu
age to english, perform id36 in english and project these relatio
ns back to the foreign language. the paper describes the application of the algo
rithm to extract multilingual relations for 10 languages from wikipedia. the pap
er also reports the performance of the algorithms against human annotations on t
hree languages: french, russian and hindi. this dataset provides the set of auto
matically extracted relations obtained using this algorithm as well as the set o
f human annotations that were used in evaluating the algorithm.



2. data source
the id36 was performed on sentences from the multilingual wikiped
ia corpus in the following languages: french, russian, chinese, arabic, hindi, i
ndonesian, tagalog, latvian, swahili and georgian.


3. corpus preparation


for each source (non-english) language in wikipedia, the following steps were ru
n:
1. wikipedia documents in the language were processed to extract sentences. the
sentences were tokenized into words. sentences with number of tokens in the rang
e of 2-39 words were kept.
2. sentences were translated from source language to english using google transl
ate.
3. the ollie system (mausam et al. '12) was employed to extract relation tuples
(argument1, argument2, relation) from each english sentence.
4. the relation tuples were projected back to the source sentence using the cros
s-lingual projection algorithm described in faruqui and kumar (2015).
5. for each pair of arguments (a1,a2), a id155 was computed fo
r each relation phrase: p(r|a1,a2).  this id155 is a measure o
f the confidence score for the relation phrase conditioned on the pair of argume
nts.
6. for each relation phrase r, a global id203 of occurrence was computed f
rom the entire corpus. the id203 reflects the prevalence of the relation p
hrase in the corpus.


4. directory structure and file format

the tarball containing the corpus can be downloaded from http://storage.googleap
is.com/wikipedia_multilingual_relations_v1/multilingual_relations_data.tar
the directory consists of two subdirectories, "auto" and "human"
4.1. the "auto" subdirectory contains the following data for each language:


a) the relations extracted for each sentence from the wikipedia corpus in the la
nguage. the relations were obtained using cross-lingual projection. we provide t
he original sentence, extracted relation tuple (argument1, argument2, relation)
as well as the translation of the relation tuple into english.
b) the automatically extraction relation tuples with confidence scores.


4.1.1. the extractions are placed in the sub-directory 'extractions'. the relati
on tuples for language 'x' are placed in the file x.bz2.
each line in this file has the following format:
wikipedia url ||| source language (sl) sentence ||| argument 1 in sl ||| relatio
n in sl ||| argument 2 in sl ||| english translation ||| argument 1 in english |
|| relation in english ||| argument 2 in english


here is an example line from the file in french:
http://fr.wikipedia.org/w/index.php?curid=632375 ||| 0 , amesim utilise le compi
lateur intel sur toutes les plateformes . ||| amesim ||| utilise le ||| compilat
eur intel ||| 0 amesim uses intel compiler across platforms . ||| amesim ||| use
s ||| intel compiler


if a sentence is present in multiple wikipedia urls, we retain one of these urls
 in the above file.


4.1.2. the relation tuples with confidence scores are placed in the sub-director
y 'confidences'.
the tuples for language 'x' are placed in the file x.bz2


each file contains the set of automatically extracted relations for a given lang
uage. each line in this file provides the list of automatically extracted relati
ons along with their confidence scores for a given pair of arguments.  these con
fidence scores add up to 1 for a given pair of arguments.


each line has 3 fields separated by the delimiter '|||'. the line has the format
:
argument 1 ||| argument 2 ||| relations_and_confidences,
where "relations_and_confidences" is a list of relations with their confidence s
cores, delimiter is the character sequence '&&&'.


here is an example in french.
le nid ||| un arbre ||| construit&&&2.5e-01&&&construite&&&2.5e-01&&&est constru
it dans&&&2.5e-01&&&est une construite dans&&&2.5e-01
for the argument 1 'le nid' and the argument 2 'un arbre', we extract 4 relation
 phrases, each with a confidence score of 0.25.


each file also contains a special line that starts with @@@total@@@. this line c
ontains the global id203 of the relation phrases in the wikipedia corpus f
or that language. here is an example showing a few entries from french.
@@@total@@@ ||| - ||| est&&&3.9e-03&&&fait partie&&&2.7e-03,....


this implies that the relation phrases 'est' and 'fait partie' have probabilitie
s of 0.0039 and 0.0027 respectively in the corpus.



4.2. the "human" subdirectory contains the human annotations of relation tuples
for 675, 958, 962 sentences from french, russian and hindi respectively.


each line in this file contains 5 fields separated by the delimiter '|||'


the format is:
wikipedia url ||| sentence ||| arg1 ||| arg2 ||| relation
i.e. the 1st field is the url, 2nd field is the sentence in wikipedia, 2nd and 3
rd fields correspond to the two arguments and 4th field is the relation phrase e
xtracted by human annotators.

here is an example line from the french file:
http://fr.wikipedia.org/w/index.php?curid=4659630 ||| la fondation d ' entrepris
e est reconnue comme une personne morale . ||| la fondation d ' entreprise ||| u
ne personne morale ||| est reconnue comme

in the above line, the two arguments are "la fondation d ' entreprise" and "une
personne morale" and the relation phrase is "est reconnue comme".

we note that the wikipedia urls were extracted on april 03, 2015.

5. limitations
   wikipedia documents in a non-english language x typically contain sentences i
n x as well as english. in the current version of the dataset, we did not make a
ny attempts to discard these english sentences. thus, our extracted relations fo
r a non-english language also contain english relations.

6. acknowledging the data

we are very pleased to be able to release this dataset, and we hope that many gr
oups find it useful in their work. if you use this data, we would like to ask yo
u to acknowledge it in your presentations and publications. we are also interest
ed in hearing what uses this data finds, so we would appreciate hearing from you
 how you used the data.

7. contact information

we would welcome comments, suggestions, questions about the contents of this dat
aset, suggestions for possible future data sets, and any other feedback. please
send email to: wikimultilingualrelations@google.com

8. references

manaal faruqui and shankar kumar 2015, multilingual open id36 usi
ng cross-lingual projection, in proceedings of the north american chapter of the
 association for computational linguistics. http://static.googleusercontent.com/
media/research.google.com/en//pubs/archive/43449.pdf

mausam, michael schmitz, robert bart, stephen soderland, and oren etzioni. 2012.
 open language learning for information extraction. in proceedings of the joint
conference of the empirical methods in natural language processing and the confe
rence on natural language learning. https://homes.cs.washington.edu/~mausam/pape
rs/emnlp12a.pdf

9. usage
this data release is licensed under the terms and conditions of the creative com
mons attribution-sharealike 3.0 unported license.


shankar kumar
google research

manaal faruqui
carnegie mellon university


april 2015

