   #[1]github [2]recent commits to id97:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]2
     * [35]star [36]13
     * [37]fork [38]8

[39]mrt033/[40]id97

   [41]code [42]issues 35 [43]pull requests 0 [44]projects 1 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   automatically exported from code.google.com/p/id97
     * [47]52 commits
     * [48]2 branches
     * [49]0 releases
     * [50]fetching contributors
     * [51]apache-2.0

    1. [52]c 84.4%
    2. [53]shell 14.4%
    3. [54]makefile 1.2%

   (button) c shell makefile
   branch: master (button) new pull request
   [55]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/m
   [56]download zip

downloading...

   want to be notified of new releases in mrt033/id97?
   [57]sign in [58]sign up

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [61]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [62]download the github extension for visual studio
   and try again.

   (button) go back
   [63]@mrt033
   [64]mrt033 [65]update readme.md
   latest commit [66]3704467 dec 11, 2015
   [67]permalink
   type              name                 latest commit message   commit time
        failed to load latest commit information.
        [68]license
        [69]readme.md
        [70]readme.txt
        [71]compute-accuracy.c
        [72]demo-analogy.sh
        [73]demo-classes.sh
        [74]demo-phrase-accuracy.sh
        [75]demo-phrases.sh
        [76]demo-train-big-model-v1.sh
        [77]demo-word-accuracy.sh
        [78]demo-word.sh               [79]update to 0.1c version sep 6, 2014
        [80]distance.c
        [81]makefile
        [82]questions-phrases.txt
        [83]questions-words.txt
        [84]word-analogy.c
        [85]word2phrase.c
        [86]id97.c

readme.md

   #notice

   the id97 algorithms has been ported to the teensorflow
   architecture. see:
   [87]https://www.tensorflow.org/versions/master/tutorials/id97/index
   .html

   below is the deprecated id97 implementation.

   ##introduction

   this tool provides an efficient implementation of the continuous
   bag-of-words and skip-gram architectures for computing vector
   representations of words. these representations can be subsequently
   used in many natural language processing applications and for further
   research.

   ##quick start
     * run 'make' to compile id97 tool
     * run the demo scripts: ./demo-word.sh and ./demo-phrases.sh
     * for questions about the toolkit, see
       [88]http://groups.google.com/group/id97-toolkit

   ##how does it work the id97 tool takes a text corpus as input and
   produces the word vectors as output. it first constructs a vocabulary
   from the training text data and then learns vector representation of
   words. the resulting word vector file can be used as features in many
   natural language processing and machine learning applications.

   a simple way to investigate the learned representations is to find the
   closest words for a user-specified word. the distance tool serves that
   purpose. for example, if you enter 'france', distance will display the
   most similar words and their distances to 'france', which should look
   like:
                 word       cosine distance
-------------------------------------------
                spain              0.678515
              belgium              0.665923
          netherlands              0.652428
                italy              0.633130
          switzerland              0.622323
           luxembourg              0.610033
             portugal              0.577154
               russia              0.571507
              germany              0.563291
            catalonia              0.534176

   there are two main learning algorithms in id97 : continuous
   bag-of-words and continuous skip-gram. the switch -cbow allows the user
   to pick one of these learning algorithms. both algorithms learn the
   representation of a word that is useful for prediction of other words
   in the sentence. these algorithms are described in detail in [1,2].

   ##interesting properties of the word vectors it was recently shown that
   the word vectors capture many linguistic regularities, for example
   vector operations vector('paris') - vector('france') + vector('italy')
   results in a vector that is very close to vector('rome'), and
   vector('king') - vector('man') + vector('woman') is close to
   vector('queen') [3, 1]. you can try out a simple demo by running
   demo-analogy.sh.

   ##how to measure quality of the word vectors several factors influence
   the quality of the word vectors:
     * amount and quality of the training data
     * size of the vectors
     * training algorithm

   the quality of the vectors is crucial for any application. however,
   exploration of different hyper-parameter settings for complex tasks
   might be too time demanding. thus, we designed simple test sets that
   can be used to quickly evaluate the word vector quality.

   for the word relation test set described in [1], see
   ./demo-word-accuracy.sh, for the phrase relation test set described in
   [2], see ./demo-phrase-accuracy.sh. note that the accuracy depends
   heavily on the amount of the training data; our best results for both
   test sets are above 70% accuracy with coverage close to 100%.

   ##word id91 the word vectors can be also used for deriving word
   classes from huge data sets. this is achieved by performing id116
   id91 on top of the word vectors. the script that demonstrates
   this is ./demo-classes.sh. the output is a vocabulary file with words
   and their corresponding class ids, such as:
carnivores 234
carnivorous 234
cetaceans 234
cormorant 234
coyotes 234
crocodile 234
crocodiles 234
crustaceans 234
cultivated 234
danios 234
.
.
.
acceptance 412
argue 412
argues 412
arguing 412
argument 412
arguments 412
belief 412
believe 412
challenge 412
claim 412

   ##performance the training speed can be significantly improved by using
   parallel training on multiple-cpu machine (use the switch '-threads
   n'). the hyper-parameter choice is crucial for performance (both speed
   and accuracy), however varies for different applications. the main
   choices to make are:
     * architecture: skip-gram (slower, better for infrequent words) vs
       cbow (fast)
     * the training algorithm: hierarchical softmax (better for infrequent
       words) vs negative sampling (better for frequent words, better with
       low dimensional vectors)
     * sub-sampling of frequent words: can improve both accuracy and speed
       for large data sets (useful values are in range 1e-3 to 1e-5)
     * dimensionality of the word vectors: usually more is better, but not
       always
     * context (window) size: for skip-gram usually around 10, for cbow
       around 5

   ##where to obtain the training data the quality of the word vectors
   increases significantly with amount of the training data. for research
   purposes, you can consider using data sets that are available on-line:
     * [89]first billion characters from wikipedia (use the pre-processing
       perl script from the bottom of [90]matt mahoney's page)
     * [91]latest wikipedia dump use the same script as above to obtain
       clean text. should be more than 3 billion words.
     * [92]wmt11 site: text data for several languages (duplicate
       sentences should be removed before training the models)
     * [93]dataset from "one billion word id38 benchmark"
       almost 1b words, already pre-processed text.
     * [94]umbc webbase corpus around 3 billion words, more info [95]here.
       needs further processing (mainly id121).
     * text data from more languages can be obtained at [96]statmt.org and
       in the [97]polyglot project.

   ##pre-trained word and phrase vectors we are publishing pre-trained
   vectors trained on part of google news dataset (about 100 billion
   words). the model contains 300-dimensional vectors for 3 million words
   and phrases. the phrases were obtained using a simple data-driven
   approach described in [2]. the archive is available here:
   [98]googlenews-vectors-negative300.bin.gz.

   an example output of ./distance googlenews-vectors-negative300.bin:
enter word or sentence (exit to break): chinese river

                word       cosine distance
------------------------------------------
       yangtze_river              0.667376
             yangtze              0.644091
      qiantang_river              0.632979
   yangtze_tributary              0.623527
    xiangjiang_river              0.615482
       huangpu_river              0.604726
      hanjiang_river              0.598110
       yangtze_river              0.597621
         hongze_lake              0.594108
             yangtse              0.593442

   the above example will average vectors for words 'chinese' and 'river'
   and will return the closest neighbors to the resulting vector. more
   examples that demonstrate results of vector addition are presented in
   [2]. note that more precise and disambiguated entity vectors can be
   found in the following dataset that uses freebase naming.

   ##pre-trained entity vectors with freebase naming we are also offering
   more than 1.4m pre-trained entity vectors with naming from
   [99]freebase. this is especially helpful for projects related to
   knowledge mining.
     * entity vectors trained on 100b words from various news articles:
       [100]freebase-vectors-skipgram1000.bin.gz
     * entity vectors trained on 100b words from various news articles,
       using the deprecated /en/ naming (more easily readable); the
       vectors are sorted by frequency:
       [101]freebase-vectors-skipgram1000-en.bin.gz

   here is an example output of ./distance
   freebase-vectors-skipgram1000-en.bin:
enter word or sentence (exit to break): /en/geoffrey_hinton

                        word       cosine distance
--------------------------------------------------
           /en/marvin_minsky              0.457204
             /en/paul_corkum              0.443342
 /en/william_richard_peltier              0.432396
           /en/brenda_milner              0.430886
    /en/john_charles_polanyi              0.419538
          /en/leslie_valiant              0.416399
         /en/hava_siegelmann              0.411895
            /en/hans_moravec              0.406726
         /en/david_rumelhart              0.405275
             /en/godel_prize              0.405176

   ##final words thank you for trying out this toolkit, and do not forget
   to let us know when you obtain some amazing results! we hope that the
   distributed representations will significantly improve the state of the
   art in nlp.

   ##references

   [1] tomas mikolov, kai chen, greg corrado, and jeffrey dean.
   [102]efficient estimation of word representations in vector space. in
   proceedings of workshop at iclr, 2013.

   [2] tomas mikolov, ilya sutskever, kai chen, greg corrado, and jeffrey
   dean. [103]distributed representations of words and phrases and their
   compositionality. in proceedings of nips, 2013.

   [3] tomas mikolov, wen-tau yih, and geoffrey zweig. [104]linguistic
   regularities in continuous space word representations. in proceedings
   of naacl hlt, 2013.

   ##other useful links

   feel free to send us a link to your project or research paper related
   to id97 that you think will be useful or interesting for the
   others.

   tomas mikolov, quoc v. le and ilya sutskever. [105]exploiting
   similarities among languages for machine translation. we show how the
   word vectors can be applied to machine translation. code for improved
   version from georgiana dinu [106]here.

   id97 in python by radim rehurek in [107]gensim (plus [108]tutorial
   and [109]demo that uses the above model trained on google news).

   id97 in java as part of the [110]deeplearning4j project. another
   java version from medallia [111]here.

   id97 implementation in [112]spark mllib.

   comparison with traditional count-based vectors and cbow model trained
   on a different corpus by [113]cimec unitn.

   link to slides about word vectors from nips 2013 deep learning
   workshop: [114]nnfortext.pdf

   ##disclaimer

   this open source project is not a google product, and is released for
   research purposes only.

     *    2019 github, inc.
     * [115]terms
     * [116]privacy
     * [117]security
     * [118]status
     * [119]help

     * [120]contact github
     * [121]pricing
     * [122]api
     * [123]training
     * [124]blog
     * [125]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [126]reload to refresh your
   session. you signed out in another tab or window. [127]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/mrt033/id97/commits/master.atom
   3. https://github.com/mrt033/id97#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/mrt033/id97
  32. https://github.com/join
  33. https://github.com/login?return_to=/mrt033/id97
  34. https://github.com/mrt033/id97/watchers
  35. https://github.com/login?return_to=/mrt033/id97
  36. https://github.com/mrt033/id97/stargazers
  37. https://github.com/login?return_to=/mrt033/id97
  38. https://github.com/mrt033/id97/network/members
  39. https://github.com/mrt033
  40. https://github.com/mrt033/id97
  41. https://github.com/mrt033/id97
  42. https://github.com/mrt033/id97/issues
  43. https://github.com/mrt033/id97/pulls
  44. https://github.com/mrt033/id97/projects
  45. https://github.com/mrt033/id97/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/mrt033/id97/commits/master
  48. https://github.com/mrt033/id97/branches
  49. https://github.com/mrt033/id97/releases
  50. https://github.com/mrt033/id97/graphs/contributors
  51. https://github.com/mrt033/id97/blob/master/license
  52. https://github.com/mrt033/id97/search?l=c
  53. https://github.com/mrt033/id97/search?l=shell
  54. https://github.com/mrt033/id97/search?l=makefile
  55. https://github.com/mrt033/id97/find/master
  56. https://github.com/mrt033/id97/archive/master.zip
  57. https://github.com/login?return_to=https://github.com/mrt033/id97
  58. https://github.com/join?return_to=/mrt033/id97
  59. https://desktop.github.com/
  60. https://desktop.github.com/
  61. https://developer.apple.com/xcode/
  62. https://visualstudio.github.com/
  63. https://github.com/mrt033
  64. https://github.com/mrt033/id97/commits?author=mrt033
  65. https://github.com/mrt033/id97/commit/3704467f6bf50882d808562dd312d90aead297ec
  66. https://github.com/mrt033/id97/commit/3704467f6bf50882d808562dd312d90aead297ec
  67. https://github.com/mrt033/id97/tree/3704467f6bf50882d808562dd312d90aead297ec
  68. https://github.com/mrt033/id97/blob/master/license
  69. https://github.com/mrt033/id97/blob/master/readme.md
  70. https://github.com/mrt033/id97/blob/master/readme.txt
  71. https://github.com/mrt033/id97/blob/master/compute-accuracy.c
  72. https://github.com/mrt033/id97/blob/master/demo-analogy.sh
  73. https://github.com/mrt033/id97/blob/master/demo-classes.sh
  74. https://github.com/mrt033/id97/blob/master/demo-phrase-accuracy.sh
  75. https://github.com/mrt033/id97/blob/master/demo-phrases.sh
  76. https://github.com/mrt033/id97/blob/master/demo-train-big-model-v1.sh
  77. https://github.com/mrt033/id97/blob/master/demo-word-accuracy.sh
  78. https://github.com/mrt033/id97/blob/master/demo-word.sh
  79. https://github.com/mrt033/id97/commit/891d84c630958287a0493ec80c83ff89458cf1ed
  80. https://github.com/mrt033/id97/blob/master/distance.c
  81. https://github.com/mrt033/id97/blob/master/makefile
  82. https://github.com/mrt033/id97/blob/master/questions-phrases.txt
  83. https://github.com/mrt033/id97/blob/master/questions-words.txt
  84. https://github.com/mrt033/id97/blob/master/word-analogy.c
  85. https://github.com/mrt033/id97/blob/master/word2phrase.c
  86. https://github.com/mrt033/id97/blob/master/id97.c
  87. https://www.tensorflow.org/versions/master/tutorials/id97/index.html
  88. http://groups.google.com/group/id97-toolkit
  89. http://mattmahoney.net/dc/enwik9.zip
  90. http://mattmahoney.net/dc/textdata.html
  91. http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
  92. http://www.statmt.org/wmt11/translation-task.html#download
  93. http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz
  94. http://ebiquity.umbc.edu/redirect/to/resource/id/351/umbc-webbase-corpus
  95. http://ebiquity.umbc.edu/blogger/2013/05/01/umbc-webbase-corpus-of-3b-english-words/
  96. http://statmt.org/
  97. https://sites.google.com/site/rmyeid/projects/polyglot#toc-download-wikipedia-text-dumps
  98. https://drive.google.com/file/d/0b7xkcwpi5kdynlnuttlss21pqmm/edit?usp=sharing
  99. http://www.freebase.com/
 100. https://docs.google.com/file/d/0b7xkcwpi5kdyadbdqm1tzgndrhc/edit?usp=sharing
 101. https://docs.google.com/file/d/0b7xkcwpi5kdyefdmcvltwkhtbmm/edit?usp=sharing
 102. http://arxiv.org/pdf/1301.3781.pdf
 103. http://arxiv.org/pdf/1310.4546.pdf
 104. http://research.microsoft.com/pubs/189726/rvecs.pdf
 105. http://arxiv.org/pdf/1309.4168
 106. http://clic.cimec.unitn.it/~georgiana.dinu/down/
 107. http://radimrehurek.com/2013/09/deep-learning-with-id97-and-gensim/
 108. http://radimrehurek.com/2014/02/id97-tutorial/
 109. http://radimrehurek.com/2014/02/id97-tutorial/#app
 110. http://deeplearning4j.org/id97.html
 111. https://github.com/medallia/id97java
 112. https://spark.apache.org/docs/latest/mllib-feature-extraction.html#id97
 113. http://clic.cimec.unitn.it/composes/semantic-vectors.html
 114. https://drive.google.com/file/d/0b7xkcwpi5kdyrwrnd1rzwxq2twc/edit?usp=sharing
 115. https://github.com/site/terms
 116. https://github.com/site/privacy
 117. https://github.com/security
 118. https://githubstatus.com/
 119. https://help.github.com/
 120. https://github.com/contact
 121. https://github.com/pricing
 122. https://developer.github.com/
 123. https://training.github.com/
 124. https://github.blog/
 125. https://github.com/about
 126. https://github.com/mrt033/id97
 127. https://github.com/mrt033/id97

   hidden links:
 129. https://github.com/
 130. https://github.com/mrt033/id97
 131. https://github.com/mrt033/id97
 132. https://github.com/mrt033/id97
 133. https://help.github.com/articles/which-remote-url-should-i-use
 134. https://github.com/
