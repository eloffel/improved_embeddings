   [1]lemur project logo
     * [2]home
     * [3]components
          + [4]indri
          + [5]lemur
          + [6]galago
          + [7]ranklib
          + [8]sifaka
          + [9]wordentityduet
          + [10]clueweb09
          + [11]clueweb12
          + [12]webap
          + [13]nfl6
     * [14]support
     * [15]about
     __________________________________________________________________

     * [16]clueweb09
     * [17]how to get it
     * [18]dataset details
     * [19]related data
     * [20]online services
     * [21]indexing with indri
     * [22]wiki & email
     * [23]faq

                           clueweb09 related data:
           freebase annotations of the clueweb corpora, v1 (facc1)

   researchers at google annotated english-language web pages from the
   [24]clueweb09 and [25]clueweb12 corpora. the annotation process was
   automatic, and hence imperfect. however, the annotations are of
   generally high quality, as they strove for high precision (and, by
   necessity, lower recall). for each entity they recognized with high
   confidence, they provide the beginning and end byte offsets of the
   entity mention in the input text, its freebase identifier (mid), and
   two confidence levels (computed differently, see below).

   you might consider using this data in conjunction with the recently
   released [26]freebase annotations of several trec query sets.


data description

   the annotations for each corpus are provided as a collection of 500
   files (the partition into individual files is somewhat arbitrary). each
   file contains annotations of multiple web pages, and each page url is
   followed by a list of entities identified in that page.

   here is an excerpt from an annotation file:

   clueweb09-en0000-00-04720.html
   pdf                          21089 21092 0.99763662 6.6723776e-05 /m/0600q
   fda                          21303 21306 0.9998256  0.00057182228 /m/032mx
   food and drug administration 21312 21340 0.9998256  0.00057182228
   /m/032mx

   in this example,
     * "clueweb09-en0000-00-04720.html" is the name of the document that
       was annotated
     * "pdf" is the entity mention in text
     * 21089 and 21092 are the beginning and end byte offsets of the
       entity mention in the input text. the zero (0) location used for
       calculating the annotation offsets is the beginning of the http
       headers. this is the first byte after the warc document header.
     * 0.99763662 is the posterior of an entity given both the mention and
       the context (of the mention)
     * 6.6723776e-05 is the posterior given just the context of the
       mention (ignoring the mention string itself)
     * /m/0600q - freebase identifier for the entity. to look up the
       entity in freebase, just prepend the string
       "http://www.freebase.com" before the identifier, like so:
       "http://www.freebase.com/m/0600q".

   some documents do not have any annotations (and are not included in the
   annotation files) because no freebase entity was recognized in them
   with high confidence.

   there are 340,451,982 documents in clueweb09 and 456,498,584 documents
   in clueweb12 with at least one entity annotated. on average, clueweb09
   documents have 15 entity mentions annotated, and clueweb12 documents
   have 13 mentions annotated. additional statistics are available in the
   companion files "[27]clueweb09_stats.txt" and
   "[28]clueweb12_stats.txt".

   due to the sheer size of the data, it was not possible to verify all
   the automatic annotations manually. based on a small-scale human
   evaluation, the precision (at the currently chosen threshold) is
   believed to be around 80-85%. estimating the recall is of course
   difficult; however, it is believed to be around 70-85%.


processed data

   the following files are the annotations provided by google, inc and
   processed by the lemur project. the processed files are text files with
   the extension .anns.tsv. the annotations are converted to the a format
   that we believe is easier to process. each .anns.tsv file contains the
   annotations for the documents corresponding to the warc.gz file. for
   instance, disk1/clueweb09_english_1/en0000/00.anns.tsv contains the
   annotations for the documents contained in the corpus file
   disk1/clueweb09_english_1/en0000/00.warc.gz. the file format adds two
   additional columns to the beginning of each line:

     * document identifier (warc-trec-id)
     * original encoding: the name of the encoding used to process the
       entry. this encoding is used to calulate the beginning and end byte
       offsets of the entity in the corpus document.

   the following is an example of the processed data format:

   clueweb09-en0000-00-00005 utf-8 g e 9188 9196 1.000000 0.000067
   /m/03bnb
   clueweb09-en0000-00-00005 utf-8 com 10850 10853 0.960598 0.000052
   /m/01gj0k
   clueweb09-en0000-00-00005 utf-8 us 10856 10858 0.662877 0.005153
   /m/09c7w0
   clueweb09-en0000-00-00005 utf-8 domain names 10871 10883 0.999885
   0.000565 /m/09y1k
   clueweb09-en0000-00-00011 iso-8859-1 american 8115 8123 0.985868
   0.005697 /m/09c7w0
   clueweb09-en0000-00-00011 iso-8859-1 us 14201 14203 0.985868 0.005697
   /m/09c7w0
   clueweb09-en0000-00-00011 iso-8859-1 us 16075 16077 0.985868 0.005697
   /m/09c7w0

   note: you can extract the content of the following files ( compressed
   tar file) using the command: tar -zxvf clueweb09_english_*
     * [29]clueweb09_english_1.tgz (8.2g, compressed): the annotations for
       clueweb09 disk1/clueweb09_english_1 documents.
     * [30]clueweb09_english_2.tgz (8.9g, compressed): the annotations for
       clueweb09 disk1/clueweb09_english_2 documents.
     * [31]clueweb09_english_3.tgz (8.4g, compressed): the annotations for
       clueweb09 disk1/clueweb09_english_3 documents.
     * [32]clueweb09_english_4.tgz (7.9g, compressed): the annotations for
       clueweb09 disk1/clueweb09_english_4 documents.
     * [33]clueweb09_english_5.tgz (7.3g, compressed): the annotations for
       clueweb09 disk1/clueweb09_english_5 documents.
     * [34]clueweb09_english_6.tgz (7.3g, compressed): the annotations for
       clueweb09 disk2/clueweb09_english_6 documents.
     * [35]clueweb09_english_7.tgz (7.4g, compressed): the annotations for
       clueweb09 disk2/clueweb09_english_7 documents.
     * [36]clueweb09_english_8.tgz (5.4g, compressed): the annotations for
       clueweb09 disk2/clueweb09_english_8 documents.
     * [37]clueweb09_english_9.tgz (6.6g, compressed): the annotations for
       clueweb09 disk2/clueweb09_english_9 documents.
     * [38]clueweb09_english_10.tgz (4.9g, compressed): the annotations
       for clueweb09 disk2/clueweb09_english_10 documents.

     * [39]checksums.md5 (4k): file "checksums.md5" contains the md5 sums
       of all 10 of the *.tgz files. these md5 sums are in the format:
       <md5 checksum hash> <file>


using the annotations data with indri query environment

   the follow code snippet is an example on how to use the indri query
   environment to grab a clueweb document using (a) an indri index with
   storedocs=true (parseddocument data structure is stored with the
   index), and (b) a document id. the code also shows how to find the
   beginning of the http headers, which is used as zero (0) when
   calculating the annotation offsets.

   the annotation entries include the mime-type used to calculate the
   offset to the annotation. the [40]generic charset conversion interface
   is one way to convert the text so you can locate the annotation in the
   document using the entry's offsets.

   #include <stdio.h>
   #include <string>
   #include <iconv.h>
   #include "indri/queryenvironment.hpp"
   #include "indri/queryexpander.hpp"
   .
   .
   .
   /* hard code document id for this example. */
   string docid="clueweb09-en0000-00-04720";
   indri::api::queryenvironment *env = new indri::api::queryenvironment();
   /* set whether there should be one single background model or context
   sensitive models; (background true for one background model; false for
   context sensitive models) */
   env->setsinglebackgroundmodel(false);
   /* add a local repository using the full pathname to the repository */
   env->addindex("/path/to/index/");
   std::vector idlist;
   idlist.push_back(docid.c_str());
   /* fetch all documents with a metadata key docno that matches docid. */
   std::vector docids = env->documentidsfrommetadata("docno", idlist);
   std::vector documents = env->documents(docids);
   /* check to see if we found a document */
   if( documents.size() ) {
     /* just take the first document (since docids are unique, there
   should be only one document) */
     indri::api::parseddocument* document = documents[0];
     /* document->getcontent() will not include the http headers,
   therefore we have to grab the complete document which includes the warc
   header and content block, http headers and document.*/
     string t = document->text;
     /* convert document text to correct mime-type - possibly using
   iconv_open function. */
     .
     .
     .
     /* find the beginning of the http headers. a warc record consists of
   a record header followed by a record content block and two newlines
   (newlines are crlf as per other internet standards.). the http headers
   and document follow.*/
     size_t start = correctmimetypestring.find("\n\n") + 2 ;
     /* we now have start location (zero) of the annotation offsets. you
   can proceed with finding annotations using the provided offsets. */
     .
     .
     .
   } else {
     cout << "document not found" << endl;
   }
   env->close();


citation

   if you use this data in a publication, please cite it as:
     * evgeniy gabrilovich, michael ringgaard, and amarnag subramanya,
       "facc1: freebase annotation of clueweb corpora, version 1 (release
       date 2013-06-26, format version 1, correction level 0)", june 2013.

   please also include in the citation the following url(s) where the data
   is available ([41]http://lemurproject.org/clueweb09/ and/or
   [42]http://lemurproject.org/clueweb12/).


other data from google

   if you would like to learn more about data releases from google, you
   may wish to consider subscribing to this low-traffic mailing list:
   [43]http://goo.gl/mjb3a.


acknowledgments

   this data set was prepared by evgeniy gabrilovich, michael ringgaard,
   and amarnag subramanya (google), and juan caicedo carvajal (cmu).

   thanks to johnny chen, john giannandrea, rahul gupta, jesse saba
   kirchner, ruichao li, eisar lipkovitz, jeremy o'brien, dave orr,
   fernando pereira, dave price, and chuck wu for making this release
   possible.

   [44]the lemur project [45]the lemur project
                         last modified:november 04, 2013. 09:46:09 am
   [46]umass-ciir [47]cmu lti logo [48]the lemur project at sourceforge

references

   1. http://lemurproject.org/index.php
   2. http://lemurproject.org/
   3. http://lemurproject.org/components.php
   4. http://lemurproject.org/indri.php
   5. http://lemurproject.org/lemur.php
   6. http://lemurproject.org/galago.php
   7. http://lemurproject.org/ranklib.php
   8. http://lemurproject.org/sifaka.php
   9. http://lemurproject.org/wordentityduet.php
  10. http://lemurproject.org/clueweb09.php
  11. http://lemurproject.org/clueweb12.php
  12. https://ciir.cs.umass.edu/downloads/webap/index.html
  13. https://ciir.cs.umass.edu/downloads/nfl6/index.html
  14. http://lemurproject.org/support.php
  15. http://lemurproject.org/about.php
  16. http://lemurproject.org/clueweb09/index.php
  17. http://lemurproject.org/clueweb09/index.php#using
  18. http://lemurproject.org/clueweb09/index.php#specs
  19. http://lemurproject.org/clueweb09/related-data.php
  20. http://lemurproject.org/clueweb09/index.php#services
  21. http://lemurproject.org/clueweb09/indri-howto.php
  22. http://lemurproject.org/clueweb09/index.php#beinformed
  23. http://lemurproject.org/clueweb09/faq.php
  24. http://lemurproject.org/clueweb09/
  25. http://lemurproject.org/clueweb12/
  26. http://lemurproject.org/clueweb09/related-data.php
  27. http://lemurproject.org/clueweb09/facc1/clueweb09_stats.txt
  28. http://lemurproject.org/clueweb12/facc1/clueweb12_stats.txt
  29. http://lemurproject.org/clueweb09/facc1/annotations/disk1/clueweb09_english_1.tgz
  30. http://lemurproject.org/clueweb09/facc1/annotations/disk1/clueweb09_english_2.tgz
  31. http://lemurproject.org/clueweb09/facc1/annotations/disk1/clueweb09_english_3.tgz
  32. http://lemurproject.org/clueweb09/facc1/annotations/disk1/clueweb09_english_4.tgz
  33. http://lemurproject.org/clueweb09/facc1/annotations/disk1/clueweb09_english_5.tgz
  34. http://lemurproject.org/clueweb09/facc1/annotations/disk2/clueweb09_english_6.tgz
  35. http://lemurproject.org/clueweb09/facc1/annotations/disk2/clueweb09_english_7.tgz
  36. http://lemurproject.org/clueweb09/facc1/annotations/disk2/clueweb09_english_8.tgz
  37. http://lemurproject.org/clueweb09/facc1/annotations/disk2/clueweb09_english_9.tgz
  38. http://lemurproject.org/clueweb09/facc1/annotations/disk2/clueweb09_english_10.tgz
  39. http://lemurproject.org/clueweb09/facc1/annotations/checksums.md5
  40. http://www.gnu.org/software/libc/manual/html_node/generic-charset-conversion.html#generic-charset-conversion
  41. http://lemurproject.org/clueweb09/
  42. http://lemurproject.org/clueweb12/
  43. http://goo.gl/mjb3a
  44. http://www.lemurproject.org/
  45. mailto:admin@lemurproject.org
  46. http://ciir.cs.umass.edu/
  47. http://www.lti.cs.cmu.edu/
  48. http://sourceforge.net/projects/lemur/
