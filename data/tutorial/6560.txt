   #[1]tensorflow

   (button)
   [2]tensorflow
     *

   [3]install [4]learn
     * [5]introduction
       new to tensorflow?
     * [6]tensorflow
       the core open source ml library
     * [7]for javascript
       tensorflow.js for ml using javascript
     * [8]for mobile & iot
       tensorflow lite for mobile and embedded devices
     * [9]for production
       tensorflow extended for end-to-end ml components
     * [10]swift for tensorflow (in beta)

   [11]api
     * api r1
     * [12]r1.13 (stable)
     * [13]r1.12
     * [14]r1.11
     * [15]r1.10
     * [16]r1.9
     * [17]more   

     * api r2
     * [18]r2.0 (preview)

   [19]resources
     * [20]models & datasets
       pre-trained models and datasets built by google and the community
     * [21]tools
       ecosystem of tools to help you use tensorflow
     * [22]libraries & extensions
       libraries and extensions built on tensorflow

   [23]community [24]why tensorflow
     * [25]about
     * [26]case studies

   ____________________
   (button)
   (button)
   [27]github
     * [28]tensorflow core

   [29]overview [30]tutorials [31]guide [32]tf 2.0 alpha

   (button)
     * [33]install
     * [34]learn
          + more
          + [35]overview
          + [36]tutorials
          + [37]guide
          + [38]tf 2.0 alpha
     * [39]api
          + more
     * [40]resources
          + more
     * [41]community
     * [42]why tensorflow
          + more
     * [43]github

     * [44]get started with tensorflow
     * learn and use ml
          + [45]overview
          + [46]basic classification
          + [47]text classification
          + [48]regression
          + [49]overfitting and underfitting
          + [50]save and restore models
     * research and experimentation
          + [51]overview
          + [52]eager execution
          + [53]automatic differentiation
          + [54]custom training: basics
          + [55]custom layers
          + [56]custom training: walkthrough
     * ml at production scale
          + [57]linear model with estimators
          + [58]wide and deep learning
          + [59]boosted trees
          + [60]boosted trees model understanding
          + [61]build a id98 using estimators
     * generative models
          + [62]translation with attention
          + [63]image captioning
          + [64]dcgan
          + [65]vae
     * images
          + [66]image recognition
          + [67]pix2pix
          + [68]neural style transfer
          + [69]image segmentation
          + [70]advanced id98
     * sequences
          + [71]text generation with an id56
          + [72]recurrent neural network
          + [73]drawing classification
          + [74]simple audio recognition
          + [75]id4
     * load data
          + [76]load images
          + [77]tfrecords and tf.example
     * data representation
          + [78]vector representations of words
          + [79]kernel methods
          + [80]large-scale linear models
          + [81]unicode
     * non-ml
          + [82]mandelbrot set
          + [83]partial differential equations

     * [84]introduction
     * [85]tensorflow
     * [86]for javascript
     * [87]for mobile & iot
     * [88]for production
     * [89]swift for tensorflow (in beta)

     * api r1
     * [90]r1.13 (stable)
     * [91]r1.12
     * [92]r1.11
     * [93]r1.10
     * [94]r1.9
     * [95]more   
     * api r2
     * [96]r2.0 (preview)

     * [97]models & datasets
     * [98]tools
     * [99]libraries & extensions

     * [100]about
     * [101]case studies

   watch talks from the 2019 tensorflow dev summit [102]watch now
     * [103]tensorflow
     * [104]learn
     * [105]tensorflow core
     * [106]tutorials

simple audio recognition

   this tutorial will show you how to build a basic id103
   network that recognizes ten different words. it's important to know
   that real speech and audio recognition systems are much more complex,
   but like mnist for images, it should give you a basic understanding of
   the techniques involved. once you've completed this tutorial, you'll
   have a model that tries to classify a one second audio clip as either
   silence, an unknown word, "yes", "no", "up", "down", "left", "right",
   "on", "off", "stop", or "go". you'll also be able to take this model
   and run it in an android application.

preparation

   you should make sure you have tensorflow installed, and since the
   script downloads over 1gb of training data, you'll need a good internet
   connection and enough free space on your machine. the training process
   itself can take several hours, so make sure you have a machine
   available for that long.

training

   to begin the training process, go to the tensorflow source tree and
   run:
python tensorflow/examples/speech_commands/train.py

   the script will start off by downloading the [107]speech commands
   dataset, which consists of over 105,000 wave audio files of people
   saying thirty different words. this data was collected by google and
   released under a cc by license, and you can help improve it by
   [108]contributing five minutes of your own voice. the archive is over
   2gb, so this part may take a while, but you should see progress logs,
   and once it's been downloaded once you won't need to do this step
   again. you can find more information about this dataset in this
   [109]speech commands paper.

   once the downloading has completed, you'll see logging information that
   looks like this:
i0730 16:53:44.766740   55030 train.py:176] training from step: 1
i0730 16:53:47.289078   55030 train.py:217] step #1: rate 0.001000, accuracy 7.0
%, cross id178 2.611571

   this shows that the initialization process is done and the training
   loop has begun. you'll see that it outputs information for every
   training step. here's a break down of what it means:

   step #1 shows that we're on the first step of the training loop. in
   this case there are going to be 18,000 steps in total, so you can look
   at the step number to get an idea of how close it is to finishing.

   rate 0.001000 is the learning rate that's controlling the speed of the
   network's weight updates. early on this is a comparatively high number
   (0.001), but for later training cycles it will be reduced 10x, to
   0.0001.

   accuracy 7.0% is the how many classes were correctly predicted on this
   training step. this value will often fluctuate a lot, but should
   increase on average as training progresses. the model outputs an array
   of numbers, one for each label, and each number is the predicted
   likelihood of the input being that class. the predicted label is picked
   by choosing the entry with the highest score. the scores are always
   between zero and one, with higher values representing more confidence
   in the result.

   cross id178 2.611571 is the result of the id168 that we're
   using to guide the training process. this is a score that's obtained by
   comparing the vector of scores from the current training run to the
   correct labels, and this should trend downwards during training.

   after a hundred steps, you should see a line like this:

   i0730 16:54:41.813438 55030 train.py:252] saving to
   "/tmp/speech_commands_train/conv.ckpt-100"

   this is saving out the current trained weights to a checkpoint file. if
   your training script gets interrupted, you can look for the last saved
   checkpoint and then restart the script with
   --start_checkpoint=/tmp/speech_commands_train/conv.ckpt-100 as a
   command line argument to start from that point.

confusion matrix

   after four hundred steps, this information will be logged:
i0730 16:57:38.073667   55030 train.py:243] confusion matrix:
 [[258   0   0   0   0   0   0   0   0   0   0   0]
 [  7   6  26  94   7  49   1  15  40   2   0  11]
 [ 10   1 107  80  13  22   0  13  10   1   0   4]
 [  1   3  16 163   6  48   0   5  10   1   0  17]
 [ 15   1  17 114  55  13   0   9  22   5   0   9]
 [  1   1   6  97   3  87   1  12  46   0   0  10]
 [  8   6  86  84  13  24   1   9   9   1   0   6]
 [  9   3  32 112   9  26   1  36  19   0   0   9]
 [  8   2  12  94   9  52   0   6  72   0   0   2]
 [ 16   1  39  74  29  42   0   6  37   9   0   3]
 [ 15   6  17  71  50  37   0   6  32   2   1   9]
 [ 11   1   6 151   5  42   0   8  16   0   0  20]]

   the first section is a [110]confusion matrix. to understand what it
   means, you first need to know the labels being used, which in this case
   are "silence", "unknown", "yes", "no", "up", "down", "left", "right",
   "on", "off", "stop", and "go". each column represents a set of samples
   that were predicted to be each label, so the first column represents
   all the clips that were predicted to be silence, the second all those
   that were predicted to be unknown words, the third "yes", and so on.

   each row represents clips by their correct, ground truth labels. the
   first row is all the clips that were silence, the second clips that
   were unknown words, the third "yes", etc.

   this matrix can be more useful than just a single accuracy score
   because it gives a good summary of what mistakes the network is making.
   in this example you can see that all of the entries in the first row
   are zero, apart from the initial one. because the first row is all the
   clips that are actually silence, this means that none of them were
   mistakenly labeled as words, so we have no false negatives for silence.
   this shows the network is already getting pretty good at distinguishing
   silence from words.

   if we look down the first column though, we see a lot of non-zero
   values. the column represents all the clips that were predicted to be
   silence, so positive numbers outside of the first cell are errors. this
   means that some clips of real spoken words are actually being predicted
   to be silence, so we do have quite a few false positives.

   a perfect model would produce a confusion matrix where all of the
   entries were zero apart from a diagonal line through the center.
   spotting deviations from that pattern can help you figure out how the
   model is most easily confused, and once you've identified the problems
   you can address them by adding more data or cleaning up categories.

validation

   after the confusion matrix, you should see a line like this:

   i0730 16:57:38.073777 55030 train.py:245] step 400: validation accuracy
   = 26.3% (n=3093)

   it's good practice to separate your data set into three categories. the
   largest (in this case roughly 80% of the data) is used for training the
   network, a smaller set (10% here, known as "validation") is reserved
   for evaluation of the accuracy during training, and another set (the
   last 10%, "testing") is used to evaluate the accuracy once after the
   training is complete.

   the reason for this split is that there's always a danger that networks
   will start memorizing their inputs during training. by keeping the
   validation set separate, you can ensure that the model works with data
   it's never seen before. the testing set is an additional safeguard to
   make sure that you haven't just been tweaking your model in a way that
   happens to work for both the training and validation sets, but not a
   broader range of inputs.

   the training script automatically separates the data set into these
   three categories, and the logging line above shows the accuracy of
   model when run on the validation set. ideally, this should stick fairly
   close to the training accuracy. if the training accuracy increases but
   the validation doesn't, that's a sign that overfitting is occurring,
   and your model is only learning things about the training clips, not
   broader patterns that generalize.

tensorboard

   a good way to visualize how the training is progressing is using
   tensorboard. by default, the script saves out events to
   /tmp/retrain_logs, and you can load these by running:

   tensorboard --logdir /tmp/retrain_logs

   then navigate to [111]http://localhost:6006 in your browser, and you'll
   see charts and graphs showing your models progress.
   [speech_commands_tensorflow.png]

training finished

   after a few hours of training (depending on your machine's speed), the
   script should have completed all 18,000 steps. it will print out a
   final confusion matrix, along with an accuracy score, all run on the
   testing set. with the default settings, you should see an accuracy of
   between 85% and 90%.

   because audio recognition is particularly useful on mobile devices,
   next we'll export it to a compact format that's easy to work with on
   those platforms. to do that, run this command line:
python tensorflow/examples/speech_commands/freeze.py \
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \
--output_file=/tmp/my_frozen_graph.pb

   once the frozen model has been created, you can test it with the
   label_wav.py script, like this:
python tensorflow/examples/speech_commands/label_wav.py \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav

   this should print out three labels:
left (score = 0.81477)
right (score = 0.14139)
_unknown_ (score = 0.03808)

   hopefully "left" is the top score since that's the correct label, but
   since the training is random it may not for the first file you try.
   experiment with some of the other .wav files in that same folder to see
   how well it does.

   the scores are between zero and one, and higher values mean the model
   is more confident in its prediction.

running the model in an android app

   the easiest way to see how this model works in a real application is to
   download [112]the prebuilt android demo applications and install them
   on your phone. you'll see 'tf speech' appear in your app list, and
   opening it will show you the same list of action words we've just
   trained our model on, starting with "yes" and "no". once you've given
   the app permission to use the microphone, you should be able to try
   saying those words and see them highlighted in the ui when the model
   recognizes one of them.

   you can also build this application yourself, since it's open source
   and [113]available as part of the tensorflow repository on github. by
   default it downloads [114]a pretrained model from tensorflow.org, but
   you can easily [115]replace it with a model you've trained yourself. if
   you do this, you'll need to make sure that the constants in [116]the
   main speechactivity java source file like sample_rate and
   sample_duration match any changes you've made to the defaults while
   training. you'll also see that there's a [117]java version of the
   recognizecommands module that's very similar to the c++ version in this
   tutorial. if you've tweaked parameters for that, you can also update
   them in speechactivity to get the same results as in your server
   testing.

   the demo app updates its ui list of results automatically based on the
   labels text file you copy into assets alongside your frozen graph,
   which means you can easily try out different models without needing to
   make any code changes. you will need to update label_filename and
   model_filename to point to the files you've added if you change the
   paths though.

how does this model work?

   the architecture used in this tutorial is based on some described in
   the paper [118]convolutional neural networks for small-footprint
   keyword spotting. it was chosen because it's comparatively simple,
   quick to train, and easy to understand, rather than being state of the
   art. there are lots of different approaches to building neural network
   models to work with audio, including [119]recurrent networks or
   [120]dilated (atrous) convolutions. this tutorial is based on the kind
   of convolutional network that will feel very familiar to anyone who's
   worked with image recognition. that may seem surprising at first
   though, since audio is inherently a one-dimensional continuous signal
   across time, not a 2d spatial problem.

   we solve that issue by defining a window of time we believe our spoken
   words should fit into, and converting the audio signal in that window
   into an image. this is done by grouping the incoming audio samples into
   short segments, just a few milliseconds long, and calculating the
   strength of the frequencies across a set of bands. each set of
   frequency strengths from a segment is treated as a vector of numbers,
   and those vectors are arranged in time order to form a two-dimensional
   array. this array of values can then be treated like a single-channel
   image, and is known as a [121]spectrogram. if you want to view what
   kind of image an audio sample produces, you can run the
   `wav_to_spectrogram tool:
bazel run tensorflow/examples/wav_to_spectrogram:wav_to_spectrogram -- \
--input_wav=/tmp/speech_dataset/happy/ab00c4b2_nohash_0.wav \
--output_image=/tmp/spectrogram.png

   if you open up /tmp/spectrogram.png you should see something like this:
   [spectrogram.png]

   because of tensorflow's memory order, time in this image is increasing
   from top to bottom, with frequencies going from left to right, unlike
   the usual convention for spectrograms where time is left to right. you
   should be able to see a couple of distinct parts, with the first
   syllable "ha" distinct from "ppy".

   because the human ear is more sensitive to some frequencies than
   others, it's been traditional in id103 to do further
   processing to this representation to turn it into a set of
   [122]mel-frequency cepstral coefficients, or mfccs for short. this is
   also a two-dimensional, one-channel representation so it can be treated
   like an image too. if you're targeting general sounds rather than
   speech you may find you can skip this step and operate directly on the
   spectrograms.

   the image that's produced by these processing steps is then fed into a
   multi-layer convolutional neural network, with a fully-connected layer
   followed by a softmax at the end. you can see the definition of this
   portion in [123]tensorflow/examples/speech_commands/models.py.

streaming accuracy

   most audio recognition applications need to run on a continuous stream
   of audio, rather than on individual clips. a typical way to use a model
   in this environment is to apply it repeatedly at different offsets in
   time and average the results over a short window to produce a smoothed
   prediction. if you think of the input as an image, it's continuously
   scrolling along the time axis. the words we want to recognize can start
   at any time, so we need to take a series of snapshots to have a chance
   of having an alignment that captures most of the utterance in the time
   window we feed into the model. if we sample at a high enough rate, then
   we have a good chance of capturing the word in multiple windows, so
   averaging the results improves the overall confidence of the
   prediction.

   for an example of how you can use your model on streaming data, you can
   look at [124]test_streaming_accuracy.cc. this uses the
   [125]recognizecommands class to run through a long-form input audio,
   try to spot words, and compare those predictions against a ground truth
   list of labels and times. this makes it a good example of applying a
   model to a stream of audio signals over time.

   you'll need a long audio file to test it against, along with labels
   showing where each word was spoken. if you don't want to record one
   yourself, you can generate some synthetic test data using the
   generate_streaming_test_wav utility. by default this will create a ten
   minute .wav file with words roughly every three seconds, and a text
   file containing the ground truth of when each word was spoken. these
   words are pulled from the test portion of your current dataset, mixed
   in with background noise. to run it, use:
bazel run tensorflow/examples/speech_commands:generate_streaming_test_wav

   this will save a .wav file to
   /tmp/speech_commands_train/streaming_test.wav, and a text file listing
   the labels to /tmp/speech_commands_train/streaming_test_labels.txt. you
   can then run accuracy testing with:
bazel run tensorflow/examples/speech_commands:test_streaming_accuracy -- \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_commands_train/streaming_test.wav \
--ground_truth=/tmp/speech_commands_train/streaming_test_labels.txt \
--verbose

   this will output information about the number of words correctly
   matched, how many were given the wrong labels, and how many times the
   model triggered when there was no real word spoken. there are various
   parameters that control how the signal averaging works, including
   --average_window_ms which sets the length of time to average results
   over, --clip_stride_ms which is the time between applications of the
   model, --suppression_ms which stops subsequent word detections from
   triggering for a certain time after an initial one is found, and
   --detection_threshold, which controls how high the average score must
   be before it's considered a solid result.

   you'll see that the streaming accuracy outputs three numbers, rather
   than just the one metric used in training. this is because different
   applications have varying requirements, with some being able to
   tolerate frequent incorrect results as long as real words are found
   (high recall), while others very focused on ensuring the predicted
   labels are highly likely to be correct even if some aren't detected
   (high precision). the numbers from the tool give you an idea of how
   your model will perform in an application, and you can try tweaking the
   signal averaging parameters to tune it to give the kind of performance
   you want. to understand what the right parameters are for your
   application, you can look at generating an [126]roc curve to help you
   understand the tradeoffs.

recognizecommands

   the streaming accuracy tool uses a simple decoder contained in a small
   c++ class called [127]recognizecommands. this class is fed the output
   of running the tensorflow model over time, it averages the signals, and
   returns information about a label when it has enough evidence to think
   that a recognized word has been found. the implementation is fairly
   small, just keeping track of the last few predictions and averaging
   them, so it's easy to port to other platforms and languages as needed.
   for example, it's convenient to do something similar at the java level
   on android, or python on the raspberry pi. as long as these
   implementations share the same logic, you can tune the parameters that
   control the averaging using the streaming test tool, and then transfer
   them over to your application to get similar results.

advanced training

   the defaults for the training script are designed to produce good end
   to end results in a comparatively small file, but there are a lot of
   options you can change to customize the results for your own
   requirements.

custom training data

   by default the script will download the [128]speech commands dataset,
   but you can also supply your own training data. to train on your own
   data, you should make sure that you have at least several hundred
   recordings of each sound you would like to recognize, and arrange them
   into folders by class. for example, if you were trying to recognize dog
   barks from cat miaows, you would create a root folder called
   animal_sounds, and then within that two sub-folders called bark and
   miaow. you would then organize your audio files into the appropriate
   folders.

   to point the script to your new audio files, you'll need to set
   --data_url= to disable downloading of the speech commands dataset, and
   --data_dir=/your/data/folder/ to find the files you've just created.

   the files themselves should be 16-bit little-endian pcm-encoded wave
   format. the sample rate defaults to 16,000, but as long as all your
   audio is consistently the same rate (the script doesn't support
   resampling) you can change this with the --sample_rate argument. the
   clips should also all be roughly the same duration. the default
   expected duration is one second, but you can set this with the
   --clip_duration_ms flag. if you have clips with variable amounts of
   silence at the start, you can look at word alignment tools to
   standardize them ([129]here's a quick and dirty approach you can use
   too).

   one issue to watch out for is that you may have very similar
   repetitions of the same sounds in your dataset, and these can give
   misleading metrics if they're spread across your training, validation,
   and test sets. for example, the speech commands set has people
   repeating the same word multiple times. each one of those repetitions
   is likely to be pretty close to the others, so if training was
   overfitting and memorizing one, it could perform unrealistically well
   when it saw a very similar copy in the test set. to avoid this danger,
   speech commands trys to ensure that all clips featuring the same word
   spoken by a single person are put into the same partition. clips are
   assigned to training, test, or validation sets based on a hash of their
   filename, to ensure that the assignments remain steady even as new
   clips are added and avoid any training samples migrating into the other
   sets. to make sure that all a given speaker's words are in the same
   bucket, [130]the hashing function ignores anything in a filename after
   'nohash' when calculating the assignments. this means that if you have
   file names like pete_nohash_0.wav and pete_nohash_1.wav, they're
   guaranteed to be in the same set.

unknown class

   it's likely that your application will hear sounds that aren't in your
   training set, and you'll want the model to indicate that it doesn't
   recognize the noise in those cases. to help the network learn what
   sounds to ignore, you need to provide some clips of audio that are
   neither of your classes. to do this, you'd create quack, oink, and moo
   subfolders and populate them with noises from other animals your users
   might encounter. the --wanted_words argument to the script defines
   which classes you care about, all the others mentioned in subfolder
   names will be used to populate an _unknown_ class during training. the
   speech commands dataset has twenty words in its unknown classes,
   including the digits zero through nine and random names like "sheila".

   by default 10% of the training examples are picked from the unknown
   classes, but you can control this with the --unknown_percentage flag.
   increasing this will make the model less likely to mistake unknown
   words for wanted ones, but making it too large can backfire as the
   model might decide it's safest to categorize all words as unknown!

background noise

   real applications have to recognize audio even when there are other
   irrelevant sounds happening in the environment. to build a model that's
   robust to this kind of interference, we need to train against recorded
   audio with similar properties. the files in the speech commands dataset
   were captured on a variety of devices by users in many different
   environments, not in a studio, so that helps add some realism to the
   training. to add even more, you can mix in random segments of
   environmental audio to the training inputs. in the speech commands set
   there's a special folder called _background_noise_ which contains
   minute-long wave files with white noise and recordings of machinery and
   everyday household activity.

   small snippets of these files are chosen at random and mixed at a low
   volume into clips during training. the loudness is also chosen
   randomly, and controlled by the --background_volume argument as a
   proportion where 0 is silence, and 1 is full volume. not all clips have
   background added, so the --background_frequency flag controls what
   proportion have them mixed in.

   your own application might operate in its own environment with
   different background noise patterns than these defaults, so you can
   supply your own audio clips in the _background_noise_ folder. these
   should be the same sample rate as your main dataset, but much longer in
   duration so that a good set of random segments can be selected from
   them.

silence

   in most cases the sounds you care about will be intermittent and so
   it's important to know when there's no matching audio. to support this,
   there's a special _silence_ label that indicates when the model detects
   nothing interesting. because there's never complete silence in real
   environments, we actually have to supply examples with quiet and
   irrelevant audio. for this, we reuse the _background_noise_ folder
   that's also mixed in to real clips, pulling short sections of the audio
   data and feeding those in with the ground truth class of _silence_. by
   default 10% of the training data is supplied like this, but the
   --silence_percentage can be used to control the proportion. as with
   unknown words, setting this higher can weight the model results in
   favor of true positives for silence, at the expense of false negatives
   for words, but too large a proportion can cause it to fall into the
   trap of always guessing silence.

time shifting

   adding in background noise is one way of distorting the training data
   in a realistic way to effectively increase the size of the dataset, and
   so increase overall accuracy, and time shifting is another. this
   involves a random offset in time of the training sample data, so that a
   small part of the start or end is cut off and the opposite section is
   padded with zeroes. this mimics the natural variations in starting time
   in the training data, and is controlled with the --time_shift_ms flag,
   which defaults to 100ms. increasing this value will provide more
   variation, but at the risk of cutting off important parts of the audio.
   a related way of augmenting the data with realistic distortions is by
   using [131]time stretching and pitch scaling, but that's outside the
   scope of this tutorial.

customizing the model

   the default model used for this script is pretty large, taking over 800
   million flops for each id136 and using 940,000 weight parameters.
   this runs at usable speeds on desktop machines or modern phones, but it
   involves too many calculations to run at interactive speeds on devices
   with more limited resources. to support these use cases, there's a
   couple of alternatives available:

   low_latency_conv based on the 'id98-one-fstride4' topology described in
   the [132]convolutional neural networks for small-footprint keyword
   spotting paper. the accuracy is slightly lower than 'conv' but the
   number of weight parameters is about the same, and it only needs 11
   million flops to run one prediction, making it much faster.

   to use this model, you specify --model_architecture=low_latency_conv on
   the command line. you'll also need to update the training rates and the
   number of steps, so the full command will look like:
python tensorflow/examples/speech_commands/train \
--model_architecture=low_latency_conv \
--how_many_training_steps=20000,6000 \
--learning_rate=0.01,0.001

   this asks the script to train with a learning rate of 0.01 for 20,000
   steps, and then do a fine-tuning pass of 6,000 steps with a 10x smaller
   rate.

   low_latency_svdf based on the topology presented in the
   [133]compressing deep neural networks using a rank-constrained topology
   paper. the accuracy is also lower than 'conv' but it only uses about
   750 thousand parameters, and most significantly, it allows for an
   optimized execution at test time (i.e. when you will actually use it in
   your application), resulting in 750 thousand flops.

   to use this model, you specify --model_architecture=low_latency_svdf on
   the command line, and update the training rates and the number of
   steps, so the full command will look like:
python tensorflow/examples/speech_commands/train \
--model_architecture=low_latency_svdf \
--how_many_training_steps=100000,35000 \
--learning_rate=0.01,0.005

   note that despite requiring a larger number of steps than the previous
   two topologies, the reduced number of computations means that training
   should take about the same time, and at the end reach an accuracy of
   around 85%. you can also further tune the topology fairly easily for
   computation and accuracy by changing these parameters in the svdf
   layer:
     * rank - the rank of the approximation (higher typically better, but
       results in more computation).
     * num_units - similar to other layer types, specifies the number of
       nodes in the layer (more nodes better quality, and more
       computation).

   regarding runtime, since the layer allows optimizations by caching some
   of the internal neural network activations, you need to make sure to
   use a consistent stride (e.g. 'clip_stride_ms' flag) both when you
   freeze the graph, and when executing the model in streaming mode (e.g.
   test_streaming_accuracy.cc).

   other parameters to customize if you want to experiment with
   customizing models, a good place to start is by tweaking the
   spectrogram creation parameters. this has the effect of altering the
   size of the input image to the model, and the creation code in
   [134]models.py will adjust the number of computations and weights
   automatically to fit with different dimensions. if you make the input
   smaller, the model will need fewer computations to process it, so it
   can be a great way to trade off some accuracy for improved latency. the
   --window_stride_ms controls how far apart each frequency analysis
   sample is from the previous. if you increase this value, then fewer
   samples will be taken for a given duration, and the time axis of the
   input will shrink. the --dct_coefficient_count flag controls how many
   buckets are used for the frequency counting, so reducing this will
   shrink the input in the other dimension. the --window_size_ms argument
   doesn't affect the size, but does control how wide the area used to
   calculate the frequencies is for each sample. reducing the duration of
   the training samples, controlled by --clip_duration_ms, can also help
   if the sounds you're looking for are short, since that also reduces the
   time dimension of the input. you'll need to make sure that all your
   training data contains the right audio in the initial portion of the
   clip though.

   if you have an entirely different model in mind for your problem, you
   may find that you can plug it into [135]models.py and have the rest of
   the script handle all of the preprocessing and training mechanics. you
   would add a new clause to create_model, looking for the name of your
   architecture and then calling a model creation function. this function
   is given the size of the spectrogram input, along with other model
   information, and is expected to create tensorflow ops to read that in
   and produce an output prediction vector, and a placeholder to control
   the dropout rate. the rest of the script will handle integrating this
   model into a larger graph doing the input calculations and applying
   softmax and a id168 to train it.

   one common problem when you're adjusting models and training
   hyper-parameters is that not-a-number values can creep in, thanks to
   numerical precision issues. in general you can solve these by reducing
   the magnitude of things like learning rates and weight initialization
   functions, but if they're persistent you can enable the --check_nans
   flag to track down the source of the errors. this will insert check ops
   between most regular operations in tensorflow, and abort the training
   process with a useful error message when they're encountered.

   except as otherwise noted, the content of this page is licensed under
   the [136]creative commons attribution 3.0 license, and code samples are
   licensed under the [137]apache 2.0 license. for details, see the
   [138]google developers site policies. java is a registered trademark of
   oracle and/or its affiliates.

     * stay connected
          + [139]blog
          + [140]github
          + [141]twitter
          + [142]youtube
     * support
          + [143]issue tracker
          + [144]release notes
          + [145]stack overflow
          + [146]brand guidelines

     * [147]terms
     * [148]privacy

   [english_____]

references

   visible links
   1. https://www.tensorflow.org/s/opensearch.xml
   2. https://www.tensorflow.org/
   3. https://www.tensorflow.org/install
   4. https://www.tensorflow.org/learn
   5. https://www.tensorflow.org/learn
   6. https://www.tensorflow.org/overview
   7. https://www.tensorflow.org/js
   8. https://www.tensorflow.org/lite
   9. https://www.tensorflow.org/tfx
  10. https://www.tensorflow.org/swift
  11. https://www.tensorflow.org/api_docs/python/tf
  12. https://www.tensorflow.org/api_docs/python/tf
  13. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  14. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  15. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  16. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  17. https://www.tensorflow.org/versions
  18. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  19. https://www.tensorflow.org/resources/models-datasets
  20. https://www.tensorflow.org/resources/models-datasets
  21. https://www.tensorflow.org/resources/tools
  22. https://www.tensorflow.org/resources/libraries-extensions
  23. https://www.tensorflow.org/community
  24. https://www.tensorflow.org/about
  25. https://www.tensorflow.org/about
  26. https://www.tensorflow.org/about/case-studies
  27. https://github.com/tensorflow
  28. https://www.tensorflow.org/overview
  29. https://www.tensorflow.org/overview
  30. https://www.tensorflow.org/tutorials
  31. https://www.tensorflow.org/guide
  32. https://www.tensorflow.org/alpha
  33. https://www.tensorflow.org/install
  34. https://www.tensorflow.org/learn
  35. https://www.tensorflow.org/overview
  36. https://www.tensorflow.org/tutorials
  37. https://www.tensorflow.org/guide
  38. https://www.tensorflow.org/alpha
  39. https://www.tensorflow.org/api_docs/python/tf
  40. https://www.tensorflow.org/resources/models-datasets
  41. https://www.tensorflow.org/community
  42. https://www.tensorflow.org/about
  43. https://github.com/tensorflow
  44. https://www.tensorflow.org/tutorials
  45. https://www.tensorflow.org/tutorials/keras
  46. https://www.tensorflow.org/tutorials/keras/basic_classification
  47. https://www.tensorflow.org/tutorials/keras/basic_text_classification
  48. https://www.tensorflow.org/tutorials/keras/basic_regression
  49. https://www.tensorflow.org/tutorials/keras/overfit_and_underfit
  50. https://www.tensorflow.org/tutorials/keras/save_and_restore_models
  51. https://www.tensorflow.org/tutorials/eager
  52. https://www.tensorflow.org/tutorials/eager/eager_basics
  53. https://www.tensorflow.org/tutorials/eager/automatic_differentiation
  54. https://www.tensorflow.org/tutorials/eager/custom_training
  55. https://www.tensorflow.org/tutorials/eager/custom_layers
  56. https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough
  57. https://www.tensorflow.org/tutorials/estimators/linear
  58. https://github.com/tensorflow/models/tree/master/official/wide_deep
  59. https://www.tensorflow.org/tutorials/estimators/boosted_trees
  60. https://www.tensorflow.org/tutorials/estimators/boosted_trees_model_understanding
  61. https://www.tensorflow.org/tutorials/estimators/id98
  62. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/id4_with_attention/id4_with_attention.ipynb
  63. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb
  64. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb
  65. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb
  66. https://www.tensorflow.org/tutorials/images/hub_with_keras
  67. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb
  68. https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_neural_style_transfer_with_eager_execution.ipynb
  69. https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb
  70. https://www.tensorflow.org/tutorials/images/deep_id98
  71. https://www.tensorflow.org/tutorials/sequences/text_generation
  72. https://www.tensorflow.org/tutorials/sequences/recurrent
  73. https://www.tensorflow.org/tutorials/sequences/recurrent_quickdraw
  74. https://www.tensorflow.org/tutorials/sequences/audio_recognition
  75. https://github.com/tensorflow/id4
  76. https://www.tensorflow.org/tutorials/load_data/images
  77. https://www.tensorflow.org/tutorials/load_data/tf_records
  78. https://www.tensorflow.org/tutorials/representation/id97
  79. https://www.tensorflow.org/tutorials/representation/kernel_methods
  80. https://www.tensorflow.org/tutorials/representation/linear
  81. https://www.tensorflow.org/tutorials/representation/unicode
  82. https://www.tensorflow.org/tutorials/non-ml/mandelbrot
  83. https://www.tensorflow.org/tutorials/non-ml/pdes
  84. https://www.tensorflow.org/learn
  85. https://www.tensorflow.org/overview
  86. https://www.tensorflow.org/js
  87. https://www.tensorflow.org/lite
  88. https://www.tensorflow.org/tfx
  89. https://www.tensorflow.org/swift
  90. https://www.tensorflow.org/api_docs/python/tf
  91. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  92. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  93. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  94. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  95. https://www.tensorflow.org/versions
  96. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  97. https://www.tensorflow.org/resources/models-datasets
  98. https://www.tensorflow.org/resources/tools
  99. https://www.tensorflow.org/resources/libraries-extensions
 100. https://www.tensorflow.org/about
 101. https://www.tensorflow.org/about/case-studies
 102. https://www.youtube.com/playlist?list=plqy2h8rroyvzouyi26khmksjbedn3squb
 103. https://www.tensorflow.org/
 104. https://www.tensorflow.org/learn
 105. https://www.tensorflow.org/overview
 106. https://www.tensorflow.org/tutorials
 107. https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz
 108. https://aiyprojects.withgoogle.com/open_speech_recording
 109. https://arxiv.org/abs/1804.03209
 110. https://www.tensorflow.org/api_docs/python/tf/confusion_matrix
 111. http://localhost:6006/
 112. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#prebuilt-components
 113. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#building-in-android-studio-using-the-tensorflow-aar-from-jcenter
 114. http://download.tensorflow.org/models/speech_commands_v0.02.zip
 115. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#install-model-files-optional
 116. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/src/org/tensorflow/demo/speechactivity.java
 117. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/src/org/tensorflow/demo/recognizecommands.java
 118. http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf
 119. https://svds.com/tensorflow-id56-tutorial/
 120. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
 121. https://en.wikipedia.org/wiki/spectrogram
 122. https://en.wikipedia.org/wiki/mel-frequency_cepstrum
 123. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands/models.py
 124. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands/
 125. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands/recognize_commands.h
 126. https://en.wikipedia.org/wiki/receiver_operating_characteristic
 127. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands/recognize_commands.h
 128. https://download.tensorflow.org/data/speech_commands_v0.01.tgz
 129. https://petewarden.com/2017/07/17/a-quick-hack-to-align-single-word-audio-recordings/
 130. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands/input_data.py
 131. https://en.wikipedia.org/wiki/audio_time_stretching_and_pitch_scaling
 132. http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf
 133. https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43813.pdf
 134. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands/models.py
 135. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands/models.py
 136. https://creativecommons.org/licenses/by/3.0/
 137. https://www.apache.org/licenses/license-2.0
 138. https://developers.google.com/site-policies
 139. https://medium.com/tensorflow
 140. https://github.com/tensorflow/
 141. https://twitter.com/tensorflow
 142. https://youtube.com/tensorflow
 143. https://github.com/tensorflow/tensorflow/issues
 144. https://github.com/tensorflow/tensorflow/blob/master/release.md
 145. https://stackoverflow.com/questions/tagged/tensorflow
 146. https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf
 147. https://policies.google.com/terms
 148. https://policies.google.com/privacy

   hidden links:
 150. https://www.tensorflow.org/tutorials/sequences/audio_recognition
 151. https://www.tensorflow.org/tutorials/sequences/audio_recognition
 152. https://www.tensorflow.org/tutorials/sequences/audio_recognition
 153. https://www.tensorflow.org/tutorials/sequences/audio_recognition
