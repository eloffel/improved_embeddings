   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

extracting data from websites using scrapy

   [9]go to the profile of kais hassan
   [10]kais hassan (button) blockedunblock (button) followfollowing
   apr 29, 2016

1 introduction

   a big percent of the world   s data is unstructured, estimated around
   [11]70%-80%. websites are a rich source for unstructured text that can
   be mined and turned into useful insights. the process of extracting
   information from websites is usually referred to as [12]web scraping.

   there are several good open source web scraping frameworks, including
   [13]scrapy, [14]nutch and [15]heritrix. a good review on open source
   crawlers can be found [16]here, also this [17]quora question contains
   some good related answers.

   for medium sized scraping projects, scrapy stands out from the rest
   since it is:
     * easy to setup and use
     * great documentation
     * mature and focused solution
     * built-in support for proxies, redirection, authentication, cookies
       and others
     * built-in support for exporting to csv, json and xml

   this article will walk you through installing scrapy, extracting data
   from a site and analyzing it.

   note:
     * this guide is written for ubuntu 14.04 and 16.04, but it should
       work with other linuxes.

2 installing scrapy

2.1 python

   scrapy framework is developed in python, which is preinstalled in
   ubuntu and almost all linux distributions. as of scrapy 1.05, it
   requires python 2.x, to make sure python 2.x is installed, issue the
   following command:
python --version

   the result should be something like the following:
python 2.7.6

   note:
     * ubuntu 16.04 minimal install does not come with python 2
       preinstalled anymore. to install it, issue the following command:

sudo apt-get install python-minimal

2.2 pip

   there are several ways to install scrapy on ubuntu. in order to get the
   latest scrapy version, this guide we will use the [18]pip (python
   package management system) method. to install pip on ubuntu along with
   all the needed dependency, issue the following command:
sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev l
ibffi-dev libssl-dev

2.3 scrapy

   after installing pip, install scrapy with the following command:
sudo -h pip install scrapy
     * make sure you use the [19]-h flag, in order for the environment
       variables to be set correctly

   to make sure that scrapy is installed correctly, issue the following
   command:
scrapy version

   the result should be something like the following:
scrapy 1.0.5

3 scrapy in action

3.1 using scrapy

   there are various methods to use scrapy, it all depends on your use
   case and needs, for example:
     * basic usage: create a python file containing a spider. a spider in
       scrapy is a class that contains the extraction logic for a website.
       then run the spider from the command line.
     * medium usage: create a scrapy project that contains multiple
       spiders, configuration and pipelines.
     * scheduled scraping: use [20]scrapyd to run scrapy as a service,
       deploy projects and schedule the spiders.
     * testing and debugging: use scrapy [21]interactive shell console for
       trying out things

   in this guide, i will focus on running spiders from the command line,
   since all other methods are similar and somehow straightforward.

3.2 understanding selectors

   before scraping our first website, it is important to understand the
   concept of [22]selectors in scrapy. basicly, selectors are the path (or
   formula) of the items we need to extract data from inside a html page.

   from scrapy documentation:

     scrapy comes with its own mechanism for extracting data. they   re
     called selectors because they    select    certain parts of the html
     document specified either by [23]xpath or [24]css expressions.

     [25]xpath is a language for selecting nodes in xml documents, which
     can also be used with html. [26]css is a language for applying
     styles to html documents. it defines selectors to associate those
     styles with specific html elements.

   you can either use the css or xpath selectors to point to the items you
   want to extract. personally, i prefer the css selectors since they are
   easier to read. to find the css/xpath of items i greatly recommend that
   you install the chrome [27]plugin [28]selectorgadget. on firefox you
   can install [29]firebug and [30]firefinder.

   we will use the selectorgadget chrome plugin for this guide, so install
   from [31]here.

3.3 scraping walkthrough: getting the selectors

   for this guide we will extract deals information from [32]souq.com, the
   largest ecommerce site in arabia. we will start with the [33]deals page
   and follow the links to each product and scrape some data from each
   page.

   after opening the [34]deals page, click on selectorgadget icon to
   activate the plugin, afterwards click on any product name, as shown in
   figure [1].
   [35][0*axqemhqvzje1deed.]
   figure 1: using selectorgadget to select product title

   notice, that the selectorgadget selected all the links in the page and
   highlighted them yellow, which is something we don   t want. now, click
   on one of the unwanted links, the product image will do. you will end
   up with only the relevant links highlighted as shown in the annotated
   figure [2]. look closely at the selectorgadget toolbar, and notice:

   1. css selector: this text box will contain the css selector based on
   your selection. you can also manually edit it to test out selectors. in
   souq   s deals page, it contains
.sp-cms_unit     ttl a

   2. number of results: this label counts the number of found items based
   on your selection.
   [36][0*bmitx_tkzjel2rpx.]
   figure 2: annotated page of all the required links selected

   click on the selectorgadget icon again to deactivate it, click on the
   title for any product so you can be redirected to that product page.
   use the selectorgadget plugin to determine the selectors for title,
   seller and other interesting information. alternatively, have look at
   the code in the next section to to view the selectors values.

3.4 scraping walkthrough: coding the spider

   copy the following code listing into your favourite text editor and
   save it as souq_spider.py
# -*- coding: utf-8 -*-
import scrapy
class souqspider(scrapy.spider):
    name = "souq"  # name of the spider, required value
    start_urls = ["http://deals.souq.com/ae-en/"]  # the starting url, scrapy wi
ll request this url in parse
    # entry point for the spider
    def parse(self, response):
        for href in response.css('.sp-cms_unit--ttl a::attr(href)'):
            url = href.extract()
            yield scrapy.request(url, callback=self.parse_item)
    # method for parsing a product page
    def parse_item(self, response):
        original_price = -1
        savings=0
        discounted = false
        seller_rating = response.css('.vip-product-info .stats .inline-block sma
ll::text'). extract()[0]
        seller_rating = int(filter(unicode.isdigit,seller_rating))
        # not all deals are discounted
        if response.css('.vip-product-info .subhead::text').extract():
            original_price = response.css('.vip-product-info .subhead::text').ex
tract()[0].replace("aed", "")
            discounted = true
            savings = response.css('.vip-product-info .message .nowrap::text').e
xtract()[0].replace("aed", "")
        yield {
            'title': response.css('.product-title h1::text').extract()[0],
            'category': response.css('.product-title h1+ span a+ a::text').extra
ct()[0],
            'originalprice': original_price,
            'currentprice': response.css('.vip-product-info .price::text').extra
ct()[0].replace(u"\xa0", ""),
            'discounted': discounted,
            'savings': savings,
            'soldby': response.css('.vip-product-info .stats a::text').extract()
[0],
            'sellerrating': seller_rating,
            'url': response.url
        }

   the previous code will request the deals page at souq, loop on each
   product link and extract the required data inside the parse_item
   method.

3.5 scraping walkthrough: running the spider

   to run the spider we are going to use the runspider command, issue the
   following command:
scrapy runspider souq_spider.py -o deals.csv

   when the extractions finishes you will have a csv file (deals.csv)
   containing souq deals for the day, figure [3] contains a sample out.
   you can also change the format of the output to json or xml by changing
   the output file extension in the runspider command, for example, to get
   the results in json issue:
scrapy runspider souq_spider.py -o deals.json

   [37][0*uas7yhmbij92ymfe.]
   figure 3: deals.csv file containing the extracted deals

4 data analysis

   looking at the data of 1,200 product deals, i came with the following
   analysis:

4.1 top categories

   there are 113 different categories in the deals data. i found it a bit
   surprising that the top two deals categories are watches (10%) and
   handbags (9%) as shown in figure [4]. but if we combine mobile phone
   (7.6%), laptops (5.6%) and tablets (4%) into a general electronics
   category, we get (17%), which bring it to the top category and makes
   much more sense.
   [38][0*wtw5_y5waxpgnayz.]
   figure 4: top product categories in deals

4.2 top sellers

   this data is more interesting, from the 86 different sellers, around
   78% of the 1,200 deals are provided by one seller (dod_uae). also, as
   illustrated in figure [5], the next seller (jumboelectronics) accounts
   for only 2%. seller dod_uae deals are in 98 different categories (87%
   of categories) ranging from baby food, jewelry to laptops and fitness
   technology. in comparison, jumboelectronics deals are in 3 related
   categories.one can conclude that dod_uae is somehow related to souq or
   a very active seller!
   [39][0*t9sgihfbbqhj9jrb.]
   figure 5: top sellers in deals

4.3 sellers ratings

   there are 14 sellers with 0% positive rating, i am not sure what does
   that mean in souq terminology. it might be that these sellers don   t
   have a rating yet or their overall rating is too low, so it was decided
   to hide it. the average rating for the remaining non-zero rating is
   86.542 with a standard deviation of 8.952. there does not seem to be a
   correlation between seller rating and discount percentage as shown in
   figure [6], it seems other variables such as service and quality
   factors more in rating than discount percentage.
   [40][0*erh57wo3gkwknsqw.]
   figure 6: discount vs rating

4.4 product prices and discounts

   we can get a lot of useful information looking at the deals prices and
   discounts, for example:
     * the most expensive item before discount is a [41]jbw watch (29323
       aed), coincidentally it is also the most discounted item (94%)
       discount, so you can get it now for 1599 aed.
     * the most expensive item after discount is a [42]nikon camera,
       selling for 7699 aed (around 2000$)
     * the average discount percentage for all the deals is 42%

   if you have the urge to buy something from souq after scraping and
   analyzing the deals, but you are on a tight budget, you can always find
   the cheapest item. currently, it is a [43]selfie stick, selling for
   only 6 aed ( < 2$)

5 related links

     * [44]scrapy documentation
     * [45]scrapy github wiki
     * [46]scrapy code snippets

   if you have found this article useful and managed to scrap interesting
   data with scrapy, share your results via the comments section.

     * [47]data science
     * [48]web scraping
     * [49]python

   (button)
   (button)
   (button) 131 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [50]go to the profile of kais hassan

[51]kais hassan

   data scientist

     * (button)
       (button) 131
     * (button)
     *
     *

   [52]go to the profile of kais hassan
   never miss a story from kais hassan, when you sign up for medium.
   [53]learn more
   never miss a story from kais hassan
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e1e1e357651a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@kaismh?source=post_header_lockup
  10. https://medium.com/@kaismh
  11. https://breakthroughanalysis.com/2008/08/01/unstructured-data-and-the-80-percent-rule/
  12. https://en.wikipedia.org/wiki/web_scraping
  13. http://scrapy.org/
  14. http://nutch.apache.org/
  15. https://webarchive.jira.com/wiki/display/heritrix
  16. http://www.ijser.org/researchpaper\comparison-of-open-source-crawlers--a-review.pdf
  17. https://www.quora.com/what-is-the-best-open-source-web-crawler-and-why
  18. https://pip.pypa.io/
  19. https://www.sudo.ws/man/sudo.man.html
  20. http://scrapyd.readthedocs.io/en/latest/
  21. http://doc.scrapy.org/en/latest/topics/shell.html
  22. http://doc.scrapy.org/en/latest/topics/selectors.html
  23. http://www.w3.org/tr/xpath
  24. http://www.w3.org/tr/selectors
  25. http://www.w3.org/tr/xpath
  26. http://www.w3.org/tr/selectors
  27. https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb
  28. http://selectorgadget.com/
  29. https://addons.mozilla.org/en-us/firefox/addon/firebug/
  30. https://addons.mozilla.org/en-us/firefox/addon/firefinder-for-firebug/
  31. https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb
  32. http://uae.souq.com/ae-en/
  33. http://deals.souq.com/ae-en/
  34. http://deals.souq.com/ae-en/
  35. https://kaismh.files.wordpress.com/2016/04/souq1.png
  36. https://kaismh.files.wordpress.com/2016/04/souq2.png
  37. https://kaismh.files.wordpress.com/2016/04/souq3.png
  38. https://kaismh.files.wordpress.com/2016/04/souqcat.png
  39. https://kaismh.files.wordpress.com/2016/04/souqsellers.png
  40. https://kaismh.files.wordpress.com/2016/04/souqsellersdiscount.png
  41. http://uae.souq.com/ae-en/jbw-jet-setter-men-s-234-diamonds-gold-dial-gold-plated-stainless-steel-band-watch-jb-6213-a-67117500490/u/
  42. http://uae.souq.com/ae-en/nikon-d810-body-only-36-3-megapixel-3321500014/u/
  43. http://uae.souq.com/ae-en/mini-wired-selfie-stick-monopod-for-smartphones-orange-66160600026/u/
  44. http://doc.scrapy.org/en/latest/intro/overview.html
  45. https://github.com/scrapy/scrapy/wiki
  46. http://snipplr.com/all/tags/scrapy/
  47. https://medium.com/tag/data-science?source=post
  48. https://medium.com/tag/web-scraping?source=post
  49. https://medium.com/tag/python?source=post
  50. https://medium.com/@kaismh?source=footer_card
  51. https://medium.com/@kaismh
  52. https://medium.com/@kaismh
  53. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  55. https://medium.com/p/e1e1e357651a/share/twitter
  56. https://medium.com/p/e1e1e357651a/share/facebook
  57. https://medium.com/p/e1e1e357651a/share/twitter
  58. https://medium.com/p/e1e1e357651a/share/facebook
