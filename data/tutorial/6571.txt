   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    practical guide to principal component analysis
   (pca) in r & python comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]python [94]practical guide to principal component analysis
   (pca) in r & python

   [95]python[96]r

practical guide to principal component analysis (pca) in r & python

   [97]analytics vidhya content team, march 21, 2016

introduction

     too much of anything is good for nothing!

   picture this     you are working on a large scale [98]data science
   project. what happens when the given data set has too many variables?
   here are few possible situations which you might come across:
    1. you find that most of the variables are correlated.
    2. you lose patience and decide to run a model on the whole data. this
       returns poor accuracy and you feel terrible.
    3. you become indecisive about what to do
    4. you start thinking of some strategic method to find few important
       variables

   trust me, dealing with such situations isn   t as difficult as it sounds.
   [99]statistical techniques such as factor analysis and principal
   component analysis help to overcome such difficulties.

   in this post, i   ve explained the concept of principal component
   analysis in detail. i   ve kept the explanation to be simple and
   informative. for practical understanding, i   ve also demonstrated using
   this technique in r with interpretations.

   note: understanding this concept requires prior knowledge of
   statistics.

   update (as on 28th july): process of [100]predictive modeling with pca
   components in r is added below.

   [101]pca, principal component analysis


what is principal component analysis ?

   in simple words, principal component analysis is a method of extracting
   important variables (in form of components) from a large set of
   variables available in a data set. it extracts low dimensional set of
   features from a high dimensional data set with a motive to capture as
   much information as possible. with fewer variables, visualization also
   becomes much more meaningful. pca is more useful when dealing with 3 or
   higher dimensional data.

   it is always performed on a symmetric correlation or covariance matrix.
   this means the matrix should be numeric and have standardized data.

   let   s understand it using an example:

   let   s say we have a data set of dimension 300 (n)    50 (p). n
   represents the number of observations and p represents number of
   predictors. since we have a large p = 50, there can be p(p-1)/2 scatter
   plots i.e more than 1000 plots possible to analyze the variable
   relationship. wouldn   t is be a tedious job to perform exploratory
   analysis on this data ?

   in this case, it would be a lucid approach to select a subset of p (p
   << 50) predictor which captures as much information. followed
   by plotting the observation in the resultant low dimensional space.

   the image below shows the transformation of a high dimensional data (3
   dimension) to low dimensional data (2 dimension) using pca. not to
   forget, each resultant dimension is a linear combination of p features

   1 source: [102]nlpca



what are principal components ?

   a principal component is a normalized linear combination of
   the original predictors in a data set. in image above, pc1 and pc2 are
   the principal components. let   s say we have a set of predictors as
   x  , x  ...,x^p

   the principal component can be written as:

   z   =       x   +       x   +       x   + .... +  ^p  x^p

   where,
     * z   is first principal component
     *   ^p   is the loading vector comprising of loadings (    ,     ..) of
       first principal component. the loadings are constrained to a sum of
       square equals to 1. this is because large magnitude of loadings may
       lead to large variance. it also defines the direction of the
       principal component (z  ) along which data varies the most. it
       results in a line in p dimensional space which is closest to the n
       observations. closeness is measured using average squared euclidean
       distance.
     * x  ..x^p are normalized predictors. normalized predictors have mean
       equals to zero and standard deviation equals to one.

   therefore,

   first principal component is a linear combination of original predictor
   variables which captures the maximum variance in the data
   set. it determines the direction of highest variability in the
   data. larger the variability captured in first component, larger the
   information captured by component. no other component can have
   variability higher than first principal component.

   the first principal component results in a line which is closest to the
   data i.e. it minimizes the sum of squared distance between a data point
   and the line.

   similarly, we can compute the second principal component also.


   second principal component (z  ) is also a linear combination of
   original predictors which captures the remaining variance in the data
   set and is uncorrelated with z  . in other words, the correlation
   between first and second component should is zero. it can be
   represented as:

   z   =       x   +       x   +       x   + .... +   ^p2x^p

   if the two components are uncorrelated, their directions should be
   orthogonal (image below). this image is based on a simulated data with
   2 predictors. notice the direction of the components, as expected they
   are orthogonal. this suggests the correlation b/w these components in
   zero.

   2 all succeeding principal component follows a similar concept i.e.
   they capture the remaining variation without being correlated with the
   previous component. in general, for n    p dimensional data, min(n-1, p)
   principal component can be constructed.

   the directions of these components are identified in an unsupervised
   way i.e. the response variable(y) is not used to determine the
   component direction. therefore, it is an unsupervised approach.

   note: partial least square (pls) is a supervised alternative to pca.
   pls assigns higher weight to variables which are strongly related to
   response variable to determine principal components.


why is id172 of variables necessary ?

   the principal components are supplied with normalized version of
   original predictors. this is because, the original predictors may have
   different scales. for example: imagine a data set with variables   
   measuring units as gallons, kilometers, light years etc. it is definite
   that the scale of variances in these variables will be large.

   performing pca on un-normalized variables will lead to insanely large
   loadings for variables with high variance. in turn, this will lead to
   dependence of a principal component on the variable with high variance.
   this is undesirable.

   as shown in image below, pca was run on a data set twice (with unscaled
   and scaled predictors). this data set has ~40 variables. you can see,
   first principal component is dominated by a variable item_mrp. and,
   second principal component is dominated by a variable item_weight. this
   domination prevails due to high value of variance associated with a
   variable. when the variables are scaled, we get a much better
   representation of variables in 2d space.

   6


implement pca in r & python (with interpretation)

   how many principal components to choose ? i could dive deep in theory,
   but it would be better to answer these question practically.

   for this demonstration, i   ll be using the data set from [103]big mart
   prediction challenge iii.

   remember, pca can be applied only on numerical data. therefore, if the
   data has categorical variables they must be converted to numerical.
   also, make sure you have done the basic data cleaning prior to
   implementing this technique. let   s quickly finish with initial data
   loading and cleaning steps:

   #directory path
   > path <- ".../data/big_mart_sales"

   #set working directory
   > setwd(path)

   #load train and test file
   > train <- read.csv("train_big.csv")
   > test <- read.csv("test_big.csv")

   #add a column
   > test$item_outlet_sales <- 1

   #combine the data set
   > combi <- rbind(train, test)

   #impute missing values with median
   > combi$item_weight[is.na(combi$item_weight)] <-
   median(combi$item_weight, na.rm = true)

   #impute 0 with median
   > combi$item_visibility <- ifelse(combi$item_visibility == 0,
   median(combi$item_visibility),
   combi$item_visibility)

   #find mode and impute
   > table(combi$outlet_size, combi$outlet_type)
   > levels(combi$outlet_size)[1] <- "other"

   till here, we   ve imputed missing values. now we are left with removing
   the dependent (response) variable and other identifier variables( if
   any). as we said above, we are practicing an unsupervised learning
   technique, hence response variable must be removed.

   #remove the dependent and identifier variables
   > my_data <- subset(combi, select = -c(item_outlet_sales,
   item_identifier,
   outlet_identifier))

   let   s check the available variables ( a.k.a predictors) in the data
   set.

   #check available variables
   > colnames(my_data)

   since pca works on numeric variables, let   s see if we have any variable
   other than numeric.

   #check variable class
   > str(my_data)

   'data.frame': 14204 obs. of 9 variables:
   $ item_weight : num 9.3 5.92 17.5 19.2 8.93 ...
   $ item_fat_content : factor w/ 5 levels "lf","low fat",..: 3 5 3 5 3 5
   5 3 5 5 ...
   $ item_visibility : num 0.016 0.0193 0.0168 0.054 0.054 ...
   $ item_type : factor w/ 16 levels "baking goods",..: 5 15 11 7 10 1 14
   14 6 6 ...
   $ item_mrp : num 249.8 48.3 141.6 182.1 53.9 ...
   $ outlet_establishment_year: int 1999 2009 1999 1998 1987 2009 1987
   1985 2002 2007 ...
   $ outlet_size : factor w/ 4 levels "other","high",..: 3 3 3 1 2 3 2 3 1
   1 ...
   $ outlet_location_type : factor w/ 3 levels "tier 1","tier 2",..: 1 3 1
   3 3 3 3 3 2 2 ...
   $ outlet_type : factor w/ 4 levels "grocery store",..: 2 3 2 1 2 3 2 4
   2 2 ...

   sadly, 6 out of 9 variables are categorical in nature. we have some
   additional work to do now. we   ll convert these categorical variables
   into numeric using one hot encoding.

   #load library
   > library(dummies)

   #create a dummy data frame
   > new_my_data <- dummy.data.frame(my_data, names =
   c("item_fat_content","item_type",

   "outlet_establishment_year","outlet_size",
                                   "outlet_location_type","outlet_type"))

   to check, if we now have a data set of integer values, simple write:

   #check the data set
   > str(new_my_data)

   and, we now have all the numerical values. let   s divide the data into
   test and train.

   #divide the new data
   > pca.train <- new_my_data[1:nrow(train),]
   > pca.test <- new_my_data[-(1:nrow(train)),]

   we can now go ahead with pca.

   the base r function prcomp() is used to perform pca. by default, it
   centers the variable to have mean equals to zero. with parameter scale.
   = t, we normalize the variables to have standard deviation equals to 1.

   #principal component analysis
   > prin_comp <- prcomp(pca.train, scale. = t)
   > names(prin_comp)
   [1] "sdev"     "rotation" "center"   "scale"    "x"

   the prcomp() function results in 5 useful measures:

   1. center and scale refers to respective mean and standard deviation of
   the variables that are used for id172 prior to implementing pca

   #outputs the mean of variables
   prin_comp$center

   #outputs the standard deviation of variables
   prin_comp$scale

   2. the rotation measure provides the principal component loading. each
   column of rotation matrix contains the principal component loading
   vector. this is the most important measure we should be interested in.

   > prin_comp$rotation

   this returns 44 principal components loadings. is that correct ?
   absolutely. in a data set, the maximum number of principal component
   loadings is a minimum of (n-1, p). let   s look at first 4 principal
   components and first 5 rows.

   > prin_comp$rotation[1:5,1:4]
                                   pc1            pc2            pc3
         pc4
   item_weight                0.0054429225   -0.001285666   0.011246194
   0.011887106
   item_fat_contentlf        -0.0021983314    0.003768557  -0.009790094
   -0.016789483
   item_fat_contentlow fat   -0.0019042710    0.001866905  -0.003066415
   -0.018396143
   item_fat_contentlow fat    0.0027936467   -0.002234328   0.028309811
   0.056822747
   item_fat_contentreg        0.0002936319    0.001120931   0.009033254
   -0.001026615

   3. in order to compute the principal component score vector, we don   t
   need to multiply the loading with data. rather, the matrix x has the
   principal component score vectors in a 8523    44 dimension.

   > dim(prin_comp$x)
   [1] 8523    44

   let   s plot the resultant principal components.

   > biplot(prin_comp, scale = 0)

   4

   the parameter scale = 0 ensures that arrows are scaled to represent the
   loadings. to make id136 from image above, focus on the extreme ends
   (top, bottom, left, right) of this graph.

   we infer than first principal component corresponds to a measure of
   outlet_typesupermarket, outlet_establishment_year 2007. similarly, it
   can be said that the second component corresponds to a measure of
   outlet_location_typetier1, outlet_sizeother. for exact measure of a
   variable in a component, you should look at rotation matrix(above)
   again.

   4. the prcomp() function also provides the facility to compute standard
   deviation of each principal component. sdev refers to the standard
   deviation of principal components.

   #compute standard deviation of each principal component
   > std_dev <- prin_comp$sdev

   #compute variance
   > pr_var <- std_dev^2

   #check variance of first 10 components
   > pr_var[1:10]
   [1] 4.563615 3.217702 2.744726 2.541091 2.198152 2.015320 1.932076
   1.256831
   [9] 1.203791 1.168101

   we aim to find the components which explain the maximum variance. this
   is because, we want to retain as much information as possible using
   these components. so, higher is the explained variance, higher will be
   the information contained in those components.

   to compute the proportion of variance explained by each component, we
   simply divide the variance by sum of total variance. this results in:

   #proportion of variance explained
   > prop_varex <- pr_var/sum(pr_var)
   > prop_varex[1:20]
   [1] 0.10371853 0.07312958 0.06238014 0.05775207 0.04995800 0.04580274
   [7] 0.04391081 0.02856433 0.02735888 0.02654774 0.02559876 0.02556797
   [13] 0.02549516 0.02508831 0.02493932 0.02490938 0.02468313 0.02446016
   [19] 0.02390367 0.02371118

   this shows that first principal component explains 10.3% variance.
   second component explains 7.3% variance. third component explains 6.2%
   variance and so on. so, how do we decide how many components should we
   select for modeling stage ?

   the answer to this question is provided by a scree plot. a scree plot
   is used to access components or factors which explains the most of
   variability in the data. it represents values in descending order.

   #scree plot
   > plot(prop_varex, xlab = "principal component",
                ylab = "proportion of variance explained",
                type = "b")

   scree plot principal component analysis

   the plot above shows that ~ 30 components explains around 98.4%
   variance in the data set. in order words, using pca we have reduced 44
   predictors to 30 without compromising on explained variance. this is
   the power of pca> let   s do a confirmation check, by plotting a
   cumulative variance plot. this will give us a clear picture of number
   of components.

   #cumulative scree plot
   > plot(cumsum(prop_varex), xlab = "principal component",
                 ylab = "cumulative proportion of variance explained",
                 type = "b")

   8

   this plot shows that 30 components results in variance close to ~ 98%.
   therefore, in this case, we   ll select number of components as 30 [pc1
   to pc30] and proceed to the modeling stage. this completes the steps to
   implement pca on train data. for modeling, we   ll use these 30
   components as predictor variables and follow the normal procedures.


predictive modeling with pca components

   after we   ve calculated the principal components on training set, let   s
   now understand the process of predicting on test data using these
   components. the process is simple. just like we   ve obtained pca
   components on training set, we   ll get another bunch of components on
   testing set. finally, we train the model.

   but, few important points to understand:
    1. we should not combine the train and test set to obtain pca
       components of whole data at once. because, this would violate the
       entire assumption of generalization since test data would get
          leaked    into the training set. in other words, the test data set
       would no longer remain    unseen   . eventually, this will hammer
       down the generalization capability of the model.
    2. we should not perform pca on test and train data sets separately.
       because, the resultant vectors from train and test pcas will have
       different directions ( due to unequal variance). due to this, we   ll
       end up comparing data registered on different axes. therefore, the
       resulting vectors from train and test data should have same axes.

   so, what should we do?

   we should do exactly the same transformation to the test set as we did
   to training set, including the center and scaling feature. let   s do it
   in r:

   #add a training set with principal components
   > train.data <- data.frame(item_outlet_sales = train$item_outlet_sales,
   prin_comp$x)

   #we are interested in first 30 pcas
   > train.data <- train.data[,1:31]

   #run a decision tree
   > install.packages("rpart")
   > library(rpart)
   > rpart.model <- rpart(item_outlet_sales ~ .,data = train.data, method
   = "anova")
   > rpart.model

   #transform test into pca
   > test.data <- predict(prin_comp, newdata = pca.test)
   > test.data <- as.data.frame(test.data)

   #select the first 30 components
   > test.data <- test.data[,1:30]

   #make prediction on test data
   > rpart.prediction <- predict(rpart.model, test.data)

   #for fun, finally check your score of leaderboard
   > sample <- read.csv("samplesubmission_tmno39y.csv")
   > final.sub <- data.frame(item_identifier = sample$item_identifier,
   outlet_identifier = sample$outlet_identifier, item_outlet_sales =
   rpart.prediction)
   > write.csv(final.sub, "pca.csv",row.names = f)

   that   s the complete modeling process after pca extraction. i   m sure you
   wouldn   t be happy with your leaderboard rank after you upload the
   solution. try using id79!


   for python users: to implement pca in python, simply import pca from
   sklearn library. the interpretation remains same as explained for r
   users above. ofcourse, the result is some as derived after using r. the
   data set used for python is a cleaned version where missing values have
   been imputed, and categorical variables are converted into numeric. the
   modeling process remains same, as explained for r users above.

   import numpy as np
   from sklearn.decomposition import pca
   import pandas as pd
   import matplotlib.pyplot as plt
   from sklearn.preprocessing import scale
   %matplotlib inline

   #load data set
   data = pd.read_csv('big_mart_pca.csv')

   #convert it to numpy arrays
   x=data.values

   #scaling the values
   x = scale(x)

   pca = pca(n_components=44)

   pca.fit(x)

   #the amount of variance that each pc explains
   var= pca.explained_variance_ratio_

   #cumulative variance explains
   var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)

   print var1
   [  10.37   17.68   23.92   29.7    34.7    39.28   43.67   46.53
   49.27
   51.92   54.48   57.04   59.59   62.1    64.59   67.08   69.55   72.
   74.39   76.76   79.1    81.44   83.77   86.06   88.33   90.59   92.7
   94.76   96.78   98.44  100.01  100.01  100.01  100.01  100.01  100.01
   100.01  100.01  100.01  100.01  100.01  100.01  100.01  100.01]

   plt.plot(var1)

   9

   #looking at above plot i'm taking 30 variables
   pca = pca(n_components=30)
   pca.fit(x)
   x1=pca.fit_transform(x)

   print x1

   for more information on pca in python, visit [104]scikit learn
   documentation.


points to remember

    1. pca is used to overcome features redundancy in a data set.
    2. these features are low dimensional in nature.
    3. these features a.k.a components are a resultant of normalized
       linear combination of original predictor variables.
    4. these components aim to capture as much information as possible
       with high explained variance.
    5. the first component has the highest variance followed by second,
       third and so on.
    6. the components must be uncorrelated (remember orthogonal direction
       ? ). see above.
    7. normalizing data becomes extremely important when the predictors
       are measured in different units.
    8. pca works best on data set having 3 or higher dimensions. because,
       with higher dimensions, it becomes increasingly difficult to make
       interpretations from the resultant cloud of data.
    9. pca is applied on a data set with numeric variables.
   10. pca is a tool which helps to produce better visualizations of high
       dimensional data.


end notes

   this brings me to the end of this tutorial. without delving deep into
   mathematics, i   ve tried to make you familiar with most important
   concepts required to use this technique. it   s simple but needs special
   attention while deciding the number of components.  practically, we
   should strive to retain only first few k components

   the idea behind pca is to construct some principal components( z << xp
   ) which satisfactorily explains most of the variability in the data, as
   well as relationship with the response variable.

   did you like reading this article ? did you understand this technique ?
   do share your suggestions / opinions in the comments section below.

you can test your skills and knowledge. check out [105]live competitions and
compete with best data scientists from all over the world.

   you can also read this article on analytics vidhya's android app
   [106]get it on google play

share this:

     * [107]click to share on linkedin (opens in new window)
     * [108]click to share on facebook (opens in new window)
     * [109]click to share on twitter (opens in new window)
     * [110]click to share on pocket (opens in new window)
     * [111]click to share on reddit (opens in new window)
     *

related articles

   [ins: :ins]

   tags : [112]explained variance, [113]factor analysis, [114]first
   components, [115]id172, [116]pca in python, [117]pca in r,
   [118]principal component analysis, [119]scree plot, [120]statistics
   next article

course review     big data and hadoop developer certification course by
simplilearn

   previous article

winning solutions of dyd competition     r and xgboost ruled

[121]analytics vidhya content team

   analytics vidhya content team

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [122]discussion portal to get your queries resolved

65 comments

     * [123]tuhin chattopadhyay says:
       [124]march 21, 2016 at 6:19 am
       excellent manish
       [125]reply
          + analytics vidhya content team says:
            [126]march 21, 2016 at 6:37 am
            your appreciation means a lot.
            thank you tuhin sir     
            [127]reply
     * surobhi says:
       [128]march 21, 2016 at 7:24 am
       hi manish,
       information given about pca in your article was very comprehensive
       as you have covered both the theoretical and the implementation
       part very well. it was fun and simple to understand too. can you
       please write a similar one for factor analysis? how is it different
       from pca and how to decide on the method of dimensional reduction
       case to case. thanks
       [129]reply
          + analytics vidhya content team says:
            [130]march 21, 2016 at 10:39 am
            thanks surobhi !
            i already have it in my plan to write soon one detailed post
            on factor analysis. wish me luck!
            [131]reply
               o basma alkerm says:
                 [132]july 29, 2016 at 4:26 pm
                 looking forward to read it     
                 [133]reply
     * prasoon saxena says:
       [134]march 21, 2016 at 7:32 am
       this is good explanation manish and thank you for sharing it.
       quick question, model created using these 30pca will have all 50
       independent variable but if i want to figure out what among those
       50 independent variables which are most critical one then how we
       figure that so that we can build model using those specific
       variables.
       will appreciate your help.
       thanks
       [135]reply
          + analytics vidhya content team says:
            [136]march 21, 2016 at 10:47 am
            hello
            for model building, we   ll use the resultant 30 components as
            independent variables. remember, each component is a vector
            comprising of principal component score derived from each
            predictor variable (in this case we have 50). check
            prin_comp$rotation for principal component scores in each
            vector. this technique is used to shrink the dimension of a
            data set such that it becomes easier to analyze, visualize and
            interpret.
            by    critical   , i assume you are talking about measuring
            variable importance. if that   s the case, you can look for p
            values, t statistics in regression. for variable selection,
            regression is equipped with various approaches such as forward
            selection, backward selection, step wise selection etc.
            [137]reply
               o muthupandi says:
                 [138]march 21, 2016 at 12:45 pm
                 hi manish
                 thanks for the article, had good explanation and walk
                 through. in pca on r u have shown hot to get the name of
                 the variable and its missing in python, can u explain how
                 i can get the names of the variables after reduction so
                 that we can use it for model building? or am i
                 understanding it wrongly
                 [139]reply
                    # analytics vidhya content team says:
                      [140]march 21, 2016 at 4:31 pm
                      hi
                      i didn   t understand when you say    name of the
                      variable   . did you mean rotation matrix ? please
                      elaborate on that.
                      secondly, once you have done pca, you don   t need to
                      use variables for modeling, instead use the
                      resultant combination of components as independent
                      variables which leads to highest explained variance.
                      [141]reply
                         @ muthupandi says:
                           [142]march 22, 2016 at 5:24 am
                           variable name am referring to is
                           item_weight,item_fat_contentlf
                           item_fat_contentlow fat item_fat_contentlow
                           fat,item_fat_contentreg etc.,
                           so we have to use x1 to train the model?
                           [143]reply
     * [144]hunaidkhan says:
       [145]march 21, 2016 at 8:40 am
       really informative manish, also variables derived from pca can be
       used for regression analysis. regression analysis with pca gives a
       better prediction and less error.
       [146]reply
          + analytics vidhya content team says:
            [147]march 21, 2016 at 11:00 am
            rightly said. pca when used for regression takes a form of
            supervised approach known as pls (partial least squares). in
            pls, the response variable is used for identification of
            principal components.
            [148]reply
          + amit srivastava says:
            [149]august 16, 2016 at 5:41 pm
            agreed. pca with lm has smaller rmse as compared to pca with
            randomforest
            [150]reply
     * debarshi says:
       [151]march 21, 2016 at 8:46 am
       i have used pca recently in one projects, and would like to add few
       points:
       -pca reduce the dimension but the the result is not very intuitive,
       as each pcs are combination of all the original variables. so use
          factor analysis    (factor rotation) on top of pca to get a better
       relationship between pcs (rather factors) and original variable,
       this result was brilliant in an insurance data.
       -if you have perfectly correlated variables (a & b) then also pca
       will not suggest you to drop one, rather it will suggest to use a
       combination of these two (a+b), but off course it will reduce the
       dimension
       -this is different from feature selection, don   t mix these two
       concept
       -there is a concept of    nonlinear pca    which helps to include non
       numeric values as well.
       -if you want to reduce the dimension (or numbers) of predictors (x)
       remember pca does not consider response (y) while reducing the
       dimension, your original variables may be (??) a better predictors.
       [152]reply
          + analytics vidhya content team says:
            [153]march 21, 2016 at 11:02 am
            thanks a lot debarshi. these are so much helpful.
            [154]reply
          + manisha says:
            [155]april 7, 2016 at 2:44 am
            hi debarshi,
            can you shed some light on factor rotation? i have considered
            pca or simple correlation matrix approach to identify
            correlation among variables. then at regression stage i have
            used vif.
            [156]reply
               o debarshi says:
                 [157]april 14, 2016 at 11:40 am
                 sadly all my analysis is in company sensitive data so
                 cant share it here, but you can read the following
                 articles:
                 [158]http://ftp.utdallas.edu/~herve/abdi-rotations-pretty
                 .pdf
                 [159]http://pareonline.net/getvn.asp?v=20&n=2
                 [160]http://www.tqmp.org/content/vol09-2/p079/p079.pdf
                 these are very useful
                 .
                 i did that in sas, but it   s possible to do that in r as
                 well.
                     i have considered pca or simple correlation matrix
                 approach to identify correlation among variables       
                 correlation matrix gives you the pair wise correlation,
                 if there are linear dependencies between three or more
                 factors, you cant trace that in correlation matrix, and
                 that   s why pca is so useful.
                 [161]reply
     * venu says:
       [162]march 21, 2016 at 8:49 am
       good one
       [163]reply
     * pallavi says:
       [164]march 21, 2016 at 10:42 am
       hi manish,
       another good article! i have always found it difficult to explain
       the principle components to business users. would really
       appreciate, if you also write how do you explain the pca to
       business users    what general questions you get from business users
       and how to handle those.
       thanks
       [165]reply
     * [166]dox vk says:
       [167]march 21, 2016 at 12:17 pm
       i understand there is a pca for qualitative data     could some one
       provide me with a good intutive resource for suvh?
       [168]reply
     * krishna says:
       [169]march 21, 2016 at 1:17 pm
       hi manish,
       the article is very helpful. while we normalize the data for
       numeric variables, do we need to remove outliers if any exists in
       the data before performing pca?
       also looks like , implementation of final model in production is
       quite tedious, as we always have to compute components prior
       scoring.
       thanks,
       krishna
       [170]reply
          + analytics vidhya content team says:
            [171]march 21, 2016 at 4:36 pm
            hello
            also mentioned in the article, data cleaning (removal of
            outliers, imputing missing values) are important prior to
            implementing principal component analysis. such things only
            adds noise and inconsistency in the data. hence, it is a good
            practice to sort them out first.
            i beg to differ on this procedure being    tedious   . for the
            sake of understanding, i   ve explained each and every step used
            in this technique, may be this makes it tedious. however, if
            you think you have understood it correctly, just pick the
            important commands (codes) and get to the scree plot stage in
            no time.
            [172]reply
     * sandy says:
       [173]march 21, 2016 at 3:48 pm
       nice one explanation
       [174]reply
     * ankur says:
       [175]march 22, 2016 at 8:58 am
       hi manish,
       while running the command >prin_comp <- prcomp(new_my_data, scale.
       = t)
       it giving error "error in svd(x, nu = 0) : infinite or missing
       values in 'x'"
       how to rectify it   .
       btw a great article   ..
       [176]reply
          + analytics vidhya content team says:
            [177]march 22, 2016 at 12:52 pm
            hello
            this error says    your data set has missing values. please
            impute   .
            to rectify, run the code once again where i dealt with missing
            values. good luck!
            [178]reply
     * anupam basu says:
       [179]march 29, 2016 at 6:01 pm
       hi manish,
       great article. i am new to r & this provides a very clear
       implementation obviously. i just had one quick question though. the
       30 components that we will be using for further analysis which data
       frame is that stored in? if not stored (for the purpose of this
       illustration) how can i create a data frame containing the 30
       components & their scores that we can use further?
       thanks again!
       [180]reply
     * patrick hagan says:
       [181]march 30, 2016 at 8:50 pm
       hello, very good article, but there seems to be a typo at the end
       of this line:    for python users: to implement pca in python, simply
       import pca from sklearn library. the interpretation remains same as
       explained for r users above. ofcourse,        ofcourse    should be    of
       course   .
       [182]reply
     * ravi adannavar says:
       [183]april 1, 2016 at 1:54 pm
       good article
       [184]reply
     * thanish batcha says:
       [185]april 4, 2016 at 9:16 am
       hi manish, first of all your article is super cool for real. but
       every single tutorial about pca talks about only extracting the
       important features from the data frame. no where i have come across
       they are talking about how we build a model with the extracted
       important pca components. since i am new to r i would love to see
       you explain it in r .
       consider that i am handling a classification problem
       data frame called train that has columns var1, var2, var3         var19 ,
       output
       the output column is the classifier(the one i want to predict in my
       test dataset) with features var1    var19
       here are my questions
       i remove the output variable and apply prcomp to the remaining
       dataset(new_dataset)
       how do i merge the output variable to the pca components ?
       consider am trying to use simple id28
       logmodel = glm(output~. data= new_dataset)
       predict (logmodel, newdata= testdata)
       is this correct ?
       should i apply the pca to the test data too ?
       [186]reply
          + rahul agarwal says:
            [187]may 12, 2016 at 2:45 am
            hello thanish,
            in my understanding, you combine the training and testing data
            to eliminate the missing values and initial operations. then
            this combined data frame is used to generate the pca
            components. each row of this pca component refers to the
            corresponding output value (total number of rows being equal
            to number of rows of training data + number of rows of testing
            data). so while building the model all you can do is split the
            data frame in training and testing (by simply using subset
            function).
            the reason for the ith value of any pca component correspond
            the ith value of output is because different principal
            component loadings are multiplied with the ith value of
            original variables.
            [188]reply
          + prashant roy says:
            [189]june 20, 2016 at 11:25 am
            hi,
            i am also not able to find any help on using these extracted
            principal components to build a predictive model.
            [190]reply
     * manisha says:
       [191]april 7, 2016 at 2:51 am
       hi manish,
       thanks for the informative article. i have used pca in sas during
       scorecard development and it suggested to drop way too many
       variables than what i would have preferred to (i prefer to keep a
       few vars from each var category atleast to start with). even after
       adjusting the eigen value threshold the number of vars being
       sacrificed was a lot. so i ended up using a simple correlation
       matrix approach which selects and retains highest iv variable from
       a group of correlated vars based on the correlation matrix with a
       80% or 70% correlation threshold. then at the regression stage i
       used vif option to capture multi collinearity.
       [192]reply
     * sumanta says:
       [193]april 10, 2016 at 2:29 pm
       very nice article and quite informative. thanks a lot for making us
       aware of variable reduction technique. it   ll be very good if you
       can further show how these 30 components can be used for modelling?
       an example will be very good to know.
       [194]reply
     * [195][email protected] says:
       [196]april 11, 2016 at 12:26 pm
       hi manish can please also explain me how do you use those
       components to create a model and then predict. i would love to see
       the code for building the model and prediction in r. because every
       tutorial i see they explain only till the point of extracting the
       components and nobody proceeds further, that is were i am struck.
       kindly help me with that.
       [197]reply
     * prashant sharma says:
       [198]april 19, 2016 at 6:46 am
       hello manish, this is really great article. i learned a lot from
       this article. can you please write a article on selection of
       logistic vs id90 vs id112 vs id166 for a given
       dataset?how to select which method is good for certain kind of
       dataset?
       [199]reply
     * leon kalmakrian says:
       [200]april 25, 2016 at 2:12 pm
       i never usually respond to blog posts or articles but i feel
       sufficiently impressed (and grateful!) to do so here. thank you so
       much for a well structured breakdown of pca, taking the reader
       through, step by step, the technique used and the underlying
       rationale.
       [201]reply
     * pchavan says:
       [202]april 25, 2016 at 4:33 pm
       is it ok if i less 10 pcas in stead of 44 as an o/p?
       [203]reply
     * [204]d. l. von kleeck says:
       [205]april 27, 2016 at 3:57 pm
       hi manish, doc vk here. i love your article, but have one question.
       in the python for pc analysis you used a
       clean data, where missing values have been imputed, and categorical
       variables are converted into numeric. does python contain libraries
       similar to the ones used in r? fie example/ what would be the
       python code similar to the r library    dummies   ?     i would
       appreciate seeing the python code similar to the r code. thanks!
       [206]reply
     * nikhil thakur says:
       [207]may 5, 2016 at 6:40 pm
       nice post, when will you publish the post on factor analysis?
       [208]reply
     * m says:
       [209]may 13, 2016 at 12:07 am
       hi,
       thanks for this article. i have a question. i have 50 observations
       (10x5groups) of 231 variables and i   d like to use pca with r in
       order to select the best variables. the problem is that
          prcomp(mydata)    yields 50 components. thus, if i understood, it
       will allow me remove some observations    but i need to select
       variables to model all my observations.
       [210]reply
     * rehana mahfuz says:
       [211]june 24, 2016 at 4:00 pm
       in the part where you use r, in the last paragraph of number 3, i
       don   t understand how we can infer from the figure what the first
       and second principal components correspond to.
       i would appreciate any explanation. thank you.
       [212]reply
     * siva says:
       [213]july 5, 2016 at 9:47 am
       article is very informative.thank you manish.
       [214]reply
     * norman says:
       [215]july 27, 2016 at 12:45 am
       hi manish,
       a great article. i have few questions.
       1 how do we find features that contribute for pc1 to pc30?
       2 do you have the article for modelling stage?
       3 how do we validate the model in pca?
       thanks
       [216]reply
          + analytics vidhya content team says:
            [217]july 28, 2016 at 7:42 am
            hi norman
            1. you can decide on pc1 to pc30 by looking at the cumulative
            variance bar plot. basically, this plot says how many
            component combined can explain variance in the data. if you
            see carefully, after pc30, the line saturates and adding any
            further component doesn   t help in more explained variance.
            2. just added today.
            3. for validation, divide the training set into n parts. run
            pca on one part. then, apply this resultant pca on other parts
            and finally make predictions (as explained above)
            [218]reply
     * james says:
       [219]july 28, 2016 at 7:09 am
       hi
       i refer to your statement :
       if the two components are uncorrelated, their directions should be
       orthogonal (image below).
       can i said that :
       to be a    valid    predictors, does it mean there must be no
       co-relational directional arrow pointing ? in another words, the
       independent predictors must not arrow in the same direction ? what
       if 2 components arrow in a pictures goes in the same directions ?
       [220]reply
     * james says:
       [221]july 28, 2016 at 8:37 am
       biplot(prin_comp, scale = 0)
       the black smudges on the graphics     is it a indication that these
       are the predictors that contribute to the data variance ?
       [222]reply
     * [223]gonzalo moreno says:
       [224]july 28, 2016 at 8:55 pm
       regards from colombia. great tutorial!!! very well explained.
       congratulations
       [225]reply
     * sanchit says:
       [226]july 29, 2016 at 10:09 am
       hi i have one doubt.
       after predicting the item_outlet_sales if i want to know which
       original predictors contributes most towards the target variable
       how i can find this ?? because now all the predictors are converted
       into principal components . please tell me a way to find out the
       relative importance of all predictor variable after reducing dthe
       dimension of data using pca.
       [227]reply
          + analytics vidhya content team says:
            [228]july 30, 2016 at 12:04 pm
            hi sanchit
            see biplot, it would help you to figure out which variables
            contribute to which component.
            [229]reply
               o sanchit says:
                 [230]august 1, 2016 at 6:01 am
                 thanks ! for replying .
                 [231]reply
     * mithilesh singh says:
       [232]july 30, 2016 at 5:33 am
       hi manish
       i applied linear reg on same dataset big mart sales with pcs as ind
       variables. however my r2 reduced drastically compare to reg using
       original ind variables. any idea what went wrong.
       regards
       mithilesh
       [233]reply
          + analytics vidhya content team says:
            [234]july 30, 2016 at 11:54 am
            hi mithilesh
            pca works best when we   ve several numeric features. in this
            data set, since majority of the variables are categorical, i
            converted those categorical variables into numeric using one
            hot encoding. for a id75, this approach doesn   t
            work since encoded variables might add to non-linearity in the
            data. therefore, your regression model on pca components is
            giving poor results. to summarize, pca also has limitations.
            it wouldn   t work well in all situations.
            if you really want to leverage its power, download data from
            numer.ai website, you   ll enjoy it.
            [235]reply
               o mithilesh singh says:
                 [236]july 30, 2016 at 1:26 pm
                 thanks manish. also i missed to add the point that your
                 article was well explained. really appreciate your
                 effort. also wanted to know that prediction using pcs can
                 be better than original variables or it is just a
                 technique to reduce the dimensionality.
                 [237]reply
     * olumide michael oyalola says:
       [238]august 1, 2016 at 10:20 am
       hi manish,
       many thanks for this detailed work on pca. greetings from nigeria
       [239]reply
     * barsa nayak says:
       [240]august 2, 2016 at 6:03 am
       hi!
       i always enjoy your articles. got a query. in the statement    in
       general, for n    p dimensional data, min(n-1, p) principal
       component can be constructed.    do u mean maximum here? if not can
       you please explain why it is min(n-1,p)?
       [241]reply
     * aditya jain says:
       [242]august 3, 2016 at 10:17 am
       beautifully explained manish. really liked the part where you
       clarified on how to do it on test data,
       [243]reply
     * ramasubramanian says:
       [244]august 3, 2016 at 11:24 am
       superb manish. what a command (over both statistics and r!).
       [245]reply
     * dr s.s.senapati says:
       [246]august 3, 2016 at 2:09 pm
       excellent . this is at par with some of the best online courses of
       us universities. very well explained in the most simple way.
       waiting for your article in feature selection in r and once again
       xgboost.
       [247]reply
     * faiza says:
       [248]august 5, 2016 at 2:50 pm
       kindly tell me how to find out the percentage of variance
       expreienced by each principal component?any command.i m using r for
       my analysis
       [249]reply
     * vij says:
       [250]august 8, 2016 at 8:24 am
       absolutely. in a data set, the maximum number of principal
       component loadings is a minimum of (n-1, p).
       why is this?
       [251]reply
     * rajen choudhari says:
       [252]august 19, 2016 at 10:10 pm
       nice article manish.     
       [253]reply
     * priyanka gupta says:
       [254]september 19, 2016 at 10:34 am
       hey, the variable    item_fat_content    has different levels but i
       think 3 of them are just the same: lf, low fat & low fat.. the
       table that is posted in the article (post this command:
       prin_comp$rotation[1:5,1:4] ) has all 3 of them too against the
       principal components. so my doubt is , don   t we need to club all
       those categories in to one? sorry, v silly question but really new
       to pca so thought should clear it out.
       another question: i wanted to have a look at the correlation matrix
       but the cor(dataframe, method=      ) approach doesn   t give a good
       graph (could be because of factor variables or due to high
       dimensionality of the data frame). so, what can i do to see the
       correlation graph/numbers or just plotting the principal components
       is enough?
       will be glad to receive any help on this. thanks
       [255]reply
     * amit srivastava says:
       [256]september 19, 2016 at 2:58 pm
       wanted to understand why you have calculated the std deviation and
       variances as these are already provided by summary(prin_comp).
       similarily why did you write separate code for plotting the
       screeplot, when again you could have used plot(prin_comp,
       type=   lines   ) or the screeplot() function
       [257]reply
     * vivek says:
       [258]may 12, 2017 at 5:53 am
       this is excellent explanations!!! thank you so much for your help!
       [259]reply
     * dina says:
       [260]march 5, 2018 at 4:03 am
       hi manish,
       thanks for the great article. i have a question about applying the
       modeling part in python. how do we apply on test pca and scaling on
       test data?
       [261]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [262]srk       3924
   2    [2.jpg?date=2019-04-06] [263]mark12    3510
   3    [3.jpg?date=2019-04-06] [264]nilabha   3261
   4    [4.jpg?date=2019-04-06] [265]nitish007 3237
   5    [5.jpg?date=2019-04-06] [266]tezdhar   3082
   [267]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [268]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [269]understanding support vector machine algorithm from examples
       (along with code)
     * [270]essentials of machine learning algorithms (with python and r
       codes)
     * [271]a complete tutorial to learn data science with python from
       scratch
     * [272]7 types of regression techniques you should know!
     * [273]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [274]a simple introduction to anova (with applications in excel)
     * [275]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [276]top 5 machine learning github repositories and reddit discussions
   from march 2019

[277]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [278]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[279]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [280]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[281]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [282]16 opencv functions to start your id161 journey (with
   python code)

[283]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [284][ds-finhack.jpg]

   [285][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [286]about us
     * [287]our team
     * [288]career
     * [289]contact us
     * [290]write for us

   [291]about us
   [292]   
   [293]our team
   [294]   
   [295]careers
   [296]   
   [297]contact us

data scientists

     * [298]blog
     * [299]hackathon
     * [300]discussions
     * [301]apply jobs
     * [302]leaderboard

companies

     * [303]post jobs
     * [304]trainings
     * [305]hiring hackathons
     * [306]advertising
     * [307]reach us

   don't have an account? [308]sign up here.

join our community :

   [309]46336 [310]followers
   [311]20222 [312]followers
   [313]followers
   [314]7513 [315]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [316]privacy policy
     * [317]terms of use
     * [318]refund policy

   don't have an account? [319]sign up here

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [320](button) join now

   subscribe!

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [321](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/python-2/
  94. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  95. https://www.analyticsvidhya.com/blog/category/python-2/
  96. https://www.analyticsvidhya.com/blog/category/r/
  97. https://www.analyticsvidhya.com/blog/author/avcontentteam/
  98. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=practicalguideprincipalcomponentanalysisarticle
  99. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=practicalguideprincipalcomponentanalysisarticle
 100. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=practicalguideprincipalcomponentanalysisarticle
 101. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2016/03/practical-guide-to-principal-component-analysis-pca-in-r-python.png
 102. http://www.nlpca.org/pca_principal_component_analysis.html
 103. http://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii
 104. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.pca.html
 105. http://datahack.analyticsvidhya.com/contest/all
 106. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 107. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/?share=linkedin
 108. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/?share=facebook
 109. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/?share=twitter
 110. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/?share=pocket
 111. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/?share=reddit
 112. https://www.analyticsvidhya.com/blog/tag/explained-variance/
 113. https://www.analyticsvidhya.com/blog/tag/factor-analysis/
 114. https://www.analyticsvidhya.com/blog/tag/first-components/
 115. https://www.analyticsvidhya.com/blog/tag/id172/
 116. https://www.analyticsvidhya.com/blog/tag/pca-in-python/
 117. https://www.analyticsvidhya.com/blog/tag/pca-in-r/
 118. https://www.analyticsvidhya.com/blog/tag/principal-component-analysis/
 119. https://www.analyticsvidhya.com/blog/tag/scree-plot/
 120. https://www.analyticsvidhya.com/blog/tag/statistics/
 121. https://www.analyticsvidhya.com/blog/author/avcontentteam/
 122. https://discuss.analyticsvidhya.com/
 123. http://www.tuhinchattopadhyay.com/
 124. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107888
 125. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107888
 126. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107890
 127. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107890
 128. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107894
 129. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107894
 130. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107909
 131. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107909
 132. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114262
 133. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114262
 134. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107897
 135. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107897
 136. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107912
 137. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107912
 138. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107922
 139. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107922
 140. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107951
 141. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107951
 142. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107981
 143. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107981
 144. http://none/
 145. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107901
 146. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107901
 147. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107914
 148. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107914
 149. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114879
 150. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114879
 151. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107902
 152. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107902
 153. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107915
 154. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107915
 155. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109090
 156. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109090
 157. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109459
 158. http://ftp.utdallas.edu/~herve/abdi-rotations-pretty.pdf
 159. http://pareonline.net/getvn.asp?v=20&n=2
 160. http://www.tqmp.org/content/vol09-2/p079/p079.pdf
 161. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109459
 162. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107903
 163. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107903
 164. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107910
 165. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107910
 166. http://none/
 167. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107920
 168. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107920
 169. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107925
 170. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107925
 171. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107952
 172. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107952
 173. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107945
 174. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107945
 175. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107989
 176. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107989
 177. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107996
 178. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-107996
 179. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108593
 180. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108593
 181. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108685
 182. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108685
 183. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108824
 184. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108824
 185. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108918
 186. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-108918
 187. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110851
 188. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110851
 189. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-112436
 190. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-112436
 191. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109091
 192. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109091
 193. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109259
 194. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109259
 195. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection
 196. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109289
 197. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109289
 198. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109668
 199. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109668
 200. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109995
 201. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-109995
 202. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110003
 203. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110003
 204. http://none/
 205. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110104
 206. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110104
 207. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110491
 208. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110491
 209. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110914
 210. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-110914
 211. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-112650
 212. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-112650
 213. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-113113
 214. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-113113
 215. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114086
 216. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114086
 217. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114177
 218. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114177
 219. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114176
 220. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114176
 221. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114178
 222. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114178
 223. https://co.linkedin.com/in/gonzalomoreno25
 224. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114218
 225. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114218
 226. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114245
 227. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114245
 228. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114280
 229. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114280
 230. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114319
 231. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114319
 232. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114274
 233. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114274
 234. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114279
 235. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114279
 236. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114282
 237. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114282
 238. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114327
 239. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114327
 240. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114354
 241. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114354
 242. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114398
 243. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114398
 244. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114402
 245. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114402
 246. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114406
 247. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114406
 248. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114482
 249. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114482
 250. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114559
 251. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114559
 252. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114965
 253. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-114965
 254. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-116201
 255. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-116201
 256. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-116215
 257. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-116215
 258. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-128481
 259. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-128481
 260. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-151701
 261. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/#comment-151701
 262. https://datahack.analyticsvidhya.com/user/profile/srk
 263. https://datahack.analyticsvidhya.com/user/profile/mark12
 264. https://datahack.analyticsvidhya.com/user/profile/nilabha
 265. https://datahack.analyticsvidhya.com/user/profile/nitish007
 266. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 267. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 268. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 269. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 270. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 271. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 272. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 273. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 274. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 275. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 276. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 277. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 278. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 279. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 280. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 281. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 282. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 283. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 284. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 285. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 286. http://www.analyticsvidhya.com/about-me/
 287. https://www.analyticsvidhya.com/about-me/team/
 288. https://www.analyticsvidhya.com/career-analytics-vidhya/
 289. https://www.analyticsvidhya.com/contact/
 290. https://www.analyticsvidhya.com/about-me/write/
 291. http://www.analyticsvidhya.com/about-me/
 292. https://www.analyticsvidhya.com/about-me/team/
 293. https://www.analyticsvidhya.com/about-me/team/
 294. https://www.analyticsvidhya.com/about-me/team/
 295. https://www.analyticsvidhya.com/career-analytics-vidhya/
 296. https://www.analyticsvidhya.com/about-me/team/
 297. https://www.analyticsvidhya.com/contact/
 298. https://www.analyticsvidhya.com/blog
 299. https://datahack.analyticsvidhya.com/
 300. https://discuss.analyticsvidhya.com/
 301. https://www.analyticsvidhya.com/jobs/
 302. https://datahack.analyticsvidhya.com/users/
 303. https://www.analyticsvidhya.com/corporate/
 304. https://trainings.analyticsvidhya.com/
 305. https://datahack.analyticsvidhya.com/
 306. https://www.analyticsvidhya.com/contact/
 307. https://www.analyticsvidhya.com/contact/
 308. https://datahack.analyticsvidhya.com/signup/
 309. https://www.facebook.com/analyticsvidhya/
 310. https://www.facebook.com/analyticsvidhya/
 311. https://twitter.com/analyticsvidhya
 312. https://twitter.com/analyticsvidhya
 313. https://plus.google.com/+analyticsvidhya
 314. https://in.linkedin.com/company/analytics-vidhya
 315. https://in.linkedin.com/company/analytics-vidhya
 316. https://www.analyticsvidhya.com/privacy-policy/
 317. https://www.analyticsvidhya.com/terms/
 318. https://www.analyticsvidhya.com/refund-policy/
 319. https://id.analyticsvidhya.com/accounts/signup/
 320. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 321. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 323. https://www.facebook.com/analyticsvidhya
 324. https://twitter.com/analyticsvidhya
 325. https://plus.google.com/+analyticsvidhya/posts
 326. https://in.linkedin.com/company/analytics-vidhya
 327. https://www.analyticsvidhya.com/blog/2016/03/review-big-data-hadoop-developer-certification-simplilearn/
 328. https://www.analyticsvidhya.com/blog/2016/03/winning-solutions-dyd-competition-xgboost-ruled/
 329. https://www.analyticsvidhya.com/blog/author/avcontentteam/
 330. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 331. https://www.facebook.com/analyticsvidhya/
 332. https://twitter.com/analyticsvidhya
 333. https://plus.google.com/+analyticsvidhya
 334. https://plus.google.com/+analyticsvidhya
 335. https://in.linkedin.com/company/analytics-vidhya
 336. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 337. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 338. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 339. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 340. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 341. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 342. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 343. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 344. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 345. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 346. javascript:void(0);
 347. javascript:void(0);
 348. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 349. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 350. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 351. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 352. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 353. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 354. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 355. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 356. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 357. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fpractical-guide-principal-component-analysis-python%2f&linkname=practical%20guide%20to%20principal%20component%20analysis%20%28pca%29%20in%20r%20%26amp%3b%20python
 358. javascript:void(0);
 359. javascript:void(0);
