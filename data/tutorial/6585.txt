   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

building a translation system in minutes

using openid4-py to create baseline id4 models

   [16]go to the profile of ceshine lee
   [17]ceshine lee (button) blockedunblock (button) followfollowing
   oct 12, 2017

   sequence-to-sequence(id195)[1] is a versatile structure and capable
   of many things (language translation, text summarization[2], video
   captioning[3], etc.). for a short introduction to id195, here are
   some good posts: [18][4][19][5].

   sean robertson   s [20]tutorial notebook[6] and jeremy howard   s lectures
   [21][6][22][7] are great starting points to get a firm grasp on the
   technical details of id195. however, i   d try to avoid implementing
   all these details myself when dealing with real-world problems. it   s
   usually not a good idea to reinvent the wheel, especially when you   re
   very new to this field. i   ve found that openid4 project is very active,
   has good documentation, and can be used out-of-the-box:
   [23]openid4 - open-source id4
   openid4 is an industrial-strength, open-source (mit) neural machine
   translation system utilizing the torch/ pytorch   openid4.net

   there also are some more general frameworks [24](for example, [8]), but
   may need some customization to make it work on your specific problem.

   there are two official versions of openid4:

     [25]openid4-lua (a.k.a. openid4): the main project developed with
     [26]luatorch.
     optimized and stable code for production and large scale
     experiments.

     [27]openid4-py: light version of openid4 using [28]pytorch.
     initially created by the facebook ai research team as a sample
     project for pytorch, this version is easier to extend and is suited
     for research purpose but does not include all features.

   we   re going to use the pytorch version in the following sections. we
   will walk you through the steps needed to create a very basic
   translation system with a medium-sized dataset.

step 1    get openid4-py

   clone the openid4-py git repository on github into a local folder:
   [29]openid4/openid4-py
   openid4-py - open-source id4 in pytorch
   http://openid4.net/github.com

   you might want to fork the repository on github if you   re planning to
   customize or extend it later. also it is suggested in the readme:

     codebase is nearing a stable 0.1 version. we currently recommend
     forking if you want stable code.

step 2: download the dataset

   here we   re going to use the dataset from [30]ai
   challenger         english-chinese machine translation competition. it is a
   dataset with 10 million english-chinese sentence pairs. the english
   copora are conversational english extracted from english learning
   websites and movie subtitles. from my understanding, most of the
   translation are submitted by enthusiasts, not necessarily
   professionals. the translated chinese sentences are checked by human
   annotators.
   [31]english-chinese machine translation - ai challenger
   english-chinese machine translation - prize:  300,000 - improve the
   performance of english-chinese machine translation   challenger.ai

   downloading the dataset requires account sign-up and possibly id
   verification (can   t remember whether the latter is mandatory). if
   that   s a problem for you, you can try [32]datasets from wmt17.

   there are some problems to the ai challenger dataset: 1. the quality of
   the translation is not consistent. 2. because many of sentences are
   from movie subtitles, the translation are often context-dependent
   (related to the previous or the next sentence). however, there are no
   context information available in the dataset.

   let   s see how the out-of-the-box model perform on this dataset. because
   of memory restriction, i down-sampled the dataset to 1 million
   sentences.

      we   ll assume that you put the dataset into folder challenger under the
   openid4 root directory.   

step 3: convert dataset to plain texts

   the validation and test dataset comes in xml format. we need to convert
   it to plain text files where a line consists of a single sentence. a
   simple way to do that is using beautifulsoup. here   s a sample chunk of
   code:
with open(input_file, "r") as f:
    soup = beautifulsoup(f.read(), "lxml")
    lines = [
      (int(x["id"]), x.text.strip()) for x in soup.findall("seg")]
    # ensure the same order
    lines = sorted(lines, key=lambda x: x[0])

step 4: tokenize english and chinese sentences

   the input sentence must be tokenized with tokens space-separated.

   for english, there are a few tokenizers to choose from. one example is
   nltk.tokenize.word_tokenize :
with open(output_file, "w") as f:
    f.write(
        "\n".join([
             " ".join(word_tokenize(l[1]))
             for l in lines
         ])
    )

   it turns    it   s a neat one         two. walker to burton.    into    it    s a neat
   one         two . walker to burton .   .

   for chinese, we use the simplest character-level id121, that is,
   treat each character as a token:
with open(output_file, "w") as f:
    f.write(
        "\n".join([
            " ".join([c if c != " " else "<s>" for c in l[1]])
            for l in lines
        ])
    )

   it turns                24                                        into                    2 4                                            
         . (note because the token are space-separated, we need a special
   token    <s>    to represent the space characters.)

   (i didn   t provide full code for step 3 and step 4 because it   s really
   beginner-level python programming. you should be able to complete these
   tasks by yourself.)

step 5: preprocess the dataset

   simply run the following command in the root directory:
python preprocess.py -train_src challenger/train.en.sample \
      -train_tg challenger/train.zh.sample \
      -valid_src challenger/valid.en \
      -valid_tgt challenger/valid.zh  \
      -save_data challenger/openid4 -report_every 10000

   the preprocessing script will go through the dataset, keep track of
   token frequencies, and construct a vocabulary list. i ran into memory
   problem here and had to down-sample the training dataset to 1 million
   rows, but i think the raw dataset should fit into 16gb memory with some
   optimization.

step 6: train the model

python train.py -data challenger/openid4 \
    -save_model models/baseline -gpuid 0 \
    -learning_rate 0.001 -opt adam -epochs 20

   it   ll use your first gpu to train a model. the default model structure
   is:
id4model (
  (encoder): id56encoder (
    (embeddings): embeddings (
      (make_embedding): sequential (
        (emb_luts): elementwise (
          (0): embedding(50002, 500, padding_idx=1)
        )
      )
    )
    (id56): lstm(500, 500, num_layers=2, dropout=0.3)
  )
  (decoder): inputfeedid56decoder (
    (embeddings): embeddings (
      (make_embedding): sequential (
        (emb_luts): elementwise (
          (0): embedding(6370, 500, padding_idx=1)
        )
      )
    )
    (dropout): dropout (p = 0.3)
    (id56): stackedlstm (
      (dropout): dropout (p = 0.3)
      (layers): modulelist (
        (0): lstmcell(1000, 500)
        (1): lstmcell(500, 500)
      )
    )
    (attn): globalattention (
      (linear_in): linear (500 -> 500)
      (linear_out): linear (1000 -> 500)
      (sm): softmax ()
      (tanh): tanh ()
    )
  )
  (generator): sequential (
    (0): linear (500 -> 6370)
    (1): logsoftmax ()
  )
)

   the vocabulary size of source and target corpora is 50,002 and 6,370,
   respectively. the source vocabulary is obviously truncated to 50,000.
   the target vocabulary is relatively small because there are not that
   many common chinese characters.

step 7: translate test/validation sentences

python translate.py \
    -model models/baseline_acc_58.79_ppl_7.51_e14  \
    -src challenger/valid.en -tgt challenger/valid.zh \
    -output challenger/valid_pred.58.79 -gpu 0 -replace_unk

   replace models/baseline_acc_58.79_ppl_7.51_e14 with your own model. the
   model naming should be obvious: this is a model after 14 epochs of
   training, with 58.79 accuracy and 7.51 perplexity on validation set.

   you can also calculate id7 score with the following:
wget [33]https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts
/generic/multi-id7.perl
perl multi-id7.perl challenger/valid.zh \
     < challenger/valid_pred.58.79

   now you have a working translation system!

step 8: (optional) detokenize and convert the output

   if you want to submit the translation to ai challenger, you need to
   reverse step 4 and then step 3. again, they should be quite simple to
   implement.

some examples

   english: you knew it in your heart you haven   t washed your hair
   chinese(pred):                               
   chinese(gold):                                        

   english: i never dreamed that one of my own would be going off to a
   university, but here i stand,
   chinese(pred):                                                                            
   chinese(gold):                                                             

   english: we just don   t have time to waste on the wrong man.
   chinese(pred):                                  
   chinese(gold):                                        

   the above three examples are, from top to bottom, semantically correct,
   partially correct, and entirely incomprehensible. after examining a few
   examples, i found most of the machine translated sentences were
   partially correct, and there were surprising amount of semantically
   correct ones. not a bad result, considering how little effort we   ve had
   put in so far.

the next step

   if you submit the result you should get around .22 id7. the current
   top id7 score is .33, so there   s a lot of rooms for improvement. you
   can check out opts.py in the root folder for more built-in model
   parameters. or dive deep into the codebase to figure out how things
   work and where might be improved.

   the other paths include applying id40 on chinese
   sentences, adding id39, using pronunciation
   dictionary[34][10] to guess translation to unseen english names, etc.

   (update on 2017/10/14: if you use [35]jieba and jieba.cut with default
   settings to tokenize the chinese sentence, you   d get around .20 id7 on
   public leaderboad. one of the possible reasons to the score drop is its
   much larger chinese vocabulary size. you can tell from the number of
   <unk> in the output.)

references:

    1. [36]sutskever, i., vinyals, o., & le, q. v. (2014). sequence to
       sequence learning with neural networks.
    2. [37]nallapati, r., zhou, b., dos santos, c., & xiang, b. (2016).
       abstractive text summarization using sequence-to-sequence id56s and
       beyond.
    3. [38]venugopalan, s., rohrbach, m., donahue, j., mooney, r.,
       darrell, t., & saenko, k. (2015). sequence to sequence
    4. [39]sequence to sequence model: introduction and concepts
    5. [40]id195: the clown car of deep learning
    6. [41]practical pytorch: translation with a sequence to sequence
       network and attention
    7. [42]cutting edge deep learning for coders, part 2, lecture
       12         id12
    8. [43]cutting edge deep learning for coders, part 2, lecture
       13         neural translation
    9. [44]google/id195: a general-purpose encoder-decoder framework for
       tensorflow
   10. [45]the cmu pronouncing dictionary

     * [46]machine learning
     * [47]deep learning
     * [48]machine translation
     * [49]tutorial

   (button)
   (button)
   (button) 332 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [50]go to the profile of ceshine lee

[51]ceshine lee

   humanist. data geek.

     (button) follow
   [52]towards data science

[53]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 332
     * (button)
     *
     *

   [54]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [55]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d82a154f603e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/building-a-translation-system-in-minutes-d82a154f603e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/building-a-translation-system-in-minutes-d82a154f603e&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_zd9vk2vilhby---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ceshine?source=post_header_lockup
  17. https://towardsdatascience.com/@ceshine
  18. https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d
  19. https://medium.com/@devnag/id195-the-clown-car-of-deep-learning-f88e1204dac3
  20. https://github.com/spro/practical-pytorch/blob/master/id195-translation/id195-translation.ipynb
  21. http://course.fast.ai/lessons/lesson12.html
  22. http://course.fast.ai/lessons/lesson13.html
  23. http://openid4.net/
  24. https://github.com/google/id195
  25. https://github.com/openid4/openid4
  26. http://torch.ch/
  27. https://github.com/openid4/openid4-py
  28. http://pytorch.org/
  29. https://github.com/openid4/openid4-py
  30. https://challenger.ai/competition/translation/subject?lan=en
  31. https://challenger.ai/competition/translation/subject?lan=en
  32. http://www.statmt.org/wmt17/translation-task.html#download
  33. https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-id7.perl
  34. http://www.speech.cs.cmu.edu/cgi-bin/cmudict
  35. https://github.com/fxsjy/jieba
  36. https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
  37. https://arxiv.org/pdf/1602.06023.pdf
  38. http://arxiv.org/abs/1505.00487
  39. https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d
  40. https://medium.com/@devnag/id195-the-clown-car-of-deep-learning-f88e1204dac3
  41. https://github.com/spro/practical-pytorch/blob/master/id195-translation/id195-translation.ipynb
  42. http://course.fast.ai/lessons/lesson12.html
  43. http://course.fast.ai/lessons/lesson13.html
  44. https://github.com/google/id195
  45. http://www.speech.cs.cmu.edu/cgi-bin/cmudict
  46. https://towardsdatascience.com/tagged/machine-learning?source=post
  47. https://towardsdatascience.com/tagged/deep-learning?source=post
  48. https://towardsdatascience.com/tagged/machine-translation?source=post
  49. https://towardsdatascience.com/tagged/tutorial?source=post
  50. https://towardsdatascience.com/@ceshine?source=footer_card
  51. https://towardsdatascience.com/@ceshine
  52. https://towardsdatascience.com/?source=footer_card
  53. https://towardsdatascience.com/?source=footer_card
  54. https://towardsdatascience.com/
  55. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  57. http://openid4.net/
  58. https://github.com/openid4/openid4-py
  59. https://challenger.ai/competition/translation/subject?lan=en
  60. https://medium.com/p/d82a154f603e/share/twitter
  61. https://medium.com/p/d82a154f603e/share/facebook
  62. https://medium.com/p/d82a154f603e/share/twitter
  63. https://medium.com/p/d82a154f603e/share/facebook
