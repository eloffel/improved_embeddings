   #[1]antonio verdone blog - ai

   [2][]

[3]antonio verdone blog

   ai

   [4]blog [5]about

id21 using tensorflow's id164 api: detecting r2-d2
and bb-8

   in this post, i   m going to train an object detector to locate r2-d2 and
   bb-8 in an image or video. all the files can be found on my [6]github
   repo. but let   s not wait and see some results!
   [result_1.gif] detection of r2-d2 and bb-8

what is id21

   training an entire convolutional neural network (convnet or id98) from
   scratch is something that few people can afford. in order to accomplish
   such task, two elements are required in large amounts: data and
   computational power. the former is hard to get: huge and reliable
   datasets are normally the result of a joint effort between different
   people or research groups working together for an extended period of
   time. the latter could be very expensive (in terms of money or time) if
   a huge model needs to be trained.

   for these reasons, what many people do is what is called transfer
   learning: use an already trained (also called pre-trained) network as
   the initialization point for their own model, or more specifically, use
   the weights of the pre-trained convnet as the starting weights of their
   own model. at this point, two options exist:
     * leave the net as it is, perhaps modifying and re-training only the
       last layer (the fully connected one). this is the easiest case
       where we can start making predictions right away (if we don   t even
       re-train the last layer). the [7]demo in the tensorflow repo covers
       this situation and it can be set up in a few minutes.
     * fine-tune the net. in this case, we re-train the weights of the
       convnet using regular id26. since the initial layers of
       a id98 tend to explain more primitive shapes, common to many
       different objects, we could choose to re-train only the higher
       layers and fix the lower ones (the ones closer to the input). for
       the same reason, this method is convenient if we want to detect
       different objects from the ones used to train the pre-trained
       convnet. here, i will show you this process in order to detect
       r2-d2 and bb-8 from star wars in any image or video.

   if you   re asking yourself    why do you want to detect r2-d2 and bb-8 in
   a video?   , i guess the easiest answer would be    why not?   . the
   experiment at hand is an engineering one or more colloquially a problem
   of how to put the pieces together. i wanted to try tensorflow   s object
   detection api and make it work. my objective was not to achieve
   state-of-the-art scores. therefore, i figured i   d use something cool
   that i like. so    star wars! of course, you can collect your own images
   and detect whatever object you want.

tensorflow   s id164 api

   some time ago, the tensorflow team made available an id164
   api that makes the process of fine-tuning a pre-trained model easier.
   in order to use the api, we only need to tweak some lines of code from
   the files already made available to us. here, i won   t go into the
   details of the net architecture, the optimization algorithm used (the
   default will be rmsprop) or the several other hyper-parameters.

   following, i will list the main steps needed to build your own object
   detection model (it is assumed that you already followed the
   [8]installation instructions for the api):
    1. collect your images and annotate them (i used [9]labelimg).
       normally, you would want to have a few hundred images but it
       depends on the problem. i managed to gather a hundred for each
       class (r2-d2 and bb-8). they are probably too few and too similar
       based on the results obtained, but at least the model works as
       expected.
    2. create tf records using the [10]file provided by the api. these
       files will be the input for the api. if your dataset (images and
       annotations) has an analogous format as the one of the
       [11]oxford-iiit pet dataset, you shouldn   t have any trouble
       creating your tf records. tensorflow has also a [12]helper code to
       create tf records of the [13]pascal voc dataset. otherwise, this
       step could be a bit tricky, since you should code the program by
       yourself. the command to run the script that creates the tf records
       is the following (this is windows code, so ^ just splits a long
       line and it   s analogous to \ in linux):
python create_sw_tf_record.py -- ^
label_map_path=d:/python_projects/detection/object_detection/data/sw_label_map.p
btxt -- ^
data_dir=d:/python_projects/detection --output_dir=d:/python_projects/detection/
object_detection/data

       of course, you need to use your own path. i used the full path
       because i had troubles with the short ones (a windows matter most
       likely).
    3. download a pre-trained model from [14]here. i chose the
       ssd_inception_v2_coco because it was fast and had a higher
       precision (map) than ssd_mobilenet_v1_coco, but you can use any
       other.
    4. create a configuration file for the model that will be trained. you
       can choose from [15]here the corresponding configuration file to
       your model. you only need to change the paths and the number of
       classes (num_classes: 2 in line 9). in my case there are just two.
    5. train the model. you can use the following code from inside of the
       id164 directory of the api (that you should   ve clone
       previously while following the installation instructions):
python train.py --logtostderr --train_dir=d:/python_projects/detection/object_de
tection/data ^
--pipeline_config_path=d:/python_projects/detection/object_detection/data/ssd_in
ception_v2_pets.config

       this step could last for hours to get to a stable training loss,
       depending on your gpu. i don   t know how long would it take on a
       cpu. a more rigorous study would require to check the id168
       or our preferred metric on a validation set. however, since i
       couldn   t gather many images and this is just an experiment on the
       tensorflow api, i won   t care here. you can check the evolution of
       your model using tensorboard. open a new command prompt or terminal
       and write:
cd d:/projects_python/detection/object_detection
tensorboard --logdir=data

       on windows, if you pass the full or shortened path to logdir, you
       won   t see any results (at least it happened to me and this worked
       after a few tries).
    6. export the frozen graph from the trained model. you just need to
       use the [16]script provided by tensorflow without any
       modifications. the code is:
python export_id136_graph.py --input_type image_tensor ^
--pipeline_config_path d:/python_projects/detection/object_detection/data/ssd_in
ception_v2_pets.config ^
--trained_checkpoint_prefix d:/python_projects/detection/object_detection/data/m
odel.ckpt-5576 ^
--output_directory d:/python_projects/detection/sw_id136_graph

    7. try your model!. you can use this [17]ipython notebook to test your
       model on images and videos.

detecting r2-d2 and bb-8

   after i trained my model following the previous steps, i tested it in a
   sequence that didn   t contain any frames used for the training:

   iframe: [18]https://www.youtube.com/embed/tiygobvra6e

   as we can see, the model gets confused quite easily, leading to some
   false positives (harrison ford getting detected as bb-8, but with low
   id203) and to missed detections (r2-d2 doesn   t get detected in
   some dark frames). all these failures are most likely a consequence of
   having such a small dataset and with images that are quite similar to
   each other. for example, the dataset doesn   t contain almost any image
   of the robots being small and part of the background. hence, the video
   fails to detect them when they are not in a close-up or a medium shot.
   nonetheless, the results obtained are the ones expected and we can see
   that the model works most of the times. here are some final test images
   where the model succeed.

   [test_image_1.png]

   [test_image_2.png]

   [test_image_3.png]

   [test_image_4.png]

   [test_image_5.png]

   [test_image_6.png]

   in this last one, i don   t know if i got a lucky frame or if in the
   video the model works worse for some reason.
   written on october 17, 2017
   please enable javascript to view the [19]comments powered by disqus.

references

   visible links
   1. https://averdones.github.io/feed.xml
   2. https://averdones.github.io/
   3. https://averdones.github.io/
   4. https://averdones.github.io/
   5. https://averdones.github.io/about
   6. https://github.com/averdones/star_wars_object_detection
   7. https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
   8. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md
   9. https://github.com/tzutalin/labelimg
  10. https://github.com/averdones/star_wars_object_detection/blob/master/create_sw_tf_record.py
  11. http://www.robots.ox.ac.uk/~vgg/data/pets/
  12. https://github.com/tensorflow/models/blob/master/research/object_detection/create_pascal_tf_record.py
  13. http://host.robots.ox.ac.uk/pascal/voc/
  14. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
  15. https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs
  16. https://github.com/averdones/star_wars_object_detection/blob/master/export_id136_graph.py
  17. https://github.com/averdones/star_wars_object_detection/blob/master/object_detection_sw.ipynb
  18. https://www.youtube.com/embed/tiygobvra6e
  19. http://disqus.com/?ref_noscript

   hidden links:
  21. mailto:averdones@gmail.com
  22. https://github.com/averdones
  23. https://www.linkedin.com/in/antonioverdone
