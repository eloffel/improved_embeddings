   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]intuition machine
     * [9]patterns
     * [10]methodology
     * [11]strategy
     * [12]deep learning playbook
     __________________________________________________________________

google and uber   s best practices for deep learning

   go to the profile of carlos e. perez
   [13]carlos e. perez (button) blockedunblock (button) followfollowing
   oct 3, 2017

   there is more to building a sustainable deep learning solution than
   what is provided by deep learning frameworks like tensorflow and
   pytorch. these frameworks are good enough for research, but they don   t
   take into account the problems that crop up with production deployment.
   i   ve [14]written previously about technical debt and the need from more
   adaptive biological like architectures. to support a viable business
   using deep learning, you absolutely need an architecture that supports
   sustainable improvement in the presence of frequent and unexpected
   changes in the environment. current deep learning framework only
   provide a single part of a complete solution.

   fortunately, google and uber have provided a glimpse of their internal
   architectures. the architectures of these two giants can be two
   excellent base-camps if you need to build your own production ready
   deep learning solution.

   the primary motivations of uber   s system named michelangelo was that
      there were no systems in place to build reliable, uniform, and
   reproducible pipelines for creating and managing training and
   prediction data at scale.    in their [15]paper, they describe the
   limitations of existing frameworks with the issues of deployment and
   managing technical debt. the paper has enough arguments that should
   convince any skeptic that existing frameworks are insufficient for the
   production.

   i   m not going to go through uber   s paper with you in its entirety.
   rather, i   m just going to highlight some important points about their
   architecture. the uber system is not a strictly deep learning system,
   but rather a machine learning system that can employ many ml methods
   depending on suitability. it is built on the following open source
   components: [16]hdfs, [17]spark, [18]samza, [19]cassandra, [20]mllib,
   [21]xgboost, and [22]tensorflow. so, it   s a conventional bigdata system
   that incorporates machine learning components for its analytics:

     michelangelo is built on top of uber   s data and compute
     infrastructure, providing a data lake that stores all of uber   s
     transactional and logged data, kafka brokers that aggregate logged
     messages from all uber   s services, a samza streaming compute engine,
     managed cassandra clusters, and uber   s in-house service provisioning
     and deployment tools.

   the architecture supports the following workflow:
    1. manage data
    2. train models
    3. evaluate models
    4. deploy, predict and monitor

   uber   s michaelangelo architectures is depicted as follows:
   [1*qozokbfrz_-1_bg9vm6zba.png]

   i am going to skip over the usual big data architecture concerns and
   point out some notable ideas that relates more to machine learning.

   michaelangelo divides the management of data between online and offline
   pipelines. in addition, to permit knowledge sharing and reuse across
   the organization, a    feature store    is made available:

     at the moment, we have approximately 10,000 features in feature
     store that are used to accelerate machine learning projects, and
     teams across the company are adding new ones all the time. features
     in the feature store are automatically calculated and updated daily.

   uber created a domain specific language (dsl) for modelers to select,
   transform and combine feature prior to sending a model to training and
   prediction. currently supported ml methods are id90, linear
   and logistic models, id116, time-series and deep neural networks.

   the model configuration specifies type, hyper-parameters, data source
   references, the feature dsl expressions and compute resource
   requirements (i.e. cpus, memory, use of gpu, etc.). training is
   performed in either a yarn or mesos cluster.

   after model training, performance metrics are calculated and provided
   in an evaluation report. all of the information, that is the model
   configuration, the learned model and the evaluation report are stored
   in the a versioned model repository for analysis and deployment. the
   model information contains:
     * who trained the model
     * start and end time of the training job
     * full model configuration (features used, hyper-parameter values,
       etc.)
     * reference to training and test data sets
     * distribution and relative importance of each feature
     * model accuracy metrics
     * standard charts and graphs for each model type (e.g. roc curve, pr
       curve, and confusion matrix for a binary classifier)
     * full learned parameters of the model
     * summary statistics for model visualization

   the idea is to democratize access to ml models, sharing it with other
   to improve organizational knowledge. the unique feature of uber   s
   approach is the surfacing of a    feature store    that allows many
   different parties to share their data across different ml models.

   the folks at google have a recent paper    [23]tfx: a tensorflow-based
   production scale machine learning platform    that details their internal
   system.
   [1*8a0icfzragrqxsz5mwisea.png]

   the paper is structured similarly to uber   s paper in that they cover
   the same workflow:
    1. manage data         data analysis, transformation and validation
    2. train models         model training: warm-starting and model
       specification
    3. evaluate models         model evaluation and validation
    4. deploy, predict and monitor         model serving

   google   s architecture is driven by the following stated high level
   guidelines:
     * capture data anomalies early.
     * automate data validation.
     * treat data errors with the same rigor as code.
     * support continuous training.
     * uniform configuration to improve sharing.
     * reliable and scalable production deployment and serving.

   let   s dig a little deeper into the unique capabilities of google   s tfx.
   there are plenty of tidbits of wisdom as well as an introduction of
   several unique capabilities.

   tfx provides several capabilities in the scope of data management. data
   analysis performs statistics on each dataset providing information
   about value distribution, quantiles, mean, standard-deviation etc. the
   idea is that this allows users to quickly gain insights on the shape of
   dataset. this automated analysis is used to improve the continuous
   training and serving environment.

   tfx handles the data wrangling and stores the transformations to
   maintain consistency. furthermore, the system provides are uniform and
   consistent framework for managing feature-to-integer mappings.

   tfx proves a schema that is version that specifies the expectations on
   the data. this schema is used to flag any anomalies found and also
   provide recommendations of actions such as blocking training or
   deprecating features. the tooling provide auto-generation of this
   schema to make it easy to use for new projects. this is a unique
   capability that draws inspiration from the static type checking found
   in programming languages.

   tfx uses tensorflow as its model description. tfx has this notion of
      warm-starting    that is inspired by id21 technique found
   in deep learning. the idea is to reduce the amount of training by
   leveraging existing training. unlike id21 that employs an
   existing pre-trained network, warm-starting selectively identifies a
   general features network as its starting point. the network that is
   trained on general features are used as the basis for training more
   specialized networks. this feature appears to be implememented in
   [24]tf-slim.

   tfx uses a common high level tensorflow specification (see:
   [25]tensorflow estimators: managing simplicity vs. flexibility in
   high-level machine learning frameworks ) to provide uniformity and
   encode best practices across different implementations. see this
   article on [26]estimators for more detail.

   tfx uses the [27]tensorflow serving framework for deployment and
   serving. the framework allows different models to be served while keep
   the same architecture and api. tensorflow serving provies a    soft
   model-isolation    to allow multi-tenant deployment of models. the
   framework is also designed to support scalable id136s.

   the tfx paper mentioned the need to optimize the deserialization of
   models. apparently, a customized protocol buffer parses was created to
   improve performance up to 2   5 times.

   dissecting uber and google   s internal architecture provides good
   insight on pain-points and solutions for building your own internal
   platform. as compared to available open source dl frameworks, there is
   a greater emphasis in managing and sharing of meta-information.
   google   s approach also demands additional effort to ensure uniformity
   as well as automated validation. these are practices that we have seen
   previously in conventional software-engineering projects.

   software engineering practices such as test driven development (tdd),
   continuous integration, rollback and recovery, change control etc. are
   being introduced into advanced machine learning practices. it is not
   enough for a specialist to develop on a [28]jupyter notebook and throw
   it over the wall to a team to make operational. the same end-to-end
   devops practices that we find today in the best engineering companies
   are also going to be demanded in machine learning endeavors. we see
   this today in both uber and google, and thus we should expect it in any
   sustainable ml/dl practice.

   update: [29]https://www.linkedin.com/pulse/ai-layer-diego-oppenheimer ,
   [30]https://arxiv.org/abs/1804.09997v1
   [1*klghw8y2ldrko117bnqswq.png]
   explore deep learning: [31]artificial intuition: the improbable deep
   learning revolution
   [1*j9kar_3vwdjk8twhtmxc0g.png]
   exploit deep learning: [32]the deep learning ai playbook

     * [33]machine learning
     * [34]deep learning
     * [35]artificial intelligence

   (button)
   (button)
   (button) 1.97k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of carlos e. perez

[36]carlos e. perez

   medium member since feb 2018

   author of artificial intuition and the deep learning playbook    
   linkedin.com/in/ceperez

     (button) follow
   [37]intuition machine

[38]intuition machine

   deep learning patterns, methodology and strategy

     * (button)
       (button) 1.97k
     * (button)
     *
     *

   [39]intuition machine
   never miss a story from intuition machine, when you sign up for medium.
   [40]learn more
   never miss a story from intuition machine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/58488a8899b6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/google-and-ubers-best-practices-for-deep-learning-58488a8899b6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/google-and-ubers-best-practices-for-deep-learning-58488a8899b6&source=--------------------------nav_reg&operation=register
   8. https://medium.com/intuitionmachine?source=logo-lo_zne6i290595k---d777623c68cf
   9. https://medium.com/intuitionmachine/tagged/design-patterns
  10. https://medium.com/intuitionmachine/tagged/agile-methodology
  11. https://medium.com/intuitionmachine/tagged/strategy
  12. https://deeplearningplaybook.com/
  13. https://medium.com/@intuitmachine
  14. https://medium.com/intuitionmachine/biologically-inspired-software-architecture-for-deep-learning-e64db295bb2f
  15. https://eng.uber.com/michelangelo/
  16. http://hadoop.apache.org/
  17. https://spark.apache.org/
  18. http://samza.apache.org/
  19. http://cassandra.apache.org/
  20. https://spark.apache.org/mllib/
  21. https://github.com/dmlc/xgboost
  22. https://www.tensorflow.org/
  23. http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform
  24. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim
  25. https://arxiv.org/abs/1708.02637
  26. https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0
  27. https://www.tensorflow.org/serving/
  28. https://owainkenwayucl.github.io/2017/10/03/whyidontlikenotebooks.html
  29. https://www.linkedin.com/pulse/ai-layer-diego-oppenheimer/?trackingid=f5kpzvjb1sguyrjjepj2ra==
  30. https://arxiv.org/abs/1804.09997v1
  31. https://gumroad.com/products/ihdj
  32. https://deeplearningplaybook.com/
  33. https://medium.com/tag/machine-learning?source=post
  34. https://medium.com/tag/deep-learning?source=post
  35. https://medium.com/tag/artificial-intelligence?source=post
  36. https://medium.com/@intuitmachine
  37. https://medium.com/intuitionmachine?source=footer_card
  38. https://medium.com/intuitionmachine?source=footer_card
  39. https://medium.com/intuitionmachine
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/@intuitmachine?source=post_header_lockup
  43. https://medium.com/p/58488a8899b6/share/twitter
  44. https://medium.com/p/58488a8899b6/share/facebook
  45. https://medium.com/@intuitmachine?source=footer_card
  46. https://medium.com/p/58488a8899b6/share/twitter
  47. https://medium.com/p/58488a8899b6/share/facebook
