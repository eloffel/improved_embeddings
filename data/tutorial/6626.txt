   #[1]alternate

   [2][profile-thumbnail.png] nikhil buduma         musings of an mit
   student

deep learning in a nutshell

   29 december 2014

   deep learning. neural networks. id26. over the past year or
   two, i've heard these buzz words being tossed around a lot, and it's
   something that has definitely seized my curiosity recently. deep
   learning is an area of active research these days, and if you've kept
   up with the field of computer science, i'm sure you've come across at
   least some of these terms at least once.

   deep learning can be an indimidating concept, but it's becoming
   increasingly important these days. google's already making huge strides
   in the space with the [3]google brain project and its recent
   acquisition of the london-based deep learning startup [4]deepmind.
   moreover, deep learning methods are beating out traditional machine
   learning approaches on virtually every single metric.

   so what exactly is deep learning? how does it work? and most
   importantly, why should you even care?

note to the reader

   if you're new to computer science, and you've followed me up till this
   point, please stick with me. certain optional sections of this article
   may get a little math heavy (marked with a *), but i want to make this
   subject accessible to everyone, computer science major or not. in fact,
   if you are reading through this article and, at any point, you find
   yourself confused about the material, please email me. i will make
   whatever edits and clarifications that are necessary to make the
   article clearer.

what is machine learning?

   before we dive into deep learning, i want to take a step back and talk
   a little bit about the broader field of "machine learning" and what it
   means when we say that we're programming machines to learn.

   sometimes we encounter problems for which it's really hard to write a
   computer program to solve. for example, let's say we wanted to program
   a computer to recognize hand-written digits:

   handwritten digits

image provided by the mnist handwritten database

   you could imagine trying to devise a set of rules to distinguish each
   individual digit. zeros, for instance, are basically one closed loop.
   but what if the person didn't perfectly close the loop. or what if the
   right top of the loop closes below where the left top of the loop
   starts?

   handwritten zero or six

a zero that's difficult to distinguish from a six algorithmically

   in this case, we have difficulty differentiating zeroes from sixes. we
   could establish some sort of cutoff, but how would you decide the
   cutoff in the first place? as you can see, it quickly becomes quite
   complicated to compile a list of heuristics (i.e., rules and guesses)
   that accurately classifies handwritten digits.

   and there are so many more classes of problems that fall into this
   category. recognizing objects, understanding concepts, comprehending
   speech. we don't know what program to write because we still don't know
   how it's done by our own brains. and even if we did have a good idea
   about how to do it, the program might be horrendously complicated.

   so instead of trying to write a program, we try to develop an algorithm
   that a computer can use to look at hundreds or thousands of examples
   (and the correct answers), and then the computer uses that experience
   to solve the same problem in new situations. essentially, our goal is
   to teach the computer to solve by example, very similar to how we might
   teach a young child to distinguish a cat from a dog.

   over the past few decades, computer scientists have developed a number
   of algorithms that try to allow computers to learn to solve problems
   through examples. deep learning, which was first theorized in the early
   80's (and perhaps even earlier), is one paradigm for performing machine
   learning. and because of a flurry of modern research, deep learning is
   again on the rise because it's been shown to be quite good at teaching
   computers to do what our brains can do naturally.

   one of the big challenges with traditional machine learning models is a
   process called feature extraction. specifically, the programmer needs
   to tell the computer what kinds of things it should be looking for that
   will be informative in making a decision. feeding the algorithm raw
   data rarely ever works, so feature extraction is a critical part of the
   traditional machine learning workflow. this places a huge burden on the
   programmer, and the algorithm's effectiveness relies heavily on how
   insightful the programmer is. for complex problems such as object
   recognition or handwriting recognition, this is a huge challenge.

   deep learning is one of the only methods by which we can circumvent the
   challenges of feature extraction. this is because deep learning models
   are capable of learning to focus on the right features by themselves,
   requiring little guidance from the programmer. this makes deep learning
   an extremely powerful tool for modern machine learning.

a first look at neural networks

   deep learning is a form of machine learning that uses a model of
   computing that's very much inspired by the structure of the brain.
   hence we call this model a neural network. the basic foundational unit
   of a neural network is the neuron, which is actually conceptually quite
   simple.

   neuron

schematic for a neuron in a neural net

   each neuron has a set of inputs, each of which is given a specific
   weight. the neuron computes some function on these weighted inputs. a
   linear neuron takes a linear combination of the weighted inputs. a
   sigmoidal neuron does something a little more complicated:

   sigmoid

the function of a sigmoidal neuron

   it feeds the weighted sum of the inputs into the logistic function. the
   logistic function returns a value between 0 and 1. when the weighted
   sum is very negative, the return value is very close to 0. when the
   weighted sum is very large and positive, the return value is very close
   to 1. for the more mathematically inclined, the logistic function is a
   good choice because it has a nice looking derivative, which makes
   learning a simpler process. but technical details aside, whatever
   function the neuron uses, the value it computes is transmitted to other
   neurons as its output. in practice, sigmoidal neurons are used much
   more often than linear neurons because they enable much more versatile
   learning algorithms compared to linear neurons.

   a neural network comes about when we start hooking up neurons to each
   other, to the input data, and to the "outlets," which correspond to the
   network's answer to the learning problem. to make this structure easier
   to visualize, i've included a simple example of a neural net below. we
   let be the weight of the link connecting the neuron in the layer with
   the neuron in the layer:

   neural net

an example of a neural net with 3 layers and 3 neurons per layer

   similar to how neurons are generally organized in layers in the human
   brain, neurons in neural nets are often organized in layers as well,
   where neurons on the bottom layer receive signals from the inputs,
   where neurons in the top layers have their outlets connected to the
   "answer," and where are usually no connections between neurons in the
   same layer (although this is an optional restriction, more complex
   connectivities require more involved mathematical analysis). we also
   note that in this example, there are no connections that lead from a
   neuron in a higher layer to a neuron in a lower layer (i.e., no
   directed cycles). these neural networks are called feed-forward neural
   networks as opposed to their counterparts, which are called recursive
   neural networks (again these are much more complicated to analyze and
   train). for the sake of simplicity, we focus only on feed-forward
   networks throughout this discussion. here's a set of some more
   important notes to keep in mind:

   1) although every layer has the same number of neurons in this example,
   this is not necessary.

   2) it is not required that a neuron has its outlet connected to the
   inputs of every neuron in the next layer. in fact, selecting which
   neurons to connect to which other neurons in the next layer is an art
   that comes from experience. allowing maximal connectivity will more
   often than not result in overfitting, a concept which we will discusss
   in more depth later.

   3) the inputs and outputs are vectorized representations. for example,
   you might imagine a neural network where the inputs are the individual
   pixel rgb values in an image represented as a vector. the last layer
   might have 2 neurons which correspond to the answer to our problem: if
   the image contains a dog, if the image contains a cat, if it contains
   neither, and if it contains both.

   4) the layers of neurons that lie sandwiched between the first layer of
   neurons (input layer) and the last layer of neurons (output layer), are
   called hidden layers. this is because this is where most of the magic
   is happening when the neural net tries to solve problems. taking a look
   at the activities of hidden layers can tell you a lot about the
   features the network has learned to extract from the data.

training a single neuron

   well okay, things are starting to get interesting, but we're still
   missing a big chunk of the picture. we know how a neural net can
   compute answers from inputs, but we've been assuming that we know what
   weights to use to begin with. finding out what those weights should be
   is the hard part of the problem, and that's done through a process
   called training. during training, we show the neural net a large number
   of training examples and iteratively modify the weights to minimize the
   errors we make on the training examples.

   let's start off with a toy example involving a single linear neuron to
   motivate the process. every day you grab lunch in the dining hall where
   your meal consists completely of burgers, fries, and soda. you buy some
   number of servings of each item. you want to be able to predict how
   much your meal will cost you, but you don't know the prices of each
   individual item. the only thing the cashier will tell you is the total
   price of the meal.

   how do we solve this problem? well, we could begin by being smart about
   picking our training cases, right? for one meal we could buy only a
   single serving of burgers, for another we could only buy a single
   serving of fries, and then for our last meal we could buy a single
   serving of soda. in general, choosing smart training cases is a very
   good idea. there's lots of research that shows that by engineering a
   clever training set, you can make your neural net a lot more effective.
   the issue with this approach is that in real situations, this rarely
   ever gets you even 10% of the way to the solution. for example, what's
   the analog of this strategy in image recognition?

   let's try to motivate a solution that works in general. take a look at
   the single neuron we want to train:

   dining hall neuron

the neuron we want to train for the dining hall problem

   let's say we have a bunch of training examples. then we can calculate
   what the neural network will output on the training example using the
   simple formula in the diagram. we want to train the neuron so that we
   pick the optimal weights possible - the weights that minimize the
   errors we make on the training examples. in this case, let's say we
   want to minimize the square error over all of the training examples
   that we encounter. more formally, if we know that is the true answer
   for the training example and is the value computed by the neural
   network, we want to minimize the value of the error function :

   now at this point you might be thinking, wait up... why do we need to
   bother ourselves with this error function nonsense when we have a bunch
   of variables (weights) and we have a set of equations (one for each
   training example)? couldn't we just solve this problem by setting up a
   system of linear system of equations? that would automaically give us
   an error of zero assuming that we have a consistent set of training
   examples, right?

   that's a smart observation, but the insight unfortunately doesn't
   generalize well. remember that although we're using a linear neuron
   here, linear neurons aren't used very much in practice because they're
   constrained in what they can learn. and the moment you start using
   nonlinear neurons like the sigmoidal neurons we talked about, we can no
   longer set up a system of linear equations!

   so maybe we can use an iterative approach instead that generalizes to
   nonlinear examples. let's try to visualize how we might minimize the
   squared error over all of the training examples by simplifying the
   problem. let's say we're dealing with a linear neuron with only two
   inputs (and thus only two weights, and ). then we can imagine a
   3-dimensional space where the horizontal dimensions correspond to the
   weights and , and there is one vertical dimension that corresponds to
   the value of the error function . so in this space, points in the
   horizontal plane correspond to different settings of the weights, and
   the height at those points corresponds to the error that we're
   incurring, summed over all training cases. if we consider the errors we
   make over all possible weights, we get a surface in this 3-dimensional
   space, in particular a quadratic bowl:

   quadratic error surface

the quadratic error surface for a linear neuron

   we can also conveniently visualize this surface as a set of elliptical
   contours, where the minimum error is at the center of the ellipses:

   contour error surface

visualizing the error surface as a set of contours

   so now let's say we find ourselves somewhere on the horizontal plane
   (by picking a random initialization for the weights). how would we get
   ourselves to the point on the horizontal plane with the smallest error
   value? one strategy is to always move perpendicularly to the contour
   lines. take a look, for instance, at the path denoted by the red
   arrows. quite clearly, you can see that following this strategy will
   eventually get us to the point of minimum error.

   what's particularly interesting is that moving perpendicularly to the
   contour lines is equivalent to taking the path of steepest descent down
   the parabolic bowl. this is a pretty amazing result from calculus, and
   it gives us the name of this general strategy for training neural nets:
   id119.

learning rates and the delta rule

   in practice at each step of moving perpendicular to the contour, we
   need to determine how far we want to walk before recalculating our new
   direction. this distance needs to depend on the steepness of the
   surface. why? the closer we are to the minimum, the shorter we want to
   step forward. we know we are close to the minimum, because the surface
   is a lot flatter, so we can use the steepness as an indicator of how
   close we are to the minimum. we multiply this measure of steepness with
   a pre-determined constant factor , the learning rate. picking the
   learning rate is a hard problem. if we pick a learning rate that's too
   small, we risk taking too long during the training process. if we pick
   a learning rate that's too big, we'll mostly likely start diverging
   away from the minimum (this pretty easy to visualize). modern training
   algorithms adapt the learning rate to overcome this difficult
   challenge.

   for those who are interested, putting all the pieces results in what is
   called the delta rule for training the linear neuron. the delta rule
   states that given a learning rate , we ought to change the weight at
   each iteration of training by . deriving this formula is left as an
   exercise for the experienced reader. for a hint, study our derivation
   for a sigmoidal neuron in the next section.

   unfortunately, just taking the path of steepest descent doesn't always
   do the trick when we have nonlinear neurons. the error surface can get
   complicated and there could be multiple local minimum. as a result,
   using this procedure could potentially get us to a bad local minimum
   that isn't the global minimum. as a result, in practice, training
   neural nets involves a modification of id119 called
   stochastic id119, that tries to use randomization and noise
   to find the global minimum with high id203 on a complex error
   surface.

moving onto the sigmoidal neuron *

   this section and the next will get a little heavy with the math, so
   just be forewarned. if you're not comfortable with multivariate
   calculus, feel free to skip them and move onto the remaining sections.
   otherwise, let's just dive right into it!

   let's recall the mechanism by which logistic neurons compute their
   output value from their inputs:

   the neuron computes the weighted sum of its inputs, the logit, . it
   then feeds into the input function to compute , its final output. these
   functions have very nice derivatives, which makes learning easy! for
   learning, we want to compute the gradient of the error function with
   respect to the weights. to do so, we start by taking the derivative of
   the logit, , with respect to the inputs and the weights. by linearity
   of the logit:

   also, quite surprisingly, the derivative of the output with respect to
   the logit is quite simple if you express it in terms of the output.
   verifying this is left as an exercise for the reader:

   we then use the chain rule to get the derivative of the output with
   respect to each weight:

   putting all of this together, we can now compute the derivative of the
   error function with respect to each weight:

   thus, the final rule for modifying the weights becomes:

   as you may notice, the new modification rule is just like the delta
   rule, except with extra multiplicative terms included to account for
   the logistic component of the sigmoidal neuron.

the id26 algorithm *

   now we're finally ready to tackle the problem of training multilayer
   neural networks (instead of just single neurons). so what's the idea
   behind id26? we don't know what the hidden units ought to be
   doing, but what we can do is compute how fast the error changes as we
   change a hidden activity. essentially we'll be trying to find the path
   of steepest descent!

   each hidden unit can affect many output units. thus, we'll have to
   combine many separate effects on the error in an informative way. our
   strategy will be one of id145. once we have the error
   derivatives for one layer of hidden units, we'll use them to compute
   the error derivatives for the activites of the layer below. and once we
   find the error derivatives for the activities of the hidden units, it's
   quite easy to get the error derivatives for the weights leading into a
   hidden unit. we'll redefine some notation for ease of discussion and
   refer to the following diagram:

   id26 diagram

reference diagram for the derivation of the id26 algorithm

   the subscript we use will refer to the layer of the neuron. the symbol
   will refer to the activity of a neuron, as usual. similarly the symbol
   will refer to the logit of a neuron. we start by taking a look at the
   base case of the id145 problem, the error function
   derivatives at the output layer:

   now we tackle the inductive step. let's presume we have the error
   derivatives for layer . we now aim to calculate the error derivatives
   for the layer below it, layer . to do so, we must accumulate
   information for how the output of a neuron in layer affects the logits
   of every neuron in layer . this can be done as follows, using the fact
   that the partial derivative of the logit with respect to the incoming
   output data from the layer beneath is merely the weight of the
   connection :

   now we can use the following to complete the inductive step:

   combining these two together, we can finally express the partial
   derivatives of layer in terms of the partial derivatives of layer .

   then once we've gone through the whole id145 routine,
   having filled up the table appropriately with all of our partial
   derivatives (of the error function with respect to the hidden unit
   activities), we can then determine how the error changes with respect
   to the weights. this gives us how to modify the weights after each
   training example:

   in order to do id26 with batching of training examples, we
   merely sum up the partial derivatives over all the training examples in
   the batch. this gives us the following modification formula:

   we have succeeded in deriving the id26 algorithm for a
   feed-forward neural net utilizing sigmoidal neurons!

the problem of overfitting

   now let's say you decide you're very excited about deep learning and so
   you want to try to train a neural network of your own to identify
   objects in an image. you know this is a complicated problem so you use
   a huge neural network (let's say 20 layers) and you have 1,000 training
   examples. you train your neural network using the algorithm we
   describe, but something's clearly wrong. your neural net performs
   virtually perfectly on your training examples, but when you put it in
   practice, it performs very poorly! what's going on here?

   the problem we've encountered is called overfitting, and it happens
   when you have way too many parameters in your model and not enough
   training data. to visualize this, let's consider the figure below,
   where you want to fit a model to the data points:

   overfitting

an example that illustrates the concept of overfitting

   which curve would you trust? the line which gets almost no training
   example exactly? or the complicated curve that hits every single point
   in the training set? most likely you would trust the linear fit instead
   of the complicated curve because it seems less contrived. this
   situation is analagous to the neural network we trained. we have way
   too many parameters, over 100 trillion trillion (or 100 septillion)
   parameters. it's no wonder that we're overfitting!

   so how do we prevent overfitting? the two simplest ways are:

   1) limit the connectivities of neurons in your model. architecting a
   good neural network requires a lot of experience and intuition, and it
   boils down to giving your model freedom to discover relationships while
   also constraining it so it doesn't overfit.

   2) adding more training examples! often times you can cleverly add
   amplify your existing training set (changing illumination, applying
   shifts and other transformations, etc.).

   there are more sophisticated methods of training that try to directly
   solve overfitting such as including a dropout layers/neurons, but these
   methods are beyond the scope of this article

conclusions

   we've covered a lot of ground, but there's still a lot more that's
   going on in deep learning research. in future articles, i will probably
   talk more about different kinds of neural architectures (convolutional
   networks, soft max layers, etc.). i'll probably also write an article
   about how to train your own neural network using some of the awesome
   open source libraries out there, such as caffe (which allows you to gpu
   accelerate the training of neural networks). those of you who are
   interested in pursuing deep learning further, please get in touch! i
   love talking about new ideas and projects    

   [5]read next

the cell... reimagined

   i   ve been obsessed with biology ever since i was a kid. it started all
   the way back from the time when i had to make regular visits to the
   cardiologist       one of the side effects of being born with a serious
   heart defect. my heart defect has (thankfully) since recovered, but
   those visits had a ...

   [6]continue reading

        2015 nikhil buduma. all rights reserved. suscribe via [7]rss or
                                 [8]feedly.

references

   visible links
   1. http://nikhilbuduma.com/feed.xml
   2. http://nikhilbuduma.com/
   3. http://www.wired.com/2014/07/google_brain/
   4. http://deepmind.com/
   5. http://nikhilbuduma.com/2014/12/27/the-cell-reimagined/
   6. http://nikhilbuduma.com/2014/12/27/the-cell-reimagined/
   7. http://nikhilbuduma.com/feed.xml
   8. http://cloud.feedly.com/#subscription/feed/http://nikhilbuduma.com/feed.xml

   hidden links:
  10. https://twitter.com/nkbuduma
  11. https://github.com/darksigma
  12. https:www.linkedin.com/in/nkbuduma
