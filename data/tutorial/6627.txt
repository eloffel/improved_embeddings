    #[1]index [2]search [3]id119 [4]machine learning cheatsheet

   [5]ml cheatsheet
   ____________________

   basics
     * [6]id75
          + [7]introduction
          + [8]simple regression
               o [9]making predictions
               o [10]cost function
               o [11]id119
               o [12]training
               o [13]model evaluation
               o [14]summary
          + [15]multivariable regression
               o [16]growing complexity
               o [17]id172
               o [18]making predictions
               o [19]initialize weights
               o [20]cost function
               o [21]id119
               o [22]simplifying with matrices
               o [23]bias term
               o [24]model evaluation
     * [25]id119
          + [26]introduction
          + [27]learning rate
          + [28]cost function
          + [29]step-by-step
     * [30]id28
          + [31]introduction
               o [32]comparison to id75
               o [33]types of id28
          + [34]binary id28
               o [35]sigmoid activation
               o [36]decision boundary
               o [37]making predictions
               o [38]cost function
               o [39]id119
               o [40]mapping probabilities to classes
               o [41]training
               o [42]model evaluation
          + [43]multiclass id28
               o [44]procedure
               o [45]softmax activation
               o [46]scikit-learn example
     * [47]glossary

   math
     * [48]calculus
          + [49]introduction
          + [50]derivatives
               o [51]geometric definition
               o [52]taking the derivative
               o [53]step-by-step
               o [54]machine learning use cases
          + [55]chain rule
               o [56]how it works
               o [57]step-by-step
               o [58]multiple functions
          + [59]gradients
               o [60]partial derivatives
               o [61]step-by-step
               o [62]directional derivatives
               o [63]useful properties
          + [64]integrals
               o [65]computing integrals
               o [66]applications of integration
                    # [67]computing probabilities
                    # [68]expected value
                    # [69]variance
     * [70]id202
          + [71]vectors
               o [72]notation
               o [73]vectors in geometry
               o [74]scalar operations
               o [75]elementwise operations
               o [76]dot product
               o [77]hadamard product
               o [78]vector fields
          + [79]matrices
               o [80]dimensions
               o [81]scalar operations
               o [82]elementwise operations
               o [83]hadamard product
               o [84]matrix transpose
               o [85]id127
               o [86]test yourself
          + [87]numpy
               o [88]dot product
               o [89]broadcasting
     * [90]id203 (todo)
          + [91]links
          + [92]screenshots
          + [93]license
     * [94]statistics (todo)
     * [95]notation
          + [96]algebra
          + [97]calculus
          + [98]id202
          + [99]id203
          + [100]set theory
          + [101]statistics

   neural networks
     * [102]concepts
          + [103]neural network
          + [104]neuron
          + [105]synapse
          + [106]weights
          + [107]bias
          + [108]layers
          + [109]weighted input
          + [110]id180
          + [111]id168s
          + [112]optimization algorithms
     * [113]forwardpropagation
          + [114]simple network
               o [115]steps
               o [116]code
          + [117]larger network
               o [118]architecture
               o [119]weight initialization
               o [120]bias terms
               o [121]working with matrices
               o [122]dynamic resizing
               o [123]refactoring our code
               o [124]final result
     * [125]id26
          + [126]chain rule refresher
          + [127]applying the chain rule
          + [128]saving work with memoization
          + [129]code example
     * [130]id180
          + [131]linear
          + [132]elu
          + [133]relu
          + [134]leakyrelu
          + [135]sigmoid
          + [136]tanh
          + [137]softmax
     * [138]layers
          + [139]batchnorm
          + [140]convolution
          + [141]dropout
          + [142]linear
          + [143]lstm
          + [144]pooling
          + [145]id56
     * [146]id168s
          + [147]cross-id178
          + [148]hinge
          + [149]huber
          + [150]kullback-leibler
          + [151]mae (l1)
          + [152]mse (l2)
     * [153]optimizers
          + [154]adadelta
          + [155]adagrad
          + [156]adam
          + [157]conjugate gradients
          + [158]bfgs
          + [159]momentum
          + [160]nesterov momentum
          + [161]newton   s method
          + [162]rmsprop
          + [163]sgd
     * [164]id173
          + [165]data augmentation
          + [166]dropout
          + [167]early stopping
          + [168]ensembling
          + [169]injecting noise
          + [170]l1 id173
          + [171]l2 id173
     * [172]architectures
          + [173]autoencoder
          + [174]id98
          + [175]gan
          + [176]mlp
          + [177]id56
          + [178]vae

   algorithms (todo)
     * [179]classification
          + [180]bayesian
          + [181]boosting
          + [182]id90
          + [183]k-nearest neighbor
          + [184]id28
          + [185]id79s
          + [186]support vector machines
     * [187]id91
          + [188]centroid
          + [189]density
          + [190]distribution
          + [191]hierarchical
          + [192]id116
          + [193]mean shift
     * [194]regression
          + [195]lasso
          + [196]linear
          + [197]ordinary least squares
          + [198]polynomial
          + [199]ridge
          + [200]splines
          + [201]stepwise
     * [202]id23

   resources
     * [203]datasets
     * [204]libraries
     * [205]papers
     * [206]other

   contributing
     * [207]how to contribute

   [208]ml cheatsheet
     * [209]docs   
     * id75
     * [210]edit on github
     __________________________________________________________________

id75[211]  

     * [212]introduction
     * [213]simple regression
          + [214]making predictions
          + [215]cost function
          + [216]id119
          + [217]training
          + [218]model evaluation
          + [219]summary
     * [220]multivariable regression
          + [221]growing complexity
          + [222]id172
          + [223]making predictions
          + [224]initialize weights
          + [225]cost function
          + [226]id119
          + [227]simplifying with matrices
          + [228]bias term
          + [229]model evaluation

[230]introduction[231]  

   id75 is a supervised machine learning algorithm where the
   predicted output is continuous and has a constant slope. it   s used to
   predict values within a continuous range, (e.g. sales, price) rather
   than trying to classify them into categories (e.g. cat, dog). there are
   two main types:

   simple regression

   simple id75 uses traditional slope-intercept form, where
   \(m\) and \(b\) are the variables our algorithm will try to    learn    to
   produce the most accurate predictions. \(x\) represents our input data
   and \(y\) represents our prediction.
   \[y = mx + b\]

   multivariable regression

   a more complex, multi-variable linear equation might look like this,
   where \(w\) represents the coefficients, or weights, our model will try
   to learn.
   \[f(x,y,z) = w_1 x + w_2 y + w_3 z\]

   the variables \(x, y, z\) represent the attributes, or distinct pieces
   of information, we have about each observation. for sales predictions,
   these attributes might include a company   s advertising spend on radio,
   tv, and newspapers.
   \[sales = w_1 radio + w_2 tv + w_3 news\]

[232]simple regression[233]  

   let   s say we are given a [234]dataset with the following columns
   (features): how much a company spends on radio advertising each year
   and its annual sales in terms of units sold. we are trying to develop
   an equation that will let us to predict units sold based on how much a
   company spends on radio advertising. the rows (observations) represent
   companies.
   company  radio ($) sales
   amazon   37.8      22.1
   google   39.3      10.4
   facebook 45.9      18.3
   apple    41.3      18.5

[235]making predictions[236]  

   our prediction function outputs an estimate of sales given a company   s
   radio advertising spend and our current values for weight and bias.
   \[sales = weight \cdot radio + bias\]

   weight
          the coefficient for the radio independent variable. in machine
          learning we call coefficients weights.

   radio
          the independent variable. in machine learning we call these
          variables features.

   bias
          the intercept where our line intercepts the y-axis. in machine
          learning we can call intercepts bias. bias offsets all
          predictions that we make.

   our algorithm will try to learn the correct values for weight and bias.
   by the end of our training, our equation will approximate the line of
   best fit.
   _images/linear_regression_line_intro.png

   code
def predict_sales(radio, weight, bias):
    return weight*radio + bias

[237]cost function[238]  

   the prediction function is nice, but for our purposes we don   t really
   need it. what we need is a [239]cost function so we can start
   optimizing our weights.

   let   s use [240]mse (l2) as our cost function. mse measures the average
   squared difference between an observation   s actual and predicted
   values. the output is a single number representing the cost, or score,
   associated with our current set of weights. our goal is to minimize mse
   to improve the accuracy of our model.

   math

   given our simple linear equation \(y = mx + b\), we can calculate mse
   as:
   \[mse = \frac{1}{n} \sum_{i=1}^{n} (y_i - (m x_i + b))^2\]

   note
     * \(n\) is the total number of observations (data points)
     * \(\frac{1}{n} \sum_{i=1}^{n}\) is the mean
     * \(y_i\) is the actual value of an observation and \(m x_i + b\) is
       our prediction

   code
def cost_function(radio, sales, weight, bias):
    companies = len(radio)
    total_error = 0.0
    for i in range(companies):
        total_error += (sales[i] - (weight*radio[i] + bias))**2
    return total_error / companies

[241]id119[242]  

   to minimize mse we use [243]id119 to calculate the gradient
   of our cost function. [todo: slightly longer explanation].

   math

   there are two [244]parameters (coefficients) in our cost function we
   can control: weight \(m\) and bias \(b\). since we need to consider the
   impact each one has on the final prediction, we use partial
   derivatives. to find the partial derivatives, we use the [245]chain
   rule. we need the chain rule because \((y - (mx + b))^2\) is really 2
   nested functions: the inner function \(y - (mx + b)\) and the outer
   function \(x^2\).

   returning to our cost function:
   \[f(m,b) = \frac{1}{n} \sum_{i=1}^{n} (y_i - (mx_i + b))^2\]

   we can calculate the gradient of this cost function as:
   \[\begin{split}\begin{align} f'(m,b) = \begin{bmatrix} \frac{df}{dm}\\
   \frac{df}{db}\\ \end{bmatrix} &= \begin{bmatrix} \frac{1}{n} \sum -x_i
   \cdot 2(y_i - (mx_i + b)) \\ \frac{1}{n} \sum -1 \cdot 2(y_i - (mx_i +
   b)) \\ \end{bmatrix}\\ &= \begin{bmatrix} \frac{1}{n} \sum -2x_i(y_i -
   (mx_i + b)) \\ \frac{1}{n} \sum -2(y_i - (mx_i + b)) \\ \end{bmatrix}
   \end{align}\end{split}\]

   code

   to solve for the gradient, we iterate through our data points using our
   new weight and bias values and take the average of the partial
   derivatives. the resulting gradient tells us the slope of our cost
   function at our current position (i.e. weight and bias) and the
   direction we should update to reduce our cost function (we move in the
   direction opposite the gradient). the size of our update is controlled
   by the [246]learning rate.
def update_weights(radio, sales, weight, bias, learning_rate):
    weight_deriv = 0
    bias_deriv = 0
    companies = len(radio)

    for i in range(companies):
        # calculate partial derivatives
        # -2x(y - (mx + b))
        weight_deriv += -2*radio[i] * (sales[i] - (weight*radio[i] + bias))

        # -2(y - (mx + b))
        bias_deriv += -2*(sales[i] - (weight*radio[i] + bias))

    # we subtract because the derivatives point in direction of steepest ascent
    weight -= (weight_deriv / companies) * learning_rate
    bias -= (bias_deriv / companies) * learning_rate

    return weight, bias

[247]training[248]  

   training a model is the process of iteratively improving your
   prediction equation by looping through the dataset multiple times, each
   time updating the weight and bias values in the direction indicated by
   the slope of the cost function (gradient). training is complete when we
   reach an acceptable error threshold, or when subsequent training
   iterations fail to reduce our cost.

   before training we need to initialize our weights (set default values),
   set our [249]hyperparameters (learning rate and number of iterations),
   and prepare to log our progress over each iteration.

   code
def train(radio, sales, weight, bias, learning_rate, iters):
    cost_history = []

    for i in range(iters):
        weight,bias = update_weights(radio, sales, weight, bias, learning_rate)

        #calculate cost for auditing purposes
        cost = cost_function(radio, sales, weight, bias)
        cost_history.append(cost)

        # log progress
        if i % 10 == 0:
            print "iter={:d}    weight={:.2f}    bias={:.4f}    cost={:.2}".form
at(i, weight, bias, cost)

    return weight, bias, cost_history

[250]model evaluation[251]  

   if our model is working, we should see our cost decrease after every
   iteration.

   logging
iter=1     weight=.03    bias=.0014    cost=197.25
iter=10    weight=.28    bias=.0116    cost=74.65
iter=20    weight=.39    bias=.0177    cost=49.48
iter=30    weight=.44    bias=.0219    cost=44.31
iter=30    weight=.46    bias=.0249    cost=43.28

   visualizing
   _images/linear_regression_line_1.png
   _images/linear_regression_line_2.png
   _images/linear_regression_line_3.png
   _images/linear_regression_line_4.png

   cost history
   _images/linear_regression_training_cost.png

[252]summary[253]  

   by learning the best values for weight (.46) and bias (.25), we now
   have an equation that predicts future sales based on radio advertising
   investment.
   \[sales = .46 radio + .025\]

   how would our model perform in the real world? i   ll let you think about
   it :)

[254]multivariable regression[255]  

   let   s say we are given [256]data on tv, radio, and newspaper
   advertising spend for a list of companies, and our goal is to predict
   sales in terms of units sold.
   company  tv    radio news units
   amazon   230.1 37.8  69.1 22.1
   google   44.5  39.3  23.1 10.4
   facebook 17.2  45.9  34.7 18.3
   apple    151.5 41.3  13.2 18.5

[257]growing complexity[258]  

   as the number of features grows, the complexity of our model increases
   and it becomes increasingly difficult to visualize, or even comprehend,
   our data.
   _images/linear_regression_3d_plane_mlr.png

   one solution is to break the data apart and compare 1-2 features at a
   time. in this example we explore how radio and tv investment impacts
   sales.

[259]id172[260]  

   as the number of features grows, calculating gradient takes longer to
   compute. we can speed this up by    normalizing    our input data to ensure
   all values are within the same range. this is especially important for
   datasets with high standard deviations or differences in the ranges of
   the attributes. our goal now will be to normalize our features so they
   are all in the range -1 to 1.

   code
for each feature column {
    #1 subtract the mean of the column (mean id172)
    #2 divide by the range of the column (feature scaling)
}

   our input is a 200 x 3 matrix containing tv, radio, and newspaper data.
   our output is a normalized matrix of the same shape with all values
   between -1 and 1.
def normalize(features):
    **
    features     -   (200, 3)
    features.t   -   (3, 200)

    we transpose the input matrix, swapping
    cols and rows to make vector math easier
    **

    for feature in features.t:
        fmean = np.mean(feature)
        frange = np.amax(feature) - np.amin(feature)

        #vector subtraction
        feature -= fmean

        #vector division
        feature /= frange

    return features

   note

   matrix math. before we continue, it   s important to understand basic
   [261]id202 concepts as well as numpy functions like
   [262]numpy.dot().

[263]making predictions[264]  

   our predict function outputs an estimate of sales given our current
   weights (coefficients) and a company   s tv, radio, and newspaper spend.
   our model will try to identify weight values that most reduce our cost
   function.
   \[sales = w_1 tv + w_2 radio + w_3 newspaper\]
def predict(features, weights):
  **
  features - (200, 3)
  weights - (3, 1)
  predictions - (200,1)
  **
  predictions = np.dot(features, weights)
  return predictions

[265]initialize weights[266]  

w1 = 0.0
w2 = 0.0
w3 = 0.0
weights = np.array([
    [w1],
    [w2],
    [w3]
])

[267]cost function[268]  

   now we need a cost function to audit how our model is performing. the
   math is the same, except we swap the \(mx + b\) expression for \(w_1
   x_1 + w_2 x_2 + w_3 x_3\). we also divide the expression by 2 to make
   derivative calculations simpler.
   \[mse = \frac{1}{2n} \sum_{i=1}^{n} (y_i - (w_1 x_1 + w_2 x_2 + w_3
   x_3))^2\]
def cost_function(features, targets, weights):
    **
    features:(200,3)
    targets: (200,1)
    weights:(3,1)
    returns average squared error among predictions
    **
    n = len(targets)

    predictions = predict(features, weights)

    # matrix math lets use do this without looping
    sq_error = (predictions - targets)**2

    # return average squared error among predictions
    return 1.0/(2*n) * sq_error.sum()

[269]id119[270]  

   again using the [271]chain rule we can compute the gradient   a vector of
   partial derivatives describing the slope of the cost function for each
   weight.
   \[\begin{split}\begin{align} f'(w_1) = -x_1(y - (w_1 x_1 + w_2 x_2 +
   w_3 x_3)) \\ f'(w_2) = -x_2(y - (w_1 x_1 + w_2 x_2 + w_3 x_3)) \\
   f'(w_3) = -x_3(y - (w_1 x_1 + w_2 x_2 + w_3 x_3))
   \end{align}\end{split}\]
def update_weights(features, targets, weights, lr):
    '''
    features:(200, 3)
    targets: (200, 1)
    weights:(3, 1)
    '''
    predictions = predict(features, weights)

    #extract our features
    x1 = features[:,0]
    x2 = features[:,1]
    x3 = features[:,2]

    # use matrix cross product (*) to simultaneously
    # calculate the derivative for each weight
    d_w1 = -x1*(targets - predictions)
    d_w2 = -x2*(targets - predictions)
    d_w3 = -x3*(targets - predictions)

    # multiply the mean derivative by the learning rate
    # and subtract from our weights (remember gradient points in direction of st
eepest ascent)
    weights[0][0] -= (lr * np.mean(d_w1))
    weights[1][0] -= (lr * np.mean(d_w2))
    weights[2][0] -= (lr * np.mean(d_w3))

    return weights

   and that   s it! multivariate id75.

[272]simplifying with matrices[273]  

   the id119 code above has a lot of duplication. can we
   improve it somehow? one way to refactor would be to loop through our
   features and weights   allowing our function to handle any number of
   features. however there is another even better technique: vectorized
   id119.

   math

   we use the same formula as above, but instead of operating on a single
   feature at a time, we use id127 to operative on all
   features and weights simultaneously. we replace the \(x_i\) terms with
   a single feature matrix \(x\).
   \[gradient = -x(targets - predictions)\]

   code
x = [
    [x1, x2, x3]
    [x1, x2, x3]
    .
    .
    .
    [x1, x2, x3]
]

targets = [
    [1],
    [2],
    [3]
]

def update_weights_vectorized(x, targets, weights, lr):
    **
    gradient = x.t * (predictions - targets) / n
    x: (200, 3)
    targets: (200, 1)
    weights: (3, 1)
    **
    companies = len(x)

    #1 - get predictions
    predictions = predict(x, weights)

    #2 - calculate error/loss
    error = targets - predictions

    #3 transpose features from (200, 3) to (3, 200)
    # so we can multiply w the (200,1)  error matrix.
    # returns a (3,1) matrix holding 3 partial derivatives --
    # one for each feature -- representing the aggregate
    # slope of the cost function across all observations
    gradient = np.dot(-x.t,  error)

    #4 take the average error derivative for each feature
    gradient /= companies

    #5 - multiply the gradient by our learning rate
    gradient *= lr

    #6 - subtract from our weights to minimize cost
    weights -= gradient

    return weights

[274]bias term[275]  

   our train function is the same as for simple id75, however
   we   re going to make one final tweak before running: add a [276]bias
   term to our feature matrix.

   in our example, it   s very unlikely that sales would be zero if
   companies stopped advertising. possible reasons for this might include
   past advertising, existing customer relationships, retail locations,
   and salespeople. a bias term will help us capture this base case.

   code

   below we add a constant 1 to our features matrix. by setting this value
   to 1, it turns our bias term into a constant.
bias = np.ones(shape=(len(features),1))
features = np.append(bias, features, axis=1)

[277]model evaluation[278]  

   after training our model through 1000 iterations with a learning rate
   of .0005, we finally arrive at a set of weights we can use to make
   predictions:
   \[sales = 4.7tv + 3.5radio + .81newspaper + 13.9\]

   our mse cost dropped from 110.86 to 6.25.
   _images/multiple_regression_error_history.png

   references
   [1] [279]https://en.wikipedia.org/wiki/linear_regression
   [2]
   [280]http://www.holehouse.org/mlclass/04_linear_regression_with_multipl
   e_variables.html
   [3]
   [281]http://machinelearningmastery.com/simple-linear-regression-tutoria
   l-for-machine-learning
   [4] [282]http://people.duke.edu/~rnau/regintro.htm
   [5]
   [283]https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-r
   egression
   [6]
   [284]https://www.analyticsvidhya.com/blog/2015/08/common-machine-learni
   ng-algorithms

   [285]next [286]previous
     __________________________________________________________________

      copyright 2017 revision 5f00adef.
   built with [287]sphinx using a [288]theme provided by [289]read the
   docs.

   read the docs v: latest

   versions
          [290]latest

   downloads
          [291]pdf
          [292]htmlzip
          [293]epub

   on read the docs
          [294]project home
          [295]builds
     __________________________________________________________________

   free document hosting provided by [296]read the docs.

references

   1. https://ml-cheatsheet.readthedocs.io/en/latest/genindex.html
   2. https://ml-cheatsheet.readthedocs.io/en/latest/search.html
   3. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
   4. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
   5. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
   6. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html
   7. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#introduction
   8. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simple-regression
   9. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#making-predictions
  10. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#cost-function
  11. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#gradient-descent
  12. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#training
  13. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#model-evaluation
  14. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#summary
  15. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multivariable-regression
  16. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#growing-complexity
  17. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#id172
  18. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multiple-linear-regression-predict
  19. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#initialize-weights
  20. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  21. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  22. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simplifying-with-matrices
  23. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#bias-term
  24. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  25. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
  26. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#introduction
  27. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#learning-rate
  28. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#cost-function
  29. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#step-by-step
  30. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html
  31. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#introduction
  32. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#comparison-to-linear-regression
  33. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#types-of-logistic-regression
  34. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#binary-logistic-regression
  35. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#sigmoid-activation
  36. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#decision-boundary
  37. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#making-predictions
  38. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function
  39. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent
  40. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#mapping-probabilities-to-classes
  41. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training
  42. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#model-evaluation
  43. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#multiclass-logistic-regression
  44. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#procedure
  45. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#softmax-activation
  46. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#scikit-learn-example
  47. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html
  48. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html
  49. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#introduction
  50. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#derivatives
  51. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#geometric-definition
  52. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#taking-the-derivative
  53. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#step-by-step
  54. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#machine-learning-use-cases
  55. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#chain-rule
  56. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#how-it-works
  57. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
  58. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#multiple-functions
  59. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#gradients
  60. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#partial-derivatives
  61. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
  62. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#directional-derivatives
  63. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#useful-properties
  64. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#integrals
  65. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-integrals
  66. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#applications-of-integration
  67. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-probabilities
  68. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#expected-value
  69. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#variance
  70. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html
  71. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vectors
  72. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#notation
  73. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vectors-in-geometry
  74. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#scalar-operations
  75. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations
  76. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#dot-product
  77. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#hadamard-product
  78. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vector-fields
  79. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrices
  80. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#dimensions
  81. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  82. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  83. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  84. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrix-transpose
  85. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrix-multiplication
  86. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#test-yourself
  87. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#numpy
  88. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  89. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#broadcasting
  90. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html
  91. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#links
  92. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#screenshots
  93. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#license
  94. https://ml-cheatsheet.readthedocs.io/en/latest/statistics.html
  95. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html
  96. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#algebra
  97. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#calculus
  98. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#linear-algebra
  99. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#id203
 100. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#set-theory
 101. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#statistics
 102. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html
 103. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#neural-network
 104. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#neuron
 105. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#synapse
 106. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#weights
 107. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#bias
 108. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#layers
 109. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#weighted-input
 110. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#activation-functions
 111. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#loss-functions
 112. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#optimization-algorithms
 113. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html
 114. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#simple-network
 115. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#steps
 116. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#code
 117. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#larger-network
 118. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#architecture
 119. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#weight-initialization
 120. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#bias-terms
 121. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#working-with-matrices
 122. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#dynamic-resizing
 123. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#refactoring-our-code
 124. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#final-result
 125. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html
 126. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#chain-rule-refresher
 127. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#applying-the-chain-rule
 128. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#saving-work-with-memoization
 129. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#code-example
 130. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
 131. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear
 132. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu
 133. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu
 134. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu
 135. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid
 136. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh
 137. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax
 138. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html
 139. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#batchnorm
 140. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#convolution
 141. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#dropout
 142. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#linear
 143. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#lstm
 144. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#pooling
 145. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#id56
 146. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
 147. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-id178
 148. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#hinge
 149. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#huber
 150. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#kullback-leibler
 151. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mae-l1
 152. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse-l2
 153. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html
 154. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adadelta
 155. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adagrad
 156. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adam
 157. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#conjugate-gradients
 158. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#bfgs
 159. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum
 160. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#nesterov-momentum
 161. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#newton-s-method
 162. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#rmsprop
 163. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#sgd
 164. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html
 165. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#data-augmentation
 166. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#dropout
 167. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#early-stopping
 168. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#ensembling
 169. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#injecting-noise
 170. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#l1-id173
 171. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#l2-id173
 172. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html
 173. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#autoencoder
 174. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#id98
 175. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#gan
 176. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#mlp
 177. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#id56
 178. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#vae
 179. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html
 180. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#bayesian
 181. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#boosting
 182. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#decision-trees
 183. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#k-nearest-neighbor
 184. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#logistic-regression
 185. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#random-forests
 186. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#support-vector-machines
 187. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html
 188. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#centroid
 189. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#density
 190. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#distribution
 191. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#hierarchical
 192. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#id116
 193. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#mean-shift
 194. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html
 195. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#lasso
 196. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#linear
 197. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#ordinary-least-squares
 198. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#polynomial
 199. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#ridge
 200. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#splines
 201. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#stepwise
 202. https://ml-cheatsheet.readthedocs.io/en/latest/reinforcement_learning.html
 203. https://ml-cheatsheet.readthedocs.io/en/latest/datasets.html
 204. https://ml-cheatsheet.readthedocs.io/en/latest/libraries.html
 205. https://ml-cheatsheet.readthedocs.io/en/latest/papers.html
 206. https://ml-cheatsheet.readthedocs.io/en/latest/other_content.html
 207. https://ml-cheatsheet.readthedocs.io/en/latest/contribute.html
 208. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
 209. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
 210. https://github.com/bfortuner/ml-cheatsheet/blob/master/docs/linear_regression.rst
 211. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#linear-regression
 212. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#introduction
 213. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simple-regression
 214. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#making-predictions
 215. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#cost-function
 216. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#gradient-descent
 217. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#training
 218. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#model-evaluation
 219. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#summary
 220. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multivariable-regression
 221. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#growing-complexity
 222. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#id172
 223. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multiple-linear-regression-predict
 224. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#initialize-weights
 225. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 226. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 227. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simplifying-with-matrices
 228. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#bias-term
 229. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 230. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 231. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#introduction
 232. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 233. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simple-regression
 234. http://www-bcf.usc.edu/~gareth/isl/advertising.csv
 235. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 236. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#making-predictions
 237. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 238. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#cost-function
 239. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
 240. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse
 241. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 242. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#gradient-descent
 243. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
 244. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html#glossary-parameters
 245. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#chain-rule
 246. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html#glossary-learning-rate
 247. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 248. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#training
 249. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html#glossary-hyperparameters
 250. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 251. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#model-evaluation
 252. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 253. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#summary
 254. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 0
 255. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multivariable-regression
 256. http://www-bcf.usc.edu/~gareth/isl/advertising.csv
 257. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 1
 258. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#growing-complexity
 259. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 2
 260. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#id172
 261. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html
 262. https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html
 263. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 3
 264. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multiple-linear-regression-predict
 265. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 4
 266. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#initialize-weights
 267. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 5
 268. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 269. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 6
 270. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 271. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#chain-rule
 272. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 7
 273. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simplifying-with-matrices
 274. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 8
 275. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#bias-term
 276. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html#glossary-bias-term
 277. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 9
 278. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
 279. https://en.wikipedia.org/wiki/linear_regression
 280. http://www.holehouse.org/mlclass/04_linear_regression_with_multiple_variables.html
 281. http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning
 282. http://people.duke.edu/~rnau/regintro.htm
 283. https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression
 284. https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms
 285. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
 286. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
 287. http://sphinx-doc.org/
 288. https://github.com/rtfd/sphinx_rtd_theme
 289. https://readthedocs.org/
 290. https://ml-cheatsheet.readthedocs.io/en/latest/
 291. https://readthedocs.org/projects/ml-cheatsheet/downloads/pdf/latest/
 292. https://readthedocs.org/projects/ml-cheatsheet/downloads/htmlzip/latest/
 293. https://readthedocs.org/projects/ml-cheatsheet/downloads/epub/latest/
 294. https://readthedocs.org/projects/ml-cheatsheet/?fromdocs=ml-cheatsheet
 295. https://readthedocs.org/builds/ml-cheatsheet/?fromdocs=ml-cheatsheet
 296. http://www.readthedocs.org/
