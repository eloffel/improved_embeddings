    #[1]index [2]search [3]glossary [4]id119

   [5]ml cheatsheet
   ____________________

   basics
     * [6]id75
          + [7]introduction
          + [8]simple regression
               o [9]making predictions
               o [10]cost function
               o [11]id119
               o [12]training
               o [13]model evaluation
               o [14]summary
          + [15]multivariable regression
               o [16]growing complexity
               o [17]id172
               o [18]making predictions
               o [19]initialize weights
               o [20]cost function
               o [21]id119
               o [22]simplifying with matrices
               o [23]bias term
               o [24]model evaluation
     * [25]id119
          + [26]introduction
          + [27]learning rate
          + [28]cost function
          + [29]step-by-step
     * [30]id28
          + [31]introduction
               o [32]comparison to id75
               o [33]types of id28
          + [34]binary id28
               o [35]sigmoid activation
               o [36]decision boundary
               o [37]making predictions
               o [38]cost function
               o [39]id119
               o [40]mapping probabilities to classes
               o [41]training
               o [42]model evaluation
          + [43]multiclass id28
               o [44]procedure
               o [45]softmax activation
               o [46]scikit-learn example
     * [47]glossary

   math
     * [48]calculus
          + [49]introduction
          + [50]derivatives
               o [51]geometric definition
               o [52]taking the derivative
               o [53]step-by-step
               o [54]machine learning use cases
          + [55]chain rule
               o [56]how it works
               o [57]step-by-step
               o [58]multiple functions
          + [59]gradients
               o [60]partial derivatives
               o [61]step-by-step
               o [62]directional derivatives
               o [63]useful properties
          + [64]integrals
               o [65]computing integrals
               o [66]applications of integration
                    # [67]computing probabilities
                    # [68]expected value
                    # [69]variance
     * [70]id202
          + [71]vectors
               o [72]notation
               o [73]vectors in geometry
               o [74]scalar operations
               o [75]elementwise operations
               o [76]dot product
               o [77]hadamard product
               o [78]vector fields
          + [79]matrices
               o [80]dimensions
               o [81]scalar operations
               o [82]elementwise operations
               o [83]hadamard product
               o [84]matrix transpose
               o [85]id127
               o [86]test yourself
          + [87]numpy
               o [88]dot product
               o [89]broadcasting
     * [90]id203 (todo)
          + [91]links
          + [92]screenshots
          + [93]license
     * [94]statistics (todo)
     * [95]notation
          + [96]algebra
          + [97]calculus
          + [98]id202
          + [99]id203
          + [100]set theory
          + [101]statistics

   neural networks
     * [102]concepts
          + [103]neural network
          + [104]neuron
          + [105]synapse
          + [106]weights
          + [107]bias
          + [108]layers
          + [109]weighted input
          + [110]id180
          + [111]id168s
          + [112]optimization algorithms
     * [113]forwardpropagation
          + [114]simple network
               o [115]steps
               o [116]code
          + [117]larger network
               o [118]architecture
               o [119]weight initialization
               o [120]bias terms
               o [121]working with matrices
               o [122]dynamic resizing
               o [123]refactoring our code
               o [124]final result
     * [125]id26
          + [126]chain rule refresher
          + [127]applying the chain rule
          + [128]saving work with memoization
          + [129]code example
     * [130]id180
          + [131]linear
          + [132]elu
          + [133]relu
          + [134]leakyrelu
          + [135]sigmoid
          + [136]tanh
          + [137]softmax
     * [138]layers
          + [139]batchnorm
          + [140]convolution
          + [141]dropout
          + [142]linear
          + [143]lstm
          + [144]pooling
          + [145]id56
     * [146]id168s
          + [147]cross-id178
          + [148]hinge
          + [149]huber
          + [150]kullback-leibler
          + [151]mae (l1)
          + [152]mse (l2)
     * [153]optimizers
          + [154]adadelta
          + [155]adagrad
          + [156]adam
          + [157]conjugate gradients
          + [158]bfgs
          + [159]momentum
          + [160]nesterov momentum
          + [161]newton   s method
          + [162]rmsprop
          + [163]sgd
     * [164]id173
          + [165]data augmentation
          + [166]dropout
          + [167]early stopping
          + [168]ensembling
          + [169]injecting noise
          + [170]l1 id173
          + [171]l2 id173
     * [172]architectures
          + [173]autoencoder
          + [174]id98
          + [175]gan
          + [176]mlp
          + [177]id56
          + [178]vae

   algorithms (todo)
     * [179]classification
          + [180]bayesian
          + [181]boosting
          + [182]id90
          + [183]k-nearest neighbor
          + [184]id28
          + [185]id79s
          + [186]support vector machines
     * [187]id91
          + [188]centroid
          + [189]density
          + [190]distribution
          + [191]hierarchical
          + [192]id116
          + [193]mean shift
     * [194]regression
          + [195]lasso
          + [196]linear
          + [197]ordinary least squares
          + [198]polynomial
          + [199]ridge
          + [200]splines
          + [201]stepwise
     * [202]id23

   resources
     * [203]datasets
     * [204]libraries
     * [205]papers
     * [206]other

   contributing
     * [207]how to contribute

   [208]ml cheatsheet
     * [209]docs   
     * id28
     * [210]edit on github
     __________________________________________________________________

id28[211]  

     * [212]introduction
          + [213]comparison to id75
          + [214]types of id28
     * [215]binary id28
          + [216]sigmoid activation
          + [217]decision boundary
          + [218]making predictions
          + [219]cost function
          + [220]id119
          + [221]mapping probabilities to classes
          + [222]training
          + [223]model evaluation
     * [224]multiclass id28
          + [225]procedure
          + [226]softmax activation
          + [227]scikit-learn example

[228]introduction[229]  

   id28 is a classification algorithm used to assign
   observations to a discrete set of classes. unlike id75
   which outputs continuous number values, id28 transforms
   its output using the logistic sigmoid function to return a id203
   value which can then be mapped to two or more discrete classes.

[230]comparison to id75[231]  

   given data on time spent studying and exam scores. [232]linear
   regression and id28 can predict different things:

     * id75 could help us predict the student   s test score on
       a scale of 0 - 100. id75 predictions are continuous
       (numbers in a range).
     * id28 could help use predict whether the student
       passed or failed. id28 predictions are discrete
       (only specific values or categories are allowed). we can also view
       id203 scores underlying the model   s classifications.

[233]types of id28[234]  

     * binary (pass/fail)
     * multi (cats, dogs, sheep)
     * ordinal (low, medium, high)

[235]binary id28[236]  

   say we   re given [237]data on student exam results and our goal is to
   predict whether a student will pass or fail based on number of hours
   slept and hours spent studying. we have two features (hours slept,
   hours studied) and two classes: passed (1) and failed (0).
   studied slept passed
   4.85    9.63  1
   8.62    3.23  0
   5.43    8.23  1
   9.21    6.34  0

   graphically we could represent our data with a scatter plot.
   _images/logistic_regression_exam_scores_scatter.png

[238]sigmoid activation[239]  

   in order to map predicted values to probabilities, we use the
   [240]sigmoid function. the function maps any real value into another
   value between 0 and 1. in machine learning, we use sigmoid to map
   predictions to probabilities.

   math
   \[s(z) = \frac{1} {1 + e^{-z}}\]

   note
     * \(s(z)\) = output between 0 and 1 (id203 estimate)
     * \(z\) = input to the function (your algorithm   s prediction e.g. mx
       + b)
     * \(e\) = base of natural log

   graph
   _images/sigmoid.png

   code

[241]decision boundary[242]  

   our current prediction function returns a id203 score between 0
   and 1. in order to map this to a discrete class (true/false, cat/dog),
   we select a threshold value or tipping point above which we will
   classify values into class 1 and below which we classify values into
   class 2.
   \[\begin{split}p \geq 0.5, class=1 \\ p < 0.5, class=0\end{split}\]

   for example, if our threshold was .5 and our prediction function
   returned .7, we would classify this observation as positive. if our
   prediction was .2 we would classify the observation as negative. for
   id28 with multiple classes we could select the class
   with the highest predicted id203.
   _images/logistic_regression_sigmoid_w_threshold.png

[243]making predictions[244]  

   using our knowledge of sigmoid functions and decision boundaries, we
   can now write a prediction function. a prediction function in logistic
   regression returns the id203 of our observation being positive,
   true, or    yes   . we call this class 1 and its notation is
   \(p(class=1)\). as the id203 gets closer to 1, our model is more
   confident that the observation is in class 1.

   math

   let   s use the same [245]multiple id75 equation from our
   id75 tutorial.
   \[z = w_0 + w_1 studied + w_2 slept\]

   this time however we will transform the output using the sigmoid
   function to return a id203 value between 0 and 1.
   \[p(class=1) = \frac{1} {1 + e^{-z}}\]

   if the model returns .4 it believes there is only a 40% chance of
   passing. if our decision boundary was .5, we would categorize this
   observation as    fail.      

   code

   we wrap the sigmoid function over the same prediction function we used
   in [246]multiple id75
def predict(features, weights):
  '''
  returns 1d array of probabilities
  that the class label == 1
  '''
  z = np.dot(features, weights)
  return sigmoid(z)

[247]cost function[248]  

   unfortunately we can   t (or at least shouldn   t) use the same cost
   function [249]mse (l2) as we did for id75. why? there is a
   great math explanation in chapter 3 of michael neilson   s deep learning
   book [250][5], but for now i   ll simply say it   s because our prediction
   function is non-linear (due to sigmoid transform). squaring this
   prediction as we do in mse results in a non-convex function with many
   local minimums. if our cost function has many local minimums, gradient
   descent may not find the optimal global minimum.

   math

   instead of mean squared error, we use a cost function called
   [251]cross-id178, also known as log loss. cross-id178 loss can be
   divided into two separate cost functions: one for \(y=1\) and one for
   \(y=0\).
   _images/ng_cost_function_logistic.png

   the benefits of taking the logarithm reveal themselves when you look at
   the cost function graphs for y=1 and y=0. these smooth monotonic
   functions [252][7] (always increasing or always decreasing) make it
   easy to calculate the gradient and minimize cost. image from andrew
   ng   s slides on id28 [253][1].
   _images/y1andy2_logistic_function.png

   the key thing to note is the cost function penalizes confident and
   wrong predictions more than it rewards confident and right predictions!
   the corollary is increasing prediction accuracy (closer to 0 or 1) has
   diminishing returns on reducing cost due to the logistic nature of our
   cost function.

   above functions compressed into one
   _images/logistic_cost_function_joined.png

   multiplying by \(y\) and \((1-y)\) in the above equation is a sneaky
   trick that let   s us use the same equation to solve for both y=1 and y=0
   cases. if y=0, the first side cancels out. if y=1, the second side
   cancels out. in both cases we only perform the operation we need to
   perform.

   vectorized cost function
   _images/logistic_cost_function_vectorized.png

   code
def cost_function(features, labels, weights):
    '''
    using mean absolute error

    features:(100,3)
    labels: (100,1)
    weights:(3,1)
    returns 1d matrix of predictions
    cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labe
ls)
    '''
    observations = len(labels)

    predictions = predict(features, weights)

    #take the error when label=1
    class1_cost = -labels*np.log(predictions)

    #take the error when label=0
    class2_cost = (1-labels)*np.log(1-predictions)

    #take the sum of both costs
    cost = class1_cost + class2_cost

    #take the average cost
    cost = cost.sum() / observations

    return cost

[254]id119[255]  

   to minimize our cost, we use [256]id119 just like before in
   [257]id75. there are other more sophisticated optimization
   algorithms out there such as conjugate gradient like [258]bfgs, but you
   don   t have to worry about these. machine learning libraries like
   scikit-learn hide their implementations so you can focus on more
   interesting things!

   math

   one of the neat properties of the sigmoid function is its derivative is
   easy to calculate. if you   re curious, there is a good walk-through
   derivation on stack overflow [259][6]. michael neilson also covers the
   topic in chapter 3 of his book.
   \[\begin{align} s'(z) & = s(z)(1 - s(z)) \end{align}\]

   which leads to an equally beautiful and convenient cost function
   derivative:
   \[c' = x(s(z) - y)\]

   note
     * \(c'\) is the derivative of cost with respect to weights
     * \(y\) is the actual class label (0 or 1)
     * \(s(z)\) is your model   s prediction
     * \(x\) is your feature or feature vector.

   notice how this gradient is the same as the [260]mse (l2) gradient, the
   only difference is the hypothesis function.

   pseudocode
repeat {

  1. calculate gradient average
  2. multiply by learning rate
  3. subtract from weights

}

   code
def update_weights(features, labels, weights, lr):
    '''
    vectorized id119

    features:(200, 3)
    labels: (200, 1)
    weights:(3, 1)
    '''
    n = len(features)

    #1 - get predictions
    predictions = predict(features, weights)

    #2 transpose features from (200, 3) to (3, 200)
    # so we can multiply w the (200,1)  cost matrix.
    # returns a (3,1) matrix holding 3 partial derivatives --
    # one for each feature -- representing the aggregate
    # slope of the cost function across all observations
    gradient = np.dot(features.t,  predictions - labels)

    #3 take the average cost derivative for each feature
    gradient /= n

    #4 - multiply the gradient by our learning rate
    gradient *= lr

    #5 - subtract from our weights to minimize cost
    weights -= gradient

    return weights

[261]mapping probabilities to classes[262]  

   the final step is assign class labels (0 or 1) to our predicted
   probabilities.

   decision boundary
def decision_boundary(prob):
  return 1 if prob >= .5 else 0

   convert probabilities to classes
def classify(predictions):
  '''
  input  - n element array of predictions between 0 and 1
  output - n element array of 0s (false) and 1s (true)
  '''
  decision_boundary = np.vectorize(decision_boundary)
  return decision_boundary(predictions).flatten()

   example output
probabilities = [ 0.967, 0.448, 0.015, 0.780, 0.978, 0.004]
classifications = [1, 0, 0, 1, 1, 0]

[263]training[264]  

   our training code is the same as we used for [265]id75.
def train(features, labels, weights, lr, iters):
    cost_history = []

    for i in range(iters):
        weights = update_weights(features, labels, weights, lr)

        #calculate error for auditing purposes
        cost = cost_function(features, labels, weights)
        cost_history.append(cost)

        # log progress
        if i % 1000 == 0:
            print "iter: "+str(i) + " cost: "+str(cost)

    return weights, cost_history

[266]model evaluation[267]  

   if our model is working, we should see our cost decrease after every
   iteration.
iter: 0 cost: 0.635
iter: 1000 cost: 0.302
iter: 2000 cost: 0.264

   final cost: 0.2487. final weights: [-8.197, .921, .738]

   cost history
   _images/logistic_regression_loss_history.png

   accuracy

   [268]accuracy measures how correct our predictions were. in this case
   we simply compare predicted labels to true labels and divide by the
   total.
def accuracy(predicted_labels, actual_labels):
    diff = predicted_labels - actual_labels
    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))

   decision boundary

   another helpful technique is to plot the decision boundary on top of
   our predictions to see how our labels compare to the actual labels.
   this involves plotting our predicted probabilities and coloring them
   with their true labels.
   _images/logistic_regression_final_decision_boundary.png

   code to plot the decision boundary
def plot_decision_boundary(trues, falses):
    fig = plt.figure()
    ax = fig.add_subplot(111)

    no_of_preds = len(trues) + len(falses)

    ax.scatter([i for i in range(len(trues))], trues, s=25, c='b', marker="o", l
abel='trues')
    ax.scatter([i for i in range(len(falses))], falses, s=25, c='r', marker="s",
 label='falses')

    plt.legend(loc='upper right');
    ax.set_title("decision boundary")
    ax.set_xlabel('n/2')
    ax.set_ylabel('predicted id203')
    plt.axhline(.5, color='black')
    plt.show()

[269]multiclass id28[270]  

   instead of \(y = {0,1}\) we will expand our definition so that \(y =
   {0,1...n}\). basically we re-run binary classification multiple times,
   once for each class.

[271]procedure[272]  

    1. divide the problem into n+1 binary classification problems (+1
       because the index starts at 0?).
    2. for each class   
    3. predict the id203 the observations are in that single class.
    4. prediction = <math>max(id203 of the classes)

   for each sub-problem, we select one class (yes) and lump all the others
   into a second class (no). then we take the class with the highest
   predicted value.

[273]softmax activation[274]  

   something about softmax here   

[275]scikit-learn example[276]  

   let   s compare our performance to the logisticregression model provided
   by scikit-learn [277][8].
import sklearn
from sklearn.linear_model import logisticregression
from sklearn.cross_validation import train_test_split

# normalize grades to values between 0 and 1 for more efficient computation
normalized_range = sklearn.preprocessing.minmaxscaler(feature_range=(-1,1))

# extract features + labels
labels.shape =  (100,) #scikit expects this
features = normalized_range.fit_transform(features)

# create test/train
features_train,features_test,labels_train,labels_test = train_test_split(feature
s,labels,test_size=0.4)

# scikit id28
scikit_log_reg = logisticregression()
scikit_log_reg.fit(features_train,labels_train)

#score is mean accuracy
scikit_score = clf.score(features_test,labels_test)
print 'scikit score: ', scikit_score

#our mean accuracy
observations, features, labels, weights = run()
probabilities = predict(features, weights).flatten()
classifications = classifier(probabilities)
our_acc = accuracy(classifications,labels.flatten())
print 'our score: ',our_acc

   scikit score: 0.88. our score: 0.89

   references
   [278][1]
   [279]http://www.holehouse.org/mlclass/06_logistic_regression.html
   [2]
   [280]http://machinelearningmastery.com/logistic-regression-tutorial-for
   -machine-learning
   [3]
   [281]https://scilab.io/machine-learning-logistic-regression-tutorial/
   [4]
   [282]https://github.com/perborgen/logisticregression/blob/master/logist
   ic.py
   [283][5] [284]http://neuralnetworksanddeeplearning.com/chap3.html
   [285][6]
   [286]http://math.stackexchange.com/questions/78575/derivative-of-sigmoi
   d-function-sigma-x-frac11e-x
   [287][7] [288]https://en.wikipedia.org/wiki/monotoniconotonic_function
   [289][8]
   [290]http://scikit-learn.org/stable/modules/linear_model.html#logistic-
   regression>

   [291]next [292]previous
     __________________________________________________________________

      copyright 2017 revision 5f00adef.
   built with [293]sphinx using a [294]theme provided by [295]read the
   docs.

   read the docs v: latest

   versions
          [296]latest

   downloads
          [297]pdf
          [298]htmlzip
          [299]epub

   on read the docs
          [300]project home
          [301]builds
     __________________________________________________________________

   free document hosting provided by [302]read the docs.

references

   1. https://ml-cheatsheet.readthedocs.io/en/latest/genindex.html
   2. https://ml-cheatsheet.readthedocs.io/en/latest/search.html
   3. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html
   4. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
   5. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
   6. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html
   7. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#introduction
   8. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simple-regression
   9. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#making-predictions
  10. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#cost-function
  11. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#gradient-descent
  12. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#training
  13. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#model-evaluation
  14. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#summary
  15. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multivariable-regression
  16. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#growing-complexity
  17. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#id172
  18. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multiple-linear-regression-predict
  19. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#initialize-weights
  20. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  21. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  22. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simplifying-with-matrices
  23. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#bias-term
  24. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  25. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
  26. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#introduction
  27. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#learning-rate
  28. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#cost-function
  29. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#step-by-step
  30. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html
  31. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#introduction
  32. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#comparison-to-linear-regression
  33. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#types-of-logistic-regression
  34. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#binary-logistic-regression
  35. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#sigmoid-activation
  36. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#decision-boundary
  37. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#making-predictions
  38. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function
  39. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent
  40. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#mapping-probabilities-to-classes
  41. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training
  42. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#model-evaluation
  43. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#multiclass-logistic-regression
  44. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#procedure
  45. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#softmax-activation
  46. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#scikit-learn-example
  47. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html
  48. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html
  49. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#introduction
  50. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#derivatives
  51. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#geometric-definition
  52. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#taking-the-derivative
  53. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#step-by-step
  54. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#machine-learning-use-cases
  55. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#chain-rule
  56. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#how-it-works
  57. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
  58. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#multiple-functions
  59. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#gradients
  60. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#partial-derivatives
  61. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
  62. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#directional-derivatives
  63. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#useful-properties
  64. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#integrals
  65. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-integrals
  66. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#applications-of-integration
  67. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-probabilities
  68. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#expected-value
  69. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#variance
  70. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html
  71. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vectors
  72. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#notation
  73. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vectors-in-geometry
  74. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#scalar-operations
  75. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations
  76. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#dot-product
  77. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#hadamard-product
  78. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vector-fields
  79. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrices
  80. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#dimensions
  81. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  82. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  83. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  84. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrix-transpose
  85. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrix-multiplication
  86. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#test-yourself
  87. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#numpy
  88. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  89. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#broadcasting
  90. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html
  91. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#links
  92. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#screenshots
  93. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#license
  94. https://ml-cheatsheet.readthedocs.io/en/latest/statistics.html
  95. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html
  96. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#algebra
  97. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#calculus
  98. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#linear-algebra
  99. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#id203
 100. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#set-theory
 101. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#statistics
 102. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html
 103. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#neural-network
 104. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#neuron
 105. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#synapse
 106. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#weights
 107. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#bias
 108. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#layers
 109. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#weighted-input
 110. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#activation-functions
 111. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#loss-functions
 112. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#optimization-algorithms
 113. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html
 114. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#simple-network
 115. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#steps
 116. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#code
 117. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#larger-network
 118. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#architecture
 119. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#weight-initialization
 120. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#bias-terms
 121. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#working-with-matrices
 122. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#dynamic-resizing
 123. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#refactoring-our-code
 124. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#final-result
 125. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html
 126. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#chain-rule-refresher
 127. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#applying-the-chain-rule
 128. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#saving-work-with-memoization
 129. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#code-example
 130. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
 131. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear
 132. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu
 133. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu
 134. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu
 135. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid
 136. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh
 137. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax
 138. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html
 139. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#batchnorm
 140. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#convolution
 141. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#dropout
 142. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#linear
 143. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#lstm
 144. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#pooling
 145. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#id56
 146. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
 147. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-id178
 148. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#hinge
 149. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#huber
 150. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#kullback-leibler
 151. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mae-l1
 152. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse-l2
 153. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html
 154. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adadelta
 155. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adagrad
 156. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adam
 157. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#conjugate-gradients
 158. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#bfgs
 159. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum
 160. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#nesterov-momentum
 161. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#newton-s-method
 162. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#rmsprop
 163. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#sgd
 164. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html
 165. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#data-augmentation
 166. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#dropout
 167. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#early-stopping
 168. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#ensembling
 169. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#injecting-noise
 170. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#l1-id173
 171. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#l2-id173
 172. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html
 173. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#autoencoder
 174. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#id98
 175. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#gan
 176. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#mlp
 177. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#id56
 178. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#vae
 179. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html
 180. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#bayesian
 181. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#boosting
 182. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#decision-trees
 183. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#k-nearest-neighbor
 184. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#logistic-regression
 185. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#random-forests
 186. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#support-vector-machines
 187. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html
 188. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#centroid
 189. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#density
 190. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#distribution
 191. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#hierarchical
 192. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#id116
 193. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#mean-shift
 194. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html
 195. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#lasso
 196. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#linear
 197. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#ordinary-least-squares
 198. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#polynomial
 199. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#ridge
 200. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#splines
 201. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#stepwise
 202. https://ml-cheatsheet.readthedocs.io/en/latest/reinforcement_learning.html
 203. https://ml-cheatsheet.readthedocs.io/en/latest/datasets.html
 204. https://ml-cheatsheet.readthedocs.io/en/latest/libraries.html
 205. https://ml-cheatsheet.readthedocs.io/en/latest/papers.html
 206. https://ml-cheatsheet.readthedocs.io/en/latest/other_content.html
 207. https://ml-cheatsheet.readthedocs.io/en/latest/contribute.html
 208. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
 209. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
 210. https://github.com/bfortuner/ml-cheatsheet/blob/master/docs/logistic_regression.rst
 211. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#logistic-regression
 212. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#introduction
 213. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#comparison-to-linear-regression
 214. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#types-of-logistic-regression
 215. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#binary-logistic-regression
 216. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#sigmoid-activation
 217. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#decision-boundary
 218. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#making-predictions
 219. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function
 220. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent
 221. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#mapping-probabilities-to-classes
 222. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training
 223. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#model-evaluation
 224. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#multiclass-logistic-regression
 225. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#procedure
 226. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#softmax-activation
 227. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#scikit-learn-example
 228. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 229. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#introduction
 230. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 231. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#comparison-to-linear-regression
 232. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html
 233. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 234. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#types-of-logistic-regression
 235. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 236. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#binary-logistic-regression
 237. http://scilab.io/wp-content/uploads/2016/07/data_classification.csv
 238. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 239. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#sigmoid-activation
 240. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#activation-sigmoid
 241. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 0
 242. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#decision-boundary
 243. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 1
 244. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#making-predictions
 245. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multiple-linear-regression-predict
 246. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multiple-linear-regression-predict
 247. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 2
 248. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function
 249. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse
 250. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 251. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#loss-cross-id178
 252. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 253. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 254. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 3
 255. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent
 256. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
 257. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html
 258. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#optimizers-lbfgs
 259. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 260. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse
 261. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 4
 262. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#mapping-probabilities-to-classes
 263. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 5
 264. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training
 265. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simple-linear-regression-training
 266. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 6
 267. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#model-evaluation
 268. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html#glossary-accuracy
 269. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 7
 270. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#multiclass-logistic-regression
 271. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 8
 272. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#procedure
 273. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 9
 274. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#softmax-activation
 275. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 0
 276. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#scikit-learn-example
 277. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 278. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 279. http://www.holehouse.org/mlclass/06_logistic_regression.html
 280. http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning
 281. https://scilab.io/machine-learning-logistic-regression-tutorial/
 282. https://github.com/perborgen/logisticregression/blob/master/logistic.py
 283. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 284. http://neuralnetworksanddeeplearning.com/chap3.html
 285. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 286. http://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x
 287. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 288. https://en.wikipedia.org/wiki/monotoniconotonic_function
 289. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html# 
 290. http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
 291. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html
 292. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
 293. http://sphinx-doc.org/
 294. https://github.com/rtfd/sphinx_rtd_theme
 295. https://readthedocs.org/
 296. https://ml-cheatsheet.readthedocs.io/en/latest/
 297. https://readthedocs.org/projects/ml-cheatsheet/downloads/pdf/latest/
 298. https://readthedocs.org/projects/ml-cheatsheet/downloads/htmlzip/latest/
 299. https://readthedocs.org/projects/ml-cheatsheet/downloads/epub/latest/
 300. https://readthedocs.org/projects/ml-cheatsheet/?fromdocs=ml-cheatsheet
 301. https://readthedocs.org/builds/ml-cheatsheet/?fromdocs=ml-cheatsheet
 302. http://www.readthedocs.org/
