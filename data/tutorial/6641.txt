
   (button)
     * [1]home
     * [2]research
          + [3]publications
          + [4]alphago
          + [5]id25
          + [6]dnc
          + [7]open source
          + [8]access to science
     * [9]applied
          + [10]deepmind health
          + [11]deepmind for google
          + [12]deepmind ethics & society
     * [13]news & blog
     * [14]about us
     * [15]careers

   (button) search (button) search
   [deepmind_logo_swirl.png]

[16]research

highlighted research

     * [17]alphago
     * [18]id25
     * [19]dnc

[20]publications

[21]open source

latest research news

   [22]towards robust and verified ai: specification testing, robust
   training, and formal verification

[23]applied

[24]deepmind health

[25]deepmind for google

[26]deepmind ethics & society

latest applied news

   [27]scaling streams with google

[28]careers

     * [29]home
     * [30]news & blog
     * [31]about us
     * [32]press
     * [33]terms and conditions
     * [34]privacy policy     updated

     *
     *
     *

   [wav.2e16d0ba.fill-1100x400_zay3poq.png]

wavenet: a generative model for raw audio

   this post presents [35]wavenet, a deep generative model of raw audio
   waveforms. we show that wavenets are able to generate speech which
   mimics any human voice and which sounds more natural than the best
   existing text-to-speech systems, reducing the gap with human
   performance by over 50%.

   we also demonstrate that the same network can be used to synthesize
   other audio signals such as music, and present some striking samples of
   automatically generated piano pieces.

talking machines

   allowing people to converse with machines is a long-standing dream of
   human-computer interaction. the ability of computers to understand
   natural speech has been revolutionised in the last few years by the
   application of deep neural networks (e.g., [36]google voice search).
   however, generating speech with computers      a process usually referred
   to as [37]id133 or text-to-speech (tts)     is still largely
   based on so-called [38]concatenative tts, where a very large database
   of short speech fragments are recorded from a single speaker and then
   recombined to form complete utterances. this makes it difficult to
   modify the voice (for example switching to a different speaker, or
   altering the emphasis or emotion of their speech) without recording a
   whole new database.

   this has led to a great demand for [39]parametric tts, where all the
   information required to generate the data is stored in the parameters
   of the model, and the contents and characteristics of the speech can be
   controlled via the inputs to the model. so far, however, parametric tts
   has tended to sound less natural than concatenative. existing
   parametric models typically generate audio signals by passing their
   outputs through signal processing algorithms known as [40]vocoders.

   wavenet changes this paradigm by directly modelling the raw waveform of
   the audio signal, one sample at a time. as well as yielding more
   natural-sounding speech, using raw waveforms means that wavenet can
   model any kind of audio, including music.

wavenets

   wave animation

   researchers usually avoid modelling raw audio because it ticks so
   quickly: typically 16,000 samples per second or more, with important
   structure at many time-scales. building a completely autoregressive
   model, in which the prediction for every one of those samples is
   influenced by all previous ones (in statistics-speak, each predictive
   distribution is conditioned on all previous observations), is clearly a
   challenging task.

   however, our [41]pixelid56 and [42]pixelid98 models, published earlier
   this year, showed that it was possible to generate complex natural
   images not only one pixel at a time, but one colour-channel at a
   time, requiring thousands of predictions per image. this inspired us to
   adapt our two-dimensional pixelnets to a one-dimensional wavenet.

   architecture animation

   the above animation shows how a wavenet is structured. it is a fully
   convolutional neural network, where the convolutional layers have
   various dilation factors that allow its receptive field to grow
   exponentially with depth and cover thousands of timesteps.

   at training time, the input sequences are real waveforms recorded from
   human speakers. after training, we can sample the network to generate
   synthetic utterances. at each step during sampling a value is drawn
   from the id203 distribution computed by the network. this value
   is then fed back into the input and a new prediction for the next step
   is made. building up samples one step at a time like this is
   computationally expensive, but we have found it essential for
   generating complex, realistic-sounding audio.

   improving the state of the art

   we trained wavenet using some of google   s tts datasets so we could
   evaluate its performance. the following figure shows the quality of
   wavenets on a scale from 1 to 5, compared with google   s current best
   tts systems ([43]parametric and [44]concatenative), and with human
   speech using [45]mean opinion scores (mos). mos are a standard measure
   for subjective sound quality tests, and were obtained in blind tests
   with human subjects (from over 500 ratings on 100 test sentences). as
   we can see, wavenets reduce the gap between the state of the art and
   human-level performance by over 50% for both us english and mandarin
   chinese.

   for both chinese and english, google   s current tts systems are
   considered among the best worldwide, so improving on both with a single
   model is a major achievement.

   [mos2.width-400_t7vkkgz.png]

   here are some samples from all three systems so you can listen and
   compare yourself:

   us english:

   iframe:
   [46]https://storage.googleapis.com/deepmind-media/pixie/iframe1.html

   mandarin chinese:

   iframe:
   [47]https://storage.googleapis.com/deepmind-media/pixie/iframe2.html

   knowing what to say

   in order to use wavenet to turn text into speech, we have to tell it
   what the text is. we do this by transforming the text into a sequence
   of linguistic and phonetic features (which contain information about
   the current phoneme, syllable, word, etc.) and by feeding it into
   wavenet. this means the network   s predictions are conditioned not only
   on the previous audio samples, but also on the text we want it to say.

   if we train the network without the text sequence, it still generates
   speech, but now it has to make up what to say. as you can hear from the
   samples below, this results in a kind of babbling, where real words are
   interspersed with made-up word-like sounds:

   iframe:
   [48]https://storage.googleapis.com/deepmind-media/pixie/iframe3.html

   notice that non-speech sounds, such as breathing and mouth movements,
   are also sometimes generated by wavenet; this reflects the greater
   flexibility of a raw-audio model.

   as you can hear from these samples, a single wavenet is able to learn
   the characteristics of many different voices, male and female. to make
   sure it knew which voice to use for any given utterance, we conditioned
   the network on the identity of the speaker. interestingly, we found
   that training on many speakers made it better at modelling a single
   speaker than training on that speaker alone, suggesting a form of
   id21.

   by changing the speaker identity, we can use wavenet to say the same
   thing in different voices:

   iframe:
   [49]https://storage.googleapis.com/deepmind-media/pixie/iframe4.html

   similarly, we could provide additional inputs to the model, such as
   emotions or accents, to make the speech even more diverse and
   interesting.

   making music

   since wavenets can be used to model any audio signal, we thought it
   would also be fun to try to generate music. unlike the tts experiments,
   we didn   t condition the networks on an input sequence telling it what
   to play (such as a musical score); instead, we simply let it generate
   whatever it wanted to. when we trained it on a dataset of classical
   piano music, it produced fascinating samples like the ones below:

   iframe:
   [50]https://storage.googleapis.com/deepmind-media/pixie/iframe5.html

   wavenets open up a lot of possibilities for tts, music generation and
   audio modelling in general. the fact that directly generating timestep
   per timestep with deep neural networks works at all for 16khz audio is
   really surprising, let alone that it outperforms state-of-the-art tts
   systems. we are excited to see what we can do with them next.

   for more details, take a look at our [51]paper.

   share article
     *
     *
     *
     *
     * [ ]
          + [52]linkedin
          + whatsapp
          + sms
          + [53]reddit

   authors
   thursday, 8 september 2016
     * [avdnoord.2e16d0ba.fill-80x80_fbmu1cv.png]
       a  ron van den oord
       research scientist, deepmind
     * sander dieleman
       research scientist, deepmind
     * heiga zen
       research scientist, google

   ____________________
   ____________________
   [54]show all results
   (button) close

                               [55]deepmind logo

   follow
     *
     *
     *

     * [56]research [57]research
     * [58]applied [59]applied
     * [60]news & blog [61]news & blog
     * [62]about us [63]about us
     * [64]careers [65]careers

     * [66]press
     * [67]terms and conditions
     * [68]privacy policy     updated
     * [69]modern slavery statement
     * [70]alphabet inc

      2019 deepmind technologies limited

   deepmind.com uses cookies to help give you the best possible user
   experience and to allow us to see how the site is used. by using this
   site, you agree that we can set and use these cookies. for more
   information on cookies and how to change your settings, see our
   [71]privacy policy.

references

   visible links
   1. https://deepmind.com/
   2. https://deepmind.com/research/
   3. https://deepmind.com/research/publications/
   4. https://deepmind.com/research/alphago/
   5. https://deepmind.com/research/id25/
   6. https://deepmind.com/research/dnc/
   7. https://deepmind.com/research/open-source/
   8. https://deepmind.com/research/access-science/
   9. https://deepmind.com/applied/
  10. https://deepmind.com/applied/deepmind-health/
  11. https://deepmind.com/applied/deepmind-google/
  12. https://deepmind.com/applied/deepmind-ethics-society/
  13. https://deepmind.com/blog/
  14. https://deepmind.com/about/
  15. https://deepmind.com/careers/
  16. https://deepmind.com/research/
  17. https://deepmind.com/research/alphago/
  18. https://deepmind.com/research/id25/
  19. https://deepmind.com/research/dnc/
  20. https://deepmind.com/research/publications/
  21. https://deepmind.com/research/open-source/
  22. https://deepmind.com/blog/robust-and-verified-ai/
  23. https://deepmind.com/applied/
  24. https://deepmind.com/applied/deepmind-health/
  25. https://deepmind.com/applied/deepmind-google/
  26. https://deepmind.com/applied/deepmind-ethics-society/
  27. https://deepmind.com/blog/scaling-streams-google/
  28. https://deepmind.com/careers/
  29. https://deepmind.com/
  30. https://deepmind.com/blog/
  31. https://deepmind.com/about/
  32. https://deepmind.com/press/
  33. https://deepmind.com/terms-and-conditions/
  34. https://deepmind.com/privacy-policy/
  35. https://arxiv.org/pdf/1609.03499.pdf
  36. https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html
  37. https://en.wikipedia.org/wiki/speech_synthesis
  38. https://scholar.google.com/citations?view_op=view_citation&hl=en&user=es-yrkmaaaaj&citation_for_view=es-yrkmaaaaj:u5hhmvd_uo8c
  39. https://scholar.google.com/citations?view_op=view_citation&hl=en&user=z3irvdwaaaaj&citation_for_view=z3irvdwaaaaj:d1gkvwhdpl0c
  40. https://en.wikipedia.org/wiki/vocoder
  41. https://arxiv.org/abs/1601.06759
  42. https://arxiv.org/abs/1606.05328
  43. http://research.google.com/pubs/pub45379.html
  44. http://research.google.com/pubs/pub45564.html
  45. https://en.wikipedia.org/wiki/mean_opinion_score
  46. https://storage.googleapis.com/deepmind-media/pixie/iframe1.html
  47. https://storage.googleapis.com/deepmind-media/pixie/iframe2.html
  48. https://storage.googleapis.com/deepmind-media/pixie/iframe3.html
  49. https://storage.googleapis.com/deepmind-media/pixie/iframe4.html
  50. https://storage.googleapis.com/deepmind-media/pixie/iframe5.html
  51. https://arxiv.org/pdf/1609.03499.pdf
  52. https://www.linkedin.com/sharearticle?mini=true&url=https://deepmind.com/blog/wavenet-generative-model-raw-audio/&title=wavenet: a generative model for raw audio&summary=&source=google deepmind
  53. http://www.reddit.com/submit?url=https://deepmind.com/blog/wavenet-generative-model-raw-audio/&title=wavenet: a generative model for raw audio
  54. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  55. https://deepmind.com/
  56. https://deepmind.com/research/
  57. https://deepmind.com/research/
  58. https://deepmind.com/applied/
  59. https://deepmind.com/applied/
  60. https://deepmind.com/blog/
  61. https://deepmind.com/blog/
  62. https://deepmind.com/about/
  63. https://deepmind.com/about/
  64. https://deepmind.com/careers/
  65. https://deepmind.com/careers/
  66. https://deepmind.com/press/
  67. https://deepmind.com/terms-and-conditions/
  68. https://deepmind.com/privacy-policy/
  69. https://storage.googleapis.com/deepmind-media/modern_slavery/final_201_google_modern_slavery_statement.pdf
  70. https://abc.xyz/
  71. https://deepmind.com/privacy-policy/

   hidden links:
  73. https://twitter.com/deepmindai
  74. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  75. https://plus.google.com/+deepmindai
  76. http://twitter.com/intent/tweet/?text=&url=https%3a//deepmind.com/blog/wavenet-generative-model-raw-audio/
  77. http://www.facebook.com/share.php?u=https%3a//deepmind.com/blog/wavenet-generative-model-raw-audio/&t=
  78. https://plus.google.com/share?url=https%3a//deepmind.com/blog/wavenet-generative-model-raw-audio/
  79. mailto:?subject=wavenet%3a%20a%20generative%20model%20for%20raw%20audio&body=%0d%0a%0d%0ahttps%3a//deepmind.com/blog/wavenet-generative-model-raw-audio/
  80. https://twitter.com/deepmindai
  81. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  82. https://plus.google.com/+deepmindai
