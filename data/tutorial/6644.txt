   #[1]gist [2]atom

   [3]skip to content
   ____________________

     * [4]all gists
     * [5]back to github

   [6]sign up for a github account [7]sign in

   instantly share code, notes, and snippets.

[8]@edersantana [9]edersantana/[10]catch_keras_rl.md

   last active mar 20, 2019
     * [11]star [12]145
     * [13]fork [14]59

   [15]code [16]revisions 13 [17]stars 145 [18]forks 59
   embed
   what would you like to do?
   (<script src="https://gist.github.com/edersantana/c7222daa328f0e885093.
   js"></script>)
   embed embed this gist in your website.
   (https://gist.github.com/edersantana/c7222daa328f0e885093)
   share copy sharable link for this gist.
   (https://gist.github.com/c7222daa328f0e885093.git)
   clone via https clone with git or checkout with svn using the
   repository   s web address.
   learn more about clone urls
   <script src="https:/
   [19]download zip
   keras plays catch - a single file id23 example
   [20]raw
   [21]catch_keras_rl.md

   code for [22]keras plays catch blog post

train

python qlearn.py

test

    1. generate figures

python test.py

    2. make gif

ffmpeg -i %03d.png output.gif -vf fps=1

   alternatively, check [23]cadurosar ipython notebook, there you should
   run cell 2 before cell 1.

requirements

     * prior supervised learning and keras knowledge
     * python science stack (numpy, scipy, matplotlib) - install anaconda!
     * theano or tensorflow
     * keras (last testest on commit b0303f03ff03)
     * ffmpeg (optional)

license

   this code is released under [24]mit license. (note that deep id24
   has its own patent by google)
   [25]raw
   [26]qlearn.py
   import json
   import numpy as np
   from keras.models import sequential
   from keras.layers.core import dense
   from keras.optimizers import sgd
   class catch(object):
   def __init__(self, grid_size=10):
   self.grid_size = grid_size
   self.reset()
   def _update_state(self, action):
   """
   input: action and states
   ouput: new states and reward
   """
   state = self.state
   if action == 0: # left
   action = -1
   elif action == 1: # stay
   action = 0
   else:
   action = 1 # right
   f0, f1, basket = state[0]
   new_basket = min(max(1, basket + action), self.grid_size-1)
   f0 += 1
   out = np.asarray([f0, f1, new_basket])
   out = out[np.newaxis]
   assert len(out.shape) == 2
   self.state = out
   def _draw_state(self):
   im_size = (self.grid_size,)*2
   state = self.state[0]
   canvas = np.zeros(im_size)
   canvas[state[0], state[1]] = 1 # draw fruit
   canvas[-1, state[2]-1:state[2] + 2] = 1 # draw basket
   return canvas
   def _get_reward(self):
   fruit_row, fruit_col, basket = self.state[0]
   if fruit_row == self.grid_size-1:
   if abs(fruit_col - basket) <= 1:
   return 1
   else:
   return -1
   else:
   return 0
   def _is_over(self):
   if self.state[0, 0] == self.grid_size-1:
   return true
   else:
   return false
   def observe(self):
   canvas = self._draw_state()
   return canvas.reshape((1, -1))
   def act(self, action):
   self._update_state(action)
   reward = self._get_reward()
   game_over = self._is_over()
   return self.observe(), reward, game_over
   def reset(self):
   n = np.random.randint(0, self.grid_size-1, size=1)
   m = np.random.randint(1, self.grid_size-2, size=1)
   self.state = np.asarray([0, n, m])[np.newaxis]
   class experiencereplay(object):
   def __init__(self, max_memory=100, discount=.9):
   self.max_memory = max_memory
   self.memory = list()
   self.discount = discount
   def remember(self, states, game_over):
   # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]
   self.memory.append([states, game_over])
   if len(self.memory) > self.max_memory:
   del self.memory[0]
   def get_batch(self, model, batch_size=10):
   len_memory = len(self.memory)
   num_actions = model.output_shape[-1]
   env_dim = self.memory[0][0][0].shape[1]
   inputs = np.zeros((min(len_memory, batch_size), env_dim))
   targets = np.zeros((inputs.shape[0], num_actions))
   for i, idx in enumerate(np.random.randint(0, len_memory,
   size=inputs.shape[0])):
   state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]
   game_over = self.memory[idx][1]
   inputs[i:i+1] = state_t
   # there should be no target values for actions not taken.
   # thou shalt not correct actions not taken #deep
   targets[i] = model.predict(state_t)[0]
   q_sa = np.max(model.predict(state_tp1)[0])
   if game_over: # if game_over is true
   targets[i, action_t] = reward_t
   else:
   # reward_t + gamma * max_a' q(s', a')
   targets[i, action_t] = reward_t + self.discount * q_sa
   return inputs, targets
   if __name__ == "__main__":
   # parameters
   epsilon = .1 # exploration
   num_actions = 3 # [move_left, stay, move_right]
   epoch = 1000
   max_memory = 500
   hidden_size = 100
   batch_size = 50
   grid_size = 10
   model = sequential()
   model.add(dense(hidden_size, input_shape=(grid_size**2,),
   activation='relu'))
   model.add(dense(hidden_size, activation='relu'))
   model.add(dense(num_actions))
   model.compile(sgd(lr=.2), "mse")
   # if you want to continue training from a previous model, just
   uncomment the line bellow
   # model.load_weights("model.h5")
   # define environment/game
   env = catch(grid_size)
   # initialize experience replay object
   exp_replay = experiencereplay(max_memory=max_memory)
   # train
   win_cnt = 0
   for e in range(epoch):
   loss = 0.
   env.reset()
   game_over = false
   # get initial input
   input_t = env.observe()
   while not game_over:
   input_tm1 = input_t
   # get next action
   if np.random.rand() <= epsilon:
   action = np.random.randint(0, num_actions, size=1)
   else:
   q = model.predict(input_tm1)
   action = np.argmax(q[0])
   # apply action, get rewards and new state
   input_t, reward, game_over = env.act(action)
   if reward == 1:
   win_cnt += 1
   # store experience
   exp_replay.remember([input_tm1, action, reward, input_t], game_over)
   # adapt model
   inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)
   loss += model.train_on_batch(inputs, targets)[0]
   print("epoch {:03d}/999 | loss {:.4f} | win count {}".format(e, loss,
   win_cnt))
   # save trained model weights and architecture, this will be used by the
   visualization code
   model.save_weights("model.h5", overwrite=true)
   with open("model.json", "w") as outfile:
   json.dump(model.to_json(), outfile)
   [27]raw
   [28]test.py
   import json
   import matplotlib.pyplot as plt
   import numpy as np
   from keras.models import model_from_json
   from qlearn import catch
   if __name__ == "__main__":
   # make sure this grid size matches the value used fro training
   grid_size = 10
   with open("model.json", "r") as jfile:
   model = model_from_json(json.load(jfile))
   model.load_weights("model.h5")
   model.compile("sgd", "mse")
   # define environment, game
   env = catch(grid_size)
   c = 0
   for e in range(10):
   loss = 0.
   env.reset()
   game_over = false
   # get initial input
   input_t = env.observe()
   plt.imshow(input_t.reshape((grid_size,)*2),
   interpolation='none', cmap='gray')
   plt.savefig("%03d.png" % c)
   c += 1
   while not game_over:
   input_tm1 = input_t
   # get next action
   q = model.predict(input_tm1)
   action = np.argmax(q[0])
   # apply action, get rewards and new state
   input_t, reward, game_over = env.act(action)
   plt.imshow(input_t.reshape((grid_size,)*2),
   interpolation='none', cmap='gray')
   plt.savefig("%03d.png" % c)
   c += 1
   [29]@cadurosar

this comment has been minimized.

   [30]sign in to view
   copy link (button) quote reply

[31]cadurosar commented [32]mar 19, 2016

   really nice id23 example, i made a ipython notebook
   version of the test that instead of saving the figure it refreshes
   itself, its not that good (you have to execute cell 2 before cell 1)
   but could be usefull if you want to easily see the evolution of the
   model.

   [33]https://gist.github.com/cadurosar/bd54c723c1d6335a43c8

   ps: revision 6 has some bugs like targest instead of targets
   [34]@edersantana

this comment has been minimized.

   [35]sign in to view
   copy link (button) quote reply
   owner author

[36]edersantana commented [37]mar 19, 2016

   [38]@cadurosar tkx for the contribution! i added a link to your code on
   readme.
   [39]@hashbangcoder

this comment has been minimized.

   [40]sign in to view
   copy link (button) quote reply

[41]hashbangcoder commented [42]may 7, 2016

   hi eder,

   thanks for the really useful keras example. i have a question on your
   experience replay implementation.

   the loss is calculated between the output of experience replay samples
   (lets call it oer) and calculated targets. now, the action chosen at
   oer is the exact same as the ones that were stored in the experience.
   this is implemented indirectly in line 103 targets[i, action_t] =
   reward_t + self.discount * q_sa by inserting the target in appropriate
   column. when you calculate mse loss between the targets tensor and
   network output (each of shape inputs.shape[0]*num_actions), wouldn't
   all the values in network output other than the value corresponding to
   the non-zero target add to the loss? since each target row has only one
   non-zero value.

   really hope i'm not misreading your code or being confusing.
   [43]@k3nt0

this comment has been minimized.

   [44]sign in to view
   copy link (button) quote reply

[45]k3nt0 commented [46]may 8, 2016

   hello, eder

   thanks for the nice reinforcement example. i could study about
   id23 efficiently.

   i'm not good at english, but i hope it's understable to you.

   by the way, i have an idea for more good train. i think the basket
   should wait under the fruit before it get fall to the ground. so i
   changed definition of _get_reward() like this.
def _get_reward_2nd(self):
        fruit_row, fruit_col, basket = self.state[0]
        if abs(fruit_col - basket) <= 1:
            return 1
        else:
            return -1

   after changing definition, the win_cnt soared like this,
epoch 996/999 | loss 0.0043 | win count 7166
epoch 997/999 | loss 0.0051 | win count 7173
epoch 998/999 | loss 0.0048 | win count 7182
epoch 999/999 | loss 0.0058 | win count 7186

   i'm sorry if you have already know this. i hope my opinion would be
   your help.
   [47]@tigerneil

this comment has been minimized.

   [48]sign in to view
   copy link (button) quote reply

[49]tigerneil commented [50]may 18, 2016

   thanks for sharing
   [51]@qrpike

this comment has been minimized.

   [52]sign in to view
   copy link (button) quote reply

[53]qrpike commented [54]may 24, 2016    

   edited

   copy pasted and got:
$ python qlearn.py
using theano backend.
traceback (most recent call last):
  file "qlearn.py", line 164, in <module>
    loss += model.train_on_batch(inputs, targets)[0]
indexerror: too many indices for array

   [55]@sohojoe

this comment has been minimized.

   [56]sign in to view
   copy link (button) quote reply

[57]sohojoe commented [58]may 24, 2016    

   edited

   i get the following error; running on keras (1.0.3) pythion 2.7
  file "qlearn.py", line 164, in <module>
    loss += model.train_on_batch(inputs, targets)[0]
indexerror: invalid index to scalar variable.

   i was able to fix it by removing the [0] from line 164
   loss += model.train_on_batch(inputs, targets)
   [59]@qrpike

this comment has been minimized.

   [60]sign in to view
   copy link (button) quote reply

[61]qrpike commented [62]may 25, 2016

   [63]@sohojoe sorry, i don't usually work in python. thank you for the
   solution though. works fine now
   [64]@cloudyangyy

this comment has been minimized.

   [65]sign in to view
   copy link (button) quote reply

[66]cloudyangyy commented [67]jun 6, 2016    

   edited

   i'm learning rl, thanks for your share.
   if i set grid_size =50 ,it could not work well.
   i have tryed convolutional layer,it also not work well.
   do you have any idea? thanks.
   [68]@danijar

this comment has been minimized.

   [69]sign in to view
   copy link (button) quote reply

[70]danijar commented [71]jul 17, 2016

   i think q_sa = np.max(model.predict(state_tp1)[0]) should be predicted
   by the "target network", meaning using the weights from the previous
   timestep. am i mistaken of is this missing from the code?
   [72]@luli395

this comment has been minimized.

   [73]sign in to view
   copy link (button) quote reply

[74]luli395 commented [75]dec 12, 2016    

   edited

   the code is nice and clear. in the code, the original game assumes the
   fruit falls straight down from the top, and does not move horizontally.
   i wonder what if the fruit move horizontally while falling, so i added
   few codes to make the fruit move horizontally. the result is
   interesting: the algorithm can easily learn good policy to catch the
   fruit even if the fruit move randomly along x-axis while falling as
   well.

   def _update_state(self, action):
   """
   input: action and states
   ouput: new states and reward
   """
   state = self.state
   if action == 0: # left
   action = -1
   elif action == 1: # stay
   action = 0
   else:
   action = 1 # right
   f0, f1, basket = state[0]
   new_basket = min(max(1, basket + action), self.grid_size-1)
   df1=np.random.randint(0,3,1)
   if df1==0:
   f1-=1
   elif df1==2:
   f1+=1
   else:
   pass
    if f1>self.grid_size-1:
        f1=self.grid_size-1
    if f1<0:
        f1=0

    f0 += 1

    out = np.asarray([f0, f1, new_basket])
    out = out[np.newaxis]

   [76]@wonchul-kim

this comment has been minimized.

   [77]sign in to view
   copy link (button) quote reply

[78]wonchul-kim commented [79]jan 11, 2017

   first of all, thanks for your sharing!!!!

   i tried to implement your code, but there happened error such that:
     __________________________________________________________________

   ioerror traceback (most recent call last)
   in ()
   11
   12
   ---> 13 with open("model.json", "r") as jfile:
   14 model = model_from_json(json.load(jfile))
   15 model.load_weights("model.h5")

   ioerror: [errno 2] no such file or directory: 'model.json'

   i did googling for solving this problem... but,, i failed..

   could you help me?
   [80]@arr28

this comment has been minimized.

   [81]sign in to view
   copy link (button) quote reply

[82]arr28 commented [83]feb 3, 2017

   [84]@wonchul-kim: looks like you ran test.py before running qlearn.py.
   [85]@damowerko

this comment has been minimized.

   [86]sign in to view
   copy link (button) quote reply

[87]damowerko commented [88]feb 11, 2017

   hey, so i was playing around with the code and i have a big (pun
   intended) problem with the neural network output. that is,
   model.predicts very quickly diverges to infinity (even on the first
   batch). did you ever encounter something like this or any ideas where
   this could be coming from. i am using the code to do reinforcement on a
   different game.
   [89]@suryavanshi

this comment has been minimized.

   [90]sign in to view
   copy link (button) quote reply

[91]suryavanshi commented [92]mar 24, 2017    

   edited

   how can i incrementally fine tune a trained model if i change the the
   falling speed or some other parameters of the game slightly?
   [93]@beyhangl

this comment has been minimized.

   [94]sign in to view
   copy link (button) quote reply

[95]beyhangl commented [96]apr 14, 2017

   [97]@qrpike what was the solution of your problem? you should write
   answer instead 'say thanks'!
   [98]@azhar2205

this comment has been minimized.

   [99]sign in to view
   copy link (button) quote reply

[100]azhar2205 commented [101]jun 22, 2017

   i've created a paddle ball game using deep id24 based on this
   work. thanks eder for the basics! it is really helpful for a biginner
   like me.
   [102]https://github.com/azhar2205/paddle-ball-using-dqlearn
   [103]@nadernazemi

this comment has been minimized.

   [104]sign in to view
   copy link (button) quote reply

[105]nadernazemi commented [106]jun 22, 2017

   where does one get the code for these simple games like catch ?
   thank you
   [107]@jinminghe

this comment has been minimized.

   [108]sign in to view
   copy link (button) quote reply

[109]jinminghe commented [110]jun 23, 2017

   [111]@nadernazemi
   you may want to check out [112]http://karpathy.github.io/2016/05/31/rl/
   and also [113]https://gym.openai.com/
   [114]@jannesklaas

this comment has been minimized.

   [115]sign in to view
   copy link (button) quote reply

[116]jannesklaas commented [117]jul 1, 2017

   [118]@edersantana thanks for the great and informative project!
   [119]@cadurosar thank you for contributing the notebook version!
   i made a version with more comments and explanations for teaching
   purposes. it can be run cell by cell like a walk through tutorial. it
   runs in python 2.7 as well as 3.5 and fixes some of the errors
   mentioned in the comments above. you can find it here:
   [120]https://github.com/jannesklaas/sometimes_deep_sometimes_learning/b
   lob/master/reinforcement.ipynb
   [121]@aimorerrd

this comment has been minimized.

   [122]sign in to view
   copy link (button) quote reply

[123]aimorerrd commented [124]sep 8, 2017

   thank you [125]@sohojoe commented on may 24, 2016     edited
   that worked for me, i am using python too!
   [126]@aimorerrd

this comment has been minimized.

   [127]sign in to view
   copy link (button) quote reply

[128]aimorerrd commented [129]sep 8, 2017

   thanks for the comment [130]@arr28
   [131]@shreshtha-s

this comment has been minimized.

   [132]sign in to view
   copy link (button) quote reply

[133]shreshtha-s commented [134]feb 6, 2018

   i did not understand the code. could you please explain it line by
   line? that will be of great help.
   [135]@yitzhaksp

this comment has been minimized.

   [136]sign in to view
   copy link (button) quote reply

[137]yitzhaksp commented [138]may 14, 2018

   thanks, very nice code !
   [139]@upstarsong

this comment has been minimized.

   [140]sign in to view
   copy link (button) quote reply

[141]upstarsong commented [142]sep 21, 2018

     hello, eder

     thanks for the nice reinforcement example. i could study about
     id23 efficiently.

     i'm not good at english, but i hope it's understable to you.

     by the way, i have an idea for more good train. i think the basket
     should wait under the fruit before it get fall to the ground. so i
     changed definition of _get_reward() like this.

def _get_reward_2nd(self):
        fruit_row, fruit_col, basket = self.state[0]
        if abs(fruit_col - basket) <= 1:
            return 1
        else:
            return -1

     after changing definition, the win_cnt soared like this,
epoch 996/999 | loss 0.0043 | win count 7166
epoch 997/999 | loss 0.0051 | win count 7173
epoch 998/999 | loss 0.0048 | win count 7182
epoch 999/999 | loss 0.0058 | win count 7186

     i'm sorry if you have already know this. i hope my opinion would be
     your help.

   i think there might be some misunderstanding of your code. you get more
   count because you will get reward when the fruit has not reached the
   last row but near to the basket in column. but it doesn't mean the code
   has got the better training. but i am a beginner too and i have many
   question of the code. so maybe i am wrong. i just hope it will help you
   to get better. thank you.
   [143]sign up for free to join this conversation on github. already have
   an account? [144]sign in to comment

     *    2019 github, inc.
     * [145]terms
     * [146]privacy
     * [147]security
     * [148]status
     * [149]help

     * [150]contact github
     * [151]pricing
     * [152]api
     * [153]training
     * [154]blog
     * [155]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [156]reload to refresh your
   session. you signed out in another tab or window. [157]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://gist.github.com/opensearch-gist.xml
   2. https://gist.github.com/edersantana.atom
   3. https://gist.github.com/edersantana/c7222daa328f0e885093#start-of-content
   4. https://gist.github.com/discover
   5. https://github.com/
   6. https://gist.github.com/join?source=header-gist
   7. https://gist.github.com/auth/github?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
   8. https://gist.github.com/edersantana
   9. https://gist.github.com/edersantana
  10. https://gist.github.com/edersantana/c7222daa328f0e885093
  11. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  12. https://gist.github.com/edersantana/c7222daa328f0e885093/stargazers
  13. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  14. https://gist.github.com/edersantana/c7222daa328f0e885093/forks
  15. https://gist.github.com/edersantana/c7222daa328f0e885093
  16. https://gist.github.com/edersantana/c7222daa328f0e885093/revisions
  17. https://gist.github.com/edersantana/c7222daa328f0e885093/stargazers
  18. https://gist.github.com/edersantana/c7222daa328f0e885093/forks
  19. https://gist.github.com/edersantana/c7222daa328f0e885093/archive/90a8dafa550b581abbe2d578eb10acec0fe3dcd3.zip
  20. https://gist.github.com/edersantana/c7222daa328f0e885093/raw/90a8dafa550b581abbe2d578eb10acec0fe3dcd3/catch_keras_rl.md
  21. https://gist.github.com/edersantana/c7222daa328f0e885093#file-catch_keras_rl-md
  22. https://edersantana.github.io/articles/keras_rl/
  23. https://gist.github.com/cadurosar/bd54c723c1d6335a43c8
  24. https://en.wikipedia.org/wiki/mit_license
  25. https://gist.github.com/edersantana/c7222daa328f0e885093/raw/90a8dafa550b581abbe2d578eb10acec0fe3dcd3/qlearn.py
  26. https://gist.github.com/edersantana/c7222daa328f0e885093#file-qlearn-py
  27. https://gist.github.com/edersantana/c7222daa328f0e885093/raw/90a8dafa550b581abbe2d578eb10acec0fe3dcd3/test.py
  28. https://gist.github.com/edersantana/c7222daa328f0e885093#file-test-py
  29. https://gist.github.com/cadurosar
  30. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  31. https://gist.github.com/cadurosar
  32. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1728403
  33. https://gist.github.com/cadurosar/bd54c723c1d6335a43c8
  34. https://gist.github.com/edersantana
  35. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  36. https://gist.github.com/edersantana
  37. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1728521
  38. https://github.com/cadurosar
  39. https://gist.github.com/hashbangcoder
  40. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  41. https://gist.github.com/hashbangcoder
  42. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1771268
  43. https://gist.github.com/k3nt0
  44. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  45. https://gist.github.com/k3nt0
  46. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1772092
  47. https://gist.github.com/tigerneil
  48. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  49. https://gist.github.com/tigerneil
  50. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1780262
  51. https://gist.github.com/qrpike
  52. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  53. https://gist.github.com/qrpike
  54. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1784985
  55. https://gist.github.com/sohojoe
  56. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  57. https://gist.github.com/sohojoe
  58. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1785700
  59. https://gist.github.com/qrpike
  60. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  61. https://gist.github.com/qrpike
  62. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1785957
  63. https://github.com/sohojoe
  64. https://gist.github.com/cloudyangyy
  65. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  66. https://gist.github.com/cloudyangyy
  67. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1794883
  68. https://gist.github.com/danijar
  69. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  70. https://gist.github.com/danijar
  71. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1827596
  72. https://gist.github.com/luli395
  73. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  74. https://gist.github.com/luli395
  75. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1944230
  76. https://gist.github.com/wonchul-kim
  77. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  78. https://gist.github.com/wonchul-kim
  79. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1968035
  80. https://gist.github.com/arr28
  81. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  82. https://gist.github.com/arr28
  83. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1988051
  84. https://github.com/wonchul-kim
  85. https://gist.github.com/damowerko
  86. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  87. https://gist.github.com/damowerko
  88. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-1994856
  89. https://gist.github.com/suryavanshi
  90. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  91. https://gist.github.com/suryavanshi
  92. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2036408
  93. https://gist.github.com/beyhangl
  94. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
  95. https://gist.github.com/beyhangl
  96. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2060149
  97. https://github.com/qrpike
  98. https://gist.github.com/azhar2205
  99. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 100. https://gist.github.com/azhar2205
 101. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2129963
 102. https://github.com/azhar2205/paddle-ball-using-dqlearn
 103. https://gist.github.com/nadernazemi
 104. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 105. https://gist.github.com/nadernazemi
 106. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2130337
 107. https://gist.github.com/jinminghe
 108. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 109. https://gist.github.com/jinminghe
 110. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2130775
 111. https://github.com/nadernazemi
 112. http://karpathy.github.io/2016/05/31/rl/
 113. https://gym.openai.com/
 114. https://gist.github.com/jannesklaas
 115. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 116. https://gist.github.com/jannesklaas
 117. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2137503
 118. https://github.com/edersantana
 119. https://github.com/cadurosar
 120. https://github.com/jannesklaas/sometimes_deep_sometimes_learning/blob/master/reinforcement.ipynb
 121. https://gist.github.com/aimorerrd
 122. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 123. https://gist.github.com/aimorerrd
 124. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2197556
 125. https://github.com/sohojoe
 126. https://gist.github.com/aimorerrd
 127. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 128. https://gist.github.com/aimorerrd
 129. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2197559
 130. https://github.com/arr28
 131. https://gist.github.com/shreshtha-s
 132. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 133. https://gist.github.com/shreshtha-s
 134. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2342867
 135. https://gist.github.com/yitzhaksp
 136. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 137. https://gist.github.com/yitzhaksp
 138. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2588614
 139. https://gist.github.com/upstarsong
 140. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 141. https://gist.github.com/upstarsong
 142. https://gist.github.com/edersantana/c7222daa328f0e885093#gistcomment-2712921
 143. https://gist.github.com/join?source=comment-gist
 144. https://gist.github.com/login?return_to=https://gist.github.com/edersantana/c7222daa328f0e885093
 145. https://github.com/site/terms
 146. https://github.com/site/privacy
 147. https://github.com/security
 148. https://githubstatus.com/
 149. https://help.github.com/
 150. https://github.com/contact
 151. https://github.com/pricing
 152. https://developer.github.com/
 153. https://training.github.com/
 154. https://github.blog/
 155. https://github.com/about
 156. https://gist.github.com/edersantana/c7222daa328f0e885093
 157. https://gist.github.com/edersantana/c7222daa328f0e885093

   hidden links:
 159. https://gist.github.com/
 160. https://help.github.com/articles/which-remote-url-should-i-use
 161. https://gist.github.com/edersantana/c7222daa328f0e885093#train
 162. https://gist.github.com/edersantana/c7222daa328f0e885093#test
 163. https://gist.github.com/edersantana/c7222daa328f0e885093#requirements
 164. https://gist.github.com/edersantana/c7222daa328f0e885093#license
 165. https://github.com/
