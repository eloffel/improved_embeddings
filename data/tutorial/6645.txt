   #[1]alternate

keras plays catch, a single file id23 example

   written by [2]eder santana

   get started with id23 in less than 200 lines of code
   with keras (theano or tensorflow, it   s your choice).

   keras play catch

   so you are a (supervised) machine learning practitioner that was also
   sold the hype of making your labels weaker and to the possibility of
   getting neural networks to play your favorite games. you want to do
   id23 (rl), but you find it hard to read all those
   full featured libraries just to get a feeling of what is actually
   going on.

   here we   ve got your back: we took the game engine complexities out of
   the way and show a minimal id23 example with less
   than 200 lines of code. and yes, the example does use keras, your
   favorite deep learning library!

   before i give you a link to the code make sure you read nervana   s blog
   post [3]demystifying deep id23. there you will learn
   about id24, which is one of the many ways of doing rl. also, at
   this point you already know that neural nets love mini-batches and
   there you will see what experience replay is and how to use it to get
   you them batches - even in problems where an agent only sees one sample
   of the environment state at a time.

   so here is the [4]link to our code. in that code keras plays the catch
   game, where it should catch a single pixel    fruit    using a three pixel
      basket   . the fruit falls one pixel per step and the keras network gets
   a reward of +1 if it catches the fruit and -1 otherwise. the networks
   see the entire [5]10x10 pixels grid as input and outputs [6]three
   values, each value corresponds to an action (move left, stay, move
   right). since these values represent the expected accumulated future
   reward, we just go greedy and pick the [7]action corresponding to the
   largest value.

   one thing to note though, is that this network is not quite like you in
   exotic restaurants, it doesn   t take the very same action exploiting
   what it already knows at every time, once in a while we force system to
   take a [8]random action. this would be the equivalent of you learning
   that life is more than just penang curry with fried tempeh by trial
   and error.

   in the link you will also find scripts that plays the game with no
   random actions and generates the pictures for the animation above.

   enjoy!

faq

   1) how does this id24 thing even work?

   c   mon read the blog post i just mentioned above    anyway, think like
   this: the fruit is almost hitting the ground and your model is just one
   pixel away from a    catching    position. the model will face similar
   cases many many times. if it decides to stay or move left, it will be
   punished (imagine it smelling a bunch of rotten fruits in the ground
   because it was lazy). thus, it learns to assign a small q-value (sounds
   much better than just    output of neural net   , han?) to those two
   actions whenever it sees that picture as input. but, since catching the
   fruit also gives a juicy +1 reward, the model will learn to assign a
   larger q-value to the    move right    action in that case. this is what
   minimizing the [9]reward - q-value error does.

   one step before that, there will be no reward in the next step.

   i liked how the previous phrase sounded, so i decided to give it its
   own paragraph. but, although in that case there is no juicy reward
   right after, the model can be trained using the maximum q-value of the
   future state in the next step. think about it. if you   re in the kitchen
   you know that you can just open the fridge to get food. but now you   re
   in your bedroom writing bad jokes and feel hungry. but you have this
   vague memory that going to the kitchen could help with that. you just
   go to the kitchen and there you figure how to help yourself. you have
   to learn all that by living the game. i know, being markovian is hard!
   but then the rest is just propagating these reward expectations further
   and further into the past, assigning high values for good choices and
   low values for bad choices (don   t forget that sometimes you hit those
   random choices in college so you learn the parts of life they don   t
   talk about in school). for everything else, if you believe in
   stochastic id119 then it is easy to see this actually making
   sense    i hope   

   2) how different is that from alphago?

   not much    but instead of learning q-values, alphago thought it was
   smarter to use reinforce and learn to output actions probabilities
   directly. after that, she played several games against herself, so many
   that it could later learn the id203 of winning from each
   position. using all that information, during play time she uses a
   search technique to look for possible actions that would take her to
   positions with higher id203 of winning. but she told me to
   mention here that she doesn   t search as many possibilities in the
   future as her older cousin deepblue did. she also said that she can
   play pretty well using just one gpu, the other 99 were running high
   resolution netflix series so she can catch up with human culture.

   that being said, you should be able to modify this script in 2 or 3
   days to get a reimplementation or alphago and skynet should be 4
   weeks away?

   jk

   3) your code sucks why don   t you write something better?

   [10]i   m trying   

   4) did you learn that by yourself?

   the bad parts, yes. the good things were taught to me by my friends
   evan kriminger and matthew emigh.

   [11]   full blog

   these are personal words. here you may find references to
   unconventional ideas. please, read it at your own risk or don   t read it
   at all. if you are looking for my conventional science publications,
   please visit [12]this website.

      2016 eder santana     powered by [13]wintersmith

references

   1. http://edersantana.github.io/feed.xml
   2. mailto:edersantana@ufl.edu
   3. http://www.nervanasys.com/demystifying-deep-reinforcement-learning/
   4. https://gist.github.com/edersantana/c7222daa328f0e885093
   5. https://gist.github.com/edersantana/c7222daa328f0e885093#file-qlearn-py-l34-l40
   6. https://gist.github.com/edersantana/c7222daa328f0e885093#file-qlearn-py-l122
   7. https://gist.github.com/edersantana/c7222daa328f0e885093#file-qlearn-py-l147-l148
   8. https://gist.github.com/edersantana/c7222daa328f0e885093#file-qlearn-py-l144-l145
   9. https://gist.github.com/edersantana/c7222daa328f0e885093#file-qlearn-py-l98-l106
  10. https://github.com/edersantana/x/blob/master/examples/catcher.py
  11. https://edersantana.github.io/
  12. http://cnel.ufl.edu/people/people.php?name=eder
  13. https://github.com/jnordberg/wintersmith
