   deprecated: __autoload() is deprecated, use spl_autoload_register()
   instead in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-includes/compat.php on line
   502 deprecated: function create_function() is deprecated in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/revslider/i
   ncludes/framework/functions-wordpress.class.php on line 257 deprecated:
   function create_function() is deprecated in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-includes/pomo/translations.
   php on line 208 deprecated: function create_function() is deprecated in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-includes/pomo/translations.
   php on line 208 deprecated: function create_function() is deprecated in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-includes/pomo/translations.
   php on line 208 deprecated: function create_function() is deprecated in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-includes/pomo/translations.
   php on line 208 deprecated: the each() function is deprecated. this
   message will be suppressed on further calls in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/js_composer
   /include/classes/core/class-vc-mapper.php on line 111 #[1]priorlabs   
   feed [2]priorlabs    comments feed [3]priorlabs    character-level text
   classification: id98 comments feed [4]two stream convolutional network
   for predicting social matches in linkedin-data [5]alternate
   [6]alternate

   [7]logo logo logo logo priorlabs
   (button)
     * [8]about
     * [9]blog

character-level text classification: id98

   17 oct 2017 puya sharif [10]deep learning, [11]machine learning
   notice: a non well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 118 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 119 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 118 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 119 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 118 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 119 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 118 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 119 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 118 notice: a non
   well formed numeric value encountered in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/crayon-synt
   ax-highlighter/crayon_formatter.class.php on line 119
   ________________________________________________________

   the code associated with this post can be found in
   [12]text-classification.
   ________________________________________________________

introduction

   we will be implementing a convolutional neural network in [13]keras for
   character-level text classification. the implementation will be done in
   two flavors, one with an embedding layer transforming the input from
   70-dimensional one-hot encoded input arrays down to 32-dimensional
   real-valued arrays, and one without the embedding. the setup with
   follow the steps and architecture outlined in [14]character-level
   convolutional networks for text classification, but as a shallower
   implementation, consisting of three convolutional layers. the code in
   the [15]associated repository can easily be transformed for deeper
   implementations, although parameter tuning will pose a challenge beyond
   the scope of this post.

notable libraries

    1. keras (tensorflow backend)
    2. pickle
    3. pandas
    4. numpy

data

   the code is general enough to be used with a wide range datasets, but
   the current post, we will be using the rotten tomatoes movie
   review (included in the repo). the dataset contains 5000 movie reviews,
   half of which are of positive sentiment and half of which are of
   negative sentiment (two-class problem). the task at hand is to classify
   the input strings into one of the two classes, feeding the network
   arrays of arrays each representing a character from the review.

preprocessing

   all preprocessing steps are summarized in one function    char_preproc   ,
   which in turn calls a set of other functions (check the code repo for
   details).
    1. clean text strings (remove all characters except letters, numbers
       and basic punctuation, and make into lower-case).
    2. transform each text string to an array of characters.
    3. tokenize (transform each character to an integer reference to a
       vocabulary of 69 unique character entries, represented by integers
       1 to 69).
    4. pad all reviews into the same length (add integer 0   s to the end
       each array, resulting in each array being exactly 250 elements
       long. in the rare case of a review containing more than 250
       letters, truncate to 250).

   for the case where we use an embedding layer, the preprocessing ends
   here. it should be emphasized though, that the addition of an embedding
   layer was just experimental and i never managed to make it add to the
   performance or to reduce training times (slightly better convergence),
   but with right parameter tuning and/or with pre-trained embeddings, one
   might be able to at least reduce training times by feeding a smaller
   representation to the convolutional layers.

   for the default case of running the setup by feeding the input directly
   into the first convolutional layer, we one-hot encode the data.

loading data and running preprocessing steps

   in order to not having to run the preprocessing every time, we use the
   function    load_preprocessed_data    as our point of entry and
   conveniently control the saving and loading of preprocessed data.
   python

   def load_processed_data(load=true, binarize=false):_________
       table = none____________________________________________
   ____________________________________________________________
       if os.path.isfile('data/processed/data-ready.pkl') and l
           print("data exists - loading")______________________
   ____________________________________________________________
           with open('data/processed/data-ready.pkl', 'rb') as 
               data = pickle.load(file)________________________
       else:___________________________________________________
           print("reading raw data and preprocessing..")_______
           table = pd.read_csv('data/rt-polarity.csv')_________
           data = char_preproc(table.text, table.label, 70, bin
   ____________________________________________________________
           with open('data/processed/data-ready.pkl', 'wb') as 
               pickle.dump(data, file)_________________________
   ____________________________________________________________
       return (data, table)____________________________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18

   def load_processed_data(load=true, binarize=false):
       table = none

       if os.path.isfile('data/processed/data-ready.pkl') and load:
           print("data exists - loading")

           with open('data/processed/data-ready.pkl', 'rb') as file:
               data = pickle.load(file)
       else:
           print("reading raw data and preprocessing..")
           table = pd.read_csv('data/rt-polarity.csv')
           data = char_preproc(table.text, table.label, 70, binarize)

           with open('data/processed/data-ready.pkl', 'wb') as file:
               pickle.dump(data, file)

       return (data, table)

   python

   def char_preproc(x, y, vocab_len, binarize=false):__________
       # -----------------------------_________________________
       # preproc x's------------------_________________________
   ____________________________________________________________
       # cleanup_______________________________________________
       x = cleanup_col(x, numbers=true)________________________
       # split in arrays of characters_________________________
       char_arrs = [[x for x in y] for y in x]_________________
   ____________________________________________________________
       # tokenize______________________________________________
       tokenizer = tokenizer(char_level=true)__________________
       tokenizer.fit_on_texts(char_arrs)_______________________
   ____________________________________________________________
       # token sequences_______________________________________
       seq = tokenizer.texts_to_sequences(x)___________________
   ____________________________________________________________
       # pad to same length____________________________________
       seq = pad_sequences(seq, maxlen=250, padding='post', tru
   ____________________________________________________________
       # make to on-hot________________________________________
       if binarize:____________________________________________
           x = binarize_tokenized(seq, vocab_len)______________
       else:___________________________________________________
           x = seq_____________________________________________
   ____________________________________________________________
       # ----------------------------__________________________
       # preproce y's and return data__________________________
   ____________________________________________________________
       # one-hot encode y's____________________________________
       y = np.array([[1, 0] if x == 1 else [0, 1] for x in y])_
   ____________________________________________________________
       # generate and return final dataset_____________________
       data = dataset(x, y, shuffle=true, testsize=0.1)________
   ____________________________________________________________
       return data_____________________________________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36

   def char_preproc(x, y, vocab_len, binarize=false):
       # -----------------------------
       # preproc x's------------------

       # cleanup
       x = cleanup_col(x, numbers=true)
       # split in arrays of characters
       char_arrs = [[x for x in y] for y in x]

       # tokenize
       tokenizer = tokenizer(char_level=true)
       tokenizer.fit_on_texts(char_arrs)

       # token sequences
       seq = tokenizer.texts_to_sequences(x)

       # pad to same length
       seq = pad_sequences(seq, maxlen=250, padding='post',
   truncating='post', value=0)

       # make to on-hot
       if binarize:
           x = binarize_tokenized(seq, vocab_len)
       else:
           x = seq

       # ----------------------------
       # preproce y's and return data

       # one-hot encode y's
       y = np.array([[1, 0] if x == 1 else [0, 1] for x in y])

       # generate and return final dataset
       data = dataset(x, y, shuffle=true, testsize=0.1)

       return data

training

   the training is done on a setup with three convolutional layers (1d)
   with 8 character wide kernels. the two first layers are followed by
   pooling layers with 2 characters wide filters. one fully connected
   layer containing 1024 neurons with relu activation function feeds the
   output layer. parameters and architecture are controlled in the
   settings section in the beginning of the file.
   python

   # settings ---------------------____________________________
   # ------------------------------____________________________
   ____________________________________________________________
   embedding = true____________________________________________
   type = 'embedding' if embedding else 'standard'_____________
   modelpath ='models/char-conv-' + type + '-{epoch:02d}-{val_a
   filters = 500_______________________________________________
   lr = 0.0001 if embedding else 0.00001_______________________
   ____________________________________________________________
   conv = [____________________________________________________
       {'filters':500, 'kernel':8, 'strides':1, 'padding':'same
       {'filters':500, 'kernel':8, 'strides':1, 'padding':'same
       {'filters':500, 'kernel':8, 'strides':1, 'padding':'same
   ]___________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   # generate dataset -------------____________________________
   # ------------------------------____________________________
   ____________________________________________________________
   data, table = load_processed_data(false, not embedding)_____
   print("input shape: ", np.shape(data.x_train))______________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23

   # settings ---------------------
   # ------------------------------

   embedding = true
   type = 'embedding' if embedding else 'standard'
   modelpath ='models/char-conv-' + type +
   '-{epoch:02d}-{val_acc:.3f}-{val_loss:.3f}.hdf5'
   filters = 500
   lr = 0.0001 if embedding else 0.00001

   conv = [
       {'filters':500, 'kernel':8, 'strides':1, 'padding':'same', 'reg':
   0, 'pool':2},
       {'filters':500, 'kernel':8, 'strides':1, 'padding':'same', 'reg':
   0, 'pool':2},
       {'filters':500, 'kernel':8, 'strides':1, 'padding':'same', 'reg':
   0, 'pool':''}
   ]



   # generate dataset -------------
   # ------------------------------

   data, table = load_processed_data(false, not embedding)
   print("input shape: ", np.shape(data.x_train))

   the convolutional layers are added through a loop that allows for quick
   changes in the architecture by editing the settings.
   python

   # model architecture ---------------------------------------
   # ----------------------------------------------------------
   ____________________________________________________________
   ____________________________________________________________
   # input and embedding ----------____________________________
   # ------------------------------____________________________
   ____________________________________________________________
   if embedding:_______________________________________________
   ____________________________________________________________
       inputlayer = input(shape=(250,))________________________
       network = embedding(70, 16, input_length=250)(inputlayer
   ____________________________________________________________
   else:_______________________________________________________
       inputlayer = input(shape=(250 ,70))_____________________
       network = inputlayer____________________________________
   ____________________________________________________________
   # convolutional layers ---------____________________________
   # ------------------------------____________________________
   ____________________________________________________________
   for c in conv:______________________________________________
   ____________________________________________________________
       # conv layer____________________________________________
       network = conv1d(filters=c['filters'], kernel_size=c['ke
                        strides=c['strides'], padding=c['paddin
                        kernel_regularizer=regularizers.l2(c['r
   ____________________________________________________________
       if type(c['pool']) != int:______________________________
           continue____________________________________________
   ____________________________________________________________
       # pooling layer_________________________________________
       network = maxpooling1d(c['pool'])(network)______________
   ____________________________________________________________
   # fully connected --------------____________________________
   # ------------------------------____________________________
   network = flatten()(network)________________________________
   network = dense(1024, activation='relu')(network)___________
   network = dropout(0)(network)_______________________________
   ____________________________________________________________
   # output____________________________________________________
   ypred = dense(2, activation='softmax')(network)_____________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36
   37
   38
   39
   40
   41

   # model architecture ------------------------------------------
   # -------------------------------------------------------------


   # input and embedding ----------
   # ------------------------------

   if embedding:

       inputlayer = input(shape=(250,))
       network = embedding(70, 16, input_length=250)(inputlayer)

   else:
       inputlayer = input(shape=(250 ,70))
       network = inputlayer

   # convolutional layers ---------
   # ------------------------------

   for c in conv:

       # conv layer
       network = conv1d(filters=c['filters'], kernel_size=c['kernel'], \
                        strides=c['strides'], padding=c['padding'],
   activation='relu', \

   kernel_regularizer=regularizers.l2(c['reg']))(network)

       if type(c['pool']) != int:
           continue

       # pooling layer
       network = maxpooling1d(c['pool'])(network)

   # fully connected --------------
   # ------------------------------
   network = flatten()(network)
   network = dense(1024, activation='relu')(network)
   network = dropout(0)(network)

   # output
   ypred = dense(2, activation='softmax')(network)

   a cross id178 id168 and rmsprop optimizer are used for the
   optimization and the training is done in batches of 50.
   callbacks such as early stopping (patience of 10, stops training if no
   improvement observed in the validation accuracy after 10 epochs), model
   checkpointing and logging (tensoroard) are added for training control
   and saving of results.

   the training (current results) was done on a nvidia gtx 1080 ti gpu and
   training time was seldom above 15s per epoch.
   python

   # training -------------------------------------------------
   # ----------------------------------------------------------
   ____________________________________________________________
   ____________________________________________________________
   # callbacks --------------------____________________________
   # ------------------------------____________________________
   ____________________________________________________________
   # tensorboard_______________________________________________
   ____________________________________________________________
   tb_dir = 'logs/' + datetime.now().strftime("%y-%m-%d %h:%m:%
   ____________________________________________________________
   os.makedirs(tb_dir)_________________________________________
   tensorboard = tensorboard(log_dir=tb_dir)___________________
   ____________________________________________________________
   # early stopping and checkpoint_____________________________
   estopping = earlystopping(monitor='val_acc', patience=10)___
   checkpoint = modelcheckpoint(filepath=modelpath, save_best_o
   ____________________________________________________________
   # model-------------------------____________________________
   # ------------------------------____________________________
   ____________________________________________________________
   optimizer = rmsprop(lr=lr)__________________________________
   ____________________________________________________________
   ____________________________________________________________
   model = model(inputs=inputlayer, outputs=ypred)_____________
   model.compile(loss='categorical_crossid178',______________
                 optimizer=optimizer,__________________________
                 metrics=['acc'])______________________________
   ____________________________________________________________
   print(tb_dir)_______________________________________________
   print(model.summary())______________________________________
   ____________________________________________________________
   # fit and run ------------------____________________________
   # ------------------------------____________________________
   try:________________________________________________________
       hist = model.fit(data.x_train,__________________________
                        data.y_train,__________________________
                        validation_data=(data.x_test, data.y_te
                        epochs=500,____________________________
                        batch_size=50,_________________________
                        shuffle=false,_________________________
                        verbose=2,_____________________________
                        callbacks=[checkpoint, estopping, tenso
   ____________________________________________________________
   except keyboardinterrupt:    _______________________________
       print("training terminated by user")____________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36
   37
   38
   39
   40
   41
   42
   43
   44
   45
   46
   47

   # training ----------------------------------------------------
   # -------------------------------------------------------------


   # callbacks --------------------
   # ------------------------------

   # tensorboard

   tb_dir = 'logs/' + datetime.now().strftime("%y-%m-%d %h:%m:%s") + '_' +
   type

   os.makedirs(tb_dir)
   tensorboard = tensorboard(log_dir=tb_dir)

   # early stopping and checkpoint
   estopping = earlystopping(monitor='val_acc', patience=10)
   checkpoint = modelcheckpoint(filepath=modelpath, save_best_only=true)

   # model-------------------------
   # ------------------------------

   optimizer = rmsprop(lr=lr)


   model = model(inputs=inputlayer, outputs=ypred)
   model.compile(loss='categorical_crossid178',
                 optimizer=optimizer,
                 metrics=['acc'])

   print(tb_dir)
   print(model.summary())

   # fit and run ------------------
   # ------------------------------
   try:
       hist = model.fit(data.x_train,
                        data.y_train,
                        validation_data=(data.x_test, data.y_test),
                        epochs=500,
                        batch_size=50,
                        shuffle=false,
                        verbose=2,
                        callbacks=[checkpoint, estopping, tensorboard])

   except keyboardinterrupt:
       print("training terminated by user")


results


   validation accuracy for setup with (blue) and without (purple)
   embdedding layer.

   with minimal parameter tuning the validation set accuracy reaches 72%
   for the version without embedding layer and 70% for the version with an
   embedding layer, after about 20 epochs (the graph above is smoothed    
   leading to a shift forward in time). the addition of an embedding
   layer, in this case, leads to a slightly faster convergence, although
   the best accuracy is obtained by the version without. comparing these
   results with what can be achieved using say, a word-level convolutional
   network, they are still a bit behind (~80%), but one must keep in mind
   that we have been feeding the network character by character and with
   only three convolutional layers are still able to reach quite
   interesting results!

share this:

     * [16]click to share on twitter (opens in new window)
     * [17]click to share on facebook (opens in new window)
     * [18]click to share on google+ (opens in new window)
     *

related

   [19]two stream convolutional ...

   puya

this entry has 0 replies

leave a reply

   be the first to comment!
   notify of
   [new follow-up comments____]
   ____________________
      
   avatar

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________
   ____________________
   ____________________
   ____________________
   [20]wpdiscuz_captcha [21]refresh
   [ ] notify of new replies to this comment
   post comment
   avatar

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________
   ____________________
   ____________________
   ____________________
   [22]wpdiscuz_captcha [23]refresh
   [ ] notify of new replies to this comment
   post comment
   wpdiscuz

   ____________________ (search)

archives

     * [24]october 2017
     * [25]april 2017

categories

     * [26]deep learning
     * [27]machine learning

meta

     * [28]log in
     * [29]entries rss
     * [30]comments rss
     * [31]wordpress.org

   about

   data science in general and machine learning in particular has in the
   recent years paved the way for a revolution in how web platforms
   operate and software is designed. user behavior patterns, market data,
   sensor data and social network trends are the new currencies from which
   organizations can extract immense value by making more intelligent
   decisions, increase automation, build and use new products that were
   infeasible just years ago. priorlabs is committed to be a partner
   throughout the process of specification, research, prototyping and
   deployment.
   puya sharif, founder.

   our contacts
     * [32]info@priorlabs.com

   recent
     * [33]character-level text classification... october 17, 2017
     * [34]two stream convolutional network fo... april 6, 2017

   notice: trying to get property 'user_email' of non-object in
   /customers/0/f/6/priorlabs.com/httpd.www/wp-content/plugins/jetpack/mod
   ules/gravatar-hovercards.php on line 238

references

   visible links
   1. http://www.priorlabs.com/feed/
   2. http://www.priorlabs.com/comments/feed/
   3. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/feed/
   4. http://www.priorlabs.com/2017/04/06/two-stream-convolutional-network-for-predicting-social-matches-in-linkedin-data/
   5. http://www.priorlabs.com/wp-json/oembed/1.0/embed?url=http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/
   6. http://www.priorlabs.com/wp-json/oembed/1.0/embed?url=http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/&format=xml
   7. http://www.priorlabs.com/
   8. http://www.priorlabs.com/
   9. http://www.priorlabs.com/blog/
  10. http://www.priorlabs.com/category/deep-learning/
  11. http://www.priorlabs.com/category/machinelearning/
  12. https://github.com/puyash/text-classification
  13. https://keras.io/
  14. https://arxiv.org/pdf/1509.01626.pdf
  15. https://github.com/puyash/text-classification
  16. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/?share=twitter
  17. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/?share=facebook
  18. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/?share=google-plus-1
  19. http://www.priorlabs.com/2017/04/06/two-stream-convolutional-network-for-predicting-social-matches-in-linkedin-data/
  20. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/
  21. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/
  22. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/
  23. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/
  24. http://www.priorlabs.com/2017/10/
  25. http://www.priorlabs.com/2017/04/
  26. http://www.priorlabs.com/category/deep-learning/
  27. http://www.priorlabs.com/category/machinelearning/
  28. http://www.priorlabs.com/wp-login.php
  29. http://www.priorlabs.com/feed/
  30. http://www.priorlabs.com/comments/feed/
  31. https://wordpress.org/
  32. mailto:info@priorlabs.com
  33. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/
  34. http://www.priorlabs.com/2017/04/06/two-stream-convolutional-network-for-predicting-social-matches-in-linkedin-data/

   hidden links:
  36. javascript:;
  37. javascript:;
  38. javascript:;
  39. javascript:;
  40. javascript:;
  41. http://www.priorlabs.com/
  42. http://www.priorlabs.com/2017/10/17/character-level-text-classification-id98/
