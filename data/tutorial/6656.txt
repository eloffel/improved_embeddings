   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]artificial intelligence

                 id3 for beginners

   build a neural network that learns to generate handwritten digits.

   by [27]jon bruner[28]adit deshpande

   june 7, 2017

   id3 for beginners generative adversarial
   networks for beginners (source: [29]o'reilly)

   [30]check out the session, "adversarial machine learning in digital
   forensics," at the artificial intelligence conference in new york,
   april 15-18, 2019.

practical id3 for beginners

   [31]you can download and modify the code from this generative
   adversarial networks tutorial on github here.

   according to yann lecun,    adversarial training is the coolest thing
   since sliced bread.    sliced bread certainly never created this much
   excitement within the deep learning community. generative adversarial
   networks   or gans, for short   have dramatically sharpened the possibility
   of ai-generated content, and have drawn active research efforts since
   they were [32]first described by ian goodfellow et al. in 2014.

   gans are neural networks that learn to create synthetic data similar to
   some known input data. for instance, researchers have generated
   convincing images from photographs of everything from bedrooms to album
   covers, and they display a remarkable ability to reflect
   [33]higher-order semantic logic.

   those examples are fairly complex, but it's easy to build a gan that
   generates very simple images. in this tutorial, we'll build a gan that
   analyzes lots of images of handwritten digits and gradually learns to
   generate new images from scratch   essentially, we'll be teaching a
   neural network how to write.

   [gan-images-final-d7bdb862726f6fd928a7c859a69c3248.gif] sample images
   from the generative adversarial network that we'll build in this
   tutorial. during training, it gradually refines its ability to generate
   digits.

gan architecture

   id3 consist of two models: a generative
   model and a discriminative model.

   caption

   the discriminator model is a classifier that determines whether a given
   image looks like a real image from the dataset or like an artificially
   created image. this is basically a binary classifier that will take the
   form of a normal convolutional neural network (id98).

   the generator model takes random input values and transforms them into
   images through a deconvolutional neural network.

   over the course of many training iterations, the weights and biases in
   the discriminator and the generator are trained through
   id26. the discriminator learns to tell "real" images of
   handwritten digits apart from "fake" images created by the generator.
   at the same time, the generator uses feedback from the discriminator to
   learn how to produce convincing images that the discriminator can't
   distinguish from real images.

getting started

   we   re going to create a gan that will generate handwritten digits that
   can fool even the best classifiers (and humans too, of course). we'll
   use [34]tensorflow, a deep learning library open-sourced by google that
   makes it easy to train neural networks on gpus.

   this tutorial expects that you're already at least a little bit
   familiar with tensorflow. if you're not, we recommend reading
   [35]"hello, tensorflow!" or watching the [36]"hello, tensorflow!"
   interactive tutorial on safari before proceeding.

loading mnist data

   we need a set of real handwritten digits to give the discriminator a
   starting point in distinguishing between real and fake images. we'll
   use [37]mnist, a benchmark dataset in deep learning. it consists of
   70,000 images of handwritten digits compiled by the u.s. national
   institute of standards and technology from census bureau employees and
   high school students.

   let's start by importing tensorflow along with a couple of other
   helpful libraries. we'll also import our mnist images using a
   tensorflow convenience function called read_data_sets.
import tensorflow as tf
import numpy as np
import datetime
import matplotlib.pyplot as plt
%matplotlib inline

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("mnist_data/")

   the mnist variable we created above contains both the images and their
   labels, divided into a training set called train and a validation set
   called validation. (we won't need to worry about the labels in this
   tutorial.) we can retrieve batches of images by calling next_batch on
   mnist. let's load one image and look at it.

   the images are initially formatted as a single row of 784 pixels. we
   can reshape them into 28 x 28 pixel images and view them using pyplot.
sample_image = mnist.train.next_batch(1)[0]
print(sample_image.shape)

sample_image = sample_image.reshape([28, 28])
plt.imshow(sample_image, cmap='greys')

   if you run the cell above again, you'll see a different image from the
   mnist training set.

discriminator network

   our discriminator is a convolutional neural network that takes in an
   image of size 28 x 28 x 1 as input and returns a single scalar number
   that describes whether or not the input image is "real" or "fake"   that
   is, whether it's drawn from the set of mnist images or generated by the
   generator.

   caption

   the structure of our discriminator network is based closely on
   [38]tensorflow's sample id98 classifier model. it features two
   convolutional layers that find 5x5 pixel features, and two "fully
   connected" layers that multiply weights by every pixel in the image.

   to set up each layer, we start by creating weight and bias variables
   through [39]tf.get_variable. weights are initialized from a
   [40]truncated normal distribution, and biases are initialized at zero.

   [41]tf.nn.conv2d() is tensorflow's standard convolution function. it
   takes 4 arguments. the first is the input volume (our 28 x 28 x 1
   images in this case). the next argument is the filter/weight matrix.
   finally, you can also change the stride and padding of the convolution.
   those two values affect the dimensions of the output volume.

   if you're already comfortable with id98s, you'll recognize this as a
   simple binary classifier   nothing fancy.
def discriminator(images, reuse=false):
    if (reuse):
        tf.get_variable_scope().reuse_variables()

    # first convolutional and pool layers
    # this finds 32 different 5 x 5 pixel features
    d_w1 = tf.get_variable('d_w1', [5, 5, 1, 32], initializer=tf.truncated_norma
l_initializer(stddev=0.02))
    d_b1 = tf.get_variable('d_b1', [32], initializer=tf.constant_initializer(0))
    d1 = tf.nn.conv2d(input=images, filter=d_w1, strides=[1, 1, 1, 1], padding='
same')
    d1 = d1 + d_b1
    d1 = tf.nn.relu(d1)
    d1 = tf.nn.avg_pool(d1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='s
ame')

    # second convolutional and pool layers
    # this finds 64 different 5 x 5 pixel features
    d_w2 = tf.get_variable('d_w2', [5, 5, 32, 64], initializer=tf.truncated_norm
al_initializer(stddev=0.02))
    d_b2 = tf.get_variable('d_b2', [64], initializer=tf.constant_initializer(0))
    d2 = tf.nn.conv2d(input=d1, filter=d_w2, strides=[1, 1, 1, 1], padding='same
')
    d2 = d2 + d_b2
    d2 = tf.nn.relu(d2)
    d2 = tf.nn.avg_pool(d2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='s
ame')

    # first fully connected layer
    d_w3 = tf.get_variable('d_w3', [7 * 7 * 64, 1024], initializer=tf.truncated_
normal_initializer(stddev=0.02))
    d_b3 = tf.get_variable('d_b3', [1024], initializer=tf.constant_initializer(0
))
    d3 = tf.reshape(d2, [-1, 7 * 7 * 64])
    d3 = tf.matmul(d3, d_w3)
    d3 = d3 + d_b3
    d3 = tf.nn.relu(d3)

    # second fully connected layer
    d_w4 = tf.get_variable('d_w4', [1024, 1], initializer=tf.truncated_normal_in
itializer(stddev=0.02))
    d_b4 = tf.get_variable('d_b4', [1], initializer=tf.constant_initializer(0))
    d4 = tf.matmul(d3, d_w4) + d_b4

    # d4 contains unscaled values
    return d4

   caption

   now that we have our discriminator defined, let   s take a look at the
   generator model. we'll base the overall structure of our model on a
   simple generator published by [42]tim o'shea.

   you can think of the generator as a kind of reverse convolutional
   neural network. a typical id98 like our discriminator network transforms
   a 2- or 3-dimensional matrix of pixel values into a single id203.
   a generator, however, takes a d-dimensional vector of noise and
   upsamples it to become a 28 x 28 image. relu and batch id172
   are used to stabilize the outputs of each layer.

   in our generator network, we use three convolutional layers along with
   interpolation until a 28 x 28 pixel image is formed. (actually, as
   you'll see below, we've taken care to form 28 x 28 x 1 images; many
   tensorflow tools for dealing with images anticipate that the images
   will have some number of channels   usually 1 for greyscale images or 3
   for rgb color images.)

   at the output layer we add a [43]tf.sigmoid() activation function; this
   squeezes pixels that would appear grey toward either black or white,
   resulting in a crisper image.
def generator(z, batch_size, z_dim):
    g_w1 = tf.get_variable('g_w1', [z_dim, 3136], dtype=tf.float32, initializer=
tf.truncated_normal_initializer(stddev=0.02))
    g_b1 = tf.get_variable('g_b1', [3136], initializer=tf.truncated_normal_initi
alizer(stddev=0.02))
    g1 = tf.matmul(z, g_w1) + g_b1
    g1 = tf.reshape(g1, [-1, 56, 56, 1])
    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')
    g1 = tf.nn.relu(g1)

    # generate 50 features
    g_w2 = tf.get_variable('g_w2', [3, 3, 1, z_dim/2], dtype=tf.float32, initial
izer=tf.truncated_normal_initializer(stddev=0.02))
    g_b2 = tf.get_variable('g_b2', [z_dim/2], initializer=tf.truncated_normal_in
itializer(stddev=0.02))
    g2 = tf.nn.conv2d(g1, g_w2, strides=[1, 2, 2, 1], padding='same')
    g2 = g2 + g_b2
    g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope='bn2')
    g2 = tf.nn.relu(g2)
    g2 = tf.image.resize_images(g2, [56, 56])

    # generate 25 features
    g_w3 = tf.get_variable('g_w3', [3, 3, z_dim/2, z_dim/4], dtype=tf.float32, i
nitializer=tf.truncated_normal_initializer(stddev=0.02))
    g_b3 = tf.get_variable('g_b3', [z_dim/4], initializer=tf.truncated_normal_in
itializer(stddev=0.02))
    g3 = tf.nn.conv2d(g2, g_w3, strides=[1, 2, 2, 1], padding='same')
    g3 = g3 + g_b3
    g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='bn3')
    g3 = tf.nn.relu(g3)
    g3 = tf.image.resize_images(g3, [56, 56])

    # final convolution with one output channel
    g_w4 = tf.get_variable('g_w4', [1, 1, z_dim/4, 1], dtype=tf.float32, initial
izer=tf.truncated_normal_initializer(stddev=0.02))
    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initiali
zer(stddev=0.02))
    g4 = tf.nn.conv2d(g3, g_w4, strides=[1, 2, 2, 1], padding='same')
    g4 = g4 + g_b4
    g4 = tf.sigmoid(g4)

    # dimensions of g4: batch_size x 28 x 28 x 1
    return g4

generating a sample image

   now we   ve defined both the generator and discriminator functions. let   s
   see what a sample output from an untrained generator looks like.

   we need to open a tensorflow session and create a placeholder for the
   input to our generator. the shape of the placeholder will be none x
   z_dimensions. the none keyword means that the value can be determined
   at session runtime. we normally have none as our first dimension so
   that we can have variable batch sizes. (with a batch size of 50, the
   input to the generator would be 50 x 100). with the none keywoard, we
   don't have to specify batch_size until later.
z_dimensions = 100
z_placeholder = tf.placeholder(tf.float32, [none, z_dimensions])

   now, we create a variable (generated_image_output) that holds the
   output of the generator, and we'll also initialize the random noise
   vector that we're going to use as input. the [44]np.random.normal()
   function has three arguments. the first and second define the mean and
   standard deviation for the normal distribution (0 and 1 in our case),
   and the third defines the the shape of the vector (1 x 100).
generated_image_output = generator(z_placeholder, 1, z_dimensions)
z_batch = np.random.normal(0, 1, [1, z_dimensions])

   next, we initialize all the variables, feed our z_batch into the
   placeholder, and run the session.

   the [45]sess.run() function has two arguments. the first is called the
   "fetches" argument; it defines the value you're interested in
   computing. in our case, we want to see what the output of the generator
   is. if you look back at the last code snippet, you'll see that the
   output of the generator function is stored in generated_image_output,
   so we'll use generated_image_output for our first argument.

   the second argument takes a dictionary of inputs that are substituted
   into the graph when it runs. this is where we feed in our placeholders.
   in our example, we need to feed our z_batch variable into the
   z_placeholder that we defined earlier. as before, we'll view the image
   by reshaping it to 28 x 28 pixels and show it with pyplot.
with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    generated_image = sess.run(generated_image_output,
                                feed_dict={z_placeholder: z_batch})
    generated_image = generated_image.reshape([28, 28])
    plt.imshow(generated_image, cmap='greys')

   that looks like noise, right? now we need to train the weights and
   biases in the generator network to convert random numbers into
   recognizable digits. let's look at id168s and optimization!

training a gan

   one of the trickiest parts of building and tuning gans is that they
   have two id168s: one that encourages the generator to create
   better images, and the other that encourages the discriminator to
   distinguish generated images from real images.

   we train both the generator and the discriminator simultaneously. as
   the discriminator gets better at distinguishing real images from
   generated images, the generator is able to better tune its weights and
   biases to generate convincing images.

   here are the inputs and outputs for our networks.
tf.reset_default_graph()
batch_size = 50

z_placeholder = tf.placeholder(tf.float32, [none, z_dimensions], name='z_placeho
lder')
# z_placeholder is for feeding input noise to the generator

x_placeholder = tf.placeholder(tf.float32, shape = [none,28,28,1], name='x_place
holder')
# x_placeholder is for feeding input images to the discriminator

gz = generator(z_placeholder, batch_size, z_dimensions)
# gz holds the generated images

dx = discriminator(x_placeholder)
# dx will hold discriminator prediction probabilities
# for the real mnist images

dg = discriminator(gz, reuse=true)
# dg will hold discriminator prediction probabilities for generated images

   so, let   s first think about what we want out of our networks. the
   discriminator's goal is to correctly label real mnist images as real
   (return a higher output) and generated images as fake (return a lower
   output). we'll calculate two losses for the discriminator: one loss
   that compares dx and 1 for real images from the mnist set, as well as a
   loss that compares dg and 0 for images from the generator. we'll do
   this with tensorflow's [46]tf.nn.sigmoid_cross_id178_with_logits()
   function, which calculates the cross-id178 losses between dx and 1
   and between dg and 0.

   sigmoid_cross_id178_with_logits operates on unscaled values rather
   than id203 values from 0 to 1. take a look at the last line of
   our discriminator: there's no softmax or sigmoid layer at the end. gans
   can fail if their discriminators "saturate," or become confident enough
   to return exactly 0 when they're given a generated image; that leaves
   the discriminator without a useful gradient to descend.

   the [47]tf.reduce_mean() function takes the mean value of all of the
   components in the matrix returned by the cross-id178 function. this
   is a way of reducing the loss to a single scalar value, instead of a
   vector or matrix.
d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_id178_with_logits(dx, tf.ones
_like(dx)))
d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_id178_with_logits(dg, tf.zero
s_like(dg)))

   now let's set up the generator's id168. we want the generator
   network to create images that will fool the discriminator: the
   generator wants the discriminator to output a value close to 1 when
   it's given an image from the generator. therefore, we want to compute
   the loss between dg and 1.
g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_id178_with_logits(dg, tf.ones_like
(dg)))

   now that we have our id168s, we need to define our optimizers.
   the optimizer for the generator network needs to only update the
   generator   s weights, not those of the discriminator. likewise, when we
   train the discriminator, we want to hold the generator's weights fixed.

   in order to make this distinction, we need to create two lists of
   variables, one with the discriminator   s weights and biases, and another
   with the generator   s weights and biases. this is where naming all of
   your tensorflow variables with a thoughtful scheme can come in handy.
tvars = tf.trainable_variables()

d_vars = [var for var in tvars if 'd_' in var.name]
g_vars = [var for var in tvars if 'g_' in var.name]

print([v.name for v in d_vars])
print([v.name for v in g_vars])

   next, we specify our two optimizers. adam is usually the optimization
   algorithm of choice for gans; it utilizes adaptive learning rates and
   momentum. we call adam's minimize function and also specify the
   variables that we want it to update   the generator's weights and biases
   when we train the generator, and the discriminator's weights and biases
   when we train the discriminator.

   we're setting up two different training operations for the
   discriminator here: one that trains the discriminator on real images
   and one that trains the discrmnator on fake images. it's sometimes
   useful to use different learning rates for these two training
   operations, or to use them separately to [48]regulate learning in other
   ways.
# train the discriminator
d_trainer_fake = tf.train.adamoptimizer(0.0003).minimize(d_loss_fake, var_list=d
_vars)
d_trainer_real = tf.train.adamoptimizer(0.0003).minimize(d_loss_real, var_list=d
_vars)

# train the generator
g_trainer = tf.train.adamoptimizer(0.0001).minimize(g_loss, var_list=g_vars)

   it can be tricky to get gans to converge, and moreover they often need
   to train for a very long time. [49]tensorboard is useful for tracking
   the training process; it can graph scalar properties like losses,
   display sample images during training, and illustrate the topology of
   the neural networks.

   if you run this script on your own machine, include the cell below.
   then, in a terminal window, run tensorboard --logdir=tensorboard/ and
   open tensorboard by visiting http://localhost:6006 in your web browser.
tf.summary.scalar('generator_loss', g_loss)
tf.summary.scalar('discriminator_loss_real', d_loss_real)
tf.summary.scalar('discriminator_loss_fake', d_loss_fake)

images_for_tensorboard = generator(z_placeholder, batch_size, z_dimensions)
tf.summary.image('generated_images', images_for_tensorboard, 5)
merged = tf.summary.merge_all()
logdir = "tensorboard/" + datetime.datetime.now().strftime("%y%m%d-%h%m%s") + "/
"
writer = tf.summary.filewriter(logdir, sess.graph)

   and now we iterate. we begin by briefly giving the discriminator some
   initial training; this helps it develop a gradient that's useful to the
   generator.

   then we move on to the main training loop. when we train the generator,
   we   ll feed a random z vector into the generator and pass its output to
   the discriminator (this is the dg variable we specified earlier). the
   generator   s weights and biases will be updated in order to produce
   images that the discriminator is more likely to classify as real.

   to train the discriminator, we   ll feed it a batch of images from the
   mnist set to serve as the positive examples, and then train the
   discriminator again on generated images, using them as negative
   examples. remember that as the generator improves its output, the
   discriminator continues to learn to classify the improved generator
   images as fake.

   because it takes a long time to train a gan, we recommend not running
   this code block if you're going through this tutorial for the first
   time. instead, follow along but then run the following code block,
   which loads a pre-trained model for us to continue the tutorial.

   if you want to run this code yourself, prepare to wait: it takes about
   3 hours on a fast gpu, but could take ten times that long on a desktop
   cpu.
sess = tf.session()
sess.run(tf.global_variables_initializer())

# pre-train discriminator
for i in range(300):
    z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])
    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size
, 28, 28, 1])
    _, __, dlossreal, dlossfake = sess.run([d_trainer_real, d_trainer_fake, d_lo
ss_real, d_loss_fake],
                                           {x_placeholder: real_image_batch, z_p
laceholder: z_batch})

    if(i % 100 == 0):
        print("dlossreal:", dlossreal, "dlossfake:", dlossfake)

# train generator and discriminator together
for i in range(100000):
    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size
, 28, 28, 1])
    z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])

    # train discriminator on both real and fake images
    _, __, dlossreal, dlossfake = sess.run([d_trainer_real, d_trainer_fake, d_lo
ss_real, d_loss_fake],
                                           {x_placeholder: real_image_batch, z_p
laceholder: z_batch})

    # train generator
    z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])
    _ = sess.run(g_trainer, feed_dict={z_placeholder: z_batch})

    if i % 10 == 0:
        # update tensorboard with summary statistics
        z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])
        summary = sess.run(merged, {z_placeholder: z_batch, x_placeholder: real_
image_batch})
        writer.add_summary(summary, i)

    if i % 100 == 0:
        # every 100 iterations, show a generated image
        print("iteration:", i, "at", datetime.datetime.now())
        z_batch = np.random.normal(0, 1, size=[1, z_dimensions])
        generated_images = generator(z_placeholder, 1, z_dimensions)
        images = sess.run(generated_images, {z_placeholder: z_batch})
        plt.imshow(images[0].reshape([28, 28]), cmap='greys')
        plt.show()

        # show discriminator's estimate
        im = images[0].reshape([1, 28, 28, 1])
        result = discriminator(x_placeholder)
        estimate = sess.run(result, {x_placeholder: im})
        print("estimate:", estimate)

   because it can take so long to train a gan, we recommend that you skip
   the cell above and execute the following cell. it loads a model that
   we've already trained for several hours on a fast gpu machine, and lets
   you experiment with the output of a trained gan.
saver = tf.train.saver()
with tf.session() as sess:
    saver.restore(sess, 'pretrained-model/pretrained_gan.ckpt')
    z_batch = np.random.normal(0, 1, size=[10, z_dimensions])
    z_placeholder = tf.placeholder(tf.float32, [none, z_dimensions], name='z_pla
ceholder')
    generated_images = generator(z_placeholder, 10, z_dimensions)
    images = sess.run(generated_images, {z_placeholder: z_batch})
    for i in range(10):
        plt.imshow(images[i].reshape([28, 28]), cmap='greys')
        plt.show()

training difficulties

   gans are notoriously difficult to train. without the right
   hyperparameters, network architecture, and training procedure, the
   discriminator can overpower the generator, or vice-versa.

   in one common failure mode, the discriminator overpowers the generator,
   classifying generated images as fake with absolute certainty. when the
   discriminator responds with absolute certainty, it leaves no gradient
   for the generator to descend. this is partly why we built our
   discriminator to produce unscaled output rather than passing its output
   through a sigmoid function that would push its evaluation toward either
   0 or 1.

   in another common failure mode, known as mode collapse, the generator
   discovers and exploits some weakness in the discriminator. you can
   recognize mode collapse in your gan if it generates many very similar
   images regardless of variation in the generator input z. mode collapse
   can sometimes be corrected by "strengthening" the discriminator in some
   way   for instance, by adjusting its training rate or by reconfiguring
   its layers.

   researchers have identified a handful of [50]"gan hacks" that can be
   helpful in building stable gans.

closing thoughts

   gans have tremendous potential to reshape the digital world that we
   interact with every day. the field is still very young, and the next
   great gan discovery could be yours!

other resources

     * [51]the original gan paper by ian goodfellow and his collaborators,
       published in 2014
     * [52]a more recent tutorial by goodfellow that explains gans in
       somewhat more accessible terms
     * [53]a paper by alec radford, luke metz, and soumith chintala that
       introduces deep convolutional gans, whose basic structure we use in
       our generator in this tutorial. also see [54]their dcgan code on
       github.
     __________________________________________________________________

   this post is part of a collaboration between o'reilly and
   [55]tensorflow. [56]see our statement of editorial independence.
   article image: id3 for beginners (source:
   [57]o'reilly).

   share
    1. [58]tweet
    2.
    3.
     __________________________________________________________________

   [59][jon_bruner-294c63fe4dcd5d97f03012baf55ea2e8.jpg]

[60]jon bruner

   jon bruner is a journalist and programmer who runs the digital factory
   program at formlabs, a company that builds professional-grade 3d
   printers. before joining formlabs, he oversaw o'reilly's publications
   on data, artificial intelligence, hardware, the internet of things,
   manufacturing, and electronics, and was program chair, along with joi
   ito, of the o'reilly solid conference, focused on the intersection
   between software and the physical world. he was previously data editor
   at forbes magazine, where he combined writing and programming to a...
   [61]more

[62]adit deshpande

   adit deshpande is a 2nd year undergraduate student majoring in computer
   science at ucla. he is vice president of acm ai, the artificial
   intelligence club on campus. with regular posts on his personal blog,
   he's interested in sharing and communicating his knowledge of different
   topics in computer science. he's passionate about applying knowledge of
   machine learning to important fields such as healthcare and education.
   [63]more
     __________________________________________________________________

   [64]inner circle at stonehenge.

   [65]artificial intelligence

[66]the dynamic forces shaping ai

   by [67]beau cronin

   it   s time to debate scenarios that will shift the balance of data,
   compute resources, algorithms, and talent.

   runnable code
   [68]how do you find the parts of speech in a sentence using python?

   [69]artificial intelligence

[70]how do you find the parts of speech in a sentence using python?

   by [71]jonathan mugan

   learn how to use spacy to parse a sentence to return the parts of
   speech (noun, verb, etc.) and dependencies.

   video
   [72]screenshot from "how do i remove boilerplate code with
   tensorflow-slim's meta-operator?"

   [73]artificial intelligence

[74]how do i remove boilerplate code with tensorflow-slim's meta-operator?

   by [75]marvin bertin

   learn how to use tensorflow-slim   s meta-operators to build deep
   learning models with a substantially reduced amount of code.

   video
   [76]screenshot from "how can i create a neural network training routine
   with tensorflow-slim?"

   [77]artificial intelligence

[78]how can i create a neural network training routine with tensorflow-slim?

   by [79]marvin bertin

   learn how to create an automated training routine for any of your deep
   learning models.

about us

     * [80]our company
     * [81]teach/speak/write
     * [82]careers
     * [83]customer service
     * [84]contact us

site map

     * [85]ideas
     * [86]learning
     * [87]topics
     * [88]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [89]terms of service     [90]privacy policy     [91]editorial independence

   animal

   iframe: [92]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/artificial-intelligence
  27. https://www.oreilly.com/people/b1d73-jon-bruner
  28. https://www.oreilly.com/people/adit-deshpande
  29. http://www.oreilly.com/
  30. https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/73729
  31. https://github.com/jonbruner/generative-adversarial-networks
  32. https://arxiv.org/abs/1406.2661
  33. https://github.com/newmu/dcgan_code
  34. https://www.tensorflow.org/
  35. https://www.oreilly.com/learning/hello-tensorflow
  36. https://www.safaribooksonline.com/oriole/hello-tensorflow-oriole
  37. http://yann.lecun.com/exdb/mnist/
  38. https://www.tensorflow.org/get_started/mnist/pros
  39. https://www.tensorflow.org/api_docs/python/tf/get_variable
  40. https://www.tensorflow.org/api_docs/python/tf/truncated_normal
  41. https://www.tensorflow.org/api_docs/python/tf/nn/conv2d
  42. https://github.com/osh/kerasgan
  43. https://www.tensorflow.org/api_docs/python/tf/sigmoid
  44. https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html
  45. https://www.tensorflow.org/api_docs/python/tf/session#run
  46. https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_id178_with_logits
  47. https://www.tensorflow.org/api_docs/python/tf/reduce_mean
  48. https://github.com/jonbruner/ezgan
  49. https://www.tensorflow.org/how_tos/summaries_and_tensorboard/
  50. https://github.com/soumith/ganhacks
  51. https://arxiv.org/abs/1406.2661
  52. https://arxiv.org/abs/1701.00160
  53. https://arxiv.org/abs/1511.06434
  54. https://github.com/newmu/dcgan_code
  55. https://www.tensorflow.org/
  56. http://www.oreilly.com/about/editorial_independence.html
  57. http://www.oreilly.com/
  58. https://twitter.com/share
  59. https://www.oreilly.com/people/b1d73-jon-bruner
  60. https://www.oreilly.com/people/b1d73-jon-bruner
  61. https://www.oreilly.com/people/b1d73-jon-bruner
  62. https://www.oreilly.com/people/adit-deshpande
  63. https://www.oreilly.com/people/adit-deshpande
  64. https://www.oreilly.com/ideas/the-four-dynamic-forces-shaping-ai
  65. https://www.oreilly.com/topics/artificial-intelligence
  66. https://www.oreilly.com/ideas/the-four-dynamic-forces-shaping-ai
  67. https://www.oreilly.com/people/89289-beau-cronin
  68. https://www.oreilly.com/learning/how-do-you-find-the-parts-of-speech-in-a-sentence-using-python
  69. https://www.oreilly.com/topics/artificial-intelligence
  70. https://www.oreilly.com/learning/how-do-you-find-the-parts-of-speech-in-a-sentence-using-python
  71. https://www.oreilly.com/people/e95f6-jonathan-mugan
  72. https://www.oreilly.com/learning/how-do-i-remove-boilerplate-code-with-tensorflow-slims-meta-operator
  73. https://www.oreilly.com/topics/artificial-intelligence
  74. https://www.oreilly.com/learning/how-do-i-remove-boilerplate-code-with-tensorflow-slims-meta-operator
  75. https://www.oreilly.com/people/marvin-bertin
  76. https://www.oreilly.com/learning/how-can-i-create-a-neural-network-training-routine-with-tensorflow-slim
  77. https://www.oreilly.com/topics/artificial-intelligence
  78. https://www.oreilly.com/learning/how-can-i-create-a-neural-network-training-routine-with-tensorflow-slim
  79. https://www.oreilly.com/people/marvin-bertin
  80. http://oreilly.com/about/
  81. http://oreilly.com/work-with-us.html
  82. http://oreilly.com/careers/
  83. http://shop.oreilly.com/category/customer-service.do
  84. http://shop.oreilly.com/category/customer-service.do
  85. https://www.oreilly.com/ideas
  86. https://www.oreilly.com/topics/oreilly-learning
  87. https://www.oreilly.com/topics
  88. https://www.oreilly.com/all
  89. http://oreilly.com/terms/
  90. http://oreilly.com/privacy.html
  91. http://www.oreilly.com/about/editorial_independence.html
  92. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  94. https://www.oreilly.com/
  95. https://www.facebook.com/oreilly/
  96. https://twitter.com/oreillymedia
  97. https://www.youtube.com/user/oreillymedia
  98. https://plus.google.com/+oreillymedia
  99. https://www.linkedin.com/company/oreilly-media
 100. https://www.oreilly.com/
