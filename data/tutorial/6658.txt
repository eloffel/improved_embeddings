   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]twentybn
     * [9]frontier
     * [10]embodied ai
     * [11]tutorials
     * [12]videos
     * [13]news
     __________________________________________________________________

hello, world: building an ai that understands the world through video

machines today can identify objects in images, but they are unable to fully
decipher the most important aspect: what   s actually happening in front of the
camera. at [14]twentybn, we have created the world   s first ai technology that
shows an awareness of its environment and of the actions occurring within it.
our system observes the world through live video and automatically interprets
the unfolding visual scene. check it out yourself:

   [15]go to the profile of twenty billion neurons
   [16]twenty billion neurons (button) blockedunblock (button)
   followfollowing
   oct 10, 2017

   iframe: [17]/media/07e54bf89e3f38ea799efbf23b9ff8b3?postid=9e2796400176

   here   s a demo of our ai system that was shot in a single continuous
   take. the blue labels show the system   s live predictions of what it
   thinks it   s seeing. the white labels below show the network   s top-5
   classifications.

deep learning as a succession of bold attempts

   evolution is rarely linear. as with other technologies before it, deep
   learning has followed a series of step functions defined by sudden,
   often unexpected, outbreaks of capability. each step function
   fundamentally pushed the envelope beyond what computers were previously
   able to achieve. one of the first such breakthroughs came in 2012 when
   alex krizhevsky, ilya sutskever and geoffrey hinton showed that deep
   neural networks, trained using id26, could beat
   state-of-the-art systems in image recognition. since then, similar
   breakthroughs have occurred in previously intractable problems. these
   range from machine translation and voice synthesis to beating the world
   champion in the game of go. importantly, each milestones seemed out of
   reach at the time, which made their achievement even more surprising.
   [0*zm85js8iechjaw9t.]
   deep learning as a succession of bold attempts: expert opinions at
   the time

   at twentybn, we believe that the next breakthrough will concern video
   understanding. as with previous discoveries in deep learning, there has
   been a widespread consensus that human-level video understanding is far
   out into the future. the best available technology is merely capable of
   identifying or segmenting objects and people in frames, but not more.
   human-level visual intelligence seems like a far cry from what   s
   technically possible.

providing ai systems with an awareness of the physical world

   today, we present the first proof point that our conviction is paying
   off. we show that neural networks trained with id26 are
   perfectly capable of learning enough physics about the world to
   understand exactly what is happening in visual scenes. the video
   footage presented here shows a deep neural network that was trained on
   hundreds of thousands of labeled video clips that show highly complex
   human actions. the training videos were designed to capture many of the
   complex and subtle ways in which objects and humans can move and
   physically influence each other in the world. the resulting system must
   learn to understand the verbs (motions) not just the nouns (objects) of
   a video   s content. it also has to infer a wealth of cues about object
   behaviors and relations to predict the correct labels.
   [1*nkth6g10t_bergsu4edqxw.gif]

   many of the videos were recorded by crowd workers using a web-platform
   we built specifically for the purpose of recording training material to
   teach neural networks    intuitive physics   . the videos range from simple
   hand motions and actions (like swiping left or jumping forward) to
   complex object manipulations and interactions (such as    trying to pour
   water into a cup, but missing so it spills next to it   ). children can
   learn about the world in part by running countless    experiments    during
   daily play. while we cannot ask our networks to do the same, we tried
   to achieve an analogous result by instead asking our networks to watch
   and then draw correct id136s about the videos in our database.

   the tasks were designed to be so hard that to successfully solve them
   the network would have to develop a deep understanding of many
   real-world concepts. for example, the network is asked to tell
      throwing [something] in the air and catching it    from    throwing
   [something] in the air and letting it fall   , and    putting something
   behind something    from    pretending to put something behind something,
   but not actually leaving it there   . in the demonstration above and
   below, we intentionally used placeholders of [something] in order to
   put emphasis on the motions.
   [1*h4tm0pdlg5d6rtnnhyar2a.gif]

   when training the first networks on this data, we were prepared to be
      tutoring    them by using architectural tricks and coaching signals to
   allow them to learn at all on these extraordinarily difficult tasks.
   however, we found that as our database grew, the system started to
   figure out the internal representations they needed to output correct
   solutions by themselves. while there are a lot of engineering,
   infrastructure and tricks required to gather and then have neural
   networks cope with the large amount of data, we found that beyond
   these, once again, id26 and well-designed data go a much
   longer way than widely assumed.

teaching machines visual common sense

   let   s put our video demonstration into context. the vast majority of
   commercial use cases in video understanding are stretching the envelope
   of what current technology is capable of. the reason is that machines
   today are missing a common-sense understanding of the real world. we
   humans rely constantly on our innate ability to understand and make
   id136s about our environment as we navigate the physical world. our
   common-sense reasoning is built up through lifelong experience,
   beginning in early childhood and extending well into adulthood.
   [1*onsld8dipromsp33uf2xgg.gif]

   videos are ideally suited for teaching a lot of knowledge about the
   world because they implicitly encode the physical constraints that
   define it. unfortunately, most existing systems approach the video
   understanding problem with image understanding components. as a result,
   virtually all solutions use frame-by-frame analysis that can identify
   and segment objects in isolated frames, instead of understanding motion
   patterns and object behaviours based on holistically labeled video
   segments.

   our bold attempt to reimagine video understanding is starting to bear
   fruit. it has allowed us to successfully train neural networks
   end-to-end on a wide range of action understanding tasks (see the
   example predictions at the beginning of this post). no ai system had
   appeared anywhere near solving these tasks just a few months ago.

   the animation below illustrates this further on the basis of one of the
   more difficult examples from the dataset. a system trained to perform
   frame-by-frame analysis can at best predict the dominant object class
   in each frame. our systems are trained to integrate information across
   the whole clip and can arrive at predictions like    someone is
   pretending to pick up a black remote control   .
   [1*cf2lkyh4emra5oi5dhl_sg.gif]

   continuing in the tradition of classic id161, some skepticism
   towards end-to-end learning has been driving a strong push towards
      pixel labeling    and object segmentation, after the initial id163
   successes happened in recent years. the hope was that a system that can
   segment hands and objects (possibly using a neural network), can then
   be used to engineer a solution that can detect that an object is picked
   up.

   in complete opposition to that approach, we are training our networks
   end-to-end on a lot of carefully collected data. this ensures that an
   understanding of objects and their relations have to emerge in response
   to the subtle distinctions required by the task. over the next months,
   we will continue our push towards the goal of building machines that
   can truly perceive the world like humans.

   iframe: [18]/media/7155373d7b63959d8c34752a19149b23?postid=9e2796400176

   here   s another demonstration video that was shot in a single continuous
   take.
     __________________________________________________________________

   join us this week at the [19]re   work deep learning summit in montr  al
   and [20]nvidia   s gpu technology conference 2017 in munich where we will
   have live demonstrations of our system in action. we will also explain
   how our systems at twentybn drive commercial value for our customers,
   and speak about our long-term ai and product agenda to learn
   common-sense world knowledge through video.
   [1*3zjbb-iy8juxuodt5i3sbw.png]

   if you want to learn more about our work, please [21]reach out to us.

     * [22]artificial intelligence
     * [23]deep learning
     * [24]id161
     * [25]neural networks
     * [26]machine learning

   (button)
   (button)
   (button) 662 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of twenty billion neurons

[28]twenty billion neurons

   we are a german/canadian a.i. company based in berlin and toronto.
   visit us at [29]https://20bn.com/

     (button) follow
   [30]twentybn

[31]twentybn

   we teach machines to perceive the world like humans.

     * (button)
       (button) 662
     * (button)
     *
     *

   [32]twentybn
   never miss a story from twentybn, when you sign up for medium.
   [33]learn more
   never miss a story from twentybn
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9e2796400176
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/twentybn/watch-and-learn-building-an-ai-that-understands-the-world-through-video-9e2796400176&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/twentybn/watch-and-learn-building-an-ai-that-understands-the-world-through-video-9e2796400176&source=--------------------------nav_reg&operation=register
   8. https://medium.com/twentybn?source=logo-lo_rq00z7d178z8---6b97bd931b7e
   9. https://medium.com/twentybn/tagged/computer-vision
  10. https://medium.com/twentybn/tagged/ai-avatars
  11. https://medium.com/twentybn/tagged/software-development
  12. https://medium.com/twentybn/tagged/videos
  13. https://medium.com/twentybn/tagged/news
  14. https://www.twentybn.com/
  15. https://medium.com/@twentybn?source=post_header_lockup
  16. https://medium.com/@twentybn
  17. https://medium.com/media/07e54bf89e3f38ea799efbf23b9ff8b3?postid=9e2796400176
  18. https://medium.com/media/7155373d7b63959d8c34752a19149b23?postid=9e2796400176
  19. https://www.re-work.co/events/deep-learning-summit-montreal-canada-track1-2017
  20. https://www.gputechconf.eu/home.aspx
  21. mailto:info@twentybn.com
  22. https://medium.com/tag/artificial-intelligence?source=post
  23. https://medium.com/tag/deep-learning?source=post
  24. https://medium.com/tag/computer-vision?source=post
  25. https://medium.com/tag/neural-networks?source=post
  26. https://medium.com/tag/machine-learning?source=post
  27. https://medium.com/@twentybn?source=footer_card
  28. https://medium.com/@twentybn
  29. https://20bn.com/
  30. https://medium.com/twentybn?source=footer_card
  31. https://medium.com/twentybn?source=footer_card
  32. https://medium.com/twentybn
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/9e2796400176/share/twitter
  36. https://medium.com/p/9e2796400176/share/facebook
  37. https://medium.com/p/9e2796400176/share/twitter
  38. https://medium.com/p/9e2796400176/share/facebook
