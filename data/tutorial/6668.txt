   #[1]adventures in machine learning    recurrent neural networks and lstm
   tutorial in python and tensorflow comments feed [2]alternate
   [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

recurrent neural networks and lstm tutorial in python and tensorflow

   by [9]admin | [10]deep learning

     * you are here:
     * [11]home
     * [12]deep learning
     * [13]recurrent neural networks and lstm tutorial in python and
       tensorflow

   oct 09
   [14]9
   recurrent lstm tutorial - unrolled id56

   in the deep learning journey so far on this website, i   ve introduced
   [15]dense neural networks and [16]convolutional neural networks (id98s)
   which explain how to perform classification tasks on static images.
   we   ve seen good results, especially with id98   s. however, what happens
   if we want to analyze dynamic data? what about videos, voice
   recognition or sequences of text? there are ways to do some of this
   using id98   s, but the most popular method of performing classification
   and other analysis on sequences of data is recurrent neural networks.
   this tutorial will be a very comprehensive introduction to recurrent
   neural networks and a subset of such networks     long-short term memory
   networks (or id137). i   ll also show you how to implement such
   networks in tensorflow     including the data preparation step. it   s
   going to be a long one, so settle in and enjoy these pivotal networks
   in deep learning     at the end of this post, you   ll have a very solid
   understanding of recurrent neural networks and lstms. by the way, if
   you   d like to learn how to build id137 in keras, see [17]this
   tutorial.

   as always, all the code for this post can be found on [18]this site   s
   github repository.
     __________________________________________________________________

eager to learn more? get the book [19]here
     __________________________________________________________________


an introduction to recurrent neural networks

   a recurrent neural network, at its most fundamental level, is simply a
   type of densely connected neural network (for an introduction to such
   networks, [20]see my tutorial). however, the key difference to normal
   feed forward networks is the introduction of time     in particular, the
   output of the hidden layer in a recurrent neural network is fed
   back into itself. diagrams help here, so observe:
   recurrent lstm tutorial - id56 diagram with nodes

   recurrent neural network diagram with nodes shown

   in the diagram above, we have a simple recurrent neural network with
   three input nodes.  these input nodes are fed into a hidden layer, with
   sigmoid activations, as per any normal [21]densely connected neural
   network. what happens next is what is interesting     the output of the
   hidden layer is then fed back into the same hidden layer. as you can
   see the hidden layer outputs are passed through
   a conceptual delay block to allow the input of $\textbf{h}^{t-1}$ into
   the hidden layer.  what is the point of this? simply, the point is that
   we can now model time or sequence-dependent data.

   a particularly good example of this is predicting text sequences.
   consider the following text string:    a girl walked into a bar, and she
   said    can i have a drink please?   .  the bartender said    certainly {}   .
   there are many options for what could fill in the {} symbol in the
   above string, for instance,    miss   ,    ma   am    and so on. however, other
   words could also fit, such as    sir   ,    mister    etc. in order to get the
   correct gender of the noun, the neural network needs to    recall    that
   two previous words designating the likely gender (i.e.    girl    and
      she   ) were used. this type of flow of information through time (or
   sequence) in a recurrent neural network is shown in the diagram below,
   which unrolls the sequence:
   recurrent lstm tutorial - unrolled id56

   unrolled recurrent neural network

   on the left-hand side of the above diagram, we have basically the same
   diagram as the first (the one which shows all the nodes explicitly).
   what the previous diagram neglected to show explicitly was that we in
   fact only ever supply finite length sequences to such networks    
   therefore we can unroll the network as shown on the right-hand side of
   the diagram above. this unrolled network shows how we can supply a
   stream of data to the recurrent neural network. for instance, first, we
   supply the word vector for    a    (more about word vectors later) to the
   network f     the output of the nodes in f are fed into the    next   
   network and also act as a stand-alone output ($h_0$).  the next network
   (though it is really the same network) f at time t=1 takes the next
   word vector for    girl    and the previous output $h_0$ into its hidden
   nodes, producing the next output $h_1$ and so on.

   as discussed above, the words themselves i.e.    a   ,    girl    etc. aren   t
   input directly into the neural network. neither are their one-hot
   vector type representations     rather, an embedding vector is used for
   each word. an embedding vector is an efficient vector representation of
   the word (often between 50-300 in length), which should maintain some
   meaning or context of the word. id27 won   t be entered into
   detail here, as i have covered it extensively in other posts
       [22]id97 id27 tutorial in python and tensorflow, [23]a
   id97 keras tutorial and [24]python gensim id97 tutorial with
   tensorflow and keras. it is an interesting topic and well worth the
   time investigating.

   now, back to recurrent neural networks themselves. recurrent neural
   networks are very flexible. in the implementation shown above, we have
   a many-to-many model     in other words, we have the input sequence    a
   girl walked into a bar       and many outputs     $h_0$ to $h_t$. we could
   also have multiple other configurations.  another option is one-to-many
   i.e. supplying one input, say    girl    and predicting multiple
   outputs $h_0$ to $h_t$ (i.e. trying to generate sentences based on a
   single starting word). a further configuration is many-to-one i.e.
   supplying many words as input, like the sentence    a girl walked into a
   bar, and she said    can i have a drink please?   .  the bartender said
      certainly {}    and predicting the next word i.e. {}. the diagram below
   shows an example one-to-many and many-to-one configuration,
   respectively (the words next to the outputs are the target words which
   we would supply during training).
   recurrent neural network lstm - one-to-many configuration

   recurrent neural network     one-to-many configuration
   recurrent neural network lstm - many-to-one configuration

   recurrent neural network     many-to-one configuration

   there are also different many-to-many configurations that can be
   constructed     but you get the idea: recurrent neural networks are quite
   flexible. one last thing to note     the weights of the connections
   between time steps are shared i.e. there isn   t a different set of
   weights for each time step.

   now you have a pretty good idea of what recurrent neural networks are,
   it is time to point out their dominant problem.

the problem with basic recurrent neural networks

   vanilla recurrent neural networks aren   t actually used very often in
   practice. why? the main reason is the vanishing gradient problem. for
   recurrent neural networks, ideally, we would want to have long
   memories, so the network can connect data relationships at significant
   distances in time. that sort of network could make real progress in
   understanding how language and narrative works, how stock market events
   are correlated and so on. however, the more time steps we have, the
   more chance we have of back-propagation gradients either accumulating
   and exploding or vanishing down to nothing.

   consider the following representation of a recurrent neural network:

   $$\textbf{h}_t = \sigma (\textbf{ux}_t + \textbf{vh}_{t-1})$$

   where u and v are the weight matrices connecting the inputs and the
   recurrent outputs respectively. we then often will perform a softmax of
   all the $\textbf{h}_t$ outputs (if we have some sort of many-to-many or
   one-to-many configuration). notice, however, that if we go back three
   time steps in our recurrent neural network, we have the following:

   $$\textbf{h}_t = \sigma (\textbf{ux}_t +
   \textbf{v}(\sigma(\textbf{ux}_{t-1} +
   \textbf{v}(\sigma(\textbf{ux}_{t-2})))$$

   from the above you can see, as we work our way back in time, we are
   essentially adding deeper and deeper layers to our network. this causes
   a problem     consider the gradient of the error with respect to the
   weight matrix u during id26 through time, it looks something
   along the lines of this:

   $$\frac{\partial e_3}{\partial u} = \frac{\partial e_3}{\partial
   out_3}\frac{\partial out_3}{\partial h_3}\frac{\partial h_3}{\partial
   h_2}\frac{\partial h_2}{\partial h_1}\frac{\partial h_1}{\partial u}$$

   the equation above is only a rough approximation of what is going on
   during id26 through time, but it will suffice for our
   purposes (for more on back-propagation, see my [25]comprehensive neural
   networks tutorial). each of these gradients will involve calculating
   the gradient of the sigmoid function. the problem with the sigmoid
   function occurs when the input values are such that the output is close
   to either 0 or 1     at this point, the gradient is very small, see the
   plot below.
   recurrent neural network and lstm tutorial - sigmoid gradient

   sigmoid gradient

   as you can observe, the values of the gradient (orange line) are always
   <0.25 and get to very low values when the output gets close to 0 or 1.
   what does this mean? it means that when you multiply many sigmoid
   gradients together you are multiplying many values which are
   potentially much less than zero     this leads to a vanishing gradient
   $\frac{\partial e}{\partial u}$. because the gradient will become
   basically zero when dealing with many prior time steps, the weights
   won   t adjust to take into account these values, and therefore the
   network won   t learn relationships separated by significant periods of
   time. this makes vanilla recurrent neural networks not very useful. if
   you   d like to learn more about the vanishing gradient problem, see my
   dedicated post about it [26]here.

   we could use [27]relu id180 to reduce this problem,
   though not eliminate it. however, the most popular way of dealing with
   this issue in recurrent neural networks is by using long-short term
   memory (lstm) networks, which will be introduced in the next section.

introduction to id137

   to reduce the vanishing (and exploding) gradient problem, and therefore
   allow deeper networks and recurrent neural networks to perform well in
   practical settings, there needs to be a way to reduce the
   multiplication of gradients which are less than zero. the lstm cell is
   a specifically designed unit of logic that will help reduce the
   vanishing gradient problem sufficiently to make recurrent neural
   networks more useful for long-term memory tasks i.e. text sequence
   predictions. the way it does so is by creating an internal memory state
   which is simply added to the processed input, which greatly reduces the
   multiplicative effect of small gradients. the time dependence and
   effects of previous inputs are controlled by an interesting concept
   called a forget gate, which determines which states are remembered or
   forgotten. two other gates, the input gate and output gate, are also
   featured in lstm cells.

   let   s first have a look at lstm cells more carefully, then i   ll discuss
   how they help reduce the vanishing gradient problem.

the structure of an lstm cell

   the structure of a typical lstm cell is shown in the diagram below:
   recurrent neural network lstm tutorial - lstm cell diagram

   lstm cell diagram

   the data flow is from left-to-right in the diagram above, with the
   current input $x_t$ and the previous cell output $h_{t-1}$ concatenated
   together and entering the top    data rail   . here   s where things get
   interesting.

the input gate

   first, the input is squashed between -1 and 1 using a tanh activation
   function. this can be expressed by:

   $$g = tanh(b^g + x_tu^g + h_{t-1}v^g)$$

   where $u^g$ and $v^g$ are the weights for the input and previous cell
   output, respectively, and $b^g$ is the input bias. note that the
   exponents g are not a raised power, but rather signify that these are
   the input weights and bias values (as opposed to the input gate, forget
   gate, output gate etc.).

   this squashed input is then multiplied element-wise by the output of
   the input gate. the input gate is basically a hidden layer of sigmoid
   activated nodes, with weighted $x_t$ and $h_{t-1}$ input values, which
   outputs values of between 0 and 1 and when multiplied element-wise by
   the input determines which inputs are switched on and off. in other
   words, it is a kind of input filter or gate. the expression for the
   input gate is:

   $$i = \sigma(b^i + x_tu^i + h_{t-1}v^i)$$

   the output of the input stage of the lstm cell can be expressed below,
   where the $\circ$ operator expresses element-wise multiplication:

   $$g \circ i$$

   as you can observe, the input gate output i acts as the weights for the
   squashed input g.  we now move onto the next stage of the lstm cell    
   the internal state and the forget gate.

the internal state and the forget gate

   this stage in the lstm is where most of the magic happens. as can be
   observed, there is a new variable $s_t$ which is the inner state of the
   lstm cell. this state is delayed by one-time step and is ultimately
   added to the $g \circ i$ input to provide an internal recurrence loop
   to learn the relationship between inputs separated by time. two things
   to notice     first, there is a forget gate here     this forget gate is
   again a sigmoid activated set of nodes which is element-wise multiplied
   by $s_{t-1}$ to determine which previous states should be remembered
   (i.e. forget gate output close to 1) and which should be forgotten
   (i.e. forget gate output close to 0). this allows the lstm cell to
   learn appropriate context. consider the sentence    clare took helen to
   paris and she was very grateful        for the lstm cell to learn who    she   
   refers to, it needs to forget the subject    clare    and replace it with
   the subject    helen   . the forget gate can facilitate such operations and
   is expressed as: recurrent neural network lstm tutorial - forget gate
   snippet

   $$f = \sigma(b^f + x_tu^f + h_{t-1}v^f)$$

   the output of the element-wise product of the previous state and the
   forget gate is expressed as $s_{t-1} \circ f$. again, the forget gate
   output acts as weights for the internal state. the second thing to
   notice about this stage is that the forget-gate-   filtered    state is
   simply added to the input, rather than multiplied by it, or mixed with
   it via weights and a sigmoid activation function as occurs in a
   standard recurrent neural network. this is important to reduce the
   issue of vanishing gradients. the output from this stage, $s_t$ is
   expressed by:

   $$s_t = s_{t-1} \circ f + g \circ i$$

   the final stage of the lstm cell is the output gate.

the output gate

   the final stage of the lstm cell is the output gate. the output gate
   has two components     another tanh squashing function and an output
   sigmoid gating function. the output sigmoid gating function, like the
   other gating functions in the cell, is multiplied by the squashed state
   $s_t$ to determine which values of the state are output from the cell.
   as you can tell, the lstm cell is very flexible, with gating functions
   controlling what is input, what is    remembered    in the internal state
   variable, and finally what is output from the lstm cell.  recurrent
   neural network lstm tutorial - output gate snippet

   the output gate is expressed as:

   $$o = \sigma(b^o + x_tu^o + h_{t-1}v^o)$$

   so the final output of the cell can be expressed as:

   $$h_t = tanh(s_t) \circ o$$

   the next question is, how does the lstm cell reduce the vanishing
   gradient problem?

reducing the vanishing gradient problem

   recall before that the issue with vanilla recurrent neural networks is
   that calculating the gradient to update the weights involves cascading
   terms like:

   $$\frac {\partial h_n}{\partial h_{n-1}} \frac {\partial
   h_{n-1}}{\partial h_{n-2}} \frac {\partial h_{n-2}}{\partial h_{n-3}}
      $$

   this is a problem because of the sigmoid derivative, which is present
   in all of the partial derivatives above, being <0.25 (often greatly
   so). there is also a factorial of the weights involved, so if they are
   consistently <1, we get a similar result     a vanishing gradient.

   in an lstm cell, the recurrency of the internal state of the lstm cell
   involves, as shown above, an addition     like so:

   $$s_t = s_{t-1} \circ f + g \circ i$$

   if we take the partial derivative of this recurrency like we did above
   for a vanilla recurrent neural network, we find the following:

   $$\frac{\partial s_t}{\partial s_{t-1}} = f$$

   notice that the $g \circ i$ term drops away and we are just left with a
   repeated multiplication of $f$. so for three time steps, we would have
   $f x f x f$. notice that if the output of $f=1$, there will be no decay
   of the gradient. generally, the bias of the sigmoid in $f$ is made
   large at the beginning of training so that $f$ starts out as 1 ,
   meaning that all past input states will be    remembered    in the cell.
   during training, the forget gate will reduce or eliminate the memory of
   certain components of the state $s_{t-1}$.

   this might be a bit confusing, so i   ll explain another way before we
   move on. imagine if we let in a single input during the first time
   step, but then we block all future inputs (by setting the input gate to
   output zeros) and remember all previous states (by setting the forget
   gate to output ones). we would have a kind of circulating memory of
   $s_t$ which never decays i.e. $s_t$ = $s_{t-1}$. a back-propagated
   error    entering    this loop would also never decay. with the vanilla
   recurrent neural network, however, if we did the same thing our
   back-propagated error would be continuously degraded by the gradient of
   the activation function of the hidden nodes, and therefore eventually
   decay to zero.

   hopefully, that helps you to understand, at least in part, why lstm
   cells are a great solution to the vanishing gradient problem, and
   therefore why they are currently used so extensively. now, so far, we
   have been dealing with the data in the lstm cells as if they were
   single values (i.e. scalars), however, in reality, they are tensors or
   vectors, and this can get confusing. so in the next section, i   ll spend
   a bit of time explaining the tensor sizes we can expect to be flowing
   around our unrolled id137.

the dimensions of data inside an lstm cell

   in the example code that is going to be discussed below, we are going
   to be performing text prediction. now, as discussed in [28]previous
   tutorials on the id97 algorithm, words are input into neural
   networks using meaningful word vectors i.e. the word    cat    might be
   represented by, say, a 650 length vector. this vector is encoded in
   such a way as to capture some aspect of the meaning of the word (where
   meaning is usually construed as the context the word is usually found
   in). so each word input into our lstm network below will be a 650
   length vector. next, because we will be inputting a sequence of words
   into our unrolled lstm network, for each input row we will be inputting
   35 of these word vectors. so the input for each row will be (35 x 650)
   in size. finally, with tensorflow, we can process batches of data via
   multi-dimensional tensors (to learn more about basic tensorflow, see
   [29]this tensorflow tutorial). if we have a batch size of 20,
   our training input data will be (20 x 35 x 650). for future reference,
   the way i have presented the tensor size here (i.e. (20 x 35 x 650)) is
   called a    batch-major    arrangement, where the batch size is the first
   dimension of the tensor. we could also alternatively arrange the data
   in    time-major    format, which would be (35 x 20 x 650)     same data,
   just a different arrangement.

   now, the next thing to consider is that each of the input, forget and
   output gates, along with the inner state variable $s_t$ and the
   squashing functions, are not single functions with single/scalar
   weights. rather, they comprise the hidden layer of the network and
   therefore include multiple nodes, connecting weights, bias values and
   so on. it is up to us to set the size of the hidden layer. the output
   from the unrolled lstm network will, therefore, include the size of the
   hidden layer. the size of the output from the unrolled lstm network
   with a size 650 hidden layer, and a 20 length batch-size and 35 time
   steps will be (20, 35, 650). often, the output of an unrolled lstm will
   be partially flattened and fed into a softmax layer for classification
       so, for instance, the first two dimensions of the tensor are
   flattened to give a softmax layer input size of (700, 650). the output
   of the softmax is then matched against the expected training outputs
   during training. the diagram below shows all this:
   tensorflow lstm network architecture

   lstm network architecture

   as can be observed in the architecture above (which we will be creating
   in the code below), it is possible to stack layers of lstm cells on top
   of each other     this increases the model complexity and
   predictive power but at the expense of training times and difficulties.
   the architecture shown above is what we will implement in tensorflow in
   the next section. note the small batch size     this is to allow a more
   stochastic id119 which will avoid settling in local minima
   during many training iterations (see [30]here).

creating an lstm network in tensorflow

   we are now going to create an lstm network in tensorflow. the code will
   loosely follow the tensorflow team tutorial found [31]here, but with
   updates and my own substantial modifications. the text dataset that
   will be used and is a common benchmarking corpus is the [32]penn tree
   bank (ptb) dataset. as usual, all the code for this post can be found
   on the [33]adventuresinml github site. to run this code, you   ll first
   have to download and extract the .tgz file from [34]here. first off,
   we   ll go through the data preparation part of the code.

preparing the data

   this code will use, verbatim, the following functions from the
   [35]previously mentioned tensorflow tutorial: read_words, build_vocab
   and file_to_word_ids. i won   t go into these functions in detail, but
   basically, they first split the given text file into separate words and
   sentence based characters (i.e. end-of-sentence <eos>). then, each
   unique word is identified and assigned a unique integer. finally, the
   original text file is converted into a list of these unique integers,
   where each word is substituted with its new integer identifier. this
   allows the text data to be consumed in the neural network.

   the code below shows how these functions are used in my code:
def load_data():
    # get the data paths
    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    # build the complete vocabulary, then convert text data to list of integers
    word_to_id = build_vocab(train_path)
    train_data = file_to_word_ids(train_path, word_to_id)
    valid_data = file_to_word_ids(valid_path, word_to_id)
    test_data = file_to_word_ids(test_path, word_to_id)
    vocabulary = len(word_to_id)
    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))

    print(train_data[:5])
    print(word_to_id)
    print(vocabulary)
    print(" ".join([reversed_dictionary[x] for x in train_data[:10]]))
    return train_data, valid_data, test_data, vocabulary, reversed_dictionary

   first, we simply setup the directory paths for the train, validation
   and test datasets respectively. then, build_vocab() is invoked on the
   training data to create a dictionary that has each word as a key, and a
   unique integer as the associated value. here is a sample of what
   the word_to_id dictionary looks like:

     {   write-off   : 7229,    ports   : 8314,    fundamentals   : 4478,
        toronto-based   : 5034,    head   : 638,    fairness   : 6417,   

   next, we convert the text data for each file into a list of integers
   using the word_to_id dictionary. the first 5 items of the
   list train_data looks like:

     [9970, 9971, 9972, 9974, 9975]

   i   ve also created a reverse dictionary which allows you to go the other
   direction     from a unique integer identifier to the corresponding word.
   this will be used later when we are reconstructing the outputs of our
   lstm network back into plain english sentences.

   the next step is to develop an input data pipeline that allows the
   extraction of batches of data in an efficient manner.

creating an input data pipeline

   as discussed in my [36]tensorflow queues and threads tutorial, the use
   of a feed dictionary to supply data to your model during training,
   while common in tutorials, is not efficient     as can be read [37]here
   on the tensorflow site. rather, it is more efficient to use tensorflow
   queues and threading. note, that there is a new way of doing things,
   using the dataset api, which won   t be used in this tutorial, but i will
   perhaps update it in the future to include this new way of doing
   things. i   ve packaged up this code in a function called batch_producer
       this function extracts batches of x, y training data     the x batch is
   formatted as the time stepped text data. the y batch is the same data,
   except delayed one time step. so, for instance, a single x, y sample in
   a batch, with the number of time steps being 8, looks like:
     * x =    a girl walked into a bar, and she   
     * y =    girl walked into a bar, and she said   

   remember that x and y will be batches of integer data, with the size
   (batch_size, num_steps), not text as shown above     however, i have
   shown the above x and y sample in text form to aid understanding. so,
   as demonstrated in the model architecture diagram above, we are
   producing a many-to-many lstm model, where the model will be trained to
   predict the very next word in the sequence for each word in the number
   of time steps.

   here   s what the code looks like:
def batch_producer(raw_data, batch_size, num_steps):
    raw_data = tf.convert_to_tensor(raw_data, name="raw_data", dtype=tf.int32)

    data_len = tf.size(raw_data)
    batch_len = data_len // batch_size
    data = tf.reshape(raw_data[0: batch_size * batch_len],
                      [batch_size, batch_len])

    epoch_size = (batch_len - 1) // num_steps

    i = tf.train.range_input_producer(epoch_size, shuffle=false).dequeue()
    x = data[:, i * num_steps:(i + 1) * num_steps]
    x.set_shape([batch_size, num_steps])
    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]
    y.set_shape([batch_size, num_steps])
    return x, y

   in the code above, first, the raw text data is converted into an int32
   tensor. next, the length of the full data set is calculated and stored
   in data_len and this is then divided by the batch size in an integer
   division (//) to get the number of full batches of data available
   within the dataset. the next line reshapes the raw_data tensor
   (restricted in size to the number of full batches of data i.e. 0
   to batch_size * batch_len) into a (batch_size, batch_len) shape. the
   next line sets the number of iterations in each epoch     usually, this
   is set so that all the training data is passed through the algorithm in
   each epoch. this is what occurs here     the number of batches in the
   data (batch_len) is integer divided by the number of time steps     this
   gives the number of time-step-sized batches that are available to be
   iterated through in a single epoch.

   the next line sets up an input range producer queue     this is a simple
   queue which allows the asynchronous and threaded extraction of data
   batches from a pre-existing dataset. for more on threads and queues,
   check out [38]my tutorial. basically, each time more data is required
   in the training of the model, a new integer is extracted between 0
   and epoch_size     this is then used in the following lines to extract
   a batch of data asynchronously from the data tensor. with the shuffle
   argument set to false, this integer simply cycles from 0 to epoch_size
   and then resets back at 0 to repeat.

   to produce the x, y batches of data, data slices are extracted from the
   data tensor based on the dequeued integer i. to see how this works, it
   is easier to imagine a dummy dataset of integers up to 20     [0, 1, 2,
   3, 4, 5, 6,    , 19, 20]. let   s say we set the batch size to 3, and the
   number of steps to 2. the variables batch_len and epoch_size will
   therefore be equal to 6 and 2, respectively. the dummy reshaped data
   will look like:

   $$\begin{bmatrix}
   1 & 2 & 3 & 4 & 5 & 6 \\
   7 & 8 & 9 & 10 & 11 & 12 \\
   13 & 14 & 15 & 16 & 17 & 18 \\
   \end{bmatrix}$$

   for the first data batch extraction, i = 0, therefore the extracted x
   for our dummy dataset will be data[:, 0:2]:

   $$\begin{bmatrix}
   1 & 2\\
   7 & 8\\
   13 & 14\\
   \end{bmatrix}$$

   the extracted y will be data[:, 1:3]:

   $$\begin{bmatrix}
   2 & 3\\
   8 & 9\\
   14 & 15\\
   \end{bmatrix}$$

   as can be observed, each row of the extracted x and y tensors will be
   an individual sample of length num_steps and the number of rows is the
   batch length. by organizing the data in this fashion, it is
   straight-forward to extract batch data while still maintaining the
   correct sentence sequence within each data sample.

creating the model

   in this code example, in order to have nice encapsulation and
   better-looking code, i   ll be building the model in [39]python classes.
   the first class is a simple class that contains the input data:
class input(object):
    def __init__(self, batch_size, num_steps, data):
        self.batch_size = batch_size
        self.num_steps = num_steps
        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps
        self.input_data, self.targets = batch_producer(data, batch_size, num_ste
ps)

   we pass this object important input data information such as batch
   size, the number of recurrent time steps and finally the raw data file
   we wish to extract batch data from. the previously
   explained batch_producer function, when called, will return our input
   data batch x and the associated time step + 1 target data batch, y.

   the next step is to create our lstm model. again, i   ve used a python
   class to hold all the information and tensorflow operations:
# create the main model
class model(object):
    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,
                 dropout=0.5, init_scale=0.05):
        self.is_training = is_training
        self.input_obj = input
        self.batch_size = input.batch_size
        self.num_steps = input.num_steps

   the first part of initialization is pretty self-explanatory, with the
   input data information and batch producer operation found in input_obj.
   another important input is the boolean is_training     this allows the
   model instance to be created either as a model setup for training, or
   alternatively setup for validation or testing only.
# create the id27s
with tf.device("/cpu:0"):
    embedding = tf.variable(tf.random_uniform([vocab_size, self.hidden_size], -i
nit_scale, init_scale))
    inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)

   the block of code above creates the id27s. as previously
   discussed and shown in [40]my tutorial, id27 creates
   meaningful vectors to represent each word. first, we initialize the
   embedding variable with size (vocab_size, hidden_size) which creates
   the    lookup table    where each row represents a word in the dataset, and
   the set of columns is the embedding vector. in this case, our embedding
   vector length is set equal to the size of our lstm hidden layer.

   the next line performs a lookup action on the embedding tensor, where
   each word in the input data set is matched with a row in the embedding
   tensor, with the matched embedding vector being returned within inputs.

   in this model, the embedding layer / vectors will be learned during the
   model training     however, if we so desired, we could also pre-learn
   embedding vectors using another model and upload these into our models.
   i   ve shown how to do this in [41]my gensim tutorial if you want to
   check it out.

   the next step adds a [42]drop-out wrapper to the input data     this
   helps prevent overfitting by continually changing the structure of the
   network connections:
if is_training and dropout < 1:
    inputs = tf.nn.dropout(inputs, dropout)

creating the lstm network

   the next step is to setup the initial state tensorflow placeholder.
   this placeholder will be loaded with the initial state of the lstm
   cells for each training batch. at the beginning of each training epoch,
   the input data will reset to the beginning of the text data set, so we
   want to reset the state variables to zero. however, during the multiple
   training batches executed in each epoch, we want to load the final
   state variables from the previous training batch into our lstm cells
   for the current training batch. this keeps a certain continuity of
   state in our model, as we are progressing linearly through our text
   data set. we define the placeholder by:
# set up the state storage / extraction
self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, se
lf.hidden_size])

   the second argument to the placeholder function is the size of the
   variable     (num_layers, 2, batch_size, hidden_size) and requires some
   explanation. if we consider an individual lstm cell, for each training
   sample it processes it has two other inputs     the previous output from
   the cell ($h_{t-1}$) and the previous state variable ($s_{t-1}$). these
   two inputs, h and s, are what is required to load the full state data
   into an lstm cell. remember also that h and s for each sample are
   actually vectors with the size equal to the hidden layer size.
   therefore, for all the samples in the batch, for a single lstm cell we
   have state data required of shape (2, batch_size, hidden_size).
   finally, if we have stacked lstm cell layers, we need state variables
   for each layer     num_layers. this gives the final shape of the state
   variables: (num_layers, 2, batch_size, hidden_size).

   the next two steps involve setting up this state data variable in the
   format required to feed it into the tensorflow lstm data structure:
state_per_layer_list = tf.unstack(self.init_state, axis=0)
id56_tuple_state = tuple(
            [tf.contrib.id56.lstmstatetuple(state_per_layer_list[idx][0], state_p
er_layer_list[idx][1])
             for idx in range(num_layers)]
        )

   the tensorflow lstm cell can accept the state as a tuple if a flag is
   set to true (more on this later). the tf.unstack command creates a
   number of tensors, each of shape (2, batch_size, hidden_size), from the
   init_state tensor, one for each stacked lstm layer (num_layer). these
   tensors are then loaded into a specific tensorflow data structure,
   lstmstatetuple, which is the required for input into the lstm cells.

   next, we create an lstm cell which will be    unrolled    over the number
   of time steps. following this, we apply a drop-out wrapper to again
   protect against overfitting. notice that we set the forget bias values
   to be equal to 1.0, which helps guard against repeated low forget gate
   outputs causing vanishing gradients, as explained above:
# create an lstm cell to be unrolled
cell = tf.contrib.id56.lstmcell(hidden_size, forget_bias=1.0)
# add a dropout wrapper if training
if is_training and dropout < 1:
    cell = tf.contrib.id56.dropoutwrapper(cell, output_keep_prob=dropout)

   next, if we include many layers of stacked lstm cells in the model, we
   need to use another tensorflow object called multiid56cell which
   performs the requisite cell stacking / layering:
if num_layers > 1:
    cell = tf.contrib.id56.multiid56cell([cell for _ in range(num_layers)], state_
is_tuple=true)

   note that we tell multiid56cell to expect the state variables in the
   form of a lstmstatetuple by setting the flag state_is_tuple to true.

   the final step in creating the lstm network structure is to create a
   dynamic id56 object in tensorflow. this object will dynamically perform
   the unrolling of the lstm cell over each time step.
output, self.state = tf.nn.dynamic_id56(cell, inputs, dtype=tf.float32, initial_s
tate=id56_tuple_state)

   the dynamic_id56 object takes our defined lstm cell as the first
   argument, and the embedding vector tensor inputs as the second
   argument. the final argument, initial_state is where we load our
   time-step zero state variables, that we created earlier, into the
   unrolled lstm network.

   this operation creates two outputs, the first is the output from all
   the unrolled lstm cells, and will have a shape of (batch_size,
   num_steps, hidden_size). this data will be flattened in the next step
   to feed into a softmax classification layer. the second output, state,
   is the (s, h) state tuple taken from the final time step of the lstm
   cells. this state operation / tuple will be extracted during each batch
   training operation to be used as inputs (via init_state) into the next
   training batch.

creating the softmax, loss and optimizer operations

   next we have to flatten the outputs so that we can feed them into our
   proposed softmax classification layer. we can use the -1 notation to
   reshape our output tensor, with the second axis set to be equal to the
   hidden layer size:
# reshape to (batch_size * num_steps, hidden_size)
output = tf.reshape(output, [-1, hidden_size])

   next we setup our softmax weight variables and the standard $xw+b$
   operation:
softmax_w = tf.variable(tf.random_uniform([hidden_size, vocab_size], -init_scale
, init_scale))
softmax_b = tf.variable(tf.random_uniform([vocab_size], -init_scale, init_scale)
)
logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)

   note that the logits operation is simply the output of our tensor
   multiplication     we haven   t yet added the softmax operation     this will
   occur in the loss calculations below (and also in our ancillary
   accuracy calculations).

   following this, we have to setup our loss or cost function which will
   be used to train our lstm network. in this case, we will use the
   specialized [43]tensorflow sequence to sequence id168. this
   id168 allows one to calculate (a potentially) weighted cross
   id178 loss over a sequence of values. the first argument to this loss
   function is the logits argument, which requires tensors with the shape
   (batch_size, num_steps, vocab_size)     so we   ll need to reshape our
   logits tensor. the second argument to the id168 is
   the targets tensor which has a shape (batch_size, num_steps) with each
   value being an integer (which corresponds to a unique word in our case)
       in other words, this tensor contains the true values of the word
   sequence that we want our lstm network to predict. the third important
   argument is the weights tensor, of shape (batch_size, num_steps), which
   allows you to weight different samples or time steps with respect to
   the loss i.e. you might want the loss to favor the latter time steps
   rather than the earlier ones. no weighting is applied in this model, so
   a tensor of ones is passed to this argument.
# reshape logits to be a 3-d tensor for sequence loss
logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])

# use the contrib sequence loss and average over the batches
loss = tf.contrib.id195.sequence_loss(
            logits,
            self.input_obj.targets,
            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),
            average_across_timesteps=false,
            average_across_batch=true)
# update the cost
self.cost = tf.reduce_sum(loss)

   there are two more important arguments for this function
       average_across_timesteps and average_across_batch.
   if average_across_timesteps is set to true, the cost will be summed
   across the time dimension, if average_across_batch is true, then the
   cost will be summed across the batch dimension. in this case we are
   favoring the latter option.

   finally, we produce the cost operation which reduces the loss to a
   single scalar value     we could also do something similar by
   setting average_across_timesteps to true     however, i am keeping things
   consistent with the tensorflow tutorial.

   in the next few steps, we set up some operations to calculate the
   accuracy off predictions over the batch samples:
# get the prediction accuracy
self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))
self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)
correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [
-1]))
self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   first we apply a softmax operation to get the predicted probabilities
   of each word for each output of the lstm network. we then make the
   network predictions equal to those words with the highest softmax
   id203 by using the argmax function. these predictions are then
   compared to the actual target words and then averaged to get the
   accuracy.

   now we move onto constructing the optimization operations     in this
   case we aren   t using a simple    out of the box    optimizer     rather we
   are doing a few manipulations to improve results:
if not is_training:
   return
self.learning_rate = tf.variable(0.0, trainable=false)

tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)
optimizer = tf.train.gradientdescentoptimizer(self.learning_rate)
self.train_op = optimizer.apply_gradients(
            zip(grads, tvars),
            global_step=tf.contrib.framework.get_or_create_global_step())

   first off, if the model has been created for predictions, validations
   or testing only, these operations do not need to be created. the first
   step if the model is being used for training, is to create a learning
   rate variable. this will be used so that we can decrease the learning
   rate during training     this improves the final outcome of the model.

   next we wish to clip the size of the gradients in our network during
   back-propagation     this is recommended in recurrent neural networks to
   improve outcomes. clipping values of between 1 and 5 are commonly used.
   finally, we create the optimizer operation, using the
   learning_rate variable, and apply the clipped gradients.. then a
   id119 step is performed     assigning this operation
   to train_op. this operation, train_op, will be called for each training
   batch.

   the final two lines of the model creation involve the updating of
   the learning_rate:
self.new_lr = tf.placeholder(tf.float32, shape=[])
self.lr_update = tf.assign(self.learning_rate, self.new_lr)

   first, a placeholder is created which will be input via the feed_dict
   argument when running the training, new_lr. this new learning rate is
   then assigned to learning_rate via a tf.assign operation. this
   operation, lr_update, will be run at the beginning of each epoch.

   now that the model structure is fully created, we can move onto the
   training loops:

training the lstm model

   the training function will take as input the training data, along with
   various model parameters (batch sizes, number of steps etc.). the first
   part of the function looks like:
def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save
_name,
          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93):
    # setup data and models
    training_input = input(batch_size=batch_size, num_steps=35, data=train_data)
    m = model(training_input, is_training=true, hidden_size=650, vocab_size=voca
bulary,
              num_layers=num_layers)
    init_op = tf.global_variables_initializer()

   first we create an input object instance and a model object instance,
   passing in the necessary parameters. because the tensorflow graph is
   being created during the initialization of these objects, the
   tensorflow global variable initializer operation can only be properly
   run after the creation of these instances.
orig_decay = lr_decay
with tf.session() as sess:
    # start threads
    sess.run([init_op])
    coord = tf.train.coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    saver = tf.train.saver()

   next we start the session, and run the variable initializer operation.
   because we are using queuing in the input object, we also need to
   create a thread coordinator and start the running of the threads (for
   more information, see [44]this tutorial). if you skip this step, or put
   it before the creation of training_input, your program will hang.
   finally, a saver instance is created as we want to store model training
   checkpoints and the final trained model.

   next, the epochal training loop is entered into:
for epoch in range(num_epochs):
    new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)
    m.assign_lr(sess, learning_rate * new_lr_decay)
    current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))
    for step in range(training_input.epoch_size):
        if step % 50 != 0:
            cost, _, current_state = sess.run([m.cost, m.train_op, m.state],
                                                             feed_dict={m.init_s
tate: current_state})
        else:
            cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state,
 m.accuracy],
                                                      feed_dict={m.init_state: c
urrent_state})
            print("epoch {}, step {}, cost: {:.3f}, accuracy: {:.3f}".format(epo
ch, step, cost, acc))
    # save a model checkpoint
    saver.save(sess, data_path + '\\' + model_save_name, global_step=epoch)
# do a final save
saver.save(sess, data_path + '\\' + model_save_name + '-final')
# close threads
coord.request_stop()
coord.join(threads)

   the first step in every epoch is to calculate the learning rate decay
   factor, which gradually decreases after max_lr_epoch number of epochs
   has been reached. this learning rate decay factor, new_lr_decay, is
   multiplied by the learning rate and assigned to the model by calling
   the model method assign_lr. this method looks like:
def assign_lr(self, session, lr_value):
    session.run(self.lr_update, feed_dict={self.new_lr: lr_value})

   as can be observed, this function simply runs the lr_update operation
   which was explained in the prior section.

   the next step is to create a zeroed initial state tensor for our lstm
   model     we assign this zeroed tensor to the variable current_state.
   then each training operation is looped through within our specified
   epoch size. every iteration we run the following operations: m.train_op
   and m.state. the train_op operation, as previously shown, calculates
   the clipped gradients of the model and takes a batched step to minimize
   the cost. the state operation returns the state of the final unrolled
   lstm cell which we will require to input as the state for the next
   training batch     note that it replaces the contents of
   the current_state variable. this current_state variable is inserted
   into the m.init_state placeholder via the feed_dict.

   every 50 iterations we also extract the current cost of the model in
   training, as well as the accuracy against the current training batch,
   to provide printed feedback during training. the outputs look like
   this:

     epoch 9, step 1850, cost: 96.185, accuracy: 0.198
     epoch 9, step 1900, cost: 94.755, accuracy: 0.235

   finally, at the end of each epoch, we use the saver object to save a
   model checkpoint, and finally at the end of the training a final save
   of the state of the model is performed.

expected training outcomes

   the expected cost and accuracy progress through the epochs depends on
   the multitude of parameters supplied to the models and also the results
   of the random initialization of the variables. training time is also
   dependent on whether you are using only cpus, or whether you are using
   gpus too (note, i have not tested the code on the github repository
   with gpus).

   my model achieved an average cost and training batch accuracy on the
   order of 110-120 and 30%, respectively, after 38 epochs with the
   following paramters:

   hidden size:650, number of steps:35, initialization scale:0.05, batch
   size:20, number of stacked lstm layers:2, keep id203 / dropout:
   0.5

   you are probably thinking the accuracy isn   t very high, and you are
   correct, however further training and a larger hidden layer would
   provide better final accuracy values. to perform further training on a
   larger network you really need to be using gpus to accelerate the
   training     i   ll do this in a future post and present the results.

testing the model

   to test the model on the test or validation data, i   ve created another
   function called test which looks like so:
def test(model_path, test_data, reversed_dictionary):
    test_input = input(batch_size=20, num_steps=35, data=test_data)
    m = model(test_input, is_training=false, hidden_size=650, vocab_size=vocabul
ary,
              num_layers=2)
    saver = tf.train.saver()
    with tf.session() as sess:
        # start threads
        coord = tf.train.coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        current_state = np.zeros((2, 2, m.batch_size, m.hidden_size))
        # restore the trained model
        saver.restore(sess, model_path)
        # get an average accuracy over num_acc_batches
        num_acc_batches = 30
        check_batch_idx = 25
        acc_check_thresh = 5
        accuracy = 0
        for batch in range(num_acc_batches):
            if batch == check_batch_idx:
                true_vals, pred, current_state, acc = sess.run([m.input_obj.targ
ets, m.predict, m.state, m.accuracy],
                                                               feed_dict={m.init
_state: current_state})
                pred_string = [reversed_dictionary[x] for x in pred[:m.num_steps
]]
                true_vals_string = [reversed_dictionary[x] for x in true_vals[0]
]
                print("true values (1st line) vs predicted values (2nd line):")
                print(" ".join(true_vals_string))
                print(" ".join(pred_string))
            else:
                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={
m.init_state: current_state})
            if batch >= acc_check_thresh:
                accuracy += acc
        print("average accuracy: {:.3f}".format(accuracy / (num_acc_batches-acc_
check_thresh)))
        # close threads
        coord.request_stop()
        coord.join(threads)

   we start with creating an input and model class that matches our
   training input and model classes. it is important that key parameters
   match the training model, such as the hidden size, number of steps,
   batch size etc. we are going to load our saved model variables into the
   computational graph created by the test model instance, and if the
   dimensions don   t match tensorflow will throw an error.

   next we create a tf.train.saver() operation     this will load all our
   saved model variables into our test model when we run the
   line saver.restore(sess, model_path). after dealing with all of the
   threads and creating a zeroed state variable, we setup some variables
   which relate to how we are going to assess the accuracy and look at
   some specific instances of predicted strings. because we have to    warm
   up    the model by feeding it some data to get good state variables, we
   only measure the accuracy after a certain number of batches
   i.e. acc_check_thresh.

   when the batch number is equal to check_batch_idx the code runs
   the m.predict operation to extract the predictions for the particular
   batch of data. the first prediction of the batch is passed through the
   reverse dictionary to convert them back to actual words (along with the
   batch target words) and then compared with what should have been
   predicted via printing.

   using the trained model, we can see the following output:

   true values (1st line) vs predicted values (2nd line):
   stock market is headed many traders were afraid to trust stock prices
   quoted on the big board <eos> the futures halt was even <unk> by big
   board floor traders <eos> it <unk> things up said
   market market is n   t for traders say willing to buy the prices <eos>
   <eos> the big board <eos> the dow market is a worse <eos> the board
   traders traders <eos> the    s the to to
   average accuracy: 0.283

   the accuracy isn   t fantastic, but you can see the network is matching
   the    gist    of the sentence i.e. not producing all of the exact words
   but matching the general subject matter. as i mentioned above, in a
   future post i   ll present the data from a model trained for longer using
   gpus.

   i hope you enjoyed the post     it   s been a long one, but i hope that
   this gives you a solid foundation in understanding recurrent neural
   networks and lstms and how to implement them in tensorflow. if
   you   d like to learn how to build id137 in keras, see [45]this
   tutorial.
     __________________________________________________________________

eager to learn more? get the book [46]here
     __________________________________________________________________


about the author

     dime says:
   [47]october 19, 2017 at 12:23 pm

   kudos!

     kumar says:
   [48]november 8, 2017 at 2:24 am

   i learned a lot from this tutorial. thank! just one question. in the
   image    lstm sample many-to-many classifier   , should the indices go from
   x0   x35, likewise h0   h35. in the current illustration, i do not
   understand why there is feedback within a batch (i.e., across rows    
   which is of size 20). please clarify. thanks.

     lew says:
   [49]november 10, 2017 at 9:07 pm

   can   t find the code on your github site. how can i access it?
   cheers
     * andy says:
       [50]november 10, 2017 at 9:56 pm
       hi lew     the file is called lstm_tutorial.py @
       [51]https://github.com/adventuresinml/adventures-in-ml-code

     mark says:
   [52]november 28, 2017 at 7:08 pm

   another excellent post andy, thanks for taking the time.
   i will make an attempt on gpu also, so that when you post the update to
   this i can see how on track i was.

     jon says:
   [53]january 6, 2018 at 12:09 am

   hi andy,

   thank you for all the time you   ve spent making these neural network
   tutorials, they have helped me a great deal. i have a question, though,
   that i haven   t seen specifically answered anywhere and was wondering if
   you would be able to help.

   do you know if tensorflow lstms can handle multiple datastreams at
   once? working from your example, it would be like having 4 35  650
   inputs at once, for a 20x4x35x650 input. i know i could convert
   4x35x650 into 140  650 but i was wondering if there was a more elegant
   way to do this that would allow all four inputs to be considered at
   once instead of in some sort of sequential fashion.

   thanks for your time

     jack says:
   [54]february 6, 2018 at 3:31 pm

   hi andy, i have the same question that kumar asked before:
   > in the image    lstm sample many-to-many classifier   , should the
   indices go from x0   x35, likewise h0   h35. in the current illustration, i
   do not understand why there is feedback within a batch (i.e., across
   rows     which is of size 20)

   could you please help me on this? thanks!     
     * andy says:
       [55]february 6, 2018 at 8:16 pm
       hi jack and kumar     you are correct, i have updated the diagram
       now. thanks for picking it up
          + jack says:
            [56]february 7, 2018 at 2:40 pm
            thank you for the tutorial and the answer!

   ____________________ (button)

   recent posts
     * [57]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [58]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [59]keras, eager and tensorflow 2.0     a new tf paradigm
     * [60]introduction to tensorboard and tensorflow visualization
     * [61]tensorflow eager tutorial

   recent comments
     * andry on [62]neural networks tutorial     a pathway to deep learning
     * sandipan on [63]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [64]neural networks tutorial     a pathway to deep learning
     * martin on [65]neural networks tutorial     a pathway to deep learning
     * uri on [66]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [67]march 2019
     * [68]january 2019
     * [69]october 2018
     * [70]september 2018
     * [71]august 2018
     * [72]july 2018
     * [73]june 2018
     * [74]may 2018
     * [75]april 2018
     * [76]march 2018
     * [77]february 2018
     * [78]november 2017
     * [79]october 2017
     * [80]september 2017
     * [81]august 2017
     * [82]july 2017
     * [83]may 2017
     * [84]april 2017
     * [85]march 2017

   categories
     * [86]amazon aws
     * [87]cntk
     * [88]convolutional neural networks
     * [89]cross id178
     * [90]deep learning
     * [91]gensim
     * [92]gpus
     * [93]keras
     * [94]id168s
     * [95]lstms
     * [96]neural networks
     * [97]nlp
     * [98]optimisation
     * [99]pytorch
     * [100]recurrent neural networks
     * [101]id23
     * [102]tensorboard
     * [103]tensorflow
     * [104]tensorflow 2.0
     * [105]weight initialization
     * [106]id97

   meta
     * [107]log in
     * [108]entries rss
     * [109]comments rss
     * [110]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [111]thrive themes | powered by [112]wordpress

   (button) close dialog

   session expired

   [113]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[114]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/
  13. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
  14. http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments
  15. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  16. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  17. https://adventuresinmachinelearning.com/keras-lstm-tutorial/
  18. https://github.com/adventuresinml/adventures-in-ml-code
  19. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  20. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  21. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  22. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  23. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  24. https://adventuresinmachinelearning.com/gensim-id97-tutorial/
  25. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  26. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/
  27. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  28. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  29. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  30. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  31. https://github.com/tensorflow/models/tree/master/tutorials/id56/ptb
  32. https://catalog.ldc.upenn.edu/ldc99t42
  33. https://github.com/adventuresinml/adventures-in-ml-code
  34. http://www.fit.vutbr.cz/~imikolov/id56lm/simple-examples.tgz
  35. https://github.com/tensorflow/models/tree/master/tutorials/id56/ptb
  36. https://adventuresinmachinelearning.com/introduction-tensorflow-queuing/
  37. https://www.tensorflow.org/performance/performance_guide#input_pipeline_optimization
  38. https://adventuresinmachinelearning.com/introduction-tensorflow-queuing/
  39. https://docs.python.org/3/tutorial/classes.html
  40. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  41. https://adventuresinmachinelearning.com/gensim-id97-tutorial/
  42. https://en.wikipedia.org/wiki/dropout_(neural_networks)
  43. https://www.tensorflow.org/api_docs/python/tf/contrib/id195/sequence_loss
  44. https://adventuresinmachinelearning.com/introduction-tensorflow-queuing/
  45. https://adventuresinmachinelearning.com/keras-lstm-tutorial/
  46. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  47. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4941
  48. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4943
  49. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4944
  50. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4945
  51. https://github.com/adventuresinml/adventures-in-ml-code
  52. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4946
  53. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4952
  54. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4953
  55. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4954
  56. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comments/4955
  57. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  58. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  59. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  60. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  61. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  62. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  63. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  64. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  65. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  66. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  67. https://adventuresinmachinelearning.com/2019/03/
  68. https://adventuresinmachinelearning.com/2019/01/
  69. https://adventuresinmachinelearning.com/2018/10/
  70. https://adventuresinmachinelearning.com/2018/09/
  71. https://adventuresinmachinelearning.com/2018/08/
  72. https://adventuresinmachinelearning.com/2018/07/
  73. https://adventuresinmachinelearning.com/2018/06/
  74. https://adventuresinmachinelearning.com/2018/05/
  75. https://adventuresinmachinelearning.com/2018/04/
  76. https://adventuresinmachinelearning.com/2018/03/
  77. https://adventuresinmachinelearning.com/2018/02/
  78. https://adventuresinmachinelearning.com/2017/11/
  79. https://adventuresinmachinelearning.com/2017/10/
  80. https://adventuresinmachinelearning.com/2017/09/
  81. https://adventuresinmachinelearning.com/2017/08/
  82. https://adventuresinmachinelearning.com/2017/07/
  83. https://adventuresinmachinelearning.com/2017/05/
  84. https://adventuresinmachinelearning.com/2017/04/
  85. https://adventuresinmachinelearning.com/2017/03/
  86. https://adventuresinmachinelearning.com/category/amazon-aws/
  87. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  88. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  89. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  90. https://adventuresinmachinelearning.com/category/deep-learning/
  91. https://adventuresinmachinelearning.com/category/nlp/gensim/
  92. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  93. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  94. https://adventuresinmachinelearning.com/category/loss-functions/
  95. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  96. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  97. https://adventuresinmachinelearning.com/category/nlp/
  98. https://adventuresinmachinelearning.com/category/optimisation/
  99. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
 100. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
 101. https://adventuresinmachinelearning.com/category/reinforcement-learning/
 102. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
 103. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
 104. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
 105. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
 106. https://adventuresinmachinelearning.com/category/nlp/id97/
 107. https://adventuresinmachinelearning.com/wp-login.php
 108. https://adventuresinmachinelearning.com/feed/
 109. https://adventuresinmachinelearning.com/comments/feed/
 110. https://wordpress.org/
 111. https://www.thrivethemes.com/
 112. http://www.wordpress.org/
 113. https://adventuresinmachinelearning.com/wp-login.php
 114. http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/

   hidden links:
 116. https://adventuresinmachinelearning.com/author/admin/
