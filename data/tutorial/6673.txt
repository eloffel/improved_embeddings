   this app works best with javascript enabled.

   [1]

spacy

   [usage___]
     * [2]usage
     * [3]models
     * [4]api
     * [5]universe
     *

   ____________________
    guides
       [guides     linguistic features____..]
          + get started
          + [6]installation
          + [7]models & languages
          + [8]facts & figures
          + [9]spacy 101
          + [10]new in v2.1
          + [11]new in v2.0
          + guides
          + [12]linguistic features
               o [13]id52
               o [14]dependency parse
               o [15]named entities
               o [16]id121
               o [17]merging & splitting
               o [18]sentence segmentation
          + [19]rule-based matching
          + [20]processing pipelines
          + [21]vectors & similarity
          + [22]training models
          + [23]saving & loading
          + [24]adding languages
          + [25]visualizers
          + in-depth
          + [26]code examples

[27]linguistic features

   processing raw text intelligently is difficult: most words are rare,
   and it   s common for words that look completely different to mean almost
   the same thing. the same words in a different order can mean something
   completely different. even splitting text into useful word-like units
   can be difficult in many languages. while it   s possible to solve some
   problems starting from only the raw characters, it   s usually better to
   use linguistic knowledge to add useful information. that   s exactly what
   spacy is designed to do: you put in raw text, and get back a [28]doc
   object, that comes with a variety of annotations.

[29]part-of-speech tagging needs model

   after id121, spacy can parse and tag a given doc. this is where
   the statistical model comes in, which enables spacy to make a
   prediction of which tag or label most likely applies in this context. a
   model consists of binary data and is produced by showing a system
   enough examples for it to make predictions that generalize across the
   language     for example, a word following    the    in english is most
   likely a noun.

   linguistic annotations are available as [30]token attributes. like many
   nlp libraries, spacy encodes all strings to hash values to reduce
   memory usage and improve efficiency. so to get the readable string
   representation of an attribute, we need to add an underscore _ to its
   name:
import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'apple is looking at buying u.k. startup for $1 billion')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)

     * text: the original word text.
     * lemma: the base form of the word.
     * pos: the simple part-of-speech tag.
     * tag: the detailed part-of-speech tag.
     * dep: syntactic dependency, i.e. the relation between tokens.
     * shape: the word shape     capitalization, punctuation, digits.
     * is alpha: is the token an alpha character?
     * is stop: is the token part of a stop list, i.e. the most common
       words of the language?

    text    lemma   pos  tag   dep    shape alpha stop
   apple   apple   propn nnp nsubj    xxxxx true  false
   is      be      verb  vbz aux      xx    true  true
   looking look    verb  vbg root     xxxx  true  false
   at      at      adp   in  prep     xx    true  true
   buying  buy     verb  vbg pcomp    xxxx  true  false
   u.k.    u.k.    propn nnp compound x.x.  false false
   startup startup noun  nn  dobj     xxxx  true  false
   for     for     adp   in  prep     xxx   true  true
   $       $       sym   $   quantmod $     false false
   1       1       num   cd  compound d     false false
   billion billion num   cd  probj    xxxx  true  false

tip: understanding tags and labels

   most of the tags and labels look pretty abstract, and they vary between
   languages. spacy.explain will show you a short description     for
   example, spacy.explain("vbz") returns    verb, 3rd person singular
   present   .

   using spacy   s built-in [31]displacy visualizer, here   s what our example
   sentence and its dependencies look like:

   iframe: [32]data:text/html,<html><head><meta
   charset="utf-8"></head><body>%3csvg%0a%20%20%20%20xmlns%3d%22http%3a%2f
   %2fwww.w3.org%2f2000%2fsvg%22%0a%20%20%20%20xmlns%3axlink%3d%22http%3a%
   2f%2fwww.w3.org%2f1999%2fxlink%22%0a%20%20%20%20id%3d%22e109581593f245c
   e9c4ac12f78e0c74e-0%22%0a%20%20%20%20class%3d%22displacy%22%0a%20%20%20
   %20width%3d%221975%22%0a%20%20%20%20height%3d%22399.5%22%0a%20%20%20%20
   style%3d%22max-width%3a%20none%3b%20height%3a%20399.5px%3b%20color%3a%2
   0%23000000%3b%20background%3a%20%23ffffff%3b%20font-family%3a%20arial%2
   2%0a%3e%0a%20%20%20%20%3ctext%20class%3d%22displacy-token%22%20fill%3d%
   22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%
   20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d
   %22currentcolor%22%20x%3d%2250%22%3eapple%3c%2ftspan%3e%0a%20%20%20%20%
   20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fil
   l%3d%22currentcolor%22%20x%3d%2250%22%3epropn%3c%2ftspan%3e%0a%20%20%20
   %20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displacy-token%2
   2%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309
   .5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-word%
   22%20fill%3d%22currentcolor%22%20x%3d%22225%22%3eis%3c%2ftspan%3e%0a%20
   %20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222e
   m%22%20fill%3d%22currentcolor%22%20x%3d%22225%22%3everb%3c%2ftspan%3e%0
   a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displa
   cy-token%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20
   y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22disp
   lacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22400%22%3elooking%3c%2
   ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%
   22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%22400%22%3everb%
   3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20cl
   ass%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20text-anchor%3d
   %22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20
   class%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22575%22
   %3eat%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22dis
   placy-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%22575%
   22%3eadp%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3c
   text%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20text-
   anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3
   ctspan%20class%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d
   %22750%22%3ebuying%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20c
   lass%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%
   20x%3d%22750%22%3everb%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%
   20%20%20%20%3ctext%20class%3d%22displacy-token%22%20fill%3d%22currentco
   lor%22%20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20
   %20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d%22currentc
   olor%22%20x%3d%22925%22%3eu.k.%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20
   %3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22curr
   entcolor%22%20x%3d%22925%22%3epropn%3c%2ftspan%3e%0a%20%20%20%20%3c%2ft
   ext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displacy-token%22%20fill%3
   d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0
   a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%
   3d%22currentcolor%22%20x%3d%221100%22%3estartup%3c%2ftspan%3e%0a%20%20%
   20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22
   %20fill%3d%22currentcolor%22%20x%3d%221100%22%3enoun%3c%2ftspan%3e%0a%2
   0%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displacy-
   token%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3
   d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displac
   y-word%22%20fill%3d%22currentcolor%22%20x%3d%221275%22%3efor%3c%2ftspan
   %3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20d
   y%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%221275%22%3eadp%3c%2ft
   span%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d
   %22displacy-token%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22mid
   dle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%
   3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%221450%22%3e%2
   4%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displac
   y-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%221450%22%
   3esym%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctex
   t%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20text-anc
   hor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3cts
   pan%20class%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22
   1625%22%3e1%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d
   %22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%
   221625%22%3enum%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%2
   0%20%3ctext%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%
   20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%
   20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d%22currentcolor%22
   %20x%3d%221800%22%3ebillion%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3c
   tspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22current
   color%22%20x%3d%221800%22%3enum%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%
   3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%
   20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22
   displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e10
   9581593f245ce9c4ac12f78e0c74e-0-0%22%0a%20%20%20%20%20%20%20%20%20%20%2
   0%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d
   %22m70%2c264.5%20c70%2c89.5%20395.0%2c89.5%20395.0%2c264.5%22%0a%20%20%
   20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%20%
   20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f
   %3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22
   font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%
   20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0c7
   4e-0-0%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22
   displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20sta
   rtoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%
   20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%2
   0%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20nsubj%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%2
   0%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%20class%3d%22displacy-
   arrowhead%22%20d%3d%22m70%2c266.5%20l62%2c254.5%2078%2c254.5%22%20fill%
   3d%22currentcolor%22%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%2
   0%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpa
   th%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%0a%
   20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4ac1
   2f78e0c74e-0-1%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width%3d
   %222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m245%2c264.5%20c
   245%2c177.0%20390.0%2c177.0%20390.0%2c264.5%22%0a%20%20%20%20%20%20%20%
   20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20s
   troke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20
   %20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%20
   0.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%
   20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20x
   link%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-1%22%0a%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label%
   22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%225
   0%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22cur
   rentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-anc
   hor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20%20%20%20%20aux%0a%20%20%20%20%20%20%20%20%
   20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a
   %20%20%20%20%20%20%20%20%3cpath%20class%3d%22displacy-arrowhead%22%20d%
   3d%22m245%2c266.5%20l237%2c254.5%20253%2c254.5%22%20fill%3d%22currentco
   lor%22%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%
   3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20
   %20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%20
   %20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-2
   %22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20d%3d%22m420%2c264.5%20c420%2c177.0%20
   565.0%2c177.0%20565.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20
   fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22cur
   rentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%2
   0%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20let
   ter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctex
   tpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d
   %22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-2%22%0a%20%20%20%20%20%2
   0%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label%22%0a%20%20%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%20%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0
   a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22middl
   e%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%
   20%20%20%20%20%20%20%20%20prep%0a%20%20%20%20%20%20%20%20%20%20%20%20%3
   c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%
   20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22dis
   placy-arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m565.0%
   2c266.5%20l573.0%2c254.5%20557.0%2c254.5%22%0a%20%20%20%20%20%20%20%20%
   20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0
   a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arr
   ow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%2
   0%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%2
   0%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-3%22%0a%20%20%20%2
   0%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%
   20%20%20%20%20%20d%3d%22m595%2c264.5%20c595%2c177.0%20740.0%2c177.0%207
   40.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22
   %0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22currentcolor%22%0a%2
   0%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%
   221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%20
   1px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e1095
   81593f245ce9c4ac12f78e0c74e-0-3%22%0a%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20class%3d%22displacy-label%22%0a%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20
   %20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%
   20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22middle%22%0a%20%20%20%
   20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20
   %20%20%20pcomp%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%
   0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpa
   th%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arrowhead%
   22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m740.0%2c266.5%20l748.0
   %2c254.5%20732.0%2c254.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%
   3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%3c
   %2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%2
   0%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%
   3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arro
   w-e109581593f245ce9c4ac12f78e0c74e-0-4%22%0a%20%20%20%20%20%20%20%20%20
   %20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0d%3d%22m945%2c264.5%20c945%2c177.0%201090.0%2c177.0%201090.0%2c264.5%2
   2%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%20%20%2
   0%20%20%20%20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%20%20%
   20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20
   style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4
   ac12f78e0c74e-0-4%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20
   class%3d%22displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%20%20
   %20%20%20%20%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20%20%20
   %20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20compo
   und%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20
   %20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%20class%
   3d%22displacy-arrowhead%22%20d%3d%22m945%2c266.5%20l937%2c254.5%20953%2
   c254.5%22%20fill%3d%22currentcolor%22%20%2f%3e%0a%20%20%20%20%3c%2fg%3e
   %0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20
   %20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22di
   splacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e1095
   81593f245ce9c4ac12f78e0c74e-0-5%22%0a%20%20%20%20%20%20%20%20%20%20%20%
   20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%2
   2m770%2c264.5%20c770%2c89.5%201095.0%2c89.5%201095.0%2c264.5%22%0a%20%2
   0%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%2
   0%20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%
   2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%
   22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%2
   0%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0
   c74e-0-5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%
   22displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20s
   tartoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%2
   0%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20
   %20%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20
   %20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20dobj%0a%20%20%
   20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%
   20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%
   20%20%20%20%20%20class%3d%22displacy-arrowhead%22%0a%20%20%20%20%20%20%
   20%20%20%20%20%20d%3d%22m1095.0%2c266.5%20l1103.0%2c254.5%201087.0%2c25
   4.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%
   0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%2
   0%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3
   cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%
   0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4
   ac12f78e0c74e-0-6%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width
   %3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m770%2c264.5%
   20c770%2c2.0%201275.0%2c2.0%201275.0%2c264.5%22%0a%20%20%20%20%20%20%20
   %20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20
   stroke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%2
   0%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%2
   00.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20
   %20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20
   xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-6%22%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label
   %22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%22
   50%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22cu
   rrentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-an
   chor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20prep%0a%20%20%20%20%20%20%20%2
   0%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%
   0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0class%3d%22displacy-arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0d%3d%22m1275.0%2c266.5%20l1283.0%2c254.5%201267.0%2c254.5%22%0a%20%20%
   20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%2
   0%20%20%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class
   %3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%2
   0%20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-
   7%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%
   20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m1470%2c264.5%20c1470%2c89.5%
   201795.0%2c89.5%201795.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20
   %20fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22
   currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%2
   0%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20
   letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c
   textpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref
   %3d%22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-7%22%0a%20%20%20%20%2
   0%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label%22%0a%20%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%2
   2%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22mi
   ddle%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%
   20%20%20%20%20%20%20%20%20%20quantmod%0a%20%20%20%20%20%20%20%20%20%20%
   20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20
   %20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3
   d%22displacy-arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22
   m1470%2c266.5%20l1462%2c254.5%201478%2c254.5%22%0a%20%20%20%20%20%20%20
   %20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%
   3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy
   -arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%
   20%20%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%
   20%20%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-8%22%0a%20%20%
   20%20%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20
   %20%20%20%20%20%20%20d%3d%22m1645%2c264.5%20c1645%2c177.0%201790.0%2c17
   7.0%201790.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%2
   2none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22currentcolor
   %22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%
   20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20letter-spaci
   ng%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arr
   ow-e109581593f245ce9c4ac12f78e0c74e-0-8%22%0a%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20%20class%3d%22displacy-label%22%0a%20%20%20%20%20%20
   %20%20%20%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%
   20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22middle%22%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20%20compound%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ft
   extpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20
   %20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy
   -arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m1645%2c266.
   5%20l1637%2c254.5%201653%2c254.5%22%0a%20%20%20%20%20%20%20%20%20%20%20
   %20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%
   20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e
   %0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%
   20class%3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3
   d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-9%22%0a%20%20%20%20%20%20%
   20%20%20%20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20
   %20%20%20d%3d%22m1295%2c264.5%20c1295%2c2.0%201800.0%2c2.0%201800.0%2c2
   64.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%2
   0%20%20%20%20%20%20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%
   20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em
   %22%20style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3
   e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%2
   0%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f24
   5ce9c4ac12f78e0c74e-0-9%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20
   %20%20class%3d%22displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20
   %20%20%20%20%20%20%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20
   %20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%2
   0pobj%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%
   20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%
   20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arrowhead%22%0a%20%
   20%20%20%20%20%20%20%20%20%20%20d%3d%22m1800.0%2c266.5%20l1808.0%2c254.
   5%201792.0%2c254.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22c
   urrentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%3c%2fg%3
   e%0a%3c%2fsvg%3e%0a</body></html>

[33]rule-based morphology

   inflectional morphology is the process by which a root form of a word
   is modified by adding prefixes or suffixes that specify its grammatical
   function but do not changes its part-of-speech. we say that a lemma
   (root form) is inflected (modified/combined) with one or more
   morphological features to create a surface form. here are some
   examples:
   context surface lemma pos  morphological features
   i was reading the paper reading read verb verbform=ger
   i don   t watch the news, i read the paper read read verb verbform=fin,
   mood=ind, tense=pres
   i read the paper yesterday read read verb verbform=fin, mood=ind,
   tense=past

   english has a relatively simple morphological system, which spacy
   handles using rules that can be keyed by the token, the part-of-speech
   tag, or the combination of the two. the system works as follows:
    1. the tokenizer consults a [34]mapping tabletokenizer_exceptions,
       which allows sequences of characters to be mapped to multiple
       tokens. each token may be assigned a part of speech and one or more
       morphological features.
    2. the part-of-speech tagger then assigns each token an extended pos
       tag. in the api, these tags are known as token.tag. they express
       the part-of-speech (e.g. verb) and some amount of morphological
       information, e.g. that the verb is past tense.
    3. for words whose pos is not set by a prior process, a [35]mapping
       table tag_map maps the tags to a part-of-speech and a set of
       morphological features.
    4. finally, a rule-based deterministic lemmatizer maps the surface
       form, to a lemma in light of the previously assigned extended
       part-of-speech and morphological information, without consulting
       the context of the token. the lemmatizer also accepts list-based
       exception files, acquired from [36]id138.

     part-of-speech tag scheme

   for a list of the fine-grained and coarse-grained part-of-speech tags
   assigned by spacy   s models across different languages, see the [37]pos
   tag scheme documentation.

[38]id33 needs model

   spacy features a fast and accurate syntactic dependency parser, and has
   a rich api for navigating the tree. the parser also powers the sentence
   boundary detection, and lets you iterate over base noun phrases, or
      chunks   . you can check whether a [39]doc object has been parsed with
   the doc.is_parsed attribute, which returns a boolean value. if this
   attribute is false, the default sentence iterator will raise an
   exception.

[40]noun chunks

   noun chunks are    base noun phrases        flat phrases that have a noun as
   their head. you can think of noun chunks as a noun plus the words
   describing the noun     for example,    the lavish green grass    or    the
   world   s largest tech fund   . to get the noun chunks in a document,
   simply iterate over [41]doc.noun_chunks
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"id101 shift insurance liability toward manufacturers")
for chunk in doc.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_,
            chunk.root.head.text)

     * text: the original noun chunk text.
     * root text: the original text of the word connecting the noun chunk
       to the rest of the parse.
     * root dep: dependency relation connecting the root to its head.
     * root head text: the text of the root token   s head.

          text           root.text   root.dep_ root.head.text
   id101     cars          nsubj     shift
   insurance liability liability     dobj      shift
   manufacturers       manufacturers pobj      toward

[42]navigating the parse tree

   spacy uses the terms head and child to describe the words connected by
   a single arc in the dependency tree. the term dep is used for the arc
   label, which describes the type of syntactic relation that connects the
   child to the head. as with other attributes, the value of .dep is a
   hash value. you can get the string value with .dep_.
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"id101 shift insurance liability toward manufacturers")
for token in doc:
    print(token.text, token.dep_, token.head.text, token.head.pos_,
            [child for child in token.children])

     * text: the original token text.
     * dep: the syntactic relation connecting child to head.
     * head text: the original text of the token head.
     * head pos: the part-of-speech tag of the token head.
     * children: the immediate syntactic dependents of the token.

       text        dep    head text head pos        children
   autonomous    amod     cars      noun
   cars          nsubj    shift     verb     autonomous
   shift         root     shift     verb     cars, liability, toward
   insurance     compound liability noun
   liability     dobj     shift     verb     insurance
   toward        prep     shift     noun     manufacturers
   manufacturers pobj     toward    adp

   iframe: [43]data:text/html,<html><head><meta
   charset="utf-8"></head><body>%3csvg%20xmlns%3d%22http%3a%2f%2fwww.w3.or
   g%2f2000%2fsvg%22%20xmlns%3axlink%3d%22http%3a%2f%2fwww.w3.org%2f1999%2
   fxlink%22%20id%3d%220%22%20class%3d%22displacy%22%20width%3d%221275%22%
   20height%3d%22399.5%22%20style%3d%22max-width%3a%20none%3b%20height%3a%
   20399.5px%3b%20color%3a%20%23000000%3b%20background%3a%20%23ffffff%3b%2
   0font-family%3a%20arial%22%3e%0a%3ctext%20class%3d%22displacy-token%22%
   20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309.5
   %22%3e%0a%20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d%2
   2currentcolor%22%20x%3d%2250%22%3eautonomous%3c%2ftspan%3e%0a%20%20%20%
   20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22cu
   rrentcolor%22%20x%3d%2250%22%3eadj%3c%2ftspan%3e%0a%3c%2ftext%3e%0a%0a%
   3ctext%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20tex
   t-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%3ctspan%20c
   lass%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22225%22%
   3ecars%3c%2ftspan%3e%0a%20%20%20%20%3ctspan%20class%3d%22displacy-tag%2
   2%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%22225%22%3enoun%3
   c%2ftspan%3e%0a%3c%2ftext%3e%0a%0a%3ctext%20class%3d%22displacy-token%2
   2%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309
   .5%22%3e%0a%20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d
   %22currentcolor%22%20x%3d%22400%22%3eshift%3c%2ftspan%3e%0a%20%20%20%20
   %3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22curr
   entcolor%22%20x%3d%22400%22%3everb%3c%2ftspan%3e%0a%3c%2ftext%3e%0a%0a%
   3ctext%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20tex
   t-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%3ctspan%20c
   lass%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22575%22%
   3einsurance%3c%2ftspan%3e%0a%20%20%20%20%3ctspan%20class%3d%22displacy-
   tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%22575%22%3en
   oun%3c%2ftspan%3e%0a%3c%2ftext%3e%0a%0a%3ctext%20class%3d%22displacy-to
   ken%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%
   22309.5%22%3e%0a%20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fi
   ll%3d%22currentcolor%22%20x%3d%22750%22%3eliability%3c%2ftspan%3e%0a%20
   %20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%
   3d%22currentcolor%22%20x%3d%22750%22%3enoun%3c%2ftspan%3e%0a%3c%2ftext%
   3e%0a%0a%3ctext%20class%3d%22displacy-token%22%20fill%3d%22currentcolor
   %22%20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%3c
   tspan%20class%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%
   22925%22%3etoward%3c%2ftspan%3e%0a%20%20%20%20%3ctspan%20class%3d%22dis
   placy-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%22925%
   22%3eadp%3c%2ftspan%3e%0a%3c%2ftext%3e%0a%0a%3ctext%20class%3d%22displa
   cy-token%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20
   y%3d%22309.5%22%3e%0a%20%20%20%20%3ctspan%20class%3d%22displacy-word%22
   %20fill%3d%22currentcolor%22%20x%3d%221100%22%3emanufacturers%3c%2ftspa
   n%3e%0a%20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%
   22%20fill%3d%22currentcolor%22%20x%3d%221100%22%3enoun%3c%2ftspan%3e%0a
   %3c%2ftext%3e%0a%0a%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%2
   0%3cpath%20class%3d%22displacy-arc%22%20id%3d%22arrow-0-0%22%20stroke-w
   idth%3d%222px%22%20d%3d%22m70%2c264.5%20c70%2c177.0%20215.0%2c177.0%202
   15.0%2c264.5%22%20fill%3d%22none%22%20stroke%3d%22currentcolor%22%3e%3c
   %2fpath%3e%0a%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-
   size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20
   %20%20%3ctextpath%20xlink%3ahref%3d%22%23arrow-0-0%22%20class%3d%22disp
   lacy-label%22%20startoffset%3d%2250%25%22%20fill%3d%22currentcolor%22%2
   0text-anchor%3d%22middle%22%3eamod%3c%2ftextpath%3e%0a%20%20%20%20%3c%2
   ftext%3e%0a%20%20%20%20%3cpath%20class%3d%22displacy-arrowhead%22%20d%3
   d%22m70%2c266.5%20l62%2c254.5%2078%2c254.5%22%20fill%3d%22currentcolor%
   22%3e%3c%2fpath%3e%0a%3c%2fg%3e%0a%0a%3cg%20class%3d%22displacy-arrow%2
   2%3e%0a%20%20%20%20%3cpath%20class%3d%22displacy-arc%22%20id%3d%22arrow
   -0-1%22%20stroke-width%3d%222px%22%20d%3d%22m245%2c264.5%20c245%2c177.0
   %20390.0%2c177.0%20390.0%2c264.5%22%20fill%3d%22none%22%20stroke%3d%22c
   urrentcolor%22%3e%3c%2fpath%3e%0a%20%20%20%20%3ctext%20dy%3d%221.25em%2
   2%20style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%
   0a%20%20%20%20%20%20%20%20%3ctextpath%20xlink%3ahref%3d%22%23arrow-0-1%
   22%20class%3d%22displacy-label%22%20startoffset%3d%2250%25%22%20fill%3d
   %22currentcolor%22%20text-anchor%3d%22middle%22%3ensubj%3c%2ftextpath%3
   e%0a%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%3cpath%20class%3d%22displa
   cy-arrowhead%22%20d%3d%22m245%2c266.5%20l237%2c254.5%20253%2c254.5%22%2
   0fill%3d%22currentcolor%22%3e%3c%2fpath%3e%0a%3c%2fg%3e%0a%0a%3cg%20cla
   ss%3d%22displacy-arrow%22%3e%0a%20%20%20%20%3cpath%20class%3d%22displac
   y-arc%22%20id%3d%22arrow-0-2%22%20stroke-width%3d%222px%22%20d%3d%22m59
   5%2c264.5%20c595%2c177.0%20740.0%2c177.0%20740.0%2c264.5%22%20fill%3d%2
   2none%22%20stroke%3d%22currentcolor%22%3e%3c%2fpath%3e%0a%20%20%20%20%3
   ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20letter
   -spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%3ctextpath%20xlink%3
   ahref%3d%22%23arrow-0-2%22%20class%3d%22displacy-label%22%20startoffset
   %3d%2250%25%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22
   %3ecompound%3c%2ftextpath%3e%0a%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20
   %3cpath%20class%3d%22displacy-arrowhead%22%20d%3d%22m595%2c266.5%20l587
   %2c254.5%20603%2c254.5%22%20fill%3d%22currentcolor%22%3e%3c%2fpath%3e%0
   a%3c%2fg%3e%0a%0a%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20%
   3cpath%20class%3d%22displacy-arc%22%20id%3d%22arrow-0-3%22%20stroke-wid
   th%3d%222px%22%20d%3d%22m420%2c264.5%20c420%2c89.5%20745.0%2c89.5%20745
   .0%2c264.5%22%20fill%3d%22none%22%20stroke%3d%22currentcolor%22%3e%3c%2
   fpath%3e%0a%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-si
   ze%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%2
   0%20%3ctextpath%20xlink%3ahref%3d%22%23arrow-0-3%22%20class%3d%22displa
   cy-label%22%20startoffset%3d%2250%25%22%20fill%3d%22currentcolor%22%20t
   ext-anchor%3d%22middle%22%3edobj%3c%2ftextpath%3e%0a%20%20%20%20%3c%2ft
   ext%3e%0a%20%20%20%20%3cpath%20class%3d%22displacy-arrowhead%22%20d%3d%
   22m745.0%2c266.5%20l753.0%2c254.5%20737.0%2c254.5%22%20fill%3d%22curren
   tcolor%22%3e%3c%2fpath%3e%0a%3c%2fg%3e%0a%0a%3cg%20class%3d%22displacy-
   arrow%22%3e%0a%20%20%20%20%3cpath%20class%3d%22displacy-arc%22%20id%3d%
   22arrow-0-4%22%20stroke-width%3d%222px%22%20d%3d%22m420%2c264.5%20c420%
   2c2.0%20925.0%2c2.0%20925.0%2c264.5%22%20fill%3d%22none%22%20stroke%3d%
   22currentcolor%22%3e%3c%2fpath%3e%0a%20%20%20%20%3ctext%20dy%3d%221.25e
   m%22%20style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%
   3e%0a%20%20%20%20%20%20%20%20%3ctextpath%20xlink%3ahref%3d%22%23arrow-0
   -4%22%20class%3d%22displacy-label%22%20startoffset%3d%2250%25%22%20fill
   %3d%22currentcolor%22%20text-anchor%3d%22middle%22%3eprep%3c%2ftextpath
   %3e%0a%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%3cpath%20class%3d%22disp
   lacy-arrowhead%22%20d%3d%22m925.0%2c266.5%20l933.0%2c254.5%20917.0%2c25
   4.5%22%20fill%3d%22currentcolor%22%3e%3c%2fpath%3e%0a%3c%2fg%3e%0a%0a%3
   cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20%3cpath%20class%3d%2
   2displacy-arc%22%20id%3d%22arrow-0-5%22%20stroke-width%3d%222px%22%20d%
   3d%22m945%2c264.5%20c945%2c177.0%201090.0%2c177.0%201090.0%2c264.5%22%2
   0fill%3d%22none%22%20stroke%3d%22currentcolor%22%3e%3c%2fpath%3e%0a%20%
   20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3
   b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%3ctextpath
   %20xlink%3ahref%3d%22%23arrow-0-5%22%20class%3d%22displacy-label%22%20s
   tartoffset%3d%2250%25%22%20fill%3d%22currentcolor%22%20text-anchor%3d%2
   2middle%22%3epobj%3c%2ftextpath%3e%0a%20%20%20%20%3c%2ftext%3e%0a%20%20
   %20%20%3cpath%20class%3d%22displacy-arrowhead%22%20d%3d%22m1090.0%2c266
   .5%20l1098.0%2c254.5%201082.0%2c254.5%22%20fill%3d%22currentcolor%22%3e
   %3c%2fpath%3e%0a%3c%2fg%3e%0a%3c%2fsvg%3e%0a</body></html>

   because the syntactic relations form a tree, every word has exactly one
   head. you can therefore iterate over the arcs in the tree by iterating
   over the words in the sentence. this is usually the best way to match
   an arc of interest     from below:
import spacy
from spacy.symbols import nsubj, verb

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"id101 shift insurance liability toward manufacturers")

# finding a verb with a subject from below     good
verbs = set()
for possible_subject in doc:
    if possible_subject.dep == nsubj and possible_subject.head.pos == verb:
        verbs.add(possible_subject.head)
print(verbs)

   if you try to match from above, you   ll have to iterate twice. once for
   the head, and then again through the children:
# finding a verb with a subject from above     less good
verbs = []
for possible_verb in doc:
    if possible_verb.pos == verb:
        for possible_subject in possible_verb.children:
            if possible_subject.dep == nsubj:
                verbs.append(possible_verb)
                break

   to iterate through the children, use the token.children attribute,
   which provides a sequence of [44]token objects.

[45]iterating around the local tree

   a few more convenience attributes are provided for iterating around the
   local tree from the token. [46]token.lefts and [47]token.rights
   attributes provide sequences of syntactic children that occur before
   and after the token. both sequences are in sentence order. there are
   also two integer-typed attributes, [48]token.n_lefts and
   [49]token.n_rights that give the number of left and right children.
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"bright red apples on the tree")
print([token.text for token in doc[2].lefts])  # ['bright', 'red']
print([token.text for token in doc[2].rights])  # ['on']
print(doc[2].n_lefts)  # 2
print(doc[2].n_rights)  # 1

import spacy

nlp = spacy.load("de_core_news_sm")
doc = nlp(u"sch  ne rote   pfel auf dem baum")
print([token.text for token in doc[2].lefts])  # ['sch  ne', 'rote']
print([token.text for token in doc[2].rights])  # ['auf']

   you can get a whole phrase by its syntactic head using the
   [50]token.subtree attribute. this returns an ordered sequence of
   tokens. you can walk up the tree with the [51]token.ancestors
   attribute, and check dominance with [52]token.is_ancestor

projective vs. non-projective

   for the [53]default english model, the parse tree is projective, which
   means that there are no crossing brackets. the tokens returned by
   .subtree are therefore guaranteed to be contiguous. this is not true
   for the german model, which has many [54]non-projective dependencies.
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"credit and mortgage account holders must submit their requests")

root = [token for token in doc if token.head == token][0]
subject = list(root.lefts)[0]
for descendant in subject.subtree:
    assert subject is descendant or subject.is_ancestor(descendant)
    print(descendant.text, descendant.dep_, descendant.n_lefts,
            descendant.n_rights,
            [ancestor.text for ancestor in descendant.ancestors])

     text     dep    n_lefts n_rights            ancestors
   credit   nmod     0       2        holders, submit
   and      cc       0       0        holders, submit
   mortgage compound 0       0        account, credit, holders, submit
   account  conj     1       0        credit, holders, submit
   holders  nsubj    1       0        submit

   finally, the .left_edge and .right_edge attributes can be especially
   useful, because they give you the first and last token of the subtree.
   this is the easiest way to create a span object for a syntactic phrase.
   note that .right_edge gives a token within the subtree     so if you use
   it as the end-point of a range, don   t forget to +1!
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"credit and mortgage account holders must submit their requests")
span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]
with doc.retokenize() as retokenizer:
    retokenizer.merge(span)
for token in doc:
    print(token.text, token.pos_, token.dep_, token.head.text)

                  text                  pos  dep  head text
   credit and mortgage account holders noun nsubj submit
   must                                verb aux   submit
   submit                              verb root  submit
   their                               adj  poss  requests
   requests                            noun dobj  submit

     dependency label scheme

   for a list of the syntactic dependency labels assigned by spacy   s
   models across different languages, see the [55]dependency label scheme
   documentation.

[56]visualizing dependencies

   the best way to understand spacy   s dependency parser is interactively.
   to make this easier, spacy v2.0+ comes with a visualization module. you
   can pass a doc or a list of doc objects to displacy and run
   [57]displacy.serve to run the web server, or [58]displacy.render to
   generate the raw markup. if you want to know how to write rules that
   hook into some type of syntactic construction, just plug the sentence
   into the visualizer and see how spacy annotates it.
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"id101 shift insurance liability toward manufacturers")
# since this is an interactive jupyter environment, we can use displacy.render h
ere
displacy.render(doc, style='dep')

   for more details and examples, see the [59]usage guide on visualizing
   spacy. you can also test displacy in our [60]online demo..

[61]disabling the parser

   in the [62]default models, the parser is loaded and enabled as part of
   the [63]standard processing pipeline. if you don   t need any of the
   syntactic information, you should disable the parser. disabling the
   parser will make spacy load and run much faster. if you want to load
   the parser, but need to disable it for specific documents, you can also
   control its use on the nlp object.
nlp = spacy.load("en_core_web_sm", disable=["parser"])
nlp = english().from_disk("/model", disable=["parser"])
doc = nlp(u"i don't want parsed", disable=["parser"])

important note: disabling pipeline components

   since spacy v2.0 comes with better support for customizing the
   processing pipeline components, the parser keyword argument has been
   replaced with disable, which takes a list of [64]pipeline component
   names. this lets you disable both default and custom components when
   loading a model, or initializing a language class via [65]from_disk.
+ nlp = spacy.load("en_core_web_sm", disable=["parser"])
+ doc = nlp(u"i don't want parsed", disable=["parser"])

- nlp = spacy.load("en_core_web_sm", parser=false)
- doc = nlp(u"i don't want parsed", parse=false)

[66]id39

   spacy features an extremely fast statistical entity recognition system,
   that assigns labels to contiguous spans of tokens. the default model
   identifies a variety of named and numeric entities, including
   companies, locations, organizations and products. you can add arbitrary
   classes to the entity recognition system, and update the model with new
   examples.

[67]id39 101

   a named entity is a    real-world object    that   s assigned a name     for
   example, a person, a country, a product or a book title. spacy can
   recognize[68]various types of named entities in a document, by asking
   the model for a prediction. because models are statistical and strongly
   depend on the examples they were trained on, this doesn   t always work
   perfectly and might need some tuning later, depending on your use case.

   named entities are available as the ents property of a doc:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"apple is looking at buying u.k. startup for $1 billion")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

     * text: the original entity text.
     * start: index of start of entity in the doc.
     * end: index of end of entity in the doc.
     * label: entity label, i.e. type.

    text    start end label                     description
 apple        0    5  org   companies, agencies, institutions.
 u.k.        27   31  gpe   geopolitical entity, i.e. countries, cities, states.
 $1 billion  44   54  money monetary values, including unit.

   using spacy   s built-in [69]displacy visualizer, here   s what our example
   sentence and its named entities look like:

   iframe: [70]data:text/html,<html><head><meta
   charset="utf-8"></head><body>%3cdiv%20class%3d%22entities%22%20style%3d
   %22line-height%3a%202.5%3b%20font-family%3a%20-apple-system%2c%20blinkm
   acsystemfont%2c%20'segoe%20ui'%2c%20helvetica%2c%20arial%2c%20sans-seri
   f%2c%20'apple%20color%20emoji'%2c%20'segoe%20ui%20emoji'%2c%20'segoe%20
   ui%20symbol'%3b%20font-size%3a%2018px%22%3ebut%20%0a%3cmark%20class%3d%
   22entity%22%20style%3d%22background%3a%20%237aecec%3b%20padding%3a%200.
   45em%200.6em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20b
   order-radius%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-web
   kit-box-decoration-break%3a%20clone%22%3egoogle%20%0a%3cspan%20style%3d
   %22font-size%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%
   201%3b%20border-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3
   b%20vertical-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg%3c
   %2fspan%3e%3c%2fmark%3eis%20starting%20from%20behind.%20the%20company%2
   0made%20a%20late%20push%20into%20hardware%2c%0aand%20%0a%3cmark%20class
   %3d%22entity%22%20style%3d%22background%3a%20%237aecec%3b%20padding%3a%
   200.45em%200.6em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b
   %20border-radius%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20
   -webkit-box-decoration-break%3a%20clone%22%3eapple%20%0a%3cspan%20style
   %3d%22font-size%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%
   3a%201%3b%20border-radius%3a%200.35em%3b%20text-transform%3a%20uppercas
   e%3b%20vertical-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg
   %3c%2fspan%3e%3c%2fmark%3e%e2%80%99s%20%0a%3cmark%20class%3d%22entity%2
   2%20style%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6
   em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radi
   us%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-de
   coration-break%3a%20clone%22%3esiri%20%0a%3cspan%20style%3d%22font-size
   %3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20bor
   der-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical
   -align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%
   3e%3c%2fmark%3e%2c%20available%20on%20%0a%3cmark%20class%3d%22entity%22
   %20style%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6e
   m%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radiu
   s%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-dec
   oration-break%3a%20clone%22%3eiphones%20%0a%3cspan%20style%3d%22font-si
   ze%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20b
   order-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertic
   al-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspa
   n%3e%3c%2fmark%3e%2c%20and%20%0a%3cmark%20class%3d%22entity%22%20style%
   3d%22background%3a%20%237aecec%3b%20padding%3a%200.45em%200.6em%3b%20ma
   rgin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.
   35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-decoration-b
   reak%3a%20clone%22%3eamazon%20%0a%3cspan%20style%3d%22font-size%3a%200.
   8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20border-radi
   us%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical-align%3
   a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg%3c%2fspan%3e%3c%2fmark
   %3e%e2%80%99s%20%0a%3cmark%20class%3d%22entity%22%20style%3d%22backgrou
   nd%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%20margin%3a%200%2
   00.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b%20box
   -decoration-break%3a%20clone%3b%20-webkit-box-decoration-break%3a%20clo
   ne%22%3ealexa%20%0a%3cspan%20style%3d%22font-size%3a%200.8em%3b%20font-
   weight%3a%20bold%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em
   %3b%20text-transform%3a%20uppercase%3b%20vertical-align%3a%20middle%3b%
   20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%2fmark%3esoftwar
   e%2c%20which%20runs%20on%20its%20%0a%3cmark%20class%3d%22entity%22%20st
   yle%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%
   20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radius%3a%
   200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-decorati
   on-break%3a%20clone%22%3eecho%20%0a%3cspan%20style%3d%22font-size%3a%20
   0.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20border-ra
   dius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical-align
   %3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%
   2fmark%3eand%20%0a%3cmark%20class%3d%22entity%22%20style%3d%22backgroun
   d%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%20margin%3a%200%20
   0.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b%20box-
   decoration-break%3a%20clone%3b%20-webkit-box-decoration-break%3a%20clon
   e%22%3edot%20%0a%3cspan%20style%3d%22font-size%3a%200.8em%3b%20font-wei
   ght%3a%20bold%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b
   %20text-transform%3a%20uppercase%3b%20vertical-align%3a%20middle%3b%20m
   argin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%2fmark%3edevices%2c
   %20have%20clear%20leads%20in%20consumer%20adoption.%3c%2fdiv%3e%0a</bod
   y></html>

[71]accessing entity annotations

   the standard way to access entity annotations is the [72]doc.ents
   property, which produces a sequence of [73]span objects. the entity
   type is accessible either as a hash value or as a string, using the
   attributes ent.label and ent.label_. the span object acts as a sequence
   of tokens, so you can iterate over the entity or index into it. you can
   also get the text form of the whole entity, as though it were a single
   token.

   you can also access token entity annotations using the
   [74]token.ent_iob and [75]token.ent_type attributes. token.ent_iob
   indicates whether an entity starts, continues or ends on the tag. if no
   entity type is set on a token, it will return an empty string.

iob scheme

     * i     token is inside an entity.
     * o     token is outside an entity.
     * b     token is the beginning of an entity.

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"san francisco considers banning sidewalk delivery robots")

# document level
ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]
print(ents)

# token level
ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]
ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]
print(ent_san)  # [u'san', u'b', u'gpe']
print(ent_francisco)  # [u'francisco', u'i', u'gpe']

     text    ent_iob ent_iob_ ent_type_      description
   san       3       b        "gpe"     beginning of an entity
   francisco 1       i        "gpe"     inside an entity
   considers 2       o        ""        outside an entity
   banning   2       o        ""        outside an entity
   sidewalk  2       o        ""        outside an entity
   delivery  2       o        ""        outside an entity
   robots    2       o        ""        outside an entity

[76]setting entity annotations

   to ensure that the sequence of token annotations remains consistent,
   you have to set entity annotations at the document level. however, you
   can   t write directly to the token.ent_iob or token.ent_type attributes,
   so the easiest way to set entities is to assign to the [77]doc.ents
   attribute and create the new entity as a [78]span.
import spacy
from spacy.tokens import span

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"fb is hiring a new vice president of global policy")
ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]
print('before', ents)
# the model didn't recognise "fb" as an entity :(

org = doc.vocab.strings[u"org"]  # get hash value of entity label
fb_ent = span(doc, 0, 1, label=org) # create a span for the new entity
doc.ents = list(doc.ents) + [fb_ent]

ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]
print('after', ents)
# [(u'fb', 0, 2, 'org')]     

   keep in mind that you need to create a span with the start and end
   index of the token, not the start and end index of the entity in the
   document. in this case,    fb    is token (0, 1)     but at the document
   level, the entity will have the start and end indices (0, 2).

[79]setting entity annotations from array

   you can also assign entity annotations using the [80]doc.from_array
   method. to do this, you should include both the ent_type and the
   ent_iob attributes in the array you   re importing from.
import numpy
import spacy
from spacy.attrs import ent_iob, ent_type

nlp = spacy.load("en_core_web_sm")
doc = nlp.make_doc(u"london is a big city in the united kingdom.")
print("before", doc.ents)  # []

header = [ent_iob, ent_type]
attr_array = numpy.zeros((len(doc), len(header)))
attr_array[0, 0] = 3  # b
attr_array[0, 1] = doc.vocab.strings[u"gpe"]
doc.from_array(header, attr_array)
print("after", doc.ents)  # [london]

[81]setting entity annotations in cython

   finally, you can always write to the underlying struct, if you compile
   a [82]cython function. this is easy to do, and allows you to write
   efficient native code.
# cython: infer_types=true
from spacy.tokens.doc cimport doc

cpdef set_entity(doc doc, int start, int end, int ent_type):
    for i in range(start, end):
        doc.c[i].ent_type = ent_type
    doc.c[start].ent_iob = 3
    for i in range(start+1, end):
        doc.c[i].ent_iob = 2

   obviously, if you write directly to the array of tokenc* structs,
   you   ll have responsibility for ensuring that the data is left in a
   consistent state.

[83]built-in entity types

tip: understanding entity types

   you can also use spacy.explain() to get the description for the string
   representation of an entity label. for example,
   spacy.explain("language") will return    any named language   .

annotation scheme

   for details on the entity types available in spacy   s pre-trained
   models, see the [84]ner annotation scheme.

[85]training and updating

   to provide training examples to the entity recognizer, you   ll first
   need to create an instance of the [86]goldparse class. you can specify
   your annotations in a stand-off format or as token tags. if a character
   offset in your entity annotations doesn   t fall on a token boundary, the
   goldparse class will treat that annotation as a missing value. this
   allows for more realistic training, because the entity recognizer is
   allowed to learn from examples that may feature tokenizer errors.
train_data = [
    ("who is chaka khan?", [(7, 17, "person")]),
    ("i like london and berlin.", [(7, 13, "loc"), (18, 24, "loc")]),
]

doc = doc(nlp.vocab, [u"rats", u"make", u"good", u"pets"])
gold = goldparse(doc, entities=[u"u-animal", u"o", u"o", u"o"])

   for more details on training and updating the named entity recognizer,
   see the usage guides on [87]training or check out the runnable
   [88]training script on github.

[89]visualizing named entities

   the [90]displacy ^ent visualizer lets you explore an entity recognition
   model   s behavior interactively. if you   re training a model, it   s very
   useful to run the visualization yourself. to help you do that, spacy
   v2.0+ comes with a visualization module. you can pass a doc or a list
   of doc objects to displacy and run [91]displacy.serve to run the web
   server, or [92]displacy.render to generate the raw markup.

   for more details and examples, see the [93]usage guide on visualizing
   spacy.

named entity example
import spacy
from spacy import displacy

text = """but google is starting from behind. the company made a late push
into hardware, and apple   s siri, available on iphones, and amazon   s alexa
software, which runs on its echo and dot devices, have clear leads in
consumer adoption."""

nlp = spacy.load("custom_ner_model")
doc = nlp(text)
displacy.serve(doc, style="ent")

   iframe: [94]data:text/html,<html><head><meta
   charset="utf-8"></head><body>%3cdiv%20class%3d%22entities%22%20style%3d
   %22line-height%3a%202.5%3b%20font-family%3a%20-apple-system%2c%20blinkm
   acsystemfont%2c%20'segoe%20ui'%2c%20helvetica%2c%20arial%2c%20sans-seri
   f%2c%20'apple%20color%20emoji'%2c%20'segoe%20ui%20emoji'%2c%20'segoe%20
   ui%20symbol'%3b%20font-size%3a%2018px%22%3ebut%20%0a%3cmark%20class%3d%
   22entity%22%20style%3d%22background%3a%20%237aecec%3b%20padding%3a%200.
   45em%200.6em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20b
   order-radius%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-web
   kit-box-decoration-break%3a%20clone%22%3egoogle%20%0a%3cspan%20style%3d
   %22font-size%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%
   201%3b%20border-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3
   b%20vertical-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg%3c
   %2fspan%3e%3c%2fmark%3eis%20starting%20from%20behind.%20the%20company%2
   0made%20a%20late%20push%20into%20hardware%2c%0aand%20%0a%3cmark%20class
   %3d%22entity%22%20style%3d%22background%3a%20%237aecec%3b%20padding%3a%
   200.45em%200.6em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b
   %20border-radius%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20
   -webkit-box-decoration-break%3a%20clone%22%3eapple%20%0a%3cspan%20style
   %3d%22font-size%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%
   3a%201%3b%20border-radius%3a%200.35em%3b%20text-transform%3a%20uppercas
   e%3b%20vertical-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg
   %3c%2fspan%3e%3c%2fmark%3e%e2%80%99s%20%0a%3cmark%20class%3d%22entity%2
   2%20style%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6
   em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radi
   us%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-de
   coration-break%3a%20clone%22%3esiri%20%0a%3cspan%20style%3d%22font-size
   %3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20bor
   der-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical
   -align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%
   3e%3c%2fmark%3e%2c%20available%20on%20%0a%3cmark%20class%3d%22entity%22
   %20style%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6e
   m%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radiu
   s%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-dec
   oration-break%3a%20clone%22%3eiphones%20%0a%3cspan%20style%3d%22font-si
   ze%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20b
   order-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertic
   al-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspa
   n%3e%3c%2fmark%3e%2c%20and%20%0a%3cmark%20class%3d%22entity%22%20style%
   3d%22background%3a%20%237aecec%3b%20padding%3a%200.45em%200.6em%3b%20ma
   rgin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.
   35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-decoration-b
   reak%3a%20clone%22%3eamazon%20%0a%3cspan%20style%3d%22font-size%3a%200.
   8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20border-radi
   us%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical-align%3
   a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg%3c%2fspan%3e%3c%2fmark
   %3e%e2%80%99s%20%0a%3cmark%20class%3d%22entity%22%20style%3d%22backgrou
   nd%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%20margin%3a%200%2
   00.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b%20box
   -decoration-break%3a%20clone%3b%20-webkit-box-decoration-break%3a%20clo
   ne%22%3ealexa%20%0a%3cspan%20style%3d%22font-size%3a%200.8em%3b%20font-
   weight%3a%20bold%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em
   %3b%20text-transform%3a%20uppercase%3b%20vertical-align%3a%20middle%3b%
   20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%2fmark%3esoftwar
   e%2c%20which%20runs%20on%20its%20%0a%3cmark%20class%3d%22entity%22%20st
   yle%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%
   20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radius%3a%
   200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-decorati
   on-break%3a%20clone%22%3eecho%20%0a%3cspan%20style%3d%22font-size%3a%20
   0.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20border-ra
   dius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical-align
   %3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%
   2fmark%3eand%20%0a%3cmark%20class%3d%22entity%22%20style%3d%22backgroun
   d%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%20margin%3a%200%20
   0.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b%20box-
   decoration-break%3a%20clone%3b%20-webkit-box-decoration-break%3a%20clon
   e%22%3edot%20%0a%3cspan%20style%3d%22font-size%3a%200.8em%3b%20font-wei
   ght%3a%20bold%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b
   %20text-transform%3a%20uppercase%3b%20vertical-align%3a%20middle%3b%20m
   argin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%2fmark%3edevices%2c
   %20have%20clear%20leads%20in%20consumer%20adoption.%3c%2fdiv%3e%0a</bod
   y></html>

[95]id121

   id121 is the task of splitting a text into meaningful segments,
   called tokens. the input to the tokenizer is a unicode text, and the
   output is a [96]doc object. to construct a doc object, you need a
   [97]vocab instance, a sequence of word strings, and optionally a
   sequence of spaces booleans, which allow you to maintain alignment of
   the tokens into the original string.

important note

   spacy   s id121 is non-destructive, which means that you   ll always
   be able to reconstruct the original input from the tokenized output.
   whitespace information is preserved in the tokens and no information is
   added or removed during id121. this is kind of a core principle
   of spacy   s doc object: doc.text == input_text should always hold true.

   during processing, spacy first tokenizes the text, i.e. segments it
   into words, punctuation and so on. this is done by applying rules
   specific to each language. for example, punctuation at the end of a
   sentence should be split off     whereas    u.k.    should remain one token.
   each doc consists of individual tokens, and we can iterate over them:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"apple is looking at buying u.k. startup for $1 billion")
for token in doc:
    print(token.text)

     0   1     2    3    4     5      6     7  8 9   10
   apple is looking at buying u.k. startup for $ 1 billion

   first, the raw text is split on whitespace characters, similar to
   text.split(' '). then, the tokenizer processes the text from left to
   right. on each substring, it performs two checks:
    1. does the substring match a tokenizer exception rule? for example,
          don   t    does not contain whitespace, but should be split into two
       tokens,    do    and    n   t   , while    u.k.    should always remain one
       token.
    2. can a prefix, suffix or infix be split off? for example punctuation
       like commas, periods, hyphens or quotes.

   if there   s a match, the rule is applied and the tokenizer continues its
   loop, starting with the newly split substrings. this way, spacy can
   split complex, nested tokens like combinations of abbreviations and
   multiple punctuation marks.
     * tokenizer exception: special-case rule to split a string into
       several tokens or prevent a token from being split when punctuation
       rules are applied.
     * prefix: character(s) at the beginning, e.g. $, (,    ,   .
     * suffix: character(s) at the end, e.g. km, ),    , !.
     * infix: character(s) in between, e.g. -, --, /,    .

   [98]example of the id121 process

   while punctuation rules are usually pretty general, tokenizer
   exceptions strongly depend on the specifics of the individual language.
   this is why each [99]available language has its own subclass like
   english or german, that loads in lists of hard-coded data and exception
   rules.

[100]tokenizer data

   global and language-specific tokenizer data is supplied via the
   language data in [101]spacy/lang. the tokenizer exceptions define
   special cases like    don   t    in english, which needs to be split into two
   tokens: {orth: "do"} and {orth: "n't", lemma: "not"}. the prefixes,
   suffixes and infixes mostly define punctuation rules     for example,
   when to split off periods (at the end of a sentence), and when to leave
   tokens containing periods intact (abbreviations like    u.s.   ).
   [102]language data architecture

     language data

   for more details on the language-specific data, see the usage guide on
   [103]adding languages.

(button) should i change the language data or add custom tokenizer
rules?[104]  

   id121 rules that are specific to one language, but can be
   generalized across that language should ideally live in the language
   data in [105]spacy/lang     we always appreciate pull requests! anything
   that   s specific to a domain or text type     like financial trading
   abbreviations, or bavarian youth slang     should be added as a special
   case rule to your tokenizer instance. if you   re dealing with a lot of
   customizations, it might make sense to create an entirely custom
   subclass.
     __________________________________________________________________

[106]adding special case id121 rules

   most domains have at least some idiosyncrasies that require custom
   id121 rules. this could be very certain expressions, or
   abbreviations only used in this specific field. here   s how to add a
   special case rule to an existing [107]tokenizer instance:
import spacy
from spacy.symbols import orth, lemma, pos, tag

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"gimme that")  # phrase to tokenize
print([w.text for w in doc])  # ['gimme', 'that']

# add special case rule
special_case = [{orth: u"gim", lemma: u"give", pos: u"verb"}, {orth: u"me"}]
nlp.tokenizer.add_special_case(u"gimme", special_case)

# check new id121
print([w.text for w in nlp(u"gimme that")])  # ['gim', 'me', 'that']

# pronoun lemma is returned as -pron-!
print([w.lemma_ for w in nlp(u"gimme that")])  # ['give', '-pron-', 'that']

why -pron-?

   for details on spacy   s custom pronoun lemma -pron-, [108]see here.

   the special case doesn   t have to match an entire whitespace-delimited
   substring. the tokenizer will incrementally split off punctuation, and
   keep looking up the remaining substring:
assert "gimme" not in [w.text for w in nlp(u"gimme!")]
assert "gimme" not in [w.text for w in nlp(u'("...gimme...?")')]

   the special case rules have precedence over the punctuation splitting:
special_case = [{orth: u"...gimme...?", lemma: u"give", tag: u"vb"}]
nlp.tokenizer.add_special_case(u"...gimme...?", special_case)
assert len(nlp(u"...gimme...?")) == 1

   because the special-case rules allow you to set arbitrary token
   attributes, such as the part-of-speech, lemma, etc, they make a good
   mechanism for arbitrary fix-up rules. having this logic live in the
   tokenizer isn   t very satisfying from a design perspective, however, so
   the api may eventually be exposed on the [109]language class itself.

[110]how spacy   s tokenizer works

   spacy introduces a novel id121 algorithm, that gives a better
   balance between performance, ease of definition, and ease of alignment
   into the original string.

   after consuming a prefix or infix, we consult the special cases again.
   we want the special cases to handle things like    don   t    in english, and
   we want the same rule to work for    (don   t)!   . we do this by splitting
   off the open bracket, then the exclamation, then the close bracket, and
   finally matching the special-case. here   s an implementation of the
   algorithm in python, optimized for readability rather than performance:
def tokenizer_pseudo_code(text, special_cases,
                          find_prefix, find_suffix, find_infixes):
    tokens = []
    for substring in text.split(' '):
        suffixes = []
        while substring:
            if substring in special_cases:
                tokens.extend(special_cases[substring])
                substring = ''
            elif find_prefix(substring) is not none:
                split = find_prefix(substring)
                tokens.append(substring[:split])
                substring = substring[split:]
            elif find_suffix(substring) is not none:
                split = find_suffix(substring)
                suffixes.append(substring[-split:])
                substring = substring[:-split]
            elif find_infixes(substring):
                infixes = find_infixes(substring)
                offset = 0
                for match in infixes:
                    tokens.append(substring[offset : match.start()])
                    tokens.append(substring[match.start() : match.end()])
                    offset = match.end()
                substring = substring[offset:]
            else:
                tokens.append(substring)
                substring = ''
        tokens.extend(reversed(suffixes))
    return tokens

   the algorithm can be summarized as follows:
    1. iterate over space-separated substrings
    2. check whether we have an explicitly defined rule for this
       substring. if we do, use it.
    3. otherwise, try to consume a prefix.
    4. if we consumed a prefix, go back to the beginning of the loop, so
       that special-cases always get priority.
    5. if we didn   t consume a prefix, try to consume a suffix.
    6. if we can   t consume a prefix or suffix, look for    infixes        stuff
       like hyphens etc.
    7. once we can   t consume any more of the string, handle it as a single
       token.

[111]customizing spacy   s tokenizer class

   let   s imagine you wanted to create a tokenizer for a new language or
   specific domain. there are five things you would need to define:
    1. a dictionary of special cases. this handles things like
       contractions, units of measurement, emoticons, certain
       abbreviations, etc.
    2. a function prefix_search, to handle preceding punctuation, such as
       open quotes, open brackets, etc.
    3. a function suffix_search, to handle succeeding punctuation, such as
       commas, periods, close quotes, etc.
    4. a function infixes_finditer, to handle non-whitespace separators,
       such as hyphens etc.
    5. an optional boolean function token_match matching strings that
       should never be split, overriding the previous rules. useful for
       things like urls or numbers.

   you shouldn   t usually need to create a tokenizer subclass. standard
   usage is to use re.compile() to build a regular expression object, and
   pass its .search() and .finditer() methods:
import re
import spacy
from spacy.tokenizer import tokenizer

prefix_re = re.compile(r'''^[[("']''')
suffix_re = re.compile(r'''[])"']$''')
infix_re = re.compile(r'''[-~]''')
simple_url_re = re.compile(r'''^https?://''')

def custom_tokenizer(nlp):
    return tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=simple_url_re.match)

nlp = spacy.load("en_core_web_sm")
nlp.tokenizer = custom_tokenizer(nlp)
doc = nlp(u"hello-world.")
print([t.text for t in doc])

   if you need to subclass the tokenizer instead, the relevant methods to
   specialize are find_prefix, find_suffix and find_infix.

important note

   when customizing the prefix, suffix and infix handling, remember that
   you   re passing in functions for spacy to execute, e.g. prefix_re.search
       not just the id157. this means that your functions also
   need to define how the rules should be applied. for example, if you   re
   adding your own prefix rules, you need to make sure they   re only
   applied to characters at the beginning of a token, e.g. by adding ^.
   similarly, suffix rules should only be applied at the end of a token,
   so your expression should end with a $.

[112]adding to existing rule sets

   in many situations, you don   t necessarily need entirely custom rules.
   sometimes you just want to add another character to the prefixes,
   suffixes or infixes. the default prefix, suffix and infix rules are
   available via the nlp object   s defaults and the
   [113]tokenizer.suffix_search attribute is writable, so you can
   overwrite it with a compiled regular expression object using of the
   modified default rules. spacy ships with utility functions to help you
   compile the id157     for example,
   [114]compile_suffix_regex:
suffixes = nlp.defaults.suffixes + (r'''-+$''',)
suffix_regex = spacy.util.compile_suffix_regex(suffixes)
nlp.tokenizer.suffix_search = suffix_regex.search

   for an overview of the default id157, see
   [115]lang/punctuation.py. the tokenizer.suffix_search attribute should
   be a function which takes a unicode string and returns a regex match
   object or none. usually we use the .search attribute of a compiled
   regex object, but you can use some other function that behaves the same
   way.

important note

   if you   re using a statistical model, writing to the nlp.defaults or
   english.defaults directly won   t work, since the id157 are
   read from the model and will be compiled when you load it. you   ll only
   see the effect if you call [116]spacy.blank or
   defaults.create_tokenizer().

[117]hooking an arbitrary tokenizer into the pipeline

   the tokenizer is the first component of the processing pipeline and the
   only one that can   t be replaced by writing to nlp.pipeline. this is
   because it has a different signature from all the other components: it
   takes a text and returns a doc, whereas all other components expect to
   already receive a tokenized doc.
   [118]the processing pipeline

   to overwrite the existing tokenizer, you need to replace nlp.tokenizer
   with a custom function that takes a text, and returns a doc.
nlp = spacy.load("en_core_web_sm")
nlp.tokenizer = my_tokenizer

   argument  type          description
   text     unicode the raw text to tokenize.
   returns  doc     the tokenized document.

important note: using a custom tokenizer

   in spacy v1.x, you had to add a custom tokenizer by passing it to the
   make_doc keyword argument, or by passing a tokenizer    factory    to
   create_make_doc. this was unnecessarily complicated. since spacy v2.0,
   you can write to nlp.tokenizer instead. if your tokenizer needs the
   vocab, you can write a function and use nlp.vocab.
- nlp = spacy.load("en_core_web_sm", make_doc=my_tokenizer)
- nlp = spacy.load("en_core_web_sm", create_make_doc=my_tokenizer_factory)

+ nlp.tokenizer = my_tokenizer
+ nlp.tokenizer = my_tokenizer_factory(nlp.vocab)

[119]example: a custom whitespace tokenizer

   to construct the tokenizer, we usually want attributes of the nlp
   pipeline. specifically, we want the tokenizer to hold a reference to
   the vocabulary object. let   s say we have the following class as our
   tokenizer:
import spacy
from spacy.tokens import doc

class whitespacetokenizer(object):
    def __init__(self, vocab):
        self.vocab = vocab

    def __call__(self, text):
        words = text.split(' ')
        # all tokens 'own' a subsequent space character in this tokenizer
        spaces = [true] * len(words)
        return doc(self.vocab, words=words, spaces=spaces)

nlp = spacy.load("en_core_web_sm")
nlp.tokenizer = whitespacetokenizer(nlp.vocab)
doc = nlp(u"what's happened to me? he thought. it wasn't a dream.")
print([t.text for t in doc])

   as you can see, we need a vocab instance to construct this     but we
   won   t have it until we get back the loaded nlp object. the simplest
   solution is to build the tokenizer in two steps. this also means that
   you can reuse the    tokenizer factory    and initialize it with different
   instances of vocab.

[120]bringing your own annotations

   spacy generally assumes by default that your data is raw text. however,
   sometimes your data is partially annotated, e.g. with pre-existing
   id121, part-of-speech tags, etc. the most common situation is
   that you have pre-defined id121. if you have a list of strings,
   you can create a doc object directly. optionally, you can also specify
   a list of boolean values, indicating whether each word has a subsequent
   space.
import spacy
from spacy.tokens import doc
from spacy.lang.en import english

nlp = english()
doc = doc(nlp.vocab, words=[u"hello", u",", u"world", u"!"],
          spaces=[false, true, false, false])
print([(t.text, t.text_with_ws, t.whitespace_) for t in doc])

   if provided, the spaces list must be the same length as the words list.
   the spaces list affects the doc.text, span.text, token.idx,
   span.start_char and span.end_char attributes. if you don   t provide a
   spaces sequence, spacy will assume that all words are whitespace
   delimited.
import spacy
from spacy.tokens import doc
from spacy.lang.en import english

nlp = english()
bad_spaces = doc(nlp.vocab, words=[u"hello", u",", u"world", u"!"])
good_spaces = doc(nlp.vocab, words=[u"hello", u",", u"world", u"!"],
                  spaces=[false, true, false, false])

print(bad_spaces.text)   # 'hello , world !'
print(good_spaces.text)  # 'hello, world!'

   once you have a [121]doc object, you can write to its attributes to set
   the part-of-speech tags, syntactic dependencies, named entities and
   other attributes. for details, see the respective usage pages.

[122]merging and splitting v2.1

   the [123]doc.retokenize context manager lets you merge and split
   tokens. modifications to the id121 are stored and performed all
   at once when the context manager exits. to merge several tokens into
   one single token, pass a span to [124]retokenizer.merge. an optional
   dictionary of attrs lets you set attributes that will be assigned to
   the merged token     for example, the lemma, part-of-speech tag or entity
   type. by default, the merged token will receive the same attributes as
   the merged span   s root.

       things to try

    1. inspect the token.lemma_ attribute with and without setting the
       attrs. you   ll see that the lemma defaults to    new   , the lemma of
       the span   s root.
    2. overwrite other attributes like the "ent_type". since    new york    is
       also recognized as a named entity, this change will also be
       reflected in the doc.ents.

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("i live in new york")
print("before:", [token.text for token in doc])

with doc.retokenize() as retokenizer:
    retokenizer.merge(doc[3:5], attrs={"lemma": "new york"})
print("after:", [token.text for token in doc])

   if an attribute in the attrs is a context-dependent token attribute, it
   will be applied to the underlying [125]token. for example lemma, pos or
   dep only apply to a word in context, so they   re token attributes. if an
   attribute is a context-independent lexical attribute, it will be
   applied to the underlying [126]lexeme, the entry in the vocabulary. for
   example, lower or is_stop apply to all words of the same spelling,
   regardless of the context.

tip: merging entities and noun phrases

   if you need to merge named entities or noun chunks, check out the
   built-in [127]merge_entities and [128]merge_noun_chunks pipeline
   components. when added to your pipeline using nlp.add_pipe, they   ll
   take care of merging the spans automatically.

   the [129]retokenizer.split method allows splitting one token into two
   or more tokens. this can be useful for cases where id121 rules
   alone aren   t sufficient. for example, you might want to split    its   
   into the tokens    it    and    is        but not the possessive pronoun    its   .
   you can write rule-based logic that can find only the correct    its    to
   split, but by that time, the doc will already be tokenized.

   this process of splitting a token requires more settings, because you
   need to specify the text of the individual tokens, optional per-token
   attributes and how the should be attached to the existing syntax tree.
   this can be done by supplying a list of heads     either the token to
   attach the newly split token to, or a (token, subtoken) tuple if the
   newly split token should be attached to another subtoken. in this case,
      new    should be attached to    york    (the second split subtoken) and
      york    should be attached to    in   .

       things to try

    1. assign different attributes to the subtokens and compare the
       result.
    2. change the heads so that    new    is attached to    in    and    york    is
       attached to    new   .
    3. split the token into three tokens instead of two     for example,
       ["new", "yo", "rk"].

import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("i live in newyork")
print("before:", [token.text for token in doc])
displacy.render(doc)  # displacy.serve if you're not in a jupyter environment

with doc.retokenize() as retokenizer:
    heads = [(doc[3], 1), doc[2]]
    attrs = {"pos": ["propn", "propn"], "dep": ["pobj", "compound"]}
    retokenizer.split(doc[3], ["new", "york"], heads=heads, attrs=attrs)
print("after:", [token.text for token in doc])
displacy.render(doc)  # displacy.serve if you're not in a jupyter environment

   specifying the heads as a list of token or (token, subtoken) tuples
   allows attaching split subtokens to other subtokens, without having to
   keep track of the token indices after splitting.
   token head description
   "new" (doc[3], 1) attach this token to the second subtoken (index 1)
   that doc[3] will be split into, i.e.    york   .
   "york" doc[2] attach this token to doc[1] in the original doc, i.e.
      in   .

   if you don   t care about the heads (for example, if you   re only running
   the tokenizer and not the parser), you can each subtoken to itself:
doc = nlp("i live in newyorkcity")
with doc.retokenize() as retokenizer:
    heads = [(doc[3], 0), (doc[3], 1), (doc[3], 2)]    retokenizer.split(doc[3],
 ["new", "york", "city"], heads=heads)

important note

   when splitting tokens, the subtoken texts always have to match the
   original token text     or, put differently ''.join(subtokens) ==
   token.text always needs to hold true. if this wasn   t the case,
   splitting tokens could easily end up producing confusing and unexpected
   results that would contradict spacy   s non-destructive id121
   policy.
doc = nlp("i live in l.a.")
with doc.retokenize() as retokenizer:
-    retokenizer.split(doc[3], ["los", "angeles"], heads=[(doc[3], 1), doc[2]])
+    retokenizer.split(doc[3], ["l.", "a."], heads=[(doc[3], 1), doc[2]])

[130]overwriting custom extension attributes

   if you   ve registered custom [131]extension attributes, you can
   overwrite them during id121 by providing a dictionary of
   attribute names mapped to new values as the "_" key in the attrs. for
   merging, you need to provide one dictionary of attributes for the
   resulting merged token. for splitting, you need to provide a list of
   dictionaries with custom attributes, one per split subtoken.

important note

   to set extension attributes during reid121, the attributes need
   to be registered using the [132]token.set_extension method and they
   need to be writable. this means that they should either have a default
   value that can be overwritten, or a getter and setter. method
   extensions or extensions with only a getter are computed dynamically,
   so their values can   t be overwritten. for more details, see the
   [133]extension attribute docs.

       things to try

    1. add another custom extension     maybe "music_style"?     and overwrite
       it.
    2. change the extension attribute to use only a getter function. you
       should see that spacy raises an error, because the attribute is not
       writable anymore.
    3. rewrite the code to split a token with retokenizer.split. remember
       that you need to provide a list of extension attribute values as
       the "_" property, one for each split subtoken.

import spacy
from spacy.tokens import token

# register a custom token attribute, token._.is_musician
token.set_extension("is_musician", default=false)

nlp = spacy.load("en_core_web_sm")
doc = nlp("i like david bowie")
print("before:", [(token.text, token._.is_musician) for token in doc])

with doc.retokenize() as retokenizer:
    retokenizer.merge(doc[2:4], attrs={"_": {"is_musician": true}})
print("after:", [(token.text, token._.is_musician) for token in doc])

[134]sentence segmentation

   a [135]doc object   s sentences are available via the doc.sents property.
   unlike other libraries, spacy uses the dependency parse to determine
   sentence boundaries. this is usually more accurate than a rule-based
   approach, but it also means you   ll need a statistical model and
   accurate predictions. if your texts are closer to general-purpose news
   or web text, this should work well out-of-the-box. for social media or
   conversational text that doesn   t follow the same rules, your
   application may benefit from a custom rule-based implementation. you
   can either use the built-in [136]sentencizer or plug an entirely custom
   rule-based function into your [137]processing pipeline.

   spacy   s dependency parser respects already set boundaries, so you can
   preprocess your doc using custom rules before it   s parsed. depending on
   your text, this may also improve accuracy, since the parser is
   constrained to predict parses consistent with the sentence boundaries.

[138]default: using the dependency parse needs model

   to view a doc   s sentences, you can iterate over the doc.sents, a
   generator that yields [139]span objects.
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"this is a sentence. this is another sentence.")
for sent in doc.sents:
    print(sent.text)

[140]rule-based pipeline component

   the [141]sentencizer component is a [142]pipeline component that splits
   sentences on punctuation like ., ! or ?. you can plug it into your
   pipeline if you only need sentence boundaries without the dependency
   parse.
import spacy
from spacy.lang.en import english

nlp = english()  # just the language with no model
sentencizer = nlp.create_pipe("sentencizer")
nlp.add_pipe(sentencizer)
doc = nlp(u"this is a sentence. this is another sentence.")
for sent in doc.sents:
    print(sent.text)

[143]custom rule-based strategy

   if you want to implement your own strategy that differs from the
   default rule-based approach of splitting on sentences, you can also
   create a [144]custom pipeline component that takes a doc object and
   sets the token.is_sent_start attribute on each individual token. if set
   to false, the token is explicitly marked as not the start of a
   sentence. if set to none (default), it   s treated as a missing value and
   can still be overwritten by the parser.

important note

   to prevent inconsistent state, you can only set boundaries before a
   document is parsed (and doc.is_parsed is false). to ensure that your
   component is added in the right place, you can set before='parser' or
   first=true when adding it to the pipeline using [145]nlp.add_pipe.

   here   s an example of a component that implements a pre-processing rule
   for splitting on '...' tokens. the component is added before the
   parser, which is then used to further segment the text. that   s
   possible, because is_sent_start is only set to true for some of the
   tokens     all others still specify none for unset sentence boundaries.
   this approach can be useful if you want to implement additional rules
   specific to your data, while still being able to take advantage of
   dependency-based sentence segmentation.
import spacy

text = u"this is a sentence...hello...and another sentence."

nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
print("before:", [sent.text for sent in doc.sents])

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == "...":
            doc[token.i+1].is_sent_start = true
    return doc

nlp.add_pipe(set_custom_boundaries, before="parser")
doc = nlp(text)
print("after:", [sent.text for sent in doc.sents])

     rule-based matching

   the documentation on rule-based matching [146]has moved to its own
   page.
   [147]suggest edits
   [148]read nextrule-based matching
     * spacy
     * [149]usage
     * [150]models
     * [151]api
     * [152]universe

     * support
     * [153]issue tracker
     * [154]stack overflow
     * [155]reddit user group
     * [156]gitter chat

     * connect
     * [157]twitter
     * [158]github
     * [159]blog

     * stay in the loop!
     * receive updates about new releases, tutorials and more.
     * ____________________
       ____________________ (button) sign up

      2016-2019 [160]explosion ai[161]legal / imprint

references

   visible links
   1. https://spacy.io/
   2. https://spacy.io/usage
   3. https://spacy.io/models
   4. https://spacy.io/api
   5. https://spacy.io/universe
   6. https://spacy.io/usage
   7. https://spacy.io/usage/models
   8. https://spacy.io/usage/facts-figures
   9. https://spacy.io/usage/spacy-101
  10. https://spacy.io/usage/v2-1
  11. https://spacy.io/usage/v2
  12. https://spacy.io/usage/linguistic-features
  13. https://spacy.io/usage/linguistic-features/#pos-tagging
  14. https://spacy.io/usage/linguistic-features/#dependency-parse
  15. https://spacy.io/usage/linguistic-features/#named-entities
  16. https://spacy.io/usage/linguistic-features/#id121
  17. https://spacy.io/usage/linguistic-features/#reid121
  18. https://spacy.io/usage/linguistic-features/#sbd
  19. https://spacy.io/usage/rule-based-matching
  20. https://spacy.io/usage/processing-pipelines
  21. https://spacy.io/usage/vectors-similarity
  22. https://spacy.io/usage/training
  23. https://spacy.io/usage/saving-loading
  24. https://spacy.io/usage/adding-languages
  25. https://spacy.io/usage/visualizers
  26. https://spacy.io/usage/examples
  27. https://spacy.io/usage/linguistic-features/#_title
  28. https://spacy.io/api/doc
  29. https://spacy.io/usage/linguistic-features/#pos-tagging
  30. https://spacy.io/api/token#attributes
  31. https://spacy.io/usage/visualizers
  32. 
  33. https://spacy.io/usage/linguistic-features/#rule-based-morphology
  34. https://spacy.io/usage/adding-languages#tokenizer-exceptions
  35. https://spacy.io/usage/adding-languages#tag-map
  36. https://id138.princeton.edu/
  37. https://spacy.io/api/annotation#pos-tagging
  38. https://spacy.io/usage/linguistic-features/#dependency-parse
  39. https://spacy.io/api/doc
  40. https://spacy.io/usage/linguistic-features/#noun-chunks
  41. https://spacy.io/api/doc#noun_chunks
  42. https://spacy.io/usage/linguistic-features/#navigating
  43. 
  44. https://spacy.io/api/token
  45. https://spacy.io/usage/linguistic-features/#navigating-around
  46. https://spacy.io/api/token#lefts
  47. https://spacy.io/api/token#rights
  48. https://spacy.io/api/token#n_lefts
  49. https://spacy.io/api/token#n_rights
  50. https://spacy.io/api/token#subtree
  51. https://spacy.io/api/token#ancestors
  52. https://spacy.io/api/token#is_ancestor
  53. https://spacy.io/models/en
  54. https://explosion.ai/blog/german-model#word-order
  55. https://spacy.io/api/annotation#pos-tagging
  56. https://spacy.io/usage/linguistic-features/#displacy
  57. https://spacy.io/api/top-level#displacy.serve
  58. https://spacy.io/api/top-level#displacy.render
  59. https://spacy.io/usage/visualizers
  60. https://explosion.ai/demos/displacy
  61. https://spacy.io/usage/linguistic-features/#disabling
  62. https://spacy.io/models
  63. https://spacy.io/usage/processing-pipelin
  64. https://spacy.io/usage/processing-pipelines
  65. https://spacy.io/api/language#from_disk
  66. https://spacy.io/usage/linguistic-features/#named-entities
  67. https://spacy.io/usage/linguistic-features/#named-entities-101
  68. https://spacy.io/api/annotation#named-entities
  69. https://spacy.io/usage/visualizers
  70. 
  71. https://spacy.io/usage/linguistic-features/#accessing
  72. https://spacy.io/api/doc#ents
  73. https://spacy.io/api/span
  74. https://spacy.io/api/token#attributes
  75. https://spacy.io/api/token#attributes
  76. https://spacy.io/usage/linguistic-features/#setting-entities
  77. https://spacy.io/api/doc#ents
  78. https://spacy.io/api/span
  79. https://spacy.io/usage/linguistic-features/#setting-from-array
  80. https://spacy.io/api/doc#from_array
  81. https://spacy.io/usage/linguistic-features/#setting-cython
  82. http://cython.org/
  83. https://spacy.io/usage/linguistic-features/#entity-types
  84. https://spacy.io/api/annotation#named-entities
  85. https://spacy.io/usage/linguistic-features/#updating
  86. https://spacy.io/api/goldparse
  87. https://spacy.io/usage/training
  88. https://github.com/explosion/spacy/tree/master/examples/training/train_ner.py
  89. https://spacy.io/usage/linguistic-features/#displacy
  90. https://explosion.ai/demos/displacy-ent
  91. https://spacy.io/api/top-level#displacy.serve
  92. https://spacy.io/api/top-level#displacy.render
  93. https://spacy.io/usage/visualizers
  94. 
  95. https://spacy.io/usage/linguistic-features/#id121
  96. https://spacy.io/api/doc
  97. https://spacy.io/api/vocab
  98. https://spacy.io/id121-57e618bd79d933c4ccd308b5739062d6.svg
  99. https://spacy.io/usage/models#languages
 100. https://spacy.io/usage/linguistic-features/#101-data
 101. https://github.com/explosion/spacy/tree/master/spacy/lang
 102. https://spacy.io/language_data-ef63e6a58b7ec47c073fb59857a76e5f.svg
 103. https://spacy.io/usage/adding-languages
 104. https://spacy.io/usage/linguistic-features/#lang-data-vs-tokenizer
 105. https://github.com/explosion/spacy/tree/master/spacy/lang
 106. https://spacy.io/usage/linguistic-features/#special-cases
 107. https://spacy.io/api/tokenizer
 108. https://spacy.io/usage/#pron-lemma
 109. https://spacy.io/api/language
 110. https://spacy.io/usage/linguistic-features/#how-tokenizer-works
 111. https://spacy.io/usage/linguistic-features/#native-tokenizers
 112. https://spacy.io/usage/linguistic-features/#native-tokenizer-additions
 113. https://spacy.io/api/tokenizer#attributes
 114. https://spacy.io/api/top-level#util.compile_suffix_regex
 115. https://github.com/explosion/spacy/blob/master/spacy/lang/punctuation.py
 116. https://spacy.io/api/top-level#spacy.blank
 117. https://spacy.io/usage/linguistic-features/#custom-tokenizer
 118. https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg
 119. https://spacy.io/usage/linguistic-features/#custom-tokenizer-example
 120. https://spacy.io/usage/linguistic-features/#own-annotations
 121. https://spacy.io/api/doc
 122. https://spacy.io/usage/linguistic-features/#reid121
 123. https://spacy.io/api/doc#retokenize
 124. https://spacy.io/api/doc#retokenizer.merge
 125. https://spacy.io/api/token
 126. https://spacy.io/api/lexeme
 127. https://spacy.io/api/pipeline-functions#merge_entities
 128. https://spacy.io/api/pipeline-functions#merge_noun_chunks
 129. https://spacy.io/api/doc#retokenizer.split
 130. https://spacy.io/usage/linguistic-features/#reid121-extensions
 131. https://spacy.io/usage/processing-pipelines##custom-components-attributes
 132. https://spacy.io/api/token#set_extension
 133. https://spacy.io/usage/processing-pipelines/#custom-components-attributes
 134. https://spacy.io/usage/linguistic-features/#sbd
 135. https://spacy.io/api/doc
 136. https://spacy.io/api/sentencizer
 137. https://spacy.io/usage/processing-pipelines
 138. https://spacy.io/usage/linguistic-features/#sbd-parser
 139. https://spacy.io/api/span
 140. https://spacy.io/usage/linguistic-features/#sbd-component
 141. https://spacy.io/api/sentencizer
 142. https://spacy.io/usage/processing-pipelines
 143. https://spacy.io/usage/linguistic-features/#sbd-custom
 144. https://spacy.io/usage/processing-pipelines#custom-components
 145. https://spacy.io/api/language#add_pipe
 146. https://spacy.io/usage/rule-based-matching
 147. https://github.com/explosion/spacy/tree/master/website/docs/usage/linguistic-features.md
 148. https://spacy.io/usage/rule-based-matching
 149. https://spacy.io/usage
 150. https://spacy.io/models
 151. https://spacy.io/api
 152. https://spacy.io/universe
 153. https://github.com/explosion/spacy/issues
 154. http://stackoverflow.com/questions/tagged/spacy
 155. https://www.reddit.com/r/spacynlp/
 156. https://gitter.im/explosion/spacy
 157. https://twitter.com/spacy_io
 158. https://github.com/explosion/spacy
 159. https://explosion.ai/blog
 160. https://explosion.ai/
 161. https://explosion.ai/legal

   hidden links:
 163. https://github.com/explosion/spacy
 164. https://spacy.io/usage/rule-based-matching
 165. https://explosion.ai/
