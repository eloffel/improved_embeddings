   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

under the hood of neural network forward propagation         the dreaded matrix
multiplication

   [16]go to the profile of matt ross
   [17]matt ross (button) blockedunblock (button) followfollowing
   sep 9, 2017

   introduction
   [1*wi3ou9y9iylc0yyuqzopxg.png]

   this post was motivated by a frustrating bug in a neural network i was
   building that finally forced me to go under the hood and really
   understand the id202 at the heart of neural networks. i had
   found that i got by fine just making sure the inner dimensions of the
   the two matrices i was multiplying matched and when bugs would occur i
   would just kind of transpose a matrix here and transpose a matrix there
   until things worked out but this hid the truth that i didn   t really
   understand each step of how the id127 worked.

   we   re going to go through each step of using forward propagation to
   compute the cost function of a rather simple neural network. oh and if
   you are wondering the error my id127 ignorance caused,
   it was that i added my bias units (vector of 1   s) as a column when it
   should have been a row. i did this because i didn   t really understand
   the full output of the id127 prior to this step so
   didn   t realize i had to make the change. to begin i   ll explain the high
   level background of what is happening in forward propagation in a
   neural network, then we   ll take a much closer look in a specific
   example, with indexed values and code to keep things clear.

   so, neural networks are incredible at modelling complex relationships.
   we are only going to talk about the feed-forward propagation part of
   the network. now, a neural network   s input unit   s could be anything.
   they could for example be the grayscale intensity (between 0 and 1) of
   a 20 pixel by 20 pixel image that represents a bunch of handwritten
   digits. in that case you would have 400 input units. here we have 2
   input units, plus our +1 bias unit (for an awesome explanation as to
   why have the bias unit at all, go [18]here). forward propagation is
   essentially taking each input from an example (say one of those images
   with a hand written digit) then multiplying the input values by the
   weight of each connection between the units/nodes (see figure 5), then
   adding up all the products of all the connections to the node you are
   computing the activation of and taking that sum (z) and putting that
   through the sigmoid function (see below).
   [1*_rfidsx4eqkomj82dfgdeq.png]
   figure 1: sigmoid function

   that gives you the activation of each of the units of the hidden layer.
   then you do the same to compute the next layer but you use the
   activations of the hidden layer as the    input    values this time. you
   multiply all the a   activations (i.e. the hidden layer) units by the
   second set of weights theta2, sum each product connected to a single
   final output unit and pass that product through the sigmoid function to
   get yourself the final output activations a  . g(z) is the sigmoid
   function and z is the product of the x input (or activation in hidden
   layers) and the weight theta (represented by a single arrow in the
   normal neural network diagram figure 5).
   [1*wpgwxzezx15zza1hxvm66a.png]
   figure 2: hypothesis function, using the sigmoid function.

   once you have all this, you want to calculate the cost of the
   network(figure 4). your cost function essentially calculates the
   cost/difference between the output hypothesis h(x) and the actual y
   values for the examples given. so, in the example i keep using, the y
   being the value of the actual digit represented by the inputs. if there
   is a image of a    4    feed through the network, the y is the value    4   .
   since there are multiple output units, the cost function compares h(x),
   the output, against a column vector where the 4th row is a 1, all the
   rest 0   s. meaning the output unit representing a    4    output it true,
   and all the rest false. for a output of, 1, 2 or n, see below.
   [1*i_3p36l9akqdb0ndjzw2tw.png]
   figure 3: our example data y values represented as logical true/false
   column vectors.
   [1*xvewta6-pn9phzo_viigxq.png]
   figure 4: the multi-class id28 cost function

   the two sigma   s in the cost function j(theta) above are there to sum up
   the cost for every single example you feed through the network (m) and
   for every single output class (k). now, you could do this by doing each
   computation individually, but as it turns out the way that humans have
   defined id127 makes it perfect for doing all of these
   forward propagation computations simultaneously. our friends in
   numerical computing have optimized the id127 functions
   so that a neural network can output a hypothesis extremely efficiently.
   to write our code so that we are doing all the computations
   simultaneously rather than say running everything in a for loop over
   all the input examples, is a process called vectorizing our code. it is
   very important in neural networks since they are already
   computationally expensive enough, we don   t need any for loops in there
   slowing us down even more.

   our network example

   in our network, we will have four classes, 1, 2, 3, 4 and will walk
   through each step of this computation. we   ll assume the network is
   already trained and we have our theta parameters/weights already
   trained through back propagation. it will be a 3 layer network (with
   two input units, 2 hidden layer units, and 4 output units). the network
   and parameters (a.k.a. weights) can be represented as follows.
   [1*wi3ou9y9iylc0yyuqzopxg.png]
   figure 5: our neural network, with indexed weights.

   before we go much farther, if you don   t know how id127
   works, then check out [19]khan academy spend the 7 minutes, then work
   through an example or two and make sure you have the intuition of how
   it works. it is important to know this before going forward.

   let   s begin with all our data. our 3 pieces of example data and the
   corresponding y output values. this data doesn   t represent anything,
   they are just numbers to show the calculations we   ll be doing:
   [1*4gbwggspu7trv26kjaku3q.png]
   figure 6: our data.

   of course as i mentioned, since there are 4 output units, our data must
   be represented as a matrix of logical vectors for each of the three
   example outputs. i   m working in matlab so to turn our y vector into a
   matrix of logical vectors:
yv=[1:4] == y;   %creating logical vectors of y values

   [1*utif9u3du9bs4didkhzr7a.jpeg]
   figure 7: matrix of example output y data turned into logical vectors.

   also, notice that our x data doesn   t have enough features. in our
   figure 5 neural network, we have that dotted line bias unit x(0) that
   is necessary when we compute the product of the weights/parameters and
   the input value. this means we need to add our bias units to the data.
   meaning we add a column to the beginning of the matrix:
x = [ones(m,1),x];

   [1*saymvjpct-5d1agfjehmcg.png]
   figure 8: data with bias added. bias represented by dotted-line
   unit/node in neural net figure 5

   the data x is defined as the first activation values of the first input
   layer a   so if you ever see a   in the code (line 3), it is just
   referring to the initial input data. our weights or parameters of each
   of the connections/arrows in the network are as follows:
   [1*njcncliy1yq6xkkqxqrdqq.png]
   figure 9: first set of weights/parameters for our neural network with
   indices that match those on the arrows of the figure 5 neural
   network diagram.

   below is the full code we will use to compute our logistic cost
   function, we   ve tackled line 2 and 9 but we will slowly break down the
   id127 and important matrix manipulations in the rest of
   this code:
1:  m = size(x, 1);
2:  x = [ones(m,1),x];
3:  a1 = x;
4:  z2 = theta1*a1';
5:  a2 = sigmoid(z2);
6:  a2 = [ones(1,m);a2];
7:  z3 = theta2*a2;
8:  a3 = sigmoid(z3);
9:  yv=[1:4] == y;
10: j = (1/m) * (sum(-yv    .* log(a3)     ((1     yv   ) .* log(1     a3))));
11: j = sum(j);

   let   s do the first step of the forward propagation, line 4 in the code
   above. multiplying the input value for each example by their
   corresponding weights. i always imagine the input value flowing in and
   along the arrow in our network figure 5, getting hit/multiplied by the
   weight then waiting at the activation unit/node for the other arrows to
   do their multiplication. then the whole activation value for a
   particular unit is made up by first the sum of each of these arrow
   (input x weight), then that sum is passed through the sigmoid function
   (see figure 1 above).

   so here it   s easy to make your first id127 mistake.
   since our data with bias units added to x (also called a1 here) is a
   3x3 matrix and our theta1 is a 2x3 matrix. it would be easy to simply
   matrix multiply the two together since the two inner dimensions of
   theta: 2x3 and x: 3x3 are the same and so viola that should work right
   and give us our 2x3 resultant matrix? wrong!
z2 = theta1 * a1;      %wrong! this doesn't give us what we want

   though running this computation will output a matrix of the correct
   dimension that we would expect and need for the next step, all the
   values computed will be wrong and thus all computations will be wrong
   from here on in. plus, since there was no computer error, it will be
   hard to tell why your network cost function is computing the wrong
   cost, if you even notice it. remember, when you do matrix
   multiplication, each element ab of the resulting matrix is the dot
   product sum of the row in the first matrix row a by column of the
   second matrix column b. if we used the above code for computing z  
   above, this first element in the resulting matrix would result from
   multiplying our 1st row of theta   s [0.1 0.3 . 0.5] with an entire
   column of bias units, [1.000; 1.000; 1.000], which is useless to us.
   this means we need to transpose our matrix of example input data so
   that the matrix will multiply each theta with each input correctly:
z2 = theta1*a1';

   the id127 of this is as follows:
   [1*acb_m6pn3daomgac-seuna.png]
   figure 10: the indexed symbolic representation of the matrix
   multiplication. the resultant elements in the columns each representing
   a single example, and the rows being the different activation units in
   the hidden layer. 2 hidden layers results in two values (or rows)
   per example.

   then we element-wise apply the sigmoid function to each of the 6
   elements in the z   matrix above:
a2 = sigmoid(z2);

   this just gives us a 2x3 matrix of the hidden layer activation values
   for both hidden units for each of the three examples:
   [1*bvfe_i37s8etg0lg616dww.png]
   figure 11: the activation values of the hidden units.

   because this was done as id127, we were able to compute
   the activation values of the hidden layers all at once, rather than
   using say a for loop over all the examples which becomes extremely
   computationally expensive when you work with larger data sets. not to
   mention then needing to do back propagation as well.

   now that we have the values of our activation units in the second
   layer, these act as the inputs into the next and final layer, the
   output layer. this layer has a new set of weights/parameters, theta2,
   for each of the arrows in figure 5 between the 2nd and 3rd layer, and
   we will do the same things we did above. multiply the activation value
   (the input) by the weight connected to each activation node, separately
   sum up the products connected to each activation node, then run each
   activation node sum through the sigmoid function to get your final
   outputs. our a   above acts as our input data and our weights/parameters
   are as follows:
   [1*u9vxh_5jcjhuxfpfwpbe6g.png]
   figure 12: theta2 weights/parameters with indices. each row represents
   a different output unit with the weights contributing to each output
   unit across each column.

   we want to do the following computation:
z3 = theta2*a2;

   but before we do that, we must again add our bias units to our data, in
   this case the hidden layer activations a  . if you notice again in
   figure 5, the dotted line circle in the hidden layer, a(0), the bias
   unit which is only added when we do the next computation. so, we add
   this to the activations matrix seen in figure 11 above.

   this is where i introduced my bug that was the motivation for this
   post. to forward propagate the activation values, we will multiply each
   element of a row in theta with each element of a column in a   and the
   sum of these products will give us a single element of the resulting z  
   matrix. often the way data is structured has you adding the bias unit
   as a column, but if you did that (which i stupidly did), this would
   give us the wrong result. so we add the bias units as a row to a  .
a2 = [ones(1,m);a2];

   [1*vwiiclqni9iy1tncl79hpg.jpeg]
   figure 13: adding the bias row to the a   activations.

   before we run our id127 to compute z   notice that where
   before in z   you had to transpose the input data a   to make it    line
   up    correctly for the id127 to result in the
   computations we wanted. here, our matrices are lined up the way we
   want, so there is no transpose of the a   matrix. this is another common
   mistake and it is easy to do it if you don   t understand the computation
   at the heart of it all (i   ve been very guilty of this in the past). now
   we can run our id127 on a 4x3 and 3x3 matrix, resulting
   in a 4x3 matrix of output hypotheses for each of the 3 examples:
z3 = theta2*a2;

   [1*kmlhiuleovwijmo0nkrufw.png]
   figure 14: the indexed symbolic representation of the matrix
   multiplication. the resultant elements in the columns each representing
   a single example, and the rows being the different activation units of
   the output layer, with four output units. in a classification problem
   this would mean four classes/categories. also notice the [m]
   superscript index on all the a   s in each element is the example number.

   then we element-wise apply the sigmoid function to each of the 12
   elements in the z   matrix:
a3 = sigmoid(z3);

   this just gives us a 4x3 matrix of the output layer activations (a.k.a.
   hypothesis) for each of the output units/classes:
   [1*teogq5px6k78jeaetubslg.png]
   figure 15: the activation values for each of the output units of the
   network, for each example. if you were doing a for loop over all your
   examples, this would be a column vector rather than a matrix.

   from here, you are just computing the cost function. the only thing to
   note is that you have to transpose the matrix of y vectors to make sure
   the element-wise operations you are doing in the cost function line up
   properly with each example and with the output units.
   [1*3cxwxyfgbiol2a7rnuynzq.jpeg]
   figure 16: transpose of the logical y vectors matrix.

   then we put it all together to compute the cost function:
   [1*xvewta6-pn9phzo_viigxq.png]
   figure 4: the multi-class id28 cost function
j = (1/m) * (sum(-yv    .* log(a3)     ((1     yv   ) .* log(1     a3))));
j = sum(j);

   this gives us our cost, notice the double sum to account for summing
   over all the classes as well as over all the examples. and that   s all
   folks. the id127 can make this code very clean and
   efficient, no need to have for loops slowing things down, but it is
   essential you know what is happening in id127 so that
   you can adjust the matrices appropriately, whether it be order of
   multiplication, transposing when necessary and adding the bias units to
   the correct area of the matrix. once you break it down, it is much more
   intuitive to grasp and i highly recommend going through an example
   slowly like this yourself if you are still unsure, it always comes down
   to some really simple fundamentals.

   i hope this was helpful at walking through and demystifying the linear
   algebra necessary for forward propagation. also, this is one of my
   first posts and definitely my first technical post, so any feedback,
   questions, comments, etc, is greatly appreciated.

     * [20]machine learning
     * [21]neural networks
     * [22]id158
     * [23]artificial intelligence
     * [24]id202

   (button)
   (button)
   (button) 433 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of matt ross

[26]matt ross

   cataloging my journey in machine learning.

     (button) follow
   [27]towards data science

[28]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 433
     * (button)
     *
     *

   [29]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [30]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a5360b33426
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_vbylimbajozu---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@matt.as.ross?source=post_header_lockup
  17. https://towardsdatascience.com/@matt.as.ross
  18. https://www.quora.com/what-is-bias-in-artificial-neural-network
  19. https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro
  20. https://towardsdatascience.com/tagged/machine-learning?source=post
  21. https://towardsdatascience.com/tagged/neural-networks?source=post
  22. https://towardsdatascience.com/tagged/artificial-neural-network?source=post
  23. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  24. https://towardsdatascience.com/tagged/linear-algebra?source=post
  25. https://towardsdatascience.com/@matt.as.ross?source=footer_card
  26. https://towardsdatascience.com/@matt.as.ross
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/a5360b33426/share/twitter
  33. https://medium.com/p/a5360b33426/share/facebook
  34. https://medium.com/p/a5360b33426/share/twitter
  35. https://medium.com/p/a5360b33426/share/facebook
