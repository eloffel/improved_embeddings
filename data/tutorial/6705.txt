
   (button)
     * [1]home
     * [2]research
          + [3]publications
          + [4]alphago
          + [5]id25
          + [6]dnc
          + [7]open source
          + [8]access to science
     * [9]applied
          + [10]deepmind health
          + [11]deepmind for google
          + [12]deepmind ethics & society
     * [13]news & blog
     * [14]about us
     * [15]careers

   (button) search (button) search
   [deepmind_logo_swirl.png]

[16]research

highlighted research

     * [17]alphago
     * [18]id25
     * [19]dnc

[20]publications

[21]open source

latest research news

   [22]towards robust and verified ai: specification testing, robust
   training, and formal verification

[23]applied

[24]deepmind health

[25]deepmind for google

[26]deepmind ethics & society

latest applied news

   [27]scaling streams with google

[28]careers

     * [29]home
     * [30]news & blog
     * [31]about us
     * [32]press
     * [33]terms and conditions
     * [34]privacy policy     updated

     *
     *
     *

   [id25cover.2e16d0ba.fill-1100x400_z4oyfkn.jpg]

deep id23

   humans excel at solving a wide variety of challenging problems, from
   low-level motor control through to high-level cognitive tasks. our goal
   at deepmind is to create artificial agents that can achieve a similar
   level of performance and generality. like a human, our agents learn for
   themselves to achieve successful strategies that lead to the greatest
   long-term rewards. this paradigm of learning by trial-and-error, solely
   from rewards or punishments, is known as [35]id23
   (rl). also like a human, our agents construct and learn their own
   knowledge directly from raw inputs, such as vision, without any
   hand-engineered features or domain heuristics. this is achieved by
   [36]deep learning of neural networks. at deepmind we have pioneered the
   combination of these approaches - deep id23 - to
   create the first artificial agents to achieve human-level performance
   across many challenging domains.

   our agents must continually make value judgements so as to select good
   actions over bad. this knowledge is represented by a q-network that
   estimates the total reward that an agent can expect to receive after
   taking a particular action. two years ago we introduced the first
   widely successful [37]algorithm for deep id23. the
   key idea was to use deep neural networks to represent the q-network,
   and to train this q-network to predict total reward. previous attempts
   to combine rl with neural networks had largely failed due to unstable
   learning. to address these instabilities, our deep q-networks (id25)
   algorithm stores all of the agent's experiences and then randomly
   samples and replays these experiences to provide diverse and
   decorrelated training data. we applied id25 to learn to play games on
   the atari 2600 console. at each time-step the agent observes the raw
   pixels on the screen, a reward signal corresponding to the game score,
   and selects a joystick direction. in our [38]nature paper we trained
   separate id25 agents for 50 different atari games, without any prior
   knowledge of the game rules.

   [mnih_fig3_r3%2520sm.width-400_hhronzg.png]

   amazingly, id25 achieved human-level performance in almost half of the
   50 games to which it was applied; far beyond any previous method. the
   [39]id25 source code and [40]atari 2600 emulator are freely available to
   anyone who wishes to experiment for themselves.

   id25 breakout
   [breakout.width-320_kbybnuk.png]

   we have subsequently improved the id25 algorithm in many ways: further
   stabilising the [41]learning [42]dynamics; prioritising
   the [43]replayed experiences; [44]normalising, [45]aggregating and
   [46]re-scaling the outputs. combining several of these improvements
   together led to a 300% improvement in mean score across atari games;
   human-level performance has now been achieved in almost all of the
   atari games. we can even train a [47]single neural network to learn
   about [48]multiple atari games. we have also built a massively
   distributed deep rl system, known as [49]gorila, that utilises the
   google cloud platform to speed up training time by an order of
   magnitude; this system has been applied to recommender systems within
   google.

   however, deep q-networks are only one way to solve the deep rl problem.
   we recently introduced an even more practical and effective method
   based on asynchronous rl. this approach exploits the multithreading
   capabilities of standard cpus. the idea is to execute many instances of
   our agent in parallel, but using a shared model. this provides a viable
   alternative to experience replay, since parallelisation also
   diversifies and decorrelates the data. our asynchronous actor-critic
   algorithm, [50]a3c, combines a deep q-network with a deep policy
   network for selecting actions. it achieves state-of-the-art results,
   using a fraction of the training time of id25 and a fraction of the
   resource consumption of gorila. by building novel approaches to
   [51]intrinsic motivation and [52]temporally abstract planning, we have
   also achieved breakthrough results in the most notoriously challenging
   atari games, such as montezuma   s revenge.

   while atari games demonstrate a wide degree of diversity, they are
   limited to 2d sprite-based video games. we have recently introduced
   labyrinth: a challenging suite of 3d navigation and puzzle-solving
   environments. again, the agent only observes pixel-based inputs from
   its immediate field-of-view, and must figure out the map to discover
   and exploit rewards.

   id25 space invaders
   [spaceinvaders.width-320_zgrdmrt.png]

   [labyrinth%2520medley_sm.width-400_mnts3wy.jpg]

   amazingly, the a3c algorithm achieves human-level performance,
   out-of-the-box, on many labyrinth tasks. an [53]alternative approach
   based on episodic memory has also proven successful. labyrinth will
   also be released open source in the coming months.

   asynchronous methods for deep id23: labyrinth
   [labyrinth.width-320_jyldlxh.png]

   we have also developed a number of deep rl methods for continuous
   control problems such as robotic manipulation and locomotion. our
   deterministic policy gradients algorithm ([54]dpg) provides a
   continuous analogue to id25, exploiting the differentiability of the
   q-network to solve a [55]wide [56]variety of continuous control tasks.
   [57]asynchronous rl also performs well in these domains and, when
   augmented with a hierarchical control strategy, can solve challenging
   problems such as ant soccer and a 54-dimensional humanoid slalom,
   without any prior knowledge of the dynamics.

   ant soccer
   [antsoccer.width-320_hk5f1pq.png]

   the game of go is the most challenging of classic games. despite
   decades of effort, prior methods had only achieved amateur level
   performance. we developed a deep rl algorithm that learns both a value
   network (which predicts the winner) and a policy network (which selects
   actions) through games of self-play. our program alphago combined these
   deep neural networks with a state-of-the-art tree search. in october
   2015, alphago became [58]the first program to defeat a professional
   human player. in march 2016, alphago [59]defeated lee sedol (the
   strongest player of the last decade with an incredible 18 world titles)
   by 4 games to 1, in a match that was watched by an estimated 200
   million viewers.

   [a26u3069.width-400_wujhsp7.jpg]

   separately, we have also developed [60]game theoretic approaches to
   [61]deep rl, culminating in a [62]super-human poker player for heads-up
   limit texas hold   em.

   from atari to labyrinth, from locomotion through manipulation, to poker
   and even the game of go, our deep id23 agents have
   demonstrated remarkable progress on a wide variety of challenging
   tasks. our goal is to continue to improve the capabilities of our
   agents, and to use them to make a positive impact on society, in
   important applications such as [63]healthcare.

   share article
     *
     *
     *
     *
     * [ ]
          + [64]linkedin
          + whatsapp
          + sms
          + [65]reddit

   author
   friday, 17 june 2016
     * [image.2e16d0ba.fill-80x80_qlergrd.png]
       david silver
       research scientist, deepmind

   ____________________
   ____________________
   [66]show all results
   (button) close

                               [67]deepmind logo

   follow
     *
     *
     *

     * [68]research [69]research
     * [70]applied [71]applied
     * [72]news & blog [73]news & blog
     * [74]about us [75]about us
     * [76]careers [77]careers

     * [78]press
     * [79]terms and conditions
     * [80]privacy policy     updated
     * [81]modern slavery statement
     * [82]alphabet inc

      2019 deepmind technologies limited

   deepmind.com uses cookies to help give you the best possible user
   experience and to allow us to see how the site is used. by using this
   site, you agree that we can set and use these cookies. for more
   information on cookies and how to change your settings, see our
   [83]privacy policy.

references

   visible links
   1. https://deepmind.com/
   2. https://deepmind.com/research/
   3. https://deepmind.com/research/publications/
   4. https://deepmind.com/research/alphago/
   5. https://deepmind.com/research/id25/
   6. https://deepmind.com/research/dnc/
   7. https://deepmind.com/research/open-source/
   8. https://deepmind.com/research/access-science/
   9. https://deepmind.com/applied/
  10. https://deepmind.com/applied/deepmind-health/
  11. https://deepmind.com/applied/deepmind-google/
  12. https://deepmind.com/applied/deepmind-ethics-society/
  13. https://deepmind.com/blog/
  14. https://deepmind.com/about/
  15. https://deepmind.com/careers/
  16. https://deepmind.com/research/
  17. https://deepmind.com/research/alphago/
  18. https://deepmind.com/research/id25/
  19. https://deepmind.com/research/dnc/
  20. https://deepmind.com/research/publications/
  21. https://deepmind.com/research/open-source/
  22. https://deepmind.com/blog/robust-and-verified-ai/
  23. https://deepmind.com/applied/
  24. https://deepmind.com/applied/deepmind-health/
  25. https://deepmind.com/applied/deepmind-google/
  26. https://deepmind.com/applied/deepmind-ethics-society/
  27. https://deepmind.com/blog/scaling-streams-google/
  28. https://deepmind.com/careers/
  29. https://deepmind.com/
  30. https://deepmind.com/blog/
  31. https://deepmind.com/about/
  32. https://deepmind.com/press/
  33. https://deepmind.com/terms-and-conditions/
  34. https://deepmind.com/privacy-policy/
  35. https://en.wikipedia.org/wiki/reinforcement_learning
  36. https://en.wikipedia.org/wiki/deep_learning
  37. http://arxiv.org/pdf/1312.5602.pdf
  38. https://storage.googleapis.com/deepmind-data/assets/papers/deepmindnature14236paper.pdf
  39. https://sites.google.com/a/deepmind.com/id25/
  40. http://stella.sourceforge.net/
  41. http://arxiv.org/pdf/1509.06461
  42. http://arxiv.org/pdf/1512.04860
  43. http://arxiv.org/pdf/1511.05952.pdf
  44. http://arxiv.org/pdf/1511.06581
  45. http://arxiv.org/pdf/1602.04621
  46. http://arxiv.org/pdf/1602.07714
  47. http://jmlr.org/proceedings/papers/v37/schaul15.pdf
  48. http://arxiv.org/pdf/1511.06295
  49. https://8109f4a4-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearning2015/1.pdf?attachauth=anoy7codlgnh08pypvtc6s86hnykrd-u_rcqywallgilfkovyp1mdji0mr8wf2-5um261x1v8krff-x5zm9fhkcmhgc5b3a50_8iwutggpw6n38udzypcvwn0ispgasgbuxao1e7oi2vjfurq4m7s-oxqlntzvo3r6p1myyduc6pwnkovthtdzoayrosfrvsvrork2qun7dn00z4juz8jzpx1-nddjmqxw==&attredirects=2
  50. http://arxiv.org/pdf/1602.01783
  51. https://arxiv.org/abs/1606.01868
  52. http://arxiv.org/pdf/1606.04695
  53. https://arxiv.org/pdf/1606.04460
  54. http://jmlr.org/proceedings/papers/v32/silver14.pdf
  55. http://arxiv.org/pdf/1509.02971
  56. http://arxiv.org/pdf/1510.09142
  57. http://arxiv.org/pdf/1602.01783
  58. http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html
  59. https://deepmind.com/alpha-go
  60. http://jmlr.org/proceedings/papers/v37/heinrich15.pdf
  61. https://arxiv.org/pdf/1603.01121
  62. http://www.aaai.org/ocs/index.php/ijcai/ijcai15/paper/view/11230/10741
  63. https://deepmind.com/health
  64. https://www.linkedin.com/sharearticle?mini=true&url=https://deepmind.com/blog/deep-reinforcement-learning/&title=deep id23&summary=&source=google deepmind
  65. http://www.reddit.com/submit?url=https://deepmind.com/blog/deep-reinforcement-learning/&title=deep id23
  66. https://deepmind.com/blog/deep-reinforcement-learning/
  67. https://deepmind.com/
  68. https://deepmind.com/research/
  69. https://deepmind.com/research/
  70. https://deepmind.com/applied/
  71. https://deepmind.com/applied/
  72. https://deepmind.com/blog/
  73. https://deepmind.com/blog/
  74. https://deepmind.com/about/
  75. https://deepmind.com/about/
  76. https://deepmind.com/careers/
  77. https://deepmind.com/careers/
  78. https://deepmind.com/press/
  79. https://deepmind.com/terms-and-conditions/
  80. https://deepmind.com/privacy-policy/
  81. https://storage.googleapis.com/deepmind-media/modern_slavery/final_201_google_modern_slavery_statement.pdf
  82. https://abc.xyz/
  83. https://deepmind.com/privacy-policy/

   hidden links:
  85. https://twitter.com/deepmindai
  86. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  87. https://plus.google.com/+deepmindai
  88. http://twitter.com/intent/tweet/?text=&url=https%3a//deepmind.com/blog/deep-reinforcement-learning/
  89. http://www.facebook.com/share.php?u=https%3a//deepmind.com/blog/deep-reinforcement-learning/&t=
  90. https://plus.google.com/share?url=https%3a//deepmind.com/blog/deep-reinforcement-learning/
  91. mailto:?subject=deep%20reinforcement%20learning&body=%0d%0a%0d%0ahttps%3a//deepmind.com/blog/deep-reinforcement-learning/
  92. https://twitter.com/deepmindai
  93. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  94. https://plus.google.com/+deepmindai
