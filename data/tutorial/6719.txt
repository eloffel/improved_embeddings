   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]techburst
     * [9]learn cryptocurrency
     * [10]build your website
     __________________________________________________________________

the basics of video object segmentation

   [11]go to the profile of eddie smolyansky
   [12]eddie smolyansky (button) blockedunblock (button) followfollowing
   sep 12, 2017
   [1*ucvbqwz2mngw-ocviljlca.png]
   annotated ground truth frames from the davis-2016 video object
   segmentation dataset

   [update: the article has also been translated to [13]chinese.]

   this is the first in a [14]two part series about the state of the art
   in algorithms for video object segmentation. the first part will be an
   introduction to the problem and it   s    classic    solutions. we will
   briefly cover:
    1. the problem, the datasets, the challenge
    2. a new dataset that we   re announcing today!
    3. the two main approaches from 2016: masktrack and osvos. these are
       the algorithms upon which all other works are based.

   in the [15]second part, which is more advanced, i will present a
   comparison table of all the published approaches to the davis-2017
   video object segmentation challenge, summarize and highlight selected
   works and point to some emerging trends and directions.

   the posts assume familiarity with some concepts in id161 and
   deep learning, but are quite accessible. i hope to make a good
   introduction to this id161 challenge and bring newcomers
   quickly up to speed.

introduction

   there are three classic tasks related to objects in id161:
   classification, detection and segmentation. while classification aims
   to answer the    what?   , the goal of the latter two is to also answer the
      where?   , and segmentation specifically aims to do it at the pixel
   level.
   [1*twcmmxxuumsdrvgay2ocqa.png]
   classical id161 tasks (image from stanford   s cs231n
   course slides)

   in 2016 we have seen semantic segmentation mature and perhaps even
   begin to saturate existing datasets. meanwhile, 2017 has been somewhat
   of a breakout year for video related tasks: action classification,
   action (temporal) segmentation, semantic segmentation, etc. in these
   posts we will focus on video object segmentation.

the problem, the datasets, the challenge

   assuming reader familiarity with semantic segmentation, the task of
   video object segmentation basically introduces two differences:
     * we are segmenting general, non-semantic objects.
     * a temporal component has been added: our task is to find the pixels
       corresponding to the object(s) of interest in each consecutive
       frame of a video.

   this can also be thought of as a pixel-level object tracking problem.
   [1*glrypzvhjzga8sdxz4yuxa.png]
   segmentation: sub-divisions of the space. an example dataset is given
   for each leaf in the graph.

   in the video formulation, we can split the problem into two
   subcategories:
     * unsupervised (aka video saliency detection): the task is to find
       and segment the main object in the video. this means the algorithm
       should decide by itself what the    main    object is.
     * semi-supervised, which we   ll cover in these posts: given the ground
       truth segmentation mask of (only) the first frame as input, segment
       the annotated object in every consecutive frame.

   the semi-supervised case can be extended to multiple object
   segmentation as can be seen in the [16]davis-2017 challenge.
   [1*qoorkob2wpnkvgs-c_mi-a.png]
   the main difference between annotations of davis-2016 (left) and
   davis-2017 (right): multi-instance segmentation

       [17]press here to view a video of the davis-2017 dataset and
   challenge    

   as you can see, davis is a dataset with pixel-perfect ground truth
   annotations. it aims to recreate real-life video scenarios such as
   camera shake, background clutter, occlusions and other complexities.
   [1*y5iuhawd4elznm9jrt9yvg.png]
   davis-2016 complexity attributes

   there are two primary metrics to measure segmentation success:
   [1*pz2lqms4d5l--u1_8lx0qq.png]
   region similarity is the intersection-over-union between mask m and
   ground truth g
     * region similarity: defined as the intersection-over-union between
       the estimated segmentation and the ground truth mask.

   [1*od0eozzvkjfa_v9nmpevta.png]
   contour accuracy is the f-measure for the contour based precision
   and recall
     * contour accuracy: interprets the masks as a set of closed contours
       and computes the contour-based f-measure which is a function of
       precision and recall.

   intuitively         region similarity measures the amount of mislabeled
   pixels, while contour accuracy measures the precision of the
   segmentation boundaries.

announcing a new dataset! gygo: e-commerce video object segmentation by
visualead

   gygo, which we will release in parts over the following weeks, is a
   dataset that is focused on a specific, simple use case for video object
   segmentation: e-commerce. it will consist of about 150 short videos.

   [18]https://github.com/ilchemla/gygo-dataset

   on the one hand, the sequences are quite simple in that they are
   virtually devoid of occlusions, fast motions or many of the other
   complexity inducing attributes mentioned above. on the other hand, the
   objects in these videos have more varied categories than the davis-2016
   dataset, in which many of the sequences contain known semantic classes
   (humans, cars, etc). the gygo dataset specializes in smartphone
   captured videos and its frames are relatively sparse (the annotated
   video speed is ~5 fps).

   iframe: [19]/media/c75efa2c96955d46a51937b37c4b58da?postid=758e77321914

   gygo e-commerce video object segmentation dataset: teaser

   we release it publicly with two goals in mind:
    1. there is a severe lack of data in the space of video object
       segmentation at the moment. with only hundreds of annotated videos,
       we believe every contribution has the potential to increase
       performance. in our analysis we have shown that a joint training on
       the gygo and davis datasets yields improved id136 results.
    2. to promote a more open, sharing culture and encourage other
       researchers to join our efforts :) the davis dataset and the
       research ecosystem that grew it have been massively useful for us.
       we hope the community will find our datasets useful as well.

the two main approaches to davis-2016

   with the release of the davis-2016 dataset for single object video
   segmentation, two main leading approaches emerged: masktrack and osvos.
   looking at the contestants of the davis-2017 challenge, it seems that
   every single team decided to build their solution on top of one of
   these two approaches         giving them the status of instant classics. lets
   see how they work:

one shot video object segmentation

   the concept behind osvos is simple yet powerful:
   [1*9eir2boeejds22svgctopa.png]
   osvos training pipeline
    1. take a net (say vgg-16) pre-trained for classification for example,
       on id163.
    2. convert it to a fully convolutional network,    la [20]fcn, thus
       preserving spatial information:
       - remove the fc layers in the end.
       - insert a new loss: pixel-wise sigmoid balanced cross id178
       (previously used by [21]hed). now each pixel is separately
       classified into foreground or background.
    3. train the new fully convolutional network on the davis-2016
       training set.
    4. one-shot training:
       at id136 time, given a new input video for segmentation and a
       ground-truth annotation for the first frame (remember, this is a
       semi-supervised problem), create a new model, initialized with the
       weights trained in [3] and fine-tuned on the first frame.

   the result of this process is a unique, one-time-use model for every
   new video that is overfitted for that specific video according to the
   first frame annotation. because for most videos the appearance of the
   object and the background does not change drastically, this model
   produces good results. naturally, this model would work less well for
   another random video sequence.

   note: the osvos method segments the frames independently         there is no
   use of temporal information in the video.

masktrack (learning video object segmentation from static images)

   while osvos works on each frame of the video independently, masktrack
   also takes into consideration the temporal information contained within
   it:
   [1*8njo91nrpkdjbmg7e9hscq.png]
   masktrack mask propagation module
    1. for each frame, feed the predicted mask of the previous frame as
       additional input to the network: the input is now 4 channels
       (rgb+previous mask). initialize this process with the ground truth
       annotation given for the first frame.
    2. the net, originally based on deeplab vgg-16 (but modular), is
       trained from scratch on a combination of semantic segmentation and
       image saliency datasets. the input for the previous mask channel is
       artificially synthesized by small transformations of the ground
       truth annotation of each still image.
    3. add an identical second stream network, based on optical flow
       input. the model weights are the same as in the rgb stream. fuse
       the outputs of both streams by averaging the results.
    4. online training: use the first frame ground truth annotation to
       synthesize additional, video specific training data.

   note: both of these methods rely on still-images training (as opposed
   to video, where the datasets are scarce and small).
     __________________________________________________________________

   to summarize, in this introductory post we   ve established the problem
   of video object segmentation and the leading methods to solve it in
   2016. armed with this knowledge, we are ready to tackle the twice
   improved, state of the art algorithms proposed in 2017.

   [22]go on to part 2: a meta-analysis of davis-2017 video object
   segmentation challenge   

   p.s. i   d like to say thank you to the wonderful [23]team behind the
   davis datasets and challenge for all their hard work. without you none
   of this would exist.

references

   the main papers described and analyzed in this post are cited below.
    1. benchmark dataset and evaluation methodology for video object
       segmentation
       f. perazzi, j. pont-tuset, b. mcwilliams, l. van gool, m. gross,
       and a. sorkine-hornung, id161 and pattern recognition
       (cvpr) 2016
    2. the 2017 davis challenge on video object segmentation
       j. pont-tuset, f. perazzi, s. caelles, p. arbel  ez, a.
       sorkine-hornung, and l. van gool, arxiv:1704.00675, 2017
    3. learning video object segmentation from static images
       f. perazzi*, a. khoreva*, r. benenson, b. schiele, a.
       sorkine-hornung
       cvpr 2017, honolulu, usa.
    4. one-shot video object segmentation,
       s. caelles*, k.k. maninis*, j. pont-tuset, l. leal-taix  , d.
       cremers, and l. van gool, id161 and pattern recognition
       (cvpr), 2017

     * [24]machine learning
     * [25]deep learning
     * [26]id161
     * [27]introduction
     * [28]dataset

   (button)
   (button)
   (button) 520 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of eddie smolyansky

[30]eddie smolyansky

   id161 and machine learning research lead [31]@alibaba israel.
   interested in rationality, startups, photography and gaming.

     (button) follow
   [32]techburst

[33]techburst

   bursts of tech to power through your day

     * (button)
       (button) 520
     * (button)
     *
     *

   [34]techburst
   never miss a story from techburst, when you sign up for medium.
   [35]learn more
   never miss a story from techburst
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://techburst.io/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/758e77321914
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://techburst.io/video-object-segmentation-the-basics-758e77321914&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://techburst.io/video-object-segmentation-the-basics-758e77321914&source=--------------------------nav_reg&operation=register
   8. https://techburst.io/?source=logo-lo_g2gcug8u31bc---6e84842e6882
   9. https://techburst.io/learn-cryptocurrency-bitcoin-ethereum-altcoins-and-more-2e47243d8fb3
  10. http://bit.ly/best-website-builder-cb
  11. https://techburst.io/@eddiesmo?source=post_header_lockup
  12. https://techburst.io/@eddiesmo
  13. https://www.jiqizhixin.com/articles/2017-10-06
  14. https://medium.com/@eddiesmo/a-meta-analysis-of-davis-2017-video-object-segmentation-challenge-c438790b3b56
  15. https://medium.com/@eddiesmo/a-meta-analysis-of-davis-2017-video-object-segmentation-challenge-c438790b3b56
  16. http://davischallenge.org/challenge2017/index.html
  17. http://davischallenge.org/images/davis-2017-trainval.mp4
  18. https://github.com/ilchemla/gygo-dataset
  19. https://techburst.io/media/c75efa2c96955d46a51937b37c4b58da?postid=758e77321914
  20. https://arxiv.org/abs/1605.06211
  21. https://arxiv.org/abs/1504.06375
  22. https://medium.com/@eddiesmo/a-meta-analysis-of-davis-2017-video-object-segmentation-challenge-c438790b3b56
  23. http://davischallenge.org/challenge2017/index.html
  24. https://techburst.io/tagged/machine-learning?source=post
  25. https://techburst.io/tagged/deep-learning?source=post
  26. https://techburst.io/tagged/computer-vision?source=post
  27. https://techburst.io/tagged/introduction?source=post
  28. https://techburst.io/tagged/dataset?source=post
  29. https://techburst.io/@eddiesmo?source=footer_card
  30. https://techburst.io/@eddiesmo
  31. http://twitter.com/alibaba
  32. https://techburst.io/?source=footer_card
  33. https://techburst.io/?source=footer_card
  34. https://techburst.io/
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/758e77321914/share/twitter
  38. https://medium.com/p/758e77321914/share/facebook
  39. https://medium.com/p/758e77321914/share/twitter
  40. https://medium.com/p/758e77321914/share/facebook
