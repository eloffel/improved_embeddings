   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]artists + machine intelligence
     * [9]all articles
     * [10]about
     * [11]submit an article
     __________________________________________________________________

neural nets for generating music

   [12]go to the profile of kyle mcdonald
   [13]kyle mcdonald (button) blockedunblock (button) followfollowing
   aug 25, 2017

   algorithmic music composition has developed a lot in the last few
   years, but the idea has a long history. in some sense, the first
   automatic music came from nature: [14]chinese windchimes, ancient greek
   wind-powered [15]aeolian harps, or the japanese water instrument
   [16]suikinkutsu. but in the 1700s automatic music became    algorithmic   :
   [17]musikalisches w  rfelspiel, a game that generates short piano
   compositions from fragments, with choices made by dice.

   iframe: [18]/media/cd31f3b7dbb17ff72ac8d4f399031321?postid=f46dffac21c0

   an example session of musikalisches w  rfelspiel.

   [19]markov chains, formalized in the early 1900s to model probabilistic
   systems, can also be used to generate new musical compositions. they
   take the motivations behind the dice game a step further, in two ways.
   first, markov chains can be built from existing material rather than
   needing fragments explicitly composed as interchangeable components.
   second, instead of assuming fragments have equal probabilities, markov
   chains encode the variation in probabilities with respect to context.
   [1*d92wxnh_pcc3clzavnwxbg.png]
      [20]remixing noon    by rev dan catt: a visualization of one possible
   path through a markov chain trained on prose.

   [21]iannis xenakis used markov chains in his 1958 compositions,
      [22]analogique   . he describes his process in    [23]formalized music:
   thought and mathematics in composition   , down to the details of
   transition matrices that define the probabilities of certain notes
   being produced.
   [1*vu1t_ytsf0cm-kwzt8_qig.png]
   an excerpt from chapter 3,    markovian stochastic music: applications   .

   iframe: [24]/media/b433c731a46fe71fd5ea44dd05bc0845?postid=f46dffac21c0

      analogique a and b    (1958   1959) by iannis xenakis.

   in 1981 [25]david cope began working with algorithmic composition to
   [26]solve his writers block. he combined markov chains and other
   techniques (musical grammars and combinatorics) into a semi-automatic
   system he calls experiments in musical intelligence, or emmy. david
   cites iannis xenakis and lejaren hiller ([27]illiac suite 1955,
   [28]experimental music 1959) as early inspirations, and he describes
   emmy in [29]papers, [30]patents, and even source code on [31]github.
   emmy is most famous for learning from and imitating other composers.

   iframe: [32]/media/93e96fee593d506b47f2164e1d7ee535?postid=f46dffac21c0

   a composition by david cope using emmy, in the style of a chopin
   mazurka. more performances [33]here.

   while markov chains trained on a set of compositions [34]can only
   produce subsequences that also exist in the original data,
   [35]recurrent neural networks (id56s) attempt to extrapolate beyond
   those exact subsequences. in 1989 the [36]first attempts to generate
   music with id56s, developed first by [37]peter m. todd, then michael c.
   mozer and others, were limited by their short-term coherence.

   iframe: [38]/media/5ae785beb68dc0bff7adde62ca1368a7?postid=f46dffac21c0

   three short compositions in the style of bach generated by the
   [39]concert system by michael c. mozer.

   in 2002 doug eck [40]updated this approach by switching from standard
   id56 cells to    long short term memory    (lstms) cells. doug used his
   architecture to [41]improvise blues based on a short recording. he
   writes,    remarkably [   ] lstm is able to play the blues with good timing
   and proper structure as long as one is willing to listen.   

   iframe: [42]/media/1c8476d7a29ced63742f743e9140fd94?postid=f46dffac21c0

   more variations on this approach from doug are available [43]here.

   doug now leads the [44]magenta team at [45]google brain, where they
   have been [46]developing and sharing code related to machine learning &
   creativity since early 2016. magenta has applied doug   s lstm-based
   approaches to [47]drum pattern generation, [48]melody generation, and
   [49]polyphonic music generation. they   ve built systems that
   [50]improvise duets with human performers, and tools that generate
   [51]expressive dynamics and timing along with the polyphonic
   compositions. initially, magenta released examples using tensorflow in
   python with the hope that artists and musicians would explore these
   demos. in 2018 with the release [52]tensorflow.js they have started to
   promote more [53]interactive demos in javascript and even [54]plugins
   for ableton live. two favorites: [55]multitrack vae for interpolating
   between short melodic loops, and [56]beat blender for interpolating
   between short drum loops.

   iframe: [57]/media/d38512abe0bbf2466c22167f54f43897?postid=f46dffac21c0

   original post with more example audio available [58]here. for endless
   music in your browser go [59]here.

   a big leap in compositional complexity came out of magenta in september
   2018 with [60]music transformer by huang et al. unlike performance id56,
   [61]the samples from music transformer do not succumb to chaos after
   the first few measures. they trained on bach chorales (without
   dynamics) as well as a piano competition data (with dynamics).

   iframe: [62]/media/44d018b9e4a10ea5cef8c6e4db142129?postid=f46dffac21c0

   piano-e-competition unconditioned samples from [63]music transformer.
   more example audio available [64]here.

   one of the recurring difficulties encountered when training these
   systems is deciding on a representation of music. designing an encoding
   for a id56 might start with a metaphor of text: the id56 is processing a
   sequence of states (letters) unfolding over time or space (the page).
   but unlike text, a single moment in music can contain more than one
   symbol: it can be a chord, or it can have a combination of qualities
   that is best described by its components. there can also be long
   durations of silence, or states can have wildly varying lengths. these
   differences may be resolved by carefully crafting the representation,
   or by heavily augmenting the dataset and designing the architecture
   with the capacity to learn all the invariance.

   another significant challenge with data-driven algorithmic composition
   is: what data to use? whose music counts? when any automated creative
   system needs to be trained on a large number of cultural artifacts, it
   can only perpetuate the dominance of what is already well-documented.
   in music, this means a lot of bach, beethoven, and other old white
   european men. (two exceptions: some english and irish [65]folk
   [66]music, and some [67]video game music.) the data is also selected by
   machine learning researchers, who are also a relatively homogenous
   group (though decreasingly so).
     __________________________________________________________________

   while lstms and transformers manage to maintain long-term consistency
   better than a standard id56 or markov chain, there is still a gap
   between generating shorter phrases and generating an entire
   composition; something that has not yet been bridged without lots of
   tricks and hand-tuning. startups like [68]jukedeck, [69]aiva,
   [70]amper, and others are trying to fill this space of on-demand,
   hand-tuned formulaic generative music. some going so far as to produce
   [71]entire pop albums as marketing. big companies are getting in on the
   action, too. fran  ois pachet, formerly at sony computer science
   laboratories and [72]now at spotify, has been working with algorithmic
   music for some time, from his [73]continuator to the more recent
   [74]flow machines.

   iframe: [75]/media/e0bd40e1d54ca80e58ed5b5ad7702e8c?postid=f46dffac21c0

      the continuator    (2000) by fran  ois pachet, is designed to    extend the
   technical ability of musicians with stylistically consistent,
   automatically learnt musical material   .

   iframe: [76]/media/a9e13845c318edfd146ce9dcb2451c42?postid=f46dffac21c0

      daddy   s car    (2016) by flow machines, a research project at sony csl
   coordinated by fran  ois pachet. flow machines plans to    research and
   develop artificial intelligence systems able to generate music
   autonomously or in collaboration with human artists   . the arrangement,
   lyrics, and production are by composer beno  t carr  .

   [77]eduardo reck miranda, a composer and researcher previously at sony
   csl, has released an entire album of    computer-aided symphonic works   
   called    [78]mind pieces, sound to sea    through an [79]otherwise
   traditional label specializing in classical and jazz. while the
   technologies behind groups like sony csl are proprietary, we can make
   some guesses based on the researchers involved. for example: it   s
   likely that flow machines has continued with the same approach as
   continuator, more akin to david cope than doug eck. (but for id56-based
   approaches to    duets    and    continuations   , check out [80]deep musical
   dialogue by mason bretan, and [81]ai duet by magenta.)

   at ibm the watson team has developed a system called [82]watson beat
   that can produce complete tracks in a limited number of styles, based
   on a melodic prompt.

   iframe: [83]/media/77855052ab07eb294422da0c398a83ef?postid=f46dffac21c0

   other researchers on the watson team have [84]worked with alex da kid
   to suggest themes and inspiration for music based on data mined from
   social media and culture.
     __________________________________________________________________

   dice games, markov chains, and id56s aren   t the only ways to make
   algorithmic music. some machine learning practitioners explore
   alternative approaches like [85]hierarchical temporal memory, or
   [86]principal components analysis. but i   m focusing on neural nets
   because they are responsible for most of the big changes recently.
   (though even within the domain of neural nets there are some directions
   i   m leaving out that have fewer examples, such as restricted boltzmann
   machines for composing [87]4-bar jazz licks, [88]short variations on a
   single song, or hybrid [89]id56-rbm models, or hybrid
   [90]autoencoder-lstm models, or even [91]neuroevolutionary strategies).

   the power of id56s wasn   t common knowledge until [92]andrej karpathy   s
   viral post    [93]the unreasonable effectiveness of recurrent neural
   networks    in may 2015. andrej showed that a relatively simple neural
   network called [94]char-id56 could reliably recreate the    look and feel   
   of any text, from shakespeare to c++. the same way that the popularity
   of dice games was [95]buffeted by a resurgence of rationalism and
   interest in mathematics, andrej   s article came at a time when interest
   in neural networks was exploding, triggering a renewed interest in
   recurrent networks. some of the first people to test andrej   s code
   applied it to symbolic music notation.

   iframe: [96]/media/bdd2571dac6ed93b924dcb0de89f0381?postid=f46dffac21c0

      eight short outputs    by bob sturm, using char-id56 and 23,000 abc
   transcriptions of [97]irish folk music. he has also [98]lead groups to
   perform these compositions.

   iframe: [99]/media/7f2d0c427ef091dbde6b93a0ca8957d1?postid=f46dffac21c0

   by [100]gaurav trivedi, using char-id56 and 207 tabla rhythms.

   some people started with char-id56 as inspiration, but developed their
   own architecture specifically for working with music. notable examples
   come from [101]daniel johnson and [102]ji-sung kim.

   iframe:
   [103]/media/33f01e3e0a57a8a1f8f89a192375054c?postid=f46dffac21c0

   custom id56 architecture trained on [104]classical music for piano.

   iframe:
   [105]/media/8fdf504fc6db4708f30a581f8ad6bd30?postid=f46dffac21c0

   [106]deepjazz uses the same architecture as char-id56 and trains on a
   single song.

   [107]christian walder uses lstms in a more unusual way: starting with a
   pre-defined rhythm, and asking the neural net to fill in the pitches.
   this provides a lot of the global structure that is otherwise usually
   missing, but heavily constrains the possibilities.

   iframe:
   [108]/media/7a396c8c7e918f2a51c0c32013a81836?postid=f46dffac21c0

   example from    [109]modeling symbolic music: beyond the piano roll    by
   christian walder, trained on baroque sonatas.

   while all the examples so far are based on symbolic representations of
   music, some enthusiasts pushed char-id56 to its limits by feeding it raw
   audio.

   iframe:
   [110]/media/2f33abb071350b3346c334575250020d?postid=f46dffac21c0

   by joseph l. chu, trained on [111]30 minutes of    [112]a japanese pop
   rock band   .

   iframe:
   [113]/media/e3d61cc2785d50c66513ae582639958f?postid=f46dffac21c0

   by priya pramesi, trained on joanna newsom.

   unfortunately it seems that char-id56 is fundamentally limited in its
   capacity to abstract higher level representations of raw audio. the
   [114]most inspiring results on audio turned out to be nothing more than
   noisy copies of the source material (some people explain this when
   sharing their work, see [115]somethingunreal modeling his own speech).
   in machine learning this is related to the concept of    overfitting   :
   when a model can recreate the training data faithfully, but can   t
   effectively generalize to anything novel that it hasn   t been trained
   on. during training, initially first the model performs poorly on both
   the training and novel data, then it starts to perform better on both.
   but if you let it train too long, it gets worse at generalizing to
   novel data at the expense of recreating the training data. researchers
   stop the training just before hitting that point. but overfitting is
   not so clearly a    problem    in creative contexts, where recombination of
   existing material is a common strategy that is hard to distinguish from
      generalization   . some people like david cope go so far as to say
      [116]all music [is] essentially inspired plagiarism    (but he has also
   been accused of [117]publishing pseudoscience and [118]straight-up
   [119]plagiarism).

   in september 2016 [120]deepmind published their [121]wavenet research
   demonstrating an architecture that can build higher level abstractions
   of audio[122] sample-by-sample.
   [1*3s2ilkuygdc6x6sxv2x07q.png]
   diagram of the dilated convolutions used in the wavenet architecture.
   [1*qfoyn2zc4l8uktdtjw6v9q.png]
   wavenet sample-by-sample id203 distributions across the range of
   8-bit values.

   instead of using a recurrent network to learn representations over
   time, they used a [123]convolutional network. convolutional networks
   learn combinations of filters. they   re normally used for processing
   images, but wavenet treats time like a spatial dimension.

   iframe:
   [124]/media/a62ee4cf4ee2dc7039fba5c17b7ed684?postid=f46dffac21c0

   [125]samples of wavenet trained on piano music from youtube.

   looking into the background of the co-authors, there are some
   interesting predecessors to wavenet.
     * [126]sander dieleman is first author on [127]end-to-end learning
       for music audio (2014), a rare and early example of processing raw
       audio sample-by-sample with a neural net; in this case for genre
       classification (first use of neural nets for this task was
       [128]five years earlier).
     * [129]a  ron van den oord is first author on [130]pixel recurrent
       neural networks (2016), introducing networks that generate images
       pixel-by-pixel.
     * alex graves, besides having a long history working with speech and
       recurrent neural networks, showed a demo of end-to-end trained
       neural net generated [131]synthetic speech in march 2015.

   one of my favorite things to emerge from the wavenet research is this
   rough piano imitation by sageev oore, who was on sabbatical at google
   brain at the time.

   iframe:
   [132]/media/efc5700ad6ffb3b261e3a32f54e90dd4?postid=f46dffac21c0

   sageev oore performs    [133]sample_3.wav    by wavenet

   in april 2017, magenta built on wavenet to create [134]nsynth, a model
   for analyzing and generating monophonic instrument sounds. they created
   an nsynth-powered    [135]sound maker    experiment in collaboration with
   google creative lab new york. i worked with the google creative lab in
   london to build nsynth into an open-source portable midi synthesizer,
   called    nsynth super   .
   [1*wal4ykynxow7txjxfxmexq.png]
   demonstration of linear interpolation between two sounds compared to
   nsynth interpolation.

   iframe:
   [136]/media/f0cc30fe904bf0f2f24c1003181aba5e?postid=f46dffac21c0

   [137]nsynth super (2018) by google creative lab.

   in february 2017 a team from montreal lead by yoshua bengio published
   [138]sampleid56 (with [139]code) for sample-by-sample generation of
   audio using a set of recurrent networks in a hierarchical structure.
   this research was influenced by experiments from [140]ishaan gulrajani
   who trained a hierarchical version of char-id56 on raw audio.
   [1*cuq2khbnsmuon34m_ahseq.png]
   simplified snapshot of the sampleid56 architecture: a hierarchy of
   recurrent networks (tier 2 and 3) at slower time scales, combined with
   one standard neural network (tier 1) at the fastest time scale, all
   using the same upsampling ratio (4).

   iframe:
   [141]/media/ca24cd3d471a3db50e15251efd43c8a2?postid=f46dffac21c0

   sampleid56 trained on all over a hundred hours of speech from a single
   person (the blizzard dataset).

   iframe:
   [142]/media/501c1e7831580b2e35f9869e03ca7dca?postid=f46dffac21c0

   sampleid56 trained on all 32 of beethoven   s piano sonatas.

   iframe:
   [143]/media/ca6e8d4dbb20aeb2aa37f731dbef2be4?postid=f46dffac21c0

   by richard assar, trained on 32 hours of tangerine dream, using his
   [144]port of the original code.

   iframe:
   [145]/media/e96d9b9b2f0fe96ea1b5783b58828b09?postid=f46dffac21c0

   by [146]dadabots, trained on the album [147]diotima by krallice,
   accepted to nips 2017.

   both sampleid56 and wavenet take an unusually long time to train (more
   than a week), and without optimizations (like [148]fast-wavenet) they
   are many times slower than realtime for generation. to reduce the
   training and generation time researchers use audio at 16khz and 8 bits.

   but for companies like google or baidu, the primary application of
   audio generation is text to speech, where fast generation is essential.
   in march 2017 google published their [149]tacotron research, which
   generates audio frame-by-frame using a [150]spectral representation as
   an intermediate output step and a sequence of characters (text) as
   input.
   [1*zlyf5yioh1eem6dpuhw3_q.png]
   tacotron architecture, showing a mixture of techniques including
   attention, bidirectional id56s, and convolution.

   the tacotron [151]demo samples are similar to wavenet, with some small
   discrepancies. in may 2017, baidu built on the tacotron architecture
   with their [152]deep voice 2 research, increasing the audio quality by
   adding some final stages specific to speech generation. because
   generating audio from amplitude spectra requires a phase reconstruction
   step, the quality of polyphonic and noisy audio from this approach can
   be limited. but this hasn   t stopped folks like [153]dmitry ulyanov from
   using spectra for audio stylization, while [154]leon fedden, [155]memo
   akten and [156]max frenzel have used spectra for generation. for phase
   reconstruction, tacotron, dmitry and max use [157]griffin-lim, while
   leon and memo use [158]lws . leon, memo and max all use an autoencoder
   to build a latent space across spectrograms.

   iframe:
   [159]/media/9ea3c7a9a97999dea903fd1eb7f43708?postid=f46dffac21c0

   besides dmitry, other researchers who have looked into style transfer
   include [160]parag mital in november 2017 (focused on audio
   stylization) and [161]mor et al in may 2018 (focused on musical style
   transfer across instruments/genres). for more early work on audio style
   transfer with only concatenative synthesis,    [162]audio analogies   
   (2005) provides a lot of inspiration.

   in november 2017, deepmind published their    [163]parallel wavenet   
   technique where a slow-to-train wavenet teaches a fast-to-generate
   student. instead of predicting a 256-way 8-bit output, they use a
   [164]discretized mixture of logistics (dmol), which allows for 16-bit
   output. google immediately started using parallel wavenet in
   production. in december 2017, google published [165]tacotron 2 using a
   parallel wavenet as the synthesis (vocoder) step instead of griffin-lim
   phase reconstruction. this kicked off a wave of papers focusing on
   id133 conditioned on mel spectra, including [166]clarinet
   (which also introduces an end-to-end text-to-wave architecture),
   [167]waveglow and [168]flowavenet. in october 2018, google published a
   [169]controllable version of their tacotron system, allowing them to
   synthesize voice in different styles (something they proposed in the
   original tacotron blog post). there is a wealth of other research
   related to id133, but it isn   t always relevant to the more
   general task of generating audio in a musical context.

   in february 2018, deepmind published    [170]efficient neural audio
   synthesis    or    waveid56    which solves fast generation using a handful of
   optimizations. instead of using dmol outputs, they achieve 16-bit
   output by using two separate 8-bit outputs: one for the high bits, and
   one for the low bits.
     __________________________________________________________________

   where might this research head next?

   one domain that seems under explored is corpus-based synthesis
   (granular or concatenative) combined with frame-level representations.
   concatenative synthesis is common in id133 (where it   s
   called    unit selection   ). these techniques also have a long history in
   sound design for texture synthesis with tools like [171]catart. one
   significant limitation of this sort of corpus-based approach is that
   it   s impossible to generate a    moment    of audio that never appeared in
   your original corpus. if you trained a corpus-based model on all of
   bach, and bach never wrote a c minor major 7th chord, then you will
   never be able to generate a c minor major 7th. even if the model learns
   how to produce each of the notes in the chord, and even if it even
   learns how to represent the corresponding frame, you won   t have source
   material to sample for synthesis. to overcome this constraint, perhaps
   there is something waiting to be discovered at the intersection of
   frame-by-frame granular modeling and research on [172]audio
   decomposition/factorization.

   in terms of the research approach, i see at least two recurring
   questions. first, what kind of representations should we use? should we
   treat sound as individual samples, as spectral frames with mostly
   monophonic tonal content, as a pitches in a grid, as properties of a
   vocal synthesizer? how much domain-specific knowledge should we embed
   into our representation of sound? and second, how do we want to
   interact with these systems? do we want them to learn from the entire
   documented history of music with a vague goal of producing something
   similar, or something novel? to construct entire compositions, or to
   improvise with us? i   m wary of anyone who suggests that there is only
   one answer to these questions, and if anything we need to expand our
   imagination in terms of sound representation and modes of interaction.

   i   ve noticed the more    accessible    algorithmic compositions are likely
   to trigger the question from journalists:    does this make human
   musicians obsolete?    usually the researchers say they   re    not trying to
   replace humans   , but they   re trying to    build new tools   , or they
   encourage musicians to    think of the algorithms as collaborators   .
   talking about creative ai as    augmenting    the human creative process
   feels reassuring. but is there any reason that an ai won   t eventually
   create a pop hit from scratch? or not a pop hit, but just one of your
   favorite songs? i think the big question is less about whether human
   artists and musicians are obsolete in the face of ai, and more about
   what work we will accept as    art   , or as    music   . maybe your favorite
   singer-songwriter can   t be replaced because you need to know there is a
   human behind those chords and lyrics for it to    work   . but when you   re
   dancing to a club hit you don   t need a human behind it, you just need
   to know that everyone else is dancing too.

   there   s also an opportunity here to look beyond traditional models for
   what makes music    work   . [173]vocaloids like [174]hatsune miku have
   shown that a virtual persona backed by a vocal synthesizer can bring
   together millions of people in a massively crowdsourced act of
   composition and listening. music is probably [175]older than language,
   but we   re still discovering all the things music can be, and all the
   ways it might be crafted.
     __________________________________________________________________

   thanks to lauren mccarthy, brian whitman, kyle kastner, and parag mital
   for feedback.

   thanks to [176]brian whitman. [177]some rights reserved
     * [178]machine learning
     * [179]music
     * [180]artificial intelligence
     * [181]deep learning
     * [182]creativity

   (button)
   (button)
   (button) 5k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [183]go to the profile of kyle mcdonald

[184]kyle mcdonald

   artist working with code.

     (button) follow
   [185]artists + machine intelligence

[186]artists + machine intelligence

   this publication showcases collaborations with artists, engineers and
   researchers as part of google   s artists and machine intelligence
   program.

     * (button)
       (button) 5k
     * (button)
     *
     *

   [187]artists + machine intelligence
   never miss a story from artists + machine intelligence, when you sign
   up for medium. [188]learn more
   never miss a story from artists + machine intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f46dffac21c0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/artists-and-machine-intelligence/neural-nets-for-generating-music-f46dffac21c0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/artists-and-machine-intelligence/neural-nets-for-generating-music-f46dffac21c0&source=--------------------------nav_reg&operation=register
   8. https://medium.com/artists-and-machine-intelligence?source=logo-lo_p6zicu87nnnh---e1df37a4e9c1
   9. https://medium.com/artists-and-machine-intelligence/archive
  10. https://medium.com/artists-and-machine-intelligence/what-is-ami-96cd9ff49dde
  11. https://medium.com/artists-and-machine-intelligence/submit-an-ami-article-598634cbd631
  12. https://medium.com/@kcimc?source=post_header_lockup
  13. https://medium.com/@kcimc
  14. https://en.wikipedia.org/wiki/wind_chime#eastern_and_southern_asia
  15. https://en.wikipedia.org/wiki/aeolian_harp
  16. https://en.wikipedia.org/wiki/suikinkutsu
  17. https://en.wikipedia.org/wiki/musikalisches_w  rfelspiel
  18. https://medium.com/media/cd31f3b7dbb17ff72ac8d4f399031321?postid=f46dffac21c0
  19. https://en.wikipedia.org/wiki/markov_chain
  20. https://chatbotslife.com/notes-on-remixing-noon-generative-text-and-markov-chains-84ff4ec23937
  21. https://en.wikipedia.org/wiki/iannis_xenakis
  22. https://www.youtube.com/watch?v=mxijo-af_u8
  23. https://monoskop.org/images/7/74/xenakis_iannis_formalized_music_thought_and_mathematics_in_composition.pdf
  24. https://medium.com/media/b433c731a46fe71fd5ea44dd05bc0845?postid=f46dffac21c0
  25. https://en.wikipedia.org/wiki/david_cope
  26. http://artsites.ucsc.edu/faculty/cope/experiments.htm
  27. http://www.musicainformatica.org/topics/illiac-suite.php
  28. https://archive.org/details/experimentalmusi00hill
  29. http://quod.lib.umich.edu/cgi/p/pod/dod-idx/experiments-in-music-intelligence-emi.pdf?c=icmc;idno=bbp2372.1987.025
  30. https://www.google.com/patents/us7696426
  31. https://github.com/heinrichapfelmus/david-cope-cmmc
  32. https://medium.com/media/93e96fee593d506b47f2164e1d7ee535?postid=f46dffac21c0
  33. http://artsites.ucsc.edu/faculty/cope/mp3page.htm
  34. http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139
  35. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  36. http://www.indiana.edu/~abcwest/pmwiki/pdf/todd.compmusic.1989.pdf
  37. https://mitpress.mit.edu/books/music-and-connectionism
  38. https://medium.com/media/5ae785beb68dc0bff7adde62ca1368a7?postid=f46dffac21c0
  39. http://www.cs.colorado.edu/~mozer/research/selected publications/music.html
  40. http://www.iro.umontreal.ca/~eckdoug/papers/2002_ieee.pdf
  41. http://www.iro.umontreal.ca/~eckdoug/blues/index.html
  42. https://medium.com/media/1c8476d7a29ced63742f743e9140fd94?postid=f46dffac21c0
  43. http://www.iro.umontreal.ca/~eckdoug/blues/
  44. https://magenta.tensorflow.org/welcome-to-magenta
  45. https://research.google.com/teams/brain/
  46. https://github.com/tensorflow/magenta/tree/master/magenta/models
  47. https://github.com/tensorflow/magenta/tree/master/magenta/models/drums_id56
  48. https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_id56
  49. https://github.com/tensorflow/magenta/tree/master/magenta/models/polyphony_id56
  50. https://aiexperiments.withgoogle.com/ai-duet/
  51. https://magenta.tensorflow.org/performance-id56
  52. https://js.tensorflow.org/
  53. https://magenta.tensorflow.org/demos
  54. https://magenta.tensorflow.org/studio
  55. https://magenta.tensorflow.org/multitrack
  56. https://experiments.withgoogle.com/ai/beat-blender/view/
  57. https://medium.com/media/d38512abe0bbf2466c22167f54f43897?postid=f46dffac21c0
  58. https://magenta.tensorflow.org/performance-id56
  59. https://goo.gl/magenta/performanceid56-demo
  60. https://arxiv.org/abs/1809.04281v2
  61. https://storage.googleapis.com/music-transformer/index.html
  62. https://medium.com/media/44d018b9e4a10ea5cef8c6e4db142129?postid=f46dffac21c0
  63. https://arxiv.org/abs/1809.04281v2
  64. https://storage.googleapis.com/music-transformer/index.html
  65. https://thesession.org/
  66. http://ifdo.ca/~seymour/nottingham/nottingham.html
  67. https://arxiv.org/abs/1806.04278
  68. https://www.jukedeck.com/
  69. http://www.aiva.ai/
  70. https://www.ampermusic.com/
  71. https://www.youtube.com/watch?v=xus6cznn8pw
  72. https://www.musicbusinessworldwide.com/welcome-future-spotify-poaches-ai-music-expert-sony/
  73. http://francoispachet.fr/continuator/continuator.html
  74. http://www.flow-machines.com/
  75. https://medium.com/media/e0bd40e1d54ca80e58ed5b5ad7702e8c?postid=f46dffac21c0
  76. https://medium.com/media/a9e13845c318edfd146ce9dcb2451c42?postid=f46dffac21c0
  77. https://en.wikipedia.org/wiki/eduardo_reck_miranda
  78. https://open.spotify.com/album/0ynvagem7p8x488xdyeieb?si=o3ay8lihtjq5abdxauqakq
  79. https://davinci-edition.com/product/c00107/
  80. https://www.youtube.com/watch?v=aiazf2euar8
  81. https://experiments.withgoogle.com/ai/ai-duet
  82. https://soundcloud.com/ibmresearch/fallen-star-amped
  83. https://medium.com/media/77855052ab07eb294422da0c398a83ef?postid=f46dffac21c0
  84. https://www.ibm.com/watson/music/
  85. https://www.youtube.com/watch?v=2y4549ajgee
  86. https://soundcloud.com/bwhitman/01-radiant-bells
  87. https://www.cs.hmc.edu/~keller/jazz/improvisor/icccx-bickerman-bosley-swire-keller.pdf
  88. https://www.youtube.com/watch?v=theako6vi_a&nr=1
  89. http://deeplearning.net/tutorial/id56rbm.html
  90. https://www.youtube.com/watch?v=bbyvbo2f7ug
  91. http://maestrogenesis.org/
  92. https://twitter.com/karpathy
  93. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  94. https://github.com/karpathy/char-id56
  95. https://www.jstor.org/stable/734136
  96. https://medium.com/media/bdd2571dac6ed93b924dcb0de89f0381?postid=f46dffac21c0
  97. https://thesession.org/
  98. https://highnoongmt.wordpress.com/2017/07/04/folk-id56-at-the-qmul-ideas-festival-2017/
  99. https://medium.com/media/7f2d0c427ef091dbde6b93a0ca8957d1?postid=f46dffac21c0
 100. http://www.trivedigaurav.com/blog/machines-learn-to-play-tabla/
 101. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
 102. https://deepjazz.io/
 103. https://medium.com/media/33f01e3e0a57a8a1f8f89a192375054c?postid=f46dffac21c0
 104. http://www.piano-midi.de/
 105. https://medium.com/media/8fdf504fc6db4708f30a581f8ad6bd30?postid=f46dffac21c0
 106. https://deepjazz.io/
 107. http://users.cecs.anu.edu.au/~christian.walder/
 108. https://medium.com/media/7a396c8c7e918f2a51c0c32013a81836?postid=f46dffac21c0
 109. https://arxiv.org/abs/1606.01368
 110. https://medium.com/media/2f33abb071350b3346c334575250020d?postid=f46dffac21c0
 111. https://www.reddit.com/r/machinelearning/comments/3c87b3/training_a_id56_with_audio_data/csv4bla/
 112. https://www.reddit.com/r/machinelearning/comments/3x7poc/generating_sound_with_recurrent_neural_nets/cy39off/
 113. https://medium.com/media/e3d61cc2785d50c66513ae582639958f?postid=f46dffac21c0
 114. https://www.youtube.com/watch?v=0vti1bblyde
 115. https://www.youtube.com/watch?v=ng-latbznbs
 116. https://www.theguardian.com/technology/2010/jul/11/david-cope-computer-composer
 117. http://slab.org/tmp/wiggins-cope.pdf
 118. https://www.youtube.com/watch?v=lt7fechgfru&lc=z13deduomr31zdyag04cfbfpgqe4jp0yoos0k
 119. https://www.youtube.com/watch?v=id25cnikym4s&lc=z12bcr0qklm1u13w004cj3qromunsjdism40k
 120. https://deepmind.com/
 121. https://arxiv.org/abs/1609.03499
 122. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
 123. http://cs231n.github.io/convolutional-networks/
 124. https://medium.com/media/a62ee4cf4ee2dc7039fba5c17b7ed684?postid=f46dffac21c0
 125. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
 126. https://twitter.com/sedielem
 127. https://dl.dropboxusercontent.com/u/19706734/paper_pt.pdf
 128. https://papers.nips.cc/paper/3674-unsupervised-feature-learning-for-audio-classification-using-convolutional-deep-belief-networks
 129. https://twitter.com/avdnoord
 130. https://arxiv.org/abs/1601.06759
 131. https://youtu.be/-yx1syedhbg?t=2545
 132. https://medium.com/media/efc5700ad6ffb3b261e3a32f54e90dd4?postid=f46dffac21c0
 133. https://storage.googleapis.com/deepmind-media/pixie/making-music/sample_3.wav
 134. https://magenta.tensorflow.org/nsynth
 135. https://experiments.withgoogle.com/ai/sound-maker
 136. https://medium.com/media/f0cc30fe904bf0f2f24c1003181aba5e?postid=f46dffac21c0
 137. https://nsynthsuper.withgoogle.com/
 138. https://arxiv.org/abs/1612.07837
 139. https://github.com/soroushmehr/sampleid56_iclr2017
 140. https://github.com/igul222
 141. https://medium.com/media/ca24cd3d471a3db50e15251efd43c8a2?postid=f46dffac21c0
 142. https://medium.com/media/501c1e7831580b2e35f9869e03ca7dca?postid=f46dffac21c0
 143. https://medium.com/media/ca6e8d4dbb20aeb2aa37f731dbef2be4?postid=f46dffac21c0
 144. https://github.com/richardassar/sampleid56_torch
 145. https://medium.com/media/e96d9b9b2f0fe96ea1b5783b58828b09?postid=f46dffac21c0
 146. http://dadabots.com/
 147. https://krallice.bandcamp.com/album/diotima
 148. https://github.com/toid113paine/fast-wavenet
 149. https://arxiv.org/abs/1703.10135
 150. https://en.wikipedia.org/wiki/spectrogram
 151. https://google.github.io/tacotron/publications/tacotron/index.html
 152. https://arxiv.org/abs/1705.08947
 153. https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/
 154. https://twitter.com/leonfedden/status/996179566828933120
 155. http://www.memo.tv/portfolio/grannma-magnet/
 156. https://towardsdatascience.com/neuralfunk-combining-deep-learning-with-sound-design-91935759d628
 157. https://ieeexplore.ieee.org/document/1164317
 158. https://github.com/jonathan-leroux/lws
 159. https://medium.com/media/9ea3c7a9a97999dea903fd1eb7f43708?postid=f46dffac21c0
 160. https://arxiv.org/abs/1711.11160
 161. https://arxiv.org/abs/1805.07848
 162. http://www.iansimon.org/audio_analogies/
 163. https://arxiv.org/abs/1711.10433
 164. https://arxiv.org/abs/1701.05517
 165. https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html
 166. https://arxiv.org/abs/1807.07281
 167. https://nv-adlr.github.io/waveglow
 168. https://arxiv.org/abs/1811.02155
 169. https://google.github.io/tacotron/publications/gmvae_controllable_tts/index.html
 170. https://arxiv.org/abs/1802.08435
 171. http://imtr.ircam.fr/imtr/catart
 172. https://arxiv.org/abs/1609.03296
 173. https://en.wikipedia.org/wiki/vocaloid
 174. https://en.wikipedia.org/wiki/hatsune_miku
 175. http://www.npr.org/templates/story/story.php?storyid=129155123
 176. https://medium.com/@bwhitman?source=post_page
 177. https://creativecommons.org/licenses/by-nc-sa/4.0/
 178. https://medium.com/tag/machine-learning?source=post
 179. https://medium.com/tag/music?source=post
 180. https://medium.com/tag/artificial-intelligence?source=post
 181. https://medium.com/tag/deep-learning?source=post
 182. https://medium.com/tag/creativity?source=post
 183. https://medium.com/@kcimc?source=footer_card
 184. https://medium.com/@kcimc
 185. https://medium.com/artists-and-machine-intelligence?source=footer_card
 186. https://medium.com/artists-and-machine-intelligence?source=footer_card
 187. https://medium.com/artists-and-machine-intelligence
 188. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 190. https://medium.com/p/f46dffac21c0/share/twitter
 191. https://medium.com/p/f46dffac21c0/share/facebook
 192. https://medium.com/p/f46dffac21c0/share/twitter
 193. https://medium.com/p/f46dffac21c0/share/facebook
