   #[1]rss

   [2]20bits
     * [3]about
     * [4]writing

introduction to id145

   by [5]jesse farmer on saturday, november 15, 2008

   id145 is a method for efficiently solving a broad range
   of search and optimization problems which exhibit the characteristics
   of [6]overlappling subproblems and [7]optimal substructure. i'll try to
   illustrate these characteristics through some simple examples and end
   with an exercise. happy coding!

contents

    1. [8]overlapping subproblems
    2. [9]optimal substructure
    3. [10]the knapsack problem
    4. [11]everyday id145

overlapping subproblems

   a problem is said to have overlapping subproblems if it can be broken
   down into subproblems which are reused multiple times. this is closely
   related to recursion. to see the difference consider the [12]factorial
   function, defined as follows (in python):
def factorial(n):
        if n == 0: return 1
        return n*factorial(n-1)

   thus the problem of calculating factorial(n) depends on calculating the
   subproblem factorial(n-1). this problem does not exhibit overlapping
   subproblems since factorial is called exactly once for each positive
   integer less than n.

fibonacci numbers

   the problem of calculating the n^th [13]fibonacci number does, however,
   exhibit overlapping subproblems. the na    ve recursive implementation
   would be
def fib(n):
        if n == 0: return 0
        if n == 1: return 1
        return fib(n-1) + fib(n-2)

   the problem of calculating fib(n) thus depends on both fib(n-1) and
   fib(n-2). to see how these subproblems overlap look at how many times
   fib is called and with what arguments when we try to calculate fib(5):
fib(5)
fib(4) + fib(3)
fib(3) + fib(2) + fib(2) + fib(1)
fib(2) + fib(1) + fib(1) + fib(0) + fib(1) + fib(0) + fib(1)
fib(1) + fib(0) + fib(1) + fib(1) + fib(0) + fib(1) + fib(0) + fib(1)

   at the k^th stage we only need to know the values of fib(k-1) and
   fib(k-2), but we wind up calling each multiple times. starting from the
   bottom and going up we can calculate the numbers we need for the next
   step, removing the massive redundancy.
def fib2(n):
        n2, n1 = 0, 1
        for i in range(n-2):
                n2, n1 = n1, n1 + n2
        return n2+n1

   in [14]big-o notation the fib function takes o(c^n) time, i.e.,
   exponential in n, while the fib2 function takes o(n) time. if this is
   all too abstract take a look at this graph comparing the runtime (in
   microseconds) of fib and fib2 versus the input parameter.
   [15][fib_performance_thumb.png]

   the above problem is pretty easy and for most programmers is one of the
   first examples of the performance issues surrounding recursion versus
   iteration. in fact, i've seen many instances where the fibonacci
   example leads people to believe that recursion is inherently slow. this
   is not true, but in cases where we can define a problem with
   overlapping subproblems recursively using the above technique will
   always reduce the execution time.

   now, for the second characteristic of id145: optimal
   substructure.

optimal substructure

   a problem is said to have [16]optimal substructure if the globally
   optimal solution can be constructed from locally optimal solutions to
   subproblems. the general form of problems in which optimal substructure
   plays a roll goes something like this. let's say we have a collection
   of objects called a. for each object o in a we have a "cost," c(o). now
   find the subset of a with the maximum (or minimum) cost, perhaps
   subject to certain constraints.

   the brute-force method would be to generate every subset of a,
   calculate the cost, and then find the maximum (or minimum) among those
   values. but if a has n elements in it we are looking at a search space
   of size 2^n if there are no constraints on a. oftentimes n is huge
   making a brute-force method computationally infeasible. let's take a
   look at an example.

maximum subarry sum

   let's say we're given an array of integers. what (contiguous) subarray
   has the largest sum? for example, if our array is [1,2,-5,4,7,-2] then
   the subarray with the largest sum is [4,7] with a sum of 11. one might
   think at first that this problem reduces to finding the subarray with
   all positive entries, if one exists, that maximizes the sum. but
   consider the array [1,5,-3,4,-2,1]. the subarray with the largest sum
   is [1, 5, -3, 4] with a sum of 7.

   first, the brute-force solution. because of the constraints on the
   problem, namely that the subsets under consideration are contiguous, we
   only have to check o(n^2) subarrays (why?). here it is, in python:
def msum(a):
        return max([(sum(a[j:i]), (j,i)) for i in range(1,len(a)+1) for j in ran
ge(i)])

   this returns both the sum and the offsets of the subarray. let's see if
   we can't find an optimal substructure to exploit.

   we are given an input array a. i'm going to use python notation so that
   a[0:k] is the subarray starting at 0 and including every element up to
   and including k-1. let's say we know the subarray of a[0:i] with the
   largest sum (and that sum). using just this information can we find the
   subarray of a[0:i+1] with the largest sum?

   let a[j:k+1] be the optimal subarray, t the sum of a[j:i], and s the
   optimal sum. if t+a[i] is greater than s then set a[j:i+1] as the
   optimal array and set s = t. if t + a[i] is negative, however, the
   contiguity constraint means that we cannot include a[j:i+1] in our
   subarray since any such subarray will have a smaller sum than a
   subarray without it. so, if t+a[i] is negative set t = 0 and set the
   left-hand bound of the optimal subarray to i+1.

   to visualize consider the array [1,2,-5,4,7,-2].
set s = -infinity, t = 0, j = 0, bounds = (0,0)
(1   2  -5   4   7  -2
(1)| 2  -5   4   7  -2  (set t=1.  since t > s, set s=1 and bounds = (0,1))
(1   2)|-5   4   7  -2  (set t=3.  since t > s, set s=3, and bounds = (0,2))
 1   2  -5(| 4   7  -2  (set t=-2. since t < 0, set t=0 and j = 3 )
 1   2  -5  (4)| 7  -2  (set t=4.  since t > s, set s=4 and bounds = (3,4))
 1   2  -5  (4   7)|-2  (set t=11. since t > s, set s=11 and bounds = (3,5))
 1   2  -5  (4   7) -2| (set t=9.  nothing happens since t < s)

   this requires only one pass through the array and at each step we're
   only keeping track of three variables: the current sum from the
   left-hand edge of the bounds to the current point (t), the maximal sum
   (s), and the bounds of the current optimal subarray (bounds). in
   python:
def msum2(a):
        bounds, s, t, j = (0,0), -float('infinity'), 0, 0

        for i in range(len(a)):
                t = t + a[i]
                if t > s: bounds, s = (j, i+1), t
                if t < 0: t, j = 0, i+1
        return (s, bounds)

   in this problem the "globally optimal" solution corresponds to a
   subarray with a globally maximal sum, but at each each step we only
   make a decision relative to what we have already seen. that is, at each
   step we know the best solution thus far, but might change our decision
   later based on our previous information and the current information.
   this is the sense in the problem has optimal substructure. because we
   can make decisions locally we only need to traverse the list once,
   reducing the run-time of the solution to o(n) from o(n^2). again, a
   graph:
   [17][msum_performance_thumb.png]

the knapsack problem

   let's apply what we're learned so far to a slightly more interesting
   problem. you are an art thief who has found a way to break into the
   impressionist wing at the art institute of chicago. obviously you can't
   take everything. in particular, you're constrained to take only what
   your knapsack can hold     let's say it can only hold w pounds. you also
   know the market value for each painting. given that you can only carry
   w pounds what paintings should you steal in order to maximize your
   profit?

   first let's see how this problem exhibits both overlapping subproblems
   and optimal substructure. say there are n paintings with weights w[1],
   ..., w[n] and market values v[1], ..., v[n]. define a(i,j) as the
   maximum value that can be attained from considering only the first i
   items weighting at most j pounds as follows.

   obviously a(0,j) = 0 and a(i,0) = 0 for any i     n and j     w. if w[i] >
   j then a(i,j) = a(i-1, j) since we cannot include the i^th item. if,
   however, w[i]     j then a(i,j) then we have a choice: include the i^th
   item or not. if we do not include it then the value will be a(i-1, j).
   if we do include it, however, the value will be v[i] + a(i-1, j -
   w[i]). which choice should we make? well, whichever is larger, i.e.,
   the maximum of the two.

   expressed formally we have the following recursive definition knapsack
   function

   this problem exhibits both overlapping subproblems and optimal
   substructure and is therefore a good candidate for id145.
   the subproblems overlap because at any stage (i,j) we might need to
   calculate a(k,l) for several k < i and l < j. we have optimal
   substructure since at any point we only need information about the
   choices we have already made.

   the recursive solution is not hard to write:
def a(w, v, i,j):
    if i == 0 or j == 0: return 0
    if w[i-1] > j:  return a(w, v, i-1, j)
    if w[i-1] <= j: return max(a(w,v, i-1, j), v[i-1] + a(w,v, i-1, j - w[i-1]))

   remember we need to calculate a(n,w). to do so we're going to need to
   create an n-by-w table whose entry at (i,j) contains the value of
   a(i,j). the first time we calculate the value of a(i,j) we store it in
   the table at the appropriate location. this technique is called
   [18]memoization and is one way to exploit overlapping subproblems.
   there's also a ruby module called [19]memoize which does it for ruby.

   to exploit the optimal substructure we iterate over all i <= n and j <=
   w, at each step applying the recursion formula to generate the a(i,j)
   entry by using the memoized table rather than calling a() again. this
   gives an algorithm which takes o(nw) time using o(nw) space and our
   desired result is stored in the a(n,w) entry in the table.

everyday id145

   the above examples might make id145 look like a technique
   which only applies to a narrow range of problems, but many algorithms
   from a wide range of fields use id145. here's a very
   partial list.
    1. the [20]needleman-wunsch algorithm, used in bioinformatics.
    2. the [21]cyk algorithm which is used the theory of formal languages
       and natural language processing.
    3. the [22]viterbi algorithm used in relation to [23]hidden markov
       models.
    4. finding the [24]string-id153 between two strings, useful in
       writing spellcheckers.
    5. the [25]d/l method used in the sport of [26]cricket.

   that's all for today. cheers!

   [27]tweet

   copyright    2006-2012 [28]jesse farmer

references

   1. http://feeds.feedburner.com/20bits
   2. http://20bits.com/
   3. http://20bits.com/
   4. http://20bits.com/articles
   5. mailto:jesse farmer <jesse@20bits.com>?subject=introduction to id145
   6. http://en.wikipedia.org/wiki/overlapping_subproblem
   7. http://en.wikipedia.org/wiki/optimal_substructure
   8. http://20bits.com/article/introduction-to-dynamic-programming#subproblems
   9. http://20bits.com/article/introduction-to-dynamic-programming#optimal
  10. http://20bits.com/article/introduction-to-dynamic-programming#knapsack
  11. http://20bits.com/article/introduction-to-dynamic-programming#everyday
  12. http://mathworld.wolfram.com/factorial.html
  13. http://en.wikipedia.org/wiki/fibonacci_number
  14. http://en.wikipedia.org/wiki/big_o_notation
  15. http://20bits.com/include/images/fib_performance.png
  16. http://en.wikipedia.org/wiki/optimal_substructure
  17. http://20bits.com/include/images/msum_performance.png
  18. http://en.wikipedia.org/wiki/memoization
  19. http://raa.ruby-lang.org/project/memoize/
  20. http://en.wikipedia.org/wiki/needleman-wunsch_algorithm
  21. http://en.wikipedia.org/wiki/cyk_algorithm
  22. http://en.wikipedia.org/wiki/viterbi_algorithm
  23. http://en.wikipedia.org/wiki/hidden_markov_model
  24. http://en.wikipedia.org/wiki/levenshtein_distance
  25. http://en.wikipedia.org/wiki/duckworth-lewis_method
  26. http://en.wikipedia.org/wiki/one-day_cricket
  27. https://twitter.com/share
  28. mailto:jesse farmer <jesse@20bits.com>
