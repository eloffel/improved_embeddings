   #[1]hexahedria

[2]hexahedria

daniel johnson's personal fragment of the web

     * [3]home
     * [4]about
     * [5]publications
     * [6]projects

composing music with recurrent neural networks

   [7][diags-figure8.pdf.png]

   (update: a paper based on this work has been accepted at evomusart
   2017! see [8]here for more details.)

   it   s hard not to be blown away by the surprising power of neural
   networks these days. with enough training, so called    deep neural
   networks   , with many nodes and hidden layers, can do impressively well
   on modeling and predicting all kinds of data. (if you don   t know what
   i   m talking about, i recommend reading about [9]recurrent
   character-level language models, [10]google deep dream, and [11]neural
   turing machines. very cool stuff!) now seems like as good a time as
   ever to experiment with what a neural network can do.

   for a while now, i   ve been floating around vague ideas about writing a
   program to compose music. my original idea was based on a fractal
   decomposition of time and some sort of repetition mechanism, but after
   reading more about neural networks, i decided that they would be a
   better fit. so a few weeks ago, i got to work designing my network. and
   after training for a while, i am happy to report remarkable success!

   here   s a taste of things to come:

   but first, some background about neural networks, and id56s in
   particular. (if you already know all about neural networks, feel free
   to skip this part!)

feedforward neural networks:

   a single node in a simple neural network takes some number of inputs,
   and then performs a weighted sum of those inputs, multiplying them each
   by some weight before adding them all together. then, some constant
   (called    bias   ) is added, and the overall sum is then squashed into a
   range (usually -1 to 1 or 0 to 1) using a nonlinear activation
   function, such as a sigmoid function.

   [640px-logistic-curve.svg.png]

                             a sigmoid function.

   we can visualize this node by drawing its inputs and single output as
   arrows, and denoting the weighted sum and activation by a circle:

   we can then take multiple nodes and feed them all the same inputs, but
   allow them to have different weights and biases. this is known as a
   layer.

   (note: because each node in the layer performs a weighted sum, but they
   all share the same inputs, we can calculate the outputs using a matrix
   multiplication, followed by elementwise activation! this is one reason
   why neural networks can be trained so effectively.)

   then we can connect multiple layers together:

   and voila, we have a neural network. (quick note on terminology: the
   set of inputs is called the    input layer   , the last layer of nodes is
   called the    output layer   , and all intermediate node layers are called
      hidden layers   . also, in case it isn   t clear, all the arrows from each
   node carry the same value, since each node has a single output value.)

   for simplicity, we can visualize the layers as single objects, since
   that   s how they are implemented most of the time:

   from this point on, when you see a single circle, that represents an
   entire layer of the network, and the arrows represent vectors of
   values.

recurrent neural networks

   notice that in the basic feedforward network, there is a single
   direction in which the information flows: from input to output. but in
   a recurrent neural network, this direction constraint does not exist.
   there are a lot of possible networks that can be classified as
   recurrent, but we will focus on one of the simplest and most practical.

   basically, what we can do is take the output of each hidden layer, and
   feed it back to itself as an additional input. each node of the hidden
   layer receives both the list of inputs from the previous layer and the
   list of outputs of the current layer in the last time step. (so if the
   input layer has 5 values, and the hidden layer has 3 nodes, each hidden
   node receives as input a total of 5+3=8 values.)

   we can show this more clearly by unwrapping the network along the time
   axis:

   in this representation, each horizontal line of layers is the network
   running at a single time step. each hidden layer receives both input
   from the previous layer and input from itself one time step in the
   past.

   the power of this is that it enables the network to have a simple
   version of memory, with very minimal overhead. this opens up the
   possibility of variable-length input and output: we can feed in inputs
   one-at-a-time, and let the network combine them using the state passed
   from each time step.

   one problem with this is that the memory is very short-term. any value
   that is output in one time step becomes input in the next, but unless
   that same value is output again, it is lost at the next tick. to solve
   this, we can use a long short-term memory (lstm) node instead of a
   normal node. this introduces a    memory cell    value that is passed down
   for multiple time steps, and which can be added to or subtracted from
   at each tick. (i   m not going to go into all of the details, but you can
   read more about lstms in the [12]original paper.)

   we can think of the memory cell data being sent in parallel with the
   activation output. above, i have shown it using a blue arrow, and in my
   following diagrams i will omit it for simplicity.

training neural networks

   all those pretty pictures are nice, but how do we actually get the
   networks to output what we want? well, the neural network behavior is
   determined by the set of weights and biases that each node has, so we
   need to adjust those to some correct value.

   first, we need to define how good or bad any given output is, given the
   input. this value is called the cost. for example, if we were trying to
   use a neural network to model a mathematical function, the cost might
   be the difference between the function answer and the network output,
   squared. or if we were trying to model the likelihood of letters
   appearing in a particular order, the cost might be one minus the
   id203 of predicting the correct letter each time.

   once we have this cost value, we can use id26. this boils
   down to calculating the gradient of the cost with respect to the
   weights (i.e the derivative of cost with respect to each weight for
   each node in each layer), and then using some optimization method to
   adjust the weights to reduce the cost. the bad news is that these
   optimization methods are often very complex. but the good news is that
   many of them are already implemented in libraries, so we can just feed
   our gradient to the right function and let it adjust our weights
   correctly. (if you   re curious, and don   t mind some math, some
   optimization methods include [13]stochastic id119,
   [14]hessian-free optimization, [15]adagrad, and [16]adadelta.)

   [2dkcqhh.jpg]

          visualization of some commonly used optimization methods.

can they make music?

   all right, enough background. from here on, i   ll be talking mostly
   about my own thought process and the design of my network architecture.

   when i was starting to design my network   s architecture, i naturally
   looked up how other people have approached this problem. a few existing
   methods are collected below:
     * bob sturm uses a [17]character-based model with an lstm to generate
       a textual representation of a song (in [18]abc notation). the
       network seems to only be able to play one note at a time, but
       achieves interesting temporal patterns.
     * doug eck, in [19]a first look at music composition using lstm
       recurrent neural networks, uses lstms to do blues improvization.
       the sequences chosen all have the same set of chords, and the
       network has a single output node for each note, outputting the
       id203 of that note being played at each time step. results
       are promising in that it learns temporal structure, but is pretty
       restricted as to what it can output. also, there is no distinction
       between playing a note and holding it, so the network cannot
       rearticulate held notes.
     * nicolas boulanger-lewandowski, in [20]modeling temporal
       dependencies in high-dimensional sequences: application to
       polyphonic music generation and transcription, uses a network with
       two parts. there is an id56 to handle time dependency, which
       produces a set of outputs that are then used as the parameters for
       a [21]restricted id82, which in turn models the
       conditional distribution of which notes should be played with which
       other notes. this model actually produces quite nice-sounding
       music, but does not seem to have a real sense of time, and only
       plays a couple of chords. (see [22]here for the algorithm and
       sample output.)

   for my network design, there were a few properties i wanted it to have:
     * have some understanding of time signature: i wanted to give the
       neural network its current time in reference to a time signature,
       since most music is composed with a fixed time signature.
     * be time-invariant: i wanted the network to be able to compose
       indefinitely, so it needed to be identical for each time step.
     * be (mostly) note-invariant: music can be freely transposed up and
       down, and it stays fundamentally the same. thus, i wanted the
       structure of the neural network to be almost identical for each
       note.
     * allow multiple notes to be played simultaneously, and allow
       selection of coherent chords.
     * allow the same note to be repeated: playing c twice should be
       different than holding a single c for two beats.

   i   ll talk a bit more about the invariance properties, because i decided
   those were the most important. most existing id56-based music
   composition approaches are invariant in time, since each time step is a
   single iteration of the network. but they are in general not invariant
   in note. there is usually some specific output node that represents
   each note. so transposing everything up by, say, one whole step, would
   produce a completely different output. for most sequences, this is
   something you would want:    hello    is completely different from    ifmmp   ,
   which is just    transposed    one letter. but for music, you want to
   emphasize the relative relationships over the absolute positions: a c
   major chords sounds more like a d major chord than like a c minor
   chord, even though the c minor chord is closer with regard to absolute
   note positions.

   there is one kind of neural network that is widely in use today that
   has this invariant property along multiple directions: convolutional
   neural networks for image recognition. these work by basically learning
   a convolution kernel and then applying that same convolution kernel
   across every pixel of the input image.

   [kernel_convolution.jpg]

   how convolution works. each pixel is replaced by a weighted sum of the
      surrounding pixels. the neural network has to learn the weights.
                    picture from [23]developer.apple.com.

   hypothetically, what would happen if we replaced the convolution kernel
   with something else? say, a recurrent neural network? then each pixel
   would have its own neural network, which would take input from an area
   around the pixel. each neural network would in turn have its own memory
   cells and recurrent connections across time.

   now replace pixels with notes, and we have an idea for what we can do.
   if we make a stack of identical recurrent neural networks, one for each
   output note, and give each one a local neighborhood (for example, one
   octave above and below) around the note as its input, then we have a
   system that is invariant in both time and notes: the network can work
   with relative inputs in both directions.

   note: i have rotated the time axis here! notice that time steps now
   come out of the page, as do the recurrent connections. you can think of
   each of the    flat    slices as a copy of the basic id56 picture from
   above. also, i am showing each layer getting input from one note above
   and below. this is a simplification: the real network gets input from
   12 notes (the number of half steps in an octave) in each direction.

   however, there is still a problem with this network. the recurrent
   connections allow patterns in time, but we have no mechanism to attain
   nice chords: each note   s output is completely independent of every
   other note   s output. here we can draw inspiration from the id56-rbm
   combination above: let the first part of our network deal with time,
   and let the second part create the nice chords. but an rbm gives a
   single conditional distribution of a bunch of outputs, which is
   incompatible with using one network per note.

   the solution i decided to go with is something i am calling a    biaxial
   id56   . the idea is that we have two axes (and one pseudo-axis): there is
   the time axis and the note axis (and the direction-of-computation
   pseudo-axis). each recurrent layer transforms inputs to outputs, and
   also sends recurrent connections along one of these axes. but there is
   no reason why they all have to send connections along the same axis!

   notice that the first two layers have connections across time steps,
   but are independent across notes. the last two layers, on the other
   hand, have connections between notes, but are independent between time
   steps. together, this allows us to have patterns both in time and in
   note-space without sacrificing invariance!

   it   s a bit easier to see if we collapse one of the dimensions:

   now the time connections are shown as loops. it   s important to remember
   that the loops are always delayed by one time step: the output at time
   t is part of the input at time t+1.

input and output details

   my network is based on this architectural idea, but of course the
   actual implementation is a bit more complex. first, we have the input
   to the first time-axis layer at each time step: (the number in brackets
   is the number of elements in the input vector that correspond to each
   part)
     * position [1]: the midi note value of the current note. used to get
       a vague idea of how high or low a given note is, to allow for
       differences (like the concept that lower notes are typically
       chords, upper notes are typically melody).
     * pitchclass [12]: will be 1 at the position of the current note,
       starting at a for 0 and increasing by 1 per half-step, and 0 for
       all the others. used to allow selection of more common chords (i.e.
       it's more common to have a c major chord than an e-flat major
       chord)
     * previous vicinity [50]: gives context for surrounding notes in the
       last timestep, one octave in each direction. the value at index
       2(i+12) is 1 if the note at offset i from current note was played
       last timestep, and 0 if it was not. the value at 2(i+12) + 1 is 1
       if that note was articulated last timestep, and 0 if it was not.
       (so if you play a note and hold it, first timestep has 1 in both,
       second has it only in first. if you repeat a note, second will have
       1 both times.)
     * previous context [12]: value at index i will be the number of times
       any note x where (x-i-pitchclass) mod 12 was played last timestep.
       thus if current note is c and there were 2 e's last timestep, the
       value at index 4 (since e is 4 half steps above c) would be 2.
     * beat [4]: essentially a binary representation of position within
       the measure, assuming 4/4 time. with each row being one of the beat
       inputs, and each column being a time step, it basically just
       repeats the following pattern:
0101010101010101
0011001100110011
0000111100001111
0000000011111111

       however, it is scaled to [-1, 1] instead of [0,1].</p>

   then there is the first hidden lstm stack, which consists of lstms that
   have recurrent connections along the time-axis. the last time-axis
   layer outputs some note state that represents any time patterns. the
   second lstm stack, which is recurrent along the note axis, then scans
   up from low notes to high notes. at each note-step (equivalent of
   time-steps) it gets as input
     * the corresponding note-state vector from the previous lstm stack
     * a value (0 or 1) for whether the previous (half-step lower) note
       was chosen to be played (based on previous note-step, starts 0)
     * a value (0 or 1) for whether the previous (half-step lower) note
       was chosen to be articulated (based on previous note-step, starts
       0)

   after the last lstm, there is a simple, non-recurrent output layer that
   outputs 2 values:
     * play id203, which is the id203 that this note should be
       chosen to be played
     * articulate id203, which is the id203 the note is
       articulated, given that it is played. (this is only used to
       determine rearticulation for held notes.)

   the model is implemented in [24]theano, a python library that makes it
   easy to generate fast neural networks by compiling the network to
   gpu-optimized code and by automatically calculating gradients for you.
   the error messages can be a bit confusing (since exceptions tend to get
   thrown while it is running its generated code, not yours), but it's
   well worth the hassle. ## using the model

   during training, we can feed in a randomly-selected batch of short
   music segments. we then take all of the output probabilities, and
   calculate the cross-id178, which is a fancy way of saying we find the
   likelihood of generating the correct output given the output
   probabilities. after some manipulation using logarithms to make the
   probabilities not ridiculously tiny, followed by negating it so that it
   becomes a minimization problem, we plug that in as the cost into the
   adadelta optimizer and let it optimize our weights.

   we can make training faster by taking advantage of the fact that we
   already know exactly which output we will choose at each time step.
   basically, we can first batch all of the notes together and train the
   time-axis layers, and then we can reorder the output to batch all of
   the times together and train all the note-axis layers. this allows us
   to more effectively utilize the gpu, which is good at multiplying huge
   matrices.

   to prevent our model from being overfit (which would mean learning
   specific parts of specific pieces instead of overall patterns and
   features), we can use something called dropout. applying dropout
   essentially means randomly removing half of the hidden nodes from each
   layer during each training step. this prevents the nodes from
   gravitating toward fragile dependencies on each other and instead
   promotes specialization. (we can implement this by multiplying a mask
   with the outputs of each layer. nodes are "removed" by zeroing their
   output in the given time step.)

   during composition, we unfortunately cannot batch everything as
   effectively. at each time step, we have to first run the time-axis
   layers by one tick, and then run an entire recurrent sequence of the
   note-axis layers to determine what input to give to the time-axis
   layers at the next tick. this makes composition slower. in addition, we
   have to add a correction factor to account for the dropout during
   training. practically, this means multiplying the output of each node
   by 0.5. this prevents the network from becoming overexcited due to the
   higher number of active nodes.

   i trained the model using a g2.2xlarge [25]amazon web services
   instance. i was able to save money by using "spot instances", which are
   cheaper, ephemeral instances that can be shut down by amazon and which
   are priced based on supply and demand. prices fluctuated between \$0.10
   and \$0.15 an hour for me, as opposed to \$0.70 for a dedicated
   on-demand instance. my model used two hidden time-axis layers, each
   with 300 nodes, and two note-axis layers, with 100 and 50 nodes,
   respectively. i trained it using a dump of all of the midi files on the
   [26]classical piano midi page, in batches of ten randomly-chosen
   8-measure chunks at a time.

   warning about using spot instances: be prepared for the system to go
   down without warning! (this happened to me twice over a few days of
   experimenting and training.) make sure you are saving often during
   training, so that you do not lose too much data. also, make sure to
   uncheck "delete on termination" so that amazon doesn't wipe your
   instance memory when it shuts it down!

results

   without further ado, here is a selection of some of the output of the
   network:
   sometimes it seems to get stuck playing the same chords for a really
   long time. it hasn't seemed to have learned how long to hold those. but
   overall, the output is pretty interesting. (i was tempted to remove
   some of the repetitive bits, but i decided that would be unfaithful to
   the point of the project, so i left them in.)
   the source code for everything is available on [27]github. it's not
   super polished, but it should be possible to figure out what's going
   on.
   you might also be interested in the discussions on [28]hacker news and
   [29]reddit.
   if you want to replicate results yourself, you may be interested in the
   [30]final training weights of my network.
   [31]creative commons license the generated audio files above are
   licensed under a [32]creative commons attribution 4.0 international
   license.
   02 aug 2015
   tags: [33]composition [34]music [35]neural network [36]project writeup
   [37]python [38]theano
   please enable javascript to view the [39]comments powered by disqus.

      2019. all rights reserved. | [40]top

references

   1. http://www.hexahedria.com/atom.xml
   2. http://www.hexahedria.com/
   3. http://www.hexahedria.com/
   4. http://www.hexahedria.com/about
   5. http://www.hexahedria.com/publications
   6. http://www.hexahedria.com/projects
   7. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
   8. http://www.hexahedria.com/2017/02/23/exciting-news-papers-accepted.html
   9. https://karpathy.github.io/2015/05/21/id56-effectiveness/
  10. http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html
  11. http://arxiv.org/pdf/1410.5401v2.pdf
  12. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  13. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  14. http://www.cs.toronto.edu/~jmartens/docs/deep_hessianfree.pdf
  15. http://www.magicbroom.info/papers/duchihasi10.pdf
  16. http://arxiv.org/pdf/1212.5701v1.pdf
  17. https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/
  18. https://en.wikipedia.org/wiki/abc_notation
  19. http://people.idsia.ch/~juergen/blues/idsia-07-02.pdf
  20. http://www-etud.iro.umontreal.ca/~boulanni/icml2012.pdf
  21. https://en.wikipedia.org/wiki/restricted_boltzmann_machine
  22. http://deeplearning.net/tutorial/id56rbm.html
  23. https://developer.apple.com/library/ios/documentation/performance/conceptual/vimage/convolutionoperations/convolutionoperations.html
  24. http://www.deeplearning.net/software/theano/
  25. http://aws.amazon.com/ec2
  26. http://www.piano-midi.de/
  27. https://github.com/hexahedria/biaxial-id56-music-composition
  28. https://news.ycombinator.com/item?id=10028878
  29. http://www.reddit.com/r/machinelearning/comments/3gaosx/composing_music_with_recurrent_neural_networks/
  30. http://www.hexahedria.com/files/nnet_music_final_learned_config.p.zip
  31. http://creativecommons.org/licenses/by/4.0/
  32. http://creativecommons.org/licenses/by/4.0/
  33. http://www.hexahedria.com/tag/composition/
  34. http://www.hexahedria.com/tag/music/
  35. http://www.hexahedria.com/tag/neural network/
  36. http://www.hexahedria.com/tag/project writeup/
  37. http://www.hexahedria.com/tag/python/
  38. http://www.hexahedria.com/tag/theano/
  39. https://disqus.com/?ref_noscript
  40. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/#top
