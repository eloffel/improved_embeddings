   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

game theory         minimax

   [16]go to the profile of nerdzlab
   [17]nerdzlab (button) blockedunblock (button) followfollowing
   sep 2, 2017
   [1*ckrawdocyhdxq_bzpzxocw.png]

   this article will be a bit different from previous ones which are based
   on some new technologies to use in your projects.

   interesting? i will describe minimax algorithm from the perspective of
   game theory. just letting you know what you are to expect :

   1. so what   s minimax algorithm?
   2. plan and code
   3. algorithm description
   4. optimizations
    4.1. deepness optimization
    4.2. alpha-beta optimization
   5. advice
   6. conclusion

   for coding, we will use language objective-c.don   t worry though, there
   will be more theory than just code.

   the direction is set, let   s go.

   so what   s minimax algorithm? minimax         is a decision rule used in
   decision theory, game theory, statistics and philosophy for minimizing
   the possible loss for a worst case (maximum loss) scenario. originally
   formulated for two-player zero-sum game theory, covering both the cases
   where players take alternate moves and those where they make
   simultaneous moves, it has also been extended to more complex games and
   to general decision-making in the presence of uncertainty.

   coooooooooool!
   now that we have the definition, what logic is embedded in it? let   s
   suggest that you are playing a game against your friend. and then each
   step you take you, want to maximize your win and your friend also wants
   to minimize his loss. eventually, it   s the same definition for both of
   you. your next decision should be maximizing your current win position
   knowing that your friend in the next step will minimize his loss
   position and knowing that the next step you will also maximize your win
   position   

   catching this recursion smell? so the main idea of this algorithm is to
   make the best decision knowing that your opponent will do the same.

   plan and code

   we will build this algorithm using tree representation. each new
   generation of children is possible the next step of another player. for
   example, the first generation is your possible steps, each step will
   lead to some list of the opponent   s possible steps. in this situation,
   your step is the    father vertex    and possible next steps of your
   opponent are it   s    children vertexes   .
   here is the final algorithm code with all optimizations.
#pragma mark - public
- (nsuinteger)startalgorithmwithaiturn:(bool)aiturn; {
    return [self alphabetaalgorithm:_searchingdeapth    alpha:nsintegermin beta:
nsintegermax maximizing:aiturn];
}
#pragma mark - private
- (nsinteger)alphabetaalgorithm:(nsinteger)depth alpha:(nsinteger)alpha beta:(ns
integer)beta maximizing:(bool)maximizing {
    self.currentcheckingdepth = _searchingdeapth - depth;
    if (self.datasource == nil || self.delegate == nil) {
        return 0;
    }
    if (depth == 0 || [self.datasource checkstopconditionforalgorithm:self onait
urn:maximizing]) {
        return [self.datasource evaluateforalgorithm:self  onaiturn:maximizing];
    }
    nsarray *nextstates = [self.datasource  possiblenextstatesforalgorithm:self
onaiturn:maximizing];
    for (id state in nextstates) {
        [self.delegate performactionforalgorithm:self andstate:state onaiturn:ma
ximizing];
        if (maximizing) {
            alpha = max(alpha, [self alphabetaalgorithm:depth - 1 alpha:alpha be
ta:beta maximizing:no]);
        } else {
            beta = min(beta, [self alphabetaalgorithm:depth - 1  alpha:alpha bet
a:beta maximizing:yes]);
        }
        [self.delegate undoactionforalgorithm:self andstate:state onaiturn:maxim
izing];
        if (beta <= alpha) {
            break;
        }
    }
    return (maximizing ? alpha : beta);
}

   algorithm description
   [1*h_aqt2fvka3uacn8ruayug.png]
   example

   let   s close our eyes on optimizations and start with the initial things
   that we need. first of all, we need an algorithm that will give back
   the list of possible next steps based on a made step. we use this to
   produce other children vertexes as described previously.

   secondly, we need an algorithm that will calculate evaluation of the
   game result at the end of the game. each vertex(made step) will have
   assigned value with the evaluated result using this algorithm. how it
   will be assigned will be described later.

   so how does this work using a tree and these algorithms? logically
   algorithm is divided into two parts:
   1. our turn
   2. opponents turn

   on    our turn    as next vertex, we choose one of our children that have
   the best evaluation value. then as next chosen step generates possible
   opponents steps with its evaluation, he will choose worst evaluation
   value(worst for us means best for him).

   this simply means that we will take the maximum evaluated step from
   possible opponents steps and the opponent will take the minimum
   evaluated step from our possible next steps.

   the next question is         when will we evaluate step? the first thing that
   comes to mind         just when it   s created. but hold on a minute, how can
   we calculate it immediately if it depends on the next generation, or in
   other words on another player   s step? it means that our step value will
   be maximum from the next generation step value, which is an opponent
   step. from another point of view, opponents step value will be minimum
   from the next generation step values, which is mine.

   following this rule, we can say that evaluation will be made when the
   game ends, as it   s final generation of steps. after that, it will
   unwind in back direction marking all vertexes with previously described
   calculated value until it gets to the root.

   recursive generation ends if:
   we get a winner
   it   s not possible to have the next move, we get a draw

   evaluation of game state can be calculated in a next way:
   if we win, then evaluated value should be bigger than some positive
   value
   if we lose, then it should be lower than some negative value
   if we have a draw then it should be in between this values
   it means that the better position we have, the bigger value it should
   be.

   this evaluation may depend on different things. let   s take for example
   that for evaluation, we give it a situation where we are winners.
   then it may evaluate depending on next values:
   1. how much steps i took to get the win
   2. how close was your opponent to the win
   etc   

   this function is one of the most important parts of the algorithm. the
   better organized it is, the better results you get. a function that
   depends only on one characteristic will not be as accurate as a
   function that depends on ten. also, characteristics you choose for
   evaluation should have logical meaning. if you will treat that
   evaluation value is bigger because the player is a woman other than a
   man, it makes some sense for the sexists but not for the algorithm. as
   a result, you will receive wrong results. and everything is because
   you   re a sexist.

   great!, now we generally understand the algorithm and what idea lays
   under it. sound cool, but still, it   s not as good when it comes to
   execution time. why?

   optimizations

   for usage, for example, let   s select    four in a row    game. if you don   t
   know what it is, here is a short description:
   [1*vvf1fhp_hy4-wxcck4tywa.jpeg]

      it is the goal of the game to connect four of your tokens in a line.
   all directions (vertical, horizontal, diagonal) are allowed. players
   take turns putting one of their tokens into one of the seven slots. a
   token falls down as far as possible within a slot. the player with the
   red tokens begins. the game ends immediately when the first player
   connects four stones.   

   let   s think how our algorithm will work in this case. the field has
   7*6=42 steps. at the beginning, each user can make 7 different steps.
   what does this data give to us? these values are worst ever. to make
   them more real, let   s say, for instance, that game will end in 30 steps
   and averagely user will have a possibility to select 5 rows. using our
   algorithm we will get next approximate data. the first user can produce
   5 steps; each step will produce 5 own steps. now we   re having 5 + 25.
   these 25 steps, each can produce 5 new steps and that   s equal to 125.
   which means on the third step of calculation we will have about155
   steps. each next step will produce more and more possible steps to
   follow. you can count them using the next function:
   5   + 5   + 5   +     + 5     . this value is huge and a computer will take a
   lot of time to calculate so many steps.

   thankfully there are ready optimizations for minimax algorithm, that
   will lower this value. we will talk about the two of them shortly.

   deepness optimization

   once upon a time, a very wise man said    why should we count it till the
   end? can we make a decision somewhere in the middle?   . yes, we can. of
   course, in the end, accuracy will be much better than somewhere in the
   middle, but with a good algorithm for evaluation, we can lower this
   problem. so then what should we do, just add one more rule for
   "algorithm stops". this rule is, to stop if current step deepness is on
   critical value. this value you can select on your own after some
   testing. i think with a bunch of tests you will find those that are
   matching your special situation. when we start evaluation based on this
   rule, we   re treating it as a draw.

   this rule was not as hard, and can really optimize our algorithms.
   however still, it   s lowering our accuracy, and to make it work, we just
   can   t use super low deepness otherwise it will give us wrong, and
   non-accurate values. i will tell you next, we selected that deepness 8
   is fine for us. even though it was taking a lot of time to get its
   final result. next optimization has no influence on accuracy and
   reduces algorithm time really good. its name is alpha beta and it's
   this thing that breaks my mind out.

   alpha-beta optimization
   [1*pvwej78fiw38ym6r-pajlw.png]
   example

   it   s meaning is simple. the idea is next         no need to count next
   branches if we know that already founded branch will have a better
   result. it   s just a waste of resources. the idea is simple, what about
   implementation?

   to count that, we need two other variables to be passed to the
   recursive algorithm.
   1. alpha         it   s representing evaluation value on maximizing part
   2. beta         it   s representing evaluation value on minimizing part

   from previous parts, we understood that we have to go to the end of our
   tree and only then after can we evaluate our results. so the first
   evaluation will be made on the last vertex if we would always choose
   left vertex. after that evaluation process will start for its immediate
   vertex and will go on for all left vertexes till one of the possible
   ones end and so on and so forth.

   why am i telling you this? it   s all because of understanding how
   evaluating vertexes is crucial for this optimization.

   to make it easier for us to imagine, let   s suppose that we   re on the
   maximizing turn. to us, it means that we   re counting alpha value. this
   value is counted as a maximum of all evaluation values of his
   offsprings. at the same time when we   re counting alpha(we   re on the
   maximizing process) we have a beta value. here, it represents a current
   beta value of his parent. great!, now we know what alpha and beta means
   in a specific time. to avoid misunderstanding, alpha is our current
   evaluation value and beta is current further evaluation value.

   the last thing is optimization check. if beta is lower or equal the
   alpha we   re stopping evaluation for all next child.

   the last thing is optimization check. if beta is lower or equals to the
   alpha we   re stopping evaluation for all next offsprings.

   why does it work? because beta already is smaller than alpha         value of
   its offspring, at the same time alpha is counted as a maximum of
   possible offspring values. it means that now evaluation value for our
   step can   t become lower than the current value. from another point of
   view, the beta that we have now is a current evaluation value of our
   parent. from the idea that currently, we   re maximizing, we may
   understand that our parent is minimizing which means that it will
   select a minimum of its current value and its offsprings calculated
   value.

   putting all of these together         because we know that our calculated
   value is higher than the parent value, we make a decision that it will
   not select us as its next possible step(it has a better offspring and
   that gives him lower value if it selects them). from that point of view
   doing the next calculation is ridiculous, because they will not change
   the final result. so we, as the parent-offspring, we stop all
   calculations and return the current value. using this, our parent will
   compare us and the current best choice. by previously described logic
   it will select its current value.

   using this optimization, we may cut huge branches and save a lot of
   work that would do nothing to the final result.

   advice

   great!, now that we know two optimizations that will make our algorithm
   work faster, i would like to give you one advice. usually, when a
   machine makes a choice, the real user need some time to decide what to
   do next. we can use this time to prepare for next the decision.

   so when we make our choice, let   s continue to work for all possible
   user choices. what does this give to us?
   1. bunch of them are already calculated from previous user iteration.
   2. by the time user will select his choice, we will go far ahead with
   our calculation. it will increase a deepness of our tree, in other
   words         accuracy.

   in the end, when the user makes a choice, we can simply cut off all
   other branches that were not selected and only continue with selected
   one.
   it   s great, but then let   s make a global conclusion on what i
   suppose         do not start your algorithm each time it   s your turn. make it
   run once during the game and when a user selects some step, get
   calculated data for it and continue our calculation until the end.

   someone will say, that it   s has a heavy memory issue. yes, it does, but
   this   s just an advice, you can combine two solutions to find what best
   fits your time, memory, and accuracy for your particular situation.

   also, i would recommend performing calculations for each possible step
   in a new thread, but it   s quite a different story.

   conclusion

   where should you use this information? anywhere you have to make or do
   decision-based on user action. most often it might be games, but then,
   you can use your fantasy to integrate it in a place where you would
   have to reflect on any performed action. the only thing you need to
   understand is the algorithm for two parts that are    playing    against
   each other.

   the end!

     * [18]machine learning
     * [19]minimax
     * [20]algorithms
     * [21]game theory
     * [22]artificial intelligence

   (button)
   (button)
   (button) 41 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of nerdzlab

[24]nerdzlab

   for ideas never seen before

     (button) follow
   [25]towards data science

[26]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 41
     * (button)
     *
     *

   [27]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [28]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f84ee6e4ae6e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/game-theory-minimax-f84ee6e4ae6e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/game-theory-minimax-f84ee6e4ae6e&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_gllzwydr6npd---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@supernerd?source=post_header_lockup
  17. https://towardsdatascience.com/@supernerd
  18. https://towardsdatascience.com/tagged/machine-learning?source=post
  19. https://towardsdatascience.com/tagged/minimax?source=post
  20. https://towardsdatascience.com/tagged/algorithms?source=post
  21. https://towardsdatascience.com/tagged/game-theory?source=post
  22. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  23. https://towardsdatascience.com/@supernerd?source=footer_card
  24. https://towardsdatascience.com/@supernerd
  25. https://towardsdatascience.com/?source=footer_card
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/
  28. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  30. https://medium.com/p/f84ee6e4ae6e/share/twitter
  31. https://medium.com/p/f84ee6e4ae6e/share/facebook
  32. https://medium.com/p/f84ee6e4ae6e/share/twitter
  33. https://medium.com/p/f84ee6e4ae6e/share/facebook
