word vector space specialisation

nikola mrk  i  , mohammad taher pilehvar, and ivan vuli  

university of cambridge

tutorial; eacl 2017; valencia; april 3, 2017

{nm480, mp792, iv250}@cam.ac.uk

1 / 207

before we start...

taher

nikola

ivan

tutorial slides are available here:

http://people.ds.cam.ac.uk/iv250/tutorial/wv-tutorial.pdf

#eacl2017

2 / 207

vector specialisation: motivation

drawbacks of popular word vector collections
unsupervised methods which induce vector representations from
large textual corpora coalesce several types of information.

user: i   m looking for a cheaper restaurant
inform(price=cheap)
system: what kind of food?
user: english, in eastern cambridge
inform(price=cheap, food=british, area=east)
system: the green man is the best choice
user: where is it?
inform(price=cheap, food=british, area=east);
request(address)
system: the green man is at 59 high st, grantchester

3 / 207

word vectors solve this, right?

distributional hypothesis
learning id27s from co-occurrence information in
corpora coalesces the notions of semantic similarity and relatedness.

word

east
west
north
south

expensive

pricey
cheaper
costly

southeast
northeast

overpriced
inexpensive

english
spanish
german
french

portuguese

italian

table: nearest neighbours in glove vectors; non-synonyms are bold

4 / 207

vector specialisation: motivation

drawbacks of popular word vector collections
unsupervised methods which induce vector representations from
large textual corpora coalesce several types of information.

user: i   m looking for a cheaper restaurant
inform(price=expensive)
system: what kind of food?
user: english, in eastern cambridge
inform(price=expensive, food=spanish, area=east)
system: the green man is the best choice
user: where is it?
inform(price=expensive, food=spanish, area=east);
request(address)
system: the green man is at 59 high st, grantchester

5 / 207

moving past distributional models

making use of semantic lexicons
unsupervised methods making use of distributional information are
both theoretically interesting and require no manual annotation.
however, if our goal is optimising downstream performance, why not
make use of all the lexical resources already available?

6 / 207

babelnet 3.7: general statistics

7 / 207

tutorial overview

part i: learning from context(s)

(35 mins)

de   ning context; learning from data (only)

context types and output vector spaces

specialising representations for di   erent word classes

8 / 207

tutorial overview

part ii: lexical/linguistic resources

(35 mins)

why linguistic resources?

manually constructed linguistic resources

automatically constructed and hybrid linguistic resources

how to link linguistic resources to representation learning?

synonyms, antonyms, other lexical relations

9 / 207

tutorial overview

part iii: vector space specialisation

(55 mins)

joint approaches vs. post-processing methods

checkpoint: co   ee break @11am

an overview of joint approaches

constraint-driven specialisation

post-processors: retro   tting, counter-   tting, paragram

a novel (generic) model: attract-repel

10 / 207

tutorial overview

part iv: evaluation and application

(55 mins)

intrinsic evaluation: similarity vs relatedness vs other relations

specialised vectors and downstream nlp applications

case study: dialogue state tracking

multi-prototype word representations

conclusion, discussion, and future work

11 / 207

m

12 / 207

part i: learning from

context(s)

motivation

the nlp community has developed useful features for several tasks but    nding
features that are...

1. task-invariant (id52, srl, ner, parsing, ...)

(monolingual id27s)

13 / 207

motivation

the nlp community has developed useful features for several tasks but    nding
features that are...

1. task-invariant (id52, srl, ner, parsing, ...)

(monolingual id27s)

2. language-invariant (english, dutch, chinese, spanish, ...)

(nlp should not be    english language    processing)

...is non-trivial and time-consuming (20+ years of feature engineering...)

14 / 207

motivation

the nlp community has developed useful features for several tasks but    nding
features that are...

1. task-invariant (id52, srl, ner, parsing, ...)

(monolingual id27s)

2. language-invariant (english, dutch, chinese, spanish, ...)

(nlp should not be    english language    processing)

...is non-trivial and time-consuming (20+ years of feature engineering...)

learn word-level features which generalise across tasks and
languages...

15 / 207

motivation

the nlp community has developed useful features for several tasks but    nding
features that are...

1. task-invariant (id52, srl, ner, parsing, ...)

(monolingual id27s)

2. language-invariant (english, dutch, chinese, spanish, ...)

(nlp should not be    english language    processing)

...is non-trivial and time-consuming (20+ years of feature engineering...)

learn word-level features which generalise across tasks and
languages...

...and embed external knowledge into these features

16 / 207

word vectors

representation of each word w     v :

      w = [f1, f2, . . . , fdim]

word representations in the same shared semantic (or embedding) space!

image courtesy of [gouws et al., icml 2015]

17 / 207

word vectors are based on...

...distributional hypothesis
words with similar meanings are likely to appear in similar contexts.

[harris, word 1954]

shouts:

   meaning as use!   

calmly states:

   you shall know a word by the company it keeps.   

18 / 207

learning from context

skip-gram with negative sampling (sgns)
[mikolov et al.; nips 2013]

learning from the set d of (word, context) pairs observed in a corpus:
(w, v) = (wt, wt  c); i = 1, ..., c; c = context window size

sg learns to predict the context of each pivot word.

john saw a cute gray huhblub running in the    eld.

d = (huhblub, cute), (huhblub, gray), (huhblub, running), (huhblub, in)
vec(huhblub) = [   0.23, 0.44,   0.76, 0.33, 0.19, . . .]

19 / 207

methodology

representation model     skip-gram with negative sampling (sgns)

sgns may be trained with arbitrary contexts
[levy and goldberg, acl 2014]

context is crucial
di   erent context types result in di   erent sgns vectors.

[schwartz et al, naacl 2016; melamud et al, naacl 2016]

some standard context types:
1. (ordinary) bag-of-words (bow)
2. positional (posit)
3. dependency-based: basic (deps-naive)
4. dependency-based: with prepositional arc collapsing (deps-arc)

20 / 207

sgns - a more formal summary

id203 for one word-context pair (w, v):

general objective:

p (d = 1|w, v,   ) =

1

1 + exp(    (cid:126)w    (cid:126)vc)

(cid:88)

(w,v)   d

j = arg max

  

log

1

1 + exp(    (cid:126)w    (cid:126)vc)

general objective with negative sampling:

(cid:88)

j = arg max

  

(w,v)   d

log

1

1 + exp(    (cid:126)w    (cid:126)vc)

+

(cid:88)

(w,v(cid:48))   d(cid:48)

log

1

1 + exp( (cid:126)w    (cid:126)v(cid:48)
c)

[goldberg and levy: id97 explained; arxiv 2014]

21 / 207

context types: 1. bow

1. (ordinary) bag-of-words

(word, context) = {(scienzato, australiano), (scienzato, scopre), (australiano,
scienzato), (australiano, scopre), (australiano, stelle), ...}

22 / 207

scienziatoaustralianoscoprestellecontelescopioamodnsubjdobjcasenmodcontext types: 2. posit

2. (positional) bag-of-words

{(scienzato, australiano_+1), (scienzato, scopre_+2), (australiano,
scienzato_-1), (australiano, scopre_+1), (australiano, stelle_+2), ...}

23 / 207

scienziatoaustralianoscoprestellecontelescopioamodnsubjdobjcasenmodcontext types: 2. posit

positional contexts
1. more expressive; richer in annotation ++
2. should be better than bow in syntactic tasks +-
3. sparse    

example taken from [ling et al., naacl 2015]

24 / 207

context types: 3. deps-naive

3. (universal) dependency-based: naive

{(discovers, scientist_nsubj), (discovers, stars_dobj), (discovers,
telescope_nmod), (stars, discovers_dobj-1), (telescope, with_case), (with,
telescope_case-1), (scientist, australian_amod) ...}

    extracting from the parse directly

25 / 207

australianscientistdiscoversstarswithtelescopeamodnsubjdobjcasenmodcontext types: 4. deps-arc

4. (universal) dependency-based: with prepositional arc collapsing

{(discovers, scientist_nsubj), (discovers, stars_dobj), (discovers,
telescope_nmod), (stars, discovers_dobj-1), (scientist, australian_amod),
(discovers, telescope_case_with), (telescope, discovers_case_with-1))}, ...
    simple but important post-processing: prepositional arc
collapsing

26 / 207

australianscientistdiscoversstarswithtelescopeamodnsubjdobjcasenmodprep:withdependency contexts: other languages

    prepositional arc collapsing
    universal dependencies: syntactic contexts in multiple languages

27 / 207

australianscientistdiscoversstarswithtelescopeamodnsubjdobjcasenmodscienziatoaustralianoscoprestellecontelescopioamodnsubjdobjcasenmodaustralianscientistdiscoversstarswithtelescopeamodnsubjdobjcasenmodprep:withcross-lingual dependency contexts

injecting syntactic information into cross-lingual embedding spaces:
    online training with monolingual and cross-lingual dependency-based
contexts:

[vuli  , eacl 2017]

28 / 207

training with context subsets

not all contexts are created equal:
    using only coordination structures from deps-* may boost verb and
adjective similarity:

[schwartz et al., naacl 2016]

context
verb
0.307
sgns-bow
0.386
sgns-deps (arc)
sgns-coord
0.413
sgns-deps   coord 0.372
0.459
sgns-sympat

adj
0.604
0.586
0.629
0.560
0.651

noun
0.501
0.499
0.428
0.494
0.415

time #contexts
320
551
23
677
11

13g
14.5g
550m
14g
270m

table: results on siid113x-999 v/a/n subsets with sgns and di   erent
context types.

29 / 207

finding context con   gurations

context group
conjlr (a+n+v)
obj (n+v)
prep (n+v)
amod (a+n)
compound (n)
adv (v)
nummod (-)

adj

0.415
-0.028
0.188
0.479
-0.124
0.197
-0.142

verb

0.281
0.309
0.344
0.058
-0.019
0.342
-0.065

noun

0.401
0.390
0.387
0.398
0.416
0.104
0.029

table: results of di   erent context bags on the noun, verb and adjective
subsets of siid113x-999.

adjectives
amod,
conjlr,
conjll

verbs
prep, acl, obj,
comp, adv, conjlr,
conjll

nouns
amod, prep, compound, subj, obj,
appos, acl, nmod, conjlr, conjll

30 / 207

table: context bags for each word class.

finding context con   gurations

31 / 207

ri+rj+rk+rlri+rj+rkri+rj+rlri+rk+rlrj+rk+rlri+rjri+rkrj+rkri+rlrj+rlrk+rlrirjrkrle(rpool  ri)>e(rpool)e(rpool)e(rpool  rl)<e(rpool)e(rpool  ri  rj)<e(rpool  ri)finding context con   gurations

32 / 207

other context types: attention-based cbow

[ling et al., emnlp 2015]
another extension of id97 models: assign weights/attention to each
context word used to predict the pivot word

context(wt) =(cid:80)

                           

33 / 207

i   [t   c,t+c]   {t} ai(wi)          wi

other context types: substitute vectors

[yatbaz et al., emnlp 2012, melamud et al., naacl 2015]
context     potential    ller words for the target word slot according
to how       t    they are to    ll the target slot

   it requires a language model trained on large data before the
representation training.

34 / 207

comparing di   erent context types i

[levy et al., acl 2014]

sgns-bow (c = 2)

sgns-bow (c = 5)

sgns-deps

dancing
singing
dance
dances

breakdancing

clowning

dancing
singing
dance
dances
dancers

tap-dancing

dancing
singing
rapping

miming
busking

breakdancing

table: nearest neighbours using sgns with di   erent context types.

35 / 207

comparing di   erent context types ii

[melamud et al., naacl 2016]

36 / 207

comparing di   erent context types iii

[melamud et al., naacl 2016]

37 / 207

other context types: context2vec

[melamud et al., conll 2016]
context     sentential context modelled by a bidirectional lstm

38 / 207

other context types: context2vec

[melamud et al., conll 2016]
context     targeting functional similarity by modelling
long-distance dependencies implicitly

    context2vec shows very good results on the intrinsic task of verb similarity:
0.388 on simverb-3500 [gerz et al., emnlp 2016] (...to be continued...)

39 / 207

other context types: symmetric patterns

[schwartz et al., conll 2015]

[davidov and rappaport, acl 2006]
a di   erent vector space     a co-occurrence matrix is constructed
according to co-occurrences in symmetric patterns, weighted by ppmi

    dimensionality can be reduced by svd

40 / 207

other context types: symmetric patterns

[schwartz et al., conll 2015]

two antonymy patterns:

- either x or y
- from x to y

excellent results on verb and adjective similarity on siid113x-999 subsets;
problems with data sparsity

one open question
does it extend to other languages?

another open question
how to combine it with other contexts?

41 / 207

useful software i
unsupervised reps

1. id97f: sgns with arbitrary contexts

[levy and goldberg, acl 2014]
https://bitbucket.org/yoavgo/id97f

2. hypervectors: tuning a wide variety of hyperparameters, often taken for
granted in id97 training

[levy et al., tacl 2015]
https://bitbucket.org/omerlevy/hyperwords

3. fastsubs: generating word substitutes for substitute vectors

[yatbaz et al., emnlp 2012]
https://github.com/ai-ku/fastsubs

4. context2vec: training vectors with lstm contexts

[melamud et al., conll 2016]
https://github.com/orenmel/context2vec

42 / 207

useful software ii
unsupervised reps

5. sympat: extracting symmetric patterns (in any language)

[schwartz et al., conll 2015]
http://homes.cs.washington.edu/   roysch/papers/sp_embeddings/sp_embeddings.html

6. fasttext: using subword information in training

[bojanowski et al., arxiv 2016]
https://github.com/facebookresearch/fasttext
(    it comes with pre-trained word vectors in 90+ languages)

7. charagram: this model also exploits character-level information: char id165s

[wieting et al., emnlp 2016]
https://github.com/jwieting/charagram

43 / 207

m

44 / 207

part ii: linguistic

constraints

lexical resourcesthe	paraphrase	databaseid138a database (or a machine readable dictionary) that providesstructured knowledge for words, e.g., synonyms of	words,	semantic	and	phonological	relations	between	different	words.id138:the de facto standard lexical databaseliving	plantplant,	flora,	plant_lifethe	middle	of	the	daynoon,	twelve	noon,	high	noon,	midday,	noonday,	noontidesynsetword sensethe basic constituents in id138 are synsets(sets of synonymous words that correspond to a unique concept)id138: semantic relationsid138as a hypernymyhierarchytruckis a car, caris a motor vehicle,    id138 as a sense inventoryonline browserid138: limitations   difficult to update (needs expert curation)omost recent major update (v3.0) was 10 years ago.   limited vocabularyomisses many named entities and domain specific terms.lexical resources: collaborative resourcesresource diaspora: wikipediaeach wikipedia article corresponds to a concept or named entity  a lot of (structured and unstructured) information  inter-article links  multilingualitylexical resources: collaborative resourceswikipedia, structured knowledgelexical resources: collaborative resourcesresource diaspora: wiktionarya free contentdictionaryofall words in all languages.lexical resources: collaborative resources0100200300400500600700800wiktionaryid138oxford	dic	(oed)vocabulary size (english)wiktionary isavailable in 172 languages!lexical resources: collaborative resourcesbut all these wiktionariesare independent!lots of redundancy!wiktionary isavailable in 172 languages!lexical resources: collaborative resourcesresource diaspora: omegawikibased on concepts, addresses the redundancy issue! babelnet: multilingual encyclopedic dictionaryid138babelnet: a merger of many resources   id138: the most popular computational lexicon of english   open id73net: a collection of open id138s   wonef: a french id138   wikipedia: the largest collaborative encyclopedia   wikidata: the largest collaborative knowledge base   wiktionary: the largest collaborative dictionary   omegawiki: a medium-size collaborative multilingual dictionary   geonames: a worldwide geographical database   framenetlexical units   verbnetentries   microsoft terminology: a computer science thesaurus   high-quality automatic sense-based translationsbabelnet: a merger of many resources   babelnetcollect lexicalizations, definitions, translations, images, etc. from each of the merged resources.   plant, flora, plant_life: a living organism lacking the power of locomotion id138babelnet: a merger of many resources   plant, flora, plant_life: a living organism lacking the power of locomotion babelnet: anatomyquando il linguaggio incontra l   informaticaroberto navigli17babelnet: anatomyquando il linguaggio incontra l   informaticaroberto navigli18lemmalemma languagedefinitionpicturefurtherlanguagessynonymssynonyms in other languagesbabelnet: anatomybabelnet: anatomypronunciationdefinition speechusage examplesdefinitionsgeneralizations and other relationsbabelnet: coverage of named entitiesjaguarconceptnamed entitiesbabelnet: multilingualitythe same concept is expressed in tens of languagesbabelnet: multilingualitythe	same	concept	is	expressed	in	tens	of	languages.babelnet: multilingualitysearch and translate:babelnet: multilingualitybabelnet: features   multilinguality:the same concept is expressed in tens of languages   coverage:271 languages and 14 million entries!   concepts and named entities together:dictionary and encyclopedic knowledge is semantically interconnected   "dictionary of the future":semantic network structure with labeled relations, pictures, multilingual synsets   full-fledged taxonomy:is-a relations are available for both concepts and named entities (wikipedia bitaxonomy)   easy access:java and http restfulapis; sparql endpoint (2 billion triples)babelnet: semantic networkbabelnet: semantic networkexplore the networkbabelnet: semantic networkbabelnetlivecontinuously growing, being fed with everyday updates from all the sources it is composed of, including wikipedia, wiktionary, user input, etc.ppdb: the paraphrase databasean automatically extracted database containing millions of paraphrasesin 16 different languages.extracted from bilingual parallel corpora through bilingual pivoting(bannardand callison-burch, 2005)illustration from ganitkevitchet al (2013)ppdb: the paraphrase databasethree types of paraphrases:   lexical -single word to single word   reinforced ||| strengthened   bibliography ||| references   phrasal -multiword to single/multiword   power plants ||| power stations   free trade area ||| free trade zone   syntactic -paraphrase rules containing non-terminal symbols   dt characteristic of np ||| dt feature of npppdb: the paraphrase databasetime alterations09:00 | 9 a.m. 09:00 | 9 hours 09:00 | nine hours 09:00 | nine o 'clockverbparticlesspeed up | accelerateblow up | explode throw up | puke set up | establish speed up | expedite give up | abandonexamples from @ppdbabbreviationssme| small and medium enterprises unicef| united nations children 's fundroi| return on investmentcomparativessafer | more secure denser | more dense wetter | more humid fairer | more justppdb: the paraphrase databaseeach paraphrase in ppdb is provided with a scoreaim ||| objective ||| 3.78071 aim||| targets||| 3.70702aim||| purpose||| 3.63588aim||| attempts||| 3.52036ppdb: the paraphrase databaseseveral features are used for computing the similarity scores of paraphrasesppdb: release 2.0   improved re-ranking of paraphrases   added fine-grained entailment relations, id27 similarities, and style annotations.paraphrases	of	the	end,	ranked	from	most	complex	to	most	simple	according	to	the	stylescores	included	in	ppdb	2.0.multilingual ppdbextended to 23 different languagesfigure from ganitkevitchand chris callison-burch (2014)millions of paraphrase pairsppdb: release 2.0m

83 / 207

part iii: vector space

specialisation

moving past distributional models

distributional hypothesis
learning id27s from co-occurrence information in
corpora coalesces the notions of semantic similarity and conceptual
association.

word

east
west
north
south

expensive

pricey
cheaper
costly

southeast
northeast

overpriced
inexpensive

british

american
australian

britain

european (tbc)

england

table: nearest neighbours using glove vectors

84 / 207

moving past distributional models

    are the following concepts similar or related?

?

?

?

and

and

and

and

85 / 207

or

and

?

moving past distributional models

chosen context types may in   uence the output vector space, but...

86 / 207

moving past distributional models

chosen context types may in   uence the output vector space, but...

semantic lexicons and external knowledge
while distributional statistics are semantically informative, why
discard valuable semantic information from semantic lexicons and
knowledge bases?

87 / 207

moving past distributional models

chosen context types may in   uence the output vector space, but...

semantic lexicons and external knowledge
while distributional statistics are semantically informative, why
discard valuable semantic information from semantic lexicons and
knowledge bases?

distributional vs. non-distributional
the two sources may contain complementary information: why not
combining them?

88 / 207

semantic specialisation

semantically-specialising word vector spaces
injecting semantic constraints (such as synonymy and antonymy)
into pre-trained word vectors can mitigate some of the negative
aspects of the distributional hypothesis.

word

before

after

east
west
north
south

southeast
northeast
eastward
eastern
easterly

-
-

expensive

pricey
cheaper
costly

overpriced
inexpensive

costly
pricy

overpriced

pricey
a   ord

british

american
australian

britain

european
england

brits
london
bbc
uk

britain

table: nearest neighbours before and after semantic specialisation

89 / 207

how to (properly!) evaluate

intrinsic evaluation resources
most intrinsic evaluation datasets rank word pairs by similarity,
relatedness, lexical entailment, or other relevant properties.

similarity and relatedness:

rg-65 (rubinstein and goodenough, 1965)
ws-353 (finkelstein et al., 2002)
ws-353 sym/rel split (agirre et al., 2009)
rare words (luong et al., 2013)
men (bruni et al., 2014)

conceptual association:

usf-wa (vuli   et al., 2017)

90 / 207

how to (properly!) evaluate

lexical entailment:

hyperlex (vuli   et al., 2016)

word analogy:

microsoft and google analogy (mikolov et al., 2013)

semantic similarity (decoupled from relatedness/association):

toefl synonym questions (landauer and dumais, 1997)
gre antonymy (mohammad et al., 2008)
siid113x-999 (hill et al., 2015)
simverb (gerz et al., 2016)
multilingual siid113x-999 (leviant and reichart, 2015)
semeval 2017 task 2: multilingual and cross-lingual semantic word
similarity (camacho-collados et al., 2017)

91 / 207

ws-353(-syn) measures association!

decoupling synonymy from relatedness in evaluation
as noted by the dataset authors and shown empirically in (hill et al.,
2015), the ws-353 annotator instructions result in pairs being rated
according to association rather than similarity.

there are three major issues with ws-353 (and its synonymy versus
relatedness split):

1 many dissimilar word pairs receive high ratings, e.g.:

[(japanese-american, 6.5), (mexico-brazil, 7.4)].

2 associated, but dissimilar concepts do not receive low ratings.
3 low annotator agreement: state-of-the-art models (e.g.

(collobert and weston, 2008)) signi   cantly outperform the
ceiling ws-353 performance.

92 / 207

semantic specialisation: typology

one way to classify semantic specialisation methods is according to
how they combine distributional information from large textual
corpora with linguistic constraints extracted from semantic lexicons.

1. heavy-weight approaches
induce representations jointly, by learning from contextual
information while taking linguistic constraints into account.

2. post-processing approaches
inject semantic constraints into existing distributional vector spaces,
treating them as black boxes.

93 / 207

   heavy-weight    specialisation approaches

modifying the distributional objective: context-based learning +
kb-based learning coded in the same objective

example 0 ([yih et al., emnlp 2012]):
polarity inducing latent semantic analysis

    combining information from a thesaurus with a regular lsa training
    the idea of injecting explicit knowledge into vector spaces existed in
pre-embedding times

94 / 207

   heavy-weight    specialisation approaches

modifying the distributional objective: context-based learning +
kb-based learning coded in the same objective

example 1 ([yu and dredze, acl 2014]): relation constrained model

    combining 1) distributional cbow objective with 2) a set of linguistic
constraints r describing a lexical relation (e.g., similarity)

t(cid:88)
(cid:124)

t=1

j =

1
t

log p (wt|wt+c
(cid:125)
t   c)

(cid:123)(cid:122)

distributional: cbow

+

c
n

(cid:124)

n(cid:88)

i=1

(cid:88)
log p (w|wi)
(cid:123)(cid:122)
(cid:125)

w   rwi

knowledge resource

t = corpus size (tokens); c = interpolation weight; n = vocabulary size; c = window
size; rwi = set of constraints (w, v) containing the word wi as one word in a pair

95 / 207

   heavy-weight    specialisation approaches

modifying the distributional objective: context-based learning +
kb-based learning coded in the same objective

example 2 ([kiela et al., emnlp 2015])

    combining 1) distributional sgns objective with 2) a set of linguistic
constraints r describing a lexical relation (e.g., similarity or relatedness)
    three modelling variants proposed, similar to rcm:
1. use all linguistic constraints as additional contexts

t(cid:88)

t=1

(cid:16)
(cid:124)

(cid:88)

log p (wt+c

+

(cid:123)(cid:122)

(cid:125)
t   c|wt)

distributional: sgns

(cid:124)

w   rwt

(cid:123)(cid:122)

log p (w|wt)

knowledge resource

(cid:17)

(cid:125)

1
t

96 / 207

   heavy-weight    specialisation approaches

example 2 ([kiela et al., emnlp 2015])

2. sample one additional context for each token in corpus

t(cid:88)

t=1

(cid:16)
(cid:124)

1
t

log p (wt+c

(cid:123)(cid:122)

(cid:125)
t   c|wt)

distributional: sgns

log p (w|wt)

(cid:17)

(cid:125)

(cid:88)
(cid:123)(cid:122)

w   rwt

]

+ [w     urwt
(cid:124)
(cid:88)

log p (w|wt)

knowledge resource

t(cid:88)

1
t

t=1

w   rwt

3. sgns retro   tting: distributional first

    similarity vs. relatedness: driven by di   erent sets of constraints r

97 / 207

   heavy-weight    specialisation approaches

example 3 ([liu et al., acl 2015])

linguistic knowledge is transformed into ordinal constraints:

similarity(wi, wj) > similarity(wi, wk)

ordinal constraints are constructed using a selection of intuitive rules:

- synonymy/antonymy rule: (foolish, stupid) > (foolish, smart)
- semantic category rule: similarities of words that belong to the same
category (direct co-hyponyms) are larger than similarities of words
belonging to di   erent categories: (mallet, plessor) > (mallet, hacksaw)
- semantic hierarchy rule: similarities between words that have shorter
distances in a semantic hierarchy should be larger than similarities of words
that have longer distances: (mallet, hammer) > (mallet, tool)

98 / 207

   heavy-weight    specialisation approaches

example 3 ([liu et al., acl 2015])

linguistic knowledge is transformed into ordinal constraints:

constrained optimisation problem using ordinal constraints

99 / 207

   heavy-weight    specialisation approaches

example 4 ([osborne et al., tacl 2016]): incorporating prior knowledge
into canonical correlation analysis
    two data views: pivot word view and context view

100 / 207

similarity, relatedness and association

semantic similarity is not the only interesting semantic relation

t(cid:88)
(cid:124)

t=1

j =

1
t

log p (wt|wt+c
(cid:125)
t   c)

(cid:123)(cid:122)

distributional: cbow

+

c
n

(cid:124)

n(cid:88)

i=1

(cid:88)
log p (w|wi)
(cid:125)
(cid:123)(cid:122)

w   rwi

knowledge resource

the set rwi may also contain pairwise constraints targeting relatedness
    moving away from true similarity.
[kiela et al., acl 2015]: using (cue, target) pairs from the usf word
association data to generate pairwise    relatedness constraints   

101 / 207

similarity, relatedness and association

semantic similarity is not the only interesting semantic relation

t(cid:88)
(cid:124)

t=1

j =

1
t

log p (wt|wt+c
(cid:125)
t   c)

(cid:123)(cid:122)

distributional: cbow

+

c
n

(cid:124)

n(cid:88)

i=1

(cid:88)
log p (w|wi)
(cid:125)
(cid:123)(cid:122)

w   rwi

knowledge resource

the set rwi may also contain pairwise constraints targeting relatedness
    moving away from true similarity.
[kiela et al., acl 2015]: using (cue, target) pairs from the usf word
association data to generate pairwise    relatedness constraints   
one open question
although strongly correlated, are relatedness and association really
equivalent?

102 / 207

similarity, relatedness and association

[kiela et al., acl 2015]

not distinguishing between similarity and relatedness may be bene   cial for
certain applications such as text classi   cation or id96.

103 / 207

similarity, relatedness and association: cut ii

similarity and association are not correlated

similarity and relatedness are correlated more strongly than expected

relatedness and association are correlated, but they are not equivalent

104 / 207

similarity, relatedness and association

the concept of semantic similarity is more speci   c than semantic
relatedness, as the latter includes concepts as antonymy and meronymy,
while similarity does not.

[budanitsky and hirst, naacl 2001]

specialising for relatedness/association and linking such vector spaces to
downstream nlp tasks are still underexplored problems.

larger word association tables for english have been collected recently
[de deyne et al., coling 2016]

word association tables exist for other languages:
    german, dutch, italian
    japanese, cantonese, vietnamese

105 / 207

specialising for other relations

[ono et al., naacl 2015] designed a very similar    heavy-weight   
specialisation approach targeting antonymy detection

combining distributional information with information from a lexical
resource (e.g., id138)

antonymy specialisation? what is it good for besides evaluating on
antonymy detection?

106 / 207

post-processing methods

heavy-weight methods are demanding
   heavy-weight    methods model distributional information and
external semantic knowledge jointly.

while theoretically interesting, these models are less competitive
than post-processing ones. the reasons for this are two-fold:

1 balancing the contribution between the two sources of

supervision is non-trivial.

2 extortionate computational complexity.

107 / 207

retro   tting (faruqui et al., naacl 2015)

first post-processing approach
optimise a cost function which brings semantically similar words
close together while keeping them (relatively) close to their initial
distributional vectors.

let v be the vocabulary (with n words), and s the set of
synonymous word pairs (e.g. sophisticated and re   ned). let each
word pair (xl, xr)     s correspond to vector pairs (xl, xr). the
retro   tting cost function is:

      ||xi     (cid:98)xi||2 +

(cid:88)

xi   v

(cid:88)

(xi,xj )   s

      

  i,j||xi     xj||2

  (v, s) =

where   i,j = 1

which feature xi. (cid:98)xi is the initial distributional vector for xi.

deg(xi), and deg(xi) is the number of constraints in s

108 / 207

retro   tting (faruqui et al., naacl 2015)

      ||xi     (cid:98)xi||2 +

(cid:88)

xi   v

(cid:88)

(xi,xj )   s

      

  i,j||xi     xj||2

  (v, s) =

109 / 207

counter-   tting (mrk  i   et al., naacl 2016)

1. antonym repel (ar):

2. synonym attract (sa):

ar(v

(cid:48)

) =

sa(v

(cid:48)

) =

(cid:88)
(cid:88)

(u,w)   a

(u,w)   s

n(cid:88)

(cid:88)

i=1

j   n (i)

u, v(cid:48)

relu(cid:0)       d(v(cid:48)
relu(cid:0)d(v(cid:48)
relu(cid:0)d(v(cid:48)

w)(cid:1)
w)       (cid:1)
j)     d(vi, vj)(cid:1)

u, v(cid:48)

i, v(cid:48)

vsp(v, v

(cid:48)

) =

3. vector space preservation (vsp):

the cost function of the transformed vector space v (cid:48) sums these:

110 / 207

c(v, v

(cid:48)

) = k1ar(v

) + k2sa(v

) + k3v sp (v, v

(cid:48)

(cid:48)

(cid:48)

)

paragram (wieting et al., 2015)

the paragram method (wieting et al., 2015) improves on
retro   tting by using a more sophisticated    attract    term.

if s is again the set of synonymous word pairs, the procedure
iterates over mini-batches of such constraints bs, optimising the
following cost function:

(cid:88)

s(bs) =

(xl,xr)   bs

(relu (  sim + xltl     xlxr)
+ relu (  sim + xrtr     xlxr))

where   sim is the similarity margin and tl and tr are negative
examples for the given word pair (xl, xr).

111 / 207

paragram: the attract term

negative examples for each synonymy pair
for each synonymy pair (xl, xr), the negative example pair (tl, tr) is
chosen from the remaining in-batch vectors so that tl is the one
closest (cosine similarity) to xl and tr is closest to xr.

s(bs) =

(cid:88)

(xl,xr)   bs

(relu (  sim + xltl     xlxr)
+ relu (  sim + xrtr     xlxr))

the two negative examples are used to force synonymous pairs to be
closer to each other than to their respective negative examples (i.e.
to any of the remaining words in the current mini-batch).

112 / 207

paragram: negative examples

negative examples for each synonymy pair
for each synonymy pair (xl, xr), the negative example pair (tl, tr) is
chosen from the remaining in-batch vectors so that tl is the one
closest (cosine similarity) to xl and tr is closest to xr.

113 / 207

paragram: regularisation

the second term tries to retain the bene   cial semantic content
embedded in the initial vector space.

l2 regularisation

r(v ) =

(cid:88)

xi   v

  reg (cid:107)(cid:98)xi     xi(cid:107)2

this term is near-identical to the one in retro   tting: here,   reg is
   ne-tuned to optimise performance. again, it preserves semantic
relations learned by distributional models that do not contradict the
injected similarity constraints.

114 / 207

the attract-repel model

the attract-repel model (mrk  i  , vuli  , et al., 2017) extends the
paragram model with an additional    repel    term.

the repel term

(cid:88)

a(ba) =

(xl,xr)   ba

(relu (  rpl + xlxr     xltr)
+ relu (  rpl + xlxr     xrtr))

the    repel    term pushes words in undesirable relations (such as
antonymy) away from each other in the reshaped vector space.
these constraints can be monolingual (e.g., en_brave and en_timid)
or cross-lingual (en_peace and fr_guerre).

115 / 207

repel term: negative examples

negative examples for each antonymy pair
for each antonymy pair (xl, xr), the negative example pair (tl, tr) is
chosen from the remaining in-batch vectors so that tl is the one
furthest away from xl and tr is the one furthest from xr.

116 / 207

linguistic constraints revisited

in the monolingual scenario, we use linguistic constraints from a
diverse collection of semantic lexicons:

constraint
(response, reply)
(enemy, foe)
(wait, anticipate)
(doctorate, postgraduate)
(costs, expense)
(miserable, poor)

relation
syn
syn
syn
syn
syn
syn

ant
(demand, supply)
ant
(stand, sit)
(worthless, valuable)
ant
(commencement,    nishing) ant
ant
(dishonour, honored)
(intellect, stupidly)
ant

source
id138
id138
babelnet
babelnet
ppdb
ppdb

id138
id138
babelnet
babelnet
ppdb
ppdb

117 / 207

linguistic constraints: multilingual

in the cross-lingual scenario, we rely on babelnet to extract
cross-lingual constraints (synonymy and antonymy):

constraint
(en_sweet, it_dolce)
(en_work, fr_travail)
(en_school, de_schule)
(fr_montagne, de_gebirge)
(sh_gradona  elnik, en_mayor)
(nl_vrouw, it_donna)

relation
syn
syn
syn
syn
syn
syn

ant
(en_sour, it_dolce)
ant
(en_asleep, fr_  veill  )
ant
(en_cheap, de_teuer)
(de_langsam, es_despacio)
ant
(sh_obeshrabriti, en_encourage) ant
(fr_jour, nl_nacht)
ant

source
babelnet
babelnet
babelnet
babelnet
babelnet
babelnet

babelnet
babelnet
babelnet
babelnet
babelnet
babelnet

118 / 207

(around 12 million) constraints

en
en 64/1
de 25/1
nl
31/2
sv 21/1
fr 36/2
es
32/2
it 35/2
pt 26/1
ru 20/1
pl
28/1
sh 19/1
bg 14/1

de
25/ 1
14/0
29/1
22/1
28/1
27/1
28/1
22/1
18/1
23/1
17/1
13/0

nl
31/2
29/1
68/3
24/1
35/2
32/2
34/2
26/1
20/1
28/1
20/1
15/1

sv
21/1
22/1
24/1
0/0
24/1
22/1
23/1
19/1
14/1
20/1
15/1
11/0

fr
36/2
29/1
35/2
24/1
103/1
36/2
39/2
28/1
22/1
31/1
21/1
15/1

es
32/2
27/1
32/2
22/1
36/2
114/0
34/2
26/1
20/1
28/1
19/1
14/1

it
36/2
28/1
34/2
23/1
39/2
34/2
16/1
28/2
22/1
30/2
21/1
15/1

pt
26/1
22/1
26/1
19/1
28/1
26/1
28/2
72/0
16/1
23/1
16/1
12/1

ru
120/1
18/1
20/1
14/1
22/1
20/1
22/1
16/1
5/0
17/1
12/1
9/0

pl
28/1
23/1
28/1
20/1
31/1
28/1
30/2
23/1
17/1
39/0
19/1
13/1

sh
19/1
17/1
20/1
15/1
21/1
19/1
21/1
17/1
12/1
19/1
0/0
10/1

bg
14/1
13/0
15/1
11/0
15/1
14/1
15/1
12/1
9/0
13/1
10/1
18/0

table: linguistic constraint counts (in tens of thousands). for each
language pair, the two    gures show the number of synonymy and
antonymy constraints. monolingual similarity constraints were extracted
from ppdb; cross-lingual similarity and both monolingual and
cross-lingual antonymy were extracted from babelnet.

119 / 207

cross-lingual semantic specialisation

cross-lingual vector spaces
constraint-based optimisation can be extended to cross-lingual
specialisation. babelnet constraints are used to bring the word
vector spaces of various languages into a single uni   ed vector space.

120 / 207

m

part iv: evaluation and

application

121 / 207

intrinsic and extrinsic evaluation

state-of-the-art
can we make some order in the siid113x jungle?

intrinsic / extrinsic performance correlation
the correlation between intrinsic and downstream performance is
under-explored, especially for specialised vectors.

specialised vectors in downstream tasks
question 1: which semantic tasks can bene   t from specialised
semantic spaces?
question 2: can we design other intrinsic tasks which could be a
proxy to downstream tasks?

122 / 207

siid113x-999: state-of-the-art

model / word vectors
neural mt model (hill et al., 2014)
symmetric patterns (schwartz et al., 2015)
non-distributional vectors (faruqui, 2015)
skipgram-retro   t (kiela et al., 2015)
glove vectors (pennington et al., 2014)
glove vectors + retro   tting
glove + counter-   tting
paragram-ws353 (wieting et al., 2015)
paragram-ws353 + retro   tting
paragram-ws353 + counter-   tting
dlce (nguyen et al., 2016)
charagram (wieting et al., 2016)
attract-repel (en-fr vectors)

figure: siid113x-999 performance

  

0.52
0.56
0.58
0.47
0.41
0.53
0.58
0.67
0.67
0.72
0.59
0.71
0.75

123 / 207

why does attract-repel do so well?

word vectors

monolingual distributional vectors

counter-fitting: mono-syn
counter-fitting: mono-ant

counter-fitting: mono-syn + mono-ant

counter-fitting: cross-syn

counter-fitting: mono-syn + cross-syn

counter-fitting: mono-syn + mono-ant + cross-syn + cross-ant

attract-repel: mono-syn
attract-repel: mono-ant

attract-repel: mono-syn + mono-ant

attract-repel: cross-syn

attract-repel: mono-syn + cross-syn

attract-repel: mono-syn + mono-ant + cross-syn + cross-ant

english german

0.32
0.45
0.33
0.50
0.46
0.47
0.53
0.56
0.42
0.65
0.57
0.61
0.70

0.28
0.24
0.28
0.26
0.43
0.40
0.41
0.40
0.30
0.43
0.53
0.58
0.62

italian russian
0.36
0.29
0.47
0.35
0.45
0.43
0.49
0.46
0.45
0.56
0.58
0.59
0.68

0.38
0.46
0.42
0.49
0.37
0.45
0.48
0.53
0.41
0.56
0.46
0.54
0.61

table: multilingual siid113x-999 performance of en-de-it-ru vectors.

124 / 207

how important is the starting vector space?

two sources of information
final vectors combine initial distributional vectors with external
knowledge. which one is more important?

word vectors

random vectors (no information)

a-r: monolingual cons.
a-r: mono + cross-ling.
distributional wiki vectors

a-r: monolingual cons.
a-r: mono + cross-ling.

a-r: non-wiki + mono + cross

en
0.01
0.54
0.66
0.32
0.61
0.66
0.70

de
-0.03
0.33
0.49
0.31
0.48
0.60
0.62

it
0.02
0.29
0.59
0.28
0.53
0.65
0.68

ru
-0.03
0.35
0.51
0.19
0.52
0.54
0.61

table: the e   ect of attract-repel (a-r) on alternative sets of starting
word vectors - random word vectors initialised using the xavier initialisation
(top), and distributional vectors trained on vanilla wikipedia dumps
(bottom).

125 / 207

comparison to existing bilingual spaces

model

(mikolov et al., 2013)
(hermann et al., 2014a)

(gouws et al., 2015)
(vuli   et al., 2016a)
(vuli   et al., 2016)

attract-repel (mrk  i   et al., 2017)

en-it
en
0.32
0.40
0.25
0.32
0.23
0.70

it
0.26
0.34
0.18
0.27
0.25
0.69

en-de
en de
0.32
0.33
0.35
0.38
0.14
0.25
0.33
0.32
0.20
0.25
0.61
0.69

table: comparison of the intrinsic quality (siid113x-999 scores) of bilingual
spaces produced by semantic specialisation methods to those produced by
well-known methods for constructing bilingual vector spaces. n.b: the two
groups of methods use disparate source of information/supervision.

126 / 207

specialisation for lower-resource languages?

cross-lingual constraints - a disambiguation signal?
even low resource-languages such as irish gaelic (ga) or hebrew
(he) boost performance in resource-rich languages such as english!

siid113x languages

ppdb available

no ppdb available

it
0.70
0.58
0.56
0.62

ru
0.70
0.56
0.64
0.56

fr
0.72
0.60
0.68
0.61

de
0.69
0.43
0.65
0.59

en
0.65
0.61
0.69
0.63

bg
0.68
0.52
0.62
0.61

nl
0.70
0.55
0.67
0.61

pt
0.70
0.56
0.66
0.58

es
0.72
0.59
0.68
0.62

pl
0.70
0.54
0.66
0.60

vi
english
0.67
german
0.48
italian
0.58
russian
0.58
table: the e   ect on the four siid113x scores of cross-lingual semantic
specialisation with each combination of the four siid113x languages with
each of the twelve languages. the four    gures on the    rst diagonal
indicate monolingual semantic specialisation for the siid113x languages.
the    gures in bold indicate improvements over these baselines.

sh
0.69
0.53
0.63
0.59

sv
0.70
0.55
0.63
0.60

ga
0.67
0.49
0.60
0.57

he
0.66
0.50
0.59
0.56

fa
0.68
0.51
0.61
0.58

127 / 207

evaluation of lower-resource languages?

intrinsic evaluation datasets are scarce
babelnet provides an abundance of cross-lingual semantic
constraints. however, how do we know we are improving?

hebrew
croatian
english
german
italian
russian

0.51
0.62

distrib. + en + de + it + ru
0.45
0.52
0.63
0.49
0.63

0.52
0.60
0.66
0.55

0.46
0.51
0.61

-

-

-

0.28
0.21
0.32
0.28
0.36
0.38

0.58
0.69
0.56

0.55

-

0.66
0.52

table: bilingual semantic specialisation for the four siid113x languages,
with each row modelling the given language as low-resource and pairing it
with three high-resource languages. figures in bold indicate improvements
over these baselines.

128 / 207

downstream tasks

intrinsic / extrinsic performance correlation
the correlation between intrinsic and downstream performance is
under-explored. in this part of the tutorial, we present recent results
which show the extent of this correlation.

1 dialogue state tracking (dst)
2 spoken language understanding (slu)
3 id53
4 nli / id123

129 / 207

downstream task: dialogue state tracking

domain ontologies for task-oriented dialogue
task-oriented systems    ontologies consist of a collection of slots
s     s (i.e. food, price, etc.) and their slot values vs (cheap,
expensive, etc.).

130 / 207

dialogue state tracking challenge 2

the belief state is a id203 distribution over the possible
dialogue states de   ned by the domain ontology. the downstream
dialogue manager component uses this distribution to decide on the
next system action.

dstc2 ontology: three informable slots...
area: north, east, south, west, centre +dontcare
price: cheap, moderate, expensive +dontcare
food: indian, chinese, thai, japanese, greek, french + 84 others

... and seven (turn-level) requestable slots
food, area, price, address, phone, postcode, address

131 / 207

the power of delexicalised features

delexicalised features
this model is powered by delexicalised features: all occurrences of
slot names and/or slot values in an utterance are replaced with
generic tags

the use of delexicalised id165 features facilitates:

1 faster learning by enabling id21 across slot values
2 generalisation to unseen slot values (or even entirely new slots)
3 id64 dialogue systems to domains with limited data

132 / 207

shortcomings of delexicalisation

delexicalised features = exact matching
given any domain ontology, delexicalisation-based models provide
data-e   cient language understanding - as long as users use only the
actual ontology values to express their search constraints!

user: i   m looking for an a   ordable restaurant
inform(price=cheap)
system: how about thai food?
user: yes please, in central cambridge
inform(price=cheap, food=thai, area=centre)
system: the house serves cheap thai food
user: where is it?
inform(price=cheap, food=thai, area=centre);
request(address)
system: the house is at 106 regent street

133 / 207

traditional    solution   : semantic dictionaries

delexicalised features = exact matching
delexicalisation-based models allow fast deployment to new dialogue
domains, but introduce a complete dependency on semantic
dictionaries.

food=cheap: [a   ordable, budget, low-cost,
low-priced, inexpensive, cheaper, economic, ...]
rating=high: [best, high-rated, highly rated,
top-rated, cool, chic, popular, trendy, ...]
area=centre: [center, downtown, central,
city centre, midtown, town centre, ...]

figure: a subsample of a semantic dictionary with rephrasings for three
ontology values in a restaurant search domain akin to dstc2.

134 / 207

nbt: data-driven (end-to-end) dst

semantic dictionaries break the model pipeline
semantic dictionaries can be hand-crafted or learned - but only for simple
toy domains such as dstc2. moreover, the amazon mechanical turk data
collection framework forces the users to use very simple (and unnatural)
language, understating the challenge of dealing with linguistic variation.

[mrk  i   et al., acl 2017]

135 / 207

nbt: data-driven (end-to-end) dst

semantic dictionaries break the model pipeline
semantic dictionaries can be hand-crafted or learned - but only for simple
toy domains such as dstc2. moreover, the amazon mechanical turk data
collection framework forces the users to use very simple (and unnatural)
language, understating the challenge of dealing with linguistic variation.

the neural belief tracker is a novel dst model/framework which aims
to satisfy the following design goals:

1 end-to-end learnable (no slu modules or semantic dictionaries).
2 generalisation to unseen slot values.
3 capability of leveraging the semantic content of pre-trained word

vector spaces without human supervision.

[mrk  i   et al., acl 2017]

136 / 207

the neural belief tracker (nbt)

representation learning + label embedding +
separate binary decisions
to overcome data sparsity, nbt models use label embedding to
decompose multi-class classi   cation into many binary ones.

137 / 207

representation learning: nbt-id98

n     rl  nd denote the collection of    lters for each value of n,
i denotes the

let f s
where d = 300 is the word vector dimensionality. if vn
concatenation of n    xed word vectors starting at index i, let
mn = [vn
convolutional    lters of length n run over.

ku   n+1] be the list of id165s that

2 ; . . . ; vn

1 ; vn

rn = f s

n mn

r(cid:48)
n = maxpool (relu (rn + bs

n))

138 / 207

r = r(cid:48)

1 + r(cid:48)

2 + r(cid:48)

3

nbt evaluation: dstc2 and woz 2.0

woz 2.0 (wen et al., 2016) and (mrk  i   et al., 2017)
1,200 dialogues collected using the wizard-of-oz setup; users typed
instead of using speech, giving them freedom to use more
sophisticated language.

139 / 207

nbt evaluation: dstc2 and woz 2.0

woz 2.0 (wen et al., 2016) and (mrk  i   et al., 2017)
1,200 dialogues collected using the wizard-of-oz setup; users typed
instead of using speech, giving them freedom to use more
sophisticated language.

model

dstc2

woz 2.0

goals requests goals requests

baseline dst 69.1
+ sem. dict.
72.9*
nbt-dnn
72.6*
nbt-id98
73.4*

95.7
95.7
96.4
96.5

70.8
83.7*
84.4*
84.2*

87.1
87.6
91.2*
91.6*

table: test set accuracies for: a) joint goals; and b) turn-level requests.
the asterisk indicates statistically signi   cant improvement over the
baseline delexicalisation-based trackers (paired t-test; p < 0.05).

140 / 207

the importance of semantic specialisation

three di   erent word vector collections: 1)    random    word vectors
initialised using the xavier initialisation; 2) distributional glove
vectors; and 3) semantically specialised paragram-sl999 vectors.

word vectors

xavier
glove

paragram-sl999

dstc2

woz 2.0

goals requests goals requests
64.2
69.0*
73.4*

81.2
80.1
84.2*

81.2
96.4*
96.5*

90.7
91.4
91.6

table: dstc2 and woz 2.0 test set performance (joint goals and
requests) of the nbt-id98 model making use of three di   erent word
vector collections. the asterisk indicates statistically signi   cant
improvement over the baseline xavier word vectors (paired t-test;
p < 0.05).

141 / 207

dst in italian and german

multilingual woz 2.0 (mrk  i   et al., 2017); italian
and german
the 1,200 dialogues in woz 2.0 were translated by native italian and
german speakers instructed to consider preceding dialogue context.

142 / 207

dst in italian and german

multilingual woz 2.0 (mrk  i   et al., 2017); italian
and german
the 1,200 dialogues in woz 2.0 were translated by native italian and
german speakers instructed to consider preceding dialogue context.

word vector space
best baseline vector space
monolingual distributional vectors
+ monolingual specialisation
++ cross-lingual specialisation
+++ multilingual dst model

en
81.6
77.6
80.9
80.3
82.8

it
71.8
71.2
72.7
75.3
77.1

de
50.5
46.6
52.4
55.7
57.7

143 / 207

dst models for resource-poor languages

ontology grounding: multilingual dst models
the domain ontology (i.e. the concepts it expresses) is language
agnostic, which means that    labels    persist across languages. using
training data for two (or more) languages, and cross-lingual vectors
of high quality, we train the    rst-ever multilingual dst model.

144 / 207

morph-   tting: illustration

using di   erent sources of supervision
morphological phenomena provide an inexpensive source of
supervision. consider both in   ectional and derivational forms.

[vuli   et al., acl 2017]

145 / 207

rispettosorispettosarispettosiirrispettosoirrispettosairrispettosisemantic specialisation with morphology

simple morphological rules yield thousands of constraints:
    in   ectional synonyms
    derivational antonyms

constraint

relation

rule

(sun   ower, sun   owers)
(su   er, su   ered)
(ambiguous, unambiguous)

(zucchero, zuccheri)
(vincere, vincono)
(visibilit  , invisibilit  )

syn (inf)
syn (inf)
ant (deriv) derivational antonymy

singular-plural
past participle

syn (inf)
syn (inf)
ant (deriv) derivational antonymy

singular-plural
conjugation

(kategorie, kategorien)
(katalanisch, katalanischem)
(dokumentiert, undokumentiert)

syn (inf)
syn (inf)
ant (deriv) derivational antonymy

singular-plural
declension

146 / 207

morphological forms - first neighbours?

morph-fitting: expanding the vocabulary coverage
using the automatically constructed morphological synonyms, we
can bring word pairs which represent di   erent morphological forms of
the same word together.

en_expensive

costly
costlier
cheaper

de_teure

teuren

kostspielige
aufw  ndige

prohibitively

pricey

kostenintensive

aufwendige

en_slow de_langsam

fast
slower
slower
slowed
slowing

allm  hlich

rasch

gem  chlich

schnell

explosionsartig

it_lento
lentissimo

lenta

inesorabile
rapidissimo
graduale

it_costoso
dispendioso
remunerativo

redditizio
rischioso
costosa
costosa
costose
costosi

slow

una   ordable

expensiveness

costly
costlier
ruinously

teures
teuren
teurem
teurer
teurerer

libri
libra
librare
libre
librano
table: the nearest neighbours of three example words (expensive, slow
and book) in english, german and italian before (top) and after (bottom)
morph-   tting.

langsamer
langsames
langsame
langsamem
langsamen

lenti
lente
lenta
veloce
rapido

slowing
slowed
slowness

dispendioso
dispendiose

books

slows

it_libro
romanzo
racconto
volumetto

saggio

ecclesiaste

en_book

books
memoir
novel

storybooks

blurb
booked
rebook
booking
rebooked

de_buch
sachbuch
buches

romandeb  t

b  chlein
pamphlet
b  cher
b  ch
b  che
b  ches
b  chen

147 / 207

morph-fitting: gains across languages

vectors
en: glove-6b (300)
en: sg-bow2-pw (300)

de: sg-deps-pw (300)
(vuli   and korhonen, 2016)
de: biskip-de (256)
(luong et al., 2015)

it: sg-deps-pw (300)
(vuli   and korhonen, 2016)
it: cbow5-wacky (300)
(dinu et al., 2015))

distrib. mfit-a mfit-ar

.324
.339

.267

.354

.237

.363

.376
.385

.318

.414

.351

.417

.438
.439

.325

.421

.391

.446

table: results on multilingual siid113x-999 (en, de, and it) with two
morph-   tting variants (synonyms / synonyms + antonyms).

148 / 207

morph-fitting leads to further improvements

(a) english

(b) german

149 / 207

(c) italian

(d) russian

0.10.150.20.250.30.350.40.45distributionalmfixmfit-amfit-ar0.60.650.70.750.8siid113x(spearman   s  )envectorcollectionsdstperformace(joint)siid113xmorph-siid113xdst0.10.150.20.250.30.350.40.45distributionalmfixmfit-amfit-ar0.60.650.70.750.8siid113x(spearman   s  )devectorcollectionsdstperformace(joint)siid113xmorph-siid113xdst0.10.150.20.250.30.350.40.45distributionalmfixmfit-amfit-ar0.60.650.70.750.8siid113x(spearman   s  )itvectorcollectionsdstperformace(joint)siid113xmorph-siid113xdst0.10.150.20.250.30.350.40.45distributionalmfixmfit-amfit-arsiid113x(spearman   s  )ruvectorcollectionssiid113xmorph-siid113xother directions: order-embeddings

specialising word vector spaces to capture ontological/hierarchical relations

order-embeddings of (images and) language [vendrov et al., iclr 2016]

mapping from words to vectors
it is not distance-preserving but order-preserving between the semantic hierarchy
and a partial order over the embedding space.

(cid:126)x (cid:22) (cid:126)y

i   

n(cid:94)

i=1

xi     yi

150 / 207

other directions: order-embeddings

specialising word vector spaces to capture ontological/hierarchical relations

order-embeddings of (images and) language [vendrov et al., iclr 2016]

it is de   ned similarly to the standard sgns objective (but now with order
violation penalty):

l( (cid:126)x, (cid:126)y ) = || max(0, (cid:126)y     (cid:126)x)||2

(cid:88)

(x,y )   p p

(cid:88)

(x(cid:48),y (cid:48))   n p

l( (cid:126)x, (cid:126)y ) +

max(0,        l( (cid:126)x(cid:48), (cid:126)y (cid:48)))

    order-embeddings improve hypernymy prediction and natural
language id136

151 / 207

semantic conflation deficiencyword representations conflate different meanings of a word into a single representation: they cannot capture polysemysemantic conflation deficiencyrelated words to unrelated senses of a word will be unwantedly represented as relatedplantpollenrefineryexample from neelakantanet al (2014)semantic conflation deficiencyone can address this deficiency by representing individual meanings of words (word senses)plant1pollenrefineryplant2sense representationsone can address this deficiency by representing individual meanings of words (word senses)sense representations: paradigms   not linkedunsupervised(multi prototype)linked to sense inventoriesknowledge-basedunsupervised sense representationsinduce senses, then learn representations for the induced sensesusually coupled with id91image courtesy of reisingerand mooney (2010)............position1position2position3position4unsupervised sense representationsfeatures:   do not rely on external sense inventories   id91algorithms are generally used for distinguishing senses from each other   resulting sense representations are not linkedto any inventoryknowledge-based representationsrepresent word senses as defined by sense inventoriesplantplant, works, industrialplant(buildings for carrying on industrial labor) plant, flora, plantlife((botany) a living organism lacking the power of locomotion)plant(an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience)plant(something planted secretly for discovery by another)plant1plant2plant3plant4............knowledge-based representationsfeatures:   use knowledge from lexical-semantic resourcesfor distinguishing senses from each other   the resulting sense representations are linked to the inventory, hence useful for applications such as wsd   exploit various types of knowledge encoded in these resources: sense definitions, synonymy, polysemy, semantic relations, structure, etc.evaluation of sense representationswould moving from words to senses results in improved performance in downstream nlp applications?unfortunately, most existing sense representation papers suffice to evaluate on artificial intrinsic tasks, usually word similarity.evaluation of sense representationsintegration of senses in downstream nlp applicationsm. t. pilehvar, j. camacho-collados, r. navigliand n. colliertowards a seaid113ss integration of word sensesintodownstream nlp applications(acl 2017)sense 2017 workshophttps://sites.google.com/site/senseworkshop2017/invited speakers:hinrichsch  tze(university of munich)roberto navigli(sapienzauniversity of rome)tomorrow!eacl invited talk on senseshinrichsch  tzedon't cram two completely different meanings into a single !&??@#^$% vector! or should you?friday, 9:30amsense representations for word similarityusually, four techniques are used (reisingerand mooney, 2010):1.maxsim2.avgsim3.maxsimc4.avgsimcsense representations for word similarity1-maxsim: pick the similarity between the most similar senses across two words plant1tree1plant2plant3tree2sense representations for word similarity2-avgsim: average the similarities between senses across two words plant1tree1plant2plant3tree2sense representations for word similarityfor some datasets, words are provided with contexts, e.g., stanford contextual word similarity (scws)plant treein	a	thermal	powerplantheat	energy	is	converted	to	electric	power.almost	400	billion	treesgrow	in	the	amazon	rainforest.sense representations for word similarity3-maxsimc: the similarity between the    most appropriate    senses of the two wordsin a thermal powerheat energy is converted to electric power.almost 400 billion grow in the amazon rainforest.plant1plant2plant3tree1tree2the most appropriate sense of the word w given the contextsense representations for word similarity4-avgsimc: average of pairwise similarities weighted by their appropriateness in contextin a thermal powerheat energy is converted to electric power.almost 400 billion grow in the amazon rainforest.plant1plant2plant3tree1tree2the likelihood of a sense in the contextsense representations: recent workknowledge-based:   chen et al: a unified model for word sense representation and disambiguation(emnlp 2014)   rotheand schutze: autoextend: extending id27sto embeddingsfor synsetsand lexemes(acl 2015)   johansson and nieto pin  a: embedding a semantic network in a word space(naacl 2015, short)   jauharet al: ontologically grounded multi-sense representation learning for semantic vector space models(naacl 2015)   pilehvarand collier: de-conflated semantic representations(emnlp 2016)   pilehvaret al: align, disambiguate and walk: a unified approach for measuring semantic similarity(acl 2013)sense representations: recent workunsupervised:   reisingerand mooneyz: multi-prototype vector-space models of word meaning (naacl 2010)   huang et al.: improving word representations via global context and multiple word prototypes (acl 2012)   wu and giles: sense-aware semantic analysis: a multi-prototype word representation model using wikipedia (aaai 2015)   vu and parker: k-embeddings: learning conceptual embeddingsfor words using context (naacl 2016)   neelakantanet al.: efficient non-parametric estimation of multipxleembeddingsper word in vector space (emnlp 2014)   tian et al.: a probabilistic model for learning multi-prototype id27s(coling 2014)   liu et at.: topical id27s(aaai 2015)    li and jurafsky: do multi-sense embeddingsimprove natural language understanding?(emnlp 2015)   guoet al.: learning sense-specific id27sby exploiting bilingual resources (coling 2014)     usteret al.: bilingual learning of multi-sense embeddingswith discrete autoencoders(naacl 2016)knowledge-based representations: jauharet al 2015sense vectorsinitial word vectorsretros. k. jauhar, c. dyer and e. hovy: ontologically grounded multi-sense representation learning for semantic vector space models(naacl 2015)knowledge-based representations: jauharet al 2015em: extends the skip-gram model to learn ontologically-grounded sense vectorsontological priorknowledge-based representations: deconfpilehvarand collier, de-conflated semantic representations(emnlp 2016)deconfmainstream approachesknowledge-based representations: deconfuses personalized id95 algorithm to exploit id138for sense specific informationdigitknowledge-based representations: deconflearns a representation        for a sense        that is:close to its lemmaembeddingclose to a weighted average of embeddingsof its sense biasing wordsknowledge-based representations: deconffingertoethumbnailappendagefootlimbbonewristlobeanklehipknowledge-based representations: deconfknowledge-based representations: deconfevaluation: word similarityunsupervised representations: reisingerand mooney, 2010reisingerand mooneyz: multi-prototype vector-space models of word meaning (naacl 2010)unsupervised representationsliu et at.: topical id27s(aaai 2015) assumesthatwordshavedifferentembeddingsunderdifferenttopics.threedifferentmodels:twe-1,twe-2,andtwe-3id27stopic embeddingsunsupervised representationsneelakantanet al.: efficient non-parametric estimation of multipxleembeddingsper word in vector space (emnlp 2014)multi-senseskip-gram(mssg)anextensionofskip-gramforjointid91andtrainingofsenserepresentations(computationallycheaper).np-mssgdynamicnumberofsensesperwordapplication of sense representationsseveral sense representation techniquesbut, they have mainly relied on word similarity for their evaluation.research should focus on the integration of sense representations in downstream applications (rather than artificial intrinsic tasks).conclusions

1. lexical resources are abundant - and useful
specialisation using lexical resources leads to state-of-the-art
performance in intrinsic tasks and downstream applications.

185 / 207

conclusions

1. lexical resources are abundant - and useful
specialisation using lexical resources leads to state-of-the-art
performance in intrinsic tasks and downstream applications.

2. use of such resources is not cheating!
the coverage of babelnet/ppdb is not task- or even language-
speci   c. these heavyweight resources are fruit of generations of
linguists    and nlpers    e   ort. we should not shy away from using
them - especially to enable novel downstream applications!

186 / 207

conclusions

1. lexical resources are abundant - and useful
specialisation using lexical resources leads to state-of-the-art
performance in intrinsic tasks and downstream applications.

2. use of such resources is not cheating!
the coverage of babelnet/ppdb is not task- or even language-
speci   c. these heavyweight resources are fruit of generations of
linguists    and nlpers    e   ort. we should not shy away from using
them - especially to enable novel downstream applications!

3. however, distributional information still helps
distributional/unsupervised methods are not competitive with
knowledge-based ones. however, combining the two sources of
information/supervision leads to further performance improvements.

187 / 207

further work / research directions

1. intrinsic evaluation     downstream performance?
performance on intrinsic evaluation datasets correlates with
downstream tasks such as dst. however, substantial intrinsic gains
do not always lead to large downstream gains. why?

188 / 207

further work / research directions

1. intrinsic evaluation     downstream performance?
performance on intrinsic evaluation datasets correlates with
downstream tasks such as dst. however, substantial intrinsic gains
do not always lead to large downstream gains. why?

2. other kinds of specialisation?
other kinds of specialisation (relatedness, entailment, etc.) are still
under-explored. we have yet to    nd downstream tasks which
meaningfully correlate with these kinds of specialisation.

189 / 207

further work / research directions

1. intrinsic evaluation     downstream performance?
performance on intrinsic evaluation datasets correlates with
downstream tasks such as dst. however, substantial intrinsic gains
do not always lead to large downstream gains. why?

2. other kinds of specialisation?
other kinds of specialisation (relatedness, entailment, etc.) are still
under-explored. we have yet to    nd downstream tasks which
meaningfully correlate with these kinds of specialisation.

1 moving towards multi-word/sentence representations
2 expansion to other languages
3 dealing with polysemous words

190 / 207

references i

eneko agirre, enrique alfonseca, keith b. hall, jana kravalova, marius pasca,
and aitor soroa. 2009.
a study on similarity and relatedness using distributional and id138-based
approaches.
in proceedings of naacl. pages 19   27.

collin f. baker, charles j. fillmore, and john b. lowe. 1998.
the berkeley framenet project.
in proceedings of acl. pages 86   90.

mohit bansal, kevin gimpel, and karen livescu. 2014.
tailoring continuous word representations for id33.
in proceedings of acl. pages 809   815.

marco baroni, georgiana dinu, and germ  n kruszewski. 2014.
don   t count, predict! a systematic comparison of context-counting vs.
context-predicting semantic vectors.
in proceedings of acl. pages 238   247.

jiang bian, bin gao, and tie-yan liu. 2014.
knowledge-powered deep learning for id27.
in proceedings of ecml-pkdd. pages 132   148.

191 / 207

references ii

piotr bojanowski, edouard grave, armand joulin, and tomas mikolov. 2016.
enriching word vectors with subword information.
corr abs/1607.04606.

antoine bordes, jason weston, ronan collobert, and yoshua bengio. 2011.
learning structured embeddings of knowledge bases.
in proceedings of aaai . pages 301   306.

elia bruni, nam-khanh tran, and marco baroni. 2014.
multimodal id65.
journal of arti   cial intelligence research 49:1   47.

xinxiong chen, zhiyuan liu, and maosong sun. 2014.
a uni   ed model for word sense representation and disambiguation.
in proceedings of emnlp. pages 1025   1035.

vincent claveau, ewa kijak, and olivier ferret. 2014.
improving distributional thesauri by exploring the graph of neighbors.
in proceedings of coling. pages 709   720.

192 / 207

references iii

ronan collobert and jason weston. 2008.
a uni   ed architecture for natural language processing: deep neural networks
with multitask learning.
in proceedings of icml. pages 160   167.

ronan collobert, jason weston, l  on bottou, michael karlen, koray
kavukcuoglu, and pavel p. kuksa. 2011.
natural language processing (almost) from scratch.
journal of machine learning research 12:2493   2537.

dmitry davidov and ari rappoport. 2006.
e   cient unsupervised discovery of word categories using symmetric patterns and
high frequency words.
in proceedings of acl. pages 297   304.

maud ehrmann, francesco cecconi, daniele vannella, john philip mccrae,
philipp cimiano, and roberto navigli. 2014.
representing multilingual data as linked data: the case of babelnet 2.0.
in proceedings of lrec. pages 401   408.

193 / 207

references iv

manaal faruqui, jesse dodge, sujay kumar jauhar, chris dyer, eduard hovy,
and noah a. smith. 2015.
retro   tting word vectors to semantic lexicons.
in proceedings of naacl-hlt . pages 1606   1615.

christiane fellbaum. 1998.
id138.
https://mitpress.mit.edu/books/id138.

lev finkelstein, evgeniy gabrilovich, yossi matias, ehud rivlin, zach solan,
gadi wolfman, and eytan ruppin. 2002.
placing search in context: the concept revisited.
acm transactions on information systems 20(1):116   131.

juri ganitkevitch and chris callison-burch. 2014.
the multilingual paraphrase database.
in proceedings of lrec. pages 4276   4283.

juri ganitkevitch, benjamin van durme, and chris callison-burch. 2013.
ppdb: the paraphrase database.
in proceedings of naacl-hlt . pages 758   764.

194 / 207

references v

daniela gerz, ivan vuli  , felix hill, roi reichart, and anna korhonen. 2016.
simverb-3500: a large-scale evaluation set of verb similarity.
in proceedings of emnlp.

xavier glorot and yoshua bengio. 2010.
understanding the di   culty of training deep feedforward neural networks.
in proceedings of aistats. pages 249   256.

jiang guo, wanxiang che, haifeng wang, and ting liu. 2014.
learning sense-speci   c id27s by exploiting bilingual resources.
in proceedings of coling. pages 497   507.

felix hill, kyunghyun cho, s  bastien jean, coline devin, and yoshua bengio.
2014.
embedding word similarity with id4.
corr abs/1412.6448.

felix hill, roi reichart, and anna korhonen. 2015.
siid113x-999: evaluating semantic models with (genuine) similarity estimation.
computational linguistics 41(4):665   695.

195 / 207

references vi

ignacio iacobacci, mohammad taher pilehvar, and roberto navigli. 2015.
sensembed: learning sense embeddings for word and relational similarity.
in proceedings of acl. pages 95   105.

sujay kumar jauhar, chris dyer, and eduard h. hovy. 2015.
ontologically grounded multi-sense representation learning for semantic vector
space models.
in proceedings of naacl. pages 683   693.

richard johansson and luis nieto pi  a. 2015.
embedding a semantic network in a word space.
in proceedings of naacl-hlt . pages 1428   1433.

douwe kiela, felix hill, and stephen clark. 2015.
specializing id27s for similarity or relatedness.
in proceedings of emnlp. pages 2044   2048.

joo-kyung kim, marie-catherine de marne   e, and eric fosler-lussier. 2016a.
adjusting id27s with semantic intensity orders.
in proceedings of the 1st workshop on representation learning for nlp. pages
62   69.

196 / 207

references vii

joo-kyung kim, gokhan tur, asli celikyilmaz, bin cao, and ye-yi wang. 2016b.

intent detection using semantically enriched id27s.
in proceedings of ieee slt .

thomas k. landauer and susan t. dumais. 1997.
solutions to plato   s problem: the latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
psychological review 104(2):211   240.

omer levy and yoav goldberg. 2014a.
dependency-based id27s.
in proceedings of acl. pages 302   308.

omer levy and yoav goldberg. 2014b.
neural id27 as implicit id105.
in proceedings of nips. pages 2177   2185.

omer levy, yoav goldberg, and ido dagan. 2015.
improving distributional similarity with lessons learned from id27s.
transactions of the acl 3:211   225.

197 / 207

references viii

wang ling, chris dyer, alan w black, and isabel trancoso. 2015a.
two/too simple adaptations of id97 for syntax problems.
in proceedings of naacl-hlt . pages 1299   1304.

wang ling, chris dyer, alan w. black, isabel trancoso, ramon fermandez,
silvio amir, luis marujo, and tiago luis. 2015b.
finding function in form: compositional character models for open vocabulary
word representation.
in proceedings of emnlp. pages 1520   1530.

quan liu, hui jiang, si wei, zhen-hua ling, and yu hu. 2015.
learning semantic id27s based on ordinal knowledge constraints.
in proceedings of acl. pages 1501   1511.

thang luong, richard socher, and christopher manning. 2013.
better word representations with id56s for morphology.
in proceedings of conll. pages 104   113.

oren melamud, ido dagan, and jacob goldberger. 2015.
modeling word meaning in context with substitute vectors.
in proceedings of naacl-hlt . pages 472   482.

198 / 207

references ix

oren melamud, jacob goldberger, and ido dagan. 2016a.
context2vec: learning generic context embedding with bidirectional lstm.
in proceedings of conll. pages 51   61.

oren melamud, david mcclosky, siddharth patwardhan, and mohit bansal.
2016b.
the role of context types and dimensionality in learning id27s.
in proceedings of naacl-hlt . pages 1030   1040.

tomas mikolov, quoc v. le, and ilya sutskever. 2013a.
exploiting similarities among languages for machine translation.
arxiv preprint, corr abs/1309.4168.

tomas mikolov, ilya sutskever, kai chen, gregory s. corrado, and je   rey dean.
2013b.
distributed representations of words and phrases and their compositionality.
in proceedings of nips. pages 3111   3119.

george a. miller. 1995.
id138: a lexical database for english.
communications of the acm pages 39   41.

199 / 207

references x

ivan mrks  i  , diarmuid    s  aghdha, tsung-hsien wen, blaise thomson, and
steve young. 2017.
the neural belief tracker: data-driven dialogue state tracking.
in proceedings of acl (to appear).

nikola mrk  i  , diarmuid    s  aghdha, blaise thomson, milica ga  i  , lina maria
rojas-barahona, pei-hao su, david vandyke, tsung-hsien wen, and steve
young. 2016a.
counter-   tting word vectors to linguistic constraints.
in proceedings of naacl-hlt .

nikola mrk  i  , diarmuid    s  aghdha, tsung-hsien wen, blaise thomson, and
steve young. 2016b.
neural belief tracker: data-driven dialogue state tracking.
corr abs/1606.03777.

roberto navigli and simone paolo ponzetto. 2012.
babelnet: the automatic construction, evaluation and application of a
wide-coverage multilingual semantic network.
arti   cial intelligence 193:217   250.

200 / 207

references xi

arvind neelakantan, jeevan shankar, alexandre passos, and andrew mccallum.
2014.
e   cient non-parametric estimation of multiple embeddings per word in vector
space.
in proceedings of emnlp. pages 1059   1069.

kim anh nguyen, sabine schulte im walde, and ngoc thang vu. 2016.
integrating distributional lexical contrast into id27s for
antonym-synonym distinction.
in proceedings of acl. pages 454   459.

kim anh nguyen, sabine schulte im walde, and ngoc thang vu. 2017.
distinguishing antonyms and synonyms in a pattern-based neural network.
in proceedings of eacl.

masataka ono, makoto miwa, and yutaka sasaki. 2015.
id27-based antonym detection using thesauri and distributional
information.
in proceedings of naacl-hlt . pages 984   989.

dominique osborne, shashi narayan, and shay cohen. 2016.
encoding prior knowledge with eigenid27s.
transactions of the acl 4:417   430.

201 / 207

references xii

sebastian pad   and mirella lapata. 2007.
dependency-based construction of semantic space models.
computational linguistics 33(2):161   199.

ellie pavlick, pushpendre rastogi, juri ganitkevitch, benjamin van durme, and
chris callison-burch. 2015.
ppdb 2.0: better paraphrase ranking,    ne-grained entailment relations, word
embeddings, and style classi   cation.
in proceedings of acl. pages 425   430.

mohammad taher pilehvar and nigel collier. 2016.
de-con   ated semantic representations.
in proceedings of emnlp. pages 1680   1690.

mohammad taher pilehvar, david jurgens, and roberto navigli. 2013.
align, disambiguate and walk: a uni   ed approach for measuring semantic
similarity.
in proceedings of acl. pages 1341   1351.

joseph reisinger and raymond j. mooney. 2010.
multi-prototype vector-space models of word meaning.
in proceedings of naacl-hlt . pages 109   117.

202 / 207

references xiii

sascha rothe and hinrich sch  tze. 2015.
autoextend: extending id27s to embeddings for synsets and lexemes.

in proceedings of acl. pages 1793   1803.

roy schwartz, roi reichart, and ari rappoport. 2015.
symmetric pattern based id27s for improved word similarity
prediction.
in proceedings of conll. pages 258   267.

roy schwartz, roi reichart, and ari rappoport. 2016.
symmetric patterns and coordinations: fast and enhanced representations of
verbs and adjectives.
in proceedings of naacl-hlt . pages 499   505.

fei tian, hanjun dai, jiang bian, bin gao, rui zhang, enhong chen, and
tie-yan liu. 2014.
a probabilistic model for learning multi-prototype id27s.
in proceedings of coling. pages 151   160.

peter d. turney and patrick pantel. 2010.
from frequency to meaning: vector space models of semantics.
journal of arti   cal intelligence research 37(1):141   188.

203 / 207

references xiv

jason utt and sebastian pad  . 2014.
crosslingual and multilingual construction of syntax-based vector space models.
transactions of the acl 2:245   258.
ivan vendrov, ryan kiros, sanja fidler, and raquel urtasun. 2016.
order-embeddings of images and language.
in iclr.
simon   uster, ivan titov, and gertjan van noord. 2016.
bilingual learning of multi-sense embeddings with discrete autoencoders.
in proceedings of naacl. pages 1346   1356.

thuy vu and d. stott parker. 2016.
k-embeddings: learning conceptual embeddings for words using context.
in proceedings of naacl-hlt . pages 1262   1267.

ivan vuli  , douwe kiela, and anna korhonen. 2017a.
evaluation by association: a systematic study of quantitative word association
evaluation.
in proceedings of eacl.

204 / 207

references xv

ivan vuli   and anna korhonen. 2016.
is "universal syntax" universally useful for learning distributed word
representations?
in proceedings of acl. pages 518   524.

ivan vuli  , nikola mrk  i  , roi reichart, diarmuid    s  aghdha, steve young, and
anna korhonen. 2017b.
morph-   tting: fine-tuning word vector spaces with simple language-speci   c rules.

in proceedings of acl (to appear).

ivan vuli  , roy schwartz, ari rappoport, roi reichart, and anna korhonen.
2016.
automatic selection of context con   gurations for improved class-speci   c word
representations.
corr abs/1608.05528.

john wieting, mohit bansal, kevin gimpel, and karen livescu. 2015.
from paraphrase database to compositional paraphrase model and back.
transactions of the acl 3:345   358.

205 / 207

references xvi

john wieting, mohit bansal, kevin gimpel, and karen livescu. 2016.
charagram: embedding words and sentences via character id165s.
in proceedings of emnlp. pages 1504   1515.

wen-tau yih, geo   rey zweig, and john c. platt. 2012.
polarity inducing latent semantic analysis.
in proceedings of emnlp-conll. pages 1212   1222.

mo yu and mark dredze. 2014.
improving lexical embeddings with semantic knowledge.
in proceedings of acl. pages 545   550.

206 / 207

questions?

207 / 207

