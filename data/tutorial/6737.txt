   #[1]learn opencv    feed [2]learn opencv    comments feed [3]learn opencv
      understanding id180 in deep learning comments feed
   [4]image classification using feedforward neural network in keras
   [5]best project award : id161 for faces [6]alternate
   [7]alternate

   [tr?id=193036094536652&ev=pageview&noscript=1]

     * [8]skip to primary navigation
     * [9]skip to content
     * [10]skip to primary sidebar

   [11]learn opencv

   opencv examples and tutorials ( c++ / python )

     * [12]home
     * [13]about
     * [14]resources
     * [15]ai consulting
     * [16]courses

understanding id180 in deep learning

   october 30, 2017 by [17]aditya sharma [18]6 comments

   this post is part of the series on deep learning for beginners, which
   consists of the following tutorials :
    1. [19]neural networks : a 30,000 feet view for beginners
    2. [20]installation of deep learning frameworks (tensorflow and keras
       with cuda support )
    3. [21]introduction to keras
    4. [22]understanding feedforward neural networks
    5. [23]image classification using feedforward neural networks
    6. [24]image recognition using convolutional neural network
    7. understanding id180
    8. [25]understanding autoencoders using tensorflow
    9. [26]image classification using pre-trained models in keras
   10. [27]id21 using pre-trained models in keras
   11. [28]fine-tuning pre-trained models in keras
   12. more to come . . .

   in this post, we will learn about different kinds of activation
   functions; we will also see which activation function is better than
   the other. this post assumes that you have a basic idea of artificial
   neural networks (ann), but in case you don   t, i recommend you first
   read the post on [29]understanding feedforward neural networks.

1. what is an activation function?

   biological neural networks inspired the development of artificial
   neural networks. however, anns are not even an approximate
   representation of how the brain works. it is still useful to understand
   the relevance of an activation function in a biological neural network
   before we know as to why we use it in an id158.

   a typical neuron has a physical structure that consists of a cell
   body,  an axon that sends messages to other neurons, and dendrites that
   receives signals or information from other neurons.
   biological neural network ([30]image credit)

   in the above picture, the red circle indicates the region where the two
   neurons communicate. the neuron receives signals from other neurons
   through the dendrites. the weight (strength) associated with a
   dendrite, called synaptic weights, gets multiplied by the incoming
   signal. the signals from the dendrites are accumulated in the cell
   body, and if the strength of the resulting signal is above a certain
   threshold, the neuron passes the message to the axon. otherwise, the
   signal is killed by the neuron and is not propagated further.

   the activation function takes the decision of whether or not to pass
   the signal. in this case, it is a simple step function with a single
   parameter     the threshold. now, when we learn something new ( or
   unlearn something ), the threshold and the synaptic weights of some
   neurons change. this creates new connections among neurons making the
   brain learn new things.

   let us understand the same concept again but this time using an
   artificial neuron.

   [31]neuron

   in the above figure, (x_1, \cdots , x_n) is the signal vector that gets
   multiplied with the weights \left( w_1, w_2, \cdots, w_n \right) . this
   is followed by accumulation ( i.e. summation + addition of bias b ).
   finally, an activation function f is applied to this sum.

   note that the weights \left( w_1, w_2, \cdots, w_n \right) and the bias
   b transform the input signal linearly. the activation, on the other
   hand, transforms the signal non-linearly and it is this non-linearity
   that allows us to learn arbitrarily complex transformations between the
   input and the output.

   over the years, various functions have been used, and it is still an
   active area of research to find a proper activation function that makes
   the neural network learn better and faster.

2. how does the network learn?

   it is essential to get a basic idea of how the neural network learns.
   let   s say that the desired output of the network is y . the network
   produces an output y' . the difference between the predicted output and
   the desired output (y - y) is converted to a metric known as the loss
   function ( j ). the loss is high when the neural network makes a lot of
   mistakes, and it is low when it makes fewer mistakes. the goal of the
   training process is to find the weights and bias that minimise the loss
   function over the training set.

   in the figure below, the id168 is shaped like a bowl. at any
   point in the training process, the partial derivatives of the loss
   function w.r.t to the weights is nothing but the slope of the bowl at
   that location. one can see that by moving in the direction predicted by
   the partial derivatives, we can reach the bottom of the bowl and
   therefore minimize the id168. this idea of using the partial
   derivatives of a function to iteratively find its local minimum is
   called the id119.

   [32]diagram explaining id119

   in id158s the weights are updated using a method
   called id26. the partial derivatives of the id168
   w.r.t the weights are used to update the weights. in a sense, the error
   is backpropagated in the network using derivatives. this is done in an
   iterative manner and after many iterations, the loss reaches a minimum
   value, and the derivative of the loss becomes zero.

   we plan to cover id26 in a separate blog post. the main
   thing to note here is the presence of derivatives in the training
   process.

3. types of id180

     * linear activation function: it is a simple linear function of the
       form f(x) = x basically, the input passes to the output without any
       modification.

   figure : linear activation function
     * non-linear id180: these functions are used to
       separate the data that is not linearly separable and are the most
       used id180.a non-linear equation governs the mapping
       from inputs to outputs. few examples of different types of
       non-linear id180 are sigmoid, tanh, relu, lrelu,
       prelu, swish, etc. we will be discussing all these
       id180 in detail.

   figure: non-linear activation function

4. why do we need a non-linear activation function in an artificial neural
network?

   neural networks are used to implement complex functions, and non-linear
   id180 enable them to approximate arbitrarily complex
   functions. without the non-linearity introduced by the activation
   function, multiple layers of a neural network are equivalent to a
   single layer neural network.

   let   s see a simple example to understand why without non-linearity it
   is impossible to approximate even simple functions like xor and xnor
   gate. in the figure below, we graphically show an xor gate. there are
   two classes in our dataset represented by a cross and a circle. when
   the two features, x1 and x2 are the same, the class label is a red
   cross, otherwise, it is a blue circle. the two red crosses have an
   output of 0 for input value (0,0) and (1,1) and the two blue rings have
   an output of 1 for input value (0,1) and (1,0).
   figure: graphical representation of xor gate

   from the above picture, we can see that the data points are not
   linearly separable. in other words, we can not draw a straight line to
   separate the blue circles and the red crosses from each other. hence,
   we will need a non-linear decision boundary to separate them.

   the activation function is also crucial for squashing the output of the
   neural network to be within certain bounds. the output of a neuron
   \sum^n_i w_i x_i + b can take on very large values. this output, when
   fed to the next layer neuron without modification, can be transformed
   to even larger numbers thus making the process computationally
   intractable. one of the tasks of the activation function is to map the
   output of a neuron to something that is bounded ( e.g., between 0 and
   1).

   with this background, we are ready to understand different types of
   id180.

5. types of non-linear id180

5.1. sigmoid

   it is also known as logistic activation function. it takes a
   real-valued number and squashes it into a range between 0 and 1. it is
   also used in the output layer where our end goal is to predict
   id203. it converts large negative numbers to 0 and large positive
   numbers to 1. mathematically it is represented as

       \[ \sigma(x) = \frac{1}{1 + e^{-x}} \]

   the figure below shows the sigmoid function and its derivative
   graphically
   figure: sigmoid activation function figure: sigmoid derivative


   the three major drawbacks of sigmoid are:
    1. vanishing gradients: notice, the sigmoid function is flat near 0
       and 1. in other words, the gradient of the sigmoid is 0 near 0 and
       1. during id26 through the network with sigmoid
       activation, the gradients in neurons whose output is near 0 or 1
       are nearly 0. these neurons are called saturated neurons. thus, the
       weights in these neurons do not update. not only that, the weights
       of neurons connected to such neurons are also slowly updated. this
       problem is also known as vanishing gradient. so, imagine if there
       was a large network comprising of sigmoid neurons in which many of
       them are in a saturated regime, then the network will not be able
       to backpropagate.
    2. not zero centered: sigmoid outputs are not zero-centered.
    3. computationally expensive: the exp() function is computationally
       expensive compared with the other non-linear id180.

   the next non-linear activation function that i am going to discuss
   addresses the zero-centered problem in sigmoid.

5.2. tanh

   figure: tanh activation function figure: tanh derivative

   it is also known as the hyperbolic tangent activation function. similar
   to sigmoid, tanh also takes a real-valued number but squashes it into a
   range between -1 and 1. unlike sigmoid, tanh outputs are zero-centered
   since the scope is between -1 and 1. you can think of a tanh function
   as two sigmoids put together. in practice, tanh is preferable over
   sigmoid. the negative inputs considered as strongly negative, zero
   input values mapped near zero, and the positive inputs regarded as
   positive. the only drawback of tanh is:
    1. the tanh function also suffers from the vanishing gradient problem
       and therefore kills gradients when saturated.

   to address the vanishing gradient problem, let us discuss another
   non-linear activation function known as the rectified linear unit
   (relu) which is a lot better than the previous two id180
   and is most widely used these days.

5.3. rectified linear unit (relu)

   figure: relu activation function figure: relu derivative

   relu is half-rectified from the bottom as you can see from the figure
   above. mathematically, it is given by this simple expression

       \[ f(x) = \max(0,x) \]

   this means that when the input x < 0 the output is 0 and if x > 0 the
   output is x. this activation makes the network converge much faster. it
   does not saturate which means it is resistant to the vanishing gradient
   problem at least in the positive region ( when x > 0), so the neurons
   do not backpropagate all zeros at least in half of their regions. relu
   is computationally very efficient because it is implemented using
   simple thresholding. but there are few drawbacks of relu neuron :
    1. not zero-centered: the outputs are not zero centered similar to the
       sigmoid activation function.
    2. the other issue with relu is that if x < 0 during the forward pass,
       the neuron remains inactive and it kills the gradient during the
       backward pass. thus weights do not get updated, and the network
       does not learn. when x = 0 the slope is undefined at that point,
       but this problem is taken care of during implementation by picking
       either the left or the right gradient.

   to address the vanishing gradient issue in relu activation function
   when x < 0 we have something called leaky relu which was an attempt to
   fix the dead relu problem. let   s understand leaky relu in detail.

5.4. leaky relu

   figure : leaky relu activation function

   this was an attempt to mitigate the dying relu problem. the function
   computes

       \[ f(x) = max(0.1x, x) \]

   the concept of leaky relu is when x < 0, it will have a small positive
   slope of 0.1. this function somewhat eliminates the dying relu problem,
   but the results achieved with it are not consistent. though it has all
   the characteristics of a relu activation function, i.e.,
   computationally efficient, converges much faster, does not saturate in
   positive region.

   the idea of leaky relu can be extended even further. instead of
   multiplying x with a constant term we can multiply it with a
   hyperparameter which seems to work better the leaky relu. this
   extension to leaky relu is known as parametric relu.

5.5. parametric relu

   the prelu function is given by

       \[ f(x) = \max(\alpha x, x) \]

   where \alpha is a hyperparameter. the idea here was to introduce an
   arbitrary hyperparameter \alpha , and this  \alpha can be learned since
   you can backpropagate into it. this gives the neurons the ability to
   choose what slope is best in the negative region, and with this
   ability, they can become a relu or a leaky relu.

   in summary, it is better to use relu, but you can experiment with leaky
   relu or parametric relu to see if they give better results for your
   problem

5.6. swish

   figure: swish activation function

   also known as a self-gated activation function, has recently been
   released by researchers at google. mathematically it is represented as

       \[ \sigma(x) = \frac{x}{1 + e^{-x}} \]

   according to the [33]paper, the swish activation function performs
   better than relu

   from the above figure, we can observe that in the negative region of
   the x-axis the shape of the tail is different from the relu activation
   function and because of this the output from the swish activation
   function may decrease even when the input value increases. most
   id180 are monotonic, i.e., their value never decreases
   as the input increases. swish has one-sided boundedness property at
   zero, it is smooth and is non-monotonic. it will be interesting to see
   how well it performs by changing just one line of code.

subscribe & download code

   if you liked this article and would like to download code (c++ and
   python) and example images used in other posts of this blog, please
   [34]subscribe to our newsletter. you will also receive a free
   [35]id161 resource guide. in our newsletter, we share opencv
   tutorials and examples written in c++/python, and id161 and
   machine learning algorithms and news.

   [36]subscribe now

   filed under: [37]deep learning, [38]machine learning, [39]theory tagged
   with: [40]activation function, [41]leaky relu, [42]parameteric relu,
   [43]relu, [44]sigmoid, [45]swish
   (button) load comments

   search this website ____________________ search

join course

   [46]id161 for faces

resources

   [47]download code (c++ / python)

disclaimer

   this site is not affiliated with opencv.org

   i am an entrepreneur with a love for id161 and machine
   learning with a dozen years of experience (and a ph.d.) in the field.

   in 2007, right after finishing my ph.d., i co-founded taaz inc. with my
   advisor dr. david kriegman and kevin barnes. the scalability, and
   robustness of our id161 and machine learning algorithms have
   been put to rigorous test by more than 100m users who have tried our
   products. [48]read more   

recent posts

     * [49]image inpainting with opencv (c++/python)
     * [50]hough transform with opencv (c++/python)
     * [51]xeus-cling: run c++ code in jupyter notebook
     * [52]pose detection comparison : wrnchai vs openpose
     * [53]gender & age classification using opencv deep learning (
       c++/python )

   copyright    2019    big vision llc

references

   visible links
   1. https://www.learnopencv.com/feed/
   2. https://www.learnopencv.com/comments/feed/
   3. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/feed/
   4. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/
   5. https://www.learnopencv.com/best-project-award-computer-vision-for-faces/
   6. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/
   7. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/&format=xml
   8. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/#genesis-nav-primary
   9. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/#genesis-content
  10. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/#genesis-sidebar-primary
  11. https://www.learnopencv.com/
  12. https://www.learnopencv.com/
  13. https://www.learnopencv.com/about/
  14. https://www.learnopencv.com/computer-vision-resources
  15. https://www.learnopencv.com/computer-vision-machine-learning-artificial-intelligence-consulting/
  16. http://courses.learnopencv.com/
  17. https://www.learnopencv.com/author/adityasharma/
  18. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/#disqus_thread
  19. https://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/
  20. https://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/
  21. https://www.learnopencv.com/deep-learning-using-keras-the-basics/
  22. https://www.learnopencv.com/understanding-feedforward-neural-networks/
  23. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/
  24. https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/
  25. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/
  26. https://www.learnopencv.com/keras-tutorial-using-pre-trained-id163-models/
  27. https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/
  28. https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models
  29. https://www.learnopencv.com/understanding-feedforward-neural-networks/
  30. https://www.tutorialspoint.com/artificial_intelligence/artificial_intelligence_neural_networks.htm
  31. https://www.learnopencv.com/wp-content/uploads/2017/10/neuron-diagram.jpg
  32. https://www.learnopencv.com/wp-content/uploads/2017/10/gradient-descent-2d-diagram.png
  33. https://arxiv.org/abs/1710.05941v1
  34. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  35. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  36. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  37. https://www.learnopencv.com/category/deep-learning/
  38. https://www.learnopencv.com/category/machine-learning/
  39. https://www.learnopencv.com/category/theory/
  40. https://www.learnopencv.com/tag/activation-function/
  41. https://www.learnopencv.com/tag/leaky-relu/
  42. https://www.learnopencv.com/tag/parameteric-relu/
  43. https://www.learnopencv.com/tag/relu/
  44. https://www.learnopencv.com/tag/sigmoid/
  45. https://www.learnopencv.com/tag/swish/
  46. https://courses.learnopencv.com/p/computer-vision-for-faces
  47. https://bigvisionllc.lpages.co/leadbox/143044973f72a2:173c9390c346dc/5720147234914304/
  48. https://www.learnopencv.com/about/
  49. https://www.learnopencv.com/image-inpainting-with-opencv-c-python/
  50. https://www.learnopencv.com/hough-transform-with-opencv-c-python/
  51. https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/
  52. https://www.learnopencv.com/pose-detection-comparison-wrnchai-vs-openpose/
  53. https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/

   hidden links:
  55. https://www.learnopencv.com/wp-content/uploads/2017/10/biological-neural-network.jpg
  56. https://www.learnopencv.com/wp-content/uploads/2017/10/linear-activation-function-1.png
  57. https://www.learnopencv.com/wp-content/uploads/2017/10/non-linear-activation-function.png
  58. https://www.learnopencv.com/wp-content/uploads/2017/10/xor.png
  59. https://www.learnopencv.com/wp-content/uploads/2017/10/sigmoid-activation-function.png
  60. https://www.learnopencv.com/wp-content/uploads/2017/10/sigmoid-derivative.png
  61. https://www.learnopencv.com/wp-content/uploads/2017/10/tanh-1.png
  62. https://www.learnopencv.com/wp-content/uploads/2017/10/tanh-derivative.png
  63. https://www.learnopencv.com/wp-content/uploads/2017/10/relu-activation-function-1.png
  64. https://www.learnopencv.com/wp-content/uploads/2017/10/relu-derivative.png
  65. https://www.learnopencv.com/wp-content/uploads/2017/10/leaky-relu-activation.png
  66. https://www.learnopencv.com/wp-content/uploads/2017/10/swish.png
