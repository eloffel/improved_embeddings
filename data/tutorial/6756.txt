lorelei language packs: data, tools, and resources for technology 

development in low resource languages 

stephanie strassel and jennifer tracey 

linguistic data consortium 
3600 market street, suite 810 

philadelphia, pa 19104 

e-mail: strassel@ldc.upenn.edu, garjen@ldc.upenn.edu  

abstract 

in this paper, we describe the textual linguistic resources in nearly 3 dozen languages being produced by linguistic data consortium 
for  darpa   s  lorelei  (low  resource  languages  for  emergent  incidents)  program.  the  goal  of  lorelei  is  to  improve  the 
performance of human language technologies for low-resource languages and enable rapid re-training of such technologies for new 
languages, with a focus on the use case of deployment of resources in sudden emergencies such as natural disasters. representative 
languages have been selected to provide broad typological coverage for training, and surprise incident languages for testing will be 
selected  over  the  course  of  the  program.  our  approach  treats  the  full  set  of  language  packs  as  a  coherent  whole,  maintaining 
lorelei-wide specifications, tag sets and guidelines, while allowing for adaptation to the specific needs created by each language. 
each representative language corpus, therefore, both stands on its own as a resource for the specific language and forms part of a 
large multilingual resource for broader cross-language technology development.  
 
keywords: low resource languages, multilingual resources, situational awareness 

1.  introduction 

 

of 

the 

tools, 

human 

required 

performance 

language  processing 

the  goal  of  darpa   s  lorelei  (low  resource 
languages  for  emergent  incidents)  program  is  to 
improve 
language 
technologies  for  low-resource  languages,  particularly  in 
the  context  of  a  rapidly  emerging  and  quickly  evolving 
situation  like  a  natural  disaster  or  disease  outbreak. 
lorelei  systems  will  be 
to  process 
information  about  topics,  entities,  events  and  sentiment 
found in the lorelei data, with the goal of providing 
situational  awareness  within  days  or  even  hours  of  the 
outbreak of an incident.  
linguistic  data  consortium  is  building  text  language 
packs  for  lorelei 1 ,  comprising  data,  annotations, 
natural 
lexicons  and 
grammatical resources for 23 representative languages 
as  well  as  12  incident  languages,  listed  in  table  1. 
representative  languages  (rl)  have  been  selected  to 
provide  broad  typological  coverage,  while  incident 
to  enable 
languages 
system 
development 
and 
capabilities.  the 
incident 
languages remains unknown to performers until the start 
of the evaluation.  
there 
language 
technology  for  low  resource  languages  (lrls).  the 
iarpa  babel  program 
in 
automatic  speech  recognition  (asr)  and  keyword 
spotting  (kws)  system  performance,  and  language 
packs  for  babel  primarily  consist  of  transcribed  and 
untranscribed speech in a variety of acoustic conditions 
(iarpa  2016).  while  babel 
focuses  on  general 
improvements  to  speech  processing  technology  for 
                                                             
1 a small amount of speech data will also be created for 
each  lorelei  language,  under  a  separate  effort  by 
appen. 

testing  of  lorelei 
choice  of 
evaluation 

(il)  have  been 

is  a  growing 

improvements 

in  human 

selected 

interest 

targets 

lrls, lorelei language packs (and the corresponding 
technology  evaluations)  are  designed  specifically  with 
the  goal  of 
for  situational 
awareness in emergent situations, and their composition 
reflects this.  
year 1 

technology 

improved 

year 2 

representative 

languages 

0: uzbek 
1: turkish 

representative 

languages 
13: akan (twi) 
14: bengali 

2: hausa 

15: hindi 

3: amharic 

16: indonesian 

4: arabic 

17: swahili 

5: farsi 

18: tagalog 

6: hungarian 

19: tamil 

7: mandarin 

20: thai 

incident 
languages 

1: uzbek 
2: mandarin 
3: y1 eval 
(undisclosed) 
4-5: y1 dev 
(undisclosed) 
6: y2 eval 
(undisclosed) 
7-8: y2 dev 
(undisclosed) 
9: y3 eval 
(undisclosed) 
10-11: y3 dev 
(undisclosed) 
12: y4 eval 
(undisclosed) 

21: wolof 
22: zulu 

8: russian 
9: somali 
10: spanish 
11: vietnamese 
12: yoruba 

 
 
 

 
 
 
 

table 1: lorelei program languages 

 
this  focus  also  provides  a  contrast  between  lorelei 
and ldc   s earlier work building language packs for the 
reflex  less  commonly  taught  languages  (lctl) 
project  (simpson  et.  al.  2008).  while  reflex  and 
lorelei  language  packs  have  much  in  common, 

3273

lorelei adds several new kinds of annotation as well 
as  a  surprise  language  element,  all  with  an  eye  toward 
the  lorelei  use  case.  finally,  ldc   s  approach  to 
lorelei  resource  creation  treats  the  full  set  of  35 
language  packs  as  a  coherent  whole,  in  order  to  enable 
research approaches focused on rapid adaptation through 
use  of 
from 
related-language  resources.  lorelei  specifications, 
guidelines and tag sets have been informed by language 
universals  while 
language-specific 
adaptation  as  needed.  the  sections  that  follow  describe 
our  approach  to  building  the  lorelei  text  language 
packs in detail. 

language  universals  and  projection 

allowing 

for 

2.  representative language packs 

two 

include 

text,  several 

text,  parallel 

representative  language  packs  for  lorelei  contain 
monolingual 
types  of 
annotation,  tools  for  text  processing,  segmentation,  and 
entity  tagging,  as  well  as  lexicons  and  grammatical 
sketches.  annotations 
types  of  entity 
annotation,  noun  phrase  chunking,  simple  semantic 
annotation of limited predicates and argument roles, and 
morphological and part-of-speech analysis. in most cases 
language  pack  components  are  newly  produced  for 
lorelei,  but  the  first  three  language  packs  (uzbek, 
turkish  and  hausa)  were  produced  under  the  darpa 
bolt program and therefore may not be fully compliant 
with the final set of lorelei requirements. moreover, 
for a handful of languages some portion of the language 
pack is drawn from data first produced in reflex, with 
some updating of the content to meet current lorelei 
standards. the sections below describe the rl packs in 
more detail.  

2.1  monolingual text 
lorelei requires collection of at least 2 million words 
of monolingual text for each rl, including news (50%), 
blogs and discussion forums (40%), and microblogs like 
twitter (10%). a special emphasis is placed on collection 
of  data  in  the  lorelei  domain  (natural  disasters  and 
the like), to support the requirement that at least half of 
the  material  selected  for  downstream  translation  and 
annotation  is  in-domain.  data  scouts  in  each  language 
search  the  web  for  suitable  sources,  designating  entire 
data  sources  or  websites  for  collection  as  well  as 
selecting  individual  documents  that  discuss  specific 
incidents.  each  website  or  document  selected  for 
inclusion  in  the  corpus  is  then  harvested  and  reviewed 
for  intellectual  property  rights  (ipr)  issues,  using  an 
extension  of  ldc   s  webcol 
first 
developed in the darpa bolt program (garland et. al. 
2012).  for  sources  like  twitter  whose  terms  do  not 
permit redistribution by ldc, we release a list of urls 
or  ids  plus  utilities  for  performers  to  harvest,  process 
and validate the data themselves. 
harvested 
and 
sentence-segmented using a combination of open source 
tools and approaches developed by ldc for lorelei. 

automatically 

infrastructure 

tokenized 

text 

is 

structure 

in  a  simple,  plain  utf-8 

data is converted to utf-8 encoding, and original files 
are converted into a variety of derived formats to support 
subsequent  translation,  annotation  and  distribution.  the 
conversion  process  is  intended  to  address  things  like 
variable (lack of) compliance with established standards 
for  markup,  character  encoding,  orthography  and 
punctuation;  absence  or  flexibility  of  orthographic 
standards  in  some  languages;  and  unknown  scope  of 
variability in data input methods used by content authors. 
procedurally,  we  create  separate  data  streams  for 
linguistic  content  versus  structural  features  for  use  in 
ldc   s  internal  data  pipeline.  raw  linguistic  content  is 
preserved 
text  only 
representation.  essential  document 
and 
metadata  (e.g.  paragraph  boundaries,  <quote>  tags  in 
threaded  conversations)  is  kept  in  a  uniform  stand-off 
xml.  we  then  create  a  recombined  data  stream  for 
inclusion in language packs, with id121, sentence 
segmentation  and  other  post-processing  applied  as 
required.  
processed  linguistic  content  files  are  run  through  the 
google  cld2  language  detector  with  some  subsequent 
manually  verification  of  language  content.  at  least  for 
some 
languages,  code  switching  and  orthographic 
variation  are  expected;  for  instance  uzbek  text  may  be 
written 
in  cyrillic,  latin  or  perso-arabic  script 
depending  on  the  source.  this  presents  a  challenge  for 
lid,  as  does  the  prevalence  of  highly  informal  short 
message  data  like  twitter.  therefore,  automatic  lid  is 
intended to identify pervasive problems with a given data 
source,  and  some  amount  of  language  mixture  in  the 
monolingual  text  for  representative  language  packs  is 
expected.   
finally,  we  standardize  file  naming  and  register  each 
document  in  a  lorelei-wide  tracking  database.  a 
portion of the processed monolingual data is added to the 
translation  queue,  and  a  portion  of  the  files  in  the 
translation  queue  are  further  added  to  the  annotation 
queue, with some manual review to confirm that the data 
is suitable for further treatment.  

2.2  parallel text 
each  representative  language  requires  900,000  words 
from  the  monolingual  text  corpus  to  be  translated  into 
english.  in  addition,  a  fixed  set  of  approximately 
100,000  words  of  text  is  translated  from  english  into 
each  representative 
language.  this     english  core    
includes  a  set  of  domain-focused  news  text  as  well  as 
some  general  news,  a  phrasebook  containing  everyday 
colloquial phrases, and an elicitation corpus of sentences 
designed  by  a  team  at  carnegie  mellon  university  to 
elicit various linguistic structures (alvarez et. al. 2006). 
translations  of  the  monolingual  data  are  acquired 
through a combination of methods. to the greatest extent 
possible, we locate and harvest parallel text from the web 
using ldc   s bilingual internet text search (bits) tool; 
the harvested parallel text is then sentence-aligned using 
champollion (ma 1999; ma & liberman 2006). we also 
use id104 to obtain a portion of the translation 

3274

in  id104 

for  some  languages,  although  not  all  languages  have 
sufficient  numbers  of  crowd  workers  with  the  required 
skills to perform translation. however, based on previous 
research 
translation,  we  expect 
coverage of at least some of the representative languages 
(pavlick,  et  al.  2014),  although  recent  changes  to 
amazon   s  policies  for  workers  on  mechanical  turk 
appear to have reduced the number of workers available 
through that site. for languages where id104 is 
not viable, and for portions of the data that require more 
translation expertise (like the elicitation corpus), we rely 
on  professional  translators.  prior  to  manual  translation 
(whether  crowdsourced  or  professional),  data 
is 
automatically  segmented,  and  translators  preserve  the 
alignment between the source language segment and the 
resulting target language segment.  
the heavy reliance on found data and id104 in 
lorelei  compared 
recent  darpa  machine 
translation programs like gale and bolt yields highly 
variable translation quality, reflecting the lorelei use 
case  in  which  few  high-quality,  high-volume  resources 
are likely to be available at the start of an incident. ldc 
conducts limited quality control on the translations, using 
methods  that  vary  based  on  the  translation  type,  but 
which  are  primarily  automated  rather  than  manual  and 
which  serve  primarily  to  exclude  egregiously  bad  data 
rather than improve the quality to a true    gold standard   . 

to 

2.3  annotation 
lorelei  rls  also  require  several  types  of  linguistic 
annotation,  which  vary  in  complexity  and  level  of 
required  linguistic  knowledge.  a  portion  of  data  in  the 
translation  pool  for  each  representative  language  is 
selected for manual annotation.  most of the data comes 
from  the  source  data  translated  into  english  (following 
the  same  genre  and  domain  distribution  as  the  general 
translation  pool),  but  approximately  2000  words  per 
language  is  drawn  from  the  english  core  set  translated 
from  english,  thereby  creating  a  small  set  of  parallel 
annotated  data  across  all  rls.  this  english  core  set  is 
not  included  in  the  annotation  for  turkish,  uzbek  or 
hausa  since  those  language  packs  were  created  under 
bolt.  to  the  greatest  extent  possible,  the  same  set  of 
data  is  annotated  for  all  lorelei  annotation  tasks,  as 
shown in figure 1 below.  

figure 1: representative language packs composition 

and 

require 

delivering 

annotation 

issues.  the  project 

language  packs,  which  means 

one  of  the  key  challenges  in  building  lorelei  rl 
packs  is  the  need  to  rapidly  train  native  speakers  in 
nearly two dozen languages to perform annotation tasks 
fairly  sophisticated  understanding  of 
that 
linguistic 
timelines  are  quite 
compressed, with only a few months turnaround between 
completed, 
starting 
quality-checked 
that 
annotator training also has to be very efficient. further, 
for  at  least  some  lorelei  languages,  the  pool  of 
available  annotators 
is  already  small,  and  finding 
annotators  with  a  strong  linguistics  background  isn   t 
feasible.  accordingly,  annotation 
tasks  have  been 
designed with a novice, non-linguist annotator in mind. 
each  annotation  task  is  broken  down  into  a  series  of 
simple decision points (e.g. is there a name to tag in this 
sentence? where does the name start and end? is it the 
name  of  a  person,  organization,  geopolitical  entity  or 
location?  etc.).  annotator  training,  guidelines  and  user 
interfaces  have  been  designed  to  directly  reflect  these 
decision points.  

types  used 

entity annotation 

location/facility  names, 

2.3.1. 
we  perform  two  types  of  manual  entity  annotation  for 
the  representative  languages.  in  simple  named  entity 
(sne)  annotation,  text  is  labeled  for  person  names, 
organization  names, 
and 
geopolitical  entity  names.  full  entity  (fe)  annotation 
includes nominal and pronominal entities in addition to 
names  and  includes  annotation  of  titles  as  well  as  the 
other  entity 
in  simple  named  entity 
annotation.  full  entity  annotation  also 
includes 
within-document  entity  co-reference.  for  both  simple 
named  entity  and  full  entity  annotation,  entities  that 
can 
or 
locations/facilities are tagged depending on their usage 
in a given sentence. embedded names are not annotated, 
so  that  the  name     africa     would  not  be  separately 
labeled in the organization name africa rice center. 
a total of 75,000 words per language is labeled for sne, 
while an additional 25,000 words is labeled for fe; this 
25kw fe set includes the 2000-word english core. this 
labeled  data  is  in  turn  used  to  train  the  named  entity 
taggers described in section 2.4 below. 

organizations 

function 

either 

as 

simple semantic annotation 

2.3.2. 
all of the data labeled for fe is also subject to simple 
semantic  annotation  (ssa).  the  goal  of  ssa  is  to 
capture  a  basic  understanding  of  what  is  happening 
and/or  what  is  the  case  in  a  sentence.  using  broad 
predicate  and  argument  categories,  annotators  label 
specified  types  of  acts  (events)  and  states  (situations) 
along  with  their  associated  arguments.  procedurally, 
annotators  first  identify  a  taggable  act  or  state  in  the 
sentence and select the    trigger word    that most directly 
evokes  the  act  or  state.  this  is  typically  the  head  of  a 
verb  phrase  or  noun  phrase,  but  annotators  may  select 
multiple  words  when  this  seems  intuitively  preferable 
(for  instance  in  the  case  of  multi-word  expressions). 

3275

annotators then label three types of arguments for each 
act  or  state,  selecting  the  most salient  and  informative 
minimal  text  string  for  each  argument  type.  an  agent 
argument is defined as the entity, event or situation that 
does  or  causes  an  act/state  to  occur.  patient  is  the 
undergoer, receiver or experiencer of an act/state, or the 
goal  of  an  act/state.  place  is  the  place  where  the 
act/state  occurred,  place  headed  to,  or  place  leaving 
from. 
the current version of ssa limits annotation to physical 
acts  and  domain-relevant  states.  physical  acts  are 
concrete events, actions or activities that take place in the 
observable, material world. this includes human-caused 
and  non-human  caused  events  but  excludes  abstract, 
cognitive 
events.  domain- 
relevant  states  are  situations  that  describe,  are  caused 
by  or  provide  information  about  lorelei  domain 
events, like natural disasters. this restriction to physical 
acts and domain-relevant states was adopted after the 
creation of the turkish and uzbek language packs, where 
we  found  that  the  inclusion  of  abstract  acts  and 
non-domain  states  slowed  down  annotation 
to  an 
unacceptable pace.   
an  example  of  a  turkish  sentence  annotated  for  ssa 
appears in figure 2 below. 
 

and  verbal/attribution 

figure 2: turkish sentence labeled for ssa 

np chunking 

2.3.3. 
a 10,000-word subset of the data labeled for both fe and 
ssa,  including  the  2000-word  english  core,  is  further 
labeled  to  identify  maximal,  non-overlapping  noun 
phrases. some nps are also decomposed to mark smaller 
nps  within  them,  resulting  in  annotation  like  the 
following sentence: 
 

[the  government]  will  send  [aid  workers]  to  [[the 
region]  [that]  was  struck  by  [the  earthquake]  [last 
month]].  
annotators  follow  surface  syntactic  structure,  applying 
specific  tests  such  as  constituency,  to  determine  which 
nps  to  mark.  unlike  entity  annotation  tasks,  names 
within larger nps are extracted/labeled as their own nps 
when syntactic structure dictates, as in: [[university] of 
[pennsylvania]]. 

segmented, 

individually 

is  part-of-speech 

2.3.4.  morph and pos annotation 
the  final  and  most  challenging  annotation  task  for  the 
rl  packs 
and  morphological 
annotation. the same 10kw labeled for np chunking is 
also subject to morph/pos annotation.  
in the three lorelei languages produced under bolt 
(uzbek,  hausa  and  turkish),  our  approach 
to 
morphological  annotation  was  tightly  integrated  with 
creation  of  language-specific  analyzers  at  ldc  (kulick 
and bies, 2016). for subsequent lorelei rl packs we 
will  utilize  a  universal  analyzer  being  developed  by 
lorelei  performers,  rather 
than  creating  custom 
analyzers  for  each  language.  the  universal  analyzer  is 
expected to produce multiple (possibly ranked) analyses 
for  each  token,  consisting  of  a  lemma  plus  features 
(possibly  with 
labeled 
morphemes).    annotators  select  the  best  solution  from 
the list, or choose "unanalyzable" if the analyzer has no 
correct  solution  for  the  token.  part-of-speech  labels  are 
not  directly  annotated,  but  are  instead  derived  from  the 
morph  annotation.  in  cases  of  unanalyzable  tokens, 
annotators  choose  the  appropriate  pos  label  from  the 
   google  12     universal  pos  set  (petrov  et.  al.  2011). 
prior 
to  manual  annotation,  mappings  are  created 
between 
the  universal  analyzer  feature  set  and  a 
simplified  set  of  labels  that  are  more  suitable  for 
non-linguist human annotators.  
one additional type of morphological annotation appears 
in the turkish and uzbek representative language packs: 
morpheme alignment. this task was designed to identify 
translational  correspondence  at  the  morpheme  level  in 
parallel text. although the task was completed for these 
two  preliminary 
it  proved  extremely 
challenging  and  costly  for  non-linguist  annotators. 
moreover, 
from  lorelei  performers 
suggested that this resource was lower priority than some 
other  options  under  consideration;  therefore,  a  decision 
was  made  to  exclude  morphological  annotation  from 
remaining  rl  packs  in  favor  of  additional  evaluation 
resources.  

languages, 

feedback 

2.3.5. 
language universal annotation principles  
to encourage consistency across different rl packs, and 
across  different  annotation  tasks  within  each  rl  pack, 
we have enumerated the types of language features that 
directly  affect  lorelei  annotation  decisions;  this 
includes 
the  presence  or  absence  of 
whitespace  around  words,  the  use  of  clitics  and/or 
contractions, and the syntax of possessive noun phrases 

things 

like 

3276

in 

the 

language-specific  version  of 

(genitive case, possessive case, compounding, idafa-like 
constructions, adpositions, etc.). these issues are initially 
documented  in  the  grammatical  sketches  (section  2.5) 
for  each  language,  and  then  folded  into  annotation 
guidelines as required.   
all annotation tasks  begin with a language-independent 
guidelines template, laying out the expected sections for 
each task, including generic verbiage that can be copied 
into  language-specific  versions  of  the  guidelines  and 
placeholders  for  language-specific  examples.  but  the 
template  guidelines  also  contain  indications  of  areas 
where 
language-specific  decisions  must  be  made, 
outlining  the  various     annotation  rules     that  can  be 
invoked 
the 
guidelines,  depending  on  the  particular  features  of  a 
given language. for instance, one such rule states that: 
 
pronominal  mentions  of  entities  are  only  annotated  if 
they  are  separate  words.  if  they  appear  as  verb 
morphology, they are not annotated.  
 
implementation  of  these  rules  is  designed  to  produce 
more  consistent  treatment  of  similar  phenomena  across 
languages. 
care  was  also  taken  to  ensure  that  similar  concepts 
across different annotation tasks are treated in the same 
way,  unless  there  is  a  need  for  variation  in  order  to 
support the goals of a specific task. for example, name is 
a  relevant  concept  in  multiple  annotation  tasks,  and 
decisions  about  such  details  as  whether  to  include  a 
definite  article  in  the  name  of  an  organization  such  as 
   the  red  cross     are  synchronized  across  tasks,  so  that 
annotators  in  simple  semantic  annotation  (ssa)  and 
both  simple  named  entity  (sne)  and  full  entity  (fe) 
annotation  exclude  the  definite  article  from  the  extent; 
noun phrase (np) annotators, on the other hand, include 
the definite article because it is clearly part of the noun 
phrase.  figure  3  illustrates  the  treatment  of  named  and 
nominal  entity  extents  across  different  annotation  tasks 
for an english example. 

 

figure 3: cross-task treatment of entity extents 

2.4  tools 
in  addition  to  monolingual,  parallel  and  annotated  text, 
lorelei  rl  packs  include  some  simple  nlp  tools. 
these  tools  are  not  intended  to  provide  state  of  the  art 
results, but rather to simulate the kinds of baseline tools 
that  may  be  available  at  the  outbreak  of  a  disaster 
situation involving some new language. 
to the extent that original sources of data use encodings 

segmenters. 

other  than  utf-8,  we  provide  a  simple  encoding 
converter.  where  needed,  we  also  create  name 
transliterators,  which  are  integrated  with  the  lexicon  to 
ensure  coverage  of  most  common  name  variants. 
language  packs  also  include  baseline  tokenizers  and 
sentence 
for  whitespace-delimited 
languages we create a custom tokenizer that operates on 
a  series  of  regular  expressions  that  dictate  how  to 
tokenize; special attention is given to handling web text 
artifacts  like  hash  tags  and  urls.  for  non-whitespace 
languages we rely on existing widely-used tokenizers.  
sentence segmentation utilizes an implementation of the 
punkt  algorithm  based  on  the  version  found  in  nltk 
(kiss et. al. 2006). this is an unsupervised, re-trainable 
algorithm and considerable tuning is required to handle 
the  types  of  informal  data  prevalent  in  lorelei. 
finally,  for  each  representative  language  we  create  a 
custom  conditional  random  field-based  named  entity 
tagger.   
in  addition  to  the  basic  nlp  tools,  we  also  provide 
utilities  for  downloading  and  processing  text  from  the 
web  so  that  lorelei  performers  can  replicate  ldc   s 
data  processing  pipeline  internally.  this  is  particularly 
important  given  that  during  a  real-world  (or  simulated) 
disaster  situation,  systems  will  be  expected  to  process 
data     in  the  wild     instead  of  relying  on  pre-processed 
data.  

is 

found 

tightly 

lexicon 

database 

resources 

2.5  lexicon and grammatical sketches 
each rl pack also includes a lexicon encompassing an 
inventory  of  at  least  10,000  headwords/lemmas  with 
part-of-speech,  english  gloss,  and  optionally  (where 
appropriate  and  available)  morphology.  the  lexicon  is 
comprised  of 
(existing  online 
dictionaries,  etc.)  and  existing  ldc  resources,  with 
limited  manual  effort  to  create  new  entries  as  needed. 
the 
linked  with 
morphological annotation of selected text, so that we can 
measure  the  coverage  in  terms  of  both  type  count  and 
token  count.  manual  annotation  to  supplement  lexical 
entries is driven primarily by token frequency, to ensure 
that  the  coverage  target  can  be  reached  as  quickly  as 
possible.  
language  packs  also  include  grammatical  sketches 
intended  to  convey  practical  information  about  how  to 
work  with  the  language,  focusing  on  paradigms  and 
basic  grammatical  descriptions  over  deep  theoretical 
discussions or nuanced explication of exceptional cases. 
sketches  for  all  languages  follow  a  single  template,  so 
that  the  same  topics  are  covered  across  languages  and 
can  be  found  in  predictable  sections  within  the  sketch. 
issues 
impacting  annotation  are  documented  first, 
addressing  questions  like:    are determiners attached to 
nouns?  is  there  white  space  around  case  markers  and 
adpositions?    describe  adjectival  forms  of  loc,  org, 
and  gpe  names  such  as     american   .  the  answers  to 
annotation-relevant questions are passed on to annotation 
teams for use in guidelines development, and are added 
to one of the eight chapters of the grammatical sketch: 

3277

     about 

the 

language 

of 
classification, iso code, word order, etc.) 
variation, 

(characters, 

(overview 

     orthography 

basics: 

word 

boundaries, etc.) 

     encoding (unicode chart, etc.) 
     morphology  (inflection  and  productive  derivational 
classes, 

morphology 
morphophonemics where relevant to orthography) 

major 

word 

     syntax  (constituent  order,  phrasal  and  clausal 

for 

     specialized  subgrammars  (personal  names  and 

phenomena) 

locations, numbers) 

     variation  (register/dialect  where  relevant  to  text, 

codeswitching/borrowing) 

sketch 

template  also 

     references 
the  grammatical 
includes 
suggestions to the sketch author for the relative level of 
effort and approximate number of pages for each chapter, 
with  a  targeted  page  length  of  around  50  pages  per 
language. sketches are typically authored by theoretical 
linguists  in  consultation  with  native  speakers,  and  are 
independently  reviewed  for  structural  and  content 
completeness and cohesion prior to distribution in the rl 
packs. 

3.  incident language packs 

incident 

low-resource 

involving  a 

in addition to the representative language packs, we will 
produce  language  packs  for  12  incident  languages  (il) 
over  the  course  of  the  program,  with  one  il  per  year 
designated  for  evaluation  and  the  remainder  to  be  used 
for system development. il packs are intended to reflect 
the kind of data that might be available at the outbreak of 
an 
language. 
compared  to  representative  language  packs,  il  packs 
contain smaller volumes of monolingual text and found 
parallel 
text,  plus  an  assortment  of  grammatical 
resources.  additional  evaluation  data  is  created  for  one 
language  per  year.  lorelei  performers  are  evaluated 
annually  on  a  variety  of  component  tasks;  in  2016, 
evaluations  include  machine  translation,  named  entity 
recognition  and  topic  labeling.  lorelei 
task 
evaluations  are  conducted  by  the  national  institute  of 
standards and technology (nist). a corresponding open 
evaluation  campaign,  lorehlt,  allows  non-lorelei 
performers  to  participate  in  the  same  evaluations  using 
the same data conditions (nist 2016). 
the  lorelei  evaluations 
include 
three  different 
checkpoints at which system output is delivered to nist. 
the  amount  of  data  available  for  system  training  and 
development prior to each checkpoint varies, as does the 
amount of time between checkpoints. prior to the start of 
the  evaluation,  performers 
receive  an  encrypted 
evaluation  incident  language  pack.  the  il  pack 
components described in section 3.1 are de-encrypted at 
the start of the evaluation, as is the monolingual il test 
set described in section 3.2. additional monolingual il 
text  that  post-dates  the  specific  evaluation  incident  is 
de-encrypted  just  after  the  first  checkpoint,  with  more 
post-incident 
as 

il  monolingual 

as  well 

text 

post-incident  english  monolingual  text  de-encrypted 
after the second checkpoint.  gold standard annotations 
on  the  test  set  are  used  by  nist  for  scoring  system 
submissions,  and  remain  blind  to  performers  until  after 
the evaluation. 

four 

share 

informal 

languages, 

larger  volumes  of  comparable 

3.1  common il components 
all  incident  language  packs,  whether  evaluation  or 
development 
components 
designated for system development and training.  
first,  il  packs  include  a  minimum  of  225,000  of 
monolingual text, nominally comprising 100kw of news, 
75kw of blogs, discussion forums or other informal text, 
and  50kw  of  microblogs.  some  ils  may  have  no 
available microblog text, in which case the other genres 
will  be  increased.  in  most  il  packs  the  amount  of 
monolingual text is exceeded by 500% or more. 
next,  il  packs  include  300,000  of  found  parallel  text, 
equally  divided  between  news, 
text  and 
microblogs.  it  is  important  to  note  that  this  is  found 
parallel  data  harvested  from  the  web;  no  manual  or 
crowdsourced  translations  are  included  in  the  il  packs. 
when  sufficient  volumes  of  found  parallel  text  are  not 
available, 
text  are 
provided.  
third, all il packs include a found il-english dictionary 
containing  at 
lemmas.  this  parallel 
dictionary  is  not  a  full-fledged  lexicon  of  the  type 
created for the representative language packs (section 
2.5),  and  the  quality  and  structure  of  this  component  is 
expected to be highly variable across languages.   
finally, all il packs include a set of found grammatical 
resources.  there  are  eight  types  of  allowable  resources 
for this category, of which at least five must appear in all 
incident  language  packs;  these  include  monolingual  il, 
regional  english  or  bilingual  il-english  gazetteers, 
bilingual  il-english  or  monolingual  il  grammars, 
monolingual  il  dictionaries  or  dictionaries  that  are 
parallel  with  a 
than  english,  and 
monolingual il primers. where possible we harvest the 
resource for direct inclusion in the il packs, or provide a 
url pointer where the resource can   t be harvested and 
redistributed. for ils that lack available digital or online 
resources  of  this  type,  we  acquire  hardcopies  for 
distribution to performers concurrent with distribution of 
the il packs. 

language  other 

least  10,000 

3.2  evaluation-only il components 
creation of the evaluation data begins with identification 
of  a  specific  real-world  incident  for  the  evaluation  il, 
e.g. a recent disaster that took place in the region where 
the  language  is  primarily  spoken.  using  the  selected 
evaluation 
incident  as  a  guide,  we  produce  a 
200,000-word monolingual test set in the evaluation il, 
with  half  of  the  data  drawn  from  news  and  half  drawn 
from  informal  text  and/or  microblogs.  approximately 
half  of 
is  lorelei 
domain-relevant, with a portion of that domain-relevant 
text  discussing  the  specific  evaluation  incident.  (note 

this  monolingual 

test  set 

3278

that all of the monolingual and parallel text described in 
section 3.1 must pre-date the evaluation incident, since 
that  data  is  to  be  used  for  system  development  and 
training.) 
a  portion  of  the  monolingual  test  set  (75kw  total, 
divided  across  the  genres  and  domains)  is  manually 
translated, with 4 independent translations per document. 
this  provides  reference  data  for  evaluation  of  machine 
translation technology.  
the entire 200kw test set is also labeled for the simple 
named entity annotation task, using the same guidelines 
that are used in the representative language packs. this 
provides  reference  data  for  evaluation  of  named  entity 
recognition.  a  portion  of  this  test  set  is  independently 
labeled  for  sne  by  two  separate  annotators,  to  provide 
baseline information about human agreement. 

or 

permanent 

semi-permanent 

situation  frame  annotation  for  topic 

3.2.1. 
labeling evaluation 
finally,  a  portion  of  the  200kw  test  set  is  labeled  for 
situation frames, providing reference data for evaluation 
of  topic  labeling.  for  each  document,  annotators  create 
one  or  more  situation  frames  describing     actionable    
situations  discussed  in  the  document.    annotators  label 
three  information  elements  per  situation  frame:  a 
situation  type  drawn  from  a  fixed  inventory,  with  one 
type per frame; a localization of the situation (limited to 
named entities in year 1); and any sentiment, emotion or 
cognitive states relevant to that situation.  
there are roughly a dozen situation frame types defined 
in  year  1,  covering  a  range  of  possible  incident  types. 
for instance, the type    infrastructure    is defined as any 
issue  involving  buildings,  roads,  bridges,  facilities,  or 
other 
physical 
infrastructure 
that  has  been  damaged  or  made 
non-operational;  when  this  kind  of  incident  is  detected 
the expected action from a mission planner might be to 
send  building  materials  or  equipment  to  the  scene. 
annotators  begin  situation  frame  annotation  by 
identifying  all  the  taggable  situations  in  the  document 
and assigning each one a type. multiple mentions of the 
same  event/situation  are  part  of  same  frame,  but  more 
than one frame with same situation type is possible. for 
instance,  if  the  document  discusses  a  landslide  that 
destroyed critical infrastructure in 2015, and compares it 
with  an  earthquake  that  also  destroyed  infrastructure  in 
2012, the annotator would produce two separate situation 
frames of the type    infrastructure   . completed, planned 
and  near-future  events  can  generate  situation  frames, 
but imagined hypothetical events cannot. this means that 
speculation  about  the  chances  of  a  meteor  impact 
destroying  all  the  bridges  in  new  york  city  does  not 
yield  an  infrastructure  situation  frame,  but  expressions 
of  concern  over  likely  destruction  of  bridges  in  a  town 
just hit by an earthquake does.   
the 
after 
document, 
adds  sentiment/ 
emotion/cognitive state (sec) attributes to the frame as 
needed  to  reflect  sec  that  is  conveyed  or  expressed  in 

taggable  frames 
then 

the 
annotator 

identifying  all 

the 

in 

the  document  about  the  situation.  there  are  three 
possible attributes for sec, outlined in table 2 below. 

table 2: sec attributes for situation frames 

to 

the 

frame, 

 
for instance, given the following sentence: 
 
local officials told id98 that they feared the town would 
experience  significant  damage  to  roads,  bridges  and 
other  transportation  infrastructure  in  the  flood-affected 
areas.  
 
annotators  would  add  a  negative  sec  attribute  to  the 
reflect 
infrastructure  situation 
fear 
expressed by the officials about this situation.   
finally,  annotators     localize     the  situation  frame  by 
linking  any  entity  associated  with  the  situation  to  the 
frame.  in  year  1,  localization  is  limited  to  entities  that 
are  named  somewhere  in  the  document.  so  in  the 
example  above,  even  though     the  town     isn   t  named 
within the sentence, as long as it is named somewhere in 
the document then that entity (which encompasses both 
   the  town     and  the  named  mentions  of  the  town)  is 
linked  to  the  situation  frame,  providing  information 
about the location of the infrastructure situation.  
language-specific  guidelines 
for  situation  frame 
annotation include detailed examples and rules of thumb 
for dealing with common challenges, for instance how to 
handle  cases  where  multiple  nested  geopolitical  and 
location  entities  are  mentioned  in  connection  with  the 
same situation, as in the following sentence: 
 
eyewitnesses  said  a 
the  village  of 
guinsaugon in the south of the philippine island of leyte. 
governor rosette lerias described the village as totally 
flattened with virtually all of housing destroyed. 
 
in  this  example,  the  village  of  guinsaugon  (gpe)  is 
located  on  the  island  leyte  (loc)  in  the  country 
philippines  (gpe).  the  guiding  rule  for  annotation  is 
that annotators should always tag the most specific entity 
that  is  associated  with  the  situation,  so  in  this  case  the 
guinsaugon entity is associated with the situation frame, 
while the leyte and philippines entities are not. 
as  with  the  sne  evaluation  data,  a  portion  of  the 
situation frame evaluation data is independently labeled 
by  two  separate  annotators  to  provide  baseline  human 
agreement numbers. 

landslide  hit 

4.  distribution of language packs 

all  completed  language  packs  (both  rl  and  il)  are 
subject to sanity checks and validation at ldc, followed 
by  independent  quality  control  by  the  university  of 
maryland  center  for  advanced  study  of  language 
(casl), prior to their release to lorelei performers. 

3279

text 

include 

completed 

language  id 
checks  on  monolingual 
verification  using  standard  character  id165  based 
methods, automated dictionary lookup and spot checking 
by native speakers. character sets are validated against a 
list  of  known  valid  code  points  for  language.  manual 
spot-checking  by  linguists  and/or  native  speakers  will 
also  be  used  to  identify  any  systematic  issues  with 
language  packs  concerning  content  of  domain  text; 
id121 or segmentation; parallel text accuracy and 
fluency;  consistency  of  pos  and  morph  tagsets;  and 
annotation  quality.  grammatical  sketches  are  also 
validated for stylistic and content issues. 
to  date  we  have 
and  distributed 
representative  language  packs  for  turkish,  hausa  and 
uzbek,  and  incident  language  packs  for  uzbek  and 
mandarin.  because  (parts  of)  of  these  language  packs 
were created under bolt they do not always reflect the 
current lorelei requirements. some components (e.g. 
simple  named  entity  annotations)  will  be  updated  to 
reflect  the  current  guidelines,  while  other  components 
(e.g. ssa and np annotation) will remain slightly out of 
alignment with current standards.  
representative  and  development  language  packs  are 
delivered to lorelei at the end of each program year. 
representative language packs are also deposited in the 
ldc  catalog  as  they  are  completed,  while  incident 
language  packs  are  published  after  they  are  no  longer 
sequestered 
in  lorelei  or  lorehlt 
evaluations.  lorelei  performers  and  ldc  members 
will receive language packs at no cost. members of the 
general  research  community  will  pay  a  minimal  fee  to 
defray the costs of data curation, storage and distribution. 
all  deliverables  are  provided  to  the  government  under 
ldc   s existing government-wide license. the first set of 
lorelei  language  packs  is  expected  to  appear  in 
ldc   s catalog in late 2016. 

for  use 

5.  acknowledgements 

this  material  is  based  upon  work  supported  by  the 
defense advanced research projects agency (darpa) 
under  contract  no.  hr0011-15-c-0123.  any  opinions, 
findings and conclusions or recommendations expressed 
in  this  material  are  those  of  the  author(s)  and  do  not 
necessarily reflect the views of darpa.  

6.  references 

genre-independent  language  technologies:  user- 
generated  content  in  bolt.  proceedings  of  lrec 
2012:  8th  international  conference  on  language 
resources and evaluation, istanbul, may 21-27. 

iarpa  babel  website,  retrieved  march  8,  2016. 
http://www.iarpa.gov/index.php/research-programs/ba
bel 

tibor kiss, jan strunkt. 2006. unsupervised multilingual 
computational 

detection. 

sentence 
linguistics 32: 455-525 

boundary 

seth  kulick,  ann  bies.  2016.  rapid  development  of 
morphological  analyzers  for  typologically  diverse 
languages.  proceedings  of  lrec  2016:  10th 
international conference on language resources and 
evaluation, portoro  , may 23-28. 

xiaoyi  ma.  2006.  champollion:  a  robust  parallel  text 
sentence  aligner.  proceedings  of  lrec  2006:  5th 
international conference on language resources and 
evaluation, genoa, may 22-28. 

xiaoyi ma, mark liberman. 1999. bits: a method for 
bilingual  text  search  over 
the  web  machine 
translation summit vii: singapore, september 13-17. 

nist lorehlt 2016 evaluations website, retrieved 

march 8, 2016. http://www.nist.gov/itl/iad/mig/ 
lorehlt16.cfm 

ellie  pavlick,  matt  post,  ann  irvine,  dmitry  kachaev, 
and  chris  callison-burch.  2014.  the  language 
demographics 
of  amazon  mechanical  turk. 
transactions  of  the  association  for  computational 
linguistics 2: 79-92. 

slav  petrov,  dipanjan  das,  ryan  mcdonald.  2011.  a 
universal  part-of-speech  tagset.  proceedings  of 
lrec  2012:  8th 
international  conference  on 
language  resources  and  evaluation,  istanbul,  may 
21-27. 

heather  simpson,  christopher  cieri,  kazuaki  maeda, 
kathryn  baker,  boyan  onyshkevych.  2008.  human 
language technology resources for less commonly 
taught  languages:  lessons  learned  toward  creation 
of  basic  language  resources.  proceedings  of  lrec 
2008,  workshop  on  collaboration:  interoperability 
between  people 
the  creation  of  language 
resources for less-resourced languages. 

in 

alison  alvarez,  lori  levin,  robert  frederking,  erik 
peterson,  simon  fung.  2006.  tools  for  elicitation 
corpus  creation.  proceedings  of  the  emeld   06 
workshop on digital language documentation:  tools 
and  standards:  the  state  of  the  art.    lansing,  mi.  
june 20-22. 

cdl2  website, 

retrieved  october 
https://github.com/cld2owners/cld2 

25, 

2015. 

darpa lorelei website, retrieved october 25, 2015. 
http://www.darpa.mil/program/low-resource-language
s-for-emergent-incidents 

jennifer garland, stephanie strassel, safa ismael, zhiyi 
song,  haejoong  lee.  2012.  linguistic  resources  for 

3280

