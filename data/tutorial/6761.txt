cross-lingual word representations:

induction and evaluation

ivan vuli  , anders s  gaard, manaal faruqui

emnlp 2017 tutorial

copenhagen; september 8, 2017

before we start...

ivan

anders

manaal

tutorial slides are available here:

http://people.ds.cam.ac.uk/iv250/tutorial/xlingrep-tutorial.pdf

#emnlp2017 #xlingwordrep

1 / 219

motivation

we want to understand and model the meaning of...

source: dreamstime.com

...without manual/human input and without perfect mt

2 / 219

motivation

the nlp community has developed useful features for several tasks but  nding
features that are...

1. task-invariant (id52, srl, ner, parsing, ...)

(monolingual id27s)

2. language-invariant (english, dutch, chinese, spanish, ...)

(cross-lingual id27s     this tutorial)

...is non-trivial and time-consuming (20+ years of feature engineering...)

3 / 219

motivation

the nlp community has developed useful features for several tasks but  nding
features that are...

1. task-invariant (id52, srl, ner, parsing, ...)

(monolingual id27s)

2. language-invariant (english, dutch, chinese, spanish, ...)

(cross-lingual id27s     this tutorial)

...is non-trivial and time-consuming (20+ years of feature engineering...)

learn word-level features which generalise across tasks and languages

4 / 219

the world existed b.e. (before embeddings)

b.e. example 1: cross-lingual (brown) clusters
[tackstrom et al., naacl 2012, faruqui and dyer, acl 2013]

5 / 219

the world existed b.e.

b.e. example 2: traditional  count-based  cross-lingual spaces
[gaussier et al., acl 2004; peirsman and pad  , naacl 2010]

6 / 219

the world existed b.e.

b.e. example 3: cross-lingual topical spaces
[mimno et al., emnlp 2009; vuli   et al., acl 2011]

7 / 219

the world existed b.e.

b.e. example 4: cross-lingual cca-based vector spaces based on distributional
and orthographic information
[haghighi et al., acl 2008]

8 / 219

motivation revisited

cross-lingual id27s:

- simple: quick and e cient to train
- state-of-the-art and omnipresent
- lightweight and cheap

 cross-lingual id27 models are to mt what
(monolingual) id27 models are to id38. 

by sebastian ruder

are they?

they support cross-lingual nlp: enabling cross-lingual modeling
and cross-lingual transfer

9 / 219

tutorial goals

link cross-lingual id27s to prior work in cross-lingual nlp

provide a systematic typology of cross-lingual id27 models

analyze the importance of data requirements vs. algorithm choices

demonstrate that many current architectures are in fact very similar in their
modeling assumptions and formulations

stress the importance of cross-lingual id27s in cross-lingual
downstream tasks and applications

10 / 219

tutorial overview

introduction (15 mins)

id27s and cross-lingual id27s

bilingual/multilingual data requirements:
motivating the model typology

part i: learning from word-level supervision (45 mins)

word-level supervision: word alignments and dictionaries

mapping-based approaches, extension and variations

approaches based on pseudo-bilingual corpora

joint online models

from bilingual to multilingual spaces

11 / 219

tutorial overview

part ii: learning from sentence-level supervision (30 mins)

sentence-level supervision: sentence alignments without word
alignment, sentence ids

overview of standard approaches: bilingual autoencoders,
bilbowa, bicvm, id105 models

similarities to word-level model formulations

from bilingual to multilingual spaces

checkpoint: co ee break

12 / 219

tutorial overview

part iii: learning from document-level supervision (20 mins)

document alignments vs sentence alignments: analogies and
di erences

inverse indexing and approaches based on pseudo-bilingual corpora

similarities to word-level and sentence-level model formulations

part iv: learning from other sources (15 mins)

other modalities and data sources (e.g., image captions, geospatial
data, eye-tracking data)

multi-modal bilingual representations

13 / 219

tutorial overview

part v: evaluation and application (40 mins)

intrinsic vs. extrinsic evaluation, current evaluation challenges

standard tasks: bilingual lexicon extraction, cross-lingual document
classi cation

other possible (new and emerging) evaluation protocols

cross-lingual knowledge transfer (semantic vs. syntactic
information)

applications in cross-lingual nlp and ir

multilingual embeddings: supporting massively multilingual nlp?

outro: useful software, future challenges, concluding remarks
(15 mins)

14 / 219

id27s

representation of each word w     v :

vec(w) = [f1, f2, . . . , fdim]

word representations in the same semantic (or embedding) space!

image courtesy of [gouws et al., icml 2015]

15 / 219

cross-lingual id27s (clwes)

representation of a word ws

1     v s:

vec(ws

1 ) = [f 1

1 , f 1

2 , . . . , f 1

dim]

exactly the same representation for wt

2     v t :

vec(wt

2 ) = [f 2

1 , f 2

2 , . . . , f 2

dim]

language-independent word representations in the same shared
semantic (or embedding) space!

16 / 219

cross-lingual id27s

[luong et al., naacl 2015]

17 / 219

cross-lingual id27s

vs.

monolingual

cross-lingual
q1     algorithm design: how to align semantic spaces in two
di erent languages?
q2     data requirements: which bilingual signals are used for
the alignment?

18 / 219

data requirements: bilingual signals

parallel

comparable

word
sentence
document

images

dictionaries
translations captions
-
wikipedia

nature and alignment level of data sources required by clwe models

data requirements are more important for the  nal clwe model
performance than the actual underlying architecture
[levy et al., eacl 2017]

19 / 219

data requirements: bilingual signals

[upadhyay et al., acl 2016]

illustration of di erent data requirements and bilingual signals

1. word-level: word alignments and translation dictionaries
(in part i)

2. sentence-level: sentence alignments
(in part ii)

3. document-level: wikipedia/news articles
(in part iii)

4. other: visual alignment: image captions, eye-tracking data
(in part iv)

20 / 219

welcome to the clwe jungle i

comparable
kiela et al. (2015)
vuli   et al. (2016)
vuli   et al. (2017)
zhang et al. (2017)
hauer et al. (2017)

parallel

word mikolov et al. (2013)

faruqui & dyer (2014)
xing et al. (2015)
dinu et al. (2015)
lazaridou et al. (2015)
zhang et al. (2016)
zou et al. (2013)
ammar et al. (2016)
artexte et al. (2016)
xiao & guo (2014)
gouws and s  gaard (2015)
duong et al. (2016)
gardner et al. (2015)
smith et al. (2017)
mrk  i   et al. (2017)
artetxe et al. (2017)

21 / 219

welcome to the clwe jungle ii

comparable
calixto et al. (2017)
gella et al. (2017)

sentence alignments guo et al. (2015)

parallel

vyas and carpuat (2016)
hermann and blunsom (2013)
lauly et al. (2013)
ko  isk   et al. (2014)
chandar et al. (2014)
pham et al. (2015)
klementiev et al. (2012)
soyer et al. (2015)
luong et al. (2015)
gouws et al. (2015)
coulmance et al. (2015)
shi et al. (2015)
rajendran et al. (2016)
levy et al. (2017)

document

hermann and blunsom (2014) vuli   & korhonen (2016)
vuli   and moens (2016)
s  gaard et al. (2015)
mogadala and rettinger (2016)

22 / 219

general (simpli ed) methodology

[upadhyay et al., acl 2016]

1. initialize w     w0,v     v0
2. (wf , vf )     arg min   l1(w) +   l2(v) +    (w, v )

23 / 219

general (simpli ed) methodology

(bilingual) data sources are more important than the chosen algorithm
most algorithms are formulated as: l1(w) + l2(v) +    (w, v )

24 / 219

m

part i: learning from

word-level signal

25 / 219

learning from context

most clwe models are multilingual extensions of standard mono we models

skip-gram with negative sampling (sgns)
[mikolov et al.; nips 2013]

learning from the set d of (word, context) pairs observed in a corpus:
(w, v) = (wt, wt  c); i = 1, ..., c; c = context window size

sg learns to predict the context of each pivot word.

john saw a cute gray huhblub running in the  eld.

d = (huhblub, cute), (huhblub, gray), (huhblub, running), (huhblub, in)
vec(huhblub) = [   0.23, 0.44,   0.76, 0.33, 0.19, . . .]

26 / 219

three clusters of word-level signal models

1. mapping-based or o ine approaches: train monolingual word
representations independently and then learn a transformation matrix
from one language to the other

2. pseudo-bilingual approaches: train a monolingual we model on
automatically constructed corpora containing words from both languages

3. joint online approaches: take parallel text as input and minimize the
source and target language losses jointly with the id173 term

these approaches are equivalent, modulo optimization strategies

27 / 219

basic mapping approach

the geometric relations that hold between words are similar across languages:
e.g., numbers and animals in english show a similar geometric constellation as
their spanish counterparts

[mikolov et al., arxiv 2013]

28 / 219

basic mapping approach

[mikolov et al., arxiv 2013]

learn to transform the pre-trained source language embeddings into a space
where the distance between a word and its translation pair is minimised

29 / 219

basic mapping approach

based on given word translation pairs (xs

i , xt

i), i = 1, . . . , n

mean-squared error between the euclidean distances (of the actual
and transformed vector)

n(cid:88)

   mse =

(cid:107)wxs

i     xt
i(cid:107)2

i=1

   mse = (cid:107)wxs     xt(cid:107)2

f

standard (re)formulation:

j = l1

sgns + l2

sgns +    mse

30 / 219

mapping approach: variations and extensions

one tip: matrix w may be computed more e ciently by taking the
moore-penrose pseudoinverse:

x+ = (xs(cid:62)

xs)

   1xs(cid:62)

as w = x+xt

[artetxe et al., emnlp 2016; glava   et al., emnlp 2017]

31 / 219

mapping approach: variations and extensions

[xing et al., naacl 2015]: there is a mismatch between the initial objective
function, the distance measure, and the transformation objective

solution: 1. id172 of word vectors to unit length + 2. replacing mse
with cosine similarity for learning the mapping

n(cid:88)

i=1

   cos = max

w

cos(w xs

i , xt
i)

32 / 219

mapping approach: variations and extensions

[zhang et al., naacl 2016; artetxe et al., emnlp 2016; smith et al., iclr
2017]: analytical solution to the transformation problem

unit length id172 and orthogonality imposed

another requirement [artetxe et al., emnlp 2016]:
two randomly selected words are generally not expected to be similar     the
cosine of their embeddings in any dimension as well as their cosine similarity
should be zero     adding centering matrix

n(cid:88)

i=1

cov(w xs

i , xt
i)

   centered = max

w

33 / 219

mapping approach: variations and extensions

[lazaridou et al., acl 2015]:
max-margin hinge loss (mmhl) with intruders instead of mse

this reduces hubness and improves similarity computations

k(cid:88)

j(cid:54)=i

max{0,   sim + cos(   yi, yi)     cos(   yi, yj)}

j = l1

cbow + l2

cbow +    mmhl

34 / 219

mapping approach: variations and extensions

[vuli   and korhonen, acl 2016]: analysis of seed lexicons

dictionary size and translation reliability are important factors

hybrid model: it learns reliable translation pairs from non-parallel data and
then seeds the transformation matrix learning

35 / 219

id64 from small seed dictionaries

minimising the requirements: the id64 idea dates back to
 pre-embedding  times
[peirsman and pad  , naacl 2010; vuli   and moens; emnlp 2013]

application to truly low-resource cross-lingual settings?

36 / 219

id64 from small seed dictionaries

the latest instance of the id64 idea
[artetxe et al., acl 2017]

self-learning iterative framework: starting from cognates or numerals

37 / 219

id64 from small seed dictionaries

lexicon induction results from [artetxe et al., acl 2017]

id64 helps with really small seed dictionaries

a performance plateau with simple mapping approaches has been reached?

38 / 219

mapping approach: bilingual to multilingual

fixing one space (english) and learning transformations for all monolingual
id27s in other languages

this constructs a multilingual space from input monolingual spaces

[smith et al., iclr 2017]:

a uni ed multilingual space for 89 languages using fasttext vectors and 5k
google translate pairs for each language pair

39 / 219

mapping approach: bilingual to multilingual

multiple transformations with pivoting

this constructs a multilingual space from input bilingual spaces

[duong et al., eacl 2017: multilingual training of crosslingual word
embeddings]

40 / 219

mapping approach: bilingual to multilingual

multilingual semantic space is more expressive than separate
bilingual spaces?

[duong et al., eacl 2017]

41 / 219

pseudo-bilingual training

[gouws and s  gaard, naacl 2015]
barista: bilingual adaptive reshu ing with individual stochastic alternatives

1. build a pseudo-bilingual corpus (or corrupt the original training data) using
a set of prede ned equivalences (e.g., pos tags, word translation pairs)

2. train a monolingual we model on the corrupted/shu ed corpus

original: we build the house

pos: we build la voiture / they run la house

translations: we construire the maison / nous build la house

42 / 219

pseudo-bilingual training

[gouws and s  gaard, naacl 2015]
barista: bilingual adaptive reshu ing with individual stochastic alternatives

1

input: source corpus cs, target corpus ct, dictionary d

2 concatenate cs and ct and then shu e the concatenated corpus

3 pass over the corpus, for each word w: if {w(cid:48)|(w, w(cid:48))     (w(cid:48), v)     d} is
non-empty and of cardinality k, replace w with w(cid:48) with id203 1/2k;
keep w otherwise

4 train a standard we model on the pseudo-bilingual corpus c(cid:48)

43 / 219

pseudo-bilingual training

the chosen equivalence class dictates the shape of the output space

cross-lingual embedding space with pos equivalences

44 / 219

pseudo-bilingual training

back to the future...

a similar idea was developed with sentence and document alignments

shu ing aligned sentences/documents

45 / 219

online monolingual and cross-lingual training

combining monolingual and cross-lingual objectives in joint training

monolingual     using monolingual data and preserving monolingual relations
cross-lingual     tying the two spaces together

46 / 219

online monolingual and cross-lingual training

[zou et al., emnlp 2013]: using normalized word alignment counts estimated
from parallel texts stored in two matrices: al1   l2 and al2   l1

monolingual objectives: global context with noise contrastive estimation

cross-lingual objective

jl1   l2 = ||xt     al1   l2 xs||2
jl2   l1 = ||xs     al2   l1 xt||2

   wa = jl1   l2 + jl2   l1

again, standard formulation:

j = l1

nce + l2

nce +    wa

47 / 219

online monolingual and cross-lingual training

[klementiev et al., coling 2012]: using word alignment counts plus neural
language models on both sides

monolingual objectives: two full- edged neural language models

l =     log p (wi | wi   c+1:i   1)

cross-lingual objective: id173 based on word alignments

|v |s(cid:88)

i=1

1
2

jl1   l2 =

(cid:62)

(as   t     i)xs

i

xs
i

jl2   l1 computed in an analogous fashion

   wa = jl1   l2 + jl2   l1

again, standard formulation:

j = l1

nlm + l2

nlm +    wa

(1)

(2)

48 / 219

online monolingual and cross-lingual training:
bilingual to multilingual

trivial extension to multilingual training

combining n monolingual objectives with(cid:0)n

2

(cid:1) cross-lingual objectives

j = l1

nlm + l2

nlm + l3

nlm +    wa,l1,l2 +    wa,l1,l3 +    wa,l2,l3

in practice:

using full nlms makes the models ine cient: using monolingual sgns
objectives instead

49 / 219

online monolingual and cross-lingual training:
bow sgns with word-level information

bilingual skip-gram (biskip) proposed by luong et al., naacl 2015

very similar to the original mapping approach, but formulated as online
monolingual and cross-lingual training on pseudo-bilingual corpora

j = l1

sgns + l2

sgns +    l1,l2

   l1,l2 = sgn sl1   l2 + sgn sl2   l1

50 / 219

online monolingual and cross-lingual training:
bilingual to multilingual

multilingual skip-gram (multiskip) by ammar et al., naacl 2016

simple extension (again)     summing up bilingual objectives for all
available parallel corpora
sgns + l2

sgns +    l1,l2 +    l2,l3 +    l1,l3

sgns + l3

j = l1

51 / 219

online monolingual and cross-lingual training:
bilingual to multilingual

using bilingual dictionaries in all language pairs or word alignments (per
language pair)

1. select the closest translation (if more are possible)

2. (see the  gure below)

52 / 219

online monolingual and cross-lingual training:
sgns with dependency-based contexts

[vuli  , eacl 2017]

extracting context pairs from hybrid  cross-lingual  trees

53 / 219

online monolingual and cross-lingual training

extracting monolingual and cross-lingual dependency-based contexts

(discovers, scientist_nsubj)
(stars, discovers_dobj   1)

(scienzato, australiano_amod)

(scopre, stelle_dobj)

(scientist, australiano_amod)
(australiano, scientist_amod   1)

(stars, scopre_dobj   1)

(discovers, scienzato_nsubj)

training id97f sgns on these (word, context) pairs

54 / 219

combining contextual and orthographic info

metric learning using translation pairs with contextual and orthographic info
[heyman et al., eacl 2017]

55 / 219

combining contextual and orthographic info

a historical note: learning word representations relying on distributional and
ortographic evidence is not a brand new idea

we just have more powerful algorithms and more data today

[haghighi et al., acl 2008]

56 / 219

cca-based approaches

[faruqui and dyer, eacl 2014]

57 / 219

cca-based approaches

cross-lingual projection using canonical correlation analysis

the objective may (again) be formulated as:

j = l1

lsa + l2

lsa +    cca

where    cca =     n(cid:88)
(cid:113)

i , xt(cid:48)

i ) =

i=1

where   (xs(cid:48)

where xs(cid:48)

i = xs

i vi

  (xs

cov(xs(cid:48)

iwi)

i vi, xt
i , z(cid:48)
i)
2)cov(xt(cid:48)

i

cov(xs(cid:48)
i
xt(cid:48)
i = xt

iwi

2)

where vi and wi are two projection vectors

58 / 219

cca-based approaches: extensions

extension of the basic approach: replacing cca with deep cca

[lu et al., naacl 2015]

59 / 219

cca-based approaches: extensions

cca-based approaches with pivoting: other language or visual
modality

[rajendran et al., naacl 2016; gella et al., emnlp 2017]

multi-view and generalized cca: bilingual     multilingual

[rastogi et al., naacl 2015, ammar et al., arxiv 2016]

60 / 219

cross-lingual word representations via semantic
specialisation

combining resource-based and distributional information within a
semantic specialisation framework

state-of-the-art: paragram and attract-repel
[wieting et al., tacl 2015; mrk  i   et al., tacl 2017]

if s is the set of synonymous word pairs, the procedure iterates over
mini-batches of such constraints bs, optimising the following cost function:

(cid:88)

(xl,xr )   bs

s(bs) =

(relu (  sim + xltl     xlxr)

+ relu (  sim + xrtr     xlxr))

where   sim is the similarity margin and tl and tr are negative examples for the
given word pair (xl, xr).

61 / 219

clwes via semantic specialisation

cross-lingual supervision: again word-level  linguistic constraints 
(e.g., synonyms)

constraint

relation

source

(en_sweet, it_dolce)
(en_work, fr_travail)
(en_school, de_schule)
(fr_montagne, de_gebirge)
(sh_gradona  elnik, en_mayor)
(nl_vrouw, it_donna)

syn
syn
syn
syn
syn
syn

ant
(en_sour, it_dolce)
ant
(en_asleep, fr_  veill  )
ant
(en_cheap, de_teuer)
(de_langsam, es_r  pido)
ant
(sh_obeshrabriti, en_encourage) ant
(fr_jour, nl_nacht)
ant

babelnet
babelnet
babelnet
babelnet
babelnet
babelnet

babelnet
babelnet
babelnet
babelnet
babelnet
babelnet

62 / 219

clwes via semantic specialisation

the attract term

negative examples for each synonymy pair
for each synonymy pair (xl, xr), the negative example pair (tl, tr)
is chosen from the remaining in-batch vectors so that tl is the one
closest (cosine similarity) to xl and tr is closest to xr.

s(bs) =

(cid:88)

(xl,xr)   bs

(relu (  sim + xltl     xlxr)
+ relu (  sim + xrtr     xlxr))

the two negative examples are used to force synonymous pairs to
be closer to each other than to their respective negative examples
(i.e. to any of the remaining words in the current mini-batch).

63 / 219

clwes via semantic specialisation

negative examples for each synonymy pair
for each synonymy pair (xl, xr), the negative example pair (tl, tr)
is chosen from the remaining in-batch vectors so that tl is the one
closest (cosine similarity) to xl and tr is closest to xr.

64 / 219

clwes via semantic specialisation

the id173 term

the second term tries to retain the bene cial semantic content
embedded in the initial vector space.

l2 id173

r(v ) =

(cid:88)

xi   v

  reg (cid:107)(cid:98)xi     xi(cid:107)2

this term preserves semantic relations learned by distributional
models that do not contradict the injected similarity constraints.

65 / 219

clwes via semantic specialisation

j = l1

mono + l2

mono +    spec

cross-lingual connections may be used for direct transfer of semantic resources
(e.g., verbnet, framenet)

[vuli   et al., emnlp 2017]

66 / 219

clwes via semantic specialisation

trivially multilingual
cross-lingual word links between all languages are used to bring the word vector
spaces of various languages into a single uni ed vector space.

[mrk  i   et al., tacl 2017]

67 / 219

comparable word-level signal: images

bilingual space need not be linguistic in nature

visual and multi-modal cross-lingual word representations

[kiela et al., emnlp 2015; vuli   et al., acl 2016]

68 / 219

comparable word-level signal: images

visual features: transferred id98 features from the id163 task

recently: multilingual image captioning data: multi-modal representations
beyond word-level

[specia et al., wmt 2016; elliott et al., vl 2016; gella et al., emnlp 2017]

69 / 219

parts ii-ivanders s  gaard http://cst.dk/anders/m

a cold open...

71 / 219

sentence-level (alignment) signal

trans-gram model: very similar to biskip, but it relies on full sentential
contexts

[coulmance et al., emnlp 2015]

j = l1

sgns + l2

sgns +    l1,l2

   l1,l2 = sgn sl1   l2 + sgn sl2   l1

deja vu, right?

72 / 219

sentence-level (alignment) signal

bilbowa model: very similar to online models, but using sentence-level
cross-lingual information

[gouws et al., icml 2017]

deja vu, right?

j = l1

m(cid:88)
sgns + l2

   bilbow a = (cid:107) 1
m

sgns +    bilbow a

n(cid:88)

i     1
xs
n

i    sents
ws

j   sentt
wt

j(cid:107)2
xt

73 / 219

outline of my part   recap    part ii: learning from sentences    part iii: learning from documents    part iv: learning from extra-linguistic contextrecap, so far..cross-lingual id27s   learned from aligned words, sentences, or dictionaries    alignments can either be of parallel or comparable units    most, if not all, approaches minimize a sum of n losses (for n languages) + a regularizercross-lingual id27s   many approaches optimize for the same objectives:    if you sample contexts through word alignments (duong et al., 2015), you add a penalty    if you create a mixed corpus using a translation function (gouws and s  gaard, 2015), you add penalty     if you hard code your translation (xiao and guo, 2014), you add a strong penaltycross-lingual id27swordssentencesdocumentsparallelcomparableshout-out to upadhyay et al. (2016)cross-lingual id27swordssentencesdocumentsparallelcomparablecross-lingual id27swordssentencesdocumentsparallelcomparablesyntactic contextsaligned sentencessocial media threadwikipedia concept idreal-world assumptions    na as    beke?google translatewiktionaryeuroparlpos taggers4+ million speakers65-70% internet500b usd gdp150 universitiessupervision   online parallel text: the bible, the watchtower, a few declarations    ocr printed dictionaries?    comparable text through reference points: wiktionary conceptids (not for tiv, though), real-life events, hashtags, urls, linguistic annotation, images,    cross-lingual id27sbible#hashtags, images (search, co-clicks), time stampscaptionswikipediawordssentencesdocumentsparallelcomparableparallel sentencesbible#hashtags, images (search, co-clicks), time stampscaptionswikipediawordssentencesdocumentsparallelcomparablefeature spacesis valencia on the coast? so you will go swimming? if you meet a shark, eat it.  don   t step on corals.  name a spanish hip hop band. ist valencia an der k  ste? also wirst du schwimmen gehen? wenn sie einen hai treffen, essen sie es. treten sie nicht auf korallen. nennen sie eine spanische hip-hop-band.sentence alignments?strand?strand?strandbeachis valencia on the coast? ist valencia an der k  ste?is valencia on the coast? ist valencia an der k  ste?monolingual contextis valencia on the coast? ist valencia an der k  ste?monolingual contextcross lingual contextis valencia on the coast? ist valencia an der k  ste?monolingual contextcross lingual contextsentence idfour algorithmsmonolingualcross-lingualsentence idbilbowa (gouws et al.)yesyesbwe (vulic and moens)yesyesba (chandar et al.)yesyesyesinverted (s  gaard et al.)yesbilbowa (gouws et al.)   cbow source language losses    equivalence class is weighted by mean of     p(w in s | v in s)    p(v in s |  w in s)bwe (vulic and moens)   sngs source language losses    equiv class is weighted by (weighted) mean of     p(w in s | v in s)    p(v in s |  w in s)bwe (vulic and moens)   sngs source language losses    equiv class is weighted by (weighted) mean of     p(w in s | v in s)    p(v in s |  w in s)very similar to bilbowa,  but performs much better  (levy et al., 2017)bwe (vulic and moens)   sngs source language losses    equiv class is weighted by (weighted) mean of     p(w in s | v in s)    p(v in s |  w in s)very similar to bilbowa,  but performs much better  (levy et al., 2017)ba (chandar et al.)   superior to much of previous work (levy et al., 2017)    why?a dog runsa dog barksa cat runsidtermdocument1dog1, 22runs1, 33barks24cat3inverted (s  gaard et al.)a dog runsa dog barksa cat runsidtermdocument1dog1, 22runs1, 33barks24cat35hund1, 26l  uft1, 37bellt28katze3inverted (s  gaard et al.)ein hund l  uftein hund bellteine katze l  uftidtermdocument1dog1, 22runs1, 33barks24cat35hund1, 26l  uft1, 37bellt28katze3inverted (s  gaard et al.)svddata and taskssentence id = verse idsentence id = verse idword alignment strand disappointed the venue is not by the beach?sentence id = verse idword alignment strand disappointed the venue is not by the beach?dictionary induction strand shore, kantian, yo-yo, beach, pokemon, dried?gracahansardslambertmihalceaholmqvistcakmakwiktionarygracahansardslambertmihalceaholmqvistcakmakwiktionaryword alignmentdictionary induction00,0850,170,2550,34bilbowabwebainverteddiceibm-1sid-sgnsxling-sid0,3290,2830,2920,1840,2870,2860,2510,16400,0850,170,2550,34bilbowabwebainverteddiceibm-1sid-sgnsxling-sid0,3290,2830,2920,1840,2870,2860,2510,164state of the art03,51513,5generalizing over inverted indexingstep 1counting{0,1}/log((#w,#f)(*,*)/(#w,*)(*,#f))step 2id84svd/sgnsstep 3there is no step 3dicel1-normalized binarynoneinvertedl2-normalized binarysvdbabinaryauto encoderthis0.1positive pmisvdthisbinarysgns00,0850,170,2550,34bilbowabwebainverteddiceibm-1sid-sgnsxling-sid0,3290,2830,2920,1840,2870,2860,2510,164sentence id only87,50115,5multilingual support00,0850,170,2550,34bilbowabwebainverteddiceibm-1sid-sgnsxling-sid0,3290,2830,2920,1840,2870,2860,2510,164multilingual  supportother work on multi-lingual support   ammar et al. (2016): massively multilingual id27s    duong et al. (2017): multilingual training of crosslingual id27s    smith et al. (2017): offline bilingual word vectors, orthogonal transformations and the inverted softmaxcomparable sentencesbible#hashtags, images (search, co-clicks), time stampscaptionswikipediacomparable sentences   image captioningthe brown dog is running after the black dog.  ein brauner hund und ein schwarzer hund.comparable sentences   image captioning    tweets with similar time stamps and hashtags       august 7 diebesbanden machen nrw-krankenh  user unsicher [url] via @rponline dank offener grenzen und #merkel  s neub  rger @junckereu needs to listen to bill gates and control european borders. #refugeecrisis #merkel #btw17comparable documentsbible#hashtags, images (search, co-clicks), time stampscaptionswikipediacomparable documents   previously discussed: bwes, inverted indexing    multilingual id96    document classification?comparable wordsbible#hashtags, images (search, co-clicks), time stampscaptionswikipediacomparable words   ortography    images    hashtags, names, and urls    co-clicks, timestamps, and demographics    wikipedia conceptids?comparable words   ortography    images    hashtags, names, and urls    co-clicks, timestamps, and demographics    gaze data, fmri    wikipedia conceptids?imagesdogredrunhundr  dl  besearch & siftdogredrunhundr  dl  besearch & id98sdogredrunhundr  dl  beword-object pmithe brown dog is running after the black dog.  ein brauner hund und ein schwarzer hund.petr  n hansen, hartmann and s  gaard (2017)gaze data & fmri00,250,50,751mean fix durfix probn regr from...n refixationsgaze data & fmritargetew30fmriokay?studentsteachersmistakenocreepdriftlongnopeacedeatheatmaybetightnastyholdmaybesqueaktwistedbrokeyesadmiringcursingstunnedyesamazeddelightedimpressedyesin sumsoasoaprematureclose, but no cigarclose, but no cigarm

part v: evaluation and

application

143 / 219

evaluationquality of embeddingshow good are they in representing word meaning?how good are they as features in downstream tasks?intrinsicextrinsicevaluationintrinsic  >> extrinsic?intrinsic  << extrinsic?intrinsic  extrinsic?most ideal but hardly true![schnabel et al, emnlp 2015; tsvetkov et al, emnlp 2015]intrinsic evaluationproperties of intrinsic evaluation metric:1.correlated with downstream applications2.fast and easy to use3.provides insights on the model, facilitates error analysis4.approximates a range of related tasksintrinsic evaluationword similarity tasks[rubenstein & goodenough, 1965; finkelstein et al, 2002; bruni et al, 2012; hill et al 2014]tigertiger10kingcabbage0.2......sugar approach0.8tigertiger1kingcabbage0.3......sugar approach0.4humansvectorsspearman   s correlationcross-lingual word similarityintrinsic evaluation[semeval 2017 task 2: camacho-collados et al 2017]words of different languages annotated for similarity!problems with word similarity[faruqui et al, repeval 2016]low correlation with extrinsic taskssubjectivity: relatedness & similarity frequency & hubness effect in vector-space affecting cosine similarityinability to account for polysemyintrinsic evaluation: qveccca0.6correlation coefficientlinguistic vector matrixword vector matrix   grounds vectors in linguistic properties!   high correlation shown with extrinsic tasks like classification, parsing etc.[tsvetkov et al, emnlp 2015]intrinsic evaluation: qveclinguistic vector matrixword vector matrixinextensible easily to other languages!similarity-based vs feature-based?

parallel to the division to intrinsic and extrinsic tasks, there is another
perspective:

similarity-based use of embeddings: applications that rely on (constrained)
nearest neighbour search in the cross-lingual id27 graph

feature-based use of embeddings: use cross-lingual embeddings directly as
features: information retrieval, cross-lingual transfer?

152 / 219

cross-lingual dictionary induction   for a word in english, find top-10 neighbors in foreign language   evaluate these neighbours against a gold dictionarycandidate generationmatching with gold data[vuli   and moens, naacl 2013]cross-lingual lexicon induction

framed as cross-lingual synonymy in a shared embedding space?

metrics: accn , m rr?

154 / 219

cross-lingual lexicon induction

similarity-based vs. classi cation-based lexicon induction

id27s used as features for classi cation

155 / 219

extrinsic evaluation
cross-lingual document classi cation

quite popular in the clwe literature

cross-lingual knowledge transfer?

[klementiev et al., coling 2012; heyman et al., dami 2015]

156 / 219

extrinsic evaluationcross-lingual document classificationtrain classifier in l1test classifier in l2tests the transferability of vectors across languages![klementiev et al., coling 2012]https://github.com/shyamupa/biling-surveyextrinsic evaluation
cross-lingual document classi cation

using word-level information composed into a document-level representation

[hermann and blunsom, acl 2014]

1. is this really an optimal way to approach this task?
2. is this really an optimal way to evaluate id27s?

158 / 219

digression
cross-lingal representation beyond the word level

towards (direct) bilingual phrase and sentence representations

[pham et al., naacl 2015; mogadala and rettinger, naacl 2016]

but this is a completely di erent story...

159 / 219

extrinsic evaluation
cross-lingual document classi cation

some results... ( nally?)

method
majority class
mt

en-de
46.8
68.1

de-en
46.8
67.4

klementiev et al. (2012)
chandar et al. (2014)
hermann and blunsom (2014)
kocisky et al. (2014)
gouws et al. (2015)
luong et al. (2015)
coulmance et al. (2015)
mogadala and rettinger (2016)

77.6
91.8
86.4
83.1
86.5
87.6
87.8
88.1

71.1
74.2
74.7
75.4
75.0
77.8
78.7
78.9

en-fr
22.5
76.3

74.5
84.6

fr-en
25.0
71.1

61.9
74.2

en-es
15.3
52.0

31.3
49.0

es-en
22.2
58.4

63.0
64.4

-
-
-
-
-

-
-
-
-
-

-
-
-
-
-

-
-
-
-
-

79.2

77.8

56.9

67.6

160 / 219

extrinsic evaluationcross-lingual id33train parser in l1test parser in l2tests the transferability of vectors across languages![guo et al., acl 2015]https://github.com/shyamupa/biling-surveyextrinsic evaluation
cross-lingual id33

delexicalized transfer plus lexical features: clusters, embeddings, dictionaries

[s  gaard et al., acl 2015; guo et al., jair 2016; upadhyay et al., acl 2016;
dehouck and denis, eacl 2017]

162 / 219

extrinsic evaluation and application
cross-lingual task x, cross-lingual task y, ...

cross-lingual lexical and id123

[vyas and carpuat, naacl 2016; agi   and schluter, arxiv 2017]

163 / 219

extrinsic evaluation and application
cross-lingual task x, cross-lingual task y, ...

cross-lingual supersense tagging

[gouws and s  gaard, naacl 2015]

word alignment

[levy et al., eacl 2017]

id52

[s  gaard et al., acl 2015; zhang et al., naacl 2016]

wiki cation

[tsai and roth, naacl 2016]

164 / 219

extrinsic evaluation and application
cross-lingual task x, cross-lingual task y, ...

cross-lingual discourse parsing

[braud et al., eacl 2017]

frame-id29

[johannsen et al., emnlp 2013]

id14

[kozhevnikov and titov, acl 2013; akbik et al., acl 2016]

id31

[zhou et al., acl 2016; abdalla and hirst, arxiv 2017]

165 / 219

extrinsic evaluation and application
cross-lingual information retrieval

semantic representations for cross-lingual information ir

[vuli   and moens, sigir 2015; mitra and craswell, arxiv 2017]

166 / 219

extrinsic evaluation and application
cross-lingual information retrieval

document and query embeddings

composition of id27s:
[mitchell and lapata, acl 2008]

      
d =       w1 +       w2 + . . . +             w|nd|

the dim-dimensional document embedding in the same
cross-lingual id27 space:

      
d = [fd,1, . . . , fd,k, . . . , fd,dim]

167 / 219

extrinsic evaluation and application
cross-lingual information retrieval

document and query embeddings

    the same principles with queries

      
q =       q1 +       q2 + . . . +       qm

the dim-dimensional query embedding in the same bilingual
id27 space:

      
q = [fq,1, . . . , fq,k, . . . , fq,dim]

168 / 219

cross-lingual transferanders s  gaard http://cst.dk/anders/discourse                                                                                      braud easemantics                                                               kozh.&titovsyntax       yarowsky ea      hwa eamorphology                                                kim eaphonology                                                                    zbal20012005201120132017approachesdelexicalized transferannotation projectionid7220012005201120132017downstream [  ] diversity [  ]translationstreebankstranslationstreebanksapproaches   annotation projection    cross-lingual re-lexicalized transfer    combinations of both??discourse                                   semanticssyntax      s  gaard et al. (2015), upadhyay et al. (2016), ammar et al. (2016)morphology                                           gouws and s  gaard (2015), s  gaard et al. (2015)phonology                                                             cross-lingual embeddings with downstream loss?approaches   fine-tuning pretrained models (e.g, id4)    downstream training with posterior id173 (e.g., ferreira et al., 2016)    id72rich caruana   when humans tackle new problems, they bring to bear what they have learned before for related problems."               task a            task b                  task atask b               during training, we randomly choose a dataset, then a data point. note that therefore the datasets can be disjoint.task atask b               you can do the same with embeddings, in theory, but open question whether it works, and with what curriculum.task atask bwork on id72 of monolingual id27s   collobert et al. (2011): nlp (almost) from scratch    luo et al. (2014): pre-trained multi-view id27 using two-side neural network    liu et al. (2015): representation learning using multi-task deep neural networks for semantic classification and information retrievalextrinsic evaluation and application
language understanding: multilingual dialog state tracking

the neural belief tracker is a novel dst model/framework which
aims to satisfy the following design goals:

1 end-to-end learnable (no slu modules or semantic dictionaries).

2 generalisation to unseen slot values.

3 capability of leveraging the semantic content of pre-trained word

vector spaces without human supervision.

[mrk  i   et al., acl 2017]

191 / 219

extrinsic evaluation and application
language understanding: multilingual dialog state tracking

representation learning + label embedding +
separate binary decisions
to overcome data sparsity, nbt models use label embedding to
decompose multi-class classi cation into many binary ones.

192 / 219

extrinsic evaluation and application
language understanding: multilingual dialog state tracking

dst in italian and german

multilingual woz 2.0 (mrk  i   et al., 2017); italian
and german
the 1,200 dialogues in woz 2.0 were translated by native italian and german
speakers instructed to consider preceding dialogue context.

word vector space
best baseline vector space
monolingual distributional vectors
+ monolingual specialisation
++ cross-lingual specialisation
+++ multilingual dst model

en
81.6
77.6
80.9
80.3
82.8

it
71.8
71.2
72.7
75.3
77.1

de
50.5
46.6
52.4
55.7
57.7

193 / 219

extrinsic evaluation and application
language understanding: multilingual dialog state tracking

dst models for resource-poor languages

ontology grounding: multilingual dst models
the domain ontology (i.e. the concepts it expresses) is language agnostic,
which means that `labels' persist across languages. using training data for two
(or more) languages, and cross-lingual vectors of high quality, we train the
 rst-ever multilingual dst model.

194 / 219

take-home messages

1. cross-lingual id27 models are more similar to
old(er) nlp ideas than it seems
but they are simple, e cient, and state-of-the-art...

2. clwe models are more similar than it seems
we have addressed similarities between a plethora of cross-lingual word
embedding models

195 / 219

take-home messages

1. cross-lingual id27 models are more similar to
old(er) nlp ideas than it seems
but they are simple, e cient, and state-of-the-art...

2. clwe models are more similar than it seems
we have addressed similarities between a plethora of cross-lingual word
embedding models

3. data is crucial (more than the chosen algorithm)
optimization and various modeling tricks matter, but the chosen multilingual
signal is the most important factor

196 / 219

take-home messages

1. cross-lingual id27 models are more similar to
old(er) nlp ideas than it seems
but they are simple, e cient, and state-of-the-art...

2. clwe models are more similar than it seems
we have addressed similarities between a plethora of cross-lingual word
embedding models

3. data is crucial (more than the chosen algorithm)
optimization and various modeling tricks matter, but the chosen multilingual
signal is the most important factor

4. cl id27s support cross-lingual nlp
they are useful across a wide variety of cross-lingual nlp tasks, for
cross-lingual (re-lexicalized) transfer, language understanding, ir, ...

197 / 219

further work / research directions

1. intrinsic evaluation     downstream performance?

performance on intrinsic evaluation datasets correlates with downstream tasks
such as dst. however, substantial intrinsic gains do not always lead to large
downstream gains. why?

198 / 219

further work / research directions

1. intrinsic evaluation     downstream performance?

performance on intrinsic evaluation datasets correlates with downstream tasks
such as dst. however, substantial intrinsic gains do not always lead to large
downstream gains. why?

2. other (and better) evaluation protocols
is cldc really the best extrinsic task we can come up with? language
understanding task seem much more interesting...

199 / 219

further work / research directions

1. intrinsic evaluation     downstream performance?

performance on intrinsic evaluation datasets correlates with downstream tasks
such as dst. however, substantial intrinsic gains do not always lead to large
downstream gains. why?

2. other (and better) evaluation protocols
is cldc really the best extrinsic task we can come up with? language
understanding task seem much more interesting...

3. con ating and de-con ating word vectors
generalizing multi-prototype and multi-sense id27s to cross-lingual
settings. how?

200 / 219

further work / research directions

1. intrinsic evaluation     downstream performance?

performance on intrinsic evaluation datasets correlates with downstream tasks
such as dst. however, substantial intrinsic gains do not always lead to large
downstream gains. why?

2. other (and better) evaluation protocols
is cldc really the best extrinsic task we can come up with? language
understanding task seem much more interesting...

3. con ating and de-con ating word vectors
generalizing multi-prototype and multi-sense id27s to cross-lingual
settings. how?

4. combining distributional and resource-based information
recent initiative with strong results. lexical resources are abundant, we should
not shy away from using them (e.g., babelnet, panlex)

201 / 219

useful software i

simple python implementation of the basic mapping approach

[mikolov et al., arxiv 2013; dinu et al., iclr 2015]
http://clic.cimec.unitn.it/   georgiana.dinu/down/

python implementation of an extended (iterative) mapping approach

[artetxe et al., emnlp 2016; artetxe et al., acl 2017]
https://github.com/artetxem/vecmap

clwes (mapping based on fasttext vectors) for 78 languages

[smith et al., iclr 2017]
https://github.com/babylonpartners/fasttext_multilingual

sota (cross-lingual) semantic specialisation model (in tensorflow) plus
bilingual vectors for 52 languages

[mrk  i   et al., tacl 2017; vuli   et al., acl 2017]
https://github.com/nmrksic/attract-repel

202 / 219

useful software ii

bilbowa model implementation

[gouws et al., icml 2015]
https://github.com/gouwsmeister/bilbowa

biskip model implementation and some vectors

[luong et al., naacl 2015]
https://nlp.stanford.edu/   lmthang/bivec/

multicluster, multicca, multiskip models plus some vectors

[ammaer et al., arxiv 2016]
http://128.2.220.95/multilingual/data/

test data and best practices for several tasks

[upadhyay et al., acl 2016]
https://github.com/shyamupa/biling-survey

203 / 219

useful software iii

(re)implementations of several clwe models

[b  rard et al., lrec 2016]
https://github.com/eske/multivec

bilingual training of multi-sense embeddings

[  uster et al., naacl 2016]
https://github.com/rug-compling/bimu

cca-based model implementation

[faruqui and dyer, eacl 2014]
https://github.com/mfaruqui/crosslingual-cca

a clwe which exploits bilingual dictionaries

[duong et al., emnlp 2016]
https://github.com/longdt219/xlingualemb

204 / 219

cross-lingual space of thankyous

book in preparation: morgan & claypool synthesis lectures
the three of us + sebastian ruder

205 / 219

references i

oliver adams, adam makarucha, graham neubig, steven bird, and trevor
cohn. 2017.
cross-lingual id27s for low-resource id38.
in eacl. pages 937 947.

  eljko agi  , dirk hovy, and anders s  gaard. 2015.
if all you have is a bit of the bible: learning pos taggers for truly low-resource
languages.
in acl. pages 268 272.

  eljko agi  , anders johannsen, barbara plank, h  ctor mart  nez alonso, natalie
schluter, and anders s  gaard. 2016.
multilingual projection for parsing truly low-resource languages.
transactions of the association for computational linguistics 4:301 312.

waleed ammar, george mulcaire, yulia tsvetkov, guillaume lample, chris dyer,
and noah a. smith. 2016.
massively multilingual id27s.
corr abs/1602.01925.

206 / 219

references ii

mikel artetxe, gorka labaka, and eneko agirre. 2016.
learning principled bilingual mappings of id27s while preserving
monolingual invariance.
in emnlp. pages 2289 2294.

mikel artetxe, gorka labaka, and eneko agirre. 2017.
learning bilingual id27s with (almost) no bilingual data.
in acl. pages 451 462.

alexandre berard, christophe servan, olivier pietquin, and laurent besacier.
2016.
multivec: a multilingual and multilevel representation learning toolkit for nlp.
in lrec .
shane bergsma and benjamin van durme. 2011.
learning bilingual lexicons using the visual similarity of labeled web images.
in ijcai . pages 1764 1769.

chlo   braud, maximin coavoux, and anders s  gaard. 2017.
cross-lingual rst discourse parsing.
in eacl. pages 292 304.

207 / 219

references iii

sarath a.p. chandar, stanislas lauly, hugo larochelle, mitesh m. khapra,
balaraman ravindran, vikas c. raykar, and amrita saha. 2014.
an autoencoder approach to learning bilingual word representations.
in nips. pages 1853 1861.

jocelyn coulmance, jean-marc marty, guillaume wenzek, and amine
benhalloum. 2015.
trans-gram, fast cross-lingual id27s.
in emnlp. pages 1109 1113.

mathieu dehouck and pascal denis. 2017.
delexicalized id27s for cross-lingual id33.
in eacl. pages 241 250.

georgiana dinu, angeliki lazaridou, and marco baroni. 2015.
improving zero-shot learning by mitigating the hubness problem.
in iclr workshop papers.

long duong, hiroshi kanayama, tengfei ma, steven bird, and trevor cohn.
2016.
learning crosslingual id27s without bilingual corpora.
in emnlp. pages 1285 1295.

208 / 219

references iv

long duong, hiroshi kanayama, tengfei ma, steven bird, and trevor cohn.
2017.
multilingual training of crosslingual id27s.
in eacl. pages 894 904.

greg durrett, adam pauls, and dan klein. 2012.
syntactic transfer using a bilingual lexicon.
in emnlp. pages 1 11.

manaal faruqui and chris dyer. 2013.
an information theoretic approach to bilingual word id91.
in acl. pages 777 783.

manaal faruqui and chris dyer. 2014.
improving vector space word representations using multilingual correlation.
in eacl. pages 462 471.

  ric gaussier, jean-michel renders, irina matveeva, cyril goutte, and herv  
d  jean. 2004.
a geometric view on bilingual lexicon extraction from comparable corpora.
in acl. pages 526 533.

209 / 219

references v

stephan gouws, yoshua bengio, and greg corrado. 2015.
bilbowa: fast bilingual distributed representations without word alignments.
in icml. pages 748 756.

stephan gouws and anders s  gaard. 2015.
simple task-speci c bilingual id27s.
in naacl-hlt . pages 1386 1390.

jiang guo, wanxiang che, david yarowsky, haifeng wang, and ting liu. 2015.
cross-lingual id33 based on distributed representations.
in acl. pages 1234 1244.

aria haghighi, percy liang, taylor berg-kirkpatrick, and dan klein. 2008.
learning bilingual lexicons from monolingual corpora.
in acl. pages 771 779.

bradley hauer, garrett nicolai, and grzegorz kondrak. 2017.
id64 unsupervised bilingual lexicon induction.
in eacl. pages 619 624.

karl moritz hermann and phil blunsom. 2014a.
multilingual distributed representations without word alignment.
in iclr.

210 / 219

references vi

karl moritz hermann and phil blunsom. 2014b.
multilingual models for compositional distributed semantics.
in acl. pages 58 68.

geert heyman, ivan vuli  , and marie-francine moens. 2017.
bilingual lexicon induction by learning to combine word-level and character-level
representations.
in eacl. pages 1085 1095.

kejun huang, matt gardner, evangelos papalexakis, christos faloutsos, nikos
sidiropoulos, tom mitchell, partha p. talukdar, and xiao fu. 2015.
translation invariant id27s.
in emnlp. pages 1084 1088.

anders johannsen, h  ctor mart  nez alonso, and anders s  gaard. 2015.
any-language frame-id29.
in emnlp. pages 2062 2066.

alexandre klementiev, ivan titov, and binod bhattarai. 2012.
inducing crosslingual distributed representations of words.
in coling . pages 1459 1474.

211 / 219

references vii

tom     ko  isk  , karl moritz hermann, and phil blunsom. 2014.
learning bilingual word representations by marginalizing alignments.
in acl. pages 224 229.

angeliki lazaridou, georgiana dinu, and marco baroni. 2015a.
hubness and pollution: delving into cross-space mapping for zero-shot learning.
in acl. pages 270 280.

angeliki lazaridou, nghia the pham, and marco baroni. 2015b.
combining language and vision with a multimodal skip-gram model.
in naacl-hlt . pages 153 163.

omer levy, anders s  gaard, and yoav goldberg. 2017.
a strong baseline for learning cross-lingual id27s from sentence
alignments.
in eacl. pages 765 774.

thang luong, hieu pham, and christopher d. manning. 2015.
bilingual word representations with monolingual quality in mind.
in workshop on vector space modeling for natural language processing. pages
151 159.

212 / 219

references viii

tomas mikolov, quoc v. le, and ilya sutskever. 2013a.
exploiting similarities among languages for machine translation.
corr abs/1309.4168.

tomas mikolov, ilya sutskever, kai chen, gregory s. corrado, and je rey dean.
2013b.
distributed representations of words and phrases and their compositionality.
in nips. pages 3111 3119.

david m. mimno, hanna m. wallach, jason naradowsky, david a. smith, and
andrew mccallum. 2009.
polylingual topic models.
in emnlp. pages 880 889.

aditya mogadala and achim rettinger. 2016.
bilingual id27s from parallel and non-parallel corpora for
cross-language text classi cation.
in naacl-hlt . pages 692 702.

nikola mrk  i  , diarmuid    s  aghdha, tsung-hsien wen, blaise thomson, and
steve young. ????
neural belief tracker: data-driven dialogue state tracking.
in acl.

213 / 219

references ix

nikola mrk  i  , ivan vuli  , diarmuid    s  aghdha, ira leviant, roi reichart,
milica ga  i  , anna korhonen, and steve young. 2017.
semantic specialisation of distributional word vector spaces using monolingual
and cross-lingual constraints.
transactions of the acl .
yves peirsman and sebastian pad  . 2010.
cross-lingual induction of selectional preferences with bilingual vector spaces.
in naacl. pages 921 929.

janarthanan rajendran, mitesh m. khapra, sarath chandar, and balaraman
ravindran. 2016.
bridge correlational neural networks for multilingual multimodal representation
learning.
in naacl-hlt . pages 171 181.

sebastian ruder. 2017.
a survey of cross-lingual embedding models.
corr abs/1706.04902.

tianze shi, zhiyuan liu, yang liu, and maosong sun. 2015.
learning cross-lingual id27s via matrix co-factorization.
in acl. pages 567 572.

214 / 219

references x

samuel l. smith, david h. p. turban, steven hamblin, and nils y. hammerla.
2017.
o ine bilingual word vectors, orthogonal transformations and the inverted
softmax .
anders s  gaard,   eljko agi  , h  ctor mart  nez alonso, barbara plank, bernd
bohnet, and anders johannsen. 2015.
inverted indexing for cross-lingual nlp.
in acl. pages 1713 1722.

hubert soyer, pontus stenetorp, and akiko aizawa. 2015.
leveraging monolingual data for crosslingual compositional word representations.
in iclr.
oscar t  ckstr  m, ryan t. mcdonald, and jakob uszkoreit. 2012.
cross-lingual word clusters for direct transfer of linguistic structure.
in naacl-hlt . pages 477 487.

chen-tse tsai and dan roth. 2016.
cross-lingual wiki cation using multilingual embeddings.
in naacl-hlt . pages 589 598.

215 / 219

references xi

shyam upadhyay, manaal faruqui, chris dyer, and dan roth. 2016.
cross-lingual models of id27s: an empirical comparison.
in acl. pages 1661 1670.

simon   uster, ivan titov, and gertjan van noord. 2016.
bilingual learning of multi-sense embeddings with discrete autoencoders.
in naacl-hlt . pages 1346 1356.

ivan vuli  . 2017.
cross-lingual syntactically informed distributed word representations.
in eacl. pages 408 414.

ivan vuli  , douwe kiela, stephen clark, and marie-francine moens. 2016.
multi-modal representations for improved bilingual lexicon learning.
in acl. pages 188 194.

ivan vuli   and anna korhonen. 2016.
on the role of seed lexicons in learning bilingual id27s.
in acl. pages 247 257.

216 / 219

references xii

ivan vuli   and marie-francine moens. 2013.
a study on id64 bilingual vector spaces from non-parallel data (and
nothing else).
in emnlp. pages 1613 1624.

ivan vuli   and marie-francine moens. 2015a.
bilingual id27s from non-parallel document-aligned data applied to
bilingual lexicon induction.
in acl. pages 719 725.

ivan vuli   and marie-francine moens. 2015b.
monolingual and cross-lingual information retrieval models based on (bilingual)
id27s.
in sigir. pages 363 372.

ivan vuli   and marie-francine moens. 2016.
bilingual distributed word representations from document-aligned comparable
data.
journal of arti cial intelligence research 55:953 994.

217 / 219

references xiii

ivan vuli  , nikola mrk  i  , and anna korhonen. 2017.
cross-lingual induction and transfer of verb classes based on word vector space
specialisation.
in emnlp.
ivan vuli  , wim de smet, and marie-francine moens. 2011.
identifying word translations from comparable corpora using latent topic models.
in acl short papers. pages 479 484.

yogarshi vyas and marine carpuat. 2016.
sparse bilingual word representations for cross-lingual lexical entailment.
in naacl-hlt . pages 1187 1197.

min xiao and yuhong guo. 2014.
distributed word representation learning for cross-lingual id33.
in conll. pages 119 129.

chao xing, dong wang, chao liu, and yiye lin. 2015.
normalized id27 and orthogonal transform for bilingual word
translation.
in naacl-hlt . pages 1006 1011.

218 / 219

references xiv

meng zhang, yang liu, huanbo luan, and maosong sun. 2017.
adversarial training for unsupervised bilingual lexicon induction.
in acl. pages 1959 1970.

yuan zhang, david gaddy, regina barzilay, and tommi jaakkola. 2016.
ten pairs to tag   multilingual id52 via coarse mapping between
embeddings.
in naacl-hlt . pages 1307 1317.

will y. zou, richard socher, daniel cer, and christopher d. manning. 2013.
bilingual id27s for phrase-based machine translation.
in emnlp. pages 1393 1398.

219 / 219

