   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

visualizing layer representations in neural networks

   [14]go to the profile of siddhartha banerjee
   [15]siddhartha banerjee (button) blockedunblock (button)
   followfollowing
   sep 27, 2017

   visualizing and interpreting representations learned by machine
   learning / deep learning algorithms is pretty interesting! as the
   saying goes            a picture is worth a thousand words   , the same holds
   true with visualizations. a lot can be interpreted using the correct
   tools for visualization. in this post, i will cover some details on
   visualizing intermediate (hidden) layer features using dimension
   reduction techniques.

   we will work with the imdb sentiment classification task (25000
   training and 25000 test examples). the script to create a simple
   bidirectional lstm model using a dropout and predicting the sentiment
   (1 for positive and 0 for negative) using sigmoid activation is already
   provided in the [16]keras examples [17]here.

   note: if you have doubts on lstm, please read [18]this excellent blog
   by colah.

   ok, let   s get started!!

   the first step is to build the model and train it. we will use the
   example code as-is with a minor modification. we will keep the test
   data aside and use 20% of the training data itself as the validation
   set. the following part of the code will retrieve the imdb dataset
   (from keras.datasets), create the lstm model and train the model with
   the training data.

   iframe: [19]/media/0d3f16ab891d59c42f357c87319f8b83?postid=bd9b62447e38

   now, comes the interesting part! we want to see how has the lstm been
   able to learn the representations so as to differentiate between
   positive imdb reviews from the negative ones. obviously, we can get an
   idea from precision, recall and f1-score measures. however, being able
   to visually see the differences in a low-dimensional space would be
   much more fun!

   in order to obtain the hidden-layer representation, we will first
   truncate the model at the lstm layer. thereafter, we will load the
   model with the weights that the model has learnt. a better way to do
   this is create a new model with the same steps (until the layer you
   want) and load the weights from the model. layers in keras models are
   iterable. the code below shows how you can iterate through the model
   layers and see the configuration.
for layer in model.layers:
    print(layer.name, layer.trainable)
    print('layer configuration:')
    print(layer.get_config(), end='\n{}\n'.format('----'*10))

   for example, the bidirectional lstm layer configuration is the
   following:
bidirectional_2 true
layer configuration:
{'name': 'bidirectional_2', 'trainable': true, 'layer': {'class_name': 'lstm', '
config': {'name': 'lstm_2', 'trainable': true, 'return_sequences': false, 'go_ba
ckwards': false, 'stateful': false, 'unroll': false, 'implementation': 0, 'units
': 64, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias':
 true, 'kernel_initializer': {'class_name': 'variancescaling', 'config': {'scale
': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': none}}, 'recurrent
_initializer': {'class_name': 'orthogonal', 'config': {'gain': 1.0, 'seed': none
}}, 'bias_initializer': {'class_name': 'zeros', 'config': {}}, 'unit_forget_bias
': true, 'kernel_regularizer': none, 'recurrent_regularizer': none, 'bias_regula
rizer': none, 'activity_regularizer': none, 'kernel_constraint': none, 'recurren
t_constraint': none, 'bias_constraint': none, 'dropout': 0.0, 'recurrent_dropout
': 0.0}}, 'merge_mode': 'concat'}

   the weights of each layer can be obtained using:
trained_model.layers[i].get_weights()

   the code to create the truncated model is given below. first, we create
   a truncated model. note that we do model.add(..) only until the
   bidirectional lstm layer. then we set the weights from the trained
   model (model). then, we predict the features for the test instances
   (x_test).

   iframe: [20]/media/69c1753b4d717bfef4d8913d13a3edc2?postid=bd9b62447e38

   the hidden_features has a shape of (25000, 128) for 25000 instances
   with 128 dimensions. we get 128 as the dimensionality of lstm is 64 and
   there are 2 classes. hence, 64 x 2 = 128.

   next, we will apply id84 to reduce the 128 features
   to a lower dimension. for visualization, id167 ([21]maaten and hinton,
   2008) has become really popular. however, as per my experience, id167
   does not scale very well with several features and more than a few
   thousand instances. therefore, i decided to first reduce dimensions
   using principal component analysis (pca) following by id167 to
   2d-space.

     if you are interested on details about id167, please read [22]this
     amazing blog.

   combining pca (from 128 to 20) and id167 (from 20 to 2) for
   id84, here is the code. in this code, we used the
   pca results for the first 5000 test instances. you can increase it.

   our pca variance is ~0.99, which implies that the reduced dimensions do
   represent the hidden features well (scale is 0 to 1). please note that
   running id167 will take some time. (so may be you can go grab a cup of
   coffee.)

     i am not aware of faster id167 implementations than the one that
     ships with scikit-learn package. if you are, please let me know by
     commenting below.

   iframe: [23]/media/7aae903c2ee2dea40987b4190dcc7151?postid=bd9b62447e38

   now that we have the dimensionality reduced features, we will plot. we
   will label them with their actual classes (0 and 1). here is the code
   for visualization.

   iframe: [24]/media/ce447be6fb6b8ecc756d3cbec2df1da5?postid=bd9b62447e38

   we convert the test class array (y_test) to make it one-hot using the
   to_categorical function. then, we create a color map and based on the
   values of y, plot the reduced dimensions (tsne_results) on the scatter
   plot.
   [1*ecudv_u_pv6eeqsbz25eua.png]
   id167 visualization of hidden features for lstm model trained on imdb
   sentiment classification dataset

     please note that we reduced y_test_cat to 5000 instances too just
     like the tsne_results. you can change it and allow it to run longer.

   also, the classification report is shown for all the 25000 test
   instances. about 84% f1-score with a model trained for just 4 epochs.
   cool! here is the scatter plot we obtained.

   as can be seen from the plot, the blue (0         negative class) is fairly
   separable from the orange (1-positive class). obviously, there are
   certain overlaps and the reason why our f-score is around 84 and not
   closer to 100 :). understanding and visualizing the outputs at
   different layers can help understand which layer is causing major
   errors in learning representations.

     i hope you find this article useful. i would love to hear your
     comments and thoughts. also, do share your experiences with
     visualization.

     also, feel free to get in touch with me via [25]linkedin.

   iframe: [26]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=bd9b62447e38

   [27][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [28][1*v-ppfkswhbvlwwamsvhhwg.png]
   [29][1*wt2auqisieaozxj-i7brdq.png]

     * [30]machine learning
     * [31]deep learning
     * [32]keras
     * [33]data visualization
     * [34]artificial intelligence

   (button)
   (button)
   (button) 183 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of siddhartha banerjee

[36]siddhartha banerjee

   research scientist by profession. loves working on nlp.

     (button) follow
   [37]becoming human: artificial intelligence magazine

[38]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 183
     * (button)
     *
     *

   [39]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [40]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bd9b62447e38
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/visualizing-representations-bd9b62447e38&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/visualizing-representations-bd9b62447e38&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_5sarmrjc6iec---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@sidd2006?source=post_header_lockup
  15. https://becominghuman.ai/@sidd2006
  16. https://keras.io/
  17. https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py
  18. http://colah.github.io/posts/2015-08-understanding-lstms/
  19. https://becominghuman.ai/media/0d3f16ab891d59c42f357c87319f8b83?postid=bd9b62447e38
  20. https://becominghuman.ai/media/69c1753b4d717bfef4d8913d13a3edc2?postid=bd9b62447e38
  21. http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  22. https://distill.pub/2016/misread-tsne/
  23. https://becominghuman.ai/media/7aae903c2ee2dea40987b4190dcc7151?postid=bd9b62447e38
  24. https://becominghuman.ai/media/ce447be6fb6b8ecc756d3cbec2df1da5?postid=bd9b62447e38
  25. https://www.linkedin.com/in/sidbankgp/
  26. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=bd9b62447e38
  27. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  28. https://upscri.be/8f5f8b
  29. https://becominghuman.ai/write-for-us-48270209de63
  30. https://becominghuman.ai/tagged/machine-learning?source=post
  31. https://becominghuman.ai/tagged/deep-learning?source=post
  32. https://becominghuman.ai/tagged/keras?source=post
  33. https://becominghuman.ai/tagged/data-visualization?source=post
  34. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  35. https://becominghuman.ai/@sidd2006?source=footer_card
  36. https://becominghuman.ai/@sidd2006
  37. https://becominghuman.ai/?source=footer_card
  38. https://becominghuman.ai/?source=footer_card
  39. https://becominghuman.ai/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/bd9b62447e38/share/twitter
  43. https://medium.com/p/bd9b62447e38/share/facebook
  44. https://medium.com/p/bd9b62447e38/share/twitter
  45. https://medium.com/p/bd9b62447e38/share/facebook
