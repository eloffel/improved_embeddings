   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

   [1*fbxs211lj7zweqzramdedq.png]

7 types of id158s for natural language processing

   [9]go to the profile of data monsters
   [10]data monsters (button) blockedunblock (button) followfollowing
   sep 26, 2017

   by olga davydova

   what is an id158? how does it work? what types of
   id158s exist? how are different types of artificial
   neural networks used in natural language processing? we will discuss
   all these questions in the following article.

   [11]an id158 (ann) is a computational nonlinear
   model based on the neural structure of the brain that is able to learn
   to perform tasks like classification, prediction, decision-making,
   visualization, and others just by considering examples.

   an id158 consists of artificial neurons or
   processing elements and is organized in three interconnected layers:
   input, hidden that may include more than one layer, and output.
   [0*wi9kgvldd_q0bwtt.]

   an id158
   [12]https://en.wikipedia.org/wiki/artificial_neural_network#/media/file
   :colored_neural_network.svg

   the input layer contains input neurons that send information to the
   hidden layer. the hidden layer sends data to the output layer. every
   neuron has weighted inputs ([13]synapses), an [14]activation function
   (defines the output given an input), and one output. synapses are the
   adjustable parameters that convert a neural network to a parameterized
   system.
   [0*po7d6t18vwdulacg.]

   artificial neuron with four inputs
   [15]http://en.citizendium.org/wiki/file:artificialneuron.png

   the weighted sum of the inputs produces the activation signal that is
   passed to the activation function to obtain one output from the neuron.
   the commonly used id180 are linear, step, sigmoid, tanh,
   and rectified linear unit (relu) functions.

   linear function

   f(x)=ax
   [0*ihzanim0qx1nrlts.]

   step function
   [0*vx0yjlp7vgvpie88.]
   [0*c-isbldwdoezvto9.]

   logistic (sigmoid) function
   [0*tzwfgagvef43kiks.]
   [0*xcscqxamvjbbfyk7.]

   tanh function
   [0*4ljhoie3z60ivc5g.]
   [0*mks2p9ty2blzd4fn.]

   rectified linear unit (relu) function
   [0*d2loxl28zol52blt.]
   [0*6w7bcyz0r1x8prkv.]

   training is the weights optimizing process in which the error of
   predictions is minimized and the network reaches a specified level of
   accuracy. the method mostly used to determine the error contribution of
   each neuron is called [16]id26 that calculates the gradient
   of the id168.

   it is possible to make the system more flexible and more powerful by
   using additional hidden layers. id158s with
   multiple hidden layers between the input and output layers are called
   [17]deep neural networks (dnns), and they can model complex nonlinear
   relationships.

1. multilayer id88 (mlp)

   [0*jac99pp0pwl6ypxk.]

   a id88
   [18]https://upload.wikimedia.org/wikipedia/ru/d/de/neuro.png

   a multilayer id88 (mlp) has three or more layers. it utilizes a
   nonlinear activation function (mainly hyperbolic tangent or logistic
   function) that lets it classify data that is not linearly separable.
   every node in a layer connects to each node in the following layer
   making the network fully connected. for example, multilayer id88
   natural language processing (nlp) applications are id103
   and machine translation.

2. convolutional neural network (id98)

   [0*3rz2me5bhsks4o7h.]

   typical id98 architecture
   [19]https://en.wikipedia.org/wiki/convolutional_neural_network#/media/f
   ile:typical_id98.png

   a convolutional neural network (id98) contains one or more convolutional
   layers, pooling or fully connected, and uses a variation of multilayer
   id88s discussed above. convolutional layers use a [20]convolution
   operation to the input passing the result to the next layer. this
   operation allows the network to be deeper with much fewer parameters.

   convolutional neural networks show outstanding results in image and
   speech applications. yoon kim in [21]convolutional neural networks for
   sentence classification describes the process and the results of text
   classification tasks using id98s [22][1]. he presents a model built on
   top of [23]id97, conducts a series of experiments with it, and
   tests it against several benchmarks, demonstrating that the model
   performs excellent.

   in [24]text understanding from scratch, xiang zhang and yann lecun,
   demonstrate that id98s can achieve outstanding performance without the
   knowledge of words, phrases, sentences and any other syntactic or
   semantic structures with regards to a human language [25][2]. semantic
   parsing [26][3], paraphrase detection [27][4], id103
   [28][5] are also the applications of id98s.

3. id56 (id56)

   [0*ik5hraagt7x4yo9k.]

   a simple id56 architecture
   [29]https://upload.wikimedia.org/wikipedia/commons/6/60/simple_recursiv
   e_neural_network.svg

   a id56 (id56) is a type of deep neural network
   formed by applying the same set of weights recursively over a structure
   to make a id170 over variable-size input structures, or
   a scalar prediction on it, by traversing a given structure in
   topological order [30][6]. in the simplest architecture, a nonlinearity
   such as tanh, and a weight matrix that is shared across the whole
   network are used to combine nodes into parents.

4. recurrent neural network (id56)

   a recurrent neural network (id56), unlike a [31]feedforward neural
   network, is a variant of a recursive id158 in which
   connections between neurons make a directed cycle. it means that output
   depends not only on the present inputs but also on the previous step   s
   neuron state. this memory lets users solve nlp problems like connected
   handwriting recognition or id103. in a paper, [32]natural
   language generation, id141 and summarization of user reviews
   with recurrent neural networks, authors demonstrate a recurrent neural
   network (id56) model that can generate novel sentences and document
   summaries [33][7].

   siwei lai, liheng xu, kang liu, and jun zhao created a recurrent
   convolutional neural network for text classification without
   human-designed features and described it in [34]recurrent convolutional
   neural networks for text classification. their model was compared to
   existing text classification methods like bag of words, bigrams + lr,
   id166, lda, tree kernels, id56, and id98. it was shown
   that their model outperforms traditional methods for all used data sets
   [35][8].

5. long short-term memory (lstm)

   [0*yuyrqiy1wkuar-3f.]

   a peephole lstm block with input, output, and forget gates.
   [36]https://upload.wikimedia.org/wikipedia/commons/5/53/peephole_long_s
   hort-term_memory.svg

   long short-term memory (lstm) is a specific recurrent neural network
   (id56) architecture that was designed to model temporal sequences and
   their long-range dependencies more accurately than conventional id56s
   [37][9]. lstm does not use activation function within its recurrent
   components, the stored values are not modified, and the gradient does
   not tend to vanish during training. usually, lstm units are implemented
   in    blocks    with several units. these blocks have three or four    gates   
   (for example, input gate, forget gate, output gate) that control
   information flow drawing on the logistic function.

   in [38]long short-term memory recurrent neural network architectures
   for large scale acoustic modeling, hasim sak, andrew senior, and
   fran  oise beaufays showed that the deep lstm id56 architectures achieve
   state-of-the-art performance for large scale acoustic modeling.

   in the work, [39]part-of-speech tagging with bidirectional long
   short-term memory recurrent neural network by peilu wang, yao qian,
   frank k. soong, lei he, and hai zhao, a model for [40]part-of-speech
   (pos) tagging was presented [41][10]. the model achieved a performance
   of 97.40% tagging accuracy. apple, amazon, google, microsoft and other
   companies incorporated lstm as a fundamental element into their
   products.

6. sequence-to-sequence models

   usually, a sequence-to-sequence model consists of two recurrent neural
   networks: an [42]encoder that processes the input and a [43]decoder
   that produces the output. encoder and decoder can use the same or
   different sets of parameters.

   sequence-to-sequence models are mainly used in id53
   systems, chatbots, and machine translation. such multi-layer cells have
   been successfully used in sequence-to-sequence models for translation
   in [44]sequence to sequence learning with neural networks study
   [45][11].

   in [46]paraphrase detection using recursive autoencoder, a novel
   recursive autoencoder architecture is presented. the representations
   are vectors in an n-dimensional semantic space where phrases with
   similar meanings are close to each other [47][12].

7. shallow neural networks

   besides deep neural networks, shallow models are also popular and
   useful tools. for example, [48]id97 is a group of shallow two-layer
   models that are used for producing [49]id27s. presented in
   [50]efficient estimation of word representations in vector space,
   id97 takes a large corpus of text as its input and produces a
   vector space [51][13]. every word in the corpus obtains the
   corresponding vector in this space. the distinctive feature is that
   words from common contexts in the corpus are located close to one
   another in the vector space.

summary

   in this paper, we described different variants of artificial neural
   networks, such as deep multilayer id88 (mlp), convolutional
   neural network (id98), id56 (id56), recurrent neural
   network (id56), long short-term memory (lstm), sequence-to-sequence
   model, and shallow neural networks including id97 for word
   embeddings. we showed how these networks function and how different
   types of them are used in natural language processing tasks. we
   demonstrated that convolutional neural networks are primarily utilized
   for text classification tasks while recurrent neural networks are
   commonly used for id86 or machine translation.
   in the next part of this series, we will study existing tools and
   libraries for the discussed neural network types.

resources

   1. [52]http://www.aclweb.org/anthology/d14-1181

   2. [53]https://arxiv.org/pdf/1502.01710.pdf

   3. [54]http://www.aclweb.org/anthology/p15-1128

   4. [55]https://www.aclweb.org/anthology/k15-1013

   5.
   [56]https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02
   /id98_aslptrans2-14.pdf

   6. [57]https://en.wikipedia.org/wiki/recursive_neural_network

   7. [58]http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf

   8.
   [59]https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552

   9.
   [60]https://wiki.inf.ed.ac.uk/twiki/pub/cstr/listenterm1201415/sak2.pdf

   10. [61]https://arxiv.org/pdf/1510.06168.pdf

   11. [62]https://arxiv.org/pdf/1409.3215.pdf

   12.
   [63]https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf

   13. [64]https://arxiv.org/pdf/1301.3781.pdf

     * [65]machine learning
     * [66]neural networks
     * [67]nlp
     * [68]deep learning

   (button)
   (button)
   (button) 1.6k claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [69]go to the profile of data monsters

[70]data monsters

   [71]https://datamonsters.com

     * (button)
       (button) 1.6k
     * (button)
     *
     *

   [72]go to the profile of data monsters
   never miss a story from data monsters, when you sign up for medium.
   [73]learn more
   never miss a story from data monsters
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/64ca9ebfa3b2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@datamonsters?source=post_header_lockup
  10. https://medium.com/@datamonsters
  11. https://en.wikipedia.org/wiki/artificial_neural_network
  12. https://en.wikipedia.org/wiki/artificial_neural_network#/media/file:colored_neural_network.svg
  13. https://en.wikipedia.org/wiki/synaptic_weight
  14. https://en.wikipedia.org/wiki/activation_function
  15. http://en.citizendium.org/wiki/file:artificialneuron.png
  16. https://en.wikipedia.org/wiki/id26
  17. https://en.wikipedia.org/wiki/deep_learning#deep_neural_networks
  18. https://upload.wikimedia.org/wikipedia/ru/d/de/neuro.png
  19. https://en.wikipedia.org/wiki/convolutional_neural_network#/media/file:typical_id98.png
  20. https://en.wikipedia.org/wiki/convolution
  21. http://www.aclweb.org/anthology/d14-1181
  22. http://www.aclweb.org/anthology/d14-1181
  23. https://en.wikipedia.org/wiki/id97
  24. https://arxiv.org/pdf/1502.01710.pdf
  25. https://arxiv.org/pdf/1502.01710.pdf
  26. http://www.aclweb.org/anthology/p15-1128
  27. https://www.aclweb.org/anthology/k15-1013
  28. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/id98_aslptrans2-14.pdf
  29. https://upload.wikimedia.org/wikipedia/commons/6/60/simple_recursive_neural_network.svg
  30. https://en.wikipedia.org/wiki/recursive_neural_network
  31. https://en.wikipedia.org/wiki/feedforward_neural_network
  32. http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf
  33. http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf
  34. https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552
  35. https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552
  36. https://upload.wikimedia.org/wikipedia/commons/5/53/peephole_long_short-term_memory.svg
  37. https://wiki.inf.ed.ac.uk/twiki/pub/cstr/listenterm1201415/sak2.pdf
  38. https://wiki.inf.ed.ac.uk/twiki/pub/cstr/listenterm1201415/sak2.pdf
  39. https://arxiv.org/pdf/1510.06168.pdf
  40. https://en.wikipedia.org/wiki/part-of-speech_tagging
  41. https://arxiv.org/pdf/1510.06168.pdf
  42. https://en.wikipedia.org/wiki/artificial_neural_network#encoder.e2.80.93decoder_networks
  43. https://en.wikipedia.org/wiki/artificial_neural_network#encoder.e2.80.93decoder_networks
  44. https://arxiv.org/pdf/1409.3215.pdf
  45. https://arxiv.org/pdf/1409.3215.pdf
  46. https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf
  47. https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf
  48. https://ru.wikipedia.org/wiki/id97
  49. https://en.wikipedia.org/wiki/word_embedding
  50. https://arxiv.org/pdf/1301.3781.pdf
  51. https://arxiv.org/pdf/1301.3781.pdf
  52. http://www.aclweb.org/anthology/d14-1181
  53. https://arxiv.org/pdf/1502.01710.pdf
  54. http://www.aclweb.org/anthology/p15-1128
  55. https://www.aclweb.org/anthology/k15-1013
  56. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/id98_aslptrans2-14.pdf
  57. https://en.wikipedia.org/wiki/recursive_neural_network
  58. http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf
  59. https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552
  60. https://wiki.inf.ed.ac.uk/twiki/pub/cstr/listenterm1201415/sak2.pdf
  61. https://arxiv.org/pdf/1510.06168.pdf
  62. https://arxiv.org/pdf/1409.3215.pdf
  63. https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf
  64. https://arxiv.org/pdf/1301.3781.pdf
  65. https://medium.com/tag/machine-learning?source=post
  66. https://medium.com/tag/neural-networks?source=post
  67. https://medium.com/tag/nlp?source=post
  68. https://medium.com/tag/deep-learning?source=post
  69. https://medium.com/@datamonsters?source=footer_card
  70. https://medium.com/@datamonsters
  71. https://datamonsters.com/
  72. https://medium.com/@datamonsters
  73. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  75. https://medium.com/p/64ca9ebfa3b2/share/twitter
  76. https://medium.com/p/64ca9ebfa3b2/share/facebook
  77. https://medium.com/p/64ca9ebfa3b2/share/twitter
  78. https://medium.com/p/64ca9ebfa3b2/share/facebook
