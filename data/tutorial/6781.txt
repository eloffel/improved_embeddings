   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

what metrics should be used for evaluating a model on an imbalanced data set?
([16]precision + recall or [17]roc=tpr+fpr)

   [18]go to the profile of shir meir lador
   [19]shir meir lador (button) blockedunblock (button) followfollowing
   sep 5, 2017

   i always thought the subject of metrics to be somehow confusing,
   specifically when the data set is imbalanced (as happens so often in
   our usual problems). in order to clarify things i   ve decided to test a
   few simple examples of an imbalanced data sets with the different type
   of metrics and see which reflects more correctly the model
   performance         roc curve metrics         tpr and fpr or precision or recall.

definitions

   so let   s start by reminding ourselves the definitions.

   in the [20]roc curve we look at:
   tpr (true positive rate) = # true positives / # positives = recall = tp
   / (tp+fn)
   fpr (false positive rate) = # false positives / # negatives = fp /
   (fp+tn)

   here we will focus on the tpr (true positive rate) and fpr (false
   positive rate) of a single point (this will indicate the general
   performance of the [21]roc curve which consists of the tpr and fpr
   through various id203 thresholds).

   [22]precision and recall are:
   precision =# true positives / # predicted positive = tp/(tp+fp)
   recall = # true positives / # positives = tp / (tp+fn)

what is the difference?

   recall and true positive rate (tpr) are exactly the same. so the
   difference is in the precision and the false positive rate.

   the main difference between these two types of metrics is that
   precision denominator contains the false positives while false positive
   rate denominator contains the true negatives.
   while precision measures the id203 of a sample classified as
   positive to actually be positive, the false positive rate measures the
   ratio of false positives within the negative samples.

with a large number of negative samples         precision is probably better

   if the number of negative samples is very large (a.k.a imbalance data
   set) the false positive rate increases more slowly. because the true
   negatives (in the fpr denominator         (fp+tn)) would probably be very
   high and make this metric smaller.
   precision however, is not affected by a large number of negative
   samples, that   s because it measures the number of true positives out of
   the samples predicted as positives (tp+fp).

   precision is more focused in the positive class than in the negative
   class, it actually measures the id203 of correct detection of
   positive values, while fpr and tpr (roc metrics) measure the ability to
   distinguish between the classes.

examples

   the examples are just a way to illustrate the metrics with a visual
   image.

   for visualisation purposes, here is a simple function which plot the
   decision region of a given model.
def plot_model_boundaries(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5, npoints=40)
:
    xx = np.linspace(xmin, xmax, npoints)
    yy = np.linspace(ymin, ymax, npoints)
    xv, yv = np.meshgrid(xx, yy)
    xv, yv = xv.flatten(), yv.flatten()
    labels = model.predict(np.c_[xv,yv])
    plt.scatter(xv[labels==1],yv[labels==1],color='r', alpha=0.02, marker='o', s
=300)
    plt.scatter(xv[labels==0],yv[labels==0],color='b', alpha=0.02, marker='o', s
=300)
    plt.ylim([xmin, xmax])
    plt.xlim([ymin, ymax])

   i created a very simple data set (with 10 points) and trained a linear
   id166 model. below is the figure of the data set and the model perfect
   decision regions.
from matplotlib import pyplot as plt
from sklearn.id166 import linearsvc
from sklearn.metrics import precision_score, recall_score, roc_curve
np.random.seed(1)
x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.8*n
p.random.rand(10,2)
x[-1,0]+=1
x[-1,-1]+=1
y = np.concatenate([np.ones(9), np.zeros(1)])
model = linearsvc()
model.fit(x,y)
predicted_labels = model.predict(x)
plot_model_boundaries(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5)
plt.scatter(x[y==0,0],x[y==0,1], color='b')
plt.scatter(x[y==1,0],x[y==1,1], color='r');
fpr, tpr, thresholds = roc_curve(y, predicted_labels)
plt.title("precision = {}, recall = {}, fpr = {}, tpr = {}".format(
    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr
[0], tpr[0]));

   [1*hp33ag4y3tns724d00vvpa.png]
   my simple data set and the model decision region

example #1         majority of positive samples and         all positive samples are
detected but there are also false positives         roc is a better metric

   we have 10 samples in test dataset. 9 samples are positive and 1 is
   negative.

   as we saw before         all metrics are perfect in the case of the perfect
   model, but now we look at a naive model which predicts everything
   positive.

   in this case our basic metrics are: tp = 9, fp = 1, tn = 0, fn = 0.
   then we can calculate the advanced metrics:

   precision = tp/(tp+fp) = 0.9, recall = tp/(tp+fn)= 1.0.
   in this case the precision and recall are both very high, but we have a
   poor classifier.

   tpr = tp/(tp+fn) = 1.0, fpr = fp/(fp+tn) = 1.0.
   because the fpr is very high, we can identify that this is not a good
   classifier.
np.random.seed(1)
x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 1.2*n
p.random.rand(10,2)
x[-1,0]+=1
x[-1,-1]+=1
y = np.concatenate([np.ones(9), np.zeros(1)])
model = linearsvc()
model.fit(x,y)
predicted_labels = model.predict(x)
plot_model_boundaries(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5)
plt.scatter(x[y==0,0],x[y==0,1], color='b')
plt.scatter(x[y==1,0],x[y==1,1], color='r');
fpr, tpr, thresholds = roc_curve(y, predicted_labels)
plt.title("precision = {}, recall = {}, fpr = {}, tpr = {}".format(
    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr
[1], tpr[1]));

   [1*savpjdq-zl4c6vlbnl8ejq.png]
   the classifier predicts all the labels as positive

example #1    same data set with opposite labels         both metrics are 0 because
there is no    detection   

   now, we switch the labels         9 samples are negative and 1 is positive.
   we have a model which predicts everything negative. our basic metrics
   are now: tp = 0, fp = 0, tn = 9, fn = 1. and the advanced metrics are
   all zeros because there are no true positive and no false positives.
np.random.seed(1)
x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 1.2*n
p.random.rand(10,2)
x[-1,0]+=1
x[-1,-1]+=1
y = np.concatenate([np.zeros(9), np.ones(1)])
model = linearsvc()
model.fit(x,y)
predicted_labels = model.predict(x)
plot_model_boundaries(model, xmin=0, xmax=1.5, ymin=0, ymax=1.5)
plt.scatter(x[y==0,0],x[y==0,1], color='b')
plt.scatter(x[y==1,0],x[y==1,1], color='r');
fpr, tpr, thresholds = roc_curve(y, predicted_labels)
plt.title("precision = {}, recall = {}, fpr = {}, tpr = {}".format(
    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr
[0], tpr[0]));

   [1*kedwvhfb0w9h8b9wb5owww.png]
   the classifier predicts all the labels as negatives so all the metrics
   are zeros

example #2         majority of positive samples         all positive samples are detected
but there are also false positives    roc is a better metric

   this example is similar to example #1 and shows basically the same
   thing.
   in this case 8 samples are positive and 2 are negative.
   the model predicts 9 of the samples as positives (8 true positives and
   1 negative) and one as negative.
   the basic metrics are: tp = 8, fp = 1, tn = 1, fn = 0.

   the advanced metrics are:

   precision = tp/(tp+fp) = 8/9 = 0.89, recall = tp/(tp+fn)= 1.
   the precision and recall are both very high, because the performance on
   the positive class is good.

   tpr = tp/(tp+fn) = 1, fpr = fp/(fp+tn) = 1/2 = 0.5.
   the false positive rate is 0.5, which is pretty high, because we have 1
   false positive out of two negatives         that   s a lot!
   notice that high fpr is actually the same thing as a low recall for the
   negative class.
x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*n
p.random.rand(10,2)
x[-1,0]+=1
x[-1,-1]+=1
y = np.concatenate([np.zeros(1), np.ones(8), np.zeros(1)])
model = linearsvc()
model.fit(x,y)
predicted_labels = model.predict(x)
plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)
plt.scatter(x[y==0,0],x[y==0,1], color='b')
plt.scatter(x[y==1,0],x[y==1,1], color='r');
fpr, tpr, thresholds = roc_curve(y, predicted_labels)
plt.title("precision = {:.2f}, recall = {}, fpr = {}, tpr = {}".format(
    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr
[1], tpr[1]));

   [1*hu6ctpmo9wvmwr-huz_vua.png]
   one negative sample is classified correctly as negative and the other
   is classified as positive         this caused a small decrease in precision
   and a relatively high value in fpr

example #2         opposite labels         both metrics are the same in this case

   now 8 samples are negative and 2 are positive.
   we have a model which predicts everything negative but one positive
   sample (which is actually positive).
   the basic metrics are: tp = 1, fp = 0, tn = 8, fn = 1.

   the advanced metrics are:

   precision = tp/(tp+fp) = 1, recall = tp/(tp+fn)= 1/(1+1) = 0.5.
   tpr = tp/(tp+fn) = 1/2 = 0.5, fpr = fp/(fp+tn) = 0.

   in this case both metrics give the same information.
x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*n
p.random.rand(10,2)
x[-1,0]+=1
x[-1,-1]+=1
y = np.concatenate([np.ones(1), np.zeros(8), np.ones(1)])
model = linearsvc()
model.fit(x,y)
predicted_labels = model.predict(x)
plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)
plt.scatter(x[y==0,0],x[y==0,1], color=   b   )
plt.scatter(x[y==1,0],x[y==1,1], color=   r   );
fpr, tpr, thresholds = roc_curve(y, predicted_labels)
plt.title(   precision = {:.2f}, recall = {}, fpr = {}, tpr = {}   .format(
 precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr[0]
, tpr[0]));

   [1*m8pj3m26ns1ygfy629xcaw.png]
   one positive sample is detected correctly and the other is not

example #3         majority of positive samples and not all positive samples are
detected         both metrics are the same in this case

   now, 9 samples are positive and 1 is negative. the model predicts 7 of
   the samples as positive (all are positive) and 3 as negative.
   the basic metrics are: tp = 7, fp = 0, tn = 1, fn = 2.

   the advanced metrics are:

   precision = tp/(tp+fp) = 1, recall = tp/(tp+fn)= 7/9 = 0.78
   the precision and recall are both very high, because the performance on
   the positive class is good.
   tpr = tp/(tp+fn) = 7/9 = 0.78, fpr = fp/(fp+tn) = 0.

   both metrics give similar results in this case.
np.random.seed(1)
x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*n
p.random.rand(10,2)
x[-1,0]+=1
x[-1,-1]+=1
x[-2,0]+=0.8
x[-2,-1]+=0.8
x[-3,0]+=0.8
x[-3,-1]+=0.8
y = np.concatenate([np.zeros(7), np.ones(3)])
y = 1-y
model = linearsvc()
model.fit(x,y)
y = np.concatenate([np.zeros(9), np.ones(1)])
y = 1-y
predicted_labels = model.predict(x)
plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)
plt.scatter(x[y==0,0],x[y==0,1], color='b')
plt.scatter(x[y==1,0],x[y==1,1], color='r');
fpr, tpr, thresholds = roc_curve(y, predicted_labels)
plt.title("precision = {:.2f}, recall = {:.2f}, fpr = {:.2f}, tpr = {:.2f}".form
at(
    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr
[0], tpr[0]));

   [1*kosdzatxl8uh--ogx05fwg.png]
   both metrics are the same in this case

example #3         opposite labels    majority of negative samples and not all
positive samples are detected    precision and recall are better

   now we switch the labels         9 samples are negative and 1 is positive.
   the model predicts 3 of the samples as positive (only one of them is
   actually positive) and 7 as negative.
   the basic metrics are: tp = 1, fp = 2, tn = 7, fn = 0.

   the advanced metrics are:

   precision = tp/(tp+fp) = 0.33, recall = tp/(tp+fn)= 1.

   tpr = tp/(tp+fn) = 1, fpr = fp/(fp+tn) = 2/9 = 0.22.

   in this case the precision is low and the fpr is not as high as we
   would want it to be. the fpr is not high because we have a lot of true
   negatives, due to the imbalanced data set. in this case the poor
   detection ability is reflected best in the precision.
np.random.seed(1)
x = np.concatenate([np.zeros((8, 2)), np.zeros((1,2)), np.zeros((1,2))]) + 0.9*n
p.random.rand(10,2)
x[-1,0]+=1
x[-1,-1]+=1
x[-2,0]+=0.8
x[-2,-1]+=0.8
x[-3,0]+=0.8
x[-3,-1]+=0.8
y = np.concatenate([np.zeros(7), np.ones(3)])
model = linearsvc()
model.fit(x,y)
y = np.concatenate([np.zeros(9), np.ones(1)])
predicted_labels = model.predict(x)
plot_model_boundaries(model, xmin=0, xmax=2, ymin=0, ymax=2)
plt.scatter(x[y==0,0],x[y==0,1], color='b')
plt.scatter(x[y==1,0],x[y==1,1], color='r');
fpr, tpr, thresholds = roc_curve(y, predicted_labels)
plt.title("precision = {:.2f}, recall = {}, fpr = {:.2f}, tpr = {}".format(
    precision_score(y, predicted_labels), recall_score(y, predicted_labels), fpr
[1], tpr[1]));

   [1*sb-zahnu4xyvgoaqnqzghw.png]
   in this case the poor detection ability is reflected best in the
   precision while the fpr is relatively not that high because of the
   large amount of negative samples

final intuition to metric selection

    1. use precision and recall to focus on small positive class         when
       the positive class is smaller and the ability to detect correctly
       positive samples is our main focus (correct detection of negatives
       examples is less important to the problem) we should use precision
       and recall.
    2. use roc when both classes detection is equally important         when we
       want to give equal weight to both classes prediction ability we
       should look at the roc curve.
    3. use roc when the positives are the majority or switch the labels
       and use precision and recall         when the positive class is larger we
       should probably use the roc metrics because the precision and
       recall would reflect mostly the ability of prediction of the
       positive class and not the negative class which will naturally be
       harder to detect due to the smaller number of samples. if the
       negative class (the minority in this case) is more important, we
       can switch the labels and use precision and recall (as we saw in
       the examples above         switching the labels can change everything).

     * [23]machine learning
     * [24]data science
     * [25]metrics
     * [26]tech
     * [27]towards data science

   (button)
   (button)
   (button) 900 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of shir meir lador

[29]shir meir lador

   data scientist and organizer of pydata tel aviv meetups

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 900
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e2e79252aeba
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_saufk4ttdhcz---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://en.wikipedia.org/wiki/precision_and_recall
  17. https://en.wikipedia.org/wiki/receiver_operating_characteristic
  18. https://towardsdatascience.com/@datalady?source=post_header_lockup
  19. https://towardsdatascience.com/@datalady
  20. https://en.wikipedia.org/wiki/receiver_operating_characteristic
  21. https://en.wikipedia.org/wiki/receiver_operating_characteristic
  22. https://en.wikipedia.org/wiki/precision_and_recall
  23. https://towardsdatascience.com/tagged/machine-learning?source=post
  24. https://towardsdatascience.com/tagged/data-science?source=post
  25. https://towardsdatascience.com/tagged/metrics?source=post
  26. https://towardsdatascience.com/tagged/tech?source=post
  27. https://towardsdatascience.com/tagged/towards-data-science?source=post
  28. https://towardsdatascience.com/@datalady?source=footer_card
  29. https://towardsdatascience.com/@datalady
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/e2e79252aeba/share/twitter
  36. https://medium.com/p/e2e79252aeba/share/facebook
  37. https://medium.com/p/e2e79252aeba/share/twitter
  38. https://medium.com/p/e2e79252aeba/share/facebook
