   #[1]rss

     * [2]about
     * [3]software
     * [4]demos
     * [5]blog

   [prodigy.jpg]    [6]frederique matti

prodigy: a new tool for radically efficient machine teaching

   august 4, 2017    by matthew honnibal and ines montani

   machine learning systems are built from both code and data. it's easy
   to reuse the code but hard to reuse the data, so building ai mostly
   means doing annotation. this is good, because the examples are how you
   program the behaviour     the learner itself is really just a compiler.
   what's not good is the current technology for creating the examples.
   that's why we're pleased to introduce [7]prodigy, a downloadable tool
   for radically efficient machine teaching.

   we've been working on prodigy since we first launched explosion ai last
   year, alongside our open-source nlp library [8]spacy and our consulting
   projects (it's been a busy year!). during that time, spacy has grown
   into the most popular library of its type, giving us a lot of insight
   into what's driving success and failure for language understanding
   technologies. most of those insights have been used to make spacy
   better: ai devops was hard, so we made sure models could be installed
   via pip. large models made ci tricky, so the new models are less than
   1/10th the size.

   prodigy addresses the big remaining problem: annotation and training.
   the typical approach to annotation forces projects into an
   uncomfortable waterfall process. the experiments can't begin until the
   first batch of annotations are complete, but the annotation team can't
   start until they receive the annotation manuals. to produce the
   annotation manuals, you need to know what statistical models will be
   required for the features you're trying to build. machine learning is
   an inherently uncertain technology, but the waterfall annotation
   process relies on accurate upfront planning. the net result is a lot of
   wasted effort.

   prodigy solves this problem by letting data scientists conduct their
   own annotations, for rapid prototyping. ideas can be tested faster than
   the first planning meeting could even be scheduled. we also expect
   prodigy to reduce costs for larger projects, but it's the increased
   agility we're most excited about. data science projects are said to
   have uneven returns, like start-ups: a minority of projects are very
   successful, recouping costs for a larger number of failures. if so, the
   most important problem is to find more winners. prodigy helps you do
   that, because you get to try things much faster.

[9]prodigy   s recipe for efficient annotation

   most annotation tools avoid making any suggestions to the user, to
   avoid biasing the annotations. prodigy takes the opposite approach: ask
   the user as little as possible, and try to guess the rest. prodigy puts
   the model in the loop, so that it can actively participate in the
   training process and learns as you go. the model uses what it already
   knows to figure out what to ask you next. as you answer the questions,
   the model is updated, influencing which examples it asks you about
   next. in order to take full advantage of this strategy, prodigy is
   provided as a python library and command line utility, with a flexible
   web application. there's a thin, and optional hosted component to make
   it easy to share annotation queues, but the tool itself is entirely
   under your control.
   [prodigy_example01.jpg] why not cloud?active learning works best when
   you have a lot of raw input to stream through the model, so that more
   informative examples can be chosen for annotation. the model must be
   updated during the annotation session, and the updates must be specific
   to each user. solutions to these problems could surely be developed    
   but    why? as attractive as saas is to investors, it only makes sense if
   the hosted component is adding value, instead of removing it.

   prodigy comes with [10]built-in recipes for training and evaluating
   text classification, id39, image classification and
   word vector models. there's also a neat built-in component for doing
   a/b evaluations, which we expect to be particularly useful for
   developing generative models and translation systems. to keep the
   system requirements to a minimum, data is stored in an sqlite database
   by default. it's easy to use a different sql backend, or to specify a
   custom storage solution.
   rest api web app command-line interface data stream database model
   state recipe

   the components are wired togther into a recipe, by adding the @recipe
   decorator to any python function. the decorator lets you invoke your
   function from the command-line, as a prodigy subcommand. recipes can
   start the web service by return a dictionary of components. the recipe
   system provides a good balance of declarative and procedural
   approaches. if yo just need to wire together built-in components,
   return a python dictionary is no more typing than the equivalent json
   representation. but the python function also lets you implement more
   complicated behaviours, and reuse logic across your recipes.
recipe.pyimport prodigy
import your_arbitrary_etl_logic

@prodigy.recipe('custom_stream', dataset=("dataset"), db=("database"), label=("l
abel", "option"))
def custom_stream(dataset, db=none, label=''):
    db = your_arbitrary_etl_logic.load(db)
    return {
        'dataset': dataset,
        'stream': ({'text': row.text, 'label': label} for row in db)
        'view_id': 'classification'
    }

[11]ux-driven data collection with prodigy

   when humans interact with machines, their experience is what decides
   about the success of the interaction. most annotation tools avoid
   making suggestions to the user, to avoid biasing the annotations.
   prodigy takes the opposite approach: ask the user as little as
   possible. the more complicated the structure your model has to produce,
   the more benefit you can get from prodigy's binary interface. the web
   app lets you annotate text, entities, classification, images and custom
   html tasks straight from your browser     even on mobile devices.
   the prodigy web application try the live demoto see the web application
   and different annotation interfaces in action, check out the
   [12]prodigy live demo.

   human time and attention is precious. instead of presenting the
   annotators with a span of text that contains an entity, asking them to
   highlight it, select one of many labels from a dropdown and confirm,
   you can break the whole interaction down into a simple binary decision.
   you'll have to ask more questions to get the same information, but each
   question will be simple and focused. you'll collect more user actions,
   giving you lots of smaller pieces to learn from, and a much tighter
   feedback loop between the human and the model.

[13]why machine learning systems need annotated examples

   most ai systems today rely on supervised learning: you provide labelled
   input and output pairs, and get a program that can perform analogous
   computation for new data. supervised learning algorithms have been
   improving quickly, leading many people to anticipate a new wave of
   entirely unsupervised algorithms: algorithms so "advanced" they can
   compute whatever you want, without you specifying what that might be.
   this is like hoping for a programming language so advanced you don't
   even need to write a program.

     rather than spending a month figuring out an unsupervised machine
     learning problem, just label some data for a week and train a
     classifier.

     [14]@richardsocher

     the beauty of ml is that the complexity of the final system comes
     much from the data than from the human-written code.

     [15]@andrewyng

   unsupervised algorithms return meaning representations, based on the
   internal structure of the data. by definition, you can't directly
   control what the process returns. sometimes the meaning representation
   will directly address a useful question. if you're looking for
   suspicious activity on your platform, you might find that an outlier
   detection process is all you need. however, the unsupervised algorithm
   won't usually return clusters that map neatly to the labels you care
   about. with the right feature weightings, you might be able to come up
   with a model that sorts your data more usefully, but doing this by hand
   is unproductive: this is exactly the problem supervised learning is
   designed to solve.

[16]example: text classification

   text classification models can be trained to perform a wide variety of
   useful tasks, including id31, chatbot intent detection,
   and flagging abusive or fraudulent content. one of the problems with
   text classification is that it's usually hard to guess how accurately
   the system will perform. some problems turn out to be unexpectedly
   easy, while others are so difficult the intended functionality needs to
   be redesigned. prodigy lets you perform very rapid prototyping, so that
   you can quickly find out which ideas are worth further
   exploration.workflow and data setfor more info on how to do text
   classification with prodigy, see the detailed [17]text classification
   workflow. you can also [18]download the annotated data set we've
   created with prodigy for this example.

   text classification really shines when the task would otherwise be
   performed by hand. for instance, we regularly categorise [19]github
   issues for our library, [20]spacy. keeping the issue tracker tidy is
   something many open source projects struggle with     so automated tools
   could definitely be helpful. how easy would it be to create a bot to
   tag the issues automatically?

[21]getting started with prodigy

   prodigy is a python library, so it's easy to stream in data from any
   source     all you have to do is create a generator that yields out your
   examples. prodigy also includes several [22]built-in api loaders,
   including one for the [23]github api. to get started, we'll want to
   search for a query that returns a decent number of documentation
   issues. the model can't know what we're looking for until we've said
   "yes" to some examples. to find a good query, it's useful to pipe the
   stream into less, so we can look at the results:
   [prodigy_example02.jpg]

   now it's time to start annotating. we first add initialise a new
   dataset, adding a quick description for future reference. the next
   command starts the annotation server. the textcat.teach subcommand
   tells prodigy to run the built-in recipe function teach(), using the
   rest of the arguments supplied on the command line.custom recipesthe
   subcommand system is fully extensible. all you have to do is add the
   @recipe decorator to your function, and you'll be able to call it from
   the command line. to start the annotation server, your recipe just has
   to return a dictionary of components, like the stream of examples, the
   annotation interface, and optional callbacks to update and save your
   model.
   [prodigy_example03.jpg]

   opening localhost:8080, we get a sequence of recent github issue
   titles, displayed with our category as the title. if the category is
   correct, click accept, press a, or swipe left on a touch interface. if
   the category does not apply, click reject, press x, or swipe right.
   some examples are unclear or exceptions that you don't want the model
   to learn from. in these cases, you can click ignore or press space.
   [prodigy_example04.jpg]

   prodigy trains a model during annotation, on the answers you're
   providing. this lets prodigy rank the examples in the stream, to ask
   less redundant questions. learning from streaming data is a tricky
   problem, so we can usually get better results by training a new from
   scratch, once all the annotations are collected. this also lets us
   study the model in more detail, and try different hyper-parameters.

   after around 40 minutes of annotating the stream of issue titles for
   the search queries "[24]docs", "[25]documentation", "[26]readme" and
   "[27]instructions", we end up with a total of 830 annotations that
   break down as follows:
   decision count
   accept   261
   reject   525
   ignore   44
   total    830
   [prodigy_example05.jpg]

   by default, prodigy uses [28]spacy v2.0's new text classification
   system (currently in alpha). the model is a convolutional neural
   network stacked with a unigram bag-of-words. the bag-of-words model
   learns quickly, while the convolutional network lets the model pick up
   cues from longer phrases, once a few hundred examples are available.

   using a different text classification strategy with prodigy is very
   easy. if you want to keep using spacy, you can simply pass a new model
   instance to the textclassifier component. for an entirely custom nlp
   solution, you only need to provide two functions: one which assigns
   scores to the text, and another which updates the model on a new batch
   of examples. if your text classification solution only supports batch
   training, you can use the built-in model during annotation, and then
   export the annotations to train your solution separately.

[29]evaluating the model

   within the first hour of annotation, the system classified 140 out of
   the 156 evaluation issues correctly. to put this into some context, we
   have to look at the class balance of the data. in the evaluation data,
   65% of the examples were labelled reject, i.e. they were tagged as not
   documentation issues. this gives a baseline accuracy of 65%, which the
   classifier easily exceeded. we can get some sense of how the system
   will improve as more data is annotated by retraining the system with
   fewer examples.
   [prodigy_example06.jpg] interpreting the curveeach row of the table
   shows an experiment where the model was evaluated on 20% of the data,
   and trained with a subset of the remaining examples. this lets you see
   the relationship between the data set size and accuracy, so you can
   predict how much accuracy might improve if you collect more
   annotations.

   the chart shows the accuracy achieved with 10%, 25%, 50% and 75% of the
   training data. the last 25% of the training data brought 3% improvement
   in accuracy, indicating that further training will improve the system.
   similar logic is used to estimate the progress indicator during
   training.

[30]exporting and using the model

   after training the model, prodigy outputs a ready-to-use [31]spacy
   model, making it easy to put into production. spacy comes with a handy
   package [32]command that converts a model directory into a python
   package, allowing the data dependency to be specified in your
   requirements.txt. this gives a smooth path from prototype to
   production, making it easy to really test the model, in the context of
   a larger system.
   [prodigy_example07.jpg]

   if annotation projects are expensive to start, you have to guess which
   ideas look promising. these guesses will often be wrong, because it's
   difficult to predict the performance of a statistical model before the
   data has been collected. [33]prodigy helps you break through this
   bottleneck by dramatically reducing the cost of investigating new
   ideas. the whole annotation process is cheaper with prodigy, but it's
   the time-to-first-evidence that's most important. there's no shortage
   of ideas that would be incredibly valuable if they could be made to
   work. the shortage is in time to investigate those opportunities    
   which is exactly what prodigy [34]gives you more of.

   [35]try prodigy!

   matthew honnibal
   about the author

matthew honnibal

   matthew is a leading expert in ai technology, known for his research,
   software and writings. he completed his phd in 2009, and spent a
   further 5 years publishing research on state-of-the-art natural
   language understanding systems. anticipating the ai boom, he left
   academia in 2014 to develop spacy, an open-source library for
   industrial-strength nlp.
   ines montani
   about the author

ines montani

   ines is a developer specialising in applications for ai technology.
   she's a core developer of the spacy natural language processing library
   and the prodigy annotation tool. before founding explosion ai, she was
   a freelance front-end developer and strategist, using her four years
   executive experience in ad sales and digital marketing.

read more

[36]introducing spacy v2.1

[37]explosion ai in 2017: our year in review

[38]introducing custom pipelines and extensions for spacy v2.0

[39]pseudo-rehearsal: a simple solution to catastrophic forgetting for nlp

[40]supervised learning is great     it   s data collection that   s broken

[41]supervised similarity: learning symmetric relations from duplicate
question data

    about us

   explosion ai is a digital studio specialising in artificial
   intelligence and natural language processing. we   re the makers of
   spacy, the leading open-source nlp library.

    navigation

     * [42]home
     * [43]about
     * [44]software
     * [45]demos
     * [46]blog
     * [47]legal / imprint

    our software

     * [48]spacy
       industrial-strength nlp
     * [49]prodigy
       radically efficient machine teaching

   [50]see more    

references

   visible links
   1. https://explosion.ai/feed.xml
   2. https://explosion.ai/about
   3. https://explosion.ai/software
   4. https://explosion.ai/demos
   5. https://explosion.ai/blog
   6. http://frederiquematti.com/
   7. https://prodi.gy/
   8. https://spacy.io/
  10. https://prodi.gy/docs/recipes
  12. https://prodi.gy/demo
  14. https://twitter.com/richardsocher/status/840333380130553856
  15. https://twitter.com/andrewyng/status/883464017280454657
  17. https://prodi.gy/docs/workflow-text-classification
  18. https://prodi.gy/assets/data/github.jsonl
  19. https://github.com/
  20. https://spacy.io/
  22. https://prodi.gy/docs/#apis
  23. https://developer.github.com/v3/
  24. https://github.com/search?o=desc&q=docs+is:issue&s=created&type=issues&utf8=   
  25. https://github.com/search?o=desc&q=documentation+is:issue&s=created&type=issues&utf8=   
  26. https://github.com/search?o=desc&q=readme+is:issue&s=created&type=issues&utf8=   
  27. https://github.com/search?o=desc&q=instructions+is:issue&s=created&type=issues&utf8=   
  28. https://alpha.spacy.io/docs/usage/v2
  31. https://alpha.spacy.io/docs/usage/models
  32. http://alpha.spacy.io/docs/api/cli#package
  33. https://prodi.gy/
  34. https://prodi.gy/docs/workflow-text-classification
  35. https://prodi.gy/
  36. https://explosion.ai/blog/spacy-v2-1
  37. https://explosion.ai/blog/year-in-review-2017
  38. https://explosion.ai/blog/spacy-v2-pipelines-extensions
  39. https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
  40. https://explosion.ai/blog/supervised-learning-data-collection
  41. https://explosion.ai/blog/supervised-similarity-siamese-id98
  42. https://explosion.ai/
  43. https://explosion.ai/about
  44. https://explosion.ai/software
  45. https://explosion.ai/demos
  46. https://explosion.ai/blog
  47. https://explosion.ai/legal
  48. https://spacy.io/
  49. https://prodi.gy/
  50. https://explosion.ai/software

   hidden links:
  52. https://explosion.ai/
  53. mailto:matt@explosion.ai
  54. https://twitter.com/honnibal
  55. https://github.com/honnibal
  56. https://www.semanticscholar.org/search?q=matthew%20honnibal
  57. mailto:ines@explosion.ai
  58. https://twitter.com/_inesmontani
  59. https://github.com/ines
  60. https://ines.io/
  61. https://explosion.ai/blog/spacy-v2-1
  62. https://explosion.ai/blog/year-in-review-2017
  63. https://explosion.ai/blog/spacy-v2-pipelines-extensions
  64. https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
  65. https://explosion.ai/blog/supervised-learning-data-collection
  66. https://explosion.ai/blog/supervised-similarity-siamese-id98
  67. mailto:contact@explosion.ai
  68. https://twitter.com/explosion_ai
  69. https://github.com/explosion
  70. https://youtube.com/c/explosionai
  71. https://explosion.ai/feed
