   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

simple and multiple id75 in python

   [16]go to the profile of adi bronshtein
   [17]adi bronshtein (button) blockedunblock (button) followfollowing
   may 8, 2017

   quick introduction to id75 in python

   hi everyone! after [18]briefly introducing the    pandas    library as well
   as the [19]numpy library, i wanted to provide a quick introduction to
   building models in python, and what better place to start than one of
   the very basic models, id75? this will be the first post
   about machine learning and i plan to write about more complex models in
   the future. stay tuned! but for right now, let   s focus on linear
   regression.

   in this blog post, i want to focus on the concept of id75
   and mainly on the implementation of it in python. [20]id75
   is a statistical model that examines the linear relationship between
   two (simple id75 ) or more (multiple id75)
   variables         a dependent variable and independent variable(s). linear
   relationship basically means that when one (or more) independent
   variables increases (or decreases), the dependent variable increases
   (or decreases) too:
   [1*n2usf10akcq1jbiqxj-yfq.png]

   as you can see, a linear relationship can be positive (independent
   variable goes up, dependent variable goes up) or negative (independent
   variable goes up, dependent variable goes down). like i said, i will
   focus on the implementation of regression models in python, so i don   t
   want to delve too much into the math under the regression hood, but i
   will write a little bit about it. if you   d like a blog post about that,
   please don   t hesitate to write me in the responses!
     __________________________________________________________________

a little bit about the math

   a relationship between variables y and x is represented by this
   equation:
y`i = mx + b

   in this equation, y is the dependent variable         or the variable we are
   trying to predict or estimate; x is the independent variable         the
   variable we are using to make predictions; m is the slope of the
   regression line         it represent the effect x has on y. in other words,
   if x increases by 1 unit, y will increase by exactly m units. (   full
   disclosure   : this is true only if we know that x and y have a linear
   relationship. in almost all id75 cases, this will not be
   true!) b is a constant, also known as the y-intercept. if x equals 0, y
   would be equal to b (caveat: see full disclosure from earlier!). this
   is not necessarily applicable in real life         we won   t always know the
   exact relationship between x and y or have an exact linear
   relationship.

   these caveats lead us to a simple id75 (slr). in a slr
   model, we build a model based on data         the slope and y-intercept
   derive from the data; furthermore, we don   t need the relationship
   between x and y to be exactly linear. slr models also include the
   errors in the data (also known as residuals). i won   t go too much into
   it now, maybe in a later post, but residuals are basically the
   differences between the true value of y and the predicted/estimated
   value of y. it is important to note that in a id75, we are
   trying to predict a continuous variable. in a regression model, we are
   trying to minimize these errors by finding the    line of best fit            the
   regression line from the errors would be minimal. we are trying to
   minimize the length of the black lines (or more accurately, the
   distance of the blue dots) from the red line         as close to zero as
   possible. it is related to (or equivalent to) minimizing the [21]mean
   squared error (mse) or the sum of [22]squares of error (sse), also
   called the    residual sum of squares.    (rss) but this might be beyond
   the scope of this blog post :-)
   [1*a71ztd6_qquzlhmkj1rgiw.png]

   in most cases, we will have more than one independent variable         we   ll
   have multiple variables; it can be as little as two independent
   variables and up to hundreds (or theoretically even thousands) of
   variables. in those cases we will use a multiple id75
   model (mlr). the regression equation is pretty much the same as the
   simple regression equation, just with more variables:
y   i = b0 + b1x1i + b2x2i

   this concludes the math portion of this post :) ready to get to
   implementing it in python?
     __________________________________________________________________

id75 in python

   there are two main ways to perform id75 in python         with
   [23]statsmodels and [24]scikit-learn. it is also possible to use the
   [25]scipy library, but i feel this is not as common as the two other
   libraries i   ve mentioned. let   s look into doing id75 in
   both of them:

id75 in statsmodels

   [26]statsmodels is    a python module that provides classes and functions
   for the estimation of many different statistical models, as well as for
   conducting statistical tests, and statistical data exploration.    (from
   the documentation)

   as in with [27]pandas and [28]numpy, the easiest way to get or install
   statsmodels is through the [29]anaconda package. if, for some reason
   you are interested in installing in another way, check out [30]this
   link. after installing it, you will need to import it every time you
   want to use it:
import statsmodels.api as sm

   let   s see how to actually use statsmodels for id75. i   ll
   use an example from the [31]data science class i took at [32]general
   assembly dc:

   first, we import a [33]dataset from sklearn (the other library i   ve
   mentioned):
from sklearn import datasets ## imports datasets from scikit-learn
data = datasets.load_boston() ## loads boston dataset from datasets library

   this is a [34]dataset of the boston house prices (link to the
   description). because it is a dataset designated for testing and
   learning machine learning tools, it comes with a description of the
   dataset, and we can see it by using the command print data.descr (this
   is only true for sklearn datasets, not every dataset! would have been
   cool though   ). i   m adding the beginning of the description, for better
   understanding of the variables:
boston house prices dataset
===========================
notes
------
data set characteristics:
    :number of instances: 506
    :number of attributes: 13 numeric/categorical predictive

    :median value (attribute 14) is usually the target
    :attribute information (in order):
        - crim     per capita crime rate by town
        - zn       proportion of residential land zoned for lots over 25,000 sq.
ft.
        - indus    proportion of non-retail business acres per town
        - chas     charles river dummy variable (= 1 if tract bounds river; 0 ot
herwise)
        - nox      nitric oxides concentration (parts per 10 million)
        - rm       average number of rooms per dwelling
        - age      proportion of owner-occupied units built prior to 1940
        - dis      weighted distances to five boston employment centres
        - rad      index of accessibility to radial highways
        - tax      full-value property-tax rate per $10,000
        - ptratio  pupil-teacher ratio by town
        - b        1000(bk - 0.63)^2 where bk is the proportion of blacks by tow
n
        - lstat    % lower status of the population
        - medv     median value of owner-occupied homes in $1000's
    :missing attribute values: none
    :creator: harrison, d. and rubinfeld, d.l.
this is a copy of uci ml housing dataset.
http://archive.ics.uci.edu/ml/datasets/housing
this dataset was taken from the statlib library which is maintained at carnegie
mellon university.

   running data.feature_names and data.target would print the column names
   of the independent variables and the dependent variable, respectively.
   meaning, scikit-learn has already set the house value/price data as a
   target variable and 13 other variables are set as predictors. let   s see
   how to run a id75 on this dataset.

   first, we should load the data as a pandas data frame for easier
   analysis and set the median home value as our target variable:
import numpy as np
import pandas as pd
# define the data/predictors as the pre-set feature names
df = pd.dataframe(data.data, columns=data.feature_names)
# put the target (housing value -- medv) in another dataframe
target = pd.dataframe(data.target, columns=["medv"])

   what we   ve done here is to take the dataset and load it as a pandas
   data frame; after that, we   re setting the predictors (as df)         the
   independent variables that are pre-set in the dataset. we   re also
   setting the target         the dependent variable, or the variable we   re
   trying to predict/estimate.

   next we   ll want to fit a id75 model. we need to choose
   variables that we think we   ll be good predictors for the dependent
   variable         that can be done by checking the correlation(s) between
   variables, by plotting the data and searching visually for
   relationship, by conducting preliminary research on what variables are
   good predictors of y etc. for this first example, let   s take rm         the
   average number of rooms and lstat         percentage of lower status of the
   population. it   s important to note that statsmodels does not add a
   constant by default. let   s see it first without a constant in our
   regression model:
## without a constant
import statsmodels.api as sm
x = df["rm"]
y = target["medv"]
# note the difference in argument order
model = sm.ols(y, x).fit()
predictions = model.predict(x) # make the predictions by the model
# print out the statistics
model.summary()

   the output:
   [1*xtfrhbi48lb3up1sgco_ug.png]

   interpreting the table    this is a very long table, isn   t it? first we
   have what   s the dependent variable and the model and the method. ols
   stands for [35]ordinary least squares and the method    least squares   
   means that we   re trying to fit a regression line that would minimize
   the square of distance from the regression line (see the previous
   section of this post). date and time are pretty self-explanatory :) so
   as number of observations. df of residuals and models relates to the
   [36]degrees of freedom            the number of values in the final calculation
   of a statistic that are free to vary.   

   the coefficient of 3.634 means that as the rm variable increases by 1,
   the predicted value of mdev increases by 3.634. a few other important
   values are the r-squared         the percentage of variance our model
   explains; the standard error (is the standard deviation of the sampling
   distribution of a statistic, most commonly of the mean); the t scores
   and p-values, for hypothesis test         the rm has statistically
   significant p-value; there is a 95% confidence intervals for the rm
   (meaning we predict at a 95% percent confidence that the value of rm is
   between 3.548 to 3.759).

   if we do want to add a constant to our model         we have to set it by
   using the command x = sm.add_constant(x) where x is the name of your
   data frame containing your input (independent) variables.
import statsmodels.api as sm # import statsmodels
x = df["rm"] ## x usually means our input variables (or independent variables)
y = target["medv"] ## y usually means our output/dependent variable
x = sm.add_constant(x) ## let's add an intercept (beta_0) to our model
# note the difference in argument order
model = sm.ols(y, x).fit() ## sm.ols(output, input)
predictions = model.predict(x)
# print out the statistics
model.summary()

   the output:
   [1*x6iv-_exatqm7y0p5fc2zg.png]

   interpreting the table         with the constant term the coefficients are
   different. without a constant we are forcing our model to go through
   the origin, but now we have a y-intercept at -34.67. we also changed
   the slope of the rm predictor from 3.634 to 9.1021.

   now let   s try fitting a regression model with more than one
   variable         we   ll be using rm and lstat i   ve mentioned before. model
   fitting is the same:
x = df[[   rm   ,    lstat   ]]
y = target[   medv   ]
model = sm.ols(y, x).fit()
predictions = model.predict(x)
model.summary()

   and the output:
   [1*ptovdktq-qoclml4rzybaa.png]
   note: this table looks different because i   ve updated my
   jupyter notebook

   interpreting the output         we can see here that this model has a much
   higher r-squared value         0.948, meaning that this model explains 94.8%
   of the variance in our dependent variable. whenever we add variables to
   a regression model, r   will be higher, but this is a pretty high r  . we
   can see that both rm and lstat are statistically significant in
   predicting (or estimating) the median house value; not surprisingly ,
   we see that as rm increases by 1, medv will increase by 4.9069 and when
   lstat increases by 1, medv will decrease by -0.6557. as you may
   remember, lstat is the percentage of lower status of the population,
   and unfortunately we can expect that it will lower the median value of
   houses. with this same logic, the more rooms in a house, usually the
   higher its value will be.

   this was the example of both single and multiple id75 in
   statsmodels. we could have used as little or as many variables we
   wanted in our regression model(s)         up to all the 13! next, i will
   demonstrate how to run id75 models in sklearn.

id75 in sklearn

   sklearn is pretty much the golden standard when it comes to machine
   learning in python. it has many learning algorithms, for regression,
   classification, id91 and id84. check out
   [37]my post on the knn algorithm for a map of the different algorithms
   and more links to sklearn. in order to use id75, we need
   to import it:
from sklearn import linear_model

   let   s use the same dataset we used before, the boston housing prices.
   the process would be the same in the beginning         importing the datasets
   from sklearn and loading in the boston dataset:
from sklearn import datasets ## imports datasets from scikit-learn
data = datasets.load_boston() ## loads boston dataset from datasets library

   next, we   ll load the data to pandas (same as before):
# define the data/predictors as the pre-set feature names
df = pd.dataframe(data.data, columns=data.feature_names)
# put the target (housing value -- medv) in another dataframe
target = pd.dataframe(data.target, columns=["medv"])

   so now, as before, we have the data frame that contains the independent
   variables (marked as    df   ) and the data frame with the dependent
   variable (marked as    target   ). let   s fit a regression model using
   sklearn. first we   ll define our x and y         this time i   ll use all the
   variables in the data frame to predict the housing price:
x = df
y = target[   medv   ]

   and then i   ll fit a model:
lm = linear_model.linearregression()
model = lm.fit(x,y)

   the lm.fit() function fits a linear model. we want to use the model to
   make predictions (that   s what we   re here for!), so we   ll use
   lm.predict():
predictions = lm.predict(x)
print(predictions)[0:5]

   the print function would print the first 5 predictions for y (i didn   t
   print the entire list to    save room   . removing [0:5] would print the
   entire list):
[ 30.00821269  25.0298606   30.5702317   28.60814055  27.94288232]

   remember, lm.predict() predicts the y (dependent variable) using the
   linear model we fitted. you must have noticed that when we run a linear
   regression with sklearn, we don   t get a pretty table (okay, it   s not
   that pretty    but it   s pretty useful) like in statsmodels. what we can
   do is use built-in functions to return the score, the coefficients and
   the estimated intercepts. let   s see how it works:
lm.score(x,y)

   would give this output:
0.7406077428649428

   this is the r   score of our model. as you probably remember, this the
   percentage of explained variance of the predictions. if you   re
   interested, [38]read more here. next, let   s check out the coefficients
   for the predictors:
lm.coef_

   will give this output:
array([ -1.07170557e-01,   4.63952195e-02,   2.08602395e-02,
         2.68856140e+00,  -1.77957587e+01,   3.80475246e+00,
         7.51061703e-04,  -1.47575880e+00,   3.05655038e-01,
        -1.23293463e-02,  -9.53463555e-01,   9.39251272e-03,
        -5.25466633e-01])

   and the intercept:
lm.intercept_

   that will give this output:
36.491103280363134

   these are all (estimated/predicted) parts of the multiple regression
   equation i   ve mentioned earlier. check out the documentation to read
   more about coef_ and intercept_.
     __________________________________________________________________

   so, this is has a been a quick (but rather long!) introduction on how
   to conduct id75 in python. in practice, you would not use
   the entire dataset, but you will split your data into a training data
   to train your model on, and a test data         to, you guessed it, test your
   model/predictions on. if you would like to read about it, [39]please
   check out my next blog post. in the meanwhile, i hope you enjoyed this
   post and that i   ll    see    you on the next one.

   thank you for reading!

     * [40]machine learning
     * [41]data science
     * [42]id75
     * [43]statistics

   (button)
   (button)
   (button) 4k claps
   (button) (button) (button) 42 (button) (button)

     (button) blockedunblock (button) followfollowing
   [44]go to the profile of adi bronshtein

[45]adi bronshtein

   data scientist

     (button) follow
   [46]towards data science

[47]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 4k
     * (button)
     *
     *

   [48]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [49]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c928425168f9
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_e3smeuaox46g---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@adi.bronshtein?source=post_header_lockup
  17. https://towardsdatascience.com/@adi.bronshtein
  18. https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-pandas-python-library-f1b678f34673
  19. https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-numpy-library-6f61b7dee4db
  20. https://en.wikipedia.org/wiki/linear_regression
  21. https://en.wikipedia.org/wiki/mean_squared_error
  22. https://en.wikipedia.org/wiki/residual_sum_of_squares
  23. http://www.statsmodels.org/stable/regression.html
  24. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.linearregression.html
  25. https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html
  26. http://www.statsmodels.org/stable/index.html
  27. https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-pandas-python-library-f1b678f34673
  28. https://medium.com/@adi.bronshtein/a-quick-introduction-to-the-numpy-library-6f61b7dee4db
  29. https://www.continuum.io/downloads
  30. http://www.statsmodels.org/stable/install.html
  31. https://generalassemb.ly/education/data-science-immersive
  32. https://generalassemb.ly/locations/washington-dc/downtown-dc
  33. http://scikit-learn.org/stable/datasets/
  34. https://archive.ics.uci.edu/ml/datasets/housing
  35. https://en.wikipedia.org/wiki/ordinary_least_squares
  36. https://en.wikipedia.org/wiki/degrees_of_freedom_(statistics)
  37. https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7
  38. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.linearregression.html#sklearn.linear_model.linearregression.score
  39. https://medium.com/@adi.bronshtein/train-test-split-and-cross-validation-in-python-80b61beca4b6
  40. https://towardsdatascience.com/tagged/machine-learning?source=post
  41. https://towardsdatascience.com/tagged/data-science?source=post
  42. https://towardsdatascience.com/tagged/linear-regression?source=post
  43. https://towardsdatascience.com/tagged/statistics?source=post
  44. https://towardsdatascience.com/@adi.bronshtein?source=footer_card
  45. https://towardsdatascience.com/@adi.bronshtein
  46. https://towardsdatascience.com/?source=footer_card
  47. https://towardsdatascience.com/?source=footer_card
  48. https://towardsdatascience.com/
  49. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  51. https://medium.com/p/c928425168f9/share/twitter
  52. https://medium.com/p/c928425168f9/share/facebook
  53. https://medium.com/p/c928425168f9/share/twitter
  54. https://medium.com/p/c928425168f9/share/facebook
