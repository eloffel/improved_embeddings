   #[1]apple machine learning journal

[2]machine learning journal[3]     machine learning journal

hey siri: an on-device dnn-powered voice trigger for apple   s personal
assistant

   vol. 1, issue 6     october 2017 october two thousand seventeen by siri
   team
     __________________________________________________________________

   the    hey siri    feature allows users to invoke siri hands-free. a very
   small speech recognizer runs all the time and listens for just those
   two words. when it detects    hey siri   , the rest of siri parses the
   following speech as a command or query. the    hey siri    detector uses a
   deep neural network (dnn) to convert the acoustic pattern of your voice
   at each instant into a id203 distribution over speech sounds. it
   then uses a temporal integration process to compute a confidence score
   that the phrase you uttered was    hey siri   . if the score is high
   enough, siri wakes up. this article takes a look at the underlying
   technology. it is aimed primarily at readers who know something of
   machine learning but less about id103.

hands-free access to siri

   to get siri   s help, say    hey siri   . no need to press a button as    hey
   siri    makes siri hands-free. it seems simple, but quite a lot goes on
   behind the scenes to wake up siri quickly and efficiently. hardware,
   software, and internet services work seaid113ssly together to provide a
   great experience.
   figure 1. the hey siri flow on iphone [4]a diagram that shows how the
   acoustical signal from the user is processed. the signal is first
   processed by core audio then sent to a detector that works with the
   voice trigger. the voice trigger can be updated by the server. the
   voice trigger framework controls the detection threshold and sends wake
   up events to siri assistant. finally, the siri server checks the first
   words to make sure they are the hey siri trigger.

   being able to use siri without pressing buttons is particularly useful
   when hands are busy, such as when cooking or driving, or when using the
   apple watch. as figure 1 shows, the whole system has several parts.
   most of the implementation of siri is    in the cloud   , including the
   main automatic id103, the natural language interpretation
   and the various information services. there are also servers that can
   provide updates to the acoustic models used by the detector. this
   article concentrates on the part that runs on your local device, such
   as an iphone or apple watch. in particular, it focusses on the
   detector: a specialized speech recognizer which is always listening
   just for its wake-up phrase (on a recent iphone with the    hey siri   
   feature enabled).

the detector: listening for    hey siri   

   the microphone in an iphone or apple watch turns your voice into a
   stream of instantaneous waveform samples, at a rate of 16000 per
   second. a spectrum analysis stage converts the waveform sample stream
   to a sequence of frames, each describing the sound spectrum of
   approximately 0.01 sec. about twenty of these frames at a time (0.2 sec
   of audio) are fed to the acoustic model, a deep neural network (dnn)
   which converts each of these acoustic patterns into a id203
   distribution over a set of speech sound classes: those used in the    hey
   siri    phrase, plus silence and other speech, for a total of about 20
   sound classes. see figure 2.

   the dnn consists mostly of id127s and logistic
   nonlinearities. each    hidden    layer is an intermediate representation
   discovered by the dnn during its training to convert the filter bank
   inputs to sound classes. the final nonlinearity is essentially a
   softmax function (a.k.a. a general logistic or normalized exponential),
   but since we want log probabilities the actual math is somewhat
   simpler.
   figure 2. the deep neural network used to detect "hey siri." the hidden
   layers are actually fully connected. the top layer performs temporal
   integration. the actual dnn is indicated by the dashed box. [5]a
   diagram that depicts a deep neural network. the bottom layer in a
   stream of feature vectors. there are four sigmoidal layers, each of
   which has a bias unit. these layers feed into softmax function values
   which in turn feed into units that output a trigger score. the last
   layer for the tigger score maintains recurrent state.

   we choose the number of units in each hidden layer of the dnn to fit
   the computational resources available when the    hey siri    detector
   runs. networks we use typically have five hidden layers, all the same
   size: 32, 128, or 192 units depending on the memory and power
   constraints. on iphone we use two networks   one for initial detection
   and another as a secondary checker. the initial detector uses fewer
   units than the secondary checker.

   the output of the acoustic model provides a distribution of scores over
   phonetic classes for every frame. a phonetic class is typically
   something like    the first part of an /s/ preceded by a high front vowel
   and followed by a front vowel.   

   we want to detect    hey siri    if the outputs of the acoustic model are
   high in the right sequence for the target phrase. to produce a single
   score for each frame we accumulate those local values in a valid
   sequence over time. this is indicated in the final (top) layer of
   figure 2 as a recurrent network with connections to the same unit and
   the next in sequence. inside each unit there is a maximum operation and
   an add:
   [math: <mrow> <msub> <mi>f</mi> <mrow> <mi>i</mi> <mo>,</mo> <mi>t</mi>
   </mrow> </msub> <mo>=</mo> <mi>max</mi> <mo form="prefix">{</mo> <msub>
   <mi>s</mi> <mi>i</mi> </msub> <mo>+</mo> <msub> <mi>f</mi> <mrow>
   <mi>i</mi> <mo>,</mo> <mi>t</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub>
   <mo>,</mo> <msub> <mi>m</mi> <mrow> <mi>i</mi> <mo>-</mo> <mn>1</mn>
   </mrow> </msub> <mo>+</mo> <msub> <mi>f</mi> <mrow> <mi>i</mi>
   <mo>-</mo> <mn>1</mn> <mo>,</mo> <mi>t</mi> <mo>-</mo> <mn>1</mn>
   </mrow> </msub> <mo form="postfix">}</mo> <mo>+</mo> <msub> <mi>q</mi>
   <mrow> <mi>i</mi> <mo>,</mo> <mi>t</mi> </mrow> </msub> </mrow> :math]

   where
     * f[i,t] is the accumulated score for state i of the model
     * q[i,t] is the output of the acoustic model   the log score for the
       phonetic class associated with the ith state given the acoustic
       pattern around time t
     * s[i] is a cost associated with staying in state i
     * m[i] is a cost for moving on from state i

   both s[i] and m[i] are based on analysis of durations of segments with
   the relevant labels in the training data. (this procedure is an
   application of id145, and can be derived based on ideas
   about id48   id48s.)
   figure 3. visual depiction of the equation [6]a diagram that attempts
   to show a visual depiction of the mathematical equation.

   each accumulated score f[i,t] is associated with a labelling of
   previous frames with states, as given by the sequence of decisions by
   the maximum operation. the final score at each frame is f[i,t], where
   the last state of the phrase is state i and there are n frames in the
   sequence of frames leading to that score. (n could be found by tracing
   back through the sequence of max decisions, but is actually done by
   propagating forwards the number of frames since the path entered the
   first state of the phrase.)

   almost all the computation in the    hey siri    detector is in the
   acoustic model. the temporal integration computation is relatively
   cheap, so we disregard it when assessing size or computational
   resources.

   you may get a better idea of how the detector works by looking at
   figure 4, which shows the acoustic signal at various stages, assuming
   that we are using the smallest dnn. at the very bottom is a spectrogram
   of the waveform from the microphone. in this case, someone is saying
      hey siri what        the brighter parts are the loudest parts of the
   phrase. the hey siri pattern is between the vertical blue lines.
   figure 4. the acoustic pattern as it moves through the detector [7]the
   acoustic pattern as it moves through the detector.

   the second horizontal strip up from the bottom shows the result of
   analyzing the same waveform with a mel filter bank, which gives weight
   to frequencies based on perceptual measurements. this conversion also
   smooths out the detail that is visible in the spectrogram and due to
   the fine-structure of the excitation of the vocal tract: either random,
   as in the /s/, or periodic, seen here as vertical striations.

   the alternating green and blue horizontal strips labelled h1 to h5 show
   the numerical values (activations) of the units in each of the five
   hidden layers. the 32 hidden units in each layer have been arranged for
   this figure so as to put units with similar outputs together.

   the next strip up (with the yellow diagonal) shows the output of the
   acoustic model. at each frame there is one output for each position in
   the phrase, plus others for silence and other speech sounds. the final
   score, shown at the top, is obtained by adding up the local scores
   along the bright diagonal according to equation 1. note that the score
   rises to a peak just after the whole phrase enters the system.

   we compare the score with a threshold to decide whether to activate
   siri. in fact the threshold is not a fixed value. we built in some
   flexibility to make it easier to activate siri in difficult conditions
   while not significantly increasing the number of false activations.
   there is a primary, or normal threshold, and a lower threshold that
   does not normally trigger siri. if the score exceeds the lower
   threshold but not the upper threshold, then it may be that we missed a
   genuine    hey siri    event. when the score is in this range, the system
   enters a more sensitive state for a few seconds, so that if the user
   repeats the phrase, even without making more effort, then siri
   triggers. this second-chance mechanism improves the usability of the
   system significantly, without increasing the false alarm rate too much
   because it is only in this extra-sensitive state for a short time. (we
   discuss testing and tuning for accuracy later.)

responsiveness and power: two pass detection

   the    hey siri    detector not only has to be accurate, but it needs to be
   fast and not have a significant effect on battery life. we also need to
   minimize memory use and processor demand   particularly peak processor
   demand.

   to avoid running the main processor all day just to listen for the
   trigger phrase, the iphone   s always on processor (aop) (a small,
   low-power auxiliary processor, that is, the embedded motion
   coprocessor) has access to the microphone signal (on 6s and later). we
   use a small proportion of the aop   s limited processing power to run a
   detector with a small version of the acoustic model (dnn). when the
   score exceeds a threshold the motion coprocessor wakes up the main
   processor, which analyzes the signal using a larger dnn. in the first
   versions with aop support, the first detector used a dnn with 5 layers
   of 32 hidden units and the second detector had 5 layers of 192 hidden
   units.
   figure 5. two-pass detection [8]a diagram of the two-pass detection
   process. the first pass is fast and does not use a lot of computation
   power because is uses a small dnn. the second pass is more accurate and
   uses a lager dnn.

   apple watch presents some special challenges because of the much
   smaller battery. apple watch uses a single-pass    hey siri    detector
   with an acoustic model intermediate in size between those used for the
   first and second passes on other ios devices. the    hey siri    detector
   runs only when the watch motion coprocessor detects a wrist raise
   gesture, which turns the screen on. at that point there is a lot for
   watchos to do   power up, prepare the screen, etc.   so the system
   allocates    hey siri    only a small proportion (~5%) of the rather
   limited compute budget. it is a challenge to start audio capture in
   time to catch the start of the trigger phrase, so we make allowances
   for possible truncation in the way that we initialize the detector.

   hey siri    personalized

   we designed the always-on    hey siri    detector to respond whenever
   anyone in the vicinity says the trigger phrase. to reduce the annoyance
   of false triggers, we invite the user to go through a short enrollment
   session. during enrollment, the user says five phrases that each begin
   with    hey siri.    we save these examples on the device.

   we compare any possible new    hey siri    utterance with the stored
   examples as follows. the (second-pass) detector produces timing
   information that is used to convert the acoustic pattern into a
   fixed-length vector, by taking the average over the frames aligned to
   each state. a separate, specially trained dnn transforms this vector
   into a    speaker space    where, by design, patterns from the same speaker
   tend to be close, whereas patterns from different speakers tend to be
   further apart. we compare the distances to the reference patterns
   created during enrollment with another threshold to decide whether the
   sound that triggered the detector is likely to be    hey siri    spoken by
   the enrolled user.

   this process not only reduces the id203 that    hey siri    spoken by
   another person will trigger the iphone, but also reduces the rate at
   which other, similar-sounding phrases trigger siri.

further checks

   if the various stages on the iphone pass it on, the waveform arrives at
   the siri server. if the main speech recognizer hears it as something
   other than    hey siri    (for example    hey seriously   ) then the server
   sends a cancellation signal to the phone to put it back to sleep, as
   indicated in fig 1. on some systems we run a cut-down version of the
   main recognizer on the device to provide an extra check earlier.

the acoustic model: training

   the dnn acoustic model is at the heart of the    hey siri    detector. so
   let   s take a look at how we trained it. well before there was a hey
   siri feature, a small proportion of users would say    hey siri    at the
   start of a request, having started by pressing the button. we used such
      hey siri    utterances for the initial training set for the us english
   detector model. we also included general speech examples, as used for
   training the main speech recognizer. in both cases, we used automatic
   transcription on the training phrases. siri team members checked a
   subset of the transcriptions for accuracy.

   we created a language-specific phonetic specification of the    hey siri   
   phrase. in us english, we had two variants, with different first vowels
   in    siri      one as in    serious    and the other as in    syria.    we also
   tried to cope with a short break between the two words, especially as
   the phrase is often written with a comma:    hey, siri.    each phonetic
   symbol results in three speech sound classes (beginning, middle and
   end) each of which has its own output from the acoustic model.

   we used a corpus of speech to train the dnn for which the main siri
   recognizer provided a sound class label for each frame. there are
   thousands of sound classes used by the main recognizer, but only about
   twenty are needed to account for the target phrase (including an
   initial silence), and one large class class for everything else. the
   training process attempts to produce dnn outputs approaching 1 for
   frames that are labelled with the relevant states and phones, based
   only on the local sound pattern. the training process adjusts the
   weights using standard back-propagation and stochastic gradient
   descent. we have used a variety of neural network training software
   toolkits, including theano, tensorflow, and kaldi.

   this training process produces estimates of the probabilities of the
   phones and states given the local acoustic observations, but those
   estimates include the frequencies of the phones in the training set
   (the priors), which may be very uneven, and have little to do with the
   circumstances in which the detector will be used, so we compensate for
   the priors before the acoustic model outputs are used.

   training one model takes about a day, and there are usually a few
   models in training at any one time. we generally train three versions:
   a small model for the first pass on the motion coprocessor, a
   larger-size model for the second pass, and a medium-size model for
   apple watch.

      hey siri    works in all languages that siri supports, but    hey siri   
   isn   t necessarily the phrase that starts siri listening. for instance,
   french-speaking users need to say    dis siri    while korean-speaking
   users say    siri        (sounds like    siri ya.   ) in russian it is                
   siri     (sounds like    privet siri   ), and in thai                       siri   . (sounds
   like    wadi siri   .)

testing and tuning

   an ideal detector would fire whenever the user says    hey siri,    and not
   fire at other times. we describe the accuracy of the detector in terms
   of two kinds of error: firing at the wrong time, and failing to fire at
   the right time. the false-accept rate (far or false-alarm rate), is the
   number of false activations per hour (or mean hours between
   activations) and the false-reject rate (frr) is the proportion of
   attempted activations that fail. (note that the units we use to measure
   far are not the same as those we use for frr. even the dimensions are
   different. so there is no notion of an equal error rate.)

   for a given model we can change the balance between the two kinds of
   error by changing the activation threshold. figure 6 shows examples of
   this trade-off, for two sizes of early-development models. changing the
   threshold moves along the curve.

   during development we try to estimate the accuracy of the system by
   using a large test set, which is quite expensive to collect and
   prepare, but essential. there is    positive    data and    negative    data.
   the    positive    data does contain the target phrase. you might think
   that we could use utterances picked up by the    hey siri    system, but
   the system doesn   t capture the attempts that failed to trigger, and we
   want to improve the system to include as many of such failed attempts
   as possible.

   at first we used the utterances of    hey siri    that some users said as
   they pressed the home button, but these users are not attempting to
   catch siri   s attention, (the button does that) and the microphone is
   bound to be within arm   s reach, whereas we also want    hey siri    to work
   across a room. we made recordings specially in various conditions, such
   as in the kitchen (both close and far), car, bedroom, and restaurant,
   by native speakers of each language.

   we use the    negative    data to test for false activations (and false
   wakes). the data represent thousands of hours of recordings, from
   various sources, including podcasts and non-   hey siri    inputs to siri
   in many languages, to represent both background sounds (especially
   speech) and the kinds of phrases that a user might say to another
   person. we need such a lot of data because we are trying to estimate
   false-alarm rates as low as one per week. (if there are any occurrences
   of the target phrase in the negative data we label them as such, so
   that we do not count responses to them as errors.)
   figure 6. detector accuracy. trade-offs against detection threshold for
   small and larger dnns [9]a graph that shows the trade-offs against
   detection threshold for large and small dnns. the larger dnn is more
   accurate.

   tuning is largely a matter of deciding what thresholds to use. in
   figure 6, the two dots on the lower trade-off curve for the larger
   model show possible normal and second-chance thresholds. the operating
   point for the smaller (first-pass) model would be is at the right-hand
   side. these curves are just for the two stages of the detector, and do
   not include the personalized stage or subsequent checks.

   while we are confident that models that appear to perform better on the
   test set probably are really better, it is quite difficult to convert
   offline test results into useful predictions of the experience of
   users. so in addition to the offline measurements described previously,
   we estimate false-alarm rates (when siri turns on without the user
   saying    hey siri   ) and imposter-accept rates (when siri turns on when
   someone other than the user who trained the detector says    hey siri   )
   weekly by sampling from production data, on the latest ios devices and
   apple watch. this does not give us rejection rates (when the system
   fails to respond to a valid    hey siri   ) but we can estimate rejection
   rates from the proportion of activations just above the threshold that
   are valid, and a sampling of just-below threshold events on devices
   carried by development staff.

   we continually evaluate and improve    hey siri,    and the model that
   powers it, by training and testing using variations of the approach
   described here. we train in many different languages and test under a
   wide range of conditions.

   next time you say    hey siri    you may think of all that goes on to make
   responding to that phrase happen, but we hope that it    just works!   

contact us

   [10]send questions or feedback via email

jobs at apple

   [11]apply now for a career at apple

tools for innovation

   [12]apple developer program

   [13]    copyright    2018 apple inc. all rights reserved. copyright two
   thousand seventeen apple inc. all rights reserved.
   [14]subscribe [15]privacy policy [16]terms of use [17]legal

references

   1. https://machinelearning.apple.com/feed.xml
   2. https://machinelearning.apple.com/
   3. https://machinelearning.apple.com/
   4. https://machinelearning.apple.com/images/journals/hey-siri/heysiriflow-1.png
   5. https://machinelearning.apple.com/images/journals/hey-siri/trainingdnn-1.png
   6. https://machinelearning.apple.com/images/journals/hey-siri/equation-1.png
   7. https://machinelearning.apple.com/images/journals/hey-siri/hswr-1.png
   8. https://machinelearning.apple.com/images/journals/hey-siri/twopassdetector-1.png
   9. https://machinelearning.apple.com/images/journals/hey-siri/graphfrr-far-1.png
  10. mailto:machine-learning@apple.com
  11. https://www.apple.com/jobs/
  12. https://developer.apple.com/
  13. https://www.apple.com/
  14. https://machinelearning.apple.com/feed.xml
  15. https://www.apple.com/privacy/privacy-policy/
  16. https://www.apple.com/legal/internet-services/terms/site.html
  17. https://www.apple.com/legal/
