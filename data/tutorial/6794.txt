   #[1]github [2]recent commits to multiagent-particle-envs:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]74
     * [35]star [36]512
     * [37]fork [38]172

[39]openai/[40]multiagent-particle-envs

   [41]code [42]issues 10 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   code for a multi-agent particle environment used in the paper
   "multi-agent actor-critic for mixed cooperative-competitive
   environments" [47]https://arxiv.org/pdf/1706.02275.pdf
   [48]paper
     * [49]41 commits
     * [50]1 branch
     * [51]0 releases
     * [52]fetching contributors
     * [53]mit

    1. [54]python 100.0%

   (button) python
   branch: master (button) new pull request
   [55]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/o
   [56]download zip

downloading...

   want to be notified of new releases in openai/multiagent-particle-envs?
   [57]sign in [58]sign up

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [61]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [62]download the github extension for visual studio
   and try again.

   (button) go back
   [63]@christopherhesse
   [64]christopherhesse [65]merge pull request [66]#38 [67]from
   christopherhesse/update-readme (button)    
update readme with repo status

   latest commit [68]69ee7f8 nov 22, 2018
   [69]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [70]bin [71]updated default scenario for interactive.py, fixed
   directory error feb 1, 2018
   [72]multiagent [73]fixed bugs, now works with latest gym. fixed shared
   reward bug. clean    jul 9, 2018
   [74].gitignore
   [75]license.txt
   [76]readme.md
   [77]make_env.py [78]removed arglist from make_env oct 20, 2017
   [79]setup.py

readme.md

   status: archive (code is provided as-is, no updates expected)

multi-agent particle environment

   a simple multi-agent particle world with a continuous observation and
   discrete action space, along with some basic simulated physics. used in
   the paper [80]multi-agent actor-critic for mixed
   cooperative-competitive environments.

getting started:

     * to install, cd into the root directory and type pip install -e .
     * to interactively view moving to landmark scenario (see others in
       ./scenarios/): bin/interactive.py --scenario simple.py
     * known dependencies: python (3.5.4), openai gym (0.10.5), numpy
       (1.14.5)
     * to use the environments, look at the code for importing them in
       make_env.py.

code structure

     * make_env.py: contains code for importing a multiagent environment
       as an openai gym-like object.
     * ./multiagent/environment.py: contains code for environment
       simulation (interaction physics, _step() function, etc.)
     * ./multiagent/core.py: contains classes for various objects
       (entities, landmarks, agents, etc.) that are used throughout the
       code.
     * ./multiagent/rendering.py: used for displaying agent behaviors on
       the screen.
     * ./multiagent/policy.py: contains code for interactive policy based
       on keyboard input.
     * ./multiagent/scenario.py: contains base scenario object that is
       extended for all scenarios.
     * ./multiagent/scenarios/: folder where various scenarios/
       environments are stored. scenario code consists of several
       functions:
         1. make_world(): creates all of the entities that inhabit the
            world (landmarks, agents, etc.), assigns their capabilities
            (whether they can communicate, or move, or both). called once
            at the beginning of each training session
         2. reset_world(): resets the world by assigning properties
            (position, color, etc.) to all entities in the world called
            before every episode (including after make_world() before the
            first episode)
         3. reward(): defines the reward function for a given agent
         4. observation(): defines the observation space of a given agent
         5. (optional) benchmark_data(): provides diagnostic data for
            policies trained on the environment (e.g. id74)

creating new environments

   you can create new scenarios by implementing the first 4 functions
   above (make_world(), reset_world(), reward(), and observation()).

list of environments

   env name in code (name in paper) communication? competitive? notes
   simple.py n n single agent sees landmark position, rewarded based on
   how close it gets to landmark. not a multiagent environment -- used for
   debugging policies.
   simple_adversary.py (physical deception) n y 1 adversary (red), n good
   agents (green), n landmarks (usually n=2). all agents observe position
   of landmarks and other agents. one landmark is the    target landmark   
   (colored green). good agents rewarded based on how close one of them is
   to the target landmark, but negatively rewarded if the adversary is
   close to target landmark. adversary is rewarded based on how close it
   is to the target, but it doesn   t know which landmark is the target
   landmark. so good agents have to learn to    split up    and cover all
   landmarks to deceive the adversary.
   simple_crypto.py (covert communication) y y two good agents (alice and
   bob), one adversary (eve). alice must sent a private message to bob
   over a public channel. alice and bob are rewarded based on how well bob
   reconstructs the message, but negatively rewarded if eve can
   reconstruct the message. alice and bob have a private key (randomly
   generated at beginning of each episode), which they must learn to use
   to encrypt the message.
   simple_push.py (keep-away) n y 1 agent, 1 adversary, 1 landmark. agent
   is rewarded based on distance to landmark. adversary is rewarded if it
   is close to the landmark, and if the agent is far from the landmark. so
   the adversary learns to push agent away from the landmark.
   simple_reference.py y n 2 agents, 3 landmarks of different colors. each
   agent wants to get to their target landmark, which is known only by
   other agent. reward is collective. so agents have to learn to
   communicate the goal of the other agent, and navigate to their
   landmark. this is the same as the simple_speaker_listener scenario
   where both agents are simultaneous speakers and listeners.
   simple_speaker_listener.py (cooperative communication) y n same as
   simple_reference, except one agent is the    speaker    (gray) that does
   not move (observes goal of other agent), and other agent is the
   listener (cannot speak, but must navigate to correct landmark).
   simple_spread.py (cooperative navigation) n n n agents, n landmarks.
   agents are rewarded based on how far any agent is from each landmark.
   agents are penalized if they collide with other agents. so, agents have
   to learn to cover all the landmarks while avoiding collisions.
   simple_tag.py (predator-prey) n y predator-prey environment. good
   agents (green) are faster and want to avoid being hit by adversaries
   (red). adversaries are slower and want to hit good agents. obstacles
   (large black circles) block the way.
   simple_world_comm.py y y environment seen in the video accompanying the
   paper. same as simple_tag, except (1) there is food (small blue balls)
   that the good agents are rewarded for being near, (2) we now have
      forests    that hide agents inside from being seen from outside; (3)
   there is a    leader adversary    that can see the agents at all times, and
   can communicate with the other adversaries to help coordinate the
   chase.

paper citation

   if you used this environment for your experiments or found it helpful,
   consider citing the following papers:

   environments in this repo:
@article{lowe2017multi,
  title={multi-agent actor-critic for mixed cooperative-competitive environments
},
  author={lowe, ryan and wu, yi and tamar, aviv and harb, jean and abbeel, piete
r and mordatch, igor},
  journal={neural information processing systems (nips)},
  year={2017}
}

   original particle world environment:
@article{mordatch2017emergence,
  title={emergence of grounded compositional language in multi-agent populations
},
  author={mordatch, igor and abbeel, pieter},
  journal={arxiv preprint arxiv:1703.04908},
  year={2017}
}

     *    2019 github, inc.
     * [81]terms
     * [82]privacy
     * [83]security
     * [84]status
     * [85]help

     * [86]contact github
     * [87]pricing
     * [88]api
     * [89]training
     * [90]blog
     * [91]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [92]reload to refresh your
   session. you signed out in another tab or window. [93]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/openai/multiagent-particle-envs/commits/master.atom
   3. https://github.com/openai/multiagent-particle-envs#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/openai/multiagent-particle-envs
  32. https://github.com/join
  33. https://github.com/login?return_to=/openai/multiagent-particle-envs
  34. https://github.com/openai/multiagent-particle-envs/watchers
  35. https://github.com/login?return_to=/openai/multiagent-particle-envs
  36. https://github.com/openai/multiagent-particle-envs/stargazers
  37. https://github.com/login?return_to=/openai/multiagent-particle-envs
  38. https://github.com/openai/multiagent-particle-envs/network/members
  39. https://github.com/openai
  40. https://github.com/openai/multiagent-particle-envs
  41. https://github.com/openai/multiagent-particle-envs
  42. https://github.com/openai/multiagent-particle-envs/issues
  43. https://github.com/openai/multiagent-particle-envs/pulls
  44. https://github.com/openai/multiagent-particle-envs/projects
  45. https://github.com/openai/multiagent-particle-envs/pulse
  46. https://github.com/join?source=prompt-code
  47. https://arxiv.org/pdf/1706.02275.pdf
  48. https://github.com/topics/paper
  49. https://github.com/openai/multiagent-particle-envs/commits/master
  50. https://github.com/openai/multiagent-particle-envs/branches
  51. https://github.com/openai/multiagent-particle-envs/releases
  52. https://github.com/openai/multiagent-particle-envs/graphs/contributors
  53. https://github.com/openai/multiagent-particle-envs/blob/master/license.txt
  54. https://github.com/openai/multiagent-particle-envs/search?l=python
  55. https://github.com/openai/multiagent-particle-envs/find/master
  56. https://github.com/openai/multiagent-particle-envs/archive/master.zip
  57. https://github.com/login?return_to=https://github.com/openai/multiagent-particle-envs
  58. https://github.com/join?return_to=/openai/multiagent-particle-envs
  59. https://desktop.github.com/
  60. https://desktop.github.com/
  61. https://developer.apple.com/xcode/
  62. https://visualstudio.github.com/
  63. https://github.com/christopherhesse
  64. https://github.com/openai/multiagent-particle-envs/commits?author=christopherhesse
  65. https://github.com/openai/multiagent-particle-envs/commit/69ee7f85811c77ee651722bc3c332677b2195da1
  66. https://github.com/openai/multiagent-particle-envs/pull/38
  67. https://github.com/openai/multiagent-particle-envs/commit/69ee7f85811c77ee651722bc3c332677b2195da1
  68. https://github.com/openai/multiagent-particle-envs/commit/69ee7f85811c77ee651722bc3c332677b2195da1
  69. https://github.com/openai/multiagent-particle-envs/tree/69ee7f85811c77ee651722bc3c332677b2195da1
  70. https://github.com/openai/multiagent-particle-envs/tree/master/bin
  71. https://github.com/openai/multiagent-particle-envs/commit/ac6d0b572c8d4419bfdf29dad7bb074bba5b22a4
  72. https://github.com/openai/multiagent-particle-envs/tree/master/multiagent
  73. https://github.com/openai/multiagent-particle-envs/commit/6ed7cac026f0eb345d4c20232bafa1dc951c68e7
  74. https://github.com/openai/multiagent-particle-envs/blob/master/.gitignore
  75. https://github.com/openai/multiagent-particle-envs/blob/master/license.txt
  76. https://github.com/openai/multiagent-particle-envs/blob/master/readme.md
  77. https://github.com/openai/multiagent-particle-envs/blob/master/make_env.py
  78. https://github.com/openai/multiagent-particle-envs/commit/926a1612eb2143953fc157e69be99f7148d6125e
  79. https://github.com/openai/multiagent-particle-envs/blob/master/setup.py
  80. https://arxiv.org/pdf/1706.02275.pdf
  81. https://github.com/site/terms
  82. https://github.com/site/privacy
  83. https://github.com/security
  84. https://githubstatus.com/
  85. https://help.github.com/
  86. https://github.com/contact
  87. https://github.com/pricing
  88. https://developer.github.com/
  89. https://training.github.com/
  90. https://github.blog/
  91. https://github.com/about
  92. https://github.com/openai/multiagent-particle-envs
  93. https://github.com/openai/multiagent-particle-envs

   hidden links:
  95. https://github.com/
  96. https://github.com/openai/multiagent-particle-envs
  97. https://github.com/openai/multiagent-particle-envs
  98. https://github.com/openai/multiagent-particle-envs
  99. https://help.github.com/articles/which-remote-url-should-i-use
 100. https://github.com/openai/multiagent-particle-envs#multi-agent-particle-environment
 101. https://github.com/openai/multiagent-particle-envs#getting-started
 102. https://github.com/openai/multiagent-particle-envs#code-structure
 103. https://github.com/openai/multiagent-particle-envs#creating-new-environments
 104. https://github.com/openai/multiagent-particle-envs#list-of-environments
 105. https://github.com/openai/multiagent-particle-envs#paper-citation
 106. https://github.com/
