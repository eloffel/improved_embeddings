   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

experiments with swish activation function on mnist dataset

   [9]go to the profile of jaiyam sharma
   [10]jaiyam sharma (button) blockedunblock (button) followfollowing
   oct 20, 2017

   edit: a much better comparison of swish with relus is available on the
   [11]second part of this blog post.

   hi all,

   as you may probably know by now, there is a new activation function in
   deep learning town called swish. brought to us by researchers at
   google, the swish activation function is f(x)=x*sigmoid(x). as simple
   as that!

   according to [12]their paper, the swish activation provides better
   performance than rectified linear units (relu(x)= max(0,x)), even when
   the hyperparameters are tuned for relu! these are very impressive
   results from a rather simple looking function. in the rest of this
   blog, i will give a brief introduction to swish (basically a gist of
   the paper in plain english) and then i will show some interesting
   results i got by using the swish activation on mnist dataset.
   hopefully, this will help you decide whether or not you want to use
   swish in your own models. let   s go for it.
     __________________________________________________________________

intuition behind swish

   graphically swish looks very similar to the relu:
   [1*6tuakb3_1ugybwsufey3za.tiff]
   swish and relu activations

   the important difference is in the negative region of the x-axis.
   notice that due to the weird shape of the tail in the negative region,
   the output from the swish activation may decrease even when the input
   is increased. this is very interesting and perhaps unique to swish.
   most id180 (such as sigmoid, tanh & relu) are monotonic
   i.e. their value never decreases as the input increases (the value may
   remain same, as is the case for relus when x<0).

   intuitively, swish sits somewhere between linear and relu activations.
   if we consider a simple variation of swish f(x)=2x*sigmoid(beta*x)
   where beta is a learnable parameter, it can be seen that if beta = 0
   the sigmoid part is always 1/2, so f(x) becomes linear. on the other
   hand if beta is very high, the sigmoid part becomes almost like a
   binary activation (0 for x<0 and 1 for x>0). thus, f(x) approaches the
   relu function. therefore, the standard swish function (beta=1) provides
   a smooth interpolation between these two extremes. very neat! this is
   the most credible argument in the paper as to why swish should work.
   the other arguments are that swish is, unlike relu, non-monotonic but,
   like relu, bounded from below and unbounded from above (so you don   t
   end up with vanishing gradients in a deep network).
     __________________________________________________________________

experiments:

   now that we have a good understanding of the idea behind swish, lets
   see how it well works on the time tested mnist dataset. i used a very
   simple feed-forward architecture with 2 hidden layers and trained five
   models for five different activations: linear (no activation), sigmoid,
   relu, standard swish (where beta is fixed to 1.0) and a variation of
   swish where the beta is initially set to 1.0 but learned over time by
   the network. the weights were initialized from a gaussian distribution
   with same random seed in all five models so the only difference should
   be due to the activation function. also, i tried three different
   standard deviations (sigma=0.1,0.5,1.0) of the gaussian distribution.
   if the standard deviation is too large, the weights will be initialized
   too far away from their optimal values and the network will have a hard
   time converging. i wanted to see if swish can help convergence if the
   initialization is a bit off. here are the results:
   [1*0b5e33uksl6slidviex30g.png]

   when using a good initialization of weights, relu, swish_1 (beta=1) and
   swish_beta all perform nearly the same. perhaps since the problem is
   very simple, there is no distinct advantage of learning the parameter
   beta.
   [1*wl0xg_ea_e-py_133g3-ra.png]

   this was a rather bad initialization. this problem is mildly
   challenging. surprisingly sigmoid worked very well here. among relu &
   swish, swish works quite a bit better than relu (although both
   eventually converge to the same performance). again here, as last time,
   there is no great advantage of learning beta and it doesn   t improve
   performance.
   [1*wb2bff4h8nqsvwnyw2f5_g.png]

   this was a very bad initialization! sigmoid and linear activations did
   very well. that is more attributable to the linear separability of
   mnist dataset itself than to the activations. among the others, swish_1
   shows better performance than relu. given enough training steps, both
   converge to the same performance. here we see a big advantage of
   learning the beta. swish_beta gives noticeably better performance on
   this difficult problem.

   it is also interesting to evaluate the evolution of beta in each of the
   cases:
   [1*6aqpenjs-uorshsvdvnspa.png]

   when the initialiations are somewhat close to optimal values, beta
   evolves smoothly. the training steps were not enough to saturate the
   value of beta. please remember that learning beta did not lead to
   better performance in this case. it seems it would be difficult to
   predict the final performance of a netowrk while monitoring the value
   of beta during training (see sigma=1.0 below).
   [1*gaum913jwhkwv4_uccrjwq.png]

   as the initializations are somewhat far from the optimal values, beta
   seems to be not so stable. this also does not lead to noticeable gains
   or losses in performance compared to a fixed beta=1.0
   [1*eyskn7tsclf9t1muy9tprq.png]

   beta evolves smoothly even though the initial weights were far away
   from optimal values. even though this trend looks very similar to
   sigma=0.1, there were substantial gains in performance by learning beta
   in this case but not in the earlier case. this further suggests that it
   will be difficult to tell much about how the network will perform by
   monitoring the evolution of beta during training.
     __________________________________________________________________

takeaways

   these results are obtained from very simple neural networks on a simple
   dataset. so, we should not read too much into them. nevertheless, there
   are some key takeaways here:
    1. if your network is simple enough, relu and swish should perform
       about the same. in this case, relu would have an advantage since
       swish requires a multiplication step (x*sigmoid(x)), which means
       doing more computation for no noticeable gains in performance.
    2. even if standard swish works well on your problem, there may not be
       additonal gains from learning the parameter beta. learning beta
       seems to work best when the problem cannot be solved by using relu
       (i.e. there are lot of dead relus)
    3. it would have been wonderful if the evolution of the parameter beta
       during training provided some hints about the eventual performance
       of the model. from these limited tests, this does not seem to be
       the case.
     __________________________________________________________________

code

   the full code for reproducing these results is uploaded on my
   [13]github. i especially encourage you to look at interactive html
   versions of the figures shown above since some of the details are not
   easy to see with png files.

     * [14]machine learning
     * [15]deep learning
     * [16]artificial intelligence
     * [17]google
     * [18]data science

   (button)
   (button)
   (button) 436 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [19]go to the profile of jaiyam sharma

[20]jaiyam sharma

   blogs about replicating research papers in machine learning

     * (button)
       (button) 436
     * (button)
     *
     *

   [21]go to the profile of jaiyam sharma
   never miss a story from jaiyam sharma, when you sign up for medium.
   [22]learn more
   never miss a story from jaiyam sharma
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/fc89a8c79ff7
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@jaiyamsharma/experiments-with-swish-activation-function-on-mnist-dataset-fc89a8c79ff7&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@jaiyamsharma/experiments-with-swish-activation-function-on-mnist-dataset-fc89a8c79ff7&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@jaiyamsharma?source=post_header_lockup
  10. https://medium.com/@jaiyamsharma
  11. https://medium.com/@jaiyamsharma/swish-in-depth-a-comparison-of-swish-relu-on-cifar-10-1c798e70ee08
  12. https://arxiv.org/abs/1710.05941v1
  13. https://github.com/dataplayer12/swish-activation
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/deep-learning?source=post
  16. https://medium.com/tag/artificial-intelligence?source=post
  17. https://medium.com/tag/google?source=post
  18. https://medium.com/tag/data-science?source=post
  19. https://medium.com/@jaiyamsharma?source=footer_card
  20. https://medium.com/@jaiyamsharma
  21. https://medium.com/@jaiyamsharma
  22. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  24. https://medium.com/p/fc89a8c79ff7/share/twitter
  25. https://medium.com/p/fc89a8c79ff7/share/facebook
  26. https://medium.com/p/fc89a8c79ff7/share/twitter
  27. https://medium.com/p/fc89a8c79ff7/share/facebook
