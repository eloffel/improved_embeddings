   #[1]jonathan hui blog posts

   [2][rssicon.svg]
   [3]jonathan hui blog

   [4]about

   tensorboard - visualize your learning.   

   mar 12, 2017

tensorboard

   tensorboard is a browser based application that helps you to visualize
   your training parameters (like weights & biases), metrics (like loss),
   hyper parameters or any statistics. for example, we plot the histogram
   distribution of the weight for the first fully connected layer every 20
   iterations.
   [tb_hist.png]

namespace

   to create some data hierarchy structure when we view the data like:
   [tb_scalar_summary.png]

   we use namespace
with tf.name_scope('id981'):
    with tf.name_scope('w'):
        mean = tf.reduce_mean(w)
        tf.summary.scalar('mean', mean)
        stddev = tf.sqrt(tf.reduce_mean(tf.square(w - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.histogram('histogram', var)

   which create the following data hierarchy and can be browsed with the
   tensorboard later:
   [name_space.png]

implement tensorboard

   to add & view data summaries to the tensorboard. we need to:
    1. define all the summary information to be logged.
    2. add summary information to the writer to flush it out to a log
       file.
    3. view the data in the tensorboard.

define summary information

   number can be added to the tensorboard with tf_summary.scalar and array
   with tf.summary.histogram
with tf.name_scope('id981'):
    with tf.name_scope('w'):
        mean = tf.reduce_mean(w)
        tf.summary.scalar('mean', mean)
        tf.summary.histogram('histogram', var)

   for data logged with tf_summary.scalar. the accuracy increases while
   cost drops.
   [tb_scalar.png]

   more statistics on the weights for layer 1.
   [ss1.png]

   for summary logged with tf.summary.histogram
   [tb_hist.png]

example

   here is a more complicated example in which we try to summarize the
   information of the weight in the first fully connected layer.
h1, _ = affine_layer(x, 'layer1', [784, 256], keep_prob)

def affine_layer(x, name, shape, keep_prob, act_fn=tf.nn.relu):
    with tf.name_scope(name):
        with tf.name_scope('weights'):
            w = tf.variable(tf.truncated_normal(shape, stddev=np.sqrt(2.0 / shap
e[0])))
            variable_summaries(w)

def variable_summaries(var):
  with tf.name_scope('summaries'):
    mean = tf.reduce_mean(var)
    tf.summary.scalar('mean', mean)
    with tf.name_scope('stddev'):
      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
    tf.summary.scalar('stddev', stddev)
    tf.summary.scalar('max', tf.reduce_max(var))
    tf.summary.scalar('min', tf.reduce_min(var))
    tf.summary.histogram('histogram', var)

add summary information to a writer

   after we define what summary information to be logged, we merge all the
   summary data into one single operation node with
   tf.summary.merge_all(). we create a summary writer with
   tf.summary.filewriter, and then write and flush out the information to
   the log file every 20 iterations:
def main(_):
  ...
  train_step = tf.train.adamoptimizer(learning_rate=0.001).minimize(cross_entrop
y)

  # merge all summary inforation.
  summary = tf.summary.merge_all()

  init = tf.global_variables_initializer()
  with tf.session() as sess:

      # create a writer for the summary data.
      summary_writer = tf.summary.filewriter(flags.log_dir, sess.graph)

      sess.run(init)
      for step in range(100):
        ...
        sess.run(train_step, feed_dict={x: batch_xs, labels: batch_ys, lmbda:5e-
5, keep_prob:0.5})
        if step % 20 == 0:
                 # write summary
          summary_str = sess.run(summary, feed_dict={x: batch_xs, labels: batch_
ys, lmbda:5e-5, keep_prob:0.5})
          summary_writer.add_summary(summary_str, step)
          summary_writer.flush()
        ...

multimodal distributions

   we can concatenate 2 tensors to be plotted together in the same graph:
normal_combined = tf.concat([mean_moving_normal, variance_shrinking_normal], 0)
tf.summary.histogram("normal/bimodal", normal_combined)

plot training and validation loss/accuracy

   we use the same tensor to compute the loss for the training samples and
   validation samples. to plot the same tensor with different datasets
   together, barzin provides a solution using 2 file writers:
import tensorflow as tf
from numpy import random

writer_1 = tf.summary.filewriter("./logs/plot_1")
writer_2 = tf.summary.filewriter("./logs/plot_2")

log_var = tf.variable(0.0)
tf.summary.scalar("loss", log_var)

write_op = tf.summary.merge_all()

session = tf.interactivesession()
session.run(tf.global_variables_initializer())

for i in range(100):
    # for writer 1
    summary = session.run(write_op, {log_var: random.rand()})
    writer_1.add_summary(summary, i)
    writer_1.flush()

    # for writer 2
    summary = session.run(write_op, {log_var: random.rand()})
    writer_2.add_summary(summary, i)
    writer_2.flush()

view the tensorboard

   open the terminal and run the tensorboard command with you log file
   location.
$ tensorboard --logdir=/tmp/tensorflow/mnist/log
starting tensorboard b'41' on port 6006
(you can navigate to http://192.134.44.11:6006)

runtime metadata

   we can add runtime performance information into the tensorboard
      merged = tf.summary.merge_all()
      ...

      run_options = tf.runoptions(trace_level=tf.runoptions.full_trace)
      run_metadata = tf.runmetadata()
      summary, _ = sess.run([merged, train_step],
                              feed_dict=feed_dict(true),
                              options=run_options,
                              run_metadata=run_metadata)
      train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
      train_writer.add_summary(summary, i)

   [run.png]

full program listing

# copyright 2015 the tensorflow authors. all rights reserved.
#
# licensed under the apache license, version 2.0 (the 'license');
# you may not use this file except in compliance with the license.
# you may obtain a copy of the license at
#
#     http://www.apache.org/licenses/license-2.0
#
# unless required by applicable law or agreed to in writing, software
# distributed under the license is distributed on an 'as is' basis,
# without warranties or conditions of any kind, either express or implied.
# see the license for the specific language governing permissions and
# limitations under the license.
# ==============================================================================
"""a simple mnist classifier which displays summaries in tensorboard.
this is an unimpressive mnist model, but it is a good example of using
tf.name_scope to make a graph legible in the tensorboard graph explorer, and of
naming summary tags so that they are grouped meaningfully in tensorboard.
it demonstrates the functionality of every tensorboard dashboard.
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import os
import sys
z
import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data

flags = none


def train():
  # import data
  mnist = input_data.read_data_sets(flags.data_dir,
                                    one_hot=true,
                                    fake_data=flags.fake_data)

  sess = tf.interactivesession()
  # create a multilayer model.

  # input placeholders
  with tf.name_scope('input'):
    x = tf.placeholder(tf.float32, [none, 784], name='x-input')
    y_ = tf.placeholder(tf.float32, [none, 10], name='y-input')

  with tf.name_scope('input_reshape'):
    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])
    tf.summary.image('input', image_shaped_input, 10)

  # we can't initialize these variables to 0 - the network will get stuck.
  def weight_variable(shape):
    """create a weight variable with appropriate initialization."""
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.variable(initial)

  def bias_variable(shape):
    """create a bias variable with appropriate initialization."""
    initial = tf.constant(0.1, shape=shape)
    return tf.variable(initial)

  def variable_summaries(var):
    """attach a lot of summaries to a tensor (for tensorboard visualization)."""
    with tf.name_scope('summaries'):
      mean = tf.reduce_mean(var)
      tf.summary.scalar('mean', mean)
      with tf.name_scope('stddev'):
        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
      tf.summary.scalar('stddev', stddev)
      tf.summary.scalar('max', tf.reduce_max(var))
      tf.summary.scalar('min', tf.reduce_min(var))
      tf.summary.histogram('histogram', var)

  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
    """reusable code for making a simple neural net layer.
    it does a matrix multiply, bias add, and then uses relu to nonlinearize.
    it also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    """
    # adding a name scope ensures logical grouping of the layers in the graph.
    with tf.name_scope(layer_name):
      # this variable will hold the state of the weights for the layer
      with tf.name_scope('weights'):
        weights = weight_variable([input_dim, output_dim])
        variable_summaries(weights)
      with tf.name_scope('biases'):
        biases = bias_variable([output_dim])
        variable_summaries(biases)
      with tf.name_scope('wx_plus_b'):
        preactivate = tf.matmul(input_tensor, weights) + biases
        tf.summary.histogram('pre_activations', preactivate)
      activations = act(preactivate, name='activation')
      tf.summary.histogram('activations', activations)
      return activations

  hidden1 = nn_layer(x, 784, 500, 'layer1')

  with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    tf.summary.scalar('dropout_keep_id203', keep_prob)
    dropped = tf.nn.dropout(hidden1, keep_prob)

  # do not apply softmax activation yet, see below.
  y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)

  with tf.name_scope('cross_id178'):
    # the raw formulation of cross-id178,
    #
    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),
    #                               reduction_indices=[1]))
    #
    # can be numerically unstable.
    #
    # so here we use tf.nn.softmax_cross_id178_with_logits on the
    # raw outputs of the nn_layer above, and then average across
    # the batch.
    diff = tf.nn.softmax_cross_id178_with_logits(labels=y_, logits=y)
    with tf.name_scope('total'):
      cross_id178 = tf.reduce_mean(diff)
  tf.summary.scalar('cross_id178', cross_id178)

  with tf.name_scope('train'):
    train_step = tf.train.adamoptimizer(flags.learning_rate).minimize(
        cross_id178)

  with tf.name_scope('accuracy'):
    with tf.name_scope('correct_prediction'):
      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    with tf.name_scope('accuracy'):
      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  tf.summary.scalar('accuracy', accuracy)

  # merge all the summaries and write them out to
  # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)
  merged = tf.summary.merge_all()
  train_writer = tf.summary.filewriter(flags.log_dir + '/train', sess.graph)
  test_writer = tf.summary.filewriter(flags.log_dir + '/test')
  tf.global_variables_initializer().run()

  # train the model, and also write summaries.
  # every 10th step, measure test-set accuracy, and write test summaries
  # all other steps, run train_step on training data, & add training summaries

  def feed_dict(train):
    """make a tensorflow feed_dict: maps data onto tensor placeholders."""
    if train or flags.fake_data:
      xs, ys = mnist.train.next_batch(100, fake_data=flags.fake_data)
      k = flags.dropout
    else:
      xs, ys = mnist.test.images, mnist.test.labels
      k = 1.0
    return {x: xs, y_: ys, keep_prob: k}

  for i in range(flags.max_steps):
    if i % 10 == 0:  # record summaries and test-set accuracy
      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(false))
      test_writer.add_summary(summary, i)
      print('accuracy at step %s: %s' % (i, acc))
    else:  # record train set summaries, and train
      if i % 100 == 99:  # record execution stats
        run_options = tf.runoptions(trace_level=tf.runoptions.full_trace)
        run_metadata = tf.runmetadata()
        summary, _ = sess.run([merged, train_step],
                              feed_dict=feed_dict(true),
                              options=run_options,
                              run_metadata=run_metadata)
        train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
        train_writer.add_summary(summary, i)
        print('adding run metadata for', i)
      else:  # record a summary
        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(true))
        train_writer.add_summary(summary, i)
  train_writer.close()
  test_writer.close()


def main(_):
  if tf.gfile.exists(flags.log_dir):
    tf.gfile.deleterecursively(flags.log_dir)
  tf.gfile.makedirs(flags.log_dir)
  train()


if __name__ == '__main__':
  parser = argparse.argumentparser()
  parser.add_argument('--fake_data', nargs='?', const=true, type=bool,
                      default=false,
                      help='if true, uses fake data for unit testing.')
  parser.add_argument('--max_steps', type=int, default=1000,
                      help='number of steps to run trainer.')
  parser.add_argument('--learning_rate', type=float, default=0.001,
                      help='initial learning rate')
  parser.add_argument('--dropout', type=float, default=0.9,
                      help='keep id203 for training dropout.')
  parser.add_argument(
      '--data_dir',
      type=str,
      default=os.path.join(os.getenv('test_tmpdir', '/tmp'),
                           'tensorflow/mnist/input_data'),
      help='directory for storing input data')
  parser.add_argument(
      '--log_dir',
      type=str,
      default=os.path.join(os.getenv('test_tmpdir', '/tmp'),
                           'tensorflow/mnist/logs/mnist_with_summaries'),
      help='summaries log directory')
  flags, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

# 0.9816

tensorboard images & embedding

   tensorflow can also plot many different kinds of information including
   images and id27. to add image summary:
image = tf.reshape(x[:1], [-1, 28, 28, 1])
tf.summary.image("image", image)

   [tb_image.png]

   image source from tensorflow:
   [tb_embedding.png]

tensorboard graph & distribution

   tensorflow automatically plots the computational graph and can be
   viewed under the graph tab.
   [tb_graph.png]

   all histogram can also view as distribution.
   [tb_dist.png]
   please enable javascript to view the [5]comments powered by disqus.
   [6]comments powered by disqus

     * jonathan hui blog

     * [7]jhui

   deep learning

references

   visible links
   1. https://jhui.github.io/feed.xml
   2. https://jhui.github.io/feed.xml
   3. https://jhui.github.io/
   4. https://jhui.github.io/about/
   5. http://disqus.com/?ref_noscript
   6. http://disqus.com/
   7. https://github.com/jhui

   hidden links:
   9. https://jhui.github.io/2017/03/12/tensorboard-visualize-your-learning/
