   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   21 october 2017 / [12]id27s

id27s in 2017: trends and future directions

   id27s in 2017: trends and future directions

   this post discusses the deficiencies of id27s and how recent
   approaches have tried to resolve them.

   table of contents:
     * [13]subword-level embeddings
     * [14]oov handling
     * [15]evaluation
     * [16]multi-sense embeddings
     * [17]beyond words as points
     * [18]phrases and multi-word expressions
     * [19]bias
     * [20]temporal dimension
     * [21]lack of theoretical understanding
     * [22]task and domain-specific embeddings
     * [23]id21
     * [24]embeddings for multiple languages
     * [25]embeddings based on other contexts

   the id97 method based on skip-gram with negative sampling (mikolov
   et al., 2013) ^[26][1] was published in 2013 and had a large impact on
   the field, mainly through its accompanying software package, which
   enabled efficient training of dense word representations and a
   straightforward integration into downstream models. in some respects,
   we have come far since then: id27s have established
   themselves as an integral part of natural language processing (nlp)
   models. in other aspects, we might as well be in 2013 as we have not
   found ways to pre-train id27s that have managed to supersede
   the original id97.

   this post will focus on the deficiencies of id27s and how
   recent approaches have tried to resolve them. if not otherwise stated,
   this post discusses pre-trained id27s, i.e. word
   representations that have been learned on a large corpus using id97
   and its variants. pre-trained id27s are most effective if not
   millions of training examples are available (and thus transferring
   knowledge from a large unlabelled corpus is useful), which is true for
   most tasks in nlp. for an introduction to id27s, refer to
   [27]this blog post.

subword-level embeddings

   id27s have been augmented with subword-level information for
   many applications such as id39 (lample et al.,
   2016) ^[28][2], part-of-speech tagging (plank et al., 2016) ^[29][3],
   id33 (ballesteros et al., 2015; yu & vu, 2017) ^[30][4],
   ^[31][5], and language modelling (kim et al., 2016) ^[32][6]. most of
   these models employ a id98 or a bilstm that takes as input the
   characters of a word and outputs a character-based word representation.

   for incorporating character information into pre-trained embeddings,
   however, character id165s features have been shown to be more powerful
   than composition functions over individual characters (wieting et al.,
   2016; bojanowski et al., 2017) ^[33][7], ^[34][8]. character id165s --
   by far not a novel feature for text categorization (cavnar et al.,
   1994) ^[35][9] -- are particularly efficient and also form the basis of
   facebook's fasttext classifier (joulin et al., 2016) ^[36][10].
   embeddings learned using fasttext are [37]available in 294 languages.

   subword units based on byte-pair encoding have been found to be
   particularly useful for machine translation (sennrich et al., 2016)
   ^[38][11] where they have replaced words as the standard input units.
   they are also useful for tasks with many unknown words such as entity
   typing (heinzerling & strube, 2017) ^[39][12], but have not been shown
   to be helpful yet for standard nlp tasks, where this is not a major
   concern. while they can be learned easily, it is difficult to see their
   advantage over character-based representations for most tasks (vania &
   lopez, 2017) ^[40][13].

   another choice for using pre-trained embeddings that integrate
   character information is to leverage a state-of-the-art language model
   (jozefowicz et al., 2016) ^[41][14] trained on a large in-domain
   corpus, e.g. the 1 billion word benchmark (a pre-trained tensorflow
   model can be found [42]here). while language modelling has been found
   to be useful for different tasks as auxiliary objective (rei, 2017)
   ^[43][15], pre-trained language model embeddings have also been used to
   augment id27s (peters et al., 2017) ^[44][16]. as we start to
   better understand how to pre-train and initialize our models,
   pre-trained language model embeddings are poised to become more
   effective. they might even supersede id97 as the go-to choice for
   initializing id27s by virtue of having become more expressive
   and easier to train due to better frameworks and more computational
   resources over the last years.

oov handling

   one of the main problems of using pre-trained id27s is that
   they are unable to deal with out-of-vocabulary (oov) words, i.e. words
   that have not been seen during training. typically, such words are set
   to the unk token and are assigned the same vector, which is an
   ineffective choice if the number of oov words is large. subword-level
   embeddings as discussed in the last section are one way to mitigate
   this issue. another way, which is effective for reading comprehension
   (dhingra et al., 2017) ^[45][17] is to assign oov words their
   pre-trained id27, if one is available.

   recently, different approaches have been proposed for generating
   embeddings for oov words on-the-fly. herbelot and baroni (2017)
   ^[46][18] initialize the embedding of oov words as the sum of their
   context words and then rapidly refine only the oov embedding with a
   high learning rate. their approach is successful for a dataset that
   explicitly requires to model nonce words, but it is unclear if it can
   be scaled up to work reliably for more typical nlp tasks. another
   interesting approach for generating oov id27s is to train a
   character-based model to explicitly re-create pre-trained embeddings
   (pinter et al., 2017) ^[47][19]. this is particularly useful in
   low-resource scenarios, where a large corpus is inaccessible and only
   pre-trained embeddings are available.

evaluation

   evaluation of pre-trained embeddings has been a contentious issue since
   their inception as the commonly used evaluation via word similarity or
   analogy datasets has been shown to only correlate weakly with
   downstream performance (tsvetkov et al., 2015) ^[48][20]. the
   [49]repeval workshop at acl 2016 exclusively focused on better ways to
   evaluate pre-trained embeddings. as it stands, the consensus seems to
   be that -- while pre-trained embeddings can be evaluated on intrinsic
   tasks such as word similarity for comparison against previous
   approaches -- the best way to evaluate them is extrinsic evaluation on
   downstream tasks.

multi-sense embeddings

   a commonly cited criticism of id27s is that they are unable
   to capture polysemy. [50]a tutorial at acl 2016 outlined the work in
   recent years that focused on learning separate embeddings for multiple
   senses of a word (neelakantan et al., 2014; iacobacci et al., 2015;
   pilehvar & collier, 2016) ^[51][21], ^[52][22], ^[53][23]. however,
   most existing approaches for learning multi-sense embeddings solely
   evaluate on word similarity. pilehvar et al. (2017) ^[54][24] are one
   of the first to show results on topic categorization as a downstream
   task; while multi-sense embeddings outperform randomly initialized word
   embeddings in their experiments, they are outperformed by pre-trained
   id27s.

   given the stellar results id4 systems using word
   embeddings have achieved in recent years (johnson et al., 2016)
   ^[55][25], it seems that the current generation of models is expressive
   enough to contextualize and disambiguate words in context without
   having to rely on a dedicated disambiguation pipeline or multi-sense
   embeddings. however, we still need better ways to understand whether
   our models are actually able to sufficiently disambiguate words and how
   to improve this disambiguation behaviour if necessary.

beyond words as points

   while we might not need separate embeddings for every sense of each
   word for good downstream performance, reducing each word to a point in
   a vector space is unarguably overly simplistic and causes us to miss
   out on nuances that might be useful for downstream tasks. an
   interesting direction is thus to employ other representations that are
   better able to capture these facets. vilnis & mccallum (2015) ^[56][26]
   propose to model each word as a id203 distribution rather than a
   point vector, which allows us to represent id203 mass and
   uncertainty across certain dimensions. athiwaratkun & wilson (2017)
   ^[57][27] extend this approach to a multimodal distribution that allows
   to deal with polysemy, entailment, uncertainty, and enhances
   interpretability.

   rather than altering the representation, the embedding space can also
   be changed to better represent certain features. nickel and kiela
   (2017) ^[58][28], for instance, embed words in a hyperbolic space, to
   learn hierarchical representations. finding other ways to represent
   words that incorporate linguistic assumptions or better deal with the
   characteristics of downstream tasks is a compelling research direction.

phrases and multi-word expressions

   in addition to not being able to capture multiple senses of words, word
   embeddings also fail to capture the meanings of phrases and multi-word
   expressions, which can be a function of the meaning of their
   constituent words, or have an entirely new meaning. phrase embeddings
   have been proposed already in the original id97 paper (mikolov et
   al., 2013) ^[59][29] and there has been consistent work on learning
   better compositional and non-compositional phrase embeddings (yu &
   dredze, 2015; hashimoto & tsuruoka, 2016) ^[60][30], ^[61][31].
   however, similar to multi-sense embeddings, explicitly modelling
   phrases has so far not shown significant improvements on downstream
   tasks that would justify the additional complexity. analogously, a
   better understanding of how phrases are modelled in neural networks
   would pave the way to methods that augment the capabilities of our
   models to capture compositionality and non-compositionality of
   expressions.

bias

   bias in our models is becoming a larger issue and we are only starting
   to understand its implications for training and evaluating our models.
   even id27s trained on google news articles exhibit
   female/male gender stereotypes to a disturbing extent (bolukbasi et
   al., 2016) ^[62][32]. understanding what other biases id27s
   capture and finding better ways to remove theses biases will be key to
   developing fair algorithms for natural language processing.

temporal dimension

   words are a mirror of the zeitgeist and their meanings are subject to
   continuous change; current representations of words might differ
   substantially from the way these words where used in the past and will
   be used in the future. an interesting direction is thus to take into
   account the temporal dimension and the diachronic nature of words. this
   can allows us to reveal laws of semantic change (hamilton et al., 2016;
   baid113r & mandt, 2017; dubossarsky et al., 2017) ^[63][33], ^[64][34],
   ^[65][35], to model temporal word analogy or relatedness (szymanski,
   2017; rosin et al., 2017) ^[66][36], ^[67][37], or to capture the
   dynamics of semantic relations (kutuzov et al., 2017) ^[68][37:1].

lack of theoretical understanding

   besides the insight that id97 with skip-gram negative sampling
   implicitly factorizes a pmi matrix (levy & goldberg, 2014) ^[69][38],
   there has been comparatively little work on gaining a better
   theoretical understanding of the id27 space and its
   properties, e.g. that summation captures analogy relations. arora et
   al. (2016) ^[70][39] propose a new generative model for word
   embeddings, which treats corpus generation as a random walk of a
   discourse vector and establishes some theoretical motivations regarding
   the analogy behaviour. gittens et al. (2017) ^[71][40] provide a more
   thorough theoretical justification of additive compositionality and
   show that skip-gram word vectors are optimal in an
   information-theoretic sense. mimno & thompson (2017) ^[72][41]
   furthermore reveal an interesting relation between id27s and
   the embeddings of context words, i.e. that they are not evenly
   dispersed across the vector space, but occupy a narrow cone that is
   diametrically opposite to the context id27s. despite these
   additional insights, our understanding regarding the location and
   properties of id27s is still lacking and more theoretical
   work is necessary.

task and domain-specific embeddings

   one of the major downsides of using pre-trained embeddings is that the
   news data used for training them is often very different from the data
   on which we would like to use them. in most cases, however, we do not
   have access to millions of unlabelled documents in our target domain
   that would allow for pre-training good embeddings from scratch. we
   would thus like to be able to adapt embeddings pre-trained on large
   news corpora, so that they capture the characteristics of our target
   domain, but still retain all relevant existing knowledge. lu & zheng
   (2017) ^[73][42] proposed a regularized skip-gram model for learning
   such cross-domain embeddings. in the future, we will need even better
   ways to adapt pre-trained embeddings to new domains or to incorporate
   the knowledge from multiple relevant domains.

   rather than adapting to a new domain, we can also use existing
   knowledge encoded in semantic lexicons to augment pre-trained
   embeddings with information that is relevant for our task. an effective
   way to inject such relations into the embedding space is retro-fitting
   (faruqui et al., 2015) ^[74][43], which has been expanded to other
   resources such as conceptnet (speer et al., 2017) ^[75][44] and
   extended with an intelligent selection of positive and negative
   examples (mrk  i   et al., 2017) ^[76][45]. injecting additional prior
   knowledge into id27s such as monotonicity (you et al., 2017)
   ^[77][46], word similarity (niebler et al., 2017) ^[78][47],
   task-related grading or intensity, or logical relations is an important
   research direction that will allow to make our models more robust.

   id27s are useful for a wide variety of applications beyond
   nlp such as information retrieval, recommendation, and link prediction
   in knowledge bases, which all have their own task-specific approaches.
   wu et al. (2017) ^[79][48] propose a general-purpose model that is
   compatible with many of these applications and can serve as a strong
   baseline.

id21

   rather than adapting id27s to any particular task, recent
   work has sought to create contextualized word vectors by augmenting
   id27s with embeddings based on the hidden states of models
   pre-trained for certain tasks, such as machine translation (mccann et
   al., 2017) ^[80][49] or language modelling (peters et al., 2018)
   ^[81][50]. together with fine-tuning pre-trained models (howard and
   ruder, 2018) ^[82][51], this is one of the most promising research
   directions.

embeddings for multiple languages

   as nlp models are being increasingly employed and evaluated on multiple
   languages, creating multilingual id27s is becoming a more
   important issue and has received increased interest over recent years.
   a promising direction is to develop methods that learn cross-lingual
   representations with as few parallel data as possible, so that they can
   be easily applied to learn representations even for low-resource
   languages. for a recent survey in this area, refer to ruder et al.
   (2017) ^[83][52].

embeddings based on other contexts

   id27s are typically learned only based on the window of
   surrounding context words. levy & goldberg (2014) ^[84][53] have shown
   that dependency structures can be used as context to capture more
   syntactic word relations; k  hn (2015) ^[85][54] finds that such
   dependency-based embeddings perform best for a particular multilingual
   evaluation method that clusters embeddings along different syntactic
   features.

   melamud et al. (2016) ^[86][55] observe that different context types
   work well for different downstream tasks and that simple concatenation
   of id27s learned with different context types can yield
   further performance gains. given the recent success of incorporating
   graph structures into neural models for different tasks as -- for
   instance -- exhibited by graph-convolutional neural networks (bastings
   et al., 2017; marcheggiani & titov, 2017) ^[87][56] ^[88][57], we can
   conjecture that incorporating such structures for learning embeddings
   for downstream tasks may also be beneficial.

   besides selecting context words differently, additional context may
   also be used in other ways: tissier et al. (2017) ^[89][58] incorporate
   co-occurrence information from dictionary definitions into the negative
   sampling process to move related works closer together and prevent them
   from being used as negative samples. we can think of topical or
   relatedness information derived from other contexts such as article
   headlines or wikipedia intro paragraphs that could similarly be used to
   make the representations more applicable to a particular downstream
   task.

conclusion

   it is nice to see that as a community we are progressing from applying
   id27s to every possible problem to gaining a more principled,
   nuanced, and practical understanding of them. this post was meant to
   highlight some of the current trends and future directions for learning
   id27s that i found most compelling. i've undoubtedly failed
   to mention many other areas that are equally important and noteworthy.
   please let me know in the comments below what i missed, where i made a
   mistake or misrepresented a method, or just which aspect of word
   embeddings you find particularly exciting or unexplored.

hacker news

   refer to the [90]discussion on hacker news for some more insights on
   id27s.

other blog posts on id27s

   if you want to learn more about id27s, these other blog posts
   on id27s are also available:
     * [91]on id27s - part 1
     * [92]on id27s - part 2: approximating the softmax
     * [93]on id27s - part 3: the secret ingredients of id97
     * [94]unofficial part 4: a survey of cross-lingual embedding models

references

   cover image credit: hamilton et al. (2016)
     __________________________________________________________________

    1. mikolov, t., corrado, g., chen, k., & dean, j. (2013). efficient
       estimation of word representations in vector space. proceedings of
       the international conference on learning representations (iclr
       2013), 1   12. [95]      
    2. lample, g., ballesteros, m., subramanian, s., kawakami, k., & dyer,
       c. (2016). neural architectures for id39. in
       naacl-hlt 2016. [96]      
    3. plank, b., s  gaard, a., & goldberg, y. (2016). multilingual
       part-of-speech tagging with bidirectional long short-term memory
       models and auxiliary loss. in proceedings of the 54th annual
       meeting of the association for computational linguistics. [97]      
    4. ballesteros, m., dyer, c., & smith, n. a. (2015). improved
       transition-based parsing by modeling characters instead of words
       with lstms. in proceedings of emnlp 2015.
       [98]https://doi.org/10.18653/v1/d15-1041 [99]      
    5. yu, x., & vu, n. t. (2017). character composition model with
       convolutional neural networks for id33 on
       morphologically rich languages. in proceedings of the 55th annual
       meeting of the association for computational linguistics (pp.
       672   678). [100]      
    6. kim, y., jernite, y., sontag, d., & rush, a. m. (2016).
       character-aware neural language models. aaai. retrieved from
       [101]http://arxiv.org/abs/1508.06615 [102]      
    7. wieting, j., bansal, m., gimpel, k., & livescu, k. (2016).
       charagram: embedding words and sentences via character id165s.
       retrieved from [103]http://arxiv.org/abs/1607.02789 [104]      
    8. bojanowski, p., grave, e., joulin, a., & mikolov, t. (2017).
       enriching word vectors with subword information. transactions of
       the association for computational linguistics. retrieved from
       [105]http://arxiv.org/abs/1607.04606 [106]      
    9. cavnar, w. b., trenkle, j. m., & mi, a. a. (1994). id165-based
       text categorization. ann arbor mi 48113.2, 161   175.
       [107]https://doi.org/10.1.1.53.9367 [108]      
   10. joulin, a., grave, e., bojanowski, p., & mikolov, t. (2016). bag of
       tricks for efficient text classification. arxiv preprint
       arxiv:1607.01759. retrieved from
       [109]http://arxiv.org/abs/1607.01759 [110]      
   11. sennrich, r., haddow, b., & birch, a. (2016). neural machine
       translation of rare words with subword units. in proceedings of the
       54th annual meeting of the association for computational
       linguistics (acl 2016). retrieved from
       [111]http://arxiv.org/abs/1508.07909 [112]      
   12. heinzerling, b., & strube, m. (2017). bpemb: id121-free
       pre-trained subid27s in 275 languages. retrieved from
       [113]http://arxiv.org/abs/1710.02187 [114]      
   13. vania, c., & lopez, a. (2017). from characters to words to in
       between: do we capture morphology? in proceedings of the 55th
       annual meeting of the association for computational linguistics
       (pp. 2016   2027). [115]      
   14. jozefowicz, r., vinyals, o., schuster, m., shazeer, n., & wu, y.
       (2016). exploring the limits of id38. arxiv preprint
       arxiv:1602.02410. retrieved from
       [116]http://arxiv.org/abs/1602.02410 [117]      
   15. rei, m. (2017). semi-supervised multitask learning for sequence
       labeling. in proceedings of acl 2017. [118]      
   16. peters, m. e., ammar, w., bhagavatula, c., & power, r. (2017).
       semi-supervised sequence tagging with bidirectional language
       models. in proceedings of the 55th annual meeting of the
       association for computational linguistics (pp. 1756   1765). [119]      
   17. dhingra, b., liu, h., salakhutdinov, r., & cohen, w. w. (2017). a
       comparative study of id27s for reading comprehension.
       arxiv preprint arxiv:1703.00993. [120]      
   18. herbelot, a., & baroni, m. (2017). high-risk learning: acquiring
       new word vectors from tiny data. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       [121]      
   19. pinter, y., guthrie, r., & eisenstein, j. (2017). mimicking word
       embeddings using subword id56s. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       retrieved from [122]http://arxiv.org/abs/1707.06961 [123]      
   20. tsvetkov, y., faruqui, m., ling, w., lample, g., & dyer, c. (2015).
       evaluation of word vector representations by subspace alignment.
       proceedings of the 2015 conference on empirical methods in natural
       language processing, lisbon, portugal, 17-21 september 2015,
       2049   2054. [124]      
   21. neelakantan, a., shankar, j., passos, a., & mccallum, a. (2014).
       efficient non-parametric estimation of multiple embeddings per word
       in vector space. in proceedings fo (pp. 1059   1069). [125]      
   22. iacobacci, i., pilehvar, m. t., & navigli, r. (2015). sensembed:
       learning sense embeddings for word and relational similarity. in
       proceedings of the 53rd annual meeting of the association for
       computational linguistics and the 7th international joint
       conference on natural language processing (pp. 95   105). [126]      
   23. pilehvar, m. t., & collier, n. (2016). de-conflated semantic
       representations. in proceedings of emnlp. [127]      
   24. pilehvar, m. t., camacho-collados, j., navigli, r., & collier, n.
       (2017). towards a seaid113ss integration of word senses into
       downstream nlp applications. in proceedings of the 55th annual
       meeting of the association for computational linguistics (volume 1:
       long papers) (pp. 1857   1869).
       [128]https://doi.org/10.18653/v1/p17-1170 [129]      
   25. johnson, m., schuster, m., le, q. v, krikun, m., wu, y., chen, z.,
           dean, j. (2016). google   s multilingual id4
       system: enabling zero-shot translation. arxiv preprint
       arxiv:1611.0455. [130]      
   26. vilnis, l., & mccallum, a. (2015). word representations via
       gaussian embedding. iclr. retrieved from
       [131]http://arxiv.org/abs/1412.6623 [132]      
   27. athiwaratkun, b., & wilson, a. g. (2017). multimodal word
       distributions. in proceedings of the 55th annual meeting of the
       association for computational linguistics (acl 2017). [133]      
   28. nickel, m., & kiela, d. (2017). poincar   embeddings for learning
       hierarchical representations. arxiv preprint arxiv:1705.08039.
       retrieved from [134]http://arxiv.org/abs/1705.08039 [135]      
   29. mikolov, t., chen, k., corrado, g., & dean, j. (2013). distributed
       representations of words and phrases and their compositionality.
       nips. [136]      
   30. yu, m., & dredze, m. (2015). learning composition models for phrase
       embeddings. transactions of the acl, 3, 227   242. [137]      
   31. hashimoto, k., & tsuruoka, y. (2016). adaptive joint learning of
       compositional and non-compositional phrase embeddings. acl,
       205   215. retrieved from [138]http://arxiv.org/abs/1603.06067
       [139]      
   32. bolukbasi, t., chang, k.-w., zou, j., saligrama, v., & kalai, a.
       (2016). man is to computer programmer as woman is to homemaker?
       debiasing id27s. in 30th conference on neural information
       processing systems (nips 2016). retrieved from
       [140]http://arxiv.org/abs/1607.06520 [141]      
   33. hamilton, w. l., leskovec, j., & jurafsky, d. (2016). diachronic
       id27s reveal statistical laws of semantic change. in
       proceedings of the 54th annual meeting of the association for
       computational linguistics (pp. 1489   1501). [142]      
   34. baid113r, r., & mandt, s. (2017). dynamic id27s via
       skip-gram filtering. in proceedings of icml 2017. retrieved from
       [143]http://arxiv.org/abs/1702.08359 [144]      
   35. dubossarsky, h., grossman, e., & weinshall, d. (2017). outta
       control: laws of semantic change and inherent biases in word
       representation models. in conference on empirical methods in
       natural language processing (pp. 1147   1156). retrieved from
       [145]http://aclweb.org/anthology/d17-1119 [146]      
   36. szymanski, t. (2017). temporal word analogies : identifying lexical
       replacement with diachronic id27s. in proceedings of the
       55th annual meeting of the association for computational
       linguistics (pp. 448   453). [147]      
   37. rosin, g., radinsky, k., & adar, e. (2017). learning word
       relatedness over time. in proceedings of the 2017 conference on
       empirical methods in natural language processing. retrieved from
       [148]https://arxiv.org/pdf/1707.08081.pdf [149]       [150]      
   38. levy, o., & goldberg, y. (2014). neural id27 as implicit
       id105. advances in neural information processing
       systems (nips), 2177   2185. retrieved from
       [151]http://papers.nips.cc/paper/5477-neural-word-embedding-as-impl
       icit-matrix-factorization [152]      
   39. arora, s., li, y., liang, y., ma, t., & risteski, a. (2016). a
       latent variable model approach to pmi-based id27s. tacl,
       4, 385   399. retrieved from
       [153]https://transacl.org/ojs/index.php/tacl/article/viewfile/742/2
       04 [154]      
   40. gittens, a., achlioptas, d., & mahoney, m. w. (2017). skip-gram    
       zipf + uniform = vector additivity. in proceedings of the 55th
       annual meeting of the association for computational linguistics
       (pp. 69   76). [155]https://doi.org/10.18653/v1/p17-1007 [156]      
   41. mimno, d., & thompson, l. (2017). the strange geometry of skip-gram
       with negative sampling. in proceedings of the 2017 conference on
       empirical methods in natural language processing (pp. 2863   2868).
       [157]      
   42. lu, w., & zheng, v. w. (2017). a simple id173-based
       algorithm for learning cross-domain id27s. in proceedings
       of the 2017 conference on empirical methods in natural language
       processing (pp. 2888   2894). [158]      
   43. faruqui, m., dodge, j., jauhar, s. k., dyer, c., hovy, e., & smith,
       n. a. (2015). retrofitting word vectors to semantic lexicons. in
       naacl 2015. [159]      
   44. speer, r., chin, j., & havasi, c. (2017). conceptnet 5.5: an open
       multilingual graph of general knowledge. in aaai 31 (pp.
       4444   4451). retrieved from [160]http://arxiv.org/abs/1612.03975
       [161]      
   45. mrk  i  , n., vuli  , i., s  aghdha, d.   ., leviant, i., reichart, r.,
       ga  i  , m.,     young, s. (2017). semantic specialisation of
       distributional word vector spaces using monolingual and
       cross-lingual constraints. tacl. retrieved from
       [162]http://arxiv.org/abs/1706.00374 [163]      
   46. you, s., ding, d., canini, k., pfeifer, j., & gupta, m. (2017).
       deep lattice networks and partial monotonic functions. in 31st
       conference on neural information processing systems (nips 2017).
       retrieved from [164]http://arxiv.org/abs/1709.06680 [165]      
   47. niebler, t., becker, m., p  litz, c., & hotho, a. (2017). learning
       semantic relatedness from human feedback using metric learning. in
       proceedings of iswc 2017. retrieved from
       [166]http://arxiv.org/abs/1705.07425 [167]      
   48. wu, l., fisch, a., chopra, s., adams, k., bordes, a., & weston, j.
       (2017). starspace: embed all the things! arxiv preprint
       arxiv:1709.03856. [168]      
   49. mccann, b., bradbury, j., xiong, c., & socher, r. (2017). learned
       in translation: contextualized word vectors. in advances in neural
       information processing systems. [169]      
   50. peters, m. e., neumann, m., iyyer, m., gardner, m., clark, c., lee,
       k., & zettlemoyer, l. (2018). deep contextualized word
       representations. proceedings of naacl-hlt 2018. [170]      
   51. howard, j., & ruder, s. (2018). universal language model
       fine-tuning for text classification. in proceedings of acl 2018.
       retrieved from [171]http://arxiv.org/abs/1801.06146 [172]      
   52. ruder, s., vuli  , i., & s  gaard, a. (2017). a survey of
       cross-lingual id27 models sebastian. arxiv preprint
       arxiv:1706.04902. retrieved from
       [173]http://arxiv.org/abs/1706.04902 [174]      
   53. levy, o., & goldberg, y. (2014). dependency-based id27s.
       proceedings of the 52nd annual meeting of the association for
       computational linguistics (short papers), 302   308.
       [175]https://doi.org/10.3115/v1/p14-2050 [176]      
   54. k  hn, a. (2015). what   s in an embedding? analyzing id27s
       through multilingual evaluation. proceedings of the 2015 conference
       on empirical methods in natural language processing, lisbon,
       portugal, 17-21 september 2015, (2014), 2067   2073. [177]      
   55. melamud, o., mcclosky, d., patwardhan, s., & bansal, m. (2016). the
       role of context types and dimensionality in learning word
       embeddings. in proceedings of naacl-hlt 2016 (pp. 1030   1040).
       retrieved from [178]http://arxiv.org/abs/1601.00893 [179]      
   56. bastings, j., titov, i., aziz, w., marcheggiani, d., & sima   an, k.
       (2017). graph convolutional encoders for syntax-aware neural
       machine translation. in proceedings of the 2017 conference on
       empirical methods in natural language processing. [180]      
   57. marcheggiani, d., & titov, i. (2017). encoding sentences with graph
       convolutional networks for id14. in proceedings
       of the 2017 conference on empirical methods in natural language
       processing. [181]      
   58. tissier, j., gravier, c., & habrard, a. (2017). dict2vec : learning
       id27s using lexical dictionaries. in proceedings of the
       2017 conference on empirical methods in natural language
       processing. retrieved from
       [182]http://aclweb.org/anthology/d17-1024 [183]      

   sebastian ruder

[184]sebastian ruder

   read [185]more posts by this author.
   [186]read more

       sebastian ruder    

[187]id27s

     * [188]aaai 2019 highlights: dialogue, reproducibility, and more
     * [189]emnlp 2018 highlights: inductive bias, cross-lingual learning,
       and more
     * [190]a review of the neural history of natural language processing

   [191]see all 9 posts    

   [192]optimization for deep learning highlights in 2017

   optimization

optimization for deep learning highlights in 2017

   different id119 optimization algorithms have been proposed
   in recent years but adam is still most commonly used. this post
   discusses the most exciting highlights and most promising recent
   approaches that may shape the way we will optimize our models in the
   future.

     * sebastian ruder
       [193]sebastian ruder

   [194]id72 objectives for natural language processing

   id72

id72 objectives for natural language processing

   id72 is becoming increasingly popular in nlp but it is
   still not understood very well which tasks are useful. as inspiration,
   this post gives an overview of the most common auxiliary tasks used for
   id72 for nlp.

     * sebastian ruder
       [195]sebastian ruder

   [196]sebastian ruder
      
   id27s in 2017: trends and future directions
   share this
   please enable javascript to view the [197]comments powered by disqus.

   [198]sebastian ruder    2019

   [199]latest posts [200]twitter [201]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/word-embeddings/index.html
  13. http://ruder.io/word-embeddings-2017/index.html#subwordlevelembeddings
  14. http://ruder.io/word-embeddings-2017/index.html#oovhandling
  15. http://ruder.io/word-embeddings-2017/index.html#evaluation
  16. http://ruder.io/word-embeddings-2017/index.html#multisenseembeddings
  17. http://ruder.io/word-embeddings-2017/index.html#beyondwordsaspoints
  18. http://ruder.io/word-embeddings-2017/index.html#phrasesandmultiwordexpressions
  19. http://ruder.io/word-embeddings-2017/index.html#bias
  20. http://ruder.io/word-embeddings-2017/index.html#temporaldimension
  21. http://ruder.io/word-embeddings-2017/index.html#lackoftheoreticalunderstanding
  22. http://ruder.io/word-embeddings-2017/index.html#taskanddomainspecificembeddings
  23. http://ruder.io/word-embeddings-2017/index.html#transferlearning
  24. http://ruder.io/word-embeddings-2017/index.html#embeddingsformultiplelanguages
  25. http://ruder.io/word-embeddings-2017/index.html#embeddingsbasedonothercontexts
  26. http://ruder.io/word-embeddings-2017/index.html#fn1
  27. http://ruder.io/word-embeddings-1/
  28. http://ruder.io/word-embeddings-2017/index.html#fn2
  29. http://ruder.io/word-embeddings-2017/index.html#fn3
  30. http://ruder.io/word-embeddings-2017/index.html#fn4
  31. http://ruder.io/word-embeddings-2017/index.html#fn5
  32. http://ruder.io/word-embeddings-2017/index.html#fn6
  33. http://ruder.io/word-embeddings-2017/index.html#fn7
  34. http://ruder.io/word-embeddings-2017/index.html#fn8
  35. http://ruder.io/word-embeddings-2017/index.html#fn9
  36. http://ruder.io/word-embeddings-2017/index.html#fn10
  37. https://github.com/facebookresearch/fasttext/blob/master/pretrained-vectors.md
  38. http://ruder.io/word-embeddings-2017/index.html#fn11
  39. http://ruder.io/word-embeddings-2017/index.html#fn12
  40. http://ruder.io/word-embeddings-2017/index.html#fn13
  41. http://ruder.io/word-embeddings-2017/index.html#fn14
  42. https://github.com/tensorflow/models/tree/master/research/lm_1b
  43. http://ruder.io/word-embeddings-2017/index.html#fn15
  44. http://ruder.io/word-embeddings-2017/index.html#fn16
  45. http://ruder.io/word-embeddings-2017/index.html#fn17
  46. http://ruder.io/word-embeddings-2017/index.html#fn18
  47. http://ruder.io/word-embeddings-2017/index.html#fn19
  48. http://ruder.io/word-embeddings-2017/index.html#fn20
  49. https://sites.google.com/site/repevalacl16/
  50. http://wwwusers.di.uniroma1.it/~collados/slides_acl16tutorial_semanticrepresentation.pdf
  51. http://ruder.io/word-embeddings-2017/index.html#fn21
  52. http://ruder.io/word-embeddings-2017/index.html#fn22
  53. http://ruder.io/word-embeddings-2017/index.html#fn23
  54. http://ruder.io/word-embeddings-2017/index.html#fn24
  55. http://ruder.io/word-embeddings-2017/index.html#fn25
  56. http://ruder.io/word-embeddings-2017/index.html#fn26
  57. http://ruder.io/word-embeddings-2017/index.html#fn27
  58. http://ruder.io/word-embeddings-2017/index.html#fn28
  59. http://ruder.io/word-embeddings-2017/index.html#fn29
  60. http://ruder.io/word-embeddings-2017/index.html#fn30
  61. http://ruder.io/word-embeddings-2017/index.html#fn31
  62. http://ruder.io/word-embeddings-2017/index.html#fn32
  63. http://ruder.io/word-embeddings-2017/index.html#fn33
  64. http://ruder.io/word-embeddings-2017/index.html#fn34
  65. http://ruder.io/word-embeddings-2017/index.html#fn35
  66. http://ruder.io/word-embeddings-2017/index.html#fn36
  67. http://ruder.io/word-embeddings-2017/index.html#fn37
  68. http://ruder.io/word-embeddings-2017/index.html#fn37
  69. http://ruder.io/word-embeddings-2017/index.html#fn38
  70. http://ruder.io/word-embeddings-2017/index.html#fn39
  71. http://ruder.io/word-embeddings-2017/index.html#fn40
  72. http://ruder.io/word-embeddings-2017/index.html#fn41
  73. http://ruder.io/word-embeddings-2017/index.html#fn42
  74. http://ruder.io/word-embeddings-2017/index.html#fn43
  75. http://ruder.io/word-embeddings-2017/index.html#fn44
  76. http://ruder.io/word-embeddings-2017/index.html#fn45
  77. http://ruder.io/word-embeddings-2017/index.html#fn46
  78. http://ruder.io/word-embeddings-2017/index.html#fn47
  79. http://ruder.io/word-embeddings-2017/index.html#fn48
  80. http://ruder.io/word-embeddings-2017/index.html#fn49
  81. http://ruder.io/word-embeddings-2017/index.html#fn50
  82. http://ruder.io/word-embeddings-2017/index.html#fn51
  83. http://ruder.io/word-embeddings-2017/index.html#fn52
  84. http://ruder.io/word-embeddings-2017/index.html#fn53
  85. http://ruder.io/word-embeddings-2017/index.html#fn54
  86. http://ruder.io/word-embeddings-2017/index.html#fn55
  87. http://ruder.io/word-embeddings-2017/index.html#fn56
  88. http://ruder.io/word-embeddings-2017/index.html#fn57
  89. http://ruder.io/word-embeddings-2017/index.html#fn58
  90. https://news.ycombinator.com/item?id=15521957
  91. http://ruder.io/word-embeddings-1/index.html
  92. http://ruder.io/word-embeddings-softmax/index.html
  93. http://ruder.io/secret-id97/index.html
  94. http://ruder.io/cross-lingual-embeddings/index.html
  95. http://ruder.io/word-embeddings-2017/index.html#fnref1
  96. http://ruder.io/word-embeddings-2017/index.html#fnref2
  97. http://ruder.io/word-embeddings-2017/index.html#fnref3
  98. https://doi.org/10.18653/v1/d15-1041
  99. http://ruder.io/word-embeddings-2017/index.html#fnref4
 100. http://ruder.io/word-embeddings-2017/index.html#fnref5
 101. http://arxiv.org/abs/1508.06615
 102. http://ruder.io/word-embeddings-2017/index.html#fnref6
 103. http://arxiv.org/abs/1607.02789
 104. http://ruder.io/word-embeddings-2017/index.html#fnref7
 105. http://arxiv.org/abs/1607.04606
 106. http://ruder.io/word-embeddings-2017/index.html#fnref8
 107. https://doi.org/10.1.1.53.9367
 108. http://ruder.io/word-embeddings-2017/index.html#fnref9
 109. http://arxiv.org/abs/1607.01759
 110. http://ruder.io/word-embeddings-2017/index.html#fnref10
 111. http://arxiv.org/abs/1508.07909
 112. http://ruder.io/word-embeddings-2017/index.html#fnref11
 113. http://arxiv.org/abs/1710.02187
 114. http://ruder.io/word-embeddings-2017/index.html#fnref12
 115. http://ruder.io/word-embeddings-2017/index.html#fnref13
 116. http://arxiv.org/abs/1602.02410
 117. http://ruder.io/word-embeddings-2017/index.html#fnref14
 118. http://ruder.io/word-embeddings-2017/index.html#fnref15
 119. http://ruder.io/word-embeddings-2017/index.html#fnref16
 120. http://ruder.io/word-embeddings-2017/index.html#fnref17
 121. http://ruder.io/word-embeddings-2017/index.html#fnref18
 122. http://arxiv.org/abs/1707.06961
 123. http://ruder.io/word-embeddings-2017/index.html#fnref19
 124. http://ruder.io/word-embeddings-2017/index.html#fnref20
 125. http://ruder.io/word-embeddings-2017/index.html#fnref21
 126. http://ruder.io/word-embeddings-2017/index.html#fnref22
 127. http://ruder.io/word-embeddings-2017/index.html#fnref23
 128. https://doi.org/10.18653/v1/p17-1170
 129. http://ruder.io/word-embeddings-2017/index.html#fnref24
 130. http://ruder.io/word-embeddings-2017/index.html#fnref25
 131. http://arxiv.org/abs/1412.6623
 132. http://ruder.io/word-embeddings-2017/index.html#fnref26
 133. http://ruder.io/word-embeddings-2017/index.html#fnref27
 134. http://arxiv.org/abs/1705.08039
 135. http://ruder.io/word-embeddings-2017/index.html#fnref28
 136. http://ruder.io/word-embeddings-2017/index.html#fnref29
 137. http://ruder.io/word-embeddings-2017/index.html#fnref30
 138. http://arxiv.org/abs/1603.06067
 139. http://ruder.io/word-embeddings-2017/index.html#fnref31
 140. http://arxiv.org/abs/1607.06520
 141. http://ruder.io/word-embeddings-2017/index.html#fnref32
 142. http://ruder.io/word-embeddings-2017/index.html#fnref33
 143. http://arxiv.org/abs/1702.08359
 144. http://ruder.io/word-embeddings-2017/index.html#fnref34
 145. http://aclweb.org/anthology/d17-1119
 146. http://ruder.io/word-embeddings-2017/index.html#fnref35
 147. http://ruder.io/word-embeddings-2017/index.html#fnref36
 148. https://arxiv.org/pdf/1707.08081.pdf
 149. http://ruder.io/word-embeddings-2017/index.html#fnref37
 150. http://ruder.io/word-embeddings-2017/index.html#fnref37:1
 151. http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization
 152. http://ruder.io/word-embeddings-2017/index.html#fnref38
 153. https://transacl.org/ojs/index.php/tacl/article/viewfile/742/204
 154. http://ruder.io/word-embeddings-2017/index.html#fnref39
 155. https://doi.org/10.18653/v1/p17-1007
 156. http://ruder.io/word-embeddings-2017/index.html#fnref40
 157. http://ruder.io/word-embeddings-2017/index.html#fnref41
 158. http://ruder.io/word-embeddings-2017/index.html#fnref42
 159. http://ruder.io/word-embeddings-2017/index.html#fnref43
 160. http://arxiv.org/abs/1612.03975
 161. http://ruder.io/word-embeddings-2017/index.html#fnref44
 162. http://arxiv.org/abs/1706.00374
 163. http://ruder.io/word-embeddings-2017/index.html#fnref45
 164. http://arxiv.org/abs/1709.06680
 165. http://ruder.io/word-embeddings-2017/index.html#fnref46
 166. http://arxiv.org/abs/1705.07425
 167. http://ruder.io/word-embeddings-2017/index.html#fnref47
 168. http://ruder.io/word-embeddings-2017/index.html#fnref48
 169. http://ruder.io/word-embeddings-2017/index.html#fnref49
 170. http://ruder.io/word-embeddings-2017/index.html#fnref50
 171. http://arxiv.org/abs/1801.06146
 172. http://ruder.io/word-embeddings-2017/index.html#fnref51
 173. http://arxiv.org/abs/1706.04902
 174. http://ruder.io/word-embeddings-2017/index.html#fnref52
 175. https://doi.org/10.3115/v1/p14-2050
 176. http://ruder.io/word-embeddings-2017/index.html#fnref53
 177. http://ruder.io/word-embeddings-2017/index.html#fnref54
 178. http://arxiv.org/abs/1601.00893
 179. http://ruder.io/word-embeddings-2017/index.html#fnref55
 180. http://ruder.io/word-embeddings-2017/index.html#fnref56
 181. http://ruder.io/word-embeddings-2017/index.html#fnref57
 182. http://aclweb.org/anthology/d17-1024
 183. http://ruder.io/word-embeddings-2017/index.html#fnref58
 184. http://ruder.io/author/sebastian/index.html
 185. http://ruder.io/author/sebastian/index.html
 186. http://ruder.io/author/sebastian/index.html
 187. http://ruder.io/tag/word-embeddings/index.html
 188. http://ruder.io/aaai-2019-highlights/index.html
 189. http://ruder.io/emnlp-2018-highlights/index.html
 190. http://ruder.io/a-review-of-the-recent-history-of-nlp/index.html
 191. http://ruder.io/tag/word-embeddings/index.html
 192. http://ruder.io/index.html
 193. http://ruder.io/author/sebastian/index.html
 194. http://ruder.io/index.html
 195. http://ruder.io/author/sebastian/index.html
 196. http://ruder.io/
 197. https://disqus.com/?ref_noscript
 198. http://ruder.io/
 199. http://ruder.io/
 200. https://twitter.com/seb_ruder
 201. https://ghost.org/

   hidden links:
 203. https://twitter.com/seb_ruder
 204. http://ruder.io/rss/index.rss
 205. http://ruder.io/index.html
 206. http://ruder.io/index.html
 207. https://twitter.com/share?text=word%20embeddings%20in%202017%3a%20trends%20and%20future%20directions&url=http://ruder.io/word-embeddings-2017/
 208. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/word-embeddings-2017/
