   [93d5f08a4f82d4c.png]

   in this codelab, you will learn how to build and train a neural network
   that recognises handwritten digits. along the way, as you enhance your
   neural network to achieve 99% accuracy, you will also discover the
   tools of the trade that deep learning professionals use to train their
   models efficiently.

   this codelab uses the [1]mnist dataset, a collection of 60,000 labeled
   digits that has kept generations of phds busy for almost two decades.
   you will solve the problem with less than 100 lines of python /
   tensorflow code.

  what you'll learn

     * what is a neural network and how to train it
     * how to build a basic 1-layer neural network using tensorflow
     * how to add more layers
     * training tips and tricks: overfitting, dropout, learning rate
       decay...
     * how to troubleshoot deep neural networks
     * how to build convolutional networks

  what you'll need

     * python 2 or 3 (python 3 recommended)
     * tensorflow
     * matplotlib (python visualisation library)

   installation instructions are given in the next step of the lab.

   install the necessary software on your computer: python, tensorflow and
   matplotlib. full installation instructions are given here:
   [2]install.txt

   clone the github repository:
$ git clone https://github.com/googlecloudplatform/tensorflow-without-a-phd.git
$ cd tensorflow-without-a-phd/tensorflow-mnist-tutorial

   the sample code for this tutorial is in the folder
   tensorflow-mnist-tutorial. the folder contains multiple files. the only
   one you will be working in is mnist_1.0_softmax.py. other files are
   either solutions or support code for loading the data and visualising
   results.

   when you launch the initial python script, you should see a real-time
   visualisation of the training process:
$ python3 mnist_1.0_softmax.py

   [96b63511fa2e9561.png]

   troubleshooting: if you cannot get the real-time visualisation to run
   or if you prefer working with only the text output, you can de-activate
   the visualisation by commenting out one line and de-commenting another.
   see instructions at the bottom of the file.

   the visualisation tool built for tensorflow is [3]tensorboard. its main
   goal is more ambitious than what we need here. it is built so that you
   can follow your distributed tensorflow jobs on remote servers. for what
   we need in this lab matplotlib will do and we get real-time animations
   as a bonus. but if you do serious work with tensorflow, make sure you
   check out tensorboard.

   we will first watch a neural network being trained. the code is
   explained in the next section so you do not have to look at it now.

   our neural network takes in handwritten digits and classifies them,
   i.e. states if it recognises them as a 0, a 1, a 2 and so on up to a 9.
   it does so based on internal variables ("weights" and "biases",
   explained later) that need to have a correct value for the
   classification to work well. this "correct value" is learned through a
   training process, also explained in detail later. what you need to know
   for now is that the training loop looks like this:

   training digits => updates to weights and biases => better recognition
   (loop)

   let us go through the six panels of the visualisation one by one to see
   what it takes to train a neural network.

   [6a54f12d0f63c9bc.png] [4db69fb45bbff06d.png]

   here you see the training digits being fed into the training loop, 100
   at a time. you also see if the neural network, in its current state of
   training, has recognized them (white background) or mis-classified them
   (red background with correct label in small print on the left side, bad
   computed label on the right of each digit).

   there are 50,000 training digits in this dataset. we feed 100 of them
   into the training loop at each iteration so the system will have seen
   all the training digits once after 500 iterations. we call this an
   "epoch".

   [e0a267f42349d949.png] [c4fb780b36379fe4.png]

   to test the quality of the recognition in real-world conditions, we
   must use digits that the system has not seen during training.
   otherwise, it could learn all the training digits by heart and still
   fail at recognising an "8" that i just wrote. the mnist dataset
   contains 10,000 test digits. here you see about 1000 of them with all
   the mis-recognised ones sorted at the top (on a red background). the
   scale on the left gives you a rough idea of the accuracy of the
   classifier (% of correctly recognised test digits)

   [d8da0646c2680934.png] [3df124def9ad2fba.png]

   to drive the training, we will define a id168, i.e. a value
   representing how badly the system recognises the digits and try to
   minimise it. the choice of a id168 (here, "cross-id178") is
   explained later. what you see here is that the loss goes down on both
   the training and the test data as the training progresses: that is
   good. it means the neural network is learning. the x-axis represents
   iterations through the learning loop.

   [5b261e5f3aa19e64.png] [4b5d68b11f26c664.png]

   the accuracy is simply the % of correctly recognised digits. this is
   computed both on the training and the test set. you will see it go up
   if the training goes well.

   [5cb32946dd388353.png] [f3da4b4f30228cb1.png] [11eeec8462f74739.png]

   the final two graphs represent the spread of all the values taken by
   the internal variables, i.e. weights and biases as the training
   progresses. here you see for example that biases started at 0 initially
   and ended up taking values spread roughly evenly between -1.5 and 1.5.
   these graphs can be useful if the system does not converge well. if you
   see weights and biases spreading into the 100s or 1000s, you might have
   a problem.

   the bands in the graphs are percentiles. there are 7 bands so each band
   is where 100/7=14% of all the values are.

   keyboard shortcuts for the visualisation gui:
   1 ......... display 1st graph only
   2 ......... display 2nd graph only
   3 ......... display 3rd graph only
   4 ......... display 4th graph only
   5 ......... display 5th graph only
   6 ......... display 6th graph only
   7 ......... display graphs 1 and 2
   8 ......... display graphs 4 and 5
   9 ......... display graphs 3 and 6
   esc or 0 .. back to displaying all graphs
   space ..... pause/resume
   o ......... box zoom mode (then use mouse)
   h ......... reset all zooms
   ctrl-s .... save current image

   what are "weights" and "biases" ? how is the "cross-id178" computed ?
   how exactly does the training algorithm work ? jump to the next section
   to find out.

   [bdd8f1f362889583.png]

   handwritten digits in the mnist dataset are 28x28 pixel greyscale
   images. the simplest approach for classifying them is to use the
   28x28=784 pixels as inputs for a 1-layer neural network.

   [d5222c6e3d15770a.png]

   each "neuron" in a neural network does a weighted sum of all of its
   inputs, adds a constant called the "bias" and then feeds the result
   through some non-linear activation function.

   here we design a 1-layer neural network with 10 output neurons since we
   want to classify digits into 10 classes (0 to 9).

   for a classification problem, an activation function that works well is
   softmax. applying softmax on a vector is done by taking the exponential
   of each element and then normalising the vector (using any norm, for
   example the ordinary euclidean length of the vector).

   [604a9797da2a48d7.png]

   why is "softmax" called softmax ? the exponential is a steeply
   increasing function. it will increase differences between the elements
   of the vector. it also quickly produces large values. then, as you
   normalise the vector, the largest element, which dominates the norm,
   will be normalised to a value close to 1 while all the other elements
   will end up divided by a large value and normalised to something close
   to 0. the resulting vector clearly shows which was its largest element,
   the "max", but retains the original relative order of its values, hence
   the "soft".

   we will now summarise the behaviour of this single layer of neurons
   into a simple formula using a matrix multiply. let us do so directly
   for a "mini-batch" of 100 images as the input, producing 100
   predictions (10-element vectors) as the output.

   [21dabcf6d44e4d6f.png]

   using the first column of weights in the weights matrix w, we compute
   the weighted sum of all the pixels of the first image. this sum
   corresponds to the first neuron. using the second column of weights, we
   do the same for the second neuron and so on until the 10th neuron. we
   can then repeat the operation for the remaining 99 images. if we call x
   the matrix containing our 100 images, all the weighted sums for our 10
   neurons, computed on 100 images are simply x.w (matrix multiply).

   each neuron must now add its bias (a constant). since we have 10
   neurons, we have 10 bias constants. we will call this vector of 10
   values b. it must be added to each line of the previously computed
   matrix. using a bit of magic called "broadcasting" we will write this
   with a simple plus sign.

   "broadcasting" is a standard trick used in python and numpy, its
   scientific computation library. it extends how normal operations work
   on matrices with incompatible dimensions. "broadcasting add" means "if
   you are adding two matrices but you cannot because their dimensions are
   not compatible, try to replicate the small one as much as needed to
   make it work."

   we finally apply the softmax activation function and obtain the formula
   describing a 1-layer neural network, applied to 100 images:

   [206327168bc85294.png]

   by the way, what is a "tensor"?
   a "tensor" is like a matrix but with an arbitrary number of dimensions.
   a 1-dimensional tensor is a vector. a 2-dimensions tensor is a matrix.
   and then you can have tensors with 3, 4, 5 or more dimensions.

   now that our neural network produces predictions from input images, we
   need to measure how good they are, i.e. the distance between what the
   network tells us and what we know to be the truth. remember that we
   have true labels for all the images in this dataset.

   any distance would work, the ordinary euclidian distance is fine but
   for classification problems one distance, called the "cross-id178" is
   more efficient.

   [1d8fc59e6a674f1c.png]

   "one-hot" encoding means that you represent the label "6" by using a
   vector of 10 values, all zeros but the 6th value which is 1. it is
   handy here because the format is very similar to how our neural network
   outputs ts predictions, also as a vector of 10 values.

   "training" the neural network actually means using training images and
   labels to adjust weights and biases so as to minimise the cross-id178
   id168. here is how it works.

   the cross-id178 is a function of weights, biases, pixels of the
   training image and its known label.

   if we compute the partial derivatives of the cross-id178 relatively
   to all the weights and all the biases we obtain a "gradient", computed
   for a given image, label and present value of weights and biases.
   remember that we have 7850 weights and biases so computing the gradient
   sounds like a lot of work. fortunately, tensorflow will do it for us.

   the mathematical property of a gradient is that it points "up". since
   we want to go where the cross-id178 is low, we go in the opposite
   direction. we update weights and biases by a fraction of the gradient
   and do the same thing again using the next batch of training images.
   hopefully, this gets us to the bottom of the pit where the
   cross-id178 is minimal.

   [34e9e76c7715b719.png]

   in this picture, cross-id178 is represented as a function of 2
   weights. in reality, there are many more. the id119
   algorithm follows the path of steepest descent into a local minimum.
   the training images are changed at each iteration too so that we
   converge towards a local minimum that works for all images.

   "learning rate": you cannot update your weights and biases by the whole
   length of the gradient at each iteration. it would be like trying to
   get to the bottom of a valley while wearing seven-league boots. you
   would be jumping from one side of the valley to the other. to get to
   the bottom, you need to do smaller steps, i.e. use only a fraction of
   the gradient, typically in the 1/1000th region. we call this fraction
   the "learning rate".

   to sum it up, here is how the training loop looks like:

   training digits and labels => id168 => gradient (partial
   derivatives) => steepest descent => update weights and biases => repeat
   with next mini-batch of training images and labels

   why work with "mini-batches" of 100 images and labels ?

   you can definitely compute your gradient on just one example image and
   update the weights and biases immediately (it's called "stochastic
   id119" in scientific literature). doing so on 100 examples
   gives a gradient that better represents the constraints imposed by
   different example images and is therefore likely to converge towards
   the solution faster. the size of the mini-batch is an adjustable
   parameter though. there is another, more technical reason: working with
   batches also means working with larger matrices and these are usually
   easier to optimise on gpus.

  frequently asked questions

     * [4]why is the cross-id178 the right distance to use for
       classification problems ?

   the code for the 1-layer neural network is already written. please open
   the mnist_1.0_softmax.py file and follow along with the explanations.

   your task in this section is to understand this starting code so that
   you can improve it later.

   you should see there are only minor differences between the
   explanations and the starter code in the file. they correspond to
   functions used for the visualisation and are marked as such in
   comments. you can ignore them.

  [5]mnist_1.0_softmax.py

import tensorflow as tf

x = tf.placeholder(tf.float32, [none, 28, 28, 1])
w = tf.variable(tf.zeros([784, 10]))
b = tf.variable(tf.zeros([10]))

init = tf.initialize_all_variables()

   first we define tensorflow variables and placeholders. variables are
   all the parameters that you want the training algorithm to determine
   for you. in our case, our weights and biases.

   placeholders are parameters that will be filled with actual data during
   training, typically training images. the shape of the tensor holding
   the training images is [none, 28, 28, 1] which stands for:
     * 28, 28, 1: our images are 28x28 pixels x 1 value per pixel
       (grayscale). the last number would be 3 for color images and is not
       really necessary here.
     * none: this dimension will be the number of images in the
       mini-batch. it will be known at training time.

  [6]mnist_1.0_softmax.py

# model
y = tf.nn.softmax(tf.matmul(tf.reshape(x, [-1, 784]), w) + b)
# placeholder for correct labels
y_ = tf.placeholder(tf.float32, [none, 10])

# id168
cross_id178 = -tf.reduce_sum(y_ * tf.log(y))

# % of correct answers found in batch
is_correct = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

   the first line is the model for our 1-layer neural network. the formula
   is the one we established in the previous theory section. the
   tf.reshape command transforms our 28x28 images into single vectors of
   784 pixels. the "-1" in the reshape command means "computer, figure it
   out, there is only one possibility". in practice it will be the number
   of images in a mini-batch.

   we then need an additional placeholder for the training labels that
   will be provided alongside training images.

   now, we have model predictions and correct labels so we can compute the
   cross-id178. tf.reduce_sum sums all the elements of a vector.

   the last two lines compute the percentage of correctly recognised
   digits. they are left as an exercise for the reader to understand,
   using the [7]tensorflow api reference. you can also skip them.

  [8]mnist_1.0_softmax.py

optimizer = tf.train.gradientdescentoptimizer(0.003)
train_step = optimizer.minimize(cross_id178)

   this where the tensorflow magic happens. you select an optimiser (there
   are many available) and ask it to minimise the cross-id178 loss. in
   this step, tensorflow computes the partial derivatives of the loss
   function relatively to all the weights and all the biases (the
   gradient). this is a formal derivation, not a numerical one which would
   be far too time-consuming.

   the gradient is then used to update the weights and biases. 0.003 is
   the learning rate.

   finally, it is time to run the training loop. all the tensorflow
   instructions up to this point have been preparing a computation graph
   in memory but nothing has been computed yet.

   tensorflow's "deferred execution" model: tensorflow was build for
   distributed computing. it has to know what you are going to compute,
   your execution graph, before it starts actually sending compute tasks
   to various computers. that is why it has a deferred execution model
   where you first use tensorflow functions to create a computation graph
   in memory, then start an execution session and perform actual
   computations using session.run. at this point the graph cannot be
   changed anymore.

   thanks to this model, tensorflow can take over a lot of the logistics
   of distributed computing. for example, if your instruct it to run one
   part of the computation on computer 1 and another part on computer 2,
   it can make the necessary data transfers happen automatically.

   the computation requires actual data to be fed into the placeholders
   you have defined in your tensorflow code. this is supplied in the form
   of a python dictionary where the keys are the names of the
   placeholders.

  [9]mnist_1.0_softmax.py

sess = tf.session()
sess.run(init)

for i in range(1000):
    # load batch of images and correct answers
    batch_x, batch_y = mnist.train.next_batch(100)
    train_data={x: batch_x, y_: batch_y}

    # train
    sess.run(train_step, feed_dict=train_data)

   the train_step that is executed here was obtained when we asked
   tensorflow to minimise out cross-id178. that is the step that
   computes the gradient and updates weights and biases.

   finally, we also need to compute a couple of values for display so that
   we can follow how our model is performing.

   the accuracy and cross id178 are computed on training data using this
   code in the training loop (every 10 iterations for example):
# success ?
a,c = sess.run([accuracy, cross_id178], feed_dict=train_data)

   the same can be computed on test data by supplying test instead of
   training data in the feed dictionary (do this every 100 iterations for
   example. there are 10,000 test digits so this takes some cpu time):
# success on test data ?
test_data={x: mnist.test.images, y_: mnist.test.labels}
a,c = sess.run([accuracy, cross_id178], feed=test_data)

   tensorflow and numpy are friends: when preparing the computation graph,
   you only manipulate tensorflow tensors and commands such as tf.matmul,
   tf.reshape and so on.

   however, as soon as you execute a session.run command, the values it
   returns are numpy tensors, i.e. numpy.ndarray objects that can be
   consumed by numpy and all the scientific comptation libraries based on
   it. that is how the real-time visualisation was built for this lab,
   using matplotlib, the standard python plotting library, which is based
   on numpy.

   this simple model already recognises 92% of the digits. not bad, but
   you will now improve this significantly.

   [e102f513bec53e08.png]

   [a2832d28e7a4d272.png]

   to improve the recognition accuracy we will add more layers to the
   neural network. the neurons in the second layer, instead of computing
   weighted sums of pixels will compute weighted sums of neuron outputs
   from the previous layer. here is for example a 5-layer fully connected
   neural network:

   [77bc41f211c9fb29.png]

   we keep softmax as the activation function on the last layer because
   that is what works best for classification. on intermediate layers
   however we will use the the most classical activation function: the
   sigmoid:

   [e5d46c389470df62.png]

   your task in this section is to add one or two intermediate layers to
   your model to increase its performance.

   the solution can be found in file [10]mnist_2.0_five_layers_sigmoid.py.
   use it if you are stuck only!

   to add a layer, you need an additional weights matrix and an additional
   bias vector for the intermediate layer:
w1 = tf.variable(tf.truncated_normal([28*28, 200] ,stddev=0.1))
b1 = tf.variable(tf.zeros([200]))

w2 = tf.variable(tf.truncated_normal([200, 10], stddev=0.1))
b2 = tf.variable(tf.zeros([10]))

   the shape of the weights matrix for a layer is [n, m] where n is the
   number of inputs and m of outputs for the layer. in the code above, we
   use 200 neurons in the intermediate layer and still 10 neurons in the
   last layer.

   tip: as you go deep, it becomes important to initialise weights with
   random values. the optimiser can get stuck in its initial position if
   you do not. tf.truncated_normal is a tensorflow function that produces
   random values following the normal (gaussian) distribution between
   -2*stddev and +2*stddev.

   and now change your 1-layer model into a 2-layer model:
xx = tf.reshape(x, [-1, 28*28])

y1 = tf.nn.sigmoid(tf.matmul(xx, w1) + b1)
y  = tf.nn.softmax(tf.matmul(y1, w2) + b2)

   that's it. you should now be able to push your network above 97%
   accuracy with 2 intermediate layer with for example 200 and 100
   neurons.

   [dbbf4c8edae90438.png]

   [35a0e96b3da8a465.png]

   as layers were added, neural networks tended to converge with more
   difficulties. but we know today how to make them behave. here are a
   couple of 1-line updates that will help if you see an accuracy curve
   like this:

   [56ac913e3330c484.png]

relu activation function

   the sigmoid activation function is actually quite problematic in deep
   networks. it squashes all values between 0 and 1 and when you do so
   repeatedly, neuron outputs and their gradients can vanish entirely. it
   was mentioned for historical reasons but modern networks use the relu
   (rectified linear unit) which looks like this:

   [60cac06459b3cc08.png]

   update 1/4: replace all your sigmoids with relus now and you will get
   faster initial convergence and avoid problems later as we add layers.
   simply swap tf.nn.sigmoid with tf.nn.relu in your code.

a better optimizer

   in very high dimensional spaces like here - we have in the order of 10k
   weights and biases - "saddle points" are frequent. these are points
   that are not local minima but where the gradient is nevertheless zero
   and the id119 optimizer stays stuck there. tensorflow has a
   full array of available optimizers, including some that work with an
   amount of inertia and will safely sail past saddle points.

   update 2/4: replace your tf.train.gradientdescentoptimiser with a
   tf.train.adamoptimizer now.

random initialisations

   accuracy still stuck at 0.1 ? have you initialised your weights with
   random values ? for biases, when working with relus, the best practice
   is to initialise them to small positive values so that neurons operate
   in the non-zero range of the relu initially.
w = tf.variable(tf.truncated_normal([k, l] ,stddev=0.1))
b = tf.variable(tf.ones([l])/10)

   update 3/4: check now that all your weights and biases are initialised
   appropriately. 0.1 as pictured above will do for biases.

nan ???

   [796280524370a9b5.png]

   if you see your accuracy curve crashing and the console outputting nan
   for the cross-id178, don't panic, you are attempting to compute a
   log(0), which is indeed not a number (nan). remember that the
   cross-id178 involves a log, computed on the output of the softmax
   layer. since softmax is essentially an exponential, which is never
   zero, we should be fine but with 32 bit precision floating-point
   operations, exp(-100) is already a genuine zero.

   fortunately, tensorflow has a handy function that computes the softmax
   and the cross-id178 in a single step, implemented in a numerically
   stable way. to use it, you will need to isolate the raw weighted sum
   plus bias on your last layer, before softmax is applied ("logits" in
   neural network jargon).

   if the last line of your model was:
y = tf.nn.softmax(tf.matmul(y4, w5) + b5)

   you need to replace it with:
ylogits = tf.matmul(y4, w5) + b5
y = tf.nn.softmax(ylogits)

   and now you can compute your cross-id178 in a safe way:
cross_id178 = tf.nn.softmax_cross_id178_with_logits(logits=ylogits, labels=y
_)

   also add this line to bring the test and training cross-id178 to the
   same scale for display:
cross_id178 = tf.reduce_mean(cross_id178)*100

   update 4/4: please add tf.nn.softmax_cross_id178_with_logits to your
   code. you can also skip this step and come back to it when you actually
   see nans in your output.

   you are now ready to go deep.

   [80456ff8ba0a3dac.png]

   with two, three or four intermediate layers, you can now get close to
   98% accuracy, if you push the iterations to 5000 or beyond. but you
   will see that results are not very consistent.

   [19b544d307a09804.png]

   these curves are really noisy and look at the test accuracy: it's
   jumping up and down by a whole percent. this means that even with a
   learning rate of 0.003, we are going too fast. but we cannot just
   divide the learning rate by ten or the training would take forever. the
   good solution is to start fast and decay the learning rate
   exponentially to 0.0001 for example.

   the impact of this little change is spectacular. you see that most of
   the noise is gone and the test accuracy is now above 98% in a sustained
   way.

   [36c4ff32da84a637.png]

   look also at the training accuracy curve. it is now reaching 100%
   across several epochs (1 epoch = 500 iterations = trained on all
   training images once). for the first time, we are able to learn to
   recognise the training images perfectly.

   please add learning rate decay to your code. in your model, use the
   following formula instead of the fixed learning rate we used previously
   in adamoptimizer:

   lr = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1/math.e)

   it implements al learning rate that decays exponentially from 0.003 to
   0.0001.

   you will need to pass the "step" parameter to your model at every
   iteration through the feed_dict parameter. you will need a new
   placeholder for that:

   step = tf.placeholder(tf.int32)

   the solution can be found in file
   [11]mnist_2.1_five_layers_relu_lrdecay.py. use it if you are stuck.

   [9a204ebcd25ed434.png]

   [ff192183d1cf2023.png]

   you will have noticed that cross-id178 curves for test and training
   data start disconnecting after a couple thousand iterations. the
   learning algorithm works on training data only and optimises the
   training cross-id178 accordingly. it never sees test data so it is
   not surprising that after a while its work no longer has an effect on
   the test cross-id178 which stops dropping and sometimes even bounces
   back up.

   [d1a460e8334d6b1c.png]

   this does not immediately affect the real-world recognition
   capabilities of your model but it will prevent you from running many
   iterations and is generally a sign that the training is no longer
   having a positive effect. this disconnect is usually labeled
   "overfitting" and when you see it, you can try to apply a
   regularisation technique called "dropout".

   [5ee25552f4c216c.png]

   in dropout, at each training iteration, you drop random neurons from
   the network. you choose a id203 pkeep for a neuron to be kept,
   usually between 50% and 75%, and then at each iteration of the training
   loop, you randomly remove neurons with all their weights and biases.
   different neurons will be dropped at each iteration (and you also need
   to boost the output of the remaining neurons in proportion to make sure
   activations on the next layer do not shift). when testing the
   performance of your network of course you put all the neurons back
   (pkeep=1).

   tensorflow offers a dropout function to be used on the outputs of a
   layer of neurons. it randomly zeroes-out some of the outputs and boosts
   the remaining ones by 1/pkeep. here is how you use it in a 2-layer
   network:
# feed in 1 when testing, 0.75 when training
pkeep = tf.placeholder(tf.float32)

y1 = tf.nn.relu(tf.matmul(x, w1) + b1)
y1d = tf.nn.dropout(y1, pkeep)

y = tf.nn.softmax(tf.matmul(y1d, w2) + b2)

   you can add dropout after each intermediate layer in the network now.
   this is an optional step in the lab, if you are pressed for time keep
   reading.

   the solution can be found in file
   [12]mnist_2.2_five_layers_relu_lrdecay_dropout.py. use it if you are
   stuck.

   [600deebea5fff672.png]

   you should see that the test loss is largely brought back under
   control, noise reappears (unsurprisingly given how dropout works) but
   in this case at least, the test accuracy remains unchanged which is a
   little disappointing. there must be another reason for the
   "overfitting".

   before we continue, a recap of all the tools we have tried so far:

   [b68d008cb24b363d.png]

   whatever we do, we do not seem to be able to break the 98% barrier in a
   significant way and our loss curves still exhibit the "overfitting"
   disconnect. what is really "overfitting" ? overfitting happens when a
   neural network learns "badly", in a way that works for the training
   examples but not so well on real-world data. there are regularisation
   techniques like dropout that can force it to learn in a better way but
   overfitting also has deeper roots.

   [3291d60208e7ce7f.png]

   basic overfitting happens when a neural network has too many degrees of
   freedom for the problem at hand. imagine we have so many neurons that
   the network can store all of our training images in them and then
   recognise them by pattern matching. it would fail on real-world data
   completely. a neural network must be somewhat constrained so that it is
   forced to generalise what it learns during training.

   if you have very little training data, even a small network can learn
   it by heart. generally speaking, you always need lots of data to train
   neural networks.

   finally, if you have done everything well, experimented with different
   sizes of network to make sure its degrees of freedom are constrained,
   applied dropout, and trained on lots of data you might still be stuck
   at a performance level that nothing seems to be able to improve. this
   means that your neural network, in its present shape, is not capable of
   extracting more information from your data, as in our case here.

   remember how we are using our images, all pixels flattened into a
   single vector ? that was a really bad idea. handwritten digits are made
   of shapes and we discarded the shape information when we flattened the
   pixels. however, there is a type of neural network that can take
   advantage of shape information: convolutional networks. let us try
   them.

   [53c160301db12a6e.png]

   in a layer of a convolutional network, one "neuron" does a weighted sum
   of the pixels just above it, across a small region of the image only.
   it then acts normally by adding a bias and feeding the result through
   its activation function. the big difference is that each neuron reuses
   the same weights whereas in the fully-connected networks seen
   previously, each neuron had its own set of weights.

   in the animation above, you can see that by sliding the patch of
   weights across the image in both directions (a convolution) you obtain
   as many output values as there were pixels in the image (some padding
   is necessary at the edges though).

   to generate one plane of output values using a patch size of 4x4 and a
   color image as the input, as in the animation, we need 4x4x3=48
   weights. that is not enough. to add more degrees of freedom, we repeat
   the same thing with a different set of weights.

   [40fd4b6ad8dfb6d2.png]

   the two (or more) sets of weights can be rewritten as one by adding a
   dimension to the tensor and this gives us the generic shape of the
   weights tensor for a convolutional layer. since the number of input and
   output channels are parameters, we can start stacking and chaining
   convolutional layers.

   [6eff0308ba98370e.png]

   one last issue remains. we still need to boil the information down. in
   the last layer, we still want only 10 neurons for our 10 classes of
   digits. traditionally, this was done by a "max-pooling" layer. even if
   there are simpler ways today, "max-pooling" helps understand
   intuitively how convolutional networks operate: if you assume that
   during training, our little patches of weights evolve into filters that
   recognise basic shapes (horizontal and vertical lines, curves, ...)
   then one way of boiling useful information down is to keep through the
   layers the outputs where a shape was recognised with the maximum
   intensity. in practice, in a max-pool layer neuron outputs are
   processed in groups of 2x2 and only the one max one retained.

   there is a simpler way though: if you slide the patches across the
   image with a stride of 2 pixels instead of 1, you also obtain fewer
   output values. this approach has proven just as effective and today's
   convolutional networks use convolutional layers only.

   [f4575a9eb3ea8c9e.png]

   let us build a convolutional network for handwritten digit recognition.
   we will use three convolutional layers at the top, our traditional
   softmax readout layer at the bottom and connect them with one
   fully-connected layer:

   [b564b381aac8bc42.png]

   notice that the second and third convolutional layers have a stride of
   two which explains why they bring the number of output values down from
   28x28 to 14x14 and then 7x7. the sizing of the layers is done so that
   the number of neurons goes down roughly by a factor of two at each
   layer: 28x28x4   3000     14x14x8   1500     7x7x12   500     200. jump to the next
   section for the implementation.

   [b564b381aac8bc42.png]

   to switch our code to a convolutional model, we need to define
   appropriate weights tensors for the convolutional layers and then add
   the convolutional layers to the model.

   we have seen that a convolutional layer requires a weights tensor of
   the following shape. here is the tensorflow syntax for their
   initialisation:

   [b0de36a8c59a3526.png]
w = tf.variable(tf.truncated_normal([4, 4, 3, 2], stddev=0.1))
b = tf.variable(tf.ones([2])/10) # 2 is the number of output channels

   convolutional layers can be implemented in tensorflow using the
   tf.nn.conv2d function which performs the scanning of the input image in
   both directions using the supplied weights. this is only the weighted
   sum part of the neuron. you still need to add a bias and feed the
   result through an activation function.
stride = 1  # output is still 28x28
ycnv = tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding='same')
y = tf.nn.relu(ycnv + b)

   do not pay too much attention to the complex syntax for the stride.
   look up the documentation for full details. the padding strategy that
   works here is to copy pixels from the sides of the image. all digits
   are on a uniform background so this just extends the background and
   should not add any unwanted shapes.

   your turn to play. modify your model to turn it into a convolutional
   model. you can use the values from the drawing above to size it. you
   can keep your learning rate decay as it was but please remove dropout
   at this point.

   the solution can be found in file [13]mnist_3.0_convolutional.py. use
   it if you are stuck.

   your model should break the 98% barrier comfortably and end up just a
   hair under 99%. we cannot stop so close! look at the test cross-id178
   curve. does a solution spring to your mind ?

   [881c4f6265de877b.png]

   a good approach to sizing your neural networks is to implement a
   network that is a little too constrained, then give it a bit more
   degrees of freedom and add dropout to make sure it is not overfitting.
   this ends up with a fairly optimal network for your problem.

   here for example, we used only 4 patches in the first convolutional
   layer. if you accept that those patches of weights evolve during
   training into shape recognisers, you can intuitively see that this
   might not be enough for our problem. handwritten digits are mode from
   more than 4 elemental shapes.

   so let us bump up the patch sizes a little, increase the number of
   patches in our convolutional layers from 4, 8, 12 to 6, 12, 24 and then
   add dropout on the fully-connected layer. why not on the convolutional
   layers? their neurons reuse the same weights, so dropout, which
   effectively works by freezing some weights during one training
   iteration, would not work on them.

   [ec9a4fb16ee7b309.png]

   go for it and break the 99% limit. increase the patch sizes and channel
   numbers as on the picture above and add dropout on the convolutional
   layer.

   the solution can be found in file
   [14]mnist_3.1_convolutional_bigger_dropout.py. use it if you are stuck.

   [59472e85398457c7.png]

   the model pictured above misses only 72 out of the 10,000 test digits.
   the world record, which you can find on the mnist website is around
   99.7%. we are only 0.4 percentage points away from it with our model
   built with 100 lines of python / tensorflow.

   to finish, here is the difference dropout makes to our bigger
   convolutional network. giving the neural network the additional degrees
   of freedom it needed bumped the final accuracy from 98.9% to 99.1%.
   adding dropout not only tamed the test loss but also allowed us to sail
   safely above 99% and even reach 99.3%

   [e4f164be456fcf92.png]

   [3e5f236d495de1e.png]

   you will find a cloud-ready version of the code in the [15]id113ngine
   folder on github, along with instructions for running it on [16]google
   cloud ml engine. before you can run this part, you will have to create
   a google cloud account and enable billing. the resources necessary to
   complete the lab should be less that a couple of dollars (assuming 1h
   of training time on one gpu). to prepare your account:
    1. create a google cloud platform project
       ([17]http://cloud.google.com/console)
    2. enable billing
    3. install the gcp command line tools ([18]gcp sdk here)
    4. create a google cloud storage bucket (put in in region
       us-central1). it will be used to stage the training code and store
       your trained model.
    5. enable the necessary apis and request the necessary quotas (run the
       training command once and you should get error messages telling you
       what to enable)

   you have built your first neural network and trained it all the way to
   99% accuracy. the techniques learned along the way are not specific to
   the mnist dataset, actually they are widely used when working with
   neural networks. as a parting gift, here is the "cliff's notes" card
   for the lab, in cartoon version. you can use it to remember what you
   have learned:

   [c33bb148d5014914.png]

  next steps

     * after fully-connected and convolutional networks, you should have a
       look at [19]recurrent neural networks.
     * in this tutorial, you have learned how to build a tensorflow model
       at the matrix level. tensorflow has higher-level apis too called
       [20]tf.layers.
     * to run your training or id136 in the cloud on a distributed
       infrastructure, google provides the [21]cloud ml engine service.
     * finally, we love feedback. please tell us if you see something
       amiss in this lab or if you think it should be improved. we handle
       feedback through github issues [[22]feedback link].

   [4c2925956f9292.png]

   [1dd39cb813f337e2.png]
   the author: martin g  rner
   twitter: [23]@martin_gorner

   [2863687467111708.png]
   [24]www.tensorflow.org

   all cartoon images in this lab copyright: [25]alexpokusay / 123rf stock
   photos

references

   1. http://yann.lecun.com/exdb/mnist/
   2. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/install.txt
   3. https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html
   4. https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-id178-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
   5. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_1.0_softmax.py
   6. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_1.0_softmax.py
   7. https://www.tensorflow.org/programmers_guide/
   8. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_1.0_softmax.py
   9. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_1.0_softmax.py
  10. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_2.0_five_layers_sigmoid.py
  11. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_2.1_five_layers_relu_lrdecay.py
  12. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_2.2_five_layers_relu_lrdecay_dropout.py
  13. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_3.0_convolutional.py
  14. https://github.com/googlecloudplatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_3.1_convolutional_bigger_dropout.py
  15. https://github.com/googlecloudplatform/tensorflow-without-a-phd/tree/master/tensorflow-mnist-tutorial/id113ngine
  16. https://cloud.google.com/ml-engine/
  17. http://cloud.google.com/console
  18. https://cloud.google.com/sdk/downloads#interactive
  19. https://youtu.be/ftuwdxuffi8
  20. https://www.tensorflow.org/api_docs/python/tf/layers
  21. https://cloud.google.com/ml-engine/
  22. https://github.com/googlecodelabs/feedback/issues/new?title=[cloud-tensorflow-mnist]:&labels[]=content-platform&labels[]=cloud
  23. https://twitter.com/martin_gorner
  24. http://www.tensorflow.org/
  25. http://fr.123rf.com/profile_alexpokusay
