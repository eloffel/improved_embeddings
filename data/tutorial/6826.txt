   #[1]pages rss feed

   preloaded picture


   [2]icon naraleian caterpillars


   [3]icon the scientific method is a virus


   [4]icon local minima, saddle points, and plateaus


   [5]icon robust solving of optical motion capture data by denoising


   [6]icon simple concurrency in python


   [7]icon the software thief


   [8]icon ascii : a love letter


   [9]icon my neural network isn't working! what should i do?


   [10]icon phase-functioned neural networks for character control


   [11]icon 17 line markov chain


   [12]icon 14 character random number generator


   [13]icon simple two joint ik


   [14]icon generating icons with pixel sorting


   [15]icon neural network ambient occlusion


   [16]icon three short stories about the east coast main line


   [17]icon the new alphabet


   [18]icon "the color munifni exists"


   [19]icon a deep learning framework for character motion synthesis and
   editing


   [20]icon the halting problem and the moral arbitrator


   [21]icon the witness


   [22]icon four seasons crisp omelette


   [23]icon at the bottom of the elevator


   [24]icon tracing functions in python


   [25]icon still things and moving things


   [26]icon water.cpp


   [27]icon making poetry in piet


   [28]icon learning motion manifolds with convolutional autoencoders


   [29]icon learning an inverse rig mapping for character animation


   [30]icon infinity doesn't exist


   [31]icon polyconf


   [32]icon raleigh


   [33]icon the skagerrak


   [34]icon printing a stack trace with mingw


   [35]icon the border pines


   [36]icon you could have invented parser combinators


   [37]icon ready for the fight


   [38]icon earthbound


   [39]icon turing drawings


   [40]icon lost child announcement


   [41]icon shelter


   [42]icon data science, how hard can it be?


   [43]icon denki furo


   [44]icon in defence of the unitype


   [45]icon maya velocity node


   [46]icon sandy denny


   [47]icon what type of machine is the c preprocessor?


   [48]icon which ai is more human?


   [49]icon gone home


   [50]icon thoughts on japan


   [51]icon can computers think?


   [52]icon counting sheep & infinity


   [53]icon how nature builds computers


   [54]icon painkillers


   [55]icon correct box sphere intersection


   [56]icon avoiding shader conditionals


   [57]icon writing portable opengl


   [58]icon the only cable car in ireland


   [59]icon is the c preprocessor turing complete?


   [60]icon the aesthetics of code


   [61]icon issues with sdl on ios and android


   [62]icon how i learned to stop worrying and love statistics


   [63]icon pymark


   [64]icon autoc tools


   [65]icon scripting xnormal with python


   [66]icon six myths about ray tracing


   [67]icon the web giants will fall


   [68]icon pyautoc


   [69]icon the pirate song


   [70]icon dear esther


   [71]icon unsharp anti aliasing


   [72]icon the first boy


   [73]icon parallel programming isn't hard, optimisation is.


   [74]icon skyrim


   [75]icon recognizing a language is solving a problem


   [76]icon could an animal learn to program?


   [77]icon rage


   [78]icon pure depth ssao


   [79]icon synchronized in python


   [80]icon 3d printing


   [81]icon real time graphics is virtual reality


   [82]icon painting style renderer


   [83]icon a very hard problem


   [84]icon indie development vs modding


   [85]icon corange


   [86]icon 3ds max ply exporter


   [87]icon a case for the technical artist


   [88]icon enums


   [89]icon scorpions have won evolution


   [90]icon dirt and ashes


   [91]icon lazy python


   [92]icon subdivision modelling


   [93]icon the owl


   [94]icon mouse traps


   [95]icon updated art reel


   [96]icon tech reel


   [97]icon graphics aren't the enemy


   [98]icon on being a games artist


   [99]icon the bluebird


   [100]icon everything2


   [101]icon duck engine


   [102]icon boarding preview


   [103]icon sailing preview


   [104]icon exodus village flyover


   [105]icon art reel


   [106]icon lol i drew this dragon


   [107]icon one cat just leads to another

   [108]    projects     [109]    publications     [110]    about     [111]    all    

my neural network isn't working! what should i do?

created on aug. 19, 2017, 5:56 p.m.

   so you're developing the next great breakthrough in deep learning but
   you've hit an unfortunate setback: your neural network isn't working
   and you have no idea what to do. you go to your boss/supervisor but
   they don't know either - they are just as new to all of this as you -
   so what now?

   well luckily for you i'm here with a list of all the things you've
   probably done wrong and compiled from my own experiences implementing
   neural networks and supervising other students with their projects:
    1. [112]you forgot to normalize your data
    2. [113]you forgot to check your results
    3. [114]you forgot to preprocess your data
    4. [115]you forgot to use any id173
    5. [116]you used a too large batch size
    6. [117]you used an incorrect learning rate
    7. [118]you used the wrong activation function on the final layer
    8. [119]your network contains bad gradients
    9. [120]you initialized your network weights incorrectly
   10. [121]you used a network that was too deep
   11. [122]you used the wrong number of hidden units
     __________________________________________________________________

you forgot to normalize your data

what?

   when using neural networks it is essential to think exactly how you are
   going to normalize your data. this is a non-negotiable step - there is
   very little chance of your network working at all without doing this
   correctly and with some care. since this step is so essential and so
   well known in the deep learning community it is very rarely mentioned
   in papers and so almost always trips up beginners.

how?

   in general id172 means this - subtract the mean from your data
   and divide your data by it's standard deviation. usually this is done
   individually for each input and output feature but you may often want
   to do it for groups of features or to treat the id172 of some
   features with special care.

why?

   the primary reason we need to normalize our data is that most parts of
   a neural network pipeline assume that both the input and output data
   are distributed with a standard deviation of around one and a mean of
   roughly zero. these assumptions appear everywhere in deep learning
   literature, from weight initialization, to id180, to the
   optimization algorithms which train the network.

and?

   an untrained neural network will typically output values roughly in the
   range -1 to 1. if you are expecting it to output values in some other
   range, (for example rgb images which are stored as bytes are in the
   range 0 to 255) you are going to have some problems. when starting
   training the network will be hugely unstable as it will be producing
   values of -1 or 1 when values like 255 are expected - an error which is
   considered huge by most optimization algorithms used to train neural
   networks. this will produce huge gradients and likely your training
   error will explode. if somehow your training does not explode then the
   first few stages of the training will still be a waste as the first
   thing the network will learn is to scale and shift the output values
   into roughly the desired range. if you normalize your data (in this
   case you could simply divide by 128 and subtract 1) then none of this
   will be an issue.

   in general, the scale of features in the neural network will also
   govern their importance. if you have a feature in the output with a
   large scale then it will generate a larger error compared to other
   features. similarly, large scale features in the input will dominate
   the network and cause larger changes downstream. for this reason it
   isn't always enough to use the automatic id172 of many neural
   network libraries which blindly subtract the mean and divide by the
   standard deviation on a per-feature basis. you may have an input
   feature which typically ranges between 0.0 and 0.001 - is the range of
   this feature so small because it is an unimportant feature (in which
   case perhaps you don't want to re-scale it), or because it has some
   small unit in comparison to other features (in which case you do)?
   similarly, be careful with features that have such a small range that
   their standard deviation becomes close to, or exactly, zero - these
   will produce instabilities of nans if you normalize them. it is
   important to think carefully about these issues - think about what each
   of your features really represent and consider id172 as the
   process of making the "units" of all the input features equal. this is
   one of the few aspects of deep learning where i believe a human is
   really required in the loop.
     __________________________________________________________________

you forgot to check your results

what?

   you've trained your network for a few epochs and you can see the error
   going down - success! does this mean you've done it? phd awarded?
   unfortunately not - it is almost certain there is still something wrong
   with your code. it could be a bug in the data pre-processing, the
   training code, or even the id136. just because the error goes down
   doesn't mean your network is learning anything useful.

how?

   checking your data looks correct at each stage of the pipeline is
   incredibly important. usually this means finding some way to visualize
   the results. if you have image data then it is easy - animation data
   can also be visualized without too much trouble. if you have something
   more exotic you must find a way to sanity check it to make sure it
   looks correct at each stage of your pre-processing, training, and
   id136 pipeline and compare it to ground truth data.

why?

   unlike traditional programming, machine learning systems will fail
   silently in almost all cases. with traditional programming we are used
   to the computer throwing an error when things go wrong and using this
   as the signal to go back and check for bugs. unfortunately this process
   doesn't work with machine learning applications and so instead we
   should be incredibly careful about checking our processes at every
   stage with a human eye so that we know when a bug has been produced and
   when we might need to go back and check our code more thoroughly.

and?

   there are many ways to check if your network is working. part of this
   is finding out exactly what the reported training error really means.
   visualize the result of your network applied to data in the training
   set - how does the result of your network compare to the ground truth
   in practice? you might see the error go from 1.0 to 0.01 during
   training but the results could still be unusable if an error of 0.01 is
   still in practice an unacceptable result. if it works on the training
   set check it on the validation set - does it still work for data it
   hasn't seen before? my advice would be to get used to visualizing
   everything from the beginning - don't start only when your network
   isn't working - make sure that before you start experimenting with
   different neural network structures you have your full pipeline in
   place all the way to the end user with sanity checks along the way.
   this is the only way you can accurately evaluate a number of potential
   different approaches.
     __________________________________________________________________

you forgot to preprocess your data

what?

   most data is tricky - and often data for things we know are similar can
   have vastly different numerical representations. to take an example
   from character animation - if we represent our data using the 3d
   positions of the character's joints relative to the center of the
   motion capture studio then performing a motion in one location, or
   facing one direction, may have a massively different numerical
   representation to performing the same motion in a different location,
   or facing a different direction. what we need to do instead is
   represent the data differently - for example in some local reference
   frame (such as relative to the character's center of mass) so that both
   motions we know are similar get a similar numerical representation.

how?

   think of exactly what you're features represent - is there some simple
   transformation you can do on them to ensure that data points which
   represent things we know are similar always get similar numerical
   representation? is there a local coordinate system you can represent
   your data in that makes things more natural - perhaps a better color
   space - a different format?

why?

   neural networks make only a few basic assumptions about the data they
   take as input - but one of these essential assumptions is that the
   space the data lies in is somewhat continuous - that for most of the
   space, a point between two data points is at least somewhat "a mix" of
   these two data points and that two nearby data points are in some sense
   representing "similar" things. having big discontinuities in the data
   space, or large clusters of separated data which represent the same
   thing, is going to make the learning task much more difficult.

and?

   another way to think about data pre-processing is as an attempt to
   reduce the combinatorial explosion of data variations that might be
   required. for example, if a neural network trained on character
   animation data has to learn the same set of motions for the character
   in every location and orientation then a lot of the capacity of the
   network is being wasted and a lot of the learning process is
   duplicated.
     __________________________________________________________________

you forgot to use any id173

what?

   id173 - typically in the form of dropout, noise, or some form
   of stochastic process injected to the network is another non-negotiable
   aspect of training modern neural networks. even if you think you have
   vastly more data than parameters, or that you have some situation where
   over-fitting does not matter or appears impossible, you should still
   often add dropout or some other form of noise.

how?

   the most basic way to regularize a neural network is to add dropout
   before each linear layer (convolutional or dense) in your network.
   start with a medium to high retainment id203 such as 0.75 or 0.9.
   tweak based on how possible you think over-fitting is and if you find
   any evidence of it. if you still think over-fitting is impossible
   consider setting the retainment id203 to something very high such
   as 0.99.

why?

   id173 isn't just about controlling over-fitting. by
   introducing some stochastic process into the training procedure you are
   in some sense "smoothing" out the cost landscape. this can speed up
   training, help deal with noise or outliers in the data, and prevent
   extreme weight configurations of the network.

and?

   data augmentation or other kinds of noise can also act as
   id173 just like dropout does. while the common way to think
   about dropout is as a technique for combining the predictions of many
   random sub-networks, it is also possible to think about dropout as a
   way of dynamically expanding the size of the training set by producing
   many similar variations of the input data during training. and as we
   know, the best way to avoid over-fitting and increase network accuracy
   is to have so much data that the neural network never sees the same
   thing twice!
     __________________________________________________________________

you used a too large batch size

what?

   using too large a batch size can have a negative effect on the accuracy
   of your network during training since it reduces the stochasticity of
   the id119.

how?

   find the minimum batch size with which you can tolerate the training
   time. the batch size which makes optimal use of the gpu parallelism
   might not be the best when it comes to accuracy as at some point a
   larger batch size will require training the network for more epochs to
   achieve the same level of accuracy. don't be scared to start with a
   very small batch size such as 16, 8, or even 1.

why?

   using a smaller batch size produces choppier, more stochastic weight
   updates. this can have two positive effects. firstly, it can help the
   training to "jump" out of local minima in which it might previously
   have gotten stuck, and secondly it can cause the training to settle in
   minima which are "flatter", something that generally indicates better
   generalization performance.

and?

   some other elements in the data can sometimes effectively act like a
   batch size. for example processing images at twice the resolution as
   before can have a similar effect as using four times the batch size. to
   get an intuition for this, consider that in a id98 the weight updates
   for each filter will be averaged over all the pixels to which it was
   applied in the input image, as well as for each image in the batch.
   increasing the image resolution by two will produce an averaging effect
   over four times as many pixels in a very similar way as to if you
   increased the batch size by four. overall the important thing is to
   consider how much the final gradient updates will be averaged at each
   iteration and make sure you balance the detrimental effect this has
   against the need to use as much of the potential parallelism of the gpu
   as possible.
     __________________________________________________________________

you used an incorrect learning rate

what?

   the learning rate can have a huge impact on how easy it is to train
   your network and if you are a newcomer it is almost certain you've set
   it incorrectly thanks the various default options used in common deep
   learning frameworks.

how?

   turn gradient clipping off. find the highest value for the learning
   rate which doesn't make the error explode during training. set the
   learning rate a bit lower than this - this is probably pretty close to
   the optimal learning rate.

why?

   many deep learning frameworks turn on gradient clipping by default.
   this option prevents the optimization used during training from
   exploding by enforcing a maximum amount the weights can change at each
   step. this can be useful - in particular if your data contains many
   outliers which produce large errors and therefore large gradients and
   weight updates - but having it on by default also makes it very
   difficult to find the optimal learning rate by hand. i've found that
   most newcomers to deep learning have the learning rate set way too high
   and account for this with gradient clipping, making the overall
   training behavior slow and the effect of changing the learning rate
   unpredictable.

and?

   if you've cleaned your data properly, removed most of the outliers, and
   set the learning rate correctly then you really shouldn't need gradient
   clipping. if without it you find your training error exploding
   occasionally then by all means turn gradient clipping on, but just
   remember that seeing your training error exploding is almost always an
   indication that something else is wrong with some of your data -
   clipping is a temporary fix.
     __________________________________________________________________

you used the wrong activation function on the final layer

what?

   using an activation function on the final layer can sometimes mean that
   your network cannot produce the full range of required values. the most
   common error is using a relu on the final layer - resulting in a
   network can only produce positive values as output.

how?

   if you are doing a regression then most often you don't want to use any
   kind of activation function on the final layer unless you know
   something specific about the kind of values you wish to produce as
   output.

why?

   think again what your data values actually represent and what their
   ranges are after id172. it is most likely the case that your
   output values are unbounded positive or negative numbers - in which
   case you shouldn't use an activation function on the final layer. if
   your output value may only make sense in some range e.g. it consists of
   probabilities in the range 0-1 there is most likely a specific
   activation function that should be used on the final layer such as a
   sigmoid activation function.

and?

   there are a number of subtleties to using id180 on the
   final layer. perhaps you know your system will eventually clip outputs
   into the range -1, 1 after they are produced by the neural network.
   then it might seem to make sense to add this clipping process as the
   final layer activation as this will ensure your network error function
   doesn't penalize values greater than 1 or less than -1. but no error
   means there will also be no gradient for these values which are greater
   or less than one - which in some cases will make your network
   impossible to train. alternatively you may be tempted to use tanh on
   the final layer, knowing that this activation function outputs values
   in the range -1 to 1 - but this can cause problems too as the gradients
   of this function near 1 or -1 grow very small which could cause your
   weights to grow huge in an attempt to produce -1 or 1 exactly. in
   general your best bet is often to play it safe and use no activation
   function at all on the final layer rather than trying to do something
   clever that may backfire.
     __________________________________________________________________

your network contains bad gradients

what?

   deep networks using relu id180 can often suffer from so
   called "dead neurons" caused by bad gradients. this can negatively
   affect the performance of the network, or in some cases make it
   completely impossible to train.

how?

   if you find that your training error does not change from epoch to
   epoch it may be that all your neurons have died due to using the relu
   activation function. try switching to another activation function such
   as leaky relus or elus and see if the same thing happens.

why?

   the gradient of the relu activation function is 1 for positive values
   and 0 for negative values. this is because a very small change in the
   input does not affect the output for inputs less than zero. this might
   not seem like a problem immediately due to the large gradient for
   positive values, but with many layers stacked on top of each other, and
   negative weights able to change large positive values with strong
   gradients into negative values with zero gradient, it can often be the
   case that some or even all of your network weights have a gradient of
   zero with respect to the cost function no matter what input they are
   given. in this situation we say that the network is "dead" since the
   weights are completely unable to update.

and?

   any operation with zero gradient such as clipping, rounding, or taking
   the maximum/minimum, will also produce bad gradients if they are used
   in computing the derivative of the cost function with respect to the
   weights. be very careful if these appear anywhere in your symbolic
   graph as they can often cause unforeseen difficulties, e.g. if they are
   used in some custom error metric that is provided as part of the cost
   function.
     __________________________________________________________________

you initialized your network weights incorrectly

what?

   if you don't initialize your neural network weights correctly then it
   is very unlikely your neural network will train at all. many other
   components in the neural network assume some form of correct or
   standardized weight initialization and setting the weights to zero, or
   using your own custom random initialization is not going to work.

how?

   the 'he', 'lecun' or 'xavier' weight initializations are all popular
   choices which should work perfectly well in practically any situation.
   just pick one (my favourite is 'lecun'), but feel free to experiment
   once your neural network is working until you find the best fit for
   your task.

why?

   you may have heard that you can initialize neural network weights using
   "small random numbers" but it isn't that simple. all of the above
   initializations were discovered using complex and detailed mathematics
   which explain exactly why they are optimal. even more importantly,
   other neural network components have been built around these
   initializations and tested empirically using them - using your own
   initialization may make it far more difficult to reproduce other
   researcher's results.

and?

   other layers may need to be carefully initialized too. network biases
   are initialized to zero, while other more complicated layers such as
   parametric id180 may come with their own initializations
   which are just as important to get correct.
     __________________________________________________________________

you used a network that was too deep

what?

   deeper is better right? well not always ... deeper is usually better
   when we are playing the benchmark game and trying to squeeze 1% more
   accuracy out of some task, but if your little 3, 4, 5 layer network is
   failing to learn anything then i can assure you that a 100 layer
   behemoth going to fail just as badly if not worse.

how?

   start with a shallow a neural network with 3 to 8 layers. start
   experimenting with deeper networks only when you already have things
   working well and are starting to investigate how to increase the
   accuracy.

why?

   although it might seem like it, neural networks didn't just suddenly
   start getting ground breaking results when someone decided to stack
   hundreds of layers. all of the improvements to neural networks that
   have been researched over the last decade have been small, fundamental
   changes which are just as applicable to the performance of smaller
   networks as deep ones. if your network is not working it is more likely
   something else is wrong other than the depth.

and?

   starting small also means that training your network will be faster,
   id136 will be faster, and iterating on different designs and setups
   will be faster. initially, all of these things will have a much bigger
   impact on your accuracy than simply stacking a few more layers.
     __________________________________________________________________

you used the wrong number of hidden units

what?

   in some cases using vastly too many or too few hidden units can make
   your network difficult to train. too few units and it may not have the
   capacity to express the task required, while too many and it may become
   slow and unwieldy to train with residual noise that is hard to remove.

how?

   start with somewhere between 256 and 1024 hidden units. then, look at
   the numbers being used by fellow researchers working on similar
   applications and use those as inspiration. if fellow researchers are
   using vastly different numbers to the ones given above then there may
   be some specific reason why which is probably important for you to
   understand.

why?

   when deciding on the number of hidden units to use the key is to
   consider roughly what you think may be the fewest number of real values
   required to express the information you wish to pass through the
   network. you should then probably scale this number up a bit. this will
   allow for dropout, for the network to use a more redundant
   representation, and for a bit of leeway in your estimate. if you are
   doing classification you can probably use five to ten times the number
   of classes as a good initial guess, while if you are doing regression
   you can probably use two to three times the number of input or output
   variables. of course - all of this is highly context dependent and
   there is no simple automatic solution - a good intuition is still the
   most important thing deciding on the number of hidden units.

and?

   in reality the number of hidden units often has quite a small impact on
   neural network performance when compared to other factors, and in many
   cases overestimating the number of hidden units required will have
   little negative effect other than making training slower. once your
   network is working, if you are still concerned, just try a whole bunch
   of different numbers and measure the accuracy until you find the one
   that works best.
     __________________________________________________________________

see also

     * [123]debugging neural networks
     * [124]a practical guide to training restricted id82s

   any other suggestions for common issues? e-mail me at
   [125]contact@theorangeduck.com and i will try to add them to the list.
   [126]github [127]twitter [128]rss

references

   visible links
   1. http://theorangeduck.com/feeds/pages
   2. http://theorangeduck.com/page/naraleian-caterpillars
   3. http://theorangeduck.com/page/scientific-method-virus
   4. http://theorangeduck.com/page/local-minima-saddle-points-plateaus
   5. http://theorangeduck.com/page/robust-solving-optical-motion-capture-data-denoising
   6. http://theorangeduck.com/page/simple-concurrency-python
   7. http://theorangeduck.com/page/the-software-thief
   8. http://theorangeduck.com/page/ascii-love-letter
   9. http://theorangeduck.com/page/neural-network-not-working
  10. http://theorangeduck.com/page/phase-functioned-neural-networks-character-control
  11. http://theorangeduck.com/page/17-line-markov-chain
  12. http://theorangeduck.com/page/14-character-random-number-generator
  13. http://theorangeduck.com/page/simple-two-joint
  14. http://theorangeduck.com/page/generating-icons-pixel-sorting
  15. http://theorangeduck.com/page/neural-network-ambient-occlusion
  16. http://theorangeduck.com/page/three-stories-east-coast-main-line
  17. http://theorangeduck.com/page/new-alphabet
  18. http://theorangeduck.com/page/color-munifni-exists
  19. http://theorangeduck.com/page/deep-learning-framework-character-motion-synthesis-and-editing
  20. http://theorangeduck.com/page/halting-problem-and-moral-arbitrator
  21. http://theorangeduck.com/page/witness
  22. http://theorangeduck.com/page/four-seasons-crisp-omelette
  23. http://theorangeduck.com/page/bottom-elevator
  24. http://theorangeduck.com/page/tracing-functions-python
  25. http://theorangeduck.com/page/still-things-and-moving-things
  26. http://theorangeduck.com/page/watercpp
  27. http://theorangeduck.com/page/making-poetry-piet
  28. http://theorangeduck.com/page/learning-motion-manifolds-convolutional-autoencoders
  29. http://theorangeduck.com/page/learning-inverse-rig-mapping-character-animation
  30. http://theorangeduck.com/page/infinity-doesnt-exist
  31. http://theorangeduck.com/page/polyconf
  32. http://theorangeduck.com/page/raleigh
  33. http://theorangeduck.com/page/skagerrak
  34. http://theorangeduck.com/page/printing-stack-trace-mingw
  35. http://theorangeduck.com/page/border-pines
  36. http://theorangeduck.com/page/you-could-have-invented-parser-combinators
  37. http://theorangeduck.com/page/ready-fight
  38. http://theorangeduck.com/page/earthbound
  39. http://theorangeduck.com/page/turing-drawings
  40. http://theorangeduck.com/page/lost-child-announcement
  41. http://theorangeduck.com/page/shelter
  42. http://theorangeduck.com/page/data-science-how-hard
  43. http://theorangeduck.com/page/denki-furo
  44. http://theorangeduck.com/page/defence-unitype
  45. http://theorangeduck.com/page/maya-velocity-node
  46. http://theorangeduck.com/page/sandy-denny
  47. http://theorangeduck.com/page/what-type-machine-c-preprocessor
  48. http://theorangeduck.com/page/which-ai-more-human
  49. http://theorangeduck.com/page/gone-home
  50. http://theorangeduck.com/page/thoughts-japan
  51. http://theorangeduck.com/page/can-computers-think
  52. http://theorangeduck.com/page/counting-sheep-infinity
  53. http://theorangeduck.com/page/how-nature-builds-computers
  54. http://theorangeduck.com/page/painkillers
  55. http://theorangeduck.com/page/correct-box-sphere-intersection
  56. http://theorangeduck.com/page/avoiding-shader-conditionals
  57. http://theorangeduck.com/page/writing-portable-opengl
  58. http://theorangeduck.com/page/only-cable-car-ireland
  59. http://theorangeduck.com/page/c-preprocessor-turing-complete
  60. http://theorangeduck.com/page/aesthetics-code
  61. http://theorangeduck.com/page/issues-sdl-ios-and-android
  62. http://theorangeduck.com/page/how-i-learned-stop-worrying-and-love-statistics
  63. http://theorangeduck.com/page/pymark
  64. http://theorangeduck.com/page/autoc-tools
  65. http://theorangeduck.com/page/scripting-xnormal-python
  66. http://theorangeduck.com/page/six-myths-about-ray-tracing
  67. http://theorangeduck.com/page/web-giants-will-fall
  68. http://theorangeduck.com/page/pyautoc
  69. http://theorangeduck.com/page/pirate-song
  70. http://theorangeduck.com/page/dear-esther
  71. http://theorangeduck.com/page/unsharp-anti-aliasing
  72. http://theorangeduck.com/page/first-boy
  73. http://theorangeduck.com/page/parallel-programming-isnt-hard-optimisation
  74. http://theorangeduck.com/page/skyrim
  75. http://theorangeduck.com/page/recognizing-language-solving-problem
  76. http://theorangeduck.com/page/could-animal-learn-program
  77. http://theorangeduck.com/page/rage
  78. http://theorangeduck.com/page/pure-depth-ssao
  79. http://theorangeduck.com/page/synchronized-python
  80. http://theorangeduck.com/page/3d-printing
  81. http://theorangeduck.com/page/real-time-graphics-virtual-reality
  82. http://theorangeduck.com/page/painting-style-renderer
  83. http://theorangeduck.com/page/very-hard-problem
  84. http://theorangeduck.com/page/indie-development-vs-modding
  85. http://theorangeduck.com/page/corange
  86. http://theorangeduck.com/page/3ds-max-ply-exporter
  87. http://theorangeduck.com/page/case-technical-artist
  88. http://theorangeduck.com/page/enums
  89. http://theorangeduck.com/page/scorpions-have-won-evolution
  90. http://theorangeduck.com/page/dirt-and-ashes
  91. http://theorangeduck.com/page/lazy-python
  92. http://theorangeduck.com/page/subdivision-modelling
  93. http://theorangeduck.com/page/owl
  94. http://theorangeduck.com/page/mouse-traps
  95. http://theorangeduck.com/page/updated-art-reel
  96. http://theorangeduck.com/page/tech-reel
  97. http://theorangeduck.com/page/graphics-arent-enemy
  98. http://theorangeduck.com/page/being-games-artist
  99. http://theorangeduck.com/page/bluebird
 100. http://theorangeduck.com/page/everything2
 101. http://theorangeduck.com/page/duck-engine
 102. http://theorangeduck.com/page/boarding-preview
 103. http://theorangeduck.com/page/sailing-preview
 104. http://theorangeduck.com/page/exodus-village-flyover
 105. http://theorangeduck.com/page/art-reel
 106. http://theorangeduck.com/page/lol-i-drew-dragon
 107. http://theorangeduck.com/page/one-cat-just-leads-another
 108. http://theorangeduck.com/page/projects
 109. http://theorangeduck.com/page/publications
 110. http://theorangeduck.com/page/about
 111. http://theorangeduck.com/page/all
 112. http://theorangeduck.com/page/neural-network-not-working#normalize
 113. http://theorangeduck.com/page/neural-network-not-working#results
 114. http://theorangeduck.com/page/neural-network-not-working#preprocess
 115. http://theorangeduck.com/page/neural-network-not-working#dropout
 116. http://theorangeduck.com/page/neural-network-not-working#batchsize
 117. http://theorangeduck.com/page/neural-network-not-working#learningrate
 118. http://theorangeduck.com/page/neural-network-not-working#final
 119. http://theorangeduck.com/page/neural-network-not-working#dead
 120. http://theorangeduck.com/page/neural-network-not-working#init
 121. http://theorangeduck.com/page/neural-network-not-working#deep
 122. http://theorangeduck.com/page/neural-network-not-working#hidden
 123. https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class/41493375#41493375
 124. https://www.cs.toronto.edu/~hinton/absps/guidetr.pdf
 125. mailto:contact@theorangeduck.com
 126. https://github.com/orangeduck
 127. https://twitter.com/anorangeduck
 128. http://theorangeduck.com/feeds/pages

   hidden links:
 130. http://theorangeduck.com/page/neural-network-not-working
 131. http://theorangeduck.com/page/neural-network-not-working
