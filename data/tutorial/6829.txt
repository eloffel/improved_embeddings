   #[1]fasttext blog atom feed [2]fasttext blog rss feed

   [3]fasttext

     * [4]docs
     * [5]download
     * [6]blog
     * [7]github

   recent posts

recent posts

     * [8]id46
     * [9]fasttext on mobile
     * [10]releasing fasttext

[11]id46

   october 2, 2017

   [12]edouard grave
   [13]edouard grave

fast and accurate id46 using fasttext

   we are excited to announce that we are publishing a fast and accurate
   tool for text-based id46. it can recognize more than
   170 languages, takes less than 1mb of memory and can classify thousands
   of documents per second. it is based on fasttext library and is
   released [14]here as open source, free to use by everyone. we are
   releasing several versions of the model, each optimized for different
   memory usage, and compared them to the popular tool [15]langid.py.

   evaluation of our models

   our tool uses various features offered by the fasttext library, such as
   subwords or model compression. in the remainder of this blogpost, we
   will explain how these work, and how to use them to build a fast and
   small language detector.

training your own language detector

   building a fast and small language detector with fasttext can be done
   with a few command lines, as we will show below. first, we need a
   dataset to train our model. here, we propose to use sentences from the
   tatoeba website, which can be downloaded from
   [16]https://tatoeba.org/eng/downloads. note that for the sake of
   simplicity, we use a small quantity of data for this blogpost . if you
   want to train a state-of-the-art model comparable with our pre-trained
   model, you will need to use a larger quantity of data.

training data

   first, let's download the training data:
>> wget http://downloads.tatoeba.org/exports/sentences.tar.bz2
>> bunzip2 sentences.tar.bz2
>> tar xvf sentences.tar

   then, we need to put our training data into fasttext format, which is
   easily done using:
>> awk -f"\t" '{print"__label__"$2" "$3}' < sentences.csv | shuf > all.txt

   we can then split our training data into training and validation sets:
>> head -n 10000 all.txt > valid.txt
>> tail -n +10001 all.txt > train.txt

first model

   we can now train our first model
>> ./fasttext supervised -input train.txt -output langdetect -dim 16

   and test it on the held out data:
>> ./fasttext test langdetect.bin valid.txt

   this model should have an accuracy around 96.5%. let's see if we can do
   better, by changing the default parameters.

using subword features

   the first way to improve our baseline model is to use subword features,
   which enhance the classifier by taking into account the structure of
   words. it uses a simple, yet effective way of incorporating such
   information: each word is represented by the set of all character
   ngrams of a given length appearing in that word. as an example, when
   using subwords of length 3, the word skiing is represented by
{ skiing, ski, kii, iin, ing }

   a key advantage of these features is that out-of-vocabulary words, such
   as misspelled words, can still be represented at test time by their
   subwords representations. this make text classifiers much more robust,
   especially for problems with small training sets, or for
   morphologically rich languages. users can enable these features by
   simply specifying the value of the minimum and maximum character ngram
   size with the command line options -minn and -maxn:
>> ./fasttext supervised -input train.txt -output langdetect -dim 16 -minn 2 -ma
xn 4

   in that case, fasttext now uses all the character ngrams of length 2, 3
   and 4. the accuracy of the classifier should improve, and be above
   98.5%. we can also make the training and testing faster, by using the
   hierarchical softmax:
>> ./fasttext supervised -input train.txt -output langdetect -dim 16 -minn 2 -ma
xn 4 -loss hs

model compression

   finally, we can make the size of the model file much smaller, by using
   model compression:
>> ./fasttext quantize -input train.txt -output langdetect -qnorm -cutoff 50000
-retrain

   after running this command line, you should get a new model,
   langdetect.ftz, with a file size smaller than 1mb (instead of 350mb for
   the original model).

   how does model quantization work? it is quite simple, and relies on two
   operations: weight quantization and feature selection. we now briefly
   describe these two operations in detail.

   weight quantization. the first operation is to compress the weights of
   the models using a technique called vector quantization. quantization
   is the process of mapping values from a large set (e.g. floating point
   numbers) to a smaller set (e.g. bytes). here, we use a variant which is
   well suited to compress vectors, instead of scalar values. the
   algorithm, called product quantization, works as follow. first, each
   vector is split into smaller vectors, for example of dimension 2. then,
   we run the id116 algorithm on these sub-vectors, and represent each
   sub-vector by the closest centroid obtained with id116. therefore,
   each 2-dimension vector is now represented by 1 byte (to store the
   centroid), instead of 8 bytes (to store the 2 floats), therefore
   achieving a compression rate of 8. if we instead split the vectors into
   sub-vectors of dimension 4, we can achieve a compression rate of 16
   (but often with a higher distortion rate). this tradeoff between
   compression and distortion can be controlled using the -dsub command
   line option, which set the dimension of the sub-vectors.

   feature selection. the second operation we apply to compress models is
   to remove features which do not have a big influence on the decision of
   the classifier. for this, our goal is to find the model with a given
   number of feature (e.g. 50,000 in the previous example) which is the
   closest from the original model. the solution of this problem is to
   keep the features (either words, subwords, or ngrams), which have the
   vectors with the largest norms.

references

     * [17]quantization
     * [18]vector quantization
     * [19]id116 algorithm
     * [20]feature selection

iso codes of languages supported

af als am an ar arz as ast av az azb ba bar bcl be bg bh bn bo bpy br bs bxr ca
cbk ce ceb ckb co cs cv cy da de diq dsb dty dv el eml en eo es et eu fa fi fr f
rr fy ga gd gl gn gom gu gv he hi hif hr hsb ht hu hy ia id ie ilo io is it ja j
bo jv ka kk km kn ko krc ku kv kw ky la lb lez li lmo lo lrc lt lv mai mg mhr mi
n mk ml mn mr mrj ms mt mwl my myv mzn nah nap nds ne new nl nn no oc or os pa p
am pfl pl pms pnb ps pt qu rm ro ru rue sa sah sc scn sco sd sh si sk sl so sq s
r su sv sw ta te tg th tk tl tr tt tyv ug uk ur uz vec vep vi vls vo wa war wuu
xal xmf yi yo yue zh

   [21]recent posts

   [22]fasttext

support

   [23]getting started[24]tutorials[25]faqs[26]api

community

   [27]facebook group[28]stack overflow[29]google group

more

   [30]blog[31]github[32]star
   [33]facebook open source

   copyright    2019 facebook inc.

references

   visible links
   1. https://fasttext.cc/blog/atom.xml
   2. https://fasttext.cc/blog/feed.xml
   3. https://fasttext.cc/
   4. https://fasttext.cc/docs/en/support.html
   5. https://fasttext.cc/docs/en/english-vectors.html
   6. https://fasttext.cc/blog/
   7. https://github.com/facebookresearch/fasttext/
   8. https://fasttext.cc/blog/2017/10/02/blog-post.html
   9. https://fasttext.cc/blog/2017/05/02/blog-post.html
  10. https://fasttext.cc/blog/2016/08/18/blog-post.html
  11. https://fasttext.cc/blog/2017/10/02/blog-post.html
  12. https://research.fb.com/people/grave-edouard/
  13. https://research.fb.com/people/grave-edouard/
  14. https://fasttext.cc/docs/en/language-identification.html
  15. https://github.com/saffsd/langid.py
  16. https://tatoeba.org/eng/downloads
  17. https://en.wikipedia.org/wiki/quantization_(signal_processing)
  18. https://en.wikipedia.org/wiki/vector_quantization
  19. https://en.wikipedia.org/wiki/id116_id91
  20. https://en.wikipedia.org/wiki/feature_selection
  21. https://fasttext.cc/blog
  22. https://fasttext.cc/
  23. https://fasttext.cc/docs/en/support.html
  24. https://fasttext.cc/docs/en/supervised-tutorial.html
  25. https://fasttext.cc/docs/en/faqs.html
  26. https://fasttext.cc/docs/en/api.html
  27. https://www.facebook.com/groups/1174547215919768/
  28. http://stackoverflow.com/questions/tagged/fasttext
  29. https://groups.google.com/forum/#!forum/fasttext-library
  30. https://fasttext.cc/blog
  31. https://github.com/facebookresearch/fasttext
  32. https://github.com/facebookresearch/fasttext/
  33. https://code.facebook.com/projects/

   hidden links:
  35. https://fasttext.cc/blog/2017/10/02/blog-post.html#fast-and-accurate-language-identification-using-fasttext
  36. https://fasttext.cc/blog/2017/10/02/blog-post.html#training-your-own-language-detector
  37. https://fasttext.cc/blog/2017/10/02/blog-post.html#training-data
  38. https://fasttext.cc/blog/2017/10/02/blog-post.html#first-model
  39. https://fasttext.cc/blog/2017/10/02/blog-post.html#using-subword-features
  40. https://fasttext.cc/blog/2017/10/02/blog-post.html#model-compression
  41. https://fasttext.cc/blog/2017/10/02/blog-post.html#references
  42. https://fasttext.cc/blog/2017/10/02/blog-post.html#iso-codes-of-languages-supported
