   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]jungle book
   [7]jungle book
   [8]sign in[9]get started
     __________________________________________________________________

automatic feature extraction with id167

   [10]go to the profile of gon  alo rodrigues
   [11]gon  alo rodrigues (button) blockedunblock (button) followfollowing
   oct 6, 2017

   a common problem data scientists face nowadays is dealing with very
   high-dimensional data (lots of features). most of the algorithms for
   classification and prediction that work for low-dimensional datasets
   don   t work as well when you have hundreds of features, a problem
   commonly referred as the curse of dimensionality. to solve this, we can
   reduce the amount of dimensions by feature selection or feature
   extraction, usually handpicking the most relevant features or using
   [12]principal component analysis (pca) before feeding the data into our
   favorite model. even though pca is amazing in most scenarios, it still
   is a linear model, which might not be powerful enough to apply to some
   datasets.

   in this article, we   ll explore one non-linear alternative to pca.

id167

   t-distributed stochastic neighbor embedding (id167) [13][original
   paper] is a very popular and state of the art id84
   technique that is usually used to map high dimensional data to 2 or 3
   dimensions in order to visualize it. it does so by computing affinities
   between points, and trying to maintain these affinities in the new,
   low-dimensional space. a nice post to learn with examples and code
   about id167 can be found[14] here.

   quoting from the original paper:

     the similarity of datapoint x to datapoint y is defined as the
     id155, p(y|x) , that x would pick y as its
     neighbor if neighbors were picked in proportion to their id203
     density under a gaussian centered at x .

   here i denote x as a matrix containing the dataset, where each row is a
   sample and y the matrix containing the representations in
   low-dimensional space (which we want to find). similarity between 2
   points in high-dimensional space is given by:
   [1*rxbyfdmiipd3ninfjssyvg.png]
   similarity between 2 points in high-dimensional space

   the affinity metric we use is a symmetric version of this equation, so
   that the affinity of a to b is the same as b to a, which is given by:
   [1*htz4fwb37h-l2vr-ads1xa.png]
   affinities in high-dimensional space

   for the low dimensional space, id167 uses a student-t distribution
   instead, as it has an easier and cheaper to compute gradient, giving us
   the formula for affinities in low dimensional space (for d dimensions):
   [1*65oeyjx2jgm5ucadeksnqq.png]
   affinities in low-dimensional space, where d is the dimensionality of
   the desired embedded space

   so now that we have the affinities for each pair of points, and we want
   to keep the low-dimensional ones as close as possible to the original
   ones, we need to have a id168 that measures the distances
   between those similarities. since we defined the similarities as
   probabilities, a common id168 to use between id203
   distributions is the kullback-liebler divergence, defined as
   [1*wi1xqgvpvig8kkczoyosiq.png]
   cost function of id167

   and our goal is to find y (the low-dimensional embeddings) that
   minimize this id168, for which we can use for example
   [15]stochastic id119 (sgd).
   [1*armxilidgcjh-qg7uc5nhq.png]
   figure 1. id167 algorithm workflow. the loop only stops when y doesn   t
   change much between iterations

   let   s go over the algorithm step by step, illustrated by figure 1.
    1. compute matrix p from the data using equation (1)
    2. initialize y (the embeddings) randomly
    3. compute matrix q from current y using equation (2)
    4. compute cost from matrix p and q using equation (3)
    5. compute the gradient of the cost with respect to y and update y
    6. go back to step 3.

   after stopping the algorithm, we end up with our y as the embeddings of
   the whole dataset.

   there   s also a small step we are missing here to solve the problem of
   computing    in equation (1). usually a perplexity level is provided by
   the user which allows us to compute this parameter, but i won   t go into
   detail in this post. you can get a better insight about this parameter
   in [16]this post which explains very well what it is and how to use it.

   there are, however, some drawbacks related to id167:
     * it is not parametric, meaning you can   t find the low-dimensional
       embeddings of new points that weren   t used in the training phase
       without running the algorithm again
     * it is not scalable, meaning you need the whole dataset and matrices
       p and q in memory which won   t work when your dataset doesn   t fit in
       memory

   the main reason this work was done was to address these drawbacks.

dealing with new points

   the id167 algorithm finds those embeddings (y) for all the dataset that
   was used, but what if we want to find the embedding of a new point
   after we have trained the model? well, you can   t with a vanilla id167.
   in other words, originally id167 was not a parametric model, but the
   author created a model a few years later that bypasses this limitation.
   ([17]see here)

   a way to create a parametric id167 is using a feed-forward neural
   network to map a high-dimensional point (x) to its low-dimensional
   embedding (y), and optimizing the id167 id168 with respect to
   the neural networks weights. an implementation in keras can be found
   [18]here.
   [1*racawwrfjvldhiulm4lxeg.png]
   figure 2. updated id167 algorithm, parameterizing y with a
   neural network

   let   s go over the algorithm step by step, illustrated by figure 2.
    1. compute matrix p from the data using equation (1)
    2. feed the data into the neural network to get y as the output
       (a.k.a. forward pass)
    3. compute matrix q from y using equation (2)
    4. compute cost from matrix p and q using equation (3)
    5. compute the gradient of the cost with respect to neural network   s
       weights and update them (a.k.a. backward pass)
    6. go back to step 2.

   after training, we end up with a neural network that takes as input a
   high-dimensional point (x) and outputs its embedding (y), which we can
   use on data that id167 has never seen before!

dealing with big datasets

   a big limitation of id167 is the poor scalability of the matrices p and
   q with the size of the dataset. if, for example, we have 1 million
   samples (which occupies some mb of memory) and we want to run id167 on
   it, we   d end up with a matrix with 10     entries, which would occupy
   roughly 1 tb of memory!

   there is an approximation called [19]barnes-hut id167 (not parametric
   though) released by the same author. this version computes the
   (approximated) nearest neighbors for each point and then only computes
   the pairwise affinities for the nearest neighbors instead of the whole
   dataset and assumes the rest are zero. you no longer need a n   matrix
   in memory (if you use a sparse representation), but you still need the
   whole dataset in memory as the algorithm doesn   t work in batches.

   instead we   ll use a simple way around the problem which feels more
   natural with neural networks.

   what we   ll do is to feed the neural network in batches and to simply
   compute p and q (eq. 2 and 3) only within each batch, that is, we   ll
   compute the pairwise distances between points that are in the same
   batch. that reduces the complexity immensely and allows us to run id167
   on datasets of virtually any size.

experiments

   to demonstrate the power of this scalable id167, we   ll use a big
   dataset (> 1 million samples) from kaggle, the [20]nyc taxi dataset.

   the goal of this kaggle competition is to predict the trip duration of
   a taxi, given its pickup location, date, time of day, vendor and number
   of passengers. we have extracted some features from the data, such as
   day of week and month.
   [1*fgd82mmtk_l8jdpj_qxkxq.png]
   figure 3. first 5 samples of the dataset

   let   s run id167 (removing the target column) and see what we find out.
   [1*sk-dvfwhiefngwipa9aumq.png]
   figure 4. embeddings of the dataset after running id167, colored by
   trip duration, using a logarithmic scale. lighter means longer
   trip time.

   we can clearly see 3 big clusters in the data, and 2 smaller ones in
   the right with significantly longer trip duration. what could they be?
   let   s color them by different features and see if we can find the
   structure uncovered by id167.
   [1*quaxxo076junuy6qnokdoq.png]
   figure 5. emebeddings of the taxi dataset colored with different
   feature values
   [1*2zq4yvjldrpv9q5l0tzkkg.png]
   figure 6. easy to spot clusters

   the smaller, high trip duration clusters all have the similar pickup
   coordinates or dropoff coordinates, and it turns out to be the
   international airport! actually in the middle of the airport clusters
   there   s a short trip time cluster which corresponds to going from
   somewhere around the airport to not too far from it. those other 3 big
   clusters clearly correspond to a division in number of passengers, one
   for one and two passengers, one for 3 passengers, and another one for
   more than 3 passengers. every other feature is clearly organized in the
   2d space. id167 did a great job transforming this 8-dimensional space
   to a 2-dimensional space!

   it   s quite interesting visualizing high dimensional data in 2d, and it
   can be of real value when you have a huge dataset and you have no idea
   what it might be. however, if you try the scikit-learn implementation
   of id167 with this dataset, it might take a very long while to
   complete.

   after running id167 you could use the embeddings as features for a
   regressor, and get a decent score or outlier detection as seen in
   [21]this blogpost. of course it doesn   t make sense to reduce
   dimensionality in this dataset because it only has 8 features, but when
   you have thousands of features it might be worthwhile to try id167 as a
   id84 technique.

conclusion

   id167 is a very poweful method for data visualization, dimensionality
   reduction and can even be used for outlier detection. parameterizing
   id167 gives us extra flexibility and allows it to be combined with
   other kinds of neural networks. it also allows us to use mini batches
   which scale to virtually any dataset size.

   this was work was done while in an internship at jungle.ai.

   originally published by becominghuman on october, 2017

   iframe: [22]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=62826ce09268

   [23][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [24][1*v-ppfkswhbvlwwamsvhhwg.png]
   [25][1*wt2auqisieaozxj-i7brdq.png]

   thanks to [26]silvio rodrigues.
     * [27]machine learning
     * [28]computer science
     * [29]artificial intelligence
     * [30]ai

   (button)
   (button)
   (button) 511 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of gon  alo rodrigues

[32]gon  alo rodrigues

   computer science engineer. interested in machine learning

     (button) follow
   [33]jungle book

[34]jungle book

   insights from applied research in ai and refreshing perspectives on
   business, people and design. by and for explorers of unknown territory.
   see [35]https://jungle.ai

     * (button)
       (button) 511
     * (button)
     *
     *

   [36]jungle book
   never miss a story from jungle book, when you sign up for medium.
   [37]learn more
   never miss a story from jungle book
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/62826ce09268
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/jungle-book?source=avatar-lo_bwuf3co68kgi-d3d11ca83b1
   7. https://medium.com/jungle-book?source=logo-lo_bwuf3co68kgi---d3d11ca83b1
   8. https://medium.com/m/signin?redirect=https://medium.com/jungle-book/automatic-feature-extraction-with-id167-62826ce09268&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/jungle-book/automatic-feature-extraction-with-id167-62826ce09268&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@goncalo.rodrigues?source=post_header_lockup
  11. https://medium.com/@goncalo.rodrigues
  12. https://en.wikipedia.org/wiki/principal_component_analysis
  13. https://lvdmaaten.github.io/publications/papers/jmlr_2008.pdf
  14. https://nlml.github.io/in-raw-numpy/in-raw-numpy-id167/
  15. https://www.google.pt/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahukewjn793tzafwahuhfrokhbkvcoqqfggsmaa&url=https://en.wikipedia.org/wiki/stochastic_gradient_descent&usg=afqjcnhkzcxosj2zdqonetdtva3q0as0eq
  16. https://distill.pub/2016/misread-tsne/
  17. https://lvdmaaten.github.io/publications/papers/aistats_2009.pdf
  18. https://github.com/zaburo-ch/parametric-id167-in-keras/blob/master/mlp_param_tsne.py
  19. https://lvdmaaten.github.io/publications/papers/aistats_2009.pdf
  20. https://www.kaggle.com/c/nyc-taxi-trip-duration
  21. https://goo.gl/jjrfzq
  22. https://medium.com/media/c43026df6fee7cdb1aab8aaf916125ea?postid=62826ce09268
  23. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  24. https://upscri.be/8f5f8b
  25. https://becominghuman.ai/write-for-us-48270209de63
  26. https://medium.com/@sfrodrigues?source=post_page
  27. https://medium.com/tag/machine-learning?source=post
  28. https://medium.com/tag/computer-science?source=post
  29. https://medium.com/tag/artificial-intelligence?source=post
  30. https://medium.com/tag/ai?source=post
  31. https://medium.com/@goncalo.rodrigues?source=footer_card
  32. https://medium.com/@goncalo.rodrigues
  33. https://medium.com/jungle-book?source=footer_card
  34. https://medium.com/jungle-book?source=footer_card
  35. https://jungle.ai/
  36. https://medium.com/jungle-book
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/62826ce09268/share/twitter
  40. https://medium.com/p/62826ce09268/share/facebook
  41. https://medium.com/p/62826ce09268/share/twitter
  42. https://medium.com/p/62826ce09268/share/facebook
