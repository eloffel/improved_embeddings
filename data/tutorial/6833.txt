   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

data science simplified part 11: id28

   [16]go to the profile of pradeep menon
   [17]pradeep menon (button) blockedunblock (button) followfollowing
   oct 1, 2017

   in the[18] last blog post of this series, we discussed classifiers. the
   categories of classifiers and how they are evaluated were discussed. we
   have also discussed regression models in depth. in this post, we dwell
   a little deeper in how regression models can be used for classification
   tasks.

   id28 is a widely used regression model used for
   classification tasks. as usual, we will discuss by example.

   no money bank approaches us with a problem. the bank wants to build a
   model that predicts which of their customers will default on their
   loans. the dataset provided is as follows:
   [0*uh9hpogkf2f59x28.]

   the features that will help us to build the model are:
     * customer id: a unique customer identification
     * credit score: a value between 0 and 800 indicating the riskiness of
       the borrower   s credit history.
     * loan amount: this is the loan amount that was either completely
       paid off or the amount that was defaulted.
     * years in current job: a categorical variable indicating how many
       years the customer has been in their current job.
     * years of credit history: the years since the first entry in the
       customer   s credit history
     * monthly debt: the customer   s monthly payment for their existing
       loans
     * the number of credit problems: the number of credit problems in the
       customer records.
     * isdefault: this is the target. if the customer has defaulted then
       it will 1 else it 0.

   this is a classification problem.

   id28 is an avatar of the regression model. it transforms
   the regression model to become a classifier. let us first understand
   why a vanilla regression model doesn   t work as a classifier.

   the target is default has a value of 0 or 1. we can reframe this as a
   id203. the reframing is as follows:
     * if id203 of default >= 0.5 then the customer will default
       i.e. isdefault = 1
     * if id203 of default < 0.5 then the customer will not default
       i.e. isdefault = 0

   recall our discussion on id75 model. in the regression
   model, we had defined an dependent variable y which was a function of
   independent variables. for sake of simplicity, let us assume that we
   have only one independent variable x. the equation becomes.

     y =   0 +   1.x

     *   0 is the intercept.
     *   1 is the coefficient of x.

   in the example of loan default model, tim uses credit score as the
   independent variable. the dependent variable (y) is the estimate of the
   id203 that the customer will default i.e. p(default).

   the equation can be written as:

     p(default) =   0 +   1.credit score

   tim runs the regression model on the statistical package. the
   statistical package provides the following coefficient for   0 and   1:
     *   0 = 0.73257
     *   1 = -9.9238e-05

   the equation to estimate the id203 of default now becomes:

     p(default) = 0.73257 + -9.9238e-05 . credit score

   if someone takes has a high credit score say 8000 then will he default
   or not? let us pluck in some values and check.

     0.73257 + -9.9238e-05 x 8000 = -0.06134.

   if we plot p(default) with the credit score along with the regression
   line we get the following plot:
   [0*z4trx4zha_n9s85w.]

   the vanilla regression model has a challenge. the number -0.06334, a
   negative id203, doesn   t make sense. it is also evident from the
   graph. for high credit scores, the id203 is less than zero.
   id203 needs to be between 0 and 1.

   what can be done to convert the equation such the id203 is always
   between 0 and 1?

   this is where sigmoid comes in.

   a sigmoid or logistic function is a mathematical function having a
   characteristic    s   -shaped curve or sigmoid curve. mathematically, it is
   defined as follows:

     sigmoid = ey/(1+ey)

   the sigmoid takes the following shape:
   [0*knexv3xftutcylne.]

   it transforms all the values between 0 and 1. let us say that we have a
   set of numbers from -5 to 10. when these set of numbers are transformed
   using the sigmoid function, all the values are between 0 and 1.
   [0*74nvqok3_7glxr05.]

   this becomes interesting. using sigmoid, any number can be converted to
   an equivalent id203 score.

   now that we have a method of translating the target into probabilities,
   let us see how it works. the regression equation when translated using
   the sigmoid function becomes the following:

     y =   0 +   1.credit score

     p(default) = ey/(1 + ey)

     p(default) = sigmoid(y)

   let us check how sigmoid model fares when the credit rating is high
   i.e. 8000

     y = 0.73257 + -9.9238e-05 x 8000 = -0.06134.

     p(default) = sigmoid(y) = sigmoid(-0.06134) = 0.4846

     p(default) = 48.46% => isdefault = 0

   the id28 model can be enhanced by adding more variables.
   all we need to do is to enhance the simple id75 model to a
   multivariate regression model equation. an example of such a model can
   be as follows:

     y2 =   0 +   1.credit score +   2. loan amount +   3.number of credit
     problems +   4. monthly debt+   5.months since last delinquent +
       6.number of credit cards

     p(default) = sigmoid(y2)

   let us try this model to predict the potential defaulters. the loan
   dataset is split into training and test set in the ratio of 80:20 (80%
   training, 20% testing).

   recall that there are many metrics to evaluate a classifier. we will
   use auc as the metrics for the evaluation of the model. let us see how
   does the new model perform. a machine learning program is used to
   evaluate the model performance on test data.
   [0*_sbt6ksdowx3p4o3.]

   the new model doesn   t perform that well. the auc score is around 60% on
   test data.

   we now the workings of a id28 model. we now know how
   does it build classifiers. the auc score of the classifier is not good.
   we need to look for better models. in the next post of this series, we
   will look at cross-validation.
     __________________________________________________________________

   originally published at [19]datascientia.blog on october 2, 2017.

     * [20]machine learning
     * [21]towards data science
     * [22]data science

   (button)
   (button)
   (button) 187 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of pradeep menon

[24]pradeep menon

   director of #bigdata and #ai solution architecture @ alibaba cloud.
   impact driven. executive-level interpersonal skills. hands-on.
   #worldtraveller. #blogger

     (button) follow
   [25]towards data science

[26]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 187
     * (button)
     *
     *

   [27]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [28]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5ae8d994bf0e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/data-science-simplified-part-11-logistic-regression-5ae8d994bf0e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/data-science-simplified-part-11-logistic-regression-5ae8d994bf0e&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_0zchk3jfrmbu---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@rpradeepmenon?source=post_header_lockup
  17. https://towardsdatascience.com/@rpradeepmenon
  18. https://datascientia.blog/2017/09/18/dss-p10-class-models/
  19. https://datascientia.blog/2017/10/02/data-science-simplified-part-11-logistic-regression/
  20. https://towardsdatascience.com/tagged/machine-learning?source=post
  21. https://towardsdatascience.com/tagged/towards-data-science?source=post
  22. https://towardsdatascience.com/tagged/data-science?source=post
  23. https://towardsdatascience.com/@rpradeepmenon?source=footer_card
  24. https://towardsdatascience.com/@rpradeepmenon
  25. https://towardsdatascience.com/?source=footer_card
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/
  28. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  30. https://medium.com/p/5ae8d994bf0e/share/twitter
  31. https://medium.com/p/5ae8d994bf0e/share/facebook
  32. https://medium.com/p/5ae8d994bf0e/share/twitter
  33. https://medium.com/p/5ae8d994bf0e/share/facebook
