    (button) toggle navigation
     * [1]learning
          + [2]tutorials
          + [3]articles
     * [4]books
     * [5]courses
          + [6]data science
          + [7]machine learning
     * [8]contribute
     * [9][fb_bu1r2s1.svg] 1.2k
     * [10][twitter_gvy5lp8.svg] 2.6k
     * [11][github-ico.svg] 123

   you are reading id31
   4shares

be notified when we release new material

   join over 3,500 data science enthusiasts.
   ____________________
   ____________________
   subscribe

featured courses:

   [12]

machine learning by andrew ng, coursera

   [13]

data science (r) by johns hopkins, coursera

   [14]

applied data science with python, coursera

   [15]

dataquest

featured books:

   [16]

introduction to statistical learning (islr)

   [17]

data science from scratch

   [18]

agile data science 2.0

recent articles:

   [19]

top 5 machine learning courses for 2019 - learn machine learning

   [20]

top 7 online data science courses for 2019 - learn data science

   [21]

beginner's guide to using databases with python: postgres, sqlalchemy, and
alembic

   [22]

most recommended data science and machine learning books by top master's
programs

   reddit-sentiment-naive-bayes.jpg
   brendan martin
   author: [23]brendan martin
   founder of learndatasci
   data scientist author photo
   author: [24]nikos koufos
   cs & engineering post graduate

predicting reddit news sentiment with naive bayes and other text classifiers

   learn how to predict the sentiment of news headlines mined from reddit
   [lightbulb-brain.svg]

before

   tutorial

you should already know:

     * python fundamentals
     * pandas and matplotlib
     * basics of id31
     * basic machine learning concepts

   learn each interactively with [25]datacamp

   in our previous post, we covered some of the [26]basics of sentiment
   analysis, where we gathered and categorize political headlines. now, we
   can use that data to train a binary classifier to predict if a headline
   is positive or negative.

article resources

     * notebook: [27]github
     * libraries: pandas, numpy, scikit-learn, matplotlib, seaborn, nltk,
       imblearn

brief intro using classification and some problems we face

   classification is the process of identifying the category of a new,
   unseen observation based of a training set of data, which has
   categories that are known.

   in our case, our headlines are the observations and the
   positive/negative sentiment are the categories. this is a binary
   classification problem -- we're trying to predict if a headline is
   either positive or negative.

first problem: imbalanced dataset

   one of the most common problems, in machine learning, is working with
   an imbalanced dataset. as we'll see below, we have a slightly
   imbalanced dataset, where there's more negatives than positives.

   compared to some problems, like fraud detection, our dataset isn't
   super imbalanced. sometimes you'll have datasets where the positive
   class is only 1% of the training data, the rest being negatives.

   we want to be careful with interpreting results from imbalanced data.
   when producing scores with our classifier, you may experience accuracy
   up to 90%, which is commonly known as the [28]accuracy paradox.

   the reason why we might have 90% accuracy is due to our model examining
   the data and deciding to always predict negative, resulting in high
   accuracy.

   there's a number of ways to counter this problem, such as::
     * collect more data: could help balance the dataset by adding more
       minor class examples.
     * change you metric: use either the confusion matrix, precision,
       recall or f1 score (combination of precision and recall).
     * oversample the data: randomly sample the attributes from examples
       in the minority class to create more 'fake' data.
     * penalized model: implements an additional cost on the model for
       making classification mistakes on the minority class during
       training. these penalties bias the model towards the minority
       class.

   in our dataset, we have less positive examples than negative examples,
   and we will explore both different metrics and utilizing an
   oversampling technique, called smote.

   let's establish a few basic imports:
import math
import random
from collections import defaultdict
from pprint import pprint

# prevent future/deprecation warnings from showing in output
import warnings
warnings.filterwarnings(action='ignore')

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# set global styles for plots
sns.set_style(style='white')
sns.set_context(context='notebook', font_scale=1.3, rc={'figure.figsize': (16,9)
})


   these are basic imports used across the entire notebook, and are
   usually imported in every data science project. the more specific
   imports from sklearn and other libraries will be brought up when we use
   them.

loading the dataset

   first let's load the dataset that we created in the last article:
df = pd.read_csv('reddit_headlines_labels.csv', encoding='utf-8')
df.head()


                         headline                      label
   0 gillespie victory in virginia would vindicate ... 0
   1 screw ron paul and all of his "if he can't aff... -1
   2 corker: trump, 'perfectly fine,' with scrappin... 1
   3 concerning recent changes in allowed domains      0
   4 trump confidantes bossie, lewandowski urge aga... -1

   now that we have the dataset in a dataframe, let's remove the neutral
   (0) headlines labels so we can focus on only classifying positive or
   negative:
df = df[df.label != 0]
df.label.value_counts()


-1    758
 1    496
name: label, dtype: int64


   our dataframe now only contains positive and negative examples, and
   we've confirmed again that we have more negatives than positives.

   let's move into featurization of the headlines.

want to learn more?

   see best data science courses of 2019
   [29]view

transform headlines into features

   in order to train our classifier, we need to transform our headlines of
   words into numbers, since algorithms only know how to work with
   numbers.

   to do this transformation, we're going to use countvectorizer from
   sklearn. this is a very straightforward class for converting words into
   features.

   unlike in the last tutorial where we manually tokenized and lowercased
   the text, countvectorizer will handle this step for us. all we need to
   do is pass it the headlines.

   let's work with a tiny example to show how vectorizing words into
   numbers works:
from sklearn.feature_extraction.text import countvectorizer

s1 = "senate panel moving ahead with mueller bill despite mcconnell opposition"
s2 = "bill protecting robert mueller to get vote despite mcconnell opposition"

vect = countvectorizer(binary=true)
x = vect.fit_transform([s1, s2])

x.toarray()


array([[1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1],
       [0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]], dtype=int64)


   what we've done here is take two headlines about a similar topic and
   vectorized them.

   vect is set up with default params to tokenize and lowercase words. on
   top of that, we have set binary=true so we get an output of 0 (word
   doesn't exist in that sentence) or 1 (word exists in that sentence).

   vect builds a vocabulary from all the words it sees in all the text you
   give it, then assigns a 0 or 1 if that word exists in the current
   sentence. to see this more clearly, let's check out the feature names
   mapped to the first sentence:
list(zip(x.toarray()[0], vect.get_feature_names()))


[(1, 'ahead'), (1, 'bill'), (1, 'despite'), (0, 'get'), (1, 'mcconnell'), (1, 'm
oving'), (1, 'mueller'), (1, 'opposition'), (1, 'panel'), (0, 'protecting'), (0,
 'robert'), (1, 'senate'), (0, 'to'), (0, 'vote'), (1, 'with')]


   this is the vectorization mapping of the first sentence. you can see
   that there's a 1 mapped to 'ahead' because 'ahead' shows up in s1. but
   if we look at s2:
list(zip(x.toarray()[1], vect.get_feature_names()))


[(0, 'ahead'), (1, 'bill'), (1, 'despite'), (1, 'get'), (1, 'mcconnell'), (0, 'm
oving'), (1, 'mueller'), (1, 'opposition'), (0, 'panel'), (1, 'protecting'), (1,
 'robert'), (0, 'senate'), (1, 'to'), (1, 'vote'), (0, 'with')]


   there's a 0 at 'ahead' since that word doesn't show up in s2. but
   notice that each row contains every word seen so far.

   when we expand this to all of the headlines in the dataset, this
   vocabulary will grow by a lot. each mapping like the one printed above
   will end up being the length of all words the vectorizer encounters.

   let's now apply the vectorizer to all of our headlines:
vect = countvectorizer(max_features=1000, binary=true)
x = vect.fit_transform(df.headline)

x.toarray()


array([[0, 0, 0, ..., 0, 1, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)


   notice that the vectorizer by default stores everything in a sparse
   array, and using x.toarray()shows us the dense version. a sparse array
   is much more efficient since most values in each row are 0. in other
   words, most headlines are only a dozen or so words and each row
   contains every word ever seen, and sparse arrays only store the
   non-zero value indices.

   you'll also notice that we have a new keyword argument; max_features.
   this is essentially the number of words to consider, ranked by
   frequency. so the 1000 value means we only want to look at the 1000
   most common words as features.

   now that we know how vectorization works, let's use it in action.

preparing for training

   before training, and even vectorizing, let's split our data into
   training and testing sets. it's important to do this before doing
   anything with the data so we have a fresh test set.
from sklearn.model_selection import train_test_split

x = df.headline
y = df.label

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)


   our test size is 0.2, or 20%. this means that x_test and y_test
   contains 20% of our data which we reserve for testing.

   let's now fit the vectorizer on the training set only and perform the
   vectorization.

   just to reiterate, it's important to not fit the vectorizer on all of
   the data since we want a clean test set for evaluating performance.
   fitting the vectorizer on everything would result in data leakage,
   causing unreliable results since the vectorizer shouldn't know about
   future data.

   we can fit the vectorizer and transform x_train in one step:
from sklearn.feature_extraction.text import countvectorizer

vect = countvectorizer(max_features=1000, binary=true)

x_train_vect = vect.fit_transform(x_train)


   x_train_vect is now transformed into the right format to give to the
   naive bayes model, but let's first look into balancing the data.

balancing the data

   it seems that there may be a lot more negative headlines than positive
   headlines (id48), and so we have a lot more negative labels than
   positive labels.
counts = df.label.value_counts()
print(counts)

print("\npredicting only -1 = {:.2f}% accuracy".format(counts[-1] / sum(counts)
* 100))


-1    758
 1    496
name: label, dtype: int64

predicting only -1 = 60.45% accuracy


   we can see from above, we have slightly more negatives than positives,
   making our dataset slightly imbalanced.

   by calculating if our model only chose to predict -1, the larger class,
   we would get a ~60% accuracy. this means that in our binary
   classification model, where random chance is 50%, a 60% accuracy
   wouldn't tell us much. we would definitely want to look at precision
   and recall more than accuracy.

   we can balance our data by using a form of oversampling called smote.
   smote looks at the minor class, positives in our case, and creates new,
   synthetic training examples. read more about the algorithm [30]here.

   note: we have to make sure we only oversample the train data so we
   don't leak any information to the test set.

   let's perform smote with the imblearn library:
from imblearn.over_sampling import smote

sm = smote()

x_train_res, y_train_res = sm.fit_sample(x_train_vect, y_train)


unique, counts = np.unique(y_train_res, return_counts=true)
print(list(zip(unique, counts)))


[(-1, 601), (1, 601)]


   the classes are now balanced for the train set. we can move onto
   training a naive bayes model.

naive bayes

   for our first algorithm, we're going to use the extremely fast and
   versatile naive bayes model.

   let's instantiate one from sklearn and fit it to our training data:
from sklearn.naive_bayes import multinomialnb

nb = multinomialnb()

nb.fit(x_train_res, y_train_res)

nb.score(x_train_res, y_train_res)


0.9201331114808652


   naive bayes has successfully fit all of our training data and is ready
   to make predictions. you'll notice that we have a score of ~92%. this
   is the fit score, and not the actual accuracy score. you'll see next
   that we need to use our test set in order to get a good estimate of
   accuracy.

   let's vectorize the test set, then use that test set to predict if each
   test headline is either positive or negative. since we're avoiding any
   data leakage, we are only transforming, not refitting. and we won't be
   using smote to oversample either.
x_test_vect = vect.transform(x_test)

y_pred = nb.predict(x_test_vect)

y_pred


array([-1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,
        1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1, -1,  1, -1,  1,  1,
       -1, -1,  1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1, -1,  1, -1,  1,
        1,  1,  1,  1, -1, -1,  1,  1, -1, -1, -1,  1,  1,  1,  1, -1, -1,
       -1, -1,  1, -1,  1, -1, -1, -1, -1,  1, -1,  1,  1, -1, -1, -1, -1,
       -1, -1, -1, -1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1, -1,  1,
        1,  1,  1,  1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1,  1,
       -1, -1, -1, -1, -1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,
       -1, -1,  1, -1, -1, -1,  1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1,
       -1,  1, -1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1, -1,
        1, -1, -1, -1, -1,  1, -1,  1,  1,  1,  1, -1, -1, -1,  1, -1, -1,
       -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1,  1,
       -1,  1, -1, -1,  1, -1, -1,  1, -1, -1,  1, -1,  1, -1, -1, -1, -1,
       -1,  1,  1, -1,  1, -1, -1,  1,  1,  1, -1, -1,  1, -1,  1,  1, -1,
       -1, -1,  1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1], dtype=int64)


   y_pred now contains a prediction for every row of the test set. with
   this prediction result, we can pass it into an sklearn metric with the
   true labels to get an accuracy score, f1 score, and generate a
   confusion matrix:
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

print("accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred) * 100))
print("\nf1 score: {:.2f}".format(f1_score(y_test, y_pred) * 100))
print("\nconfusion matrix:\n", confusion_matrix(y_test, y_pred))


accuracy: 74.50%

f1 score: 68.93

confusion matrix:
 [[116  41]
 [ 23  71]]


   we can see that our model has predicted the sentiment of headlines with
   a 75% accuracy, but looking at the confusion matrix we can see it's not
   doing that great of a job classifying.

   for a breakdown of the confusion matrix, we have:
     * 116 predicted negative (-1), and was negative (-1). true negative.
     * 71 predicted positive (+1), and was positive (+1). true positive.
     * 23 predicted negative (-1), but was positive (+1). false negative.
     * 41 predicted positive (+1), but was negative (-1). false positive.

   so our classifier is getting a lot of the negatives right, but there's
   a high number of false predictions. we'll see if we can improve these
   metrics with other classifiers below.

cross validation

   lets now utilize cross validation, where we generate a training and
   testing set 10 different times on the same data in different positions.

   right now, we are set up with the usual 80% of the data as training and
   20% as the test. the accuracy of prediction on a single test set
   doesn't say much about generalization. to get a better insight on our
   classifier   s generalization capabilities, there's two different
   techniques we can use:

   1) k-fold cross-validation: the examples are randomly partitioned into
   kk equal sized subsets (usually 10). out of the kk subsets, a single
   subsample is used for testing the model and the remaining k   1k   1
   subsets are used as training data. the cross-validation technique is
   then repeated kk times, resulting in process where each subset is used
   exactly once as part of the test set. finally, the average of the
   kk-runs is computed. the advantage of this method is that every example
   is used in both train and test set.

   2) monte carlo cross-validation: randomly splits the dataset into train
   and test data, the model is run, and the results are then averaged. the
   advantage of this method is that the proportion of the train/test split
   is not dependent on the number of iterations, which is useful for very
   large datasets. on the other hand, the disadvantage of this method if
   you're not running through enough iterations is that some examples may
   never be selected in the test subset, whereas others may be selected
   more than once.

   for an even better explanation of the differences between these two
   methods, check out this answer:
   [31]https://stats.stackexchange.com/a/60967

   the relevant class from the sklearn library is shufflesplit. this
   performs a shuffle first and then a split of the data into train/test.
   since it's an iterator, it will perform a random shuffle and split for
   each iteration. this is an example of the monte carlo method mentioned
   above.

   normally, we could just use sklearn.model_selection.cross_val_score
   which automatically calculates a score for each fold, but we're going
   to show the manual splitting with shufflesplit.

   also, if you're familiar with cross_val_score you'll notice that
   shufflesplit works differently. the n_splits parameter in shufflesplit
   is the number of times to randomize the data and then split it 80/20,
   whereas the cv parameter in cross_val_score is the number of folds. by
   using a large n_splits, we can get a good approximation of the true
   performance on larger datasets, but it's harder to plot.
from sklearn.model_selection import shufflesplit

x = df.headline
y = df.label

ss = shufflesplit(n_splits=10, test_size=0.2)
sm = smote()

accs = []
f1s = []
cms = []

for train_index, test_index in ss.split(x):

    x_train, x_test = x.iloc[train_index], x.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # fit vectorizer and transform x train, then transform x test
    x_train_vect = vect.fit_transform(x_train)
    x_test_vect = vect.transform(x_test)

    # oversample
    x_train_res, y_train_res = sm.fit_sample(x_train_vect, y_train)

    # fit naive bayes on the vectorized x with y train labels,
    # then predict new y labels using x test
    nb.fit(x_train_res, y_train_res)
    y_pred = nb.predict(x_test_vect)

    # determine test set accuracy and f1 score on this fold using the true y lab
els and predicted y labels
    accs.append(accuracy_score(y_test, y_pred))
    f1s.append(f1_score(y_test, y_pred))
    cms.append(confusion_matrix(y_test, y_pred))

print("\naverage accuracy across folds: {:.2f}%".format(sum(accs) / len(accs) *
100))
print("\naverage f1 score across folds: {:.2f}%".format(sum(f1s) / len(f1s) * 10
0))
print("\naverage confusion matrix across folds: \n {}".format(sum(cms) / len(cms
)))


average accuracy across folds: 72.95%

average f1 score across folds: 66.43%

average confusion matrix across folds:
 [[115.6  39. ]
 [ 28.9  67.5]]


   looks like the average accuracy and f1 score are both similar to what
   we saw on a single fold above.

let's plot our results

fig, (ax1, ax2) = plt.subplots(2, 1, sharex=true, figsize=(16,9))

acc_scores = [round(a * 100, 1) for a in accs]
f1_scores = [round(f * 100, 2) for f in f1s]

x1 = np.arange(len(acc_scores))
x2 = np.arange(len(f1_scores))

ax1.bar(x1, acc_scores)
ax2.bar(x2, f1_scores, color='#559ebf')

# place values on top of bars
for i, v in enumerate(list(zip(acc_scores, f1_scores))):
    ax1.text(i - 0.25, v[0] + 2, str(v[0]) + '%')
    ax2.text(i - 0.25, v[1] + 2, str(v[1]))

ax1.set_ylabel('accuracy (%)')
ax1.set_title('naive bayes')
ax1.set_ylim([0, 100])

ax2.set_ylabel('f1 score')
ax2.set_xlabel('runs')
ax2.set_ylim([0, 100])

sns.despine(bottom=true, left=true)  # remove the ticks on axes for cleaner pres
entation

plt.show()


   result:
   naive bayes runs f1 and accuracy scores

   the f1 score fluctuates greater than 15 points between some runs, which
   could be remedied with a larger dataset. let's see how other algorithms
   do.

other classification algorithms in scikit-learn

   as you can see naive bayes performed pretty well, so let   s experiment
   with other classifiers.

   we'll use the same shuffle splitting as before, but now we'll run
   several types of models in each loop:
from sklearn.naive_bayes import bernoullinb
from sklearn.linear_model import logisticregression, sgdclassifier
from sklearn.id166 import linearsvc
from sklearn.ensemble import randomforestclassifier
from sklearn.neural_network import mlpclassifier

x = df.headline
y = df.label

cv = shufflesplit(n_splits=20, test_size=0.2)

models = [
    multinomialnb(),
    bernoullinb(),
    logisticregression(),
    sgdclassifier(),
    linearsvc(),
    randomforestclassifier(),
    mlpclassifier()
]

sm = smote()

# init a dictionary for storing results of each run for each model
results = {
    model.__class__.__name__: {
        'accuracy': [],
        'f1_score': [],
        'confusion_matrix': []
    } for model in models
}

for train_index, test_index in cv.split(x):
    x_train, x_test  = x.iloc[train_index], x.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    x_train_vect = vect.fit_transform(x_train)
    x_test_vect = vect.transform(x_test)

    x_train_res, y_train_res = sm.fit_sample(x_train_vect, y_train)

    for model in models:
        model.fit(x_train_res, y_train_res)
        y_pred = model.predict(x_test_vect)

        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)

        results[model.__class__.__name__]['accuracy'].append(acc)
        results[model.__class__.__name__]['f1_score'].append(f1)
        results[model.__class__.__name__]['confusion_matrix'].append(cm)


   we now have a bunch of accuracy scores, f1 scores, and confusion
   matrices stored for each model. let's average these together to get
   average scores across models and folds:
for model, d in results.items():
    avg_acc = sum(d['accuracy']) / len(d['accuracy']) * 100
    avg_f1 = sum(d['f1_score']) / len(d['f1_score']) * 100
    avg_cm = sum(d['confusion_matrix']) / len(d['confusion_matrix'])

    slashes = '-' * 30

    s = f"""{model}\n{slashes}
        avg. accuracy: {avg_acc:.2f}%
        avg. f1 score: {avg_f1:.2f}
        avg. confusion matrix:
        \n{avg_cm}
        """
    print(s)


multinomialnb
------------------------------
        avg. accuracy: 74.70%
        avg. f1 score: 69.63
        avg. confusion matrix:

[[114.05  36.4 ]
 [ 27.1   73.45]]

bernoullinb
------------------------------
        avg. accuracy: 75.32%
        avg. f1 score: 67.96
        avg. confusion matrix:

[[122.75  27.7 ]
 [ 34.25  66.3 ]]

logisticregression
------------------------------
        avg. accuracy: 74.80%
        avg. f1 score: 68.31
        avg. confusion matrix:

[[119.2   31.25]
 [ 32.    68.55]]

sgdclassifier
------------------------------
        avg. accuracy: 71.75%
        avg. f1 score: 65.31
        avg. confusion matrix:

[[112.6   37.85]
 [ 33.05  67.5 ]]

linearsvc
------------------------------
        avg. accuracy: 73.01%
        avg. f1 score: 66.61
        avg. confusion matrix:

[[115.55  34.9 ]
 [ 32.85  67.7 ]]

randomforestclassifier
------------------------------
        avg. accuracy: 69.64%
        avg. f1 score: 52.74
        avg. confusion matrix:

[[132.    18.45]
 [ 57.75  42.8 ]]

mlpclassifier
------------------------------
        avg. accuracy: 74.14%
        avg. f1 score: 67.43
        avg. confusion matrix:

[[118.75  31.7 ]
 [ 33.2   67.35]]


   we've gotten some pretty decent results, but overall it looks like we
   need more data to be sure which one performs the best.

   since we're only running metrics on a test set size of about 300
   examples, a 0.5% difference in accuracy would mean only ~2 more
   examples are classified correctly versus the other model(s). if we had
   a test set of 10,000, a 0.5% difference in accuracy would equal 50 more
   correctly classified headlines, which is much more reassuring.

   the difference between id79 and multinomial naive bayes is
   quite clear, but the difference between multinomial and bernoulli naive
   bayes isn't. to compare these two further, we need more data.

   let's see if ensembling can make a better difference.

esembling classifiers

   after we evaluated each classifier individually, let's see if
   ensembling helps improve our metrics.

   we're going to use sklearn's votingclassifier which defaults to a
   majority rule voting.
from sklearn.ensemble import votingclassifier

x = df.headline
y = df.label

cv = shufflesplit(n_splits=10, test_size=0.2)

models = [
    multinomialnb(),
    bernoullinb(),
    logisticregression(),
    sgdclassifier(),
    linearsvc(),
    randomforestclassifier(),
    mlpclassifier()
]

m_names = [m.__class__.__name__ for m in models]

models = list(zip(m_names, models))
vc = votingclassifier(estimators=models)

sm = smote()

# no need for dictionary now
accs = []
f1s = []
cms = []

for train_index, test_index in cv.split(x):
    x_train, x_test  = x.iloc[train_index], x.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    x_train_vect = vect.fit_transform(x_train)
    x_test_vect = vect.transform(x_test)

    x_train_res, y_train_res = sm.fit_sample(x_train_vect, y_train)

    vc.fit(x_train_res, y_train_res)

    y_pred = vc.predict(x_test_vect)

    accs.append(accuracy_score(y_test, y_pred))
    f1s.append(f1_score(y_test, y_pred))
    cms.append(confusion_matrix(y_test, y_pred))


print("voting classifier")
print("-" * 30)
print("avg. accuracy: {:.2f}%".format(sum(accs) / len(accs) * 100))
print("avg. f1 score: {:.2f}".format(sum(f1s) / len(f1s) * 100))
print("confusion matrix:\n", sum(cms) / len(cms))


voting classifier
------------------------------
avg. accuracy: 75.78%
avg. f1 score: 68.51
confusion matrix:
 [[123.7  28.7]
 [ 32.1  66.5]]


   although our majority classifier performed great, it didn't differ much
   from the results we got from multinomial naive bayes, which might have
   been suprising. surely mashing a bunch together would give better
   results, but this lack of difference in performance proves that there's
   still a lot of areas that need to be explored. for example:
     * how more data affects performance (best place to start due to our
       small dataset)
     * grid searching different parameters for each model
     * debugging the ensemble by looking at model correlations
     * trying different styles of [32]id112, boosting, and stacking

final words and where to go from here

   so far we've
     * mined data from reddit's /r/politics
     * obtained sentiment scores for headlines
     * vectorized the data
     * run the data through several types of models
     * ensembled models together

   unfortunately, there isn't an obvious winning model. there's a couple
   we've seen that definitely perform poorly, but there's a few that hover
   around the same accuracy. additionally, the confusion matrices are
   showing roughly half of the positive headlines are being misclassified,
   so there's a lot more work to be done.

   now that you've seen how this pipeline works, there's a lot of room for
   improvement on the architecture of the code and modeling. i encourage
   you to try all of this out in the provided notebook. see what other
   subreddits you can tap into for sentiment, like stocks, companies,
   products, etc.. there's a lot of valuable data to be had!

help us make this article and series better

   if you're interested in the expansion of this article and series into
   some of these areas of exploration, drop a comment below and we'll add
   it to the content pipeline.

   thanks for reading!

course recommendations

further learning:

   [33]

natural language processing     coursera

   the best nlp course i've enrolled in. learn basic to advanced sentiment
   analysis, text classification, id38, and more.
     __________________________________________________________________

be notified when we release new material

   join over 3,500 data science enthusiasts.
   ____________________
   ____________________
   subscribe
     __________________________________________________________________

meet the authors

   brendan martin
   brendan martin founder of learndatasci

   author and editor at learndatasci. python development and data science
   consultant.

   data scientist author photo
   nikos koufos cs & engineering post graduate

   learndatasci author, postgraduate in computer science & engineering at
   the university ioannina, greece, and computer science undergraduate
   teaching assistant.
     __________________________________________________________________

   [34]back to blog index

   please enable javascript to view the [35]comments powered by disqus.

     * flattened-logo-ready-for-export

     * [36]best udemy data science courses
     * [37]contact

     * [38]100+ free data science books
     * [39]privacy policy

   copyright    2019 learndatasci. all rights reserved.

be notified when we release new material

   join over 3,500 data science enthusiasts.
   ____________________
   ____________________
   subscribe

references

   visible links
   1. https://www.learndatasci.com/tutorials/predicting-reddit-news-sentiment-naive-bayes-text-classifiers/
   2. https://www.learndatasci.com/tutorials/
   3. https://www.learndatasci.com/articles/
   4. https://www.learndatasci.com/books/
   5. https://www.learndatasci.com/tutorials/predicting-reddit-news-sentiment-naive-bayes-text-classifiers/
   6. https://www.learndatasci.com/best-data-science-online-courses/
   7. https://www.learndatasci.com/best-machine-learning-courses/
   8. https://www.learndatasci.com/publish/
   9. https://facebook.com/learndatasci
  10. http://twitter.com/learndatasci
  11. https://github.com/learndatasci
  12. https://www.learndatasci.com/out/coursera-machine-learning/
  13. https://www.learndatasci.com/out/coursera-data-science-specialization/
  14. https://www.learndatasci.com/out/coursera-applied-data-science-with-python/
  15. https://www.learndatasci.com/articles/review-dataquests-data-science-python-program/
  16. https://www.learndatasci.com/out/amazon-introduction-statistical-learning/
  17. https://www.learndatasci.com/out/data-science-from-scratch/
  18. https://www.learndatasci.com/out/agile-data-science-20/
  19. https://www.learndatasci.com/best-machine-learning-courses/
  20. https://www.learndatasci.com/best-data-science-online-courses/
  21. https://www.learndatasci.com/tutorials/using-databases-python-postgres-sqlalchemy-and-alembic/
  22. https://www.learndatasci.com/articles/data-science-machine-learning-books-masters/
  23. https://www.learndatasci.com/tutorials/predicting-reddit-news-sentiment-naive-bayes-text-classifiers/
  24. https://www.learndatasci.com/tutorials/predicting-reddit-news-sentiment-naive-bayes-text-classifiers/
  25. https://www.learndatasci.com/out/datacamp/
  26. http://localhost:8888/sentiment-analysis-reddit-headlines-pythons-nltk/
  27. https://github.com/learndatasci/article-resources/tree/master/predicting reddit news sentiment
  28. https://en.wikipedia.org/wiki/accuracy_paradox
  29. https://www.learndatasci.com/best-data-science-online-courses?utm_source=predicting reddit news sentiment with naive bayes article&utm_medium=html block&utm_campaign=best data science courses page&utm_content=see best data science courses of 2019
  30. https://www.jair.org/media/953/live-953-2037-jair.pdf
  31. https://stats.stackexchange.com/a/60967
  32. https://stats.stackexchange.com/a/19053
  33. https://www.learndatasci.com/out/coursera-advanced-ml-natural-language-processing/
  34. https://www.learndatasci.com/tutorials/
  35. https://disqus.com/?ref_noscript
  36. https://www.learndatasci.com/top-udemy-data-science-courses
  37. https://www.learndatasci.com/contact-us
  38. https://www.learndatasci.com/free-data-science-books
  39. https://www.learndatasci.com/privacy-policy

   hidden links:
  41. https://www.learndatasci.com/
  42. https://www.linkedin.com/sharearticle?mini=true&url=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fpredicting-reddit-news-sentiment-naive-bayes-text-classifiers%2f&summary=predicting%20reddit%20news%20sentiment%20with%20naive%20bayes%20and%20other%20text%20classifiers
  43. http://twitter.com/share?text=predicting%20reddit%20news%20sentiment%20with%20naive%20bayes%20and%20other%20text%20classifiers&url=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fpredicting-reddit-news-sentiment-naive-bayes-text-classifiers%2f
  44. https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fpredicting-reddit-news-sentiment-naive-bayes-text-classifiers%2f
  45. http://www.reddit.com/submit?url=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fpredicting-reddit-news-sentiment-naive-bayes-text-classifiers%2f&title=predicting%20reddit%20news%20sentiment%20with%20naive%20bayes%20and%20other%20text%20classifiers
  46. https://news.ycombinator.com/submitlink?uhttps%3a%2f%2fwww.learndatasci.com%2ftutorials%2fpredicting-reddit-news-sentiment-naive-bayes-text-classifiers%2f&t=learndatasci
