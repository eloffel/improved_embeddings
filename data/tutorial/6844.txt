   #[1]github [2]recent commits to models:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]2,874
     * [35]star [36]50,805
     * [37]fork [38]31,195

[39]tensorflow/[40]models

   [41]code [42]issues 1,174 [43]pull requests 360 [44]projects 2
   [45]insights
   branch: master
   (button) create new file [46]find file [47]history
   [48]models/[49]research/textsum/
   xin pan xin pan
   xin pan and xin pan [50]remove contacts of exgoogler
   latest commit [51]ba72997 jan 31, 2018
   [52]permalink
   type name latest commit message commit time
   [53]..
   failed to load latest commit information.
   [54]data
   [55]build [56]move the research models into a research subfolder
   ([57]#2430[58]) sep 21, 2017
   [59]readme.md
   [60]batch_reader.py
   [61]beam_search.py
   [62]data.py [63]move the research models into a research subfolder
   ([64]#2430[65]) sep 21, 2017
   [66]data_convert_example.py
   [67]id195_attention.py
   [68]id195_attention_decode.py
   [69]id195_attention_model.py
   [70]id195_lib.py [71]move the research models into a research
   subfolder ([72]#2430[73]) sep 21, 2017

readme.md

   sequence-to-sequence with attention model for text summarization.

   authors:

   xin pan peter liu ([74]peterjliu@google.com, github:peterjliu)

   introduction

   the core model is the traditional sequence-to-sequence model with
   attention. it is customized (mostly inputs/outputs) for the text
   summarization task. the model has been trained on gigaword dataset and
   achieved state-of-the-art results (as of june 2016).

   the results described below are based on model trained on multi-gpu and
   multi-machine settings. it has been simplified to run on only one
   machine for open source purpose.

   dataset

   we used the gigaword dataset described in [75]rush et al. a neural
   attention model for sentence summarization.

   we cannot provide the dataset due to the license. see examplegen in
   data.py about the data format. data/data contains a toy example. also
   see data/vocab for example vocabulary format. in how to run below,
   users can use toy data and vocab provided in the data/ directory to run
   the training by replacing the data directory flag.

   data_convert_example.py contains example of convert between binary and
   text.

   experiment result

   8000 examples from testset are sampled to generate summaries and id8
   score is calculated for the generated summaries. here is the best id8
   score on gigaword dataset:

   id8-1 average_r: 0.38272 (95%-conf.int. 0.37774 - 0.38755)

   id8-1 average_p: 0.50154 (95%-conf.int. 0.49509 - 0.50780)

   id8-1 average_f: 0.42568 (95%-conf.int. 0.42016 - 0.43099)

   id8-2 average_r: 0.20576 (95%-conf.int. 0.20060 - 0.21112)

   id8-2 average_p: 0.27565 (95%-conf.int. 0.26851 - 0.28257)

   id8-2 average_f: 0.23126 (95%-conf.int. 0.22539 - 0.23708)

   configuration:

   following is the configuration for the best trained model on gigaword:

   batch_size: 64

   bidirectional encoding layer: 4

   article length: first 2 sentences, total words within 120.

   summary length: total words within 30.

   id27 size: 128

   lstm hidden units: 256

   sampled softmax: 4096

   vocabulary size: most frequent 200k words from dataset's article and
   summaries.

   how to run

   prerequisite: install tensorflow and bazel.
# cd to your workspace
# 1. clone the textsum code to your workspace 'textsum' directory.
# 2. create an empty 'workspace' file in your workspace.
# 3. move the train/eval/test data to your workspace 'data' directory.
#    in the following example, i named the data training-*, test-*, etc.
#    if your data files have different names, update the --data_path.
#    if you don't have data but want to try out the model, copy the toy
#    data from the textsum/data/data to the data/ directory in the workspace.
$ ls -r
.:
data  textsum  workspace

./data:
vocab  test-0  training-0  training-1  validation-0 ...(omitted)

./textsum:
batch_reader.py       beam_search.py       build    readme.md
 id195_attention_model.py  data
data.py  id195_attention_decode.py  id195_attention.py        id195_lib.py

./textsum/data:
data  vocab

$ bazel build -c opt --config=cuda textsum/...

# run the training.
$ bazel-bin/textsum/id195_attention \
    --mode=train \
    --article_key=article \
    --abstract_key=abstract \
    --data_path=data/training-* \
    --vocab_path=data/vocab \
    --log_root=textsum/log_root \
    --train_dir=textsum/log_root/train

# run the eval. try to avoid running on the same machine as training.
$ bazel-bin/textsum/id195_attention \
    --mode=eval \
    --article_key=article \
    --abstract_key=abstract \
    --data_path=data/validation-* \
    --vocab_path=data/vocab \
    --log_root=textsum/log_root \
    --eval_dir=textsum/log_root/eval

# run the decode. run it when the model is mostly converged.
$ bazel-bin/textsum/id195_attention \
    --mode=decode \
    --article_key=article \
    --abstract_key=abstract \
    --data_path=data/test-* \
    --vocab_path=data/vocab \
    --log_root=textsum/log_root \
    --decode_dir=textsum/log_root/decode \
    --beam_size=8

   examples:

   the following are some text summarization examples, including
   experiments using dataset other than gigaword.

   article: novell inc. chief executive officer eric schmidt has been
   named chairman of the internet search-engine company google .

   human: novell ceo named google chairman

   machine: novell chief executive named to head internet company

   ======================================

   article: gulf newspapers voiced skepticism thursday over whether newly
   re - elected us president bill clinton could help revive the troubled
   middle east peace process but saw a glimmer of hope .

   human: gulf skeptical about whether clinton will revive peace process

   machine: gulf press skeptical over clinton 's prospects for peace
   process

   ======================================

   article: the european court of justice ( ecj ) recently ruled in lock v
   british gas trading ltd that eu law requires a worker 's statutory
   holiday pay to take commission payments into account - it should not be
   based solely on basic salary . the case is not over yet , but its
   outcome could potentially be costly for employers with workers who are
   entitled to commission . mr lock , an energy salesman for british gas ,
   was paid a basic salary and sales commission on a monthly basis . his
   sales commission made up around 60 % of his remuneration package . when
   he took two weeks ' annual leave in december 2012 , he was paid his
   basic salary and also received commission from previous sales that fell
   due during that period . lock obviously did not generate new sales
   while he was on holiday , which meant that in the following period he
   suffered a reduced income through lack of commission . he brought an
   employment tribunal claim asserting that this amounted to a breach of
   the working time regulations 1998 .....deleted rest for readability...

   abstract: will british gas ecj ruling fuel holiday pay hike ?

   decode: eu law requires worker 's statutory holiday pay

   ======================================

   article: the junior all whites have been eliminated from the fifa u -
   20 world cup in colombia with results on the final day of pool play
   confirming their exit . sitting on two points , new zealand needed
   results in one of the final two groups to go their way to join the last
   16 as one of the four best third place teams . but while spain helped
   the kiwis ' cause with a 5 - 1 thrashing of australia , a 3 - 0 win for
   ecuador over costa rica saw the south americans climb to second in
   group c with costa rica 's three points also good enough to progress in
   third place . that left the junior all whites hopes hanging on the
   group d encounter between croatia and honduras finishing in a draw . a
   stalemate - and a place in the knockout stages for new zealand -
   appeared on the cards until midfielder marvin ceballos netted an 81st
   minute winner that sent guatemala through to the second round and left
   the junior all whites packing their bags . new zealand finishes the 24
   - nation tournament in 17th place , having claimed their first ever
   points at this level in just their second appearance at the finals .

   abstract: junior all whites exit world cup

   decoded: junior all whites eliminated from u- 20 world cup

     *    2019 github, inc.
     * [76]terms
     * [77]privacy
     * [78]security
     * [79]status
     * [80]help

     * [81]contact github
     * [82]pricing
     * [83]api
     * [84]training
     * [85]blog
     * [86]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [87]reload to refresh your
   session. you signed out in another tab or window. [88]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/tensorflow/models/commits/master.atom
   3. https://github.com/tensorflow/models/tree/master/research/textsum#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/tensorflow/models/tree/master/research/textsum
  32. https://github.com/join
  33. https://github.com/login?return_to=/tensorflow/models
  34. https://github.com/tensorflow/models/watchers
  35. https://github.com/login?return_to=/tensorflow/models
  36. https://github.com/tensorflow/models/stargazers
  37. https://github.com/login?return_to=/tensorflow/models
  38. https://github.com/tensorflow/models/network/members
  39. https://github.com/tensorflow
  40. https://github.com/tensorflow/models
  41. https://github.com/tensorflow/models
  42. https://github.com/tensorflow/models/issues
  43. https://github.com/tensorflow/models/pulls
  44. https://github.com/tensorflow/models/projects
  45. https://github.com/tensorflow/models/pulse
  46. https://github.com/tensorflow/models/find/master
  47. https://github.com/tensorflow/models/commits/master/research/textsum
  48. https://github.com/tensorflow/models
  49. https://github.com/tensorflow/models/tree/master/research
  50. https://github.com/tensorflow/models/commit/ba72997d46b3c5fd6f5998394e04224788ec33b2
  51. https://github.com/tensorflow/models/commit/ba72997d46b3c5fd6f5998394e04224788ec33b2
  52. https://github.com/tensorflow/models/tree/3f94db4e9fe079e58623bef8d605eead315aba4d/research/textsum
  53. https://github.com/tensorflow/models/tree/master/research
  54. https://github.com/tensorflow/models/tree/master/research/textsum/data
  55. https://github.com/tensorflow/models/blob/master/research/textsum/build
  56. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  57. https://github.com/tensorflow/models/pull/2430
  58. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  59. https://github.com/tensorflow/models/blob/master/research/textsum/readme.md
  60. https://github.com/tensorflow/models/blob/master/research/textsum/batch_reader.py
  61. https://github.com/tensorflow/models/blob/master/research/textsum/beam_search.py
  62. https://github.com/tensorflow/models/blob/master/research/textsum/data.py
  63. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  64. https://github.com/tensorflow/models/pull/2430
  65. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  66. https://github.com/tensorflow/models/blob/master/research/textsum/data_convert_example.py
  67. https://github.com/tensorflow/models/blob/master/research/textsum/id195_attention.py
  68. https://github.com/tensorflow/models/blob/master/research/textsum/id195_attention_decode.py
  69. https://github.com/tensorflow/models/blob/master/research/textsum/id195_attention_model.py
  70. https://github.com/tensorflow/models/blob/master/research/textsum/id195_lib.py
  71. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  72. https://github.com/tensorflow/models/pull/2430
  73. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  74. mailto:peterjliu@google.com
  75. https://arxiv.org/abs/1509.00685
  76. https://github.com/site/terms
  77. https://github.com/site/privacy
  78. https://github.com/security
  79. https://githubstatus.com/
  80. https://help.github.com/
  81. https://github.com/contact
  82. https://github.com/pricing
  83. https://developer.github.com/
  84. https://training.github.com/
  85. https://github.blog/
  86. https://github.com/about
  87. https://github.com/tensorflow/models/tree/master/research/textsum
  88. https://github.com/tensorflow/models/tree/master/research/textsum

   hidden links:
  90. https://github.com/
  91. https://github.com/tensorflow/models/tree/master/research/textsum
  92. https://github.com/tensorflow/models/tree/master/research/textsum
  93. https://github.com/tensorflow/models/tree/master/research/textsum
  94. https://github.com/
