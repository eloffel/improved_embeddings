   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

introduction to tensorflow as a computational framework

   [16]go to the profile of kasper fredenslund
   [17]kasper fredenslund (button) blockedunblock (button) followfollowing
   oct 10, 2017
   [1*3bnv1cttrhoac61dz_kkgg.jpeg]

   tensorflow is likely the most popular, and fastest growing machine
   learning framework that exists.

   with over 70000 stars on [18]github, and backing from google, it not
   only has more stars than [19]linux, but also has a ton of resources
   behind it.

   if that doesn   t peak your interest, i have no idea what will.

   if you   ve been following the [20]machine learning 101 series up to now,
   you will notice that we   ve used the [21]sklearn framework to
   [22]implement our models. however, as we begin venturing into neural
   networks, [23]deep learning, and the inner workings of some of the
   algorithms, we will start using the tensorflow framework which has the
   capability to access more low-level apis to give us a more nuanced
   control over the model.

   because of this, we will spend some time familiarizing ourselves with
   tensorflow, and its design philosophy, so that we in subsequent
   tutorials can start using it without introduction.

   in this tutorial we will talk about:
     * general design philosophy
     * visualization
     * examples covering common use cases
     * how it relates to machine learning

   this essay originally appeared on [24]kasperfred.com.

   in the official [25]white-paper, tensorflow is described as    an
   interface for expressing machine learning algorithms, and an
   implementation for executing such algorithms   . its main advantage over
   other frameworks is how easy it is to execute the code on a wide array
   of devices. this is related to the initial motivation for its
   development, before it was open-sourced. google initially developed
   tensorflow to bridge the gap between research and production aspiring
   to an ideal where no edits to the code had to be made to go from
   research to production.

     tensorflow is an interface for expressing machine learning
     algorithms, and an implementation for executing such algorithms.

   to achieve this, tensorflow implements a computational graph behind the
   scenes; in your code, you   re defining just defining that graph: the
   flow of tensors.

   wait, what is a tensor?

   just like a vector can be thought of as being an array, or a list, of
   scalars (ordinary numbers like 1, 2, and pi), and matrices can be
   thought of as arrays of vectors, a tensor can be thought of as an array
   of matrices. so a tensor is really just an n-dimensional matrix. it
   turns out, as we will see in the coding examples, that this
   architecture makes a lot of sense when working with machine learning.

   what is the flow?

   the flow is how tensors are passed around in the network. when the
   tensors are passed around, their values and shapes are updated by the
   graph operations.

   as an analogy, you can think of the graph as a car factory with a
   series of workstations. one station may put on the wheels of the car
   while another installs the gearbox. the flow then describes the route a
   car skeleton has to take in order to become a fully functional car. the
   tensors passed around in this analogy would be the car prototype, or
   skeleton.

installing tensorflow

   you can install tensorflow using pip using the following command:
pip install tensorflow

   or if you have a gpu:
pip install tensorflow-gpu

   note that if you   re installing the gpu version, you need to have
   [26]cuda and [27]cudnn installed.

   as of writing this, tensorflow (v1.3) supports cuda 8 and cudnn 6.

   once you have installed tensorflow, you can verify that everything
   works correctly using:
import tensorflow as tf
# figure out what devices are available
from tensorflow.python.client import device_lib
def get_devices():
    return [x.name for x in device_lib.list_local_devices()]
print (get_devices())
['/cpu:0', '/gpu:0']

   for more information, you can refer to the [28]installation page.

the atoms of tensorflow

   we already discussed how tensorflow literally is the flow of tensors,
   but we didn   t go into much detail. in order to better justify the
   architectural decisions, we will elaborate a bit on this.

three types of tensors

   in tensorflow, there are three primary types of tensors:
     * tf.variable
     * tf.constant
     * tf.placeholder

   it   s worth it to take a a look at each of these to discuss the
   differences, and when they are to be used.

tf.variable

   the tf.variable tensor is the most straight forward basic tensor, and
   is in many ways analogous to pure python variables in that the value of
   it is, well, variable.

   variables retain their value during the entire session, and are
   therefore useful when defining learnable parameters such as weights in
   neural networks, or anything else that   s going to change as the code is
   running.

   you define a variable as by the following:
a = tf.variable([1,2,3], name="a")

   here, we create a tensor variable with the initial state [1,2,3], and
   the name a. notice, that tensorflow is not able to inherit the python
   variable name, so if you want to have a name on the graph (more on that
   later), you need to specify a name.

   there are a few more options, but this is only meant to cover the
   basics. as with any of the things discussed here, you can read more
   about it on the [29]documentation page.

tf.constant

   the tf.constant is very similar to tf.variable with one major
   difference, they are immutable, that is the value is constant (wow,
   google really nailed the naming of tensors).

   the usage follows that of the tf.variable tensor:
b = tf.constant([1,2,3], name="b")

   you use this whenever you have a value that doesn   t change through the
   execution of the code for example to denote some property of the data,
   or to store the learning rate when using neural networks.

tf.placeholder

   finally, we have the tf.placeholder tensor. as the name implies, this
   tensor type is used to define variables, or graph nodes (operations),
   for which you don't have an initial value. you then defer setting a
   value until you actually do the computation using sess.run. this is
   useful for example as a proxy for your training data when defining the
   network.

   when running the operations, you need to pass actual data for the
   placeholders. this is done like so:
c = tf.placeholder(tf.int32, shape=[1,2], name="myplaceholder")
with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    res = sess.run(c,
       feed_dict={
        c:[[5,6]]
       })
    print (res)
[[5 6]]

   notice that we define a placeholder by first passing a non-optional
   parameter of the element type (here tf.int32), and then we define the
   shape using matrix dimension notation. the [1,2] denotes a matrix with
   1 row and two columns. if you haven't studied id202, this may
   seem confusing at first: why denote the height before the width?, and
   isn't [1,2] a 1 by 2 matrix itself with the values 1 and 2?

   these are valid questions, but in-depth answers are out of the scope of
   this essay. however, to give you the gist of it, the apparantly weird
   notation form has some quite neat mnemonic properties for some matrix
   operations, and yes, [1,2] can also be seen as a one by two matrix in
   itself. tensorflow uses the list like notation because it supports
   n-dimensional matrices, and it's therefore very convenient as we will
   see later.

   you can find a complete list of supported tensorflow datatypes
   [30]here.

   when we evaluate the value of c with sess.run we pass in the actual
   data using a feed_dict. notice that we use the python variable name,
   and not the name given to the tensorflow graph to target the
   placeholder. this same approach also extends to multiple placeholders
   where each variable name is mapped to a dictionary key of the same
   name.

wildcards when defining shapes

   sometimes, you don   t know some, or the entire shape of a placeholder
   when defining it. for example, you may use a variable batch size when
   training, this is where wildcards come in.

   wildcards essentially allows you to say,    i don   t know    to tensorflow,
   and let it infer the shapes from the incoming tensors.

   what   s the difference between -1 and none?

   honestly, i tried to figure out the answer to this, but i haven   t been
   able to find any documented difference between them, and the little i
   dug around in the source-code of tensorflow didn   t yield any results
   either. however, i   ve run into a couple of examples where one would
   raise an error while the other one wouldn   t.

   of the two, none seems to work better for me, so that's what i always
   use, and if i get an error related to the size of my placeholders, i
   try to change it to -1, but i do think they are supposed to be
   equivalent.

   why not just wildcard everything!?!

   having explicit shapes helps debugging as a lot of errors will be
   catches at    compile time    as opposed when training allowing you to spot
   mistakes more quickly, and ensures that errors don   t creep up on you
   silently (at least it tries to).

   so to save your future self from headaches, you should only use
   wildcards when describing something variable such as input size, and
   not something static such as network parameter size.

basic computation example

   knowing how variables work, we can now look at how to create more
   complex interactions.

   a graph in tensorflow consists of interconnected operations (ops). an
   op is essentially a function that is anything that takes some input and
   produces some output, and as we discussed before, the default datatype
   of tensorflow is the tensor, so operations can be said to be doing
   tensor manipulations.

   taking a look at a very basic example, multiplying two scalars, it can
   be done like so:
a = tf.variable(3)
b = tf.variable(4)
c = tf.multiply(a,b)
print (c)
tensor("mul:0", shape=(), dtype=int32)
print (a)
print (b)
<tf.variable 'variable_4:0' shape=() dtype=int32_ref>
<tf.variable 'variable_5:0' shape=() dtype=int32_ref>

   note that when we print the result we get another tensor, and not the
   actual result. also, notice that the variables have the shape () which
   is because a scalar is a zero dimensional tensor. finally, because we
   didn't specify a name, we get the names 'variable_4:0', and
   'variable_5:0' which means they are variable 4 and 5 on graph 0.

   to get the actual result, we have to compute the value in the context
   of a session. this can be done like so:
with tf.session() as sess:
    sess.run(tf.global_variables_initializer()) # this is important
    print (sess.run(c))
12

   you can also use [31]tf.interactivesession which is useful if you're
   using something like idle or a jupyter notebook. furthermore, it's also
   possible to start a session by declaring sess = tf.session(), and then
   close it by using sess.close(), however, i do not recommend this
   practice as it's easy to forget to close the session, and using this
   method as an interactive session may have performance implications as
   tensorflow really likes to eat as many resources as it can get its
   hands on (it's a bit like [32]chrome in this regard).

   we start by creating a session which signals to tensorflow that we want
   to start doing actual computations. behind the scenes, tensorflow does
   a few things; it chooses a device to perform the computations on (by
   default your first cpu), and it initializes the computational graph.
   while you can use multiple graph, it   s generally recommended to use
   just one because data cannot be sent between two graphs without having
   to go through python (which we established is [33]slow). this holds
   true even if you have multiple disconnected parts.

   next we initialize the variables. why you cannot do this while starting
   a session i don   t know, but it fills in the values of our variables in
   the graph, so we can use it in our computation. this is one of these
   small annoyances which you have to remember every time you want to
   compute something.

   it might help to remember that tensorflow is really lazy, and wants to
   do as little as possible. as an implication of this, you will have to
   explicitly tell tensorflow to initialize the variables.

tensorflow is lazy

   it might be useful to explore this in a bit more detail as it   s really
   important to understand how and why this was chosen in order to use
   tensorflow effectively.

   tensorflow likes to defer computation for as long as possible. it does
   so because python is slow, so it wants to run the computation outside
   python. normally, we use libraries such as numpy to accomplish this,
   but transferring data between python and optimized libraries such as
   numpy is expensive.

   tensorflow gets around this by first defining a graph using python
   without doing any computation, and then it sends all the data to the
   graph outside python where it can be run using efficient gpu libraries
   (cuda). this way, the time spent on transferring data is kept at a
   minimum.

   as a result of this, tensorflow only has to compute the part of the
   graph you actually need. it does this by propagating back through the
   network when you run an operation to discover all the dependencies the
   computation relies on, and only computes those. it ignores the rest of
   the network.

   consider the code below for example:
a = tf.variable(3)
b = tf.variable(4)
c = tf.multiply(a,b)
d = tf.add(a,c)
with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    c_value = sess.run(c)
    d_value = sess.run(d)
    print (c_value, d_value)
12 15

   here, we have two primitive values, a, and b, and two composite values,
   c, and d.
     * c relies on a, and b.
     * d relies on a, and c.

   so what happens when we compute the values of the composites? if we
   start with the simplest, c, we see that it relies on the primitive
   values, a, and b, so when computing, c, tensorflow discovers this
   through the id26 (which is not the same as id26
   through a neural network), gets the value of these primitives and
   multiplies them together.

   the value of d is computed in a similar fashion. tensorflow finds that
   d is an additions operation that relies on the value of a, and c, so
   tensorflow gets the value of each of them. for the value a, all is
   great, and tensorflow is able to use the primitive value as is, but
   with the value c, tensorflow discovers that it itself is a composite
   value, here a multiply operation that relies on a, and b. tensorflow
   now gets the value of a, and b which it uses to compute the value of c,
   so it can compute the value of d.

     tensorflow recursively computes the dependencies of an operation to
     find its computed value.

   however, this also means that values are discarded once computed, and
   can therefore not be used to speed up future computations. using the
   example above, this means that the value of c is recalculated when
   computing the value of d even though we just computed c and it hasn   t
   changed since then.

   below, this concept is explored further. we see that while the result
   of c is immediately discarded after being computed, you can save the
   result into a variable (here res), and when you do that, you can even
   access the result after the session is closed.
a = tf.variable(3)
b = tf.variable(4)
c = tf.multiply(a,b)
with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    res = sess.run(c)
    print (res,c)
12 tensor("mul:0", shape=(), dtype=int32)
with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    res = sess.run(c)
print (res,c)
12 tensor("mul:0", shape=(), dtype=int32)

choosing devices

   you can choose to compute some operations on a specific device using
   template below:
with tf.device("/gpu:0"):
    # do stuff with gpu
with tf.device("/cpu:0"):
    # do some other stuff with cpu

   where the string "/gpu:0", and "/cpu:0" can be replaced with any of the
   available device name strings you found when verifying that tensorflow
   was correctly installed.

   if you installed the gpu version, tensorflow will automatically try and
   run the graph on the gpu without you having to explicitly define it.

     if a gpu is available it will be prioritized over the cpu.

   when using multiple devices, it   s worth considering that switching
   between devices is rather slow because all the data has to be copied
   over to the memory of the new device.

distributed computing

   for when one computer simply isn   t enough.

   tensorflow allows for distributed computing. i imagine that this will
   not be relevant for most of us, so feel free to skip this section as
   you please, however, if you believe you might use multiple computers to
   work on a problem, this section might have some value to you.

   tensorflow   s distributed model can be broken down into several two
   parts:
     * server
     * cluster

   these are analogous to a server/client model. while the server contains
   the master copy, the clusters contain a set of jobs that each have a
   set of tasks which are actual computations.

   a server that manages a cluster with one job and two workers sharing
   the load between two tasks can be created like so:
cluster = tf.train.clusterspec({"my_job": ["worker1.ip:2222", "worker2.ip:2222"]
})
server = tf.train.server(cluster, job_name="my_job", task_index=1)
a = tf.variable(5)
with tf.device("/job:my_job/task:0"):
    b = tf.multiply(a, 10)
with tf.device("/job:my_job/task:1"):
    c = tf.add(b, a)
with tf.session("grpc://localhost:2222") as sess:
    res = sess.run(c)
    print(res)

   a corresponding worker-client can be created like so:
# get task number from command line
import sys
task_number = int(sys.argv[1])
import tensorflow as tf
cluster = tf.train.clusterspec({"my_job": ["worker1.ip:2222", "worker2.ip:2222"]
})
server = tf.train.server(cluster, job_name="my_job", task_index=task_number)
print("worker #{}".format(task_number))
server.start()
server.join()

   if the client code is saved to a file, you can start the workers by
   typing into a terminal:
python filename.py 0
python filename.py 1

   this will start two workers that listen for task 0 and task 1 of the
   my_job job. once the server is startedk, it will send the tasks to the
   workers which will return the answers to the server.

   for a more in-depth look at distributed computing with tensorflow,
   please refer to the [34]documentation.

saving variables (model)

   having to throw out the hard learned parameters after they have been
   computed isn   t much fun.

   luckily, saving a model in tensorflow quite simple using the saver
   object as illustrated in the example below:
a = tf.variable(5)
b = tf.variable(4, name="my_variable")
# set the value of a to 3
op = tf.assign(a, 3)
# create saver object
saver = tf.train.saver()
with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(op)
    print ("a:", sess.run(a))
    print ("my_variable:", sess.run(b))
    # use saver object to save variables
    # within the context of the current session
    saver.save(sess, "/tmp/my_model.ckpt")
a: 3
my_variable: 4

loading variables (model)

   as with saving the model, loading a model from a file is also simple.

   note: if you have specified a tensorflow name, you must use that same
   name in your loader as it has higher priority than the python name. if
   you haven   t specified a tensorflow name, the variable is saved using
   the python name.
# only necessary if you use idle or a jupyter notebook
tf.reset_default_graph()
# make a dummy variable
# the value is arbitrary, here just zero
# but the shape must the the same as in the saved model
a = tf.variable(0)
c = tf.variable(0, name="my_variable")
saver = tf.train.saver()
with tf.session() as sess:
    # use saver object to load variables from the saved model
    saver.restore(sess, "/tmp/my_model.ckpt")
    print ("a:", sess.run(a))
    print ("my_variable:", sess.run(c))
info:tensorflow:restoring parameters from /tmp/my_model.ckpt
a: 3
my_variable: 4

visualizing the graph

   it   s easy to lose the big picture when looking at the model as code,
   and it can be difficult to see the evolution of a model   s performance
   over time from print statements alone. this is where visualization
   comes in.

   tensorflow offers some tools that can take a lot of the work out of
   creating graphs.

   the visualization kit consists of two parts: tensorboard and a summary
   writer. tensorboard is where you will see the visualizations, and the
   summary writer is what will convert the model and variables into
   something tensorboard can render.

   without any work, the summary writer can give you a graphical
   representation of a model, and with very little work you can get more
   detailed summaries such as the evolution of loss, and accuracy as the
   model learns.

   let   s start by considering the simplest form for visualization that
   tensorflow supports: visualizing the graph.

   to achieve this, we simply create a summary writer, give it a path to
   save the summary, and point it to the graph we want saved. this can be
   done in one line of code:
fw = tf.summary.filewriter("/tmp/summary", sess.graph)

   integrated in an example, this becomes:
a = tf.variable(5, name="a")
b = tf.variable(10, name="b")
c = tf.multiply(a,b, name="result")

with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    print (sess.run(c))
    fw = tf.summary.filewriter("/tmp/summary", sess.graph)

   running tensorboard using the command below, and opening the url, we
   get a simple overview of the graph.
tensorboard --logdir=/tmp/summary

   [0*ks_eea_urmpvjyhp.png]

naming and scopes

   sometimes when working with large models, the graph visualization can
   become complex. to help with this, we can define scopes using
   tf.name_scope to add another level of abstraction, in fact, we can
   define scopes within scopes as illustrated in the example below:
with tf.name_scope('primitives') as scope:
    a = tf.variable(5, name='a')
    b = tf.variable(10, name='b')
with tf.name_scope('fancy_pants_procedure') as scope:
    # this procedure has no significant interpretation
    # and was purely made to illustrate why you might want
    # to work at a higher level of abstraction
    c = tf.multiply(a,b)
    with tf.name_scope('very_mean_reduction') as scope:
        d = tf.reduce_mean([a,b,c])
    e = tf.add(c,d)
with tf.name_scope('not_so_fancy_procedure') as scope:
    # this procedure suffers from imposter syndrome
    d = tf.add(a,b)
with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    print (sess.run(c))
    print (sess.run(e))
    fw = tf.summary.filewriter("/tmp/summary", sess.graph)

   note that the scope names must be one word.

   opening this summary in tensorboard we get:
   [0*lifdm-cjfykgnsew.png]

   we can expand the scopes to see the individual operations that make up
   the scope.
   [0*ptrnw36sv3d7rytt.png]

   if we expand very_mean_reduction even further, we can see rank, and
   mean which are a part of the reduce_mean function. we can even expand
   those to see how those are implemented.
   [0*hvqydgxp8kuhp5dk.png]

visualizing changing data

   while just visualizing the graph is pretty cool, when learning
   parameters, it   d be useful to be able to visualize how certain
   variables change over time.

   the simplest way of visualizing changing data is by adding a scalar
   summary. below is an example that implements this and logs the change
   of c.
import random
a = tf.variable(5, name="a")
b = tf.variable(10, name="b")
# set the intial value of c to be the product of a and b
# in order to write a summary of c, c must be a variable
init_value = tf.multiply(a,b, name="result")
c = tf.variable(init_value, name="changingnumber")
# update the value of c by incrementing it by a placeholder number
number = tf.placeholder(tf.int32, shape=[], name="number")
c_update = tf.assign(c, tf.add(c,number))

# create a summary to track to progress of c
tf.summary.scalar("changingnumber", c)
# in case we want to track multiple summaries
# merge all summaries into a single operation
summary_op = tf.summary.merge_all()

with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    # initialize our summary file writer
    fw = tf.summary.filewriter("/tmp/summary", sess.graph)
    # do 'training' operation
    for step in range(1000):
        # set placeholder number somewhere between 0 and 100
        num = int(random.random()*100)
        sess.run(c_update, feed_dict={number:num})
        # compute summary
        summary = sess.run(summary_op)
        # add merged summaries to filewriter,
        # so they are saved to disk
        fw.add_summary(summary, step)

   so what happens here?

   if we start by looking at the actual logic, we see that the value of c,
   the changing variable, starts by being the product of a, and b (50).

   we then run an update operation 1000 times which increments the value
   of c by a randomly selected amount between 0 and 100.

   this way, if we were to plot the value of c over time, we   d expect to
   see it linearly increase over time.

   with that out of the way, let   s see how we create a summary of c.

   before the session, we start by telling tensorflow that we do in fact
   want a summary of c.
tf.summary.scalar("changingnumber", c)

   in this case, we use a scalar summary because, well, c is a scalar.
   however, tensorflow supports an array of different summarizers
   including:
     * histogram (which accepts a tensor array)
     * text
     * audio
     * images

   the last three are useful if you need to summarize rich data you may be
   using to feed a network.

   next, we add all the summaries to a summary op to simplify the
   computation.
summary_op = tf.summary.merge_all()

   strictly speaking, this is not necessary here as we only record the
   summary of one value, but in a more realistic example, you   d typically
   have multiple summaries which makes this very useful. you can also use
   tf.summary.merge to merge specific summaries like so:
summary = tf.summary.merge([summ1, summ2, summ3])

   this can be powerful if coupled with scopes.

   next, we start the session where we do the actual summary writing. we
   have to tell tensorflow what and when to write; it won   t automatically
   write a summary entry every time a variable changes even though it   d be
   useful.

   therefore, every time we want a new entry in the summary, we have to
   run the summary operation. this allows for flexibility in how often, or
   with what precision, you want to log your progress. for example, you
   could choose to log progress only every thousand iterations to speed up
   computation, and free io calls.

   here we just log the progress at every iteration with the following
   line of code:
summary = sess.run(summary_op)

   we now have the summary tensorboard uses, but we haven   t written it to
   disk yet. for this, we need to add the summary to the filewriter:
fw.add_summary(summary, step)

   here, the second argument step indicates the location index for the
   summary, or the x-value in a plot of it. this can be any number you
   want, and when training networks, you can often just use the iteration
   number. by manually specifying the index number, the summary writer
   allows for a lot of flexibility when creating the graphs as you can
   walk backwards, skip values, and even compute two, or more, values for
   the same index.

   this is all we need. if we now open tensorboard, we see the resulting
   graph, and the plot that has been made from the summary.
   [0*6bl8lpx9hs4bqrdr.png]
   [0*bpc4nxytnqnx4rzw.png]

   and as predicted, the trend of the summary plot is indeed linear with a
   positive slope.

an almost practical example

   while the small examples up until now are great at demonstrating
   individual ideas, they do a poor job of showing how it all comes
   together.

   to illustrate this, we will now use everything (well, almost
   everything) we have learned about tensorflow to make something we at
   least can pretend to be somewhat practical; we will build a very simple
   neural network to classify digits from the classic [35]mnist dataset.
   if you   re not fully up to speed with neural networks, you can read this
   introduction (coming soon) before coming back to this.

   the construction and training of the neural network can be broken down
   into a couple of phases:
     * importing the data.
     * constructing the model architecture.
     * defining a id168 to optimize, and a way to optimize it.
     * actually training the model.
     * evaluating the model.

   however, before we can start creating the model, we must first prepare
   tensorflow:
import tensorflow as tf
tf.reset_default_graph() # again, this is not needed if run as a script

   next, we import the data.
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("mnist_data/", one_hot=true)
extracting mnist_data/train-images-idx3-ubyte.gz
extracting mnist_data/train-labels-idx1-ubyte.gz
extracting mnist_data/t10k-images-idx3-ubyte.gz
extracting mnist_data/t10k-labels-idx1-ubyte.gz

   since mnist is such a well known dataset, we can use the built in data
   extractor to get a nice wrapper around the data.

   now, it   s time to define the actual model that   s going to be used. for
   this task, we will use a feed forward network with two hidden layers
   that has 500 and 100 parameters respectively.

   using the idea about scopes to separate the graph into chunks, we can
   implement the model like so:
# input
with tf.name_scope('input') as scope:
    x = tf.placeholder(tf.float32, [none, 28*28], name="input")
    # a placeholder to hold the correct answer during training
    labels = tf.placeholder(tf.float32, [none, 10], name="label")
    # the id203 of a neuron being kept during dropout
    keep_prob = tf.placeholder(tf.float32, name="keep_prob")
with tf.name_scope('model') as scope:
    with tf.name_scope('fc1') as scope: # fc1 stands for 1st fully connected lay
er
        # 1st layer goes from 784 neurons (input) to 500 in the first hidden lay
er
        w1 = tf.variable(tf.truncated_normal([28*28, 500], stddev=0.1), name="we
ights")
        b1 = tf.variable(tf.constant(0.1, shape=[500]), name="biases")
        with tf.name_scope('softmax_activation') as scope:
            # softmax activation
            a1 = tf.nn.softmax(tf.matmul(x, w1) + b1)
        with tf.name_scope('dropout') as scope:
            # dropout
            drop1 = tf.nn.dropout(a1, keep_prob)
    with tf.name_scope('fc2') as scope:
        # takes the first hidden layer of 500 neurons to 100 (second hidden laye
r)
        w2 = tf.variable(tf.truncated_normal([500, 100], stddev=0.1), name="weig
hts")
        b2 = tf.variable(tf.constant(0.1, shape=[100]), name="biases")
        with tf.name_scope('relu_activation') as scope:
            # relu activation, and dropout for second hidden layer
            a2 = tf.nn.relu(tf.matmul(drop1, w2) + b2)
        with tf.name_scope('dropout') as scope:
            drop2 = tf.nn.dropout(a2, keep_prob)
    with tf.name_scope('fc3') as scope:
        # takes the second hidden layer of 100 neurons to 10 (which is the outpu
t)
        w3 = tf.variable(tf.truncated_normal([100, 10], stddev=0.1), name="weigh
ts")
        b3 = tf.variable(tf.constant(0.1, shape=[10]), name="biases")
        with tf.name_scope('logits') as scope:
            # final layer doesn't have dropout
            logits = tf.matmul(drop2, w3) + b3

   for training, we are going to use the cross id178 id168
   together with tha adam optimizer with a learning rate of 0.001.
   following the example above, we continue the use of scopes to organize
   the graph.

   we also add two summarizers for accuracy and the average loss, and
   create a merged summary operation to simplify later steps.

   finally, once we add the saver object, so we don   t lose the model after
   training (which would be a shame), we have this:
with tf.name_scope('train') as scope:
    with tf.name_scope('loss') as scope:
        # id168
        cross_id178 = tf.nn.softmax_cross_id178_with_logits(labels=labels, l
ogits=logits)
    # use adam optimizer for training with a learning rate of 0.001
    train_step = tf.train.adamoptimizer(0.001).minimize(cross_id178)
with tf.name_scope('evaluation') as scope:
    # evaluation
    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
# create a summarizer that summarizes loss and accuracy
tf.summary.scalar("accuracy", accuracy)
# add average loss summary over entire batch
tf.summary.scalar("loss", tf.reduce_mean(cross_id178))
# merge summaries
summary_op = tf.summary.merge_all()
# create saver object
saver = tf.train.saver()

   it   s now time to begin training the network. using the techniques
   discussed previously, we write a summary every 100 steps for the total
   of 20000 steps.

   at each step we train the network with a batch of 100 examples by
   running the train_step operation which will update the weights of
   network in accordance with the learning rate.

   finally, once the learning is done, we print out the test accuracy, and
   save the model.
with tf.session() as sess:
    # initialize variables
    tf.global_variables_initializer().run()
    # initialize summarizer filewriter
    fw = tf.summary.filewriter("/tmp/nn/summary", sess.graph)
    # train the network
    for step in range(20000):
        batch_xs, batch_ys = mnist.train.next_batch(100)
        sess.run(train_step, feed_dict={x: batch_xs, labels: batch_ys, keep_prob
:0.2})
        if step%1000 == 0:
            acc = sess.run(accuracy, feed_dict={
                x: batch_xs, labels: batch_ys, keep_prob:1})
            print("mid train accuracy:", acc, "at step:", step)
        if step%100 == 0:
            # compute summary using test data every 100 steps
            summary = sess.run(summary_op, feed_dict={
                x: mnist.test.images, labels: mnist.test.labels, keep_prob:1})
            # add merged summaries to filewriter,
            # so they are saved to disk
            fw.add_summary(summary, step)
    print ("final test accuracy:", sess.run(accuracy, feed_dict={
                x: mnist.test.images, labels: mnist.test.labels, keep_prob:1}))
    # save trained model
    saver.save(sess, "/tmp/nn/my_nn.ckpt")
mid train accuracy: 0.1 at step: 0
mid train accuracy: 0.91 at step: 1000
mid train accuracy: 0.89 at step: 2000
mid train accuracy: 0.91 at step: 3000
[...]
mid train accuracy: 0.97 at step: 17000
mid train accuracy: 0.98 at step: 18000
mid train accuracy: 0.97 at step: 19000
final test accuracy: 0.9613

   96% accuracy is that any good?

   no, that actually kind of sucks, but the point of this network is not
   to be the best network. instead, the point of it is to demonstrate how
   you can use tensorflow to construct a network, and get a lot of
   visualization pizzazz for very little work.

   if we run the model, and open it in tensorboard, we get:
   [0*re1_b8xuvh2eswpm.png]

   furthermore, we can see the summaries tensorflow made for the accuracy
   and loss, and that they do, as expected, behave approximately like
   inverse of each other. we also see that the accuracy increases a lot in
   the beginning, but flattens out over time which is expected partly
   because we use the adam optimizer, and partly because the nature of
   gradients.
   [0*dvmczjf9y40g6z_3.png]

   the use of nested scopes let   s us progressively change the abstraction
   level. notice how, if we expand the model, we can see the individual
   layers before the individual layer components.
   [0*6vtrq00ocksyvzpj.png]

   if you want to run this network yourself, you can access the code on
   [36]github.

conclusion

   wow, you   re still here. you deserve a cute picture of a cat.
   [0*wr2fkhf_suydskqc.jpg]

   if you have followed this far, you should now be comfortable with the
   basics of tensorflow: how it functions, how to do basic computations,
   how to visualize the graph, and finally you have seen a real example of
   how it can be used to create a basic neural network.

   also, send me a tweet [37]@kasperfredn if you made it all the way
   through: you   re awesome.

   as this was just an introduction to tensorflow, there   s a lot we didn   t
   cover, but you should know enough now to be able to understand the
   [38]api documentation where you can find modules you can incorporate
   into your code.

   if you want a challenge to test your comprehension, try to use
   tensorflow to implement another machine learning model by either
   working from the model we created here, or starting from scratch.

   for feedback, send your results to    homework [at] kasperfred.com   .
   remember to include the title of the essay in the subject line.

     * [39]machine learning
     * [40]tensorflow
     * [41]programming
     * [42]data science

   (button)
   (button)
   (button) 238 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [43]go to the profile of kasper fredenslund

[44]kasper fredenslund

   physics student who builds autonomous robots, and does research into
   modelling material properties using machine learning.
   [45]https://kasperfred.com/

     (button) follow
   [46]towards data science

[47]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 238
     * (button)
     *
     *

   [48]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [49]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/eb381f870a79
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-tensorflow-as-a-computational-framework-eb381f870a79&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-tensorflow-as-a-computational-framework-eb381f870a79&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_5hda7dyeuzjs---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@kasperfred?source=post_header_lockup
  17. https://towardsdatascience.com/@kasperfred
  18. https://github.com/tensorflow/tensorflow
  19. https://github.com/torvalds/linux
  20. https://kasperfred.com/posts/tag/ml-101/
  21. http://scikit-learn.org/stable/
  22. https://kasperfred.com/posts/creating-your-first-machine-learning-classification-model-in-sklearn/
  23. https://kasperfred.com/posts/the-future-of-deep-learning/
  24. https://kasperfred.com/posts/introduction-to-tensorflow-as-a-computational-library/
  25. https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf
  26. https://developer.nvidia.com/cuda-zone
  27. https://developer.nvidia.com/cudnn
  28. https://www.tensorflow.org/install/
  29. https://www.tensorflow.org/api_docs/python/tf/variable
  30. https://www.tensorflow.org/versions/r0.12/api_docs/python/framework/tensor_types
  31. https://www.tensorflow.org/api_docs/python/tf/interactivesession
  32. https://lifehacker.com/why-chrome-uses-so-much-freaking-ram-1702537477
  33. https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/
  34. https://www.tensorflow.org/deploy/distributed
  35. http://yann.lecun.com/exdb/mnist/
  36. https://gist.github.com/kasperfred/b470324dc5c81cef8ae24fd1d3fd4e59
  37. https://twitter.com/kasperfredn
  38. https://www.tensorflow.org/api_docs/
  39. https://towardsdatascience.com/tagged/machine-learning?source=post
  40. https://towardsdatascience.com/tagged/tensorflow?source=post
  41. https://towardsdatascience.com/tagged/programming?source=post
  42. https://towardsdatascience.com/tagged/data-science?source=post
  43. https://towardsdatascience.com/@kasperfred?source=footer_card
  44. https://towardsdatascience.com/@kasperfred
  45. https://kasperfred.com/
  46. https://towardsdatascience.com/?source=footer_card
  47. https://towardsdatascience.com/?source=footer_card
  48. https://towardsdatascience.com/
  49. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  51. https://medium.com/p/eb381f870a79/share/twitter
  52. https://medium.com/p/eb381f870a79/share/facebook
  53. https://medium.com/p/eb381f870a79/share/twitter
  54. https://medium.com/p/eb381f870a79/share/facebook
