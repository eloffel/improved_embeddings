   [1]mlpr     [2]course notes     w0d [3]pdf of this page
   previous: [4]self-test answers next: [5]programming in matlab/octave or
   python



   $$ \newcommand{\e}{\mathbb{e}} \newcommand{\n}{\mathcal{n}}
   \newcommand{\bx}{\mathbf{x}} \newcommand{\intd}{\;\mathrm{d}}
   \newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}
   \newcommand{\tdd}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
   \newcommand{\te}{\!=\!} \newcommand{\tp}{\!+\!} $$

                           maths background for mlpr

   this course assumes that you have some university-level mathematics
   experience. for example, as covered by first-year undergraduate
   mathematics courses taken by informatics, physics, or engineering
   students in edinburgh.

   please take the self-assessment test as a guide. you don   t have to be
   able to do all of it. however, the answers should make perfect sense.
   moreover, you will need to be able to do future exercises without
   looking at the answers. if you don   t have experience solving technical
   problems involving mathematics, you are likely to struggle.

   it is expected that you are used to manipulating algebraic expressions,
   and solving for unknowns. for example, it should be straightforward for
   you to rearrange an expression like \[ y = 3 + \log x^3 z, \] to give
   an explicit expression for \(x\) in terms of the other variables.

   the three main areas of mathematics we need are id203, linear
   algebra, and calculus. most of the results you should know are
   summarised on the following cribsheet:
   [6]http://homepages.inf.ed.ac.uk/imurray2/pub/cribsheet.pdf

   the rest of the document below gives some additional details and
   reading.

id203

   as stated in the cribsheet, mackay   s free textbook provides a terse
   introduction to id203, as does murphy section 2.2, or barber
   chapter 1. alternatively, sharon goldwater has a longer, more tutorial
   introduction:
   [7]http://homepages.inf.ed.ac.uk/sgwater/math_tutorials.html

   you must know the sum and product rules of id203: their
   equations, what they mean, and how to apply them for discrete and
   real-valued variables.

   expectations, or averages of random quantities, are also important. i
   have provided detailed notes on those with the mlpr course notes.

id202

   an undergraduate id202 course will usually discuss abstract
   linear spaces and operators. this course largely focusses on concrete
   operations on matrices and vectors expressed as arrays of numbers, as
   we can explicitly compute in (for example) matlab or octave.

   you need to be able to do basic algebraic manipulation of matrices and
   vectors, and know how id127 works. you should also have
   a geometric understanding of these operations, which can be relevant to
   understanding their application to machine learning. if you   re unsure,
   please work through david barber   s tutorial:

   [8]http://www.inf.ed.ac.uk/teaching/courses/mlpr/notes/mlpr-supplementa
   ry-maths.pdf

   a shortened version of this tutorial also appears as an appendix of his
   textbook.

   you will not need to be able to numerically compute matrix inverses,
   determinants, or eigenvalues of matrices by hand for this course. you
   can safely skip those exercises!

   there are many possible introductions to id202. another terse
   one is [9]chapter 2 of goodfellow et al.   s [10]deep learning textbook.
   a nice series of videos is [11]3blue1brown   s [12]essence of linear
   algebra.

differentiation

   you should know how to differentiate algebraic expressions. computer
   algebra systems can do this stuff for us, and i   ll talk about automatic
   numerical differentiation later in the course. however, in simple cases
   i still regularly differentiate expressions with pen and paper while
   doing research.

   the cribsheet summarizes the basic results i expect you to know. if the
   rules don   t make sense, you will need to consult an undergraduate level
   or advanced high-school level maths textbook, or a tutorial series such
   as those from khan academy.

   some students may not have seen or remember partial derivatives. for
   example: \[ \pdd{xy^2}{x} = y^2, \quad \pdd{xy^2}{y} = 2xy. \] the
   curly \(\partial\) simply means that you treat all other variables as
   constants when you are doing the differentiation.

   partial derivatives can be combined to create total derivatives. for
   example, imagine moving around the circumference of a circle by
   changing an angle \(\theta\). your \((x,y)\) position is given by \(x
   \te \cos\theta\) and \(y \te \sin\theta\). to compute the change in a
   function \(f(\bx)\) due to an infinitesimal change \(\mathrm{d}\theta\)
   in the angle, you can use the chain rule of differentiation: \[
   \mathrm{d}{f} \;=\; \pdd{f}{x}\,\mathrm{d}{x} \,+\,
   \pdd{f}{y}\,\mathrm{d}{y} \qquad\text{or}\qquad \tdd{f}{\theta} \;=\;
   \pdd{f}{x}\,\tdd{x}{\theta} \,+\, \pdd{f}{y}\,\tdd{y}{\theta}. \] in
   this case you could also substitute expressions for \(x\) and \(y\), to
   find \(f(\theta)\) and differentiate with respect to \(\theta\). you
   could try both methods to differentiate \(f(x,y) \te xy^2\) with
   respect to \(\theta\). you should get the same answer! the chain rule
   approach is needed in many machine learning settings.

   you should be comfortable enough with both vectors and derivatives that
   you wouldn   t find it intimidating to work with a vector containing
   derivatives. for example, if we want to find partial derivatives of a
   function \(f(\bx)\) with respect to each element of a vector \(\bx =
   [x_1~x_2]^\top\), then the vector \(\nabla_\bx f\) is defined as: \[
   \nabla_\bx f = \left[ \begin{array}{c} \pdd{f}{x_1} \\[1ex]
   \pdd{f}{x_2} \end{array} \right]. \] we will do mathematics containing
   such expressions. for example, given the chain rule of differentiation
   above, you should be happy that \[ \tdd{f}{\theta} = \left(\nabla_\bx
   f\right)^\top \left[ \begin{array}{c} \tdd{x_1}{\theta} \\[1ex]
   \tdd{x_2}{\theta} \end{array} \right]. \]

   the mathematics textbook i used and liked as an undergraduate covers
   this material:    mathematical methods for physics and engineering   ,
   riley, hobson, bence. although there are many other possible textbooks
   and online tutorials.

   [13]3blue1brown also have an [14]essence of calculus playlist.

integration

   you should also know enough about integration to understand the sum
   rule and expectations for real-valued variables.

   the two most common situations in machine learning are: 1) an integral
   is impossible, there is no closed form solution; or 2) an integral is
   easy, there is a trick to write down the answer.

   for example, the gaussian distribution (discussed in much more detail
   later in the notes) with mean \(\mu\) and variance \(\sigma^2\) has
   id203 density function: \[ \n(x;\, \mu, \sigma^2) \;=\;
   \frac{1}{\sigma\sqrt{2\pi}}\, e^{-\frac{1}{2\sigma^2} (x - \mu)^2}. \]
   we may have to compute various integrals involving this function. for
   example: \[ i = \int_{-\infty}^\infty \,(x + x^2)\, \n(x;\, \mu,
   \sigma^2) \intd{x}. \] we can express this integral in terms of
   expectations for which we already know the answers: \[ i \;=\; \e[x] +
   \e[x^2] \;=\; \mu \,+\, (\mathrm{var}[x] \tp \e[x]^2) \;=\; \mu +
   \sigma^2 + \mu^2. \]

   summary: there   s no need to revise everything about integration covered
   in a calculus course. i learned about many tricks for solving integrals
   that i never use, such as clever trigonometric substitutions and
   contour integration. however, you do need to be comfortable enough with
   what integration is, and with id203 theory, so that you can
   follow and produce mathematical arguments like those above.
     __________________________________________________________________

   [15]mlpr     [16]course notes     w0d [17]pdf of this page
   previous: [18]self-test answers next: [19]programming in matlab/octave
   or python



   notes by [20]iain murray

references

   1. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/
   2. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/
   3. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w0d_maths.pdf
   4. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w0c_background_selftest_answers.html
   5. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w0e_programming.html
   6. http://homepages.inf.ed.ac.uk/imurray2/pub/cribsheet.pdf
   7. http://homepages.inf.ed.ac.uk/sgwater/math_tutorials.html
   8. http://www.inf.ed.ac.uk/teaching/courses/mlpr/notes/mlpr-supplementary-maths.pdf
   9. http://www.deeplearningbook.org/contents/linear_algebra.html
  10. http://www.deeplearningbook.org/
  11. https://www.3blue1brown.com/
  12. https://www.youtube.com/playlist?list=plzhqobowtqdpd3mizzm2xvfitgf8he_ab
  13. https://www.3blue1brown.com/
  14. https://www.youtube.com/playlist?list=plzhqobowtqdmsr9k-rj53dwvrmyo3t5yr
  15. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/
  16. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/
  17. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w0d_maths.pdf
  18. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w0c_background_selftest_answers.html
  19. http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w0e_programming.html
  20. http://iainmurray.net/
