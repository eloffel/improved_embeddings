   #[1]github [2]recent commits to pytorch-qid56:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]50
     * [35]star [36]1,008
     * [37]fork [38]140

[39]salesforce/[40]pytorch-qid56

   [41]code [42]issues 12 [43]pull requests 7 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   pytorch implementation of the quasi-recurrent neural network - up to 16
   times faster than nvidia's cudnn lstm
     * [47]19 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]bsd-3-clause

    1. [52]python 100.0%

   (button) python
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/s
   [54]download zip

downloading...

   want to be notified of new releases in salesforce/pytorch-qid56?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]@smerity
   [62]smerity [63]increment version for those installing without
   explicitly setting pip    (button)    
    install -u

   latest commit [64]daadb0f nov 25, 2017
   [65]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [66]examples [67]qid56: add multigpu / dataparallel support for
   forgetmult and hence qid56 oct 19, 2017
   [68]images [69]qid56 release sep 28, 2017
   [70]torchqid56 [71]bugfix: cpuforgetmult did not like batch size 1 due
   to squeeze nov 25, 2017
   [72]license
   [73]readme.md
   [74]requirements.txt [75]qid56 release sep 28, 2017
   [76]setup.py [77]increment version for those installing without
   explicitly setting pip    nov 26, 2017

readme.md

quasi-recurrent neural network (qid56) for pytorch

   updated to support multi-gpu environments via dataparallel - see the
   the multigpu_dataparallel.py example.

   this repository contains a pytorch implementation of [78]salesforce
   research's [79]quasi-recurrent neural networks paper.

   the qid56 provides similar accuracy to the lstm but can be betwen 2 and
   17 times faster than the highly optimized nvidia cudnn lstm
   implementation depending on the use case.

   to install, simply run:

   pip install cupy pynvrtc git+https://github.com/salesforce/pytorch-qid56

   if you use this code or our results in your research, please cite:
@article{bradbury2016quasi,
  title={{quasi-recurrent neural networks}},
  author={bradbury, james and merity, stephen and xiong, caiming and socher, ric
hard},
  journal={international conference on learning representations (iclr 2017)},
  year={2017}
}

software requirements

   this codebase requires python 3, [80]pytorch, [81]pynvrtc (nvidia's
   python bindings to nvrtc), and [82]cupy. while the codebase contains a
   cpu implementation of the qid56, the gpu qid56 implementation is used by
   default if possible. requirements are provided in requirements.txt.

example usage

   we've updated the previously released salesforce research [83]awd-lstm
   id38 codebase to support use of the awd-qid56. with the
   same number of parameters as the lstm and less well tuned hyper
   parameters, the qid56 model trains over twice as quickly and achieves
   nearly equivalent state-of-the-art id38 results. for full
   details, refer to the [84]awd-lstm-lm repository.

usage

   the qid56 api is meant to be drop-in compatible with the [85]lstm for
   many standard use cases. as such, the easiest thing to do is replace
   any gru or lstm module with the qid56.

   note: bidirectional qid56 is not yet supported though will be in the
   near future.
import torch
from torchqid56 import qid56

seq_len, batch_size, hidden_size = 7, 20, 256
size = (seq_len, batch_size, hidden_size)
x = torch.autograd.variable(torch.rand(size), requires_grad=true).cuda()

qid56 = qid56(hidden_size, hidden_size, num_layers=2, dropout=0.4)
qid56.cuda()
output, hidden = qid56(x)

print(output.size(), hidden.size())

   the full documentation for the qid56 is listed below:
qid56(input_size, hidden_size, num_layers, dropout=0):
    applies a multiple layer quasi-recurrent neural network (qid56) to an input s
equence.

    args:
        input_size: the number of expected features in the input x.
        hidden_size: the number of features in the hidden state h. if not specif
ied, the input size is used.
        num_layers: the number of qid56 layers to produce.
        layers: list of preconstructed qid56 layers to use for the qid56 module (o
ptional).
        save_prev_x: whether to store previous inputs for use in future convolut
ional windows (i.e. for a continuing sequence such as in id38). if
true, you must call reset to remove cached previous values of x. default: false.
        window: defines the size of the convolutional window (how many previous
tokens to look when computing the qid56 values). supports 1 and 2. default: 1.
        zoneout: whether to apply zoneout (i.e. failing to update elements in th
e hidden state) to the hidden state updates. default: 0.
        output_gate: if true, performs qid56-fo (applying an output gate to the o
utput). if false, performs qid56-f. default: true.
        use_cuda: if true, uses fast custom cuda kernel. if false, uses naive fo
r loop. default: true.

    inputs: x, hidden
        - x (seq_len, batch, input_size): tensor containing the features of the
input sequence.
        - hidden (layers, batch, hidden_size): tensor containing the initial hid
den state for the qid56.

    outputs: output, h_n
        - output (seq_len, batch, hidden_size): tensor containing the output of
the qid56 for each timestep.
        - h_n (layers, batch, hidden_size): tensor containing the hidden state f
or t=seq_len

   the included qid56 layer supports convolutional windows of size 1 or 2
   but will be extended in the future to support arbitrary convolutions.

   if you are using convolutional windows of size 2 (i.e. looking at the
   inputs from two previous timesteps to compute the input) and want to
   run over a long sequence in batches, such as when using bptt, you can
   set save_prev_x=true and call reset when you wish to reset the cached
   previous inputs.

   if you want flexibility in the definition of each qid56 layer, you can
   construct individual qid56layer modules and pass them to the qid56 module
   using the layer argument.

speed

   speeds are between 2 and 17 times faster than nvidia's cudnn lstm, with
   the difference as a result of varying batch size and sequence length.
   the largest gains are for small batch sizes or long sequence lengths,
   both highlighting the lstms parallelization difficulty due to forced
   sequentiality. for full information, refer to the [86]quasi-recurrent
   neural networks paper.

   [87]figure 4 from qid56 paper

   pictured above is figure 4 from the qid56 paper:
   left: training speed for two-layer 640-unit ptb lm on a batch of 20
   examples of 105 timesteps.    id56    and    softmax    include the forward and
   backward times, while    optimization overhead    includes gradient
   clipping, l2 id173, and sgd computations.
   right: id136 speed advantage of a 320-unit qid56 layer alone over an
   equal-sized cudnn lstm layer for data with the given batch size and
   sequence length. training results are similar.

extending the qid56 speed advantage to other recurrent architectures with
forgetmult

   the qid56 architecture's speed advantage comes from two primary sources:
   the ability to batch all computations into a few large matrix
   multiplications and the use of a fast element-wise recurrence function.
   this recurrence function, named forgetmult, is general and can be used
   in other scenarios. the forgetmult takes two arguments - the candidate
   input x and forget gates f - and computes h = f * x + (1 - f) * hm1
   where hm1 is the previous hidden state output.

   the qid56 class is a thin wrapper around this that performs the large
   id127s for the candidate x, the forget gates f, and the
   output gates o. any other operation which requires recurrence and can
   have precomputed values for the candidate x and forget gates f can use
   this fast form of recurrence.

   example usage of the forgetmult module: output = forgetmult()(f, x,
   hidden).
    forgetmult computes a simple recurrent equation:
    h_t = f_t * x_t + (1 - f_t) * h_{t-1}

    this equation is equivalent to dynamic weighted averaging.

    inputs: x, hidden
        - x (seq_len, batch, input_size): tensor containing the features of the
input sequence.
        - f (seq_len, batch, input_size): tensor containing the forget gate valu
es, assumed in range [0, 1].
        - hidden_init (batch, input_size): tensor containing the initial hidden
state for the recurrence (h_{t-1}).
        - cuda: if true, use the fast element-wise cuda kernel for recurrence. i
f false, uses naive for loop. default: true.

want to help out?

   first, thanks! :)

   open tasks that are interesting:
     * modify the forgetmult cuda kernel to produce a backwardforgetmult.
       this will enable a bidirectional qid56. the input should be the same
       - f and x - but the kernel should walk backwards through the
       inputs.
     * bidirectional qid56 support (requires the modification above)
     * support pytorch's packedsequence such that variable length
       sequences are correctly masked
     * show how to use the underlying fast recurrence operator forgetmult
       in other generic ways

     *    2019 github, inc.
     * [88]terms
     * [89]privacy
     * [90]security
     * [91]status
     * [92]help

     * [93]contact github
     * [94]pricing
     * [95]api
     * [96]training
     * [97]blog
     * [98]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [99]reload to refresh your
   session. you signed out in another tab or window. [100]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/salesforce/pytorch-qid56/commits/master.atom
   3. https://github.com/salesforce/pytorch-qid56#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/salesforce/pytorch-qid56
  32. https://github.com/join
  33. https://github.com/login?return_to=/salesforce/pytorch-qid56
  34. https://github.com/salesforce/pytorch-qid56/watchers
  35. https://github.com/login?return_to=/salesforce/pytorch-qid56
  36. https://github.com/salesforce/pytorch-qid56/stargazers
  37. https://github.com/login?return_to=/salesforce/pytorch-qid56
  38. https://github.com/salesforce/pytorch-qid56/network/members
  39. https://github.com/salesforce
  40. https://github.com/salesforce/pytorch-qid56
  41. https://github.com/salesforce/pytorch-qid56
  42. https://github.com/salesforce/pytorch-qid56/issues
  43. https://github.com/salesforce/pytorch-qid56/pulls
  44. https://github.com/salesforce/pytorch-qid56/projects
  45. https://github.com/salesforce/pytorch-qid56/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/salesforce/pytorch-qid56/commits/master
  48. https://github.com/salesforce/pytorch-qid56/branches
  49. https://github.com/salesforce/pytorch-qid56/releases
  50. https://github.com/salesforce/pytorch-qid56/graphs/contributors
  51. https://github.com/salesforce/pytorch-qid56/blob/master/license
  52. https://github.com/salesforce/pytorch-qid56/search?l=python
  53. https://github.com/salesforce/pytorch-qid56/find/master
  54. https://github.com/salesforce/pytorch-qid56/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/salesforce/pytorch-qid56
  56. https://github.com/join?return_to=/salesforce/pytorch-qid56
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/smerity
  62. https://github.com/salesforce/pytorch-qid56/commits?author=smerity
  63. https://github.com/salesforce/pytorch-qid56/commit/daadb0f39a1811128df7eb03933f286aa5e319ed
  64. https://github.com/salesforce/pytorch-qid56/commit/daadb0f39a1811128df7eb03933f286aa5e319ed
  65. https://github.com/salesforce/pytorch-qid56/tree/daadb0f39a1811128df7eb03933f286aa5e319ed
  66. https://github.com/salesforce/pytorch-qid56/tree/master/examples
  67. https://github.com/salesforce/pytorch-qid56/commit/f7f3346cfc1e4a61bcdae4fcf305eddeea68875d
  68. https://github.com/salesforce/pytorch-qid56/tree/master/images
  69. https://github.com/salesforce/pytorch-qid56/commit/e1226ce5753d76ccaa0ccdba69f4ffc3495ec4d1
  70. https://github.com/salesforce/pytorch-qid56/tree/master/torchqid56
  71. https://github.com/salesforce/pytorch-qid56/commit/d045e720b1cc6610490a83aac2ff0db5d2b24bb3
  72. https://github.com/salesforce/pytorch-qid56/blob/master/license
  73. https://github.com/salesforce/pytorch-qid56/blob/master/readme.md
  74. https://github.com/salesforce/pytorch-qid56/blob/master/requirements.txt
  75. https://github.com/salesforce/pytorch-qid56/commit/e1226ce5753d76ccaa0ccdba69f4ffc3495ec4d1
  76. https://github.com/salesforce/pytorch-qid56/blob/master/setup.py
  77. https://github.com/salesforce/pytorch-qid56/commit/daadb0f39a1811128df7eb03933f286aa5e319ed
  78. https://einstein.ai/
  79. https://arxiv.org/abs/1611.01576
  80. http://pytorch.org/
  81. https://github.com/nvidia/pynvrtc
  82. https://cupy.chainer.org/
  83. https://github.com/salesforce/awd-lstm-lm
  84. https://github.com/salesforce/awd-lstm-lm
  85. http://pytorch.org/docs/master/_modules/torch/nn/modules/id56.html#lstm
  86. https://arxiv.org/abs/1611.01576
  87. https://github.com/salesforce/pytorch-qid56/blob/master/images/qid56_speed.png
  88. https://github.com/site/terms
  89. https://github.com/site/privacy
  90. https://github.com/security
  91. https://githubstatus.com/
  92. https://help.github.com/
  93. https://github.com/contact
  94. https://github.com/pricing
  95. https://developer.github.com/
  96. https://training.github.com/
  97. https://github.blog/
  98. https://github.com/about
  99. https://github.com/salesforce/pytorch-qid56
 100. https://github.com/salesforce/pytorch-qid56

   hidden links:
 102. https://github.com/
 103. https://github.com/salesforce/pytorch-qid56
 104. https://github.com/salesforce/pytorch-qid56
 105. https://github.com/salesforce/pytorch-qid56
 106. https://help.github.com/articles/which-remote-url-should-i-use
 107. https://github.com/salesforce/pytorch-qid56#quasi-recurrent-neural-network-qid56-for-pytorch
 108. https://github.com/salesforce/pytorch-qid56#software-requirements
 109. https://github.com/salesforce/pytorch-qid56#example-usage
 110. https://github.com/salesforce/pytorch-qid56#usage
 111. https://github.com/salesforce/pytorch-qid56#speed
 112. https://github.com/salesforce/pytorch-qid56#extending-the-qid56-speed-advantage-to-other-recurrent-architectures-with-forgetmult
 113. https://github.com/salesforce/pytorch-qid56#want-to-help-out
 114. https://github.com/
