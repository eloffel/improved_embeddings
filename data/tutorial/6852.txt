   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    top 10
   machine learning algorithms for beginners comments feed [5]the ways
   that ai can change your business [6]egg2017: innovate. get ahead.
   disrupt. and embrace non-conformity.

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]oct    [29]tutorials,
   overviews    top 10 machine learning algorithms for beginners
   ( [30]17:n41 )

platinum blog top 10 machine learning algorithms for beginners

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]adaboost, [34]algorithms, [35]apriori, [36]id112,
   [37]beginners, [38]boosting, [39]id90, [40]ensemble methods,
   [41]explained, [42]id116, [43]k-nearest neighbors, [44]linear
   regression, [45]id28, [46]machine learning, [47]naive
   bayes, [48]pca, [49]top 10

   a beginner's introduction to the top 10 machine learning (ml)
   algorithms, complete with figures and examples for easy understanding.

     __________________________________________________________________

   by [50]reena shaw, kdnuggets.

i. introduction


   the study of ml algorithms has gained immense traction post the harvard
   business review [51]article terming a    data scientist    as the    sexiest
   job of the 21st century   . so, for those starting out in the field of
   ml, we decided to do a reboot of our immensely popular gold blog
   [52]the 10 algorithms machine learning engineers need to know - albeit
   this post is targetted towards beginners.

   ml algorithms are those that can learn from data and improve from
   experience, without human intervention. learning tasks may include
   learning the function that maps the input to the output, learning the
   hidden structure in unlabeled data; or    instance-based learning   , where
   a class label is produced for a new instance by comparing the new
   instance (row) to instances from the training data, which were stored
   in memory.    instance-based learning    does not create an abstraction
   from specific instances.


ii. types of ml algorithms


   there are 3 types of ml algorithms:

   1. supervised learning:

   supervised learning can be explained as follows: use labeled training
   data to learn the mapping function from the input variables (x) to the
   output variable (y).

   y = f (x)

   supervised learning problems can be of two types:

   a. classification: to predict the outcome of a given sample where the
   output variable is in the form of categories. examples include labels
   such as male and female, sick and healthy.

   b. regression: to predict the outcome of a given sample where the
   output variable is in the form of real values. examples include
   real-valued labels denoting the amount of rainfall, the height of a
   person.

   the 1st 5 algorithms that we cover in this blog    id75,
   id28, cart, na  ve bayes, knn are examples of supervised
   learning.

   ensembling is a type of supervised learning. it means combining the
   predictions of multiple different weak ml models to predict on a new
   sample. algorithms 9-10 that we cover    id112 with id79s,
   boosting with xgboost are examples of ensemble techniques.

   2. unsupervised learning:

   unsupervised learning problems possess only the input variables (x) but
   no corresponding output variables. it uses unlabeled training data to
   model the underlying structure of the data.

   unsupervised learning problems can be of two types:

   a. association: to discover the id203 of the co-occurrence of
   items in a collection. it is extensively used in market-basket
   analysis. example: if a customer purchases bread, he is 80% likely to
   also purchase eggs.

   b. id91: to group samples such that objects within the same
   cluster are more similar to each other than to the objects from another
   cluster.

   c. id84: true to its name, id84
   means reducing the number of variables of a dataset while ensuring that
   important information is still conveyed. id84 can
   be done using feature extraction methods and feature selection methods.
   feature selection selects a subset of the original variables. feature
   extraction performs data transformation from a high-dimensional space
   to a low-dimensional space. example: pca algorithm is a feature
   extraction approach.

   algorithms 6-8 that we cover here - apriori, id116, pca are examples
   of unsupervised learning.

   3. id23:

   id23 is a type of machine learning algorithm that
   allows the agent to decide the best next action based on its current
   state, by learning behaviours that will maximize the reward.

   reinforcement algorithms usually learn optimal actions through trial
   and error. they are typically used in robotics     where a robot can
   learn to avoid collisions by receiving negative feedback after bumping
   into obstacles, and in video games     where trial and error reveals
   specific movements that can shoot up a player   s rewards. the agent can
   then use these rewards to understand the optimal state of game play and
   choose the next action.


iii. quantifying the popularity of ml algorithms


   survey papers [53]such as these have quantified the 10 most popular
   data mining algorithms. however, such lists are subjective and as in
   the case of the quoted paper, the sample size of the polled
   participants is very narrow and consists of advanced practitioners of
   data mining. the persons polled were the winners of the acm kdd
   innovation award, the ieee icdm research contributions award; the
   program committee members of the kdd-06, icdm   06 and sdm   06; and the
   145 attendees of the icdm   06.

   the top 10 algorithms in this blog are meant for beginners and are
   primarily those that i learnt from the    data warehousing and mining   
   (dwm) course during my bachelor   s degree in computer engineering at the
   university of mumbai. the dwm course is a great introduction to the
   field of ml algorithms. i have especially included the last 2
   algorithms (ensemble methods) based on their [54]prevalence to win
   kaggle competitions . hope you enjoy the article!


iv. supervised learning algorithms


   1. id75

   in ml, we have a set of input variables (x) that are used to determine
   the output variable (y). a relationship exists between the input
   variables and the output variable. the goal of ml is to quantify this
   relationship.

   figure 1: id75 is represented as a line in the form of y =
                             a + bx. [55]source

   in id75, the relationship between the input variables (x)
   and output variable (y) is expressed as an equation of the form y = a +
   bx. thus, the goal of id75 is to find out the values of
   coefficients a and b. here, a is the intercept and b is the slope of
   the line.

   figure 1 shows the plotted x and y values for a dataset. the goal is to
   fit a line that is nearest to most of the points. this would reduce the
   distance (   error   ) between the y value of a data point and the line.


   2. id28

   id75 predictions are continuous values (rainfall in
   cm),id28 predictions are discrete values (whether a
   student passed/failed) after applying a transformation function.

   id28 is best suited for binary classification (datasets
   where y = 0 or 1, where 1 denotes the default class. example: in
   predicting whether an event will occur or not, the event that it occurs
   is classified as 1. in predicting whether a person will be sick or not,
   the sick instances are denoted as 1). it is named after the
   transformation function used in it, called the logistic function h(x)=
   1/ (1 + e^x), which is an s-shaped curve.

   in id28, the output is in the form of probabilities of
   the default class (unlike id75, where the output is
   directly produced). as it is a id203, the output lies in the
   range of 0-1. the output (y-value) is generated by log transforming the
   x-value, using the logistic function h(x)= 1/ (1 + e^ -x) . a threshold
   is then applied to force this id203 into a binary classification.

   figure 2: id28 to determine if a tumour is malignant or
       benign. classified as malignant if the id203 h(x)>= 0.5.
                                 [56]source

   in figure 2, to determine whether a tumour is malignant or not, the
   default variable is y=1 (tumour= malignant) ; the x variable could be a
   measurement of the tumour, such as the size of the tumour. as shown in
   the figure, the logistic function transforms the x-value of the various
   instances of the dataset, into the range of 0 to 1. if the id203
   crosses the threshold of 0.5 (shown by the horizontal line), the tumour
   is classified as malignant.

   the id28 equation p(x) = e ^ (b0 +b1*x) / (1 + e^(b0 +
   b1*x)) can be transformed into ln(p(x) / 1-p(x)) = b0 + b1*x.

   the goal of id28 is to use the training data to find the
   values of coefficients b0 and b1 such that it will minimize the error
   between the predicted outcome and the actual outcome. these
   coefficients are estimated using the technique of maximum likelihood
   estimation.


   3. cart

   classification and regression trees (cart) is an implementation of
   id90, among others such as  , c4.5.

   the non-terminal nodes are the root node and the internal node. the
   terminal nodes are the leaf nodes. each non-terminal node represents a
   single input variable (x) and a splitting point on that variable; the
   leaf nodes represent the output variable (y). the model is used as
   follows to make predictions: walk the splits of the tree to arrive at a
   leaf node and output the value present at the leaf node.

   the decision tree in figure3 classifies whether a person will buy a
   sports car or a minivan depending on their age and marital status. if
   the person is over 30 years and is not married, we walk the tree as
   follows :    over 30 years?    -> yes ->    married?    -> no. hence, the model
   outputs a sportscar.

               figure 3: parts of a decision tree. [57]source


   4. na  ve bayes

   to calculate the id203 that an event will occur, given that
   another event has already occurred, we use bayes    theorem. to calculate
   the id203 of an outcome given the value of some variable, that
   is, to calculate the id203 of a hypothesis(h) being true, given
   our prior knowledge(d), we use bayes    theorem as follows:

   p(h|d)= (p(d|h) * p(h)) / p(d)

   where
     * p(h|d) = posterior id203. the id203 of hypothesis h
       being true, given the data d, where p(h|d)= p(d1| h)* p(d2|
       h)*....*p(dn| h)* p(d)
     * p(d|h) = likelihood. the id203 of data d given that the
       hypothesis h was true.
     * p(h) = class prior id203. the id203 of hypothesis h
       being true (irrespective of the data)
     * p(d) = predictor prior id203. id203 of the data
       (irrespective of the hypothesis)

   this algorithm is called    naive    because it assumes that all the
   variables are independent of each other, which is a naive assumption to
   make in real-world examples.

  figure 4: using naive bayes to predict the status of    play    using the
                           variable    weather   .

   using figure 4 as an example, what is the outcome if weather=   sunny   ?

   to determine the outcome play=    yes    or    no    given the value of
   variable weather=   sunny   , calculate p(yes|sunny) and p(no|sunny) and
   choose the outcome with higher id203.

   ->p(yes|sunny)= (p(sunny|yes) * p(yes)) /  p(sunny)

    = (3/9  * 9/14 ) / (5/14)

    = 0.60


   -> p(no|sunny)=  (p(sunny|no) * p(no)) /  p(sunny)

    = (2/5  * 5/14 ) / (5/14)

    = 0.40

   thus, if the weather =   sunny   , the outcome is play=    yes   .


   5. knn

   the k-nearest neighbours algorithm uses the entire dataset as the
   training set, rather than splitting the dataset into a trainingset and
   testset.

   when an outcome is required for a new data instance, the knn algorithm
   goes through the entire dataset to find the k-nearest instances to the
   new instance, or the k number of instances most similar to the new
   record, and then outputs the mean of the outcomes (for a regression
   problem) or the mode (most frequent class) for a classification
   problem. the value of k is user-specified.

   the similarity between instances is calculated using measures such as
   euclidean distance and hamming distance.

   pages: 1 [58]2
     __________________________________________________________________

   [59][prv.gif] previous post
   [60]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [61]another 10 free must-read books for machine learning and data
       science
    2. [62]9 must-have skills you need to become a data scientist, updated
    3. [63]who is a typical data scientist in 2019?
    4. [64]the pareto principle for data scientists
    5. [65]what no one will tell you about data science job applications
    6. [66]19 inspiring women in ai, big data, data science, machine
       learning
    7. [67]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [68]id158s optimization using genetic algorithm
       with python
    2. [69]who is a typical data scientist in 2019?
    3. [70]8 reasons why you should get a microsoft azure certification
    4. [71]the pareto principle for data scientists
    5. [72]r vs python for data visualization
    6. [73]how to work in data science, ai, big data
    7. [74]the deep learning toolset     an overview

[75]latest news

     * [76]download your datax guide to ai in marketing
     * [77]kdnuggets offer: save 20% on strata in london
     * [78]training a champion: building deep neural nets for big ...
     * [79]building a recommender system
     * [80]predict age and gender using convolutional neural netwo...
     * [81]top tweets, mar 27     apr 02: here is a great ex...

more recent stories

     * [82]top tweets, mar 27     apr 02: here is a great explanat...
     * [83]odsc east is selling out; odsc india announced
     * [84]accelerate ai and data science career expo, may 4, 2019
     * [85]grow your data career at datasciencego, san diego, sep 27-29
     * [86]getting started with nlp using the pytorch framework
     * [87]how to diy your data science education
     * [88]top 8 data science use cases in gaming
     * [89]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [90]make better data-driven business decisions
     * [91]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [92]two predictive analytics world events in europe this fall
     * [93]7 qualities your big data visualization tools absolutely must
       ...
     * [94]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [95]which face is real?
     * [96]yeshiva university: program director / tenure track faculty
       me...
     * [97]top 10 coding mistakes made by data scientists
     * [98]uber   s case study at paw industry 4.0: machine learning ...
     * [99]xai     a data scientist   s mouthpiece
     * [100]what does gpt-2 think about the ai arms race?
     * [101]openclassrooms: data freelance online course creator
       [telecomm...

   [102]kdnuggets home    [103]news    [104]2017    [105]oct   
   [106]tutorials, overviews    top 10 machine learning algorithms for
   beginners ( [107]17:n41 )
      2019 kdnuggets. [108]about kdnuggets.  [109]privacy policy.
   [110]terms of service

   [111]subscribe to kdnuggets news
   [112][tw_c48.png] [113]facebook [114]linkedin
   x
   [envelope.png] [115]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/feed
   5. https://www.kdnuggets.com/2017/10/ways-ai-change-business.html
   6. https://www.kdnuggets.com/2017/10/egg2017-innovate-get-ahead-disrupt-embrace-non-conformity.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/10/index.html
  29. https://www.kdnuggets.com/2017/10/tutorials.html
  30. https://www.kdnuggets.com/2017/n41.html
  31. https://www.kdnuggets.com/2017/10/ways-ai-change-business.html
  32. https://www.kdnuggets.com/2017/10/egg2017-innovate-get-ahead-disrupt-embrace-non-conformity.html
  33. https://www.kdnuggets.com/tag/adaboost
  34. https://www.kdnuggets.com/tag/algorithms
  35. https://www.kdnuggets.com/tag/apriori
  36. https://www.kdnuggets.com/tag/id112
  37. https://www.kdnuggets.com/tag/beginners
  38. https://www.kdnuggets.com/tag/boosting
  39. https://www.kdnuggets.com/tag/decision-trees
  40. https://www.kdnuggets.com/tag/ensemble-methods
  41. https://www.kdnuggets.com/tag/explained
  42. https://www.kdnuggets.com/tag/id116
  43. https://www.kdnuggets.com/tag/k-nearest-neighbors
  44. https://www.kdnuggets.com/tag/linear-regression
  45. https://www.kdnuggets.com/tag/logistic-regression
  46. https://www.kdnuggets.com/tag/machine-learning
  47. https://www.kdnuggets.com/tag/naive-bayes
  48. https://www.kdnuggets.com/tag/pca
  49. https://www.kdnuggets.com/tag/top-10
  50. https://www.kdnuggets.com/author/reena-shaw
  51. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century
  52. https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html
  53. http://www.cs.uvm.edu/~icdm/algorithms/10algorithms-08.pdf
  54. http://www.datasciencecentral.com/profiles/blogs/want-to-win-at-kaggle-pay-attention-to-your-ensembles
  55. http://bhagyeshvikani.blogspot.ca/2015/10/linear-regression.html
  56. https://athemathmo.github.io/2016/03/07/rusty-machine.html
  57. http://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter001/section002/green/page001.html
  58. https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2
  59. https://www.kdnuggets.com/2017/10/ways-ai-change-business.html
  60. https://www.kdnuggets.com/2017/10/egg2017-innovate-get-ahead-disrupt-embrace-non-conformity.html
  61. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  62. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  63. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  64. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  65. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  66. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  67. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  68. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  69. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  70. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  71. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  72. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  73. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  74. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  75. https://www.kdnuggets.com/news/index.html
  76. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  77. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  78. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  79. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  80. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  81. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  82. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  83. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  84. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  85. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  86. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  87. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  88. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  89. https://www.kdnuggets.com/2019/n13.html
  90. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  91. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  92. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  93. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  94. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  95. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  96. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  97. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
  98. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
  99. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
 100. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
 101. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
 102. https://www.kdnuggets.com/
 103. https://www.kdnuggets.com/news/index.html
 104. https://www.kdnuggets.com/2017/index.html
 105. https://www.kdnuggets.com/2017/10/index.html
 106. https://www.kdnuggets.com/2017/10/tutorials.html
 107. https://www.kdnuggets.com/2017/n41.html
 108. https://www.kdnuggets.com/about/index.html
 109. https://www.kdnuggets.com/news/privacy-policy.html
 110. https://www.kdnuggets.com/terms-of-service.html
 111. https://www.kdnuggets.com/news/subscribe.html
 112. https://twitter.com/kdnuggets
 113. https://facebook.com/kdnuggets
 114. https://www.linkedin.com/groups/54257
 115. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 117. https://www.kdnuggets.com/
 118. https://www.kdnuggets.com/
