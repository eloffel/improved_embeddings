   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

teaching a variational autoencoder (vae) to draw mnist characters

   [16]go to the profile of felix mohr
   [17]felix mohr (button) blockedunblock (button) followfollowing
   oct 20, 2017
   [1*wowzbxu2bmshm1czeur72g.gif]
   these characters have not been written by a human         we taught a neural
   network how to do this!

   to see the full vae code, please refer to my [18]github.

   autoencoders are a type of neural network that can be used to learn
   efficient codings of input data. given some inputs, the network first
   applies a series of transformations that map the input data into a
   lower dimensional space. this part of the network is called the
   encoder.

   then, the network uses the encoded data to try and recreate the inputs.
   this part of the network is the decoder. using the encoder, we can
   compress data of the type that is understood by the network. however,
   autoencoders are rarely used for this purpose, as usually there exist
   hand-crafted algorithms (like jpg-compression) that are more efficient.

   instead, autoencoders have repeatedly been applied to perform
   de-noising tasks. the encoder receives pictures that have been tampered
   with noise, and it learns how to reconstruct the original images.

what are id5?

   however, there are much more interesting applications for autoencoders.

   one such application is called the variational autoencoder. using
   id5, it   s not only possible to compress
   data         it   s also possible to generate new objects of the type the
   autoencoder has seen before.

   using a general autoencoder, we don   t know anything about the coding
   that   s been generated by our network. we could compare different
   encoded objects, but it   s unlikely that we   ll be able to understand
   what   s going on. this means that we won   t be able to use our decoder
   for creating new objects. we simply don   t know what the inputs should
   look like.

   using a variational autoencoder, we take the opposite approach. we will
   not try to make guesses concerning the distribution that   s being
   followed by the latent vectors. we simply tell our network what we want
   this distribution to look like.

   usually, we will constrain the network to produce latent vectors having
   entries that follow the unit normal distribution. then, when trying to
   generate data, we can simply sample some values from this distribution,
   feed them to the decoder, and the decoder will return us completely new
   objects that appear just like the objects our network has been trained
   with.

   let   s see how this can be done using python and tensorflow. we are
   going to teach our network how to draw [19]mnist characters.

first steps         loading the training data

   firstly, we perform some basic imports. tensorflow has a handy function
   that allows us to easily access the mnist data set.

   iframe: [20]/media/2f426cebd98c486e9702651f352a4a0a?postid=978675c95776

defining our input and output data

   mnist images have a dimension of 28 * 28 pixels with one color channel.
   our inputs x_in will be batches of mnist characters. the network will
   learn to reconstruct them and output them in a placeholder y, which has
   the same dimensions.

   y_flat will be used later, when computing losses. keep_prob will be
   used when applying dropouts as a means of id173. during
   training, it will have a value of 0.8. when generating new data, we
   won   t apply dropout, so the value will be 1.

   the function lrelu is being defined, as tensorflow unfortunately
   doesn   t come up with a predefined leaky relu.

   iframe: [21]/media/46beeb16cd6fd483bb84c008b5a0a56d?postid=978675c95776

defining the encoder

   as our inputs are images, it   s most reasonable to apply some
   convolutional transformations to them. what   s most noteworthy is the
   fact that we are creating two vectors in our encoder, as the encoder is
   supposed to create objects following a gaussian distribution:
     * a vector of means
     * a vector of standard deviations

   you will see later how we    force    the encoder to make sure it really
   creates values following a normal distribution. the returned values
   that will be fed to the decoder are the z-values. we will need the mean
   and standard deviation of our distributions later, when computing
   losses.

   iframe: [22]/media/3461a61ffca258b523bd7779d6080223?postid=978675c95776

defining the decoder

   the decoder does not care about whether the input values are sampled
   from some specific distribution that has been defined by us. it simply
   will try to reconstruct the input images. to this end, we use a series
   of transpose convolutions.

   iframe: [23]/media/7e1dd53eb62ece84693a3c273f71d223?postid=978675c95776

   now, we   ll wire together both parts:

   iframe: [24]/media/86b54482dc89e8d7981056df09b97c83?postid=978675c95776

computing losses and enforcing a gaussian latent distribution

   for computing the image reconstruction loss, we simply use squared
   difference (which could lead to images sometimes looking a bit fuzzy).
   this loss is combined with the id181, which makes
   sure our latent values will be sampled from a normal distribution. for
   more on this topic, please take a look a [25]jaan altosaar   s great
   article on vaes.

   iframe: [26]/media/d406811f77481b30521cd46b4c78932a?postid=978675c95776

training the network

   now, we can finally train our vae!

   every 200 steps, we   ll take a look at what the current reconstructions
   look like. after having processed about 2000 batches, most
   reconstructions will look reasonable.

   iframe: [27]/media/0a441833bb083ea1b0b4ababd846acca?postid=978675c95776

generating new data

   the most awesome part is that we are now able to create new characters.
   to this end, we simply sample values from a unit normal distribution
   and feed them to our decoder. most of the created characters look just
   like they   ve been written by humans.

   iframe: [28]/media/518e469693a231ece0c650842ae7d952?postid=978675c95776

   [1*s8rrod7ablreriwuizn_ag.jpeg]
   some of the automatically created characters.

conclusion

   now, this is a relatively simple example of an application of vaes. but
   just think about what could be possible! neural networks could learn to
   compose music. they could automatically create illustrations for books,
   games etc. with a bit of creativity, vaes will open up space for some
   awesome projects.
   [29]felixmohr/deep-learning-with-python
   contribute to deep-learning-with-python development by creating an
   account on github.github.com

     * [30]machine learning
     * [31]deep learning
     * [32]data science
     * [33]tensorflow
     * [34]towards data science

   (button)
   (button)
   (button) 738 claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of felix mohr

[36]felix mohr

   computer scientist in karlsruhe, germany

     (button) follow
   [37]towards data science

[38]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 738
     * (button)
     *
     *

   [39]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [40]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/978675c95776
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_cnb7onqf6kmf---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@felixmohr?source=post_header_lockup
  17. https://towardsdatascience.com/@felixmohr
  18. https://github.com/felixmohr/deep-learning-with-python/blob/master/vae.ipynb
  19. https://en.wikipedia.org/wiki/mnist_database
  20. https://towardsdatascience.com/media/2f426cebd98c486e9702651f352a4a0a?postid=978675c95776
  21. https://towardsdatascience.com/media/46beeb16cd6fd483bb84c008b5a0a56d?postid=978675c95776
  22. https://towardsdatascience.com/media/3461a61ffca258b523bd7779d6080223?postid=978675c95776
  23. https://towardsdatascience.com/media/7e1dd53eb62ece84693a3c273f71d223?postid=978675c95776
  24. https://towardsdatascience.com/media/86b54482dc89e8d7981056df09b97c83?postid=978675c95776
  25. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
  26. https://towardsdatascience.com/media/d406811f77481b30521cd46b4c78932a?postid=978675c95776
  27. https://towardsdatascience.com/media/0a441833bb083ea1b0b4ababd846acca?postid=978675c95776
  28. https://towardsdatascience.com/media/518e469693a231ece0c650842ae7d952?postid=978675c95776
  29. https://github.com/felixmohr/deep-learning-with-python/blob/master/vae.ipynb
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/deep-learning?source=post
  32. https://towardsdatascience.com/tagged/data-science?source=post
  33. https://towardsdatascience.com/tagged/tensorflow?source=post
  34. https://towardsdatascience.com/tagged/towards-data-science?source=post
  35. https://towardsdatascience.com/@felixmohr?source=footer_card
  36. https://towardsdatascience.com/@felixmohr
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://github.com/felixmohr/deep-learning-with-python/blob/master/vae.ipynb
  43. https://medium.com/p/978675c95776/share/twitter
  44. https://medium.com/p/978675c95776/share/facebook
  45. https://medium.com/p/978675c95776/share/twitter
  46. https://medium.com/p/978675c95776/share/facebook
