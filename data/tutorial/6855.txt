   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

welcome to deep id23 part 1 : id25

   [16]go to the profile of takuma seno
   [17]takuma seno (button) blockedunblock (button) followfollowing
   oct 19, 2017
   [1*jbhnpbd_s4vu0y7myqj4oq.jpeg]

   recent years, many ai laboratories are working on studying deep
   id23 (drl) which is expected to be a core technology
   in the future. i   m also engaging in drl research at keio university.
   here i will write my survey about recent drl in the next several
   articles. all of them assume readers have basic knowledge of
   id23.
     __________________________________________________________________

   in this article, i introduce deep q-network (id25) that is the first
   deep id23 method proposed by deepmind. after the
   paper was published on nature in 2015, a lot of research institutes
   joined this field because deep neural network can empower rl to
   directly deal with high dimensional states like images, thanks to
   techniques used in id25. let   s see what a big achievement id25 has done.

arcade learning environment (ale)

   [18]the arcade learning environment: an evaluation platform for general
   agents was published in 2013, which proposes learning environments for
   ai. ale has a lot of games originally designed for a classical game
   console, atari 2600. probably, some of the games such as pong,
   spaceinvaders, pacman and breakout are very known to everyone.
   [1*eqitx5ug7soprfbdfba79w.png]
   atari games available in ale

   this emulator has the mission, which offers the platform where ai can
   play a lot of games without any specific feature designs. before id25
   was published, rl agent needs hand-engineered features as inputs. for
   example, invader positions are explicitly extracted when rl agent plays
   spaceinvaders. but this information is completely useless in playing
   breakout. therefore agents can   t rely on that kind of features to play
   all of the games in ale.

deep neural network (dnn)

   [19]alexnet has achieved an incredible score in ilsvrc 2012, image
   classification competition by using dnn.
   [1*5kzs7lltwpqrs5es_ndzjq.png]
   alexnet

   the greatest thing of dnn is extracting feature representations through
   id26.
   [1*ymq wssacpste4ahnzcw.png]
   learned weights of a convolutional layer in alexnet

   classifiers don   t need hand-engineered features any more because of
   this ability. after appropriate many id26s, dnn knows which
   information like color or shape is important to do the task.

bringing dnn into rl

   people naturally think that dnn enables rl agent to associate images to
   values. however things are not easy.
   [1*1o0toxae9k3aeveg2gqikg.png]
   comparison between naive id25 and linear model (with id25 techniques)
   from nature

   naive id25 has 3 convolutional layers and 2 fully connected layers to
   estimate q values directly from images. on the other hand, linear model
   has only 1 fully connected layer with some learning techniques talked
   in the next section. both model learns q values in id24 way. as
   you see the above table, naive id25 has very poor results worse than
   even linear model because dnn is easily overfitting in online
   id23.

deep q-network

   id25 is introduced in 2 papers, [20]playing atari with deep
   id23 on nips in 2013 and [21]human-level control
   through deep id23 on nature in 2015. interestingly,
   there were only few papers about drn between 2013 and 2015. i guess
   that the reason was people couldn   t reproduce id25 implementation
   without information in nature version.
   [0*35fy2zkw6ztjlgo-.]
   id25 agent playing breakout

   id25 overcomes unstable learning by mainly 4 techniques.
     * experience replay
     * target network
     * clipping rewards
     * skipping frames

   i explain each technique one by one.

experience replay

   experience replay is originally proposed in [22]id23
   for robots using neural networks in 1993. dnn is easily overfitting
   current episodes. once dnn is overfitted, it   s hard to produce various
   experiences. to solve this problem, experience replay stores
   experiences including state transitions, rewards and actions, which are
   necessary data to perform id24, and makes mini-batches to update
   neural networks. this technique expects the following merits.
     * reduces correlation between experiences in updating dnn
     * increases learning speed with mini-batches
     * reuses past transitions to avoid catastrophic forgetting

target network

   in td error calculation, target function is changed frequently with
   dnn. unstable target function makes training difficult. so target
   network technique fixes parameters of target function and replaces them
   with the latest network every thousands steps.
   [1*gqg5g7pxlphv35mchecwia.png]
   target q function in the red rectangular is fixed

clipping rewards

   each game has different score scales. for example, in pong, players can
   get 1 point when wining the play. otherwise, players get -1 point.
   however, in spaceinvaders, players get 10~30 points when defeating
   invaders. this difference would make training unstable. thus clipping
   rewards technique clips scores, which all positive rewards are set +1
   and all negative rewards are set -1.

skipping frames

   ale is capable of rendering 60 images per second. but actually people
   don   t take actions so much in a second. ai doesn   t need to calculate q
   values every frame. so skipping frames technique is that id25 calculates
   q values every 4 frames and use past 4 frames as inputs. this reduces
   computational cost and gathers more experiences.

performance

   all of above techniques enables id25 to achieve stable training.
   [1*mo7vmyohej5tvr3mrjgvpg.png]
   id25 overwhelms naive id25

   in nature version, it shows how much experience replay and target
   network contribute to stability.
   [1*v81as-udbknulyi9onpc0a.png]
   performance with and without experience replay and target network

   experience replay is very important in id25. target network also
   increases its performance.

conclusion

   id25 has achieved human-level control in many of atari games with above
   4 techniques. however there are still some games id25 cannot play. i
   will introduce papers that struggle them in this series.

   next, i provide tensorflow implementation of id25.
     * welcome to deep id23 part2: id25 in tensorflow
       (coming soon)

     * [23]machine learning
     * [24]id23
     * [25]deep learning
     * [26]ai
     * [27]computer science

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of takuma seno

[29]takuma seno

   a graduated computer science student at keio university. my research
   theme is deep id23. i   m working at sony as a machine
   learning internship.

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c3cab4d41b6b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-id25-c3cab4d41b6b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-id25-c3cab4d41b6b&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_5ayah94xasvz---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@takuma.seno?source=post_header_lockup
  17. https://towardsdatascience.com/@takuma.seno
  18. https://arxiv.org/pdf/1207.4708.pdf
  19. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  20. https://arxiv.org/pdf/1312.5602.pdf
  21. http://www.davidqiu.com:8888/research/nature14236.pdf
  22. https://pdfs.semanticscholar.org/54c4/cf3a8168c1b70f91cf78a3dc98b671935492.pdf
  23. https://towardsdatascience.com/tagged/machine-learning?source=post
  24. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  25. https://towardsdatascience.com/tagged/deep-learning?source=post
  26. https://towardsdatascience.com/tagged/ai?source=post
  27. https://towardsdatascience.com/tagged/computer-science?source=post
  28. https://towardsdatascience.com/@takuma.seno?source=footer_card
  29. https://towardsdatascience.com/@takuma.seno
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/c3cab4d41b6b/share/twitter
  36. https://medium.com/p/c3cab4d41b6b/share/facebook
  37. https://medium.com/p/c3cab4d41b6b/share/twitter
  38. https://medium.com/p/c3cab4d41b6b/share/facebook
