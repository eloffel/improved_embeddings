   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    a glimpse into the self-organizing maps (som)
   comments feed [4]alternate [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

a glimpse into the self-organizing maps (som)

   [25]10/22/201710/22/2017[26]artificial intelligence, [27]computational
   neuroscience, [28]machine learning, [29]machine learning algorithms
   addenda, [30]neural networks, [31]python[32]no comments

   self-organizing maps (som) are neural structures proposed for the first
   time by the computer scientist [33]t. kohonen in the late 1980s (that   s
   why they are also known as kohonen networks). their peculiarities are
   the ability to auto-cluster data according to the topological features
   of the samples and the approach to the learning process. contrary to
   methods like gaussian mixtures or id116, a som learns through
   a competitive learning process. in other words, the model tries to
   specialize its neurons so to be able to produce a response only for a
   particular pattern family (it can also be a single input sample
   representing a family, like a handwritten letter).

   let   s consider a dataset containing n p-dimensional samples, a suitable
   som is a matrix (other shapes, like toroids, are also possible)
   containing (k    l) receptors and each of them is made up of p synaptic
   weights. the resulting structure is a tridimensional matrix w with a
   shape (k    l    p).

   during the learning process, a sample x is drawn from the data
   generating distribution x and a winning unit is computed as:

   in the previous formula, i   ve adopted the vectorized notation. the
   process must compute the distance between x and all p-dimensional
   vectors w[i, j], determining the tuple (i, j) corresponding to the unit
   has shown the maximum activation (minimum distance). once this winning
   unit has been found, a distance function must be computed. consider the
   schema showed in the following figure:

   at the beginning of the training process, we are not sure if the
   winning unit will remain the same, therefore we apply the update to a
   neighborhood centered in (i, j) and a radius which decreases
   proportionally to the training epoch. in the figure, for example, the
   winning unit can start considering also the units (2, 3)     (2, l),    ,
   (k, 3)     (k, l), but, after some epochs, the radius becomes 1,
   considering only (i, j) without any other neighbour.

   this function is represented as (k    l) matrix whose generic element
   is:

   the numerator of the exponential is the euclidean distance between the
   winning unit and the generic receptor (i, j). the distance is
   controlled by the parameter   (t) which should decrease (possibly
   exponentially) with the number of epochs. many authors (like floreano
   and mattiussi, see the reference for further information) suggest
   introducing a time-constant    and defining   (t) as:

   the competitive update rule for the weights is:

   where   (t) is the learning rate, that can be fixed (for example    =
   0.05) or exponentially decaying like   (t). the weights are updated
   summing the   w term:

   as it   s possible to see, the weights belonging to the neighboorhood of
   the winning unit (determined by   f) are simply moved closer to x, while
   the others remain fixed (  f = 0). the role of the distance function is
   to impose a maximum update (which must produce the strongest response)
   in proximity to the winning unit and a decreasing one to all the other
   units. in the following figure there   s a bidimensional representation
   of this process:

   all the units with i < i-2 and i > i+2 are kept fixed. therefore, when
   the distance function becomes narrow so to include only the winning
   unit, a maximum selectivity principle is achieved through a competitive
   process. this strategy is necessary to allow the map creating the
   responsive areas in the most efficient way. without a distance
   function, the competition could not happen at all or, in some cases,
   the same unit could be activated by the same patterns, driving the
   network to an infinite cycle.

   even if we don   t provide any proof, a kohonen network trained using a
   decreasing distance function will converge to a stable state if the
   matrix (k    l) is large enough. in general, i suggest, to try with
   smaller matrices (but with kl > n) and increasing the dimensions until
   the desired result is achieved. unfortunately, however, the process is
   quite slow and the convergence is achieved normally after thousands of
   iterations of the whole dataset. for this reason, it   s often preferable
   to reduce the dimensionality (via pca, kernel-pca or nmf) and using a
   standard id91 algorithm.

   an important element is the initialization of w. there are no best
   practices, however, if the weights are already close to the most
   significant components of the population, the convergence can be
   faster. a possibility is to perform a spectral decomposition (like in a
   pca) to determine the principal eigenvector(s). however, when the
   dataset presents non-linearities, this process is almost useless. i
   suggest initializing the weights randomly sampling the values from a
   uniform(min(x), max(x)) distribution (x is the input dataset). another
   possibility is to try different initializations computing the average
   error on the whole dataset and taking the values that minimize this
   error (it   s useful to try to sample from gaussian and uniform
   distributions with a number of trials greater than 10). sampling from a
   uniform distribution, in general, produces average errors with a
   standard deviation < 0.1, therefore it doesn   t make sense to try too
   many combinations.

   to test the som, i   ve decided to use the first 100 faces from the
   olivetti dataset (at&t laboratories cambridge), provided by
   scikit-learn. each sample is length 4096 floats [0, 1] (corresponding
   to 64    64 grayscale images). i   ve used a matrix with shape (20    20),
   5000 iterations, using a distance function with   (0) = 10 and    = 400
   for the first 100 iterations and fixing the values    = 0.01 and    =
   0.1    0.5 for the remaining ones. in order to speed up the
   block-computations, i   ve decided to use [34]cupy that works like numpy
   but exploits the gpu (it   s possible to switch to numpy simply changing
   the import and using the namespace np instead of cp). unfortunately,
   there are many cycles and it   s not so easy to parallelize all the
   operations. the code is available in this [35]gist:
   view the code on [36]gist.

   the matrix with the weight vectors reshaped as square images (like the
   original samples), is shown in the following figure:

   a sharped-eyed reader can see some slight differences between the faces
   (in the eyes, nose, mouth, and brow). the most defined faces correspond
   to winning units, while the others are neighbors. consider that each of
   them is a synaptic weight and it must match a specific pattern. we can
   think of those vectors as pseudo-eigenfaces like in pca, even if the
   competitive learning tries to find the values that minimize the
   euclidean distance, so the objective is not finding independent
   components. as the dataset is limited to 100 samples, not all the face
   details have been included in the training set.

   references:
     * floreano d., mattiussi c., [37]bio-inspired artificial
       intelligence: theories, methods, and technologies, the mit press

   see also:

[38]ml algorithms addendum: passive aggressive algorithms     giuseppe
bonaccorso

     passive aggressive algorithms are a family of online learning
     algorithms (for both classification and regression) proposed by
     crammer at al. the idea is very simple and their performance has
     been proofed to be superior to many other alternative methods like
     online id88 and mira (see the original paper in the reference
     section).

share:

     * [39]click to share on twitter (opens in new window)
     * [40]click to share on facebook (opens in new window)
     * [41]click to share on linkedin (opens in new window)
     * [42]click to share on pocket (opens in new window)
     * [43]click to share on tumblr (opens in new window)
     * [44]click to share on reddit (opens in new window)
     * [45]click to share on pinterest (opens in new window)
     * [46]click to share on skype (opens in new window)
     * [47]click to share on whatsapp (opens in new window)
     * [48]click to share on telegram (opens in new window)
     * [49]click to email this to a friend (opens in new window)
     * [50]click to print (opens in new window)
     *

you can also be interested in these articles:

   [51]id91[52]kohonen[53]machine
   learning[54]neuroscience[55]python[56]som

post navigation

   [57]ml algorithms addendum: passive aggressive algorithms
   [58]hetero-associative memories for non experts: how    stories    are
   memorized with image-associations

leave a reply [59]cancel reply

   iframe: [60]jetpack_remote_comment

follow me

     * [61]linkedin
     * [62]twitter
     * [63]facebook
     * [64]github
     * [65]instagram
     * [66]amazon
     * [67]medium
     * [68]rss

search articles

   ____________________ (button)

latest blog posts

     * [69]machine learning algorithms     second edition 08/28/2018
     * [70]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [71]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [72]a book that every data scientist should read 06/22/2018
     * [73]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [74]my tweets

   copyright    2019 [75]giuseppe bonaccorso. all rights reserved.
   [76]privacy policy - [77]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [78]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/
  26. https://www.bonaccorso.eu/category/generic/artificial-intelligence/
  27. https://www.bonaccorso.eu/category/computational-neuroscience/
  28. https://www.bonaccorso.eu/category/machine-learning/
  29. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
  30. https://www.bonaccorso.eu/category/machine-learning/neural-networks/
  31. https://www.bonaccorso.eu/category/software/python/
  32. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/#comments
  33. https://en.wikipedia.org/wiki/teuvo_kohonen
  34. https://cupy.chainer.org/
  35. https://gist.github.com/giuseppebonaccorso/ee57c79334164dcfd6d9e5c422229e19
  36. https://gist.github.com/giuseppebonaccorso/ee57c79334164dcfd6d9e5c422229e19
  37. https://amzn.to/2yzxvfm
  38. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/
  39. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=twitter
  40. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=facebook
  41. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=linkedin
  42. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=pocket
  43. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=tumblr
  44. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=reddit
  45. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=pinterest
  46. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=skype
  47. https://api.whatsapp.com/send?text=a glimpse into the self-organizing maps (som) https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/
  48. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=telegram
  49. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/?share=email
  50. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/#print
  51. https://www.bonaccorso.eu/tag/id91/
  52. https://www.bonaccorso.eu/tag/kohonen/
  53. https://www.bonaccorso.eu/tag/machine-learning/
  54. https://www.bonaccorso.eu/tag/neuroscience/
  55. https://www.bonaccorso.eu/tag/python/
  56. https://www.bonaccorso.eu/tag/som/
  57. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/
  58. https://www.bonaccorso.eu/2017/12/31/hetero-associative-memories-non-experts-stories-memorized-image-associations/
  59. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/#respond
  60. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=1726&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=6ed1f849918d2d5e28d6d982ecbd7dbcaa7e13e2#parent=https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/
  61. https://www.linkedin.com/in/giuseppebonaccorso/
  62. https://twitter.com/giuseppeb/
  63. https://www.facebook.com/giuseppe.bonaccorso/
  64. https://github.com/giuseppebonaccorso/
  65. https://www.instagram.com/giuseppebonaccorso/
  66. https://www.amazon.com/author/giuseppebonaccorso
  67. https://medium.com/@giuseppe.bonaccorso
  68. https://www.bonaccorso.eu/feed/
  69. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
  70. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
  71. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
  72. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
  73. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
  74. https://twitter.com/giuseppeb
  75. https://www.bonaccorso.eu/
  76. https://www.iubenda.com/privacy-policy/331721
  77. https://www.iubenda.com/privacy-policy/331721/cookie-policy
  78. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/#cancel

   hidden links:
  80. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
