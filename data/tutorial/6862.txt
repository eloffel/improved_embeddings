   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]intuition machine
     * [9]patterns
     * [10]methodology
     * [11]strategy
     * [12]deep learning playbook

   (button)
   loading   
   0:00
   17:55
     __________________________________________________________________

why alphago zero is a quantum leap forward in deep learning

   go to the profile of carlos e. perez
   [13]carlos e. perez (button) blockedunblock (button) followfollowing
   oct 22, 2017
   [1*2ofooa-dveroxgbzkcp-ca.jpeg]
   credit: war games (1983) [14]http://www.imdb.com/title/tt0086567/

     self-play is automated knowledge creation

   the 1983 movie    war games    has a memorable climax where the
   supercomputer known as wopr (war operation plan response) is asked to
   train on itself to discover the concept of an un-winnable game. the
   character played by mathew broderick asks    is there any way that it can
   play itself?   

   iframe: [15]/media/672b1a5a3423932477f1f4a0b64571ca?postid=6e3274fcdd9f

   34 years later, deepmind has shown how this is exactly done in real
   life! the solution is the same, set the number of players to zero (i.e.
   zero humans).

   there is plenty to digest about this latest breakthrough in deep
   learning technology. deepmind authors use the term    self-play
   id23   . as i remarked in the piece about    [16]tribes
   of ai   , deepmind is particularly fond of their id23
   (rl) approach. deepmind has taken the use of deep learning layers in
   combination with more classical rl approaches to an art form.

   alphago zero is the latest incarnation of its go-playing automation.
   one would think that it would be hard to top the alphago version that
   bested the human world champion in go. alphago zero however not only
   beats the previous system, but does it in a manner that validates a
   revolutionary approach. to be more specific, this is what alphago has
   been able to accomplish:
    1. beat the previous version of alphago (final score: 100   0).
    2. learn to perform this task from scratch, without learning from
       previous human knowledge (i.e. recorded game play).
    3. world champion level go playing in just 3 days of training.
    4. do so with an order of magnitude less neural networks ( 4 tpus vs
       48 tpus).
    5. do this with less training data (3.9 million games vs 30 millions
       games).

   each of the above bullet points is a newsworthy headline. the
   combination of each bullet point and what it reveals is completely
   overwhelming. this is my honest attempt to make sense of all of this.

   the first bullet point for many will seem unremarkable. perhaps it   s
   because incremental improvements in technology have always been the
   norm. perhaps one algorithm besting another algorithm 100 straight
   times intuitively doesn   t have the same appeal of one human besting
   another human 100 straight times. algorithms don   t have the kind of
   inconsistency that we find in humans.

   one would expect though that the game of go would have a large enough
   search space that there would be a chance of a less capable algorithm
   to be lucky enough to beat a better own. could it be that alphago zero
   has learned new alien moves that its competitors are unable to reason
   about the same search space and thus having an insurmountable
   disadvantage. this apparently seems to be the case and is sort of
   alluded to by the fact that alphago zero requires less compute
   resources to best its competitors. clearly, it   s doing a lot less work,
   but perhaps it is just working off a much richer language of go
   strategy. less work is what biological creatures aspire to do. language
   compression is a means to arrive at less cognitive work.

   the second bullet point challenges our current paradigm of supervised
   only machine learning. the original alphago was bootstrapped using
   previously recorded tournament gameplay. this was then followed with
   self-play to improve its two internal neural networks (i.e. policy and
   value networks). in contrast, alphago zero started from scratch with
   just the rules of go programmed. it also required a single network
   rather than two. it is indeed surprising that it was able to bootstrap
   itself and then eventually learning more advanced human strategies as
   well as previously unknown strategies. furthermore, the order in what
   strategies it learned first were sometimes unexpected. it is as if the
   system had learned a new internal language of how to play go. it is
   also interesting to speculate as to the effect of a single integrated
   neural network versus two disjoint neural networks. perhaps there are
   certain strategies that a disjoint network cannot learn.

   humans learn languages through metaphors and stories. the human
   strategies discovered in go are referred to with names so as to be
   recognizable by a player. it could be possible that the human language
   of go is inefficient in that it is unable to express more complex
   compound concepts. what alphago zero seems to be able to do is perform
   its moves in a way that satisfies multiple objectives at the same time.
   so humans and perhaps earlier versions of alphago were constrained to a
   relatively linear way of thinking, while alphago zero was not
   encumbered with an inefficient language of strategy. it is also
   interesting that one may consider this a system that actually doesn   t
   use the implicit bias that may reside in a language. david silver, of
   deepmind, has an even more bold claim:

     it   s more powerful than previous approaches because by not using
     human data, or human expertise in any fashion, we   ve removed the
     constraints of human knowledge and it is able to create knowledge
     itself.

   the [17]atlantic reports about some interesting observation of the game
   play of this new system:

     expert players are also noticing alphago   s idiosyncrasies. lockhart
     and others mention that it almost fights various battles
     simultaneously, adopting an approach that might seem a bit madcap to
     human players, who   d probably spend more energy focusing on smaller
     areas of the board at a time.

   the learned language is devoid of any historical baggage that it may
   have accumulated over the centuries of go study.

   the third bullet point says that training time is also surprisingly
   less than its previous incarnation. it is as if alphago zero learns how
   to improve its own learning.
   [1*mlwje5cdvc3wl76v_vierw.gif]
   source: [18]https://deepmind.com/blog/alphago-zero-learning-scratch/

   it took only 3 days to get to a level that beats the best human player.
   furthermore, it just keeps getting better even after it surpasses the
   best previous alphago implementation. how is it capable of improving
   its learning continuously? this ability to incrementally learn and
   improve the same neural network is something we   ve seen in another
   architecture known as feedbacknet. in the commonplace sgd based
   learning, the same network is fed data across multiple epochs.

   here however, each training set is entirely new and increasingly more
   challenging. it is also analogous to curriculum learning, however the
   curriculum is intrinsic in the algorithm. the training set is self
   generated and the calculation of the objective function is derived from
   the result of mcts. the network learns by comparing itself not from
   external training data but from synthetic data that is generated from a
   previous version of the neural network.

   the fourth bullet point, the paper reports that it took only 4 google
   tpus ( [19]180 teraops each ) as compared to 48 tpus for previous
   systems. even surprisingly, the nature paper notes that this ran on a
   single system and did not use distributed computing. so anyone with
   four volta based nvidia gpus has the horse power to replicate these
   results. performing a task with 1/10th the amount of compute resources
   should be a hint to anyone that something fundamentally different is
   happening over here. i have yet to analyze this in detail, but perhaps
   the explanation is due to just a simpler architecture.

   finally, the last bullet point where it appears that agz advanced its
   capabilities using less training data. it appears that the synthetic
   data generated by self-play has more    teachable moments    than data
   that   s derived from human play. usually, the way to improve a network
   is to generate more synthetic data. the usual practice is to augment
   data by doing all sorts of data manipulations (ex. cropping,
   translations, etc), however in agz   s case, the automation seemed to be
   able to select richer training data.

   almost every new deep learning paper that is published (or found in
   arxiv) tends to show at best a small percentage improvement over
   previous architectures. almost every time, the newer implementation
   also requires more resources to achieve higher prediction accuracies.
   what alphago has shown is unheard of, that is, it requires an order of
   magnitude less resources and a less complex design, while unequivocally
   besting all previous algorithms.

   many long time practitioners of id23 applied to games
   have commented that the actual design isn   t even novel and has been
   formulated decades ago. yet, the efficacy of this approach has finally
   been experimentally validated by the deepmind team. in deep learning
   like in sports, you can   t win on paper, you actually have to play the
   game to see who wins. in short, no matter a simple an idea may be, you
   just never know how well it will work unless the experiments are
   actually run.

   there is nothing new about the [20]policy iteration algorithm or the
   architecture of the neural network. policy iteration is a old algorithm
   that learns improving policies, by alternating between policy
   estimation and policy improvement . that is, between estimating the
   value function of the current policy and using the current value
   function to find a better policy.

   the single neural network that it uses is a pedestrian convolution
   network:

     the overall network depth, in the 20- or 40-block network, is 39 or
     79 parameterized layers, respectively, for the residual tower, plus
     an additional 2 layers for the policy head and 3 layers for the
     value head.

   like the previous incarnations of alphago, id169
   (mcts) is used to select the next move. alphago zero takes advantage of
   the calculations of the tree search as a way to evaluate and train the
   neural network. so basically, mcts employing a previously trained
   neural network, performs a search for winning moves. the policy
   evaluation estimates the value function from many sampled trajectories.
   the results of this search is then used to drive the learning of the
   neural network. so after every game, a new and potentially improved
   network is selected for the next self-play game. deepmind calls this
      self-play id23   :

     a novel id23 algorithm. mcts search is executed,
     guided by the neural network f  . the mcts search outputs
     probabilities    of playing each move. these search probabilities
     usually select much stronger moves than the raw move probabilities p
     of the neural network f  (s); mcts may therefore be viewed as a
     powerful policy improvement operator.

     self-play with search         using the improved mcts-based policy to
     select each move, then using the game winner z as a sample of the
     value         may be viewed as a powerful policy evaluation operator.

   with each iteration of self-play, the system learns to become a
   stronger player. i find it odd that the exploitive search mechanism is
   able to creatively discover new strategies while simultaneous using
   less training data. it is as if self-play is feeding back into itself
   and learning to learn better.

   this self-play reminds me of an earlier writing about    [21]the strange
   loop in deep learning.    i wrote about many recent advances in deep
   learning such as ladder networks and id3
   (gans) that exploited a loop based method to improve recognition and
   generation. it seems that when you have this kind of mechanism, that is
   able to perform assessments of its final outputs, that the fidelity is
   much higher with less training data. in the case of alphago zero,
   there   s is no training data to speak of. the training data is generated
   through self-play. a gan for example, collaboratively improves its
   generation by having two networks (discriminator and generator) work
   with each other. alphago zero, in contrast pits the capabilities of a
   network trained in a previous game against that of the current network.
   in both cases, you have two networks that feed of each other in
   training.

   an important question that should be in everyone   s mind is:    how
   general is alphago zero   s algorithm?    deepmind has publicly stated that
   they will be [22]applying this technology to drug discovery. earlier i
   wrote about how to assess the appropriateness of deep learning
   technologies (see: [23]reality checklist). in that assessment, there
   are six uncertainties in any domain that needs to be addressed:
   execution uncertainty, observational uncertainty, duration uncertainty,
   action uncertainty, evaluation uncertainty and training uncertainty.

   in the alphago zero, the training uncertainty, seems to have been
   addressed. alphago zero learns the best strategies by just playing
   against itself. that is, it is able to    imagine    situations and then
   discover through self-improvement the best strategies. it can do this
   efficiently because all the other uncertainties are known. that is,
   there is no indeterminism in the results of a sequence of actions.
   there is complete information. the effects of actions are predictable.
   there is a way to measure success. in short, the behavior of the game
   of go is predictable, real world systems however are not.

   in many real world contexts however, we can still build accurate
   simulations or virtual worlds. certainly the policy iteration methods
   found here may seem to be applicable to these virtual worlds.
   id23 has been applied to virtual worlds (i.e. video
   games and strategy games). deepmind has not yet reported experiments of
   using policy iteration in atari games. most games of course don   t need
   this sophisticated look ahead that requires mcts, however there are
   some games like montezuma   s revenge that do. deepmind   s atari game
   experiments were like alphago zero, in that there was no need for human
   data to teach a machine.

   the difference between alphago zero and the video game playing machines
   is that the decision making at every state in the game is much more
   sophisticated. in fact there is an entire spectrum of decision making
   required for different games. is mcts the most sophisticated algorithm
   that we will ever need?

   there is also a question on strategies that require remembering one   s
   previous move. alphago zero appears to only care about the current
   board state and does not have a bias on what it moved previously. a
   human sometimes may determine its own action based on its previous
   move. it is a way of telegraphing actions to an opponent, but it
   usually is more like a head fake. perhaps that   s a strategy that only
   works on humans and not machines! in short, a machine cannot see motion
   if it was never trained to recognize its value.

   this lack of memory affecting strategy may in fact be advantageous.
   humans when playing a strategy game will stick to a specific strategy
   until an unexpected event disrupts that strategy. so long as an
   opponent   s moves are as expected, there is no need to change a
   strategy. however, as we   ve seen in the most advanced poker playing
   automation, there is a distinct advantage of always calculating
   strategy from scratch with every move. this approach avoids
   telegraphing any plans and therefore a good strategy. however,
   misdirection is a strategy that is effective against humans but not
   machines that are not trained to be distracted by them. (editors note:
   apparently previous board states are used as input to the network, so
   appears this lack of memory observation is incorrect).

   finally, there is a question about the applicability of a turn based
   game to the real world. interactions in the real world are more dynamic
   and continuous, furthermore the time of interaction is unbounded. go
   games have a limited number of moves. perhaps, it doesn   t matter, after
   all, all interactions require two parties that act and react and
   predicting the future will always be boxed in time.

   if i were to pinpoint the one pragmatic deep learning discovery in
   alphago zero then it would be the fact that policy iteration works
   surprisingly well using deep learning networks. we   ve have hints in
   previous research that incremental learning was a capability that
   existed. however, deepmind has shown unequivocally that incremental
   learning indeed works effectively well.

   alphago zero appears also to have evolutionary aspects. that is, you
   select the best version of the newly latest trained network and you
   discard the previous one. there is indeed something going on here that
   is eluding a good explanation. the self-play is intrinsically
   competitive and the mcts mechanism is an exploratory search mechanism.
   without exploration, the system will eventually not be able to beat
   itself in play. to be effective, the system should be inclined to seek
   out novel strategies to avoid any stalemate. like nature   s own
   evolutionary process that abhors a vacuum, agz seems to discover
   unexplored areas and somehow take advantage of these finds.

   one perspective to think about these systems as well as the human mind
   is in terms of the language that we use. language is something that you
   layer more and more complex concepts on top of each other. in the case
   of alphago zero, it learned a new language that doesn   t have legacy
   baggage and it learned one that is so advanced that it is
   incomprehensible. not necessarily mutually exclusive. as humans, we
   understand the world with concepts that originate from our embodiment
   with our world. that is we have evolved to understand visual-spatial,
   sequence, rhythm and motion. all our understanding is derived from
   these basic primitives. however, a machine may possibly discover a
   concept that may simply not be decomposable to these basic primitives.

   such irony, when deepmind trained an ai without human bias, humans
   discovered they didn   t understand it! this in another dimension of
   incomprehensibility. the concept of    incomprehensibility in the large   
   in that there is just too much information. perhaps there is this other
   concept, that is    incomprehensibility in the small   . that there are
   primitive concepts that we simply are incapable of understanding. let
   this one percolate in your mind for a while. for indeed it is one that
   is fundamentally shocking and a majority will overlook what deepmind
   may have actually uncovered!.
   [1*klghw8y2ldrko117bnqswq.png]
   explore deep learning: [24]artificial intuition: the improbable deep
   learning revolution
   [1*j9kar_3vwdjk8twhtmxc0g.png]
   exploit deep learning: [25]the deep learning ai playbook

     * [26]deep learning
     * [27]artificial intelligence
     * [28]deepmind
     * [29]alphago
     * [30]machine learning

   (button)
   (button)
   (button) 4.6k claps
   (button) (button) (button) 18 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of carlos e. perez

[31]carlos e. perez

   medium member since feb 2018

   author of artificial intuition and the deep learning playbook    
   linkedin.com/in/ceperez

     (button) follow
   [32]intuition machine

[33]intuition machine

   deep learning patterns, methodology and strategy

     * (button)
       (button) 4.6k
     * (button)
     *
     *

   [34]intuition machine
   never miss a story from intuition machine, when you sign up for medium.
   [35]learn more
   never miss a story from intuition machine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6e3274fcdd9f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/the-strange-loop-in-alphago-zeros-self-play-6e3274fcdd9f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/the-strange-loop-in-alphago-zeros-self-play-6e3274fcdd9f&source=--------------------------nav_reg&operation=register
   8. https://medium.com/intuitionmachine?source=logo-lo_yprvrfqxh9yo---d777623c68cf
   9. https://medium.com/intuitionmachine/tagged/design-patterns
  10. https://medium.com/intuitionmachine/tagged/agile-methodology
  11. https://medium.com/intuitionmachine/tagged/strategy
  12. https://deeplearningplaybook.com/
  13. https://medium.com/@intuitmachine
  14. http://www.imdb.com/title/tt0086567/
  15. https://medium.com/media/672b1a5a3423932477f1f4a0b64571ca?postid=6e3274fcdd9f
  16. https://medium.com/intuitionmachine/the-many-tribes-problem-of-artificial-intelligence-ai-1300faba5b60
  17. https://www.theatlantic.com/technology/archive/2017/10/alphago-zero-the-ai-that-taught-itself-go/543450/
  18. https://deepmind.com/blog/alphago-zero-learning-scratch/
  19. https://medium.com/intuitionmachine/googles-ai-processor-is-inspired-by-the-heart-d0f01b72defe
  20. https://arxiv.org/abs/1405.2878
  21. https://medium.com/intuitionmachine/the-strange-loop-in-deep-learning-38aa7caf6d7d
  22. https://www.bloomberg.com/news/articles/2017-10-18/deepmind-s-superpowerful-ai-sets-its-sights-on-drug-discovery?lipi=urn:li:page:d_flagship3_feed;dl0kiforsos7oqq5gwtdcw==
  23. https://medium.com/intuitionmachine/where-is-deep-learning-applicable-understand-the-known-unknowns-f4272e3136ec
  24. https://gumroad.com/products/ihdj
  25. https://deeplearningplaybook.com/
  26. https://medium.com/tag/deep-learning?source=post
  27. https://medium.com/tag/artificial-intelligence?source=post
  28. https://medium.com/tag/deepmind?source=post
  29. https://medium.com/tag/alphago?source=post
  30. https://medium.com/tag/machine-learning?source=post
  31. https://medium.com/@intuitmachine
  32. https://medium.com/intuitionmachine?source=footer_card
  33. https://medium.com/intuitionmachine?source=footer_card
  34. https://medium.com/intuitionmachine
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/@intuitmachine?source=post_header_lockup
  38. https://medium.com/p/6e3274fcdd9f/share/twitter
  39. https://medium.com/p/6e3274fcdd9f/share/facebook
  40. https://medium.com/@intuitmachine?source=footer_card
  41. https://medium.com/p/6e3274fcdd9f/share/twitter
  42. https://medium.com/p/6e3274fcdd9f/share/facebook
