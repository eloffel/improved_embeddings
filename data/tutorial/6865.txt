   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

swish in depth: a comparison of swish & relu on cifar-10

   [9]go to the profile of jaiyam sharma
   [10]jaiyam sharma (button) blockedunblock (button) followfollowing
   oct 22, 2017

   hi there, this is a continuation from my [11]previous blog post about
   swish activation function recently published by a team at google. if
   you are not familiar with the swish activation (mathematically,
   f(x)=x*sigmoid(x)), please be sure to check out the paper for an
   in-depth understanding or my blog post for a tldr. according to the
   paper, swish often performs better than relus. but many people have
   pointed out that it is also more computationally intensive than relus.
   in my previous post i showed how swish performs relative to relu and
   sigmoids on a 2 hidden layer neural network trained on mnist. although
   a simple 2 layer network is a good starting point, one cannot really
   generalize the results to most problems one might encounter in
   practical use cases. in this post, i will compare the performance of
   swish on convolutional networks (id98) and show you exactly how slow is
   swish relative to relus. lets get started.
     __________________________________________________________________

byoc (build your own id98)

   since i was going to make my code open source, i decided to write it
   such that someone with more computational resources can just change one
   line of code and train as deep a model as they like. this is the idea
   behind byoc. to make this possible, i used a resnet like architecture
   with its famous computational bottleneck units consisting of 3
   convolutional layers of 1x1, 3x3 and 1x1 convolutions, as shown below:
   [1*blflm-utf2n6gqtmhrpfka.png]
   structure of the computational bottleneck (source: resnet [12]paper)

   with this idea, i designed my model such that adding convolutional
   layers is as simple as setting a variable n_layers to some number.
   hence the acronym byoc. feel free to check out my github repo and train
   a better model. for the purposes of this blog, i trained 2 models, a 6
   layer and a 12 layer one. these are end to end convolutional networks
   and there is no fully connected layer except at the output.
     __________________________________________________________________

results:

   to recap, i am comparing three activations: relu, standard swish and
   swish_beta (f(x)=x*sigmoid(beta*x), where beta is learned during
   training). since the objective was to compare id180 and
   not to build a great model, i did not tune the hyperparameters and
   trained for only 10 epochs. the results i got do not paint a rosy
   picture for swish activation. here are the results for training
   accuracy from the six layer network:

   performance
   [1*zwkrwq63x6j78zjiwit8bw.png]

   its clear to see that relu performs quite well here, much better than
   swish. the variables for all networks were initialized starting from
   the same random seed. so, initialization is not a factor here. i really
   wanted swish to do better especially considering the fact that it is
   more computationally involved. a comparison of training time for these
   activations is shown below:

   training time
   [1*jq88mbygif3pjsfuipsh-g.png]

   to be clear, i have compared the time of forward and backward passes
   through the whole network, as opposed to just the time of applying the
   activations, since one won   t use the activations in isolation. on my
   aws g2.2xlarge ami, with a batch size of 128 on an average relu took
   200 milliseconds to make one full pass, swish took 11.2% more time, and
   swish_beta took 12% more time than relu.

   id136 time

   for practical applications, it is more important to know the id136
   time of any network since when a network is deployed on a product, we
   only want to do id136. here again, as expected, swish is slower.
   with a batch size of 100 samples, on an average, relu took 44
   milliseconds, whereas swish took ~21% more time and swish_beta took
   ~28% more time.
   [1*2ysp7s6bjajq6xtnqd_qcw.png]

   12 layer network:

   the results from 12 layer network are similar. relu has higher
   accuracy, and is much faster during training and id136. the results
   are shown below:

   accuracy
   [1*vy_pefkpkuutfx56oamchg.png]

   train time:
   [1*hvatsfzdt1m8qtnkq1qvxg.png]

   id136 time:
   [1*b8b8ovny_8vq36pn5jf2na.png]

   interestingly, i the gap between relu and swish for training and
   id136 has increased as the network has gotten deeper. this is not a
   great sign. since even a    light weight    resnet is also about 50 layers
   deep, it would be just terrible if the gap between training and
   id136 time keeps on increasing with the number of layers. but one
   cannot really extrapolate from just two data points (6 & 12 layers). i
   wanted to train an 18 layer network as well, but the results from these
   two cases made me less motivated to do so. i really wanted swish to
   work better than relus but, at least in these experiments, it didn   t. i
   would be happy to have been proven wrong as more people apply it to
   real world problems, because i, like everyone else, want to be able to
   train better models.

takeaways:

   here are a few takeaways from these results:
    1. swish does not really perform as well as in these experiments as i
       expected. relus consistently beat swish on accuracy.
    2. there is a large difference between training times required for
       these activations. even if swish performs better than relus on a
       problem, the time required to train a good model will be about
       15   20% more than relus.
    3. more importantly, the run time performance of swish seems to be
       much slower than relus, by 20   30% or more. this is a non trivial
       slowdown for cases where real time performance is needed.
    4. between standard swish and swish_beta, the beta version certainly
       performs better. if you find that swish works for your problem and
       if you care ddeply about the accuracy, it is a good idea to try
       learning the parameter beta during training as well. of course, the
       higher accuracy would come at the cost of at higher training time
       and id136 time, as we saw from the graphs above.
     __________________________________________________________________

code

   all the code for reproducing these results and training more models
   with byoc is uploaded on my [13]github. if you find any bugs or have
   difficulty in understanding the code, feel free to contact me.

     * [14]machine learning
     * [15]deep learning
     * [16]artificial intelligence
     * [17]google
     * [18]data science

   (button)
   (button)
   (button) 192 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [19]go to the profile of jaiyam sharma

[20]jaiyam sharma

   blogs about replicating research papers in machine learning

     * (button)
       (button) 192
     * (button)
     *
     *

   [21]go to the profile of jaiyam sharma
   never miss a story from jaiyam sharma, when you sign up for medium.
   [22]learn more
   never miss a story from jaiyam sharma
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1c798e70ee08
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@jaiyamsharma/swish-in-depth-a-comparison-of-swish-relu-on-cifar-10-1c798e70ee08&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@jaiyamsharma/swish-in-depth-a-comparison-of-swish-relu-on-cifar-10-1c798e70ee08&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@jaiyamsharma?source=post_header_lockup
  10. https://medium.com/@jaiyamsharma
  11. https://medium.com/@jaiyamsharma/experiments-with-swish-activation-function-on-mnist-dataset-fc89a8c79ff7
  12. https://arxiv.org/pdf/1512.03385.pdf
  13. http://github.com/dataplayer12/swish-activation
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/deep-learning?source=post
  16. https://medium.com/tag/artificial-intelligence?source=post
  17. https://medium.com/tag/google?source=post
  18. https://medium.com/tag/data-science?source=post
  19. https://medium.com/@jaiyamsharma?source=footer_card
  20. https://medium.com/@jaiyamsharma
  21. https://medium.com/@jaiyamsharma
  22. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  24. https://medium.com/p/1c798e70ee08/share/twitter
  25. https://medium.com/p/1c798e70ee08/share/facebook
  26. https://medium.com/p/1c798e70ee08/share/twitter
  27. https://medium.com/p/1c798e70ee08/share/facebook
