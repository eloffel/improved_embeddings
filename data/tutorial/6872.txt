   #[1]computational statistics in python 0.1 documentation [2]using pymc2
   [3]resampling methods

   (button) [4]computational statistics in python 0.1
     * [5]site
          + [6]introduction to python
               o [7]variables
               o [8]operators
               o [9]iterators
               o [10]conditional statements
               o [11]functions
               o [12]strings and string handling
               o [13]lists, tuples, dictionaries
               o [14]classes
               o [15]modules
               o [16]the standard library
               o [17]keeping the anaconda distribution up-to-date
               o [18]exercises
          + [19]getting started with python and the ipython notebook
               o [20]cells
               o [21]code cells
               o [22]magic commands
               o [23]python as glue
               o [24]python <-> r <-> matlab <-> octave
               o [25]more glue: julia and perl
          + [26]functions are first class objects
          + [27]function argumnents
               o [28]call by    object reference   
               o [29]binding of default arguments occurs at function
                 definition
          + [30]higher-order functions
          + [31]anonymous functions
          + [32]pure functions
          + [33]recursion
          + [34]iterators
          + [35]generators
               o [36]generators and comprehensions
               o [37]utilites - enumerate, zip and the ternary if-else
                 operator
          + [38]decorators
          + [39]the operator module
          + [40]the functools module
          + [41]the itertools module
          + [42]the toolz, fn and funcy modules
          + [43]exercises
          + [44]data science is osemn
               o [45]obtaining data
               o [46]scrubbing data
               o [47]exercises
          + [48]working with text
               o [49]string methods
               o [50]splitting and joining strings
               o [51]the string module
               o [52]id157
               o [53]the nltk toolkit
               o [54]exercises
          + [55]preprocessing text data
               o [56]example: counting words in a document
          + [57]working with structured data
               o [58]using sqlite3
               o [59]basic concepts of database id172
               o [60]using hdf5
               o [61]interfacing withpandas
          + [62]using numpy
               o [63]references
               o [64]example
               o [65]ndarray
               o [66]broadcasting, row, column and matrix operations
               o [67]universal functions (ufuncs)
               o [68]generalized ufucns
               o [69]random numbers
               o [70]id202
               o [71]exercises
          + [72]using pandas
               o [73]series
               o [74]dataframe
               o [75]panels
               o [76]split-apply-combine
               o [77]using statsmodels
          + [78]using r from ipython
               o [79]using rmagic
               o [80]using r from pandas
          + [81]computational problems in statistics
               o [82]textbook example - is coin fair?
               o [83]bayesian approach
               o [84]comment
          + [85]computer numbers and mathematics
               o [86]some examples of numbers behaving badly
               o [87]finite representation of numbers
               o [88]using arbitrary precision libraries
               o [89]from numbers to functions: stability and conditioning
               o [90]exercises
          + [91]algorithmic complexity
               o [92]profling and benchmarking
               o [93]measuring algorithmic complexity
               o [94]space complexity
          + [95]id202 and linear systems
               o [96]simultaneous equations
               o [97]linear independence
               o [98]norms and distance of vectors
               o [99]trace and determinant of matrices
               o [100]column space, row space, rank and kernel
               o [101]matrices as linear transformations
               o [102]matrix norms
               o [103]special matrices
               o [104]exercises
          + [105]id202 and matrix decompositions
               o [106]large linear systems
               o [107]example: netflix competition (circa 2006-2009)
               o [108]matrix decompositions
               o [109]matrix decompositions for pca and least squares
               o [110]singular value decomposition
               o [111]stabilty and condition number
               o [112]exercises
          + [113]change of basis
               o [114]variance and covariance
               o [115]eigendecomposition of the covariance matrix
               o [116]pca
               o [117]change of basis via pca
               o [118]graphical illustration of change of basis
               o [119]dimension reduction via pca
               o [120]using singular value decomposition (svd) for pca
          + [121]optimization and non-linear methods
               o [122]example: id113 (id113)
               o [123]bisection method
               o [124]secant method
               o [125]newton-rhapson method
               o [126]gauss-newton
               o [127]inverse quadratic interpolation
               o [128]brent   s method
          + [129]practical optimizatio routines
               o [130]finding roots
               o [131]optimization primer
               o [132]using scipy.optimize
               o [133]gradient deescent
               o [134]newton   s method and variants
               o [135]constrained optimization
               o [136]curve fitting
               o [137]finding paraemeters for ode models
               o [138]optimization of graph node placement
               o [139]optimization of standard statistical models
          + [140]fitting odes with the levenberg   marquardt algorithm
               o [141]1d example
               o [142]2d example
          + [143]algorithms for optimization and root finding for
            multivariate problems
               o [144]optimizers
               o [145]solvers
               o [146]glm estimation and irls
          + [147]expectation maximizatio (em) algorithm
               o [148]jensen   s inequality
               o [149]maximum likelihood with complete information
               o [150]incomplete information
               o [151]gaussian mixture models
               o [152]using em
               o [153]vectorized version
               o [154]vectorization with einstein summation notation
               o [155]comparison of em routines
          + [156]monte carlo methods
               o [157]pseudorandom number generators (prng)
               o [158]monte carlo swindles (variance reduction techniques)
               o [159]quasi-random numbers
          + [160]resampling methods
               o [161]resampling
               o [162]simulations
               o [163]setting the random seed
               o [164]sampling with and without replacement
               o [165]calculation of cook   s distance
               o [166]permutation resampling
               o [167]design of simulation experiments
               o [168]example: simulations to estimate power
               o [169]check with r
               o [170]estimating the cdf
               o [171]estimating the pdf
               o [172]kernel density estimation
               o [173]multivariate kerndel density estimation
          + [174]id115 (mcmc)
               o [175]bayesian data analysis
               o [176]metropolis-hastings sampler
               o [177]gibbs sampler
               o [178]slice sampler
               o [179]id187
          + [180]using pymc2
               o [181]coin toss
               o [182]estimating mean and standard deviation of normal
                 distribution
               o [183]estimating parameters of a linear regreession model
               o [184]estimating parameters of a logistic model
               o [185]using a hierarchcical model
          + [186]using pymc3
               o [187]coin toss
               o [188]estimating mean and standard deviation of normal
                 distribution
               o [189]estimating parameters of a linear regreession model
               o [190]estimating parameters of a logistic model
               o [191]using a hierarchcical model
          + [192]using pystan
               o [193]references
               o [194]simple logistic model
          + [195]animations of metropolis, gibbs and slice sampler
            dynamics
          + [196]c crash course
               o [197]hello world
               o [198]a tutorial example - coding a fibonacci function in
                 c
               o [199]types in c
               o [200]operators
               o [201]control of program flow
               o [202]arrays and pointers
               o [203]functions
               o [204]function pointers
               o [205]using make to compile c programs
               o [206]exercise
          + [207]code optimization
               o [208]profiling
               o [209]using better algorihtms and data structures
               o [210]i/o bound problems
               o [211]problem set for optimization
          + [212]using c code in python
               o [213]example: the fibonacci sequence
               o [214]using clang and bitey
               o [215]using gcc and ctypes
               o [216]using cython
               o [217]benchmark
          + [218]using functions from various compiled languages in python
               o [219]c
               o [220]c++
               o [221]fortran
               o [222]benchmarking
               o [223]wrapping a function from a c library for use in
                 python
               o [224]wrapping functions from c++ library for use in pyton
          + [225]julia and python
               o [226]defining a function in julia
               o [227]using it in python
               o [228]using python libraries in julia
          + [229]converting python code to c for speed
               o [230]example: fibonacci
               o [231]example: id127
               o [232]example: pairwise distance matrix
               o [233]profiling code
               o [234]numba
               o [235]cython
               o [236]comparison with optimized c from scipy
          + [237]optimization bake-off
               o [238]python version
               o [239]numpy version
               o [240]numexpr version
               o [241]numba version
               o [242]numbapro version
               o [243]parakeet version
               o [244]cython version
               o [245]c version
               o [246]c++ version
               o [247]fortran version
               o [248]bake-off
               o [249]summary
               o [250]recommendations for optimizing python code
          + [251]writing parallel code
               o [252]concepts
               o [253]embarassingly parallel programs
               o [254]using multiprocessing
               o [255]using ipython parallel for interactive parallel
                 computing
               o [256]other parallel programming approaches not covered
               o [257]references
          + [258]massively parallel programming with gpus
               o [259]programming gpus
               o [260]gpu architecture
               o [261]cuda python
               o [262]getting started with cuda
               o [263]vector addition - the    hello, world    of cuda
               o [264]performing a reduction on cuda
               o [265]recreational
               o [266]more examples
          + [267]writing cuda in c
               o [268]review of gpu architechture - a simplification
               o [269]cuda c program - an outline
          + [270]distributed computing for big data
               o [271]why and when does distributed computing matter?
               o [272]ingredients for effiicient distributed computing
               o [273]what is hadoop?
               o [274]review of functional programming
               o [275]the hadoop mapreduce workflow
               o [276]using hadoop mapreduce
               o [277]spark
          + [278]hadoop mapreduce on aws emr with mrjob
               o [279]mapreduce code
               o [280]configuration file
               o [281]launching job
          + [282]spark on a local mahcine using 4 nodes
               o [283]using spark in standalone prograsm
               o [284]introduction to spark concepts with a data
                 manipulation example
               o [285]using the mllib for regression
               o [286]references
          + [287]modules and packaging
               o [288]modules
               o [289]distributing your package
          + [290]tour of the jupyter (ipython3) notebook
               o [291]installing jupyter
               o [292]installing other kernels
               o [293]installing extensions
               o [294]installing python3 while keeping python2
               o [295]now, restart your notebook server
          + [296]polyglot programming
               o [297]python 2
               o [298]python 3
               o [299]bash
               o [300]r
               o [301]scala
               o [302]julia
               o [303]processing
          + [304]what you should know and learn more about
               o [305]statistical foundations
               o [306]computing foundations
               o [307]mathematical foundations
               o [308]statistical algorithms
               o [309]libraries worth knowing about after numpy, scipy and
                 matplotlib

     [310]page

     * [311]id115 (mcmc)
          + [312]bayesian data analysis
               o [313]motivating example
                    # [314]analytical solution
                    # [315]numerical integration
          + [316]metropolis-hastings sampler
               o [317]intuition
          + [318]gibbs sampler
               o [319]motivating example
                    # [320]setup
                    # [321]analytic solution
                    # [322]grid approximation
                    # [323]metropolis
                    # [324]gibbs
          + [325]slice sampler
          + [326]id187
               o [327]latex for markov chain diagram

     [328]   resampling metho...

     [329]using pymc2   

   ____________________

     * [330]introduction to python
          + [331]variables
          + [332]operators
          + [333]iterators
          + [334]conditional statements
          + [335]functions
          + [336]strings and string handling
          + [337]lists, tuples, dictionaries
          + [338]classes
          + [339]modules
          + [340]the standard library
          + [341]keeping the anaconda distribution up-to-date
          + [342]exercises
     * [343]getting started with python and the ipython notebook
          + [344]cells
          + [345]code cells
          + [346]magic commands
          + [347]python as glue
          + [348]python <-> r <-> matlab <-> octave
          + [349]more glue: julia and perl
     * [350]functions are first class objects
     * [351]function argumnents
          + [352]call by    object reference   
          + [353]binding of default arguments occurs at function
            definition
     * [354]higher-order functions
     * [355]anonymous functions
     * [356]pure functions
     * [357]recursion
     * [358]iterators
     * [359]generators
          + [360]generators and comprehensions
          + [361]utilites - enumerate, zip and the ternary if-else
            operator
     * [362]decorators
     * [363]the operator module
     * [364]the functools module
     * [365]the itertools module
     * [366]the toolz, fn and funcy modules
     * [367]exercises
     * [368]data science is osemn
          + [369]obtaining data
          + [370]scrubbing data
          + [371]exercises
     * [372]working with text
          + [373]string methods
          + [374]splitting and joining strings
          + [375]the string module
          + [376]id157
          + [377]the nltk toolkit
          + [378]exercises
     * [379]preprocessing text data
          + [380]example: counting words in a document
     * [381]working with structured data
          + [382]using sqlite3
          + [383]basic concepts of database id172
          + [384]using hdf5
          + [385]interfacing withpandas
     * [386]using numpy
          + [387]references
          + [388]example
          + [389]ndarray
          + [390]broadcasting, row, column and matrix operations
          + [391]universal functions (ufuncs)
          + [392]generalized ufucns
          + [393]random numbers
          + [394]id202
          + [395]exercises
     * [396]using pandas
          + [397]series
          + [398]dataframe
          + [399]panels
          + [400]split-apply-combine
          + [401]using statsmodels
     * [402]using r from ipython
          + [403]using rmagic
          + [404]using r from pandas
     * [405]computational problems in statistics
          + [406]textbook example - is coin fair?
          + [407]bayesian approach
          + [408]comment
     * [409]computer numbers and mathematics
          + [410]some examples of numbers behaving badly
          + [411]finite representation of numbers
          + [412]using arbitrary precision libraries
          + [413]from numbers to functions: stability and conditioning
          + [414]exercises
     * [415]algorithmic complexity
          + [416]profling and benchmarking
          + [417]measuring algorithmic complexity
          + [418]space complexity
     * [419]id202 and linear systems
          + [420]simultaneous equations
          + [421]linear independence
          + [422]norms and distance of vectors
          + [423]trace and determinant of matrices
          + [424]column space, row space, rank and kernel
          + [425]matrices as linear transformations
          + [426]matrix norms
          + [427]special matrices
          + [428]exercises
     * [429]id202 and matrix decompositions
          + [430]large linear systems
          + [431]example: netflix competition (circa 2006-2009)
          + [432]matrix decompositions
          + [433]matrix decompositions for pca and least squares
          + [434]singular value decomposition
          + [435]stabilty and condition number
          + [436]exercises
     * [437]change of basis
          + [438]variance and covariance
          + [439]eigendecomposition of the covariance matrix
          + [440]pca
          + [441]change of basis via pca
          + [442]graphical illustration of change of basis
          + [443]dimension reduction via pca
          + [444]using singular value decomposition (svd) for pca
     * [445]optimization and non-linear methods
          + [446]example: id113 (id113)
          + [447]bisection method
          + [448]secant method
          + [449]newton-rhapson method
          + [450]gauss-newton
          + [451]inverse quadratic interpolation
          + [452]brent   s method
     * [453]practical optimizatio routines
          + [454]finding roots
          + [455]optimization primer
          + [456]using scipy.optimize
          + [457]gradient deescent
          + [458]newton   s method and variants
          + [459]constrained optimization
          + [460]curve fitting
          + [461]finding paraemeters for ode models
          + [462]optimization of graph node placement
          + [463]optimization of standard statistical models
     * [464]fitting odes with the levenberg   marquardt algorithm
          + [465]1d example
          + [466]2d example
     * [467]algorithms for optimization and root finding for multivariate
       problems
          + [468]optimizers
          + [469]solvers
          + [470]glm estimation and irls
     * [471]expectation maximizatio (em) algorithm
          + [472]jensen   s inequality
          + [473]maximum likelihood with complete information
          + [474]incomplete information
          + [475]gaussian mixture models
          + [476]using em
          + [477]vectorized version
          + [478]vectorization with einstein summation notation
          + [479]comparison of em routines
     * [480]monte carlo methods
          + [481]pseudorandom number generators (prng)
          + [482]monte carlo swindles (variance reduction techniques)
          + [483]quasi-random numbers
     * [484]resampling methods
          + [485]resampling
          + [486]simulations
          + [487]setting the random seed
          + [488]sampling with and without replacement
          + [489]calculation of cook   s distance
          + [490]permutation resampling
          + [491]design of simulation experiments
          + [492]example: simulations to estimate power
          + [493]check with r
          + [494]estimating the cdf
          + [495]estimating the pdf
          + [496]kernel density estimation
          + [497]multivariate kerndel density estimation
     * [498]id115 (mcmc)
          + [499]bayesian data analysis
          + [500]metropolis-hastings sampler
          + [501]gibbs sampler
          + [502]slice sampler
          + [503]id187
     * [504]using pymc2
          + [505]coin toss
          + [506]estimating mean and standard deviation of normal
            distribution
          + [507]estimating parameters of a linear regreession model
          + [508]estimating parameters of a logistic model
          + [509]using a hierarchcical model
     * [510]using pymc3
          + [511]coin toss
          + [512]estimating mean and standard deviation of normal
            distribution
          + [513]estimating parameters of a linear regreession model
          + [514]estimating parameters of a logistic model
          + [515]using a hierarchcical model
     * [516]using pystan
          + [517]references
          + [518]simple logistic model
     * [519]animations of metropolis, gibbs and slice sampler dynamics
     * [520]c crash course
          + [521]hello world
          + [522]a tutorial example - coding a fibonacci function in c
          + [523]types in c
          + [524]operators
          + [525]control of program flow
          + [526]arrays and pointers
          + [527]functions
          + [528]function pointers
          + [529]using make to compile c programs
          + [530]exercise
     * [531]code optimization
          + [532]profiling
          + [533]using better algorihtms and data structures
          + [534]i/o bound problems
          + [535]problem set for optimization
     * [536]using c code in python
          + [537]example: the fibonacci sequence
          + [538]using clang and bitey
          + [539]using gcc and ctypes
          + [540]using cython
          + [541]benchmark
     * [542]using functions from various compiled languages in python
          + [543]c
          + [544]c++
          + [545]fortran
          + [546]benchmarking
          + [547]wrapping a function from a c library for use in python
          + [548]wrapping functions from c++ library for use in pyton
     * [549]julia and python
          + [550]defining a function in julia
          + [551]using it in python
          + [552]using python libraries in julia
     * [553]converting python code to c for speed
          + [554]example: fibonacci
          + [555]example: id127
          + [556]example: pairwise distance matrix
          + [557]profiling code
          + [558]numba
          + [559]cython
          + [560]comparison with optimized c from scipy
     * [561]optimization bake-off
          + [562]python version
          + [563]numpy version
          + [564]numexpr version
          + [565]numba version
          + [566]numbapro version
          + [567]parakeet version
          + [568]cython version
          + [569]c version
          + [570]c++ version
          + [571]fortran version
          + [572]bake-off
          + [573]summary
          + [574]recommendations for optimizing python code
     * [575]writing parallel code
          + [576]concepts
          + [577]embarassingly parallel programs
          + [578]using multiprocessing
          + [579]using ipython parallel for interactive parallel computing
          + [580]other parallel programming approaches not covered
          + [581]references
     * [582]massively parallel programming with gpus
          + [583]programming gpus
          + [584]gpu architecture
          + [585]cuda python
          + [586]getting started with cuda
          + [587]vector addition - the    hello, world    of cuda
          + [588]performing a reduction on cuda
          + [589]recreational
          + [590]more examples
     * [591]writing cuda in c
          + [592]review of gpu architechture - a simplification
          + [593]cuda c program - an outline
     * [594]distributed computing for big data
          + [595]why and when does distributed computing matter?
          + [596]ingredients for effiicient distributed computing
          + [597]what is hadoop?
          + [598]review of functional programming
          + [599]the hadoop mapreduce workflow
          + [600]using hadoop mapreduce
          + [601]spark
     * [602]hadoop mapreduce on aws emr with mrjob
          + [603]mapreduce code
          + [604]configuration file
          + [605]launching job
     * [606]spark on a local mahcine using 4 nodes
          + [607]using spark in standalone prograsm
          + [608]introduction to spark concepts with a data manipulation
            example
          + [609]using the mllib for regression
          + [610]references
     * [611]modules and packaging
          + [612]modules
          + [613]distributing your package
     * [614]tour of the jupyter (ipython3) notebook
          + [615]installing jupyter
          + [616]installing other kernels
          + [617]installing extensions
          + [618]installing python3 while keeping python2
          + [619]now, restart your notebook server
     * [620]polyglot programming
          + [621]python 2
          + [622]python 3
          + [623]bash
          + [624]r
          + [625]scala
          + [626]julia
          + [627]processing
     * [628]what you should know and learn more about
          + [629]statistical foundations
          + [630]computing foundations
          + [631]mathematical foundations
          + [632]statistical algorithms
          + [633]libraries worth knowing about after numpy, scipy and
            matplotlib

   ____________________
from __future__ import division
import os
import sys
import glob
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as st

%matplotlib inline
%precision 4
plt.style.use('ggplot')

from mpl_toolkits.mplot3d import axes3d
import scipy.stats as stats
from functools import partial

np.random.seed(1234)

id115 (mcmc)[634]  

     * baye   s rule and definitions
     * estimating coin bias example
          + analytic
          + numerical integration
          + metropolis-hastings sampler
          + gibbs sampler
          + slice sampler
     * why does mcmc work?
          + markov chains and stationary states
          + conditions for convergence
          + assessing for convergence
     * visualizing mcmc in action
     * ohter examples
          + mixture models
          + id187
          + change point detection
     * using mcmc libraries
          + usign pymc
          + using pystan

bayesian data analysis[635]  

   the fundamental objective of bayesian data analysis is to determine the
   posterior distribution
   \[p(\theta \ | \ x) = \frac{p(x \ | \ \theta) p(\theta)}{p(x)}\]

   where the denominator is
   \[p(x) = \int d\theta^* p(x \ | \ \theta^*) p(\theta^*)\]

   here,
     * \(p(x \ | \ \theta)\) is the likelihood,
     * \(p(\theta)\) is the prior and
     * \(p(x)\) is a normalizing constant also known as the evidence or
       marginal likelihood

   the computational issue is the difficulty of evaluating the integral in
   the denominator. there are many ways to address this difficulty,
   inlcuding:
     * in cases with conjugate priors (with conjugate priors, the
       posterior has the same distribution as the prior), we can get
       closed form solutions
     * we can use numerical integration
     * we can approximate the functions used to calculate the posterior
       with simpler functions and show that the resulting approximate
       posterior is    close    to true posteiror (id58)
     * we can use monte carlo methods, of which the most important is
       id115 (mcmc)

motivating example[636]  

   we will use the toy example of estimating the bias of a coin given a
   sample consisting of \(n\) tosses to illustrate a few of the
   approaches.

analytical solution[637]  

   if we use a beta distribution as the prior, then the posterior
   distribution has a closed form solution. this is shown in the example
   below. some general points:
     * we need to choose a prior distribtuiton family (i.e. the beta here)
       as well as its parameters (here a=10, b=10)
          + the prior distribution may be relatively uninformative (i.e.
            more flat) or inforamtive (i.e. more peaked)
     * the posterior depends on both the prior and the data
          + as the amount of data becomes large, the posterior
            approximates the id113
          + an informative prior takes more data to shift than an
            uninformative one
     * of course, it is also important the model used (i.e. the
       likelihood) is appropriate for the fitting the data
     * the mode of the posterior distribution is known as the maximum a
       posteriori (map) estimate (cf id113 which is the mode of the
       likelihood)

n = 100
h = 61
p = h/n
rv = st.binom(n, p)
mu = rv.mean()

a, b = 10, 10
prior = st.beta(a, b)
post = st.beta(h+a, n-h+b)
ci = post.interval(0.95)

thetas = np.linspace(0, 1, 200)
plt.figure(figsize=(12, 9))
plt.style.use('ggplot')
plt.plot(thetas, prior.pdf(thetas), label='prior', c='blue')
plt.plot(thetas, post.pdf(thetas), label='posterior', c='red')
plt.plot(thetas, n*st.binom(n, thetas).pmf(h), label='likelihood', c='green')
plt.axvline((h+a-1)/(n+a+b-2), c='red', linestyle='dashed', alpha=0.4, label='ma
p')
plt.axvline(mu/n, c='green', linestyle='dashed', alpha=0.4, label='id113')
plt.xlim([0, 1])
plt.axhline(0.3, ci[0], ci[1], c='black', linewidth=2, label='95% ci');
plt.xlabel(r'$\theta$', fontsize=14)
plt.ylabel('density', fontsize=16)
plt.legend();

   _images/mcmc_7_0.png

numerical integration[638]  

   one simple way of numerical integration is to estimate the values on a
   grid of values for \(\theta\). to calculate the posterior, we find the
   prior and the likelhood for each value of \(\theta\), and for the
   marginal likelhood, we replace the integral with the equivalent sum
   \[p(x) = \sum_{\theta^*} p(x | \theta^*) p(\theta^*)\]

   one advantage of this is that the prior does not have to be conjugate
   (although the example below uses the same beta prior for ease of
   comaprsion), and so we are not restricted in our choice of an
   approproirate prior distribution. for example, the prior can be a
   mixture distribution or estimated empirically from data. the
   disadvantage, of course, is that this is computationally very expenisve
   when we need to esitmate multiple parameters, since the number of grid
   points grows as \(\mathcal{o}(n^d)\), wher \(n\) defines the grid
   resolution and \(d\) is the size of \(\theta\).
thetas = np.linspace(0, 1, 200)
prior = st.beta(a, b)

post = prior.pdf(thetas) * st.binom(n, thetas).pmf(h)
post /= (post.sum() / len(thetas))

plt.figure(figsize=(12, 9))
plt.plot(thetas, prior.pdf(thetas), label='prior', c='blue')
plt.plot(thetas, n*st.binom(n, thetas).pmf(h), label='likelihood', c='green')
plt.plot(thetas, post, label='posterior', c='red')
plt.xlim([0, 1])
plt.xlabel(r'$\theta$', fontsize=14)
plt.ylabel('density', fontsize=16)
plt.legend();

   _images/mcmc_9_0.png

metropolis-hastings sampler[639]  

   this lecture will only cover the basic ideas of mcmc and the 3 common
   veriants - metropolis-hastings, gibbs and slice sampling. all ocde will
   be built from the ground up to ilustrate what is involved in fitting an
   mcmc model, but only toy examples will be shown since the goal is
   conceptual understanding. more realiztic computational examples will be
   shown in the next lecture using the pymc and pystan packages.

   in bayesian statistics, we want to estiamte the posterior distribution,
   but this is often intractable due to the high-dimensional integral in
   the denominator (marginal likelihood). a few other ideas we have
   encountered that are also relevant here are monte carlo integration
   with inddependent samples and the use of proposal distributions (e.g.
   rejection and importance sampling). as we have seen from the monte
   carlo inttegration lectures, we can approximate the posterior
   \(p(\theta | x)\) if we can somehow draw many samples that come from
   the posterior distribution. with vanilla monte carlo integration, we
   need the samples to be independent draws from the posterior
   distribution, which is a problem if we do not actually know what the
   posterior distribution is (because we cannot integrte the marginal
   likelihood).

   with mcmc, we draw samples from a (simple) proposal distribution so
   that each draw depends only on the state of the previous draw (i.e. the
   samples form a markov chain). under certain condiitons, the markov
   chain will have a unique stationary distribution. in addition, not all
   samples are used - instead we set up acceptance criteria for each draw
   based on comparing successive states with respect to a target
   distribution that enusre that the stationary distribution is the
   posterior distribution of interest. the nice thing is that this target
   distribution only needs to be proportional to the posterior
   distribution, which means we don   t need to evaluate the potentially
   intractable marginal likelihood, which is just a normalizing constant.
   we can find such a target distribution easily, since posterior
   \(\propto\) likelihood \(\times\) prior. after some time, the markov
   chain of accepted draws will converge to the staionary distribution,
   and we can use those samples as (correlated) draws from the posterior
   distribution, and find functions of the posterior distribution in the
   same way as for vanilla monte carlo integration.

   there are several flavors of mcmc, but the simplest to understand is
   the metropolis-hastings random walk algorithm, and we will start there.

   to carry out the metropolis-hastings algorithm, we need to draw random
   samples from the folllowing distributions
     * the standard uniform distribution
     * a proposal distriution \(p(x)\) that we choose to be
       \(\mathcal{n}(0, \sigma)\)
     * the target distribution \(g(x)\) which is proportional to the
       posterior id203

   given an initial guess for \(\theta\) with positive id203 of
   being drawn, the metropolis-hastings algorithm proceeds as follows
     * choose a new proposed value (\(\theta_p\)) such that \(\theta_p =
       \theta + \delta\theta\) where \(\delta \theta \sim \mathcal{n}(0,
       \sigma)\)
     * caluculate the ratio
       \[\rho = \frac{g(\theta_p \ | \ x)}{g(\theta \ | \ x)}\]
       where \(g\) is the posterior id203.
     * if the proposal distribution is not symmetrical, we need to weight
       the accceptanc probablity to maintain detailed balance
       (reversibilty) of the stationary distribution, and insetad
       calculate
       \[\rho = \frac{g(\theta_p \ | \ x) p(\theta \ | \
       \theta_p)}{g(\theta \ | \ x) p(\theta_p \ | \ \theta)}\]
       since we are taking ratios, the denominator cancels any
       distribution proporational to \(g\) will also work - so we can use
       \[\rho = \frac{p(x | \theta_p ) p(\theta_p)}{p(x | \theta )
       p(\theta)}\]
     * if \(\rho \ge 1\), then set \(\theta = \theta_p\)
     * if \(\rho < 1\), then set \(\theta = \theta_p\) with id203
       \(\rho\), otherwise set \(\theta = \theta\) (this is where we use
       the standard uniform distribution)
     * repeat the earlier steps

   after some number of iterations \(k\), the samples \(\theta_{k+1},
   \theta_{k+2}, \dots\) will be samples from the posterior distributions.
   here are initial concepts to help your intuition about why this is so:
     * we accept a proposed move to \(\theta_{k+1}\) whenever the density
       of the (unnormalzied) target distribution at \(\theta_{k+1}\) is
       larger than the value of \(\theta_k\) - so \(\theta\) will more
       often be found in places where the target distribution is denser
     * if this was all we accepted, \(\theta\) would get stuck at a local
       mode of the target distribution, so we also accept occasional moves
       to lower density regions - it turns out that the correct
       id203 of doing so is given by the ratio \(\rho\)
     * the acceptance criteria only looks at ratios of the target
       distribution, so the denominator cancels out and does not matter -
       that is why we only need samples from a distribution proprotional
       to the posterior distribution
     * so, \(\theta\) will be expected to bounce around in such a way that
       its spends its time in places proportional to the density of the
       posterior distribution - that is, \(\theta\) is a draw from the
       posterior distribution.

   additional notes:

   different propsoal distributions can be used for metropolis-hastings:
     * the independence sampler uses a proposal distribtuion that is
       independent of the current value of \(\theta\). in this case the
       propsoal distribution needs to be similar to the posterior
       distirbution for efficincy, while ensuring that the acceptance
       ratio is bounded in the tail region of the posterior.
     * the random walk sampler (used in this example) takes a random step
       centered at the current value of \(\theta\) - efficiecny is a
       trade-off between small step size with high id203 of
       acceptance and large step sizes with low probaiity of acceptance.
       note (picture will be sketched in class) that the random walk may
       take a long time to traverse narrow regions of the probabilty
       distribution. changing the step size (e.g. scaling \(\sigma\) for a
       multivariate normal proposal distribution) so that a target
       proportion of proposlas are accepted is known as tuning.
     * much research is being conducted on different proposal
       distributions for efficient sampling of the posterior distribution.

   we will first see a numerical example and then try to understand why it
   works.
def target(lik, prior, n, h, theta):
    if theta < 0 or theta > 1:
        return 0
    else:
        return lik(n, theta).pmf(h)*prior.pdf(theta)

n = 100
h = 61
a = 10
b = 10
lik = st.binom
prior = st.beta(a, b)
sigma = 0.3

naccept = 0
theta = 0.1
niters = 10000
samples = np.zeros(niters+1)
samples[0] = theta
for i in range(niters):
    theta_p = theta + st.norm(0, sigma).rvs()
    rho = min(1, target(lik, prior, n, h, theta_p)/target(lik, prior, n, h, thet
a ))
    u = np.random.uniform()
    if u < rho:
        naccept += 1
        theta = theta_p
    samples[i+1] = theta
nmcmc = len(samples)//2
print "efficiency = ", naccept/niters

efficiency =  0.19

post = st.beta(h+a, n-h+b)

plt.figure(figsize=(12, 9))
plt.hist(samples[nmcmc:], 40, histtype='step', normed=true, linewidth=1, label='
distribution of prior samples');
plt.hist(prior.rvs(nmcmc), 40, histtype='step', normed=true, linewidth=1, label=
'distribution of posterior samples');
plt.plot(thetas, post.pdf(thetas), c='red', linestyle='--', alpha=0.5, label='tr
ue posterior')
plt.xlim([0,1]);
plt.legend(loc='best');

   _images/mcmc_12_0.png

   trace plots are often used to informally assess for stochastic
   convergence. rigorous demonstration of convergence is an unsolved
   problem, but simple ideas such as running mutliple chains and checking
   that they are converging to similar distribtions are often employed in
   practice.
def mh_coin(niters, n, h, theta, lik, prior, sigma):
    samples = [theta]
    while len(samples) < niters:
        theta_p = theta + st.norm(0, sigma).rvs()
        rho = min(1, target(lik, prior, n, h, theta_p)/target(lik, prior, n, h,
theta ))
        u = np.random.uniform()
        if u < rho:
            theta = theta_p
        samples.append(theta)
    return samples

n = 100
h = 61
lik = st.binom
prior = st.beta(a, b)
sigma = 0.05
niters = 100

sampless = [mh_coin(niters, n, h, theta, lik, prior, sigma) for theta in np.aran
ge(0.1, 1, 0.2)]

# convergence of multiple chains

for samples in sampless:
    plt.plot(samples, '-o')
plt.xlim([0, niters])
plt.ylim([0, 1]);

   _images/mcmc_16_0.png

   there are two main ideas - first that the samples generated by mcmc
   constitute a markov chain, and that this markov chain has a unique
   stationary distribution that is always reached if we geenrate a very
   large number of samples. the seocnd idea is to show that this
   stationary distribution is exactly the posterior distribution that we
   are looking for. we will only give the intuition here as a refreseher.

   since possible transitions depend only on the current and the proposed
   values of \(\theta\), the successive values of \(\theta\) in a
   metropolis-hastings sample consittute a markov chain. recall that for a
   markov chain with a transition matrix \(p\)
   \[\pi = \pi p\]

   means that \(\pi\) is a stationary distribution. if it is posssible to
   go from any state to any other state, then the matrix is irreducible.
   if in addtition, it is not possible to get stuck in an oscillation,
   then the matrix is also aperiodic or mixing. for finite state spaces,
   irreducibility and aperiodicity guarantee the existence of a unique
   stationary state. for continuous state space, we need an additional
   property of positive recurrence - starting from any state, the expected
   time to come back to the original state must be finitte. if we have all
   3 peroperties of irreducibility, aperiodicity and positive recurrence,
   then there is a unique stationary distribution. the term ergodic is a
   little confusiong - most statndard definitinos take ergodicity to be
   equivalent to irreducibiltiy, but often bayesian texts take ergoicity
   to mean irreducibility, aperiodicity and positive recurrence, and we
   wil follow the latter convention. for another intuitive perspective,
   the random walk metropolish-hasting algorithm is analogous to a
   diffusion process. since all states are commmuicating (by design),
   eventually the system will settle into an equilibrium state. this is
   analaogous to converging on the stationary state.

   we will considr the simplest possible scenario for an explicit
   calculation. suppose we have a two-state system where the posterior
   probabilities are \(\theta\) and \(1 - \theta\). suppose \(\theta <
   0.5\). so we have the following picture with the metropolish-hastings
   algorithm: markov chain and we find the stationary distribution \(\pi =
   \left( \begin{array}{cc} p & 1-p \end{array} \right)\) by solving

   to be \(\pi = \left( \begin{array}{cc} \theta & 1-\theta \end{array}
   \right)\), which is the posterior distribtion.

   the final point is that a stationary distribution has to follow the
   detailed balance (reversibitily) criterion that says that the
   id203 of being in state \(x\) and moving to state \(y\) must be
   the same as the id203 of being in state \(y\) and moving to state
   \(x\). or, more briefly,
   \[\pi(x)p(x \to y) = \pi(y)p(y \to x)\]

   and the need to make sure that this condition is true accounts for the
   strange looking acceptance criterion
   \[\min \left(1, \frac{g(\theta_p \ | \ x) p(\theta \ | \
   \theta_p)}{g(\theta \ | \ x) p(\theta_p \ | \ \theta)} \right)\]

intuition[640]  

   we want the stationary distribution \(\pi(x)\) to be the posterior
   distribution \(p(x)\). so we set
   \[p(x)p(x \to y) = p(y)p(y \to x)\]

   rearranging, we get
   \[\frac{p(x \to y)}{p(y \to x)} = \frac{p(y)}{p(x)}\]

   we split the transition id203 into separate proposal \(q\) and
   acceptance \(a\) parts, and after a little algebraic rearrangement get
   \[\frac{a(x \to y)}{a(y \to x)} = \frac{p(y) \, q(y \to x)}{p(x) \, q(x
   \to y)}\]

   an acceptance id203 that meets this conidtion is
   \[a(x \to y) = \min \left(1, \frac{p(y) \, q(y \to x)}{p(x) \, q(x \to
   y)} \right)\]

   since \(a\) in the numerator and denominator are both bounded above by
   1.

   see
   [641]http://www.cs.indiana.edu/~hauserk/downloads/metropolisexplanation
   .pdf for algebraic details.

gibbs sampler[642]  

   suppose we have a vector of parameters \(\theta = (\theta_1, \theta_2,
   \dots, \theta_k)\), and we want to estimate the joint posterior
   distribution \(p(\theta | x)\). suppose we can find and draw random
   samples from all the conditional distributions
   \[\begin{split}p(\theta_1 | \theta_2, \dots \theta_k, x) \\ p(\theta_2
   | \theta_1, \dots \theta_k, x) \\ \dots \\ p(\theta_k | \theta_1,
   \theta_2, \dots, x)\end{split}\]

   with id150, the markov chain is constructed by sampling from
   the conditional distribution for each parameter \(\theta_i\) in turn,
   treating all other parameters as observed. when we have finished
   iterating over all parameters, we are said to have completed one cycle
   of the gibbs sampler. where it is difficult to sample from a
   conditional distribution, we can sample using a metropolis-hastings
   algorithm instead - this is known as metropolis wihtin gibbs.

   id150 is a type of random walk thorugh parameter space, and
   hence can be thought of as a metroplish-hastings algorithm with a
   special proposal distribtion. at each iteration in the cycle, we are
   drawing a proposal for a new value of a particular parameter, where the
   propsal distribution is the conditional posterior id203 of that
   parameter. this means that the propsosal move is always accepted.
   hence, if we can draw ssamples from the ocnditional distributions,
   id150 can be much more efficient than regular
   metropolis-hastings.

   advantages of id150
     * no need to tune proposal distribution
     * proposals are always accepted

   disadvantages of id150
     * need to be able to derive id155 distributions
     * need to be able to draw random samples from contitional id203
       distributions
     * can be very slow if paramters are coorelated becauce you cannot
       take    diagonal    steps (draw picture to illustrate)

motivating example[643]  

   we will use the toy example of estimating the bias of two coins given
   sample pairs \((z_1, n_1)\) and \((z_2, n_2)\) where \(z_i\) is the
   number of heads in \(n_i\) tosses for coin \(i\).

setup[644]  

def bern(theta, z, n):
    """bernoulli likelihood with n trials and z successes."""
    return np.clip(theta**z * (1-theta)**(n-z), 0, 1)

def bern2(theta1, theta2, z1, z2, n1, n2):
    """bernoulli likelihood with n trials and z successes."""
    return bern(theta1, z1, n1) * bern(theta2, z2, n2)

def make_thetas(xmin, xmax, n):
    xs = np.linspace(xmin, xmax, n)
    widths =(xs[1:] - xs[:-1])/2.0
    thetas = xs[:-1]+ widths
    return thetas

def make_plots(x, y, prior, likelihood, posterior, projection=none):
    fig, ax = plt.subplots(1,3, subplot_kw=dict(projection=projection, aspect='e
qual'), figsize=(12,3))
    if projection == '3d':
        ax[0].plot_surface(x, y, prior, alpha=0.3, cmap=plt.cm.jet)
        ax[1].plot_surface(x, y, likelihood, alpha=0.3, cmap=plt.cm.jet)
        ax[2].plot_surface(x, y, posterior, alpha=0.3, cmap=plt.cm.jet)
    else:
        ax[0].contour(x, y, prior)
        ax[1].contour(x, y, likelihood)
        ax[2].contour(x, y, posterior)
    ax[0].set_title('prior')
    ax[1].set_title('likelihood')
    ax[2].set_title('posteior')
    plt.tight_layout()

thetas1 = make_thetas(0, 1, 101)
thetas2 = make_thetas(0, 1, 101)
x, y = np.meshgrid(thetas1, thetas2)

analytic solution[645]  

a = 2
b = 3

z1 = 11
n1 = 14
z2 = 7
n2 = 14

prior = stats.beta(a, b).pdf(x) * stats.beta(a, b).pdf(y)
likelihood = bern2(x, y, z1, z2, n1, n2)
posterior = stats.beta(a + z1, b + n1 - z1).pdf(x) * stats.beta(a + z2, b + n2 -
 z2).pdf(y)
make_plots(x, y, prior, likelihood, posterior)
make_plots(x, y, prior, likelihood, posterior, projection='3d')

   _images/mcmc_28_0.png _images/mcmc_28_1.png

grid approximation[646]  

def c2d(thetas1, thetas2, pdf):
    width1 = thetas1[1] - thetas1[0]
    width2 = thetas2[1] - thetas2[0]
    area = width1 * width2
    pmf = pdf * area
    pmf /= pmf.sum()
    return pmf

_prior = bern2(x, y, 2, 8, 10, 10) + bern2(x, y, 8, 2, 10, 10)
prior_grid = c2d(thetas1, thetas2, _prior)
_likelihood = bern2(x, y, 1, 1, 2, 3)
posterior_grid = _likelihood * prior_grid
posterior_grid /= posterior_grid.sum()
make_plots(x, y, prior_grid, likelihood, posterior_grid)
make_plots(x, y, prior_grid, likelihood, posterior_grid, projection='3d')

   _images/mcmc_31_0.png _images/mcmc_31_1.png

metropolis[647]  

a = 2
b = 3

z1 = 11
n1 = 14
z2 = 7
n2 = 14

prior = lambda theta1, theta2: stats.beta(a, b).pdf(theta1) * stats.beta(a, b).p
df(theta2)
lik = partial(bern2, z1=z1, z2=z2, n1=n1, n2=n2)
target = lambda theta1, theta2: prior(theta1, theta2) * lik(theta1, theta2)

theta = np.array([0.5, 0.5])
niters = 10000
burnin = 500
sigma = np.diag([0.2,0.2])

thetas = np.zeros((niters-burnin, 2), np.float)
for i in range(niters):
    new_theta = stats.multivariate_normal(theta, sigma).rvs()
    p = min(target(*new_theta)/target(*theta), 1)
    if np.random.rand() < p:
        theta = new_theta
    if i >= burnin:
        thetas[i-burnin] = theta

kde = stats.gaussian_kde(thetas.t)
xy = np.vstack([x.ravel(), y.ravel()])
posterior_metroplis = kde(xy).reshape(x.shape)
make_plots(x, y, prior(x, y), lik(x, y), posterior_metroplis)
make_plots(x, y, prior(x, y), lik(x, y), posterior_metroplis, projection='3d')

   _images/mcmc_34_0.png _images/mcmc_34_1.png

gibbs[648]  

a = 2
b = 3

z1 = 11
n1 = 14
z2 = 7
n2 = 14

prior = lambda theta1, theta2: stats.beta(a, b).pdf(theta1) * stats.beta(a, b).p
df(theta2)
lik = partial(bern2, z1=z1, z2=z2, n1=n1, n2=n2)
target = lambda theta1, theta2: prior(theta1, theta2) * lik(theta1, theta2)

theta = np.array([0.5, 0.5])
niters = 10000
burnin = 500
sigma = np.diag([0.2,0.2])

thetas = np.zeros((niters-burnin,2), np.float)
for i in range(niters):
    theta = [stats.beta(a + z1, b + n1 - z1).rvs(), theta[1]]
    theta = [theta[0], stats.beta(a + z2, b + n2 - z2).rvs()]

    if i >= burnin:
        thetas[i-burnin] = theta

kde = stats.gaussian_kde(thetas.t)
xy = np.vstack([x.ravel(), y.ravel()])
posterior_gibbs = kde(xy).reshape(x.shape)
make_plots(x, y, prior(x, y), lik(x, y), posterior_gibbs)
make_plots(x, y, prior(x, y), lik(x, y), posterior_gibbs, projection='3d')

   _images/mcmc_37_0.png _images/mcmc_37_1.png

slice sampler[649]  

   yet another mcmc algorithm is slice sampling. in slice sampling, the
   markov chain is constructed by using an auxiliary variable representing
   slices throuth the (unnomrmalized) posterior distribution that is
   constructed using only the current parmater value. like id150,
   there is no tuning processs and all proposals are accepted. for slice
   sampling, you either need the inverse distibution function or some way
   to estimate it.

   a toy example illustrates the process - suppose we want to draw random
   samples from the posterior distribution \(\mathcal{n}(0, 1)\) using
   slice sampling

   start with some value \(x\) - sample \(y\) from \(\mathcal{u}(0,
   f(x))\) - this is the horizontal    slice    that gives the method its name
   - sample the next \(x\) from \(f^{-1}(y)\) - this is typicaly done
   numerically - repeat
# code illustrating idea of slice sampler

import scipy.stats as stats

dist = stats.norm(5, 3)
w = 0.5
x = dist.rvs()

niters = 1000
xs = []
while len(xs) < niters:
    y = np.random.uniform(0, dist.pdf(x))
    lb = x
    rb = x
    while y < dist.pdf(lb):
        lb -= w
    while y < dist.pdf(rb):
        rb += w
    x = np.random.uniform(lb, rb)
    if y > dist.pdf(x):
        if np.abs(x-lb) < np.abs(x-rb):
            lb = x
        else:
            lb = y
    else:
        xs.append(x)

plt.hist(xs, 20);

   _images/mcmc_40_0.png

   notes on the slice sampler:
     * the slice may consist of disjoint pieces for multimodal
       distribtuions
     * the slice can be a rectangular hyperslab for multivariable
       posterior distributions
     * sampling from the slice (i.e. finding the boundaries at level
       \(y\)) is non-trivial and may involve iterative rejection steps -
       see figure below from wikipedia for a typical approach - the blue
       bars represent disjoint pieces of the true slice through a bimodal
       distribution and the black lines are the proposal distribution
       approximaitng the true slice

   slice sampling algorithm from wikipedia

   slice sampling algorithm from wikipedia

id187[650]  

   id187 have the following structure - first we specify
   that the data come from a distribution with parameers \(\theta\)
   \[x \sim f(x\ | \ \theta)\]

   and that the parameters themselves come from anohter distribution with
   hyperparameters \(\lambda\)
   \[\theta \sim g(\theta \ | \ \lambda)\]

   and finally that \(\lambda\) comes from a prior distribution
   \[\lambda \sim h(\lambda)\]

   more levels of hiearchy are possible - i.e you can specify
   hyper-hyperparameters for the dsitribution of \(\lambda\) and so on.

   the essential idea of the hierarchical model is because the \(\theta\)s
   are not independent but rather are drawen from a common distribution
   with parameter \(\lambda\), we can share information across the
   \(\theta\)s by also estimating \(\lambda\) at the same time.

   as an example, suppose have data about the proportion of heads after
   some number of tosses from several coins, and we want to estimate the
   bias of each coin. we also know that the coins come from the same mint
   and so might share soem common manufacturing defect. there are two
   extreme apporaches - we could estimate the bias of each coin from its
   coin toss data independently of all the others, or we could pool the
   results together and estimate the same bias for all coins. hiearchical
   models proivde a compromise where we shrink individual estiamtes
   towards a common estimate.

   note that because of the conditionally indpeendent structure of
   hiearchical models, id150 is often a natural choice for the
   mcmc sampling strategy.

   suppose we have data of the number of failures (\(y_i\)) for each of 10
   pumps in a nuclear plant. we also have the times (\(_i\)) at which each
   pump was observed. we want to model the number of failures with a
   poisson likelihood, where the expected number of failure \(\lambda_i\)
   differs for each pump. since the time which we observed each pump is
   different, we need to scale each \(\lambda_i\) by its observed time
   \(t_i\).

   we now specify the hiearchcical model - note change of notation from
   the overview above - that \(\theta\) is \(\lambda\) (parameter) and
   \(\lambda\) is \(\beta\) (hyperparameter) simply because \(\lambda\) is
   traditional for the poisson distribution parameter.

   the likelihood \(f\) is
   \[\prod_{i=1}^{10} \text{poisson}(\lambda_i t_i)\]

   we let the prior \(g\) for \(\lambda\) be
   \[\text{gamma}(\alpha, \beta)\]

   with \(\alpha = 1.8\) (an improper prior whose integral does not sum to
   1)

   and let the hyperprior \(h\) for \(\beta\) to be
   \[\text{gamma}(\gamma, \delta)\]

   with \(\gamma = 0.01\) and \(\delta = 1\).

   there are 11 unknown parameters (10 \(\lambda\)s and \(\beta\)) in this
   hierarchical model.

   the posterior is
   \[p(\lambda, \beta \ | \ y, t) = \prod_{i=1}^{10}
   \text{poisson}(\lambda_i t_i) \times \text{gamma}(\alpha, \beta) \times
   \text{gamma}(\gamma, \delta)\]

   with the condiitonal distributions needed for id150 given by
   \[p(\lambda_i \ | \ \lambda_{-i}, \beta, y, t) = \text{gamma}(y_i +
   \alpha, t_i + \beta)\]

   and
   \[p(\beta \ | \ \lambda, y, t) = \text{gamma}(10\alpha + \gamma, \delta
   + \sum_{i=1}^10 \lambda_i)\]
from numpy.random import gamma as rgamma # rename so we can use gamma for parame
ter name

def lambda_update(alpha, beta, y, t):
    return rgamma(size=len(y), shape=y+alpha, scale=1.0/(t+beta))

def beta_update(alpha, gamma, delta, lambd, y):
    return rgamma(size=1, shape=len(y) * alpha + gamma, scale=1.0/(delta + lambd
.sum()))

def gibbs(niter, y, t, alpha, gamma, delta):
    lambdas_ = np.zeros((niter, len(y)), np.float)
    betas_ = np.zeros(niter, np.float)

    lambda_ = y/t

    for i in range(niter):
        beta_ = beta_update(alpha, gamma, delta, lambda_, y)
        lambda_ = lambda_update(alpha, beta_, y, t)

        betas_[i] = beta_
        lambdas_[i,:] = lambda_

    return betas_, lambdas_

alpha = 1.8
gamma = 0.01
delta = 1.0
beta0 = 1
y = np.array([5, 1, 5, 14, 3, 19, 1, 1, 4, 22], np.int)
t = np.array([94.32, 15.72, 62.88, 125.76, 5.24, 31.44, 1.05, 1.05, 2.10, 10.48]
, np.float)
niter = 1000

betas, lambdas = gibbs(niter, y, t, alpha, gamma, delta)
print '%.3f' % betas.mean()
print '%.3f' % betas.std(ddof=1)
print lambdas.mean(axis=0)
print lambdas.std(ddof=1, axis=0)

2.469
0.692
[ 0.0697  0.1557  0.1049  0.1236  0.6155  0.619   0.809   0.8304  1.2989
  1.8404]
[ 0.027   0.0945  0.0396  0.0305  0.2914  0.1355  0.5152  0.529   0.57
  0.391 ]

plt.figure(figsize=(10, 20))
for i in range(len(lambdas.t)):
    plt.subplot(5,2,i+1)
    plt.plot(lambdas[::10, i]);
    plt.title('trace for $\lambda$%d' % i)

   _images/mcmc_50_0.png

latex for markov chain diagram[651]  

   documentclass[10pt]{article} usepackage[usenames]{color}
   usepackage{amssymb} usepackage{amsmath} usepackage[utf8]{inputenc}
   usepackage {tikz} usetikzlibrary{automata,arrows,positioning}

   begin{tikzpicture}[->,>=stealth   ,shorten >=1pt,auto,node
   distance=2.8cm, semithick] tikzstyle{every
   state}=[fill=white,draw=black,thick,text=black,scale=1] node[state] (a)
   {$theta$}; node[state] (b) [right of=a] {$1-theta$}; path (a) edge
   [bend left] node[above] {$1$} (b); path (b) edge [bend left]
   node[below] {$frac{theta}{1-theta}$} (a); path (a) edge [loop above]
   node {0} (a); path (b) edge [loop above] node
   {$1-frac{theta}{1-theta}$} (b); end{tikzpicture}

   [652]back to top

   created using [653]sphinx 1.3.1.

references

   1. https://people.duke.edu/~ccc14/sta-663/index.html
   2. https://people.duke.edu/~ccc14/sta-663/pymc2.html
   3. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html
   4. https://people.duke.edu/~ccc14/sta-663/index.html
   5. https://people.duke.edu/~ccc14/sta-663/index.html
   6. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html
   7. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#variables
   8. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#operators
   9. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#iterators
  10. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#conditional-statements
  11. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#functions
  12. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#strings-and-string-handling
  13. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#lists-tuples-dictionaries
  14. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#classes
  15. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#modules
  16. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#the-standard-library
  17. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#keeping-the-anaconda-distribution-up-to-date
  18. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#exercises
  19. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html
  20. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#cells
  21. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#code-cells
  22. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#magic-commands
  23. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#python-as-glue
  24. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#python-r-matlab-octave
  25. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#more-glue-julia-and-perl
  26. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html
  27. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#function-argumnents
  28. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#call-by-object-reference
  29. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#binding-of-default-arguments-occurs-at-function-definition
  30. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#higher-order-functions
  31. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#anonymous-functions
  32. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#pure-functions
  33. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#recursion
  34. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#iterators
  35. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#generators
  36. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#generators-and-comprehensions
  37. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#utilites-enumerate-zip-and-the-ternary-if-else-operator
  38. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#decorators
  39. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-operator-module
  40. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-functools-module
  41. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-itertools-module
  42. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-toolz-fn-and-funcy-modules
  43. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#exercises
  44. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html
  45. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html#obtaining-data
  46. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html#scrubbing-data
  47. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html#exercises
  48. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html
  49. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#string-methods
  50. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#splitting-and-joining-strings
  51. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#the-string-module
  52. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#regular-expressions
  53. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#the-nltk-toolkit
  54. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#exercises
  55. https://people.duke.edu/~ccc14/sta-663/textprocessingextras.html
  56. https://people.duke.edu/~ccc14/sta-663/textprocessingextras.html#example-counting-words-in-a-document
  57. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html
  58. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#using-sqlite3
  59. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#basic-concepts-of-database-id172
  60. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#using-hdf5
  61. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#interfacing-withpandas
  62. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html
  63. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#references
  64. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#example
  65. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#ndarray
  66. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#broadcasting-row-column-and-matrix-operations
  67. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#universal-functions-ufuncs
  68. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#generalized-ufucns
  69. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#random-numbers
  70. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#linear-algebra
  71. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#exercises
  72. https://people.duke.edu/~ccc14/sta-663/usingpandas.html
  73. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#series
  74. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#dataframe
  75. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#panels
  76. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#split-apply-combine
  77. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-statsmodels
  78. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-r-from-ipython
  79. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-rmagic
  80. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-r-from-pandas
  81. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html
  82. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html#textbook-example-is-coin-fair
  83. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html#bayesian-approach
  84. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html#comment
  85. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html
  86. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#some-examples-of-numbers-behaving-badly
  87. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#finite-representation-of-numbers
  88. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#using-arbitrary-precision-libraries
  89. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#from-numbers-to-functions-stability-and-conditioning
  90. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#exercises
  91. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html
  92. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html#profling-and-benchmarking
  93. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html#measuring-algorithmic-complexity
  94. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html#space-complexity
  95. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html
  96. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#simultaneous-equations
  97. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#linear-independence
  98. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#norms-and-distance-of-vectors
  99. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#trace-and-determinant-of-matrices
 100. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#column-space-row-space-rank-and-kernel
 101. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#matrices-as-linear-transformations
 102. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#matrix-norms
 103. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#special-matrices
 104. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#exercises
 105. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html
 106. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#large-linear-systems
 107. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#example-netflix-competition-circa-2006-2009
 108. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#matrix-decompositions
 109. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#matrix-decompositions-for-pca-and-least-squares
 110. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#singular-value-decomposition
 111. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#stabilty-and-condition-number
 112. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#exercises
 113. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html
 114. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#variance-and-covariance
 115. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#eigendecomposition-of-the-covariance-matrix
 116. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#pca
 117. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#change-of-basis-via-pca
 118. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#graphical-illustration-of-change-of-basis
 119. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#dimension-reduction-via-pca
 120. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#using-singular-value-decomposition-svd-for-pca
 121. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html
 122. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#example-maximum-likelihood-estimation-id113
 123. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#bisection-method
 124. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#secant-method
 125. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#newton-rhapson-method
 126. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#gauss-newton
 127. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#inverse-quadratic-interpolation
 128. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#brent-s-method
 129. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html
 130. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#finding-roots
 131. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#optimization-primer
 132. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#using-scipy-optimize
 133. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#gradient-deescent
 134. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#newton-s-method-and-variants
 135. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#constrained-optimization
 136. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#curve-fitting
 137. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#finding-paraemeters-for-ode-models
 138. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#optimization-of-graph-node-placement
 139. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#optimization-of-standard-statistical-models
 140. https://people.duke.edu/~ccc14/sta-663/calibratingodes.html
 141. https://people.duke.edu/~ccc14/sta-663/calibratingodes.html#d-example
 142. https://people.duke.edu/~ccc14/sta-663/calibratingodes.html#id1
 143. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html
 144. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html#optimizers
 145. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html#solvers
 146. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html#glm-estimation-and-irls
 147. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html
 148. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#jensen-s-inequality
 149. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#maximum-likelihood-with-complete-information
 150. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#incomplete-information
 151. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#gaussian-mixture-models
 152. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#using-em
 153. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#vectorized-version
 154. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#vectorization-with-einstein-summation-notation
 155. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#comparison-of-em-routines
 156. https://people.duke.edu/~ccc14/sta-663/montecarlo.html
 157. https://people.duke.edu/~ccc14/sta-663/montecarlo.html#pseudorandom-number-generators-prng
 158. https://people.duke.edu/~ccc14/sta-663/montecarlo.html#monte-carlo-swindles-variance-reduction-techniques
 159. https://people.duke.edu/~ccc14/sta-663/montecarlo.html#quasi-random-numbers
 160. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html
 161. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#resampling
 162. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#simulations
 163. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#setting-the-random-seed
 164. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#sampling-with-and-without-replacement
 165. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#calculation-of-cook-s-distance
 166. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#permutation-resampling
 167. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#design-of-simulation-experiments
 168. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#example-simulations-to-estimate-power
 169. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#check-with-r
 170. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#estimating-the-cdf
 171. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#estimating-the-pdf
 172. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#kernel-density-estimation
 173. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#multivariate-kerndel-density-estimation
 174. https://people.duke.edu/~ccc14/sta-663/mcmc.html
 175. https://people.duke.edu/~ccc14/sta-663/mcmc.html#bayesian-data-analysis
 176. https://people.duke.edu/~ccc14/sta-663/mcmc.html#metropolis-hastings-sampler
 177. https://people.duke.edu/~ccc14/sta-663/mcmc.html#gibbs-sampler
 178. https://people.duke.edu/~ccc14/sta-663/mcmc.html#slice-sampler
 179. https://people.duke.edu/~ccc14/sta-663/mcmc.html#hierarchical-models
 180. https://people.duke.edu/~ccc14/sta-663/pymc2.html
 181. https://people.duke.edu/~ccc14/sta-663/pymc2.html#coin-toss
 182. https://people.duke.edu/~ccc14/sta-663/pymc2.html#estimating-mean-and-standard-deviation-of-normal-distribution
 183. https://people.duke.edu/~ccc14/sta-663/pymc2.html#estimating-parameters-of-a-linear-regreession-model
 184. https://people.duke.edu/~ccc14/sta-663/pymc2.html#estimating-parameters-of-a-logistic-model
 185. https://people.duke.edu/~ccc14/sta-663/pymc2.html#using-a-hierarchcical-model
 186. https://people.duke.edu/~ccc14/sta-663/pymc3.html
 187. https://people.duke.edu/~ccc14/sta-663/pymc3.html#coin-toss
 188. https://people.duke.edu/~ccc14/sta-663/pymc3.html#estimating-mean-and-standard-deviation-of-normal-distribution
 189. https://people.duke.edu/~ccc14/sta-663/pymc3.html#estimating-parameters-of-a-linear-regreession-model
 190. https://people.duke.edu/~ccc14/sta-663/pymc3.html#estimating-parameters-of-a-logistic-model
 191. https://people.duke.edu/~ccc14/sta-663/pymc3.html#using-a-hierarchcical-model
 192. https://people.duke.edu/~ccc14/sta-663/pystan.html
 193. https://people.duke.edu/~ccc14/sta-663/pystan.html#references
 194. https://people.duke.edu/~ccc14/sta-663/pystan.html#simple-logistic-model
 195. https://people.duke.edu/~ccc14/sta-663/animation.html
 196. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html
 197. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#hello-world
 198. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#a-tutorial-example-coding-a-fibonacci-function-in-c
 199. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#types-in-c
 200. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#operators
 201. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#control-of-program-flow
 202. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#arrays-and-pointers
 203. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#functions
 204. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#function-pointers
 205. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#using-make-to-compile-c-programs
 206. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#exercise
 207. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html
 208. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#profiling
 209. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#using-better-algorihtms-and-data-structures
 210. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#i-o-bound-problems
 211. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#problem-set-for-optimization
 212. https://people.duke.edu/~ccc14/sta-663/fromctopython.html
 213. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#example-the-fibonacci-sequence
 214. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#using-clang-and-bitey
 215. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#using-gcc-and-ctypes
 216. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#using-cython
 217. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#benchmark
 218. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html
 219. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#c
 220. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#id1
 221. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#fortran
 222. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#benchmarking
 223. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#wrapping-a-function-from-a-c-library-for-use-in-python
 224. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#wrapping-functions-from-c-library-for-use-in-pyton
 225. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html
 226. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html#defining-a-function-in-julia
 227. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html#using-it-in-python
 228. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html#using-python-libraries-in-julia
 229. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html
 230. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#example-fibonacci
 231. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#example-matrix-multiplication
 232. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#example-pairwise-distance-matrix
 233. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#profiling-code
 234. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#numba
 235. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#cython
 236. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#comparison-with-optimized-c-from-scipy
 237. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html
 238. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#python-version
 239. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numpy-version
 240. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numexpr-version
 241. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numba-version
 242. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numbapro-version
 243. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#parakeet-version
 244. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#cython-version
 245. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#c-version
 246. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#id1
 247. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#fortran-version
 248. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#bake-off
 249. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#summary
 250. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#recommendations-for-optimizing-python-code
 251. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html
 252. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#concepts
 253. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#embarassingly-parallel-programs
 254. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#using-multiprocessing
 255. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#using-ipython-parallel-for-interactive-parallel-computing
 256. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#other-parallel-programming-approaches-not-covered
 257. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#references
 258. https://people.duke.edu/~ccc14/sta-663/cudapython.html
 259. https://people.duke.edu/~ccc14/sta-663/cudapython.html#programming-gpus
 260. https://people.duke.edu/~ccc14/sta-663/cudapython.html#gpu-architecture
 261. https://people.duke.edu/~ccc14/sta-663/cudapython.html#cuda-python
 262. https://people.duke.edu/~ccc14/sta-663/cudapython.html#getting-started-with-cuda
 263. https://people.duke.edu/~ccc14/sta-663/cudapython.html#vector-addition-the-hello-world-of-cuda
 264. https://people.duke.edu/~ccc14/sta-663/cudapython.html#performing-a-reduction-on-cuda
 265. https://people.duke.edu/~ccc14/sta-663/cudapython.html#recreational
 266. https://people.duke.edu/~ccc14/sta-663/cudapython.html#more-examples
 267. https://people.duke.edu/~ccc14/sta-663/gpusandcudac.html
 268. https://people.duke.edu/~ccc14/sta-663/gpusandcudac.html#review-of-gpu-architechture-a-simplification
 269. https://people.duke.edu/~ccc14/sta-663/gpusandcudac.html#cuda-c-program-an-outline
 270. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html
 271. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#why-and-when-does-distributed-computing-matter
 272. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#ingredients-for-effiicient-distributed-computing
 273. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#what-is-hadoop
 274. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#review-of-functional-programming
 275. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#the-hadoop-mapreduce-workflow
 276. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#using-hadoop-mapreduce
 277. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#spark
 278. https://people.duke.edu/~ccc14/sta-663/mapreduce.html
 279. https://people.duke.edu/~ccc14/sta-663/mapreduce.html#mapreduce-code
 280. https://people.duke.edu/~ccc14/sta-663/mapreduce.html#configuration-file
 281. https://people.duke.edu/~ccc14/sta-663/mapreduce.html#launching-job
 282. https://people.duke.edu/~ccc14/sta-663/spark.html
 283. https://people.duke.edu/~ccc14/sta-663/spark.html#using-spark-in-standalone-prograsm
 284. https://people.duke.edu/~ccc14/sta-663/spark.html#introduction-to-spark-concepts-with-a-data-manipulation-example
 285. https://people.duke.edu/~ccc14/sta-663/spark.html#using-the-mllib-for-regression
 286. https://people.duke.edu/~ccc14/sta-663/spark.html#references
 287. https://people.duke.edu/~ccc14/sta-663/modulesandpackaging.html
 288. https://people.duke.edu/~ccc14/sta-663/modulesandpackaging.html#modules
 289. https://people.duke.edu/~ccc14/sta-663/modulesandpackaging.html#distributing-your-package
 290. https://people.duke.edu/~ccc14/sta-663/jupyter.html
 291. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-jupyter
 292. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-other-kernels
 293. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-extensions
 294. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-python3-while-keeping-python2
 295. https://people.duke.edu/~ccc14/sta-663/jupyter.html#now-restart-your-notebook-server
 296. https://people.duke.edu/~ccc14/sta-663/multikernel.html
 297. https://people.duke.edu/~ccc14/sta-663/multikernel.html#python-2
 298. https://people.duke.edu/~ccc14/sta-663/multikernel.html#python-3
 299. https://people.duke.edu/~ccc14/sta-663/multikernel.html#bash
 300. https://people.duke.edu/~ccc14/sta-663/multikernel.html#r
 301. https://people.duke.edu/~ccc14/sta-663/multikernel.html#scala
 302. https://people.duke.edu/~ccc14/sta-663/multikernel.html#julia
 303. https://people.duke.edu/~ccc14/sta-663/multikernel.html#processing
 304. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html
 305. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#statistical-foundations
 306. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#computing-foundations
 307. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#mathematical-foundations
 308. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#statistical-algorithms
 309. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#libraries-worth-knowing-about-after-numpy-scipy-and-matplotlib
 310. https://people.duke.edu/~ccc14/sta-663/mcmc.html
 311. https://people.duke.edu/~ccc14/sta-663/mcmc.html
 312. https://people.duke.edu/~ccc14/sta-663/mcmc.html#bayesian-data-analysis
 313. https://people.duke.edu/~ccc14/sta-663/mcmc.html#motivating-example
 314. https://people.duke.edu/~ccc14/sta-663/mcmc.html#analytical-solution
 315. https://people.duke.edu/~ccc14/sta-663/mcmc.html#numerical-integration
 316. https://people.duke.edu/~ccc14/sta-663/mcmc.html#metropolis-hastings-sampler
 317. https://people.duke.edu/~ccc14/sta-663/mcmc.html#intuition
 318. https://people.duke.edu/~ccc14/sta-663/mcmc.html#gibbs-sampler
 319. https://people.duke.edu/~ccc14/sta-663/mcmc.html#id1
 320. https://people.duke.edu/~ccc14/sta-663/mcmc.html#setup
 321. https://people.duke.edu/~ccc14/sta-663/mcmc.html#analytic-solution
 322. https://people.duke.edu/~ccc14/sta-663/mcmc.html#grid-approximation
 323. https://people.duke.edu/~ccc14/sta-663/mcmc.html#metropolis
 324. https://people.duke.edu/~ccc14/sta-663/mcmc.html#gibbs
 325. https://people.duke.edu/~ccc14/sta-663/mcmc.html#slice-sampler
 326. https://people.duke.edu/~ccc14/sta-663/mcmc.html#hierarchical-models
 327. https://people.duke.edu/~ccc14/sta-663/mcmc.html#latex-for-markov-chain-diagram
 328. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html
 329. https://people.duke.edu/~ccc14/sta-663/pymc2.html
 330. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html
 331. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#variables
 332. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#operators
 333. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#iterators
 334. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#conditional-statements
 335. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#functions
 336. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#strings-and-string-handling
 337. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#lists-tuples-dictionaries
 338. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#classes
 339. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#modules
 340. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#the-standard-library
 341. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#keeping-the-anaconda-distribution-up-to-date
 342. https://people.duke.edu/~ccc14/sta-663/introductiontopythonsolutions.html#exercises
 343. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html
 344. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#cells
 345. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#code-cells
 346. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#magic-commands
 347. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#python-as-glue
 348. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#python-r-matlab-octave
 349. https://people.duke.edu/~ccc14/sta-663/ipythonnotebookintroduction.html#more-glue-julia-and-perl
 350. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html
 351. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#function-argumnents
 352. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#call-by-object-reference
 353. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#binding-of-default-arguments-occurs-at-function-definition
 354. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#higher-order-functions
 355. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#anonymous-functions
 356. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#pure-functions
 357. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#recursion
 358. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#iterators
 359. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#generators
 360. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#generators-and-comprehensions
 361. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#utilites-enumerate-zip-and-the-ternary-if-else-operator
 362. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#decorators
 363. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-operator-module
 364. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-functools-module
 365. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-itertools-module
 366. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#the-toolz-fn-and-funcy-modules
 367. https://people.duke.edu/~ccc14/sta-663/functionssolutions.html#exercises
 368. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html
 369. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html#obtaining-data
 370. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html#scrubbing-data
 371. https://people.duke.edu/~ccc14/sta-663/dataprocessingsolutions.html#exercises
 372. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html
 373. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#string-methods
 374. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#splitting-and-joining-strings
 375. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#the-string-module
 376. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#regular-expressions
 377. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#the-nltk-toolkit
 378. https://people.duke.edu/~ccc14/sta-663/textprocessingsolutions.html#exercises
 379. https://people.duke.edu/~ccc14/sta-663/textprocessingextras.html
 380. https://people.duke.edu/~ccc14/sta-663/textprocessingextras.html#example-counting-words-in-a-document
 381. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html
 382. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#using-sqlite3
 383. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#basic-concepts-of-database-id172
 384. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#using-hdf5
 385. https://people.duke.edu/~ccc14/sta-663/workingwithstructureddata.html#interfacing-withpandas
 386. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html
 387. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#references
 388. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#example
 389. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#ndarray
 390. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#broadcasting-row-column-and-matrix-operations
 391. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#universal-functions-ufuncs
 392. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#generalized-ufucns
 393. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#random-numbers
 394. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#linear-algebra
 395. https://people.duke.edu/~ccc14/sta-663/usingnumpysolutions.html#exercises
 396. https://people.duke.edu/~ccc14/sta-663/usingpandas.html
 397. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#series
 398. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#dataframe
 399. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#panels
 400. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#split-apply-combine
 401. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-statsmodels
 402. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-r-from-ipython
 403. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-rmagic
 404. https://people.duke.edu/~ccc14/sta-663/usingpandas.html#using-r-from-pandas
 405. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html
 406. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html#textbook-example-is-coin-fair
 407. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html#bayesian-approach
 408. https://people.duke.edu/~ccc14/sta-663/computationalstatisticsmotivation.html#comment
 409. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html
 410. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#some-examples-of-numbers-behaving-badly
 411. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#finite-representation-of-numbers
 412. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#using-arbitrary-precision-libraries
 413. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#from-numbers-to-functions-stability-and-conditioning
 414. https://people.duke.edu/~ccc14/sta-663/computerarithmetic.html#exercises
 415. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html
 416. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html#profling-and-benchmarking
 417. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html#measuring-algorithmic-complexity
 418. https://people.duke.edu/~ccc14/sta-663/algorithmiccomplexity.html#space-complexity
 419. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html
 420. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#simultaneous-equations
 421. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#linear-independence
 422. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#norms-and-distance-of-vectors
 423. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#trace-and-determinant-of-matrices
 424. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#column-space-row-space-rank-and-kernel
 425. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#matrices-as-linear-transformations
 426. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#matrix-norms
 427. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#special-matrices
 428. https://people.duke.edu/~ccc14/sta-663/linearalgebrareview.html#exercises
 429. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html
 430. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#large-linear-systems
 431. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#example-netflix-competition-circa-2006-2009
 432. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#matrix-decompositions
 433. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#matrix-decompositions-for-pca-and-least-squares
 434. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#singular-value-decomposition
 435. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#stabilty-and-condition-number
 436. https://people.duke.edu/~ccc14/sta-663/linearalgebramatrixdecompwithsolutions.html#exercises
 437. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html
 438. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#variance-and-covariance
 439. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#eigendecomposition-of-the-covariance-matrix
 440. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#pca
 441. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#change-of-basis-via-pca
 442. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#graphical-illustration-of-change-of-basis
 443. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#dimension-reduction-via-pca
 444. https://people.duke.edu/~ccc14/sta-663/pcasolutions.html#using-singular-value-decomposition-svd-for-pca
 445. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html
 446. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#example-maximum-likelihood-estimation-id113
 447. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#bisection-method
 448. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#secant-method
 449. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#newton-rhapson-method
 450. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#gauss-newton
 451. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#inverse-quadratic-interpolation
 452. https://people.duke.edu/~ccc14/sta-663/optimizationinonedimension.html#brent-s-method
 453. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html
 454. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#finding-roots
 455. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#optimization-primer
 456. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#using-scipy-optimize
 457. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#gradient-deescent
 458. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#newton-s-method-and-variants
 459. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#constrained-optimization
 460. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#curve-fitting
 461. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#finding-paraemeters-for-ode-models
 462. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#optimization-of-graph-node-placement
 463. https://people.duke.edu/~ccc14/sta-663/blackboxoptimization.html#optimization-of-standard-statistical-models
 464. https://people.duke.edu/~ccc14/sta-663/calibratingodes.html
 465. https://people.duke.edu/~ccc14/sta-663/calibratingodes.html#d-example
 466. https://people.duke.edu/~ccc14/sta-663/calibratingodes.html#id1
 467. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html
 468. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html#optimizers
 469. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html#solvers
 470. https://people.duke.edu/~ccc14/sta-663/multivariateoptimizationalgortihms.html#glm-estimation-and-irls
 471. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html
 472. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#jensen-s-inequality
 473. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#maximum-likelihood-with-complete-information
 474. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#incomplete-information
 475. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#gaussian-mixture-models
 476. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#using-em
 477. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#vectorized-version
 478. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#vectorization-with-einstein-summation-notation
 479. https://people.duke.edu/~ccc14/sta-663/emalgorithm.html#comparison-of-em-routines
 480. https://people.duke.edu/~ccc14/sta-663/montecarlo.html
 481. https://people.duke.edu/~ccc14/sta-663/montecarlo.html#pseudorandom-number-generators-prng
 482. https://people.duke.edu/~ccc14/sta-663/montecarlo.html#monte-carlo-swindles-variance-reduction-techniques
 483. https://people.duke.edu/~ccc14/sta-663/montecarlo.html#quasi-random-numbers
 484. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html
 485. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#resampling
 486. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#simulations
 487. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#setting-the-random-seed
 488. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#sampling-with-and-without-replacement
 489. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#calculation-of-cook-s-distance
 490. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#permutation-resampling
 491. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#design-of-simulation-experiments
 492. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#example-simulations-to-estimate-power
 493. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#check-with-r
 494. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#estimating-the-cdf
 495. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#estimating-the-pdf
 496. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#kernel-density-estimation
 497. https://people.duke.edu/~ccc14/sta-663/resamplingandmontecarlosimulations.html#multivariate-kerndel-density-estimation
 498. https://people.duke.edu/~ccc14/sta-663/mcmc.html
 499. https://people.duke.edu/~ccc14/sta-663/mcmc.html#bayesian-data-analysis
 500. https://people.duke.edu/~ccc14/sta-663/mcmc.html#metropolis-hastings-sampler
 501. https://people.duke.edu/~ccc14/sta-663/mcmc.html#gibbs-sampler
 502. https://people.duke.edu/~ccc14/sta-663/mcmc.html#slice-sampler
 503. https://people.duke.edu/~ccc14/sta-663/mcmc.html#hierarchical-models
 504. https://people.duke.edu/~ccc14/sta-663/pymc2.html
 505. https://people.duke.edu/~ccc14/sta-663/pymc2.html#coin-toss
 506. https://people.duke.edu/~ccc14/sta-663/pymc2.html#estimating-mean-and-standard-deviation-of-normal-distribution
 507. https://people.duke.edu/~ccc14/sta-663/pymc2.html#estimating-parameters-of-a-linear-regreession-model
 508. https://people.duke.edu/~ccc14/sta-663/pymc2.html#estimating-parameters-of-a-logistic-model
 509. https://people.duke.edu/~ccc14/sta-663/pymc2.html#using-a-hierarchcical-model
 510. https://people.duke.edu/~ccc14/sta-663/pymc3.html
 511. https://people.duke.edu/~ccc14/sta-663/pymc3.html#coin-toss
 512. https://people.duke.edu/~ccc14/sta-663/pymc3.html#estimating-mean-and-standard-deviation-of-normal-distribution
 513. https://people.duke.edu/~ccc14/sta-663/pymc3.html#estimating-parameters-of-a-linear-regreession-model
 514. https://people.duke.edu/~ccc14/sta-663/pymc3.html#estimating-parameters-of-a-logistic-model
 515. https://people.duke.edu/~ccc14/sta-663/pymc3.html#using-a-hierarchcical-model
 516. https://people.duke.edu/~ccc14/sta-663/pystan.html
 517. https://people.duke.edu/~ccc14/sta-663/pystan.html#references
 518. https://people.duke.edu/~ccc14/sta-663/pystan.html#simple-logistic-model
 519. https://people.duke.edu/~ccc14/sta-663/animation.html
 520. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html
 521. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#hello-world
 522. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#a-tutorial-example-coding-a-fibonacci-function-in-c
 523. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#types-in-c
 524. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#operators
 525. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#control-of-program-flow
 526. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#arrays-and-pointers
 527. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#functions
 528. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#function-pointers
 529. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#using-make-to-compile-c-programs
 530. https://people.duke.edu/~ccc14/sta-663/crashcourseinc.html#exercise
 531. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html
 532. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#profiling
 533. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#using-better-algorihtms-and-data-structures
 534. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#i-o-bound-problems
 535. https://people.duke.edu/~ccc14/sta-663/makingcodefast.html#problem-set-for-optimization
 536. https://people.duke.edu/~ccc14/sta-663/fromctopython.html
 537. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#example-the-fibonacci-sequence
 538. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#using-clang-and-bitey
 539. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#using-gcc-and-ctypes
 540. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#using-cython
 541. https://people.duke.edu/~ccc14/sta-663/fromctopython.html#benchmark
 542. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html
 543. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#c
 544. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#id1
 545. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#fortran
 546. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#benchmarking
 547. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#wrapping-a-function-from-a-c-library-for-use-in-python
 548. https://people.duke.edu/~ccc14/sta-663/fromcompiledtopython.html#wrapping-functions-from-c-library-for-use-in-pyton
 549. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html
 550. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html#defining-a-function-in-julia
 551. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html#using-it-in-python
 552. https://people.duke.edu/~ccc14/sta-663/fromjuliatopython.html#using-python-libraries-in-julia
 553. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html
 554. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#example-fibonacci
 555. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#example-matrix-multiplication
 556. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#example-pairwise-distance-matrix
 557. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#profiling-code
 558. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#numba
 559. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#cython
 560. https://people.duke.edu/~ccc14/sta-663/frompythontoc.html#comparison-with-optimized-c-from-scipy
 561. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html
 562. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#python-version
 563. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numpy-version
 564. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numexpr-version
 565. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numba-version
 566. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#numbapro-version
 567. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#parakeet-version
 568. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#cython-version
 569. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#c-version
 570. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#id1
 571. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#fortran-version
 572. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#bake-off
 573. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#summary
 574. https://people.duke.edu/~ccc14/sta-663/optimization_bakeoff.html#recommendations-for-optimizing-python-code
 575. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html
 576. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#concepts
 577. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#embarassingly-parallel-programs
 578. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#using-multiprocessing
 579. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#using-ipython-parallel-for-interactive-parallel-computing
 580. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#other-parallel-programming-approaches-not-covered
 581. https://people.duke.edu/~ccc14/sta-663/writingparallelcode.html#references
 582. https://people.duke.edu/~ccc14/sta-663/cudapython.html
 583. https://people.duke.edu/~ccc14/sta-663/cudapython.html#programming-gpus
 584. https://people.duke.edu/~ccc14/sta-663/cudapython.html#gpu-architecture
 585. https://people.duke.edu/~ccc14/sta-663/cudapython.html#cuda-python
 586. https://people.duke.edu/~ccc14/sta-663/cudapython.html#getting-started-with-cuda
 587. https://people.duke.edu/~ccc14/sta-663/cudapython.html#vector-addition-the-hello-world-of-cuda
 588. https://people.duke.edu/~ccc14/sta-663/cudapython.html#performing-a-reduction-on-cuda
 589. https://people.duke.edu/~ccc14/sta-663/cudapython.html#recreational
 590. https://people.duke.edu/~ccc14/sta-663/cudapython.html#more-examples
 591. https://people.duke.edu/~ccc14/sta-663/gpusandcudac.html
 592. https://people.duke.edu/~ccc14/sta-663/gpusandcudac.html#review-of-gpu-architechture-a-simplification
 593. https://people.duke.edu/~ccc14/sta-663/gpusandcudac.html#cuda-c-program-an-outline
 594. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html
 595. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#why-and-when-does-distributed-computing-matter
 596. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#ingredients-for-effiicient-distributed-computing
 597. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#what-is-hadoop
 598. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#review-of-functional-programming
 599. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#the-hadoop-mapreduce-workflow
 600. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#using-hadoop-mapreduce
 601. https://people.duke.edu/~ccc14/sta-663/distributedcomputing.html#spark
 602. https://people.duke.edu/~ccc14/sta-663/mapreduce.html
 603. https://people.duke.edu/~ccc14/sta-663/mapreduce.html#mapreduce-code
 604. https://people.duke.edu/~ccc14/sta-663/mapreduce.html#configuration-file
 605. https://people.duke.edu/~ccc14/sta-663/mapreduce.html#launching-job
 606. https://people.duke.edu/~ccc14/sta-663/spark.html
 607. https://people.duke.edu/~ccc14/sta-663/spark.html#using-spark-in-standalone-prograsm
 608. https://people.duke.edu/~ccc14/sta-663/spark.html#introduction-to-spark-concepts-with-a-data-manipulation-example
 609. https://people.duke.edu/~ccc14/sta-663/spark.html#using-the-mllib-for-regression
 610. https://people.duke.edu/~ccc14/sta-663/spark.html#references
 611. https://people.duke.edu/~ccc14/sta-663/modulesandpackaging.html
 612. https://people.duke.edu/~ccc14/sta-663/modulesandpackaging.html#modules
 613. https://people.duke.edu/~ccc14/sta-663/modulesandpackaging.html#distributing-your-package
 614. https://people.duke.edu/~ccc14/sta-663/jupyter.html
 615. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-jupyter
 616. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-other-kernels
 617. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-extensions
 618. https://people.duke.edu/~ccc14/sta-663/jupyter.html#installing-python3-while-keeping-python2
 619. https://people.duke.edu/~ccc14/sta-663/jupyter.html#now-restart-your-notebook-server
 620. https://people.duke.edu/~ccc14/sta-663/multikernel.html
 621. https://people.duke.edu/~ccc14/sta-663/multikernel.html#python-2
 622. https://people.duke.edu/~ccc14/sta-663/multikernel.html#python-3
 623. https://people.duke.edu/~ccc14/sta-663/multikernel.html#bash
 624. https://people.duke.edu/~ccc14/sta-663/multikernel.html#r
 625. https://people.duke.edu/~ccc14/sta-663/multikernel.html#scala
 626. https://people.duke.edu/~ccc14/sta-663/multikernel.html#julia
 627. https://people.duke.edu/~ccc14/sta-663/multikernel.html#processing
 628. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html
 629. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#statistical-foundations
 630. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#computing-foundations
 631. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#mathematical-foundations
 632. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#statistical-algorithms
 633. https://people.duke.edu/~ccc14/sta-663/reveiwandtrends.html#libraries-worth-knowing-about-after-numpy-scipy-and-matplotlib
 634. https://people.duke.edu/~ccc14/sta-663/mcmc.html#markov-chain-monte-carlo-mcmc
 635. https://people.duke.edu/~ccc14/sta-663/mcmc.html#bayesian-data-analysis
 636. https://people.duke.edu/~ccc14/sta-663/mcmc.html#motivating-example
 637. https://people.duke.edu/~ccc14/sta-663/mcmc.html#analytical-solution
 638. https://people.duke.edu/~ccc14/sta-663/mcmc.html#numerical-integration
 639. https://people.duke.edu/~ccc14/sta-663/mcmc.html#metropolis-hastings-sampler
 640. https://people.duke.edu/~ccc14/sta-663/mcmc.html#intuition
 641. http://www.cs.indiana.edu/~hauserk/downloads/metropolisexplanation.pdf
 642. https://people.duke.edu/~ccc14/sta-663/mcmc.html#gibbs-sampler
 643. https://people.duke.edu/~ccc14/sta-663/mcmc.html#id1
 644. https://people.duke.edu/~ccc14/sta-663/mcmc.html#setup
 645. https://people.duke.edu/~ccc14/sta-663/mcmc.html#analytic-solution
 646. https://people.duke.edu/~ccc14/sta-663/mcmc.html#grid-approximation
 647. https://people.duke.edu/~ccc14/sta-663/mcmc.html#metropolis
 648. https://people.duke.edu/~ccc14/sta-663/mcmc.html#gibbs
 649. https://people.duke.edu/~ccc14/sta-663/mcmc.html#slice-sampler
 650. https://people.duke.edu/~ccc14/sta-663/mcmc.html#hierarchical-models
 651. https://people.duke.edu/~ccc14/sta-663/mcmc.html#latex-for-markov-chain-diagram
 652. https://people.duke.edu/~ccc14/sta-663/mcmc.html
 653. http://sphinx.pocoo.org/
