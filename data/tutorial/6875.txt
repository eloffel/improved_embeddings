   #[1]preferred research blog [2]preferred networks research blog    acl
   2017 report comments feed [3]deep id23 bootcamp:
   event report [4]guest blog with weihua, a former intern at pfn
   [5]alternate [6]alternate

[7]preferred networks research blog

   [8]preferred networks, inc.

[9]acl 2017 report

   [2ced444145eb97dc2b1644380bac516f?s=450&d=identicon&r=g]

   yuta kikuchi

   2017-09-08 13:54:22

   writers: yuta kikuchi, sosuke kobayashi

   preferred networks (pfn) attended the 55th annual meeting of the
   association for computational linguistics (acl 2017) in vancouver,
   canada. acl is one of the largest conferences in the natural language
   processing (nlp) field.

   as in other machine learning research fields, use of deep learning in
   nlp is increasing. the most popular topic in nlp deep learning is
   sequence-to-sequence learning tasks. this model receives a sequence of
   discrete symbols (words) and learns to output a correct sequence
   conditioned by the input.

   img_4383 (1)

   in acl 2017, there were 302 accepted papers selected from 1,318
   submissions (about 23% acceptance rate). detailed and interesting
   statistics, such as hot topic or procrastination graph, are available
   at [10]pc chairs blog of acl 2017. in addition,  there is an
   [11]interesting visualization of paper titles that made by scattertext
   presented as an acl 2017 demo paper: [12]scattertext: a browser-based
   tool for visualizing how corpora differ

   some topics that were popular in acl 2017:

id4

   there were many studies of id4 (id4). although
   machine translation has long been one of the most popular research
   topics, this new paradigm brought major impact in this field.

   almost all id4 studies are based on the neural network model called
   encoder-decoder with attention mechanism. it consists of an encoder
   that converts the input sentence of source language to hidden vector
   representation and a decoder that converts the hidden vector to output
   sentence of target language. at each time-step, while decoding words
   sequentially, the attention mechanism decides which part of the source
   sentence should be focused on by the decoder. many id4 studies show
   significant improvements compared with traditional machine learning
   paradigms, such as example-based or phrase-based machine translation.

   the task id4 attempts to solve can be regarded as a more general
   problem, sequence-to-sequence (id195) learning, which can be applied
   to many other tasks such as syntactic parsing, image captioning, or
   dialogue response generation. hence, progress in id4 can be ported to
   other id195 tasks, and vise versa.

   in this post, we select a few topics on id4 from acl 2017.

incorporate structure of sentences in sequence-to-sequence learning

   many papers this year incorporated sentence structure in
   sequence-to-sequence learning. paper titles often had keywords such as
   syntax, tree, dependency, or chunk, which indicate some structural
   information of a sentence:
     * [13]modeling source syntax for id4
     * [14]sequence-to-dependency id4
     * [15]learning to parse and translate improves neural machine
       translation
     * [16]towards string-to-tree id4
     * [17]chunk-based decoder for id4
     * [18]improved id4 with a syntax-aware encoder
       and decoder
     * [19]chunk-based bi-scale decoder for id4

   although various types of structures are explored, the benefits of
   these structures is often not clear, so it is difficult to determine
   which structures to use. these techniques can be transferred to other
   sequence-to-sequence tasks as mentioned above. it is also interesting
   to conduct a task crossing investigation that what kind of structural
   information impacts for each task.
   we look forward to reports on benefits comparison between these
   structures and further discussion.

first id4 workshops

   acl 2017 held their first workshops ever on id4, with keynote talks
   from cutting-edge researchers and many interesting papers. the first
   best paper award of this workshop was given to [20]stronger baselines
   for trustable results in id4 by michael
   denkowski and graham neubig. they conducted many comparisons among
   several new id4 techniques reported in recent years. the results show
   that your new id4 method should be compared with the simple but strong
   baseline method: adam with multiple restarts, sub-word translation via
   byte pair encoding, and ensemble decoding. many other interesting
   studies were presented, which are available on [21]the workshop page.

low resource

   although id4 has shown great success in language pairs which have a
   large amount of training data (pairs of input and output sentence), one
   big problem is how to deal with low-resource language pairs or domains.

   [22]six challenges for id4, which is published
   at the first workshop of id4, showed that the size of training data has
   great impact on the performance of id4. in other words, without
   preparing enough training data, we cannot achieve good performance with
   id4 systems.

   studies which attempt to overcome this problem have been constantly
   published for long time before the introduction of id4. acl 2017 also
   had some studies that addressed this problem:
     * [23]a teacher-student framework for zero-resource neural machine
       translation
     * [24]data augmentation for low-resource id4
     * [25]model transfer for tagging low-resource languages using a
       bilingual dictionary
     * [26]bayesian modeling of lexical resources for low-resource
       settings
     * [27]learning bilingual id27s with (almost) no bilingual
       data

robotics

   acl held the first workshop on language grounding for robotics this
   year. using natural language as a means of human robot interaction is
   important to allow those can   t program well to control robots. the
   workshop had several interesting keynote talks from various fields
   including nlp, robotics, and machine learning.

   there were a poster session and a lively panel discussion. human robot
   communication and collaboration are challenging and interesting
   applications of nlp. hence, we are hoping the future direction of this
   topic will be more active.

   here are some of the keynote slides at [28]the workshop page.

evaluating id86

   although id86 systems based on neural networks
   have grown rapidly, one big problem remaining is the lack of automatic
   id74.

   dominant id74 are based on the degree of word overlap
   with human generated sentences (e.g. id7, id8). however, obviously
   it cannot handle cases where word surfaces change as in id141 or
   sentence generation.

   few weeks ago, the organizer of the workshop on asian translation (wat
   2017) published the result of their shared task of translation.
   the evaluation results showed that even models which obtained very high
   [29]id7 score could not obtain a high [30]score from humans.

   the problem is more serious in the dialogue response generation task
   because the possible output (response) is more flexible than other
   id86 (id86) tasks. liu et al. showed that
   existing automatic id74 correlate very poorly with human
   evaluation of response quality ([31]liu et al., 2016).

   in acl 2017, [32]towards an automatic turing test: learning to evaluate
   dialogue responses presented their attempt to overcome this problem.
   they trained an lstm-based model which receives an input context, a
   human response, and a system-generated response, and outputs a scalar
   value that represents the quality of the system   s response given the
   context and the human response.
   the results show that their method correlated significantly better with
   human evaluation score than the word overlap based metrics such as
   id7.
   although there are some concerns about using learning based evaluators
   in terms of reliability, reproducibility, or transportability to other
   domains, this is a very important direction for future id86 research.

id23

   utilizing id23 for deep learning recently got plenty
   of attention from the nlp community. in acl 2017, there were several
   papers which use id23 for their study.

   [33]towards end-to-end id23 of dialogue agents for
   information access and [34]hybrid code networks: practical and
   efficient end-to-end dialog control with supervised and reinforcement
   learning focused on dialogue modeling, which has a challenge in
   handling a long-term strategy of conversation. moreover,
   (task-oriented) dialogues often include non-differentiable actions such
   as retrieving database, sorting the candidates, or updating the
   database entities.

   [35]coarse-to-fine id53 for long documents combined
   models for document summarization (selecting important sentences) and
   answer generation to deal with longer documents in id53
   (qa) task. since there are no annotation of important sentences for qa
   dataset., they trained a sentence extraction module with reinforcement
   learning whose reward is the log id203 of the correct answer
   given that sentence.

   [36]from language to programs: bridging id23 and
   maximum marginal likelihood tackled a kind of id29 task,
   where a model selects a sequence of actions (program) to affect the
   environment. they argued the problem of spurious programs which get to
   the correct final state (output) in an incorrect way (a sequence of
   actions). to prevent this problem, they proposed an algorithm called
   randomer, which contains two components:   -greedy randomized beam
   search to reduce initialization sensitivity, and   -meritocratic
   parameter update rule for smoothing update weight of each action
   sequence.

   id23 has gained lots of attention in nlp. next
   september, the conference on empirical methods on natural language
   processing (emnlp 2017), which is also one of the flagship conferences
   in nlp, will be held in copenhagen, denmark. according to the
   [37]accepted paper list of emnlp, or arxiv preprint of them, the amount
   of studies who utilize id23 has increased further.
   keep your eyes on emnlp as well if you are interested in reinforcement
   learning.

adversarial training

   adversarial training comes under the spotlight in the area of machine
   learning. the most popular topic in this area is generative adversarial
   networks (gan). although challenging studies of gan for natural
   language generation have also been appeared, acl 2017 (and co-located
   conll 2017) presented many papers using adversarial training for domain
   adaptation, id72, or multilingual learning in various
   nlp tasks:
     * [38]adversarial connective-exploiting networks for implicit
       discourse relation classification
     * [39]adversarial id72 for text classification
     * [40]adversarial training for unsupervised bilingual lexicon
       induction
     * [41]adversarial adaptation of synthetic or stale data
     * [42]adversarial multi-criteria learning for chinese word
       segmentation
     * [43]adversarial training for cross-domain universal dependency
       parsing
     * [44]cross-language learning with adversarial neural networks

   in this context, adversarial training is used to make learned feature
   distributions for multiple domains closer and make knowledge from each
   domain transferrable. almost all the above studies are based on two
   related works: [45]unsupervised id20 by id26
   from icml 2015, and [46]domain separation networks from nips 2016.
   these two papers proposed many key technologies in this topic:
   shared-private model, (adversarial) domain classifier, gradient
   reversal layer, orthogonal constraints, and reconstruction. we
   recommend reading more details of those two papers before reading the
   related papers in acl 2017.

   here is a paper which is more unique among the adversarial papers in
   acl 2017; [47]adversarial training for unsupervised bilingual lexicon
   induction.
   the motivation of this paper is to project words in two languages into
   the same vector space. their method learns a linear transformation
   matrix g (generator) to map the word-vector space of one language to
   that of another without any supervision (i.e. manually created
   dictionary). they prepare a discriminator that classifies whether a
   given vector is a real word vector from the target language or a word
   vector mapped from the source language by g. the result shows their
   method obtained competitive results compared with related work which
   uses seed word translation pairs as supervision.

   img_4475.jpg

like this:

   like loading...
     * [48]conference

leave a reply

   ______________________ name (required)

   ______________________ mail (will not be published) (required)

   ______________________ website


   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________

   submit comment

     * links
          + [49]japanese research blog
     * search
       search for: ____________________ search

   copyright    preferred networks, inc. altl rights reserved.

   [50][feed.png]

   iframe: [51]likes-master

   %d bloggers like this:

references

   visible links
   1. https://preferredresearch.jp/feed/
   2. https://preferredresearch.jp/2017/09/08/acl-2017-report/feed/
   3. https://preferredresearch.jp/2017/09/04/deep-rl-bootcamp/
   4. https://preferredresearch.jp/2017/09/11/guest-weihua/
   5. https://preferredresearch.jp/wp-json/oembed/1.0/embed?url=https://preferredresearch.jp/2017/09/08/acl-2017-report/
   6. https://preferredresearch.jp/wp-json/oembed/1.0/embed?url=https://preferredresearch.jp/2017/09/08/acl-2017-report/&format=xml
   7. https://preferredresearch.jp/
   8. https://www.preferred-networks.jp/en/
   9. https://preferredresearch.jp/2017/09/08/acl-2017-report/
  10. https://chairs-blog.acl2017.org/2017/02/15/statistics-on-submissions-and-status-update/
  11. http://nbviewer.jupyter.org/github/jasonkessler/jasonkessler.github.io/blob/master/analysis-of-accepted-acl-2017-papers.ipynb
  12. http://aclweb.org/anthology/p/p17/p17-4015.pdf
  13. http://aclweb.org/anthology/p/p17/p17-1064.pdf
  14. http://aclweb.org/anthology/p/p17/p17-1065.pdf
  15. http://aclweb.org/anthology/p/p17/p17-2012.pdf
  16. http://aclweb.org/anthology/p/p17/p17-2021.pdf
  17. http://aclweb.org/anthology/p/p17/p17-1174.pdf
  18. http://aclweb.org/anthology/p/p17/p17-1177.pdf
  19. http://aclweb.org/anthology/p/p17/p17-2092.pdf
  20. http://aclweb.org/anthology/w/w17/w17-3203.pdf
  21. https://sites.google.com/site/acl17id4/
  22. http://aclweb.org/anthology/w/w17/w17-3203.pdf
  23. http://aclweb.org/anthology/p/p17/p17-1176.pdf
  24. http://aclweb.org/anthology/p/p17/p17-2090.pdf
  25. http://aclweb.org/anthology/p/p17/p17-2093.pdf
  26. http://aclweb.org/anthology/p/p17/p17-1095.pdf
  27. http://aclweb.org/anthology/p/p17/p17-1042.pdf
  28. https://robonlp2017.github.io/schedule.html
  29. http://lotus.kuee.kyoto-u.ac.jp/wat/evaluation/list.php?t=1&o=1#id7.html
  30. http://lotus.kuee.kyoto-u.ac.jp/wat/evaluation/list.php?t=1&o=1#human_wat2017.html
  31. http://aclweb.org/anthology/d/d16/d16-1230.pdf
  32. http://aclweb.org/anthology/p/p17/p17-1103.pdf
  33. http://aclweb.org/anthology/p/p17/p17-1045.pdf
  34. http://aclweb.org/anthology/p/p17/p17-1062.pdf
  35. http://aclweb.org/anthology/p/p17/p17-1020.pdf
  36. http://aclweb.org/anthology/p/p17/p17-1097.pdf
  37. http://emnlp2017.net/accepted-papers.html
  38. http://aclweb.org/anthology/p/p17/p17-1093.pdf
  39. http://aclweb.org/anthology/p/p17/p17-1001.pdf
  40. http://aclweb.org/anthology/p/p17/p17-1179.pdf
  41. http://aclweb.org/anthology/p/p17/p17-1119.pdf
  42. http://aclweb.org/anthology/p/p17/p17-1110.pdf
  43. http://aclweb.org/anthology/k/k17/k17-3007.pdf
  44. http://aclweb.org/anthology/k/k17/k17-1024.pdf
  45. http://proceedings.mlr.press/v37/ganin15.html
  46. https://papers.nips.cc/paper/6254-domain-separation-networks
  47. http://aclweb.org/anthology/p/p17/p17-1179.pdf
  48. https://preferredresearch.jp/category/conference-2/
  49. https://research.preferred.jp/
  50. https://preferredresearch.jp/feed/
  51. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914

   hidden links:
  53. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fpreferredresearch.jp%2f2017%2f09%2f08%2facl-2017-report%2f&linkname=acl%202017%20report
  54. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fpreferredresearch.jp%2f2017%2f09%2f08%2facl-2017-report%2f&linkname=acl%202017%20report
  55. https://www.addtoany.com/add_to/email?linkurl=https%3a%2f%2fpreferredresearch.jp%2f2017%2f09%2f08%2facl-2017-report%2f&linkname=acl%202017%20report
  56. https://www.addtoany.com/share
