    [1]main site

   the clearbrain blog covers learnings and thoughts in the disciplines of
   big data, data science, and machine learning.

featured posts:

   [2]discover your product   s    7 friends in 10 days   
   [3]programmatically managing an evergreen facebook marketing campaign
   [4]using apache spark for machine learning     benefits of dataframes vs.
   rdds
   [5]how to automatically segment your data with id91
   [6]4 reasons your machine learning model is wrong (and how to fix it)

explaining your machine learning model (or 5 ways to assess feature
importance)

   july 27, 2017
   [7]ml101
   [8]source: xkcd

   when productionizing a machine learning model, simply outputting a
   propensity in a black box isn   t always sufficient. we often want to
   understand which features in the model are most important. we want
   actionable insights.

   assessing the why can thus be just as important as the how. knowing
   which features, inputs, or variables in a model are influencing its
   effectiveness is valuable to improving its actionability. case in
   point, knowing which user is predicted to churn can help you fill a
   leaky bucket, but understanding why users are likely to churn can help
   to close the leak before it occurs.

   assessing feature importance though is not straightforward. there are
   many ways to assess how a variable is influencing a model, and they
   have their respective benefits and tradeoffs. certain methods produce
   more consistent results, while others are only applicable to specific
   models. below we outline five ways of addressing feature importance,
   with a focus on [9]id28 models for simplicity.

method #0: pearson correlations (or why you shouldn   t use them)

   a quick and dirty way to discern relationships between a feature and a
   goal are pearson correlations.

   correlations assess the strength of a linear relationship between two
   variables. they explain how much a change in your input variable will
   affect the output you   re trying to predict, producing a value between
   -100% and 100%.

   while simple to grasp (and straightforward to compute in even excel),
   correlations are limited in their utility. for one, they only explain
   linear relationships between variables. they provide little value in
   assessing slopes or nonlinear relationships (see the second and third
   rows in the image above). but more importantly, pearson correlations
   only explain patterns in your underlying data, not what your machine
   learning model actually learns. the standard mantra    correlations do
   not imply causation    applies.

   in machine learning, your trained model learns the underlying
   relationships between not just your input and target variable, but
   interactions between your respective variables. correlations don   t
   account for these interactions, and are not determined by causal
   id136. case in point, the # of firefighters responding to a forest
   fire are likely highly correlated to the severity of the fire, but that
   does not mean they caused the fire itself.

   so how do we assess actual causal relationships?

method #1: model coefficients

   to represent feature importance by causal relationships, one option is
   to deconstruct the machine learning model itself into its components.
   in the case of id28, this constitutes evaluating the
   trained model coefficients.

   as [10]previously noted, id28 models train a
   probabilistic function to evaluate the propensity of an outcome (p)
   based on some input variables (x). model training produces a series of
   coefficients (  ) or weights for those variables that can best classify
   an outcome.

   the coefficients can actually in turn be used to rank your respective
   features or inputs. as the coefficients indicate the relative
   contribution of each variable to the id203 function (see figure
   above), ranking them from largest to smallest is essentially a proxy
   for feature importance.

   the positives of this approach are that the feature rankings are
   identical for all users. as the coefficients are an inherent property
   of the trained model, the rankings you get are relatively stable across
   users. this produces a consistent method of analysis for feature
   importance.

   some caveats with this approach to consider though. first, before
   training your model you have to ensure all your features are on the
   same scale. without [11]scaling, variables with different ranges would
   have different coefficient ranges, and not comparable. also if you have
   a lot of features, there is a possibility of unstable coefficients due
   to interactions between collinear features. in this case you should
   implement id173 techniques like [12]l2 id173 during
   model training, to reduce the impact of these effects.

   lastly, while the rankings produced via coefficients are stable, they
   are not inherently interpretable. the coefficient values don   t
   necessarily have semantic or actionable meaning. so it can be difficult
   for non-technical stakeholders to gauge how much actionable influence a
   change in a feature will result in towards their goal.

method #2: odds ratio

   a similar methodology for feature importance, but with a little more
   semantic meaning, is the odds ratio.

   odds ratio for a feature constitutes a couple of concepts. the    odds   
   indicate the id203 of an outcome occurring vs. the id203 of
   it not. the odds ratio for a particular feature in turn refers to the
   multiplicative increase in the odds due to a 1-unit change in a
   feature.

   for a id28 model specifically, the math works out to a
   simple formula (see figure above), wherein the odds ratio for a
   particular feature can be derived from the euler's exponent of the
   respective coefficient (  ).

   the benefit of this approach is similar to method #1, in that it is
   derived from the components of the model. the output is for one a
   really simple calculation (e^  ). you also get a stable ranking of
   values for your features agnostic to an individual user (assuming
   you   ve scaled and regularized them pre model-training). and there is
   some added interpretability to the ranking values, in that each metric
   indicates how a change in a feature will affect the outcome.

   still, the interpretation of the rankings is based on the    odds    of an
   outcome - a not completely interpretable concept. the odds of an
   outcome are still a degree removed from explaining direct changes in
   behavior, so can be a bit confusing for nontechnical stakeholders.

method #3: change in id203 via dropping feature

   a more interpretable methodology for feature importance can come in
   evaluating the change in id203.

   there are several ways to evaluate a feature or variable   s contribution
   to the change in id203. the simplest is to just compute the
   id203 of an outcome with all your features, and compare it to the
   id203 with the feature of interest removed. the percentage change
   in id203 would constitute your sensitivity metric or ranking.

   a quick footnote on this approach is that as opposed to methodologies
   #1 and #2, this approach requires providing inputs for the variables
   (x) to actually compute a id203. as such, you should use the
   mean, median, or mode feature vectors, to have a consistent baseline
   for id203 that you are perturbing.

   the primary benefit of this approach is that you get an actionable
   interpretation to your feature rankings. each ranking metric or
   sensitivity constitutes the change in id203 for your average user
   given the variable of interest.

   the negatives to the approach though lie in that it has to be evaluated
   for a specific user, and by consequence each user will have different
   feature rankings. to ensure stability in the rankings you can evaluate
   the sensitivities at the mean or median feature vectors as recommended
   above, but it   s still a generalization.

   nevertheless, computing the change in id203 due to a feature
   provides one of the more interpretable methodologies for feature
   importance - permitting for explanations like    the average user who
   does feature_1 is 34% more likely to perform the goal   .

method #4: change in id203 via slopes

   [597a3be864d8ac0001474fdd_screen%20shot%202017-07-26%20at%205.51.19%20p
   m.png]

   another method for calculating a variable   s contribution to the change
   in id203 is to evaluate its slope.

   computing the slope of your probabilistic model with respect to a
   feature is also straightforward. instead of dropping a particular
   feature, you can perturb it by some small value (  ). more specifically,
   you can compare the change in id203 with a positive/negative
   perturbation (see figure above), and evaluate the percentage
   difference.

   the effect of this approach is similar to methodology #3 in dropping a
   feature - you gain an actionable and interpretable ranking system of
   your respective features. you use a change in id203 as a
   mechanism to assess the sensitivity of a feature to your probabilistic
   outcomes. it however carries some of the same tradeoffs of methodology
   # 3 as well, in that each user will have different rankings for each
   feature, and so you   ll need to assess the respective probabilities at
   an average/median value to have a stable baseline.

   one benefit however, is that this approach can be applied to models
   outside of just id28. as indicated in the derivation
   above, the outcome of the calculation is independent of the inherent
   logistic function or its coefficients, and instead involves just
   perturbing the underlying feature value to extrapolate a change in
   id203. so in theory, you could leverage this same method of
   analysis for id166, decision tree, or even neural networks to establish
   feature importance.

method #5: change in id203 via partial derivatives

   the last methodology for evaluating changes in id203 to ascertain
   feature importance involve a slight iteration of the previous method.
   instead of evaluating approximations for the slope, we can evaluate the
   true slope by assessing the derivative of the machine learning function
   instead.

   the partial derivative specifically assesses the first derivative of
   the logistic function with respect to the feature of interest.
   simplifying a lot of the derivation, you   d arrive at the function noted
   above (see figure above).

   a nice artifact of the partial derivative approach is its consistency
   from a computational approach. the right side of the equation ends up
   being a constant value for every variable evaluated, as the id203
   output is evaluated for all features. indeed, the only difference in
   the partial derivative between features is it   s respective model
   coefficient (  ).

   the caveat to this approach is similar to those prior. namely,
   calculating the partial derivative still requires choosing a respective
   median/mode feature vector to compute the respective id203
   inputs. additionally, this specific derivation for the partial
   derivative is only applicable to id28 models.

   but the partial derivative approach provides a mathematically stable
   method for evaluating contributions to the unit change in id203 -
   in turn a reasonably actionable and interpretable metric for ranking
   features.

   --

   between these five respective approaches, you have several options to
   choose from in evaluating feature importance. each have their benefits
   and tradeoffs between accuracy, stability, and interpretability, but
   all can provide reasonable approaches to understanding causal
   interactions between your user   s actions and the resulting change in
   probabilistic outcomes for your predicted goal.

   *if you're interested in solving problems related to feature importance
   and automating machine learning, check out more at [13]clearbrain.com
   or email us at founders@clearbrain.com.
   [14]    all posts

   the clearbrain blog covers learnings and thoughts in the disciplines of
   big data, data science, and machine learning.

featured posts:

   [15]discover your product   s    7 friends in 10 days   
   [16]programmatically managing an evergreen facebook marketing campaign
   [17]using apache spark for machine learning     benefits of dataframes
   vs. rdds
   [18]how to automatically segment your data with id91
   [19]4 reasons your machine learning model is wrong (and how to fix it)

references

   visible links
   1. https://clearbrain.com/
   2. https://blog.clearbrain.com/posts/discover-your-products-7-friends-in-10-days
   3. https://blog.clearbrain.com/posts/programmatically-managing-an-evergreen-facebook-marketing-campaign
   4. https://blog.clearbrain.com/posts/using-apache-spark-for-machine-learning-benefits-of-dataframes-vs-rdds
   5. https://blog.clearbrain.com/posts/how-to-automatically-segment-your-data-with-id91
   6. https://blog.clearbrain.com/posts/4-reasons-your-machine-learning-model-is-wrong-and-how-to-fix-it
   7. https://blog.clearbrain.com/categories/ml101
   8. https://xkcd.com/1838/
   9. https://blog.clearbrain.com/posts/how-to-predict-yesno-outcomes-using-logistic-regression
  10. https://blog.clearbrain.com/posts/how-to-predict-yesno-outcomes-using-logistic-regression
  11. https://spark.apache.org/docs/latest/mllib-feature-extraction.html#standardscaler
  12. https://en.wikipedia.org/wiki/tikhonov_id173
  13. http://www.clearbrain.com/
  14. https://blog.clearbrain.com/
  15. https://blog.clearbrain.com/posts/discover-your-products-7-friends-in-10-days
  16. https://blog.clearbrain.com/posts/programmatically-managing-an-evergreen-facebook-marketing-campaign
  17. https://blog.clearbrain.com/posts/using-apache-spark-for-machine-learning-benefits-of-dataframes-vs-rdds
  18. https://blog.clearbrain.com/posts/how-to-automatically-segment-your-data-with-id91
  19. https://blog.clearbrain.com/posts/4-reasons-your-machine-learning-model-is-wrong-and-how-to-fix-it

   hidden links:
  21. https://blog.clearbrain.com/
  22. https://www.facebook.com/clearbraininc/
  23. https://twitter.com/clearbraininc
  24. https://angel.co/clearbrain
  25. https://www.linkedin.com/company/clearbrain
  26. https://plus.google.com/112869594240266936119
  27. https://imgs.xkcd.com/comics/machine_learning.png
  28. https://www.facebook.com/clearbraininc/
  29. https://twitter.com/clearbraininc
  30. https://angel.co/clearbrain
  31. https://www.linkedin.com/company/clearbrain
  32. https://plus.google.com/112869594240266936119
