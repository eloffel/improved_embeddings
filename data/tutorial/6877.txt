   #[1]the keras blog atom feed

                               [2]the keras blog

   [3]keras is a deep learning library for python, that is simple,
   modular, and extensible.

     * [4]archives
     * [5]github
     * [6]documentation
     * [7]google group

     [8]a ten-minute introduction to sequence-to-sequence learning in keras

   fri 29 september 2017


    by [9]francois chollet

   in [10]tutorials.

   i see this question a lot -- how to implement id56 sequence-to-sequence
   learning in keras? here is a short introduction.

   note that this post assumes that you already have some experience with
   recurrent networks and keras.
     __________________________________________________________________

what is sequence-to-sequence learning?

   sequence-to-sequence learning (id195) is about training models to
   convert sequences from one domain (e.g. sentences in english) to
   sequences in another domain (e.g. the same sentences translated to
   french).
"the cat sat on the mat" -> [id195 model] -> "le chat etait assis sur le tapis
"

   this can be used for machine translation or for free-from question
   answering (generating a natural language answer given a natural
   language question) -- in general, it is applicable any time you need to
   generate text.

   there are multiple ways to handle this task, either using id56s or using
   1d convnets. here we will focus on id56s.

the trivial case: when input and output sequences have the same length

   when both input sequences and output sequences have the same length,
   you can implement such models simply with a keras lstm or gru layer (or
   stack thereof). this is the case in [11]this example script that shows
   how to teach a id56 to learn to add numbers, encoded as character
   strings:

   id195 id136

   one caveat of this approach is that it assumes that it is possible to
   generate target[...t] given input[...t]. that works in some cases (e.g.
   adding strings of digits) but does not work for most use cases. in the
   general case, information about the entire input sequence is necessary
   in order to start generating the target sequence.

the general case: canonical sequence-to-sequence

   in the general case, input sequences and output sequences have
   different lengths (e.g. machine translation) and the entire input
   sequence is required in order to start predicting the target. this
   requires a more advanced setup, which is what people commonly refer to
   when mentioning "sequence to sequence models" with no further context.
   here's how it works:
     * a id56 layer (or stack thereof) acts as "encoder": it processes the
       input sequence and returns its own internal state. note that we
       discard the outputs of the encoder id56, only recovering the state.
       this state will serve as the "context", or "conditioning", of the
       decoder in the next step.
     * another id56 layer (or stack thereof) acts as "decoder": it is
       trained to predict the next characters of the target sequence,
       given previous characters of the target sequence. specifically, it
       is trained to turn the target sequences into the same sequences but
       offset by one timestep in the future, a training process called
       "teacher forcing" in this context. importantly, the encoder uses as
       initial state the state vectors from the encoder, which is how the
       decoder obtains information about what it is supposed to generate.
       effectively, the decoder learns to generate targets[t+1...] given
       targets[...t], conditioned on the input sequence.

   id195 id136

   in id136 mode, i.e. when we want to decode unknown input sequences,
   we go through a slightly different process:
     * 1) encode the input sequence into state vectors.
     * 2) start with a target sequence of size 1 (just the
       start-of-sequence character).
     * 3) feed the state vectors and 1-char target sequence to the decoder
       to produce predictions for the next character.
     * 4) sample the next character using these predictions (we simply use
       argmax).
     * 5) append the sampled character to the target sequence
     * 6) repeat until we generate the end-of-sequence character or we hit
       the character limit.

   id195 id136

   the same process can also be used to train a id195 network without
   "teacher forcing", i.e. by reinjecting the decoder's predictions into
   the decoder.

a keras example

   let's illustrate these ideas with actual code.

   for our example implementation, we will use a dataset of pairs of
   english sentences and their french translation, which you can download
   from [12]manythings.org/anki. the file to download is called
   fra-eng.zip. we will implement a character-level sequence-to-sequence
   model, processing the input character-by-character and generating the
   output character-by-character. another option would be a word-level
   model, which tends to be more common for machine translation. at the
   end of this post, you will find some notes about turning our model into
   a word-level model using embedding layers.

   the full script for our example [13]can be found on github.

   here's a summary of our process:
     * 1) turn the sentences into 3 numpy arrays, encoder_input_data,
       decoder_input_data, decoder_target_data:
          + encoder_input_data is a 3d array of shape (num_pairs,
            max_english_sentence_length, num_english_characters)
            containing a one-hot vectorization of the english sentences.
          + decoder_input_data is a 3d array of shape (num_pairs,
            max_french_sentence_length, num_french_characters) containg a
            one-hot vectorization of the french sentences.
          + decoder_target_data is the same as decoder_input_data but
            offset by one timestep. decoder_target_data[:, t, :] will be
            the same as decoder_input_data[:, t + 1, :].
     * 2) train a basic lstm-based id195 model to predict
       decoder_target_data given encoder_input_data and
       decoder_input_data. our model uses teacher forcing.
     * 3) decode some sentences to check that the model is working (i.e.
       turn samples from encoder_input_data into corresponding samples
       from decoder_target_data).

   because the training process and id136 process (decoding sentences)
   are quite different, we use different models for both, albeit they all
   leverage the same inner layers.

   this is our training model. it leverages three key features of keras
   id56s:
     * the return_state contructor argument, configuring a id56 layer to
       return a list where the first entry is the outputs and the next
       entries are the internal id56 states. this is used to recover the
       states of the encoder.
     * the inital_state call argument, specifying the initial state(s) of
       a id56. this is used to pass the encoder states to the decoder as
       initial states.
     * the return_sequences constructor argument, configuring a id56 to
       return its full sequence of outputs (instead of just the last
       output, which the defaults behavior). this is used in the decoder.

from keras.models import model
from keras.layers import input, lstm, dense

# define an input sequence and process it.
encoder_inputs = input(shape=(none, num_encoder_tokens))
encoder = lstm(latent_dim, return_state=true)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
# we discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# set up the decoder, using `encoder_states` as initial state.
decoder_inputs = input(shape=(none, num_decoder_tokens))
# we set up our decoder to return full output sequences,
# and to return internal states as well. we don't use the
# return states in the training model, but we will use them in id136.
decoder_lstm = lstm(latent_dim, return_sequences=true, return_state=true)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs,
                                     initial_state=encoder_states)
decoder_dense = dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = model([encoder_inputs, decoder_inputs], decoder_outputs)

   we train our model in two lines, while monitoring the loss on a
   held-out set of 20% of the samples.
# run training
model.compile(optimizer='rmsprop', loss='categorical_crossid178')
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)

   after one hour or so on a macbook cpu, we are ready for id136. to
   decode a test sentence, we will repeatedly:
     * 1) encode the input sentence and retrieve the initial decoder state
     * 2) run one step of the decoder with this initial state and a "start
       of sequence" token as target. the output will be the next target
       character.
     * 3) append the target character predicted and repeat.

   here's our id136 setup:
encoder_model = model(encoder_inputs, encoder_states)

decoder_state_input_h = input(shape=(latent_dim,))
decoder_state_input_c = input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states)

   we use it to implement the id136 loop described above:
def decode_sequence(input_seq):
    # encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index['\t']] = 1.

    # sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = false
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value)

        # sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # exit condition: either hit max length
        # or find stop character.
        if (sampled_char == '\n' or
           len(decoded_sentence) > max_decoder_seq_length):
            stop_condition = true

        # update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.

        # update states
        states_value = [h, c]

    return decoded_sentence

   we get some nice results -- unsurprising since we are decoding samples
   taken from the training test.
input sentence: be nice.
decoded sentence: soyez gentil !
-
input sentence: drop it!
decoded sentence: laissez tomber !
-
input sentence: get out!
decoded sentence: sortez   !

   this concludes our ten-minute introduction to sequence-to-sequence
   models in keras. reminder: the full code for this script [14]can be
   found on github.

references

     * [15]sequence to sequence learning with neural networks
     * [16]learning phrase representations using id56 encoder-decoder for
       id151
     __________________________________________________________________

bonus faq

what if i want to use a gru layer instead of a lstm?

   it's actually a bit simpler, because gru has only one state, whereas
   lstm has two states. here's how to adapt the training model to use a
   gru layer:
encoder_inputs = input(shape=(none, num_encoder_tokens))
encoder = gru(latent_dim, return_state=true)
encoder_outputs, state_h = encoder(encoder_inputs)

decoder_inputs = input(shape=(none, num_decoder_tokens))
decoder_gru = gru(latent_dim, return_sequences=true)
decoder_outputs = decoder_gru(decoder_inputs, initial_state=state_h)
decoder_dense = dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)
model = model([encoder_inputs, decoder_inputs], decoder_outputs)

what if i want to use a word-level model with integer sequences?

   what if your inputs are integer sequences (e.g. representing sequences
   of words, encoded by their index in a dictionary)? you can embed these
   integer tokens via an embedding layer. here's how:
# define an input sequence and process it.
encoder_inputs = input(shape=(none,))
x = embedding(num_encoder_tokens, latent_dim)(encoder_inputs)
x, state_h, state_c = lstm(latent_dim,
                           return_state=true)(x)
encoder_states = [state_h, state_c]

# set up the decoder, using `encoder_states` as initial state.
decoder_inputs = input(shape=(none,))
x = embedding(num_decoder_tokens, latent_dim)(decoder_inputs)
x = lstm(latent_dim, return_sequences=true)(x, initial_state=encoder_states)
decoder_outputs = dense(num_decoder_tokens, activation='softmax')(x)

# define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = model([encoder_inputs, decoder_inputs], decoder_outputs)

# compile & run training
model.compile(optimizer='rmsprop', loss='categorical_crossid178')
# note that `decoder_target_data` needs to be one-hot encoded,
# rather than sequences of integers like `decoder_input_data`!
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)

what if i don't want to use teacher forcing for training?

   in some niche cases you may not be able to use teacher forcing, because
   you don't have access to the full target sequences, e.g. if you are
   doing online training on very long sequences, where buffering complete
   input-target pairs would be impossible. in that case, you may want to
   do training by reinjecting the decoder's predictions into the decoder's
   input, just like we were doing for id136.

   you can achieve this by building a model that hard-codes the output
   reinjection loop:
from keras.layers import lambda
from keras import backend as k

# the first part is unchanged
encoder_inputs = input(shape=(none, num_encoder_tokens))
encoder = lstm(latent_dim, return_state=true)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
states = [state_h, state_c]

# set up the decoder, which will only process one timestep at a time.
decoder_inputs = input(shape=(1, num_decoder_tokens))
decoder_lstm = lstm(latent_dim, return_sequences=true, return_state=true)
decoder_dense = dense(num_decoder_tokens, activation='softmax')

all_outputs = []
inputs = decoder_inputs
for _ in range(max_decoder_seq_length):
    # run the decoder on one timestep
    outputs, state_h, state_c = decoder_lstm(inputs,
                                             initial_state=states)
    outputs = decoder_dense(outputs)
    # store the current prediction (we will concatenate all predictions later)
    all_outputs.append(outputs)
    # reinject the outputs as inputs for the next loop iteration
    # as well as update the states
    inputs = outputs
    states = [state_h, state_c]

# concatenate all predictions
decoder_outputs = lambda(lambda x: k.concatenate(x, axis=1))(all_outputs)

# define and compile model as previously
model = model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='rmsprop', loss='categorical_crossid178')

# prepare decoder input data that just contains the start character
# note that we could have made it a constant hard-coded in the model
decoder_input_data = np.zeros((num_samples, 1, num_decoder_tokens))
decoder_input_data[:, 0, target_token_index['\t']] = 1.

# train model as previously
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)

   if you have more questions, please [17]reach out on twitter.


    powered by [18]pelican, which takes great advantages of [19]python.

references

   1. https://blog.keras.io/
   2. https://blog.keras.io/index.html
   3. https://github.com/fchollet/keras
   4. https://blog.keras.io/
   5. https://github.com/fchollet/keras
   6. http://keras.io/
   7. https://groups.google.com/forum/#!forum/keras-users
   8. https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
   9. https://twitter.com/fchollet
  10. https://blog.keras.io/category/tutorials.html
  11. https://github.com/fchollet/keras/blob/master/examples/addition_id56.py
  12. http://www.manythings.org/anki/
  13. https://github.com/fchollet/keras/blob/master/examples/lstm_id195.py
  14. https://github.com/fchollet/keras/blob/master/examples/lstm_id195.py
  15. https://arxiv.org/abs/1409.3215
  16. https://arxiv.org/abs/1406.1078
  17. https://twitter.com/fchollet
  18. http://alexis.notmyidea.org/pelican/
  19. http://python.org/
