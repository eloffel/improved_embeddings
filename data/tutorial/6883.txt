intro

     * [1]about this site
     * [2]what can you do with ai?
     * [3]how we picked our examples

guides

     * [4]a little history
     * [5]defining ai terms
     * [6]giving your software ai superpowers
     * [7]natural language processing
     * [8]id161
     * [9]training your own models
     * [10]neural network architectures
     * [11]ways in which machines learn
     * [12]code: (re-)training a model
     * [13]code: adding ai to your mobile app

where to next?

     * [14]software and services
     * [15]datasets
     * [16]videos, tutorials, and blogs
     * [17]books, papers, and research

   menu

id161

   there are ai areas focused on different senses, but vision is
   fundamental along with natural language. vision attempts to identify
   and extract symbols from raw visual data and then use those symbols to
   make decisions, take actions or produce information. these symbols have
   many forms: they can be labels from a set used for training, captions,
   text extracted from the image via ocr, colors, and so on. not all
   images are created alike: in general, systems that are good at
   processing attributes for still images are not necessarily as good for
   processing video, and vice-versa.

   sub-domains of id161 include scene reconstruction,
   motion/id37, tracking, object recognition, and image
   restoration among many others.

what can they do?

   current id161 apis provide significant, impressive
   functionality with very little complexity. we can compare raw output
   for the same image and different apis with the following page:

   iframe: [18]/test/image/image-analysis

   there   s plenty of information to be obtained: from tags, captions,
   labels, text (via ocr), detection of adult or inappropriate content,
   etc. some systems will return specific coordinates in the image that
   allow separation of elements, either automatically or with a person   s
   help.

   an image with text areas automatically highlighted

   as in other api types, there is significant variability between
   different services in terms of features, capabilities, and so on. when
   moving between one service and another there aren   t any shortcuts and
   each api call and response will need to be verified again.

how do they perform?

   running enough tests gives us an idea of how these apis perform:
     * image labeling, captioning, and tagging works very well for general
       categories, but precision drops quickly the more specific they try
       to be. this is to be expected given the type of generic training
       given to the model, but it is still important to note.
     * error rates are low, but high enough that you have to prepare for
       them carefully. an api with a 2% error rate will fail outright for
       2 out of every 100 images. for the 2 people that see the results of
       an incorrect analysis, the result can be jarring.
     * image rotation, complexity and quality matter. the same image
       rotated different ways can have significantly different recognition
       results. when the image is complex and has multiple features
       precision also degrades.

   the good news is that all of these things can be addressed by how your
   code uses the underlying apis. also, the systems are improving rapidly.
   for example, a specific type of deep learning system called a
   convolutional neural network (which we'll discuss later) are enabling
   much higher accuracy for rotated images these days. here are some tips
   to get the most out of the current generation of technology:
     * give information on what the system is seeing quickly but use
       smoothing (e.g. with moving averages) to prevent unexpected jumps
       between categories
     * don   t put the error on the person, but on the system.
     * allow quick and easy modifications for parameters that matter, in
       particular rotation and zoom. in the latter case, focusing on less
       cluttered sections of an image will frequently resolve recognition
       ambiguities quickly.

a word on efficiency

   when you are implementing a vision recongition system (or most any
   machine learning-based software system), you need to be aware of two
   costs:
     * training costs. iterating on over different configuration
       parameters in order to increase model performance is a
       time-consuming and expensive process called [19]hyperparameter
       optimization. how much will it cost you to train a model, and what
       kind of accuracy can you get for a given amount of training? this
       type of training consumes lots of cpu (and possibly [20]gpu), so
       you need to keep an eye on your [21]amazon bill.
     * id136 costs. once you have a trained model, you'll use that
       model to "make id136s", the practioner's fancy way of saying
       "using a trained model to make predictions". here, you might need
       to be careful with cpu/gpu usage (battery consumption) or have only
       a limited amount of memory. different algorithms are hungrier for
       power and memory than others, as [22]this handy analysis by alfredo
       canziani, eugenio culurciello (purdue university), and adam paszke
       (university of warsaw) shows. this graph shows the number of
       operations each system (one of the colored bubbles) requires to
       reach a certain accuracy on a specific image recognition test in
       id163, the definitive image-recongition test set.
       how systems compare on number of operations required for a given
       accuracy level

   keep both of these costs in mind as you are designing your system.
    1. [23]natural language processing
    2. [24]deep learning

   [25]follow @a16z on github [26]fork a16z/ai on github

references

   visible links
   1. http://aiplaybook.a16z.com/docs/intro/getting-started
   2. http://aiplaybook.a16z.com/docs/intro/survey-goals
   3. http://aiplaybook.a16z.com/docs/intro/survey-parameters
   4. http://aiplaybook.a16z.com/docs/guides/ai
   5. http://aiplaybook.a16z.com/docs/guides/ai-terms
   6. http://aiplaybook.a16z.com/docs/guides/ai-using
   7. http://aiplaybook.a16z.com/docs/guides/nlp
   8. http://aiplaybook.a16z.com/docs/guides/vision
   9. http://aiplaybook.a16z.com/docs/guides/dl
  10. http://aiplaybook.a16z.com/docs/guides/dl-architectures
  11. http://aiplaybook.a16z.com/docs/guides/dl-learning
  12. http://aiplaybook.a16z.com/docs/guides/dl-start
  13. http://aiplaybook.a16z.com/docs/guides/dl-app
  14. http://aiplaybook.a16z.com/docs/reference/software
  15. http://aiplaybook.a16z.com/docs/reference/datasets
  16. http://aiplaybook.a16z.com/docs/reference/links
  17. http://aiplaybook.a16z.com/docs/reference/bibliography
  18. http://aiplaybook.a16z.com/test/image/image-analysis
  19. https://en.wikipedia.org/wiki/hyperparameter_(machine_learning)#optimization
  20. https://devblogs.nvidia.com/parallelforall/sigopt-deep-learning-hyperparameter-optimization/
  21. https://aws.amazon.com/blogs/ai/fast-id98-tuning-with-aws-gpu-instances-and-sigopt/
  22. https://arxiv.org/pdf/1605.07678.pdf
  23. http://aiplaybook.a16z.com/docs/guides/nlp
  24. http://aiplaybook.a16z.com/docs/guides/dl
  25. https://github.com/a16z
  26. https://github.com/a16z/ai/fork

   hidden links:
  28. http://aiplaybook.a16z.com/
  29. http://aiplaybook.a16z.com/
