intro

     * [1]about this site
     * [2]what can you do with ai?
     * [3]how we picked our examples

guides

     * [4]a little history
     * [5]defining ai terms
     * [6]giving your software ai superpowers
     * [7]natural language processing
     * [8]id161
     * [9]training your own models
     * [10]neural network architectures
     * [11]ways in which machines learn
     * [12]code: (re-)training a model
     * [13]code: adding ai to your mobile app

where to next?

     * [14]software and services
     * [15]datasets
     * [16]videos, tutorials, and blogs
     * [17]books, papers, and research

   menu

neural network architectures

   the fundamental data structure of a neural network is loosely inspired
   by brains. each of your brain cells (neurons) is connected to many
   other neurons by synapses. as you experience and interact with the
   world, your brain creates new connections, strengthens some
   connections, and weakens others. a neural network's data structure has
   many similarities: its "neurons" are nodes in a network connected to
   other nodes. connections between nodes have a strength. [18]neurons
   activate (that is, generate an electrical signal) based on inputs they
   receive from other neurons.

   but don't get too carried away with the biological metaphor, or you
   will anger both neurobiologists and computer scientists who will both
   tell you that neurons in your brain behave very differently from the
   id158s in systems such as tensorflow.

     if you want to see some of this heat, read the [19]ieee interview
     with uc berkeley professor michael jordan from october 2014.

   having said that, let's walk through some of the common neural network
   architectures that you'll come across. "architecture" is a fancy way of
   describing the rules that govern how nodes connect to each other and
   what shapes they can form. the [20]wikipedia article on the types of
   id158s is a good reference for further exploration.

feedforward neural networks (ffns)

   feedforward networks were the first type of id158
   devised. in this network the information moves in only one direction:
   forward. input nodes receive data and pass it along, as seen in figure
   1.

   simple feedforward network

   from the input nodes data flows through the hidden nodes (if any) and
   to the output nodes, without cycles or loops, and may be modified along
   the way by each node. they are called feedforward because processing
   moves forward from left to right. as [21]michael nielsen points out,
   "hidden nodes" aren't some mysterious philosophical or mathematical
   construct: they are simply nodes in the middle of the network that are
   neither an input or an output.

   when feedforward networks have multiple layers, they are called
   multilayer networks.

   create your own feedforward neural networks with the [22]browser-based
   tensorflow playground. we did a brief demo in [23]our ai primer
   (begining at about 27:17), but get in there and click around on your
   own. the things that you can change (depth of the network, the
   activation function, the learning rate, etc.) are called
   hyperparameters. so by clicking around in the playground, you are
   "modifying the hyperparameters of a feedforward multilayer neural
   network". isn't that something?

convolutional (neural) networks (id98s or convnets)

   [24]convolutional neural networks are a specific type of multilayer
   feedforward network typically used in image recognition and (more
   recently) some natural language processing tasks. introduced by
   [25]yann lecun, yoshua bengio, leon bottou, and patrick haffner in
   1998, they were originally to recognize handwritten postal codes and
   check amounts. they are faster to train than traditional feedforward
   networks because they make simplifying assumptions about how nodes
   connect to each other and how many nodes you need in the network,
   drastically reducing how much math you have to do to train the model.
   here's a visual from andrej karparthy's stanford class to get us going.
   don't worry about understanding the details just yet.
   convolutional neural network

biologically inspired?

   some researchers point out that the design of these networks are
   [26]inspired by biology and in particular by the visual cortex.
   [27]hubel and wiesel discovered in the 1960s that cats have cells which
   responded specifically to certain regions of the input coming in from
   the retina, and further that cats had both so-called [28]simple cells
   which responded to lines and edges as well as so-called [complex cells]
   that responded to those same inputs, even if they were rotated or in a
   slightly different place (that is, "spatially invariant"). they
   hypothesized that cells were organized into a hierarchy exactly like
   the neural networks we've been discussing: simple cells would feed
   their output to complex cells in exactly the same fashion as nodes "to
   the left" feed their inputs to nodes further "to the right" in a
   multilayer network.

   yann lecun himself has distanced himself from the biological
   inspiration, saying in [29]a facebook post responding to the
   [30]michael jordan interview mentioned above that:

     the neural inspiration in models like convolutional nets is very
     tenuous. that's why i call them "convolutional nets" not
     "convolutional neural nets", and why we call the nodes "units" and
     not "neurons".

well known implementations of convnets

   specific implementations that you might read about include lenet,
   alexnet, zfnet, googlenet, vggnet, and resnet. some are named after
   people and companies, others by some property of the network. most of
   these have the won gold in the olympics of this space, namely the
   [31]id163 ilsvrc.

   these networks are very cool, and here are a set of resources for
   learning more about them, sorted by complexity (easiest first):
     * read a [32]good beginner's guide written by a ucla computer science
       undergrad named adit deshpande
     * sample some of the answers to the quora questions: [33]"what is a
       convolutional neural network"?, [34]"what is an intuitive
       explanation of convolutional neural networks?"
     * read andrej karpathy's [35]class notes from stanford cs class
       cs231n: convolutional neural networks for visual recognition. make
       sure to scroll down to see the cool animation that shows you what a
       convolution is.
     * play with [36]andrej karpathy's convnetjs demo which trains a
       convolutional neural network on the mnist digits dataset
       (consisting of handwritten numerical digits) in the comfort of your
       own browser. javascript ftw!
     * this [37]2013 paper by matthew zeiler and rob fergus provides some
       visual examples that help you understand the intuition behind the
       architecture.

recurrent neural networks (id56s), including long short-term memories (lstm)

   the third and last type of neural network we'll discuss is the
   recurrent neural network, partly because they are widely used and
   partly because we suspect your eyes are glazing over.

     there are many other types of neural networks. if you are interested
     in learning more, we suggest a visit to the asimov's institute
     [38]neural network zoo

   remember how in a feedforward network, computation only goes forward,
   or if you're looking at a diagram, "from left to right"? also we didn't
   say it, but feedforward (and convolutional networks) take fixed sized
   inputs and outputs. once you decide how many elements in the input and
   output vectors, that's it. you train the model and hope you get good
   results.

   id56s relax both those constraints.

   first, id56s support bi-directional data flow, propagating data from
   later processing stages back to earlier stages as well as linearly from
   input to output. this diagram from [39]christopher olah's excellent
   overview article shows the shape of an id56:
   unrolled recurrent neural network

   this architecture enables the id56 to "remember" things, which makes
   them great for processing time-series data (like events in an event
   log) or natural language processing tasks (like understanding the roles
   each word plays in a sentence, in which remembering what word came
   before can help you figure the role of the current word).

   secondly, id56s can process arbitrarily-sized inputs and outputs by
   processing vectors in a sequence, one at a time. where feedforward and
   id98s only work on fixed sized inputs and outputs, id56s can process
   vectors one after another thereby work on any shape of input and
   output. andrej kaparthy comes to the rescue with a diagram that shows
   this from his excellent blog post titled [40]the unreasonable
   effectiveness of recurrent neural networks:
   arbitrary input and output sizes in id56s

   read andrej's whole blog post, which is a great explanation of the
   structure of id56s. in it, he describes how to build a paul graham essay
   generator by training the system with the full 1m characters of his
   essays (alas, a very small corpus by ai standards) and building an id56.
   you can tune one of the hyperparameters of the id56 to generate the
   sentence that paul graham is most likely to write, and that is an
   infinite loop of:

     "is that they were all the same thing that was a startup is that
     they were all the same thing that was a startup is that they were
     all the same thing that was a startup is that they were all the
     same"

   who said neural nets weren't fun?

   together, these two enhancements over feedforward networks have made
   id56s incredibly powerful tools for solving many different types of ai
   problems including id103, id38, machine
   translation, image captioning, id126s, predicting the
   next word in a sentence for text generation systems, and others.

   a specific type of id56 that you'll see discussed is called the long
   short-term memory (lstm). bizarre, no? is the memory short or the long?
   anyway, this type of id56 was introduced by [41]hochreiter and
   schmidhuber in 1997 and does an even better job of remembering
   something from "further back in time" compared to vanilla id56s.

   to learn more:
     * [42]edwin chen's blog post complete with cartoons of snorlax and
       references to jelly donuts does an excellent job of explaining the
       basic concepts and comparing id56s with lstms.
     * read [43]christopher olah's blog post if you want to understand how
       lstms do their remembering and forgetting. it's a beautiful piece
       of explanatory writing and illustration.
     * rohan kapur's [44]medium post is also great.

how do these architectures relate to the other deep learning frameworks i've
heard of?

   one last topic before we wrap up here is: how do these neural network
   architectures relate to the libraries or frameworks such as tensorflow
   and caffe i've heard of?

   the quick answer is that you can implement most neural net
   architectures in each of the popular neural network libraries. want to
   implement a feedforward or id56 in tensorflow? you can [45]do that. how
   about a lstm in caffe? [46]yup. feedforward network in mxnet? there's
   an [47]api call for that.

   and if you can't implement a specific model in one of the popular
   libraries, you can always write your own so your software can eat the
   world.

   here are [48]15 machine learning libraries to get you started. the
   indefatigable andrej kaparthy posted a "google trends"-esque type
   analysis showing what's hot if you peek inside [49]28,303 machine
   learning research papers over the last 5 years.

key takeaways

   ok, that's enough on neural network architectures. what have we
   learned?
     * there are many different types of neural networks, each useful for
       solving specific ai problems.
     * the field is evolving quickly; ian goodfellow [50]invented gans in
       2014.
     * you got here because you looked for but couldn't find a
       higher-level api that does what your software needs, so you needed
       to train your own model.
     * reminder: neural networks aren't the only machine learning
       algorithms. you might solve your problem with a clean data set and
       a simpler machine learning algorithm like a good ol' linear
       regression.

    1. [51]training your own models with deep learning
    2. [52]learning paradigms

   [53]follow @a16z on github [54]fork a16z/ai on github

references

   visible links
   1. http://aiplaybook.a16z.com/docs/intro/getting-started
   2. http://aiplaybook.a16z.com/docs/intro/survey-goals
   3. http://aiplaybook.a16z.com/docs/intro/survey-parameters
   4. http://aiplaybook.a16z.com/docs/guides/ai
   5. http://aiplaybook.a16z.com/docs/guides/ai-terms
   6. http://aiplaybook.a16z.com/docs/guides/ai-using
   7. http://aiplaybook.a16z.com/docs/guides/nlp
   8. http://aiplaybook.a16z.com/docs/guides/vision
   9. http://aiplaybook.a16z.com/docs/guides/dl
  10. http://aiplaybook.a16z.com/docs/guides/dl-architectures
  11. http://aiplaybook.a16z.com/docs/guides/dl-learning
  12. http://aiplaybook.a16z.com/docs/guides/dl-start
  13. http://aiplaybook.a16z.com/docs/guides/dl-app
  14. http://aiplaybook.a16z.com/docs/reference/software
  15. http://aiplaybook.a16z.com/docs/reference/datasets
  16. http://aiplaybook.a16z.com/docs/reference/links
  17. http://aiplaybook.a16z.com/docs/reference/bibliography
  18. https://en.wikipedia.org/wiki/action_potential
  19. http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts
  20. https://en.wikipedia.org/wiki/types_of_artificial_neural_networks
  21. http://neuralnetworksanddeeplearning.com/chap1.html
  22. http://playground.tensorflow.org/
  23. http://a16z.com/2016/06/10/ai-deep-learning-machines/
  24. http://cs231n.github.io/convolutional-networks/
  25. http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
  26. https://www.quora.com/how-are-human-visual-perception-and-deep-learning-related
  27. https://www.ncbi.nlm.nih.gov/pmc/articles/pmc1359523/pdf/jphysiol01247-0121.pdf
  28. https://en.wikipedia.org/wiki/simple_cell
  29. https://www.facebook.com/yann.lecun/posts/10152348155137143
  30. http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts
  31. http://www.image-net.org/challenges/lsvrc/
  32. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  33. https://www.quora.com/what-is-a-convolutional-neural-network
  34. https://www.quora.com/what-is-an-intuitive-explanation-of-convolutional-neural-networks
  35. http://cs231n.github.io/convolutional-networks/
  36. http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html
  37. https://arxiv.org/abs/1311.2901
  38. http://www.asimovinstitute.org/neural-network-zoo/
  39. http://colah.github.io/posts/2015-08-understanding-lstms/
  40. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  41. https://www.researchgate.net/publication/13853244_long_short-term_memory
  42. http://blog.echen.me/2017/05/30/exploring-lstms/
  43. http://colah.github.io/posts/2015-08-understanding-lstms/
  44. https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b
  45. https://www.tensorflow.org/tutorials/recurrent
  46. https://github.com/junhyukoh/caffe-lstm
  47. http://mxnet.io/api/python/model.html#model-api-reference
  48. http://www.kdnuggets.com/2016/04/top-15-frameworks-machine-learning-experts.html
  49. https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106
  50. https://arxiv.org/abs/1406.2661
  51. http://aiplaybook.a16z.com/docs/guides/dl
  52. http://aiplaybook.a16z.com/docs/guides/dl-learning
  53. https://github.com/a16z
  54. https://github.com/a16z/ai/fork

   hidden links:
  56. http://aiplaybook.a16z.com/
  57. http://aiplaybook.a16z.com/
