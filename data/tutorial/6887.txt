   #[1]nathanael aff

   [2]nathanael aff

   [3]about [4]projects

lego color themes as topic models

   [5]2017-09-11

   so i   m back to the lego dataset. in a [6]previous post, the plot of the
   relative frequency of lego colors showed that, although there is a wide
   range of colors on the whole, just a few make up the majority of brick
   colors. this situation is similar to that encountered with texts, where
   common words     articles and prepositions, for example     occur
   frequently but those words    meaning doesn   t add much to a (statistical)
   understanding of the text.

   [plot-relative-1.png]

   in this post, i use a few techniques associated with id111 to
   explore the color themes of lego sets. in particular, i   ll build a
   topic model of lego color themes using latent dirichilet
   allocation(lda). like id116 id91, the lda model requires that
   the user choose the number of topics \(k\). i try out several scoring
   methods available in r to evaluate the best number of color themes for
   the lego    corpus   .

   [lego-all-colors-1.png]

note on code and r package shout-outs

   the code for generating the analysis and plots, along with a little
   math background on the evaluation methods, is in [7]this repo and this
   [8]kaggle notebook as the same analysis but run a smaller sample of
   sets.

   one motivation for doing this analysis was to try some methods from the
   handy [9]id111 in r book by julia silge and david robinson and
   some of my code follows their examples. in particular, in the tf-idf
   section and the analysis of the distribution of documents over the
   topics.

   the lda model is trained using the lda function from the topicmodels
   package. evaluation methods come from ldatuning, speedreader, and the
   clues packages. unit plots use the waffle package.

color tf-idf

   the lego dataset contains around 11,000 lego sets from 1950 to 2017 and
   the data includes the part numbers and brick colors that make up each
   set. following the id111 analogy, i take lego sets to be the
      documents    that make up the lego    corpus   , and colors are the    words   
   that make up each document or set. i ignored part numbers but it would
   be possible to incorporate them in a similar analysis by considering
   the color-part number pair as the unit of meaning.

   in id111, stop words are words that occur with similar frequency
   in all documents. these words are often removed before performing a
   statistical analysis. from the frequency plot, it   s clear a few primary
   colors along with black, gray, and white make up the majority of brick
   colors. these might have been treated as stop words and removed. our
   corups has a vocabulary of just 125 unique colors so i chose to leave
   all of them in for the analysis.

   term frequency inverse document freqeuncy(tf-idf) is a metric of a
   word   s importance to a specific document. term frequency is the
   frequency of a word   s appearance in each document and inverse document
   frequency is the count of documents the word appears in. weighting tf
   by idf means words appearing in all documents are down-weighted
   compared to rarely occuring words. a high tf-idf corresponds to a color
   that is distinct to a lego set.

low tf-idf colors

   first, we   ll look at low tf-idf colors. many of the colors-set pairs
   with a low tf-idf score show up as common colors in relative frequency
   plot above.
   [low-tf-idf-plot-1.png]

low tf-idf sets

   the three sets below are the 7-10th lowest tf-idf set-color
   combinations. these are large sets with with a common color that
   appears less frequently in the set than in the corpus. for example, the
   darker gray in the    first lego league    set makes up a small proportion
   of the set but occurs in many sets.
   [low-tf-idf-plots1-1.png]

high tf-idf colors

   the plot below shows the 10 set-color pairs with the highest tf-idf
   score. these are sets with a high proportion of the set made up of a
   color that shows up infrequently in lego sets overall. the    statue of
   liberty    set is an extreme example; it   s made up almost entirely of a
   single sea-green color that doesn   t occur in other sets.
   [top-tf-idf-plot-1.png]

high tf-idf sets

   [top-tf-idf-sets-1.png]

building a topic model

   after that somewhat cursory look at the lego sets, we   ll move on to
   building a topic model. the lda model is a generative model of a body
   of documents (or lego sets or genetic markers or images). the output of
   the lda algorithm is two distributions which represent the distribution
   of terms that make up a topic and the distribution of topics that make
   up a document. for our model, the term distribution is a distribution
   over colors that make up a color theme, while the topic distribution is
   a distribution of color themes that make up a lego set.

   in this generative model, the lego set can be generated by one theme or
   many themes. tuning the number of topics in the model changes both the
   number of themes a set is drawn from and the colors that make up that
   theme.

evaluation methods

   i used several methods (chosen because they were readily available in r
   packages) for evaluating the number of topics that make up the topic
   model. this is not meant to be an exhaustive list of automated topic
   evaluation methods. for gauging topic coherence, for example, the
   python gensim library has a more complete pipeline which includes
   options to modify segmentation, word co-occurence estimation, and
   scoring methods.

cross validation on perplexity

   perplexity measures the difference between distributions learned on a
   training and holdout set and the topicmodels package used to build the
   lda model has a function for computing perplexity. this was the only
   evaluation methods which required cross-validation.

topic grid

   i ended up running the cross-validation twice and refined the spacing
   on the parameter grid to capture both the larger trend and some detail
   where i thought better parameters \(k\) might be located. there appears
   to be diminishing returns to model complexity between \(k = 20\) and
   \(k = 35\).
   [cv-result-plot-1.png]

measures on the full models

   after running cross-validation on the perplexity scores i reduced the
   number of models for the remaining evaluation methods. the remaining
   methods used models trained on the full data set.

ldatuning

   the ldatuning package has several other metrics of the quality of the
   topic models. the skimmed the references in the package documentation
   but i don   t really understand these measures. at least two of the
   measures agree that fewer topics are better.
   [ldatuning-scores-1.png]

topic coherence

   there are several versions of topic coherence which measure the
   pairwise strength of the relationship of the top terms in a topic
   model. given some score, where a larger value indicates a stronger
   relationship between two words \(w_i, w_j\), a generic coherence score
   is the sum of the top terms in a topic model:

   \[ \sum_{w_i, w_j \in w_t} \text{score}(w_i, w_j), \] with top terms
   \(w_t\) for each topic \(t\).

   the coherence score used in the speedreader topic_coherence function
   uses the internal coherence(umass) of the top terms. i compared the
   scores for the top 3, 5 and 10 terms.
   [coherence-score-1.png]

external validation with cluster scoring

   we can also treat the lda models as a id91 of the lego sets by
   assign each lego set to the topic which makes up the highest proportion
   of that set, that is, the highest id203 of a topic \(t\) given a
   document \(d\) \[ \text{gamma} \equiv \gamma = p(t|d). \] we then
   assign a document to a cluster by taking \[ \text{argmax}_t p(t|d). \]

   for comparison   s sake, i also ran a kmeans id91 using the
   weighted term(color) distribution for each document as the vector that
   representing that set.

   the kmeans and lda id91s were evaluated against each sets
   parent_id label which indicated the theme of the lego set. in total
   there were around 100 unique parent themes although this included sets
   who were    childless parents   .

cluster scores

   the clusters scores include rand, adjusted rand, folkes-mallow and
   jaccard scores. all try to score a id91 on how well the
   discovered labels match the assigned parent_id. the rand index assigns
   a score based on the number pairwise agreement of the cluster labels
   with the original labels. the other scores use a similar approach and
   two are versions of the rand score but adjusted for random agreement.
   all scores except the un-adjusted rand index decrease with more topics.

   there   s no reason to assume that theme labels match color topics.
   poking around the data indicated that some themes ids are closely
   associated with a palette (duplo, for example) while other parent
   themes are general categories with a mix of color theme.
   [plot-external-scores-1.png]

topic distribution

   the last method for evaluating the learned topics is to look at how
   documents are distributed over the topic distributions. this example
   follows a [10]this section from the tidyid111 book.

   the plot below visualizes this as how the documents are distributed
   over the id203 bins for each topic. if too many topics have sets
   or documents in the low id203 bins then you may have too many
   topics, since few documents are strongly associated with any topic.
   [plot-40-topics-1.png]

   the chart above is also closely related to id91 based on lda. if
   the distribution over the high id203 bins of a topic is sparse
   then few lego sets would be assigned to that topic. (you can compare
   the plot above to the to the total distribution of lego sets over these
   topics below. topic 40 had the fewest sets assigned to it.)

evaluation results

   none of the preceding evaluation methods seem particularly conclusive.
   the pattern of diminishing returns on the perplexity scores is similar
   to other model complexity scores and suggests a value for \(k\) in the
   25-35 range. this agrees somewhat with a visual evaluation of the set
   distribution over topic probabilities (the last chart above), where at
   40 topics some topics seem to have few documents closely associated
   with them.

color distributions over topics

   aside from these scoring methods, we can also plot the color
   distribution(or relevance) for each topic or color theme directly.
   below are charts for the models with 30 and 40 topics.

   [color-distribution-1.png]
   [color-distribution-2.png]

   the above plot is based on the beta \(\beta\) matrix, which gives the
   posterior distribution of words given a topic, \(p(w|t)\). the above
   plot shows a weighted \(\beta\) (or relevance) like that used in the
   [11]ldavis package.

   \[ \text{relevance}(w|t) = \lambda \cdot p(w|t) + (1-\lambda)\cdot
   \frac{p(w|t)}{p(w)}.\]

how many themes?

   although there may be more topics that have few sets associated with
   them as we increase the number of topics, the coherence of a few topics
   appears to improve.

   for example, the two plots below are selections from sets with the
   highest gamma, \(p(d|t)\) for each topic. when we go from 30 to 40
   topics the top sets in the topic are removed by the remaining sets are
   more visually similar. (also note that the sets that stayed had the top
   relevance score or weighted \(\beta\)).

topic 2 from the 30 topic model

   [topic-waffle-2-2-1.png]

topic 2 from 40 topic model

   [topic-waffle-3-2-1.png]

   i   ll plot one more topic from the 40 topic model that looked
      suspicious    but a sampling of the top sets seem to go well together.

topic 32 from 40 topic model

   [topic-waffle-3-32-1.png]

evaluation summary

   of the automated scoring methods, the perplexity scores and the
   distribution of topics over term probabilities were the only two that
   seemed readily interpretable and matched my personal sense of which
   model best identified coherent color themes. i ran the same scores on a
   small samples of the data for this [12]kaggle notebook and the
   coherence score consistently pointed at models with fewer topics. it
   might be interesting to see how evaluation methods vary with parameters
   of the dataset like vocabulary size, variation in document size and the
   number of documents.

lego color themes

   for the final model, i used the 40 topic model. although lda models the
   lego sets as mixtures of topics or themes, for the next two charts i
   assigned each set to a single topic using the same method that i used
   for id91. and topics are a mixture of colors but i chose a color
   to represent each topic by blending the topic   s two most important
   color terms.

   in the last plot, the topics are represented by color probabilities of
   that topic: 1 brick represents roughly 1% of the distribution.
   [plot-topic-distribution-1.png]

   [plot-topic-timeline-1.png]

   [lego-color-themes.png]

     * [13]topic models
     * [14]eda

   mapping san francisco's open data with leaflet >

   please enable javascript to view the [15]comments powered by disqus.
   [16]comments powered by disqus    2017 nathanael aff  powered by
   [17]hugo with theme [18]minos

references

   visible links
   1. https://nateaff.com/2017/09/11/lego-topic-models/
   2. https://nateaff.com/
   3. https://nateaff.com/about
   4. https://nateaff.com/projects
   5. https://nateaff.com/2017/09/11/lego-topic-models/
   6. https://nateaff.com/2017/08/16/exploring-lego-dataset-with-sql-part-ii/
   7. https://github.com/nateaff/legolda
   8. https://www.kaggle.com/nateaff/finding-lego-color-themes-with-topic-models
   9. http://tidytextmining.com/
  10. http://tidytextmining.com/nasa.html#calculating-tf-idf-for-the-description-fields
  11. http://www.kennyshirley.com/ldavis/#topic=0&lambda=0.61&term=
  12. https://www.kaggle.com/nateaff/finding-lego-color-themes-with-topic-models
  13. https://nateaff.com//tags/topic-models
  14. https://nateaff.com//tags/eda
  15. http://disqus.com/?ref_noscript
  16. http://disqus.com/
  17. https://gohugo.io/
  18. https://github.com/carsonip/hugo-theme-minos

   hidden links:
  20. javascript:;
  21. https://nateaff.com/2017/08/19/sf-311-data-leaflet/
  22. https://github.com/nateaff
  23. https://twitter.com/nateaff
  24. https://www.linkedin.com/in/nathanaelaff/
