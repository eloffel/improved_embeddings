   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    fundamentals of deep learning     activation
   functions and when to use them? comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]fundamentals of deep learning    
   id180 and when to use them?

   [95]deep learning

fundamentals of deep learning     id180 and when to use them?

   [96]dishashree gupta, october 23, 2017

introduction

   internet provides access to plethora of information today. whatever we
   need is just a google (search) away. however, when we have so much of
   information, the challenge is to segregate between relevant and
   irrelevant information.

   when our brain is fed with a lot of information simultaneously, it
   tries hard to understand and classify the information between useful
   and not-so-useful information. we need a similar mechanism to classify
   incoming information as useful or less-useful in case of neural
   networks.

   this is a very important in the way a network learns because not all
   information is equally useful. some of it is just noise. well,
   id180 help the network do this segregation. they help
   the network use the useful information and suppress the irrelevant data
   points.

   let us go through these id180, how they work and figure
   out which id180 fits well into what kind of  problem
   statement.


table of contents

    1. brief overview of neural networks
    2. what is an activation function ?
    3. can we do without an activation function ?
    4. popular types of id180 and when to use them
         1. identity
         2. binary step
         3. sigmoid
         4. tanh
         5. relu
         6. leaky relu
         7. softmax
    5. choosing the right activation function


brief overview of neural networks

   before i delve into the details of id180, let   s do a
   little review of what are neural networks and how they function. a
   neural network is a very powerful machine learning mechanism which
   basically mimics how a human brain learns. the brain receives the
   stimulus from the outside world, does the processing on the input, and
   then generates the output.

   as the task gets complicated multiple neurons form a complex network,
   passing information among themselves.

   using a id158, we try to mimic a similar behavior.
   the network you see below is a neural network made of interconnected
   neurons.

   the black circles in the picture above are neurons. each neuron is
   characterized by its weight, bias and activation function. the input is
   fed to the input layer. the neurons do a linear transformation on the
   input by the weights and biases. the non linear transformation is done
   by the activation function. the information moves from the input layer
   to the hidden layers. the hidden layers would do the processing and
   send the final output to the output layer. this is the forward movement
   of information known as the forward propagation. but what if the output
   generated is far away from the expected value? in a neural network, we
   would update the weights and biases of the neurons on the basis of the
   error. this process is known as back-propagation. once the entire data
   has gone through this process, the final weights and biases are used
   for predictions.


what is an activation function?

   id180 are an extremely important feature of the
   id158s. they basically decide whether a neuron
   should be activated or not. whether the information that the neuron is
   receiving is relevant for the given information or should it be
   ignored.

   the activation function is the non linear transformation that we do
   over the input signal. this transformed output is then sen to the next
   layer of neurons as input.


can we do without an activation function?

   now the question which arises is that if the activation function
   increases the complexity so much, can we do without an activation
   function?

   when we do not have the activation function the weights and bias would
   simply do a linear transformation. a linear equation is simple to solve
   but is limited in its capacity to solve complex problems. a neural
   network without an activation function is essentially just a linear
   regression model. the activation function does the non-linear
   transformation to the input making it capable to learn and perform more
   complex tasks. we would want our neural networks to work on complicated
   tasks like language translations and image classifications. linear
   transformations would never be able to perform such tasks.

   id180 make the back-propagation possible since the
   gradients are supplied along with the error to update the weights and
   biases. without the differentiable non linear function, this would not
   be possible.


popular types of id180 and when to use them

binary step function

   the first thing that comes to our mind when we have an activation
   function would be a threshold based classifier i.e. whether or not the
   neuron should be activated. if the value y is above a given threshold
   value then activate the neuron else leave it deactivated.

   it is defined as    
f(x) = 1, x>=0

   = 0, x<0

   the binary function is extremely simple. it can be used while creating
   a binary classifier. when we simply need to say yes or no for a single
   class, step function would be the best choice, as it would either
   activate the neuron or leave it to zero.

   the function is more theoretical than practical since in most cases we
   would be classifying the data into multiple classes than just a single
   class. the step function would not be able to do that.

   moreover, the gradient of the step function is zero. this makes the
   step function not so useful since during back-propagation when the
   gradients of the id180 are sent for error calculations
   to improve and optimize the results. the gradient of the step function
   reduces it all to zero and improvement of the models doesn   t really
   happen.
f '(x) = 0, for all x


linear function

   we saw the problem with the step function, the gradient being zero, it
   was impossible to update gradient during the id26. instead
   of a simple step function, we can try using a linear function. we can
   define the function as-

   f(x)=ax

   we have taken a as 4 in the figure above. here the activation is
   proportional to the input. the input x, will be transformed to ax. this
   can be applied to various neurons and multiple neurons can be activated
   at the same time. now, when we have multiple classes, we can choose the
   one which has the maximum value. but we still have an issue here. let   s
   look at the derivative of this function.
f'(x) = a

   the derivative of a linear function is constant i.e. it does not depend
   upon the input value x.

   this means that every time we do a back propagation, the gradient would
   be the same. and this is a big problem, we are not really improving the
   error since the gradient is pretty much the same. and not just that
   suppose we are trying to perform a complicated task for which we need
   multiple layers in our network. now if each layer has a linear
   transformation, no matter how many layers we have the final output is
   nothing but a linear transformation of the input. hence, linear
   function might be ideal for simple tasks where interpretability is
   highly desired.


sigmoid

   sigmoid is a widely used activation function. it is of the form-
f(x)=1/(1+e^-x)

   let   s plot this function and take a look of it.

   this is a smooth function and is continuously differentiable. the
   biggest advantage that it has over step and linear function is that it
   is non-linear. this is an incredibly cool feature of the sigmoid
   function. this essentially means that when i have multiple neurons
   having sigmoid function as their activation function     the output is
   non linear as well. the function ranges from 0-1 having an s shape.
   let   s take a look at the shape of the curve. the gradient is very high
   between the values of -3 and 3 but gets much flatter in other regions.
   how is this of any use?

   this means that in this range small changes in x would also bring about
   large changes in the value of y. so the function essentially tries to
   push the y values towards the extremes. this is a very desirable
   quality when we   re trying to classify the values to a particular class.

   let   s take a look at the gradient of the sigmoid function as well.

   it   s smooth and is dependent on x. this means that during
   id26 we can easily use this function. the error can be
   backpropagated and the weights can be accordingly updated.

   sigmoids are widely used even today but we still have a problems that
   we need to address. as we saw previously     the function is pretty flat
   beyond the +3 and -3 region. this means that once the function falls in
   that region the gradients become very small. this means that the
   gradient is approaching to zero and the network is not really learning.

   another problem that the sigmoid function suffers is that the values
   only range from 0 to 1. this means that the sigmoid function is not
   symmetric around the origin and the values received are all positive.
   so not all times would we desire the values going to the next neuron to
   be all of the same sign. this can be addressed by scaling the sigmoid
   function. that   s exactly what happens in the tanh function. let   s read
   on.


tanh

   the tanh function is very similar to the sigmoid function. it is
   actually just a scaled version of the sigmoid function.
tanh(x)=2sigmoid(2x)-1

   it can be directly written as    
tanh(x)=2/(1+e^(-2x)) -1

   tanh works similar to the sigmoid function but is symmetric over the
   origin. it ranges from -1 to 1.

   it basically solves our problem of the values all being of the same
   sign. all other properties are the same as that of the sigmoid
   function. it is continuous and differentiable at all points. the
   function as you can see is non linear so we can easily backpropagate
   the errors.

   let   s have a look at the gradient of the tan h function.

   the gradient of the tanh function is steeper as compared to the sigmoid
   function. our choice of using sigmoid or tanh would basically depend on
   the requirement of gradient in the problem statement. but similar to
   the sigmoid function we still have the vanishing gradient problem. the
   graph of the tanh function is flat and the gradients are very low.


relu

   the relu function is the rectified linear unit. it is the most widely
   used activation function. it is defined as-
f(x)=max(0,x)

   it can be graphically represented as-

   relu is the most widely used activation function while designing
   networks today. first things first, the relu function is non linear,
   which means we can easily backpropagate the errors and have multiple
   layers of neurons being activated by the relu function.

   the main advantage of using the relu function over other activation
   functions is that it does not activate all the neurons at the same
   time. what does this mean ? if you look at the relu function if the
   input is negative it will convert it to zero and the neuron does not
   get activated. this means that at a time only a few neurons are
   activated making the network sparse making it efficient and easy for
   computation.

   let   s look at the gradient of the relu function.

   but relu also falls a prey to the gradients moving towards zero. if you
   look at the negative side of the graph, the gradient is zero, which
   means for activations in that region, the gradient is zero and the
   weights are not updated during back propagation. this can create dead
   neurons which never get activated. when we have a problem, we can
   always engineer a solution.


leaky relu

   leaky relu function is nothing but an improved version of the relu
   function. as we saw that for the relu function, the gradient is 0 for
   x<0, which made the neurons die for activations in that region. leaky
   relu is defined to address this problem. instead of defining the relu
   function as 0 for x less than 0, we define it as a small linear
   component of x. it can be defined as-
f(x)= ax, x<0
= x, x>=0

   what we have done here is that we have simply replaced the horizontal
   line with a non-zero, non-horizontal line. here a is a small value like
   0.01 or so. it can be represented on the graph as-

   the main advantage of replacing the horizontal line is to remove the
   zero gradient. so in this case the gradient of the left side of the
   graph is non zero and so we would no longer encounter dead neurons in
   that region. the gradient of the graph would look like    

   similar to the leaky relu function, we also have the parameterised relu
   function. it is defined similar to the leaky relu as    
f(x)= ax, x<0
= x, x>=0

   however, in case of a parameterised relu function,    a    is also a
   trainable parameter. the network also learns the value of    a    for
   faster and more optimum convergence. the parametrised relu function is
   used when the leaky relu function still fails to solve the problem of
   dead neurons and the relevant information is not successfully passed to
   the next layer.


softmax

   the softmax function is also a type of sigmoid function but is handy
   when we are trying to handle classification problems. the sigmoid
   function as we saw earlier was able to handle just two classes. what
   shall we do when we are trying to handle multiple classes. just
   classifying yes or no for a single class would not help then. the
   softmax function would squeeze the outputs for each class between 0 and
   1 and would also divide by the sum of the outputs. this essentially
   gives the id203 of the input being in a particular class. it can
   be defined as    

   let   s say for example we have the outputs as-
   [1.2 , 0.9 , 0.75], when we apply the softmax function we would get
   [0.42 ,  0.31, 0.27]. so now we can use these as probabilities for the
   value to be in each class.

   the softmax function is ideally used in the output layer of the
   classifier where we are actually trying to attain the probabilities to
   define the class of each input.


choosing the right activation function

   now that we have seen so many activation  functions, we need some logic
   / heuristics to know which activation function should be used in which
   situation. good or bad     there is no rule of thumb.

   however depending upon the properties of the problem we might be able
   to make a better choice for easy and quicker convergence of the
   network.
     * sigmoid functions and their combinations generally work better in
       the case of classifiers
     * sigmoids and tanh functions are sometimes avoided due to the
       vanishing gradient problem
     * relu function is a general activation function and is used in most
       cases these days
     * if we encounter a case of dead neurons in our networks the leaky
       relu function is the best choice
     * always keep in mind that relu function should only be used in the
       hidden layers
     * as a rule of thumb, you can begin with using relu function and then
       move over to other id180 in case relu doesn   t
       provide with optimum results


projects

   now, its time to take the plunge and actually play with some other real
   datasets. so are you ready to take on the challenge? accelerate your
   deep learning journey with the following practice problems:
   [97]practice problem: identify the apparels identify the type of
   apparel for given images
   [98]practice problem: identify the digits identify the digit in given
   images

end notes

   in this article i have discussed the various types of activation
   functions and what are the types of problems one might encounter while
   using each of them.

   i would suggest to begin with a relu function and explore other
   functions as you move further. you can also design your own activation
   functions giving a non-linearity component to your network. if you have
   used your own activation function which worked really well, please
   share it with us and we shall be happy to incorporate it into the list.

[99]learn, [100]engage, [101]compete, and [102]get hired!

   you can also read this article on analytics vidhya's android app
   [103]get it on google play

share this:

     * [104]click to share on linkedin (opens in new window)
     * [105]click to share on facebook (opens in new window)
     * [106]click to share on twitter (opens in new window)
     * [107]click to share on pocket (opens in new window)
     * [108]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [109]activation function, [110]relu, [111]sigmoid, [112]softmax
   next article

the essential nlp guide for data scientists (with codes for top 10 common nlp
tasks)

   previous article

the art of story telling in data science and how to create data stories?

[113]dishashree gupta

   dishashree is passionate about statistics and is a machine learning
   enthusiast. she has an experience of 1.5 years of market research using
   r, advanced excel, azure ml.

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [114]discussion portal to get your queries resolved

20 comments

     * p.rajendra says:
       [115]october 23, 2017 at 12:10 pm
       nice article.. but there is no positive values on y-axis of all of
       your graphs.
       [116]reply
     * p.rajendra says:
       [117]october 23, 2017 at 12:13 pm
       i am sorry    i am mistaken positive values are there on y axis.
       thank you for giving very nice article on activation function
       [118]reply
          + dishashree gupta says:
            [119]october 23, 2017 at 12:15 pm
            thanks. hope you enjoyed reading !
            [120]reply
     * manish says:
       [121]october 23, 2017 at 12:17 pm
       have read only first few paras i.e. till that brain functionality   
       and found the artical wonderful. appreciate such a deep thinking
       for deep learning   . machine learning   . which catch and leads the
       reader also deep in the subject.
       salute and thanks for the article.
       [122]reply
     * parvez ahmad says:
       [123]october 23, 2017 at 12:49 pm
       thanks for sharing such a useful information
       [124]reply
     * larweh mahu says:
       [125]october 23, 2017 at 9:10 pm
       this is my first time reading about id180. thank you
       for sharing this article written with such clarity.
       [126]reply
     * [127]geoffrey lee says:
       [128]october 23, 2017 at 9:43 pm
       you left out an important property of relu     it   s extremely fast to
       calculate max() versus all the complicated math in a sigmoid or
       tanh. that saves you a lot of time and allows you to run more
       iterations.
       [129]reply
     * aditya sharma says:
       [130]october 23, 2017 at 11:09 pm
       good one to read!!!
       relu is not always a good choice.
       but yah you can try, it gives advantage over training deep networks
       and over shallow ones.
       there is a good activation function developed by goggle brain team
       named    swish    function
       performs very good, used in my quality construction of image
       project.
       f(x) = x*sigmoid(x)
       [131]reply
     * govinda raju says:
       [132]october 23, 2017 at 11:25 pm
       good one.
       [133]reply
     * norman_h says:
       [134]october 24, 2017 at 12:52 am
       then there is selu.
       [135]reply
     * [136]jeff weakley says:
       [137]october 24, 2017 at 12:52 am
       great article. i searched the web for info like this a few weeks
       back and wasn   t able to find much. thanks.
       [138]reply
     * jeswin says:
       [139]october 24, 2017 at 8:08 am
       great work !
       i loved the approach to explain from base.
       keep going ?
       expecting more articles to read ?
       [140]reply
     * avinash says:
       [141]october 24, 2017 at 9:45 am
       thanks for the very informative article.
       [142]reply
     * avinash says:
       [143]october 24, 2017 at 9:46 am
       thanks for the informative article.
       [144]reply
     * samiulla says:
       [145]october 24, 2017 at 5:20 pm
       in real time, noisy relu proves to be helpful as it profiles even
       the noise factor.
       [146]reply
     * mn amin says:
       [147]october 25, 2017 at 12:54 am
       it   s outstanding article.now i am crystal clear for all terms and
       words. eagerly waiting for next article
       [148]reply
     * bala chuppala says:
       [149]october 26, 2017 at 10:22 pm
       thanks for the nice explanation.
       [150]reply
     * sachin bansal says:
       [151]november 13, 2017 at 12:01 am
       classic article on activation function.
       [152]reply
     * [153][email protected] says:
       [154]november 13, 2017 at 4:39 am
       very good article and nice information.
       [155]reply
     * advait koparkar says:
       [156]december 13, 2017 at 2:24 pm
       good read
       [157]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [158]srk       3924
   2    [2.jpg?date=2019-04-06] [159]mark12    3510
   3    [3.jpg?date=2019-04-06] [160]nilabha   3261
   4    [4.jpg?date=2019-04-06] [161]nitish007 3237
   5    [5.jpg?date=2019-04-06] [162]tezdhar   3082
   [163]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [164]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [165]understanding support vector machine algorithm from examples
       (along with code)
     * [166]essentials of machine learning algorithms (with python and r
       codes)
     * [167]a complete tutorial to learn data science with python from
       scratch
     * [168]7 types of regression techniques you should know!
     * [169]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [170]a simple introduction to anova (with applications in excel)
     * [171]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [172]top 5 machine learning github repositories and reddit discussions
   from march 2019

[173]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [174]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[175]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [176]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[177]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [178]16 opencv functions to start your id161 journey (with
   python code)

[179]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [180][ds-finhack.jpg]

   [181][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [182]about us
     * [183]our team
     * [184]career
     * [185]contact us
     * [186]write for us

   [187]about us
   [188]   
   [189]our team
   [190]   
   [191]careers
   [192]   
   [193]contact us

data scientists

     * [194]blog
     * [195]hackathon
     * [196]discussions
     * [197]apply jobs
     * [198]leaderboard

companies

     * [199]post jobs
     * [200]trainings
     * [201]hiring hackathons
     * [202]advertising
     * [203]reach us

   don't have an account? [204]sign up here.

join our community :

   [205]46336 [206]followers
   [207]20222 [208]followers
   [209]followers
   [210]7513 [211]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [212]privacy policy
     * [213]terms of use
     * [214]refund policy

   don't have an account? [215]sign up here

   iframe: [216]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [217](button) join now

   subscribe!

   iframe: [218]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [219](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/author/dishashree26/
  97. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/?utm_source=fundamentals-deep-learning-activation-functions-when-to-use-them&utm_medium=blog
  98. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/?utm_source=fundamentals-deep-learning-activation-functions-when-to-use-them&utm_medium=blog
  99. https://www.analyticsvidhya.com/blog
 100. https://discuss.analyticsvidhya.com/
 101. https://datahack.analyticsvidhya.com/
 102. https://www.analyticsvidhya.com/jobs/#/user/
 103. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 104. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/?share=linkedin
 105. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/?share=facebook
 106. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/?share=twitter
 107. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/?share=pocket
 108. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/?share=reddit
 109. https://www.analyticsvidhya.com/blog/tag/activation-function/
 110. https://www.analyticsvidhya.com/blog/tag/relu/
 111. https://www.analyticsvidhya.com/blog/tag/sigmoid/
 112. https://www.analyticsvidhya.com/blog/tag/softmax/
 113. https://www.analyticsvidhya.com/blog/author/dishashree26/
 114. https://discuss.analyticsvidhya.com/
 115. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140508
 116. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140508
 117. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140509
 118. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140509
 119. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140510
 120. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140510
 121. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140511
 122. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140511
 123. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140514
 124. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140514
 125. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140580
 126. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140580
 127. https://bionicdreamer.com/
 128. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140583
 129. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140583
 130. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140588
 131. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140588
 132. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140589
 133. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140589
 134. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140601
 135. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140601
 136. http://creativealgorithm.org/
 137. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140602
 138. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140602
 139. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140628
 140. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140628
 141. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140632
 142. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140632
 143. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140633
 144. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140633
 145. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140661
 146. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140661
 147. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140695
 148. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140695
 149. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140965
 150. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-140965
 151. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-143453
 152. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-143453
 153. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection
 154. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-143478
 155. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-143478
 156. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-147430
 157. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/#comment-147430
 158. https://datahack.analyticsvidhya.com/user/profile/srk
 159. https://datahack.analyticsvidhya.com/user/profile/mark12
 160. https://datahack.analyticsvidhya.com/user/profile/nilabha
 161. https://datahack.analyticsvidhya.com/user/profile/nitish007
 162. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 163. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 164. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 165. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 166. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 167. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 168. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 169. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 170. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 171. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 172. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 173. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 174. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 175. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 176. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 177. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 178. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 179. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 180. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 181. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 182. http://www.analyticsvidhya.com/about-me/
 183. https://www.analyticsvidhya.com/about-me/team/
 184. https://www.analyticsvidhya.com/career-analytics-vidhya/
 185. https://www.analyticsvidhya.com/contact/
 186. https://www.analyticsvidhya.com/about-me/write/
 187. http://www.analyticsvidhya.com/about-me/
 188. https://www.analyticsvidhya.com/about-me/team/
 189. https://www.analyticsvidhya.com/about-me/team/
 190. https://www.analyticsvidhya.com/about-me/team/
 191. https://www.analyticsvidhya.com/career-analytics-vidhya/
 192. https://www.analyticsvidhya.com/about-me/team/
 193. https://www.analyticsvidhya.com/contact/
 194. https://www.analyticsvidhya.com/blog
 195. https://datahack.analyticsvidhya.com/
 196. https://discuss.analyticsvidhya.com/
 197. https://www.analyticsvidhya.com/jobs/
 198. https://datahack.analyticsvidhya.com/users/
 199. https://www.analyticsvidhya.com/corporate/
 200. https://trainings.analyticsvidhya.com/
 201. https://datahack.analyticsvidhya.com/
 202. https://www.analyticsvidhya.com/contact/
 203. https://www.analyticsvidhya.com/contact/
 204. https://datahack.analyticsvidhya.com/signup/
 205. https://www.facebook.com/analyticsvidhya/
 206. https://www.facebook.com/analyticsvidhya/
 207. https://twitter.com/analyticsvidhya
 208. https://twitter.com/analyticsvidhya
 209. https://plus.google.com/+analyticsvidhya
 210. https://in.linkedin.com/company/analytics-vidhya
 211. https://in.linkedin.com/company/analytics-vidhya
 212. https://www.analyticsvidhya.com/privacy-policy/
 213. https://www.analyticsvidhya.com/terms/
 214. https://www.analyticsvidhya.com/refund-policy/
 215. https://id.analyticsvidhya.com/accounts/signup/
 216. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 217. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 218. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 219. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 221. https://www.facebook.com/analyticsvidhya
 222. https://twitter.com/analyticsvidhya
 223. https://plus.google.com/+analyticsvidhya/posts
 224. https://in.linkedin.com/company/analytics-vidhya
 225. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/?utm_source=fundamentals-deep-learning-activation-functions-when-to-use-them&utm_medium=blog
 226. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/?utm_source=fundamentals-deep-learning-activation-functions-when-to-use-them&utm_medium=blog
 227. https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/
 228. https://www.analyticsvidhya.com/blog/2017/10/art-story-telling-data-science/
 229. https://www.analyticsvidhya.com/blog/author/dishashree26/
 230. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 231. https://www.facebook.com/analyticsvidhya/
 232. https://twitter.com/analyticsvidhya
 233. https://plus.google.com/+analyticsvidhya
 234. https://plus.google.com/+analyticsvidhya
 235. https://in.linkedin.com/company/analytics-vidhya
 236. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 237. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 238. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 239. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 240. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 241. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 242. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 243. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 244. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 245. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 246. javascript:void(0);
 247. javascript:void(0);
 248. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 249. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 250. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 251. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 252. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 253. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 254. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 255. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 256. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 257. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f10%2ffundamentals-deep-learning-activation-functions-when-to-use-them%2f&linkname=fundamentals%20of%20deep%20learning%20-%20activation%20functions%20and%20their%20use
 258. javascript:void(0);
 259. javascript:void(0);
