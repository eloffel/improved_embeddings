   [1][pomdp.org.32x32.png] pomdp.org
     * home
     * tutorial
     * [2]code
          + pomdp-solve
          + command line options
          + pomdp file format
          + value function format
          + policy graph format
          + copyright and disclaimer
     * examples
     * papers
     * talks
     * faq

brief introduction to mdps

   pomdps for dummies

   when you are confronted with a decision, there are a number of
   different alternatives (actions) you have to choose from. choosing the
   best action requires thinking about more than just the immediate
   effects of your actions. the immediate effects are often easy to see,
   but the long term effects are not always as transparent. sometimes
   actions with poor immediate effects, can have better long term
   ramifications. you would like to choose the action that makes the right
   tradeoffs between the immediate rewards and the future gains, to yield
   the best possible solution. people do this type of reasoning daily:
   some do it better than others.

   what usually makes this particularly difficult is that there is a lot
   of uncertainty about the future. the outcomes of certain actions in the
   future are not entirely predicable, and one sometimes doesn't even know
   if the action will even matter in the future. a markov decision process
   is a way to model problems so that we can automate this process of
   decision making in uncertain environments.

   if you can model the problem as an mdp, then there are a number of
   algorithms that will allow you to automatically solve the decision
   problem. we will first talk about the components of the model that are
   required. we follow this by defining what it means to solve the
   problems and finally, we describe a simple algorithm for finding a
   solution to the problem given a model of this type.

   the four components of an mdp model are: a set of states, a set of
   actions, the effects of the actions and the immediate value of the
   actions. we will assume that the set of state and actions is discrete,
   and allow assume that time passes in uniform, discrete intervals. we
   now discuss the model components in more detail.

states

   when making a decision, you need to think about how your actions will
   affect things. the state is the way the world currently exists and an
   action will have the effect of changing the state of the world. if we
   think about the set of every possible way the world could be, then this
   is would be the set of state of the world. each of these states would
   be a state in the mdp.

actions

   the actions are the set of possible alternative choices you can choose
   to make. the main problem of solving an mdp is to find the best action
   to take in each particular state of the world.

transitions

   when we are deciding between different actions, we have some idea of
   how they will immediately affect the current state. the transitions
   specify how each of the actions change the state. since an action could
   have different effects, depending upon the state, we need to specify
   the action's effect for each state in the mdp

   the most powerful aspect of the mdp is that the effects of an action
   can be probabilistic. imagine we are specifying the effects of doing
   action 'a1' in state 's1'. we could say that the effect of 'a1' is to
   leave the process in state 's2', if there was no question about how
   'a1' changes the world. however, many decision processes have actions
   that are not this simple. sometimes an action usually results in state
   's2', but occasionally it might result in state 's3'. mdps allow you to
   specify these more complex actions by allowing you to specify a set of
   resulting states and the id203 that each state results.

immediate rewards

   if we want to automate the decision making process, then we must be
   able to have some measure of an action's cost or state's value so that
   we can compare different alternative action policies over the long
   term. we specify some immediate value for performing each action in
   each state.

the solution to an mdp

   the solution to an mdp is called a policy and it simply specifies the
   best action to take for each of the states. for the purposes of this
   tutorial, we will only concern ourselves with the problem of finding
   the best policy assuming we will have a limited lifetime. for instance,
   assume that each day that we wake up we must make a decision about
   something. we want to make our decision assuming that we will only have
   to make decisions for a fixed number of days. this type of solution is
   called a finite horizon solution and the number of days we will be
   making decisions for will be referred to as the horizon length.

   although the policy is what we are after, we will actually compute a
   value function. without going into the details, suffice it to say that
   we can easily derive the policy if we have the value function. thus, we
   will focus on finding this value function. a value function is similar
   to a policy, except that instead of specifying an action for each
   state, it specifies a numerical value for each state.

who's this markov guy?

   who this guy is isn't important now, but here we try to explain what
   his name means as far as mdps are concerned.

   when we talked about the transitions of the model, we said that we
   simply needed to specify the resulting next state for each starting
   state and action. this assumes that the next state depends only upon
   the current state (and action). there are situations where the effects
   of an action might depend not only on the current state, but upon the
   last few states. the mdp model will not let you model these situations
   directly. the assumption made by the mdp model is that the next state
   is solely determined by the current state (and current action). this is
   called the markov assumption.

   we would say that the dynamics of the process are markovian and this
   has important ramifications for solving the problems. the most
   important is that our policies or value functions can have a simple
   form. the result is that we can choose our action based solely upon the
   current state.

                                 [3]continue

                         [4]pomdp tutorial | [5]next

      2003-2019, anthony r. cassandra

references

   visible links
   1. http://www.pomdp.org/
   2. http://www.pomdp.org/tutorial/mdp.html
   3. http://www.pomdp.org/tutorial/mdp-vi.html
   4. http://www.pomdp.org/tutorial/index.html
   5. http://www.pomdp.org/tutorial/mdp-vi.html

   hidden links:
   7. http://www.pomdp.org/
   8. http://www.pomdp.org/tutorial/
   9. http://www.pomdp.org/code/index.html
  10. http://www.pomdp.org/code/cmd-line.html
  11. http://www.pomdp.org/code/pomdp-file-spec.html
  12. http://www.pomdp.org/code/alpha-file-spec.html
  13. http://www.pomdp.org/code/pg-file-spec.html
  14. http://www.pomdp.org/code/copyright.html
  15. http://www.pomdp.org/examples/
  16. http://www.pomdp.org/papers/
  17. http://www.pomdp.org/talks/
  18. http://www.pomdp.org/faq.html
