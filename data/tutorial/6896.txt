   [1]contact ();
   [2]links ();
   ____________________ submit
   [3]home ();
   [4]ai ();
   [5]robotics ();
   [6]notes ();
   [7]about ();
   [8]home > [9]ai main > [10]path-finding > [11]id24 > tutorial
   [12]delicious [spacer_trans8x16.png] [spacer_trans8x16.png]
   [spacer_trans8x16.png]
   [13]path
   finding
   [14]id24
   [15]a*

id24 .*;

step-by-step tutorial

   this tutorial introduces the concept of id24 through a simple but
   comprehensive numerical example.  the example describes an agent which
   uses unsupervised training to learn about an unknown environment.  you
   might also find it helpful to compare this example with the
   accompanying source code examples.

   suppose we have 5 rooms in a building connected by doors as shown in
   the figure below.  we'll number each room 0 through 4.  the outside of
   the building can be thought of as one big room (5).  notice that doors
   1 and 4 lead into the building from room 5 (outside).

                  [modeling_environment_clip_image002a.gif]

   we can represent the rooms on a graph, each room as a node, and each
   door as a link.

                                 [map1a.gif]

   for this example, we'd like to put an agent in any room, and from that
   room, go outside the building (this will be our target room). in other
   words, the goal room is number 5. to set this room as a goal, we'll
   associate a reward value to each door (i.e. link between nodes). the
   doors that lead immediately to the goal have an instant reward of 100.
   other doors not directly connected to the target room have zero reward.
   because doors are two-way ( 0 leads to 4, and 4 leads back to 0 ), two
   arrows are assigned to each room. each arrow contains an instant reward
   value, as shown below:

                                 [map2a.gif]

   of course, room 5 loops back to itself with a reward of 100, and all
   other direct connections to the goal room carry a reward of 100.  in
   id24, the goal is to reach the state with the highest reward, so
   that if the agent arrives at the goal, it will remain there forever.
   this type of goal is called an "absorbing goal".

   imagine our agent as a dumb virtual robot that can learn through
   experience. the agent can pass from one room to another but has no
   knowledge of the environment, and doesn't know which sequence of doors
   lead to the outside.

   suppose we want to model some kind of simple evacuation of an agent
   from any room in the building. now suppose we have an agent in room 2
   and we want the agent to learn to reach outside the house (5).

                          [agent_clip_image002.gif]

   the terminology in id24 includes the terms "state" and "action".

   we'll call each room, including outside, a "state", and the agent's
   movement from one room to another will be an "action".  in our diagram,
   a "state" is depicted as a node, while "action" is represented by the
   arrows.

                                 [map3a.gif]

   suppose the agent is in state 2.  from state 2, it can go to state 3
   because state 2 is connected to 3.  from state 2, however, the agent
   cannot directly go to state 1 because there is no direct door
   connecting room 1 and 2 (thus, no arrows).  from state 3, it can go
   either to state 1 or 4 or back to 2 (look at all the arrows about state
   3).  if the agent is in state 4, then the three possible actions are to
   go to state 0, 5 or 3.  if the agent is in state 1, it can go either to
   state 5 or 3.  from state 0, it can only go back to state 4.

   we can put the state diagram and the instant reward values into the
   following reward table, "matrix r".

                               [r_matrix1.gif]

     the -1's in the table represent null values (i.e.; where there isn't
      a link between nodes). for example, state 0 cannot go to state 1.

   now we'll add a similar matrix, "q", to the brain of our agent,
   representing the memory of what the agent has learned through
   experience.  the rows of matrix q represent the current state of the
   agent, and the columns represent the possible actions leading to the
   next state (the links between the nodes).

   the agent starts out knowing nothing, the matrix q is initialized to
   zero.  in this example, for the simplicity of explanation, we assume
   the number of states is known (to be six).  if we didn't know how many
   states were involved, the matrix q could start out with only one
   element.  it is a simple task to add more columns and rows in matrix q
   if a new state is found.

   the transition rule of id24 is a very simple formula:

     q(state, action) = r(state, action) + gamma * max[q(next state, all
     actions)]

   according to this formula, a value assigned to a specific element of
   matrix q, is equal to the sum of the corresponding value in matrix r
   and the learning parameter gamma, multiplied by the maximum value of q
   for all possible actions in the next state.


   our virtual agent will learn through experience, without a teacher
   (this is called unsupervised learning).  the agent will explore from
   state to state until it reaches the goal. we'll call each exploration
   an episode.  each episode consists of the agent moving from the initial
   state to the goal state.  each time the agent arrives at the goal
   state, the program goes to the next episode.


   the id24 algorithm goes as follows:

     1. set the gamma parameter, and environment rewards in matrix r.

     2. initialize matrix q to zero.

     3. for each episode:

     select a random initial state.

     do while the goal state hasn't been reached.
     * select one among all possible actions for the current state.
     * using this possible action, consider going to the next state.
     * get maximum q value for this next state based on all possible
       actions.
     * compute: q(state, action) = r(state, action) + gamma * max[q(next
       state, all actions)]
     * set the next state as the current state.

     end do

     end for

   the algorithm above is used by the agent to learn from experience.
   each episode is equivalent to one training session.  in each training
   session, the agent explores the environment (represented by matrix r ),
   receives the reward (if any) until it reaches the goal state. the
   purpose of the training is to enhance the 'brain' of our agent,
   represented by matrix q.  more training results in a more optimized
   matrix q.  in this case, if the matrix q has been enhanced, instead of
   exploring around, and going back and forth to the same rooms, the agent
   will find the fastest route to the goal state.

   the gamma parameter has a range of 0 to 1 (0 <= gamma > 1).  if gamma
   is closer to zero, the agent will tend to consider only immediate
   rewards.  if gamma is closer to one, the agent will consider future
   rewards with greater weight, willing to delay the reward.

   to use the matrix q, the agent simply traces the sequence of states,
   from the initial state to goal state.  the algorithm finds the actions
   with the highest reward values recorded in matrix q for current state:

   algorithm to utilize the q matrix:

     1. set current state = initial state.

     2. from current state, find the action with the highest q value.

     3. set current state = next state.

     4. repeat steps 2 and 3 until current state = goal state.

   the algorithm above will return the sequence of states from the initial
   state to the goal state.


id24 example by hand

   to understand how the id24 algorithm works, we'll go through a
   few episodes step by step. the rest of the steps are illustrated in the
   source code examples.

   we'll start by setting the value of the learning parameter gamma = 0.8,
   and the initial state as room 1.

   initialize matrix q as a zero matrix:

                               [q_matrix1.gif]

   look at the second row (state 1) of matrix r.  there are two possible
   actions for the current state 1: go to state 3, or go to state 5. by
   random selection, we select to go to 5 as our action.

                               [r_matrix1.gif]

   now let's imagine what would happen if our agent were in state 5.  look
   at the sixth row of the reward matrix r (i.e. state 5).  it has 3
   possible actions: go to state 1, 4 or 5.

     q(state, action) = r(state, action) + gamma * max[q(next state, all
     actions)]

     q(1, 5) = r(1, 5) + 0.8 * max[q(5, 1), q(5, 4), q(5, 5)] = 100 + 0.8
     * 0 = 100


   since matrix q is still initialized to zero, q(5, 1), q(5, 4), q(5, 5),
   are all zero.  the result of this computation for q(1, 5) is 100
   because of the instant reward from r(5, 1).

   the next state, 5, now becomes the current state.  because 5 is the
   goal state, we've finished one episode.  our agent's brain now contains
   an updated matrix q as:

                               [q_matrix2.gif]

   for the next episode, we start with a randomly chosen initial state.
   this time, we have state 3 as our initial state.

   look at the fourth row of matrix r; it has 3 possible actions: go to
   state 1, 2 or 4.  by random selection, we select to go to state 1 as
   our action.

   now we imagine that we are in state 1.  look at the second row of
   reward matrix r (i.e. state 1).  it has 2 possible actions: go to state
   3 or state 5.  then, we compute the q value:

     q(state, action) = r(state, action) + gamma * max[q(next state, all
     actions)]

     q(1, 5) = r(1, 5) + 0.8 * max[q(1, 2), q(1, 5)] = 0 + 0.8 * max(0,
     100) = 80

   we use the updated matrix q from the last episode.  q(1, 3) = 0 and
   q(1, 5) = 100.  the result of the computation is q(3, 1) = 80 because
   the reward is zero.  the matrix q becomes:

                               [q_matrix3.gif]

   the next state, 1, now becomes the current state.  we repeat the inner
   loop of the id24 algorithm because state 1 is not the goal state.


   so, starting the new loop with the current state 1, there are two
   possible actions: go to state 3, or go to state 5.  by lucky draw, our
   action selected is 5.

                                 [map3a.gif]

   now, imaging we're in state 5, there are three possible actions: go to
   state 1, 4 or 5.  we compute the q value using the maximum value of
   these possible actions.

     q(state, action) = r(state, action) + gamma * max[q(next state, all
     actions)]

     q(1, 5) = r(1, 5) + 0.8 * max[q(5, 1), q(5, 4), q(5, 5)] = 100 + 0.8
     * 0 = 100


   the updated entries of matrix q, q(5, 1), q(5, 4), q(5, 5), are all
   zero.  the result of this computation for q(1, 5) is 100 because of the
   instant reward from r(5, 1).  this result does not change the q matrix.

   because 5 is the goal state, we finish this episode.  our agent's brain
   now contain updated matrix q as:

                               [q_matrix3.gif]

   if our agent learns more through further episodes, it will finally
   reach convergence values in matrix q like:

                               [q_matrix4.gif]

   this matrix q, can then be normalized (i.e.; converted to percentage)
   by dividing all non-zero entries by the highest number (500 in this
   case):

                               [q_matrix5.gif]

   once the matrix q gets close enough to a state of convergence, we know
   our agent has learned the most optimal paths to the goal state.
   tracing the best sequences of states is as simple as following the
   links with the highest values at each state.

                                 [map5.gif]

   for example, from initial state 2, the agent can use the matrix q as a
   guide:

     from state 2 the maximum q values suggests the action to go to state
     3.

     from state 3 the maximum q values suggest two alternatives: go to
     state 1 or 4.  suppose we arbitrarily choose  to go to 1.

     from state 1 the maximum q values suggests the action to go to state
     5.

     thus the sequence is 2 - 3 - 1 - 5.


   news links:
   [16]2019 (0)
   [17]april (0)
   [18]march (0)
   [19]february (0)
   [20]january (0)
   [21]2018 (0)
   [22]2017 (0)
   [23]2016 (0)
   [24]2015 (0)
   [25]2014 (0)
   [26]2013 (32)
   [27]2012 (242)
   [28]2011 (217)
   [29]2010 (185)
   [30]2009 (20)
   search news links: ____________________ [button input] (not
   implemented)___________

   public void footer() {
   [31]about | [32]contact | privacy policy | [33]terms of service |
   [34]site map
   copyright   2009-2012 john mccullock. all rights reserved.
   }

references

   1. http://mnemstudio.org/contact.php
   2. http://mnemstudio.org/links-all-topics.htm
   3. http://mnemstudio.org/index.php
   4. http://mnemstudio.org/artificial-intelligence-introduction.htm
   5. http://mnemstudio.org/robots-introduction.htm
   6. http://mnemstudio.org/misc-notes-introduction.htm
   7. http://mnemstudio.org/about.php
   8. http://mnemstudio.org/index.php
   9. http://mnemstudio.org/artificial-intelligence-introduction.htm
  10. http://mnemstudio.org/path-finding-introduction.htm
  11. http://mnemstudio.org/path-finding-id24.htm
  12. http://delicious.com/save
  13. http://mnemstudio.org/path-finding-introduction.htm
  14. http://mnemstudio.org/path-finding-id24.htm
  15. http://mnemstudio.org/path-finding-a-star.htm
  16. http://mnemstudio.org/news-2019.htm
  17. http://mnemstudio.org/news-2019-04.htm
  18. http://mnemstudio.org/news-2019-03.htm
  19. http://mnemstudio.org/news-2019-02.htm
  20. http://mnemstudio.org/news-2019-01.htm
  21. http://mnemstudio.org/news-2018.htm
  22. http://mnemstudio.org/news-2017.htm
  23. http://mnemstudio.org/news-2016.htm
  24. http://mnemstudio.org/news-2015.htm
  25. http://mnemstudio.org/news-2014.htm
  26. http://mnemstudio.org/news-2013.htm
  27. http://mnemstudio.org/news-2012.htm
  28. http://mnemstudio.org/news-2011.htm
  29. http://mnemstudio.org/news-2010.htm
  30. http://mnemstudio.org/news-2009.htm
  31. http://mnemstudio.org/about.php
  32. http://mnemstudio.org/contact.php
  33. http://mnemstudio.org/terms-of-service.php
  34. http://mnemstudio.org/sitemap.php
