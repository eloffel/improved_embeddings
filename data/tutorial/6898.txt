   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id75 in python; predict the bay area   s home prices

   [16]go to the profile of susan li
   [17]susan li (button) blockedunblock (button) followfollowing
   oct 24, 2017
   [1*c35fclnox4oebsmncnzfkg.jpeg]

motivation

   in order to predict the bay area   s home prices, i chose the housing
   price dataset that was sourced from [18]bay area home sales database
   and [19]zillow. this dataset was based on the homes sold between
   january 2013 and december 2015. it has many characteristics of
   learning, and the dataset can be downloaded from [20]here.

id174

import pandas as pd
sf = pd.read_csv('final_data.csv')
sf.head()

   [1*vjotlfg5qupdab5dzy_ksw.png]

   there are several features that we do not need, such as    info   ,
      z_address   ,    zipcode   (we have    neighborhood    as a location variable),
      zipid    and    zestimate   (this is the price estimated by [21]zillow, we
   don   t want our model to be affected by this), so, we will drop them.
sf.drop(sf.columns[[0, 2, 3, 15, 17, 18]], axis=1, inplace=true)
sf.info()

   [1*cs3kf6rszkktrtxfoh9fsa.png]

   the data type of    zindexvalue    should be numeric, so let   s change that.
sf['zindexvalue'] = sf['zindexvalue'].str.replace(',', '')
sf['zindexvalue'] = sf['zindexvalue'].convert_objects(convert_numeric=true)
sf.lastsolddate.min(), sf.lastsolddate.max()

   (   01/02/2013   ,    12/31/2015   )

   the house sold period in the dateset was between january 2013 and
   december 2015.

   i now use the describe() method to show the summary statistics of the
   numeric variables.
sf.describe()

   [1*1evnhb5y-y6ahbcvsbuujw.png]

   the count, mean, min and max rows are self-explanatory. the std shows
   the standard deviation, and the 25%, 50% and 75% rows show the
   corresponding percentiles.

   to get a feel for the type of data we are dealing with, we plot a
   histogram for each numeric variable.
%matplotlib inline
import matplotlib.pyplot as plt
sf.hist(bins=50, figsize=(20,15))
plt.savefig("attribute_histogram_plots")
plt.show()

   [1*gga7ndayrtpoymzkmei3ta.png]
   figure 1. a histogram for each numerical variable

   some of the histograms are a little bit right skewed, but this is not
   abnormal.

   let   s create a scatter plot with latitude and longitude to visualize
   the data:
sf.plot(kind="scatter", x="longitude", y="latitude", alpha=0.2)
plt.savefig('map1.png')

   [1*dsi7yijc90ahx_jgzvl-3a.png]
   figure 2. a scatter plot of the data

   now let   s color code from the most expensive to the least expensive
   areas:
sf.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, figsize=(10,7),
    c="lastsoldprice", cmap=plt.get_cmap("jet"), colorbar=true,
    sharex=false)

   [1*cia5y6-ekivb47yzkow3zw.png]
   figure 3. the bay area housing prices

   this image tells us that the most expensive houses sold were in the
   north area.

   the variable we are going to predict is the    last sold price   . so let   s
   look at how much each independent variable correlates with this
   dependent variable.
corr_matrix = sf.corr()
corr_matrix["lastsoldprice"].sort_values(ascending=false)

   [1*od24medgaoby4a-w31zjbw.png]

   the last sold price tends to increase when the finished sqft and the
   number of bathrooms go up. you can see a small negative correlation
   between the year built and the last sold price. and finally,
   coefficients close to zero indicate that there is no linear
   correlation.

   we are now going to visualize the correlation between variables by
   using pandas    scatter_matrix function. we will just focus on a few
   promising variables, that seem the most correlated with the last sold
   price.
from pandas.tools.plotting import scatter_matrix
attributes = ["lastsoldprice", "finishedsqft", "bathrooms", "zindexvalue"]
scatter_matrix(sf[attributes], figsize=(12, 8))
plt.savefig('matrix.png')

   [1*9evexl1o2z5pvpkp3tuk2g.png]
   figure 4. a scatter matrix

   the most promising variable for predicting the last sold price is the
   finished sqft, so let   s zoom in on their correlation scatter plot.
sf.plot(kind="scatter", x="finishedsqft", y="lastsoldprice", alpha=0.5)
plt.savefig('scatter.png')

   [1*mrpjnt1g9oiosokrb1mmoa.png]
   figure 5. finished sqft vs. last sold price

   the correlation is indeed very strong; you can clearly see the upward
   trend and that the points are not too dispersed.

   because each house has different square footage and each neighborhood
   has different home prices, what we really need is the price per sqft.
   so, we add a new variable    price_per_sqft   . we then check to see how
   much this new independent variable correlates with the last sold price.
sf['price_per_sqft'] = sf['lastsoldprice']/sf['finishedsqft']
corr_matrix = sf.corr()
corr_matrix["lastsoldprice"].sort_values(ascending=false)

   [1*evajgdf5crjccnskru1xzg.png]

   unfortunately, the new price_per_sqft variable shows only a very small
   positive correlation with the last sold price. but we still need this
   variable for grouping neighborhoods.

   there are 71 neighborhoods in the data, and we are going to group them.
len(sf['neighborhood'].value_counts())

   71

   the following steps cluster the neighborhood into three groups: 1. low
   price; 2. high price low frequency; 3. high price high frequency.
freq = sf.groupby('neighborhood').count()['address']
mean = sf.groupby('neighborhood').mean()['price_per_sqft']
cluster = pd.concat([freq, mean], axis=1)
cluster['neighborhood'] = cluster.index
cluster.columns = ['freq', 'price_per_sqft','neighborhood']
cluster.describe()

   [1*qltsi1roewvkf6nui8h7pg.png]

   these are the low price neighborhoods:
cluster1 = cluster[cluster.price_per_sqft < 756]
cluster1.index

   index([   bayview   ,    central richmond   ,    central sunset   ,    crocker
   amazon   ,
       daly city   ,    diamond heights   ,    excelsior   ,    forest hill   ,
       forest hill extension   ,    golden gate heights   ,    ingleside   ,
       ingleside heights   ,    ingleside terrace   ,    inner parkside   ,
       inner richmond   ,    inner sunset   ,    lakeshore   ,    little hollywood   ,
       merced heights   ,    mission terrace   ,    mount davidson manor   ,
       oceanview   ,    outer mission   ,    outer parkside   ,    outer richmond   ,
       outer sunset   ,    parkside   ,    portola   ,    silver terrace   ,    sunnyside   ,
       visitacion valley   ,    west portal   ,    western addition   ,
       westwood highlands   ,    westwood park   ],
    dtype=   object   , name=   neighborhood   

   these are the high price and low frequency neighborhoods:
cluster_temp = cluster[cluster.price_per_sqft >= 756]
cluster2 = cluster_temp[cluster_temp.freq <123]
cluster2.index

   index([   buena vista park   ,    central waterfront         dogpatch   ,    corona
   heights   ,    haight-ashbury   ,    lakeside   ,    lone mountain   ,    midtown
   terrace   ,
       north beach   ,    north waterfront   ,    parnassus         ashbury   ,    presidio
   heights   ,    sea cliff   ,    st. francis wood   ,    telegraph hill   ,    twin
   peaks   ], dtype=   object   , name=   neighborhood   )

   these are the high price and high frequency neighborhoods:
cluster3 = cluster_temp[cluster_temp.freq >=123]
cluster3.index

   index([   bernal heights   ,    cow hollow   ,    downtown   ,    eureka
   valley         dolores heights         castro   ,    glen park   ,    hayes valley   ,
      lake   ,    lower pacific heights   ,    marina   ,    miraloma park   ,    mission   ,
      nob hill   ,    noe valley   ,    north panhandle   ,    pacific heights   ,
      potrero hill   ,    russian hill   ,    south beach   ,    south of market   ,    van
   ness         civic center   ,    yerba buena   ],
    dtype=   object   , name=   neighborhood   )

   we add a group column based on the clusters:
def get_group(x):
    if x in cluster1.index:
        return 'low_price'
    elif x in cluster2.index:
        return 'high_price_low_freq'
    else:
        return 'high_price_high_freq'
sf['group'] = sf.neighborhood.apply(get_group)

   after performing the above pre-processing, we do not need the following
   columns anymore:    address, lastsolddate, latitude, longitude,
   neighborhood, price_per_sqft   , so, we drop them from our analysis.
sf.drop(sf.columns[[0, 4, 6, 7, 8, 13]], axis=1, inplace=true)
sf = sf[['bathrooms', 'bedrooms', 'finishedsqft', 'totalrooms', 'usecode', 'year
built','zindexvalue', 'group', 'lastsoldprice']]
sf.head()

   [1*mpmf1nm1b856lsp31mbe9a.png]

   our data looks perfect!

   but before we build the model, we need to create dummy variables for
   these two categorical variables:    usecode    and    group   .
x = sf[['bathrooms', 'bedrooms', 'finishedsqft', 'totalrooms', 'usecode', 'yearb
uilt',
         'zindexvalue', 'group']]
y = sf['lastsoldprice']
n = pd.get_dummies(sf.group)
x = pd.concat([x, n], axis=1)
m = pd.get_dummies(sf.usecode)
x = pd.concat([x, m], axis=1)
drops = ['group', 'usecode']
x.drop(drops, inplace=true, axis=1)
x.head()

   this is what our data looks like after creating dummy variables:
   [1*7kjqlefwuba6ipcghbddyq.png]

train and build a id75 model

from sklearn.cross_validation import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_
state=0)
from sklearn.linear_model import linearregression
regressor = linearregression()
regressor.fit(x_train, y_train)

   linearregression(copy_x=true, fit_intercept=true, n_jobs=1,
   normalize=false)

   done! we now have a working id75 model.

   calculate r squared:
y_pred = regressor.predict(x_test)
print('id75 r squared": %.4f' % regressor.score(x_test, y_test))

   id75 r squared: 0.5619

   so, in our model, 56.19% of the variability in y can be explained using
   x. this is not that exciting.

   calculate root-mean-square error (rmse)
import numpy as np
from sklearn.metrics import mean_squared_error
lin_mse = mean_squared_error(y_pred, y_test)
lin_rmse = np.sqrt(lin_mse)
print('id75 rmse: %.4f' % lin_rmse)

   id75 rmse: 616071.5748

   our model was able to predict the value of every house in the test set
   within $616071 of the real price.

   calculate mean absolute error (mae):
from sklearn.metrics import mean_absolute_error
lin_mae = mean_absolute_error(y_pred, y_test)
print('id75 mae: %.4f' % lin_mae)

   id75 mae: 363742.1631

id79

   let   s try a more complex model to see whether results can be
   improved         the randomforestregressor:
from sklearn.ensemble import randomforestregressor
forest_reg = randomforestregressor(random_state=42)
forest_reg.fit(x_train, y_train)

   randomforestregressor(bootstrap=true, criterion=   mse   , max_depth=none,
   max_features=   auto   , max_leaf_nodes=none,
    min_impurity_split=1e-07, min_samples_leaf=1,
    min_samples_split=2, min_weight_fraction_leaf=0.0,
    n_estimators=10, n_jobs=1, oob_score=false, random_state=42,
    verbose=0, warm_start=false)
print('id79 r squared": %.4f' % forest_reg.score(x_test, y_test))

   id79 r squared   : 0.6491
y_pred = forest_reg.predict(x_test)
forest_mse = mean_squared_error(y_pred, y_test)
forest_rmse = np.sqrt(forest_mse)
print('id79 rmse: %.4f' % forest_rmse)

   id79 rmse: 551406.0926

   much better! let   s try one more.

gradient boosting

from sklearn import ensemble
from sklearn.ensemble import gradientboostingregressor
model = ensemble.gradientboostingregressor()
model.fit(x_train, y_train)

   gradientboostingregressor(alpha=0.9, criterion=   friedman_mse   ,
   init=none, learning_rate=0.1, loss=   ls   , max_depth=3,
   max_features=none, max_leaf_nodes=none, min_impurity_split=1e-07,
   min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0,
   n_estimators=100, presort=   auto   , random_state=none, subsample=1.0,
   verbose=0,warm_start=false)
print('gradient boosting r squared": %.4f' % model.score(x_test, y_test))

   gradient boosting r squared   : 0.6616
y_pred = model.predict(x_test)
model_mse = mean_squared_error(y_pred, y_test)
model_rmse = np.sqrt(model_mse)
print('gradient boosting rmse: %.4f' % model_rmse)

   gradient boosting rmse: 541503.7962

   these are the best results we have so far, so, i would consider this is
   our final model.

feature importance

   we have used 19 features (variables) in our model. let   s find out which
   features are important and vice versa.
feature_labels = np.array(['bathrooms', 'bedrooms', 'finishedsqft', 'totalrooms'
, 'yearbuilt', 'zindexvalue',
                           'high_price_high_freq', 'high_price_low_freq', 'low_p
rice', 'apartment', 'condominium', 'cooperative',
                          'duplex', 'miscellaneous', 'mobile', 'multifamily2to4'
, 'multifamily5plus', 'singlefamily',
                           'townhouse'])
importance = model.feature_importances_
feature_indexes_by_importance = importance.argsort()
for index in feature_indexes_by_importance:
    print('{}-{:.2f}%'.format(feature_labels[index], (importance[index] *100.0))
)

   [1*l9c26_1mvbprdbmizs4vbq.png]

   the most important features are finished sqft, zindex value, number of
   bathrooms, total rooms, year built and so on. and the least important
   feature is apartment, which means that regardless of whether this unit
   is an apartment or not, does not matter to the sold price. overall,
   most of these 19 features are used.

your turn!

   hopefully, this post gives you a good idea of what a machine learning
   regression project looks like. as you can see, much of the work is in
   the data wrangling and the preparation steps, and these procedures
   consume most of the time spent on machine learning.

   now it   s time to get out there and start exploring and cleaning your
   data. try two or three algorithms, and let me know how it goes.

   source code that created this post can be found [22]here. i would be
   pleased to receive feedback or questions on any of the above.

     * [23]machine learning
     * [24]data science
     * [25]predictive analytics
     * [26]id75
     * [27]python

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 13 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of susan li

[29]susan li

   becoming an expert in ml, nlp, data story telling and encouraging
   others to do the same. sr data scientist, toronto canada.
   [30]https://www.linkedin.com/in/susanli/

     (button) follow
   [31]towards data science

[32]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [33]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [34]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5c91c8378878
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ua9y3xkacdyj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@actsusanli?source=post_header_lockup
  17. https://towardsdatascience.com/@actsusanli
  18. http://www.sfgate.com/webdb/homesales/
  19. https://www.zillow.com/san-francisco-ca/home-values/
  20. https://raw.githubusercontent.com/ruichang123/regression_for_house_price_estimation/master/final_data.csv
  21. https://www.zillow.com/how-much-is-my-home-worth/
  22. https://github.com/susanli2016/machine-learning-with-python/blob/master/predict_bay_area_home_price.ipynb
  23. https://towardsdatascience.com/tagged/machine-learning?source=post
  24. https://towardsdatascience.com/tagged/data-science?source=post
  25. https://towardsdatascience.com/tagged/predictive-analytics?source=post
  26. https://towardsdatascience.com/tagged/linear-regression?source=post
  27. https://towardsdatascience.com/tagged/python?source=post
  28. https://towardsdatascience.com/@actsusanli?source=footer_card
  29. https://towardsdatascience.com/@actsusanli
  30. https://www.linkedin.com/in/susanli/
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/5c91c8378878/share/twitter
  37. https://medium.com/p/5c91c8378878/share/facebook
  38. https://medium.com/p/5c91c8378878/share/twitter
  39. https://medium.com/p/5c91c8378878/share/facebook
