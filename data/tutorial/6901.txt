   #[1]learn opencv    feed [2]learn opencv    comments feed [3]learn opencv
      image classification using feedforward neural network in keras
   comments feed [4]exposure fusion using opencv (c++/python)
   [5]understanding id180 in deep learning [6]alternate
   [7]alternate

   [tr?id=193036094536652&ev=pageview&noscript=1]

     * [8]skip to primary navigation
     * [9]skip to content
     * [10]skip to primary sidebar

   [11]learn opencv

   opencv examples and tutorials ( c++ / python )

     * [12]home
     * [13]about
     * [14]resources
     * [15]ai consulting
     * [16]courses

image classification using feedforward neural network in keras

   october 23, 2017 by [17]vikas gupta [18]17 comments

   this post is part of the series on deep learning for beginners, which
   consists of the following tutorials :
    1. [19]neural networks : a 30,000 feet view for beginners
    2. [20]installation of deep learning frameworks (tensorflow and keras
       with cuda support )
    3. [21]introduction to keras
    4. [22]understanding feedforward neural networks
    5. image classification using feedforward neural networks
    6. [23]image recognition using convolutional neural network
    7. [24]understanding id180
    8. [25]understanding autoencoders using tensorflow
    9. [26]image classification using pre-trained models in keras
   10. [27]id21 using pre-trained models in keras
   11. [28]fine-tuning pre-trained models in keras
   12. more to come . . .

   in this article, we will learn how to implement a feedforward neural
   network in keras. we will use handwritten digit classification as an
   example to illustrate the effectiveness of a feedforward network. we
   will also see how to spot and overcome overfitting during training.

   [29]mnist is a commonly used handwritten digit dataset consisting of
   60,000 images in the training set and 10,000 images in the test set.
   so, each digit has 6000 images in the training set. the digits are
   size-normalized and centered in a fixed-size ( 28  28 ) image. the task
   is to train a machine learning algorithm to recognize a new sample from
   the test set correctly.

1. the network

   for a quick understanding of feedforward neural network, you can have a
   look at our [30]previous article. we will use raw pixel values as input
   to the network. the images are matrices of size 28  28. so, we reshape
   the image matrix to an array of size 784 ( 28*28 ) and feed this array
   to the network. we will use a network with 2 hidden layers having 512
   neurons each. the output layer will have 10 layers for the 10 digits. a
   schematic diagram is shown below.
   [31]block diagram of mlp

   check out [32]this post if you don   t have keras installed yet! also,
   download the code from the link below to follow along with the post.
   download code

   to easily follow along this tutorial, please download code by clicking
   on the button below. it   s free!
   [33]download code

   let us dive into the code!

2. load the data

   keras comes with the mnist data loader. it has a function
   mnist.load_data() which downloads the data from its servers if it is
   not present on your computer. the data loaded using this function is
   divided into training and test sets. this is done by the following :
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

3. checkout the data

   let   s see how the data looks like. the data consists of handwritten
   numbers ranging from 0 to 9, along with their ground truth. it has
   60,000 train samples and 10,000 test samples. each sample is a 28  28
   grayscale image.
from keras.utils import to_categorical

print('training data shape : ', train_images.shape, train_labels.shape)

print('testing data shape : ', test_images.shape, test_labels.shape)

# find the unique numbers from the train labels
classes = np.unique(train_labels)
nclasses = len(classes)
print('total number of outputs : ', nclasses)
print('output classes : ', classes)

plt.figure(figsize=[10,5])

# display the first image in training data
plt.subplot(121)
plt.imshow(train_images[0,:,:], cmap='gray')
plt.title("ground truth : {}".format(train_labels[0]))

# display the first image in testing data
plt.subplot(122)
plt.imshow(test_images[0,:,:], cmap='gray')
plt.title("ground truth : {}".format(test_labels[0]))

   output:
   [34]mnist sample data

4. process the data

   the images are grayscale and the pixel values range from 0 to 255. we
   will apply the following preprocessing to the data before feeding it to
   the network.
    1. convert each image matrix ( 28  28 ) to an array ( 28*28 = 784
       dimenstional ) which will be fed to the network as a single
       feature.
# change from matrix to array of dimension 28x28 to array of dimention 784
dimdata = np.prod(train_images.shape[1:])
train_data = train_images.reshape(train_images.shape[0], dimdata)
test_data = test_images.reshape(test_images.shape[0], dimdata)

    2. convert the data to float and scale the values between 0 to 1.
# change to float datatype
train_data = train_data.astype('float32')
test_data = test_data.astype('float32')

# scale the data to lie between 0 to 1
train_data /= 255
test_data /= 255

    3. convert the labels from integer to categorical ( one-hot ) encoding
       since that is the format required by keras to perform multiclass
       classification. one-hot encoding is a type of boolean
       representation of integer data. it converts the integer to an array
       of all zeros except a 1 at the index of the integer.
       for example, using a one-hot encoding for 10 classes, the integer 5
       will be encoded as 0000010000
# change the labels from integer to categorical data
train_labels_one_hot = to_categorical(train_labels)
test_labels_one_hot = to_categorical(test_labels)

# display the change for category label using one-hot encoding
print('original label 0 : ', train_labels[0])
print('after conversion to categorical ( one-hot ) : ', train_labels_one_hot[0])

       output:
       original label 0 : 5
       after conversion to categorical ( one-hot ) : [ 0. 0. 0. 0. 0. 1.
       0. 0. 0. 0.]

5. keras workflow for training the network

   we have described the keras workflow in our [35]previous post. the
   block diagram is given here for reference. basically, once you have the
   training and test data, you can follow these steps to train a neural
   network in keras.
   [36]keras-workflow

5.1. create the network

   we had mentioned that we will be using a network with 2 hidden layers
   and an output layer with 10 units. the number of units in the hidden
   layers is kept to be 512. the input to the network is the
   784-dimensional array converted from the 28  28 image.

   we will use the sequential model for building the network. in the
   sequential model, we can just stack up layers by adding the desired
   layer one by one. we use the dense layer, also called fully connected
   layer since we are building a feedforward network in which all the
   neurons from one layer are connected to the neurons in the previous
   layer. apart from the dense layer, we add the relu activation function
   which is required to introduce non-linearity to the model. this will
   help the network learn non-linear decision boundaries. the last layer
   is a softmax layer as it is a multiclass classification problem. for
   binary classification, we can use sigmoid.
from keras.models import sequential
from keras.layers import dense

model = sequential()
model.add(dense(512, activation='relu', input_shape=(dimdata,)))
model.add(dense(512, activation='relu'))
model.add(dense(nclasses, activation='softmax'))

5.2. configure the network

   in this step, we configure the optimizer to be rmsprop. we also specify
   the loss type which is categorical cross id178 which is used for
   multiclass classification. we also specify the metrics ( accuracy in
   this case ) which we want to track during the training process. you can
   also try using any other optimizer such as adam or sgd.
model.compile(optimizer='rmsprop', loss='categorical_crossid178', metrics=['ac
curacy'])

5.3. train the model

   the network is ready to get trained. this is done using the fit()
   function in keras. we specify the number of epochs as 20. this means
   that the whole dataset will be fed to the network 20 times. we will be
   using the test data for validation.
history = model.fit(train_data, train_labels_one_hot, batch_size=256, epochs=20,
 verbose=1,
                   validation_data=(test_data, test_labels_one_hot))

5.4. evaluate the trained model

   we check the performance on the whole test data using the evaluate()
   method.
[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)
print("evaluation result on test data : loss = {}, accuracy = {}".format(test_lo
ss, test_acc))

   output:
   evaluation result on test data : loss = 0.135059975359, accuracy =
   0.9807

   the results look good. however, we would want to have another look at
   the results.

6. check for overfitting

   the fit() function returns a history object which has a dictionary of
   all the metrics which were required to be tracked during training. we
   can use the data in the history object to plot the loss and accuracy
   curves to check how the training process went.
   you can use the history.history.keys() function to check what metrics
   are present in the history. it should look like the following

   [   acc   ,    loss   ,    val_acc   ,    val_loss   ]

   let us plot the loss and accuracy curves.
#plot the loss curves
plt.figure(figsize=[8,6])
plt.plot(history.history['loss'],'r',linewidth=3.0)
plt.plot(history.history['val_loss'],'b',linewidth=3.0)
plt.legend(['training loss', 'validation loss'],fontsize=18)
plt.xlabel('epochs ',fontsize=16)
plt.ylabel('loss',fontsize=16)
plt.title('loss curves',fontsize=16)

#plot the accuracy curves
plt.figure(figsize=[8,6])
plt.plot(history.history['acc'],'r',linewidth=3.0)
plt.plot(history.history['val_acc'],'b',linewidth=3.0)
plt.legend(['training accuracy', 'validation accuracy'],fontsize=18)
plt.xlabel('epochs ',fontsize=16)
plt.ylabel('accuracy',fontsize=16)
plt.title('accuracy curves',fontsize=16)


   [37]loss curve without id173 mlp mnist
   [38]accuracy curves without id173 mlp mnist

   although the accuracy obtained above is very good, if you see the loss
   and accuracy curves in the above figures, you   ll notice that the
   validation loss initially decrease, but then it starts increasing
   gradually. also, there is a substantial difference between the training
   and test accuracy. this is a clear sign of overfitting which means that
   the network has memorized the training data very well, but is not
   guaranteed to work on unseen data. thus, the difference in the training
   and test accuracy.

7. add id173 to the model

   overfitting occurs mainly because the network parameters are getting
   too biased towards the training data. we can add a dropout layer to
   overcome this problem to a certain extent. in case of dropout, a
   fraction of neurons is randomly turned off during the training process,
   reducing the dependency on the training set by some amount.
from keras.layers import dropout

model_reg = sequential()
model_reg.add(dense(512, activation='relu', input_shape=(dimdata,)))
model_reg.add(dropout(0.5))
model_reg.add(dense(512, activation='relu'))
model_reg.add(dropout(0.5))
model_reg.add(dense(nclasses, activation='softmax'))

8. check performance after id173

   we will train the network again in the same way we did earlier and
   check the loss and accuracy curves.
model_reg.compile(optimizer='rmsprop', loss='categorical_crossid178', metrics=
['accuracy'])
history_reg = model_reg.fit(train_data, train_labels_one_hot, batch_size=256, ep
ochs=20, verbose=1,
                            validation_data=(test_data, test_labels_one_hot))

#plot the loss curves
plt.figure(figsize=[8,6])
plt.plot(history_reg.history['loss'],'r',linewidth=3.0)
plt.plot(history_reg.history['val_loss'],'b',linewidth=3.0)
plt.legend(['training loss', 'validation loss'],fontsize=18)
plt.xlabel('epochs ',fontsize=16)
plt.ylabel('loss',fontsize=16)
plt.title('loss curves',fontsize=16)

#plot the accuracy curves
plt.figure(figsize=[8,6])
plt.plot(history_reg.history['acc'],'r',linewidth=3.0)
plt.plot(history_reg.history['val_acc'],'b',linewidth=3.0)
plt.legend(['training accuracy', 'validation accuracy'],fontsize=18)
plt.xlabel('epochs ',fontsize=16)
plt.ylabel('accuracy',fontsize=16)
plt.title('accuracy curves',fontsize=16)


   [39]loss curves with id173

   [40]accuracy curves with id173

   from the above loss and accuracy curves, we can observe that
     * the validation loss is not increasing
     * the difference between the train and validation accuracy is not
       very high

   thus, we can say that the model has better generalization capability as
   the performance does not decrease drastically in case of unseen data
   also.

9. id136 on a single image

   we have seen that the first image in the test set is the number 7. let
   us see what the model predicts.

9.1. getting the predicted class

   during the id136 stage, it might be sufficient to know the class of
   the input data. it can be done as follows.
# predict the most likely class
model_reg.predict_classes(test_data[[0],:])

   output:
   array([7])

9.2. getting the probabilities

   in the above method there is no score which tells us about the
   confidence with which the model does the prediction. in some cases, for
   example when there are many classes, we may want the probabilities of
   the different classes which indicates how confident the model is about
   the occurence of a particular class. we can take the decision based on
   these scores.
# predict the probabilities for each class
model_reg.predict(test_data[[0],:])

   output:
   array([[ 1.46786899e-23, 1.73912635e-15, 3.05286026e-12,
   3.48179753e-12, 2.16374247e-22, 3.82367185e-19,
   2.31083363e-30, 1.00000000e+00, 2.78843536e-18,
   1.55856298e-14]], dtype=float32)

   this gives the id203 score for each class. we can see that the
   score for the 8th index is almost 1 which indicates that the predicted
   class is 7 with a confidence score of 1.

10. exercise

   we had used 2 hidden layers and relu activation. try to change the
   number of hidden layer and the activation to tanh or sigmoid and see
   what happens. also change the dropout ratio and check the performance.

   although the performance is pretty impressive with this model, we will
   see how to improve it further using a convolutional neural network in
   the next post. stay tuned!

subscribe & download code

   if you liked this article and would like to download code and example
   images used in this post, please [41]subscribe to our newsletter. you
   will also receive a free [42]id161 resource guide. in our
   newsletter, we share opencv tutorials and examples written in
   c++/python, and id161 and machine learning algorithms and
   news.

   [43]subscribe now

   filed under: [44]deep learning, [45]image classification, [46]image
   recognition, [47]tutorial tagged with: [48]deep learning,
   [49]feedforward neural networks, [50]image classification, [51]keras
   (button) load comments

   search this website ____________________ search

join course

   [52]id161 for faces

resources

   [53]download code (c++ / python)

disclaimer

   this site is not affiliated with opencv.org

   i am an entrepreneur with a love for id161 and machine
   learning with a dozen years of experience (and a ph.d.) in the field.

   in 2007, right after finishing my ph.d., i co-founded taaz inc. with my
   advisor dr. david kriegman and kevin barnes. the scalability, and
   robustness of our id161 and machine learning algorithms have
   been put to rigorous test by more than 100m users who have tried our
   products. [54]read more   

recent posts

     * [55]image inpainting with opencv (c++/python)
     * [56]hough transform with opencv (c++/python)
     * [57]xeus-cling: run c++ code in jupyter notebook
     * [58]pose detection comparison : wrnchai vs openpose
     * [59]gender & age classification using opencv deep learning (
       c++/python )

   copyright    2019    big vision llc

references

   1. https://www.learnopencv.com/feed/
   2. https://www.learnopencv.com/comments/feed/
   3. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/feed/
   4. https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/
   5. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/
   6. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/
   7. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/&format=xml
   8. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/#genesis-nav-primary
   9. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/#genesis-content
  10. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/#genesis-sidebar-primary
  11. https://www.learnopencv.com/
  12. https://www.learnopencv.com/
  13. https://www.learnopencv.com/about/
  14. https://www.learnopencv.com/computer-vision-resources
  15. https://www.learnopencv.com/computer-vision-machine-learning-artificial-intelligence-consulting/
  16. http://courses.learnopencv.com/
  17. https://www.learnopencv.com/author/vikas/
  18. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/#disqus_thread
  19. https://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/
  20. https://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/
  21. https://www.learnopencv.com/deep-learning-using-keras-the-basics/
  22. https://www.learnopencv.com/understanding-feedforward-neural-networks/
  23. https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/
  24. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/
  25. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/
  26. https://www.learnopencv.com/keras-tutorial-using-pre-trained-id163-models/
  27. https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/
  28. https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models
  29. https://en.wikipedia.org/wiki/mnist_database
  30. https://www.learnopencv.com/understanding-feedforward-neural-networks/
  31. https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-mnist-schematic.jpg
  32. https://www.learnopencv.com/deep-learning-using-keras-the-basics/
  33. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  34. https://www.learnopencv.com/wp-content/uploads/2017/10/sample-data-mnist.png
  35. https://www.learnopencv.com/deep-learning-using-keras-the-basics/
  36. https://www.learnopencv.com/wp-content/uploads/2017/09/keras-workflow.jpg
  37. https://www.learnopencv.com/wp-content/uploads/2017/10/loss-curve-without-reg.png
  38. https://www.learnopencv.com/wp-content/uploads/2017/10/acc-curve-without-reg.png
  39. https://www.learnopencv.com/wp-content/uploads/2017/10/loss-curve-with-reg.png
  40. https://www.learnopencv.com/wp-content/uploads/2017/10/acc-curve-with-reg.png
  41. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  42. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  43. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  44. https://www.learnopencv.com/category/deep-learning/
  45. https://www.learnopencv.com/category/image-classification/
  46. https://www.learnopencv.com/category/image-recognition/
  47. https://www.learnopencv.com/category/tutorial/
  48. https://www.learnopencv.com/tag/deep-learning/
  49. https://www.learnopencv.com/tag/feedforward-neural-networks/
  50. https://www.learnopencv.com/tag/image-classification/
  51. https://www.learnopencv.com/tag/keras/
  52. https://courses.learnopencv.com/p/computer-vision-for-faces
  53. https://bigvisionllc.lpages.co/leadbox/143044973f72a2:173c9390c346dc/5720147234914304/
  54. https://www.learnopencv.com/about/
  55. https://www.learnopencv.com/image-inpainting-with-opencv-c-python/
  56. https://www.learnopencv.com/hough-transform-with-opencv-c-python/
  57. https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/
  58. https://www.learnopencv.com/pose-detection-comparison-wrnchai-vs-openpose/
  59. https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/
