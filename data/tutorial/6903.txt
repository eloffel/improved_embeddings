   [1]

linkedin

     * [2]sign in
     * [3]join now

   tensorflow: building feed-forward neural networks step-by-step

   feed-forward neural network architecture with 1 hidden layer

         tensorflow: building feed-forward neural networks step-by-step

   published on october 19, 2017october 19, 2017     29 likes     0 comments

   [4]ahmed gad

[5]ahmed gad[6]follow

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

     * (button) like29
     * (button) comment0
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       17

   in this article, two basic feed-forward neural networks (ffnns) will be
   created using tensorflow deep learning library in python. the reader
   should have basic understanding of how neural networks work and its
   concepts in order to apply them programmatically.

   this article will take you through all steps required to build a simple
   feed-forward neural network in tensorflow by explaining each step in
   details. before actual building of the neural network, some preliminary
   steps are recommended to be discussed.

   the summarized steps are as follows:
    1. reading the training data (inputs and outputs)
    2. building and connect the neural networks layers (this included
       preparing weights, biases, and activation function of each layer)
    3. building a id168 to assess the prediction error
    4. create a training loop for training the network and updating its
       parameters
    5. applying some testing data to assess the network prediction
       accuracy

   here is the first classification problem that we are to solve using
   neural network.

   [:0]

   it is a binary classification problem to classify colors into either
   red or blue based on the three rgb color channels. it can be solved
   linearly and thus we don`t have to use hidden layers. just input and
   output layers are to be used. there will be a single neuron in the
   output layer with an activation function. the network architecture is
   shown in the following figure (figure 1):

   [:0]

   where x0=1 is the bias and w0 is its weight. w1 , w2, and w3 are the
   weights for the three inputs r (red), g (green), and b (blue).

   here is the complete code of the neural network solving that problem to
   be discussed later. for easy access, this code is called codesample1.
1.  import tensorflow
2.
3.  # preparing training data (inputs-outputs)
4.  training_inputs = tensorflow.placeholder(shape=[none, 3], dtype=tensorflow.f
loat32)
5.  training_outputs = tensorflow.placeholder(shape=[none, 1], dtype=tensorflow.
float32) #desired outputs for each input
6.
7.  # preparing neural network parameters (weights and bias) using tensorflow va
riables
8.  weights = tensorflow.variable(initial_value=[[.3], [.1], [.8]], dtype=tensor
flow.float32)
9.  bias = tensorflow.variable(initial_value=[[1]], dtype=tensorflow.float32)
10.
11. # preparing inputs of the activation function
12. af_input = tensorflow.matmul(training_inputs, weights) + bias
13.
14. # activation function of the output layer neuron
15. predictions = tensorflow.nn.sigmoid(af_input)
16.
17. # measuring the prediction error of the network after being trained
18. prediction_error = tensorflow.reduce_sum(training_outputs - predictions)
19.
20. # minimizing the prediction error using id119 optimizer
21. train_op = tensorflow.train.gradientdescentoptimizer(learning_rate=0.05).min
imize(prediction_error)
22.
23. # creating a tensorflow session
24. sess = tensorflow.session()
25.
26. # initializing the tensorflow variables (weights and bias)
27. sess.run(tensorflow.global_variables_initializer())
28.
29. # training data inputs
30. training_inputs_data = [[255, 0, 0],
31.                         [248, 80, 68],
32.                         [0, 0, 255],
33.                         [67, 15, 210]]
34.
35. # training data desired outputs
36. training_outputs_data = [[1],
37.                          [1],
38.                          [0],
39.                          [0]]
40.
41. # training loop of the neural network
42. for step in range(10000):
43.     sess.run(fetches=[train_op], feed_dict={
44.                                    training_inputs: training_inputs_data,
45.                                    training_outputs: training_outputs_data})

46.
47. # class scores of some testing data
48. print("expected scores : ", sess.run(fetches=predictions, feed_dict={trainin
g_inputs: [[248, 80, 68],
            [0, 0, 255]]}))
49.
50. # closing the tensorflow session to free resources
sess.close()

   reading the training data

   the data is read in the previous code in lines 4 and 5 using something
   called placeholder. but what is a placeholder? why we have not just
   used a numpy array for preparing the data? to answer these questions,
   we can explore a simpler example that reads some inputs and print it to
   the console as follows:
1.  import tensorflow
2.
3.  # creating a numpy array holding the input data
4.  numpy_inputs = [[5, 2, 13],
5.                  [7, 9, 0]]
6.
7.  # converting the numpy array to a tensorflow tensor
8.  # convert_to_tensor() doc: https://www.tensorflow.org/api_docs/python/tf/con
vert_to_tensor
9.  training_inputs = tensorflow.convert_to_tensor(value=numpy_inputs, dtype=ten
sorflow.int8)
10.
11. # creating a tensorflow session
12. sess = tensorflow.session()
13.
14. # running the session for evaluating the previously created tensor
15. print("output is : ", sess.run(fetches=training_inputs))
16.
17. # closing the tensorflow session
sess.close()

   the input is read into a numpy array away from tensorflow as in line 4.
   but tensorflow just know tensors and just we have to convert the numpy
   array into a tensor. the tensorflow.convert_to_tensor() tensorflow
   operation does that conversion as in line 9. to be able to print the
   contents of a tensor, we must at first create a session using the
   tensorflow.session() class as in line 12. in line 15, the session runs
   in order evaluate the tensor training_inputs and get its values
   printed. finally, the session got closed in line 18. the result of
   printing is as follows:
output is :  [[ 5  2 13] , [ 7  9  0]]

   this example doesn`t use placeholders. so, what is the use of a
   tensorflow placeholder? assume that we want to run the session with
   another input. to do that, we have to modify the numpy_input python
   variable each time a new input is applied.
 numpy_inputs = [[83, 49, 92],  [31, 78, 60]]

   it is not a good way to modify the code in order to get different
   inputs. a better way for doing that is to just create the tensor and
   then modify its value without modifying it in the code. this is the job
   of the tensorflow placeholder.

   placeholder in tensorflow is a way for accepting the input data. it is
   created in the code and modified multiple times in the session running
   time. the following code modifies the previous code to use
   placeholders:
1.  import tensorflow
2.
3.  # create a placeholder with data type int8 and shape 2x3.
4.  training_inputs = tensorflow.placeholder(dtype=tensorflow.int8, shape=(2, 3)
)
5.
6.  # creating a tensorflow session
7.  sess = tensorflow.session()
8.
9.  # running the session for evaluating assigning a value to the placeholder
10. print("output is : ", sess.run(fetches=training_inputs,
11.                       feed_dict={training_inputs: [[5, 2, 13],
12.                                                    [7, 9, 0]]}))
13.
14. # closing the tensorflow session
15. sess.close()

   this code prints the same outputs as before but it uses a placeholder
   as in line 4. the placeholder is created by specifying the data type
   and the shape of the data it will accept. the shape can be specified to
   restrict the input data to be of specific size. if no shape specified,
   then different inputs with different shapes can be assigned to the
   placeholder. the placeholder is assigned a value when running the
   session using the feed_dict argument of the run operation. feed_dict is
   a dictionary used to initialize the placeholders.

   but assume there is a feature vector of 50 feature and we have a
   dataset of 100 samples. assume we want to train a model two times with
   different number of samples, say 30 and 40. here the size of the
   training set has one dimension fixed (number of features=number of
   columns) and another dimension (number of rows=number of training
   samples) of variable size. setting its size to 30, then we restrict the
   input to be of size (30, 50) and thus we won`t be able to re-train the
   model with 40 samples. the same holds for using 40 as number of rows.
   the solution is to just set the number of columns but leave the number
   of rows unspecified by setting it to none as follows:
# create a placeholder with data type int8 and shape rx3.
training_inputs = tensorflow.placeholder(dtype=tensorflow.int8, shape=(none, 50)
)

   one benefit of using placeholder is that its value is modified easily.
   you have not to modify the program in order the use different inputs.
   it is like a variable in java, c++, or python but it is not exactly a
   variable in tensorflow. we can run the session multiple times with
   different values for the placeholder:
# running the session for evaluating assigning a value to the placeholder
print("output is : ", sess.run(fetches=training_inputs,
                               feed_dict={training_inputs: [[5, 2, 13],
                                                            [7, 9, 0]]}))
print("output is : ", sess.run(fetches=training_inputs,
                               feed_dict={training_inputs: [[1, 2, 3],
                                                            [4, 5, 6]]}))
print("output is : ", sess.run(fetches=training_inputs,
                               feed_dict={training_inputs: [[12, 13, 14],
                                                            [15, 16, 17]]}))

   to do that using numpy arrays we have to create a new python array for
   each new input we are to run the program with.

   this is why we are using placeholders for feeding the data. for every
   input there should be a separate placeholder. in out neural network,
   there are two inputs which are training inputs and training outputs and
   thus there should be two placeholders one for each as in lines 4 and 5
   in codesample1.
# preparing training data (inputs-outputs)
training_inputs = tensorflow.placeholder(shape=[none, 3], dtype=tensorflow.float
32)
training_outputs = tensorflow.placeholder(shape=[none, 1], dtype=tensorflow.floa
t32) #desired outputs for each input

   note that the size of these placeholders is not fixed to allow variable
   number of training samples to be used with the code unchanged. but both
   placeholders of inputs and outputs training data must have the same
   number of rows. for example, according to our currently presented
   training data, training_inputs should have a shape=(4, 2) and
   training_outputs should be of shape=(4, 1).

   preparing the neural network layers and their parameters

   our example is very simple that has no hidden layers and just there is
   a single neuron in the output layer. so, we are going to explain how to
   create such layer and prepare its parameters and create its neuron's
   activation function which is sigmoid in our case.

   there is a problem in placeholders. placeholder value can`t be changed
   once assigned. after it is given a value then placeholder can be
   regarded a constant. thus it is not the suitable option for trainable
   parameters like ones used in this example (weight and bias). trainable
   parameter is assigned an initial value and that value got changed until
   reaching the best value making the underlying model produce least
   errors.

   for our neural network, there are two trainable parameters which are
   weight and bias. these parameters are not suitable for being stored in
   placeholders as we want to update them until getting their best values.
   this is why there is something in tensorflow called variable.

   a tensorflow variable is very similar to variables in java, c++,
   python, and any language with the concept of variables. you can assign
   an initial value to a tensorflow variable and that value can get
   changed multiple times. to create a tensorflow variable, you must
   specify its data type and shape. the following example shows how to
   create a variable rather replacing the previous example`s placeholder:
1.  import tensorflow
2.
3.  # create a variable with data type int8 and shape 2x3.
4.  training_inputs = tensorflow.variable(initial_value=[[5, 2, 13],
5.                                                       [7, 9, 0]], dtype=tenso
rflow.int8)
6.
7.  # creating a tensorflow session
8.  sess = tensorflow.session()
9.
10. # initialize all variables
11. sess.run(tensorflow.global_variables_initializer())
12.
13. # running the session for evaluating assigning a value to the placeholder
14. print("output is : ", sess.run(fetches=training_inputs))
15.
16. # closing the tensorflow session
sess.close()

   the changes compared to the previous placeholder code are in lines 4,
   11, and 14. the variable is created in line 4 by specifying its initial
   value and data type. the dtype argument is not required but the
   initial_value argument must be specified. the initial_value argument
   specifies both the size and the data type (if dtype is missing). note
   that variable is class but placeholder is an operation.

   line 14 prints the value of the variable. note that the variable won`t
   be initialized until calling the global_variables_initializer()
   operation as in line 11 in order to make the variable actually
   initialized. trying to use the variable without calling this operation
   will return an error.

   in codesample1, there are two variables created for our two parameters:
   weight (line 8) and bias (line 9).
# preparing neural network parameters (weights and bias) using tensorflow variab
les
weights = tensorflow.variable(initial_value=[[.3], [.1], [.8]], dtype=tensorflow
.float32)
bias = tensorflow.variable(initial_value=[[1]], dtype=tensorflow.float32)

   parameters initial values

   note that they both weight and bias has initial values. we are using
   fixed initial values for them. the initial values are set randomly and
   there is no rule used for generating them. you may use any random
   number generation operation in tensorflow for doing that such as
   tensorflow.truncated_normal(). but note that the initial values are
   critical for creating a robust model able to predict the right class
   after being trained. bad initial values for weights and bias of a
   neural network can make its neurons to die. this is why there are many
   techniques used to generate such initial values.

   as a summary, a placeholder is used to store input data that are to be
   initialized once and used multiple times. but variable is used for
   trainable parameters that are to be changed multiple times after being
   initialized.

   activation function

   after preparing all parameters required by the output layer's neuron,
   we are ready for using the activation function as in line 15 of
   samplecode1.
# activation function of the output layer neuron
predictions = tensorflow.nn.sigmoid(af_input)

   our activation function is used to merge all inputs, weights, and bias
   into a single value describing the expected class score of each input.

   normally, there is a weight for each input. each input is multiplied by
   its corresponding weight. we don`t have to make element-by-element
   multiplications and id127 can be a good solution as in
   line 13.
# preparing inputs of the activation function
af_input = tensorflow.matmul(training_inputs, weights) + bias

   just prepare a matrix for inputs and another one for weights and
   multiply these two matrices. the tensorflow.matmul() operation make
   that for you.

   then bias is added to the summation of individual input-weight
   multiplications as in line 13. the result, af_input in our example, is
   then applied to the activation function as in line 15. we are not going
   to create the function manually as it is already supported by
   tensorflow.nn api. there are different types of id180
   and sigmoid is sufficient for our case. in our case, the result
   returned by the sigmoid activation function represents the expected
   class score of the current input.

   prediction error

   evaluating the model is an essential step after it has been trained.
   this is why there is a id168 in line 18 in codesample1.
# measuring the prediction error of the network after being trained
prediction_error = tensorflow.reduce_sum(training_outputs - predictions)

   it is very simple but at least do well for our case. just find the
   difference between each desired output and its corresponding predicted
   output by the model. the goal is to measure how far the predicted
   outputs of the trained neural network from their corresponding desired
   outputs. to find a single value representing the overall error of the
   network, the summation of individual differences is calculated using
   the tensorflow.reduce_sum() operation.

   updating network parameters

   the prediction error of the model may not be zero from the first trial
   and it may be very high. this is why there must be a way for
   automatically updating and optimizing the model parameters to get the
   least possible error. one of the common optimizers is id119
   (gd). gd tries to find the relationship between each parameter and the
   prediction error to know how each parameter affects the error. this is
   by first trying the initial parameters. if they didn`t do well, then gd
   will try to change the parameters values and moving toward the
   direction that minimizes the error. the gd optimizer is applied in our
   example in line 20 to minimize the previously calculated prediction
   error. the learning_rate of the
   tensorflow.train.gradientdescentoptimizer is just a hyper-parameter.
# minimizing the prediction error using id119 optimizer
train_op = tensorflow.train.gradientdescentoptimizer(learning_rate=0.05).minimiz
e(prediction_error)

   training loop

   previously, we prepared the steps to follow from accepting the inputs
   to generating the prediction error of the model. moreover, we described
   how the model parameters are to be updated. the remaining step is to go
   into a loop that updates the parameters automatically.

   there are some work to be done before going into the loop including:
    1. creating the session as in line 24.
    2. initializing all variables as in line 27.

# creating a tensorflow session
sess = tensorflow.session()

# initializing the tensorflow variables (weights and bias)
sess.run(tensorflow.global_variables_initializer())

   after that we can run the training loop as in lines 42 and 43. note
   that the only parameter specified to be fetched is the tensor returned
   by tensorflow.train.gradientdescentoptimizer which is train_op. this is
   because fetching train_op will cause all parameters to be updated. for
   understanding why we requested fetching train_op and no other tensors,
   please read my previous article.
   [7]https://www.linkedin.com/pulse/tensorflow-what-hyperparameters-optim
   ize-ahmed-gad

   the loop will last for 10,000 iterations and at each iteration the gd
   optimizer will generate new values for the parameters that decreases
   the error.
# training loop of the neural network
for step in range(10000):
    sess.run(fetches=[train_op], feed_dict={
                                 training_inputs: training_inputs_data,
                                 training_outputs: training_outputs_data})

   because data are stored into placeholders, then these placeholders must
   be initialized using the feed_dict argument of the
   tensorflow.session.run() operation. the weights and bias placeholders
   are initialized using a previously created numpy arrays as in lines 30
   and 36.

   we can also avoid creating a separate numpy arrays and make assign the
   data to the placeholders within the run() operation as follows:
# training loop of the neural network
for step in range(10000):
    sess.run(fetches=[train_op], feed_dict={
                    training_inputs: [[255, 0, 0],
                                      [248, 80, 68],
                                      [0, 0, 255],
                                      [67, 15, 210]]  ,
                    training_outputs: [[1],
                                       [1],
                                       [0],
                                       [0]]})

   but for code clarity, the numpy arrays are created separately from the
   run() operation.

   testing the trained neural network

   after getting out of the training loop, the neural network will be
   trained and ready for predicting unknown samples. in line 48, two new
   samples were used for testing the network accuracy.
# predicted classes of some testing data
print("expected class scores : ", sess.run(fetches=predictions,
feed_dict={training_inputs: [[255, 100, 50],
                                        [30, 50, 255]]}))

   the expected class scores are as follows:
expected class scores :  [[ 1.]
                          [ 1.]]

   this is not accurate result because it say that the expected class
   score of the two samples is indexed 1 which is the blue class. the
   first sample is of class indexed 0 which is red.

   what is the reason for such weak prediction for such very simple
   example? the reason is the bad use of the initial values for the
   network parameters (weights and bias). we can try using different
   initial values and see how the results get changed. for example, using
   the tensorflow.truncated_normal() operation, we can generate the
   initial values for both weights and bias as follows:
# preparing neural network parameters (weights and bias) using tensorflow variab
les
weights = tensorflow.variable(tensorflow.truncated_normal(shape=[3, 1], dtype=te
nsorflow.float32))
bias = tensorflow.variable(tensorflow.truncated_normal(shape=[1, 1], dtype=tenso
rflow.float32))

   then we can train the network again using the newly used initial values
   and predict the results again. the result of expectation is as follows:
expected class score :  [[  3.23404099e-23]
                         [  1.00000000e+00]]

   the new values for the weights and bias can be printed as follows:
# printing weights initially generated using tf.truncated_normal()
print("weights : ", sess.run(weights))

# printing bias initially generated using tf.truncated_normal()
print("bias : ", sess.run(bias))

   here is the result:
weights :  [[-0.20039153]
            [-0.91293597]
            [ 0.44236434]]

bias :  [[-0.685884]]

   the results enhanced so much and the error is now 0.0 compared to 1.0
   in the previous fixed initial values. this proves the importance of
   initializing the neural networks weights well.

   this is the end for our simple example. next, we will explore another
   example that creates the xor logic gate using a neural network with one
   hidden layer containing two neurons.

   xor logic gate using feed-forward neural network (ffnn)

   the same concepts applied previously will also hold in all neural
   networks. there are some changes such as adding more layers or more
   neurons, changing the type of activation function, or using different
   id168.

   the data used as input are as follows:

   [:0]

   the network architecture to be created using tensorflow as a ffnn with
   one hidden layer containing two neurons is as follows:

   [:0]

   that hidden layer accepts the inputs from the input layer. based on its
   weights and biases, its two id180 will produce two
   outputs. the outputs of the hidden layer will be regarded the inputs to
   the output layer which produces the final expected class scores of the
   input data.
1.  import tensorflow
2.  import numpy
3.
4.  # preparing training data (inputs-outputs)
5.  training_inputs = tensorflow.placeholder(shape=[none, 2], dtype=tensorflow.f
loat32)
6.  training_outputs = tensorflow.placeholder(shape=[none, 1], dtype=tensorflow.
float32) #desired outputs for each input
7.
8.  """
9.  hidden layer with two neurons
10. """
11.
12. # preparing neural network parameters (weights and bias) using tensorflow va
riables
13. weights_hidden = tensorflow.variable(tensorflow.truncated_normal(shape=[2, 2
], dtype=tensorflow.float32))
14. bias_hidden = tensorflow.variable(tensorflow.truncated_normal(shape=[1, 2],
dtype=tensorflow.float32))
15.
16. # preparing inputs of the activation function
17. af_input_hidden = tensorflow.matmul(training_inputs, weights_hidden) + bias_
hidden
18.
19. # activation function of the output layer neuron
20. hidden_layer_output = tensorflow.nn.sigmoid(af_input_hidden)
21.
22.
23. """
24. output layer with one neuron
25. """
26.
27. # preparing neural network parameters (weights and bias) using tensorflow va
riables
28. weights_output = tensorflow.variable(tensorflow.truncated_normal(shape=[2, 1
], dtype=tensorflow.float32))
29. bias_output = tensorflow.variable(tensorflow.truncated_normal(shape=[1, 1],
dtype=tensorflow.float32))
30.
31. # preparing inputs of the activation function
32. af_input_output = tensorflow.matmul(hidden_layer_output, weights_output) + b
ias_output
33.
34. # activation function of the output layer neuron
35. predictions = tensorflow.nn.sigmoid(af_input_output)
36.
37.
38. #-----------------------------------
39.
40. # measuring the prediction error of the network after being trained
41. prediction_error = 0.5 * tensorflow.reduce_sum(tensorflow.subtract(predictio
ns, training_outputs) * tensorflow.subtract(predictions, training_inputs))
42.
43. # minimizing the prediction error using id119 optimizer
44. train_op = tensorflow.train.gradientdescentoptimizer(0.05).minimize(predicti
on_error)
45.
46. # creating a tensorflow session
47. sess = tensorflow.session()
48.
49. # initializing the tensorflow variables (weights and bias)
50. sess.run(tensorflow.global_variables_initializer())
51.
52. # training data inputs
53. training_inputs_data = [[1.0, 0.0],
54.                         [1.0, 1.0],
55.                         [0.0, 1.0],
56.                         [0.0, 0.0]]
57.
58. # training data desired outputs
59. training_outputs_data = [[1.0],
60.                         [1.0],
61.                         [0.0],
62.                         [0.0]]
63.
64. # training loop of the neural network
65. for step in range(10000):
66.     op, err, p = sess.run(fetches=[train_op, prediction_error, predictions],

67.                           feed_dict={training_inputs: training_inputs_data,

68.                                      training_outputs: training_outputs_data
})
69.     print(str(step), ": ", err)
70.
71. # class scores of some testing data
72. print("expected class scroes : ", sess.run(predictions, feed_dict={training_
inputs: training_inputs_data}))
73.
74. # printing hidden layer weights initially generated using tf.truncated_norma
l()
75. print("hidden layer initial weights : ", sess.run(weights_hidden))
76.
77. # printing hidden layer bias initially generated using tf.truncated_normal()

78. print("hidden layer initial weights : ", sess.run(bias_hidden))
79.
80. # printing output layer weights initially generated using tf.truncated_norma
l()
81. print("output layer initial weights : ", sess.run(weights_output))
82.
83. # printing output layer bias initially generated using tf.truncated_normal()

84. print("output layer initial weights : ", sess.run(bias_output))
85.
86. # closing the tensorflow session to free resources
87. sess.close()

   here is the expected class scores of the test data and the weights and
   biases for both hidden and output layer.
expected class scores :  [[ 0.75373638]
                          [ 0.94796741]
                          [ 0.25110725]
                          [ 0.03870015]]

hidden layer weights :  [[ 1.68877864 -3.25296354]
                    [ 1.36028981 -1.6849252 ]]

hidden layer weights :  [[-1.27290058  2.33101916]]

output layer weights :  [[ 2.42446136]
                    [-5.42509556]]

output layer weights :  [[ 1.20168602]]

   for more information, visit my youtube channel:

   install tensorflow on windows with cpu support:
   https://www.youtube.com/watch?v=msono20mrvu

   into. to deep learning + explaining how to solve xor using neural
   network: https://www.youtube.com/watch?v=ejwdft-2n9k

   simple neural network example in arabic:
   https://www.youtube.com/watch?v=jv2d9f6qpj0

   simple neural network example in english:
   https://www.youtube.com/watch?v=ek3oxe8j1bk

   [8]ahmed gad

[9]ahmed gad

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

   [10]follow

   0 comments
   article-comment__guest-image
   [11]sign in to leave your comment
     __________________________________________________________________

more from ahmed gad

   [12]27 articles
   from y=x to building a complete id158

[13]from y=x to building a complete artificial   

   march 29, 2019

   feature reduction using genetic algorithm with python

[14]feature reduction using genetic algorithm   

   january 29, 2019

   id158s optimization using genetic algorithm with
   python

[15]id158s optimization   

   january 24, 2019

     *    2019
     * [16]about
     * [17]user agreement
     * [18]privacy policy
     * [19]cookie policy
     * [20]copyright policy
     * [21]brand policy
     * [22]manage subscription
     * [23]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   5. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/tensorflow-building-feed-forward-neural-networks-step-by-step-gad&trk=author-info__follow-button
   7. https://www.linkedin.com/pulse/tensorflow-what-hyperparameters-optimize-ahmed-gad
   8. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   9. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
  10. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/tensorflow-building-feed-forward-neural-networks-step-by-step-gad&trk=author-info__follow-button-bottom
  11. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/tensorflow-building-feed-forward-neural-networks-step-by-step-gad&trk=article-reader_leave-comment
  12. https://www.linkedin.com/today/author/ahmedfgad
  13. https://www.linkedin.com/pulse/from-yx-building-complete-artificial-neural-network-ahmed-gad?trk=related_artice_from y=x to building a complete id158_article-card_title
  14. https://www.linkedin.com/pulse/feature-reduction-using-genetic-algorithm-ahmed-gad?trk=related_artice_feature reduction using genetic algorithm with python_article-card_title
  15. https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad?trk=related_artice_id158s optimization using genetic algorithm with python_article-card_title
  16. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  17. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  18. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  19. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  20. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  21. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  22. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  23. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
