   #[1]id136

   [2]id136

                                  [3]id136

   posts on machine learning, statistics, opinions on things i'm reading
   in the space

        [4]home

   october 5, 2017

           gans are broken in more than one way: the numerics of gans

   last year, when i was on a mission to "fix gans" i had a tendency to
   focus only on what the id168 is, and completely disregard the
   issue of how do we actually find a minimum. here is the paper that has
   finally challenged that attitude:
     * mescheder, nowozin, geiger (2017): [5]the numerics of gans

   i reference [6]marr's three layers of analysis a lot, and i enjoy
   thinking about problems at the computational level: what is the
   ultimate goal we do this for? i was convinced gans were broken at this
   level: they were trying to optimize for the wrong thing or seek
   equilibria that don't exist, etc. this is why i enjoyed f-gans,
   wasserstein gans, instance noise, etc, while being mostly dismissive of
   attempts to fix things at the optimization level, like dcgan or
   improved techniques (salimans et al. 2016). to my defense, in most of
   deep learning, the algorithmic level is sorted: stochastic gradient
   descent. you can improve on it, but it's not broken, it doesn't usually
   need fixing.

   but after reading this paper i had a revelation:

     gans are broken at both the computational and algorithmic levels.

   even if we fix the objectives, we don't have algorithmic tools to
   actually find solutions.    

    summary of this post

     * as it is now expected of my posts, i am going to present the paper
       through a slightly different lens
     * i introduce the concept of conservative vs non-conservative vector
       fields, and highlight some of their properties
     * i then explain how consensus optimization proposed by mescheder et
       al can be viewed in this context: it interpolates between a nasty
       non-conservative field and its well-behaved conservative relaxation
     * finally, as it is also expected of my posts, i also highlight an
       important little detail, which was not discussed in the paper: how
       should we do all this in the minibatch setting?

intro: from gans to vector fields

   gans can be thought of as a two-player non-cooperative game. one player
   controls $\theta$ and wants to maximize its payoff $f(\theta, \phi)$,
   the other controls $\phi$ and seeks to maximize $g(\theta,\phi)$. the
   game is at a nash equilibrium when neither player can improve its
   payoff by changing their parameters slightly. now we have to design an
   algorithm to find such a nash equilibrium.

   it seems that there are two separate issues with gans:
    1. computational: no nash equilibrium exists or it may be degenerate
       ([7]arjovsky and bottou 2017, [8]s  nderby et al 2017, [9]blog
       post).
    2. algorithmic: we don't have reliable tools for finding nash
       equilibria (even though we're now pretty good at finding local
       optima)

   [10]mescheder et al (2017) address the second issue, and do it very
   elegantly. to find nash equilibria, our best tool is simultaneous
   gradient ascent, an iterative algorithm defined by the following
   recursion: $$ x_{t+1} \leftarrow x_t + h v(x_t),
   $$ where $x_t = (\theta_t, \phi_t)$ is the state of the game, $h$ is a
   scalar stepsize and $v(x)$ a the following vector field:
   $$ v(x) = \left(\begin{matrix}\frac{\partial}{\partial\theta}f(\theta,
   \phi)\\\frac{\partial}{\partial\phi}g(\theta, \phi)\end{matrix}\right).
   $$

   here is an important observation that may look paradoxical at first: it
   is natural to think of gan training as a special case of neural network
   training, but in fact, it is the other way around:

     simultaneous id119 is a generalization, rather than a
     special case, of id119

non-conservative vector fields

   one key difference between ordinary id119 and simultaneous
   id119 is that while the former finds fixed points of a
   conservative vector field, the latter has to deal with non-conservative
   fields. therefore, i want to spend most of this blog post talking about
   this difference and what these terms mean.

   a vector field on $\mathbb{r}^n$ is simply a function, $v:\mathbb{r}^n
   \rightarrow \mathbb{r}^n$, which takes a vector $x$ and outputs another
   vector $v(x)$ of the same dimensionality.

   a vector field we often work with is the gradient of a scalar function,
   say $v(x) = \frac{\partial}{\partial x} \phi(x)$, where
   $\phi:\mathbb{r}^n \rightarrow \mathbb{r}$ can be a training objective,
   energy or id168. these types of vector fields are very special.
   they are called conservative vector fields, which essentially
   translates to "nothing particularly crazy happens". conservative fields
   and gradients of scalar functions are a one-to-one mapping: if and only
   if a vector field $v$ is conservative, is there a scalar potential
   $\phi$ whose gradient equals $v$.

   another vector field we often encounter in machine learning (but don't
   often think of it as a vector field) is the one defined by an
   autoencoder. an ae, too, takes as input some vector $x$ and returns
   another vector $v(x)$ which is the same size. figure 5 of (alain and
   bengio, 2014) illustrates the vector field of denoising autoencoders
   trained on 2d data pretty nicely:

   a vector field defined by an ae is not necessarily conservative, which
   basically means that weird things can happen. what kind of weird
   things? let's have a look at an extreme example: the constant curl
   vector field $v(\theta,\phi) = (-\phi, \theta)$, a very typical example
   of a non-conservative vector field:

   this vector field arises naturally in the zero-sum game where
   $f(\theta, \phi)=-g(\theta, \phi)=-\theta\phi$. [11]salimans et al.
   (2016, section 3) mention the exact same toy example in the context of
   gans. notice how the vectors point around in circles, and the rotation
   in the field is visually apparent. indeed, if you follow the arrows on
   this vector field (which is what simultaneous id119 does),
   you just end up going in circles, as shown.

   compare this vector field to escher's impossible tower below. the
   servants think they are ascending, or descending, but in reality all
   they do is go around in circles. it is of course impossible to build
   escher's tower as a real 3d object. analogously, it is impossible to
   express the curl vector field as the gradient of a scalar function.

   even though the curl field has an equilibrium at $(\theta, \phi)=(0,
   0)$, simultaneous id119 will never find it, which is bad
   news. while we got used to id119 always converging to a
   local minimum, simultaneous id119 does not generally
   converge to equilibria. it can just get stuck in a cycle, and
   momentum-based variants can even accumulate unbounded momentum and go
   completely berserk.

consensus optimization: taming a non-con vector field

   the solution proposed by mescheder et al is to construct a conservative
   vector field from the original as follows: $-\nabla l(x) :=
   -\frac{\partial}{\partial x}\| v(x) \|_2^2$. this is clearly
   conservative since we defined it as the gradient of a scalar function
   $l$. it is easy to see that this new vector field $-\nabla l$ has the
   same fix points as $v$. below i plotted the conservative vector field
   $-\nabla l$ that corresponds to the curl field above:

   this is great. familiar id119 on $l$ converges to a local
   minimum, which is a fix point of $v$. the problem now is, we have no
   control over what kind of fix point we converge to. we seek positive
   equilibria, but $-\delta l$ can't differentiate between saddle points
   or equilibria, or between negative or positive equilibria. this is
   illustrated on this slightly more bonkers vector field $v(\theta, \phi)
   = (cos(\phi), sin(\theta))$:

   on the left-hand plot i annotated equilibria and saddle points. the
   middle plot shows the conservative relaxation $-\nabla l$ where both
   saddle points and equilibria are turned to local minima.

   so what can we do? we can simply take a linear combination of the
   original $v$ and it's conservative cousin $-\nabla l$. this is what the
   combined (still non-conservative) vector field looks like for the curl
   field (see also the third panel of the figure above):

   by combining the two fields, we may end up with a slightly
   better-behaved, but still non-conservative vector field. one way to
   quantify how well a vector field behaves is to look at eigenvalues of
   its jacobian $v'(x)$. the jacobian is the derivative of the vector
   field, and for conservative fields it is called the hessian, or second
   derivative. unlike the hessian, which is always symmetric, the jacobian
   of non-conservative fields is non-symmetric, and it can have complex
   eigenvalues. for example, the jacobian of the curl field is
   $$ v'(x) = \left[\begin{matrix}0&-1\\1&0\end{matrix}\right],
   $$ whose eigenvalues are entirely imaginary $+i$ and $-i$.

   mesceder et al show that by combining $v$ with $-\nabla l$, the
   eigenvalues of the combined field can be controlled (see paper for
   details), and if we choose a large enough $\gamma$, simultaneous
   id119 will converge to an equilibrium. hooray!

   sadly, as we increase $\gamma$, we also introduce the spurious
   equilibria as before. these are equilibria of $v - \gamma\nabla l$
   which are in fact only saddle points of $v$. so we can't just crank
   $\gamma$ up all the way, we have to find a reasonable middle-ground.
   this is a limitation of the approach, and it is unclear how much of a
   practical limitation it is.

one more thing: stochastic/minibatch variant

   i wanted to mention one more thing which is not discussed in the paper,
   but i think is important for anyone who would like to play with
   consensus optimization: how do we implement this with stochastic
   optimisation and minibatches? although the paper deals with general
   games, in the case of a gan, both $f$ and $g$ are in fact the average
   of some payoff over datapoints, e.g. $f(\theta, \psi) = \mathbb{e}_x
   f(\theta, \psi, x)$. thus, for $l$ we have:

   \begin{align} l(x) &= \left\| \frac{\partial f(\theta, \phi)}{\partial
   \theta} \right\|_2^2 + \left\| \frac{\partial g(\theta, \phi)}{\partial
   \theta} \right\|_2^2 \\
   &= \left\| \mathbb{e}_{x}\frac{\partial f(\theta, \phi, x)}{\partial
   \theta} \right\|_2^2 + \left\| \mathbb{e}_{x}\frac{\partial g(\theta,
   \phi, x)}{\partial \theta} \right\|_2^2 \end{align}

   the problem with this is that it is not immediately trivial how to
   construct an unbiased estimator for this from a minibatch of samples.
   simply taking the norm of the gradient for each datapoint and then
   averaging will give us a biased estimate, an upper bound by jensen's
   inequality. averaging gradients in the minibatch, and then calculating
   the norm is still biased, but it is a slightly tighter upper bound.

   to construct an unbiased estimator, one can use the following identity:
   $$ \mathbb{e}[x]^2 = \mathbb{e}[x^2] - \mathbb{v}[x], $$ where both the
   average norm and the population variance can be estimated in an
   unbiased fashion. i have discussed this with the authors, and i will
   ask them to leave a comment as to how exactly they did this in the
   experiment. they also promised to include more details about this in
   the camera ready version of the paper.

summary

   i really liked this paper, it was a real eye opener. i always thought
   of the id119-like algorithm we use in gans as a special case
   of id119, whereas in fact it is a generalisation. therefore,
   the nice properties of id119 can't be taken for granted. the
   paper proposes a general, and very elegant solution. good job. let's
   hope this will fix gans once and for all.

     * [12]email
     * [13]facebook
     * [14]twitter
     * [15]linkedin
     * tumblr
     * [16]reddit
     * [17]google+
     * pinterest
     * [18]pocket

   please enable javascript to view the [19]comments powered by disqus.

      2019 id136. all rights reserved. powered by [20]ghost. [21]crisp
   theme by [22]kathy qian.

references

   visible links
   1. https://www.id136.vc/rss/
   2. https://www.id136.vc/
   3. https://www.id136.vc/
   4. https://www.id136.vc/
   5. https://arxiv.org/abs/1705.10461
   6. http://blog.shakirm.com/2013/04/marrs-levels-of-analysis/
   7. https://arxiv.org/abs/1701.04862
   8. https://arxiv.org/abs/1610.04490
   9. http://www.id136.vc/instance-noise-a-trick-for-stabilising-gan-training/
  10. https://arxiv.org/abs/1705.10461
  11. https://arxiv.org/abs/1606.03498
  12. mailto:?subject=gans are broken in more than one way: the numerics of gans&body=https://www.id136.vc/my-notes-on-the-numerics-of-gans/
  13. https://www.facebook.com/sharer/sharer.php?u=https://www.id136.vc/my-notes-on-the-numerics-of-gans/
  14. http://twitter.com/home?status=gans are broken in more than one way: the numerics of gans https://www.id136.vc/my-notes-on-the-numerics-of-gans/
  15. http://www.linkedin.com/sharearticle?mini=true&url=https://www.id136.vc/my-notes-on-the-numerics-of-gans/&title=gans are broken in more than one way: the numerics of gans&summary=last year, when i was on a mission to
  16. http://www.reddit.com/submit?url=https://www.id136.vc/my-notes-on-the-numerics-of-gans/
  17. https://plus.google.com/share?url=https://www.id136.vc/my-notes-on-the-numerics-of-gans/
  18. https://getpocket.com/save?url=https://www.id136.vc/my-notes-on-the-numerics-of-gans/
  19. https://disqus.com/?ref_noscript
  20. http://ghost.org/
  21. https://github.com/kathyqian/crisp
  22. http://kathyqian.com/

   hidden links:
  24. https://www.id136.vc/my-notes-on-the-numerics-of-gans/
  25. https://twitter.com/fhuszar
  26. http://linkedin.com/in/username
  27. mailto:you@example.com
  28. https://www.id136.vc/rss
