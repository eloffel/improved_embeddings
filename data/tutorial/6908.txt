   #[1]id136

   [2]id136

                                  [3]id136

   posts on machine learning, statistics, opinions on things i'm reading
   in the space

        [4]home

   september 13, 2016

     how powerful are graph convolutions? (review of kipf & welling, 2016)

   this post is about a paper that has just come out recently on practical
   generalizations of convolutional layers to graphs:
     * thomas n. kipf and max welling (2016) [5]semi-supervised
       classification with id197

   along the way i found this earlier, related paper:
     * defferrard, bresson and vandergheynst (nips 2016) [6]convolutional
       neural networks on graphs with fast localized spectral filtering

   this post is mainly a review of (kipf and welling, 2016). the paper is
   nice to read, and while i like the general idea, i feel like the
   approximations made in the paper are too limiting and severely hurt the
   generality of the models we can build. this post explains why. since
   writing this, thomas also [7]wrote a blog post addressing some of my
   comments, which i highly recommend.

  summary of this post

     * graph convolutions are generalisation of convolutions, and easiest
       to define in spectral domain
     * general fourier transform scales poorly with size of data so we
       need relaxations
     * (kipf and welling) use first order approximation in fourier-domain
       to obtain an efficient linear-time graph-id98s
     * i illustrate here what this first-order approximation amounts to on
       a 2d lattice one would normally use for image processing, where
       actual spatial convolutions are easy to compute
     * in this application the modelling power of the proposed graph
       convolutional networks is severely impoverished, due to the
       first-order and other approximations made.

the general graph-based learning problem

   the graph learning problem is formulated as follows:
     * we are given a set of nodes, each with some observed numeric
       attributes $x_i$.
     * for each node we'd like to predict an output or label $y_i$. we
       observe these labels for some, but not all, of the nodes.
     * we are also given a set of weighted edges, summarised by an
       adjacency matrix $a$. the main assumption is that when predicting
       the output $y_i$ for node $i$, the attributes and connectivity of
       nearby nodes provide useful side information or additional context.

   kipf & welling use graph convolutional neural networks to solve this
   problem. a good way to imagine what's happening is to consider a neural
   network that receives as input features $x_j$ from all nodes $j$ in the
   local neighbourhood around a node $i$, and outputs an estimate of the
   associated label $y_i$. the information from the local neighbourhood
   gets combined over the layers via a concept of graph convolutions.

   the deeper the network, the larger the local neighbourhood - you can
   think of it as the generalisation of the receptive field of a neuron in
   a normal id98. this network is applied convolutionally across the entire
   graph, always receiving features from the relevant neighbourhood around
   each node.

how are convolutions on a graph defined?

   usually, when we talk about convolutions in machine learning we
   consider time series, 2d images, occasionally 3d tensors. you can
   understand these as special cases of the graph learning problem, where
   the graph is a regular line, 2d square or 3d cube lattice with regular
   connectivity between neighbouring pixels or tensor entries.

   these are all examples or regular graphs where all nodes have the same
   neighbourhood connectivity patterns. it's not immediately
   obvious/intuitive how to generalise the concept of convolutions to
   arbitrary graphs that can have nodes with very different local
   connectivity patterns or different degrees.

   to generalise the concept, we need to adopt a spectral view on
   convolutions, and consider what convolutions are in fourier-domain:
   simple pointwise multiplication of the fourier-transform of a signal.
   fourier-transforms do generalise to graphs, therefore we can define a
   general concept of graph convolutions as pointwise multiplication of
   the spectra of signals in fourier-domain.

   this, however, is a very impractical view: computing the spectrum of
   signal over general graphs involves matrix diagonalisation, which
   generally has cubic complexity in the number of nodes. computing exact
   convolutions over graphs is thus computationally intensive.

approximate graph convolutions

   one way to make graph convolutions work is by limiting the convolution
   kernels under consideration. for example, if the conv. kernel can be
   expressed or approximated using chebyshev polynomials of eigenvalues in
   spectral domain, computing the convolution becomes easier. like
   taylor-expansion, this chebyshev approximation can be made arbitrarily
   accurate by making $k$ higher. this is the approach proposed earlier
   this year:
     * defferrard, bresson and vandergheynst (nips 2016) [8]convolutional
       neural networks on graphs with fast localized spectral filtering

   kipf & welling also use use this trick, but go even further and only
   use a $1^{\text{st}}$ order approximation. in the fourier domain, this
   restricts convolutions to kernels whose spectrum is an affine function
   of eigenvalues.

   this makes computations pretty fast and easy, and the authors show
   competitive results on a bunch of graph-learning datasets. but what is
   really the price we pay when we use this first order approximation?
   this is what i'll try to illustrate in this post.

special case: dense prediction in id161

   i think it's intuitive to consider the behaviour of the algorithm in a
   well-studied special case: dense prediction in image processing, such
   as binary segmentation. here
     * the node values $x_i$ are rgb, yuv or for simplicity just grayscale
       pixel intensities
     * the output per node $y_i$ can be a binary or discrete label in
       semantic segmentation or pixel intensities in image denoising or
       stylization
     * the graph is defined over a regular 2d square lattice of
       8-connected pixels as shown below
     * let's assume the graph has 4-way rotational symmetry, so all
       horizontal and vertical edges share the same weight $\alpha$ and
       all diagonal edges have weight $\beta$.
     * let's further assume the lattice is infinitely large just so we
       don't have to worry about what happens at borders

   in this application, the graph convolutional network model from kipf &
   welling equates to a vanilla multilayer convolutional neural network,
   with only convolutional layers, no pooling or anything else. so far,
   this is good news, as these id98s can actually solve a lot of dense
   prediction problems just fine, even though additional operations such
   as pooling-deconvolution generally can help a lot, such as in
   [9]hourglass-shaped models.

   however, the approximation employed in the kipf & welling model
   severely restricts the convolution filters available to us. at all
   levels in this network, the filters are limited to $3\times 3$ in size
   (which in itself is still ok) and are also essentially fixed to be the
   same kernel across all layers and all units in entire network, up to a
   constant multiplier!     

   if we use the two-parameter approximation in eqn. 6, the kernel
   basically becomes a center-surround pattern like the one pictured
   above, mathematically, something like this:

   $$ \left[\matrix{ \theta_1 c_1&&\theta_1 c_2&&\theta_1 c_1\\ \theta_1
   c_2&& \theta_0 + \theta_1 c_3&& \theta_1 c_2\\ \theta_1 c_1&&\theta_1
   c_2&&\theta_1 c_1\\ }\right], $$

   where $c_1$, $c_2$ and $c_3$ are fixed constants that depend on the
   graph weight parameters $\alpha$ and $\beta$ only. the only trainable
   parameters are $\theta_0$ and $\theta_1$, and in the final version the
   authors even further fix $\theta_0 = -\theta_1$ so that the kernel only
   has a single scalar trainable parameter - essentially a scalar
   multiplier. details on how $c_1$, $c_2$ and $c_3$ are computed aside,
   this model is in fact very, very limited.

   imagine trying to solve something like image segmentation with a id98
   where only a particular, fixed centre-surround receptive field is
   allowed - pretty much impossible. i don't think it's even possible to
   learn edge-detectors, or gabor-like filters, or anything sophisticated,
   even with a multilayer network and lots of nonlinearities.

summary

   i'm not saying that the model proposed by kipf & welling cannot work
   well in some practical situations, indeed they show that it achieves
   competitive results in a number of benchmark problems. also, their
   approach applies generally to graphs, of which regular 2d-3d lattices
   are a very unlikely special case, so it is to be expected that models
   developed for a more general problem will have less power.

   however, the 2d lattice example highlights, the generality and
   flexibility of this model is, at least in its present form, seriously
   lower than the id98s we are used to in image processing. id98s are a kind
   of super-weapon, a dependable workhorse when you need to solve pretty
   much any kind of problem. when you read id197 it
   sounds like we now have a similarly powerful megaweapon to deal with
   graphs. unfortunately, this may not be the case yet, but these papers
   certainly point in that very interesting direction.

   from the perspective of generality, the more general treatment of
   [10]defferrard et al (2016) looks more promising, even with the added
   computational burden of including higher order chebyshev polynomials in
   the approximation.

     * [11]email
     * [12]facebook
     * [13]twitter
     * [14]linkedin
     * tumblr
     * [15]reddit
     * [16]google+
     * pinterest
     * [17]pocket

   please enable javascript to view the [18]comments powered by disqus.

      2019 id136. all rights reserved. powered by [19]ghost. [20]crisp
   theme by [21]kathy qian.

references

   visible links
   1. https://www.id136.vc/rss/
   2. https://www.id136.vc/
   3. https://www.id136.vc/
   4. https://www.id136.vc/
   5. http://arxiv.org/abs/1609.02907
   6. https://arxiv.org/abs/1606.09375
   7. http://tkipf.github.io/graph-convolutional-networks/
   8. https://arxiv.org/abs/1606.09375
   9. http://mi.eng.cam.ac.uk/projects/segnet/
  10. https://arxiv.org/abs/1606.09375
  11. mailto:?subject=how powerful are graph convolutions? (review of kipf & welling, 2016)&body=https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/
  12. https://www.facebook.com/sharer/sharer.php?u=https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/
  13. http://twitter.com/home?status=how powerful are graph convolutions? (review of kipf & welling, 2016) https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/
  14. http://www.linkedin.com/sharearticle?mini=true&url=https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/&title=how powerful are graph convolutions? (review of kipf & welling, 2016)&summary=this post is about a paper that has just come out recently on practical generalizations of convolutional layers to graphs: thomas n. kipf and max welling (2016) semi-supervised classification with id197 along the way i found this earlier, related paper: defferrard, bresson and vandergheynst (nips 2016) convolutional neural...
  15. http://www.reddit.com/submit?url=https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/
  16. https://plus.google.com/share?url=https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/
  17. https://getpocket.com/save?url=https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/
  18. https://disqus.com/?ref_noscript
  19. http://ghost.org/
  20. https://github.com/kathyqian/crisp
  21. http://kathyqian.com/

   hidden links:
  23. https://www.id136.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/
  24. https://twitter.com/fhuszar
  25. http://linkedin.com/in/username
  26. mailto:you@example.com
  27. https://www.id136.vc/rss
