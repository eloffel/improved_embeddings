   #[1]id136

   [2]id136

                                  [3]id136

   posts on machine learning, statistics, opinions on things i'm reading
   in the space

        [4]home

   january 12, 2017

     variational id136 using implicit models, part i: bayesian logistic
                                   regression

   this post is part of a series of tutorials on using implicit models for
   variational id136. here's a table of contents so far:
     *          [5]part i (you are here): id136 of single, global variable
       (bayesian id28)
     * [6]part ii: amortised id136 via the prior-contrastive method
       (explaining away demo)
     * [7]part iii: amortised id136 via a joint-contrastive method
       (ali, bigan)
     * [8]part iv: using denoisers instead of discriminators

   i've written a lot here about generative modeling, and in particular
   about the theory of gans. gans provide a new set of tools for dealing
   with a nasty class of generative distributions. i'm going to follow
   shakir and balaji's terminology and call them [9]implicit generative
   models: probabilistic models which
    1. we can sample from easily, and
    2. we can take the derivative of samples with respect to parameters

   such probabilistic models are useful in general, not only for
   generative modelling. here, i'm going to show a simple example of how
   to use them for approximate id136. this is part one of what i hope
   will be a series of posts, starting with the simplest setup. here's a
   table of contents for the whole series:

approximate id136 of a single global variable

   to simplify things, i'm only going to talk about is approximate
   id136 of a single, global hidden variable (rather than one hidden
   variable per observation such as for example id5).
   a simple example i'm going to use is bayesian id28,
   whose graphical model looks like this:

   here, we have a number of observed input values $x_n$ and a global
   parameter $w$. for each input we also observe a binary output label
   $y_n$. our model specifies that this label depends on the corresponding
   observation $x_n$ and the global parameter $w$ in the following way:

   $$ \mathbb{p}(y_n=1 \vert x_n, w) = \phi(w^{t}x_n), $$

   where $\phi$ denotes the logistic sigmoid.

   in this model i would like to perform bayesian id136 of $w$ which
   involves calculating $p(w\vert \mathcal{d})$, where $\mathcal{d} =
   {(x_n y_n), n=1\ldots n}$ denotes the observations. the problem is,
   once you have many observations, this posterior is intractable to
   compute exactly. we would like to approximate it with a simpler
   distribution.

variational id136 and density ratios

   one common way to perform approximate id136 is by variational
   methods. here, one aims to minimise the kl (or other) divergence
   between $q$ and the real posterior $p$. this, unfortunately, is only
   possible exactly if $q$ is simple enough and compatible with the prior
   $p$. this usually severely limits the power and expressivity of
   approximate posteriors to boring, often factorised, gaussians and other
   exponential families. we would like to use something more powerful, and
   in this post, i want to use an implicit probabilistic model $q$ which i
   can sample from. here is how one can do this:

   \begin{align} \operatorname{kl}[q(w),p(w\vert \mathcal{d})] &=
   \mathbb{e}_{w\sim q} \log \frac {q(w)}{p(w\vert \mathcal{d})}\\ &=
   \mathbb{e}_{w\sim q} \log \frac {q(w)}{p(w)} - \mathbb{e}_{w\sim q}
   \log p(\mathcal{d}\vert w) + \log p(\mathcal{d}) \end{align}

   the final term is the marginal likelihood, wihch doesn't depend on the
   parameter so it can be ignored when optimising for $q$. in this
   expression, we have the id203 ratio between the prior $p(w)$ and
   the approximate posterior $q(w)$. we know how to approximately learn
   id203 ratios, for example by using id28. this
   suggests a simple, adversarial-type iterative training algorithm with
   the following losses:

   \begin{align} \mathcal{l}(d; g) &= \mathbb{e}_{w\sim q} \log d(w) +
   \mathbb{e}_{w\sim p} \log (1-d(w))\\ \mathcal{l}(g; d) &=
   \mathbb{e}_{w\sim q} \log \frac{d(w)}{1 - d(w)} - \mathbb{e}_{w\sim q}
   \log p(\mathcal{d}\vert w)\\ &= \mathbb{e}_{z} \log \frac{d(g(z))}{1 -
   d(g(z))} - \mathbb{e}_{z} \log p(\mathcal{d} \vert g(z)) \end{align}

   we iterate two steps: we train the discriminator by minimising
   $\mathcal{l}(d; g)$ keeping $g$ fixed, and then we fix $d$ and take a
   gradient step to minimise $\mathcal{l}(g; d)$.

    does this work?

   i coded this simple toy problem of bayesian id28 up in
   theano/numpy. here is the [10]link to ipython notebook - it's pretty
   poorly organised, but i hope it's readable enough and serves as proof
   of concept. i conditioned on just three observations, manually chosen,
   and two dimensional weight vector $w$. this is what the final results
   look like:

   the left-hand plot shows a heatmap of the true log posterior (up to
   normalisation constant) and 100 samples from the implicit probabilistic
   model $q$ overlaid. on the right-hand plot i show the kernel density
   estimate of $q$ from $n=20000$ samples. we can see that the posterior
   was recovered nicely by $q$, both its mean and it's rough shape. i used
   simple 3-layer mlp with 10-20 hidden units on each layer with relu
   activations to get this, so the model is not super-powerful, nor did i
   spend a lot of time tweaking the optimization, so it is probably
   possible to get even better results.

   let's look at what the discriminator converged to (yes, they all
   actually converge, as this is a very simple toy problem):

   the left-hand plot shows the contours of our estimate of the
   log-density ratio $\log\frac{q}{p}$ extracted from the discriminator by
   taking the inverse sigmoid $\phi^{-1}$. as $q$ converges to the real
   posterior, this log density ratio should converge to the log likelihood
   (shown on the right), up to a constant. the constant difference between
   the left-hand and right-hand plots is an estimate of the log marginal
   likelihood. the left-hand plot also shows samples from the prior in red
   and samples from the approximate posterior $q$ in green.

   note that the discriminator converges to the log-likelihood, without
   seeing any data itself. the discriminator is only trained via the prior
   and $q$, only $q$ has a data-dependent term in its id168.

  a few relevant papers

   this post is not rocket science, and it was only meant to show how the
   gan idea can be used more generally than sampling pretty pictures.
   things like this have of course been used in many papers, here i'm just
   highlighting a few.

   for example, it is very similar to what [11]adversarial autoencoders
   try to do. there, adversarial training is used in an amortized
   id136 setup: instead of one global variable we have hidden
   variables sampled for each observation. in adversarial autoencoders the
   adversarial loss is introduced between the prior and the average
   variational posterior $\tilde{q}(z) = \mathbb{e}_{x\sim p}q(z\vert x)$.
   i leave it as homework for you to figure out whether one can do
   something smarter, or more principled.

   another relevant paper is the one on [12]adversarial message passing by
   theo karaletsos. here, the key observation is that the simple method
   outlined here may not scale very well to high-dimensional joint
   distributions, and in such cases it may be more useful to restrict the
   adversarial training to local computations in a message passing scheme
   on a factor graph.

   similar ideas are also discussed in ([13]donahue et al, 2016, bigans)
   and ([14]dumoulin et al, 2016, adversarially learned id136). as
   dustin pointed out in his comment below, the variational programs in
   ([15]ranganath et al. 2016, operator variational id136) can also be
   thought of as implicit probabilistic models, while stein variational
   id119 method of ([16]liu and wang, 2016) directly optimises
   a set of samples to perform variational id136.

   it is quite possible i'm missing references to other relevant papers,
   too, so please feel free to point these out in the comments.

  criticism

   i can see the following criticisms of the simple method outlined here:
    1. this method behaves too much like noise-contrastive divergence, in
       that the prior $p(w)$ is probably much wider compared to $q(w)$.
       the logistic-regression-based density ratio estimation method works
       best if the two distributions are quite similar to each other. if
       they aren't we probably need a large number of samples from the
       prior to explore the interesting parts of the prior $p$. one could
       think about using the an analytically tractable variational
       baseline as the reference distribution instead of the prior, and
       the performance would probably be better in general.
    2. the method does not exploit any knowledge we might have about the
       prior $p(w)$. it is usually well-behaved and available in an
       analytical form. if the prior is a nice gaussian, why wouldn't we
       just plug in the simple quadratic form to express its log-density,
       why do we have to learn it with an overparametrised neural network?
       this sounds wasteful. i'll address this in a follow-up post and
       suggest an alternative algorithm to fix this.
    3. are neural networks really needed here? this echoes [17]my
       criticism of adversarial autoencoders. adversarial learning is
       really useful when faced with the 'complicated' manifold-like
       distributions such as the distribution of natural images. gans work
       well at least in part because they use convolutional neural
       networks which provides a very strong and useful inductive bias as
       to what these algorithms can learn to do. when applying adversarial
       training to the problem of variational id136, we are now
       working with very different kinds of distributions. they can be
       high-dimensional but in many cases the prior is a simple gaussian,
       or a structured graphical model.

  summary

   gan style algorithms have been developed mainly for generative
   modelling to model distributions of observed data. they can, too, be
   applied to approximate id136, where one uses them to model
   distributions over latent variables. what i showed here is perhaps the
   simplest formulation one could come up with to do this, but it is not
   hard - at least in theory - to generalise this furter to, say,
   amortized approximate id136. i'll try and follow up with a few
   posts on these extensions.

     * [18]email
     * [19]facebook
     * [20]twitter
     * [21]linkedin
     * tumblr
     * [22]reddit
     * [23]google+
     * pinterest
     * [24]pocket

   please enable javascript to view the [25]comments powered by disqus.

      2019 id136. all rights reserved. powered by [26]ghost. [27]crisp
   theme by [28]kathy qian.

references

   visible links
   1. https://www.id136.vc/rss/
   2. https://www.id136.vc/
   3. https://www.id136.vc/
   4. https://www.id136.vc/
   5. http://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
   6. http://www.id136.vc/variational-id136-with-implicit-models-part-ii-amortised-id136-2/
   7. http://www.id136.vc/variational-id136-using-implicit-models-part-iii-joint-contrastive-id136-ali-and-bigan/
   8. http://www.id136.vc/variational-id136-using-implicit-models-part-iv-denoisers-instead-of-discriminators/
   9. https://arxiv.org/abs/1610.03483
  10. https://gist.github.com/fhuszar/a597906e994523a345744dc226f48f2d
  11. http://www.id136.vc/adversarial-autoencoders/
  12. https://arxiv.org/abs/1612.05048
  13. https://arxiv.org/abs/1605.09782
  14. https://arxiv.org/abs/1606.00704
  15. https://arxiv.org/abs/1610.09033
  16. https://arxiv.org/abs/1608.04471
  17. http://www.id136.vc/adversarial-autoencoders/
  18. mailto:?subject=variational id136 using implicit models, part i: bayesian id28&body=https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
  19. https://www.facebook.com/sharer/sharer.php?u=https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
  20. http://twitter.com/home?status=variational id136 using implicit models, part i: bayesian id28 https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
  21. http://www.linkedin.com/sharearticle?mini=true&url=https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/&title=variational id136 using implicit models, part i: bayesian id28&summary=this post is part of a series of tutorials on using implicit models for variational id136. here's a table of contents so far:          part i (you are here): id136 of single, global variable (bayesian id28)  part ii: amortised id136 via the prior-contrastive method (explaining away demo) part iii: amortised...
  22. http://www.reddit.com/submit?url=https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
  23. https://plus.google.com/share?url=https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
  24. https://getpocket.com/save?url=https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
  25. https://disqus.com/?ref_noscript
  26. http://ghost.org/
  27. https://github.com/kathyqian/crisp
  28. http://kathyqian.com/

   hidden links:
  30. https://www.id136.vc/variational-id136-with-implicit-probabilistic-models-part-1-2/
  31. https://twitter.com/fhuszar
  32. http://linkedin.com/in/username
  33. mailto:you@example.com
  34. https://www.id136.vc/rss
