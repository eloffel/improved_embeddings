   #[1]github [2]recent commits to lattice:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]35
     * [35]star [36]365
     * [37]fork [38]61

[39]tensorflow/[40]lattice

   [41]code [42]issues 7 [43]pull requests 1 [44]projects 0 [45]insights
   [46]permalink
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   branch: master
   [48]lattice/[49]g3doc/[50]tutorial/index.md
   [51]find file copy path
   fetching contributors   
   cannot retrieve contributors at this time
   884 lines (717 sloc) 37 kb
   [52]raw [53]blame [54]history
   (button) (button)

tensorflow lattice: lattice modeling in tensorflow

   tensorflow lattice is a library that implements lattice based models
   which are fast-to-evaluate and interpretable (optionally monotonic)
   models, also known as interpolated look-up tables. it includes a
   collection of [55]tensorflow lattice estimators, which you can use like
   any [56]tensorflow estimator, and it also includes lattices and
   piecewise linear calibration as layers that can be composed into custom
   models.

   note that tensorflow lattice is not an official google product.

   [toc]
     __________________________________________________________________

concepts

   this section is a simplified version of the description in
   [57]monotonic calibrated interpolated look-up tables)

lattices

   a lattice is an interpolated look-up table that can approximate
   arbitrary input-output relationships in your data. it overlaps a
   regular grid on your input space, and it learns values for the output
   in the vertices of the grid. for a test point $$x$$, $$f(x)$$ is
   linearly interpolated from the lattice values surrounding $$x$$.

   [58][2d_lattice.png]

   the above simple example is a function with just 2 features, and has 4
   parameters: 0, 0.2, 0.4, and 1, which are the function's values at the
   corners of the input space; the rest of the function is interpolated
   from these parameters.

   the function $$f(x)$$ can capture non-linear interactions between
   features. you can think of the lattice parameters as the height of
   poles set in the ground on a regular grid, and the resulting function
   is like cloth pulled tight against the four poles.

   with $$d$$ features, a regular lattice will have $$2^d$$ parameters. to
   fit a more flexible function, you can specify a finer-grained lattice
   over the feature space. combined with an efficient $$o(d log(d))$$
   interpolation, lattice regression gives you fast evaluation times and
   arbitrarily complex functions.

   lattice regression functions are continuous, and piecewise infinitely
   differentiable, but they are generally not analytically differentiable
   at the lattice vertices themselves. still, they tend to be very smooth.

calibration

   let's say the preceding sample lattice represents a learned user
   happiness with a suggestion of a coffee shop. furthermore, assume the
   following:
     * feature 1 is a baseline coffee price.
     * feature 2 is the distance to a local coffee shop.

   we want our model to learn user happiness with a coffee shop
   suggestion. the distance can be defined from 0km to 30km and baseline
   coffee price can be something from $0 to $20.

   tensorflow lattice models use piecewise linear functions to calibrate
   (or normalize) your input features to the range accepted by the
   lattice: from $$0.0$$ to $$1.0$$ in the example lattice above.

   the following diagrams show examples of what could be the calibration
   of the price and baseline coffee price using 10 keypoints each:
   [59][pwl_calibration_distance.png] [60][pwl_calibration_price.png]

   all tensorflow lattice pre-made models (estimator's) use calibration of
   the features: the input (the $$x$$ axis of the plot above) is set to
   the quantiles (so data will be +/- evenly distributed on the
   keypoints), and the output ($$y$$ axis) is learned along with the
   lattice(s).

   notice that the calibration also handles the negative correlation of
   distance and user happiness.

ensembles

   if you have $$d$$ features in a lattice, the number of parameters
   (vertices) of the lattice will be at least $$2^d$$. (to be precise
   replace 2s with the size of the grid for each feature.) as you can see
   lattices don't scale well with the number of features.

   tensorflow lattice offers ensembles of lattices to overcome this
   limitation. that is, several "tiny" lattices are combined (summed),
   enabling the model to grow linearly on the number of features, albeit
   exponential on the number of features in each of these "tiny" lattices,
   but the number of features per lattice are typically configured to be
   small.

   the library provides two variations of these ensembles:
     * random tiny lattices (rtl for short): an arbitrary number of
       lattices of dimension $$d_l$$, each including random $$d_l$$
       features out of the total $$d$$ input features.
     * ensembled tiny lattices (etl for short): as with rtls, an arbitrary
       number of lattices of dimension $$d_l$$ is selected, but the input
       for these lattices are linear combinations (initialized randomly)
       of all the $$d$$ inputs. it is more flexible than rtl, but less
       interpretable and may take longer to train.
     __________________________________________________________________

why tensorflow lattice ?

   you can find a brief introduction to tensorflow lattice in [61]google's
   research blog post.
     * interpretability: the parameters of the model are the output at the
       vertices.
     * powerful: abritrarily complex functions with fast evaluation times
       (in comparison to some equivalent deep neural networks for
       instance).

   as shown in the following figure, in real world usage, the training
   data is often a somewhat biased representation of where the model will
   be applied:

   [62][data_dist.png]

   tensorflow lattice provides the following types of "semantic
   id173":
     * lattice resolution: the number of vertices in your lattice allows
       control over the flexibility of the functions that can be learned.
     * monotonicity: you can specify that the output should only
       increase/decrease with respect to an input. in our example, you may
       want to specify that increased distance to a coffee shop should
       only decrease the chances of the coffee shop being a good one. (see
       the illustration below.)
     * graph laplacian: outputs of the lattice/calibration
       vertices/keypoints are regularized torwards the values of their
       respective neighbors. so corners (vertices) of the space that sees
       less training data will fit snugly with the neighbors.
     * torsion: outputs of the lattice will be regularized towards
       preventing torsion among the features. in other words, the model
       will be regularized towards the contributions of the features being
       independent of each other.

   [63][mono_1_of_4.png] [64][mono_2_of_4.png]
   [65][mono_3_of_4.png] [66][mono_4_of_4.png]
     __________________________________________________________________

tensorflow lattice estimators walk-through

   tensorflow lattice library provides generic models formatted as
   pre-made [67]estimators, which we hope will cover the typical use
   cases, or serve as example for those creating their own models.

   this section provides a walk-through of how to use the pre-made
   estimators to train a classifier of [68]census income dataset using
   tensorflow lattice. the full code used in this section, which includes
   some more details, is in [69]examples/uci_census.py.

   if you have trouble with the 'tf.estimator' interface, consider going
   over the [70]tensorflow linear model tutorial. all of the data parsing
   and formatting is very similar.

uci census income dataset

   for this walk-through, we will use the [71]uci census income dataset.
   you can download the csv train and test files directly from these
   links:
     * [72]adult.data
     * [73]adult.test

   please save the datasets into a temporary directory (for example,
   /tmp/uci_census) and change the --test and --train flags to point to
   the files when running the code that follows.

   the data is available as csv, and we use [74]pandas data analysis
   library (pip install pandas on most platforms, maybe requiring sudo) to
   make the parsing easy.

   the tf.estimator models use an input builder function, usually named
   input_fn which is reponsible to parse data and convert into tf.tensors
   (or tf.sparsetensors) with batches of data.

   our input_fn functions look like the following:
import pandas as pd
import tensorflow as tf
import tensorflow_lattice as tfl

flags = tf.flags
flags = flags.flags

flags.define_string("test", "/tmp/uci_census/adult.test", "path to test file.")
flags.define_string("train", "/tmp/uci_census/adult.data", "path to train file."
)

csv_columns = [
    "age", "workclass", "fnlwgt", "education", "education_num",
    "marital_status", "occupation", "relationship", "race", "gender",
    "capital_gain", "capital_loss", "hours_per_week", "native_country",
    "income_bracket"
]

def get_test_input_fn(batch_size, num_epochs, shuffle):
  return get_input_fn(flags.test, batch_size, num_epochs, shuffle)


def get_train_input_fn(batch_size, num_epochs, shuffle):
  return get_input_fn(flags.train, batch_size, num_epochs, shuffle)


def get_input_fn(file_path, batch_size, num_epochs, shuffle):
  df_data = pd.read_csv(
      tf.gfile.open(file_path),
      names=csv_columns,
      skipinitialspace=true,
      engine="python",
      skiprows=1)
  df_data = df_data.dropna(how="any", axis=0)
  labels = df_data["income_bracket"].apply(lambda x: ">50k" in x).astype(int)
  return tf.estimator.inputs.pandas_input_fn(
      x=df_data,
      y=labels,
      batch_size=batch_size,
      shuffle=shuffle,
      num_epochs=num_epochs,
      num_threads=1)

preparing featurecolumns

   tensorflow provides featurecolumns as a way to select and describe the
   features used for a model. numeric features require no transformations;
   we need to list the known valid values of categorical features.

   see more details in [75]tensorflow linear model tutorial.

   tensorflow lattice pre-made estimators will take any of the currently
   supported featurecolumns or alternatively the raw columns coming from
   the input_fn function, if they are properly numeric already.
def get_feature_columns():
  # categorical features.
  gender =
      tf.feature_column.categorical_column_with_vocabulary_list(
          "gender", ["female", "male"])
  education =
      tf.feature_column.categorical_column_with_vocabulary_list(
          "education", [
              "bachelors", "hs-grad", "11th", "masters", "9th", "some-college",
              "assoc-acdm", "assoc-voc", "7th-8th", "doctorate", "prof-school",
              "5th-6th", "10th", "1st-4th", "preschool", "12th"
          ])
     
  # numerical (continuous) base columns.
  age = tf.feature_column.numeric_column("age")
  education_num = tf.feature_column.numeric_column("education_num")
  capital_gain = tf.feature_column.numeric_column("capital_gain")
     
  return [
      age,
      workclass,
      education,
      education_num,
      marital_status,
      occupation,
      relationship,
      race,
      gender,
      capital_gain,
      capital_loss,
      hours_per_week,
      native_country,
  ]

   note: unlike dnn pre-made estimators ([76]dnnclassifier and
   [77]dnnregressor), tensorflow lattice pre-made estimators accept sparse
   featurecolumn without the need for embedding them.

calibration: saving the quantiles

   tensorflow lattice requires proper calibration of the input for its
   lattices (see section on [78]calibration above).

   the current default calibration algorithm requires quantiles
   information about the data on which it's going to train. this can be
   done as a simple pre-processing step.

   the following code snippet from our example does that:
import tensorflow_lattice as tfl

flags.define_bool("create_quantiles", false,
                  "run once to create histogram of features for calibration.")
flags.define_string(
    "quantiles_dir", none,
    "directory where to store quantile information, defaults to the model "
    "directory (set by --output-dir) but since quantiles can be reused by "
    "models with different parameters, you may want to have a separate "
    "directory.")
   

def create_quantiles(quantiles_dir):
  """creates quantiles directory if it doesn't yet exist."""
  batch_size = 10000
  input_fn = get_test_input_fn(
      batch_size=batch_size, num_epochs=1, shuffle=false)
  # reads until input is exhausted, 10000 at a time.
  tfl.save_quantiles_for_keypoints(
      input_fn=input_fn,
      save_dir=quantiles_dir,
      feature_columns=create_feature_columns(),
      num_steps=none)

def main(argv)
     

  # create quantiles if required.
  if flags.create_quantiles:
    if flags.run != "train":
      raise valueerror(
          "can not create_quantiles for mode --run='{}'".format(flags.run))
    create_quantiles(quantiles_dir)

   note: this only needs to be run once per dataset, and can be shared
   among different models that use the same data.

   note: this information is only needed for training. during id136
   (production), the model itself already contains all the information it
   needs, and doesn't need to read this anymore.

   advanced: if you know the range of input, instead of using quantiles,
   you can provide [79]uniform_keypoints_for_signal as function
   initializer, which will create calibration keypoints uniformly in the
   given range. or you can provide your own keypoint initializing
   function.

calibrated linear model

   calibrated linear model is the simplest model type offered in
   tensorflow lattice. it calibrates the input using piecewise-linear
   calibrated functions and then linearly combine the inputs. using it is
   trivial, if you are used to tensorflow's estimator framework (see
   [80]module tf.estimator).

   to create a calibrated linear model, you need to specify features in
   feature_columns, the model directory in model_dir, the [81]"run
   configuration" in config, and in hparams the hyperparameters settings
   in the form of a [82]tfl.calibratedlinearhparams object.

   all parameters are optional; see more details in:
     * [83]tfl.calibrated_linear_classifier
     * [84]tfl.calibrated_linear_regressor
     * configurable hyperparameters in [85]tfl.calibratedlinearhparams

   calibration can be forced to be monotonic and regularized in different
   ways. it also supports special casing of missing values (see
   missing_input_value hyperparameter); that is, the calibration of
   missing values has its own parameter that is learned independently from
   other values.

   an example of code that stitches this together is presented below. for
   now we present only the default hyperparameters. the next section
   covers the special tensorflow lattice hyperparameters, and how to
   change them.

   the following code shows how our create_calibrated_linear function gets
   called. it hinges on creating an estimator object, and then either
   training or evaluating it.
import tensorflow_lattice as tfl

def create_calibrated_linear(feature_columns, config, quantiles_dir):
  feature_names = [fc.name for fc in feature_columns]
  hparams = tfl.calibratedlinearhparams(feature_names=feature_names)
  return tfl.calibrated_linear_classifier(
      feature_columns=feature_columns,
      model_dir=config.model_dir,
      config=config,
      hparams=hparams,
      quantiles_dir=quantiles_dir)
   

def create_estimator(config, quantiles_dir):
  """creates estimator for given configuration based on --model_type."""
  feature_columns = create_feature_columns()
  if flags.model_type == "calibrated_linear":
    return create_calibrated_linear(feature_columns, config, quantiles_dir)
  elif flags.model_type == "calibrated_lattice":
    return create_calibrated_lattice(feature_columns, config, quantiles_dir)
  elif flags.model_type == "calibrated_rtl":
    return create_calibrated_rtl(feature_columns, config, quantiles_dir)
  elif flags.model_type == "calibrated_etl":
    return create_calibrated_etl(feature_columns, config, quantiles_dir)
  elif flags.model_type == "calibrated_dnn":
    return create_calibrated_dnn(feature_columns, config, quantiles_dir)

  raise valueerror("unknown model_type={}".format(flags.model_type))
     

def main(args):
     
  # create config and then model.
  config = tf.estimator.runconfig().replace(model_dir=output_dir)
  estimator = create_estimator(config, quantiles_dir)

  if flags.run == "train":
    train(estimator)

  elif flags.run == "evaluate":
    evaluate(estimator)

  else:
    raise valueerror("unknonw --run={}".format(flags.run))

hyperparameters setting

   each of the pre-made estimators offered by tensorflow lattices is
   controlled by a set of hyperparameters. some are shared among different
   estimators, some are unique. all are documented in their definition.
     * calibrated linear models: [86]tfl.calibratedlinearhparams
     * calibrated lattice models: [87]tfl.calibratedlatticehparams
     * calibrated rtl models: [88]tfl.calibratedrtlhparams
     * calibrated etl models: [89]tfl.calibratedetlhparams

   tensorflow lattices' hyperparameters classes are slightly different
   from [90]tensorflow standard hyperparameters class in that they accept
   global and per-feature parameters. for instance, in our calibrated
   linear model on the previous section, we defined the following default
   values:
def create_calibrated_linear(feature_columns, config, quantiles_dir):
  feature_names = [fc.name for fc in feature_columns]
  hparams = tfl.calibratedlinearhparams(
      feature_names=feature_names, num_keypoints=200, learning_rate=0.1)
  hparams.set_feature_param("capital_gain", "calibration_l2_laplacian_reg",
                            4.0e-8)
  hparams.parse(flags.hparams)
  _pprint_hparams(hparams)
  return tfl.calibrated_linear_classifier(
      feature_columns=feature_columns,
      model_dir=config.model_dir,
      config=config,
      hparams=hparams,
      quantiles_dir=quantiles_dir)

   the preceding code uses different default values for the following
   hyperparameters:
     * num_keypoints
     * learning_rate
     * calibration_l2_laplacian_reg (l2 laplacian id173 for the
       calibration) for the feature named "capital_gain"

   notice also the call hparams.parse(flags.hparams), which will parse a
   string with a comma-separated list of settings. feature specific values
   can also be set by prefixing the parameter (that takes feature specific
   values) with "feature__<feature_name>__<param_name>". notice that the
   separator here is double underscores ("__").

   for example, the following invocation sets learning_rate=0.001 and for
   feature capital_loss it sets calibration_l2_laplacian_reg=1.0e-5:
$ uci_census.py     --hparams=learning_rate=0.001,feature__capital_loss__calibrati
on_l2_laplacian_reg=1.0e-5    

   we define this simple pretty-print function in our example:
def _pprint_hparams(hparams):
  """pretty-print hparams."""
  print("* hparams=[")
  for (key, value) in sorted(six.iteritems(hparams.values())):
    print("\t{}={}".format(key, value))
  print("]")

calibrated lattice model

   calibrated lattice models first calibrate the input with
   piecewise-linear functions, and combine them into a lattice (see
   [91]concepts section).

   calibrated lattice models also provide:
     * enforced monotonicity: in the calibration (can be increasing or
       decreasing), and in the lattice (you must also set the calibration
       to be monotonic and enable lattice monotonicity). both can be
       selected per feature.
     * missing value handle: missing values can be calibrated
       automatically for some special value or can have their own value in
       the lattice. controlled per feature through the parameters:
       missing_input_value and missing_vertex.
     * semantic id173: rich set of id173 that can be
       applied independently to the calibration and lattice. can be set
       globally or per feature. see their description in the [92]concepts
       section.
     * flexible size: lattice can easily be adjusted to different
       granularity per feature by setting lattice_size. this allows it
       lots of power to aproximate any function.

   limitations:
     * scalability issues on number of features and lattice size: the
       total number of vertices (parameters) in the lattice is the product
       of the lattice_size for each feature. your models are gated by
       available memory and parameters update speed. to stay within
       reasonable bounds, don't use more than 14 features (or 50,000
       parameters). if that isn't possible, use the more powerful
       [93]random tiny lattices model or [94]embedded tiny lattices model.

   calibrated lattice models are available as classifier or regressor by
   [95]tfl.calibrated_lattice_classifier and
   [96]tfl.calibrated_lattice_regressor constructors.

   documentation on all hyperparameters is in
   [97]tfl.calibratedlatticehparams

   extract from the [98]uci_census example:
def create_calibrated_lattice(feature_columns, config, quantiles_dir):
  feature_names = [fc.name for fc in feature_columns]
  hparams = tfl.calibratedlatticehparams(
      feature_names=feature_names,
      num_keypoints=200,
      lattice_l2_laplacian_reg=5.0e-3,
      lattice_l2_torsion_reg=1.0e-4,
      learning_rate=0.1,
      lattice_size=2)
  hparams.parse(flags.hparams)
  _pprint_hparams(hparams)
  return tfl.calibrated_lattice_classifier(
      feature_columns=feature_columns,
      model_dir=config.model_dir,
      config=config,
      hparams=hparams,
      quantiles_dir=quantiles_dir)

   note: to see how this function gets called from main, see
   [99]calibrated linear model.

random tiny lattices model

   calibrated "random tiny lattices" (rtl) models, like calibrated lattice
   models, first calibrate the input with piecewise-linear functions. but
   then it combines them in an ensemble of num_lattices lattices built
   with inputs from random features (lattice_rank input features per
   lattice).

   extract from the [100]uci_census example:
def create_calibrated_rtl(feature_columns, config, quantiles_dir):
  feature_names = [fc.name for fc in feature_columns]
  hparams = tfl.calibratedrtlhparams(
      feature_names=feature_names,
      num_keypoints=200,
      learning_rate=0.02,
      lattice_l2_laplacian_reg=5.0e-4,
      lattice_l2_torsion_reg=1.0e-4,
      lattice_size=3,
      lattice_rank=4,
      num_lattices=100)
  # specific feature parameters.
  hparams.set_feature_param("capital_gain", "lattice_size", 8)
  hparams.set_feature_param("native_country", "lattice_size", 8)
  hparams.set_feature_param("marital_status", "lattice_size", 4)
  hparams.set_feature_param("age", "lattice_size", 8)
  hparams.parse(flags.hparams)
  _pprint_hparams(hparams)
  return tfl.calibrated_rtl_classifier(
      feature_columns=feature_columns,
      model_dir=config.model_dir,
      config=config,
      hparams=hparams,
      quantiles_dir=quantiles_dir)

   note: to see how this function gets called from main, see
   [101]calibrated linear model.

   in this example it will calibrate the inputs (using up to 200
   keypoints, per num_keypoints) and then randomly distribute them into
   100 lattices (num_lattices, a feature can be used by more than one
   lattice).

   the lattices and the calibration are all trained jointly.

   like with calibrated lattice models, but without the limitations on the
   number of features, it supports:
     * enforced monotonicity: in the calibration (can be increasing or
       decreasing), and in the lattice (one must also set the calibration
       to be monotonic, and enable lattice monotonicity). both can be
       selected per feature.
     * missing value handle: missing values can be calibrated
       automatically for some special value, or can have their own value
       in the lattice. controlled per feature through the parameters:
       missing_input_value and missing_vertex.
     * semantic id173: rich set of id173 that can be
       applied independently to the calibration and lattice. can be set
       globally or per feature. see their description in the [102]concepts
       section.
     * flexible size: lattice can easily be adjusted to different
       granularity per feature by setting lattice_size. this allows it
       lots of power to aproximate any function.

   note: the lattice_rank hyperparameter controls how many features are
   seen in combination. it is often used as a id173 on the
   complexity of interactions allowed among the features. but as with
   calibrated lattices this is limited to 10 or 20 features at most
   combined in the same lattices. if you wonder if the model could pick
   better than random features to be combined in lattices, check out the
   next session, on [103]embedded tiny lattices model

   calibrated rtl models are available as classifier or regressor by
   [104]tfl.calibrated_rtl_classifier and
   [105]tfl.calibrated_rtl_regressor constructors.

   documentation on all hyperparameters in
   [106]tfl.calibratedlatticehparams

   note: see above in section [107]calibrated linear model on how this
   function gets called from main.

embedded tiny lattices model

   calibrated "embedded tiny lattices" (etl) models, like calibrated
   [108]rtl models, first calibrate the input and connect those calibrated
   signals into an ensemble of lattices. but as opposed to have each
   lattice take as input a subset of the calibrated features, in etl
   models it takes as input an embedding of the input features: each input
   is a linear combination of the calibrated features.

   the number of lattices is defined by 'monotonic_num_lattices' and
   'non_monotonic_num_lattices': monotonic lattices can take as input
   monotonic features and non-monotonic features. non-monotonic lattices
   can only take non-monotonic features as input (otherwise monotonicity
   could be broken).

   the size of the embedding to be used in each lattice is given by
   monotonic_lattice_rank and non_monotonic_lattice_rank. each lattice has
   it's own embedding: calibration, embedding and lattices are trained
   jointly.

   the size of the lattices, which gives resolution for them is given by
   monotonic_lattice_size and non_monotonic_lattice_size.

   calibrated etl models are available as classifier or regressor by
   [109]tfl.calibrated_etl_classifier and
   [110]tfl.calibrated_etl_regressor constructors.

   embedded tiny lattices can be more powerful than [111]rtl models, but
   they sacrifice some of the "semantic id173" (same
   id173 options are available, but they apply to abstract
   embeddings), and are slower to train. monotonicity still is well
   supported.

   see details in paper [112]deep lattice networks and partial monotonic
   functions. etl implements only one layer deep lattice models, but
   deeper models can be built by composing lattice layers, in the
   [113]next session

   in our example [114]uci_census model, using only non-monotonic signals:
def create_calibrated_etl(feature_columns, config, quantiles_dir):
  # no enforced monotonicity in this example.
  feature_names = [fc.name for fc in feature_columns]
  hparams = tfl.calibratedetlhparams(
      feature_names=feature_names,
      num_keypoints=200,
      learning_rate=0.02,
      non_monotonic_num_lattices=200,
      non_monotonic_lattice_rank=2,
      non_monotonic_lattice_size=2,
      calibration_l2_laplacian_reg=4.0e-3,
      lattice_l2_laplacian_reg=1.0e-5,
      lattice_l2_torsion_reg=4.0e-4)
  hparams.parse(flags.hparams)
  _pprint_hparams(hparams)
  return tfl.calibrated_etl_classifier(
      feature_columns=feature_columns,
      model_dir=config.model_dir,
      config=config,
      hparams=hparams,
      quantiles_dir=quantiles_dir)

   note: to see how this function gets called from main, see
   [115]calibrated linear model.
     __________________________________________________________________

tensorflow lattice layers

   tensorflow lattice layer components are also provided by the library,
   so users can combine them in more flexible or advanced ways.

   the following are the layer components included in the tensorflow
   lattice library:
     * piecewise-linear calibration:
          + [116]tfl.input_calibration_layer: calibrates the "input",
            provided either as featurecolumns or as a dict of columns to
            tensors, the typical object returned by an input_fn function.
            includes support for monotonicity, id173 and special
            "missing" values.
          + [117]tfl.input_calibration_layer_from_hparams: calibrates the
            "input", provided either as featurecolumns or as a dict of
            columns to tensors, the typical object returned by an input_fn
            function. includes support for monotonicity, id173
            and special "missing" values. this version uses an
            tfl.calibratehparams to specify the hyperparameters.
          + [118]tfl.calibration_layer: calibrates a tensor of shape
            [batch_size, ...]. each element (outside the batch-dimension)
            gets its own calibration. includes support for monotonicity,
            id173 and special "missing" values.
     * lattice layer:
          + [119]tfl.lattice_layer: creates output_dim lattices that uses
            as input a tensor of shape [batch_size, input_dim]. lattice
            size is defined for each dimension of input_dim. the total
            number of parameters is the product of all these lattice sizes
            times output_dim. full support of monotonicity and
            id173.
          + [120]tfl.ensemble_lattices_layer: creates an ensemble of
            lattices connecting inputs as specified by the caller. full
            support of monotonicity and id173.

   example calibrated_dnn, a custom estimator from our example
   [121]uci_census model:
def create_calibrated_dnn(feature_columns, config, quantiles_dir):
  """creates a calibrated dnn model."""
  # this is an example of a hybrid model that uses input calibration layer
  # offered by tensorflow lattice library and connects it to dnn.
  feature_names = [fc.name for fc in feature_columns]
  hparams = tfl.calibratedhparams(
      feature_names=feature_names,
      num_keypoints=200,
      learning_rate=1.0e-3,
      calibration_output_min=-1.0,
      calibration_output_max=1.0,
      nodes_per_layer=10,  # all layers have the same number of nodes.
      layers=2,  # includes output layer, therefore >= 1.
  )
  hparams.parse(flags.hparams)
  _pprint_hparams(hparams)

  def _model_fn(features, labels, mode, params):
    """model construction closure used when creating estimator."""
    del mode
    del params  # they are read directly from the bound variable hparams

    # calibrate: since there is no monotonicity, there are no projection ops.
    # we also discard the ordered names of the features.
    (output, _, _, id173) = tfl.input_calibration_layer_from_hparams(
        features, feature_columns, hparams, quantiles_dir)

    # hidden-layers.
    for _ in range(hparams.layers - 1):
      output = tf.layers.dense(
          inputs=output, units=hparams.nodes_per_layer, activation=tf.sigmoid)

    # classifier logits and prediction.
    logits = tf.layers.dense(inputs=output, units=1)
    predictions = tf.reshape(tf.sigmoid(logits), [-1])

    # notice loss doesn't include id173, which is added separately
    # by means of tf.contrib.layers.apply_id173().
    loss_no_id173 = tf.losses.log_loss(labels, predictions)
    loss = loss_no_id173
    if id173 is not none:
      loss += id173
    optimizer = tf.train.adamoptimizer(learning_rate=hparams.learning_rate)
    train_op = optimizer.minimize(
        loss,
        global_step=tf.train.get_global_step(),
        name="calibrated_dnn_minimize")

    eval_metric_ops = {
        "accuracy": tf.metrics.accuracy(labels, predictions),

        # we want to report the loss without the id173, so metric is
        # comparable with different id173s. futurework, list both.
        "average_loss": tf.metrics.mean(loss_no_id173),
    }

    return tf.estimator.estimatorspec(mode, predictions, loss, train_op,
                                      eval_metric_ops)

  # hyperparameters are passed directly to the model_fn closure by the context.
  return tf.estimator.estimator(
      model_fn=_model_fn,
      model_dir=config.model_dir,
      config=config,
      params=none)

other potential use cases of these components

     * if integrating an embedding from another model (transfer-learning);
     * use tensorflow lattice's calibration in a dnn: works much better
       than gaussian id172 of inputs. something else that has been
       used with some success is the piecewise linear function as an
       activation function.
     * use lattices on the "upper" (closer to output) layers of a dnn, for
       its id173.
     * use the piecewise-linear calibration as an activation function for
       neural networks.
     * use piecewise-linear calibration as a id203 distribution
       function for learning continuous values in a id23
       set up (reinforce algorithm).

papers

     * [122]lattice regression, eric garcia, maya gupta, advances in
       neural information processing systems (nips), 2009
     * [123]optimized regression for efficient function evaluation, eric
       garcia, raman arora, maya r. gupta, ieee transactions on image
       processing, 2012
     * [124]monotonic calibrated interpolated look-up tables, maya gupta,
       andrew cotter, jan pfeifer, konstantin voevodski, kevin canini,
       alexander mangylov, wojciech moczydlowski, alexander van esbroeck,
       journal of machine learning research (jmlr), 2016
     * [125]fast and flexible monotonic functions with ensembles of
       lattices, mahdi milani fard, kevin canini, andrew cotter, jan
       pfeifer, maya gupta, advances in neural information processing
       systems (nips), 2016
     * [126]deep lattice networks and partial monotonic functions, seungil
       you, kevin canini, david ding, jan pfeifer, maya r. gupta, advances
       in neural information processing systems (nips), 2017

   ____________________ (button) go

     *    2019 github, inc.
     * [127]terms
     * [128]privacy
     * [129]security
     * [130]status
     * [131]help

     * [132]contact github
     * [133]pricing
     * [134]api
     * [135]training
     * [136]blog
     * [137]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [138]reload to refresh your
   session. you signed out in another tab or window. [139]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/tensorflow/lattice/commits/master.atom
   3. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/tensorflow/lattice/blob/master/g3doc/tutorial/index.md
  32. https://github.com/join
  33. https://github.com/login?return_to=/tensorflow/lattice
  34. https://github.com/tensorflow/lattice/watchers
  35. https://github.com/login?return_to=/tensorflow/lattice
  36. https://github.com/tensorflow/lattice/stargazers
  37. https://github.com/login?return_to=/tensorflow/lattice
  38. https://github.com/tensorflow/lattice/network/members
  39. https://github.com/tensorflow
  40. https://github.com/tensorflow/lattice
  41. https://github.com/tensorflow/lattice
  42. https://github.com/tensorflow/lattice/issues
  43. https://github.com/tensorflow/lattice/pulls
  44. https://github.com/tensorflow/lattice/projects
  45. https://github.com/tensorflow/lattice/pulse
  46. https://github.com/tensorflow/lattice/blob/0086643fd69d7df724779a2634ee1b2b5e21dbf9/g3doc/tutorial/index.md
  47. https://github.com/join?source=prompt-blob-show
  48. https://github.com/tensorflow/lattice
  49. https://github.com/tensorflow/lattice/tree/master/g3doc
  50. https://github.com/tensorflow/lattice/tree/master/g3doc/tutorial
  51. https://github.com/tensorflow/lattice/find/master
  52. https://github.com/tensorflow/lattice/raw/master/g3doc/tutorial/index.md
  53. https://github.com/tensorflow/lattice/blame/master/g3doc/tutorial/index.md
  54. https://github.com/tensorflow/lattice/commits/master/g3doc/tutorial/index.md
  55. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#tensorflow-lattice-estimators-walk-through
  56. https://www.tensorflow.org/guide/estimators
  57. http://jmlr.org/papers/v17/15-243.html
  58. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/2d_lattice.png
  59. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/pwl_calibration_distance.png
  60. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/pwl_calibration_price.png
  61. https://research.googleblog.com/
  62. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/data_dist.png
  63. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/mono_1_of_4.png
  64. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/mono_2_of_4.png
  65. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/mono_3_of_4.png
  66. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/images/mono_4_of_4.png
  67. https://www.tensorflow.org/guide/estimators
  68. https://archive.ics.uci.edu/ml/datasets/census+income
  69. https://github.com/tensorflow/lattice/blob/master/examples/uci_census.py
  70. https://www.tensorflow.org/tutorials/wide
  71. https://archive.ics.uci.edu/ml/datasets/census+income
  72. https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data
  73. https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test
  74. http://pandas.pydata.org/
  75. https://www.tensorflow.org/tutorials/wide
  76. https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/estimator/dnnclassifier
  77. https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/estimator/dnnclassifier
  78. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibration
  79. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/uniform_keypoints_for_signal.md
  80. https://www.tensorflow.org/api_docs/python/tf/estimator
  81. https://www.tensorflow.org/api_docs/python/tf/estimator/runconfig
  82. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedlinearhparams.md
  83. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_linear_classifier.md
  84. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_linear_regressor.md
  85. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedlinearhparams.md
  86. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedlinearhparams.md
  87. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedlatticehparams.md
  88. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedrtlhparams.md
  89. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedetlhparams.md
  90. https://www.tensorflow.org/api_docs/python/tf/contrib/training/hparams
  91. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#concepts
  92. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#concepts
  93. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#random-tiny-lattices-model
  94. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#embedded-tiny-lattices-model
  95. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_lattice_classifier.md
  96. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_lattice_regressor.md
  97. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedlatticehparams.md
  98. https://github.com/tensorflow/lattice/blob/master/examples/uci_census.py
  99. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibrated-linear-model
 100. https://github.com/tensorflow/lattice/blob/master/examples/uci_census.py
 101. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibrated-linear-model
 102. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#concepts
 103. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#embedded-tiny-lattices
 104. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_rtl_classifier.md
 105. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_rtl_regressor.md
 106. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibratedlatticehparams.md
 107. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibrated-linear-model
 108. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#random-tiny-lattices-model
 109. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_etl_classifier.md
 110. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibrated_etl_regressor.md
 111. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#random-tiny-lattices-model
 112. https://research.google.com/pubs/pub46327.html
 113. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#tensorflow-lattice-layers
 114. https://github.com/tensorflow/lattice/blob/master/examples/uci_census.py
 115. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibrated-linear-model
 116. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/input_calibration_layer.md
 117. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/input_calibration_layer_from_hparams.md
 118. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/calibration_layer.md
 119. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/lattice_layer.md
 120. https://github.com/tensorflow/lattice/blob/master/g3doc/api_docs/python/tensorflow_lattice/ensemble_lattices_layer.md
 121. https://github.com/tensorflow/lattice/blob/master/examples/uci_census.py
 122. https://papers.nips.cc/paper/3694-lattice-regression
 123. http://ieeexplore.ieee.org/document/6203580/
 124. http://jmlr.org/papers/v17/15-243.html
 125. https://papers.nips.cc/paper/6377-fast-and-flexible-monotonic-functions-with-ensembles-of-lattices
 126. https://research.google.com/pubs/pub46327.html
 127. https://github.com/site/terms
 128. https://github.com/site/privacy
 129. https://github.com/security
 130. https://githubstatus.com/
 131. https://help.github.com/
 132. https://github.com/contact
 133. https://github.com/pricing
 134. https://developer.github.com/
 135. https://training.github.com/
 136. https://github.blog/
 137. https://github.com/about
 138. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md
 139. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md

   hidden links:
 141. https://github.com/
 142. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md
 143. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md
 144. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md
 145. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#tensorflow-lattice-lattice-modeling-in-tensorflow
 146. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#concepts
 147. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#lattices
 148. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibration
 149. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#ensembles
 150. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#why-tensorflow-lattice-
 151. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#tensorflow-lattice-estimators-walk-through
 152. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#uci-census-income-dataset
 153. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#preparing-featurecolumns
 154. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibration-saving-the-quantiles
 155. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibrated-linear-model
 156. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#hyperparameters-setting
 157. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#calibrated-lattice-model
 158. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#random-tiny-lattices-model
 159. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#embedded-tiny-lattices-model
 160. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#tensorflow-lattice-layers
 161. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#other-potential-use-cases-of-these-components
 162. https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md#papers
 163. https://github.com/
