[1]antonia antonova

     * [2]blog
          + [3]data science
     * [4]projects
          + [5]text to video generation
          + [6]abstractive text generation
          + [7]the geography of terror
          + [8]diversity in hollywood
          + [9]nyc commuter traffic
     * [10]resume
     * [11]about
     * [12]contact

     * [ ] [13]blog
          + [14]data science
     * [ ] [15]projects
          + [16]text to video generation
          + [17]abstractive text generation
          + [18]the geography of terror
          + [19]diversity in hollywood
          + [20]nyc commuter traffic
     * [21]resume
     * [22]about
     * [23]contact

   text to video generation - can artificial intelligence generate new
   video content from text descriptions? ai-bot.jpg

   text to video generation

   can artificial intelligence generate new video content from text
   descriptions?

   published july 2017

   this project aims to build a deep learning pipeline that takes text
   descriptions and generates unique video depictions of the content
   described.

   the crux of the project lies with the generative adversarial network, a
   deep learning algorithm that pins two neural networks against each
   other in order to produce media that is unique and realistic.

   credit:&nbsp; scott reed credit:&nbsp; scott reed

   credit: [24]scott reed

   this model consists of a generative network and a discriminative
   network. while the generator produces new content, the discriminator
   tries to identify the generator's work from a pool of real and fake
   (aka generated) media. the discriminator produces a "'real" or "fake"
   output label for each piece of content made by the generator. the
   "fake" labels are then treated as errors in the generator's
   back-propagation.

   this adversarial design has been shown to greatly outperform many
   generative models used in the field. as the discriminator gets better
   at distinguishing the computer-generated from the human-generated, the
   generator improves in producing more realistic media.

   perhaps the two largest downsides of using generative adversarial
   networks is that they are both hard to train and hard to evaluate.
   we'll discuss some techniques used to mitigate these challenges in this
   project.
     __________________________________________________________________

dataset

   for training, i used the max-planck institute for informatics (mpii)
   movie description dataset. the dataset includes short movies snippets,
   as well as textual depictions of what is featured in each video. the
   text comes from a audio description service aimed at helping visually
   impaired people better follow a movie. more information on the dataset
   can be found in [25]this published paper.

   i used video-description pairs from 9 romantic comedies with the aim of
   training my algorithm to generate videos of humans in action.
   credit: cvpr paper credit: cvpr paper

   credit: [26]cvpr paper
     __________________________________________________________________

ml pipeline

   to recap, the goal of this project is to input text descriptions into a
   series of ml models that produce a video of said description as output.

   the overall pipeline looks something like this:
     * vectorize and embed the text into latent space
     * use gans to expand the text embeddings into a series of images
     * convert the series into a gif

embedding text descriptions

   screen shot 2017-09-04 at 11.15.53 pm.png screen shot 2017-09-04 at
   11.15.53 pm.png

   find keras code for the variational autoencoder used in the project
   [27]here.

   i first vectorized the text descriptions using facebook's fasttext
   id97. this was done by concatenating the word vectors in each
   sentence. the vast majority of the descriptions in the dataset are 25
   words or less, so i limited the concatenated vector length to 7500
   dimensions (300 dimensional word vectors * 25 words). descriptions that
   were shorter than 25 words had their vectors extended to the 7500
   dimensional size with a padding of zeros.

   i chose to concatenate the word vectors instead of average them in an
   attempt to keep some of the semantic ordering in tact. variational
   autoencoders have been shown to embed entire sentences into latent
   representations quite well, sometimes outperforming lstms ([28]study
   linked here). with this in mind, i ran the description vectors through
   a vae in order to reduce their dimensionality and get more meaningful
   embeddings.
   siamese network.png siamese network.png

   find keras code for the multimodal embedding network used in the
   project [29]here.

   the variational autoencoder works to cluster embeddings with similar
   semantic patterns. however, visualizing that text down the road
   requires a more nuanced embedding framework.

   visualizations of thought tend to bring out a lot of the implicit
   context present in the explicit text. descriptions of birds tend to
   visually elicit tree branches and bird houses. descriptions of kicking
   a ball can lead us to image soccer, green grass, and shorts. so how can
   a model learn to pick up on the implicit meaning of a language? and is
   there any way to help it along the way?

   [30]joint multimodal embedding networks have been shown to provide
   promising results in this direction. they try to cluster lower
   dimensional representations of different media with similar
   subject-matter. i used a siamese network with text and image encoders
   to develop this type of design. the model decreases the euclidian
   distance between embeddings of images and their text descriptions and
   increases the euclidian distance between embeddings of images and
   unrelated text.

   the text encoder in the trained siamese network was then used to create
   the final latent embeddings for each video description. this encoder
   added several fully-connected layers on top of frozen layers from the
   pre-trained vae encoder above.

stacking gans

   once the descriptions are embedded into a lower dimensional space, they
   can be used as inputs in a general adversarial network.

   the gans used in the project were adapted from these two papers:

   [31]text to photo-realistic image synthesis with stacked generative
   adversarial networks

   [32]generating videos with scene dynamics

   the first gan was trained to convert text descriptions into image
   depictions of the text's content. the second gan was trained to take
   those generated images as input and extend them into a series of 32
   frames.
   screen shot 2017-09-04 at 11.31.14 pm.png screen shot 2017-09-04 at
   11.31.14 pm.png

   find tensorflow code for the text-to-image gan used in the
   project [33]here.

   i recreated the study going from "text to photo-realistic image" with
   the code above. the dataset provided allowed the network to learn how
   to generate realistic bird images from detailed descriptions of birds.

   here is a sample of my results. the text descriptions on the left were
   the input that produced the bird images directly to the right of
   them. as you can see, the images coincide with the descriptions quite
   well. the generated birds are also quite diverse.
   screen shot 2017-09-06 at 3.09.12 pm.png screen shot 2017-09-06 at
   3.09.12 pm.png

   the study referenced stacked a second gan on top of the first to
   continue upsampling and thereby converting the low resolution images
   into high-res outputs. training individual epochs of this model took an
   extremely long time, even on high-tier aws instances, so i decided to
   skip this phase when training on my own data. in the future, i look
   forward to fully implementing this step of the process with a slightly
   altered and hopefully quicker high-res producing gan.

   here is an example of the output of the second gan. the images below
   are from the study itself.
   credit: han zhang github credit: han zhang github

   credit: [34]han zhang github

video gan

   the next model i ran took the images generated above as input and
   produced a horizontally long graphic that includes 32 sequential
   frames, one of which will ideally be the input image itself.
   screen shot 2017-09-04 at 11.16.22 pm.png screen shot 2017-09-04 at
   11.16.22 pm.png

   find torch code for the image-to-video gan used in the project
   [35]here.

   this model is able to generate videos on distinct subject matter quite
   well. the branching convolutional layers encourage the model to split
   the input image into it's foreground and background components.
   typically, the majority of the movement in a video occurs in the
   foreground. therefore, the model replicates the static background in
   each frame while combining the moving foreground into the frames using
   a mask.

   the output image is then sliced into its competent frames and made into
   a gif.

   here are examples of train and beach videos produced by the [36]study
   itself.
   train2.gif train2.gif
   train3.gif train3.gif
   train4.gif train4.gif
   train5.gif train5.gif
   train6.gif train6.gif
   train1.gif train1.gif
   beach5.gif beach5.gif
   beach1.gif beach1.gif
   beach6.gif beach6.gif
     __________________________________________________________________

results

   deciding when to stop training a gan can be tricky work. you can check
   the content produced by the generator at different stages in the
   training process, but this is only helpful once relatively realistic
   content begins to show. what do you do beforehand? monitoring the
   generator and discriminator's loss can be an additional method of
   evaluating the gan.

   as mentioned above, a gan's discriminator will typically begin with
   very low accuracy and therefore high loss. because the gan's generator
   also starts out quite horribly, the discriminator will very quickly be
   able to distinguish generated images from real-life ones. as the
   discriminator's error drops, the generator slowly begins to find ways
   to trick the discriminator and reduce its loss as well. typically the
   generator will improve one aspect of its images at a time. this allows
   the discriminator to once again pick up on patterns in the generated
   images, label them as fake, and once again begin increasing the
   generator's loss.

   one typically hopes to stop training the gan when the generator's and
   discriminator's losses begin to close in on each other and stabilize.
   this will ideally happen at the second local minima of the generator's
   loss plot.

   below is a screenshot from my loss plot midway through my training
   process.

   as you can see the generator's loss drops initially and then begins to
   curve upward as the discriminator picks up on its antics. following the
   6000th training step and not visualized here, the loss began to drop
   again.

   here is a sample of images produced by the first generator. as you can
   see they look like various frames from snippets of a romantic comedy.

   you can also see that the generator repeats images at different points.
   those generated frames must have fooled the discriminator better than
   the rest, risking stagnation in the training process.
   screen shot 2017-09-05 at 12.03.13 am.png screen shot 2017-09-05 at
   12.03.13 am.png

   here is a sample of videos produced by the second generator. the first
   picks up on a couple dancing or hugging, while the second seems to form
   a boxy humanoid form interestingly not found in the movies themselves.
   ezgif.com-crop (6).gif ezgif.com-crop (6).gif

   while individual photos and videos from the gans created interesting
   content reminiscent of a romantic comedy, the overall meaning of text
   descriptions broke down as they went through the process. here are
   three columns showing the initial text description, followed by the
   image output of the first generator, and the video output of the second
   generator.

   while the image output may have picked up on some of the meaning in the
   text, the videos themselves entirely lose it. they do make for some fun
   psychedelic gifs though.
   ezgif.com-crop (9).gif ezgif.com-crop (9).gif
     __________________________________________________________________

limitations

   while the ml pipeline was complete, the results ultimately need some
   improving on. the causes of the breakdown in meaning from text
   description to video may not be fully remediable, but i believe that
   another run of the models with a more refined dataset can produce much,
   much better results.

   the dataset used, video clips from nine romantic comedies, was
   unfortunately too varied in subject matter and too low resolution from
   the start to provide meaningful results.

   while the consequences of this became apparent during the project, it
   was too late to turn back. filtering the dataset for higher quality
   videos on less varied subject matter will ultimately require
   significantly more image editing work and perhaps the design of a
   classification model to aid in the process.

   another large limitation of this project design was the disconnect
   between the initial text vector and the final video-gan. while i did
   not have time to further modify the second gan's framework, i see
   several potential ways to improve it - discussed below.
     __________________________________________________________________

future runs

   i believe this pipeline will run quite well on higher quality videos
   that stick to a specific subject matter. i'm currently looking for
   available datasets and scrape-able material that will be good for this
   use.

   additionally, i believe that tokenizing for the action words in the
   text descriptions and putting an additional focus on these word vectors
   during the embedding stage will prevent the contextual breakdown that
   this project faced.

   changing video generation model to be more like the image generation
   one will also improve the results. the image generation model takes
   into account whether the image is a match with its text description
   when deriving the loss. the video generation needs a similar data and
   id168 design.

references

   1. https://antonia.space/
   2. https://antonia.space/blog-2
   3. https://antonia.space/datascience
   4. https://antonia.space/
   5. https://antonia.space/text-to-video-generation
   6. https://antonia.space/abstractive-text-generation
   7. https://antonia.space/geography-of-terror
   8. https://antonia.space/diversity-in-hollywood
   9. https://antonia.space/mta-turnstile-data
  10. https://antonia.space/resume
  11. https://antonia.space/about
  12. https://antonia.space/contact
  13. https://antonia.space/blog-2
  14. https://antonia.space/datascience
  15. https://antonia.space/
  16. https://antonia.space/text-to-video-generation
  17. https://antonia.space/abstractive-text-generation
  18. https://antonia.space/geography-of-terror
  19. https://antonia.space/diversity-in-hollywood
  20. https://antonia.space/mta-turnstile-data
  21. https://antonia.space/resume
  22. https://antonia.space/about
  23. https://antonia.space/contact
  24. https://github.com/reedscot/icml2016
  25. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/rohrbach_a_dataset_for_2015_cvpr_paper.pdf
  26. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/rohrbach_a_dataset_for_2015_cvpr_paper.pdf
  27. https://github.com/toni-antonova/vae-text-generation
  28. https://arxiv.org/pdf/1511.06349.pdf
  29. https://github.com/toni-antonova/joint-multimodal-embedding
  30. https://antonia.space/text-to-video-generation/
  31. https://arxiv.org/pdf/1612.03242v1.pdf
  32. http://carlvondrick.com/tinyvideo/paper.pdf
  33. https://github.com/hanzhanggit/stackgan
  34. https://github.com/hanzhanggit/stackgan
  35. https://github.com/cvondrick/videogan
  36. https://github.com/cvondrick/videogan
