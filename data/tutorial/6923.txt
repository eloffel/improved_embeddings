
   (button)
     * [1]home
     * [2]research
          + [3]publications
          + [4]alphago
          + [5]id25
          + [6]dnc
          + [7]open source
          + [8]access to science
     * [9]applied
          + [10]deepmind health
          + [11]deepmind for google
          + [12]deepmind ethics & society
     * [13]news & blog
     * [14]about us
     * [15]careers

   (button) search (button) search
   [deepmind_logo_swirl.png]

[16]research

highlighted research

     * [17]alphago
     * [18]id25
     * [19]dnc

[20]publications

[21]open source

latest research news

   [22]towards robust and verified ai: specification testing, robust
   training, and formal verification

[23]applied

[24]deepmind health

[25]deepmind for google

[26]deepmind ethics & society

latest applied news

   [27]scaling streams with google

[28]careers

     * [29]home
     * [30]news & blog
     * [31]about us
     * [32]press
     * [33]terms and conditions
     * [34]privacy policy     updated

     *
     *
     *

decoupled neural interfaces using synthetic gradients

   neural networks are the workhorse of many of the algorithms developed
   at deepmind. for example, [35]alphago uses convolutional neural
   networks to evaluate board positions in the game of go and [36]id25 and
   [37]deep id23 algorithms use neural networks to
   choose actions to play at super-human level on video games.

   this post introduces some of our latest research in progressing the
   capabilities and training procedures of neural networks called
   [38]decoupled neural interfaces using synthetic gradients. this work
   gives us a way to allow neural networks to communicate, to learn to
   send messages between themselves, in a decoupled, scalable manner
   paving the way for multiple neural networks to communicate with each
   other or improving the long term temporal dependency of recurrent
   networks. this is achieved by using a model to approximate error
   gradients, rather than by computing error gradients explicitly with
   id26. the rest of this post assumes some familiarity with
   neural networks and how to train them. if you   re new to this area we
   highly recommend [39]nando de freitas lecture series on youtube on deep
   learning and neural networks.

neural networks and the problem of locking

   if you consider any layer or module in a neural network, it can only be
   updated once all the subsequent modules of the network have been
   executed, and gradients have been backpropagated to it. for example
   look at this simple feed-forward network:
   [3-1.width-400_bcpybzm.png]

   here, after layer 1 has processed the input, it can only be updated
   after the output activations (black lines) have been propagated through
   the rest of the network, generated a loss, and the error gradients
   (green lines) backpropagated through every layer until layer 1 is
   reached. this sequence of operations means that layer 1 has to wait for
   the forwards and backwards computation of layer 2 and layer 3 before it
   can update. layer 1 is locked, coupled, to the rest of the network.

   why is this a problem? clearly for a simple feed-forward network as
   depicted we don   t need to worry about this issue. but consider a
   complex system of multiple networks, acting in multiple environments at
   asynchronous and irregular timescales.

   or a big distributed network spread over multiple machines. sometimes
   requiring all modules in a network to wait for all other modules to
   execute and backpropagate gradients is overly time consuming or even
   intractable. if we decouple the interfaces - the connections -  between
   modules, every module can be updated independently, and is not locked
   to the rest of the network.

   so, how can one decouple neural interfaces - that is decouple the
   connections between network modules - and still allow the modules to
   learn to interact? in this paper, we remove the reliance on
   id26 to get error gradients, and instead learn a parametric
   model which predicts what the gradients will be based upon only local
   information. we call these predicted gradients synthetic gradients.

   [3-3.width-400_nuvu2mx.png]

   the synthetic gradient model takes in the activations from a module and
   produces what it predicts will be the error gradients - the gradient of
   the loss of the network with respect to the activations.

   going back to our simple feed-forward network example, if we have a
   synthetic gradient model we can do the following:

   [3-4.width-400_cgwphny.png]

   ... and use the synthetic gradients (blue) to update layer 1 before the
   rest of the network has even been executed.

   the synthetic gradient model itself is trained to regress target
   gradients - these target gradients could be the true gradients
   backpropagated from the loss or other synthetic gradients which have
   been backpropagated from a further downstream synthetic gradient model.

   [3-5.width-400_k4dypyn.png]

   this mechanism is generic for a connection between any two modules, not
   just in a feed-forward network. the play-by-play working of this
   mechanism is shown below, where the change of colour of a module
   indicates an update to the weights of that module.

   3-6.gif

   using decoupled neural interfaces (dni) therefore removes the locking
   of preceding modules to subsequent modules in a network. in experiments
   from the paper, we show we can train convolutional neural networks
   for [40]cifar-10 image classification where every layer is decoupled
   using synthetic gradients to the same accuracy as using
   id26. it   s important to recognise that dni doesn   t magically
   allow networks to train without true gradient information. the true
   gradient information does percolate backwards through the network, but
   just slower and over many training iterations, through the losses of
   the synthetic gradient models. the synthetic gradient models
   approximate and smooth over the absence of true gradients.

   a legitimate question at this point would be to ask how much
   computational complexity do these synthetic gradient models add -
   perhaps you would need a synthetic gradient model architecture that is
   as complex as the network itself. quite surprisingly, the synthetic
   gradient models can be very simple. for feed-forward nets, we actually
   found out that even a single linear layer works well as a synthetic
   gradient model. consequently it is both very easy to train and so
   produces synthetic gradients rapidly.

   dni can be applied to any generic neural network architecture, not just
   feed-forward networks. an interesting application is to recurrent
   neural networks (id56s). an id56 has a recurrent core which is unrolled -
   repeatedly applied - to process sequential data. ideally to train an
   id56 we would unroll the core over the whole sequence (which could be
   infinitely long), and use id26 through time (bptt) to
   propagate error gradients backwards through the graph.

   [3-7.width-400_jaetrvs.png]

   however in practice, we can only afford to unroll for a limited number
   of steps due to memory constraints and the need to actually compute an
   update to our core model frequently. this is called truncated
   id26 through time, and shown below for a truncation of three
   steps:

   [3-8.width-400_5j1gnng.png]

   the change in colour of the core illustrates an update to the core,
   that the weights have been updated. in this example, truncated bptt
   seems to address some issues with training - we can now update our core
   weights every three steps and only need three cores in memory. however,
   the fact that there is no id26 of error gradients over more
   than three steps means that the update to the core will not be directly
   influenced by errors made more than two steps in the future. this
   limits the temporal dependency that the id56 can learn to model.

   what if instead of doing no id26 between the boundary of
   bptt we used dni and produce synthetic gradients, which model what the
   error gradients of the future will be? we can incorporate a synthetic
   gradient model into the core so that at every time step, the id56 core
   produces not only the output but also the synthetic gradients. in this
   case, the synthetic gradients would be the predicted gradients of the
   all future losses with respect to the hidden state activation of the
   previous timestep. the synthetic gradients are only used at the
   boundaries of truncated bptt where we would have had no gradients
   before.

   [3-9.width-400_l46dkfe.png]

   this can be performed during training very efficiently - it merely
   requires us to keep an extra core in memory as illustrated below. here
   a green dotted border indicates just computing gradients with respect
   to the input state, while a solid green border additionally computes
   gradients with respect to the core   s parameters.

   3-10

   by using dni and synthetic gradients with an id56, we are approximating
   doing id26 across an infinitely unrolled id56. in practice,
   this results in id56s which can model longer temporal dependencies.
   here   s an example result showing this from the paper.

   id32 test error during training (lower is better):

   [3-11.width-400_qyl0qxj.png]

   this graph shows the application of an id56 trained on next character
   prediction on [41]id32, a language modelling problem. on the
   y-axis the bits-per-character (bpc) is given, where smaller is better.
   the x-axis is the number of characters seen by the model as training
   progresses. the dotted blue, red and grey lines are id56s trained with
   truncated bptt, unrolled for 8 steps, 20 steps and 40 steps - the
   higher the number of steps the id56 is unrolled before performing
   id26 through time, the better the model is, but the slower
   it trains. when dni is used on the id56 unrolled 8 steps (solid blue
   line) the id56 is able to capture the long term dependency of the
   40-step model, but is trained twice as fast (both in terms of data and
   wall clock time on a regular desktop machine with a single gpu).

   to reiterate, adding synthetic gradient models allows us to decouple
   the updates between two parts of a network. dni can also be applied on
   hierarchical id56 models - system of two (or more) id56s running at
   different timescales. as we show in the [42]paper, dni significantly
   improves the training speed of these models by enabling the update rate
   of higher level modules.

   hopefully from the explanations in this post, and a brief look at some
   of the experiments we report in the [43]paper it is evident that it is
   possible to create decoupled neural interfaces. this is done by
   creating a synthetic gradient model which takes in local information
   and predicts what the error gradient will be. at a high level, this can
   be thought of as a communication protocol between two modules. one
   module sends a message (current activations), another one receives the
   message, and evaluates it using a model of utility (the synthetic
   gradient model). the model of utility allows the receiver to provide
   instant feedback (synthetic gradient) to the sender, rather than having
   to wait for the evaluation of the true utility of the message (via
   id26). this framework can also be thought about from an
   error critic point of view [[44]werbos] and is similar in flavour to
   using a critic in id23 [[45]baxter].

   these decoupled neural interfaces allow distributed training of
   networks, enhance the temporal dependency learnt with id56s, and speed
   up hierarchical id56 systems. we   re excited to explore what the future
   holds for dni, as we think this is going to be an important basis for
   opening up more modular, decoupled, and asynchronous model
   architectures. finally, there are lots more details, tricks, and full
   experiments which you can find in the paper [46]here.

   share article
     *
     *
     *
     *
     * [ ]
          + [47]linkedin
          + whatsapp
          + sms
          + [48]reddit

   author
   monday, 29 august 2016
     * [img_0022.2e16d0ba.fill-80x80_9nuehou.jpg]
       max jaderberg
       research scientist, deepmind

   ____________________
   ____________________
   [49]show all results
   (button) close

                               [50]deepmind logo

   follow
     *
     *
     *

     * [51]research [52]research
     * [53]applied [54]applied
     * [55]news & blog [56]news & blog
     * [57]about us [58]about us
     * [59]careers [60]careers

     * [61]press
     * [62]terms and conditions
     * [63]privacy policy     updated
     * [64]modern slavery statement
     * [65]alphabet inc

      2019 deepmind technologies limited

   deepmind.com uses cookies to help give you the best possible user
   experience and to allow us to see how the site is used. by using this
   site, you agree that we can set and use these cookies. for more
   information on cookies and how to change your settings, see our
   [66]privacy policy.

references

   visible links
   1. https://deepmind.com/
   2. https://deepmind.com/research/
   3. https://deepmind.com/research/publications/
   4. https://deepmind.com/research/alphago/
   5. https://deepmind.com/research/id25/
   6. https://deepmind.com/research/dnc/
   7. https://deepmind.com/research/open-source/
   8. https://deepmind.com/research/access-science/
   9. https://deepmind.com/applied/
  10. https://deepmind.com/applied/deepmind-health/
  11. https://deepmind.com/applied/deepmind-google/
  12. https://deepmind.com/applied/deepmind-ethics-society/
  13. https://deepmind.com/blog/
  14. https://deepmind.com/about/
  15. https://deepmind.com/careers/
  16. https://deepmind.com/research/
  17. https://deepmind.com/research/alphago/
  18. https://deepmind.com/research/id25/
  19. https://deepmind.com/research/dnc/
  20. https://deepmind.com/research/publications/
  21. https://deepmind.com/research/open-source/
  22. https://deepmind.com/blog/robust-and-verified-ai/
  23. https://deepmind.com/applied/
  24. https://deepmind.com/applied/deepmind-health/
  25. https://deepmind.com/applied/deepmind-google/
  26. https://deepmind.com/applied/deepmind-ethics-society/
  27. https://deepmind.com/blog/scaling-streams-google/
  28. https://deepmind.com/careers/
  29. https://deepmind.com/
  30. https://deepmind.com/blog/
  31. https://deepmind.com/about/
  32. https://deepmind.com/press/
  33. https://deepmind.com/terms-and-conditions/
  34. https://deepmind.com/privacy-policy/
  35. https://deepmind.com/research/alphago/
  36. https://deepmind.com/research/id25/
  37. https://deepmind.com/blog/deep-reinforcement-learning/
  38. https://arxiv.org/abs/1608.05343
  39. https://www.youtube.com/watch?v=plhfwt7vaew
  40. https://www.cs.toronto.edu/~kriz/cifar.html
  41. http://www.cis.upenn.edu/~treebank/
  42. https://arxiv.org/abs/1608.05343
  43. https://arxiv.org/abs/1608.05343
  44. http://www.werbos.com/hicchapter13.pdf
  45. http://www.cis.upenn.edu/~mkearns/finread/baxterweaverbartlett.pdf
  46. https://arxiv.org/abs/1608.05343
  47. https://www.linkedin.com/sharearticle?mini=true&url=https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/&title=decoupled neural interfaces using synthetic gradients&summary=&source=google deepmind
  48. http://www.reddit.com/submit?url=https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/&title=decoupled neural interfaces using synthetic gradients
  49. https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/
  50. https://deepmind.com/
  51. https://deepmind.com/research/
  52. https://deepmind.com/research/
  53. https://deepmind.com/applied/
  54. https://deepmind.com/applied/
  55. https://deepmind.com/blog/
  56. https://deepmind.com/blog/
  57. https://deepmind.com/about/
  58. https://deepmind.com/about/
  59. https://deepmind.com/careers/
  60. https://deepmind.com/careers/
  61. https://deepmind.com/press/
  62. https://deepmind.com/terms-and-conditions/
  63. https://deepmind.com/privacy-policy/
  64. https://storage.googleapis.com/deepmind-media/modern_slavery/final_201_google_modern_slavery_statement.pdf
  65. https://abc.xyz/
  66. https://deepmind.com/privacy-policy/

   hidden links:
  68. https://twitter.com/deepmindai
  69. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  70. https://plus.google.com/+deepmindai
  71. http://twitter.com/intent/tweet/?text=&url=https%3a//deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/
  72. http://www.facebook.com/share.php?u=https%3a//deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/&t=
  73. https://plus.google.com/share?url=https%3a//deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/
  74. mailto:?subject=decoupled%20neural%20interfaces%20using%20synthetic%20gradients&body=%0d%0a%0d%0ahttps%3a//deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/
  75. https://twitter.com/deepmindai
  76. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  77. https://plus.google.com/+deepmindai
