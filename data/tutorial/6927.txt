
   [1]

the stanford natural language processing group

the stanford nlp group

     * [2]people
     * [3]publications
     * [4]research blog
     * [5]software
     * [6]teaching
     * [7]local

[8]software > [9]stanford parser > shift-reduce constituency parser

                      shift-reduce constituency parser

introduction

   previous versions of the stanford parser for constituency parsing used
   chart-based algorithms (id145) to find the highest
   scoring parse under a pid18; this is accurate but slow. meanwhile, for
   id33, transition-based parsers that use shift and reduce
   operations to build dependency trees have long been known to get very
   good performance in a fraction of the time of more complicated
   algorithms. recent work has shown that similar shift-reduce algorithms
   are also effective for building constituency trees.

   based on this work, we built a shift-reduce parser which is far faster
   than previous versions of the stanford parser while being more accurate
   than any version other than the id56 parser.

acknowledgements

   stanford's shift-reduce parser was written by john bauer. it is based
   on the prior work of several other researchers:
     * [10]fast and accurate shift-reduce constituent parsing by muhua
       zhu, yue zhang, wenliang chen, min zhang and jingbo zhu
     * [11]transition-based parsing of the chinese treebank using a global
       discriminative model by yue zhang and stephen clark
     * [12]a classifier-based parser with linear run-time complexity by
       kenji sagae and alon lavie
     * [13]a dynamic oracle for arc-eager id33 by yoav
       goldberg and joakim nivre
     * [14]learning sparser id88 models by yoav goldberg and michael
       elhadad

obtaining the software

   you must download the following packages:
    1. either download
          + only the [15]latest stanford parser ([16]more details) and
            [17]latest stanford tagger ([18]more details); or
          + all of [19]stanford corenlp, which contains the parser, the
            tagger, and other things which you may or may not need.
    2. the [20]shift-reduce parser models (these are distributed
       separately because they are quite large).

using the shift-reduce parser using stanford corenlp

   read this if you just want to use the shift-reduce parser from the
   command-line.

   the new parser is integrated into [21]stanford corenlp. the simplest
   way to use it is to give stanfordcorenlp the flag

     -parse.model <location>

   using resources from a release of
   stanford-srparser-yyyy-mm-dd-models.jar, you might use:

     -parse.model edu/stanford/nlp/models/srparser/englishsr.ser.gz

   on the stanford cluster, the location is in /u/nlp/data/srparser, so

     -parse.model /u/nlp/data/srparser/englishsr.ser.gz

   there are other models as well. for example, there is a model trained
   to use [22]id125. by default, this model will use a beam of size
   4. if you want to change that, you can use the flag -parse.flags "
   -beamsize 4" note that the space after the quote is necessary for our
   options processing code. the full list of models currently distributed
   is:

     edu/stanford/nlp/models/srparser/arabicsr.ser.gz
     edu/stanford/nlp/models/srparser/englishsr.beam.ser.gz
     edu/stanford/nlp/models/srparser/englishsr.ser.gz
     edu/stanford/nlp/models/srparser/germansr.ser.gz
     edu/stanford/nlp/models/srparser/chinesesr.ser.gz
     edu/stanford/nlp/models/srparser/frenchsr.beam.ser.gz

calling parsing from java

   read this if you want to call the shift-reduce parser from your own
   code. in the parser package, there is a shiftreducedemo.java class.

   to compile it (the two jar files are provided by stanford parser and
   the stanford tagger, respectively):

     javac -cp stanford-parser.jar:stanford-postagger-3.5.0.jar
     shiftreducedemo.java

   to run it:

     java -cp
     .:stanford-parser.jar:stanford-postagger-3.5.0.jar:stanford-srparser
     -2014-10-23-models.jar shiftreducedemo -model
     edu/stanford/nlp/models/srparser/englishsr.ser.gz -tagger
     english-left3words-distsim.tagger

   note that we need to include the jar file where the parser models are
   stored, as well as specifying the tagger model (which came from the
   stanford tagger package). this should load the tagger, parser, and
   parse the example sentence, finishing in under 20 seconds. the output:

     reading pos tagger model from
     stanford-postagger-full-2014-10-23/models/english-left3words-distsim
     .tagger ... done [1.2 sec].
     loading parser from serialized file
     edu/stanford/nlp/models/srparser/englishsr.ser.gz ...done [14.8
     sec].
     (root (s (np (prp$ my) (nn dog)) (vp (vbz likes) (s (vp (to to) (vp
     (vb shake) (np (prp$ his) (vbn stuffed) (nn chickadee) (nn toy))))))
     (. .)))

how id132 works

   the shift-reduce parser parses by maintaining a state of the current
   parsed tree, with the words of the sentence on a queue and partially
   completed trees on a stack, and applying transitions to the state until
   the queue is empty and the current stack only contains a finished tree.

   the initial state is to have all of the words in order on the queue,
   with an empty stack. the transitions which can be applied are:
     * shift. a word moves from the queue onto the stack.
     * unary reduce. the label of the first constituent on the stack
       changes. there is a different unary transition for every possible
       unary node in the treebank used for training.
     * binary reduce. the first two nodes on the stack are combined with a
       new label. these are either right sided or left sided, indicating
       which child is treated as the head. once again, there is a
       different binary transition for every possible binary node. this
       includes temporary nodes, as trees are built as binarized trees and
       then later debinarized.
     * finalize. a tree is not considered finished until the parser
       chooses the finalize transition.
     * idle. in the case of id125ing, zhu et al. showed that
       training an idle transition compensates for different candidate
       trees using different numbers of transitions.

   transitions are determined by featurizing the current state and using a
   multiclass id88 to determine the next transition. various
   legality constraints are applied to the transitions to make sure the
   state remains legal and solvable.

   part-of-speech tags are not assigned by this parser, and are in fact
   used as features. this is accomplished by pretagging the text, meaning
   the pos annotator needs to be used in stanfordcorenlp, for example.

   training is conducted by iterating over the trees repeatedly until some
   measure of convergence is reached. there are various ways to process
   the trees during each iteration. the one used by default is to start
   from the basic state and apply the transitions chosen by the parser
   until it gets a transition wrong, i.e., it can no longer rebuild the
   gold tree. the features used at the time of the incorrect decision have
   their weights adjusted, with the correct transition getting the feature
   weights increased and the incorrect transition decreased.

id125

   in general, the parser uses greedy transitions, continuing until the
   sentence is finalized. it is also possible to use it in id125
   mode, though. in this mode, the parser keeps an agenda of the highest
   scoring candidate states. at each step, each of the states has a
   transition applied, updating the agenda with the new highest scoring
   states. this process continues until the highest scoring state on the
   agenda is finalized.

   models need to be specifically trained to work with id125.
   otherwise, scores actually get worse. while this increases accuracy
   quite a bit, it also has the drawback of significantly increasing the
   size of the model. a model not trained for id125 only ever has
   features which were present in states reached by the gold sequence of
   transitions on the gold training trees. a model trained to use beam
   search trains negative features for incorrect states on the beam,
   resulting in many more features and therefore a much larger model.

training the shift-reduce parser

   read this section if you want to train your own shift-reduce parser.
   this requires you have data in the standard treebank format.

basic guidelines

   new english wsj models can be trained as follows:

   java -mx10g edu.stanford.nlp.parser.shiftreduce.shiftreduceparser
   -traintreebank <trainpath> -devtreebank <devpath> -serializedpath
   model.ser.gz

   concretely, on the nlp machines, this would be

   java -mx10g edu.stanford.nlp.parser.shiftreduce.shiftreduceparser
   -traintreebank /afs/ir/data/linguistic-data/treebank/3/parsed/mrg/wsj
   200-2199 -devtreebank
   /afs/ir/data/linguistic-data/treebank/3/parsed/mrg/wsj 2200-2219
   -serializedpath model.ser.gz

   more details on how it trains are below. the key summary is that some
   time later, probably an hour on a decent machine, there will be a new
   file model.ser.gz which is the newly trained shift-reduce parser. this
   model can be tested as follows:

   java -mx6g edu.stanford.nlp.parser.shiftreduce.shiftreduceparser
   -testtreebank <testpath> -serializedpath model.ser.gz
   java -mx6g edu.stanford.nlp.parser.shiftreduce.shiftreduceparser
   -testtreebank /afs/ir/data/linguistic-data/treebank/3/parsed/mrg/wsj
   2300-2399 -serializedpath model.ser.gz

   however, this ignores a key aspect of the shift-reduce parser. this
   parser does not produce its own tags and instead expects to use
   automatically produced tags as features when parsing. the commands
   above will work, but they will train a model using the gold tags in the
   treebank. it is generally better to train on the tags provided by the
   tagger which will be used as test time. this can be done with the flags
   -pretag -taggerserializedfile <tagger>

   java -mx10g edu.stanford.nlp.parser.shiftreduce.shiftreduceparser
   -traintreebank /afs/ir/data/linguistic-data/treebank/3/parsed/mrg/wsj
   200-2199 -devtreebank
   /afs/ir/data/linguistic-data/treebank/3/parsed/mrg/wsj 2200-2219
   -pretag -taggerserializedfile
   /u/nlp/data/pos-tagger/distrib/wsj-0-18-bidirectional-nodistsim.tagger
   -serializedpath model.ser.gz
   java -mx6g edu.stanford.nlp.parser.shiftreduce.shiftreduceparser
   -devtreebank /afs/ir/data/linguistic-data/treebank/3/parsed/mrg/wsj
   2300-2399 -pretag -taggerserializedfile
   /u/nlp/data/pos-tagger/distrib/wsj-0-18-bidirectional-nodistsim.tagger
   -serializedpath model.ser.gz

   the bidirectional model is significantly slower than the left3words
   model, but is somewhat more accurate. it is still faster than the
   parsing itself. alternatively, one can just use the left3words tagger
   for better speed and slightly less accuracy.

other languages

   it is possible to train the shift-reduce parser for languages other
   than english. an appropriate headfinder needs to be provided. this and
   other options are handled by specifying the -tlpp flag, which lets you
   choose the class for a treebanklangparserparams. a language appropriate
   tagger is also required.

   for example, here is a command used to train a chinese model. the
   options not already explained are explained in the next section.

   java -mx10g edu.stanford.nlp.parser.shiftreduce.shiftreduceparser
   -traintreebank /u/nlp/data/chinese/ctb7/train.mrg -devtreebank
   /u/nlp/data/chinese/ctb7/dev_small.mrg -pretag -taggerserializedfile
   /u/nlp/data/pos-tagger/distrib/chinese-nodistsim.tagger -serializedpath
   chinese.ser.gz -tlpp
   edu.stanford.nlp.parser.lexparser.chinesetreebankparserparams
   -trainingthreads 4 -batchsize 12 -trainingiterations 100
   -stallediterationlimit 20

   the resulting model is both faster and more accurate than any other
   model we have, including the chinese id56 model.

featurefactory classes

   the features for the id88 are extracted using a featurefactory.
   by default, the parser uses
   edu.stanford.nlp.parser.shiftreduce.basicfeaturefactory. this
   featurefactory includes most of the basic features you would want, such
   as features for the head word of the current stack node and several
   previous stack nodes, the word and tag of incoming queue items, and
   combinations of those strings.

   another included featurefactory is the distsimfeaturefactory. this can
   be used by setting the -featurefactory argument:

   -featurefactory
   edu.stanford.nlp.parser.shiftreduce.distsimfeaturefactory(<path_to_dist
   sim>)

   multiple featurefactory classes can be combined by using a semicolon
   separated list. for example:

   -featurefactory
   edu.stanford.nlp.parser.shiftreduce.basicfeaturefactory;edu.stanford.nl
   p.parser.shiftreduce.distsimfeaturefactory(<path_to_distsim>)

additional options

      trainingmethod see below.
      beamsize size of the beam to use when running id125. 4 is
   already sufficient to greatly increase accuracy without affecting speed
   too badly.
      trainbeamsize size of the beam to use when training.
      trainingthreads training can be run in parallel. this is done by
   training on multiple trees simultaneously.
      batchsize how many trees to batch together when training. this allows
   training in parallel to get repeatable results, as each of the trees
   are scored using the weights at the start of the training batch, and
   then all updates are applied at once.
      trainingiterations the maximum number of iterations to train. defaults
   to 40.
      stallediterationlimit the heuristic for ending training before
   -trainingiterations iterations is to stop when the current dev set
   score has not improved for this many iterations. defaults to 20.
      averagedmodels when the id88 has finished training, in general,
   the model with the best score on the dev set is kept. this flag
   averages together the best k models and uses that as the final model
   instead. defaults to 8. this has the potential to greatly increase the
   amount of memory needed, so can be set to a lower number if memory is a
   barrier.
      featurefrequencycutoff if a feature is not seen this many times when
   training, it is removed from the final model. this can eliminate rarely
   seen features without impacting overall accuracy too much. it is
   especially useful in the case of model training using a beam (or
   oracle, if that method is ever made to work), as that training method
   results in many features that were only seen once and don't really have
   much impact on the final model.
      saveintermediatemodels by default, training does not save the
   intermediate models any more, since they basically don't do anything.
   use this flag to turn it back on.
      featurefactory the feature factory class to use.

   there are several training methods implemented. the default is
   early_termination, in which training on an individual tree is halted as
   soon as the current model is incorrect. alternatives are:
   -trainingmethod
   gold force the parser to make the correct transition while training,
   continuing after errors. takes longer than early_termination and does
   not improve accuracy.
   early_termination as soon as the current model gets a transition wrong
   when parsing a tree, stops training on that tree for this iteration.
   beam an agenda of the highest scoring candidate states is kept.
   training continues until the gold state is no longer on the agenda. at
   each step, the gold transition for the gold state gets its feature
   weights increased, and transition chosen for the highest scoring state
   gets its feature weights decreased.
   oracle an experimental training method in which an oracle is used to
   figure out what transition would result in the best possible parse from
   the current state. unfortunately, this results in poor scores, either
   because of a bug in the oracle training code or incorrect oracle logic.

performance

   the tables below summarize the shift-reduce parser's performance, based
   on experiments run in 2014.

   english penn wsj sec 23 (all lengths)
   model loading
   (sec.) parsing
   (sec.) lp/lr
   f[1]
   wsjpid18.ser.gz 1.0 426 85.54
   wsjsr.ser.gz 4.5 14 85.99
   wsjfactored.ser.gz 7.6 1759 86.53
   wsjsr.beam.ser.gz, beam size 4 15.4 34 88.55
   wsjid56.ser.gz 2.6 1038 89.96

   sr model previous best
   stanford model f[1] sr parser
   f[1]
   arabic 78.15 79.66
   chinese 75.66 80.23
   french (beam) 76.22 80.27
   german 72.19 74.04

stanford nlp group

   gates computer science building
   353 serra mall
   stanford, ca 94305-9020
   [23]directions and parking

affiliated groups

     * [24]stanford ai lab
     * [25]stanford infolab
     * [26]csli

connect

     * [27]stack overflow
     * [28]github
     * [29]twitter

local links

   [30]nlp lunch    [31]nlp reading group
   [32]nlp seminar    [33]calendar
   [34]javanlp ([35]javadocs)    [36]machines
   [37]ai speakers    [38]q&a

references

   visible links
   1. https://nlp.stanford.edu/
   2. https://nlp.stanford.edu/people/
   3. https://nlp.stanford.edu/pubs/
   4. https://nlp.stanford.edu/blog/
   5. https://nlp.stanford.edu/software/
   6. https://nlp.stanford.edu/teaching/
   7. https://nlp.stanford.edu/new_local/
   8. https://nlp.stanford.edu/software/
   9. https://nlp.stanford.edu/software/lex-parser.html
  10. http://www.aclweb.org/anthology/p13-1043.pdf
  11. http://aclweb.org/anthology//w/w09/w09-3825.pdf
  12. http://people.ict.usc.edu/~sagae/docs/sagae-iwpt05.pdf
  13. http://aclweb.org/anthology//c/c12/c12-1059.pdf
  14. http://www.cs.bgu.ac.il/~yoavg/publications/acl2011sparse.pdf
  15. https://nlp.stanford.edu/software/stanford-parser-full-2014-10-31.zip
  16. https://nlp.stanford.edu/software/lex-parser.html
  17. https://nlp.stanford.edu/software/stanford-postagger-2014-10-31.zip
  18. https://nlp.stanford.edu/software/tagger.html
  19. https://nlp.stanford.edu/software/corenlp.html
  20. https://nlp.stanford.edu/software/stanford-srparser-2014-10-23-models.jar
  21. https://nlp.stanford.edu/software/corenlp.html
  22. https://nlp.stanford.edu/software/srparser.shtml#beam
  23. http://forum.stanford.edu/visitors/directions/gates.php
  24. http://ai.stanford.edu/
  25. http://infolab.stanford.edu/
  26. https://www-csli.stanford.edu/
  27. http://stackoverflow.com/tags/stanford-nlp
  28. https://github.com/stanfordnlp/corenlp
  29. https://twitter.com/stanfordnlp
  30. https://nlp.stanford.edu/local/nlp_lunch.shtml
  31. http://nlp.stanford.edu/read/
  32. http://nlp.stanford.edu/seminar/
  33. https://nlp.stanford.edu/local/calendar.shtml
  34. https://nlp.stanford.edu/javanlp/
  35. https://nlp.stanford.edu/nlp/javadoc/javanlp/
  36. https://nlp.stanford.edu/local/machines.shtml
  37. http://ai.stanford.edu/portfolio-view/distinguished-speaker-series
  38. https://nlp.stanford.edu/local/qa/

   hidden links:
  40. https://nlp.stanford.edu/software/srparser.shtml
