   #[1]awni hannun - writing about machine learning

   [2][952ab78c5d7894a988d7cd403bbf9cf4?s=80]

[3]awni hannun

   writing about machine learning

   [4]about

id103 is not solved

   posted on october 11, 2017

   ever since deep learning hit the scene in id103, word
   error rates have fallen dramatically. but despite articles you may have
   read, we still don   t have human-level id103. speech
   recognizers have many failure modes. acknowledging these and taking
   steps towards solving them is critical to progress. it   s the only way
   to go from asr which works for some people, most of the time to asr
   which works for all people, all of the time.
   [wer.svg]
   improvements in word error rate over time on the switchboard
   conversational id103 benchmark. the test set was collected
   in 2000. it consists of 40 phone conversations between two random
   native english speakers.

   saying we   ve achieved human-level in conversational id103
   based just on switchboard results is like saying an autonomous car
   drives as well as a human after testing it in one town on a sunny day
   without traffic. the recent improvements on conversational speech are
   astounding. but, the claims about human-level performance are too
   broad. below are a few of the areas that still need improvement.

accents and noise

   one of the most visible deficiencies in id103 is dealing
   with accents^[5]1 and background noise. the straightforward reason is
   that most of the training data consists of american accented english
   with high signal-to-noise ratios. for example, the switchboard
   conversational training and test sets only have native english speakers
   (mostly american) with little background noise.

   but, more training data likely won   t solve this problem on its own.
   there are a lot of languages many of which have a lot of dialects and
   accents. it   s not feasible to collect enough annotated data for all
   cases. building a high quality speech recognizer just for american
   accented english needs upwards of 5 thousand hours of transcribed
   audio.
   [human_model.svg]
   comparison of human transcribers to baidu   s deep speech 2 model on
   various types of speech.^[6]2 notice the humans are worse at
   transcribing the non-american accents. this is probably due to an
   american bias in the transcriber pool. i would expect transcribers
   native to a given region to have much lower error rates for that
   region   s accents.

   with background noise, it   s not uncommon for the snr in a moving car to
   be as low as -5db. people don   t have much trouble understanding one
   another in these environments. speech recognizers, on the other hand,
   degrade more rapidly with noise. in the figure above we see the gap
   between the human and the model error rates increase dramatically from
   the low snr to the high snr audio.

semantic errors

   often the word error rate is not the actual objective in a speech
   recognition system. what we care about is the semantic error rate.
   that   s the fraction of utterances in which we misinterpret the meaning.

   an example of a semantic error is if someone said    let   s meet up
   tuesday    but the speech recognizer predicted    let   s meet up today   . we
   can also have word errors without semantic errors. if the speech
   recognizer dropped the    up    and predicted    let   s meet tuesday    the
   semantics of the utterance are unchanged.

   we have to be careful when using the word error rate as a proxy. let me
   give a worst-case example to show why. a wer of 5% roughly corresponds
   to 1 missed word for every 20. if each sentence has 20 words (about
   average for english), the sentence error rate could be as high as 100%.
   hopefully the mistaken words don   t change the semantic meaning of the
   sentences. otherwise the recognizer could misinterpret every sentence
   even with a 5% wer.

   when comparing models to humans, it   s important to check the nature of
   the mistakes and not just look at the wer as a conclusive number. in my
   own experience, human transcribers tend to make fewer and less drastic
   semantic errors than speech recognizers.

   researchers at microsoft recently compared mistakes made by humans and
   their human-level speech recognizer.^[7]3 one discrepancy they found
   was that the model confuses    uh    with    uh huh    much more frequently
   than humans. the two terms have very different semantics:    uh    is just
   filler whereas    uh huh    is a backchannel acknowledgement. the model and
   humans also made a lot of the same types of mistakes.

single-channel, multi-speaker

   the switchboard conversational task is also easier because each speaker
   is recorded with a separate microphone. there   s no overlap of multiple
   speakers in the same audio stream. humans on the other hand can
   understand multiple speakers sometimes talking at the same time.

   a good conversational speech recognizer must be able to segment the
   audio based on who is speaking (diarisation). it should also be able to
   make sense of audio with overlapping speakers (source separation). this
   should be doable without needing a microphone close to the mouth of
   each speaker, so that conversational speech can work well in arbitrary
   locations.

domain variation

   accents and background noise are just two factors a speech recognizer
   needs to be robust to. here are a few more:
     * reverberation from varying the acoustic environment.
     * artefacts from the hardware.
     * the codec used for the audio and compression artefacts.
     * the sample rate.
     * the age of the speaker.

   most people wouldn   t even notice the difference between an mp3 and a
   plain wav file. before we claim human-level performance, speech
   recognizers need to be robust to these sources of variability as well.

context

   you   ll notice the human-level error rate on benchmarks like switchboard
   is actually quite high. if you were conversing with a friend and they
   misinterpreted 1 of every 20 words, you   d have a tough time
   communicating.

   one reason for this is that the evaluation is done context-free. in
   real life we use many other cues to help us understand what someone is
   saying. some examples of context that people use but speech recognizers
   don   t include:
     * the history of the conversation and the topic being discussed.
     * visual cues of the person speaking including facial expressions and
       lip movement.
     * prior knowledge about the person we are speaking with.

   currently, android   s speech recognizer has knowledge of your contact
   list so it can recognize your friends    names.^[8]4 the voice search in
   maps products uses geolocation to narrow down the possible
   points-of-interest you might be asking to navigate to.^[9]5

   the accuracy of asr systems definitely improves when incorporating this
   type of signal. but, we   ve just begun to scratch the surface on the
   type of context we can include and how it   s used.

deployment

   the recent improvements in conversational speech are not deployable.
   when thinking about what makes a new speech algorithm deployable, it   s
   helpful to think in terms of latency and compute. the two are related,
   as algorithms which increase compute tend to increase latency. but for
   simplicity i   ll discuss each separately.

   latency: with latency, i mean the time from when the user is done
   speaking to when the transcription is complete. low latency is a common
   product constraint in asr. it can significantly impact the user
   experience. latency requirements in the tens of milliseconds aren   t
   uncommon for asr systems. while this may sound extreme, remember that
   producing the transcript is usually the first step in a series of
   expensive computations. for example in voice search the actual
   web-scale search has to be done after the id103.

   bidirectional recurrent layers are a good example of a latency killing
   improvement. all the recent state-of-the-art results in conversational
   speech use them. the problem is we can   t compute anything after the
   first bidirectional layer until the user is done speaking. so the
   latency scales with the length of the utterance.
   [forward_only.svg] [bidirectional.svg]
   left: with a forward only recurrence we can start computing the
   transcription immediately. right: with a bidirectional recurrence we
   have to wait until all the speech arrives before beginning to compute
   the transcription.

   a good way to efficiently incorporate future information in speech
   recognition is still an open problem.

   compute: the amount of computational power needed to transcribe an
   utterance is an economic constraint. we have to consider the
   bang-for-buck of every accuracy improvement to a speech recognizer. if
   an improvement doesn   t meet an economical threshold, then it can   t be
   deployed.

   a classic example of a consistent improvement that never gets deployed
   is an ensemble. the 1% or 2% error reduction is rarely worth the 2-8x
   increase in compute. modern id56 language models are also usually in
   this category since they are very expensive to use in a id125;
   though i expect this will change in the future.

   as a caveat, i   m not suggesting research which improves accuracy at
   great computational cost isn   t useful. we   ve seen the pattern of    first
   slow but accurate, then fast    work well before. the point is just that
   until an improvement is sufficiently fast, it   s not usable.

the next five years

   there are still many open and challenging problems in speech
   recognition. these include:
     * broadening the capabilities to new domains, accents and far-field,
       low snr speech.
     * incorporating more context into the recognition process.
     * diarisation and source-separation.
     * semantic error rates and innovative methods for evaluating
       recognizers.
     * super low-latency and efficient id136.

   i look forward to the next five years of progress on these and other
   fronts.

acknowledgements

   thanks to [10]@mrhannun for useful feedback and edits.

edit

   hacker news [11]discussion.

footnotes

    1. just ask anyone with a [12]scottish accent. [13]   
    2. these results are from [14]amodei et al, 2016. the accented speech
       comes from [15]voxforge. the noise-free and noisy speech comes from
       the third [16]chime challenge. [17]   
    3. [18]stolcke and droppo, 2017 [19]   
    4. see [20]aleksic et al., 2015 for an example of how to improve
       contact name recognition. [21]   
    5. see [22]chelba et al., 2015 for an example of how to incorporate
       speaker location. [23]   

   please enable javascript to view the [24]comments powered by disqus.

references

   visible links
   1. https://awni.github.io/feed.xml
   2. https://awni.github.io/
   3. https://awni.github.io/
   4. https://awni.github.io/about
   5. https://awni.github.io/speech-recognition/#fn:scottish_accent
   6. https://awni.github.io/speech-recognition/#fn:data_details
   7. https://awni.github.io/speech-recognition/#fn:human_comparison
   8. https://awni.github.io/speech-recognition/#fn:contacts
   9. https://awni.github.io/speech-recognition/#fn:geo_location
  10. https://twitter.com/mrhannun
  11. https://news.ycombinator.com/item?id=15542669
  12. https://www.youtube.com/watch?v=5ffroyhtjqq
  13. https://awni.github.io/speech-recognition/#fnref:scottish_accent
  14. https://arxiv.org/abs/1512.02595
  15. http://www.voxforge.org/
  16. http://ieeexplore.ieee.org/document/7404837/
  17. https://awni.github.io/speech-recognition/#fnref:data_details
  18. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-revised2.pdf
  19. https://awni.github.io/speech-recognition/#fnref:human_comparison
  20. http://ieeexplore.ieee.org/document/7178957/
  21. https://awni.github.io/speech-recognition/#fnref:contacts
  22. https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43817.pdf
  23. https://awni.github.io/speech-recognition/#fnref:geo_location
  24. http://disqus.com/?ref_noscript

   hidden links:
  26. https://github.com/awni
  27. https://www.twitter.com/awnihannun
