   #[1]adventures in machine learning    a pytorch tutorial     deep learning
   in python comments feed [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

a pytorch tutorial     deep learning in python

   by [9]admin | [10]deep learning

     * you are here:
     * [11]home
     * [12]deep learning
     * [13]a pytorch tutorial     deep learning in python

   oct 26
   [14]3
   pytorch tutorial - fully connected neural network example architecture

   so     if you   re a follower of this blog and you   ve been trying out your
   own deep learning networks in [15]tensorflow and [16]keras, you   ve
   probably come across the somewhat frustrating business of debugging
   these deep learning libraries. sure, they have python apis, but it   s
   kinda hard to figure out what exactly is happening when something goes
   wrong. they also don   t seem to play well with python libraries such as
   numpy, scipy, scikit-learn, cython and so on. enter
   the [17]pytorch deep learning library     one of it   s [18]purported
   benefits is that is a deep learning library that is more at home in
   python, which, for a python aficionado like myself, sounds great. it
   also has nifty features such as dynamic computational graph
   construction as opposed to the static computational graphs present in
   tensorflow and keras (for more on computational graphs, see below).
   it   s also on the up and up, with its development supported by companies
   such as facebook, twitter, nvidia and so on. so let   s dive into it in
   this pytorch tutorial.

   the first question to consider     is it better than tensorflow? that   s a
   fairly subjective judgement     performance-wise there doesn   t appear to
   be a great deal of difference. check out [19]this article for a quick
   comparison. in any case, its clear the pytorch is here to stay and is
   likely to be a real contender in the    contest    between deep learning
   libraries, so let   s kick start our learning of it. i   ll leave it to you
   to decide which is    better   .

   in this pytorch tutorial we will introduce some of the core features of
   pytorch, and build a fairly simple densely connected neural network to
   classify hand-written digits. to learn how to build more complex models
   in pytorch, check out my post [20]convolutional neural networks
   tutorial in pytorch.
     __________________________________________________________________

   recommended online course: if you   re more of a video course learner,
   check out this inexpensive, highly rated, udemy course: [21]practical
   deep learning with pytorch
   [show?id=jbc0n5zkdzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0]
     __________________________________________________________________

a pytorch tutorial     the basics

   in this section, we   ll go through the basic ideas of pytorch starting
   at tensors and computational graphs and finishing at the variable class
   and the pytorch autograd functionality.

installing on windows

   for starters, if you are a windows user like myself, you   ll find that
   there is no straight-forward installation options for that operating
   system on the [22]pytorch website. however, there is a successful way
   to do it, check out [23]this website for instructions. it   s well worth
   the effort to get this library installed if you are a windows user like
   myself.

computational graphs

   the first thing to understand about any deep learning library is the
   idea of a computational graph. a computational graph is a set of
   calculations, which are called nodes, and these nodes are connected in
   a directional ordering of computation. in other words, some nodes are
   dependent on other nodes for their input, and these nodes in turn
   output the results of their calculations to other nodes. a simple
   example of a computational graph for the calculation $a = (b + c) * (c
   + 2)$ can be seen below     we can break this calculation up into the
   following steps/nodes:

   \begin{align}
   d &= b + c \\
   e &= c + 2 \\
   a &= d * e
   \end{align}
   pytorch tutorial - simple computational graph

   simple computational graph


   the benefits of using a computational graph is that each node is like
   its own independently functioning piece of code (once it receives all
   its required inputs). this allows various performance optimizations to
   be performed in running the calculations such as threading and multiple
   processing / parallelism. all the major deep learning frameworks
   (tensorflow, theano, pytorch etc.) involve constructing such
   computational graphs, through which neural network operations can be
   built and through which gradients can be back-propagated (if you   re
   unfamiliar with back-propagation, see my [24]neural networks tutorial).

tensors

   tensors are matrix-like data structures which are essential components
   in deep learning libraries and efficient computation. graphical
   processing units (gpus) are especially effective at calculating
   operations between tensors, and this has spurred the surge in deep
   learning capability in recent times. in pytorch, tensors can be
   declared simply in a number of ways:
import torch
x = torch.tensor(2, 3)

   this code creates a tensor of size (2, 3)     i.e. 2 rows and 3 columns,
   filled with zero float values i.e:
 0  0  0
 0  0  0
[torch.floattensor of size 2x3]

   we can also create tensors filled random float values:
x = torch.rand(2, 3)

   multiplying tensors, adding them and so forth is straight-forward:
x = torch.ones(2,3)
y = torch.ones(2,3) * 2
x + y

   this returns:
 3  3  3
 3  3  3
[torch.floattensor of size 2x3]

   another great thing is the numpy slice functionality that is available
       for instance y[:, 1]
y[:,1] = y[:,1] + 1

   this returns:
 2  3  2
 2  3  2
[torch.floattensor of size 2x3]

   now you know how to create tensors and manipulate them in pytorch, in
   the next step of this pytorch tutorial let   s look at something a bit
   more complicated.

autograd in pytorch

   in any deep learning library, there needs to be a mechanism where error
   gradients are calculated and back-propagated through the computational
   graph. this mechanism, called autograd in pytorch, is easily accessible
   and intuitive. the variable class is the main component of this
   autograd system in pytorch. this variable class wraps a tensor, and
   allows automatic gradient computation on the tensor when the
   .backward() function is called (more on this later). the object
   contains the data of the tensor, the gradient of the tensor (once
   computed with respect to some other value i.e. the loss) and also
   contains a reference to whatever function created the variable (if it
   is a user created function, this reference will be null).

   let   s create a variable from a simple tensor:
x = variable(torch.ones(2, 2) * 2, requires_grad=true)

   in the variable declaration above, we pass in a tensor of (2, 2)
   2-values and we specify that this variable requires a gradient. if we
   were using this in a neural network, this would mean that this variable
   would be trainable. if we set this flag to false, the variable would
   not be trained. for this simple example we aren   t training anything,
   but we do want to interrogate the gradient for this variable as will be
   shown below.

   next, let   s create another variable, constructed based on operations on
   our original variable x.
z = 2 * (x * x) + 5 * x

   to get the gradient of this operation with respect to x i.e. dz/dx we
   can analytically calculate this to by 4x +5. if all elements of x are
   2, then we should expect the gradient dz/dx to be a (2, 2) shaped
   tensor with 13-values. however, first we have to run the .backwards()
   operation to compute these gradients. of course, to compute gradients,
   we need to compute them with respect to something. in this case, we can
   supply a (2,2) tensor of 1-values to be what we compute the gradients
   against     so the calculation simply becomes d/dx:
z.backward(torch.ones(2, 2))
print(x.grad)

   this produces the following output:
variable containing:
 13  13
 13  13
[torch.floattensor of size 2x2]

   as you can observe, the gradient is equal to a (2, 2), 13-valued tensor
   as we predicted. note that the gradient is stored in the x variable, in
   the property .grad.

   now that we   ve covered the basics of tensors, variables and the
   autograd functionality within pytorch, we can move onto creating a
   simple neural network in pytorch which will showcase this functionality
   further.

creating a neural network in pytorch

   this section is the main show of this pytorch tutorial. to access the
   code for this tutorial, check out this website   s [25]github repository.
   here we will create a simple 4-layer  fully connected neural network
   (including an    input layer    and two hidden layers) to classify the
   hand-written digits of the mnist dataset. the architecture we   ll use
   can be seen in the figure below:
   pytorch tutorial - fully connected neural network example architecture

   fully connected neural network example architecture

   the input layer consists of 28 x 28 (=784) greyscale pixels which
   constitute the input data of the mnist data set. this input is then
   passed through two fully connected hidden layers, each with 200 nodes,
   with the nodes utilizing a [26]relu activation function. finally, we
   have an output layer with ten nodes corresponding to the 10 possible
   classes of hand-written digits (i.e. 0 to 9). we will use a [27]softmax
   output layer to perform this classification.

   let   s create the neural network.

the neural network class

   in order to create a neural network in pytorch, you need to use the
   included class nn.module. to use this base class, we also need to use
   python [28]class inheritance     this basically allows us to use all of
   the functionality of the nn.module base class, but still have
   overwriting capabilities of the base class for the model construction /
   forward pass through the network. some actual code will help explain:
import torch.nn as nn
import torch.nn.functional as f

class net(nn.module):
    def __init__(self):
        super(net, self).__init__()
        self.fc1 = nn.linear(28 * 28, 200)
        self.fc2 = nn.linear(200, 200)
        self.fc3 = nn.linear(200, 10)

   in the class definition, you can see the inheritance of the base class
   nn.module. then, in the first line of the class initialization (def
   __init__(self):) we have the required python super() function, which
   creates an instance of the base nn.module class. the following three
   lines is where we create our fully connected layers as per the
   architecture diagram. a fully connected neural network layer is
   represented by the nn.linear object, with the first argument in the
   definition being the number of nodes in layer l and the next argument
   being the number of nodes in layer l+1. as you can observer, the first
   layer takes the 28 x 28 input pixels and connects to the first 200 node
   hidden layer. then we have another 200 to 200 hidden layer, and finally
   a connection between the last hidden layer and the output layer (with
   10 nodes).

   now we   ve setup the    skeleton    of our network architecture, we have to
   define how data flows through out network. we do this by defining
   a forward() method in our class     this method overwrites a dummy method
   in the base class, and needs to be defined for each network:
def forward(self, x):
    x = f.relu(self.fc1(x))
    x = f.relu(self.fc2(x))
    x = self.fc3(x)
    return f.log_softmax(x)

   for the forward() method, we supply the input data x as the primary
   argument. we feed this into our first fully connected
   layer (self.fc1(x)) and then apply a relu activation to the nodes in
   this layer using f.relu(). because of the hierarchical nature of this
   network, we replace x at each stage, feeding it into the next layer. we
   do this through our three fully connected layers, except for the last
   one     instead of a relu activation we return a log softmax
      activation   . this, combined with the negative log likelihood loss
   function which will be defined later, gives us a multi-class cross
   id178 based id168 which we will use to train the network.

   so that   s it     we   ve defined our neural network. pretty easy right?

   the next step is to create an instance of this network architecture:
net = net()
print(net)

   when we print the instance of the class net, we get the following
   output:

     net (
     (fc1): linear (784 -> 200)
     (fc2): linear (200 -> 200)
     (fc3): linear (200 -> 10)
     )

   this is pretty handy as it confirms the structure of our network for
   us.

training the network

   next we have to setup an optimizer and a loss criterion:
# create a stochastic id119 optimizer
optimizer = optim.sgd(net.parameters(), lr=learning_rate, momentum=0.9)
# create a id168
criterion = nn.nllloss()

   in the first line, we create a stochastic id119 optimizer,
   and we specify the learning rate (which i   ve passed to this function as
   0.01) and a momentum of 0.9. the other ingredient we need to supply to
   our optimizer is all the parameters of our network     thankfully pytorch
   make supplying these parameters easy by the .parameters() method of the
   base nn.module class that we inherit from in the net class.

   next, we set our loss criterion to be the negative log likelihood loss
       this combined with our log softmax output from the neural network
   gives us an equivalent cross id178 loss for our 10 classification
   classes.

   now it   s time to train the network. during training, i will be
   extracting data from a data loader object which is included in the
   pytorch utilities module. i won   t go into the details here (i   ll leave
   that for a future post), but you can find the code on this site   s
   [29]github repository. this data loader will supply batches of input
   and target data which we   ll supply to our network and id168
   respectively. here   s the full training code:
# run the main training loop
for epoch in range(epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = variable(data), variable(target)
        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)
        data = data.view(-1, 28*28)
        optimizer.zero_grad()
        net_out = net(data)
        loss = criterion(net_out, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('train epoch: {} [{}/{} ({:.0f}%)]\tloss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.data[0]))

   the outer training loop is the number of epochs, whereas the inner
   training loop runs through the entire training set in batch sizes which
   are specified in the code as batch_size. on the next line, we
   convert data and target into pytorch variables. the mnist input
   data-set which is supplied in the torchvision package (which you   ll
   need to install using pip if you run the code for this tutorial) has
   the size (batch_size, 1, 28, 28) when extracted from the data loader    
   this 4d tensor is more suited to [30]convolutional neural
   network architecture, and not so much our fully connected network.
   therefore we need to flatten out the (1, 28, 28) data to a single
   dimension of 28 x 28 =  784 input nodes.

   the .view() function operates on pytorch variables to reshape them. if
   we want to be agnostic about the size of a given dimension, we can use
   the    -1    notation in the size definition. so by using data.view(-1,
   28*28) we say that the second dimension must be equal to 28 x 28, but
   the first dimension should be calculated from the size of the original
   data variable. in practice, this means that data will now be of size
   (batch_size, 784). we can pass a batch of input data like this into our
   network and the magic of pytorch will do all the hard work by
   efficiently performing the required operations on the tensors.

   on the next line, we run optimizer.zero_grad()     this zeroes / resets
   all the gradients in the model, so that it is ready to go for the next
   back propagation pass. in other libraries this is performed implicitly,
   but in pytorch you have to remember to do it explicitly. let   s single
   out the next two lines:
net_out = net(data)
loss = criterion(net_out, target)

   the first line is where we pass the input data batch into the model    
   this will actually call the forward() method in our net class. after
   this line is run, the variable net_out will now hold the log softmax
   output of our neural network for the given data batch. that   s one of
   the great things about pytorch, you can activate whatever normal python
   debugger you usually use and instantly get a gauge of what is happening
   in your network. this is opposed to other deep learning libraries such
   as tensorflow and keras which require elaborate debugging sessions to
   be setup before you can check out what your network is actually
   producing. i hope you   ll play around with how useful this debugging is,
   by utilizing the code for this pytorch tutorial [31]here.

   the second line is where we get the negative log likelihood loss
   between the output of our network and our target batch data.

   let   s look at the next two lines:
loss.backward()
optimizer.step()

   the first line here runs a back-propagation operation from the loss
   variable backwards through the network. if you compare this with our
   review of the .backward() operation that we undertook earlier in this
   pytorch tutorial, you   ll notice that we aren   t supplying the
   .backward() operation with an argument. scalar variables, when we call
   .backward() on them, don   t require arguments     only tensors require a
   matching sized tensor argument to be passed to the .backward()
   operation.

   the next line is where we tell pytorch to execute a id119
   step based on the gradients calculated during the .backward()
   operation.

   finally, we print out some results every time we reach a certain number
   of iterations:
if batch_idx % log_interval == 0:
    print('train epoch: {} [{}/{} ({:.0f}%)]\tloss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.data[0]))

   this print function shows our progress through the epochs and also
   gives the network loss at that point in the training. note how you
   access the loss     you access the variable .data property, which in this
   case will be a single valued array. we access the scalar loss by
   executing loss.data[0].

   running this training loop you   ll get an output that looks something
   like this:

     train epoch: 9 [52000/60000 (87%)] loss: 0.015086

     train epoch: 9 [52000/60000 (87%)] loss: 0.015086

     train epoch: 9 [54000/60000 (90%)] loss: 0.030631

     train epoch: 9 [56000/60000 (93%)] loss: 0.052631

     train epoch: 9 [58000/60000 (97%)] loss: 0.052678

   after 10 epochs, you should get a loss value down around the <0.05
   magnitude.

testing the network

   to test the trained network on our test mnist data set, we can run the
   following code:
# run a test loop
test_loss = 0
correct = 0
for data, target in test_loader:
    data, target = variable(data, volatile=true), variable(target)
    data = data.view(-1, 28 * 28)
    net_out = net(data)
    # sum up batch loss
    test_loss += criterion(net_out, target).data[0]
    pred = net_out.data.max(1)[1]  # get the index of the max log-id203
    correct += pred.eq(target.data).sum()

test_loss /= len(test_loader.dataset)
print('\ntest set: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

   this loop is the same as the previous training loop up until the
   test_loss line     here we extract the network loss using the .data[0]
   property as before, but all in the same line. next, we have the pred
   line, where the data.max(1) method is used     this .max() method can
   return the index of the maximum value in a certain dimension of a
   tensor. now, the output of our neural network will be of size
   (batch_size, 10), where each value of the 10-length second dimension is
   a log id203 which the network assigns to each output class (i.e.
   it is the log id203 of whether the given image is a digit between
   0 and 9). so for each input sample/row in the batch, net_out.data will
   look something like this:
   [-1.3106e+01, -1.6731e+01, -1.1728e+01, -1.1995e+01, -1.5886e+01, -1.77
   00e+01, -2.4950e+01, -5.9817e-04, -1.3334e+01, -7.4527e+00]


   the value with the highest log id203 is the digit that the
   network considers to be the most probable given the input image     this
   is the best prediction of the class from the network. in the example of
   net_out.data above, it is the value -5.9817e-04 which is maximum, which
   corresponds to the digit    7   . so for this sample, the predicted digit
   is    7   . the .max(1) function will determine this maximum value in the
   second dimension (if we wanted the maximum in the first dimension, we   d
   supply an argument of 0) and returns both the maximum value that it has
   found, and the index that this maximum value was found at. it therefore
   has a size of (batch_size, 2)     in this case we are interested in the
   index where the maximum value is found at, therefore we access these
   values by calling .max(1)[1].

   now we have the prediction of the neural network for each sample in the
   batch determined, we can compare this with the actual target class from
   our training data, and count how many times in the batch the neural
   network got it right. we can use the pytorch .eq() function to do this,
   which compares the values in two tensors and if they match, returns a
   1. if they don   t match, it returns a 0:
correct += pred.eq(target.data).sum()

   by summing the output of the .eq() function, we get a count of the
   number of times the neural network has produced a correct output, and
   we take an accumulating sum of these correct predictions so that we can
   determine the overall accuracy of the network on our test data set.
   finally, after running through the test data in batches, we print out
   the averaged loss and accuracy:
test_loss /= len(test_loader.dataset)
print('\ntest set: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

   after training the network for 10 epochs, we get the following output
   from the above code on the test data:

     test set: average loss: 0.0003, accuracy: 9783/10000 (98%)

   a 98% accuracy     not bad!

   so there you have it     this pytorch tutorial has shown you the basic
   ideas in pytorch, from tensors to the autograd functionality, and
   finished with how to build a fully connected neural network using the
   nn.module. i hope it was helpful. if you   d like to learn more about
   pytorch, check out my post on [32]convolutional neural networks in
   pytorch.
     __________________________________________________________________

   recommended online course: if you   re more of a video course learner,
   check out this inexpensive, highly rated, udemy course: [33]practical
   deep learning with pytorch
   [show?id=jbc0n5zkdzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0]
     __________________________________________________________________


about the author

     luis jalabert says:
   [34]october 28, 2018 at 9:27 pm

   thank you so much!
   i   m just starting with pytorch, total noob, and as any rational person
   would do, i went to pytorch.org to follow their tutorials. they the
   tutorial with a full fledged convolutional deep network to classify the
   cifar10 images. needles to say, i barely understood anything.
   this tutorial is so much better for beginners, it actually explains
   what   s going on and what you are doing in every step. now everything is
   way clearer.
     * andy says:
       [35]october 28, 2018 at 10:12 pm
       thanks luis, glad it was a help
     * neda says:
       [36]november 22, 2018 at 4:28 pm
       i totally agree with luis. i did the same. first, i did that
       tutorial and i was ready the pytorch doc, but difficult to
       understand for beginners. this tutorial is well written and
       clarifies almost everything. thank you very much andy.

   ____________________ (button)

   recent posts
     * [37]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [38]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [39]keras, eager and tensorflow 2.0     a new tf paradigm
     * [40]introduction to tensorboard and tensorflow visualization
     * [41]tensorflow eager tutorial

   recent comments
     * andry on [42]neural networks tutorial     a pathway to deep learning
     * sandipan on [43]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [44]neural networks tutorial     a pathway to deep learning
     * martin on [45]neural networks tutorial     a pathway to deep learning
     * uri on [46]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [47]march 2019
     * [48]january 2019
     * [49]october 2018
     * [50]september 2018
     * [51]august 2018
     * [52]july 2018
     * [53]june 2018
     * [54]may 2018
     * [55]april 2018
     * [56]march 2018
     * [57]february 2018
     * [58]november 2017
     * [59]october 2017
     * [60]september 2017
     * [61]august 2017
     * [62]july 2017
     * [63]may 2017
     * [64]april 2017
     * [65]march 2017

   categories
     * [66]amazon aws
     * [67]cntk
     * [68]convolutional neural networks
     * [69]cross id178
     * [70]deep learning
     * [71]gensim
     * [72]gpus
     * [73]keras
     * [74]id168s
     * [75]lstms
     * [76]neural networks
     * [77]nlp
     * [78]optimisation
     * [79]pytorch
     * [80]recurrent neural networks
     * [81]id23
     * [82]tensorboard
     * [83]tensorflow
     * [84]tensorflow 2.0
     * [85]weight initialization
     * [86]id97

   meta
     * [87]log in
     * [88]entries rss
     * [89]comments rss
     * [90]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [91]thrive themes | powered by [92]wordpress

   (button) close dialog

   session expired

   [93]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[94]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/
  13. https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/
  14. http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/#comments
  15. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  16. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  17. http://pytorch.org/
  18. http://pytorch.org/about/
  19. https://www.forbes.com/sites/quora/2017/07/10/is-pytorch-better-than-tensorflow/
  20. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/
  21. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.1259546&type=2&murl=https://www.udemy.com/practical-deep-learning-with-pytorch/
  22. http://pytorch.org/
  23. https://www.superdatascience.com/pytorch/
  24. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  25. https://github.com/adventuresinml/adventures-in-ml-code
  26. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  27. https://en.wikipedia.org/wiki/softmax_function
  28. https://docs.python.org/2/tutorial/classes.html
  29. https://github.com/adventuresinml/adventures-in-ml-code
  30. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  31. https://github.com/adventuresinml/adventures-in-ml-code
  32. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/
  33. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.1259546&type=2&murl=https://www.udemy.com/practical-deep-learning-with-pytorch/
  34. https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/#comments/5099
  35. https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/#comments/5100
  36. https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/#comments/5102
  37. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  38. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  39. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  40. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  41. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  42. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  43. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  44. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  45. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  46. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  47. https://adventuresinmachinelearning.com/2019/03/
  48. https://adventuresinmachinelearning.com/2019/01/
  49. https://adventuresinmachinelearning.com/2018/10/
  50. https://adventuresinmachinelearning.com/2018/09/
  51. https://adventuresinmachinelearning.com/2018/08/
  52. https://adventuresinmachinelearning.com/2018/07/
  53. https://adventuresinmachinelearning.com/2018/06/
  54. https://adventuresinmachinelearning.com/2018/05/
  55. https://adventuresinmachinelearning.com/2018/04/
  56. https://adventuresinmachinelearning.com/2018/03/
  57. https://adventuresinmachinelearning.com/2018/02/
  58. https://adventuresinmachinelearning.com/2017/11/
  59. https://adventuresinmachinelearning.com/2017/10/
  60. https://adventuresinmachinelearning.com/2017/09/
  61. https://adventuresinmachinelearning.com/2017/08/
  62. https://adventuresinmachinelearning.com/2017/07/
  63. https://adventuresinmachinelearning.com/2017/05/
  64. https://adventuresinmachinelearning.com/2017/04/
  65. https://adventuresinmachinelearning.com/2017/03/
  66. https://adventuresinmachinelearning.com/category/amazon-aws/
  67. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  68. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  69. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  70. https://adventuresinmachinelearning.com/category/deep-learning/
  71. https://adventuresinmachinelearning.com/category/nlp/gensim/
  72. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  73. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  74. https://adventuresinmachinelearning.com/category/loss-functions/
  75. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  76. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  77. https://adventuresinmachinelearning.com/category/nlp/
  78. https://adventuresinmachinelearning.com/category/optimisation/
  79. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  80. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  81. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  82. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  83. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  84. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  85. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  86. https://adventuresinmachinelearning.com/category/nlp/id97/
  87. https://adventuresinmachinelearning.com/wp-login.php
  88. https://adventuresinmachinelearning.com/feed/
  89. https://adventuresinmachinelearning.com/comments/feed/
  90. https://wordpress.org/
  91. https://www.thrivethemes.com/
  92. http://www.wordpress.org/
  93. https://adventuresinmachinelearning.com/wp-login.php
  94. http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/

   hidden links:
  96. https://adventuresinmachinelearning.com/author/admin/
