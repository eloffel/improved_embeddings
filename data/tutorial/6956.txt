   #[1]         

[2]         
ml     design

     * [3]archive   
     *
     *
     *
     *
     *

a visual guide to evolution strategies

   october 29, 2017

                               [es_bear.jpeg]

                          survival of the fittest.

   in this post i explain how evolution strategies (es) work with the aid
   of a few visual examples. i try to keep the equations light, and i
   provide links to original articles if the reader wishes to understand
   more details. this is the first post in a series of articles, where i
   plan to show how to apply these algorithms to a range of tasks from
   mnist, openai gym, roboschool to pybullet environments.

introduction

   neural network models are highly expressive and flexible, and if we are
   able to find a suitable set of model parameters, we can use neural nets
   to solve many challenging problems. deep learning   s success largely
   comes from the ability to use the id26 algorithm to
   efficiently calculate the gradient of an objective function over each
   model parameter. with these gradients, we can efficiently search over
   the parameter space to find a solution that is often good enough for
   our neural net to accomplish difficult tasks.

   however, there are many problems where the id26 algorithm
   cannot be used. for example, in id23 (rl) problems,
   we can also train a neural network to make decisions to perform a
   sequence of actions to accomplish some task in an environment. however,
   it is not trivial to estimate the gradient of reward signals given to
   the agent in the future to an action performed by the agent right now,
   especially if the reward is realised many timesteps in the future. even
   if we are able to calculate accurate gradients, there is also the issue
   of being stuck in a local optimum, which exists many for rl tasks.

                          [biped_local_optima.gif]

                          stuck in a local optimum.

   a whole area within rl is devoted to studying this credit-assignment
   problem, and great progress has been made in recent years. however,
   credit assignment is still difficult when the reward signals are
   sparse. in the real world, rewards can be sparse and noisy. sometimes
   we are given just a single reward, like a bonus check at the end of the
   year, and depending on our employer, it may be difficult to figure out
   exactly why it is so low. for these problems, rather than rely on a
   very noisy and possibly meaningless gradient estimate of the future to
   our policy, we might as well just ignore any gradient information, and
   attempt to use black-box optimisation techniques such as genetic
   algorithms (ga) or es.

   openai published a paper called [4]evolution strategies as a scalable
   alternative to id23 where they showed that evolution
   strategies, while being less data efficient than rl, offer many
   benefits. the ability to abandon gradient calculation allows such
   algorithms to be evaluated more efficiently. it is also easy to
   distribute the computation for an es algorithm to thousands of machines
   for parallel computation. by running the algorithm from scratch many
   times, they also showed that policies discovered using es tend to be
   more diverse compared to policies discovered by rl algorithms.

   i would like to point out that even for the problem of identifying a
   machine learning model, such as designing a neural net   s architecture,
   is one where we cannot directly compute gradients. while [5]rl,
   [6]evolution, [7]ga etc., can be applied to search in the space of
   model architectures, in this post, i will focus only on applying these
   algorithms to search for parameters of a pre-defined model.

what is an evolution strategy?

                          [rastrigin_function.png]
      two-dimensional rastrigin function has many local optima (source:
                               [8]wikipedia).

   the diagrams below are top-down plots of shifted 2d [9]schaffer and
   rastrigin functions, two of several simple toy problems used for
   testing continuous black-box optimisation algorithms. lighter regions
   of the plots represent higher values of . as you can see, there are
   many local optimums in this function. our job is to find a set of model
   parameters , such that is as close as possible to the global maximum.

                            schaffer-2d function

   [schaffer_label.png]

                            rastrigin-2d function

   [rastrigin_label.png]

   although there are many definitions of evolution strategies, we can
   define an evolution strategy as an algorithm that provides the user a
   set of candidate solutions to evaluate a problem. the evaluation is
   based on an objective function that takes a given solution and returns
   a single fitness value. based on the fitness results of the current
   solutions, the algorithm will then produce the next generation of
   candidate solutions that is more likely to produce even better results
   than the current generation. the iterative process will stop once the
   best known solution is satisfactory for the user.

   given an evolution strategy algorithm called evolutionstrategy, we can
   use in the following way:
     __________________________________________________________________

   solver = evolutionstrategy()

   while true:

     # ask the es to give us a set of candidate solutions
     solutions = solver.ask()

     # create an array to hold the fitness results.
     fitness_list = np.zeros(solver.popsize)

     # evaluate the fitness for each given solution.
     for i in range(solver.popsize):
       fitness_list[i] = evaluate(solutions[i])

     # give list of fitness results back to es
     solver.tell(fitness_list)

     # get best parameter, fitness from es
     best_solution, best_fitness = solver.result()

     if best_fitness > my_required_fitness:
       break
     __________________________________________________________________

   although the size of the population is usually held constant for each
   generation, they don   t need to be. the es can generate as many
   candidate solutions as we want, because the solutions produced by an es
   are sampled from a distribution whose parameters are being updated by
   the es at each generation. i will explain this sampling process with an
   example of a simple evolution strategy.

simple evolution strategy

   one of the simplest evolution strategy we can imagine will just sample
   a set of solutions from a normal distribution, with a mean and a fixed
   standard deviation . in our 2d problem, and . initially, is set at the
   origin. after the fitness results are evaluated, we set to the best
   solution in the population, and sample the next generation of solutions
   around this new mean. this is how the algorithm behaves over 20
   generations on the two problems mentioned earlier:

                        [simplees.gif] [simplees.gif]

   in the visualisation above, the green dot indicates the mean of the
   distribution at each generation, the blue dots are the sampled
   solutions, and the red dot is the best solution found so far by our
   algorithm.

   this simple algorithm will generally only work for simple problems.
   given its greedy nature, it throws away all but the best solution, and
   can be prone to be stuck at a local optimum for more complicated
   problems. it would be beneficial to sample the next generation from a
   id203 distribution that represents a more diverse set of ideas,
   rather than just from the best solution from the current generation.

simple genetic algorithm

   one of the oldest black-box optimisation algorithms is the genetic
   algorithm. there are many variations with many degrees of
   sophistication, but i will illustrate the simplest version here.

   the idea is quite simple: keep only 10% of the best performing
   solutions in the current generation, and let the rest of the population
   die. in the next generation, to sample a new solution is to randomly
   select two solutions from the survivors of the previous generation, and
   recombine their parameters to form a new solution. this crossover
   recombination process uses a coin toss to determine which parent to
   take each parameter from. in the case of our 2d toy function, our new
   solution might inherit or from either parents with 50% chance. gaussian
   noise with a fixed standard deviation will also be injected into each
   new solution after this recombination process.

                        [simplega.gif] [simplega.gif]

   the figure above illustrates how the simple genetic algorithm works.
   the green dots represent members of the elite population from the
   previous generation, the blue dots are the offsprings to form the set
   of candidate solutions, and the red dot is the best solution.

   id107 help diversity by keeping track of a diverse set of
   candidate solutions to reproduce the next generation. however, in
   practice, most of the solutions in the elite surviving population tend
   to converge to a local optimum over time. there are more sophisticated
   variations of ga out there, such as [10]cosyne, [11]esp, and [12]neat,
   where the idea is to cluster similar solutions in the population
   together into different species, to maintain better diversity over
   time.

covariance-matrix adaptation evolution strategy (cma-es)

   a shortcoming of both the simple es and simple ga is that our standard
   deviation noise parameter is fixed. there are times when we want to
   explore more and increase the standard deviation of our search space,
   and there are times when we are confident we are close to a good optima
   and just want to fine tune the solution. we basically want our search
   process to behave like this:

                           [cmaes.gif] [cmaes.gif]

   amazing isn   it it? the search process shown in the figure above is
   produced by [13]covariance-matrix adaptation evolution strategy
   (cma-es). cma-es an algorithm that can take the results of each
   generation, and adaptively increase or decrease the search space for
   the next generation. it will not only adapt for the mean and sigma
   parameters, but will calculate the entire covariance matrix of the
   parameter space. at each generation, cma-es provides the parameters of
   a multi-variate normal distribution to sample solutions from. so how
   does it know how to increase or decrease the search space?

   before we discuss its methodology, let   s review how to estimate a
   [14]covariance matrix. this will be important to understand cma-es   s
   methodology later on. if we want to estimate the covariance matrix of
   our entire sampled population of size of , we can do so using the set
   of equations below to calculate the maximum likelihood estimate of a
   covariance matrix . we first calculate the means of each of the and in
   our population:

   the terms of the 2x2 covariance matrix will be:

   of course, these resulting mean estimates and , and covariance terms ,
   , will just be an estimate to the actual covariance matrix that we
   originally sampled from, and not particularly useful to us.

   cma-es modifies the above covariance calculation formula in a clever
   way to make it adapt well to an optimisation problem. i will go over
   how it does this step-by-step. firstly, it focuses on the best
   solutions in the current generation. for simplicity let   s set to be the
   best 25% of solutions. after sorting the solutions based on fitness, we
   calculate the mean of the next generation as the average of only the
   best 25% of the solutions in current population , i.e.:

   next, we use only the best 25% of the solutions to estimate the
   covariance matrix of the next generation, but the clever hack here is
   that it uses the current generation   s , rather than the updated
   parameters that we had just calculated, in the calculation:

   armed with a set of , , , , and parameters for the next generation , we
   can now sample the next generation of candidate solutions.

   below is a set of figures to visually illustrate how it uses the
   results from the current generation to construct the solutions in the
   next generation :

   [cmaes_step1.png]

                                   step 1

   [cmaes_step2.png]

                                   step 2

   [cmaes_step3.png]

                                   step 3

   [cmaes_step4.png]

                                   step 4

    1. calculate the fitness score of each candidate solution in
       generation .
    2. isolates the best 25% of the population in generation , in purple.
    3. using only the best solutions, along with the mean of the current
       generation (the green dot), calculate the covariance matrix of the
       next generation.
    4. sample a new set of candidate solutions using the updated mean and
       covariance matrix .

   let   s visualise the scheme one more time, on the entire search process
   on both problems:

                          [cmaes2.gif] [cmaes2.gif]

   because cma-es can adapt both its mean and covariance matrix using
   information from the best solutions, it can decide to cast a wider net
   when the best solutions are far away, or narrow the search space when
   the best solutions are close by. my description of the cma-es algorithm
   for a 2d toy problem is highly simplified to get the idea across. for
   more details, i suggest reading the [15]cma-es tutorial prepared by
   nikolaus hansen, the author of cma-es.

   this algorithm is one of the most popular gradient-free optimisation
   algorithms out there, and has been the algorithm of choice for many
   researchers and practitioners alike. the only real drawback is the
   performance if the number of model parameters we need to solve for is
   large, as the covariance calculation is , although recently there has
   been approximations to make it . cma-es is my algorithm of choice when
   the search space is less than a thousand parameters. i found it still
   usable up to ~ 10k parameters if i   m willing to be patient.

natural evolution strategies
     __________________________________________________________________

   imagine if you had built an artificial life simulator, and you sample a
   different neural network to control the behavior of each ant inside an
   ant colony. using the simple evolution strategy for this task will
   optimise for traits and behaviours that benefit individual ants, and
   with each successive generation, our population will be full of alpha
   ants who only care about their own well-being.

   instead of using a rule that is based on the survival of the fittest
   ants, what if you take an alternative approach where you take the sum
   of all fitness values of the entire ant population, and optimise for
   this sum instead to maximise the well-being of the entire ant
   population over successive generations? well, you would end up creating
   a marxist utopia.
     __________________________________________________________________

   a perceived weakness of the algorithms mentioned so far is that they
   discard the majority of the solutions and only keep the best solutions.
   weak solutions contain information about what not to do, and this is
   valuable information to calculate a better estimate for the next
   generation.

   many people who studied rl are familiar with the [16]reinforce paper.
   in this 1992 paper, williams outlined an approach to estimate the
   gradient of the expected rewards with respect to the model parameters
   of a policy neural network. this paper also proposed using reinforce as
   an evolution strategy, in section 6 of the paper. this special case of
   reinforce-es was expanded later on in [17]parameter-exploring policy
   gradients (pepg, 2009) and [18]natural evolution strategies (nes,
   2014).

   in this approach, we want to use all of the information from each
   member of the population, good or bad, for estimating a gradient signal
   that can move the entire population to a better direction in the next
   generation. since we are estimating a gradient, we can also use this
   gradient in a standard sgd update rule typically used for deep
   learning. we can even use this estimated gradient with momentum sgd,
   rmsprop, or adam if we want to.

   the idea is to maximise the expected value of the fitness score of a
   sampled solution. if the expected result is good enough, then the best
   performing member within a sampled population will be even better, so
   optimising for the expectation might be a sensible approach. maximising
   the expected fitness score of a sampled solution is almost the same as
   maximising the total fitness score of the entire population.

   if is a solution vector sampled from a id203 distribution
   function , we can define the expected value of the objective function
   as:

   where are the parameters of the id203 distribution function. for
   example, if is a normal distribution, then would be and . for our
   simple 2d toy problems, each ensemble is a 2d vector .

   the [19]nes paper contains a nice derivation of the gradient of with
   respect to . using the same log-likelihood trick as in the reinforce
   algorithm allows us to calculate the gradient of :

   in a population size of , where we have solutions , , , we can estimate
   this gradient as a summation:

   with this gradient , we can use a learning rate parameter (such as
   0.01) and start optimising the parameters of pdf so that our sampled
   solutions will likely get higher fitness scores on the objective
   function . using sgd (or adam), we can update for the next generation:

   and sample a new set of candidate solutions from this updated pdf, and
   continue until we arrive at a satisfactory solution.

   in section 6 of the [20]reinforce paper, williams derived closed-form
   formulas of the gradient , for the special case where is a factored
   multi-variate normal distribution (i.e., the correlation parameters are
   zero). in this special case, are the and vectors. therefore, each
   element of a solution can be sampled from a univariate normal
   distribution .

   the closed-form formulas for , for each individual element of vector on
   each solution in the population can be derived as:

   for clarity, i use the index of , to count across parameter space, and
   this is not to be confused with superscript , used to count across each
   sampled member of the population. for our 2d problems, , , , , , in
   this context.

   these two formulas can be plugged back into the approximate gradient
   formula to derive explicit update rules for and . in the papers
   mentioned above, they derived more explicit update rules, incorporated
   a baseline, and introduced other tricks such as antithetic sampling in
   pepg, which is what my implementation is based on. nes proposed
   incorporating the inverse of the fisher information matrix into the
   gradient update rule. but the concept is basically the same as other es
   algorithms, where we update the mean and standard deviation of a
   multi-variate normal distribution at each new generation, and sample a
   new set of solutions from the updated distribution. below is a
   visualization of this algorithm in action, following the formulas
   described above:

                            [pepg.gif] [pepg.gif]

   we see that this algorithm is able to dynamically change the    s to
   explore or fine tune the solution space as needed. unlike cma-es, there
   is no correlation structure in our implementation, so we don   t get the
   diagonal ellipse samples, only the vertical or horizontal ones,
   although in principle we can derive update rules to incorporate the
   entire covariance matrix if we needed to, at the expense of
   computational efficiency.

   i like this algorithm because like cma-es, the    s can adapt so our
   search space can be expanded or narrowed over time. because the
   correlation parameter is not used in this implementation, the
   efficiency of the algorithm is so i use pepg if the performance of
   cma-es becomes an issue. i usually use pepg when the number of model
   parameters exceed several thousand.

openai evolution strategy

   in openai   s [21]paper, they implement an evolution strategy that is a
   special case of the reinforce-es algorithm outlined earlier. in
   particular, is fixed to a constant number, and only the parameter is
   updated at each generation. below is how this strategy looks like, with
   a constant parameter:

                           [openes.gif] [oes.gif]

   in addition to the simplification, this paper also proposed a
   modification of the update rule that is suitable for parallel
   computation across different worker machines. in their update rule, a
   large grid of random numbers have been pre-computed using a fixed seed.
   by doing this, each worker can reproduce the parameters of every other
   worker over time, and each worker needs only to communicate a single
   number, the final fitness result, to all of the other workers. this is
   important if we want to scale evolution strategies to thousands or even
   a million workers located on different machines, since while it may not
   be feasible to transmit an entire solution vector a million times at
   each generation update, it may be feasible to transmit only the final
   fitness results. in the paper, they showed that by using 1440 workers
   on amazon ec2 they were able to solve the mujoco humanoid walking task
   in ~ 10 minutes.

   i think in principle, this parallel update rule should work with the
   original algorithm where they can also adapt , but perhaps in practice,
   they wanted to keep the number of moving parts to a minimum for
   large-scale parallel computing experiments. this inspiring paper also
   discussed many other practical aspects of deploying es for rl-style
   tasks, and i highly recommend going through it to learn more.

fitness shaping

   most of the algorithms above are usually combined with a fitness
   shaping method, such as the rank-based fitness shaping method i will
   discuss here. fitness shaping allows us to avoid outliers in the
   population from dominating the approximate gradient calculation
   mentioned earlier:

   if a particular is much larger than other in the population, then the
   gradient might become dominated by this outliers and increase the
   chance of the algorithm being stuck in a local optimum. to mitigate
   this, one can apply a rank transformation of the fitness. rather than
   use the actual fitness function, we would rank the results and use an
   augmented fitness function which is proportional to the solution   s rank
   in the population. below is a comparison of what the original set of
   fitness may look like, and what the ranked fitness looks like:

                            [ranked_fitness.svg]

   what this means is supposed we have a population size of 101. we would
   evaluate each population to the actual fitness function, and then sort
   the solutions based by their fitness. we will assign an augmented
   fitness value of -0.50 to the worse performer, -0.49 to the second
   worse solution,    , 0.49 to the second best solution, and finally a
   fitness value of 0.50 to the best solution. this augmented set of
   fitness values will be used to calculate the gradient update, instead
   of the actual fitness values. in a way, it is a similar to just
   applying batch id172 to the results, but more direct. there are
   alternative methods for fitness shaping but they all basically give
   similar results in the end.

   i find fitness shaping to be very useful for rl tasks if the objective
   function is non-deterministic for a given policy network, which is
   often the cases on rl environments where maps are randomly generated
   and various opponents have random policies. it is less useful for
   optimising for well-behaved functions that are deterministic, and the
   use of fitness shaping can sometimes slow down the time it takes to
   find a good solution.

mnist

   although es might be a way to search for more novel solutions that are
   difficult for gradient-based methods to find, it still vastly
   underperforms gradient-based methods on many problems where we can
   calculate high quality gradients. for instance, only an idiot would
   attempt to use a genetic algorithm for image classification. but
   sometimes [22]such people do exist in the world, and sometimes these
   explorations can be fruitful!

   since all ml algorithms should be tested on mnist, i also tried to
   apply these various es algorithms to find weights for a small, simple
   2-layer convnet used to classify mnist, just to see where we stand
   compared to sgd. the convnet only has ~ 11k parameters so we can
   accommodate the slower cma-es algorithm. the code and the experiments
   are available [23]here.

   below are the results for various es methods, using a population size
   of 101, over 300 epochs. we keep track of the model parameters that
   performed best on the entire training set at the end of each epoch, and
   evaluate this model once on the test set after 300 epochs. it is
   interesting how sometimes the test set   s accuracy is higher than the
   training set for the models that have lower scores.
            method          train set test set
   adam (backprop) baseline 99.8      98.9
   simple ga                82.1      82.4
   cma-es                   98.4      98.1
   openai-es                96.0      96.2
   pepg                     98.5      98.0

                             [mnist_results.svg]

   we should take these results with a grain of salt, since they are based
   on a single run, rather than the average of 5-10 runs. the results
   based on a single-run seem to indicate that cma-es is the best at the
   mnist task, but the pepg algorithm is not that far off. both of these
   algorithms achieved ~ 98% test accuracy, 1% lower than the sgd/adam
   baseline. perhaps the ability to dynamically alter its covariance
   matrix, and standard deviation parameters over each generation allowed
   it to fine-tune its weights better than openai   s simpler variation.

try it yourself

   there are probably open source implementations of all of the algorithms
   described in this article. the author of cma-es, nikolaus hansen, has
   been maintaining a numpy-based implementation of [24]cma-es with lots
   of bells and whistles. his python implementation introduced me to the
   training loop interface described earlier. since this interface is
   quite easy to use, i also implemented the other algorithms such as
   simple genetic algorithm, pepg, and openai   s es using the same
   interface, and put it in a small python file called es.py, and also
   wrapped the original cma-es library in this small library. this way, i
   can quickly compare different es algorithms by just changing one line:
     __________________________________________________________________

   import es

   #solver = es.simplega(...)
   #solver = es.pepg(...)
   #solver = es.openes(...)
   solver = es.cmaes(...)

   while true:

     solutions = solver.ask()

     fitness_list = np.zeros(solver.popsize)

     for i in range(solver.popsize):
       fitness_list[i] = evaluate(solutions[i])

     solver.tell(fitness_list)

     result = solver.result()

     if result[1] > my_required_fitness:
       break
     __________________________________________________________________

   you can look at es.py on [25]github and the ipython notebook
   [26]examples using the various es algorithms.

   in this [27]ipython notebook that accompanies es.py, i show how to use
   the es solvers in es.py to solve a 100-dimensional version of the
   rastrigin function with even more local optimum points. the 100-d
   version is somewhat more challenging than the trivial 2d version used
   to produce the visualizations in this article. below is a comparison of
   the performance for various algorithms discussed:

                             [rastrigin10d.svg]

   on this 100-d rastrigin problem, none of the optimisers got to the
   global optimum solution, although cma-es comes close. cma-es blows
   everything else away. pepg is in 2nd place, and openai-es / genetic
   algorithm falls behind. i had to use an annealing schedule to gradually
   lower for openai-es to make it perform better for this task.

                        [rastrigin_cma_solution.png]

     final solution that cma-es discovered for 100-d rastrigin function.
     global optimal solution is a 100-dimensional vector of exactly 10.

what   s next?

     so proud of my little dude ... [28]pic.twitter.com/j5j61vqxp0
         hardmaru (@hardmaru) [29]july 23, 2017

   in the [30]next article, i will look at applying es to other
   experiments and more interesting problems. please [31]check it out!

references and other links

   below are a few links to information related to evolutionary computing
   which i found useful or inspiring.

   image credits of [32]lemmings jumping off a cliff. your results may
   vary when investing in icos.

   cma-es: [33]official reference implementation on github, [34]tutorial,
   original cma-es [35]paper from 2001, overview [36]slides

   [37]simple statistical gradient-following algorithms for connectionist
   id23 (reinforce), 1992.

   [38]parameter-exploring policy gradients, 2009.

   [39]natural evolution strategies, 2014.

   [40]evolution strategies as a scalable alternative to reinforcement
   learning, openai, 2017.

   risto miikkulainen   s [41]slides on neuroevolution.

   a neuroevolution approach to [42]general atari game playing, 2013.

   kenneth stanley   s talk on [43]why greatness cannot be planned: the myth
   of the objective, 2015.

   [44]neuroevolution: a different kind of deep learning. the quest to
   evolve neural networks through evolutionary algorithms.

   [45]compressed network search finds complex neural controllers with a
   million weights.

   karl sims [46]evolved virtual creatures, 1994.

   evolved [47]step climbing creatures.

   super mario world agent [48]mario i/o, mario kart 64 [49]controller
   using using [50]neat algorithm.

   [51]ingo rechenberg, the inventor of evolution strategies.

   a tutorial on [52]differential evolution with python.

my previous evolutionary projects

   [53]pathnet: evolution channels id119 in super neural
   networks

   neural network evolution playground with [54]backprop neat

   evolved neural [55]art gallery using [56]cppn implementation

   [57]creatures avoiding planks

   [58]neural slime volleyball

   evolution of [59]inverted double pendulum swing up controller
   please enable javascript to view the [60]comments powered by disqus.

      [61]otoro.net

references

   visible links
   1. http://blog.otoro.net/feed.xml
   2. http://otoro.net/
   3. http://blog.otoro.net/archive.html
   4. https://blog.openai.com/evolution-strategies/
   5. https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html
   6. https://arxiv.org/abs/1703.00548
   7. http://blog.otoro.net/2016/05/07/backprop-neat/
   8. https://en.wikipedia.org/wiki/test_functions_for_optimization
   9. https://en.wikipedia.org/wiki/test_functions_for_optimization
  10. http://people.idsia.ch/~juergen/gomez08a.pdf
  11. http://blog.otoro.net/2015/03/10/esp-algorithm-for-double-pendulum/
  12. http://blog.otoro.net/2016/05/07/backprop-neat/
  13. https://en.wikipedia.org/wiki/cma-es
  14. https://en.wikipedia.org/wiki/covariance_matrix
  15. https://arxiv.org/abs/1604.00772
  16. http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf
  17. http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=a64d1ae8313a364b814998e9e245b40a?doi=10.1.1.180.7104&rep=rep1&type=pdf
  18. http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf
  19. http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf
  20. http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf
  21. https://blog.openai.com/evolution-strategies/
  22. https://blog.openai.com/nonlinear-computation-in-linear-networks/
  23. https://github.com/hardmaru/pytorch_notebooks/tree/master/mnist_es
  24. https://github.com/cma-es/pycma
  25. https://github.com/hardmaru/estool/blob/master/es.py
  26. https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb
  27. https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb
  28. https://t.co/j5j61vqxp0
  29. https://twitter.com/hardmaru/status/889265345172615168?ref_src=twsrc^tfw
  30. http://blog.otoro.net/2017/11/12/evolving-stable-strategies/
  31. http://blog.otoro.net/2017/11/12/evolving-stable-strategies/
  32. https://www.reddit.com/r/cryptomarkets/comments/6qpla3/investing_in_icos_results_may_vary/
  33. https://github.com/cma-es
  34. https://arxiv.org/abs/1604.00772
  35. http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf
  36. https://www.slideshare.net/osamasalaheldin2/cmaes-presentation
  37. http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf
  38. http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=a64d1ae8313a364b814998e9e245b40a?doi=10.1.1.180.7104&rep=rep1&type=pdf
  39. http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf
  40. https://blog.openai.com/evolution-strategies/
  41. http://nn.cs.utexas.edu/downloads/slides/miikkulainen.ijid9813.pdf
  42. http://www.cs.utexas.edu/~ai-lab/?atari
  43. https://youtu.be/dxqpl9gooyi
  44. https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning
  45. http://people.idsia.ch/~juergen/compressednetworksearch.html
  46. https://youtu.be/jbgg_vsp7f8
  47. https://youtu.be/eufvrfqrbli
  48. https://youtu.be/qv6uvoq0f44
  49. (https://github.com/nicknlsn/mariokart64neat)
  50. https://www.cs.ucf.edu/~kstanley/neat.html
  51. http://www.bionik.tu-berlin.de/institut/xstart.htm
  52. https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/
  53. https://deepmind.com/research/publications/pathnet-evolution-channels-gradient-descent-super-neural-networks/
  54. http://blog.otoro.net/2016/05/07/backprop-neat/
  55. http://otoro.net/gallery
  56. http://otoro.net/neurogram/
  57. http://otoro.net/planks/
  58. http://otoro.net/nabi/slimevolley/index.html
  59. http://otoro.net/ml/pendulum-esp/index.html
  60. https://disqus.com/?ref_noscript
  61. http://otoro.net/

   hidden links:
  63. http://blog.otoro.net/
  64. http://otoro.net/#contact
  65. https://www.facebook.com/otorography
  66. https://github.com/hardmaru
  67. http://instagram.com/hardmaru
  68. https://twitter.com/hardmaru
  69. http://twitter.com/share?url=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&text=a%20visual%20guide%20to%20evolution%20strategies&via=hardmaru
  70. https://plus.google.com/share?url=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/
  71. http://www.facebook.com/sharer/sharer.php?u=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/
  72. http://www.linkedin.com/sharearticle?url=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&title=a%20visual%20guide%20to%20evolution%20strategies&source=
  73. http://twitter.com/share?url=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&text=a%20visual%20guide%20to%20evolution%20strategies&via=hardmaru
  74. https://plus.google.com/share?url=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/
  75. http://www.facebook.com/sharer/sharer.php?u=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/
  76. http://www.linkedin.com/sharearticle?url=http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&title=a%20visual%20guide%20to%20evolution%20strategies&source=
