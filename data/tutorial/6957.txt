   [1]gertjan van den burg.
     * lights [ ]
     * [2]research
     * [3]blog
     * [4]photography

                     simple mnist autoencoder in tensorflow

   written by gertjan van den burg
   published on oct 26, 2017.

    github: [5]autoencoder.

   recently i   ve been playing around a bit with [6]tensorflow. even though
   my past research hasn   t used a lot of deep learning, it   s a valuable
   tool to know how to use. to get to know the basics, i   m trying to
   implement a few simple models myself. [ ] the benefit of implementing
   it yourself is of course that it's much easier to play with the code
   and extend it. in this post, i will present my tensorflow
   implementation of [7]andrej karpathy   s [8]mnist autoencoder, originally
   written in [9]convnetjs.

   an [10]autoencoder is a neural network that consists of two parts: an
   encoder and a decoder. the encoder network encodes the original data to
   a (typically) low-dimensional representation, whereas the decoder
   network converts this representation back to the original feature
   space. the idea is that the low-dimensional representation forms a
   non-linear id84 of the original data. here   s a
   graphical illustration of the network structure:
       [ ] illustration of the network structure of the autoencoder. in
   reality the size of the layers is larger than shown here: the blue
   layers have 50 nodes in the actual network and the input and output
   layers have 784 nodes. illustration of the network structure of the
   autoencoder. in reality the size of the layers is larger than shown
   here: the blue layers have 50 nodes in the actual network and the input
   and output layers have 784 nodes.

   okay, let   s get to the code. as mentioned, i   ll be reconstructing
   andrej karpathy   s structure, which consists of 2 fully connected layers
   with 50 neurons, then a layer with just 2 neurons, and then again 2
   layers with 50 neurons before a final read-out layer. since we   ll use a
   lot of fully connected layers we   ll make some quick utility functions
   to make our life easier. first, for the weight matrix we use: [ ] the
   functions for the weights and biases are taken from the [11]tensorflow
   mnist tutorial.
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.variable(initial)

   and for the biases:
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.variable(initial)

   for the fully connected layer, we   ll make use of the fact that the
   mnist data is monochrome, so we don   t have to care about the color
   channels. this means that we can define our fully connected layers
   simply as follows:
def fc_layer(prev, input_size, output_size):
    w = weight_variable([input_size, output_size])
    b = bias_variable([output_size])
    return tf.matmul(prev, w) + b

   with these utilities in place, we   re ready to build our model. recall
   that the mnist images are 28 x 28 = 784 pixels. here we consider the
   input to the model to be a single vector of length 784. the model can
   then be captured through the following function:
def autoencoder(x):
    l1 = tf.nn.tanh(fc_layer(x, 28*28, 50))
    l2 = tf.nn.tanh(fc_layer(l1, 50, 50))
    l3 = fc_layer(l2, 50, 2)
    l4 = tf.nn.tanh(fc_layer(l3, 2, 50))
    l5 = tf.nn.tanh(fc_layer(l4, 50, 50))
    out = fc_layer(l5, 50, 28*28)
    loss = tf.reduce_mean(tf.squared_difference(x, out))
    return loss, out, l3

   this function clearly shows the symmetry in the model through the
   parameters to the fc_layer function. in the end, we compute the mean
   squared error between the input and the output image and use that as
   the id168.

   before turning to the code for training the model, i   ll present some
   code for using a [12]tensorboard with this model. i wanted to be able
   to visualize the input, output, and latent layers in the model for a
   batch of input images. to do this nicely, i used the form_image_grid
   function from the [13]magenta package, with the following helper
   function:
def layer_grid_summary(name, var, image_dims):
    prod = np.prod(image_dims)
    grid = form_image_grid(tf.reshape(var, [batch_size, prod], [grid_rows,
        grid_cols], image_dims, 1))
    return tf.summary.image(name, grid)

   this creates a nice tiled image of grid_rows x grid_cols as a single
   tensor, which we can use in a summary. for the sake of presentation, we
   can wrap the tensorboard related stuff in a function:
def create_summaries(loss, x, latent, output):
    writer = tf.summary.filewriter("./logs")
    tf.summary.scalar("loss", loss)
    layer_grid_summary("input", x, [28, 28])
    layer_grid_summary("encoder", latent, [2, 1])
    layer_grid_summary("output", output, [28, 28])
    return writer, tf.summary.merge_all()

   by combining all summary statements in a single operation using
   tf.summary.merge_all(), we can use some simple code to create all
   summaries simultaneously. finally, we have the following function to
   pull everything together and run the training:
def main():
    # load the data
    mnist = input_data.read_data_sets("./mnist_data")
    # initialize inputs
    x = tf.placeholder(tf.float32, shape=[none, 28*28])
    # build the model
    loss, output, latent = autoencoder(x)
    # initialize optimizer
    train_step = tf.train.adamoptimizer(1e-4).minimize(loss)
    # initialize summaries
    writer, summary_op = create_summaries(loss, x, latent, output)

    # run the training loop
    with tf.session() as sess:
        sess.run(tf.global_variables_initializer())
        for i in range(10000):
            batch = mnist.train.next_batch(batch_size)
            feed = {x : batch[0]}
            if i % 500 == 0:
                summary, train_loss = sess.run([summary_op, loss],
                    feed_dict=feed)
                print("step: %d. loss: %g" % (i, train_loss))

                writer.add_summary(summary, i)
                writer.flush()

            train_step.run(feed_dict=feed)

   that   s the full code for the mnist autoencoder. there   s plenty of
   things to play with here, such as the network architecture, activation
   functions, the minimizer, training steps, etc. below i   ll take a brief
   look at some of the results.

   we can compare the input images to the autoencoder with the output
   images to see how accurate the encoding/decoding becomes during
   training. remember, in the architecture above we only have 2 latent
   neurons, so in a way we   re trying to encode the images with 28 x 28 =
   784 bytes of information down to 2 bytes of information. [ ] this is
   not entirely true because the latent neurons are not clipped to the
   range [0, 255]. it would be more accurate to say that the autoencoder
   is a nonlinear feature transformation that maps a 784 dimensional space
   down to a 2 dimensional space.

   here is an animation that shows the evolution over time of some input
   images and the corresponding output images of the network.
       [ ] animation of the input and output layer of the network over time.

   as you can see, there   s a bit of a grey offset in the images. this is
   because the last layer of the network doesn   t clip the pixel values to
   the range [0, 255]. i found that adding a [14]relu over the read-out
   layer can help remove this offset: [ ] i'm sure there are other ways to
   get rid of this offset too. for instance, you could apply a softmax
   over the last layer and multiply with 255.
out = tf.nn.relu(fc_layer(l5, 50, 28*28))

   with this modification, the animation looks as follows:
       [ ] animation of the input and output layer of the network with a
   relu over the read-out layer.

   from this figure it is evident that the network actually converges
   pretty quickly and then spends quite some time tweaking the few
   incorrect outcomes. notice that the encoding/decoding of the images
   removes a lot of nuance from the images: for instance the 2 1 pair on
   the second row has a lot of detail on the left that does not appear on
   the right.

   to look at how the latent space maps images from different digits, we
   can feed some test set images through the network and record the values
   of the latent neurons. here is a scatter plot of this latent space for
   the first 1000 images from the test set:
       [ ] plot of the latent space for the first 1000 digits of the test
   dataset. plot of the latent space for the first 1000 digits of the test
   dataset.

   it can be seen that the latent space for the digit 1 is quite well
   defined, as well as that for 0 and 6. however, the mappings strongly
   overlap for the digits 4 and 9 as well as for 3 and 8. note that this
   mapping is not necessarily a id91 of the data points: there is
   nothing in the network structure that requires the images of the same
   digit to lie in the same area of the latent space. [ ] for an
   exploration of id91 the mnist dataset, see [15]chris olah's blog.
   the only requirement is that the encoding is such that the decoder
   network can reconstruct the input image accurately.

   finally, we can take a point in the latent space and see the image that
   the decoder network constructs from it. below is an animation where we
   walk around a circle in the latent space and show the corresponding
   output image. this animation shows how digits are transformed into
   their neighboring digits in the latent space.
       [ ] animation of a path through the latent space and the
   corresponding output images. note that the mapping here is slightly
   different than in the previous figure, because this is generated from a
   different run.

   that   s it! the full code can be found on [16]github. i hope you learned
   something from this post, i know i did!

   0 kudos

   i don't have comments on this blog on purpose, but if you'd like to
   leave positive feedback you can mouse-over the kudo button above. you
   can contact me over [17]email, but replies are not guaranteed.

          gertjan van den burg, 2016-2019. [18]newsletter

references

   visible links
   1. https://gertjanvandenburg.com/
   2. https://gertjanvandenburg.com/research/
   3. https://gertjanvandenburg.com/blog/
   4. https://gertjanvandenburg.com/photography/
   5. https://github.com/gjjvdburg/tensorflowexperiments/tree/master/autoencoder
   6. http://www.tensorflow.org/
   7. http://cs.stanford.edu/people/karpathy/
   8. http://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html
   9. http://cs.stanford.edu/people/karpathy/convnetjs/
  10. https://en.wikipedia.org/wiki/autoencoder
  11. https://www.tensorflow.org/get_started/mnist/pros
  12. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  13. https://magenta.tensorflow.org/
  14. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  15. https://colah.github.io/posts/2014-10-visualizing-mnist
  16. https://github.com/gjjvdburg/tensorflowexperiments/tree/master/autoencoder
  17. mailto:gertjanvandenburg+blog@gmail.com
  18. https://goo.gl/forms/okh1bkjxmzezabnx1

   hidden links:
  20. https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgertjanvandenburg.com%2fblog%2fautoencoder%2f
  21. https://twitter.com/intent/tweet/?text=simple%20mnist%20autoencoder%20in%20tensorflow%c2%a0-%c2%a0gertjan%20van%20den%20burg&url=https%3a%2f%2fgertjanvandenburg.com%2fblog%2fautoencoder%2f
  22. https://plus.google.com/share?url=https%3a%2f%2fgertjanvandenburg.com%2fblog%2fautoencoder%2f
  23. mailto:?subject=simple%20mnist%20autoencoder%20in%20tensorflow&body=https%3a%2f%2fgertjanvandenburg.com%2fblog%2fautoencoder%2f
  24. http://vk.com/share.php?title=simple%20mnist%20autoencoder%20in%20tensorflow&url=https%3a%2f%2fgertjanvandenburg.com%2fblog%2fautoencoder%2f
