   #[1]learn opencv    feed [2]learn opencv    comments feed [3]learn opencv
      understanding feedforward neural networks comments feed [4]high
   dynamic range (hdr) imaging using opencv (c++/python) [5]exposure
   fusion using opencv (c++/python) [6]alternate [7]alternate

   [tr?id=193036094536652&ev=pageview&noscript=1]

     * [8]skip to primary navigation
     * [9]skip to content
     * [10]skip to primary sidebar

   [11]learn opencv

   opencv examples and tutorials ( c++ / python )

     * [12]home
     * [13]about
     * [14]resources
     * [15]ai consulting
     * [16]courses

understanding feedforward neural networks

   october 9, 2017 by [17]vikas gupta [18]16 comments

   [19]mlp diagram

   this post is part of the series on deep learning for beginners, which
   consists of the following tutorials :
    1. [20]neural networks : a 30,000 feet view for beginners
    2. [21]installation of deep learning frameworks (tensorflow and keras
       with cuda support )
    3. [22]introduction to keras
    4. understanding feedforward neural networks
    5. [23]image classification using feedforward neural networks
    6. [24]image recognition using convolutional neural network
    7. [25]understanding id180
    8. [26]understanding autoencoders using tensorflow
    9. [27]image classification using pre-trained models in keras
   10. [28]id21 using pre-trained models in keras
   11. [29]fine-tuning pre-trained models in keras
   12. more to come . . .

   in this article, we will learn about feedforward neural networks, also
   known as deep feedforward networks or multi-layer id88s. they
   form the basis of many important neural networks being used in the
   recent times, such as convolutional neural networks ( used extensively
   in id161 applications ), recurrent neural networks ( widely
   used in natural language understanding and sequence learning) and so
   on. we will try to understand the important concepts involved in an
   intuitive and interactive way, without going into the mathematics
   involved. if you are interested in diving into deep learning but don   t
   have much background in statistics and machine learning, then this
   article is a perfect starting point.

   we will use the feedforward network to solve a [30]binary
   classification problem. in machine learning, [31]classification is a
   type of supervised learning method, where the task is to divide the
   data samples into predefined groups by a decision function. when there
   are only two groups, it is called binary classification. the figure
   given below shows an example. the points in blue belong to one group (
   or class ) and orange points belong to the other. the imaginary line(s)
   which separate the groups are called decision boundaries. the decision
   function is learned from a set of labeled samples, which is called
   training data and the process of learning the decision function is
   called training.
   [32]sample-data

   in the above example, the top row shows two different data
   distributions and the bottom row shows the decision boundary. the left
   image shows an example of data which is [33]linearly separable. this
   means that a linear boundary ( e.g. a straight line ) is enough to
   separate the data into groups. on the other hand, the image on the
   right shows an example of data which is not linearly separable. the
   decision boundary, in this case, has to be circular or polygonal as
   shown in the figure.

1. understanding the neural network jargon

   given below is an example of a feedforward neural network. it is a
   directed acyclic graph which means that there are no feedback
   connections or loops in the network. it has an input layer, an output
   layer, and a hidden layer. in general, there can be multiple hidden
   layers. each node in the layer is a neuron, which can be thought of as
   the basic processing unit of a neural network.
   [34]mlp diagram

1.1. what is a neuron?

   an artifical neuron is the basic unit of a neural network. a schematic
   diagram of a neuron is given below.
   [35]neuron
   as seen above, it works in two steps     it calculates the weighted sum
   of its inputs and then applies an activation function to normalize the
   sum. the id180 can be linear or nonlinear. also, there
   are weights associated with each input of a neuron. these are the
   parameters which the network has to learn during the training phase.

1.2. id180

   the activation function is used as a decision making body at the output
   of a neuron. the neuron learns linear or non-linear decision boundaries
   based on the activation function. it also has a normalizing effect on
   the neuron output which prevents the output of neurons after several
   layers to become very large, due to the cascading effect. there are
   three most widely used id180
     * [36]sigmoid
       it maps the input ( x axis ) to values between 0 and 1.
       [37]sigmoid
     * tanh
       it is similar to the sigmoid function butmaps the input to values
       between -1 and 1.
       [38]tanh function
     * [39]rectified linear unit (relu)
       it allows only positive values to pass through it. the negative
       values are mapped to zero.
       [40]relu function

   there are other functions like the unit step function, leaky relu,
   noisy relu, exponential lu etc which have their own merits and
   demerits.

1.3. input layer

   this is the first layer of a neural network. it is used to provide the
   input data or features to the network.

1.4. output layer

   this is the layer which gives out the predictions. the activation
   function to be used in this layer is different for different problems.
   for a binary classification problem, we want the output to be either 0
   or 1. thus, a sigmoid activation function is used. for a multiclass
   classification problem, a [41]softmax ( think of it as a generalization
   of sigmoid to multiple classes ) is used. for a regression problem,
   where the output is not a predefined category, we can simply use a
   linear unit.

1.5. hidden layer

   a feedforward network applies a series of functions to the input. by
   having multiple hidden layers, we can compute complex functions by
   cascading simpler functions. suppose, we want to compute the 7th power
   of a number, but want to keep things simple ( as they are easy to
   understand and implement ). you can use simpler powers like square and
   cube to calculate the higher order functions. similarly, you can
   compute highly complex functions by this cascading effect. the most
   widely used hidden unit is the one which uses a rectified linear unit
   (relu) as the activation function.

   the choice of hidden units is a very active research area in machine
   learning. the type of hidden layer distinguishes the different types of
   neural networks like id98s, id56s etc. the number of hidden layers is
   termed as the depth of the neural network. one question you might ask
   is exactly how many layers in a network make it deep? there is no right
   answer to this. in general, deeper networks can learn more complex
   functions.

1.6. how does the network learn?

   the training samples are passed through the network and the output
   obtained from the network is compared with the actual output. this
   error is used to change the weights of the neurons such that the error
   decreases gradually. this is done using the [42]id26
   algorithm, also called backprop. iteratively passing batches of data
   through the network and updating the weights, so that the error is
   decreased, is known as [43]stochastic id119 ( sgd ). the
   amount by which the weights are changed is determined by a parameter
   called learning rate. the details of sgd and backprop will be covered
   in a separate post.

2. why use hidden layers?

   to understand the significance of hidden layers we will try to solve
   the binary classification problem without hidden layers. for this, we
   will use an interactive platform from google,
   [44]playground.tensorflow.org which is a web app where you can create
   simple feedforward neural networks and see the effects of training in
   real time. you can play around by changing the number of hidden layers,
   number of units in a hidden layer, type of activation function, type of
   data, learning rate, id173 parameters etc. given below is a
   screenshot of the web page.[45] tensorflow playground screenshot

   in the above page, you can select the data and click on the play button
   to start training. it will show you the learned decision boundary and
   the loss curves at the top right corner.

2.1. no hidden layer

   we want a network without a hidden layer which i have created in this
   [46]link. here there are no hidden layers so it becomes a simple
   neuron, which is capable of learning a linear decision boundary. we can
   select the type of data from the top left corner. in case of linearly
   separable data ( 3rd type ), it will be able to learn ( when you click
   the play button ) a linear boundary as shown below.

   [47]output of classification of linear data

   however, if you choose the 1st data it will not be able to learn the
   circular decision boundary.

   [48]output of linear classification on circular data
   since the data lies in a circular region, one may say that using
   squared values of the features as inputs might help. as it turns out,
   upon training, the neuron will be able to find the circular decision
   boundary.

   [49]output of non linear classification of circular data

   now, if you select the 2nd data, the same configuration will not be
   able to learn the appropriate decision boundary.

   [50]output of linear classification on parabola data

   again by intuition, it looks like the decision boundary is a conic
   section( like a parabola or hyperbola ). so, if we include the product
   of the feature ( i.e. x1x2 ), the neuron is able to learn the desired
   decision boundary.

   [51]output of nonlinear classification on parabola data

   from the above experiment, we observed the following:
     * using a single neuron we can only learn a linear decision boundary
     * we had to come up with feature transformations (like square of
       features or product of features) by visualizing the data. this step
       can be tricky for data which is not easy to visualize.

2.2. adding a hidden layer

   by adding a hidden layer as shown in this [52]link, we can get rid of
   this feature engineering and have a single network which can learn all
   the three decision boundaries. a neural network with a single hidden
   layer with nonlinear id180 is considered to be a
   [53]universal function approximator ( i.e. capable of learning any
   function ). however, the number of units in the hidden layer is not
   fixed. the result of adding a hidden layer with just 3 neurons is shown
   below:

   [54]output of multilayer classification on parabola data

3. id173

   as we saw in the previous section, a multilayer network can learn
   nonlinear decision boundaries. however, if there is noise in the data
   (which is often the case) the network may try to learn the nonlinearity
   introduced by the noise too, trying to fit the noisy samples. in such
   cases, the noisy samples should be treated as outliers. in this
   [55]link, i have added some noise to the linearly separable data. also,
   to demonstrate the idea, i have increased the number of hidden units.

   [56]overfitting shown on linear data
   in the above figure, it can be seen that the decision boundary is
   trying very hard to accommodate the noisy samples in order to reduce
   the error. but as you can see it is being misguided by the noisy
   samples. in other words, the network will be fragile in the presence of
   noise. this phenomenon is called overfitting. in such cases, the error
   on training data might decrease but the network performs badly on
   unseen data. it can be seen from the loss curves at the top right
   corner.

   the training loss is decreasing but the test loss is increasing. also,
   you can see that some weights have become very large ( very thick
   connections or you can see the weights if you hover above the
   connections ). this can be rectified by putting some restrictions on
   the values of weights ( like not allowing the weights to become very
   high ). this is called id173. we impose restrictions on the
   other parameters of the network. in a sense, we don   t trust the
   training data fully and want the network to learn    nice    decision
   boundaries. i have added l2 id173 to the above configuration
   in this [57]link, and the output is shown below.

   [58]output after id173
   after including l2 id173, the decision boundary learned by the
   network is smoother and similar to the case when there was no noise.
   the effect of id173 can also be seen from the loss curves and
   the value of the weights.

   in the next post, we will learn how to implement a feedforward neural
   network in keras for solving a multi-class classification problem and
   learn more about feedforward networks.

subscribe & download code

   if you liked this article and would like to download code (c++ and
   python) and example images used in all posts of this blog, please
   [59]subscribe to our newsletter. you will also receive a free
   [60]id161 resource guide. in our newsletter, we share opencv
   tutorials and examples written in c++/python, and id161 and
   machine learning algorithms and news.

   [61]subscribe now

   filed under: [62]deep learning, [63]machine learning, [64]tutorial
   tagged with: [65]beginners, [66]feedforward neural networks,
   [67]multilayer id88, [68]neural networks, [69]tutorial
   (button) load comments

   search this website ____________________ search

join course

   [70]id161 for faces

resources

   [71]download code (c++ / python)

disclaimer

   this site is not affiliated with opencv.org

   i am an entrepreneur with a love for id161 and machine
   learning with a dozen years of experience (and a ph.d.) in the field.

   in 2007, right after finishing my ph.d., i co-founded taaz inc. with my
   advisor dr. david kriegman and kevin barnes. the scalability, and
   robustness of our id161 and machine learning algorithms have
   been put to rigorous test by more than 100m users who have tried our
   products. [72]read more   

recent posts

     * [73]image inpainting with opencv (c++/python)
     * [74]hough transform with opencv (c++/python)
     * [75]xeus-cling: run c++ code in jupyter notebook
     * [76]pose detection comparison : wrnchai vs openpose
     * [77]gender & age classification using opencv deep learning (
       c++/python )

   copyright    2019    big vision llc

references

   1. https://www.learnopencv.com/feed/
   2. https://www.learnopencv.com/comments/feed/
   3. https://www.learnopencv.com/understanding-feedforward-neural-networks/feed/
   4. https://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python/
   5. https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/
   6. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/understanding-feedforward-neural-networks/
   7. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/understanding-feedforward-neural-networks/&format=xml
   8. https://www.learnopencv.com/understanding-feedforward-neural-networks/#genesis-nav-primary
   9. https://www.learnopencv.com/understanding-feedforward-neural-networks/#genesis-content
  10. https://www.learnopencv.com/understanding-feedforward-neural-networks/#genesis-sidebar-primary
  11. https://www.learnopencv.com/
  12. https://www.learnopencv.com/
  13. https://www.learnopencv.com/about/
  14. https://www.learnopencv.com/computer-vision-resources
  15. https://www.learnopencv.com/computer-vision-machine-learning-artificial-intelligence-consulting/
  16. http://courses.learnopencv.com/
  17. https://www.learnopencv.com/author/vikas/
  18. https://www.learnopencv.com/understanding-feedforward-neural-networks/#disqus_thread
  19. https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg
  20. https://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/
  21. https://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/
  22. https://www.learnopencv.com/deep-learning-using-keras-the-basics/
  23. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/
  24. https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/
  25. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/
  26. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/
  27. https://www.learnopencv.com/keras-tutorial-using-pre-trained-id163-models/
  28. https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/
  29. https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models
  30. https://en.wikipedia.org/wiki/binary_classification
  31. https://en.wikipedia.org/wiki/classification
  32. https://www.learnopencv.com/wp-content/uploads/2017/10/sample-data-mlp.jpg
  33. https://en.wikipedia.org/wiki/linear_separability
  34. https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg
  35. https://www.learnopencv.com/wp-content/uploads/2017/10/neuron-diagram.jpg
  36. https://en.wikipedia.org/wiki/sigmoid_function
  37. https://www.learnopencv.com/wp-content/uploads/2017/10/sigmoid.png
  38. https://www.learnopencv.com/wp-content/uploads/2017/10/tanh.png
  39. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  40. https://www.learnopencv.com/wp-content/uploads/2017/10/relu.png
  41. https://en.wikipedia.org/wiki/softmax_function
  42. http://neuralnetworksanddeeplearning.com/chap2.html
  43. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  44. http://playground.tensorflow.org/
  45. https://www.learnopencv.com/wp-content/uploads/2017/10/sample-playground-tensorflow.png
  46. http://playground.tensorflow.org/#activation=relu&batchsize=10&dataset=gauss&regdataset=reg-plane&learningrate=0.1&id173rate=0&noise=0&networkshape=&seed=0.75972&showtestdata=false&discretize=false&perctraindata=50&x=true&y=true&xtimesy=false&xsquared=false&ysquared=false&cosx=false&sinx=false&cosy=false&siny=false&collectstats=false&problem=classification&initzero=false&hidetext=false
  47. https://www.learnopencv.com/wp-content/uploads/2017/10/linear-output.png
  48. https://www.learnopencv.com/wp-content/uploads/2017/10/circular-output.png
  49. https://www.learnopencv.com/wp-content/uploads/2017/10/circular-output-correct.png
  50. https://www.learnopencv.com/wp-content/uploads/2017/10/parabola-output.png
  51. https://www.learnopencv.com/wp-content/uploads/2017/10/parabola-output-correct.png
  52. http://playground.tensorflow.org/#activation=relu&batchsize=10&dataset=xor&regdataset=reg-plane&learningrate=0.1&id173rate=0&noise=0&networkshape=3&seed=0.63075&showtestdata=false&discretize=false&perctraindata=50&x=true&y=true&xtimesy=false&xsquared=false&ysquared=false&cosx=false&sinx=false&cosy=false&siny=false&collectstats=false&problem=classification&initzero=false&hidetext=false
  53. https://en.wikipedia.org/wiki/universal_approximation_theorem
  54. https://www.learnopencv.com/wp-content/uploads/2017/10/nonlinear-output-correct.png
  55. http://playground.tensorflow.org/#activation=relu&batchsize=10&dataset=gauss&regdataset=reg-plane&learningrate=0.1&id173rate=0.03&noise=30&networkshape=8&seed=0.46428&showtestdata=false&discretize=false&perctraindata=50&x=true&y=true&xtimesy=false&xsquared=false&ysquared=false&cosx=false&sinx=false&cosy=false&siny=false&collectstats=false&problem=classification&initzero=false&hidetext=false
  56. https://www.learnopencv.com/wp-content/uploads/2017/10/overfitting-linear-data.png
  57. http://playground.tensorflow.org/#activation=relu&id173=l2&batchsize=10&dataset=gauss&regdataset=reg-plane&learningrate=0.1&id173rate=0.03&noise=30&networkshape=8&seed=0.46428&showtestdata=false&discretize=false&perctraindata=50&x=true&y=true&xtimesy=false&xsquared=false&ysquared=false&cosx=false&sinx=false&cosy=false&siny=false&collectstats=false&problem=classification&initzero=false&hidetext=false
  58. https://www.learnopencv.com/wp-content/uploads/2017/10/regulariztion-output.png
  59. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  60. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  61. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  62. https://www.learnopencv.com/category/deep-learning/
  63. https://www.learnopencv.com/category/machine-learning/
  64. https://www.learnopencv.com/category/tutorial/
  65. https://www.learnopencv.com/tag/beginners/
  66. https://www.learnopencv.com/tag/feedforward-neural-networks/
  67. https://www.learnopencv.com/tag/multilayer-id88/
  68. https://www.learnopencv.com/tag/neural-networks/
  69. https://www.learnopencv.com/tag/tutorial/
  70. https://courses.learnopencv.com/p/computer-vision-for-faces
  71. https://bigvisionllc.lpages.co/leadbox/143044973f72a2:173c9390c346dc/5720147234914304/
  72. https://www.learnopencv.com/about/
  73. https://www.learnopencv.com/image-inpainting-with-opencv-c-python/
  74. https://www.learnopencv.com/hough-transform-with-opencv-c-python/
  75. https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/
  76. https://www.learnopencv.com/pose-detection-comparison-wrnchai-vs-openpose/
  77. https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/
