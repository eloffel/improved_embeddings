   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

how to unit test machine learning code.

   [9]go to the profile of chase roberts
   [10]chase roberts (button) blockedunblock (button) followfollowing
   oct 18, 2017
   [1*nsc2qgsmi6esctk86onr3g.jpeg]
   credit: [11]https://provalisresearch.com/blog/machine-learning/

   edit: the popularity of this post has inspired me to write a
   [12]machine learning test library. go check it out!

   second edit: the github user [13]suriyadeepan made a [14]pytorch port
   as well!

   over the past year, i   ve spent most of my working time doing deep
   learning research and internships. and a lot of that year was making
   very big mistakes that helped me learn not just about ml, but about how
   to engineer these systems correctly and soundly. one of the main
   principles i learned during my time at google brain was that unit tests
   can make or break your algorithm and can save you weeks of debugging
   and training time.

   however, there doesn   t seem to be a solid tutorial online on how to
   actually write unit tests for neural network code. even places like
   openai only found bugs by [15]staring at every line of their code and
   try to think why it would cause a bug. clearly, most of us don   t have
   that kind of time or self hatred, so hopefully this tutorial can help
   you get started testing your systems sanely!

   let   s start off with a simple example. try to find the bug in this
   code.

   iframe: [16]/media/eff5f60ad58c1df5bdc3b08438eabb53?postid=57cf6fd81765

   do you see it? the network isn   t actually stacking. when i wrote this
   code, i copy and pasted the line slim.conv2d(   ) and only modified the
   kernel sizes, and never the actual input.

   i   m embarrassed to say that this actually happened to me about a week
   ago    but it   s an important lesson! these bugs are really hard to catch
   for a few reasons.
    1. this code never crashes, raises an error, or even slows down.
    2. this network still trains and the loss will still go down.
    3. the values converge after a few hours, but to really poor results,
       leaving you scratching your head as to what you need to fix.

   when your only feedback is the final validation error, the only place
   you have to search is your entire network architecture. needless to
   say, you   ll need a better system.

   so how do we actually catch this before we do a full multi day training
   session? well, the easiest thing to notice about this is that the
   values of the layers never actually reach any other tensors outside the
   function. so assuming we had some type of loss and an optimizer, these
   tensors never get optimized, so they will always have their default
   values.

   we can detect it by simply taking a training step and comparing their
   before and after.

   iframe: [17]/media/fe43cf1115cfdabaf1faf8013501521c?postid=57cf6fd81765

   boom. in less than 15 lines of code, we now verified that a least all
   of the variables that we created get trained.

   this test is super simple and super useful. let   s say that we fixed the
   previous issue and now we want to start adding some batch
   id172. see if you can spot the bug.

   iframe: [18]/media/c43c804af6cba56bca9add4c56e681f3?postid=57cf6fd81765

   did you see it? this one is super subtle. you see, in tensorflow
   batch_norm actually has is_training defaulted to false, so adding this
   line of code won   t actually normalize your input during training!
   thankfully, the last unit test we wrote will catch this issue
   immediately! (i know, because this happened to me 3 days ago.)

   let   s do another example. this actually comes from a [19]reddit post i
   saw one day. i won   t get into too much detail, but basically the person
   wanted to create a classifier that gave an output in the range of (0,
   1). see if you can find the bug.

   iframe: [20]/media/6b7736f22d2a014349f9f3f408a7f67e?postid=57cf6fd81765

   notice the bug? this one is really hard to spot before hand, and can
   lead to super confusing results. basically what is happening here is
   that prediction only has a single output, which, when you apply
   [21]softmax cross id178 onto it, causes the loss to be 0 always.

   an easy way to test for this is to well    make sure the loss is never 0.

   iframe: [22]/media/00bb39ac65af58a0e2890fdc75ebdae2?postid=57cf6fd81765

   another good test to do is similar to our first test, but backwards.
   you can make sure that only the variables you want to train actually
   get trained. take for example a gan. one of the common bugs to appear
   is accidentally forgetting to set which variables to train during which
   optimization. code like this happens all the time.

   iframe: [23]/media/078fec1aa99d6cd622e5f34f65be95b5?postid=57cf6fd81765

   the biggest issue here is that the optimizer has a default setting to
   optimize all of the variables. in advance architectures like gans, this
   is a death sentence to all of your training time. however, you can
   easily detect these mistakes by writing a test like this:

   iframe: [24]/media/760479e99cfec1b437ea6b8bc9644b78?postid=57cf6fd81765

   a very similar test can be written for the discriminator. and this same
   test can be used for a lot of id23 algorithms as
   well. many actor-critic models have separate networks that need to be
   optimized by different losses.

   here are some patterns i would recommend following for your tests.
    1. keep them deterministic. it would really suck to have a test fail
       in a weird way, only to never be able to recreate it. if you really
       want randomized input, make sure to seed the random number so you
       can rerun the test easily.
    2. keep the tests short. don   t have a unit test that trains to
       convergence and checks against a validation set. you are wasting
       your own time if you do this.
    3. make sure you reset the graph between each test.

   in conclusion, these black box algorithms still have lots of ways to be
   tested! spending an hour writing a test can save you days of rerunning
   training sessions, and can greatly improve your research. wouldn   t suck
   to have to throw away perfectly good ideas because our implementations
   were buggy?

   this list clearly isn   t comprehensive, but it   s a solid start! if you
   have extra advice or specific tests that you found to be helpful,
   please message me on [25]twitter! i   d love to make a part 2 of this.

   all opinions in this piece are a reflection of my experiences and are
   not sponsored or supported by google.

     * [26]machine learning

   (button)
   (button)
   (button) 3.9k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of chase roberts

[28]chase roberts

   machine learning researcher in the making.
   [29]http://thenerdstation.github.io/

     * (button)
       (button) 3.9k
     * (button)
     *
     *

   [30]go to the profile of chase roberts
   never miss a story from chase roberts, when you sign up for medium.
   [31]learn more
   never miss a story from chase roberts
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/57cf6fd81765
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@keeper6928?source=post_header_lockup
  10. https://medium.com/@keeper6928
  11. https://provalisresearch.com/blog/machine-learning/
  12. https://medium.com/@keeper6928/mltest-automatically-test-neural-network-models-in-one-function-call-eb6f1fa5019d
  13. https://github.com/suriyadeepan/torchtest
  14. https://github.com/suriyadeepan/torchtest
  15. https://blog.openai.com/openai-baselines-id25/
  16. https://medium.com/media/eff5f60ad58c1df5bdc3b08438eabb53?postid=57cf6fd81765
  17. https://medium.com/media/fe43cf1115cfdabaf1faf8013501521c?postid=57cf6fd81765
  18. https://medium.com/media/c43c804af6cba56bca9add4c56e681f3?postid=57cf6fd81765
  19. https://www.reddit.com/r/machinelearning/comments/6qyvvg/p_tensorflow_response_is_making_no_sense/
  20. https://medium.com/media/6b7736f22d2a014349f9f3f408a7f67e?postid=57cf6fd81765
  21. https://en.wikipedia.org/wiki/softmax_function
  22. https://medium.com/media/00bb39ac65af58a0e2890fdc75ebdae2?postid=57cf6fd81765
  23. https://medium.com/media/078fec1aa99d6cd622e5f34f65be95b5?postid=57cf6fd81765
  24. https://medium.com/media/760479e99cfec1b437ea6b8bc9644b78?postid=57cf6fd81765
  25. https://twitter.com/thenerdstation
  26. https://medium.com/tag/machine-learning?source=post
  27. https://medium.com/@keeper6928?source=footer_card
  28. https://medium.com/@keeper6928
  29. http://thenerdstation.github.io/
  30. https://medium.com/@keeper6928
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/57cf6fd81765/share/twitter
  34. https://medium.com/p/57cf6fd81765/share/facebook
  35. https://medium.com/p/57cf6fd81765/share/twitter
  36. https://medium.com/p/57cf6fd81765/share/facebook
