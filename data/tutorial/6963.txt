   #[1]stitch fix technology     multithreaded

   iframe: [2]https://www.googletagmanager.com/ns.html?id=gtm-7rwhc

     * [3]stitch fix logomark
     * [4]engineering
     * [5]algorithms
     * [6]algorithms tour
     * [7]tour
     * [8]careers
     * [9]blog

   [10]stitch fix logo
   [11]engineering [12]algorithms [13]algorithms tour [14]tour [15]careers
   [16]blog

stop using id97

   blog post by chris moody

chris moody

october 18, 2017 - san francisco, ca

   [17]tweet this post! [18]post on linkedin

stop using id97

   when i started playing with id97 four years ago i needed (and
   luckily had) tons of supercomputer time. but because of advances in our
   understanding of id97, computing word vectors now takes fifteen
   minutes on a single run-of-the-mill computer with standard numerical
   libraries^[19]1. word vectors are [20]awesome but you don   t need a
   neural network     and definitely don   t need deep learning     to find
   them^[21]2. so if you   re using word vectors and aren   t gunning for
   state of the art or a paper publication then stop using id97.

   when we   re finished you   ll measure word similarities:

   facebook ~ twitter, google, ...

       and the classic word vector operations: zuckerberg - facebook +
   microsoft ~ nadella

      but you   ll do it mostly by counting words and dividing, no gradients
   harmed in the making!

the recipe

   let   s assume you   ve already gotten your hands on the [22]hacker news
   corpus and cleaned & tokenized it (or downloaded the preprocessed
   version [23]here). here   s the method:
    1. unigram id203. how often do i see word1 and word2
       independently? algo visualizing unigram counts example: this is
       just a simple word count that fills in the unigram_counts array.
       then divide unigram_counts array by it   s sum to get the id203
       p and get numbers like: p('facebook') is 0.001% and p('lambda') is
       0.000001%
    2. skipgram id203. how often did i see word1 nearby word2? these
       are called    skipgrams    because we can    skip    up to a few words
       between word1 and word2.^[24]3 algo visualizing skipgram counts
       example: here we   re counting pairs of words that are near each
       other, but not necessarily right next to each other. after
       normalizing the skipgram_count array, you   ll get a measurement of
       word-near-word probabilities like p('facebook', 'twitter'). if the
       value for p('facebook', 'twitter') is 10^-9 then out of one billion
       skipgram tuples you   ll typically see    facebook    and    twitter    once.
       for another skipgram like p('morning', 'facebook') this fraction
       might be much larger, like 10^-5, simply because the word morning
       is a frequent word.
    3. normalized skipgram id203 (or pmi). was the skipgram
       frequency higher or lower than what we expected from the unigram
       frequency? some words are extremely common, some are very rare, so
       divide the skipgram frequency by the two unigram frequencies. if
       the result is more than 1.0, then that skipgram occurred more
       frequently than the unigram probabilities of the two input words
       and we   ll call the two input words    associated   . the greater the
       ratio, the more associated. for ratios less than 1.0, the more
          anti-associated   . if we take the log of this number it   s called
       the [25]pointwise mutual information (pmi) of word x and word y.
       this is a well-understood measurement in the id205
       community and represents how frequently x and y coincide    mutually   
       (or jointly) rather than independently. algo visualizing unigram
       counts example 1: if we look at the association between facebook
       and twitter we   ll see that it   s above 1.0: p('facebook', 'twitter')
       / p('facebook') / p('twitter') = 1000. so facebook and twitter have
       unusually high-cooccurrence and we infer that they must be
       associated or similar. note that we haven   t done any neural network
       stuff and no math aside from counting & dividing, but we can
       already measure how associated two words are. later on we   ll
       calculate word vectors from this data, and those vectors will be
       constrained to reproduce these word-to-word relationships. ^[26]4
       example 2: for facebook and okra the pmi (p('facebook', 'okra') /
       p('facebook') / p('okra')) is close to 1.0, so facebook and okra
       aren   t associated empirically in the data. later on we   ll form
       vectors that reconstruct and respect this relationship, and so will
       have little overlap. but of course the word counts are noisy, and
       this noise induces spurious associations between words.
    4. pmi matrix. make a big matrix where each row represents word x and
       each column represents word y and each value is the pmi we
       calculated in step 3: pmi(x, y) = log (p(x, y) / p(x) / p(y)).
       because we have as many rows and columns as we have words, the size
       of the matrix is (n_vocabulary, n_vocabulary). and because
       n_vocabulary is typically 10k-100k, this is a big matrix with lots
       of zeros, so it   s best to use sparse array data structures to
       represent the pmi matrix. pmi matrix
    5. svd. now we reduce the dimensionality of that matrix. this
       effectively compresses our giant matrix into two smaller matrices.
       each of these smaller matrices form a set of word vectors with size
       (n_vocabulary, n_dim).^[27]5 each row of one matrix represents a
       single word vector. this is a straightforward operation in any
       id202 library, and in python it looks like: u, s, v =
       scipy.sparse.linalg.svds(pmi, k=256) svd of pmi matrix example. the
       svd is one of the most fundamental and beautiful tools you can use
       in machine learning and is what   s doing most of the magic. read
       more about it in [28]jeremy kun   s excellent [29]series. here, we
       can think of it as compressing the original input matrix which
       (counting zeros) had close to ~10 billion entries (100k x 100k)
       reduced into two matrices with a total of ~50 million entries (100k
       x 256 x 2), a 200x reduction in space. the svd outputs a space that
       is orthogonal, which is where we get our    linear regularity    and is
       where we get the property of being able to add and subtract word
       vectors meaningfully.
    6. searching. each row of the svd output matrices is a word vector.
       once you have these word eigenvectors, you can search for the
       tokens closest to consolas, which is a font popular in programming.

# in psuedocode:
# get the row vector corresponding to the word 'consolas'
vector_consolas = u['consolas']
# get how similar it is to all other words
similarities = dot(u, vector_consolas)
# sort by similarity, and pick the most similar
most_similar = tokens[argmax(similarities)]
most_similar

   so searching for consolas yields verdana and inconsolata as most
   similar     which makes sense as these are other fonts. searching for
   functional programming yields fp (an acronym), haskell (a popular
   functional language), and oop (which stands for object oriented
   programming, an alternative to functional programming). further, adding
   and subtracting these vectors and then searching gets id97   s
   hallmark feature: in computing the analogy mark zuckerberg - facebook +
   amazon we can relate the ceo of facebook to that of amazon, which
   appropriately evaluates to the jeff bezos token.

   it   s a hell of a lot more intuitive & easier to count skipgrams, divide
   by the word counts to get how    associated    two words are and svd the
   result than it is to understand what even a simple neural network is
   doing.

   so stop using the neural network formulation, but still have fun making
   word vectors!

footnotes

    1. ^1 the approach outlined here isn't exactly equivalent, but it
       performs about the same as id97 skipgram negative-sampling
       [30]sgns. it turns out that id97 isn't [31]totally identical to
       id105, but for applied purposes (e.g., industry) the
       svd technique is good enough. if you care about the differences
       between id97, [32]glove and [33]swivel -- and in industrial
       purposes i rarely do -- then you'll care about id97 sgns vs
       this svd formulation. also, the svd formulation neatly fits into a
       family of count-based bag-of-words techniques like tf-idf, lsi and
       lda: [fig_006.png] tf-idf compares term frequencies to term
       frequencies within a document, with lsi doing a low-rank
       factorization of that tf-idf matrix using the svd. lda also counts
       term frequencies within documents, but instead of svd factorizes
       that result using a sparsity-inducing prior in the term and
       document vectors. that prior, among other things, makes them
       interpretable. similarly, the formulation outlined here counts
       term-to-term associations (no documents) and svd factorizes those
       into a low-rank representation. [34]   
    2. ^2 a few caveats: if you're doing academic research and spinning
       off your own embedding systems (for example as in lda2vec
       [35][code], [36][paper] [37][blog]), tweaking the neural network
       approach can be useful. also, svd scales as o(n^3) so it isn't the
       best where you have large vocabularies n >> 100k. in this case, sgd
       is nice for online problems. [38]   
    3. ^3 in this simplest case, we won't model the effect of
       distance-weighted skipgrams. we'll just consider skipgrams within a
       fixed-size moving window around each word. also note that we aren't
       regularizing our model, which could benefit from smoothing or
       forming a prior. penalizing complexity especially helps in low-data
       cases, but in my experience this isn't necessary to get decent
       results on a wide set of real-life corpora with these methods.
       [39]   
    4. ^4 in some formulations, especially in truncated pmi, an important
       ingredient is the threshold `k` (typically a value like 25.0) which
       dampens the effect of low-pmi words, ostensibly to get a handle on
       noise. i interpret the `k` constant as a form of id173,
       although it isn't clear to me that this is a disciplined prior. you
       can see [40]gauss2vec for a more careful derivation, and this
       [41]paper for an empirical exploration of this parameter. [42]   
    5. ^5 the interpretation is that these word vectors explain the
       covariance within the pmi matrix -- essentially a low-rank way of
       saying word x and y are associated because they co-occur beyond
       their base rate popularity. because the axes in the reduced space
       and are orthogonal it also explains why one can find 'linear
       regularities' that made the original id97 famous. for example
       'king' and 'queen' might be seperated along a 'gender' direction
       but 'london' and 'berlin' might share a spot in a 'capital'
       direction. these eigenevectors effectively find a small set of
       directions in the input space that can still maximially recreate
       the original large input matrix. [43]   

   [44]tweet this post! [45]post on linkedin

   multithreaded

come work with us!

   we   re a diverse team dedicated to building great products, and we   d
   love your help. do you want to build amazing products with amazing
   peers? join us!

   [46]all technology careers [47]all careers at stitch fix

   stitch fix: your partner in personal style stitch fix and fix are
   trademarks of stitch fix, inc.

     * [48]stitch fix home
     * [49]faq
     * [50]press

     * [51]tech blog
     * [52]tech careers

     * [53]terms of use
     * [54]privacy policy

follow us!

   [55]stitch fix tech on github

follow us!

   [56]stitch fix tech on github
     * [57]tech blog
     * [58]tech careers
     * [59]stitch fix home
     * [60]faq
     * [61]press
     * [62]terms of use
     * [63]privacy policy

   stitch fix and fix are trademarks of stitch fix, inc.
   stitch fix logomark

references

   1. https://multithreaded.stitchfix.com/feed.xml
   2. https://www.googletagmanager.com/ns.html?id=gtm-7rwhc
   3. https://multithreaded.stitchfix.com/
   4. https://multithreaded.stitchfix.com/engineering
   5. https://multithreaded.stitchfix.com/algorithms
   6. http://algorithms-tour.stitchfix.com/
   7. http://algorithms-tour.stitchfix.com/
   8. https://multithreaded.stitchfix.com/careers
   9. https://multithreaded.stitchfix.com/blog
  10. https://multithreaded.stitchfix.com/
  11. https://multithreaded.stitchfix.com/engineering
  12. https://multithreaded.stitchfix.com/algorithms
  13. http://algorithms-tour.stitchfix.com/
  14. http://algorithms-tour.stitchfix.com/
  15. https://multithreaded.stitchfix.com/careers
  16. https://multithreaded.stitchfix.com/blog
  17. https://twitter.com/intent/tweet?text=stop using id97&url=https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/&via=stitchfix_algo
  18. https://www.linkedin.com/sharearticle?mini=true&url=https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/&title=stop using id97&summary= when i started playing with id97 four years ago i needed (and luckily had) tons of supercomputer time. but because of advances in our understanding of w...&source=stitch fix technology     multithreaded
  19. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#1
  20. http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/
  21. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#2
  22. https://cloud.google.com/bigquery/public-data/hacker-news
  23. https://zenodo.org/record/49899
  24. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#3
  25. https://en.wikipedia.org/wiki/pointwise_mutual_information
  26. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#4
  27. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#5
  28. https://twitter.com/jeremyjkun?ref_src=twsrc^google|twcamp^serp|twgr^author
  29. https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/
  30. https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization
  31. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/
  32. https://nlp.stanford.edu/projects/glove/
  33. https://arxiv.org/abs/1602.02215
  34. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#back-1
  35. https://github.com/cemoody/lda2vec
  36. https://arxiv.org/abs/1605.02019
  37. http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&lambda=1&term=
  38. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#back-2
  39. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#back-3
  40. https://arxiv.org/abs/1412.6623
  41. https://arxiv.org/abs/1402.3722
  42. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#back-4
  43. https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/#back-5
  44. https://twitter.com/intent/tweet?text=stop using id97&url=https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/&via=stitchfix_algo
  45. https://www.linkedin.com/sharearticle?mini=true&url=https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-id97/&title=stop using id97&summary= when i started playing with id97 four years ago i needed (and luckily had) tons of supercomputer time. but because of advances in our understanding of w...&source=stitch fix technology     multithreaded
  46. https://multithreaded.stitchfix.com/careers
  47. http://stitchfix.com/careers
  48. https://www.stitchfix.com/
  49. https://www.stitchfix.com/faq
  50. https://www.stitchfix.com/press
  51. http://multithreaded.stitchfix.com/blog/
  52. http://multithreaded.stitchfix.com/careers/
  53. https://www.stitchfix.com/terms
  54. https://www.stitchfix.com/privacy
  55. http://github.com/stitchfix
  56. http://github.com/stitchfix
  57. http://multithreaded.stitchfix.com/blog/
  58. http://multithreaded.stitchfix.com/careers/
  59. https://www.stitchfix.com/
  60. https://www.stitchfix.com/faq
  61. https://www.stitchfix.com/press
  62. https://www.stitchfix.com/terms
  63. https://www.stitchfix.com/privacy
