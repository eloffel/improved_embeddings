   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

machine learning algorithms: which one to choose for your problem

intuition of using different kinds of algorithms in different tasks

   [16]go to the profile of daniil korbut
   [17]daniil korbut (button) blockedunblock (button) followfollowing
   oct 26, 2017
   [18][1*jdqusr0c9xkln4vqpc3fzq.png]

   when i was beginning my way in data science, i often faced the problem
   of choosing the most appropriate algorithm for my specific problem. if
   you   re like me, when you open some article about machine learning
   algorithms, you see dozens of detailed descriptions. the paradox is
   that they don   t ease the choice.

   in this article for [19]statsbot, i will try to explain basic concepts
   and give some intuition of using different kinds of machine learning
   algorithms in different tasks. at the end of the article, you   ll find
   the structured overview of the main features of described algorithms.
   [1*pnwq69bjvew69yn9jdzixq.jpeg]

   first of all, you should distinguish 4 types of machine learning tasks:
     * supervised learning
     * unsupervised learning
     * semi-supervised learning
     * id23

supervised learning

   supervised learning is the task of inferring a function from labeled
   training data. by fitting to the labeled training set, we want to find
   the most optimal model parameters to predict unknown labels on other
   objects (test set). if the label is a real number, we call the task
   regression. if the label is from the limited number of values, where
   these values are unordered, then it   s classification.
   [0*we3sz--1nuewbmur.]
   [20]illustration source

unsupervised learning

   in unsupervised learning we have less information about objects, in
   particular, the train set is unlabeled. what is our goal now? it   s
   possible to observe some similarities between groups of objects and
   include them in appropriate clusters. some objects can differ hugely
   from all clusters, in this way we assume these objects to be anomalies.
   [1*93dyqjjbnizfa7l9jrvloq.png]
   [21]illustration source

semi-supervised learning

   semi-supervised learning tasks include both problems we described
   earlier: they use labeled and unlabeled data. that is a great
   opportunity for those who can   t afford labeling their data. the method
   allows us to significantly improve accuracy, because we can use
   unlabeled data in the train set with a small amount of labeled data.
   [0*b0gyqxnrxvqw3bc7.]
   [22]illustration source

id23

   id23 is not like any of our previous tasks because we
   don   t have labeled or unlabeled datasets here. rl is an area of machine
   learning concerned with how software agents ought to take actions in
   some environment to maximize some notion of cumulative reward.
   [0*_khg2vxxhjzx7vze.]
   [23]illustration source

   imagine, you   re a robot in some strange place, you can perform the
   activities and get rewards from the environment for them. after each
   action your behavior is getting more complex and clever, so you are
   training to behave the most effective way on each step. in biology,
   this is called adaptation to natural environment.

commonly used machine learning algorithms

   now that we have some intuition about types of machine learning tasks,
   let   s explore the most popular algorithms with their applications in
   real life.

id75 and linear classifier

   these are probably the simplest algorithms in machine learning. you
   have features x1,   xn of objects (matrix a) and labels (vector b). your
   goal is to find the most optimal weights w1,   wn and bias for these
   features according to some id168, for example, [24]mse or
   [25]mae for a regression problem. in the case of mse there is a
   mathematical equation from the least squares method:
   [1*3swfus_pddc0eno5uszatg.jpeg]

   in practice, it   s easier to optimize it with id119, that is
   much more computationally efficient. despite the simplicity of this
   algorithm, it works pretty well when you have thousands of features,
   for example, bag of words or id165ms in [26]text analysis. more
   complex algorithms suffer from overfitting many features and not huge
   datasets, while id75 provides decent quality.
   [0*wjifsyicutir-vjq.]
   [27]illustration source

   to prevent overfitting we often use id173 techniques like
   lasso and ridge. the idea is to add the sum of modules of weights and
   the sum of squares of weights, respectively, to our id168. read
   the great tutorial on these algorithms at the end of the article.

id28

   don   t confuse these classification algorithms with regression methods
   for using    regression    in its title. id28 performs
   binary classification, so the label outputs are binary. let   s define
   p(y=1|x) as the id155 that the output y is 1 under
   the condition that there is given the input feature vector x. the
   coefficients w are the weights that the model wants to learn.
   [1*011jovnsgv0fg5ufjwrpoa.png]

   since this algorithm calculates the id203 of belonging to each
   class, you should take into account how much the id203 differs
   from 0 or 1 and average it over all objects as we did with linear
   regression. such id168 is the average of cross-entropies:
   [0*-jzq269t8zm1-gq1.]

   don   t panic, i   ll make it easy for you. allow y to be the right
   answers: 0 or 1, y_pred         predicted answers. if y equals 0, then the
   first addend under sum equals 0 and the second is the less the closer
   our predicted y_pred to 0 according to the properties of the logarithm.
   similarly, in the case when y equals 1.

   what is great about a id28? it takes linear combination
   of features and applies non-linear function (sigmoid) to it, so it   s a
   very very small instance of neural network!

id90

   another popular and easy to understand algorithm is id90.
   their graphics help you see what you   re thinking and their engine
   requires a systematic, documented thought process.

   the idea of this algorithm is quite simple. in every node we choose the
   best split among all features and all possible split points. each split
   is selected in such a way as to maximize some functional. in
   classification trees we use cross id178 and gini index. in regression
   trees we minimize the sum of a squared error between the predictive
   variable of the target values of the points that fall in that region
   and the one we assign to it.
   [0*3ifnq-pvcrcxxr2o.]
   [28]illustration source

   we make this procedure recursively for each node and finish when we
   meet a stopping criteria. they can vary from minimum number of leafs in
   a node to tree height. single trees are used very rarely, but in
   composition with many others they build very efficient algorithms such
   as id79 or [29]gradient tree boosting.

id116

   sometimes you don   t know any labels and your goal is to assign labels
   according to the features of objects. this is called clusterization
   task.

   suppose you want to divide all data-objects into k clusters. you need
   to select random k points from your data and name them centers of
   clusters. the clusters of other objects are defined by the closest
   cluster center. then, centers of the clusters are converted and the
   process repeats until convergence.
   [0*xs5sahgyfcglqaz3.]

   this is the most clear clusterization technique, which still has some
   disadvantages. first of all, you should know the amount of clusters
   that we can   t know. secondly, the result depends on the points randomly
   chosen at the beginning and the algorithm doesn   t guarantee that we   ll
   achieve the global minimum of the functional.

   there are a range of id91 methods with different advantages and
   disadvantages, which you could learn in recommended reading.

principal component analysis (pca)

   have you ever prepared for a difficult exam on the last night or during
   the last hours? you have no chance to remember all the information, but
   you want to maximize information that you can remember in the time
   available, for example, learning first the theorems that occur in many
   exam tickets and so on.

   principal component analysis is based on the same idea. this algorithm
   provides id84. sometimes you have a wide range of
   features, probably highly correlated between each other, and models can
   easily overfit on a huge amount of data. then, you can apply pca.

   you should calculate the projection on some vectors to maximize the
   variance of your data and lose as little information as possible.
   surprisingly, these vectors are eigenvectors of correlation matrix of
   features from a dataset.
   [0*xc7pvitxyfdcrfya.]
   [30]illustration source

   the algorithm now is clear:

   1. we calculate the correlation matrix of feature columns and find
   eigenvectors of this matrix.

   2. we take these multidimensional vectors and calculate the projection
   of all features on them.

   new features are coordinates from a projection and their number depends
   on the count of eigenvectors, on which you calculate the projection.

neural networks

   i have already mentioned neural networks, when we talked about logistic
   regression. there are a lot of different architectures that are
   valuable in very specific tasks. more often, it   s a range of layers or
   components with linear connections among them and following
   nonlinearities.

   if you   re working with images, convolutional deep neural networks show
   the great results. nonlinearities are represented by convolutional and
   pooling layers, capable of capturing the characteristic features of
   images.
   [0*4ou1gsah-hzyaohi.]
   [31]illustration source

   for working with texts and sequences you   d better choose recurrent
   neural networks. id56s contain lstm or gru modules and can work with
   data, for which we know the dimension in advance. perhaps, one of the
   most known applications for id56s is [32]machine translation.

conclusion

   i hope that i could explain to you common perceptions of the most used
   machine learning algorithms and give intuition on how to choose one for
   your specific problem. to make things easier for you, i   ve prepared the
   structured overview of their main features.

   id75 and linear classifier. despite an apparent
   simplicity, they are very useful on a huge amount of features where
   better algorithms suffer from overfitting.

   id28 is the simplest non-linear classifier with a linear
   combination of parameters and nonlinear function (sigmoid) for binary
   classification.

   id90 is often similar to people   s decision process and is
   easy to interpret. but they are most often used in compositions such as
   id79 or gradient boosting.

   id116 is more primal, but a very easy to understand algorithm, that
   can be perfect as a baseline in a variety of problems.

   pca is a great choice to reduce dimensionality of your feature space
   with minimum loss of information.

   neural networks are a new era of machine learning algorithms and can be
   applied for many tasks, but their training needs huge computational
   complexity.

recommended sources

     * [33]overview of id91 methods
     * [34]a complete tutorial on ridge and lasso regression in python
     * [35]youtube channel about ai for beginners with great tutorials and
       examples

   iframe: [36]/media/02231cd5403151a2a463e36cc3b88462?postid=183cc73197c

you   d also like:

   [37]etl vs elt: considering the advancement of data warehouses |
   statsbot blog
   with the advent of modern cloud-based data warehouses, such as bigquery
   or redshift, the traditional concept of etl is   statsbot.co
   [38]sql queries for funnel analysis
   a template for building sql funnel queriesblog.statsbot.co
   [39]how to reduce churn rate by handling stripe failed payments
   how we automated dunning managementblog.statsbot.co

     * [40]machine learning
     * [41]algorithms
     * [42]data science
     * [43]neural networks
     * [44]id23

   (button)
   (button)
   (button) 3.9k claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [45]go to the profile of daniil korbut

[46]daniil korbut

   deep learning research engineer at insilico medicine

     (button) follow
   [47]stats and bots

[48]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 3.9k
     * (button)
     *
     *

   [49]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [50]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/183cc73197c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/machine-learning-algorithms-183cc73197c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/machine-learning-algorithms-183cc73197c&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_5ditjzpknxe0---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup
  17. https://blog.statsbot.co/@daniilkorbut
  18. https://cube.dev/
  19. https://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=ml_algorithms
  20. https://scorecardstreet.wordpress.com/2015/12/09/is-machine-learning-the-new-epm-black/
  21. http://www.constonline.com/machine-learning
  22. https://makarandtapaswi.wordpress.com/2013/04/30/labeled-data-unlabeled-data-and-constraints/
  23. https://www.analyticsvidhya.com/blog/2016/12/getting-ready-for-ai-based-gaming-agents-overview-of-open-source-reinforcement-learning-platforms/
  24. https://en.wikipedia.org/wiki/mean_squared_error
  25. https://en.wikipedia.org/wiki/mean_absolute_error
  26. https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278
  27. http://newsdog.today/a/article/582ebdf11290711e26997ce2/
  28. http://cway-quantlab.blogspot.ru/2017/06/optimize-trading-system-with-gradient_21.html
  29. https://en.wikipedia.org/wiki/gradient_boosting#gradient_tree_boosting
  30. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
  31. http://www.smash.com/ostagram-uses-neural-networks-merge-two-pictures-psychedelic-weirdness/
  32. https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4
  33. http://scikit-learn.org/stable/modules/id91.html#overview-of-id91-methods
  34. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/
  35. https://www.youtube.com/channel/ucwn3xxrkmtpmbkwht9fue5a
  36. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=183cc73197c
  37. https://statsbot.co/blog/etl-vs-elt/
  38. https://blog.statsbot.co/sql-queries-for-funnel-analysis-35d5e456371d
  39. https://blog.statsbot.co/how-to-reduce-churn-rate-by-27-by-handling-stripe-failed-payments-e75892f8956c
  40. https://blog.statsbot.co/tagged/machine-learning?source=post
  41. https://blog.statsbot.co/tagged/algorithms?source=post
  42. https://blog.statsbot.co/tagged/data-science?source=post
  43. https://blog.statsbot.co/tagged/neural-networks?source=post
  44. https://blog.statsbot.co/tagged/reinforcement-learning?source=post
  45. https://blog.statsbot.co/@daniilkorbut?source=footer_card
  46. https://blog.statsbot.co/@daniilkorbut
  47. https://blog.statsbot.co/?source=footer_card
  48. https://blog.statsbot.co/?source=footer_card
  49. https://blog.statsbot.co/
  50. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  52. https://statsbot.co/blog/etl-vs-elt/
  53. https://blog.statsbot.co/sql-queries-for-funnel-analysis-35d5e456371d
  54. https://blog.statsbot.co/how-to-reduce-churn-rate-by-27-by-handling-stripe-failed-payments-e75892f8956c
  55. https://medium.com/p/183cc73197c/share/twitter
  56. https://medium.com/p/183cc73197c/share/facebook
  57. https://medium.com/p/183cc73197c/share/twitter
  58. https://medium.com/p/183cc73197c/share/facebook
