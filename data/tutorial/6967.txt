   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

singular value decomposition (svd) tutorial: applications, examples,
exercises

a complete tutorial on the singular value decomposition method

   [16]go to the profile of peter mills
   [17]peter mills (button) blockedunblock (button) followfollowing
   oct 5, 2017
   [18][1*jdqusr0c9xkln4vqpc3fzq.png]

   every so often, maybe once or twice a decade, a new mathematical
   technique or algorithm comes along that changes the way we do things.
   maybe the method starts out in a small niche or field but eventually
   expands to many other, completely unrelated disciplines and you cannot
   stop thinking of new uses for it. we   re talking about techniques like
   fast fourier decomposition, monte carlo integration, simulated
   annealing, runge kutta integration, and pseudo-random number
   generation.

   a friend of the [19]statsbot team, peter mills, calls such methods
      power tools.    we asked him to tell us about one of these
   tools         singular value decomposition, or svd, with examples and
   applications.
   [0*oe2zsitapy9njsx-.]

   this singular value decomposition tutorial assumes you have a good
   working knowledge of both matrix algebra and vector calculus. we start
   with a short history of the method, then move on to the basic
   definition, including a brief outline of numerical procedures. this is
   then followed by a more intuitive derivation meant to demonstrate the
   meaning of singular value decomposition and then to a pair of common
   applications.

   the final section works out a complete program that uses svd in a
   machine-learning context. to help you become more familiar with the
   material, exercises are provided throughout.

history

   the technique of singular value decomposition, or svd for short, has a
   long and somewhat surprising history. it started out in the social
   sciences with intelligence testing. early intelligence researchers
   noted that tests given to measure different aspects of intelligence,
   such as verbal and spatial, were often closely correlated.

   because of this, they hypothesized that there was a general measure of
   intelligence in common, which they called    g,    for    general
   intelligence,    now more commonly known as    i.q.    so they set about
   teasing out the different factors that made up intelligence so as to
   pull out the most important one.

   svd is known under many different names. in the early days, as the
   above passage implies, it was called,    factor analysis.    other terms
   include principal component (pc) decomposition and empirical orthogonal
   function (eof) analysis. all these are mathematically equivalent,
   although the way they are treated in the literature is often quite
   different.

   today, singular value decomposition has spread through many branches of
   science, in particular psychology and sociology, climate and
   atmospheric science, and astronomy. it is also extremely useful in
   machine learning and in both descriptive and predictive statistics.

technical introduction

   singular value decomposition is a method of decomposing a matrix into
   three other matrices:
   [0*i4rdkiae0o1zxtbd.]
   (1)

   where:
     * a is an m    n matrix
     * u is an m    n orthogonal matrix
     * s is an n    n diagonal matrix
     * v is an n    n orthogonal matrix

   the reason why the last matrix is transposed will become clear later on
   in the exposition. also, the term,    orthogonal,    will be defined (in
   case your algebra has become a little rusty) and the reason why the two
   outside matrices have this property made clear.

   for the moment, we will assume that m     n. what happens when this isn   t
   true is quite interesting and is one of the keys, in my opinion, to
   understanding singular value decomposition.

   this is already becoming quite complicated so i will rewrite equation
   (1) using summation notation. this is my go-to method of proceeding
   whenever i am having trouble with a matrix equation. in this case,
   while it doesn   t make anything simpler, it does make everything
   absolutely explicit:
   [0*mpqrslotboqfcdp7.]

   note how we   ve collapsed the diagonal matrix, s, into a vector, thus
   simplifying the expression into a single summation. the variables,
   {s   }, are called singular values and are normally arranged from largest
   to smallest:
   [0*7j7tn93bocfx5zba.]

   the columns of u are called left singular vectors, while those of v are
   called right singular vectors.

   we know that u and v are orthogonal, that is:
   [0*shyyi8toyfxcblbw.]

   where i is the identity matrix. only the diagonals of the identity
   matrix are 1, with all other values being 0. note that because u is not
   square we cannot say that u transpose(u)=i, so u is only orthogonal in
   one direction.

   using the orthogonality property, we can rearrange (1) into the
   following pair of eigenvalue equations:
   [0*06o6zgkloejmuk3r.]
   (2)
   [0*wbo4yukfzx0ktxme.]
   (3)

numerical procedure

   since transpose(a)a is the same size or smaller than a transpose(a), a
   typical procedure is to plug equation (3) into an eigenvalue calculator
   to find v and s   and then find u by projecting a onto v:
   [0*dv3o_upes5391ule.]
   (4)

   note that the method is completely symmetric; u and v change places
   when a is transposed:
   [0*32xitddple2vjbmw.]

   thus, if m < n, we can transpose a, perform the decomposition, then
   swap the roles of u and v.

   in this case, u will be an m    m square matrix since there can be at
   most m non-zero singular values, while v will be an n    m matrix.

exercises

    1. use equations (2) and (3) to show that both u and v are orthogonal
       and that the eigenvalues, {s     }, are all positive.
    2. show that if m < n there will be at most m non-zero singular
       values.
    3. show that the eigenvalues in equations (2) and (3) must be one and
       the same.

understanding svd

   above is just the dry, technical description. it doesn   t give us an
   intuitive feel for what the method is doing. so let   s imagine the
   simplest example in two dimensions. it generalizes very naturally to
   higher dimensions.

   suppose we have two, two-dimensional vectors, x   =(x   , y   ), and x   =(x   ,
   y   ). we can fit an ellipse with major axis, a, and minor axis, b, to
   these two vectors as shown in the figure. but to make things easier on
   ourselves and save typing, we write out the equations using matrix
   algebra.

   we can construct an ellipse of any size and orientation by stretching
   and rotating a unit circle.

   let x   =(x   , y   ) be the transformed coordinates:
   [0*d9xjxblqhuxu--_s.]

   where r is a rotation matrix:
   [0*sr3dslpd2t2hryz0.]

   and m is a diagonal matrix containing the major and minor axes:
   [0*pgtk67rvkcbcyn4_.]
   [0*y3ipfrxdsxothfs3.]

   lets write this out term-by-term, both for the general case:
   [0*gv2_io-rndoh3ybq.]

   where m    is the ith diagonal of the matrix, m, and for the
   two-dimension case:
   [0*l1vvg9rgqdausgfa.]

   note that the rotation is clockwise, opposite the usual sense because
   we are going from the untransformed to the transformed coordinate
   system rather than the other way around. for the resulting ellipse, the
   angle will be in the usual, counter-clockwise sense.

   the equation for a unit circle is as follows:
   [0*7ikflfwwgkqagn6e.]

   we wish to fit a set of x   s, which we collect as the rows of a matrix,
   x:
   [0*9jkononw2ccynfuj.]
   (5)

   the resulting matrix equation is given:
   [0*kjv-xcr-9dtzo5yt.]

   this is just a rearrangement of equation (3). if we regard a as a
   collection of points, then the singular values are the axes of a least
   squares fitted ellipsoid while v is its orientation. the matrix u is
   the projection of each of the points in a onto the axes.

applications

   the sum of the squares of the singular values should be equal to the
   total variance in a. thus, the size of each tells you how much of the
   total variance is accounted for by each singular vector. you can create
   a truncated svd containing, for instance, 99% of the variance:
   [0*w_zfh1dhllpntdhw.]
   (6)

   where p<n is the number of singular values that we   ve decided to keep.
   typically, this will be fewer than the top ten (p=10) singular values.
   it is in this facility of singular value decomposition to exclude the
   least significant components of a that much of its power lies.

solving matrix equations

   some more rearrangement of (1) shows that svd can be used for solving
   systems of linear equations:
   [0*qnnm6snk-k0yjn0w.]
   (7)

   or, in summation notation:
   [0*mirqth-nuke6wybo.]

   if this was all there was to it, there would be little to recommend svd
   over simpler matrix solvers, such as qr decomposition or even gaussian
   elimination. in many cases, however, the matrix will be
   ill-conditioned, making the solution unstable so that it blows up or
   produces floating point overflow. this will show up in the singular
   values: the smallest ones will be very close to zero as measured
   relative to the largest. to produce a stable solution, we just throw
   these components away as in equation (6), above.

   it also generalizes to non-square matrices. since an m    n matrix,
   where m > n, will have only n singular values, in svd this is
   equivalent to solving an m    m matrix using only n singular values. for
   non-square matrices, matrix inversion using singular value
   decomposition is equivalent to solving the normal equation:
   [0*h5gqjgwe-ldbdwyg.]
   (8)

   and produces the solution for x that is closest to the origin, that is,
   of minimal length. the normal equation is the solution to the following
   minimization problem:
   [1*2lek4wir491yqryjijotrg.png]
   (9)

   thus, they are both generalized, linear, least squares fitting
   techniques.

data reduction

   a typical machine learning problem might have several hundred or more
   variables, while many machine learning algorithms will break down if
   presented with more than a few dozen. this makes singular value
   decomposition indispensable in ml for variable reduction.

   we have already seen in equation (6) how an svd with a reduced number
   of singular values can closely approximate a matrix. this can be used
   for data compression by storing the truncated forms of u, s, and v in
   place of a and for variable reduction by replacing a with u. results
   will need to be transformed back to the original coordinate system by
   multiplying with s and v in accordance with equation (4).

   a full example, including computer code, will be worked out in the
   example section, below.

exercises

    1. show that equation (7) is equivalent to equation (8), the normal
       equation.
    2. use least squares minimization in equation (9) to derive the normal
       equation.

example: variable reduction

   suppose that the m    n matrix, a, stores a set of training data with
   each training vector taking up one row as in (5) and that n, the
   dimension of each vector, is very large.

   we want to feed a to a id91 algorithm that outputs a fixed number
   of cluster centers. because n is large, however, the algorithm takes
   too long or is unstable, so we want to reduce the number of variables
   using svd. we want to do this in a way that   s transparent so it looks
   like we are still working in the un-transformed coordinates.

step 1: reading in the data

   to begin with, we will need to read in the data to fill up a. i   m not a
   big fan of python and think that c or c++ are better languages for
   machine learning applications.
//subroutine header for performing cluster analysis:
#include "cluster.h"
//maximum number of clusters:
#define max_cluster 10
int main(int argc, char **argv) {
  char *infile;            //input file
  char *outfile;           //output file
  file *fs;                //file pointer
  double **a;              //matrix of training data/u
  int m;                   //number of rows in matrix
  int n;                   //number of columns in matrix
  int nsv;                 //number of singular values
  if (argc!=4) {
    printf("syntax: cluster_svd nsv train centers\n");
    printf("  where:\n");
    printf("nsv      = number of singular values (0 = use untransformed data)\n"
);
    printf("infile   = ascii input file containing training data\n");
    printf("output   = ascii output file containing cluster centers\n");
    printf("\n");
    printf("file format:\n");
    printf("- one line header containing number of rows and number of columns\n"
);
    printf("- row major list of each matrix element\n");
    exit(1);
  }
  if (sscanf(argv[1], "%d", &nsv)!=1) {
    fprintf(stderr, "error parsing first command line argument\n");
    exit(1);
  }
  infile=argv[2];
  outfile=argv[3];
  fs=fopen(infile, "r");
  if (fs==null) {
    fprintf(stderr, "error opening input file, %s\n", infile);
    exit(1);
  }
  if (fscanf(fs, "%d %d", &m, &n)!=2) {
    fprintf(stderr, "format error in input file: %s line 0", infile);
    exit(1);
  }
  if (nsv>n) {
    fprintf(stderr, "command line parameter nsv=%d out of range\n", nsv);
    exit(1);
  }
  a=new double *[m];
  a[0]=new double[m*n];
  for (int i=1; i<m; i++) a[i]=a[0]+i*n;
  for (int i=0; i<m; i++) {
    for (int j=0; j<n; j++) {
      if (fscanf(fs, "%lg", a[i]+j)!=1) {
        fprintf(stderr, "format error in input file, %s, line %d\n", infile, i);
        exit(1);
      }
    }
  }
  fclose(fs);

step 2: performing svd

   we are using a canned singular value decomposition routine that is
   contained in the header file, svd.h:
#ifndef svd_h
#define svd_h
//subroutine for singular value decomposition:
int                       //returns an error code (0 for success)
     svd (double **a,     //input matrix--replaced by u on output
                int m,        //number of rows
                int n,        //number of columns
                double *s,    //singular values
                double **vt); //v--right singular vectors
#endif

   svd routines are often more complicated than this, particularly in
   regards to the matrix and vector types used, but it would be
   straightforward to encapsulate the whole thing in a    wrapper    function.

   using the singular value decomposition routine is equally
   straightforward. continuing from the previous section:
  double *ave;
  double *s;               //singular values
  double **vt;             //right singular vectors
  //first we calculate and remove the arithmetic means:
  ave=new double[n];
  for (int i=0; i<n; i++) ave[i]=0;
  for (int i=0; i<m; i++) {
    for (int j=0; j<n; j++) {
      ave[j]+=a[i][j];
    }
  }
  for (int i=0; i<n; i++) ave[i]/=m;
  for (int i=0; i<m; i++) {
    for (int j=0; j<n; j++) {
      a[i][j]-=ave[j];
    }
  }
  if (nsv>0) {
    //make space for singular values:
    s=new double[n];
    //make space for right singular vectors:
    vt=new double *[n];
    vt[0]=new double[n*n];
    for (int i=1; i<n; i++) vt[i]=vt[0]+i*n;
    //perform the decomposition:
    int err=svd(a, m, n, s, vt);
    if (err!=0) {
      fprintf(stderr, "error in svd subroutine\n");
      exit(err);
    }
  }

step 3: performing the cluster analysis

   the operation of the id91 algorithm generates a set of c cluster
   centers, {        ; i     [1, c]}:
#ifndef cluster_h
#define cluster_h
int                            //returns number of cluster centers
    cluster (double ** x,      //training vectors
                int m,         //number of training vectors
                int n,         //dimension of each vector
                int max_nc,    //maximum number of cluster centers
                double **mu);  //returned cluster centers
#endif

   in the third part of the program, continuing from above, we generate
   the cluster centers:
double **mu_p;      //matrix of cluster centers
  int nc;           //number of cluster centers
  //make space for cluster centers:
  mu_p=new double *[max_cluster];
  mu_p[0]=new double[max_cluster*n];
  for (int i=1; i<max_cluster; i++) mu_p[i]=mu_p[0]+i*n;
  if (nsv>0) {
    //make space for cluster centers:
    nc=cluster(a, m, nsv, max_cluster, mu_p);
  } else {
    //make space for cluster centers:
    nc=cluster(a, m, n, max_cluster, mu_p);
  }
  if (nc <= 0) {
    fprintf(stderr, "cluster algorithm failed");
    exit(-1);
  }

   because the id91 algorithm used the transformed training data,
   cluster centers will be in the transformed system:
   [0*itixytf4_regfrig.]

   or:
   [0*ju5gofbaxmbwuest.]

   writing it out component-by-component:
   [0*vfkc1nc8uf3f1xyd.]
   (10)

   where p is the number of coordinates in the reduced system.

step 4: storing the results

   we want to store the cluster centers in the un-transformed coordinate
   system, thus we want to apply equation (10).
  double **mu;        //cluster centers in un-transformed coords
  //allocate space for the un-transformed cluster centers:
  mu=new double *[nc];
  mu[0]=new double[nc*n];
  for (int i=1; i<nc; i++) mu[i]=mu[0]+i*n;
  //perform the coordinate transformation:
  for (int i=0; i<nc; i++) {
    for (int j=0; j<n; j++) {
      mu[i][j]=ave[j];
      if (nsv>0) {
        for (int k=0; k<nsv; k++) mu[i][j]+=vt[k][j]*s[k]*mu_p[i][k];
      } else {
        mu[i][j]+=mu_p[i][j];
      }
    }
  }
  //write the results to a file:
  fs=fopen(outfile, "w");
  if (fs==null) {
    fprintf(stderr, "error opening output file, %s\n", outfile);
    exit(1);
  }
  fprintf(fs, "%d %d\n", nc, n);
  for (int i=0; i<nc; i++) {
    for (int j=0; j<n; j++) fprintf(fs, "%16.8lg", mu[i][j]);
    fprintf(fs, "\n");
  }
  fclose(fs);
  //clean up:
  delete [] mu[0];
  delete [] mu;
  delete [] mu_p[0];
  delete [] mu_p;
  delete [] ave;
  delete [] a[0];
  delete [] a;
  if (nsv>0) {
    delete [] s;
    delete [] vt[0];
    delete [] vt;
  }
  return 0;
}

exercises

    1. how would you improve the example program?
    2. do you think the example program should be split into two? if so,
       how would you split it?
    3. encapsulate the vector multiplication routine in a subroutine
       called matrix_times_vector. design the calling syntax for the
       subroutine.
    4. using struct, typedef or class, encapsulate both vectors and
       matrices into a pair of abstract types called vect and matrix,
       respectively. design an api for the types.
    5. find at least three libraries online that do the above.
    6. find at least two libraries for performing svd.
    7. encapsulate them within the svd subroutine, above.
    8. find at least two libraries for performing eigenvalue
       decomposition.
    9. encapsulate these within the svd subroutine.
   10. find an eigenvalue library with the option to return only the top k
       eigenvalues/vectors. encapsulate this within the following singular
       value decomposition subroutine:

int svd(double **a,          //matrix -- replaced by u on output
  int m,                     //number of rows
  int n,                     //number of columns
  int k,                     //desired number of singular values
  double*s,                  //singular values
  double **vt);              //right singular vectors

conclusion

   in this tutorial, we have defined singular value decomposition and
   shown just a tiny fraction of the uses to which it can be put. the
   method can also be used to retrieve atmospheric variables from
   satellite measurements; it can be used to interpolate sparse
   measurements; and it can be used directly as a machine learning
   technique both for classification and regression as well as many, many
   other things. see if you can come up with something for your own
   project.

   iframe: [20]/media/02231cd5403151a2a463e36cc3b88462?postid=52c695315254

you   d also like:

   [21]chatbots with machine learning: building neural conversational
   agents
   using machine learning approaches to build smart
   chatbotsblog.statsbot.co
   [22]google analytics audit checklist and tools
   auditing a google analytics setup like a problog.statsbot.co
   [23]bayesian learning for statistical classification
   a bayesian introduction to statistical classification
   problemsblog.statsbot.co

     * [24]machine learning
     * [25]data science
     * [26]analytics
     * [27]data analysis

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of peter mills

[29]peter mills

   passionate about science. interested in atmospheric physics, chaos
   theory and machine learning.

     (button) follow
   [30]stats and bots

[31]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [32]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [33]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/52c695315254
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_klftexhnzyml---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@peteymills?source=post_header_lockup
  17. https://blog.statsbot.co/@peteymills
  18. https://cube.dev/
  19. http://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=svd
  20. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=52c695315254
  21. https://blog.statsbot.co/chatbots-machine-learning-e83698b1a91e
  22. https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a
  23. https://blog.statsbot.co/bayesian-learning-for-statistical-classification-f2362d620428
  24. https://blog.statsbot.co/tagged/machine-learning?source=post
  25. https://blog.statsbot.co/tagged/data-science?source=post
  26. https://blog.statsbot.co/tagged/analytics?source=post
  27. https://blog.statsbot.co/tagged/data-analysis?source=post
  28. https://blog.statsbot.co/@peteymills?source=footer_card
  29. https://blog.statsbot.co/@peteymills
  30. https://blog.statsbot.co/?source=footer_card
  31. https://blog.statsbot.co/?source=footer_card
  32. https://blog.statsbot.co/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://blog.statsbot.co/chatbots-machine-learning-e83698b1a91e
  36. https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a
  37. https://blog.statsbot.co/bayesian-learning-for-statistical-classification-f2362d620428
  38. https://medium.com/p/52c695315254/share/twitter
  39. https://medium.com/p/52c695315254/share/facebook
  40. https://medium.com/p/52c695315254/share/twitter
  41. https://medium.com/p/52c695315254/share/facebook
