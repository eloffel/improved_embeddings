   #[1]danijar hafner

   [2]danijar hafner

   [3]posts [4]publications [5]materials

id38 with layer norm and gru

   i conducted some experiments to see what types of recurrent neural
   networks work well on the [6]text8 dataset. specifically, i compared
   the memory cells lstm, gru, and mgu, whether to use layer
   id172, and three methods for initializing weights.

   suprisingly, this gives you state-of-the-art performance without using
   any of the recent id173 methods. if you want to try this out
   yourself, here is my [7]tensorflow implementation of the gru cell that
   supports layer id172 and the different initializers.

   text8 is a id38 benchmark, where the neural network reads
   part of a wikipedia article and needs to predict the following
   character of the text. performance is measured in bits per character
   (bpc), which describes how well our model can compress the text. the
   fewer bits, the better.

   all experiments use a single recurrent layer of 2000 units, a batch
   size of 100, sequences of length 200 bytes, and the adam optimizer with
   a fixed learning rate of .

memory cell design

   plain recurrent neural networks compute a completely new hidden state
   at every time step. this makes it hard for them to remember details
   over many time steps. the most common solution for this is the lstm
   cell, that uses local context values that are preserved over time
   steps.

   in the last years, researcher proposed several alternatives to reduce
   the complexity and number of parameters of lstm. here, i   m comparing
   long short-term memory (lstm, [8]hochreiter97) to the gated recurrent
   unit (gru, [9]chung15) and the minimal gated unit (mgu, [10]zhou16).

   id38 with different id56 cells on text8.

   interestingly, gru performs better than lstm here, although it uses
   fewer parameters. usually, more parameters is a big advantage in
   compression tasks such as id38. mgu uses the fewest
   parameters and also performs worst on this task.

layer id172

   id172 inside neural networks is known to improve performance in
   many cases. especially recurrent networks suffer from vanishing or
   exploding gradients when their weight matrix changes the magnitude of
   hidden activations too much between time steps. layer norm ([11]ba16)
   centers and scales the activations at every time step, so that they
   stay in a similar range.

   id38 with and without layer norm on text8.

   the results shown here are averaged along the different memory cell
   designs and weight initializations. as seen in the diagram, layer norm
   both speeds up training and results in significantly better final
   performance. i am surprised by such clear results, but at least from
   this task it looks like layer norm should be a default for recurrent
   networks.

weight initialization

   sometimes, the way we initialize weights is critical in training neural
   networks. there are several methods for this, that basically sample
   weights from different distributions and scale them based on layer
   sizes.

   xavier initialization ([12]glorot10) samples uniform weights and scales
   them by the number of incoming and outgoing activations. variance
   scaling ([13]he15) is similar, but only considers the incoming
   activations for scaling and samples from a gaussian. orthogonal
   initialization ([14]saxe14) is more sophisticated and uses svd to
   compute weights that initially preserve the gradient norm.

   id38 using different initialization on text8.

   the choice of initialization did not have a big impact on performance
   in my experiments. ironically, the variance scaling initializer
   resulted in larger variance of the performance. the orthogonal
   initializer could not show a benefit over the simple xavier
   initialization that worked best.

the best model

   the best model uses gru memory cells, layer id172, and the
   xavier initializer. with 14 million parameters it gets to 1.27 bpc on
   the evaluation data, which is better than most reported results on the
   task ([15]krueger17, [16]cooijmans17, [17]melis17, [18]zilly17):

   text8 performance of gru with layer norm.

   in conclusion, try layer norm for your recurrent networks if you
   haven   t, don   t worry too much about weight initialization, and consider
   using gru, possibly with a larger layer, over lstm. feel free to check
   out my [19]tensorflow implementation of gru with layer norm and xavier
   initialization to reproduce the results and for use in your own
   projects.

   you can use this post under the open [20]cc by-sa 3.0 license and cite
   it as:
@misc{hafner2017id56bench,
  author = {hafner, danijar},
  title = {id38 with layer norm and gru},
  year = {2017},
  howpublished = {blog post},
  url = {https://danijar.com/language-modeling-with-layer-norm-and-gru/}
}

   please enable javascript to view the [21]comments powered by disqus.

danijar hafner

     * [22][email protected]
     * [23]danijar
     * [24]danijarh

   researcher aiming to build intelligent machines based on concepts of
   the human brain.

references

   visible links
   1. https://danijar.com/feed.xml
   2. https://danijar.com/
   3. https://danijar.com/blog/
   4. https://scholar.google.com/citations?user=vinmgpyaaaaj
   5. https://danijar.com/materials/
   6. http://mattmahoney.net/dc/textdata.html
   7. https://gist.github.com/danijar/ff8b4b81da55c99b5096913c4953d29b
   8. http://www.bioinf.jku.at/publications/older/2604.pdf
   9. https://arxiv.org/pdf/1502.02367.pdf
  10. https://arxiv.org/pdf/1603.09420.pdf
  11. https://arxiv.org/pdf/1607.06450.pdf
  12. http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
  13. https://arxiv.org/pdf/1502.01852v1.pdf
  14. https://arxiv.org/pdf/1312.6120v3.pdf
  15. https://arxiv.org/pdf/1606.01305.pdf
  16. https://arxiv.org/pdf/1603.09025.pdf
  17. https://arxiv.org/pdf/1707.05589.pdf
  18. https://arxiv.org/pdf/1607.03474.pdf
  19. https://gist.github.com/danijar/ff8b4b81da55c99b5096913c4953d29b
  20. https://creativecommons.org/licenses/by-sa/3.0/
  21. https://disqus.com/?ref_noscript
  22. https://danijar.com/cdn-cgi/l/email-protection#90fdf1f9fcd0f4f1fef9faf1e2bef3fffd
  23. https://github.com/danijar
  24. https://twitter.com/danijarh

   hidden links:
  26. https://danijar.com/language-modeling-with-layer-norm-and-gru/
