                             about the test data

                                matt mahoney
                   last update: sept. 1, 2011. [1]history

   the test data for the [2]large text compression benchmark is the first
   10^9 bytes of the english wikipedia dump on mar. 3, 2006.
   [3]http://download.wikipedia.org/enwiki/20060303/enwiki-20060303-pages-
   articles.xml.bz2 (1.1 gb or 4.8 gb after decompressing with bzip2 -
   link no longer works). results are also given for the first 10^8 bytes,
   which is also used for the [4]hutter prize. these files have the
   following sizes and checksums:
file     size (bytes)   md5 (gnu md5sum 1.22)             sha-1 (slavasoft fsum
2.51)
------   -------------  --------------------------------  ----------------------
------------------
enwik8     100,000,000  a1fa5ffddb56f4953e226637dabbb36a  57b8363b814821dc9d47aa
4d41f58733519076b2
enwik9   1,000,000,000  e206c3450ac99950df65bf70ef61a12d  2996e86fb978f93cca8f56
6cc56998923e7fe581

   download in [5]ppmd var. j format (requires 256 mb free memory):
   [6]enwik8.pmd (21,388,296 bytes) [7]enwik9.pmd (183,964,915 bytes).

   download in zip format: [8]enwik8.zip (36,445,475 bytes) [9]enwik9.zip
   (322,592,222 bytes).

   the data is [10]utf-8 encoded [11]xml consisting primarily of english
   text. enwik9 contains 243,426 article titles, of which 85,560 are
   #redirect to fix broken links, and the rest are regular articles. the
   example fragment below shows a redirection of "ada" to "ada programming
   language" and the start of a regular article with title "anarchism".

   the data is utf-8 clean. all characters are in the range u'0000 to
   u'10ffff with valid encodings of 1 to 4 bytes. the byte values 0xc0,
   0xc1, and 0xf5-0xff never occur. also, in the wikipedia dumps, there
   are no control characters in the range 0x00-0x1f except for 0x09 (tab)
   and 0x0a (linefeed). linebreaks occur only on paragraph boundaries, so
   they always have a semantic purpose. in the example below, lines were
   broken at 80 characters, but in reality each paragraph is one long
   line.

   the data contains some url encoded [12]xhtml tags such as &lt;ref&gt
   ... &lt;/ref&gt; and &lt;br /&gt; which decode to <ref> ... </ref>
   (citation) and <br /> (line break). however, hypertext links have their
   own encoding. external links are enclosed in square brackets in the
   form [url anchor text]. internal links are encoded as [[wikipedia title
   | anchor text]], omitting the title and vertical bar if the title and
   anchor text are identical. non-english characters are sometimes url
   encoded as &amp;#945;, meaning &#945; (greek alpha,   ), but are more
   often coded directly as a utf-8 byte sequence.
  <page>
    <title>ada</title>
    <id>11</id>
    <revision>
      <id>15898946</id>
      <timestamp>2002-09-22t16:02:58z</timestamp>
      <contributor>
        <username>andre engels</username>
        <id>300</id>
      </contributor>
      <minor />
      <text xml:space="preserve">#redirect [[ada programming language]]</text>
    </revision>
  </page>
  <page>
    <title>anarchism</title>
    <id>12</id>
    <revision>
      <id>42136831</id>
      <timestamp>2006-03-04t01:41:25z</timestamp>
      <contributor>
        <username>cjames745</username>
        <id>832382</id>
      </contributor>
      <minor />
      <comment>/* anarchist communism */  too many brackets</comment>
      <text xml:space="preserve">{{anarchism}}
'''anarchism''' originated as a term of abuse first used against early [[working
 class]] [[radical]]s including the [[diggers]] of the [[english revolution]] an
d the [[sans-culotte|''sans-culottes'']] of the [[french revolution]].[http://uk
.encarta.msn.com/encyclopedia_761568770/anarchism.html] whilst the term is still
 used in a pejorative way to describe ''&quot;any act that used violent means to
 destroy the organization of society&quot;''&lt;ref&gt;[http://www.cas.sc.edu/so
cy/faculty/deflem/zhistorintpolency.html history of international police coopera
tion], from the final protocols of the &quot;international conference of rome fo
r the social defense against anarchists&quot;, 1898&lt;/ref&gt;, it has also bee
n taken up as a positive label by self-defined anarchists.

the word '''anarchism''' is [[etymology|derived from]] the [[greek language|gree
k]] ''[[wiktionary:&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&
amp;#945;|&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&amp;#945;
]]'' (&quot;without [[archon]]s (ruler, chief, king)&quot;). anarchism as a [[po
litical philosophy]], is the belief that ''rulers'' are unnecessary and should b

   the graph below shows the incremental compressed size of enwik9 over a
   sliding window of about 2-4 mb when compressed with ppmd var. j with
   options -o10 -m256 -r1 (maximum compression as in the main table). the
   horizontal axis is the position in the file, from 0 to 1 gb. the
   vertical axis is the compression ratio on a scale of 0 to 0.3. the
   graph was produced by modifying the source code for ppmd to print the
   graph coordinates, then smoothing the data for display.

                              [ppmd_enwik9.png]

      incremental compression ratio of enwik9 with ppmd var. j set for
                            maximum compression.

   the dip in the middle of the graph is due to a large group of articles
   on towns in the us written in a similar style that appear to be
   generated automatically from a table of census data. a sample of titles
   from this region appears below. unrelated articles are occasionally
   mixed in.
    springboro, pennsylvania
    steuben township, pennsylvania
    summerhill township, crawford county, pennsylvania
    summit township, crawford county, pennsylvania
    titusville, pennsylvania
    townville, pennsylvania
    troy township, crawford county, pennsylvania
    union township, crawford county, pennsylvania
    venango, pennsylvania
    venango township, crawford county, pennsylvania
    vernon township, pennsylvania

  string repetition statistics

   the following diagrams show the distribution of string matches of
   length 1 (black), 2 (red), 4 (green), and 8 (blue). the horizontal axis
   represents the position in the file. the vertical axis shows the
   distance backwards to the previous match on a logarithmic scale. the
   major tick marks reading upwards are 1, 10, 100, 1000, etc.

                                  [e8.png]

                                   enwik8

   enwik8 is fairly uniform. the blue band at the top shows that matches
   of length 8 are most often separated by at least 10^5 bytes (5 major
   tick marks) up to the entire length of the file. the green band shows
   that matches of length 4 are most commonly separated by 10^3 to 10^4
   bytes. the red band shows that matches of length 2 are separated by
   about 10 to 300 bytes. the gray band shows that single byte matches are
   usually separated by 1 or by 3 to about 10. the light gray band shows
   an absence of matches separated by 2, such as "aba".

                                  [e9.png]

                                   enwik9

   the highly compressible region in the center of enwik9 is clearly
   visible. the dark blue-green band shows that there are frequent matches
   of length 4-8 separated by about 3000 bytes, the length of one article.
   the articles are fairly uniform in length, but not exactly so. the dark
   red bands below it show a separation of around 20-80 bytes, typical of
   tables.

   the blue region extends all the way to the top of the image, showing
   redundancy across the entire file. thus, a compressor would benefit by
   using lots of memory. breaking the file into smaller, separately
   compressed pieces would hurt compression.

                                  [e6.png]

                                   enwik6

   this shows the first 10^6 bytes of data, essentially zooming into the
   first 1% of enwik8 or first 0.1% of enwik9. the clear, vertical bands
   show regions consisting primarily of xml to encode #redirects. these
   have a regular, repeating structure of length 300-400 bytes. the white
   vertical bands extending upwards show that there are no long distance
   matches backwards (they occur closer). the upward curving white bands
   show the absence of long distance forward matches.

   these images were generated with the fv program. download: [13]fv.cpp
   (7kb, gpl, c++), [14]fv.exe (25kb, 32-bit windows). the program uses
   512 mb memory. it took about 14 minutes to generate the image for
   enwik9 on a 2.2 ghz athlon-64 under winxp home. the program outputs in
   .bmp format. you will need another program to convert to .jpg, .png,
   etc.

   for comparison, the calgary corpus (concatenated) is shown below. the
   image was hand edited to add the labels. the repetitive structure of
   book1 (70-80), geo (4), obj2 (4), and pic (216) is clearly visible. the
   image also shows that there is redundancy between text files but not
   the binary files. ([15]more fv results by leonardo maffi).

                                [calgary.gif]

                 calgary corpus (image edited to add labels)

  lexical analysis

   the table below shows the frequency distribution for the 100 most
   common words in enwik9, as well as selected ranks of lower frequency.
   the file was parsed by considering all sequences of english letters
   (a-z, a-z) as single words, and all other single characters as words.
   the most common token is the space, which occurs 139 million times. ^j
   is the linefeed character. upper and lower case are considered
   distinct.
   freq    rank
139132610     1
20216224      2 ]
20214205      3 [
13147025      4 ^j
9578832       5 .
8824782       6 '
7978922       7 ,
6498257       8 the
6199271       9 ;
6037325      10 1
5754062      11 0
5277489      12 /
5274001      14 < >
5084982      15 &
4907106      16 |
4848627      17 of
4721386      18 2
4252985      19 =
3542288      20 -
3536470      21 :
3239688      22 9
3050674      23 and
2736971      24 3
2596182      25 5
2579245      26 4
2523537      27 )
2521806      28 (
2490470      29 *
2443325      30 8
2372995      31 6
2288680      32 in
2130130      33 a
2122603      34 to
2005136      35 7
1875367      36 quot
1630357      37 is
1420384      38 id
1304048      39 the
1152218      40 lt
1151080      41 gt
 970827      42 %
 968882      43 s
 919535      44 amp
 901320      45 }
 901242      46 {
 861221      47 for
 852516      48 are
 815600      49 was
 741327      50 as
 715225      51 by
 707706      52 with
 638091      53 from
 606095      54 that
 579119      55 on
 541826      56 title
 532311      57 or
 530859      58 #
 522024      59 page
 513474      60 text
 494564      61 _
 489280      62 revision
 487656      63 contributor
 486934      64 timestamp
 486902      65 "
 442286      66
 426510      67 sup
 414409      68 at
 412849      69 http
 410993      70 username
 410788      71 s
 408867      72 it
 407150      73 category
 382932      74 comment
 382925      75 an
 378934      76 u
 373978      77 his
 361588      78 have
 356881      79 which
 348190      80 be
 344554      81 in
 328703      82 www
 303270      83
 301709      84 census
 281944      85 he
 276642      86 t
 276098      87 age
 273405      88 also
 267662      89 space
 259584      90 has
 257619      91 population
 253569      92 z
 250192      93 td
 249502      94 american
 245321      95 preserve
 244574      96 xml
 244207      97 not
 243328      98 were
 236835      99 a
 226459     100 who
...
  92138     200 john
...
  34136     500 president
...
  15918    1000 instead
...
   7394    2000 album
...
   2675    5000 episode entrance perspective
...
   1088    9996 ahmed basil chang jakob papua demanding
   1087   10003 conservatives hogan vulcan fingers hospitals nautical ...
...
    422   19968 atl berks billings clovis demons faa foundations fujian g...
    421   20001 armageddon bella champaign cleese comt counsel davidlevin...
...
    108   49714 auc abbe accelerated acupuncture adria agentsoo airport a...
    107   50021 abn alu acacius ach aemilianus agony alfalfa ardea atoka a...
...
     35   99959 aaw abr aibo aicpa aiesec aks autf abaddon abomination ad...
     34  101715 afsc arrl axn aare aaronson abai abeokuta abode absolutis...
...
     10  212858 aae aaeecc aag aams aaww aabout aapre aapro abako abz acc...
      9  227146 aaaaa aacr aaffaa aapt aav aas abcdefghijklmnopqrstuvwxyz...
      8  244837 aaba aaco aada aao abcdab abcnews absa acap activity acid...
      7  266715 aafp aaib abstracts abr academy acats acca accp acs addin...
      6  294832 aacs aaup aarticles abcdabd abend aberr abh abvv acbl aci...
      5  332370 aaahh aabb aabba aafss aaja aaron aasi aaz abcdabcdabde a...
      4  389883 aaaaff aaaba aabab aabbff aacm aadt aafla aan aanr aapg a...
      3  481186 aaad aabaa aaca aacca aaj aak aamco aapa aapbl aapl aasb a...
      2  686619 aaaaaaa aaaab aaabb aaabn aaade aaalac aabbb aabc aabebwu...
      1 1418809 aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...

   note that the data deviates from a zipf distribution (rank x frequency
   = constant). the vocabulary is 1,418,809 words. an order 0 model based
   on this distribution would have a compressed size of 400,889,188 bytes.
   in addition, an order 0 character encoding of the dictionary would
   require 7,044,509 bytes for a total of 407,933,697. this does not
   include a small amount of additional space that would be needed to
   encode the word frequencies, lengths of the dictionary and compressed
   data. for comparison, some other order 0 compressed sizes are given
   below for various parsing methods compared to this baseline parsing
   method.
table (corrected sep 2 2006) order 0 compressed size of enwik9 using various par
sing methds.
text, dict, and total are compressed sizes in bytes for an ideal coder with
an order 0 model.  vocabulary is the number of words in the dictionary.

  text       dict     total   vocabulary method
---------  -------  ---------   -------  ------
400889188  7044509  407933697   1418809  words (baseline, described above)

644561309      198  644561507       206  1-gram
565716295    40632  565756927     21377  2-gram
503362325   987242  504349567    368046  3-gram
451610359  6136119  457746478   1851158  4-gram
408622694 20819133  429441827   5351189  5-gram

407112108  3186214  410298322    686619  unique words are spelled
404716717  3186918  407903634    687011  unique words spelled with 2-grams
403950277  3266186  407216463    719061  unique words spelled with 3-grams
403378168  3725601  407103770    881331  unique words spelled with 4-grams

409885058  2171224  412056282    417010  words occurring once or twice are spell
ed
406488898  2172095  408660993    481672  words occurring once or twice are spell
ed with 2-grams

370304509  7044513  377349022   1418810  space modeling

414760783  6423232  421184015   1286437  id30

423866419  5003920  428870340   1094898  words with capital encoding
406286891  6235043  412521935   1235411  capital/lower encoding for less common
type

   in the 1-gram through 5-gram models, the text is divided into uniform
   blocks of 1 to 5 bytes. the 5-gram model compresses almost as well as
   the word model but the dictionary is almost 4 times as large.

   slight compression occurs when words that occur only once are spelled
   with 2, 3, or 4-grams rather than added to the dictionary. spelling
   such words with letters results in a slight expansion.

   spelling words that occur once or twice does not compress as well as
   spelling words that occur only once. (that might not be true for higher
   order models).

   in space modeling, words are assumed to be preceded by a space in
   certain contexts, and the space is removed from the encoded text. if a
   space is predicted but none occurs, then a special symbol is added to
   encode this fact. this results in an increase of 1 to the vocabulary
   size (compared to baseline, no words are spelled). a 7.5% improvement
   in compression over baseline was obtained by assuming that a space
   occurs before the word after any upper or lower case letter, or the
   characters "." (period), "," (comma), "]" (closing bracket), "}"
   (closing brace), or ")" (closing parenthesis). there are 2,196,539
   no-space symbols in enwik9, making it the 33rd most frequent.

   id30 consists of replacing an inflected form of a word with its
   base form (stem) plus a symbol indicating the suffix. this was
   accomplished with a low error rate by trying a set of id30 rules on
   each incoming word and testing if the resulting stem occurs frequently
   enough in the baseline dictionary (collected in an earlier pass). each
   word is tested to see whether it ends in one of the suffixes in the
   id30 table, and if so, the suffix is replaced and the frequency of
   the resulting word is tested from the baseline dictionary. if the stem
   occurs at least 1/16 as often and is at least 3 characters long, then
   the word is coded as a stem plus a suffix code. if more than one rule
   could apply, then the rule that produces the highest frequency stem is
   used.

   the id30 table is below, sorted by "freq", the number of times each
   rule was applied in enwik9.
    suffix  replacement  freq
    ------  --------  -------
     "s"     ""       7776340
     "ed"    ""       1740722
     "ed"    "e"      1396317
     "ing"   ""       1128007
     "er"    ""        928728
     "ly"    ""        925579
     "ing"   "e"       800321
     "ies"   "y"       557109
     "ion"   ""        371964
     "ion"   "e"       331435
     "er"    "e"       302743
     "ers"   ""        144059
     "ation" ""        121642
     "ence"  "ent"     102084
     "ation" "e"        97988
     "est"   ""         92200
     "ly"    "le"       69732
     "ers"   "e"        56579
     "est"   "e"        36774
     "sses"  "ss"       34569
     "ier"   "y"        32638
     "nning" "n"        23684
     "mming" "m"        19079

   for example, the words "rotates", "rotated", "rotation", and "rotating"
   would all stem to "rotate". one problem is that the baseline dictionary
   is case sensitive, so that "rotates" is stemmed only if "rotate"
   occurs. there are occasional errors such as id30 "coming" to "com"
   + "ing" and "refer" to "ref" + "er". this happens because "com" is more
   common then "come" (in links) and "ref" occurs as an xhtml tag to
   encode references. the minimum stem length of 3 prevents "as" from
   being stemmed as the plural of "a" and similar errors.

   id30 makes order 0 compression worse due to the additional suffix
   tokens, but it reduces the dictionary size and might help in a model
   that uses syntactic or semantic modeling.

   the simplest form of capital encoding is to replace each upper case
   letter with a special symbol followed by the lower case equivalent.
   this hurts compression over baseline but reduces the dictionary size
   and helps in some higher order models. enwik9 contains 41,507,612,
   making this the second most common symbol after space.

   some words such as proper nouns are always capitalized, so it is
   wasteful to use capital encoding. an improvement is to build a baseline
   dictionary and test whether the version with the first letter
   capitalized or lower case is more frequent (e.g. "pat" or "pat"), and
   store that version in the dictionary. the less common form is encoded
   by preceding it with a special symbol to indicate the case of the first
   letter should be changed. this results in a larger dictionary but
   better compression than simple capital encoding. there are 15,065,442
   change-case symbols in enwik9, making it the fourth most common symbol
   after space, [, and ].

                relationship of wikipedia text to clean text

   (june 11, 2006) abstract: the id178 of "clean" written english, in a
   27 character alphabet containing only the letters a-z and
   nonconsecutive spaces, has been estimated to be between 0.6 and 1.3
   bits per character [3,8]. we find that most of the best compressors
   will compress wikipedia text (enwik9, 1 gb) and equivalent cleaned text
   (fil9, 715 mb) to about the same ratio, usually within 3% of each
   other. low end compressors will compress clean text about 5% smaller.
   furthermore, a quick test on 100 mb of cleaned text (text8) will
   predict a compression ratio that is about 2% to 4% below the true ratio
   on fil9 for most compressors.

  introduction

   most data compression benchmarks, including the large text benchmark
   (enwik9), use data sets with unknown algorithmic complexity. for most
   benchmarks, this is not important because their purpose is to compare
   data compression algorithms to one another. however, this benchmark has
   the goal of encouraging research in natural language models, so we also
   wish to compare algorithms to human models. shannon [3] and cover and
   king [8] estimated the id178 of written english to be between 0.6 and
   1.3 bits per character, based on the ability of humans to predict
   consecutive characters from a 27 character alphabet containing only the
   monocase letters a-z and nonconsecutive spaces. however, enwik9 is not
   in this form; it contains capitalization, punctuation, foreign text,
   tables, markup, formatting, hypertext links, and xml structure such as
   timestamps, authorship, and comments. in this paper we estimate the
   effects of these artifacts on compression ratio for 25 programs.

  experimental procedure

   we filter the 1 gb test file enwik9 to produce a 715 mb file fil9, and
   compress this with 17 compressors. furthermore, we produce the file
   text8 by truncating fil9 to 100 mb, and test this on 25 compressors,
   including the 17 tested on fil9. the purpose of the smaller file is to
   allow quicker testing, and to establish the predictive value of this
   quick test on the larger data set.

   the clean version of the wikipedia was prepared with the goal of
   retaining only text that normally would be visible when displayed on a
   wikipedia web page and read by a human. only regular article text was
   retained. image captions were retained, but tables and links to foreign
   language versions were removed. citations, footnotes, and markup were
   removed. hypertext links were converted to ordinary text, retaining
   only the (visible) anchor text. numbers were spelled out ("20" becomes
   "two zero", a common practice in speech research). upper case letters
   were converted to lower case. finally, all sequences of characters not
   in the range a-z were converted to a single space. the effect of this
   filtering on enwik8 is to reduce the text to about 70% of its original
   size before spelling digits, then expand it to about 74%. the detailed
   effect of each step is shown in the table below for enwik8 (which would
   result in the first 74 mb of fil9 or text8. the effects of individual
   steps was not tested on enwik9, but the final result is a little
   smaller (71.5%)
  enwik8         step
-----------   ------------
100,000,000   original size
 96,829,911   discard all outside <text...> ... </text>
 96,604,864   discard #redirect text
 96,210,439   discard xml tags (<text...> and </text>)
 95,287,203   url-decode &lt; &gt; and &amp; to < > and &
 95,087,290   remove <ref> ... </ref> (citations)
 93,645,338   remove other xhtml tags
 91,399,021   replace [http:... anchor text] with [anchor text]
 90,868,662   replace [[image:...|thumb|left/right|nnnpx|caption]] with caption
 90,770,617   replace [[category:text|title]] with [[text]]
 88,385,654   remove  [[language:link]]  (links to same page in other languages)
 85,443,983   replace [[wiki link|anchor text]] with [[anchor text]]
 83,420,173   remove  {{...}}  (icons and special symbols)
 80,943,967   remove  { ... }  (tables)
 77,732,609   remove  [ and ]
 75,053,443   replace &...; with space (url-encoded chars)
 70,007,945   convert to lower case, replace all sequences not in a-z,0-9 with a
 single space
 74,090,640   spell digits, leaving a-z and unrepeated spaces

   the conversion was done by the perl program given in [16]appendix a.
   the following example shows what the previous example looks like after
   conversion (although in reality there are no line breaks).
 anarchism originated as a term of abuse first used against early working class
radicals including the diggers of the english revolution and the sans culottes
of the french revolution whilst the term is still used in a pejorative way to
describe any act that used violent means to destroy the organization of society
it has also been taken up as a positive label by self defined anarchists the wor
d
anarchism is derived from the greek without archons ruler chief king anarchism a
s
a political philosophy is the belief that rulers are unnecessary and should b

   the two files have the following sizes and checksums. text8 is the
   first 10^8 bytes of fil9.
file      size                md5 checksum            download
-----  -----------  --------------------------------  --------
fil9   713,069,767  2754e1cfcc34288745cd23272d976384  (use wikifil.pl to generat
e from enwik9)
text8  100,000,000  3bea1919949baf155f99411df5fada7e  [17]text8.zip, 31,344,016
bytes (or truncate fil9)

  experimental results

   compressed sizes of text8 and fil9 are given in the table below. for
   each compressor, the options are selected as in the main table (as of
   june 10, 2006), which were tuned for maximum compression on enwik9.
   (note this may bias the results toward raw text). the column t8/e8 is
   the ratio of the compressed size of text8 to the compressed size of
   enwik8. it shows that the clean text usually compresses smaller by a
   few percent. the enwik8 results are from the main table, as is the
   algorithm and memory used (in mb). decompression was not verified.
   speed was not measured.

   the fil9 results are shown as both the compressed size and compression
   ratio, since the uncompressed size is odd. the column f9/e9 is the
   compression ratio of fil9 divided by the compression ratio for enwik9,
   not including the size of the decompressor in either case. (the
   decompressor size has a very small effect). the compression ratio for
   enwik9 is from the main table. the error, t8/e8 - f9/e9 is the amount
   by which a test on text8 would underestimate the compressed size of
   fil9, usually about 2% to 4%.
program, version, options       text8       enwik8    t8/e8  alg  mem  fil9 size
    ratio   f9/e9   error  [18]note
----------------------------  ----------  ----------  -----  ---  ---  ---------
---------  ------  ------  ----
paq8h -7                      17,461,782  17,674,700  .9879  cm   854  108,316,0
91  .1519  1.0320  -.0441
durilca 05 -m800 -o12 -t2(3)  17,712,518  18,520,589  .9563  ppm  700
xwrt|ppmonstr -f800 -m800 -o8 18,059,570  19,043,178  .9483  cm   800  108,740,9
92  .1525   .9854  -.0371
ppmonstr j -m800 -o16         18,649,962  19,230,657  .9698  ppm  800  116,872,1
40  .1639  1.0149  -.0451
slim23d -m700 -o10            18,421,266  19,264,094  .9562  ppm  700  115,433,7
36  .1619   .9960  -.0398
ash 04a /m700 /o10            18,680,324  19,963,105  .9357  cm   700  121,001,8
17  .1697   .9389  -.0032
winuda 2.91 mode 3            19,282,080  20,332,366  .9488  cm   194
uhbc 1.0 -m3 -b800            19,724,021  20,930,838  .9424  bwt  800
hipp 5819 /o8                 20,026,417  20,555,951  .9743  cm   719
ppmd j -m256 -o10 -r1         20,029,751  21,388,296  .9365  ppm  256  127,644,7
85  .1790   .9731  -.0366
enc 0.15 aq                   20,361,492  22,156,982  .9190  cm    50  132,364,1
11  .1856   .9490  -.0283
m03exp 2005-02-15 (32 mb)     20,495,661  21,948,192  .9338  bwt   32
                           14
ocamyd ltcb 1.0 -s0 -m3       20,683,435  21,285,121  .9696  dmc  300
                            6
sbc 0.970r2 -ad -m3 -b63      20,723,754  22,470,539  .9222  bwt  224  133,110,7
39  .1867   .9473  -.0251
bssc 0.95a -b16383            21,395,109  23,117,061  .8948  bwt  140
ocamyd 1.65f -s0 -m8          21,419,608  21,456,536  .9983  dmc  800
grzipii 0.2.4 -b8m            22,019,644  23,846,878  .9233  bwt   58  141,150,5
32  .1979   .9471  -.0238
uharc 0.6b -mx -md32768       22,841,858  23,911,123  .9552  ppm   50  147,933,0
09  .2075   .9977  -.0425
px v1.0                       23,846,604  24,971,871  .9549  cm    66
cabarc 1.00.0601 -m lzx:21    25,662,446  28,465,607  .9015  lz77  20  165,676,7
61  .2323   .9266  -.0251
bzip2 1.0.2 -9                26,395,400  29,008,736  .9099  bwt    8  169,311,6
54  .2374   .9349  -.0250
kzip 5/13/06  /b1024          31,344,016  35,016,649  .8951  lz77 121
gzip 1.3.5 -9                 33,048,240  36,445,248  .9068  lz77   1  213,697,6
35  .2997   .9290  -.0222
pkzip 2.0.4                   33,319,889  36,934,712  .9021  lz77   1  215,527,7
00  .3023   .9226  -.0245
lzop v1.01 -9                 38,806,161  41,217,688  .9415  lz77   1  251,384,8
28  .3525   .9623  -.0408
compress 4.3d                 39,179,237  45,763,941  .8561  lzw    1  259,977,2
97  .3645   .8587  -.0026
fpaq0                         51,551,380  63,391,013  .8132  o0     1  366,426,4
23  .5139   .8011  +.0121
uncompressed                 100,000,000 100,000,000 1.0000            713,069,7
67 1.0000  1.0000   .0000

   the results show that high end compressors have about the same
   compression ratio on clean text as on raw text, while faster
   compressors will compress clean text about 5% smaller. using text8 as a
   fast test to predict the effect of cleaning enwik9 tends to
   underestimate the compressed size of clean text by about 2% to 3% on
   faster compressors and by about 4% on high end compressors.

  related work

   alexandru mosoi has produced some preprocessing tools to improve
   compression of fil9, discussed [19]here, and has produced the graph
   below which shows the vocabulary size vs. word token count of fil9. the
   graph is consistent with a zipf distribution: the n'th most frequent
   word has frequency proportional to 1/n [20].

                              [mosoi-words.jpg]

   cumulative dictionary size vs. word count of fil9 (by alexandru mosoi).

                            [mosoi-new-words.jpg]

    new words added to dictionary (per 2^17 word block) vs. word count of
       fil9. this is essentially the derivative of the above graph (by
                              alexandru mosoi).

  references

   3. shannon, cluade e.,    prediction and id178 of printed english   ,
   bell sys. tech. j (3) p. 50-64, 1950.

   8. cover, t. m., and r. c. king,    a convergent gambling estimate of the
   id178 of english   , ieee transactions on id205 (24)4
   (july) pp. 413-421, 1978.

   20. zipf, george kingley, the psycho-biology of language, an
   introduction to dynamic philology, m.i.t. press, 1935.

  appendix a

   this perl program filters wikipedia text dumps to produce 27 character
   text (lowercase letters and spaces) as described in this article. to
   use:
  perl wikifil.pl enwik9 > text

   then truncate the text to the desired length (e.g. 10^8 bytes).

   you can cut and paste the program below. (note it contains url encoding
   to display properly).

#!/usr/bin/perl

# program to filter wikipedia xml dumps to "clean" text consisting only of lower
case
# letters (a-z, converted from a-z), and spaces (never consecutive).
# all other characters are converted to spaces.  only text which normally appear
s
# in the web browser is displayed.  tables are removed.  image captions are
# preserved.  links are converted to normal text.  digits are spelled out.

# written by matt mahoney, june 10, 2006.  this program is released to the publi
c domain.

$/=">";                     # input record separator
while (<>) {
  if (/<text /) {$text=1;}  # remove all but between <text> ... </text>
  if (/#redirect/i) {$text=0;}  # remove #redirect
  if ($text) {

    # remove any text not normally visible
    if (/<\/text>/) {$text=0;}
    s/<.*>//;               # remove xml tags
    s/&amp;/&/g;            # decode url encoded chars
    s/&lt;/</g;
    s/&gt;/>/g;
    s/<ref[^<]*<\/ref>//g;  # remove references <ref...> ... </ref>
    s/<[^>]*>//g;           # remove xhtml tags
    s/\[http:[^] ]*/[/g;    # remove normal url, preserve visible text
    s/\|thumb//ig;          # remove images links, preserve caption
    s/\|left//ig;
    s/\|right//ig;
    s/\|\d+px//ig;
    s/\[\[image:[^\[\]]*\|//ig;
    s/\[\[category:([^|\]]*)[^]]*\]\]/[[$1]]/ig;  # show categories without mark
up
    s/\[\[[a-z\-]*:[^\]]*\]\]//g;  # remove links to other languages
    s/\[\[[^\|\]]*\|/[[/g;  # remove wiki url, preserve visible text
    s/{{[^}]*}}//g;         # remove {{icons}} and {tables}
    s/{[^}]*}//g;
    s/\[//g;                # remove [ and ]
    s/\]//g;
    s/&[^;]*;/ /g;          # remove url encoded chars

    # convert to lowercase letters and spaces, spell digits
    $_=" $_ ";
    tr/a-z/a-z/;
    s/0/ zero /g;
    s/1/ one /g;
    s/2/ two /g;
    s/3/ three /g;
    s/4/ four /g;
    s/5/ five /g;
    s/6/ six /g;
    s/7/ seven /g;
    s/8/ eight /g;
    s/9/ nine /g;
    tr/a-z/ /cs;
    chop;
    print $_;
  }
}

references

   1. http://mattmahoney.net/dc/text.html#history
   2. http://mattmahoney.net/dc/text.html
   3. http://download.wikipedia.org/enwiki/20060303/enwiki-20060303-pages-articles.xml.bz2
   4. http://prize.hutter1.net/
   5. http://mattmahoney.net/dc/ppmd.exe
   6. http://mattmahoney.net/dc/enwik8.pmd
   7. http://mattmahoney.net/dc/enwik9.pmd
   8. http://mattmahoney.net/dc/enwik8.zip
   9. http://mattmahoney.net/dc/enwik9.zip
  10. http://en.wikipedia.org/wiki/utf-8
  11. http://en.wikipedia.org/wiki/xml
  12. http://en.wikipedia.org/wiki/xhtml
  13. http://mattmahoney.net/dc/fv.cpp
  14. http://mattmahoney.net/dc/fv.exe
  15. http://www.fantascienza.net/leonardo/ar/string_repetition_statistics/string_repetition_statistics.html
  16. http://mattmahoney.net/dc/textdata.html#appendixa
  17. http://mattmahoney.net/dc/text8.zip
  18. http://mattmahoney.net/dc/text.html#notes
  19. http://groups.google.com/group/comp.speech.research/browse_frm/thread/3e5c29aeccdc5147/86a55cb2e433c08c?hl=en#86a55cb2e433c08c
