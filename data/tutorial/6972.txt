   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

deep id23 demystified (episode 0)

   [9]go to the profile of moustafa alzantot
   [10]moustafa alzantot (button) blockedunblock (button) followfollowing
   apr 26, 2017

   i recently became interested in learning more about deep reinforcement
   learning. recently, big news headlines were made as deep reinforcement
   learning was used to[11] build a compter program that mastered
   different atari games, and the [12]alphago program could beat the a top
   human of the go game. my main resources of learning are the [13]uc
   berkeley deep id23 course, and[14] yantex practical
   rl course lectures as well as their recommended reading materials.
   [1*kphmuxv2jvedcrdotteqbg.jpeg]

   over the past century, computer scientists have pursued the goal of
   building intelligent machines that can percieve the surrounding world
   and make rationale and smart decisions just like humans do. in the last
   decade, the progress rate is acccelereted by virtue of the huge
   investment and attention to ai by both academic researchers and
   industry giants companies. this trend is expected to continue and grow
   even further in the future. intelligent machines will penetrate more
   aspects of our daily live during the upcoming years. for example,
   [15]it is estimated that by 2020 there will be 10million self-driving
   cars on the roads.

   the goal of this series of articles, is to provide an introduction
   about    deep id23    which is an important approach to
   building intelligent machines. i will cover the basics of reinforcement
   learning from the ground up as well as providing practical examples to
   illustrate how to implement various rl algorithms. no prior knowledge
   about id23 or deep learning are required to follow
   these articles, but any previous experience about machine learning will
   be useful. i will provide code examples in python and will be using
   [16]openai gym for evaluting our algorithms.

(episode 0)

   this first article (episode 0) aims to describe what reinforcement
   learning is and provide examples for where it can be used. i also cover
   the essential terminologies for rl and provide a quick tutorial about
   openai gym as i am to use it in the programming exercises.

   what is id23 ?

   id23 is a category of machine learning and it is best
   understood as if we have an agent that interacts with an environment
   such that it can observe the environment state and perform actions.
   upon doing actions, the environment state changes into a new state and
   the agent recieves a reward (or penalty). id23 aims
   at making this agent learn from his experience of interactions with
   environment so that it chooses the best actions that maximizes the sum
   of rewards it receives from the environment.
   [1*z2ymvuq1-t5ol1ac_w4doq.png]
   source: [17]nervana deep id23 with openai gym.

   id23 is not something entirely new, in fact most of
   methods in id23 were invented decades ago. however,
   it recently gained a lot of interest when it was combined with deep
   learning (thus deep id23 was born !) and making
   benefit of the large scale computation power to achieve successes such
   as playing atari games from raw screen pixels and mastering the go
   game.

   here are an example for a typical id23 problems:

   playing chess: in this problem, the agent is a computer program that
   plays chess, and the environment is the chess board status (i.e. pieces
   on board and their locations). the agent (computer player) observes the
   board state and makes a move. as a result of its moves, the board state
   changes and eventually the game will end and the agent will either win
   (recieve a reward) or lose (recieve a penalty). id23
   here can be used by an agent to improve its game playing skill. such
   that the computer program that initially plays poorly after playing a
   lot of chess rounds, it will become able to plan well and choose the
   moves that lead to winning the game.

   notice that in rl, the agent initially does not know what actions lead
   to win / lose but it has to do ( exploration) by trying random actions
   and then it will memorize the effect of actions it made. exploration
   helps the agent to learn more about the world it is interacting with.
   after enough exploration has been made, the agent may choose one of a
   good action it has explored before; this is know as exploitation. in
   id23, there is always a tradeoff between whether the
   agent should re-use one of its good actions or try another new action
   (hoping it will lead to better result). this trade-off is known as
   exploration-exploitation dilemma.

   id100

   id23 problems are mathematically described using a
   framework called id100 (mdps). mdps are extended
   version of [18]markov chain which adds decisions and rewards elements
   to it. the word markov here refers to that [19]markovian property which
   means that future state is independent from any previous states history
   given the current state and action. this means that current state
   encapsulates all what is needed to decide the future state when an
   input action is recieved. this is a reasonable assumption in many
   problems and it simplifies things a lot. for example, in chase game,
   the chess board configuration after a move is being made can be decided
   based on the current board configuration and the action being made now
   and we don   t need to worry about previous chess board configurations or
   past actions.

   formally, mdp is defined as a tuple of 5 items <s, a, p, r,      > which
   are:
     * s: set of observations. the agent observes the environment state as
       one item of this set.
     * a: set of actions. the set of actions the agent can choose one from
       to interact with the environment.
     * p: p(s' | s, a) transition id203 matrix. this models what
       next state s' will be after the agent makes action a while being in
       the current state s.
     * r: p(r | s, a) reward model that models what reward the agent will
       recieve when it performs action a when it is in state s.
     *     : discount factor. this factor is a numerical value between 0 and
       1 that represents the relative importance between immediate and
       future rewards. i.e, if the agent has to select between two actions
       one of them will give it a high immediate reward immediately after
       performing the action but will lead into going to state from which
       the agents expects to get less future rewards than another state
       that can be reached after doing an action with less immediate
       reward ?
     __________________________________________________________________

   the goal of rl agent, is to solve the mdp by finding optimal policy
   which means finding the sequence of action it can make to maximize the
   total recieved reward.

openai gym

   openai gym is an open-source python framework developed by [20]openai,
   the non-profit ai research company, as a toolkit for developing and
   evaluating id23 algorithms. it gives us a set of test
   problems, known as environments, that we can write rl algorithms to
   solve. this lets us able to dedicate more of our time to impelment and
   improve the learning algorithm rather than spending a lot of time
   simulating the environment. in addition, it provides a medium for
   people to compare and review the algorithms of others. the l[21]ist of
   environments available in openai gym can found here. sounds cool
   right ?

installing and running openai gym

   for more detailed description about how to use and run openai gym i
   suggest reading the[22] official documentation page.

   a minimal install of the gym can be done by:
git clone [23]https://github.com/openai/gym
cd gym
pip install -e . # minimal install

   now, after the gym has been installed you can instantiate and run an
   environment:

   iframe: [24]/media/a9c42b13382d6a1957bf9d9fe992b51a?postid=2198c05a6124

   this above code snippet will first import the gym library. then it
   creates an instance of the [25]cartpole environment which is a
   classical problem in rl. in the cart-pole environment, it simulates an
   inverted pendulum mounted upwards on cart, the pendulum is intially
   vertical and your goal is to maintain it vertically balanced. the only
   way to control the pendulum is by choosing a horizontal direction for
   the cart to move (either to left or right).

   the code above, runs the environment for 500 time steps, and it choses
   a random action to perform at each time. as a result, you see in the
   video below that the pole is not kept stable for long. the reward is
   measured by the number of steps before the pole becomes more than 15
   degrees away from the vertical position. the longer you remain within
   this range, the bigger your total reward is.

   iframe: [26]/media/db2a6e053db9faa9aca97f59fe840163?postid=2198c05a6124

   openai gym documentation

solving cart-pole by policy search

   we will present a simple solution to solve the cart-pole problem based
   on random search. the idea is the following, assume your agent has a
   function that can be used to determine what action to perform (i.e.
   move the cart left or right ) based on the observation it receives from
   the environment. in id23 world, the function that
   maps observations to actions is known as the policy function. the goal
   of the agent learning process is to select the optimal policy which
   produces the maximum possible total reward.

   to find such a policy, we first choose a represntation of our policy.
   in this problem the observation is a vector of 4 numbers and the action
   space consits of only two possible actions either 0 or 1. so we choose
   the policy as a parameters of a hyper-plane in 4 dimensional space such
   that points to left of the hyper-plane will result in action 0while
   points to its right will result in action 1. in other words, each
   policy is represented by 5 numbers w1, w2, w3, w4, b such that the
   policy decision to select next action is made according to:
if w1 * x1 + w2 * x2 + w3 * x3 + w4 * x4  + b > 0:
   cart moves to right
else
   cart moves to left

   now, how to select a good values for the policy parameters ( w1, w2,
   w3, w4, b ) ? it turns out we can find a good value for these
   parameters just by random search. this method is not efficient when the
   search space for parameters becomes big. however, in this simple
   problem, we can find a good solution, if we start from random set of
   policy parameters values and evaluate each one of them and select the
   best one we have found.

   the python program below shows an implementaion for this policy search
   approach.

   get_random_policy(): a function that returns a random set of policy
   parameters.

   run_episode(..,policy, t_max,..): a function that starts an episode of
   maximum length t_max timesteps and returns the total reward that an
   agent following the given policy has recieved.

   policy_to_action(..,policy, obs) : a function that selects an action
   for the agent who follows the given policy will chose when it observes
   the state observation given by obs

   iframe: [27]/media/3e448a8f327d6a146db2e0a7841f278f?postid=2198c05a6124

   here is a video output for the agent performance after it has found the
   policy using the algorithm implemented above.

   iframe: [28]/media/a11ca9aca886da0124524bb620578d68?postid=2198c05a6124

   solution of openai gym cartpolve-v0 problem using policy search.

   next time, we will improve over this naive search solution to have a
   more intelligent solutions that can be used for solving more complex
   problems.

references

   1- john schulmann, [29]deep reinforcmenet learning tutorial

   [30]2- http://rll.berkeley.edu/deeprlcourse/

   3- [31]https://github.com/yandexdataschool/practical_rl

   4-[32] simple id23 to learn cartpole

     * [33]machine learning
     * [34]artificial intelligence
     * [35]id23
     * [36]openai
     * [37]deep learning

   (button)
   (button)
   (button) 769 claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [38]go to the profile of moustafa alzantot

[39]moustafa alzantot

   computer science phd student at ucla.
   ([40]http://web.cs.ucla.edu/~malzantot/)

     * (button)
       (button) 769
     * (button)
     *
     *

   [41]go to the profile of moustafa alzantot
   never miss a story from moustafa alzantot, when you sign up for medium.
   [42]learn more
   never miss a story from moustafa alzantot
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2198c05a6124
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@m.alzantot?source=post_header_lockup
  10. https://medium.com/@m.alzantot
  11. https://deepmind.com/research/id25/
  12. https://deepmind.com/research/alphago/
  13. http://rll.berkeley.edu/deeprlcourse/
  14. https://github.com/yandexdataschool/practical_rl
  15. http://www.businessinsider.com/report-10-million-self-driving-cars-will-be-on-the-road-by-2020-2015-5-6
  16. https://gym.openai.com/
  17. https://www.nervanasys.com/openai/
  18. https://en.wikipedia.org/wiki/markov_chain
  19. https://en.wikipedia.org/wiki/markov_property
  20. https://openai.com/
  21. https://gym.openai.com/envs
  22. https://gym.openai.com/docs
  23. https://github.com/openai/gym
  24. https://medium.com/media/a9c42b13382d6a1957bf9d9fe992b51a?postid=2198c05a6124
  25. https://gym.openai.com/envs/cartpole-v0
  26. https://medium.com/media/db2a6e053db9faa9aca97f59fe840163?postid=2198c05a6124
  27. https://medium.com/media/3e448a8f327d6a146db2e0a7841f278f?postid=2198c05a6124
  28. https://medium.com/media/a11ca9aca886da0124524bb620578d68?postid=2198c05a6124
  29. http://learning.mpi-sws.org/mlss2016/slides/2016-mlss-rl.pdf
  30. http://rll.berkeley.edu/deeprlcourse/
  31. https://github.com/yandexdataschool/practical_rl
  32. http://kvfrans.com/simple-algoritms-for-solving-cartpole/
  33. https://medium.com/tag/machine-learning?source=post
  34. https://medium.com/tag/artificial-intelligence?source=post
  35. https://medium.com/tag/reinforcement-learning?source=post
  36. https://medium.com/tag/openai?source=post
  37. https://medium.com/tag/deep-learning?source=post
  38. https://medium.com/@m.alzantot?source=footer_card
  39. https://medium.com/@m.alzantot
  40. http://web.cs.ucla.edu/~malzantot/
  41. https://medium.com/@m.alzantot
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://medium.com/p/2198c05a6124/share/twitter
  45. https://medium.com/p/2198c05a6124/share/facebook
  46. https://medium.com/p/2198c05a6124/share/twitter
  47. https://medium.com/p/2198c05a6124/share/facebook
