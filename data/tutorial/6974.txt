   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

deep id23 demysitifed (episode 2)         policy iteration, value
iteration and id24

   [9]go to the profile of moustafa alzantot
   [10]moustafa alzantot (button) blockedunblock (button) followfollowing
   jul 8, 2017

   in previous two articles, we introduced id23
   definition, examples, and simple solving strategies using [11]random
   policy search and [12]id107.

   in practice, random search does not work well for complex problems
   where the search space (that depends on the number of possible states
   and actions) is large. also, genetic algorithm is a meta-heuristic
   optimization so it does not provide a guarantee to find an optimal
   solution. in this article, we are going to introduce fundamental
   id23 algorithms.

   we start by reviewing the markov decision process formulation, then we
   describe the value-iteration and policy iteration which are algorithms
   for finding the optimal policy when the agent knows sufficient details
   about the environment model. we then, describe the id24 is a
   model-free learning that can be used when the agent does not know the
   environment model but has to discover the policy by trial and error
   making use of its history of interaction with the environment. we also
   provide demonstration examples of the three methods by using the
   [13]frozenlake8x8 and [14]mountaincar problems from openai gym.

markov decision process (mdp)

   we briefly introduced markov decision process mdpin our first article.
   to recall, in id23 problems we have an agent
   interacting with an environment. at each time step, the agent performs
   an action which leads to two things: changing the environment state and
   the agent (possibly) receiving a reward (or penalty) from the
   environment. the goal of the agent is to discover an optimal policy
   (i.e. what actions to do in each state) such that it maximizes the
   total value of rewards received from the environment in response to its
   actions. mdpis used to describe the agent/ environment interaction
   settings in a formal way.

   mdp consists of a tuple of 5 elements:
     * s : set of states. at each time step the state of the environment
       is an element s     s.
     * a: set of actions. at each time step the agent choses an action a    
       a to perform.
     * p(s_{t+1} | s_t, a_t) : state transition model that describes how
       the environment state changes when the user performs an action a
       depending on the action aand the current state s.
     * p(r_{t+1} | s_t, a_t) : reward model that describes the real-valued
       reward value that the agent receives from the environment after
       performing an action. in mdp the the reward value depends on the
       current state and the action performed.
     *      : discount factor that controls the importance of future rewards.
       we will describe it in more details later.

   the way by which the agent chooses which action to perform is named the
   agent policy which is a function that takes the current environment
   state to return an action. the policy is often denoted by the symbol     .
   [1*ns-mki7py8peqldcq64xca@2x.png]

   let   s now differentiate between two types environments.

   deterministic environment: deterministic environment means that both
   state transition model and reward model are deterministic functions. if
   the agent while in a given state repeats a given action, it will always
   go the same next state and receive the same reward value.

   stochastic environment: in a stochastic environment there is
   uncertainty about the actions effect. when the agent repeats doing the
   same action in a given state, the new state and received reward may not
   be the same each time. for example, a robot which tries to move forward
   but because of the imperfection in the robot operation or other factors
   in the environment (e.g. slippery floor), sometimes the action forward
   will make it move forward but in sometimes, it will move to left or
   right.

   deterministic environments are easier to solve, because the agent knows
   how to plan its actions with no-uncertainty given the environment mdp.
   possibly, the environment can be modeled in as a graph where each state
   is a node and edges represent transition actions from one state to
   another and edge weights are received rewards. then, the agent can use
   a graph search algorithm such as a* to find the path with maximum total
   reward form the initial state.

total reward

   remember, that the goal of the agent is to pick the best policy that
   will maximize the total rewards received from the environment.

   assume that environment is initially at state s_0

   at time 0 : agent observes the environment state s_0 and picks an
   action a_0, then upon performing its action, environment state becomes
   s_1 and the agent receives a reward r_1 .

   at time 1: agent observes current state s_1 and picks an action a_1 ,
   then upon acting its action, environment state becomes s_2 and it
   receives a reward r_2 .

   at time 2: agent observes current state s_2 and picks an action a_2 ,
   then upon acting its action, environment state becomes s_3 and it
   receives a reward r_3 .

   so the total reward received by the agent in response to the actions
   selected by its policy is going to be:

   total reward = r_1 + r_2 + r_3 + r_4 + r_5 + ..

   however, it is common to use a discount factor to give higher weight to
   near rewards received near than rewards received further in the future.

   total discounted reward = r_1 +      r_2 +        r_3 +        r_4 +         r_5+    
   so,
   [1*jix2scbmffb1e5mpzciwmg@2x.png]

   where t is the horizon (episode length) which can be infinity if there
   is maximum length for the episode.

   the reason for using discount factor is to prevent the total reward
   from going to infinity (because 0              1), it also models the agent
   behavior when the agent prefers immediate rewards than rewards that are
   potentially received far away in the future. (if i give you 1000
   dollars today and if i give you 1000 days after 10 years which one
   would you prefer ? ). imagine a robot that is trying to solve a maze
   and there are two paths to the goal state one of them is longer but
   gives higher reward while there is a shorter path with smaller reward.
   by adjusting the      value, you can control which the path the agent
   should prefer.

   now, we are ready to introduce the value-iteration and policy-iteration
   algorithms. these are two fundamental methods for solving mdps. both
   value-iteration and policy-iteration assume that the agent knows the
   mdp model of the world (i.e. the agent knows the state-transition and
   reward id203 functions). therefore, they can be used by the agent
   to (offline) plan its actions given knowledge about the environment
   before interacting with it. later, we will discuss id24 which is
   a model-free learning environment that can be used in situation where
   the agent initially knows only that are the possible states and actions
   but doesn't know the state-transition and reward id203 functions.
   in id24 the agent improves its behavior (online) through learning
   from the history of interactions with the environment.

value function

   many id23 introduce the notion of `value-function`
   which often denoted as v(s) . the value function represent how good is
   a state for an agent to be in. it is equal to expected total reward for
   an agent starting from state s. the value function depends on the
   policy by which the agent picks actions to perform. so, if the agent
   uses a given policy      to select actions, the corresponding value
   function is given by:
   [1*v6mr4fgjng1sk-g8_k_wug@2x.png]

   among all possible value-functions, there exist an optimal value
   function that has higher value than other functions for all states.
   [1*mmjt0uadc_grpy61wb1dxw@2x.png]

   the optimal policy     * is the policy that corresponds to optimal value
   function.
   [1*po1aahvf3p8kle5djev1jq@2x.png]

   in addition to the state value-function, for convenience rl algorithms
   introduce another function which is the state-action pair q function. q
   is a function of a state-action pair and returns a real value.
   [1*b4kn7-tb7aj4myyebvphcw@2x.png]

   the optimal q-function q*(s, a) means the expected total reward
   received by an agent starting in sand picks action a, then will behave
   optimally afterwards. there, q*(s, a) is an indication for how good it
   is for an agent to pick action a while being in state s.

   since v*(s) is the maximum expected total reward when starting from
   state s , it will be the maximum of q*(s, a)over all possible actions.

   therefore, the relationship between q*(s, a) and v*(s) is easily
   obtained as:
   [1*mlqa-jcuigrys8h-o0vbka@2x.png]

   and if we know the optimal q-function q*(s, a) , the optimal policy can
   be easily extracted by choosing the action a that gives maximum q*(s,
   a) for state s.
   [1*ydswi5jeq83fwqekk013-a@2x.png]

   now, lets introduce an important equation called the [15]bellman
   equation which is a super-important equation optimization and have
   applications in many fields such as id23, economics
   and control theory. bellman equation using id145 paradigm
   provides a recursive definition for the optimal q-function.
   the q*(s, a) is equal to the summation of immediate reward after
   performing action a while in state s and the discounted expected future
   reward after transition to a next state s'.
   [1*majqd3msgthgpbcbtf1lyg@2x.png]

   value-iteration and policy iteration rely on these equations to compute
   the optimal value-function.

value iteration

   value iteration computes the optimal state value function by
   iteratively improving the estimate of v(s). the algorithm initialize
   v(s) to arbitrary random values. it repeatedly updates the q(s, a) and
   v(s) values until they converges. value iteration is guaranteed to
   converge to the optimal values. this algorithm is shown in the
   following pseudo-code:
   [1*msd6og8hcredo24t8izfnw.png]
   pseudo code for value-iteration algorithm. credit: alpaydin
   introduction to machine learning, 3rd edition.

example : frozenlake8x8 (using value-iteration)

   now lets implement it in python to solve the [16]frozenlake8x8 openai
   gym. compared to the frozenlake-v0 environment we solved earlier using
   genetic algorithm, the [17]frozenlake8x8 has 64 possible states (grid
   size is 8x8) instead of 16. therefore, the problem becomes harder and
   genetic algorithm will struggle to find the optimal solution.

   iframe: [18]/media/b0f0ac0bfbf315937689bc3c25944d53?postid=978f9e89ddaa

   solution of the frozenlake8x8 environment using value-iteration

   policy iteration

   while value-iteration algorithm keeps improving the value function at
   each iteration until the value-function converges. since the agent only
   cares about the finding the optimal policy, sometimes the optimal
   policy will converge before the value function. therefore, another
   algorithm called policy-iteration instead of repeated improving the
   value-function estimate, it will re-define the policy at each step and
   compute the value according to this new policy until the policy
   converges. policy iteration is also guaranteed to converge to the
   optimal policy and it often takes less iterations to converge than the
   value-iteration algorithm.

   the pseudo code for policy iteration is shown below.
   [1*wwoalxfvddgy0uk92fo6rw.png]
   pseudo code for policy-iteration algorithm. credit: alpaydin
   introduction to machine learning, 3rd edition.

example : frozenlake8x8 (using policy-iteration)

   iframe: [19]/media/2ca8bcdc117cdb6e07013a590b3e78b4?postid=978f9e89ddaa

   solution of the frozenlake8x8 environment using policy iteration

value-iteration vs policy-iteration

   both value-iteration and policy-iteration algorithms can be used for
   offline planning where the agent is assumed to have prior knowledge
   about the effects of its actions on the environment (they assume the
   mdp model is known). comparing to each other, policy-iteration is
   computationally efficient as it often takes considerably fewer number
   of iterations to converge although each iteration is more
   computationally expensive.

id24

   now, lets consider the case where the agent does not know apriori what
   are the effects of its actions on the environment (state transition and
   reward models are not known). the agent only knows what are the set of
   possible states and actions, and can observe the environment current
   state. in this case, the agent has to actively learn through the
   experience of interactions with the environment. there are two
   categories of learning algorithms:

   model-based learning: in model-based learning, the agent will interact
   to the environment and from the history of its interactions, the agent
   will try to approximate the environment state transition and reward
   models. afterwards, given the models it learnt, the agent can use
   value-iteration or policy-iteration to find an optimal policy.

   model-free learning: in model-free learning, the agent will not try to
   learn explicit models of the environment state transition and reward
   functions. however, it directly derives an optimal policy from the
   interactions with the environment.

   id24 is an example of model-free learning algorithm. it does not
   assume that agent knows anything about the state-transition and reward
   models. however, the agent will discover what are the good and bad
   actions by trial and error.

   the basic idea of id24 is to approximate the state-action pairs
   q-function from the samples of q(s, a) that we observe during
   interaction with the enviornment. this approach is known as
   time-difference learning.
   [1*yoeaesssos7tx-8h24yqqq.png]
   id24 algorithm

   where      is the learning rate. the q(s,a)table is initialized randomly.
   then the agent starts to interact with the environment, and upon each
   interaction the agent will observe the reward of its action r(s,a)and
   the state transition (new state s'). the agent compute the observed
   q-value q_obs(s, a) and then use the above equation to update its own
   estimate of q(s,a) .

   exploration vs exploitation

   an important question is how does the agent select actions during
   learning. should the agent trust the learnt values of q(s, a) enough to
   select actions based on it ? or try other actions hoping this may give
   it a better reward. this is known as the exploration vs exploitation
   dilemma.

   a simple approach is known as the     -greedy approach where at each step.
   with small id203     , the agent will pick a random action (explore)
   or with id203 (1-    ) the agent will select an action according to
   the current estimate of q-values.      value can be decreased overtime as
   the agent becomes more confident with its estimate of q-values.

mountaincar problem (using id24)

   [1*riqcuyhupeg9adyv24kphg.gif]
   solution of mountaincar problem using id24

   now, lets demonstrate how id24 can be used to solve an
   interesting problem from openai gym, the [20]mountin-car problem. in
   the mountain car problem, there is a car on 1-dimensional track between
   two mountains. the goal of the car is to climb the mountain on its
   right. however, its engine is not strong to climb the mountain without
   having to go back to gain some momentum by climbing the mountain on the
   left.
   here, the agent is the car, and possible actions are drive left, do
   nothing, or drive right. at every time step, the agent receives a
   penalty of -1 which means that the goal of the agent is to climb the
   right mountain as fast as possible to minimize the sum of -1 penalties
   it receives.

   the observation is two continuous variables representing the velocity
   and position of the car. since, the observation variables are
   continuous, for our algorithm we discretize the observed values in
   order to use id24.
   while initially, the car is unable to climb the mountain and it will
   take forever, if you select a random action. after learning, it learns
   how to climb the mountain within less than 100 time-steps.

   iframe: [21]/media/c0b047ca70a3c59b0d2b66ce83ca2c83?postid=978f9e89ddaa

   solution of openai gym mountaincar problem using id24.

references

     * alpaydin introduction to machine learning, 3rd edition.
     * [22]stanford cs234 id23
     * [23]uc berkley cs188 introduction to ai

     * [24]machine learning
     * [25]id23
     * [26]deep learning
     * [27]artificial intelligence
     * [28]openai

   (button)
   (button)
   (button) 2.3k claps
   (button) (button) (button) 27 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of moustafa alzantot

[30]moustafa alzantot

   computer science phd student at ucla.
   ([31]http://web.cs.ucla.edu/~malzantot/)

     * (button)
       (button) 2.3k
     * (button)
     *
     *

   [32]go to the profile of moustafa alzantot
   never miss a story from moustafa alzantot, when you sign up for medium.
   [33]learn more
   never miss a story from moustafa alzantot
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/978f9e89ddaa
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@m.alzantot?source=post_header_lockup
  10. https://medium.com/@m.alzantot
  11. https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124
  12. https://becominghuman.ai/genetic-algorithm-for-reinforcement-learning-a38a5612c4dc
  13. https://gym.openai.com/envs/frozenlake8x8-v0
  14. https://gym.openai.com/envs/mountaincar-v0
  15. https://en.wikipedia.org/wiki/bellman_equation
  16. https://gym.openai.com/envs/frozenlake8x8-v0
  17. https://gym.openai.com/envs/frozenlake8x8-v0
  18. https://medium.com/media/b0f0ac0bfbf315937689bc3c25944d53?postid=978f9e89ddaa
  19. https://medium.com/media/2ca8bcdc117cdb6e07013a590b3e78b4?postid=978f9e89ddaa
  20. https://gym.openai.com/envs/mountaincar-v0
  21. https://medium.com/media/c0b047ca70a3c59b0d2b66ce83ca2c83?postid=978f9e89ddaa
  22. http://web.stanford.edu/class/cs234/index.html
  23. http://ai.berkeley.edu/course_schedule.html
  24. https://medium.com/tag/machine-learning?source=post
  25. https://medium.com/tag/reinforcement-learning?source=post
  26. https://medium.com/tag/deep-learning?source=post
  27. https://medium.com/tag/artificial-intelligence?source=post
  28. https://medium.com/tag/openai?source=post
  29. https://medium.com/@m.alzantot?source=footer_card
  30. https://medium.com/@m.alzantot
  31. http://web.cs.ucla.edu/~malzantot/
  32. https://medium.com/@m.alzantot
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/978f9e89ddaa/share/twitter
  36. https://medium.com/p/978f9e89ddaa/share/facebook
  37. https://medium.com/p/978f9e89ddaa/share/twitter
  38. https://medium.com/p/978f9e89ddaa/share/facebook
