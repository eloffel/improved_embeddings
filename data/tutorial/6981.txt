   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

how to colorize black & white photos with just 100 lines of neural
network code

   [11]go to the profile of emil wallner
   [12]emil wallner (button) blockedunblock (button) followfollowing
   oct 29, 2017

   earlier this year, amir avni used neural networks to [13]troll the
   subreddit[14]/r/colorization         a community where people colorize
   historical black and white images manually using photoshop.

   they were astonished with amir   s deep learning bot. what could take up
   to a month of manual labour could now be done in just a few seconds.

   i was fascinated by amir   s neural network, so i reproduced it and
   documented the process. first off, let   s look at some of the
   results/failures from my experiments (scroll to the bottom for the
   final result).
   [1*wk43gnwklb4kyjqt5i8ntq.png]
   the original b&w images are from unsplash

   today, colorization is usually done by hand in photoshop. to appreciate
   all the hard work behind this process, take a peek at this gorgeous
   colorization memory lane video:

   iframe: [15]/media/5262de6e215aca6ab020904c0794ad6f?postid=53d9b4449f8d

   in short, a picture can take up to one month to colorize. it requires
   extensive research. a face alone needs up to 20 layers of pink, green
   and blue shades to get it just right.

   this article is for beginners. yet, if you   re new to deep learning
   terminology, you can read my previous two posts [16]here and [17]here,
   and watch andrej karpathy   s [18]lecture for more background.

   i   ll show you how to build your own colorization neural net in three
   steps.

   the first section breaks down the core logic. we   ll build a bare-bones
   40-line neural network as an    alpha    colorization bot. there   s not a
   lot of magic in this code snippet. this well help us become familiar
   with the syntax.

   the next step is to create a neural network that can generalize         our
      beta    version. we   ll be able to color images the bot has not seen
   before.

   for our    final    version, we   ll combine our neural network with a
   classifier. we   ll use an [19]inception resnet v2 that has been trained
   on 1.2 million images. to make the coloring pop, we   ll train our neural
   network on portraits from [20]unsplash.

   if you want to look ahead, here   s a [21]jupyter notebook with the alpha
   version of our bot. you can also check out the three versions on
   [22]floydhub and [23]github, along with code for [24]all the
   experiments i ran on floydhub   s cloud gpus.

core logic

   in this section, i   ll outline how to render an image, the basics of
   digital colors, and the main logic for our neural network.

   black and white images can be represented in grids of pixels. each
   pixel has a value that corresponds to its brightness. the values span
   from 0   255, from black to white.
   [1*qvt2380cuu_h7zcsvr7t0q.png]

   color images consist of three layers: a red layer, a green layer, and a
   blue layer. this might be counter-intuitive to you. imagine splitting a
   green leaf on a white background into the three channels. intuitively,
   you might think that the plant is only present in the green layer.

   but, as you see below, the leaf is present in all three channels. the
   layers not only determine color, but also brightness.
   [1*elvp_ujvom9pxrnpat3boa.png]

   to achieve the color white, for example, you need an equal distribution
   of all colors. by adding an equal amount of red and blue, it makes the
   green brighter. thus, a color image encodes the color and the contrast
   using three layers:
   [1*n8dux1ll5p3y8njv1anoaa.png]

   just like black and white images, each layer in a color image has a
   value from 0   255. the value 0 means that it has no color in this layer.
   if the value is 0 for all color channels, then the image pixel is
   black.

   as you may know, a neural network creates a relationship between an
   input value and output value. to be more precise with our colorization
   task, the network needs to find the traits that link grayscale images
   with colored ones.

   in sum, we are searching for the features that link a grid of grayscale
   values to the three color grids.
   [1*w23sq2oede_psk-hmp4cow.png]
   f() is the neural network, [b&w] is our input, and [r],[g],[b] is
   our output.

alpha version

   we   ll start by making a simple version of our neural network to color
   an image of a woman   s face. this way, you can get familiar with the
   core syntax of our model as we add features to it.

   with just 40 lines of code, we can make the following transition. the
   middle picture is done with our neural network and the picture to the
   right is the original color photo. the network is trained and tested on
   the same image         we   ll get back to this during the beta-version.
   [1*vd4u8q34bz4hafpyjylkpw.png]
   photo by camila cordeiro

color space

   first, we   ll use an algorithm to change the color channels, from rgb to
   lab. l stands for lightness, and a and b for the color spectra
   green   red and blue   yellow.

   as you can see below, a lab encoded image has one layer for grayscale,
   and has packed three color layers into two. this means that we can use
   the original grayscale image in our final prediction. also, we only
   have two channels to predict.
   [1*ox9dwik6bohkwtap4q92pq.png]

   science fact         94% of the cells in our eyes determine brightness. that
   leaves only 6% of our receptors to act as sensors for colors. as you
   can see in the above image, the grayscale image is a lot sharper than
   the color layers. this is another reason to keep the grayscale image in
   our final prediction.

from b&w to color

   our final prediction looks like this. we have a grayscale layer for
   input, and we want to predict two color layers, the ab in lab. to
   create the final color image we   ll include the l/grayscale image we
   used for the input. the result will be creating a lab image.
   [1*pzcnvgeuituywdgemf_r3q.png]

   to turn one layer into two layers, we use convolutional filters. think
   of them as the blue/red filters in 3d glasses. each filter determines
   what we see in a picture. they can highlight or remove something to
   extract information out of the picture. the network can either create a
   new image from a filter or combine several filters into one image.

   for a convolutional neural network, each filter is automatically
   adjusted to help with the intended outcome. we   ll start by stacking
   hundreds of filters and narrow them down into two layers, the a and b
   layers.

   before we get into detail into how it works, let   s run the code.

run the code on floydhub

   click the below button you open a [25]workspace on [26]floydhub where
   you will find the same environment and dataset used for the full
   version. you can also find the trained models for [27]serving.
   [28][1*x3hl-4adknicjwpgcboy4a.png]

   you can also make a local floydhub installation with their [29]2-min
   installation, watch my [30]5-min video tutorial or check out my
   [31]step-to-step guide. it   s the best (and easiest) way to train deep
   learning models on cloud gpus.

alpha version

   once floydhub is installed, use the following commands:
git clone [32]https://github.com/emilwallner/coloring-greyscale-images-in-keras

   open the folder and initiate floydhub.
cd coloring-greyscale-images-in-keras/floydhub
floyd init colornet

   the floydhub web dashboard will open in your browser. you will be
   prompted to create a new floydhub project called colornet. once that's
   done, go back to your terminal and run the same initcommand.
floyd init colornet

   okay, let   s run our job:
floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboa
rd

   some quick notes about our job:
     * we mounted a public dataset on floydhub (which i   ve already
       uploaded) at the datadirectory with the below line:

--dataemilwallner/datasets/colornet/2:data

   you can explore and use this dataset (and many other public datasets)
   by viewing it on [33]floydhub
     * we enabled tensorboard with --tensorboard
     * we ran the job in jupyter notebook mode with --mode jupyter
     * if you have gpu credit, you can also add the gpu flag --gputo your
       command. this will make it approximately 50x faster

   go to the jupyter notebook. under the jobs tab on the floydhub website,
   click on the jupyter notebook link and navigate to this file:
floydhub/alpha version/working_floyd_pink_light_full.ipynb

   open it and click shift+enter on all the cells.

   gradually increase the epoch value to get a feel for how the neural
   network learns.
model.fit(x=x, y=y, batch_size=1, epochs=1)

   start with an epoch value of 1 and the increase it to 10, 100, 500,
   1000 and 3000. the epoch value indicates how many times the neural
   network learns from the image. you will find the image img_result.pngin
   the main folder once you   ve trained your neural network.
# get images
image = img_to_array(load_img('woman.png'))
image = np.array(image, dtype=float)
# import map images into the lab colorspace
x = rgb2lab(1.0/255*image)[:,:,0]
y = rgb2lab(1.0/255*image)[:,:,1:]
y = y / 128
x = x.reshape(1, 400, 400, 1)
y = y.reshape(1, 400, 400, 2)
# building the neural network
model = sequential()
model.add(inputlayer(input_shape=(none, none, 1)))
model.add(conv2d(8, (3, 3), activation='relu', padding='same', strides=2))
model.add(conv2d(8, (3, 3), activation='relu', padding='same'))
model.add(conv2d(16, (3, 3), activation='relu', padding='same'))
model.add(conv2d(16, (3, 3), activation='relu', padding='same', strides=2))
model.add(conv2d(32, (3, 3), activation='relu', padding='same'))
model.add(conv2d(32, (3, 3), activation='relu', padding='same', strides=2))
model.add(upsampling2d((2, 2)))
model.add(conv2d(32, (3, 3), activation='relu', padding='same'))
model.add(upsampling2d((2, 2)))
model.add(conv2d(16, (3, 3), activation='relu', padding='same'))
model.add(upsampling2d((2, 2)))
model.add(conv2d(2, (3, 3), activation='tanh', padding='same'))
# finish model
model.compile(optimizer='rmsprop',loss='mse')
#train the neural network
model.fit(x=x, y=y, batch_size=1, epochs=3000)
print(model.evaluate(x, y, batch_size=1))
# output colorizations
output = model.predict(x)
output = output * 128
canvas = np.zeros((400, 400, 3))
canvas[:,:,0] = x[0][:,:,0]
canvas[:,:,1:] = output[0]
imsave("img_result.png", lab2rgb(canvas))
imsave("img_gray_scale.png", rgb2gray(lab2rgb(canvas)))

   floydhub command to run this network:
floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboa
rd

technical explanation

   to recap, the input is a grid representing a black and white image. it
   outputs two grids with color values. between the input and output
   values, we create filters to link them together. this is a
   convolutional neural network.

   when we train the network, we use colored images. we convert rgb colors
   to the lab color space. the black and white layer is our input and the
   two colored layers are the output.
   [0*arohhoaqle_sfv3d.png]

   to the left side, we have the b&w input, our filters, and the
   prediction from our neural network.

   we map the predicted values and the real values within the same
   interval. this way, we can compare the values. the interval ranges from
   -1 to 1. to map the predicted values, we use a tanh activation
   function. for any value you give the [34]tanh function, it will return
   -1 to 1.

   the true color values range between -128 and 128. this is the default
   interval in the lab color space. by dividing them by 128, they too fall
   within the -1 to 1 interval. this    id172    enables us to compare
   the error from our prediction.

   after calculating the final error, the network updates the filters to
   reduce the total error. the network continues in this loop until the
   error is as low as possible.

   let   s clarify some syntax in the code snippet.
x = rgb2lab(1.0/255*image)[:,:,0]
y = rgb2lab(1.0/255*image)[:,:,1:]

   1.0/255 indicates that we are using a 24-bit rgb color space. it means
   that we are using numbers between 0   255 for each color channel. this
   results in 16.7 million color combinations.

   since humans can only perceive 2   10 million colors, it does not make
   much sense to use a larger color space.
y = y / 128

   the lab color space has a different range in comparison to rgb. the
   color spectrum ab in lab ranges from -128 to 128. by dividing all
   values in the output layer by 128, we bound the range between -1 and 1.

   we match it with our neural network, which also returns values between
   -1 and 1.

   after converting the color space using the function rgb2lab() we select
   the grayscale layer with: [ : , : , 0]. this is our input for the
   neural network. [ : , : , 1: ] selects the two color layers, green   red
   and blue   yellow.

   after training the neural network, we make a final prediction which we
   convert into a picture.
output = model.predict(x)
output = output * 128

   here, we use a grayscale image as input and run it through our trained
   neural network. we take all the output values between -1 and 1 and
   multiply it by 128. this gives us the correct color in the lab color
   spectrum.
canvas = np.zeros((400, 400, 3))
canvas[:,:,0] = x[0][:,:,0]
canvas[:,:,1:] = output[0]

   lastly, we create a black rgb canvas by filling it with three layers of
   0s. then we copy the grayscale layer from our test image. then we add
   our two color layers to the rgb canvas. this array of pixel values is
   then converted into a picture.

takeaways from the alpha version

     * reading research papers is challenging. once i summarized the core
       characteristics of each paper, it became easier to skim papers. it
       also allowed me to put the details into a context.
     * starting simple is key. most of the implementations i could find
       online were 2   10k lines long. that made it hard to get an overview
       of the core logic of the problem. once i had a barebones version,
       it became easier to read both the code implementation, and also the
       research papers.
     * explore public projects. to get a rough idea for what to code, i
       skimmed 50   100 projects on colorization on github.
     * things won   t always work as expected. in the beginning, it could
       only create red and yellow colors. at first, i had a relu
       activation function for the final activation. since it only maps
       numbers into positive digits, it could not create negative values,
       the blue and green spectrums. adding a tanh activation function and
       mapping the y values fixed this.
     * understanding > speed. many of the implementations i saw were fast
       but hard to work with. i chose to optimize for innovation speed
       instead of code speed.

beta version

   to understand the weakness of the alpha version, try coloring an image
   it has not been trained on. if you try it, you   ll see that it makes a
   poor attempt. it   s because the network has memorized the information.
   it has not learned how to color an image it hasn   t seen before. but
   this is what we   ll do in the beta version. we   ll teach our network to
   generalize.

   below is the result of coloring the validation images with our beta
   version.

   instead of using id163, i created [35]a public dataset on floydhub
   with higher quality images. the images are from [36]unsplash         creative
   commons pictures by professional photographers. it includes 9,500
   training images and 500 validation images.
   [1*m-r9swsz1uhlboozupsvia.png]

the feature extractor

   our neural network finds characteristics that link grayscale images
   with their colored versions.

   imagine you had to color black and white images         but with restriction
   that you can only see nine pixels at a time. you could scan each image
   from the top left to bottom right and try to predict which color each
   pixel should be.
   [1*l6ckjgg6wpg5ffx4xjsv0w.png]

   for example, these nine pixels are the edge of the nostril from the
   woman just above. as you can imagine, it   d be next to impossible to
   make a good colorization, so you break it down into steps.

   first, you look for simple patterns: a diagonal line, all black pixels,
   and so on. you look for the same exact pattern in each square and
   remove the pixels that don   t match. you generate 64 new images from
   your 64 mini filters.
   [1*m0rk1c0ziwhkhnzongzfyg.png]
   the number of filtered images for each step

   if you scan the images again, you   d see the same small patterns you   ve
   already detected. to gain a higher level understanding of the image,
   you decrease the image size in half.
   [1*xej4c-cgzxe2zh3bm1idzq.png]
   we decrease the size in three steps

   you still only have a 3x3 filter to scan each image. but by combining
   your new nine pixels with your lower level filters, you can detect more
   complex patterns. one pixel combination might form a half circle, a
   small dot, or a line. again, you repeatedly extract the same pattern
   from the image. this time, you generate 128 new filtered images.

   after a couple of steps the filtered images you produce might look
   something like these:
   [1*h2a5tzspk6husfxk2bnk9q.png]
   from keras layer tutorial

   as mentioned, you start with low-level features, such as an edge.
   layers closer to the output are combined into patterns. then, they are
   combined into details, and eventually transformed into a face. this
   [37]video tutorial provides a further explanation.

   the process is similar to that of most neural networks that deal with
   vision. the type of network here is known as a convolutional neural
   network. in these networks, you combine several filtered images to
   understand the context in the image.

from feature extraction to color

   the neural network operates in a trial and error manner. it first makes
   a random prediction for each pixel. based on the error for each pixel,
   it works backward through the network to improve the feature
   extraction.

   it starts adjusting for the situations that generate the largest
   errors. in this case, the adjustments are: whether to color or not, and
   how to locate different objects.

   the network starts by coloring all the objects brown. it   s the color
   that is most similar to all other colors, thus producing the smallest
   error.

   because most of the training data is quite similar, the network
   struggles to differentiate between different objects. it will fail to
   generate more nuanced colors. that   s what we   ll explore in the full
   version.

   below is the code for the beta version, followed by a technical
   explanation of the code.
# get images
x = []
for filename in os.listdir('../train/'):
    x.append(img_to_array(load_img('../train/'+filename)))
x = np.array(x, dtype=float)
# set up training and test data
split = int(0.95*len(x))
xtrain = x[:split]
xtrain = 1.0/255*xtrain
#design the neural network
model = sequential()
model.add(inputlayer(input_shape=(256, 256, 1)))
model.add(conv2d(64, (3, 3), activation='relu', padding='same'))
model.add(conv2d(64, (3, 3), activation='relu', padding='same', strides=2))
model.add(conv2d(128, (3, 3), activation='relu', padding='same'))
model.add(conv2d(128, (3, 3), activation='relu', padding='same', strides=2))
model.add(conv2d(256, (3, 3), activation='relu', padding='same'))
model.add(conv2d(256, (3, 3), activation='relu', padding='same', strides=2))
model.add(conv2d(512, (3, 3), activation='relu', padding='same'))
model.add(conv2d(256, (3, 3), activation='relu', padding='same'))
model.add(conv2d(128, (3, 3), activation='relu', padding='same'))
model.add(upsampling2d((2, 2)))
model.add(conv2d(64, (3, 3), activation='relu', padding='same'))
model.add(upsampling2d((2, 2)))
model.add(conv2d(32, (3, 3), activation='relu', padding='same'))
model.add(conv2d(2, (3, 3), activation='tanh', padding='same'))
model.add(upsampling2d((2, 2)))
# finish model
model.compile(optimizer='rmsprop', loss='mse')
# image transformer
datagen = imagedatagenerator(
        shear_range=0.2,
        zoom_range=0.2,
        rotation_range=20,
        horizontal_flip=true)
# generate training data
batch_size = 50
def image_a_b_gen(batch_size):
    for batch in datagen.flow(xtrain, batch_size=batch_size):
        lab_batch = rgb2lab(batch)
        x_batch = lab_batch[:,:,:,0]
        y_batch = lab_batch[:,:,:,1:] / 128
        yield (x_batch.reshape(x_batch.shape+(1,)), y_batch)
# train model
tensorboard(log_dir='/output')
model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=10000, epochs=1)
# test images
xtest = rgb2lab(1.0/255*x[split:])[:,:,:,0]
xtest = xtest.reshape(xtest.shape+(1,))
ytest = rgb2lab(1.0/255*x[split:])[:,:,:,1:]
ytest = ytest / 128
print model.evaluate(xtest, ytest, batch_size=batch_size)
# load black and white images
color_me = []
for filename in os.listdir('../test/'):
        color_me.append(img_to_array(load_img('../test/'+filename)))
color_me = np.array(color_me, dtype=float)
color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]
color_me = color_me.reshape(color_me.shape+(1,))
# test model
output = model.predict(color_me)
output = output * 128
# output colorizations
for i in range(len(output)):
        cur = np.zeros((256, 256, 3))
        cur[:,:,0] = color_me[i][:,:,0]
        cur[:,:,1:] = output[i]
        imsave("result/img_"+str(i)+".png", lab2rgb(cur))

   here   s the floydhub command to run the beta neural network:
floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboa
rd

technical explanation

   the main difference from other visual neural networks is the importance
   of pixel location. in coloring networks, the image size or ratio stays
   the same throughout the network. in other types of network, the image
   gets distorted the closer it gets to the final layer.

   the max-pooling layers in classification networks increase the
   information density, but also distort the image. it only values the
   information, but not the layout of an image. in coloring networks we
   instead use a stride of 2, to decrease the width and height by half.
   this also increases information density but does not distort the image.
   [1*b_lkunptecvqbpmuxi5hwa.png]

   two further differences are: upsampling layers and maintaining the
   image ratio. classification networks only care about the final
   classification. therefore, they keep decreasing the image size and
   quality as it moves through the network.

   coloring networks keep the image ratio constant. this is done by adding
   white padding like the visualization above. otherwise, each
   convolutional layer cuts the images. it   s done with the
   *padding='same'*parameter.

   to double the size of the image, the coloring network uses [38]an
   upsampling layer.
for filename in os.listdir('/color_300/train/'):
    x.append(img_to_array(load_img('/color_300/test'+filename)))

   this for-loop first counts all the file names in the directory. then,
   it iterates through the image directory and converts the images into an
   array of pixels. finally, it combines them into a giant vector.
datagen = imagedatagenerator(
        shear_range=0.2,
        zoom_range=0.2,
        rotation_range=20,
        horizontal_flip=true)

   with [39]imagedatagenerator, we adjust the setting for our image
   generator. this way, each image will never be the same, thus improving
   the learning rate. the shear_rangetilts the image to the left or right,
   and the other settings are zoom, rotation and horizontal-flip.
batch_size = 50
def image_a_b_gen(batch_size):
    for batch in datagen.flow(xtrain, batch_size=batch_size):
        lab_batch = rgb2lab(batch)
        x_batch = lab_batch[:,:,:,0]
        y_batch = lab_batch[:,:,:,1:] / 128
        yield (x_batch.reshape(x_batch.shape+(1,)), y_batch)

   we use the images from our folder, xtrain, to generate images based on
   the settings above. then, we extract the black and white layer for the
   x_batch and the two colors for the two color layers.
model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=1, epochs=1000)

   the stronger the gpu you have, the more images you can fit into it.
   with this setup, you can use 50   100 images. steps_per_epoch is
   calculated by dividing the number of training images with your batch
   size.

   for example: 100 images with a batch size of 50 gives 2 steps per
   epoch. the number of epochs determines how many times you want to train
   all images. 10k images with 21 epochs will take about 11 hours on a
   tesla k80 gpu.

takeaways

     * run a lot of experiments in smaller batches before you make larger
       runs. even after 20   30 experiments, i still found mistakes. just
       because it   s running doesn   t mean it   s working. bugs in a neural
       network are often more nuanced than traditional programming errors.
       one of the more bizarre ones was [40]my adam hiccup.
     * a more diverse dataset makes the pictures brownish. if you have
       [41]very similar images, you can get a decent result without
       needing a more complex architecture. the trade-off is the network
       becomes worse at generalizing.
     * shapes, shapes, and shapes. the size of each image has to be exact
       and remain proportional throughout the network. in the beginning, i
       used an image size of 300. halving this three times gives sizes of
       150, 75, and 35.5. the result is losing half a pixel! this led to
       many    hacks    until i realized it   s better to use a power of two: 2,
       8, 16, 32, 64, 256 and so on.
     * creating datasets: a) [42]disable the .ds_store file, it drove me
       crazy. b) be creative. i ended up with a chrome [43]console script
       and [44]an extension to download the files. c) make a copy of the
       raw files you scrape and structure your [45]cleaning scripts.

full-version

   our final version of the colorization neural network has four
   components. we split the network we had before into an encoder and a
   decoder. between them, we   ll use a fusion layer. if you are new to
   classification networks, i   d recommend having a look at [46]this
   tutorial.

   in parallel to the encoder, the input images also run through one of
   today   s most powerful classifiers         the [47]inception resnet v2 . this
   is a neural network trained on 1.2m images. we extract the
   classification layer and merge it with the output from the encoder.
   [1*krxxaaxlbz1psrvb1ak04q.png]

   here is a more [48]detailed visual from the original paper.

   by transferring the learning from the classifier to the coloring
   network, the network can get a sense of what   s in the picture. thus,
   enabling the network to match an object representation with a coloring
   scheme.

   here are some of the validation images, using only 20 images to train
   the network on.
   [1*fra53gip67fljqj_yqy7yg.png]

   most of the images turned out poor. but i was able to find a few decent
   ones because of a large validation set (2,500 images). training it on
   more images gave a more consistent result, but most of them turned out
   brownish. here is a full list of the [49]experiments i ran including
   the validation images.

   here are the most common architectures from previous research, with
   links:
     * manually adding small dots of color in a picture to guide the
       neural network ([50]link)
     * find a matching image and transfer the coloring (learn more
       [51]here and [52]here)
     * residual encoder and merging classification layers ([53]link)
     * merging hypercolumns from a classifying network (more detail
       [54]here and [55]here)
     * merging the final classification between the encoder and decoder
       (details [56]here and [57]here)

   colorspaces: lab, yuv, hsv, and luv (more detail [58]here and [59]here)

   loss: mean square error, classification, weighted classification
   ([60]link)

   i chose the    fusion layer    architecture (the fifth one in the list
   above).

   this was because it produces some of the best results. it is also
   easier to understand and reproduce in [61]keras. although it   s not the
   strongest colorization network design, it is a good place to start.
   it   s a great architecture to understand the dynamics of the coloring
   problem.

   i used the neural network design from [62]this paper by federico
   baldassarre and collaborators. i proceeded with my own interpretation
   in keras.

   note: in the below code i switch from keras    sequential model to their
   functional api. [[63]documentation]
# get images
x = []
for filename in os.listdir('/data/images/train/'):
    x.append(img_to_array(load_img('/data/images/train/'+filename)))
x = np.array(x, dtype=float)
xtrain = 1.0/255*x
#load weights
inception = inceptionresnetv2(weights=none, include_top=true)
inception.load_weights('/data/inception_resnet_v2_weights_tf_dim_ordering_tf_ker
nels.h5')
inception.graph = tf.get_default_graph()
embed_input = input(shape=(1000,))
#encoder
encoder_input = input(shape=(256, 256, 1,))
encoder_output = conv2d(64, (3,3), activation='relu', padding='same', strides=2)
(encoder_input)
encoder_output = conv2d(128, (3,3), activation='relu', padding='same')(encoder_o
utput)
encoder_output = conv2d(128, (3,3), activation='relu', padding='same', strides=2
)(encoder_output)
encoder_output = conv2d(256, (3,3), activation='relu', padding='same')(encoder_o
utput)
encoder_output = conv2d(256, (3,3), activation='relu', padding='same', strides=2
)(encoder_output)
encoder_output = conv2d(512, (3,3), activation='relu', padding='same')(encoder_o
utput)
encoder_output = conv2d(512, (3,3), activation='relu', padding='same')(encoder_o
utput)
encoder_output = conv2d(256, (3,3), activation='relu', padding='same')(encoder_o
utput)
#fusion
fusion_output = repeatvector(32 * 32)(embed_input)
fusion_output = reshape(([32, 32, 1000]))(fusion_output)
fusion_output = concatenate([encoder_output, fusion_output], axis=3)
fusion_output = conv2d(256, (1, 1), activation='relu', padding='same')(fusion_ou
tput)
#decoder
decoder_output = conv2d(128, (3,3), activation='relu', padding='same')(fusion_ou
tput)
decoder_output = upsampling2d((2, 2))(decoder_output)
decoder_output = conv2d(64, (3,3), activation='relu', padding='same')(decoder_ou
tput)
decoder_output = upsampling2d((2, 2))(decoder_output)
decoder_output = conv2d(32, (3,3), activation='relu', padding='same')(decoder_ou
tput)
decoder_output = conv2d(16, (3,3), activation='relu', padding='same')(decoder_ou
tput)
decoder_output = conv2d(2, (3, 3), activation='tanh', padding='same')(decoder_ou
tput)
decoder_output = upsampling2d((2, 2))(decoder_output)
model = model(inputs=[encoder_input, embed_input], outputs=decoder_output)
#create embedding
def create_inception_embedding(grayscaled_rgb):
    grayscaled_rgb_resized = []
    for i in grayscaled_rgb:
        i = resize(i, (299, 299, 3), mode='constant')
        grayscaled_rgb_resized.append(i)
    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)
    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)
    with inception.graph.as_default():
        embed = inception.predict(grayscaled_rgb_resized)
    return embed
# image transformer
datagen = imagedatagenerator(
        shear_range=0.4,
        zoom_range=0.4,
        rotation_range=40,
        horizontal_flip=true)
#generate training data
batch_size = 20
def image_a_b_gen(batch_size):
    for batch in datagen.flow(xtrain, batch_size=batch_size):
        grayscaled_rgb = gray2rgb(rgb2gray(batch))
        embed = create_inception_embedding(grayscaled_rgb)
        lab_batch = rgb2lab(batch)
        x_batch = lab_batch[:,:,:,0]
        x_batch = x_batch.reshape(x_batch.shape+(1,))
        y_batch = lab_batch[:,:,:,1:] / 128
        yield ([x_batch, create_inception_embedding(grayscaled_rgb)], y_batch)
#train model
tensorboard = tensorboard(log_dir="/output")
model.compile(optimizer='adam', loss='mse')
model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=1
000, steps_per_epoch=20)
#make a prediction on the unseen images
color_me = []
for filename in os.listdir('../test/'):
    color_me.append(img_to_array(load_img('../test/'+filename)))
color_me = np.array(color_me, dtype=float)
color_me = 1.0/255*color_me
color_me = gray2rgb(rgb2gray(color_me))
color_me_embed = create_inception_embedding(color_me)
color_me = rgb2lab(color_me)[:,:,:,0]
color_me = color_me.reshape(color_me.shape+(1,))
# test model
output = model.predict([color_me, color_me_embed])
output = output * 128
# output colorizations
for i in range(len(output)):
    cur = np.zeros((256, 256, 3))
    cur[:,:,0] = color_me[i][:,:,0]
    cur[:,:,1:] = output[i]
    imsave("result/img_"+str(i)+".png", lab2rgb(cur))

   here   s the floydhub command to run the full neural network:
floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboa
rd

technical explanation

   [64]keras    functional api is ideal when we are concatenating or merging
   several models.
   [0*naxhqryidbtce7ty.png]

   first, we download the [65]inception resnet v2 neural network and load
   the weights. since we will be using two models in parallel, we need to
   specify which model we are using. this is done in [66]tensorflow, the
   backend for keras.
inception = inceptionresnetv2(weights=none, include_top=true)
inception.load_weights('/data/inception_resnet_v2_weights_tf_dim_ordering_tf_ker
nels.h5')
inception.graph = tf.get_default_graph()

   to create our batch, we use the tweaked images. we conver them to black
   and white and run them through the inception resnet model.
grayscaled_rgb = gray2rgb(rgb2gray(batch))
embed = create_inception_embedding(grayscaled_rgb)

   first, we have to resize the image to fit into the inception model.
   then we use the preprocessor to format the pixel and color values
   according to the model. in the final step, we run it through the
   inception network and extract the final layer of the model.
def create_inception_embedding(grayscaled_rgb):
    grayscaled_rgb_resized = []
    for i in grayscaled_rgb:
        i = resize(i, (299, 299, 3), mode='constant')
        grayscaled_rgb_resized.append(i)
    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)
    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)
    with inception.graph.as_default():
        embed = inception.predict(grayscaled_rgb_resized)
    return embed

   let   s go back to the generator. for each batch, we generate 20 images
   in the below format. it takes about an hour on a tesla k80 gpu. it can
   do up to 50 images at a time with this model without having memory
   problems.
yield ([x_batch, create_inception_embedding(grayscaled_rgb)], y_batch)

   this matches with our colornet model format.
model = model(inputs=[encoder_input, embed_input], outputs=decoder_output)

   encoder_inputis fed into our encoder model, the output of the encoder
   model is then fused with the embed_inputin the fusion layer; the output
   of the fusion is then used as input in our decoder model, which then
   returns the final output, decoder_output.
fusion_output = repeatvector(32 * 32)(embed_input)
fusion_output = reshape(([32, 32, 1000]))(fusion_output)
fusion_output = concatenate([fusion_output, encoder_output], axis=3)
fusion_output = conv2d(256, (1, 1), activation='relu')(fusion_output)

   in the fusion layer, we first multiply the 1000 category layer by 1024
   (32 * 32). this way, we get 1024 rows with the final layer from the
   inception model.

   this is then reshaped from 2d to 3d, a 32 x 32 grid with the 1000
   category pillars. these are then linked together with the output from
   the encoder model. we apply a 254 filtered convolutional network with a
   1x1 kernel, the final output of the fusion layer.

takeaways

     * the research terminology was daunting. i spent three days googling
       for ways to implement the    fusion model    in keras. because it
       sounded complex, i didn   t want to face the problem. instead, i
       tricked myself into searching for short cuts.
     * i asked questions online. i didn   t have a single comment in the
       keras slack channel and stack overflow deleted my questions. but,
       by publicly breaking down the problem to make it simple to answer,
       it forced me to isolate the error, taking me closer to a solution.
     * email people. although forums can be cold, people care if you
       connect with them directly. discussing color spaces over skype with
       a researcher is inspiring!
     * after delaying on the fusion problem, i decided to build all the
       components before i stitched them together. here are a [67]few
       experiments i used to break down the fusion layer.
     * once i had something i thought would work, i was hesitant to run
       it. although i knew the core logic was okay, i didn   t believe it
       would work. after a cup of lemon tea and a long walk         i ran it. it
       produced an error after the first line in my model. but after four
       days, several hundred bugs and several thousand google searches,
          epoch 1/22    appeared under my model.

next steps

   colorizing images is a deeply fascinating problem. it is as much as a
   scientific problem as artistic one. i wrote this article so you can get
   up to speed in coloring and continue where i left off. here are some
   suggestions to get started:
     * implement it with another pre-trained model
     * try a different dataset
     * increase the network   s accuracy by using more pictures
     * build an amplifier within the rgb color space. create a similar
       model to the coloring network, that takes a saturated colored image
       as input and the correct colored image as output.
     * implement a weighted classification
     * apply it to video. don   t worry too much about the colorization, but
       make the switch between images consistent. you could also do
       something similar for larger images, by tiling smaller ones.

   you can also easily colorize your own black and white images with my
   three versions of the colorization neural network using floydhub.
     * for the alpha version, simply replace the woman.jpgfile with your
       file with the same name (image size 400x400 pixels).
     * for the beta and the full version, add your images to the
       testfolder before you run the floydhub command. you can also upload
       them directly in the notebook to the test folder while the notebook
       is running. note that these images need to be exactly 256x256
       pixels. also, you can upload all test images in color because it
       will automatically convert them into b&w.

   if you build something or get stuck, ping me on twitter:
   [68]emilwallner. i   d love to see what you are building.

   huge thanks to federico baldassarre, for answering my questions and
   their previous work on colorization. also thanks to muthu chidambaram,
   who influenced the core implementation in keras, and the unsplash
   community for providing the pictures. thanks also to marine haziza,
   valdemaras repsys, qingping hou, charlie harrington, sai soundararaj,
   jannes klaas, claudio cabral, alain demenet, and ignacio tonoli for
   reading drafts of this.

about emil wallner

   this the third part in a multi-part blog series from emil as he learns
   deep learning. emil has spent a decade exploring human learning. he   s
   worked for oxford   s business school, invested in education startups,
   and built an education technology business. last year, he enrolled at
   [69]ecole 42 to apply his knowledge of human learning to machine
   learning.

   you can follow along with emil on [70]twitter and [71]medium.

   this was first published as a community post on [72]floydhub   s blog.

     * [73]machine learning
     * [74]tech
     * [75]data science
     * [76]programming
     * [77]startup

   (button)
   (button)
   (button) 7.5k claps
   (button) (button) (button) 30 (button) (button)

     (button) blockedunblock (button) followfollowing
   [78]go to the profile of emil wallner

[79]emil wallner

   i study cs at 42 paris, write, and experiment with deep learning.

     (button) follow
   [80]freecodecamp.org

[81]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 7.5k
     * (button)
     *
     *

   [82]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [83]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/53d9b4449f8d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_eh9ex6ujfrfm---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@emilwallner?source=post_header_lockup
  12. https://medium.freecodecamp.org/@emilwallner
  13. http://www.whatimade.today/our-frst-reddit-bot-coloring-b-2/
  14. https://www.reddit.com/r/colorization/
  15. https://medium.freecodecamp.org/media/5262de6e215aca6ab020904c0794ad6f?postid=53d9b4449f8d
  16. https://blog.floydhub.com/my-first-weekend-of-deep-learning/
  17. https://blog.floydhub.com/coding-the-history-of-deep-learning/
  18. https://www.youtube.com/watch?v=lxfughug-iq
  19. https://research.googleblog.com/2016/08/improving-inception-and-image.html
  20. https://unsplash.com/
  21. https://www.floydhub.com/emilwallner/projects/color/43/code/alpha-version/alpha_version.ipynb
  22. https://www.floydhub.com/emilwallner/projects/color/43/code
  23. https://github.com/emilwallner/coloring-greyscale-images-in-keras
  24. https://www.floydhub.com/emilwallner/projects/color/jobs
  25. https://blog.floydhub.com/workspaces/
  26. https://www.floydhub.com/?utm_medium=readme&utm_source=colornet&utm_campaign=aug_2018
  27. https://github.com/floydhub/colornet-template#serve-an-interactive-web-page-for-your-own-model
  28. https://floydhub.com/run?template=https://github.com/floydhub/colornet-template
  29. https://www.floydhub.com/
  30. https://www.youtube.com/watch?v=bylq9kgjtdq&t=6s
  31. https://blog.floydhub.com/my-first-weekend-of-deep-learning/
  32. https://github.com/emilwallner/coloring-greyscale-images-in-keras
  33. https://www.floydhub.com/emilwallner/datasets/cifar-10/1
  34. http://mathworld.wolfram.com/hyperbolictangent.html
  35. https://www.floydhub.com/emilwallner/datasets/colornet
  36. https://unsplash.com/
  37. https://www.youtube.com/watch?v=agkfiq4igam
  38. https://keras.io/layers/convolutional/#upsampling2d
  39. https://keras.io/preprocessing/image/
  40. https://twitter.com/emilwallner/status/916309564966006784
  41. https://github.com/2014mchidamb/deepcolorization/tree/master/face_images
  42. http://osxdaily.com/2010/02/03/how-to-prevent-ds_store-file-creation/
  43. https://github.com/emilwallner/useful-scripts/blob/master/auto_scroll_browser_window_console
  44. https://chrome.google.com/webstore/detail/imagespark-ultimate-image/hooaoionkjogngfhjjniefmenehnopag
  45. https://github.com/emilwallner/useful-scripts
  46. http://cs231n.github.io/classification/
  47. https://research.googleblog.com/2016/08/improving-inception-and-image.html
  48. https://github.com/baldassarrefe/deep-koalarization
  49. https://www.floydhub.com/emilwallner/projects/color
  50. http://www.cs.huji.ac.il/~yweiss/colorization/
  51. https://dl.acm.org/citation.cfm?id=2393402
  52. https://arxiv.org/abs/1505.05192
  53. http://tinyclouds.org/colorize/
  54. https://arxiv.org/pdf/1603.08511.pdf
  55. https://arxiv.org/pdf/1603.06668.pdf
  56. http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf
  57. https://arxiv.org/abs/1712.03400
  58. http://cs231n.stanford.edu/reports/2016/pdfs/219_report.pdf
  59. https://arxiv.org/abs/1605.00075
  60. https://arxiv.org/pdf/1603.06668.pdf
  61. https://keras.io/
  62. https://arxiv.org/abs/1712.03400
  63. https://keras.io/getting-started/functional-api-guide/
  64. https://keras.io/getting-started/functional-api-guide/
  65. https://research.googleblog.com/2016/08/improving-inception-and-image.html
  66. https://www.tensorflow.org/
  67. https://www.floydhub.com/emilwallner/projects/color/24/code/experiments/transfer-learning-examples
  68. https://twitter.com/emilwallner
  69. https://twitter.com/paulg/status/847844863727087616
  70. https://twitter.com/emilwallner
  71. https://medium.com/@emilwallner
  72. https://blog.floydhub.com/colorizing-b&w-photos-with-neural-networks/
  73. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  74. https://medium.freecodecamp.org/tagged/tech?source=post
  75. https://medium.freecodecamp.org/tagged/data-science?source=post
  76. https://medium.freecodecamp.org/tagged/programming?source=post
  77. https://medium.freecodecamp.org/tagged/startup?source=post
  78. https://medium.freecodecamp.org/@emilwallner?source=footer_card
  79. https://medium.freecodecamp.org/@emilwallner
  80. https://medium.freecodecamp.org/?source=footer_card
  81. https://medium.freecodecamp.org/?source=footer_card
  82. https://medium.freecodecamp.org/
  83. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  85. https://medium.com/p/53d9b4449f8d/share/twitter
  86. https://medium.com/p/53d9b4449f8d/share/facebook
  87. https://medium.com/p/53d9b4449f8d/share/twitter
  88. https://medium.com/p/53d9b4449f8d/share/facebook
