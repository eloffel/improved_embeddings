   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a beginner introduction to tensorflow (part-1)

   [16]go to the profile of narasimha prasanna hn
   [17]narasimha prasanna hn (button) blockedunblock (button)
   followfollowing
   oct 28, 2017

   tensorflow is one of the widely used libraries for implementing machine
   learning and other algorithms involving large number of mathematical
   operations. tensorflow was developed by google and it   s one of the most
   popular machine learning libraries on github. google uses tensorflow
   for implementing machine learning in almost all applications. for
   example, if you are using google photos or google voice search then you
   are using tensorflow models indirectly, they work on large clusters of
   google hardware and are powerful in perceptual tasks.

   the main aim of this article is to provide a beginner friendly
   introduction to tensorflow, i assume that you already know a bit of
   python. the core component of tensorflow is the computational graph and
   tensors which traverse among all the nodes through edges. let   s have a
   brief introduction to each one of them.

tensors:

   [1*jazi88lbxu3q0-5ij1cjyw.png]

   mathematically a tensor is a n-dimensional vector, means a tensor can
   be used to represent n-dimensional datasets. the figure above is
   complex to understand. we   ll look at it   s simplified version
   [1*wv9adjswmgl4wle7lstriw.png]

   the above figure shows some simplified tensors with minimum dimensions.
   as the dimensions keep on increasing, data representation becomes more
   and more complex. for example if we take a tensor of the form (3x3)
   then i can simply call it a matrix of 3 rows and columns. if i select
   another tensor of form (1000x3x3), i can call it as a vector or set of
   1000 3x3 matrices. here we call (1000x3x3) as the shape or dimension of
   the resulting tensor. tensors can either be a constant or a variable.

computational graphs (flow):

   now we understood what tensor really means, and it   s time to understand
   flow. this flow refers to a computational graph or simply a graph, the
   graph can never be cyclic, each node in the graph represents an
   operation like addition, subtraction etc. and each operation results in
   the formation of new tensor.
   [1*7lkltjqythz8w7eeqz5zha.png]

   the figure above shows a simple computational graph. the computational
   graph has the following properties:

   the expression for above graph :

e = (a+b)x(b+1)

     * leaf vertices or start vertices are always tensors. means, an
       operation can never occur at the beginning of the graph and thus we
       can infer that each operation in the graph should accept a tensor
       and produce a new tensor. in the same way, a tensor cannot be
       present as a non-leaf node, meaning they should be always supplied
       as an input to the operation / node.
     * a computational graph always represents a complex operation in a
       hierarchial order. the above expression can be organized in a
       hierarchial way, by representing a+b as c and b+1 as d. therefore
       we can write e as:

   e = (c)x(d) where c = a+b and d = b+1.
     * traversing the graph in reverse order results in the formation of
       sub expressions which are combined to form the final expression.
     * when we traverse in forward direction, the vertex we encounter
       always becomes a dependency for the next vertex , for example c
       cannot be obtained without a and b, in the same way e cannot be
       obtained without solving for c and d.
     * operation in the nodes of same level are independent of each other.
       this is one of the important property of computational graph, when
       we construct a graph in the way shown in the figure, it is natural
       that, nodes in the same level for example c and d are independent
       of each other, means there is no need to know c before evaluating
       d. therefore they can be executed in parallel.

parallelism in computational graphs:

   last property mentioned above is of course one of the most important
   property, it clearly says that nodes at the same level are independent,
   means there is no need of sitting idle until c gets evaluated, you can
   parallely compute d while c is still being evaluated. tensorflow
   greatly exploits this property.

distributed execution :

   tensorflow allows users to make use of parallel computing devices to
   perform operations faster. the nodes or operations of a computational
   are automatically scheduled for parallel computing. this all happens
   internally, for example in the above graph, operation c can be
   scheduled on cpu and operation d can be scheduled on gpu. the figure
   below shows two prospectives of distributed execution :
   [1*cok4bmhtve93udgmrbleyw.png]

   the first one, is a single system distributed execution where a single
   tensorflow session( will be explained later) creates a single worker
   and the worker is responsible for scheduling tasks on various devices,
   in the second case, there are multiple workers , they can be on same
   machine or on different machines, each worker runs in its own context,
   in the above figure, worker process 1 runs on a separate machine and
   schedules operations on all available devices.

computational subgraphs :

   subgraphs are the part of main graph and are themselves computational
   graphs by nature. for example, in the above graph, we can obtain many
   subgraphs, one of them is shown below
   [1*qxv8vcz5rbjqoqgpvndetw.png]

   the graph above is a part of main graph, from property 2 we can say
   that a subgraph always represents a sub expression, as c is the
   subexpression of e. subgraphs also satisfy the last property. subgraphs
   in the same level are also independent of each other and can be
   executed in parallel. therefore it   s possible to schedule entire
   subgraph on a single device.
   [1*qqkvomtaeevyth7e46-mqa.png]

   the above figure explains the parallel execution of subgraphs. here
   there are 2 id127 operations, since both of them are at
   the same level, they are independent of each other, this holds good
   with the last property. the nodes are scheduled on different devices
   gpu_0 and gpu_1, this is because of the property of independence.

exchanging data between workers :

   now we know that tensorflow distributes all it   s operations on
   different devices governed by workers. it is more common that, data in
   the form of tensors are exchanged between workers, for example in the
   graph of e =(c)*(d) once c is calculated it is desirable to pass it on
   further to process e, therefore tensor flows from node to node in
   upward direction. this movement is done as shown in the figure :
   [1*btjmu1o-glfna1d1ghot7g.png]

   here tensors form device a has been passed on to device b. this induces
   some performance delays in a distributed system. the delays depends on
   an important property that is size of a tensor. the device b is in
   ideal mode until it receives input form device a.

need for compression :

   well, it   s obvious that in computational graphs, tensors flow between
   nodes. it   s important to reduce the delays caused by the flow before it
   reaches the node where it can be processed. one such idea of reducing
   the size is by using lossy compression.

   the data type of tensors has a major role to play, let   s understand
   why, it   s obvious that we go for higher degrees of precision in machine
   learning operations, for example if we use float32 as the data type of
   a tensor, them each value is represented using a 32 bit floating point
   number, so each value occupies a size of 32 bits, the same applies for
   64 bit also. assume a tensor of shape (1000,440,440,3), the number of
   values that can be contained within the tensor will be 1000*440*440*3
   if data type is 32 bit then it   s 32 times of this big number, it
   occupies a significant space in the memory and thus impose delays for
   the flow. compression techniques can be used to reduce the size.

lossy compression :

   lossy compression deals with compressing the size of data and does not
   care about it   s value, means it   s value may become corrupt or
   inaccurate during compression. but still if we have a 32 bit floating
   point number like 1.01010e-12, there is a less importance that can be
   given to least significant digits. changing or removing those values
   will not cause a much difference in our calculation. so tensorflow
   automatically converts 32 bit floating point numbers to a 16 bit
   representation by ignoring all digits which are negligible, this
   reduces the size by almost half, if it   s a 64 bit number, it   s
   compression to 16 bit will cause the reduction in size by almost 75%.
   thus space occupied by tensors can be minimized.

   once tensor reaches the nodes, the 16 bit representation can be bought
   back to it   s original form just by appending 0s. thus a 32 or 64 bit
   representation is bought back after it reaches the node for processing.
     __________________________________________________________________

   this ends part-1 of the tensorflow introduction, programming and
   constructing simple subgraphs will be explained in the next part.

   thanks    

     * [18]machine learning
     * [19]tensorflow
     * [20]deep learning
     * [21]towards data science
     * [22]data science

   (button)
   (button)
   (button) 2.6k claps
   (button) (button) (button) 17 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of narasimha prasanna hn

[24]narasimha prasanna hn

   web and android developer, hobbyist programmer ( comfortable with
   python and javascript), interested in making machines intelligent.

     (button) follow
   [25]towards data science

[26]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.6k
     * (button)
     *
     *

   [27]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [28]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6d139e038278
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_g9tqr3gqtiwb---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@narasimhaprasannahn?source=post_header_lockup
  17. https://towardsdatascience.com/@narasimhaprasannahn
  18. https://towardsdatascience.com/tagged/machine-learning?source=post
  19. https://towardsdatascience.com/tagged/tensorflow?source=post
  20. https://towardsdatascience.com/tagged/deep-learning?source=post
  21. https://towardsdatascience.com/tagged/towards-data-science?source=post
  22. https://towardsdatascience.com/tagged/data-science?source=post
  23. https://towardsdatascience.com/@narasimhaprasannahn?source=footer_card
  24. https://towardsdatascience.com/@narasimhaprasannahn
  25. https://towardsdatascience.com/?source=footer_card
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/
  28. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  30. https://medium.com/p/6d139e038278/share/twitter
  31. https://medium.com/p/6d139e038278/share/facebook
  32. https://medium.com/p/6d139e038278/share/twitter
  33. https://medium.com/p/6d139e038278/share/facebook
