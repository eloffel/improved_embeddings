                         [1]acl 2008 third workshop
                     on id151

          shared task: automatic evaluation of machine translation

           june 19, in conjunction with acl 2008 in columbus, ohio

    [[2]home] | [[3]shared translation task] | [shared evaluation task] |
                     [[4]results] | [[5]baseline system]
                 [[6]schedule] | [[7]authors] | [[8]papers]

   the shared evaluation task of the [9]workshop will examine automatic
   id74 for machine translation. we will provide all of the
   translations produced in the [10]shared translation task, as well as
   the reference translations. you will return rankings for each of each
   of the translations at the system-level and/or at the sentence-level.
   we will calculate the correlation on your rankings with the human
   evaluation when it is completed.

  goals

   the goals of the shared evaluation task are:
     * to achieve the strongest correlation with human judgments of
       translation quality
     * to illustrate the suitability of an automatic evaluation metric as
       a surrogate for human evaluations
     * to address the problems associated with comparing against a single
       reference translation
     * to move automatic evaluation beyond system-level ranking to
       finer-grained sentence-level ranking

  submission format

   once we receive the system outputs from the [11]shared translation task
   we will post all of the system translations, along with source
   documents and reference translations, for you to evaluate with your
   metric. the translations will be available in two formats:
     * the nist mt evaluation workshop's xml format, which looks like
       this:
<tstset setid="wmt08-de-en-nc-test" srclang="german" trglang="english">
<doc docid="speigel-doc1" sysid="umd_de_en_primary">
<seg id="1"> translated english text </seg>
<seg id="2"> translated english text </seg>
...
</doc>
<doc docid="speigel-doc2"  sysid="umd_de_en_primary">
<seg id="13"> translated english text </seg>
<seg id="14"> translated english text </seg>
...
</doc>
</tstset>

     * plain text files with one translation per line.

   you can use either of these as input to your software. the output of
   your software should produce scores for the translations either at the
   system-level or the segment-level (or preferably both).

    output file format for system-level rankings

   the output files for system-level rankings should be formatted in the
   following way:
<test set>   <system>   <system level score>

   where:
     * test set is the id of the test set (given by the setid attribute of
       of the tstset tag in the xml file, or by the directory structure in
       the plain text files).
     * system is the id of system being scored (given by the sysid
       attribute in the xml document, or as part of the filename for the
       plain text file).
     * system level score is the overall system level score.

   each field should be delimited by a single tab character.

    output file format for segment-level rankings

   the output files for segment-level rankings should be formatted in the
   following way:
<test set>   <system>   <document id>   <segment id>   <segment score>

   where:
     * test set is the id of the test set.
     * system is the id of system being scored.
     * document id is the document id (given by the docid tag in the xml
       document, or identical to the test set id if you're using the plain
       text input files).
     * segment id is the segment number of each segment (given by the seg
       id tag of the xml file, or the line number starting from one of the
       plain text input files).
     * segment score is the score for the particular segment.

   each field should be delimited by a single tab character. the output
   file formats are identical to the ones that will be used in the nist
   workshop on id74 for machine translation, which is going
   to be held at amta this year.

  development data

   segment-level and sentence-level development data is available for all
   of the language pairs featured in last year's workshop. the development
   data was compiled from the sentence-level rankings of last year's
   manual evaluation process. you are welcome to create customized dev
   data from the raw data from last year's human evaluation.
     * [12]evaluation-dev-data.tar.gz (11m zipped, 33m uncompressed)
     * [13]readme
     * [14]raw data from wmt07 evaluation

  dates

   march 29: system translations released (tar file here:
   [15]wmt08-eval.tar.gz)
   april 4: deadline for short paper submissions (4 pages)
   extended april 9: deadline for submitting rankings (by email to ccb@cs
   jhu .com edu)

   [16][euromatrix-logo-small.png] supported by the [17]euromatrix
   project, p6-ist-5-034291-stp
   funded by the european commission under framework programme 6

references

   1. http://www.statmt.org/wmt08/index.html
   2. http://www.statmt.org/wmt08/index.html
   3. http://www.statmt.org/wmt08/shared-task.html
   4. http://www.statmt.org/wmt08/results.html
   5. http://www.statmt.org/wmt08/baseline.html
   6. http://www.statmt.org/wmt08/program.html
   7. http://www.statmt.org/wmt08/author.html
   8. http://www.statmt.org/wmt08/paper.html
   9. http://www.statmt.org/wmt08/index.html
  10. http://www.statmt.org/wmt08/shared-task.html
  11. http://www.statmt.org/wmt08/shared-task.html
  12. http://www.statmt.org/wmt08/evaluation-dev-data.tar.gz
  13. http://www.statmt.org/wmt08/readme.evaluation-dev-data
  14. http://www.statmt.org/wmt07/judgements.gz
  15. http://www.statmt.org/wmt08/wmt08-eval.tar.gz
  16. http://www.euromatrix.net/
  17. http://www.euromatrix.net/
