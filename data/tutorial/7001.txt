   #[1]building babylon    feed [2]building babylon    comments feed
   [3]building babylon    skipgram isn't matrix factorisation comments feed
   [4]softmax parameterisation and optimisation [5]convergence rate of
   id119 [6]alternate

   [7]skip to content

   building babylon

   [8]building babylon

   notes on machine learning & mathematics

   posted on [9]may 12, 2016 by [10]benjamin

skipgram isn't matrix factorisation

   the paper neural id27s as implicit id105 of
   levy and goldberg was published in the proceedings of nips
   2014 ([11]pdf).  it claims to demonstrate that mikolov   s skipgram model
   with negative sampling is implicitly factorising the matrix of
   pointwise mutual information (pmi) of the word/context pairs, shifted
   by a global constant.  although the paper is interesting and worth
   reading, it greatly overstates what is actually established, which can
   be summarised as follows:

   suppose that the dimension of the skipgram id27 is at least
   as large as the vocabulary.  then if the matrices of parameters (w, c)
   minimise the skipgram objective, and the rows of w or the columns of c
   are linearly independent, then the matrix product wc is the pmi matrix
   shifted by a global constant.

   this is a really nice result, but it certainly doesn   t show that
   skipgram is performing (even implicitly) matrix factorisation.  rather
   it shows that the two learning tasks have the same global optimum     
   and even this is only shown when the dimension is larger than the
   vocabulary, which is precisely the case where skipgram is
   uninteresting.

the linear independence assumption

   the authors (perhaps unknowingly) implicitly assume that the word
   vectors on one of the two layers of the skipgram model are linearly
   independent.  this is a stronger assumption than what the authors
   explicitly assume, which is that the dimension of the hidden layer is
   at least as large as the vocabulary.  it is also not a very natural
   assumption, since skipgram is interesting to us precisely because it
   captures word analogies in word vector arithmetic, which are linear
   dependencies between the word vectors!  this is not a deal breaker,
   however, since these linear dependencies are only ever approximate.

   in order to see where the assumption arises, first recall some notation
   of the paper:

   levy-goldberg-setting1

   the authors consider the case where the negative samples for skipgram
   are drawn from the uniform distribution p_d over the contexts v_c ,
   and write

   levy-goldberg-setting2

   for the log likelihood.  the log likelihood is then rewritten as
   another double summation, in which each summand (as a function of the
   model parameters) depends only upon the dot product of one word vector
   with one context vector:

   11-05-2016 5-56 pm

   the authors then suppose that the values of the parameters w, c are
   such that skipgram is at equilibrium, i.e. that the partial derivatives
   of l with respect to each word- and content-vector component vanish.
   they then assume that this implies that the partial derivatives of l
   with respect to the dot products vanish also.  to see that this doesn   t
   necessarily follow, apply the chain rule to the partial derivatives:

   11-05-2016 5-56 pm(4)

   this yields systems of linear equations relating the partial
   derivatives with respect to the word- and content- vector components
   (which are zero by supposition) to the partial derivatives with respect
   to the dot products, which we want to show are zero.  but this only
   follows if one of the two systems of linear equations has a unique
   solution, which is precisely when its matrix of coefficients (which are
   just word- or context- vector components) has linearly independent rows
   or columns.  so either the family of word vectors or the family of
   context vectors must be linearly independent in order for the authors
   to proceed to their conclusion.

   word vectors that are of dimension the size of the vocabulary and
   linearly independent sound to me more akin to a one-hot or bag of words
   representations than to skipgram word vectors.

skipgram isn   t matrix factorisation (yet)

   if skipgram is matrix factorisation, then it isn   t shown in this paper.
    what has been shown is that the optima of the two methods coincide
   when the dimension is larger that the size of the vocabulary.
   unfortunately, this tells us nothing about the lower dimensional case
   where skipgram is actually interesting.  in the lower dimensional case,
   the argument of the authors can   t be applied, since it is then
   impossible for the word- or context- vectors to be linearly
   independent.  it is only in the lower dimensional case that the
   skipgram and matrix factorisation are forced to compress the word
   co-occurrence information and thereby learn anything at all.  this
   compression is necessarily lossy (since there are insufficient
   parameters) and there is nothing in the paper to suggest that the two
   methods will retain the same information (which is what it means to say
   that the two methods are the same).

appendix: comparing the objectives

   to compare skipgram with negative sampling to mf, we might compare the
   two objective functions.  skipgram maximises the log likelihood l
   (above). mf, on the other hand, typically minimises the squared error
   between the matrix and its reconstruction:

   11-05-2016 5-56 pm(3)

   the partial derivatives of e , needed for a gradient update, are easy
   to compute:

   11-05-2016 5-56 pm(2)

   compare these with the partial derivatives of the skipgram log-likehood
   l , which can be computed as follows:

   11-05-2016 5-56 pm(1)

   categories[12]uncategorised, [13]uncategorized tags[14]distributional
   word representations, [15]matrix-factorisation, [16]id97

11 replies to    skipgram isn't matrix factorisation   

    1.
   [17]profarora says:
       [18]may 14, 2016 at 5:41 am
       i completely agree with this. this is also pointed out in our
       paper, which gives a different generative model explanation for
       low-dimensional id27s, and also connects this to why
       analogies can be solved by id202.
       [19]http://arxiv.org/abs/1502.03520 (accepted to appear in tacl.)
       it took a couple of rounds for referees to accept this point as
       well.
       sanjeev arora
       princeton
       [20]reply
         1.
        [21]profarora says:
            [22]may 14, 2016 at 5:37 pm
            while on the issue of widespread miconceptions here is another
            one.
            it is well-known that 300 dimensional embeddings are better
            than say 10^4 dimensional ones. why is this? the usual answer
            is    generalization, overfitting    etc. but this is formally not
            justified at all because the task being used to test goodness
            (eg analogy solving) has nothing at all to do    a priori   with
            the training objective.
            our paper above also explains this. an informal discussion
            also appears in my blog post
            [23]http://www.offconvex.org/2016/02/14/word-embeddings-2/
            [24]reply
    2.
   [25]christopher e moody (@chrisemoody) says:
       [26]may 14, 2016 at 8:51 am
       there   s also an alternative derivation which i think allows you to
       cast the sgns objective a little more independently from fitting
       it:
       [27]https://minhlab.wordpress.com/2015/06/08/a-new-proof-for-the-eq
       uivalence-of-id97-skip-gram-and-shifted-ppmi/
       [28]reply
         1.
        [29]profarora says:
            [30]may 15, 2016 at 7:16 pm
            it doesn   t seem to address the key issue raised in the blog
            post above, namely, that the id27s have to be low
            dimensional.
            [31]reply
    3.
   [32]etali says:
       [33]may 14, 2016 at 7:11 pm
       our work [34]http://www.ijcai.org/proceedings/15/papers/513.pdf
       shows that skip-gram negative sampling is a id105
       that factorizes original co-occurrence matrix with a specific loss
       function.
       [35]reply
         1.
        [36]profarora says:
            [37]may 15, 2016 at 7:15 pm
            i was wondering if the ijcai paper suffers from the same
            problem as the original levy-goldberg proof (as pointed out in
            the above blog post): it ignores the constraint that the
            embeddings have to be low-dimensional. it appears to.
            [38]reply
              1.
             [39]yitan li says:
                 [40]may 16, 2016 at 6:55 am
                 the equivalence of our proposed id105
                 model (ijcai paper) and skip-gram negative sampling
                 doesn   t involve any approximation, and the embeddings
                 from our model are low-dimensional. therefore, it doesn   t
                 suffer the same problem as the original levy-goldberg
                 proof.
                 [41]reply
    4.
   ghandzhipeng says:
       [42]june 9, 2017 at 9:04 am
       the formula in the last pic is wrong: \frac{\partial d}{\partial x}
       \log \sigma(-x) = \sigma(x), which should be -\sigma(x).
       good idea.
       [43]reply
    5.
   andrew landgraf says:
       [44]october 18, 2017 at 3:35 pm
       a colleague and i recently showed that skip gram with negative
       sampling is weighted logistic pca, which is a id105,
       assuming a binomial distribution.
       [45]https://arxiv.org/abs/1705.09755v1
       [46]reply
    6.
   carl says:
       [47]march 17, 2018 at 6:06 pm
       the claim of this article would be correct if batch gradient
       descent were used, but not if sgd is used.
       under batch gd, minimising the gradient of the id168 (l)
       with respect to w, say, which can be expressed as a sum over all
       context word vectors (c_i), has the non-trivial solution:
           where all sub-gradients dl/dx_i = 0 (i.e. for all x_i = w^t c_i),
       which is the solution we want; or
           where not all sub-gradients are zero but the sum (s) of these
       sub-gradients multiplied by the corresponding vector c_i gives the
       zero vector     which is to say the (non-zero) vector of
       sub-gradients lies in the null space of c^t. this would be a
       solution to the minimisation process, but not one we want.
       however, using (mini-batched) stochastic id119, breaks
       the sum s into random    sub-sums    of sub-gradients. in the extreme
       (i.e. a mini-batch size of 1), for each (non-zero) word vector w,
       we have dl/dx . w = 0 and so the gradient dl/dx must be zero. thus
       stochasticity (in expectation) breaks the linear combination issue
       that allows unwanted solutions, the sub-gradients do therefore tend
       to zero and thus the matrix is factorized as required (or as
       closely as achievable, subject to dimensionality vs rank   )
       [48]reply
    7.
   [49]alex gittens says:
       [50]april 16, 2018 at 2:32 pm
       another entry in the    this is what skipgram is doing    category:
       (acl 2017) [51]http://www.aclweb.org/anthology/p17-1007
       we show among other things that skipgram is computing a sufficient
       id84 factorization (a la globerson and tishby)
       of the word-word co-occurence matrix in an online fashion. this is
       assuming that you fit the skipgram model exactly, not using
       neg-sampling. it   s not clear to me how neg-sampling can be recast
       as a regularized id105 problem.
       [52]reply

leave a reply [53]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   post comment

post navigation

   [54]previous postprevious softmax parameterisation and optimisation
   [55]next postnext convergence rate of id119

   search for: ____________________ (button) search

pages

     * [56]about
     * [57]consulting
     * [58]machine learning seminar
     * [59]posts
     * [60]publications
     * [61]representation theory

recent posts

     * [62]siegelmann & sontag   s    on the computational power of neural
       nets   
     * [63]graph embeddings in hyperbolic space
     * [64]gradient optimisation on the poincar   disc
     * [65]circle circumference in the hyperbolic plane is exponential in
       the radius: proof by computer game
     * [66]hierarchical softmax

recent comments

     * benjamin on [67]hierarchical softmax
     * insoo on [68]hierarchical softmax
     * nico on [69]hierarchical softmax
     * itay on [70]hierarchical softmax
     * [71]william wolf on [72]hierarchical softmax

meta

     * [73]log in
     * [74]entries rss
     * [75]comments rss
     * [76]wordpress.org

   [77]proudly powered by wordpress

references

   1. http://building-babylon.net/feed/
   2. http://building-babylon.net/comments/feed/
   3. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/feed/
   4. http://building-babylon.net/2016/04/25/softmax-parameterisation-and-optimisation/
   5. http://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/
   6. http://building-babylon.net/wp-json/oembed/1.0/embed?url=http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/
   7. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#content
   8. http://building-babylon.net/
   9. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/
  10. http://building-babylon.net/author/benjamin/
  11. http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/levygoldberg2014.pdf
  12. http://building-babylon.net/category/uncategorised/
  13. http://building-babylon.net/category/uncategorized/
  14. http://building-babylon.net/tag/distributional-word-representations/
  15. http://building-babylon.net/tag/matrix-factorisation/
  16. http://building-babylon.net/tag/id97/
  17. http://profarora.wordpress.com/
  18. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-31
  19. http://arxiv.org/abs/1502.03520
  20. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=31#respond
  21. http://profarora.wordpress.com/
  22. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-33
  23. http://www.offconvex.org/2016/02/14/word-embeddings-2/
  24. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=33#respond
  25. http://twitter.com/chrisemoody
  26. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-32
  27. https://minhlab.wordpress.com/2015/06/08/a-new-proof-for-the-equivalence-of-id97-skip-gram-and-shifted-ppmi/
  28. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=32#respond
  29. http://profarora.wordpress.com/
  30. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-36
  31. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=36#respond
  32. http://home.ustc.edu.cn/~etali
  33. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-34
  34. http://www.ijcai.org/proceedings/15/papers/513.pdf
  35. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=34#respond
  36. http://profarora.wordpress.com/
  37. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-35
  38. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=35#respond
  39. http://home.ustc.edu.cn/~etali
  40. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-37
  41. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=37#respond
  42. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-330
  43. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=330#respond
  44. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-1015
  45. https://arxiv.org/abs/1705.09755v1
  46. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=1015#respond
  47. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-1794
  48. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=1794#respond
  49. http://www.cs.rpi.edu/~gittea
  50. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#comment-2180
  51. http://www.aclweb.org/anthology/p17-1007
  52. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/?replytocom=2180#respond
  53. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/#respond
  54. http://building-babylon.net/2016/04/25/softmax-parameterisation-and-optimisation/
  55. http://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/
  56. http://building-babylon.net/about/
  57. http://building-babylon.net/consulting/
  58. http://building-babylon.net/berlin-machine-learning-learning-group/
  59. http://building-babylon.net/posts/
  60. http://building-babylon.net/publications/
  61. http://building-babylon.net/representation-theory/
  62. http://building-babylon.net/2018/05/08/siegelmann-sontags-on-the-computational-power-of-neural-nets/
  63. http://building-babylon.net/2018/04/10/graph-embeddings-in-hyperbolic-space/
  64. http://building-babylon.net/2018/04/10/gradient-optimisation-on-the-poincare-disc/
  65. http://building-babylon.net/2018/04/10/circle-circumference-in-the-hyperbolic-plane-is-exponential-in-the-radius-a-proof-by-computer-game/
  66. http://building-babylon.net/2017/08/01/hierarchical-softmax/
  67. http://building-babylon.net/2017/08/01/hierarchical-softmax/#comment-7168
  68. http://building-babylon.net/2017/08/01/hierarchical-softmax/#comment-6694
  69. http://building-babylon.net/2017/08/01/hierarchical-softmax/#comment-4119
  70. http://building-babylon.net/2017/08/01/hierarchical-softmax/#comment-2738
  71. http://willwolf.io/
  72. http://building-babylon.net/2017/08/01/hierarchical-softmax/#comment-2509
  73. http://building-babylon.net/wp-login.php
  74. http://building-babylon.net/feed/
  75. http://building-babylon.net/comments/feed/
  76. https://wordpress.org/
  77. https://wordpress.org/
