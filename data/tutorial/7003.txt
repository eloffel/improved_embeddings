   #[1]github [2]recent commits to tangent:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]70
     * [35]star [36]1,903
     * [37]fork [38]196

[39]google/[40]tangent

   [41]code [42]issues 13 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   source-to-source debuggable derivatives in pure python
   [47]autodiff [48]automatic-differentiation [49]machine-learning
   [50]deep-learning
     * [51]68 commits
     * [52]8 branches
     * [53]5 releases
     * [54]fetching contributors
     * [55]apache-2.0

    1. [56]python 100.0%

   (button) python
   branch: master (button) new pull request
   [57]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/g
   [58]download zip

downloading...

   want to be notified of new releases in google/tangent?
   [59]sign in [60]sign up

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [63]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [64]download the github extension for visual studio
   and try again.

   (button) go back
   [65]@bartvm [66]@alexbw
   [67]bartvm and [68]alexbw [69]fix swapping of branches ([70]#83[71])
   latest commit [72]6533e83 aug 8, 2018
   [73]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [74]docs [75]restoring toolspace image nov 6, 2017
   [76]tangent [77]fix swapping of branches ([78]#83[79]) aug 8, 2018
   [80]tests [81]remove explicit dependence on tensorflow ([82]#72[83])
   may 16, 2018
   [84].gitignore
   [85].travis.yml
   [86]contributing.md
   [87]license [88]initial import. nov 1, 2017
   [89]manifest.in
   [90]readme.md
   [91]environment.yml [92]fix tf dependency to last-known working build
   ([93]#52[94]) jan 30, 2018
   [95]requirements.txt
   [96]setup.id18
   [97]setup.py [98]remove explicit dependence on tensorflow
   ([99]#72[100]) may 16, 2018

readme.md

tangent

   [101]build status [102]join the chat at
   https://gitter.im/google/tangent

   tangent is a new, free, and open-source python library for automatic
   differentiation.

   existing libraries implement automatic differentiation by tracing a
   program's execution (at runtime, like pytorch) or by staging out a
   dynamic data-flow graph and then differentiating the graph
   (ahead-of-time, like tensorflow). in contrast, tangent performs
   ahead-of-time autodiff on the python source code itself, and produces
   python source code as its output. tangent fills a unique location in
   the space of machine learning tools.

   [103]autodiff tool space

   as a result, you can finally read your automatic derivative code just
   like the rest of your program. tangent is useful to researchers and
   students who not only want to write their models in python, but also
   read and debug automatically-generated derivative code without
   sacrificing speed and flexibility.

   tangent works on a large and growing subset of python, provides extra
   autodiff features other python ml libraries don't have, has reasonable
   performance, and is compatible with tensorflow and numpy.

   this project is an experimental release, and is under active
   development. as we continue to build tangent, and respond to feedback
   from the community, there might be api changes.

usage

   note: an interactive notebook with all the code in this page can be
   found [104]here.

   tangent has a one-function api:
import tangent
df = tangent.grad(f)

   if you want to print out derivatives at the time tangent generates the
   derivative function:
import tangent
df = tangent.grad(f, verbose=1)

   here's tangent in action in the ipython console.

   [105]live derivatives with tangent

installing and running

installation

   the easiest way to install tangent is to use pip.
pip install tangent

   we'll have a conda package soon.

automatic differentiation

   under the hood, tangent.grad grabs the source code of the python
   function you pass it (using inspect.getsource, which is available in
   the python standard library), converts the source code into an abstract
   syntax tree (ast) using ast.parse (also built into the python standard
   library), and walks the syntax tree in reverse order.

   tangent has a library of recipes for the derivatives of basic
   arithmetic (+,-,/,**,*), pieces of syntax (ast.for, ast.if, ast.while)
   and tensorflow eager functions (tf.reduce_sum, tf.exp, tf.matmul, ...
   ). for each piece of syntax it encounters (for example, c = a + b is a
   single ast node ast.assign), tangent.grad looks up the matching
   backward-pass recipe, and adds it to the end of the derivative
   function. this reverse-order processing gives the technique its name:
   reverse-mode automatic differentiation.

tf eager

   tangent supports differentiating functions that use tensorflow eager
   functions that are composed together.
def f(w,x):
  h1 = tf.matmul(x,w)
  h2 = tf.tanh(h1)
  out = tf.reduce_sum(h2)
  return out

dfdw = tangent.grad(f)

   [106]sct on tf eager

subroutines

   when model code becomes long, using subroutines makes code more
   readable and reusable. tangent handles taking derivatives of models
   that have user-defined functions.

   [107]sct on subroutines

control flow

   tangent has recipes for auto-generating derivatives for code that
   contains if statements and loops:

   [108]sct on conditionals

   you'll notice above that we have to modify the user's code to keep
   track of information that we will need in the backward pass. for
   instance, we need to save which branch of an if-statement was followed
   in the forward pass, so that we run the correct branch in the backward
   pass. we save this information from the forward pass by pushing it onto
   a stack, which we then pop off in the backward pass. this is an
   important data structure in ahead-of-time autodiff.

   for loops require a little more bookkeeping. tangent has to save the
   number of iterations of the loop on the stack. also, loops usually
   overwrite the values of variables inside the loop body. in order to
   generate a correct derivative, tangent has to keep track of all of the
   overwritten values, and restore them in the backward pass in the
   correct order.

   [109]sct on loops

custom gradients

   tangent uses python's built-in machinery to introspect and transform
   the abstract syntax tree (ast) of parsed source code at runtime. for
   each piece of supported python syntax, we have implemented a rule
   indicating how to rewrite an ast node into its backward pass
   equivalent, or "adjoint". we have defined adjoints for function calls
   to numpy and tf eager methods, as well as larger pieces of syntax, such
   as if-statements and for-loops. the adjoints are stored in function
   definitions that serve as "templates", or code macros. another
   alternative, which we found too cumbersome, would be to use a
   templating engine like [110]mustache and store adjoints as plain
   strings. our templates also use a special syntax d[x] to refer to the
   derivative of a variable x.

   while differentiating a function, if tangent encounters a function
   call, it first checks if it has a gradient registered for that
   function. if not, it tries to get the function source, and generate a
   derivative ahead-of-time. but, it's easy to register your own
   gradients. here's a toy example of defining the gradient of x^3.
import tangent
from tangent.grads import adjoint

def cube(x):
  return x * x * x

# register the gradient of cube with tangent
# note! this is not a runnable function, but instead is a code template.
# tangent will replace the names of the variables `result` and `x` with whatever
# is used in your containing function.
@adjoint(cube)
def dcube(result, x):
  d[x] = d[result] * 3 * x * x

def f(val):
    cubed_val = cube(val)
    return cubed_val

print(tangent.grad(f,verbose=1))

   should output something like:
def dfdval(val, bcubed_val=1.0):
    # grad of: cubed_val = cube(val)
    bval = bcubed_val * 3 * (val * val) # <<<< this is our inlined gradient
    return bval

   the signature for the custom gradient of some function
result = orig_function(arg1,arg2)

   is
@adjoint(orig_function)
def grad_orig_function(result, arg1, arg2):
  d[arg1] = d[result]*...
  d[arg2] = d[result]*...

   the first argument to the template is always the result of the function
   call, followed by the function arguments, in order. tangent captures
   the variable names of the result and arguments, and then will use them
   to unquote the gradient template at the appropriate place in the
   backward pass.

   check out an [111]example gradient definition of a numpy function and
   [112]of a tf eager function. also, [113]see the docstring in grads.py
   for more info.

debugging

   because tangent auto-generates derivative code you can read, you can
   also easily debug your backward pass. for instance, your nn might be
   outputting nans during training, and you want to find out where the
   nans are being generated in your model. just insert a breakpoint (e.g.,
   pdb.set_trace()) at the end of your forward pass.

   [114]sct for debugging

   for large models, setting a breakpoint at the beginning of the backward
   pass and stepping through dozens of lines might be cumbersome. instead,
   you might want the breakpoint to be placed later in the derivative
   calculation. tangent lets you insert code directly into any location in
   the backward pass. first, run from tangent import insert_grad_of, then
   add a with insert_grad_of block containing the code you'd like to
   insert into the backward pass.
from tangent import insert_grad_of
def f(x):
  ...
  with insert_grad_of(x) as dx:
    print("dc/dx = %2.2f" % dx)
    pdb.set_trace()
  ...

   [115]ad hoc gradient code

derivative surgery

   you can use the insert_grad_of feature to do more than debugging and
   logging. some nn architectures benefit from tricks that directly
   manipulate the backward pass. for example, recurrent neural networks
   (id56s) suffer from the "exploding gradient" problem, where gradients
   grow exponentially. this prevents the model from training properly. a
   typical solution is to force the derivatives inside of an id56 to not
   exceed a certain value by directly clipping them. we can implement this
   with insert_grad_of.
def f(params, x):
  h = x
  for i in range(5):
    with insert_grad_of(h) as g:
      g = tf.clip_by_value(g, -1, 1)
    h = id56(params, h)
  return h

dfdparams = tangent.grad(f)

   you can perform other backward-pass tricks with insert_grad_of, such as
   stop gradients (use a break in the inlined code to stop a for loop), or
   synthetic gradients (replace a derivative with a prediction from a
   neural network). this feature lets tangent users easily debug their
   models, or quickly try out derivative tweaks in the backward pass.

forward mode

   reverse-mode autodiff, or id26, generates efficient
   derivatives for the types of functions we use in machine learning,
   where there are usually many (perhaps millions) of input variables and
   only a single output (our loss). when the inverse is true, where there
   are many more outputs than inputs, reverse mode is not an efficient
   algorithm, as it has to be run as many times as there are output
   variables. however, a less famous algorithm, forward-mode autodiff,
   only has to be run as many times as there are input variables.).
   tangent supports forward-mode autodiff.
def f(x):
  a = x * x
  b = x * a
  c = a + b
  return c

forward_df = tangent.autodiff(f, mode='forward')

   [116]sct forward mode

hessian-vector products

   although we won't dig into the technical details, forward-mode is very
   useful when combined with reverse-mode to calculate efficient
   higher-order derivatives, particularly for hessian-vector products
   (hvp) of nns. this is useful in research applications, and usually very
   painful and slow to calculate. autograd has native forward-mode
   support, while tensorflow has 3rd-party support.

   to take higher-order derivatives, you can use any combination of
   forward- and reverse-mode autodiff in tangent. this works because the
   code tangent produces can also be fed back in as input. the autodiff
   literature recommends calculating hvps in a "forward-over-reverse"
   style. this means first apply reverse mode autodiff to the function,
   and then apply forward mode to that.
def f(x):
    a = x * x * x
    b = a * x ** 2.0
    return tf.reduce_sum(b)

hvp = tangent.autodiff(tangent.autodiff(f,mode='reverse'),mode='forward')

performance

   although we did not build tangent for performance, it is competitive
   with major ml libraries. because we are generating derivatives
   ahead-of-time, there is no interpretive overhead like there is with
   runtime autodiff libraries. we implemented a few compiler optimizations
   (dead code elimination, and constant folding), but we are still working
   on extra optimization passes to further increase performance.

   [117]small benchmark

optimization

   we are often interested in the gradients of only some of the arguments.
   in this case, many of the adjoint calculation might be dead code. in
   the optimization pass this is removed. we also perform limited constant
   folding and assignment propagation.

known limitations

   tangent is still an experiment, so expect some bugs. if you report them
   to us on github, we will do our best to fix them quickly.

   we are working to add support in tangent for more aspects of the python
   language (e.g., closures, inline function definitions, classes, more
   numpy and tensorflow functions). we also hope to add more advanced
   automatic differentiation and compiler functionality in the future,
   such as automatic trade-off between memory and compute (griewank and
   walther 2000; gruslys et al., 2016), more aggressive optimizations, and
   lambda lifting.

   many of python's advanced features are difficult to statically analyze
   or to define sensible gradients of, so we restrict python to a
   functional subset (i.e. no mutable objects).

closures

   closures are currently not supported for the following reasons:
     * ad relies on being able to resolve function names. if function
       names are resolved using the enclosing function namespace, we
       cannot be sure that they will resolve to the same function at each
       call.
     * although we can access functions from the enclosing function
       namespace, we cannot write to this namespace, which is required for
       the gradients.

classes

   classes are not currently supported, but are on our near-term roadmap.
   this will enable pytorch/chainer/tfeager-style class definitions of
   neural networks, and parameterized functions, like in tf slim.

team

   tangent is developed by alex wiltschko, bart van merrienboer and dan
   moldovan.

     *    2019 github, inc.
     * [118]terms
     * [119]privacy
     * [120]security
     * [121]status
     * [122]help

     * [123]contact github
     * [124]pricing
     * [125]api
     * [126]training
     * [127]blog
     * [128]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [129]reload to refresh your
   session. you signed out in another tab or window. [130]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/google/tangent/commits/master.atom
   3. https://github.com/google/tangent#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/google/tangent
  32. https://github.com/join
  33. https://github.com/login?return_to=/google/tangent
  34. https://github.com/google/tangent/watchers
  35. https://github.com/login?return_to=/google/tangent
  36. https://github.com/google/tangent/stargazers
  37. https://github.com/login?return_to=/google/tangent
  38. https://github.com/google/tangent/network/members
  39. https://github.com/google
  40. https://github.com/google/tangent
  41. https://github.com/google/tangent
  42. https://github.com/google/tangent/issues
  43. https://github.com/google/tangent/pulls
  44. https://github.com/google/tangent/projects
  45. https://github.com/google/tangent/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/autodiff
  48. https://github.com/topics/automatic-differentiation
  49. https://github.com/topics/machine-learning
  50. https://github.com/topics/deep-learning
  51. https://github.com/google/tangent/commits/master
  52. https://github.com/google/tangent/branches
  53. https://github.com/google/tangent/releases
  54. https://github.com/google/tangent/graphs/contributors
  55. https://github.com/google/tangent/blob/master/license
  56. https://github.com/google/tangent/search?l=python
  57. https://github.com/google/tangent/find/master
  58. https://github.com/google/tangent/archive/master.zip
  59. https://github.com/login?return_to=https://github.com/google/tangent
  60. https://github.com/join?return_to=/google/tangent
  61. https://desktop.github.com/
  62. https://desktop.github.com/
  63. https://developer.apple.com/xcode/
  64. https://visualstudio.github.com/
  65. https://github.com/bartvm
  66. https://github.com/alexbw
  67. https://github.com/google/tangent/commits?author=bartvm
  68. https://github.com/google/tangent/commits?author=alexbw
  69. https://github.com/google/tangent/commit/6533e83af09de7345d1b438512679992f080dcc9
  70. https://github.com/google/tangent/pull/83
  71. https://github.com/google/tangent/commit/6533e83af09de7345d1b438512679992f080dcc9
  72. https://github.com/google/tangent/commit/6533e83af09de7345d1b438512679992f080dcc9
  73. https://github.com/google/tangent/tree/6533e83af09de7345d1b438512679992f080dcc9
  74. https://github.com/google/tangent/tree/master/docs
  75. https://github.com/google/tangent/commit/84333a435c4082e7ac1dba1741f17ba4891567f6
  76. https://github.com/google/tangent/tree/master/tangent
  77. https://github.com/google/tangent/commit/6533e83af09de7345d1b438512679992f080dcc9
  78. https://github.com/google/tangent/pull/83
  79. https://github.com/google/tangent/commit/6533e83af09de7345d1b438512679992f080dcc9
  80. https://github.com/google/tangent/tree/master/tests
  81. https://github.com/google/tangent/commit/5cf5f2eac7a6a60959db76b744e5a2ac50305968
  82. https://github.com/google/tangent/pull/72
  83. https://github.com/google/tangent/commit/5cf5f2eac7a6a60959db76b744e5a2ac50305968
  84. https://github.com/google/tangent/blob/master/.gitignore
  85. https://github.com/google/tangent/blob/master/.travis.yml
  86. https://github.com/google/tangent/blob/master/contributing.md
  87. https://github.com/google/tangent/blob/master/license
  88. https://github.com/google/tangent/commit/3b8a7adf13da0bfa22c5256a7e07138b305c784d
  89. https://github.com/google/tangent/blob/master/manifest.in
  90. https://github.com/google/tangent/blob/master/readme.md
  91. https://github.com/google/tangent/blob/master/environment.yml
  92. https://github.com/google/tangent/commit/7ca63baca68dcf6d530c9db0231594b2d36b266e
  93. https://github.com/google/tangent/pull/52
  94. https://github.com/google/tangent/commit/7ca63baca68dcf6d530c9db0231594b2d36b266e
  95. https://github.com/google/tangent/blob/master/requirements.txt
  96. https://github.com/google/tangent/blob/master/setup.id18
  97. https://github.com/google/tangent/blob/master/setup.py
  98. https://github.com/google/tangent/commit/5cf5f2eac7a6a60959db76b744e5a2ac50305968
  99. https://github.com/google/tangent/pull/72
 100. https://github.com/google/tangent/commit/5cf5f2eac7a6a60959db76b744e5a2ac50305968
 101. https://travis-ci.org/google/tangent
 102. https://gitter.im/google/tangent?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
 103. https://github.com/google/tangent/blob/master/docs/toolspace.png
 104. https://colab.research.google.com/notebook#fileid=1cjox9gtebymbnqciknmzp1uenmcwagde
 105. https://github.com/google/tangent/blob/master/docs/sct-ad-live.gif
 106. https://github.com/google/tangent/blob/master/docs/sct-ad-tf.gif
 107. https://github.com/google/tangent/blob/master/docs/sct-ad-subroutine.gif
 108. https://github.com/google/tangent/blob/master/docs/sct-ad-conditional.gif
 109. https://github.com/google/tangent/blob/master/docs/sct-ad-loop.gif
 110. https://mustache.github.io/
 111. https://github.com/google/tangent/blob/6ee7fe31e876c7a68273aeb28ecf03aae42d970d/tangent/grads.py#l261-l263
 112. https://github.com/google/tangent/blob/6ee7fe31e876c7a68273aeb28ecf03aae42d970d/tangent/tf_extensions.py#l244-l247
 113. https://github.com/google/tangent/blob/6ee7fe31e876c7a68273aeb28ecf03aae42d970d/tangent/grads.py#l14-l36
 114. https://github.com/google/tangent/blob/master/docs/sct-ad-debugging.png
 115. https://github.com/google/tangent/blob/master/docs/sct-ad-insert_grad_of.gif
 116. https://github.com/google/tangent/blob/master/docs/sct-ad-forward.gif
 117. https://github.com/google/tangent/blob/master/docs/small-benchmark.png
 118. https://github.com/site/terms
 119. https://github.com/site/privacy
 120. https://github.com/security
 121. https://githubstatus.com/
 122. https://help.github.com/
 123. https://github.com/contact
 124. https://github.com/pricing
 125. https://developer.github.com/
 126. https://training.github.com/
 127. https://github.blog/
 128. https://github.com/about
 129. https://github.com/google/tangent
 130. https://github.com/google/tangent

   hidden links:
 132. https://github.com/
 133. https://github.com/google/tangent
 134. https://github.com/google/tangent
 135. https://github.com/google/tangent
 136. https://help.github.com/articles/which-remote-url-should-i-use
 137. https://github.com/google/tangent#tangent
 138. https://github.com/google/tangent#usage
 139. https://github.com/google/tangent#installing-and-running
 140. https://github.com/google/tangent#installation
 141. https://github.com/google/tangent#automatic-differentiation
 142. https://github.com/google/tangent#tf-eager
 143. https://github.com/google/tangent#subroutines
 144. https://github.com/google/tangent#control-flow
 145. https://github.com/google/tangent#custom-gradients
 146. https://github.com/google/tangent#debugging
 147. https://github.com/google/tangent#derivative-surgery
 148. https://github.com/google/tangent#forward-mode
 149. https://github.com/google/tangent#hessian-vector-products
 150. https://github.com/google/tangent#performance
 151. https://github.com/google/tangent#optimization
 152. https://github.com/google/tangent#known-limitations
 153. https://github.com/google/tangent#closures
 154. https://github.com/google/tangent#classes
 155. https://github.com/google/tangent#team
 156. https://github.com/
