   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning, nlp: text classification using scikit-learn, python
and nltk.

   [16]go to the profile of javed shaikh
   [17]javed shaikh (button) blockedunblock (button) followfollowing
   jul 23, 2017
   [1*ljcbykajunvazcupywm4_a.png]
   text classification

   latest update:
   i have uploaded the complete code (python and jupyter notebook) on
   github: [18]https://github.com/javedsha/text-classification

   document/text classification is one of the important and typical task
   in supervised machine learning (ml). assigning categories to documents,
   which can be a web page, library book, media articles, gallery etc. has
   many applications like e.g. spam filtering, email routing, sentiment
   analysis etc. in this article, i would like to demonstrate how we can
   do text classification using python, scikit-learn and little bit of
   nltk.

   disclaimer: i am new to machine learning and also to blogging (first).
   so, if there are any mistakes, please do let me know. all feedback
   appreciated.

   let   s divide the classification problem into below steps:
    1. prerequisite and setting up the environment.
    2. loading the data set in jupyter.
    3. extracting features from text files.
    4. running ml algorithms.
    5. grid search for parameter tuning.
    6. useful tips and a touch of nltk.

step 1: prerequisite and setting up the environment

   the prerequisites to follow this example are python version 2.7.3 and
   jupyter notebook. you can just install [19]anaconda and it will get
   everything for you. also, little bit of python and ml basics including
   text classification is required. we will be using scikit-learn (python)
   libraries for our example.

step 2: loading the data set in jupyter.

   the data set will be using for this example is the famous    20 newsgoup   
   data set. about the data from the original [20]website:

     the 20 newsgroups data set is a collection of approximately 20,000
     newsgroup documents, partitioned (nearly) evenly across 20 different
     newsgroups. to the best of my knowledge, it was originally collected
     by ken lang, probably for his [21]newsweeder: learning to filter
     netnews paper, though he does not explicitly mention this
     collection. the 20 newsgroups collection has become a popular data
     set for experiments in text applications of machine learning
     techniques, such as text classification and text id91.

   this data set is in-built in scikit, so we don   t need to download it
   explicitly.

   i. open command prompt in windows and type    jupyter notebook   . this
   will open the notebook in browser and start a session for you.

   ii. select new > python 2. you can give a name to the notebook - text
   classification demo 1
   [1*21fhnsymi_hczv25rkeema.png]

   iii. loading the data set: (this might take few minutes, so patience)
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train', shuffle=true)

   note: above, we are only loading the training data. we will load the
   test data separately later in the example.

   iv. you can check the target names (categories) and some data files by
   following commands.
twenty_train.target_names #prints all the categories
print("\n".join(twenty_train.data[0].split("\n")[:3])) #prints first line of the
 first data file

step 3: extracting features from text files.

   text files are actually series of words (ordered). in order to run
   machine learning algorithms we need to convert the text files into
   numerical feature vectors. we will be using [22]id159 for
   our example. briefly, we segment each text file into words (for english
   splitting by space), and count # of times each word occurs in each
   document and finally assign each word an integer id. each unique word
   in our dictionary will correspond to a feature (descriptive feature).

   scikit-learn has a high level component which will create feature
   vectors for us    countvectorizer   . more about it [23]here.
from sklearn.feature_extraction.text import countvectorizer
count_vect = countvectorizer()
x_train_counts = count_vect.fit_transform(twenty_train.data)
x_train_counts.shape

   here by doing    count_vect.fit_transform(twenty_train.data)   , we are
   learning the vocabulary dictionary and it returns a document-term
   matrix. [n_samples, n_features].

   tf: just counting the number of words in each document has 1 issue: it
   will give more weightage to longer documents than shorter documents. to
   avoid this, we can use frequency (tf - term frequencies) i.e.
   #count(word) / #total words, in each document.

   tf-idf: finally, we can even reduce the weightage of more common words
   like (the, is, an etc.) which occurs in all document. this is called as
   tf-idf i.e term frequency times inverse document frequency.

   we can achieve both using below line of code:
from sklearn.feature_extraction.text import tfidftransformer
tfidf_transformer = tfidftransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)
x_train_tfidf.shape

   the last line will output the dimension of the document-term matrix ->
   (11314, 130107).

step 4. running ml algorithms.

   there are various algorithms which can be used for text classification.
   we will start with the most simplest one    [24]naive bayes (nb)    (don   t
   think it is too naive!     )

   you can easily build a nbclassifier in scikit using below 2 lines of
   code: (note - there are many variants of nb, but discussion about them
   is out of scope)
from sklearn.naive_bayes import multinomialnb
clf = multinomialnb().fit(x_train_tfidf, twenty_train.target)

   this will train the nb classifier on the training data we provided.

   building a pipeline: we can write less code and do all of the above, by
   building a pipeline as follows:
>>> from sklearn.pipeline import pipeline
>>> text_clf = pipeline([('vect', countvectorizer()),
...                      ('tfidf', tfidftransformer()),
...                      ('clf', multinomialnb()),
... ])
text_clf = text_clf.fit(twenty_train.data, twenty_train.target)

   the names    vect    ,    tfidf    and    clf    are arbitrary but will be used
   later.

   performance of nb classifier: now we will test the performance of the
   nb classifier on test set.
import numpy as np
twenty_test = fetch_20newsgroups(subset='test', shuffle=true)
predicted = text_clf.predict(twenty_test.data)
np.mean(predicted == twenty_test.target)

   the accuracy we get is ~77.38%, which is not bad for start and for a
   naive classifier. also, congrats!!! you have now written successfully a
   text classification algorithm     

   support vector machines (id166): let   s try using a different algorithm
   id166, and see if we can get any better performance. more about it
   [25]here.
>>> from sklearn.linear_model import sgdclassifier
>>> text_clf_id166 = pipeline([('vect', countvectorizer()),
...                      ('tfidf', tfidftransformer()),
...                      ('clf-id166', sgdclassifier(loss='hinge', penalty='l2',
...                                            alpha=1e-3, n_iter=5, random_stat
e=42)),
... ])
>>> _ = text_clf_id166.fit(twenty_train.data, twenty_train.target)
>>> predicted_id166 = text_clf_id166.predict(twenty_test.data)
>>> np.mean(predicted_id166 == twenty_test.target)

   the accuracy we get is~82.38%. yipee, a little better     

step 5. grid search

   almost all the classifiers will have various parameters which can be
   tuned to obtain optimal performance. scikit gives an extremely useful
   tool    gridsearchcv   .
>>> from sklearn.model_selection import gridsearchcv
>>> parameters = {'vect__ngram_range': [(1, 1), (1, 2)],
...               'tfidf__use_idf': (true, false),
...               'clf__alpha': (1e-2, 1e-3),
... }

   here, we are creating a list of parameters for which we would like to
   do performance tuning. all the parameters name start with the
   classifier name (remember the arbitrary name we gave). e.g.
   vect__ngram_range; here we are telling to use unigram and bigrams and
   choose the one which is optimal.

   next, we create an instance of the grid search by passing the
   classifier, parameters and n_jobs=-1 which tells to use multiple cores
   from user machine.
gs_clf = gridsearchcv(text_clf, parameters, n_jobs=-1)
gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)

   this might take few minutes to run depending on the machine
   configuration.

   lastly, to see the best mean score and the params, run the following
   code:
gs_clf.best_score_
gs_clf.best_params_

   the accuracy has now increased to ~90.6% for the nb classifier (not so
   naive anymore!     ) and the corresponding parameters are {   clf__alpha   :
   0.01,    tfidf__use_idf   : true,    vect__ngram_range   : (1, 2)}.

   similarly, we get improved accuracy ~89.79% for id166 classifier with
   below code. note: you can further optimize the id166 classifier by tuning
   other parameters. this is left up to you to explore more.
>>> from sklearn.model_selection import gridsearchcv
>>> parameters_id166 = {'vect__ngram_range': [(1, 1), (1, 2)],
...               'tfidf__use_idf': (true, false),
...               'clf-id166__alpha': (1e-2, 1e-3),
... }
gs_clf_id166 = gridsearchcv(text_clf_id166, parameters_id166, n_jobs=-1)
gs_clf_id166 = gs_clf_id166.fit(twenty_train.data, twenty_train.target)
gs_clf_id166.best_score_
gs_clf_id166.best_params_

step 6: useful tips and a touch of nltk.

    1. removing [26]stop words: (the, then etc) from the data. you should
       do this only when stop words are not useful for the underlying
       problem. in most of the text classification problems, this is
       indeed not useful. let   s see if removing stop words increases the
       accuracy. update the code for creating object of countvectorizer as
       follows:

>>> from sklearn.pipeline import pipeline
>>> text_clf = pipeline([('vect', countvectorizer(stop_words='english')),
...                      ('tfidf', tfidftransformer()),
...                      ('clf', multinomialnb()),
... ])

   this is the pipeline we build for nb classifier. run the remaining
   steps like before. this improves the accuracy from 77.38% to 81.69%
   (that is too good). you can try the same for id166 and also while doing
   grid search.

   2. fitprior=false: when set to false for [27]multinomialnb, a uniform
   prior will be used. this doesn   t helps that much, but increases the
   accuracy from 81.69% to 82.14% (not much gain). try and see if this
   works for your data set.

   3. id30: from [28]wikipedia, id30 is the process of reducing
   inflected (or sometimes derived) words to their word stem, base or root
   form. e.g. a id30 algorithm reduces the words    fishing   ,    fished   ,
   and    fisher    to the root word,    fish   .

   we need nltk which can be installed from [29]here. nltk comes with
   various stemmers (details on how stemmers work are out of scope for
   this article) which can help reducing the words to their root form.
   again use this, if it make sense for your problem.

   below i have used snowball stemmer which works very well for english
   language.
import nltk
nltk.download()
from nltk.stem.snowball import snowballstemmer
stemmer = snowballstemmer("english", ignore_stopwords=true)
class stemmedcountvectorizer(countvectorizer):
    def build_analyzer(self):
        analyzer = super(stemmedcountvectorizer, self).build_analyzer()
        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])
stemmed_count_vect = stemmedcountvectorizer(stop_words='english')
text_mnb_stemmed = pipeline([('vect', stemmed_count_vect),
...                      ('tfidf', tfidftransformer()),
...                      ('mnb', multinomialnb(fit_prior=false)),
... ])
text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)
predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)
np.mean(predicted_mnb_stemmed == twenty_test.target)

   the accuracy with id30 we get is ~81.67%. marginal improvement in
   our case with nb classifier. you can also try out with id166 and other
   algorithms.

   conclusion: we have learned the classic problem in nlp, text
   classification. we learned about important concepts like bag of words,
   tf-idf and 2 important algorithms nb and id166. we saw that for our data
   set, both the algorithms were almost equally matched when optimized.
   sometimes, if we have enough data set, choice of algorithm can make
   hardly any difference. we also saw, how to perform grid search for
   performance tuning and used nltk id30 approach. you can use this
   code on your data set and see which algorithms works best for you.

   update: if anyone tries a different algorithm, please share the results
   in the comment section, it will be useful for everyone.

   please let me know if there were any mistakes and feedback is welcome
         

   recommend, comment, share if you liked this article.

references:

   [30]http://scikit-learn.org/ (code)

   [31]http://qwone.com/~jason/20newsgroups/ (data set)

   [32]some rights reserved
     * [33]machine learning
     * [34]python
     * [35]nltk
     * [36]classification
     * [37]nlp

   (button)
   (button)
   (button) 5.2k claps
   (button) (button) (button) 63 (button) (button)

     (button) blockedunblock (button) followfollowing
   [38]go to the profile of javed shaikh

[39]javed shaikh

   building better future inspires me. personal opinions.

     (button) follow
   [40]towards data science

[41]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 5.2k
     * (button)
     *
     *

   [42]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [43]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c52b92a7c73a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ufkxcll0d3vj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@_singularity?source=post_header_lockup
  17. https://towardsdatascience.com/@_singularity
  18. https://github.com/javedsha/text-classification
  19. https://www.continuum.io/downloads
  20. http://qwone.com/~jason/20newsgroups/
  21. http://qwone.com/~jason/20newsgroups/lang95.bib
  22. https://en.wikipedia.org/wiki/bag-of-words_model
  23. http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.countvectorizer.html
  24. http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes
  25. http://scikit-learn.org/stable/modules/id166.html
  26. https://en.wikipedia.org/wiki/stop_words
  27. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.multinomialnb.html
  28. https://en.wikipedia.org/wiki/id30
  29. http://www.nltk.org/
  30. http://scikit-learn.org/
  31. http://qwone.com/~jason/20newsgroups/
  32. http://creativecommons.org/licenses/by/4.0/
  33. https://towardsdatascience.com/tagged/machine-learning?source=post
  34. https://towardsdatascience.com/tagged/python?source=post
  35. https://towardsdatascience.com/tagged/nltk?source=post
  36. https://towardsdatascience.com/tagged/classification?source=post
  37. https://towardsdatascience.com/tagged/nlp?source=post
  38. https://towardsdatascience.com/@_singularity?source=footer_card
  39. https://towardsdatascience.com/@_singularity
  40. https://towardsdatascience.com/?source=footer_card
  41. https://towardsdatascience.com/?source=footer_card
  42. https://towardsdatascience.com/
  43. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  45. https://medium.com/p/c52b92a7c73a/share/twitter
  46. https://medium.com/p/c52b92a7c73a/share/facebook
  47. https://medium.com/p/c52b92a7c73a/share/twitter
  48. https://medium.com/p/c52b92a7c73a/share/facebook
