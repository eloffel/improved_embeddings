   #[1]fastml

                                   [2]fastml

machine learning made easy

     * [3]rss

   ____________________

     * [4]home
     * [5]contents
     * [6]popular
     * [7]links
     * [8]backgrounds
     * [9]about

deep learning architecture diagrams

   2016-09-28

   as a wild stream after a wet season in african savanna diverges into
   many smaller streams forming lakes and puddles, so deep learning has
   diverged into a myriad of specialized architectures. each architecture
   has a diagram. here are some of them.

   neural networks are conceptually simple, and that   s their beauty. a
   bunch of homogenous, uniform units, arranged in layers, weighted
   connections between them, and that   s all. at least in theory. practice
   turned out to be a bit different. instead of feature engineering, we
   now have [10]architecture engineering, as described by stephen merrity:

     the romanticized description of deep learning usually promises that
     the days of hand crafted feature engineering are gone - that the
     models are advanced enough to work this out themselves. like most
     advertising, this is simultaneously true and misleading.

     whilst deep learning has simplified feature engineering in many
     cases, it certainly hasn   t removed it. as feature engineering has
     decreased, the architectures of the machine learning models
     themselves have become increasingly more complex. most of the time,
     these model architectures are as specific to a given task as feature
     engineering used to be.

     to clarify, this is still an important step. architecture
     engineering is more general than feature engineering and provides
     many new opportunities. having said that, however, we shouldn   t be
     oblivious to the fact that where we are is still far from where we
     intended to be.

   not quite as bad as doings of [11]architecture astronauts, but not too
   good either.

   kardash chair
   an example of architecture specific to a given task

lstm diagrams

   how to explain those architectures? naturally, with a diagram. a
   diagram will make it all crystal clear.

   let   s first inspect the two most popular types of networks these days,
   id98 and lstm. you   ve already seen a [12]convnet diagram, so turning to
   the iconic lstm:

   lstm

   it   s easy, just take a closer look:

   lstm

   as they say, in mathematics you don   t understand things, you just get
   used to them.

   fortunately, there are good explanations, for example [13]understanding
   id137 and [14]written memories: understanding, deriving and
   extending the lstm.

   lstm still too complex? let   s try a simplified version, gru (gated
   recurrent unit). trivial, really.

   lstm

   especially this one, called minimal gru.

   minimal gru

more diagrams

   various modifications of lstm are now common. here   s one, called deep
   bidirectional lstm:

   db-lstm
   db-lstm, [15]pdf

   db-lstm

   the rest are pretty self-explanatory, too. let   s start with a
   combination of id98 and lstm, since you have both under your belt now:

   crmn
   convolutional residual memory network, [16]1606.05262

   dntm
   dynamic ntm, [17]1607.00036

   entm
   evolvable id63s, [18]pdf

   unsupervised id20 by id26
   unsupervised id20 by id26, [19]1409.7495

   deeply recursive id98 for super resolution
   deeply recursive id98 for image super-resolution, [20]1511.04491

   recurrent model of visual attention
   recurrent model of visual attention, [21]1406.6247

   this diagram of multilayer id88 with synthetic gradients scores
   high on clarity:

   synthetic gradients
   mlp with synthetic gradients, [22]1608.05343

   drinking cat

   every day brings more. here   s a fresh one, again from google:

   google's id4 system
   google   s id4 system, [23]1609.08144

and now for something completely different

   drawings from the [24]neural network zoo are pleasantly simple, but,
   unfortunately, serve mostly as eye candy. for example:

   esm, esn and elm

   these look like not-fully-connected id88s, but are supposed to
   represent a liquid state machine, an echo state network, and an extreme
   learning machine.

   how does lsm differ from esn? that   s easy, it has green neuron with
   triangles. but how does esn differ from elm? both have blue neurons.

   seriously, while similar, esn is a recurrent network and elm is not.
   and this kind of thing should probably be visible in an architecture
   diagram.

p.s.

   you haven   t seen anything till you   ve seen a [25]neural [26]compiler:

   the input of the compiler is a pascal program.

   the compiler produces a neural network that computes what is specified
   by the pascal program.

   the compiler generates an intermediate code called cellular code.

   weird, huh?

   [27][ezoic.png] report this ad

   posted by zygmunt z. 2016-09-28 [28]basics, [29]neural-networks

   [30]tweet

   [31]   factorized convolutional neural networks, aka separable
   convolutions [32]piping in r and in pandas   

                                    comments

   please enable javascript to view the [33]comments powered by disqus.

   [34][ezoic.png] report this ad

                                  recent posts

     * [35]one weird regularity of the stock market
     * [36]classifying time series using feature extraction
     * [37]google's principles on ai weapons, mass surveillence, and
       signing out
     * [38]how to use the python debugger
     * [39]preparing continuous features for neural networks with
       gaussrank
     * [40]two faces of overfitting
     * [41]goodbooks-10k: a new dataset for book recommendations

                                    twitter

   follow [42]@fastml for notifications about new posts.
     * status updating...

   [43]follow @fastml
   also check out [44]@fastml_extra for things related to machine learning
   and data science in general.

                                     github

   most articles come with some [45]code. we push it to github.
   [46]https://github.com/zygmuntz [47][ezoic.png] report this ad

   copyright    2019 - zygmunt z. - powered by [48]octopress

   [p?c1=2&c2=20015427&cv=2.0&cj=1]

   quantcast

references

   1. http://fastml.com/atom.xml
   2. http://fastml.com/
   3. http://fastml.com/atom.xml
   4. http://fastml.com/
   5. http://fastml.com/contents/
   6. http://fastml.com/popular/
   7. http://fastml.com/links/
   8. http://fastml.com/backgrounds/
   9. http://fastml.com/about/
  10. http://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html
  11. http://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html
  12. http://fastml.com/object-recognition-in-images-with-cuda-convnet/#architecture
  13. http://colah.github.io/posts/2015-08-understanding-lstms/
  14. http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
  15. https://pdfs.semanticscholar.org/c34e/41312b47f60986458759d5cc546c2b53f748.pdf
  16. https://arxiv.org/abs/1606.05262
  17. http://arxiv.org/abs/1607.00036
  18. http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdf
  19. https://arxiv.org/abs/1409.7495
  20. http://arxiv.org/abs/1511.04491
  21. https://arxiv.org/abs/1406.6247
  22. https://arxiv.org/abs/1608.05343
  23. http://arxiv.org/abs/1609.08144
  24. http://www.asimovinstitute.org/neural-network-zoo/
  25. https://twitter.com/notmisha/status/793619497118752768
  26. http://www.sciencedirect.com/science/article/pii/0304397594002003
  27. https://www.ezoic.com/what-is-ezoic/
  28. http://fastml.com/blog/categories/basics/
  29. http://fastml.com/blog/categories/neural-networks/
  30. http://twitter.com/share
  31. http://fastml.com/factorized-convolutional-neural-networks/
  32. http://fastml.com/piping-in-r-and-in-pandas/
  33. http://disqus.com/?ref_noscript
  34. https://www.ezoic.com/what-is-ezoic/
  35. http://fastml.com/one-weird-regularity-of-the-stock-market-intraday-vs-overnight-returns/
  36. http://fastml.com/classifying-time-series-using-feature-extraction/
  37. http://fastml.com/googles-principles-on-ai-weapons-mass-surveillence-and-signing-out/
  38. http://fastml.com/how-to-use-the-python-debugger/
  39. http://fastml.com/preparing-continuous-features-for-neural-networks-with-rankgauss/
  40. http://fastml.com/two-faces-of-overfitting/
  41. http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/
  42. http://twitter.com/fastml
  43. http://twitter.com/fastml
  44. https://twitter.com/fastml_extra
  45. http://fastml.com/blog/categories/code/
  46. https://github.com/zygmuntz
  47. https://www.ezoic.com/what-is-ezoic/
  48. http://octopress.org/
