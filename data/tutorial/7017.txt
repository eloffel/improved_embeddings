   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ai   | theory, practice, business
     * [9]theory
     * [10]practice
     * [11]business
     __________________________________________________________________

understanding hinton   s id22. part i: intuition.

   [12]go to the profile of max pechyonkin
   [13]max pechyonkin (button) blockedunblock (button) followfollowing
   nov 2, 2017

part of understanding hinton   s id22 series:

   part i: [14]intuition (you are reading it now)
   part ii: [15]how capsules work
   part iii: [16]dynamic routing between capsules
   part iv: [17]capsnet architecture
     __________________________________________________________________

   quick announcement about our new publication [18]ai  . we are getting
   the best writers together to talk about the theory, practice, and
   business of ai and machine learning. follow it to stay up to date on
   the latest trends.
     __________________________________________________________________

1. introduction

   last week, geoffrey hinton and his team published [19]two [20]papers
   that introduced a completely new type of neural network based on
   so-called capsules. in addition to that, the team published an
   algorithm, called dynamic routing between capsules, that allows to
   train such a network.
   [1*a3hl_0cj3zp3xvknc-lavg.jpeg]
   geoffrey hinton has spent decades thinking about capsules. [21]source.

   for everyone in the deep learning community, this is huge news, and for
   several reasons. first of all, hinton is one of the founders of deep
   learning and an inventor of numerous models and algorithms that are
   widely used today. secondly, these papers introduce something
   completely new, and this is very exciting because it will most likely
   stimulate additional wave of research and very cool applications.

   in this post, i will explain why this new architecture is so important,
   as well as intuition behind it. in the following posts i will dive into
   technical details.

   however, before talking about capsules, we need to have a look at id98s,
   which are the workhorse of today   s deep learning.
   [1*uitegzy1i9nk6hl1u4hpyg.png]
   architecture of capsnet from the [22]original paper.

2. id98s have important drawbacks

   id98s ([23]convolutional neural networks) are awesome. they are one of
   the reasons deep learning is so popular today. they can do [24]amazing
   things that people used to think computers would not be capable of
   doing for a long, long time. nonetheless, they have their limits and
   they have fundamental drawbacks.

   let us consider a very simple and non-technical example. imagine a
   face. what are the components? we have the face oval, two eyes, a nose
   and a mouth. for a id98, a mere presence of these objects can be a very
   strong indicator to consider that there is a face in the image.
   orientational and relative spatial relationships between these
   components are not very important to a id98.
   [1*ptu8cbna_mzrbth6ia87ha.png]
   to a id98, both pictures are similar, since they both contain similar
   elements. [25]source.

   how do id98s work? the main component of a id98 is a convolutional layer.
   its job is to detect important features in the image pixels. layers
   that are deeper (closer to the input) will learn to detect simple
   features such as [26]edges and color gradients, whereas higher layers
   will combine simple features into more complex features. finally, dense
   layers at the top of the network will combine very high level features
   and produce classification predictions.

   an important thing to understand is that higher-level features combine
   lower-level features as a weighted sum: activations of a preceding
   layer are multiplied by the following layer neuron   s weights and added,
   before being passed to activation nonlinearity. nowhere in this setup
   there is pose (translational and rotational) relationship between
   simpler features that make up a higher level feature. id98 approach to
   solve this issue is to use max pooling or successive convolutional
   layers that reduce spacial size of the data flowing through the network
   and therefore increase the    field of view    of higher layer   s neurons,
   thus allowing them to detect higher order features in a larger region
   of the input image. max pooling is a crutch that made convolutional
   networks work surprisingly well, achieving [27]superhuman performance
   in many areas. but do not be fooled by its performance: while id98s work
   better than any model before them, max pooling nonetheless is losing
   valuable information.

   hinton himself stated that the fact that max pooling is working so well
   is a [28]big mistake and a disaster:

     hinton:    the pooling operation used in convolutional neural networks
     is a big mistake and the fact that it works so well is a disaster.   

   of course, you can do away with max pooling and still get good results
   with traditional id98s, but they still do not solve the key problem:

     internal data representation of a convolutional neural network does
     not take into account important spatial hierarchies between simple
     and complex objects.

   in the example above, a mere presence of 2 eyes, a mouth and a nose in
   a picture does not mean there is a face, we also need to know how these
   objects are oriented relative to each other.

3. hardcoding 3d world into a neural net: inverse graphics approach

   computer graphics deals with constructing a visual image from some
   [29]internal hierarchical representation of geometric data. note that
   the structure of this representation needs to take into account
   relative positions of objects. that internal representation is stored
   in computer   s memory as arrays of geometrical objects and matrices that
   represent relative positions and orientation of these objects. then,
   special software takes that representation and converts it into an
   image on the screen. this is called [30]rendering.
   [1*p4mw96kn__knrojz-gcvaa.png]
   computer graphics takes internal representation of objects and produces
   an image. human brain does the opposite. id22 follow a
   similar approach to the brain. [31]source.

   inspired by this idea, hinton argues that brains, in fact, do the
   opposite of rendering. he calls it [32]inverse [33]graphics: from
   visual information received by eyes, they deconstruct a hierarchical
   representation of the world around us and try to match it with already
   learned patterns and relationships stored in the brain. this is how
   recognition happens. and the key idea is that representation of objects
   in the brain does not depend on view angle.

   so at this point the question is: how do we model these hierarchical
   relationships inside of a neural network? the answer comes from
   computer graphics. in 3d graphics, relationships between 3d objects can
   be represented by a so-called pose, which is in essence [34]translation
   plus [35]rotation.

   hinton argues that in order to correctly do classification and object
   recognition, it is important to preserve hierarchical pose
   relationships between object parts. this is the key intuition that will
   allow you to understand why capsule theory is so important. it
   incorporates relative relationships between objects and it is
   represented numerically as a 4d [36]pose matrix.

   when these relationships are built into internal representation of
   data, it becomes very easy for a model to understand that the thing
   that it sees is just another view of something that it has seen before.
   consider the image below. you can easily recognize that this is the
   statue of liberty, even though all the images show it from different
   angles. this is because internal representation of the statue of
   liberty in your brain does not depend on the view angle. you have
   probably never seen these exact pictures of it, but you still
   immediately knew what it was.
   [1*nujl2w-yvhpixykzpixmka.jpeg]
   your brain can easily recognize this is the same object, even though
   all photos are taken from different angles. id98s do not have this
   capability.

   for a id98, this task is really hard because it does not have this
   built-in understanding of 3d space, but for a capsnet it is much easier
   because these relationships are explicitly modeled. the paper that uses
   this approach was able to [37]cut error rate by 45% as compared to the
   previous state of the art, which is a huge improvement.

   another benefit of the capsule approach is that it is capable of
   learning to achieve state-of-the art performance by only using a
   fraction of the data that a id98 would use (hinton mentions this in his
   famous talk about [38]what is wrongs with id98s). in this sense, the
   capsule theory is much closer to what the human brain does in practice.
   in order to learn to tell digits apart, the human brain needs to see
   only a couple of dozens of examples, hundreds at most. id98s, on the
   other hand, need tens of thousands of examples to achieve very good
   performance, which seems like a brute force approach that is clearly
   inferior to what we do with our brains.

4. what took it so long?

   the idea is really simple, there is no way no one has come up with it
   before! and the truth is, hinton has been thinking about this for
   decades. the reason why there were no publications is simply because
   there was no technical way to make it work before. one of the reasons
   is that computers were just not powerful enough in the pre-gpu-based
   era before around 2012. another reason is that there was no algorithm
   that allowed to implement and successfully learn a capsule network (in
   the same fashion the idea of artificial neurons was around since
   1940-s, but it was not until mid 1980-s when [39]id26
   algorithm showed up and allowed to successfully train deep networks).

   in the same fashion, the idea of capsules itself is not that new and
   hinton has mentioned it before, but there was no algorithm up until now
   to make it work. this algorithm is called    dynamic routing between
   capsules   . this algorithm allows capsules to communicate with each
   other and create representations similar to [40]scene graphs in
   computer graphics.
   [1*8h3mqh4jajcjf00cpdzpma.png]
   the capsule network is much better than other models at telling that
   images in top and bottom rows belong to the same classes, only the view
   angle is different. the latest papers decreased the error rate by a
   whopping 45%. [41]source.

5. conclusion

   capsules introduce a new building block that can be used in deep
   learning to better model hierarchical relationships inside of internal
   id99 of a neural network. intuition behind them is
   very simple and elegant.

   hinton and his team proposed a way to train such a network made up of
   capsules and successfully trained it on a simple data set, achieving
   state-of-the-art performance. this is very encouraging.

   nonetheless, there are challenges. [42]current [43]implementations are
   much slower than other modern deep learning models. time will show if
   id22 can be trained quickly and efficiently. in addition,
   we need to see if they work well on more difficult data sets and in
   different domains.

   in any case, the capsule network is a very interesting and already
   working model which will definitely get more developed over time and
   contribute to further expansion of deep learning application domain.

   this concludes part one of the series on id22. in the
   [44]part ii, more technical part, i will walk you through the capsnet   s
   internal workings step by step.
     __________________________________________________________________

thanks for reading! if you enjoyed it, hit that clap button below and
[45]subscribe to updates on [46]my website! it would mean a lot to me and
encourage me to write more stories like this.

   you can [47]follow me on twitter. let   s also connect on [48]linkedin.

     * [49]machine learning
     * [50]deep learning
     * [51]geoffrey hinton
     * [52]artificial intelligence
     * [53]theory

   (button)
   (button)
   (button) 27k claps
   (button) (button) (button) 77 (button) (button)

     (button) blockedunblock (button) followfollowing
   [54]go to the profile of max pechyonkin

[55]max pechyonkin

   [56]https://pechyonkin.me/

     (button) follow
   [57]ai   | theory, practice, business

[58]ai   | theory, practice, business

   the ai revolution is here! navigate the ever changing industry with our
   thoughtfully written articles whether your a researcher, engineer, or
   entrepreneur

     * (button)
       (button) 27k
     * (button)
     *
     *

   [59]ai   | theory, practice, business
   never miss a story from ai   | theory, practice, business, when you sign
   up for medium. [60]learn more
   never miss a story from ai   | theory, practice, business
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b4b559d1159b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/ai%c2%b3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/ai%c2%b3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b&source=--------------------------nav_reg&operation=register
   8. https://medium.com/ai  -theory-practice-business?source=logo-lo_z2ceo9z20knz---ab837e78463f
   9. https://medium.com/ai  -theory-practice-business/tagged/theory
  10. https://medium.com/ai  -theory-practice-business/tagged/practice
  11. https://medium.com/ai  -theory-practice-business/tagged/business
  12. https://medium.com/@pechyonkin?source=post_header_lockup
  13. https://medium.com/@pechyonkin
  14. https://pechyonkin.me/capsules-1/
  15. https://pechyonkin.me/capsules-2/
  16. https://pechyonkin.me/capsules-3/
  17. https://pechyonkin.me/capsules-4/
  18. https://medium.com/ai  -theory-practice-business?source=logo-bc670b9a1aca---ab837e78463f
  19. https://arxiv.org/abs/1710.09829
  20. https://openreview.net/pdf?id=hjwlfgwrb
  21. http://condo.ca/wp-content/uploads/2017/03/vector-director-institute-artificial-intelligence-toronto-mars-discovery-district-hinton-google-apple-siri-alexa-condo.ca_.jpg
  22. https://arxiv.org/abs/1710.09829
  23. https://en.wikipedia.org/wiki/convolutional_neural_network
  24. http://www.yaronhadad.com/deep-learning-most-amazing-applications/
  25. http://sharenoesis.com/wp-content/uploads/2010/05/7shapefaceremoveguides.jpg
  26. https://arxiv.org/abs/1312.6199
  27. https://www.eetimes.com/document.asp?doc_id=1325712
  28. https://www.reddit.com/r/machinelearning/comments/2lmo0l/ama_geoffrey_hinton/clyj4jv/
  29. https://en.wikipedia.org/wiki/3d_computer_graphics
  30. https://en.wikipedia.org/wiki/rendering_(computer_graphics)
  31. https://upload.wikimedia.org/wikipedia/commons/a/ad/utah_teapot.png
  32. https://youtu.be/tfimqt0yt2i
  33. http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10754.pdf
  34. https://en.wikipedia.org/wiki/translation_(geometry)
  35. https://en.wikipedia.org/wiki/rotation_(mathematics)
  36. http://homepages.inf.ed.ac.uk/rbf/cvonline/local_copies/marble/high/pose/express.htm
  37. https://openreview.net/pdf?id=hjwlfgwrb
  38. https://youtu.be/rtawfwuvnle
  39. https://en.wikipedia.org/wiki/id26
  40. https://en.wikipedia.org/wiki/scene_graph
  41. https://openreview.net/pdf?id=hjwlfgwrb
  42. https://github.com/llsourcell/capsule_networks
  43. https://github.com/xifengguo/capsnet-keras
  44. https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66
  45. https://pechyonkin.me/subscribe/
  46. https://pechyonkin.me/
  47. https://twitter.com/max_pechyonkin
  48. https://www.linkedin.com/in/maxim-pechyonkin-phd/
  49. https://medium.com/tag/machine-learning?source=post
  50. https://medium.com/tag/deep-learning?source=post
  51. https://medium.com/tag/geoffrey-hinton?source=post
  52. https://medium.com/tag/artificial-intelligence?source=post
  53. https://medium.com/tag/theory?source=post
  54. https://medium.com/@pechyonkin?source=footer_card
  55. https://medium.com/@pechyonkin
  56. https://pechyonkin.me/
  57. https://medium.com/ai  -theory-practice-business?source=footer_card
  58. https://medium.com/ai  -theory-practice-business?source=footer_card
  59. https://medium.com/ai  -theory-practice-business
  60. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  62. https://medium.com/p/b4b559d1159b/share/twitter
  63. https://medium.com/p/b4b559d1159b/share/facebook
  64. https://medium.com/p/b4b559d1159b/share/twitter
  65. https://medium.com/p/b4b559d1159b/share/facebook
