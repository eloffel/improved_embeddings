   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

applied deep learning - part 4: convolutional neural networks

   [16]go to the profile of arden dertat
   [17]arden dertat (button) blockedunblock (button) followfollowing
   nov 8, 2017

overview

   welcome to part 4 of applied deep learning series. [18]part 1 was a
   hands-on introduction to id158s, covering both the
   theory and application with a lot of code examples and visualization.
   in [19]part 2 we applied deep learning to real-world datasets, covering
   the 3 most commonly encountered problems as case studies: binary
   classification, multiclass classification and regression. [20]part 3
   explored a specific deep learning architecture: autoencoders.

   now we will cover the most popular deep learning model: convolutional
   neural networks.
    1. [21]introduction
    2. [22]architecture
    3. [23]intuition
    4. [24]implementation
    5. [25]vgg model
    6. [26]visualization
    7. [27]conclusion
    8. [28]references

   the code for this article is available [29]here as a jupyter notebook,
   feel free to download and try it out yourself.

1. introduction

   convolutional neural networks (id98) are everywhere. it is arguably the
   most popular deep learning architecture. the recent surge of interest
   in deep learning is due to the immense popularity and effectiveness of
   convnets. the interest in id98 started with alexnet in 2012 and it has
   grown exponentially ever since. in just three years, researchers
   progressed from 8 layer alexnet to 152 layer resnet.

   id98 is now the go-to model on every image related problem. in terms of
   accuracy they blow competition out of the water. it is also
   successfully applied to recommender systems, natural language
   processing and more. the main advantage of id98 compared to its
   predecessors is that it automatically detects the important features
   without any human supervision. for example, given many pictures of cats
   and dogs it learns distinctive features for each class by itself.

   id98 is also computationally efficient. it uses special convolution and
   pooling operations and performs parameter sharing. this enables id98
   models to run on any device, making them universally attractive.

   all in all this sounds like pure magic. we are dealing with a very
   powerful and efficient model which performs automatic feature
   extraction to achieve superhuman accuracy (yes id98 models now do image
   classification better than humans). hopefully this article will help us
   uncover the secrets of this remarkable technique.

2. architecture

   all id98 models follow a similar architecture, as shown in the figure
   below.
   [1*uulvwmfjmidbfbh9tmvntw@2x.png]

   there is an input image that we   re working with. we perform a series
   convolution + pooling operations, followed by a number of fully
   connected layers. if we are performing multiclass classification the
   output is softmax. we will now dive into each component.

2.1) convolution

   the main building block of id98 is the convolutional layer. convolution
   is a mathematical operation to merge two sets of information. in our
   case the convolution is applied on the input data using a convolution
   filter to produce a feature map. there are a lot of terms being used so
   let   s visualize them one by one.
   [1*ctep-ivccuyptt0qpe3gjg@2x.png]

   on the left side is the input to the convolution layer, for example the
   input image. on the right is the convolution filter, also called the
   kernel, we will use these terms interchangeably. this is called a 3x3
   convolution due to the shape of the filter.

   we perform the convolution operation by sliding this filter over the
   input. at every location, we do element-wise id127 and
   sum the result. this sum goes into the feature map. the green area
   where the convolution operation takes place is called the receptive
   field. due to the size of the filter the receptive field is also 3x3.
   [1*ghaknijngolaa3dpjvdxfq@2x.png]

   here the filter is at the top left, the output of the convolution
   operation    4    is shown in the resulting feature map. we then slide the
   filter to the right and perform the same operation, adding that result
   to the feature map as well.
   [1*oxoszpfzfxggzw2ycqnenw@2x.png]

   we continue like this and aggregate the convolution results in the
   feature map. here   s an animation that shows the entire convolution
   operation.
   [1*vvvdh-bukfh2pwdd0kpera@2x.gif]

   this was an example convolution operation shown in 2d using a 3x3
   filter. but in reality these convolutions are performed in 3d. in
   reality an image is represented as a 3d matrix with dimensions of
   height, width and depth, where depth corresponds to color channels
   (rgb). a convolution filter has a specific height and width, like 3x3
   or 5x5, and by design it covers the entire depth of its input so it
   needs to be 3d as well.

   one more important point before we visualize the actual convolution
   operation. we perform multiple convolutions on an input, each using a
   different filter and resulting in a distinct feature map. we then stack
   all these feature maps together and that becomes the final output of
   the convolution layer. but first let   s start simple and visualize a
   convolution using a single filter.
   [1*bsljljf31gj98abjmct3-g@2x.png]

   let   s say we have a 32x32x3 image and we use a filter of size 5x5x3
   (note that the depth of the convolution filter matches the depth of the
   image, both being 3). when the filter is at a particular location it
   covers a small volume of the input, and we perform the convolution
   operation described above. the only difference is this time we do the
   sum of matrix multiply in 3d instead of 2d, but the result is still a
   scalar. we slide the filter over the input like above and perform the
   convolution at every location aggregating the result in a feature map.
   this feature map is of size 32x32x1, shown as the red slice on the
   right.

   if we used 10 different filters we would have 10 feature maps of size
   32x32x1 and stacking them along the depth dimension would give us the
   final output of the convolution layer: a volume of size 32x32x10, shown
   as the large blue box on the right. note that the height and width of
   the feature map are unchanged and still 32, it   s due to padding and we
   will elaborate on that shortly.

   to help with visualization, we slide the filter over the input as
   follows. at each location we get a scalar and we collect them in the
   feature map. the animation shows the sliding operation at 4 locations,
   but in reality it   s performed over the entire input.
   [1*ubmjtey3edn5qym5hnpmvg@2x.gif]

   below we can see how two feature maps are stacked along the depth
   dimension. the convolution operation for each filter is performed
   independently and the resulting feature maps are disjoint.
   [1*45gsvntvphv0oirr78dbiw@2x.png]

2.2) non-linearity

   for any kind of neural network to be powerful, it needs to contain
   non-linearity. both the ann and autoencoder we saw before achieved this
   by passing the weighted sum of its inputs through an activation
   function, and id98 is no different. we again pass the result of the
   convolution operation through relu activation function. so the values
   in the final feature maps are not actually the sums, but the relu
   function applied to them. we have omitted this in the figures above for
   simplicity. but keep in mind that any type of convolution involves a
   relu operation, without that the network won   t achieve its true
   potential.

2.3) stride and padding

   stride specifies how much we move the convolution filter at each step.
   by default the value is 1, as you can see in the figure below.
   [1*l4t6ixralwosebncjrr4wq@2x.gif]

   we can have bigger strides if we want less overlap between the
   receptive fields. this also makes the resulting feature map smaller
   since we are skipping over potential locations. the following figure
   demonstrates a stride of 2. note that the feature map got smaller.
   [1*4wzt9g7w7cchzo-5rvxl5g@2x.gif]

   we see that the size of the feature map is smaller than the input,
   because the convolution filter needs to be contained in the input. if
   we want to maintain the same dimensionality, we can use padding to
   surround the input with zeros. check the animation below.
   [1*w2d564gkad9lj3_6t9i2pa@2x.gif]

   the gray area around the input is the padding. we either pad with zeros
   or the values on the edge. now the dimensionality of the feature map
   matches the input. padding is commonly used in id98 to preserve the size
   of the feature maps, otherwise they would shrink at each layer, which
   is not desirable. the 3d convolution figures we saw above used padding,
   that   s why the height and width of the feature map was the same as the
   input (both 32x32), and only the depth changed.

2.4) pooling

   after a convolution operation we usually perform pooling to reduce the
   dimensionality. this enables us to reduce the number of parameters,
   which both shortens the training time and combats overfitting. pooling
   layers downsample each feature map independently, reducing the height
   and width, keeping the depth intact.

   the most common type of pooling is max pooling which just takes the max
   value in the pooling window. contrary to the convolution operation,
   pooling has no parameters. it slides a window over its input, and
   simply takes the max value in the window. similar to a convolution, we
   specify the window size and stride.

   here is the result of max pooling using a 2x2 window and stride 2. each
   color denotes a different window. since both the window size and stride
   are 2, the windows are not overlapping.
   [1*reznsf_yr7q1nqeggirsmq@2x.png]

   note that this window and stride configuration halves the size of the
   feature map. this is the main use case of pooling, downsampling the
   feature map while keeping the important information.

   now let   s work out the feature map dimensions before and after pooling.
   if the input to the pooling layer has the dimensionality 32x32x10,
   using the same pooling parameters described above, the result will be a
   16x16x10 feature map. both the height and width of the feature map are
   halved, but the depth doesn   t change because pooling works
   independently on each depth slice the input.
   [1*sexirx4-kgm0p66pysnq4a@2x.png]

   by halving the height and the width, we reduced the number of weights
   to 1/4 of the input. considering that we typically deal with millions
   of weights in id98 architectures, this reduction is a pretty big deal.

   in id98 architectures, pooling is typically performed with 2x2 windows,
   stride 2 and no padding. while convolution is done with 3x3 windows,
   stride 1 and with padding.

2.5) hyperparameters

   let   s now only consider a convolution layer ignoring pooling, and go
   over the hyperparameter choices we need to make. we have 4 important
   hyperparameters to decide on:
     * filter size: we typically use 3x3 filters, but 5x5 or 7x7 are also
       used depending on the application. there are also 1x1 filters which
       we will explore in another article, at first sight it might look
       strange but they have interesting applications. remember that these
       filters are 3d and have a depth dimension as well, but since the
       depth of a filter at a given layer is equal to the depth of its
       input, we omit that.
     * filter count: this is the most variable parameter, it   s a power of
       two anywhere between 32 and 1024. using more filters results in a
       more powerful model, but we risk overfitting due to increased
       parameter count. usually we start with a small number of filters at
       the initial layers, and progressively increase the count as we go
       deeper into the network.
     * stride: we keep it at the default value 1.
     * padding: we usually use padding.

2.6) fully connected

   after the convolution + pooling layers we add a couple of fully
   connected layers to wrap up the id98 architecture. this is the same
   fully connected ann architecture we talked about in [30]part 1.

   remember that the output of both convolution and pooling layers are 3d
   volumes, but a fully connected layer expects a 1d vector of numbers. so
   we flatten the output of the final pooling layer to a vector and that
   becomes the input to the fully connected layer. flattening is simply
   arranging the 3d volume of numbers into a 1d vector, nothing fancy
   happens here.

2.7) training

   id98 is trained the same way like ann, id26 with gradient
   descent. due to the convolution operation it   s more mathematically
   involved, and it   s out of the scope for this article. if you   re
   interested in the details refer [31]here.

3. intuition

   a id98 model can be thought as a combination of two components: feature
   extraction part and the classification part. the convolution + pooling
   layers perform feature extraction. for example given an image, the
   convolution layer detects features such as two eyes, long ears, four
   legs, a short tail and so on. the fully connected layers then act as a
   classifier on top of these features, and assign a id203 for the
   input image being a dog.

   the convolution layers are the main powerhouse of a id98 model.
   automatically detecting meaningful features given only an image and a
   label is not an easy task. the convolution layers learn such complex
   features by building on top of each other. the first layers detect
   edges, the next layers combine them to detect shapes, to following
   layers merge this information to infer that this is a nose. to be
   clear, the id98 doesn   t know what a nose is. by seeing a lot of them in
   images, it learns to detect that as a feature. the fully connected
   layers learn how to use these features produced by convolutions in
   order to correctly classify the images.

   all this might sound vague right now, but hopefully the visualization
   section will make everything more clear.

4. implementation

   after this lengthy explanation let   s code up our id98. we will use the
   dogs vs cats [32]dataset from kaggle to distinguish dog photos from
   cats.

   we will use the following architecture: 4 convolution + pooling layers,
   followed by 2 fully connected layers. the input is an image of a cat or
   dog and the output is binary.
   [1*uuyc126ru4mntwwckebctw@2x.png]

   here   s the code for the id98:

   iframe: [33]/media/d6178a0ae3136a517e005e73e99d2f8f?postid=584bc134c1e2

   structurally the code looks similar to the ann we have been working on.
   there are 4 new methods we haven   t seen before:
     * conv2d: this method creates a convolutional layer. the first
       parameter is the filter count, and the second one is the filter
       size. for example in the first convolution layer we create 32
       filters of size 3x3. we use relu non-linearity as activation. we
       also enable padding. in keras there are two options for padding:
       same or valid. same means we pad with the number on the edge and
       valid means no padding. stride is 1 for convolution layers by
       default so we don   t change that. this layer can be customized
       further with additional parameters, you can check the documentation
       [34]here.
     * maxpooling2d: creates a maxpooling layer, the only argument is the
       window size. we use a 2x2 window as it   s the most common. by
       default stride length is equal to the window size, which is 2 in
       our case, so we don   t change that.
     * flatten: after the convolution + pooling layers we flatten their
       output to feed into the fully connected layers as we discussed
       above.
     * dropout: we will explain this in the next section.

4.1) dropout

   dropout is by far the most popular id173 technique for deep
   neural networks. even the state-of-the-art models which have 95%
   accuracy get a 2% accuracy boost just by adding dropout, which is a
   fairly substantial gain at that level.

   dropout is used to prevent overfitting and the idea is very simple.
   during training time, at each iteration, a neuron is temporarily
      dropped    or disabled with id203 p. this means all the inputs and
   outputs to this neuron will be disabled at the current iteration. the
   dropped-out neurons are resampled with id203 p at every training
   step, so a dropped out neuron at one step can be active at the next
   one. the hyperparameter p is called the dropout-rate and it   s typically
   a number around 0.5, corresponding to 50% of the neurons being dropped
   out.

   it   s surprising that dropout works at all. we are disabling neurons on
   purpose and the network actually performs better. the reason is that
   dropout prevents the network to be too dependent on a small number of
   neurons, and forces every neuron to be able to operate independently.
   this might sound familiar from constraining the code size of the
   autoencoder in [35]part 3, in order to learn more intelligent
   representations.

   let   s visualize dropout, it will be much easier to understand.
   [1*7lrjuuxio8ewrbuuibukxq@2x.png]

   dropout can be applied to input or hidden layer nodes but not the
   output nodes. the edges in and out of the dropped out nodes are
   disabled. remember that the nodes which were dropped out change at each
   training step. also we don   t apply dropout during test time after the
   network is trained, we do so only in training.

   a real-life analogy to dropout would be as follows: let   s say you were
   the only person at your company who knows about finance. if you were
   guaranteed to be at work every day, your coworkers wouldn   t have an
   incentive to pick up finance skills. but if every morning you tossed a
   coin to decide whether you will go to work or not, then your coworkers
   will need to adapt. some days you might not be at work but finance
   tasks still need to get done, so they can   t rely only at you. your
   coworkers will need to learn about finance and this expertise needs to
   be spread out between various people. the workers need to cooperate
   with several other employees, not with a fixed set of people. this
   makes the company much more resilient overall, increasing the quality
   and skillset of the employees.

   almost all state-of-the-art deep networks now incorporate dropout.
   there is another very popular id173 technique called batch
   id172 and we will cover it in another article.

   4.2) model performance

   let   s now analyze the performance of our model. we will take a look at
   loss and accuracy curves, comparing training set performance against
   the validation set.
   [1*utomymaz2nuxpwlehbdzkg@2x.png]

   training loss keeps going down but the validation loss starts
   increasing after around epoch 10. this is the textbook definition of
   overfitting. the model is memorizing the training data, but it   s
   failing to generalize to new instances, and that   s why the validation
   performance goes worse.

   we are overfitting despite the fact that we are using dropout. the
   reason is we are training on very few examples, 1000 images per
   category. usually we need at least 100k training examples to start
   thinking about deep learning. no matter which id173 technique
   we use, we will overfit on such a small dataset. but fortunately there
   is a solution to this problem which enables us to train deep models on
   small datasets, and it   s called data augmentation.

4.3) data augmentation

   overfitting happens because of having too few examples to train on,
   resulting in a model that has poor generalization performance. if we
   had infinite training data, we wouldn   t overfit because we would see
   every possible instance.

   the common case in most machine learning applications, especially in
   image classification tasks is that obtaining new training data is not
   easy. therefore we need to make do with the training set at hand. data
   augmentation is a way to generate more training data from our current
   set. it enriches or    augments    the training data by generating new
   examples via random transformation of existing ones. this way we
   artificially boost the size of the training set, reducing overfitting.
   so data augmentation can also be considered as a id173
   technique.

   data augmentation is done dynamically during training time. we need to
   generate realistic images, and the transformations should be learnable,
   simply adding noise won   t help. common transformations are: rotation,
   shifting, resizing, exposure adjustment, contrast change etc. this way
   we can generate a lot of new samples from a single training example.
   also, data augmentation is only performed on the training data, we
   don   t touch the validation or test set.

   visualization will help understanding the concept. let   s say this is
   our original image.
   [1*h3e66n_7umdrobaky-wg4a@2x.png]

   using data augmentation we generate these artificial training
   instances. these are new training instances, applying transformations
   on the original image doesn   t change the fact that this is still a cat
   image. we can infer it as a human, so the model should be able to learn
   that as well.
   [1*s-aiji0q1fj9ni20nbkiya@2x.png]

   data augmentation can boost the size of the training set by even 50x.
   it   s a very powerful technique that is used in every single image-based
   deep learning model, no exceptions.

   there are some data cleaning tricks that we typically use on images,
   mainly whitening and mean id172. more information about them is
   available [36]here.

4.4) updated model

   now let   s use data augmentation in our id98 model. the code for the
   model definition will not change at all, since we   re not changing the
   architecture of our model. the only change is how we feed in the data,
   you can check the jupyter notebook [37]here.

   it   s pretty easy to do data augmentation with keras, it provides a
   class which does all the work for us, we only need to specify some
   parameters. the documentation is available [38]here.

   the loss and accuracy curves look as follows using data augmentation.
   [1*gwe7sajofkkefp1arrclra@2x.png]

   this time there is no apparent overfitting. and validation accuracy
   jumped from 73% with no data augmentation to 81% with data
   augmentation, 11% improvement. this is a pretty big deal. there are two
   main reasons that the accuracy improved. first, we are training on more
   images with variety. second, we made the model transformation
   invariant, meaning the model saw a lot of shifted/rotated/scaled images
   so it   s able to recognize them better.

5. vgg model

   let   s now take a look at an example state-of-the art id98 model from
   2014. vgg is a convolutional neural network from researchers at
   oxford   s visual geometry group, hence the name vgg. it was the runner
   up of the id163 classification challenge with 7.3% error rate.
   [39]id163 is the most comprehensive hand-annotated visual dataset,
   and they hold competitions every year where researchers from all around
   the world compete. all the famous id98 architectures make their debut at
   that competition.

   among the best performing id98 models, vgg is remarkable for its
   simplicity. let   s take a look at its architecture.
   [1*u8uogozds8nwzqe3tohfkw@2x.png]

   vgg is a 16 layer neural net, not counting the maxpool layers and the
   softmax at the end. it   s also referred to as vgg16. the architecture is
   the one we worked with above. stacked convolution + pooling layers
   followed by fully connected ann. a few observations about the
   architecture:
     * it only uses 3x3 convolutions throughout the network. note that two
       back to back 3x3 convolutions have the effective receptive field of
       a single 5x5 convolution. and three stacked 3x3 convolutions have
       the receptive field of a single 7x7 one. here   s the visualization
       of two stacked 3x3 convolutions resulting in 5x5.

   [1*ypxrr8bn5xyqolztkphvdw@2x.png]
     * another advantage of stacking two convolutions instead of one is
       that we use two relu operations, and more non-linearity gives more
       power to the model.
     * the number of filters increase as we go deeper into the network.
       the spatial size of the feature maps decrease since we do pooling,
       but the depth of the volumes increase as we use more filters.
     * trained on 4 gpus for 3 weeks.

   vgg is a very fundamental id98 model. it   s the first one that comes to
   mind if you need to use an off-the-shelf model for a particular task.
   the paper is also very well written, available [40]here. there are much
   more complicated models which perform better, for example microsoft   s
   resnet model was the winner of 2015 id163 challenge with 3.6% error
   rate, but the model has 152 layers! details available in the paper
   [41]here. we will cover all these id98 architectures in depth in another
   article, but if you want to jump ahead [42]here is a great post.

6. visualization

   now comes the most fun and interesting part, visualization of
   convolutional neural networks. deep learning models are known to be
   very hard to interpret, that   s why they are usually treated as black
   boxes. but id98 models are actually the opposite, and we can visualize
   various components. this will give us an in depth look into their
   internal workings and help us understand them better.

   we will visualize the 3 most crucial components of the vgg model:
     * feature maps
     * convnet filters
     * class output

6.1) visualizing feature maps

   let   s quickly recap the convolution architecture as a reminder. a
   convnet filter operates on the input performing the convolution
   operation and as a result we get a feature map. we use multiple filters
   and stack the resulting feature maps together to obtain an output
   volume. first we will visualize the feature maps, and in the next
   section we will explore the convnet filters.
   [1*hbp1vrfewnareprrlnxtqq@2x.png]

   we will visualize the feature maps to see how the input is transformed
   passing through the convolution layers. the feature maps are also
   called intermediate activations since the output of a layer is called
   the activation.

   remember that the output of a convolution layer is a 3d volume. as we
   discussed above the height and width correspond to the dimensions of
   the feature map, and each depth channel is a distinct feature map
   encoding independent features. so we will visualize individual feature
   maps by plotting each channel as a 2d image.

   how to visualize the feature maps is actually pretty simple. we pass an
   input image through the id98 and record the intermediate activations. we
   then randomly select some of the feature maps and plot them.

   vgg convolutional layers are named as follows: blockx_convy. for
   example the second filter in the third convolution block is called
   block3_conv2. in the architecture diagram above it corresponds to the
   second purple filter.

   for example one of the feature maps from the output of the very first
   layer (block1_conv1) looks as follows.
   [1*psmekqyh9yuhrdvz2qgj5q@2x.png]

   bright areas are the    activated    regions, meaning the filter detected
   the pattern it was looking for. this filter seems to encode an eye and
   nose detector.

   instead of looking at a single feature map, it would be more
   interesting to visualize multiple feature maps from a convolution
   layer. so let   s visualize the feature maps corresponding to the first
   convolution of each block, the red arrows in the figure below.
   [1*vjn03e-hictpqfugd8ezsq@2x.png]

   the following figure displays 8 feature maps per layer. block1_conv1
   actually contains 64 feature maps, since we have 64 filters in that
   layer. but we are only visualizing the first 8 per layer in this
   figure.
   [1*a86wujl-z0swddi3slkqtg@2x.png]

   there are some interesting observations about the feature maps as we
   progress through the layers. let   s take a look at one feature map per
   layer to make it more obvious.
   [1*ouxhgvj1wddfo5uo5gihga@2x.png]
     * the first layer feature maps (block1_conv1) retain most of the
       information present in the image. in id98 architectures the first
       layers usually act as edge detectors.
     * as we go deeper into the network, the feature maps look less like
       the original image and more like an abstract representation of it.
       as you can see in block3_conv1 the cat is somewhat visible, but
       after that it becomes unrecognizable. the reason is that deeper
       feature maps encode high level concepts like    cat nose    or    dog
       ear    while lower level feature maps detect simple edges and shapes.
       that   s why deeper feature maps contain less information about the
       image and more about the class of the image. they still encode
       useful features, but they are less visually interpretable by us.
     * the feature maps become sparser as we go deeper, meaning the
       filters detect less features. it makes sense because the filters in
       the first layers detect simple shapes, and every image contains
       those. but as we go deeper we start looking for more complex stuff
       like    dog tail    and they don   t appear in every image. that   s why in
       the first figure with 8 filters per layer, we see more of the
       feature maps as blank as we go deeper (block4_conv1 and
       block5_conv1).

6.2) visualizing convnet filters

   now we will visualize the main building block of a id98, the filters.
   there is one catch though, we won   t actually visualize the filters
   themselves, but instead we will display the patterns each filter
   maximally responds to. remember that the filters are of size 3x3
   meaning they have the height and width of 3 pixels, pretty small. so as
   a proxy to visualizing a filter, we will generate an input image where
   this filter activates the most.

   the full details of how to do this is somewhat technical, and you can
   check the actual code in the jupyter [43]notebook. but as a quick
   summary it works as follows:
     * choose a id168 that maximizes the value of a convnet
       filter.
     * start from a blank input image.
     * do gradient ascent in input space. meaning modify the input values
       such that the filter activates even more.
     * repeat this in a loop.

   the result of this process is an input image where the filter is very
   active. remember that each filter acts as a detector for a particular
   feature. the input image we generate will contain a lot of these
   features.

   we will visualize filters at the last layer of each convolution block.
   to clear any confusion, in the previous section we visualized the
   feature maps, the output of the convolution operation. now we are
   visualizing the filters, the main structure used in the convolution
   operation.
   [1*rbktgwvwja0w6nhwfdl0na@2x.png]

   we will visualize 8 filters per layer.
   [1*67edfn3-tzdzff0ndzgkwq@2x.png]
   [1*yo61wkzutnreu7lrglchyw@2x.png]
   [1*pcxalrsizntcj206jymw0q@2x.png]
   [1*wouaw0j7tjvdbsjrk3zsuw@2x.png]
   [1*w41f9cu7vnvts1e06vok0a@2x.png]

   they look pretty surreal! especially the ones in the last layers. here
   are some observations about the filters:
     * the first layer filters (block1_conv2 and block2_conv2) mostly
       detect colors, edges and simple shapes.
     * as we go deeper into the network, the filters build on top of each
       other, and learn to encode more complex patterns. for example
       filter 41 in block5_conv3 seems to be a bird detector. you can see
       multiple heads in different orientations, because the particular
       location of the bird in the image is not important, as long as it
       appears somewhere the filter will activate. that   s why the filter
       tries to detect the bird head in several positions by encoding it
       in multiple locations in the filter.

   these observations are similar to what we discussed in the feature map
   section. lower layers encode/detect simple structures, as we go deeper
   the layers build on top of each other and learn to encode more complex
   patterns. this is how we humans start discovering the world as babies
   too. first we learn simple structures and with practice we excel at
   understanding more complicated things, building on top of our existing
   knowledge.

6.3) visualizing class outputs

   let   s do one final visualization, it will be similar to the convnet
   filter. now we will visualize at the final softmax layer. given a
   particular category, like hammer or lamp, we will ask the id98 to
   generate an image that maximally represents the category. basically the
   id98 will draw us an image of what it thinks a hammer looks like.

   the process is similar to the convnet filter steps. we start from a
   blank image and do modifications such that the id203 assigned to
   a particular category increases. the code is again available in the
   [44]notebook.

   let   s visualize some categories.
   [1*xagx06kgydd7gyezz6-9rw@2x.png]

   they look pretty convincing. an object appears several times in the
   image, because the class id203 becomes higher that way. multiple
   tennis balls in an image is better than a single tennis ball.

   all these visualizations were performed using the library
   [45]keras-vis.

7. conclusion

   id98 is a very fundamental deep learning technique. we covered a wide
   range of topics and the visualization section in my opinion is the most
   interesting. there are very few resources on the web which do a
   thorough visual exploration of convolution filters and feature maps. i
   hope it was helpful.

   the entire code for this article is available [46]here if you want to
   hack on it yourself. if you have any feedback feel free to reach out to
   me on [47]twitter.

8. references

   there is an abundance of id98 tutorials on the web, but the most
   comprehensive one is the stanford cs231n course by andrej karpathy. the
   reading material is available [48]here, and the video lectures are
   [49]here. excellent source of information, highly recommended.

   if you want to dig more into visualization deepvis is a great resource
   available [50]here. there is an interactive tool, open source code,
   paper and a detailed article.

   a great image classification tutorial from the author of keras is
   available [51]here. this article was highly influenced that tutorial.
   it covers a lot of the material we talked about in detail.

   a very thorough online free book about deep learning can be found
   [52]here, with the id98 section available [53]here.

   if you   re interested in applying id98 to natural language processing,
   [54]this is a great article. another very detailed one is available
   [55]here.

   a 3-part article series covering state-of-the are id98 papers can be
   found [56]here.

   all articles of chris olah are packed with great information and
   visualizations. id98 related posts are available [57]here and [58]here.

   another popular id98 introductory post is [59]here.

   a very accessible 3-part series about id98 is available [60]here.

     * [61]machine learning
     * [62]deep learning
     * [63]artificial intelligence
     * [64]neural networks
     * [65]towards data science

   (button)
   (button)
   (button) 4.2k claps
   (button) (button) (button) 22 (button) (button)

     (button) blockedunblock (button) followfollowing
   [66]go to the profile of arden dertat

[67]arden dertat

   ml engineer @ pinterest. photography and travel enthusiast.

     (button) follow
   [68]towards data science

[69]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 4.2k
     * (button)
     *
     *

   [70]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [71]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/584bc134c1e2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ytdogrdvizgj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ardendertat?source=post_header_lockup
  17. https://towardsdatascience.com/@ardendertat
  18. https://medium.com/towards-data-science/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6
  19. https://medium.com/towards-data-science/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585
  20. https://medium.com/towards-data-science/applied-deep-learning-part-3-autoencoders-1c083af4d798
  21. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#a86a
  22. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#7d8a
  23. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#9a7a
  24. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#5777
  25. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#e865
  26. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#9722
  27. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#5c12
  28. https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2?gi=2dc74725f0cc#8efa
  29. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 4 (gpu) - convolutional neural networks.ipynb
  30. https://medium.com/towards-data-science/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6
  31. http://www.jefkine.com/general/2016/09/05/id26-in-convolutional-neural-networks/
  32. https://www.kaggle.com/c/dogs-vs-cats
  33. https://towardsdatascience.com/media/d6178a0ae3136a517e005e73e99d2f8f?postid=584bc134c1e2
  34. https://keras.io/layers/convolutional/
  35. https://medium.com/towards-data-science/applied-deep-learning-part-3-autoencoders-1c083af4d798
  36. http://ufldl.stanford.edu/tutorial/unsupervised/pcawhitening/
  37. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 4 (gpu) - convolutional neural networks.ipynb
  38. https://keras.io/preprocessing/image/
  39. http://www.image-net.org/
  40. https://arxiv.org/pdf/1409.1556.pdf
  41. https://arxiv.org/pdf/1512.03385.pdf
  42. https://adeshpande3.github.io/the-9-deep-learning-papers-you-need-to-know-about.html
  43. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 4 (gpu) - convolutional neural networks.ipynb
  44. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 4 (gpu) - convolutional neural networks.ipynb
  45. https://raghakot.github.io/keras-vis/
  46. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 4 (gpu) - convolutional neural networks.ipynb
  47. https://twitter.com/ardendertat
  48. http://cs231n.github.io/
  49. https://www.youtube.com/playlist?list=plkt2usq6rbvctenovbg1tpcc7oqi31alc
  50. http://yosinski.com/deepvis
  51. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  52. http://neuralnetworksanddeeplearning.com/
  53. http://neuralnetworksanddeeplearning.com/chap6.html
  54. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  55. https://medium.com/@talperry/convolutional-methods-for-text-d5260fd5675f
  56. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/
  57. http://colah.github.io/posts/2014-07-conv-nets-modular/
  58. http://colah.github.io/posts/2014-07-understanding-convolutions/
  59. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  60. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  61. https://towardsdatascience.com/tagged/machine-learning?source=post
  62. https://towardsdatascience.com/tagged/deep-learning?source=post
  63. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  64. https://towardsdatascience.com/tagged/neural-networks?source=post
  65. https://towardsdatascience.com/tagged/towards-data-science?source=post
  66. https://towardsdatascience.com/@ardendertat?source=footer_card
  67. https://towardsdatascience.com/@ardendertat
  68. https://towardsdatascience.com/?source=footer_card
  69. https://towardsdatascience.com/?source=footer_card
  70. https://towardsdatascience.com/
  71. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  73. https://medium.com/p/584bc134c1e2/share/twitter
  74. https://medium.com/p/584bc134c1e2/share/facebook
  75. https://medium.com/p/584bc134c1e2/share/twitter
  76. https://medium.com/p/584bc134c1e2/share/facebook
