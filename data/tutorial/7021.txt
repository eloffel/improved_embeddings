   #[1]learn opencv    feed [2]learn opencv    comments feed [3]learn opencv
      handwritten digits classification : an opencv ( c++ / python )
   tutorial comments feed [4]training a better haar and lbp cascade based
   eye detector using opencv [5]object tracking using opencv (c++/python)
   [6]alternate [7]alternate

   [tr?id=193036094536652&ev=pageview&noscript=1]

     * [8]skip to primary navigation
     * [9]skip to content
     * [10]skip to primary sidebar

   [11]learn opencv

   opencv examples and tutorials ( c++ / python )

     * [12]home
     * [13]about
     * [14]resources
     * [15]ai consulting
     * [16]courses

handwritten digits classification : an opencv ( c++ / python ) tutorial

   january 30, 2017 by [17]satya mallick [18]63 comments

   [19]digits classification using opencv

   this post is part of a series i am writing on image recognition and
   id164.

   the complete list of tutorials in this series is given below:
    1. [20]image recognition using traditional id161 techniques
       : part 1
    2. [21]histogram of oriented gradients : part 2
    3. example code for image recognition : part 3
    4. [22]training a better eye detector: part 4a
    5. id164 using traditional id161 techniques :
       part 4b
    6. how to train and test your own opencv object detector : part 5
    7. image recognition using deep learning : part 6
          + [23]introduction to neural networks
          + [24]understanding feedforward neural networks
          + [25]image recognition using convolutional neural networks
    8. id164 using deep learning : part 7

   in this tutorial, we will build a simple handwritten digit classifier
   using opencv. as always we will share code written in c++ and python.

   this post is the third in a series i am writing on image recognition
   and id164. the first post introduced the traditional
   id161 [26]image classification pipeline and in the second
   post, we discussed the [27]histogram of oriented gradients (hog) image
   descriptor in detail. we also had a guest post on [28]training an eye
   detector that is related to this topic.

   the last two posts were geared toward providing education needed to
   understand the basics. this post is geared toward providing the
   training needed to successfully implement an image classifier. so, what
   is the difference between education and training ? well, education
   provides largely theoretical knowledge. it is important to get that
   knowledge, but it is useless without good training. during training,
   you learn specific skills and apply the theoretical knowledge to the
   real world.
   you can download c++ and python code for this tutorial [29]here. for
   access to all code used in other tutorials of this blog please
   [30]subscribe to our newsletter

image classification pipeline

   if you have not looked at my previous post on [31]image classification,
   i encourage you to do so. in that post, a pipeline involved in most
   traditional id161 image classification algorithms is
   described.

   [32]image classification pipeline

   the image above shows that pipeline. in this post, we will use
   histogram of oriented gradients as the feature descriptor and support
   vector machine (id166) as the machine learning algorithm for
   classification.

id42 (ocr) example using opencv (c++ / python)

   i wanted to share an example with code to demonstrate image
   classification using hog + id166. at the same time, i wanted to keep
   things as simple as possible so that we do not need much in addition to
   hog and id166. the inspiration and data for this post comes from the
   opencv tutorial [33]here.

   the original tutorial is in python only, and for some strange reason
   implements it   s own simple hog descriptor. we replaced their homegrown
   hog with opencv   s hog descriptor.

digits dataset for ocr

   [34]digits dataset

   we are going to use the above image as our dataset that comes with
   opencv samples. it contains 5000 images in all     500 images of each
   digit. each image is 20  20 grayscale with a black background. 4500 of
   these digits will be used for training and the remaining 500 will be
   used for testing the performance of the algorithm. you can click on the
   image above to enlarge.

   let us go through the steps needed to build and test a classifier.

step 1 : deskewing (preprocessing)

   people often think of a learning algorithm as a block box. input an
   image at one end and out comes the result at the other end. in reality,
   you can assist the algorithm a bit and notice huge gains in
   performance. for example, if you are building a face recognition
   system, aligning the images to a reference face often leads to a quite
   substantial improvement in performance. a typical alignment operation
   uses a [35]facial feature detector to align the eyes in every image.

   aligning digits before building a classifier similarly produces
   superior results. in the case of faces, aligment is rather obvious    
   you can apply a similarity transformation to an image of a face to
   align the two corners of the eyes to the two corners of a reference
   face.

   [36]deskew example

   in the case of handwritten digits, we do not have obvious features like
   the corners of the eyes we can use for alignment. however, an obvious
   variation in writing among people is the slant of their writing. some
   writers have a right or forward slant where the digits are slanted
   forward, some have a backward or left slant, and some have no slant at
   all. we can help the algorithm quite a bit by fixing this vertical
   slant so it does not have to learn this variation of the digits. the
   image on the left shows the original digit in the first column and it   s
   deskewed (fixed) version.

   this deskewing of simple grayscale images can be achieved using
   [37]image moments. opencv has an implementation of moments and it comes
   in handy while calculating useful information like centroid, area,
   skewness of simple images with black backgrounds.

   it turns out that a measure of the skewness is the given by the ratio
   of the two central moments ( mu11 / mu02 ). the skewness thus
   calculated can be used in calculating an affine transform that deskews
   the image.

   the code for deskewing is shared below.

   python
def deskew(img):
    m = cv2.moments(img)
    if abs(m['mu02']) < 1e-2:
        # no deskewing needed.
        return img.copy()
    # calculate skew based on central momemts.
    skew = m['mu11']/m['mu02']
    # calculate affine transform to correct skewness.
    m = np.float32([[1, skew, -0.5*sz*skew], [0, 1, 0]])
    # apply affine transform
    img = cv2.warpaffine(img, m, (sz, sz), flags=cv2.warp_inverse_map | cv2.inte
r_linear)
    return img

   c++

mat deskew(mat& img)
{
    moments m = moments(img);
    if(abs(m.mu02) < 1e-2)
    {
        // no deskewing needed.
        return img.clone();
    }
    // calculate skew based on central momemts.
    double skew = m.mu11/m.mu02;
    // calculate affine transform to correct skewness.
    mat warpmat = (mat_<double>(2,3) << 1, skew, -0.5*sz*skew, 0, 1 , 0);

    mat imgout = mat::zeros(img.rows, img.cols, img.type());
    warpaffine(img, imgout, warpmat, imgout.size(),affineflags);

    return imgout;
}

step 2 : calculate the histogram of oriented gradients (hog) descriptor

   in this step, we will convert the grayscale image to a feature vector
   using the hog feature descriptor. in my [38]previous post, i had
   explained the [39]hog descriptor in great detail.

   when i was in grad school, i found a huge gap between theory and
   practice. acquiring the knowledge was easy. i could read papers and
   books. if i did not understand the concept or the math, i read more
   papers and books. that was the easy part. the hard part of putting that
   knowledge into practice. part of the reason was that a lot of these
   algorithms worked after tedious handtuning and it was not obvious how
   to set the right parameters. for example, in harris corner detector,
   why is the free parameter k set to 0.04 ? why not 1 or 2 or 0.34212
   instead? why is 42 the answer to life, universe, and everything?

   as i got more real world experience, i realized that in some cases you
   can make an educated guess but in other cases, nobody knows why. people
   often do a parameter sweep     they change different parameters in a
   principled way to see what produces the best result. sometimes, the
   best parameters have an intuitive explanation and sometimes they don   t.

   keeping that in mind, let   s see what parameters were chosen for our hog
   descriptor. we will also try to explain why they made sense, but
   instead of a rigorous proof, i will offer vigorous handwaving!

   c++
hogdescriptor hog(
        size(20,20), //winsize
        size(10,10), //blocksize
        size(5,5), //blockstride,
        size(10,10), //cellsize,
                 9, //nbins,
                  1, //derivaper,
                 -1, //winsigma,
                  0, //histogramnormtype,
                0.2, //l2hysthresh,
                  1,//gammal correction,
                  64,//nlevels=64
                  1);//use signed gradients

   python
winsize = (20,20)
blocksize = (10,10)
blockstride = (5,5)
cellsize = (10,10)
nbins = 9
derivaperture = 1
winsigma = -1.
histogramnormtype = 0
l2hysthreshold = 0.2
gammacorrection = 1
nlevels = 64
signedgradients = true

hog = cv2.hogdescriptor(winsize,blocksize,blockstride,cellsize,nbins,derivapertu
re,winsigma,histogramnormtype,l2hysthreshold,gammacorrection,nlevels, usesignedg
radients)

   i am not going to describe derivaperture, winsigma, histogramnormtype,
   l2hysthreshold, gammacorrection and nlevels because i have never had to
   change these parameters while using the hog descriptor. unless you have
   carefully read the original hog paper, i would recommend you go with
   the default values. let   s explore the choice of other parameters.

   winsize: this parameter is set to 20  20 because the size of the digit
   images in our dataset is 20  20 and we want to calculate one descriptor
   for the entire image.

   cellsize: our digits are 20  20 grayscale images. in other words, our
   image is represented by 20  20 = 400 numbers.the size of descriptor
   typically is much smaller than the number of pixels in an image. the
   cellsize is chosen based on the scale of the features important to do
   the classification. a very small cellsize would blow up the size of the
   feature vector and a very large one may not capture relevant
   information. you should test this yourself using the code shared in
   this post. we have chosen the cellsize of 10  10 in this tutorial. could
   we have chosen 8 ? yup, that would have worked too.

   blocksize: the notion of blocks exist to tackle illumination variation.
   a large block size makes local changes less significant while a smaller
   block size weights local changes more. typically blocksize is set to 2
   x cellsize, but in our example of digits classification, illumination
   does not present much of a challenge. in my experiments, a blocksize of
   10  10 gave the best results.

   blockstride: the blockstride determines the overlap between neighboring
   blocks and controls the degree of contrast id172. typically a
   blockstride is set to 50% of blocksize.

   nbins: nbins sets the number of bins in the histogram of gradients. the
   authors of the hog paper had recommended a value of 9 to capture
   gradients between 0 and 180 degrees in 20 degrees increments. in my
   experiments, increasing this value to 18 did not produce any better
   results.

   signedgradients: typically gradients can have any orientation between 0
   and 360 degrees. these gradients are referred to as    signed    gradients
   as opposed to    unsigned    gradients that drop the sign and take values
   between 0 and 180 degrees. in the original hog paper, unsigned
   gradients were used for pedestrian detection. in my experiments, for
   this problem, signed gradients produced slightly better results.

   the hog descriptor defined above can be used to compute the hog
   features of an image using the following code.

   c++
// im is of type mat
vector<float> descriptors;
hog.compute(im,descriptor);

   python
descriptor = hog.compute(im)

   the size of this descriptor is 81  1 for the parameters we have chosen.

step 3: training a model ( a.k.a learning a classifier )

   until this point, we have deskewed the original image and defined a
   descriptor for our image. this has allowed us to convert every image in
   our dataset to a vector of size 81  1.

   we are now ready to train a model that will classify the images in our
   training set. to do this we have chosen support vector machines (id166)
   as our classification algorithm. while the theory and math behind id166
   is involved and beyond the scope of this tutorial, how it works is very
   intuitive and easy to understand. you can check out my previous
   [40]post that explains linear id166s.

   to quickly recap, if you have points in an n-dimensional space and
   class labels attached to the points, a linear id166 will divide the space
   using planes such that different classes are on different sides of the
   plane. in the figure below, we have two classes represented by red and
   blue dots. if this data is fed into a linear id166, it will easily build
   a classifier by finding the line that clearly separates the two
   classes. there are many lines that could have separated this data. id166
   chooses the one that is at a maximum distance data points of either
   class.

   [41]linear id166

   the two-class example shown in the figure above may appear simple
   compared to our digits classification problem, but mathematically they
   are very similar. instead of being points in a 2d space, our images
   descriptors are points in an 81-dimensional space because they are
   represented by an 81  1 vector. the class labels attached to these
   points are the digits contained in the image, i.e. 0, 1, 2,     9.
   instead of lines in 2d, the id166 will find hyperplanes in a high
   dimensional space to do the classification.

id166 parameter c

   one of the two common parameters you need to know about while training
   an id166 is called c. real world data is not as clean as shown above.
   sometimes the training data may have mislabeled examples. at other
   times, one example of a set may be too close in appearance to another
   example. e.g. a handwritten digit 2 may look like a 3.

   in the animation below we have created this scenario. notice, the blue
   dot is too close to the red cluster. when the default value of c = 1 is
   chosen, the blue dot is misclassified. choosing the value of 100 for c
   classifies it correctly.

   [42]id166 parameter c

   but now the decision boundary represented by the black line is too
   close to one of the classes. would you rather choose c to be 1 where
   one data point is misclassified, but the separation between the classes
   is much better ( minus the one data point )? the parameter c allows you
   to control this tradeoff.

   so, how do you choose c? we choose the c that provides the best
   classification on a held out test set. the images in this set were not
   used in training.

id166 parameter gamma : non-linear id166

   did you notice, i sneaked in the word    linear    a few times? in
   classification tasks, a dataset consisting of many classes is called
   linearly separable if the space containing the data can be partitioned
   using planes ( or lines in 2d ) to separate the classes.

   what if the data is not linearly separable? the figure below shows two
   classes using red and blue dots that are not linearly separable. you
   cannot draw a line on the plane to separate the two classes. a good
   classifier, represented using the black line, is more of a circle.

   [43]non linearly separable data : id166

   in real life, data is messy and not linearly separable.

   can we still use id166s? the answer is yes!

   to accomplish this, you use a technique called the kernel trick. it is
   a neat trick that transforms non-linearly separable data into a
   linearly separable one. in our example, the red and blue dots lie on a
   2d plane. let us add a third dimension to all data points using the
   following equation.

       \[ \large{ z = e^{ -\gamma ( x^2 + y^2 ) } } \]

   if you ever hear people using the fancy term radial basis function
   (rbf) with a gaussian kernel, they are simply talking about the above
   equation. rbf is simply a real-valued function that depends only on the
   distance from the origin ( i.e. depends only on \sqrt{x^2 + y^2} ). the
   gaussian kernel refers to the gaussian form of the above equation. more
   generally, an rbf can have different kinds of kernels. you can see some
   of them [44]here.

   so, we just cooked up a third dimension based on data in the other two
   dimensions. the figure below shows this three-dimensional (x, y, z)
   data. we can see it is separable by the plane containing the black
   circle!

   [45]kernel trick (id166)

   the parameter gamma ( \gamma ) controls the stretching of data in the
   third dimension. it helps in classification but it also distorts the
   data. like goldilocks, you have to choose this parameter to be    just
   right   . it is one of the two important parameters people choose while
   training an id166.

   equipped with this knowledge, we are now ready to train an id166 using
   opencv.

training and testing an id166 using opencv

   under the hood, opencv uses [46]libid166. id166 in opencv 2.4.x still uses
   the c api. fortunately, starting 3.x, opencv now uses the much nicer
   c++ api. here is how you set up id166 using opencv in c++ and python.

   c++
// set up id166 for opencv 3
ptr<id166> id166 = id166::create();
// set id166 type
id166->settype(id166::c_svc);
// set id166 kernel to radial basis function (rbf)
id166->setkernel(id166::rbf);
// set parameter c
id166->setc(12.5);
// set parameter gamma
id166->setgamma(0.50625);

// train id166 on training data
ptr<traindata> td = traindata::create(traindata, row_sample, trainlabels);
id166->train(td);

// save trained model
id166->save("digits_id166_model.yml");

// test on a held out test set
id166->predict(testmat, testresponse);

   python

# set up id166 for opencv 3
id166 = cv2.ml.id166_create()
# set id166 type
id166.settype(cv2.ml.id166_c_svc)
# set id166 kernel to radial basis function (rbf)
id166.setkernel(cv2.ml.id166_rbf)
# set parameter c
id166.setc(c)
# set parameter gamma
id166.setgamma(gamma)

# train id166 on training data
id166.train(traindata, cv2.ml.row_sample, trainlabels)

# save trained model
id166->save("digits_id166_model.yml");

# test on a held out test set
testresponse = id166.predict(testdata)[1].ravel()


auto training id166

   as you can imagine, it can be very time consuming to select the right
   id166 parameters c and gamma. fortunately, opencv 3.x c++ api provides a
   function that automatically does this hyperparameter optimization for
   you and provides the best c and gamma values. in the code above, you
   can change id166->train(td) to the following
id166->trainauto(td);

   this training can take a very long time ( say 5x more than id166->train )
   because it is essentially training multiple times.

opencv id166 bugs

   we encountered two bugs while working with opencv id166. the first one is
   confirmed, but the other two are not.
    1. id166 model won   t load in python api. the trained id166 model you just
       saved won   t load if you are using python! is the bug fix coming ?
       nope! check it out [47]here
    2. trainauto does not appear to be exposed via the python api.
    3. id166 with rbf kernel does not work in ios / android. i would be
       happy to be proven wrong, but on mobile platforms ( ios / android
       ), we have not been able to use the id166 trained with rbf kernel.
       the id166 response is always the same. linear id166 models work just
       fine.

results

   after training and some hyperparameter optimization, we hit 98.6% on
   digits classification! not, bad for just a few seconds of training.

   out of the 500 images in the training set, 7 were misclassified. the
   images and their misclassified labels are shown below. like a father
   looking at his kid   s mistake, i would say these mistakes are
   understandable.

   [48]incorrect classifications

subscribe & download code

   if you liked this article and would like to download code (c++ and
   python) and example images used in all the tutorials of this blog,
   please [49]subscribe to our newsletter. you will also receive a free
   [50]id161 resource guide. in our newsletter we share opencv
   tutorials and examples written in c++/python, and id161 and
   machine learning algorithms and news.

   [51]subscribe now

   filed under: [52]application, [53]how-to, [54]image classification,
   [55]image recognition, [56]machine learning, [57]opencv 3, [58]tutorial
   tagged with: [59]c++, [60]histogram of oriented gradients, [61]hog,
   [62]python, [63]support vector machine, [64]id166
   (button) load comments

   search this website ____________________ search

join course

   [65]id161 for faces

resources

   [66]download code (c++ / python)

disclaimer

   this site is not affiliated with opencv.org

   i am an entrepreneur with a love for id161 and machine
   learning with a dozen years of experience (and a ph.d.) in the field.

   in 2007, right after finishing my ph.d., i co-founded taaz inc. with my
   advisor dr. david kriegman and kevin barnes. the scalability, and
   robustness of our id161 and machine learning algorithms have
   been put to rigorous test by more than 100m users who have tried our
   products. [67]read more   

recent posts

     * [68]image inpainting with opencv (c++/python)
     * [69]hough transform with opencv (c++/python)
     * [70]xeus-cling: run c++ code in jupyter notebook
     * [71]pose detection comparison : wrnchai vs openpose
     * [72]gender & age classification using opencv deep learning (
       c++/python )

   copyright    2019    big vision llc

references

   1. https://www.learnopencv.com/feed/
   2. https://www.learnopencv.com/comments/feed/
   3. https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/feed/
   4. https://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/
   5. https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/
   6. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/
   7. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/&format=xml
   8. https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/#genesis-nav-primary
   9. https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/#genesis-content
  10. https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/#genesis-sidebar-primary
  11. https://www.learnopencv.com/
  12. https://www.learnopencv.com/
  13. https://www.learnopencv.com/about/
  14. https://www.learnopencv.com/computer-vision-resources
  15. https://www.learnopencv.com/computer-vision-machine-learning-artificial-intelligence-consulting/
  16. http://courses.learnopencv.com/
  17. https://www.learnopencv.com/author/spmallick/
  18. https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/#disqus_thread
  19. https://www.learnopencv.com/wp-content/uploads/2017/01/digits-classification.jpg
  20. https://www.learnopencv.com/image-recognition-and-object-detection-part1
  21. https://www.learnopencv.com/histogram-of-oriented-gradients/
  22. https://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv
  23. https://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/
  24. https://www.learnopencv.com/understanding-feedforward-neural-networks/
  25. https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/
  26. https://www.learnopencv.com/image-recognition-and-object-detection-part1/
  27. https://www.learnopencv.com/histogram-of-oriented-gradients/
  28. https://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/
  29. https://www.learnopencv.com/wp-content/uploads/2017/01/digits-classification.zip
  30. https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/#download
  31. https://www.learnopencv.com/image-recognition-and-object-detection-part1/
  32. https://www.learnopencv.com/wp-content/uploads/2016/11/image-classification-pipeline.jpg
  33. http://docs.opencv.org/trunk/dd/d3b/tutorial_py_id166_opencv.html
  34. https://www.learnopencv.com/wp-content/uploads/2016/12/digits.png
  35. https://www.learnopencv.com/facial-landmark-detection/
  36. https://www.learnopencv.com/wp-content/uploads/2016/12/deskew.png
  37. https://en.wikipedia.org/wiki/image_moment
  38. https://www.learnopencv.com/histogram-of-oriented-gradients/
  39. https://www.learnopencv.com/histogram-of-oriented-gradients/
  40. https://www.learnopencv.com/image-recognition-and-object-detection-part1/
  41. https://www.learnopencv.com/wp-content/uploads/2017/01/linear-id166.jpg
  42. https://www.learnopencv.com/wp-content/uploads/2017/01/id166-c.gif
  43. https://www.learnopencv.com/wp-content/uploads/2017/01/non-linearly-separable-data.jpg
  44. https://en.wikipedia.org/wiki/radial_basis_function
  45. https://www.learnopencv.com/wp-content/uploads/2017/01/kernel-trick.jpg
  46. https://github.com/cjlin1/libid166
  47. https://github.com/opencv/opencv/issues/4969
  48. https://www.learnopencv.com/wp-content/uploads/2017/01/incorrect-classification.jpg
  49. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  50. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  51. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  52. https://www.learnopencv.com/category/application/
  53. https://www.learnopencv.com/category/how-to/
  54. https://www.learnopencv.com/category/image-classification/
  55. https://www.learnopencv.com/category/image-recognition/
  56. https://www.learnopencv.com/category/machine-learning/
  57. https://www.learnopencv.com/category/opencv-3/
  58. https://www.learnopencv.com/category/tutorial/
  59. https://www.learnopencv.com/tag/c/
  60. https://www.learnopencv.com/tag/histogram-of-oriented-gradients/
  61. https://www.learnopencv.com/tag/hog/
  62. https://www.learnopencv.com/tag/python/
  63. https://www.learnopencv.com/tag/support-vector-machine/
  64. https://www.learnopencv.com/tag/id166/
  65. https://courses.learnopencv.com/p/computer-vision-for-faces
  66. https://bigvisionllc.lpages.co/leadbox/143044973f72a2:173c9390c346dc/5720147234914304/
  67. https://www.learnopencv.com/about/
  68. https://www.learnopencv.com/image-inpainting-with-opencv-c-python/
  69. https://www.learnopencv.com/hough-transform-with-opencv-c-python/
  70. https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/
  71. https://www.learnopencv.com/pose-detection-comparison-wrnchai-vs-openpose/
  72. https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/
