   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

an overview of resnet and its variants

   [16]go to the profile of vincent fung
   [17]vincent fung (button) blockedunblock (button) followfollowing
   jul 15, 2017

   after the celebrated victory of alexnet [1] at the lsvrc2012
   classification contest, deep residual network [2] was arguably the most
   groundbreaking work in the id161/deep learning community in
   the last few years. resnet makes it possible to train up to hundreds or
   even thousands of layers and still achieves compelling performance.

   taking advantage of its powerful representational ability, the
   performance of many id161 applications other than image
   classification have been boosted, such as id164 and face
   recognition.

   since resnet blew people   s mind in 2015, many in the research community
   have dived into the secrets of its success, many refinements have been
   made in the architecture. this article is divided into two parts, in
   the first part i am going to give a little bit of background knowledge
   for those who are unfamiliar with resnet, in the second i will review
   some of the papers i read recently regarding different variants and
   interpretations of the resnet architecture.
     __________________________________________________________________

revisiting resnet

   according to the universal approximation theorem, given enough
   capacity, we know that a feedforward network with a single layer is
   sufficient to represent any function. however, the layer might be
   massive and the network is prone to overfitting the data. therefore,
   there is a common trend in the research community that our network
   architecture needs to go deeper.

   since alexnet, the state-of-the-art id98 architecture is going deeper
   and deeper. while alexnet had only 5 convolutional layers, the vgg
   network [3] and googlenet (also codenamed inception_v1) [4] had 19 and
   22 layers respectively.

   however, increasing network depth does not work by simply stacking
   layers together. deep networks are hard to train because of the
   notorious vanishing gradient problem         as the gradient is
   back-propagated to earlier layers, repeated multiplication may make the
   gradient infinitively small. as a result, as the network goes deeper,
   its performance gets saturated or even starts degrading rapidly.
   [1*mcwabgjja1lv_xbdg1w5xa.png]
   increasing network depth leads to worse performance

   before resnet, there had been several ways to deal the vanishing
   gradient issue, for instance, [4] adds an auxiliary loss in a middle
   layer as extra supervision, but none seemed to really tackle the
   problem once and for all.

   the core idea of resnet is introducing a so-called    identity shortcut
   connection    that skips one or more layers, as shown in the following
   figure:
   [1*byrvjspw-tefwlh7olxnkg.png]
   a residual block
   [1*2ns4ota94je5gsvjrpfq3a.png]
   the resnet architecture

   the authors of [2] argue that stacking layers shouldn   t degrade the
   network performance, because we could simply stack identity mappings
   (layer that doesn   t do anything) upon the current network, and the
   resulting architecture would perform the same. this indicates that the
   deeper model should not produce a training error higher than its
   shallower counterparts. they hypothesize that letting the stacked
   layers fit a residual mapping is easier than letting them directly fit
   the desired underlaying mapping. and the residual block above
   explicitly allows it to do precisely that.

   as a matter of fact, resnet was not the first to make use of shortcut
   connections, highway network [5] introduced gated shortcut connections.
   these parameterized gates control how much information is allowed to
   flow across the shortcut. similar idea can be found in the long term
   short memory (lstm) [6] cell, in which there is a parameterized forget
   gate that controls how much information will flow to the next time
   step. therefore, resnet can be thought of as a special case of highway
   network.

   however, experiments show that highway network performs no better than
   resnet, which is kind of strange because the solution space of highway
   network contains resnet, therefore it should perform at least as good
   as resnet. this suggests that it is more important to keep these
      gradient highways    clear than to go for larger solution space.

   following this intuition, the authors of [2] refined the residual block
   and proposed a pre-activation variant of residual block [7], in which
   the gradients can flow through the shortcut connections to any other
   earlier layer unimpededly. in fact, using the original residual block
   in [2], training a 1202-layer resnet resulted in worse performance than
   its 110-layer counterpart.
   [1*m5nielqc33en6kjwzrccoq.png]
   variants of residual blocks

   the authors of [7] demonstrated with experiments that they can now
   train a 1001-layer deep resnet to outperform its shallower
   counterparts. because of its compelling results, resnet quickly became
   one of the most popular architectures in various id161 tasks.
     __________________________________________________________________

recent variants and interpretations of resnet

   as resnet gains more and more popularity in the research community, its
   architecture is getting studied heavily. in this section, i will first
   introduce several new architectures based on resnet, then introduce a
   paper that provides an interpretation of treating resnet as an ensemble
   of many smaller networks.

resnext

   xie et al. [8] proposed a variant of resnet that is codenamed resnext
   with the following building block:
   [1*7jzj1rgh1y4vog1m4dsetw.png]
   left: a building block of [2], right: a building block of resnext with
   cardinality = 32

   this may look familiar to you as it is very similar to the inception
   module of [4], they both follow the split-transform-merge paradigm,
   except in this variant, the outputs of different paths are merged by
   adding them together, while in [4] they are depth-concatenated. another
   difference is that in [4], each path is different (1x1, 3x3 and 5x5
   convolution) from each other, while in this architecture, all paths
   share the same topology.

   the authors introduced a hyper-parameter called cardinality         the
   number of independent paths, to provide a new way of adjusting the
   model capacity. experiments show that accuracy can be gained more
   efficiently by increasing the cardinality than by going deeper or
   wider. the authors state that compared to inception, this novel
   architecture is easier to adapt to new datasets/tasks, as it has a
   simple paradigm and only one hyper-parameter to be adjusted, while
   inception has many hyper-parameters (like the kernel size of the
   convolutional layer of each path) to tune.

   this novel building block has three equivalent form as follows:
   [1*tzb5ol72dmw_sbb-gz1wja.png]

   in practice, the    split-transform-merge    is usually done by pointwise
   grouped convolutional layer, which divides its input into groups of
   feature maps and perform normal convolution respectively, their outputs
   are depth-concatenated and then fed to a 1x1 convolutional layer.

densely connected id98

   huang et al. [9] proposed a novel architecture called densenet that
   further exploits the effects of shortcut connections         it connects all
   layers directly with each other. in this novel architecture, the input
   of each layer consists of the feature maps of all earlier layer, and
   its output is passed to each subsequent layer. the feature maps are
   aggregated with depth-concatenation.
   [1*wpx_8ecetseccs8vdxtucw.png]

   other than tackling the vanishing gradients problem, the authors of [8]
   argue that this architecture also encourages feature reuse, making the
   network highly parameter-efficient. one simple interpretation of this
   is that, in [2][7], the output of the identity mapping was added to the
   next block, which might impede information flow if the feature maps of
   two layers have very different distributions. therefore, concatenating
   feature maps can preserve them all and increase the variance of the
   outputs, encouraging feature reuse.
   [1*gdfcbkmgn8at8_ip1opfma.png]

   following this paradigm, we know that the l_th layer will have k *
   (l-1) + k_0 input feature maps, where k_0 is the number of channels in
   the input image. the authors used a hyper-parameter called growth rate
   (k) to prevent the network from growing too wide, they also used a 1x1
   convolutional bottleneck layer to reduce the number of feature maps
   before the expensive 3x3 convolution. the overall archiecture is shown
   in the below table:
   [1*79h6lny8iirby1bua7ppbg.png]
   densenet architectures for id163

deep network with stochastic depth

   although resnet has proven powerful in many applications, one major
   drawback is that deeper network usually requires weeks for training,
   making it practically infeasible in real-world applications. to tackle
   this issue, huang et al. [10] introduced a counter-intuitive method of
   randomly dropping layers during training, and using the full network in
   testing.

   the authors used the residual block as their network   s building block,
   therefore, during training, when a particular residual block is enable,
   its input flows through both the identity shortcut and the weight
   layers, otherwise the input only flows only through the identity
   shortcut. in training time, each layer has a    survival id203    and
   is randomly dropped. in testing time, all blocks are kept active and
   re-calibrated according to its survival id203 during training.

   formally, let h_l be the output of the l_th residual block, f_l be the
   mapping defined by the l_th block   s weighted mapping, b_l be a
   bernoulli random variable that be only 1 or 0 (indicating whether a
   block is active), during training:
   [1*8e_cyme16v0r74aqpwhjta.png]

   when b_l = 1, this block becomes a normal residual block, and when b_l
   = 0, the above formula becomes:
   [1*ocp3agrhbsswpheej35iqw.png]

   since we know that h_(l-1) is the output of a relu, which is already
   non-negative, the above equation reduces to a identity layer that only
   passes the input through to the next layer:
   [1*jxt7dl3cxsf0h_uvijddyq.png]

   let p_l be the survival id203 of layer l during training, during
   test time, we have:
   [1*s1iko-ytmmo_xxc7swclpq.png]

   the authors applied a linear decay rule to the survival id203 of
   each layer, they argue that since earlier layers extract low-level
   features that will be used by later ones, they should not be dropped
   too frequently, the resulting rule therefore becomes:
   [1*vhf1mcnczstlh4wasvpgka.png]

   where l denotes the total number of blocks, thus p_l is the survival
   id203 of the last residual block and is fixed to 0.5 throughout
   experiments. also note that in this setting, the input is treated as
   the first layer (l = 0) and thus never dropped. the overall framework
   over stochastic depth training is demonstrated in the figure below.
   [1*trqrtovhgkjezerirsvaeq.png]
   during training, each layer has a id203 of being disabled

   similar to dropout [11], training a deep network with stochastic depth
   can be viewed as training an ensemble of many smaller resnets. the
   difference is that this method randomly drops an entire layer while
   dropout only drops part of the hidden units in one layer during
   training.

   experiments show that training a 110-layer resnet with stochastic depth
   results in better performance than training a constant-depth 110-layer
   resnet, while reduces the training time dramatically. this suggests
   that some of the layers (paths) in resnet might be redundant.

resnet as an ensemble of smaller networks

   [10] proposed a counter-intuitive way of training a very deep network
   by randomly dropping its layers during training and using the full
   network in testing time. veit et al. [14] had an even more
   counter-intuitive finding: we can actually drop some of the layers of a
   trained resnet and still have comparable performance. this makes the
   resnet architecture even more interesting as [14] also dropped layers
   of a vgg network and degraded its performance dramatically.

   [14] first provides an unraveled view of resnet to make things clearer.
   after we unroll the network architecture, it is quite clear that a
   resnet architecture with i residual blocks has 2 ** i different paths
   (because each residual block provides two independent paths).
   [1*u0yysxthezw5hw8wfwhjeq.png]

   given the above finding, it is quite clear why removing a couple of
   layers in a resnet architecture doesn   t compromise its performance too
   much         the architecture has many independent effective paths and the
   majority of them remain intact after we remove a couple of layers. on
   the contrary, the vgg network has only one effective path, so removing
   a single layer compromises this one the only path. as shown in
   extensive experiments in [14].

   the authors also conducted experiments to show that the collection of
   paths in resnet have ensemble-like behaviour. they do so by deleting
   different number of layers at test time, and see if the performance of
   the network smoothly correlates with the number of deleted layers. the
   results suggest that the network indeed behaves like ensemble, as shown
   in the below figure:
   [1*ghtikqfmwuybmhw4tskrwa.png]
   error increases smoothly as the the number of deleted layers increases

   finally the authors looked into the characteristics of the paths in
   resnet:

   it is apparent that the distribution of all possible path lengths
   follows a binomial distribution, as shown in (a) of the blow figure.
   the majority of paths go through 19 to 35 residual blocks.
   [1*kh6snoeflqli10espyd7jq.png]

   to investigate the relationship between path length and the magnitude
   of the gradients flowing through it. to get the magnitude of gradients
   in the path of length k, the authors first fed a batch of data to the
   network, and randomly sample k residual blocks. when back propagating
   the gradients, they propagated through the weight layer only for the
   sampled residual blocks. (b) shows that the magnitude of gradients
   decreases rapidly as the path becomes longer.

   we can now multiply the frequency of each path length with its expected
   magnitude of gradients to have a feel of how much paths of each length
   contribute to training, as in (c). surprisingly, most contributions
   come from paths of length 9 to 18, but they constitute only a tiny
   portion of the total paths, as in (a). this is a very interesting
   finding, as it suggests that resnet did not solve the vanishing
   gradients problem for very long paths, and that resnet actually enables
   training very deep network by shortening its effective paths.
     __________________________________________________________________

conclusion

   in this article, i revisited the compelling resnet architecture,
   briefly explained the intuitions behind its recent success. after that
   i introduced serveral papers that propose interesting variants of
   resnet or provide insightful interpretation of it. i hope it helps
   strengthen your understanding of this groundbreaking work.

   all of the figures in this article were taken from the original papers
   in the references.

references:

   [1]. a. krizhevsky, i. sutskever, and g. e. hinton. id163
   classification with deep convolutional neural networks. in advances in
   neural information processing systems,pages1097   1105,2012.

   [2]. k. he, x. zhang, s. ren, and j. sun. deep residual learning for
   image recognition. arxiv preprint arxiv:1512.03385,2015.

   [3]. k. simonyan and a. zisserman. very deep convolutional networks for
   large-scale image recognition. arxiv preprint arxiv:1409.1556,2014.

   [4]. c. szegedy, w. liu, y. jia, p. sermanet, s. reed, d. anguelov, d.
   erhan, v. vanhoucke, and a. rabinovich. going deeper with convolutions.
   in proceedings of the ieee conference on id161 and pattern
   recognition,pages 1   9,2015.

   [5]. r. srivastava, k. greff and j. schmidhuber. training very deep
   networks. arxiv preprint arxiv:1507.06228v2,2015.

   [6]. s. hochreiter and j. schmidhuber. long short-term memory. neural
   comput., 9(8):1735   1780, nov. 1997.

   [7]. k. he, x. zhang, s. ren, and j. sun. identity mappings in deep
   residual networks. arxiv preprint arxiv:1603.05027v3,2016.

   [8]. s. xie, r. girshick, p. dollar, z. tu and k. he. aggregated
   residual transformations for deep neural networks. arxiv preprint
   arxiv:1611.05431v1,2016.

   [9]. g. huang, z. liu, k. q. weinberger and l. maaten. densely
   connected convolutional networks. arxiv:1608.06993v3,2016.

   [10]. g. huang, y. sun, z. liu, d. sedra and k. q. weinberger. deep
   networks with stochastic depth. arxiv:1603.09382v3,2016.

   [11]. n. srivastava, g. hinton, a. krizhevsky, i. sutskever and r.
   salakhutdinov. dropout: a simple way to prevent neural networks from
   overfitting. the journal of machine learning research 15(1) (2014)
   1929   1958.

   [12]. a. veit, m. wilber and s. belongie. residual networks behave like
   ensembles of relatively shallow networks. arxiv:1605.06431v2,2016.

     * [18]machine learning
     * [19]deep learning
     * [20]id161
     * [21]data science
     * [22]towards data science

   (button)
   (button)
   (button) 2.5k claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of vincent fung

[24]vincent fung

   machine learning engineer [25]@tuputech; random traveller; rookie
   guitarist

     (button) follow
   [26]towards data science

[27]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.5k
     * (button)
     *
     *

   [28]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [29]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5281e2f56035
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_apdhmaaq34mm---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@vincent.fung13?source=post_header_lockup
  17. https://towardsdatascience.com/@vincent.fung13
  18. https://towardsdatascience.com/tagged/machine-learning?source=post
  19. https://towardsdatascience.com/tagged/deep-learning?source=post
  20. https://towardsdatascience.com/tagged/computer-vision?source=post
  21. https://towardsdatascience.com/tagged/data-science?source=post
  22. https://towardsdatascience.com/tagged/towards-data-science?source=post
  23. https://towardsdatascience.com/@vincent.fung13?source=footer_card
  24. https://towardsdatascience.com/@vincent.fung13
  25. http://twitter.com/tuputech
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/5281e2f56035/share/twitter
  32. https://medium.com/p/5281e2f56035/share/facebook
  33. https://medium.com/p/5281e2f56035/share/twitter
  34. https://medium.com/p/5281e2f56035/share/facebook
