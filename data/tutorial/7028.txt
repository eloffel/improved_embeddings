   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

tracking the millennium falcon with tensorflow

   go to the profile of nick bourdakos
   [11]nick bourdakos (button) blockedunblock (button) followfollowing
   nov 2, 2017

   iframe: [12]/media/2f35dc44fd9c2d00f3a51e9dc835bef3?postid=c8c86419225e

   custom id164 in action

   at the time of writing this post, most of the big tech companies (such
   as ibm, google, microsoft, and amazon) have easy-to-use visual
   recognition apis. some smaller companies also provide similar
   offerings, such as [13]clarifai. but none of them offer object
   detection.

     update: ibm and microsoft now have customizable id164
     apis.

   the following images were both tagged using the same [14]watson visual
   recognition default classifier. the first one, though, has been run
   through an id164 model first.
   [1*ucdxgfauhpewcmz3ioiuaw.png]
   [15]get started with watson

   id164 can be far superior to visual recognition on its own.
   but if you want id164, you   re going to have to get your
   hands a little dirty.

   depending on your use case, you may not need a custom id164
   model. [16]tensorflow   s id164 api provides a few models of
   varying speed and accuracy, that are based on the [17]coco dataset.

   for your convenience, i have put together a complete list of objects
   that are detectable with the coco models:
   [1*ka9vwfe4x7fgq61wngx8fq.png]

   if you wanted to detect logos or something not on this list, you   d have
   to build your own custom object detector. i wanted to be able to detect
   the millennium falcon and some tie fighters. this is obviously an
   extremely important use case, because you never know   

annotate your images

   training your own model is a lot of work. at this point, you may be
   thinking,    whoa, whoa, whoa! i don   t want to do a lot of work!    if so,
   you can check out [18]my other article about using the provided model.
   it   s a much smoother ride.

   you need to collect a lot of images and annotate them all. annotations
   include specifying the object coordinates and a corresponding label.
   for an image with two tie fighters, an annotation might look something
   like this:
<annotation>
    <folder>images</folder>
    <filename>image1.jpg</filename>
    <size>
        <width>1000</width>
        <height>563</height>
    </size>
    <segmented>0</segmented>
    <object>
        <name>tie fighter</name>
        <bndbox>
            <xmin>112</xmin>
            <ymin>281</ymin>
            <xmax>122</xmax>
            <ymax>291</ymax>
        </bndbox>
    </object>
    <object>
        <name>tie fighter</name>
        <bndbox>
            <xmin>87</xmin>
            <ymin>260</ymin>
            <xmax>95</xmax>
            <ymax>268</ymax>
        </bndbox>
    </object>
</annotation>

   for my star wars model, i collected 308 images including two or three
   objects in each. i   d recommend trying to find 200   300 examples of each
   object.

      wow,    you might be thinking,    i have to go through hundreds of images
   and write a bunch of xml for each one?   

   of course not! there are plenty of annotation tools out there, such as
   [19]labelimg and [20]rectlabel. i used [21]rectlabel, but it   s only for
   macos. it   s still a lot of work, trust me. it took me about three or
   four hours of nonstop work to annotate my entire dataset.

     update: i ended up building my own tool to annotate images and video
     frames. it   s a free online tool called cloud annotations that you
     can check out [22]here.

   if you have the money, you can pay somebody else, like an intern, to do
   it. or you can use something like [23]mechanical turk. if you are a
   broke college student like me and/or find doing hours of monotonous
   work fun, you   re on your own.

   we will need to do a little setup before we can run the script to
   prepare the data for tensorflow.

clone the repo

   start by cloning my repo [24]here.

     update: this repo is a bit out of date, i recommend checking out
     [25]this one for a much better time :)

     note: the following instructions are also out of date, i urge you to
     check out [26]the new walkthrough.

   the directory structure will need to look like this:
models
|-- annotations
|   |-- label_map.pbtxt
|   |-- trainval.txt
|   `-- xmls
|       |-- 1.xml
|       |-- 2.xml
|       |-- 3.xml
|       `-- ...
|-- images
|   |-- 1.jpg
|   |-- 2.jpg
|   |-- 3.jpg
|   `-- ...
|-- object_detection
|   `-- ...
`-- ...

   i   ve included my training data, so you should be able to run this out
   of the box. but if you want to create a model with your own data,
   you   ll need to add your training images to images, add your xml
   annotations to annotations/xmls, update trainval.txt, and
   label_map.pbtxt.

   trainval.txt is a list of file names that allows us to find and
   correlate the jpg and xml files. the following trainval.txt list would
   let us to find abc.jpg, abc.xml, 123.jpg, 123.xml, xyz.jpg and xyz.xml:
abc
123
xyz

   note: make sure your jpg and xml file names match, minus the extension.

   label_map.pbtxt is our list of objects that we are trying to detect. it
   should look something like this:
item {
  id: 1
  name: 'millennium falcon'
}
item {
  id: 2
  name: 'tie fighter'
}

running the script

   first, with python and pip installed, install the scripts requirements:
pip install -r requirements.txt

   add models and models/slim to your pythonpath:
export pythonpath=$pythonpath:`pwd`:`pwd`/slim

   important note: this must be run every time you open the terminal, or
   added to your ~/.bashrc file.

   run the script:
python object_detection/create_tf_record.py

   once the script finishes running, you will end up with a train.record
   and a val.record file. this is what we will use to train the model.

downloading a base model

   training an object detector from scratch can take days, even when using
   multiple [27]gpus. to speed up training, we   ll take an object detector
   trained on a different dataset and reuse some of it   s parameters to
   initialize our new model.

   you can download a model from this [28]model zoo. each model varies in
   accuracy and speed. i used faster_rid98_resnet101_coco.

   extract and move all the model.ckpt files to our repo   s root directory.

   you should see a file named faster_rid98_resnet101.config. it   s set to
   work with the faster_rid98_resnet101_coco model. if you used another
   model, you can find a corresponding config file [29]here.

ready to train

   run the following script, and it should start to train!
python object_detection/train.py \
        --logtostderr \
        --train_dir=train \
        --pipeline_config_path=faster_rid98_resnet101.config

   note: replace pipeline_config_path with the location of your config
   file.
global step 1:
global step 2:
global step 3:
global step 4:
...

   yay! it   s working!

   10 minutes later.
global step 41:
global step 42:
global step 43:
global step 44:
...

   computer starts smoking.
global step 71:
global step 72:
global step 73:
global step 74:
...

   how long is this thing supposed to run?

   the model that i used in the video ran for about 22,000 steps.

   wait, what?!

   i use a specked-out macbook pro. if you   re running this on something
   similar, i   ll assume you   re getting about one step every 15 seconds or
   so. at that rate it will take about three to four days of nonstop
   running to get a decent model.

   well, this is dumb. i don   t have time for this     

   [30]powerai to the rescue!

powerai

   powerai lets us train our model on ibm power systems with p100 gpus
   fast!

   it only took about an hour to train for 10,000 steps. however, this was
   just with one gpu. the real power in powerai comes from the the ability
   to do distributed deep learning across hundreds of gpus with up to 95%
   efficiency.

   with the help of powerai, ibm just set a new image recognition record
   of 33.8% accuracy in 7 hours. it surpassed the previous industry record
   set by microsoft         29.9% accuracy in 10 days.

   wayyy fast!

   since i   m not training on millions of images, i definitely didn   t need
   those kind of resources. one gpu will do.

creating a nimbix account

   nimbix provides developers a trial account with ten hours of free
   processing time on the powerai platform. you can register [31]here.

   note: this process is not automated, so it may take up to 24 hours to
   be reviewed and approved.

   once approved, you should receive an email with instructions on
   confirming and creating your account. it will ask you for a promotional
   code, but leave it blank.

   you should now be able to log in [32]here.

deploy the powerai notebooks application

   start by searching for powerai notebooks.
   [1*x41pzafftx055nnbwbaceg.png]

   click on it, and then choose tensorflow.
   [1*rfh7qvfgs_qzelrerfyaxq.png]

   choose the machine type of 32 thread power8, 128gb ram, 1x p100 gpu
   w/nvlink (np8g1).
   [1*i0yckwk54z2mdsbuma05vg.png]

   once started, the following dashboard panel will be displayed. when the
   server status turns to processing, the server is ready to be accessed.

   get the password by clicking on (click to show).

   then, click click here to connect to launch the notebook.
   [1*jlwttjt4rumxln69lkdfaa.png]

   log-in using the user name nimbix and the previously supplied password.
   [1*wxlluunvo_qpo-_p4kfjka.png]

start training

   get a new terminal window by clicking on the new pull-down and
   selecting terminal.
   [1*j8z6dljgjyvh13-kxfmajq.png]

   you should be greeted with a familiar face:
   [1*xogutc6f2nec4lxexro1rw.png]

   note: terminal may not work in safari.

   the steps for training are the same as they were when we ran this
   locally. if you   re using my training data then you can just clone my
   repo by running (if not, just clone your own repo):
git clone [33]https://github.com/bourdakos1/custom-object-detection.git

   then cd into the root directory:
cd custom-object-detection

   run this snippet, which will download the pre-trained
   faster_rid98_resnet101_coco model we downloaded earlier.
wget [34]http://storage.googleapis.com/download.tensorflow.org/models/object_det
ection/faster_rid98_resnet101_coco_11_06_2017.tar.gz
tar -xvf faster_rid98_resnet101_coco_11_06_2017.tar.gz
mv faster_rid98_resnet101_coco_11_06_2017/model.ckpt.* .

   then we need to update our pythonpath again, because this in a new
   terminal:
export pythonpath=$pythonpath:`pwd`:`pwd`/slim

   then we can finally run the training command again:
python object_detection/train.py \
        --logtostderr \
        --train_dir=train \
        --pipeline_config_path=faster_rid98_resnet101.config

downloading your model

   when is my model ready? it depends on your training data. the more
   data, the more steps you   ll need. my model was pretty solid at nearly
   4,500 steps. then, at about 20,000 steps, it peaked. i even went on and
   trained it for 200,000 steps, but it didn   t get any better.

   i recommend downloading your model every 5,000 steps or so and
   evaluating it to make sure you   re on the right path.

   click on the jupyter logo in the top left corner. then, navigate the
   file tree to custom-object-detection/train.

   download all the model.ckpt files with the highest number.
     * model.ckpt-step_number.data-00000-of-00001
     * model.ckpt-step_number.index
     * model.ckpt-step_number.meta

   note: you can only download one at a time.
   [1*2nuymsf4sovv1jm0zmwc8q.png]

   note: be sure to click the red power button on your machine when
   finished. otherwise, the clock will keep on ticking indefinitely.

export the id136 graph

   to use the model in our code, we need to convert the checkpoint files
   (model.ckpt-step_number.*) into a frozen [35]id136 graph.

   move the checkpoint files you just downloaded into the root folder of
   the repo you   ve been using.

   then run this command:
python object_detection/export_id136_graph.py \
        --input_type image_tensor \
        --pipeline_config_path faster_rid98_resnet101.config \
        --trained_checkpoint_prefix model.ckpt-step_number \
        --output_directory output_id136_graph

   remember export pythonpath=$pythonpath:`pwd`:`pwd`/slim .

   you should see a new output_id136_graph directory with a
   frozen_id136_graph.pb file. this is the file we need.

test the model

   now, run the following command:
python object_detection/object_detection_runner.py

   it will run your id164 model found at
   output_id136_graph/frozen_id136_graph.pb on all the images in
   the test_images directory and output the results in the
   output/test_images directory.

the results

   here   s what we get when we run our model over all the frames in this
   clip from star wars: the force awakens.

   iframe: [36]/media/83d34cd0e66f62ba42bafe28f888678e?postid=c8c86419225e

   thanks for reading! if you have any questions, feel free to reach out
   at bourdakos1@gmail.com, connect with me on [37]linkedin, or follow me
   on [38]medium and [39]twitter.

   if you found this article helpful, it would mean a lot if you gave it
   some applause     and shared to help others find it! and feel free to
   leave a comment below.

     * [40]tensorflow
     * [41]python
     * [42]ai
     * [43]machine learning
     * [44]tech

   (button)
   (button)
   (button) 2.1k claps
   (button) (button) (button) 9 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of nick bourdakos

[45]nick bourdakos

   medium member since sep 2017

   id161 addict at ibm watson

     (button) follow
   [46]freecodecamp.org

[47]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 2.1k
     * (button)
     *
     *

   [48]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [49]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c8c86419225e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/tracking-the-millenium-falcon-with-tensorflow-c8c86419225e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/tracking-the-millenium-falcon-with-tensorflow-c8c86419225e&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_gjdkvmhezbe2---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@bourdakos1
  12. https://medium.freecodecamp.org/media/2f35dc44fd9c2d00f3a51e9dc835bef3?postid=c8c86419225e
  13. https://www.clarifai.com/
  14. https://www.ibm.com/watson/services/visual-recognition/
  15. https://ibm.biz/bdjh2m
  16. https://www.tensorflow.org/
  17. http://cocodataset.org/#home
  18. https://medium.com/unsupervised-coding/dont-miss-your-target-object-detection-with-tensorflow-and-watson-488e24226ef3
  19. https://github.com/tzutalin/labelimg
  20. https://rectlabel.com/
  21. https://medium.com/@rectlabel
  22. https://github.com/cloud-annotations/training
  23. https://www.mturk.com/mturk/welcome
  24. https://github.com/bourdakos1/custom-object-detection
  25. https://github.com/cloud-annotations/training
  26. https://cloud-annotations.github.io/training/object-detection/cli/
  27. http://www.nvidia.com/object/what-is-gpu-computing.html
  28. https://github.com/bourdakos1/custom-object-detection/blob/master/object_detection/g3doc/detection_model_zoo.md
  29. https://github.com/bourdakos1/custom-object-detection/tree/master/object_detection/samples/configs
  30. https://developer.ibm.com/linuxonpower/deep-learning-powerai/
  31. https://www.nimbix.net/cognitive-journey/
  32. https://mc.jarvice.com/
  33. https://github.com/bourdakos1/custom-object-detection.git
  34. http://storage.googleapis.com/download.tensorflow.org/models/object_detection/faster_rid98_resnet101_coco_11_06_2017.tar.gz
  35. http://deepdive.stanford.edu/id136
  36. https://medium.freecodecamp.org/media/83d34cd0e66f62ba42bafe28f888678e?postid=c8c86419225e
  37. https://www.linkedin.com/in/nicholasbourdakos
  38. https://medium.com/@bourdakos1
  39. https://twitter.com/bourdakos1
  40. https://medium.freecodecamp.org/tagged/tensorflow?source=post
  41. https://medium.freecodecamp.org/tagged/python?source=post
  42. https://medium.freecodecamp.org/tagged/ai?source=post
  43. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  44. https://medium.freecodecamp.org/tagged/tech?source=post
  45. https://medium.freecodecamp.org/@bourdakos1
  46. https://medium.freecodecamp.org/?source=footer_card
  47. https://medium.freecodecamp.org/?source=footer_card
  48. https://medium.freecodecamp.org/
  49. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  51. https://medium.freecodecamp.org/@bourdakos1?source=post_header_lockup
  52. https://medium.com/p/c8c86419225e/share/twitter
  53. https://medium.com/p/c8c86419225e/share/facebook
  54. https://medium.freecodecamp.org/@bourdakos1?source=footer_card
  55. https://medium.com/p/c8c86419225e/share/twitter
  56. https://medium.com/p/c8c86419225e/share/facebook
