   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ml review
     * [9]       contribute
     * [10]            review.com newsletter
     __________________________________________________________________

deep neural network capsules

   [11]go to the profile of eugenio culurciello
   [12]eugenio culurciello (button) blockedunblock (button)
   followfollowing
   nov 7, 2017

   a recent paper on [13]capsules has many important insights for
   revolutionizing learning in deep neural networks. let us see why!

interesting characteristics of capsules

   here is a list of important points in the [14]paper and [15]video
   lecture:
     * coincidences in high-dimensional spaces are [very] rare, and when
       they occur, it is because the data    agrees   . objects are composed
       by parts with specific arrangements, when multiple objects parts,
       say: two eyes lay above a mouth, the id203 a face (a
       higher-level object) presence is high. but if object parts are not
       in the correct position, it means a higher-level object is not
       present. one eye with a mouth on side and another eye above is not
       a face!
     * the power of deep neural network is in how we connect layers
       together. we use fully-connected matrices to connect all features
       in one layer to all features in another layer, but this, beside for
       computational efficiencies, makes no sense! if the    eye    and
          mouth    neurons in a layer l connect to a    face    neuron in layer
       l+1, that makes sense. but if we connect    eyes   ,    wheels   ,    hands   ,
       etc to the neuron    face   , this will lead to more confusion of
       information, and poorer performance. for this reason we seek an
       algorithm that can guide the connection between layers in a more
       meaningful way than purely fully-connected layers, hoping that
       optimization algorithms will find the way    and if we have a better
       connection scheme, these optimization algorithms will more
       frequently find better and faster solutions
     * deep neural nets learn by back-propagation of errors over the
       entire network. in contrast real brains supposedly wire neurons by
       hebbian principles:    units that fire together, wire together   .
       capsules mimic hebbian learning in the way that:    a lower-level
       capsule prefers to send its output to higher level capsules whose
       activity vectors have a big scalar product with the prediction
       coming from the lower-level capsule   
     * in standard deep neural networks like alexnet and resnet, pooling
       between layers that downsample is a max operation over a fixed
       neighborhood (receptive field) of pixels (eg.: 2x2). this
       maxpooling layers have no learnable parameters. a better idea is to
       let layer learn how to pool and from a larger receptive field. an
       even better way is to do so in a dynamic way, just like in the
       [16]capsule paper.    for each possible parent, the capsule computes
       a    prediction vector    by multiplying its own output by a weight
       matrix    and as such capsules are connected via a    powerful dynamic
       routing mechanism to ensure that the output of the capsule gets
       sent to an appropriate parent in the layer above   . in summary,
          this type of    routing-by-agreement    should be far more effective
       than the very primitive form of routing implemented by max-pooling   
     * output is a vector, which allows for dynamic routing by agreement
       (last point above)
     * compared to standard id98 such as alexnet etc, capsules    replac[e]
       the scalar-output feature detectors of id98s with vector-output
       capsules and max-pooling with routing-by-agreement   
     * prediction: this is an important point that may go unobserved!
       capsules predict the activity of higher-layer capsules. predictive
       neural network capabilities is something we have been advocating
       for years, see: [17]here and [18]here         a new kind of neural
       network: predictive networks! we believe this is a extremely
       important characteristic of capsules that can set up apart from
       standard neural networks, as we argue in the linked posts
     * capsules are like cortical columns in human brains
     * capsules are supposed to produce equivariant features, like a 3d
       graphic model: given the model with just a simple transformation we
       can derive all its poses
     * capsules combination of capsules encodes objects parts and their
       relative positions, so an object instance can be accurately derived
       from the presence of the parts at the right locations, and not just
       their presence

network details

   here is a picture of capsnet, the neural network architecture using
   capsules. the interesting dynamic routing occurs between primarycaps
   and digitcaps.
   [1*0nxktteahqnyra411m3lxa.jpeg]
   capsnet, the neural network using capsules. the interesting dynamic
   routing occurs between primarycaps and digitcaps.
   [1*ea9vphiubriytdt6w11jvq.jpeg]

   dynamic routing is implemented with two main transformation as reported
   in these equations (2 in paper). u are the outputs of capsules in the
   layer below, and s are outputs from capsules on layer above. u_hat is a
   prediction of what the output from a capsule j above would be given the
   input from the capsule i in layer below. this is very interesting as an
   instantiation of [19]predictive neural networks.

   w_ij is a matrix of weights (like a linear layer) going from all
   capsules from one layer to the next. notice there are as many w
   matrices as i*j. c_ij is another matrix that combines the contribution
   of lower layer capsules into the next layer output. coefficients c_ij
   are computed with the dynamic routing algorithms described in the
   paper. the important point is that this is done by computing the
   agreement between the real output of next layer v and the prediction
   h_hat: b_ij     b_ij + u  _j|i * v_j

notes

   note 1: we like [20]this paper because capsules agrees with a lot of
   the work and thoughts we had in previous years, and that we named
      id91 learning   . see our previous publications here:
   [21]https://lnkd.in/dnsgjju
   [22]https://lnkd.in/dmnbuvs

   note 2: capsules [23]video by g. hinton

   note 3: great blog post on this [24]part i and [25]part ii.

about the author

   i have almost 20 years of experience in neural networks in both
   hardware and software (a rare combination). see about me here:
   [26]medium, [27]webpage, [28]scholar, [29]linkedin, and more   

     * [30]machine learning
     * [31]deep learning
     * [32]neural networks

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of eugenio culurciello

[34]eugenio culurciello

   i dream and build new technology

     (button) follow
   [35]ml review

[36]ml review

   highlights from machine learning research, projects and learning
   materials. from and for ml scientists, engineers an enthusiasts.

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [37]ml review
   never miss a story from ml review, when you sign up for medium.
   [38]learn more
   never miss a story from ml review
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/137be2877d44
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/mlreview/deep-neural-network-capsules-137be2877d44&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/mlreview/deep-neural-network-capsules-137be2877d44&source=--------------------------nav_reg&operation=register
   8. https://medium.com/mlreview?source=logo-lo_nwlnzidsyugt---33272dbbd858
   9. https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d
  10. http://mlreview.com/?medium
  11. https://medium.com/@culurciello?source=post_header_lockup
  12. https://medium.com/@culurciello
  13. https://arxiv.org/abs/1710.09829v1
  14. https://arxiv.org/abs/1710.09829v1
  15. https://www.youtube.com/watch?v=rtawfwuvnle
  16. https://arxiv.org/abs/1710.09829v1
  17. https://medium.com/towards-data-science/a-new-kind-of-deep-neural-networks-749bcde19108
  18. https://medium.com/towards-data-science/adversarial-predictive-networks-3aa7026d53d2
  19. https://towardsdatascience.com/a-new-kind-of-deep-neural-networks-749bcde19108?gi=cf8e3174d88a
  20. https://arxiv.org/abs/1710.09829v1
  21. https://lnkd.in/dnsgjju
  22. https://lnkd.in/dmnbuvs
  23. https://www.youtube.com/watch?v=rtawfwuvnle
  24. https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b
  25. https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66
  26. https://medium.com/@culurciello/
  27. https://e-lab.github.io/html/contact-eugenio-culurciello.html
  28. https://scholar.google.com/citations?user=segmqkiaaaaj
  29. https://www.linkedin.com/in/eugenioculurciello/
  30. https://medium.com/tag/machine-learning?source=post
  31. https://medium.com/tag/deep-learning?source=post
  32. https://medium.com/tag/neural-networks?source=post
  33. https://medium.com/@culurciello?source=footer_card
  34. https://medium.com/@culurciello
  35. https://medium.com/mlreview?source=footer_card
  36. https://medium.com/mlreview?source=footer_card
  37. https://medium.com/mlreview
  38. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  40. https://medium.com/p/137be2877d44/share/twitter
  41. https://medium.com/p/137be2877d44/share/facebook
  42. https://medium.com/p/137be2877d44/share/twitter
  43. https://medium.com/p/137be2877d44/share/facebook
