   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

understanding objective functions in neural networks.

   [16]go to the profile of lars hulstaert
   [17]lars hulstaert (button) blockedunblock (button) followfollowing
   nov 4, 2017
   [1*_paqio2ghxcekkpr0dguka.jpeg]

   this blog post is targeted towards people who have experience with
   machine learning, and want to get a better intuition on the different
   objective functions used to train neural networks.

introduction

   the reason why i decided to write this blogpost is three-fold:
     * blog posts often explain optimisation methods such as stochastic
       id119 or [18]variants thereof, but little time is spent
       explaining how objective functions are constructed for neural
       networks. why are the mean squared error (mse) and cross id178
       log loss used as objective functions for resp. regression and
       classification? why does adding a regularisation term make sense?
       the general idea is that by investigating objective functions one
       can learn why neural networks work the way they do or why they fail
       in other cases.

   [1*gnup7pn6sc42vaywvoamma.png]
   cross id178 log loss between the ground truth p and network output q,
   used in classification problems.
   [0*crzu7qetww3bwbwk.gif]
   mean squared error between the ground truth y and network output
   y_tilde, used in regression problems.
     * neural networks have the reputation of providing bad id203
       estimates and they suffer from [19]adversarial examples. in short:
       neural networks are often highly confident even when they are
       wrong. this can be an issue when they are deployed in real-life
       scenarios (e.g. self-driving cars). a self-driving car should be
       certain when making decisions at 90 mph. if we deploy deep learning
       pipelines, we should be aware of their strengths and weaknesses.
     * i always wondered how neural networks can be explained from a
       probabilistic perspective and how they fit in the wider framework
       of machine learning models. people tend to talk about network
       outputs as probabilities. is there a link between the probabilistic
       interpretation of neural networks and their objective functions?

   the main inspiration for this blog post is based on the work i did on
   bayesian neural networks with my friend [20]brian trippe at the
   [21]computational and biological learning lab in cambridge university.
   i highly recommend anyone to read brian   s [22]thesis on variational
   id136 in neural networks.

   disclaimer: at the computational and biological learning lab bayesian
   machine learning techniques are unapologetically taught as the way
   forward. as such, be aware of potential bias in this blog post (    ).
     __________________________________________________________________

supervised machine learning

   in supervised machine learning problems, we often consider a dataset d
   of observation pairs (x, y) and we try to model the following
   distribution:
   [1*-ozrvl5xsltzfl-d42u48g.jpeg]

   for example in image classification, x represents an image and y the
   corresponding image label. p(y|x,   ) represents the id203 of the
   label y given the image x and a model defined by parameters   .

   models that follow this approach are called discriminative models. in
   discriminative or conditional models the parameters that define the
   id155 distribution function p(y|x,   ) are inferred
   from the training data.

   based on the observed data x (input data or feature values) the model
   outputs a id203 distribution, which is then used to predict y
   (class or real value). different machine learning models require
   different parameters to be estimated. both linear models (e.g. logistic
   regression, defined by a set of weights equal to the number of
   features) and non-linear models (e.g. neural networks, defined by a set
   of weights for each layer) can be used to approximate the conditional
   id203 distributions.

   for typical classification problems the set of learnable parameters   
   is used to define a mapping from x to a [23]categorical distribution
   over the different labels. a discriminative classification model
   produces n probabilities as output, with n equal to the number of
   classes. each x belongs to a single class, but model uncertainty is
   reflected by outputting a distribution over the classes. typically, the
   class with maximum id203 is chosen when making a decision.
   [0*9vyzx8yvthrk9iq1.]
   in image classification, the network outputs a categorical distribution
   over image classes. the image above depicts the top 5 classes (classes
   with highest id203) for a test image.

   note that discriminative regression models often only output a single
   predicted value, instead of a distribution over all the real values.
   this is different from discriminative classification models where a
   distribution over all the possible classes is provided. does this mean
   discriminative models fall apart for regression? shouldn   t the output
   of the model tell us which regression values are more likely than
   others?

   although the single output of a discriminative regression model is
   misleading, the output of a regression model actually relates to a
   well-known id203 distribution, the gaussian distribution. as it
   turns out, the output of a discriminative regression model represents
   the mean of a gaussian distribution (a gaussian distribution is fully
   defined by a mean and a standard deviation). with this information, you
   can determine the likelihood of each real value given the input x.

   only the mean value of this distribution is typically modelled, and the
   standard deviation of the gaussian is either not modelled or chosen to
   be constant across all x. in discriminative regression models,    thus
   defines a mapping from x to the mean of a gaussian from which y is
   sampled. the mean value is almost always chosen when making a decision.
   models that output a mean and a standard deviation for a given x are
   more informative, as the model is able to express for which x it is
   uncertain (by increasing the standard deviation).
   [0*znk7xtndpid862usv.jpg]
   a model needs to be uncertain in regions where there is no training
   data and certain in regions where it has training data. such a model is
   displayed in the image above, from yarin gal   s blog post.

   other probabilistic models (such as gaussian processes) do a
   significantly better job at modelling uncertainty in regression
   problems, whereas discriminative regression models tend to be
   overconfident when modelling mean and standard deviation at the same
   time.

   a gaussian process is able to quantify uncertainty through explicitely
   modelling the standard deviation. the only downside of gaussian
   processes is that they do not scale well to large datasets. in the
   image below you can see that the gp model has small confidence
   intervals (determined with the standard deviation) around regions with
   a lot of data. in regions with few data points, the confidence
   intervals become significantly larger.
   [0*ixzjwnm8kr2wkdr-.png]
   a gaussian process model is certain at the data points, but uncertain
   at other places (image taken from [24]sklearn)

   a discriminative model is trained on the training dataset in order to
   learn the properties in the data that represent a class or real value.
   a model performs well if it is able to assign high id203 to the
   correct class of samples or a mean that is close to the true value in
   the test dataset.
     __________________________________________________________________

link with neural networks

   when neural networks are trained for a classification or regression
   task, the parameters of the aforementioned distributions (categorical
   and gaussian) are modelled using a neural network.

   this becomes clear when we attempt to determine the maximum likelihood
   estimate (id113) for the parameters    of the neural network. the id113
   corresponds with finding the parameters    for which the likelihood (or
   equivalent log likelihood) of the train data is maximised. more
   specifically, the following expression is maximised:
   [1*-gwnvmnu2giz4nsya5fw0q.png]

   p(y | x,   ) represents the probabilty of the true labels in the train
   data, when determined with the model. if p(y | x,   ) is closer to 1, it
   means that the model is able to determine the correct labels/means in
   the train set. given that the train data (x, y) consists of n
   observation pairs, the likelihood of the train data can be rewritten as
   a sum of log probabilities.

   in case of classification and regression, p(y|x,   ), the posterior
   id203 for a single pair (x, y), can be rewritten as the
   categorical and the gaussian distribution. in case of optimising neural
   networks, the goal is to shift the parameters in such a way that for a
   set of inputs x, the correct parameters of the id203 distribution
   y are given at the output (the regression value or class). this is
   typically achieved through id119 or variants thereof. in
   order to obtain a id113 estimate, the goal is to thus to optimise the
   model output with respect to the true output:
     * maximising the log of a categorical distribution corresponds with
       minimising the cross id178 between the approximated distribution
       and the true distribution.
     * maximising the log of a gaussian distribution corresponds with
       minimising the mean squared error between the approximated mean and
       true mean.

   the expression in the previous image can thus be rewritten, and results
   in respectively the cross id178 loss and the mean squared error , the
   objective functions for neural networks for classification regression.

   the non-linear function that a neural network learns to go from input
   to probabilities or means is hard to interpret compared to more
   traditional probabilistic models. while this is a significant downside
   of neural networks, the breadth of complex functions that a neural
   network is able to model also brings significant advantages. based on
   the derivation in this section it is clear that the objective functions
   for neural networks that arise when determining the id113 of the
   parameters can be interpreted probabilistically.

   an interesting interpretation of neural networks is their relation to
   generalised linear models (id75, id28,    ).
   instead of taking a linear combination of the features (as one does in
   glm   s), a neural network produces a highly non-linear combination of
   features.
     __________________________________________________________________

maximum-a-posteriori

   but, if neural networks can be interpreted as probabilistical models,
   why do they provide bad id203 estimates and suffer from
   adversarial examples? why do they require so much data?

   i like to think of different models (id28, neural
   networks,   ) as looking for good function approximators in different
   search spaces. while having an extremely large search space means that
   you have a lot of flexibility when modelling the posterior id203,
   it also comes at a cost. neural networks for example are proven to be
   universal function approximators. this means that with enough
   parameters they can approximate any function (awesome!). however, in
   order to ensure the function is well calibrated across the entire data
   space, exponentially large data sets are required (expensive!).

   it is important to know that a standard neural network is typically
   optimised using id113. optimisation using id113 tends to overfit to the
   train data and a lot of data is required to obtain decent results. the
   goal in machine learning is not to find a model that explains the
   training data well. you rather try to find a model that generalises
   well to unseen data, and is unsure for data that is significantly
   different from the train data.

   using a maximum-a-posteriori (map) approach is a valid alternative that
   is often explored when a probabilistic model suffers from overfitting.
   so what does map correspond to in the context of neural networks? wat
   impact does it have on the objective function?

   similar to id113, map can also be rewritten as an objective function in
   the context of neural networks. essentially, with map, you are
   maximising the id203 of a set of parameters    given the data
   while assuming a prior distribution on    :
   [1*cmgzfnmoaooiruohxe6k0w.jpeg]

   with id113, only the first element of the formula is taken into account
   (how well the model explains the train data). with map, it is also
   important that the model satisfies prior assumptions (how well does   
   fit the priors) in order to reduce overfitting.

   putting a gaussian prior with 0 mean on    corresponds with l2
   regularisation added to the objective (ensuring a lot of small
   weights), whereas putting a laplacian prior on    corresponds with l1
   regularisation added to the objective (ensuring a lot of weights with
   value 0).
   [1*ljy2cnh_ynk1c3piqmkhmg.png]
   l1 regularisation on the left and l2 regularisation on the right.
     __________________________________________________________________

a full bayesian approach

   both in the case of id113 and map a single model (with a single set of
   parameters) is used. especially for complex data, such as images, it is
   not unlikely that certain regions in the data space are not well
   covered. the output of the model in these regions depends on the random
   initialisation of the model and the training procedure, resulting in
   poor id203 estimates for points in uncovered segments of the data
   space.

   although map ensures that the model does not overfit too much in these
   regions, it still results in models that are too confident. in a full
   bayesian approach, this is resolved by averaging out over multiple
   models, resulting in better uncertainty estimates. instead of a single
   set of parameters, the goal is to model a distribution over the
   parameters. if all models (different parameter settings) provide
   different estimates in uncovered regions, this indicates large
   uncertainty in that region. by averaging out over these models, the end
   result is a model that is uncertain in those regions. this is exactly
   what we want!

   in the next blog post i will discuss bayesian neural networks and how
   they attempt to solve the aforementioned problems of traditional neural
   networks. bayesian neural networks (bnn   s) are still a work of active
   research, and there is no clear winner approach when training them.

   i highly recommend the blog post by yarin gal on [25]uncertainty in
   deep learning!

     * [26]machine learning
     * [27]bayesian statistics
     * [28]bayesian machine learning
     * [29]data science
     * [30]deep learning

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of lars hulstaert

[32]lars hulstaert

   data scientist at microsoft. previously masters student at cambridge,
   engineering student in ghent. i like connecting the dots.

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d217cb068138
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_bt3h16ed845z---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@lars.hulstaert?source=post_header_lockup
  17. https://towardsdatascience.com/@lars.hulstaert
  18. http://ruder.io/optimizing-gradient-descent/
  19. https://blog.openai.com/adversarial-example-research/
  20. http://www.briantrippe.com/
  21. http://learning.eng.cam.ac.uk/public/
  22. http://briantrippe.com/mphilthesissubmission.pdf
  23. https://en.wikipedia.org/wiki/categorical_distribution
  24. https://www.google.be/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=0ahukewiqjptiiqxxahwldbokhrdacdqqjhwibq&url=http://scikit-learn.org/0.17/auto_examples/gaussian_process/plot_gp_regression.html&psig=aovvaw0im8omkpy_j-hgrlffryi1&ust=1509890495868115
  25. http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html
  26. https://towardsdatascience.com/tagged/machine-learning?source=post
  27. https://towardsdatascience.com/tagged/bayesian-statistics?source=post
  28. https://towardsdatascience.com/tagged/bayesian-machine-learning?source=post
  29. https://towardsdatascience.com/tagged/data-science?source=post
  30. https://towardsdatascience.com/tagged/deep-learning?source=post
  31. https://towardsdatascience.com/@lars.hulstaert?source=footer_card
  32. https://towardsdatascience.com/@lars.hulstaert
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/d217cb068138/share/twitter
  39. https://medium.com/p/d217cb068138/share/facebook
  40. https://medium.com/p/d217cb068138/share/twitter
  41. https://medium.com/p/d217cb068138/share/facebook
