   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]intuition machine
     * [9]patterns
     * [10]methodology
     * [11]strategy
     * [12]deep learning playbook
     __________________________________________________________________

revisiting deep learning as a non-equilibrium process

   go to the profile of carlos e. perez
   [13]carlos e. perez (button) blockedunblock (button) followfollowing
   nov 10, 2017
   [1*h0a047yhtgmaghqheymqia.jpeg]
   [14]https://unsplash.com/photos/wkt3te5aqu0

   last year, the best paper award for iclr 2017 went to    [15]re-thinking
   generalization    by chiyuan zhang et al. for a good review, here is a
   video of his talk:

   iframe: [16]/media/e279f7bb700f0f42feec90fb0de3ce44?postid=9cedb93a13a2

   the key take away from his team   s discovery is that the nature of deep
   learning systems is remarkably very different from other classical
   machine learning systems. one of the biggest misunderstandings about
   deep learning is that it is just a higher dimensional form of curve
   fitting and thus solved from the perspective of optimization
   techniques. this is an incorrect notion can be due to the fact that the
   way id158s (ann) is taught to many is that it is
   just a larger form of id28. alternatively, for the more
   experienced machine learning expert, everything can be framed from the
   viewpoint of an optimization problem.

   the last viewpoint in fact has been detrimental to the field for so
   long. if you take the optimization viewpoint, then deep learning is
   just too high dimensional and non-convex that it should be
   theoretically impossible to achieve convergence to a global minima.
   unfortunately for the optimization gurus, this experimentally isn   t
   even true. the simplest of methods, stochastic id119, works
   surprisingly too well. something else is going on that has eluded the
   orthodox explanation of how optimization is supposed to work. zhang   s
   discovery provided experimental evidence that we have to rethink our
   current (obviously incomplete) theories.

   despite the thousands of papers that are submitted to the various deep
   learning conferences this year, there are very few papers that attempt
   to explore explain the true nature of deep learning. deep learning
   research is really just pure alchemy and piss-poor explanations are
   backed with lots of hand waving that   s disguised as mathematics.
   everyone in the academic community is so vested in pleasing everyone
   else that nobody wants to call out the bs. fortunately, we have some
   brave souls that work on the real theoretical issues. papers of this
   kind are [17]unfortunately the kind that usually gets rejected. it   s
   just a fact of reality that when you need to understand a system that
   you have to work with a simpler system. yet, showing results using
   mnist is considered not state-of-the-art and therefore should be
   ignored. the only folks that get a pass are celebrities like geoffrey
   hinton. it   s a sad reality where celebrity and alchemy are favored over
   real science.

   okay, i   m done with my rant. let   s look at some interesting papers that
   have just recently published.

   here   s a new paper:    [18]a bayesian perspective on generalization and
   stochastic id119   . which begs the question, why are smart
   people invoking spells like    bayesian intuition    to obfuscate that they
   are actually just doing alchemy? i suspect that the use of the term
   bayesian or gaussian in papers is more to play to the sentiments of the
   orthodoxy and is at the expense of more precise language but equivalent
   language. here are some quotes from the paper:

     the contribution is often called the    occam factor   , because it
     enforces occam   s razor; when two models describe the data equally
     well, the simpler model is usually better.

   and

     we conclude that bayesian model comparison is quantitatively
     consistent with the results of zhang et al. (2016) in linear models
     where we can compute the evidence, and qualitatively consistent with
     their results in deep networks where we cannot.

   i am not impressed at this paper   s attempt to justify    bayesian
   intuition   . fortunately, there a much better paper on exactly the same
   subject:(   [19]stochastic id119 performs variational
   id136, converges to limit cycles for deep networks   ). this also
   explores sgd as an implicit generalization method and remarks:

     we prove that such    out-of-equilibrium    behavior is a consequence of
     the fact that the gradient noise in sgd is highly non-isotropic; the
     covariance matrix of mini-batch gradients has a rank as small as 1%
     of its dimension.

   the paper has a gem of an observation:

     there is however a disconnect between these two directions due to
     the fact that while adding external gradient noise helps in theory,
     it works poorly in practice. instead,    noise tied to the
     architecture    works better, e.g., dropout, or small mini-batches.

   in short, noise in deep learning arises because of the diversity of
   training and architecture. it   s not something that you artificially add
   so you can justify bayesian intuition.

   this notion of the importance of architecture is further analyzed in a
   recent paper    [20]intriguing properties of adversarial examples    where
   the authors from google brain use their    neural architecture search   
   infrastructure to discover new architectures that are less susceptible
   to adversarial features. they conclude that the size of the model
   network does not correlate with adversarial robustness. rather
   adversarial robustness is strongly correlated with    clean accuracy   .
   the principles behind building a high    clear accuracy    architecture
   appear to be an open question. their brute force search found the
   following network:
   [1*grluhkkhi7ofyllmxbhhda.jpeg]

   which just happens to be longer and thinner than the baseline best nas
   architecture. the longer the network perhaps alluding to a larger
   effect of transient chaos (discussed later). (i don   t have an
   explanation for the narrowness other than its lower complexity)

   these two papers study the same subject but the approach is starkly
   different. in the first paper, the authors attempt to explain that
   bayesian id136 still holds with deep learning. in the second paper,
   the authors explain that this is a [21]non-equilibrium phenomenon and
   we can   t know enough because deep learning training is truncated with
   insufficient epochs.

   you can see the problem here, satisfying bayesian id136 or occam   s
   razor does not signify truth. all it signifies is that one   s own
   beliefs are validated and that the behavior of the inspected system
   validates those beliefs.

   the second paper, in contrast, explores the aspects that are different
   about deep learning and attempts to make the analogy with other
   physical theories. in short, it   s not attempting to fix a round peg
   into a square hole. reality is what it is and it is our business as
   scientists to explore a rich variety of models to explain our reality.
   the real problem is that many researchers aren   t skilled in the
   mathematics of statistical mechanics. they use whatever tool is at
   their disposal, unfortunately, it is some antiquated 18th-century math
   in the form of id47.

   in this paper, the authors argue that sgd settles not at a local minima
   but rather in a limit-cycle:
   [1*4qlj-2xhqxxffeyjekor9g.png]
   ( c ) sgd converges to limit cycles around the saddle point at
   the origin.

   the paper proposes the use of a    local id178    to discover these
   limits cycles. they cite a paper:    [22]unreasonable effectiveness of
   learning neural networks: from accessible states and robust ensembles
   to basic algorithmic schemes    that makes the claim about the smoothness
   of the local id178 as compared to the original objective function:
   [1*eegvdy6yj_w4uyms7v1kaw.png]
   [23]https://arxiv.org/pdf/1605.06444.pdf

   in a previous blog post, i pointed to recent papers that describe the
   [24]two phases of id119. the signal to noise ratio is
   extremely low at the minima, it is chaotic down there and any
   significant increase in learning rate can violently kick one out of
   that minima. in addition, there are many of these minima out there and
   finding out which one of them leads to generalization is a wide open
   question. the current consensus is a wide basin is a preferred choice.
   i don   t know if this notion should give a researcher a warm fuzzy
   feeling that it is the right choice!

   there are several papers that also come from those trained in a field
   other than statistics, that will likely not see the light of day (or
   rather accepted in a conference). the incomprehensibility to the
   reviewer trained only in statistics is grounds for rejection. here is
   one where charles martin and michael mahoney apply [25]a statistical
   mechanics approach to further understanding the    rethinking
   generalization    paper (   rethinking generalization requires revisiting
   old ideas: statistical mechanics approaches and complex learning
   behavior   ). the authors argue that:

     in particular, methods that implement id173 by, e.g.,
     adding a capacity control function to the objective and
     approximating the modified objective, performing dropout, adding
     noise to the input, and so on, do not substantially improve the
     situation. indeed, the only control parameter that has a substantial
     id173 effect is early stopping.

   there    is indeed mass confusion today about what kind of id173
   leads to generalization. in fact, there is experimental evidence that
   performing sgd [26]without id173 also leads to good
   generalization. there are even papers that show that certain kinds of
   id173 are detrimental to generalization. here   s a recent
   survey:    [27]id173 for deep learning: a taxonomy   . there is
   this notion that the learning process in deep learning is
   [28]   transient chaos   , that is convergence is in a chaotic regime and
   that given enough epochs that true chaotic phenomena will be revealed.
   compare the output at different depths as a function of the input:
   [1*pkq4sau9rujpoaw56cumwa.png]
   [29]https://arxiv.org/pdf/1606.05340.pdf

   however, one has got to at least ask, why is it    chaotic    down there
   where generalization can be found? could it be perhaps that we have
   encountered [30]a many-body problem? that is, an intelligent system
   should have multiple perspectives of reality and thus the transition to
   each perspective is of a fluid nature?

   there   s no magic measure to achieve generalization without actually
   looking at the validation set, this is what most researchers seems to
   be completely blind of. a system that generalizes well is one that
   works well with the validation set. it does not have some kind of
   mystical precognition skills that tells it that one minima are better
   than another because of some bayesian belief that wider or simpler is
   better. this is why meta-learning methods are effective because it has
   seen enough validation sets to basically learn to be adaptive.

   the paper by martin et. al. proposes to simplify id173 by
   focusing on just two knobs for controlling deep learning:

     we propose that the two parameters used by zhang et al. (and many
     others), which are control parameters used to control the learning
     process, are directly analogous to load-like and temperature-like
     parameters in the traditional sm approach to generalization.

   they explored the design space using a simple model of deep learning
   and propose the following phase diagram:
   [1*yby1duhq5fauer-iq4exyg.png]
   [31]https://arxiv.org/pdf/1710.09553.pdf

   this indeed is a refreshing idea that needs to be explored further
   using more complex deep learning architectures.

   btw, if you are lost in this discussion, meaning words like
   id173, implicit id173, generalization etc are new,
   then here   s a screenshot of the topics in a new course at stanford
   that   ll give you some bearing:
   [1*fxi9kvhnrwicnj2loc7yba.png]
   [32]https://stats385.github.io/

   reference: [33]https://arxiv.org/pdf/1708.03395.pdf ,
   [34]https://arxiv.org/pdf/1712.04741.pdf.

   explore more in this new book:
   [1*klghw8y2ldrko117bnqswq.png]
   explore deep learning: [35]artificial intuition: the improbable deep
   learning revolution

     * [36]machine learning
     * [37]deep learning
     * [38]artificial intelligence

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of carlos e. perez

[39]carlos e. perez

   medium member since feb 2018

   author of artificial intuition and the deep learning playbook    
   linkedin.com/in/ceperez

     (button) follow
   [40]intuition machine

[41]intuition machine

   deep learning patterns, methodology and strategy

     * (button)
       (button) 1k
     * (button)
     *
     *

   [42]intuition machine
   never miss a story from intuition machine, when you sign up for medium.
   [43]learn more
   never miss a story from intuition machine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9cedb93a13a2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/revisiting-deep-learning-as-a-non-equilibrium-process-9cedb93a13a2&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/intuitionmachine/revisiting-deep-learning-as-a-non-equilibrium-process-9cedb93a13a2&source=--------------------------nav_reg&operation=register
   8. https://medium.com/intuitionmachine?source=logo-lo_7n0n3bdklkoq---d777623c68cf
   9. https://medium.com/intuitionmachine/tagged/design-patterns
  10. https://medium.com/intuitionmachine/tagged/agile-methodology
  11. https://medium.com/intuitionmachine/tagged/strategy
  12. https://deeplearningplaybook.com/
  13. https://medium.com/@intuitmachine
  14. https://unsplash.com/photos/wkt3te5aqu0
  15. https://medium.com/intuitionmachine/rethinking-generalization-in-deep-learning-ec66ed684ace
  16. https://medium.com/media/e279f7bb700f0f42feec90fb0de3ce44?postid=9cedb93a13a2
  17. https://medium.com/intuitionmachine/eight-deserving-deep-learning-papers-that-were-rejected-at-iclr-2017-119e19a4c30b
  18. https://arxiv.org/pdf/1710.06451.pdf
  19. https://arxiv.org/abs/1710.11029
  20. https://arxiv.org/abs/1711.02846v1
  21. https://medium.com/intuitionmachine/deep-learning-is-non-equilibrium-information-dynamics-b00baa16b135
  22. https://arxiv.org/pdf/1605.06444.pdf
  23. https://arxiv.org/pdf/1605.06444.pdf
  24. https://medium.com/intuitionmachine/the-peculiar-behavior-of-deep-learning-loss-surfaces-330cb741ec17
  25. https://arxiv.org/abs/1710.09553
  26. http://www.keerthis.com/optimization_and_generalization_keerthi_criteo_november_08_2017.pptx
  27. https://openreview.net/pdf?id=skhkeixaw
  28. https://arxiv.org/pdf/1606.05340.pdf
  29. https://arxiv.org/pdf/1606.05340.pdf
  30. https://medium.com/intuitionmachine/the-deep-learning-many-body-problem-3665d3947628
  31. https://arxiv.org/pdf/1710.09553.pdf
  32. https://stats385.github.io/
  33. https://arxiv.org/pdf/1708.03395.pdf
  34. https://arxiv.org/pdf/1712.04741.pdf
  35. https://gumroad.com/products/ihdj
  36. https://medium.com/tag/machine-learning?source=post
  37. https://medium.com/tag/deep-learning?source=post
  38. https://medium.com/tag/artificial-intelligence?source=post
  39. https://medium.com/@intuitmachine
  40. https://medium.com/intuitionmachine?source=footer_card
  41. https://medium.com/intuitionmachine?source=footer_card
  42. https://medium.com/intuitionmachine
  43. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  45. https://medium.com/@intuitmachine?source=post_header_lockup
  46. https://medium.com/p/9cedb93a13a2/share/twitter
  47. https://medium.com/p/9cedb93a13a2/share/facebook
  48. https://medium.com/@intuitmachine?source=footer_card
  49. https://medium.com/p/9cedb93a13a2/share/twitter
  50. https://medium.com/p/9cedb93a13a2/share/facebook
