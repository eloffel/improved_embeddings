     * [1]home
     * [2]posts
     * [3]projects
     * [4]resume
     * [5]talks

id22 explained
       posted on 2017-11-10 by kendrick tan
         ______________________________________________________________

       assumed knowledge: [6]convolutional neural networks, [7]variational
       autoencoders
       disclaimer: this article does not cover the mathematics behind
       id22, but rather the intuition and motivation behind
       them.
         ______________________________________________________________

what are id22 and why do they exist?
       the [8]capsule network is a new type of neural network architecture
       conceptualized by [9]geoffrey hinton, the motivation behind capsule
       networks is to address some of the short comings of convolutional
       neural networks (convnets), which are listed below:

problem 1: convnets are translation invariant [10][[1]]
       what does that even mean? imagine that we had a model that predicts
       cats. you show it an image of a cat, it predicts that it   s a cat.
       you show it the same image, but shifted to the left, it still
       thinks that it   s a cat without predicting any additional
       information.

                                [meiuqt8.png]

                     figure 1.0: translation invariance
       what we want to strive for is translation equivariance. that means
       that when you show it an image of a cat shifted to the right, it
       predicts that it   s a cat shifted to the right. you show it the same
       cat but shifted towards the left, it predicts that its a cat
       shifted towards the left.

                                [u4ydpq6.png]

                    figure 1.1: translation equivariance
       why is this a problem? convnets are unable to identify the position
       of one object relative to another, they can only identify if the
       object exists in a certain region, or not. this results in
       difficulty correctly identifying objects that hold spatial
       relationships between features.
       for example, a bunch of randomly assembled face parts will look
       like a face to a convnet, because all the key features are there:

                                [0zyapt3.png]

                     figure 1.2: translation invariance
       if id22 do work as proposed, it should be able to
       identify that the face parts aren   t in the correct position
       relative to one another, and label it correctly:

                                [mlt9suh.png]

                    figure 1.3: translation equivariance

problem 2: convnets require a lot of data to generalize [11][[2]].
       in order for the convnets to be translation invariant, it has to
       learn different filters for each different viewpoints, and in doing
       so it requires a lot of data.

problem 3: convnets are a bad representation of the human vision system
       according to hinton, when a visual stimulus is triggered, the brain
       has an inbuilt mechanism to    route    low level visual data to parts
       of the brain where it belives can handle it best. because convnets
       uses layers of filters to extract high level information from low
       level visual data, this routing mechanism is absent in it.

                                [cvte4hg.png]

                          figure 1.4: humans vs id98
       moreover, the human vision system imposes coordinate frames on
       objects in order to represent them. for example:

                                [w8peps6.png]

                   figure 1.5: imposing a coordinate frame
       and if we wanted to compare the object in figure 1.6 to say the
       letter    r   , most people would perform mental rotation on the object
       to a point of reference which they   re familiar to before making the
       comparison. this is just not possible in convnets due to the nature
       of their design.

               [portlygracefulbichonfrise-size_restricted.gif]

        figure 1.6: mental rotation then realizing it   s not    r   
       we   ll explore this idea of imposing a bounding rectangle and
       performing rotations on objects relative to their coordinates later
       on.

how do id22 solve these issues?

inverse graphics

     you can think of (computer) vision as    inverse graphics    - geoffrey
     hinton [[[12]3]]
       what is inverse graphics? simply put, it   s the inverse of how a
       computer renders an object onto the screen. to go from a mesh
       object onto pixels on a screen, it takes the pose of the whole
       object, and multiplies it by a transformation matrix. this outputs
       the pose of the object   s part in a lower dimension (2d), which is
       what we see on our screens.

                                [dcmdyhl.png]

        figure 2.0: computer graphics rendering process (simplified)
       so why can   t we do the opposite? get pixels from a lower dimension,
       multiply it by the inverse of the transformation matrix to get the
       pose of the whole object?

                                [foqnq3c.png]

                   figure 2.1: inverse graphics (proposed)
       yes we can (on an approximation level)! and by doing that, we can
       represent the relationship between the object as a whole and the
       pose of the part as a matrix of weights. and these matrices of
       weights are viewpoint invariant, meaning that however much the pose
       of the part has changed we can get back the pose of the whole using
       the same matrix of weights.
       this gives us complete independence between the viewpoints of the
       object in a matrix of weights. the translation invariance is now
       represented in the matrix of weights, and not in the neural
       activity.

where do we get the matrix of weights to represent the relationship?

                                [2fhuqrq.png]

     figure 2.2 extract from dynamic routing between capsules [[[13]4]]
       in [14]hinton   s paper he describes that the id22 use a
       reconstruction loss as a id173 method, similiar to how an
       autoencoder operates. why is this significant?

                                [ecmc5fr.jpg]

                     figure 2.3 autoencoder architecture
       in order to reconstruct the input from a lower dimensional space,
       the encoder and decoder needs to learn a good matix representation
       to relate the relationship between the latent space and the input,
       sounds familiar?
       to summarize, by using the reconstruction loss as a regularizer,
       the capsule network is able to learn a global linear manifold
       between a whole object and the pose of the object as a matrix of
       weights via unsupervised learning. as such, the translation
       invariance is encapsulated in the matrix of weights, and not during
       neural activity, making the neural network translation
       equivariance. therefore, we are in some sense, performing a    mental
       rotation and translation    of the image when it gets multiplied by
       the global linear manifold!

dynamic routing
       routing is the act of relaying information to another actor who can
       more effectively process it. convnets currently perform routing via
       pooling layers, most commonly being max pooling.

                [main-qimg-8afedfb2f82f279781bfefa269bc6a90]

           figure 3.0: max pooling with a 2x2 kernel and 2 stride
       max pooling is a very primitive way to do routing as it only
       attends to the most active neuron in the pool. id22 is
       different as it tries to send the information to the capsule above
       it that is best at dealing with it.

                                [vd9kw7m.png]

     figure 3.1: extract from dynamic routing between capsules [[[15]4]]

conclusion
       using a novel architecture that mimics the human vision system,
       id22 strives for translation equivariance instead of
       translation invariance, allowing it to generalize to a greater
       degree from different view points with less training data.
       you can check out a [16]barebone implementation of capsule network
       here, which is just a cleaned up version of [17]gram.ai   s
       implementaion.
         ______________________________________________________________

references
         1. [18]id98: translation equivariance and invariance
         2. [19]the impact of imbalanced training data for convolutional
            neural networks
         3. [20]what is wrong with convolutional neural networks?
         4. [21]dynamic routing between capsules
         ______________________________________________________________

       please enable javascript to view the [22]comments powered by
       disqus.
       [23]github  /   [24]twitter  /   [25]linkedin  /   me [at]
       kndrck.co

references

   1. https://kndrck.co/
   2. https://kndrck.co/posts/
   3. https://kndrck.co/projects/
   4. https://goo.gl/m2egr2
   5. https://kndrck.co/talks/
   6. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
   7. http://kvfrans.com/variational-autoencoders-explained/
   8. https://arxiv.org/abs/1710.09829
   9. http://www.cs.toronto.edu/~hinton/
  10. https://aboveintelligent.com/ml-id98-translation-equivariance-and-invariance-da12e8ab7049
  11. https://www.kth.se/social/files/588617ebf2765401cfcc478c/phensmandmasko_dkand15.pdf
  12. https://youtu.be/rtawfwuvnle?t=1750
  13. https://arxiv.org/pdf/1710.09829.pdf
  14. https://arxiv.org/pdf/1710.09829.pdf
  15. https://arxiv.org/pdf/1710.09829.pdf
  16. https://gist.github.com/kendricktan/9a776ec6322abaaf03cc9befd35508d4
  17. https://github.com/gram-ai/capsule-networks
  18. https://aboveintelligent.com/ml-id98-translation-equivariance-and-invariance-da12e8ab7049
  19. https://www.kth.se/social/files/588617ebf2765401cfcc478c/phensmandmasko_dkand15.pdf
  20. https://youtu.be/rtawfwuvnle
  21. https://arxiv.org/pdf/1710.09829.pdf
  22. http://disqus.com/?ref_noscript
  23. https://github.com/kendricktan
  24. https://twitter.com/kendricktrh
  25. https://www.linkedin.com/in/tankendrick/
