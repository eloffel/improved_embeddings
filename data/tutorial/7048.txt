   [1]

linkedin

     * [2]sign in
     * [3]join now

   id5: the mental model!

                  id5: the mental model!

   published on august 28, 2016august 28, 2016     17 likes     0 comments

   [4]ibrahim sobh - phd

[5]ibrahim sobh - phd[6]follow

senior expert of artificial intelligence, valeo

     * (button) like17
     * (button) comment0
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       1

   id5 ...

     "what i cannot create, i do not understand    - richard feynman


   introduction

   in supervised learning, we have both data and labels (x, y). our goal
   is to learn a function to map x->y. for example, classification,
   regression and id164.

    on the other hand, in unsupervised learning, we only have data (x).
   the goal is to learn some structure or pattern in the data. for
   example, id91, id84 and generative models.
   generative models, simply, can generate data.

   traditional autoencoders

   as described [7]here, it is a beautiful idea! where it
   enables unsupervised id171. the features are useful for
   reconstruction of the input data, and also can be used for other
   supervised tasks.

   can we generate new data?

     the basic idea is to state explicitly id203 assumptions on how
     do we think the data is generated. given those assumptions, we try
     to fit our model to the data and use it to generate more data.

    id5 (vae)

   we assume there is a prior distribution for latent state z. and we
   assume some other conditional distribution, once we have the latent
   states we can sample from the other distribution to generate the data.

   [:0]


   process:
     * sample from prior p(z) to get z (latent state)
     * sample from p(x|z) to get x (data)

   if x is an image then z is some summary of the image, like class or
   type of the image, orientation, ...

   now, we need to find the parameters of p(z) and p(x|z) without access
   the latent states z.
     * assume p(z) is gaussian for simplecity
     * assume p(x|z) is a diagonal gaussian

    z -> decoder net -> (mean and diagonal covariance)

   now, we want to estimate latent state z from the observed data x
     * p(z|x) = p(x|z)p(z)/p(x)

    p(x) is intractable integral!

   we estimate: q(z|x)

   x-> encoder network ->  (mean and diagonal covariance)

    stack the two parts together, we get the full process:

     x -> encoder -> q(z|x) -> sample -> z -> decoder -> p(z|x) -> sample
     -> reconstruct x

   looks like traditional autoencoder, however the parameters of the model
   are trained through the sum of two id168s:
     * original: reconstruction loss forcing the decoded samples to match
       the initial inputs.
     * new: kl divergence between the learned latent distribution and the
       prior distribution, acting as a id173 term.

    after training, we can generate new data! it is easy to sample from
   the gaussian distribution z:

     sample from p(z) -> z -> decoder net -> mean and cov ->  sample from
     p(x|z) -> generated x

   if z is a two dimensional, then we can scan out the latent space, and
   for each point (z1, z2) in the latent space, pass it to the decoder and
   use it to generate some image.

   here are some examples i have generated using the [8]faces data set,
   and very simple fully connect network (and we can use a deep convnet
   for better looking images)

   [:0]

   [:0]

   and the well known and simple digits example:

   [:0]

     vae discovered this beautiful structure that smoothly moves between
     digits

   colored images

   here i used the [9]cifar-10 dataset that consists of 60000 32x32 color
   images in 10 classes (6000 images per class). each class is very rich,
   here are some samples from the horse class:

   [:0]


   for simplicity, in my implementation, i selected certain class (horse
   and car). i selected only few examples from each class, and repeated
   them after making some tiny changes and provide these images to the
   network to learn from.

   [:0]




   the original classes



     see how the images transform smoothly between classes


   [:0]


   [:0]


   the original classes



     select one car, and see how it transforms to the similar one,
     vertically or horizontally

   [:0]

     vae can model the information of high-dimensional observed pixels,
     and summarize the most important features into a structured
     distribution over reduced latent space

   summary
     * variational autoencoder is a type of autoencoders with added
       constraints on the encoded representations, it learns a latent
       variable model for the input data.
     * the model is trying to learn the parameters of a id203
       distribution that models the input data.
     * if we sample points from this distribution, we can generate new
       data samples.


     "vaes have already shown promise in generating many kinds of
     complicated data, including handwritten digits, faces, house
     numbers, cifar images, physical models of scenes, segmentation, and
     predicting the future from static images"

   [10]tutorial on id5

   regards

   [11]ibrahim sobh - phd

[12]ibrahim sobh - phd

senior expert of artificial intelligence, valeo

   [13]follow

   0 comments
   article-comment__guest-image
   [14]sign in to leave your comment
     __________________________________________________________________

more from ibrahim sobh - phd

   [15]40 articles
   how to convert your keras model to run on tensorflow.js

[16]how to convert your keras model to run on   

   january 14, 2019

   scaling up keras with estimators

[17]scaling up keras with estimators

   january 12, 2019

   better data for better machine learning

[18]better data for better machine learning

   january 8, 2019

     *    2019
     * [19]about
     * [20]user agreement
     * [21]privacy policy
     * [22]cookie policy
     * [23]copyright policy
     * [24]brand policy
     * [25]manage subscription
     * [26]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://eg.linkedin.com/in/ibrahim-sobh-phd-8681757?trk=author_mini-profile_image
   5. https://eg.linkedin.com/in/ibrahim-sobh-phd-8681757?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/deep-learning-mental-model-ibrahim-sobh&trk=author-info__follow-button
   7. https://www.linkedin.com/pulse/deep-learning-teach-your-network-how-draw-ibrahim-sobh
   8. https://downloads.sourceforge.net/project/scikit-learn/data/lfw_preprocessed.tar.gz
   9. https://www.cs.toronto.edu/~kriz/cifar.html
  10. https://arxiv.org/abs/1606.05908
  11. https://eg.linkedin.com/in/ibrahim-sobh-phd-8681757?trk=author_mini-profile_image
  12. https://eg.linkedin.com/in/ibrahim-sobh-phd-8681757?trk=author_mini-profile_title
  13. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/deep-learning-mental-model-ibrahim-sobh&trk=author-info__follow-button-bottom
  14. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/deep-learning-mental-model-ibrahim-sobh&trk=article-reader_leave-comment
  15. https://www.linkedin.com/today/author/ibrahim-sobh-phd-8681757
  16. https://www.linkedin.com/pulse/how-convert-your-keras-model-run-tensorflowjs-ibrahim-sobh-phd?trk=related_artice_how to convert your keras model to run on tensorflow.js_article-card_title
  17. https://www.linkedin.com/pulse/scaling-up-keras-estimators-ibrahim-sobh-phd?trk=related_artice_scaling up keras with estimators_article-card_title
  18. https://www.linkedin.com/pulse/better-data-machine-learning-ibrahim-sobh-phd?trk=related_artice_better data for better machine learning_article-card_title
  19. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  20. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  21. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  22. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  23. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  24. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  25. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  26. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
