mathematics for machine learning

marc deisenroth

statistical machine learning group
department of computing
imperial college london

deep learning indaba
university of the witwatersrand
johannesburg, south africa
september 10, 2017

@mpd37

m.deisenroth@imperial.ac.uk
marc@prowler.io

applications of machine learning

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

2

mathematical concepts in machine learning

   id202 and matrix decomposition
   differentiation
   optimization
   integration
   id203 theory and bayesian id136
   functional analysis

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

3

outline

introduction

differentiation

integration

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

4

overview

introduction

differentiation

integration

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

5

feedforward neural network

y       pzq
z     ax ` b

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

6

xzya,bx1x2z1z2z3y1y2y3xya,b  feedforward neural network

y       pzq
z     ax ` b

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

6

xzya,bx1x2z1z2z3y1y2y3xya,b  feedforward neural network

y       pzq
z     ax ` b

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

6

xzya,bx1x2z1z2z3y1y2y3xya,b  feedforward neural network

y       pzq
z     ax ` b

   training a neural networid116 parameter optimization:

typically via some form of id119

challenge 1: differentiation. compute gradients of a loss

function with respect to neural network parameters a, b

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

6

xzya,bx1x2z1z2z3y1y2y3xya,b  feedforward neural network

y       pzq
z     ax ` b

   training a neural networid116 parameter optimization:

typically via some form of id119

challenge 1: differentiation. compute gradients of a loss

function with respect to neural network parameters a, b

   computing statistics (e.g., means, variances) of predictions

challenge 2: integration. propagate uncertainty through a

neural network

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

6

xzya,bx1x2z1z2z3y1y2y3xya,b  background: id127

   id127 is not
commutative, i.e., ab     ba

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

7

background: id127

   id127 is not
commutative, i.e., ab     ba

   when multiplying matrices, the    neighboring    dimensions have

to    t:

aloomoon

bloomoon

    cloomoon

n  k

k  m

n  m

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

7

background: id127

   id127 is not
commutative, i.e., ab     ba

   when multiplying matrices, the    neighboring    dimensions have

to    t:

aloomoon

bloomoon

    cloomoon

n  k

k  m

n  m

  
y     ax
  
yi    
c     ab
cij    

j aijxj

y = a.dot(x)
y = np.einsum(   ij, j   , a, x)
c = a.dot(b)

k aikbkj c = np.einsum(   ik, kj   , a, b)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

7

curve fitting (regression) in machine learning (1)

   setting: given inputs x, predict outputs/targets y
   model f that depends on parameters   . examples:

   linear model: fpx,   q       jx,
   neural network: fpx,   q     nnpx,   q

x,    p rd

mathematics for machine learning

   training data, e.g., n pairs pxi, yiq of inputs xi and observations yi

@deep learning indaba, september 10, 2017

marc deisenroth

8

x-505f(x)-3-2-10123polynomial of degree 5datamaximum likelihood estimatecurve fitting (regression) in machine learning (2)

   training data, e.g., n pairs pxi, yiq of
inputs xi and observations yi

   training the model means    nding
parameters     , such that fpxi,     q    yi

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

9

x-505f(x)-3-2-10123polynomial of degree 5datamaximum likelihood estimatecurve fitting (regression) in machine learning (2)

   training data, e.g., n pairs pxi, yiq of
inputs xi and observations yi

   training the model means    nding
parameters     , such that fpxi,     q    yi

  

   de   ne a id168, e.g.,

optimize

n

i   1pyi    fpxi,   qq2, which we want to

   typically: optimization based on some form of id119

differentiation required

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

9

x-505f(x)-3-2-10123polynomial of degree 5datamaximum likelihood estimateoverview

introduction

differentiation

integration

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

10

differentiation: outline

1. scalar differentiation: f : r    r
2. multivariate case: f : rn    r
3. vector    elds: f : rn    rm
4. general derivatives: f : rm  n    rp  q

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

11

scalar differentiation f : r    r

   derivative de   ned as the limit of the difference quotient

f 1pxq     d f
dx

    lim
h  0

fp x ` h q    fpxq

h

slope of the secant line through fpxq and fpx ` hq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

12

examples

fpxq     xn
fpxq     sinpxq
fpxq     tanhpxq
fpxq     exppxq
fpxq     logpxq

f 1pxq     nxn  1
f 1pxq     cospxq
f 1pxq     1    tanh2pxq
f 1pxq     exppxq
f 1pxq     1

x

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

13

rules

   sum rule

`

  1     f 1pxq ` g1pxq     d f

` dg
dx

dx

fpxq ` gpxq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

14

rules

   sum rule

`

`

   product rule
fpxqgpxq

fpxq ` gpxq

  1     f 1pxq ` g1pxq     d f
  1     f 1pxqgpxq ` fpxqg1pxq     d f

dx

` dg
dx

gpxq ` fpxqdg
dx

dx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

14

rules

   sum rule

`

`

   product rule
fpxqgpxq

fpxq ` gpxq

  1     f 1pxq ` g1pxq     d f
  1     f 1pxqgpxq ` fpxqg1pxq     d f

dx

` dg
dx

gpxq ` fpxqdg
dx

dx

   chain rule

pg    fq1pxq    

`

gp fpxqq

  1     g1p fpxqq f 1pxq     dg

d f
dx

d f

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

14

example: chain rule

`

gp fpxqq

  1     g1p fpxqq f 1pxq     dg

d f

d f
dx

pg    fq1pxq    

gpzq     tanhpzq

z     fpxq     xn

pg    fq1pxq    

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

15

example: chain rule

`

gp fpxqq

  1     g1p fpxqq f 1pxq     dg

d f

d f
dx

pg    fq1pxq    

gpzq     tanhpzq

`
z     fpxq     xn
loooooooomoooooooon
1    tanh2pxnqq

pg    fq1pxq    

dg{d f

loomoon
nxn  1
d f{dx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

15

f : rn    r

          p rn

         x1

...
xn

y     fpxq ,

x    

   partial derivative (change one coordinate at a time):

b f
bxi

    lim
h  0

fpx1, . . . , xi  1, xi ` h , xi`1, . . . , xnq    fpxq

h

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

16

f : rn    r

          p rn

         x1

...
xn

y     fpxq ,

x    

   partial derivative (change one coordinate at a time):

b f
bxi

    lim
h  0

fpx1, . . . , xi  1, xi ` h , xi`1, . . . , xnq    fpxq

   jacobian vector (gradient) collects all partial derivatives:

   

h

  

b fbx1
note: this is a row vector.

d f
dx

   

      

b f
xn

p r1  n

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

16

example

f : r2    r
fpx1, x2q     x2

1x2 ` x1x3

2 p r

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

17

example

f : r2    r
fpx1, x2q     x2

1x2 ` x1x3

2 p r

   partial derivatives:

b fpx1, x2q
b fpx1, x2q

bx1
bx2

    2x1x2 ` x3
1 ` 3x1x2
    x2

2

2

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

17

example

f : r2    r
fpx1, x2q     x2

1x2 ` x1x3

2 p r

   partial derivatives:

   

   gradient:
   

d f
dx

b fpx1,x2q

bx1

2

    2x1x2 ` x3
1 ` 3x1x2
    x2

b fpx1, x2q
bx1
b fpx1, x2q
bx2
   
  
2x1x2 ` x3

   

2

b fpx1,x2q

bx2

2 x2

1 ` 3x1x2

2

  

p r1  2 .

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

17

rules

   sum rule

   product rule

   chain rule

`

  
fpxq ` gpxq

b
bx
`

  
fpxqgpxq

b
bx

    b f
bx
`

    b f
bx

` bg
bx

gpxq ` fpxqbg
bx

  

b
bx

pg    fqpxq     b
bx

gp fpxqq

    bg
b f

b f
bx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

18

example: chain rule

   consider the function

lpeq     1

2eje
x p rn , a p rm  n , e, y p rm
   compute dl{dx. what is the dimension/size of dl{dx?

2}e}2     1
e     y    ax ,

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

19

example: chain rule

   consider the function

lpeq     1

2}e}2     1
e     y    ax ,

2eje
x p rn , a p rm  n , e, y p rm
   compute dl{dx. what is the dimension/size of dl{dx?
   dl{dx p r1  n

dl
dx
dl
de
de
dx
   dl
dx

de
dx

    dl
de
    ej p r1  m
      a p rm  n
    ejp  aq       py    axqja p r1  n

(1)

(2)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

19

f : rn    rm

         y1

...
ym

             

y     fpxq p rm ,

             

         f1pxq

...

fmpxq

x p rn

         f1px1, . . . , xnq

fmpx1, . . . , xnq

...

         

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

20

f : rn    rm

y     fpxq p rm ,

...
ym

         y1
             
               dy1

dx

...

fmpxq

         f1pxq
             
               b f1bx1
                   

...

x p rn

fmpx1, . . . , xnq

         f1px1, . . . , xnq
         
                p rm  n

      

...
dym
dx

...
b fmbx1

      

b f1bxn
...
b fmbxn

   jacobian matrix (collection of all partial derivatives)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

20

example

fpxq     ax ,

fpxq p rm, a p rm  n,

x p rn

   compute the gradient d f{dx

   dimension of d f{dx:

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

21

example

fpxq     ax ,

fpxq p rm, a p rm  n,

x p rn

   compute the gradient d f{dx

   dimension of d f{dx:
since f : rn    rm, it follows that d f{dx p rm  n

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

21

example

fpxq     ax ,

fpxq p rm, a p rm  n,

x p rn

   compute the gradient d f{dx

   dimension of d f{dx:
since f : rn    rm, it follows that d f{dx p rm  n
   gradient:

fi     n  

j   1

aijxj

   b fibxj

    aij

(3)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

21

example

fpxq     ax ,

fpxq p rm, a p rm  n,

x p rn

   compute the gradient d f{dx

   dimension of d f{dx:
since f : rn    rm, it follows that d f{dx p rm  n
   gradient:

aijxj

fi     n  
            b f1bx1

j   1

   

...
b fmbx1

      

      

   d f
dx

   b fibxj
                
b f1bxn
...
b fmbxn

    aij

            a11

...
am1

                 a

(3)

       a1n
...
       amn

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

21

chain rule

b
bx

pg    fqpxq     b
bx

`

  

gp fpxqq

    bg
b f

b f
bx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

22

example

   consider f : r2    r,

  
   
1 ` 2x2 ,
fpxq     fpx1, x2q     x2
sinptq
xptq    
cosptq

x : r    r2
   
  
x1ptq
x2ptq

   

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

23

example

   consider f : r2    r,

  
   
1 ` 2x2 ,
fpxq     fpx1, x2q     x2
sinptq
xptq    
cosptq

x : r    r2
   
  
x1ptq
x2ptq

   

   the dimensions d f{dx and dx{dt are

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

23

example

   consider f : r2    r,

  
   
1 ` 2x2 ,
fpxq     fpx1, x2q     x2
sinptq
xptq    
cosptq

x : r    r2
   
  
x1ptq
x2ptq

   

   the dimensions d f{dx and dx{dt are 1    2 and 2    1, respectively
   compute the gradient d f{dt using the chain rule.

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

23

example

   consider f : r2    r,

  
   
1 ` 2x2 ,
fpxq     fpx1, x2q     x2
sinptq
xptq    
cosptq

x : r    r2
   
  
x1ptq
x2ptq

   

   the dimensions d f{dx and dx{dt are 1    2 and 2    1, respectively
   compute the gradient d f{dt using the chain rule.

   
   

   

d f
dt

b fbx1

b fbx2

      

       bx1bt
    
bx2bt
cos t
   sin t

   

2 sin t 2

   
    2 sin t cos t    2 sin t     2 sin tpcos t    1q

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

23

break

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

24

derivatives with matrices

   re-cap: gradient of a function f : rd    re is an e    d-matrix:

with

# target dimensions    # parameters
d fre, ds     b febxd

p re  d ,

d f
dx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

25

derivatives with matrices

   re-cap: gradient of a function f : rd    re is an e    d-matrix:

with

# target dimensions    # parameters
d fre, ds     b febxd

p re  d ,

d f
dx

   generalization to cases, where the parameters (d) or targets (e)

are matrices, apply immediately

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

25

derivatives with matrices

   re-cap: gradient of a function f : rd    re is an e    d-matrix:

with

# target dimensions    # parameters
d fre, ds     b febxd

p re  d ,

d f
dx

   generalization to cases, where the parameters (d) or targets (e)

are matrices, apply immediately
   assume f : rm  n    rp  q, then the gradient is a
pp    qq    pm    nq object (tensor) where
d frp, q, m, ns     b fpq
bxmn

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

25

derivatives with matrices: example (1)

f     ax ,

f p rm, a p rm  n, x p rn

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

26

derivatives with matrices: example (1)

f     ax ,

f p rm, a p rm  n, x p rn

d f
da

d f
da

p rm  pm  nq

          ,

         b f1ba

...
b fmba

   

b fiba

p r1  pm  nq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

26

derivatives with matrices: example (2)

fi     n  

j   1

aijxj,

i     1, . . . , m

(4)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

27

derivatives with matrices: example (2)

fi     n  
b fibaiq

j   1
    xq

aijxj,

i     1, . . . , m

(4)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

27

derivatives with matrices: example (2)

fi     n  
    xq    b fibai,:
b fibaiq

aijxj,

j   1

i     1, . . . , m

    xj p r1  1  n

(4)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

27

derivatives with matrices: example (2)

aijxj,

fi     n  
    xq    b fibai,:
b fibaiq
b fi
    0j p r1  1  n
bak   i,:

j   1

i     1, . . . , m

    xj p r1  1  n

(4)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

27

derivatives with matrices: example (2)

i     1, . . . , m

    xj p r1  1  n

aijxj,

fi     n  
    xq    b fibai,:
b fibaiq
b fi
    0j p r1  1  n
bak   i,:

j   1

                               p r1  pm  nq

                             

0j
...
xj
0j
...
0j

b fiba

   

(4)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

27

example: higher-order tensors

   consider a matrix a p r4  2 whose entries depend on a vector
x p r3
   we can compute dapxq{dx p r4  2  3 in two equivalent ways:

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

28

example: higher-order tensors

   consider a matrix a p r4  2 whose entries depend on a vector
x p r3
   we can compute dapxq{dx p r4  2  3 in two equivalent ways:

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

28

gradients of a single-layer neural network (1)

loomoon
f     tanhpax ` b
   :zprm

q p rm,

x p rn, a p rm  n, b p rm

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

29

xzfa,b  gradients of a single-layer neural network (1)

loomoon
f     tanhpax ` b
   :zprm
bbloomoon
bzloomoon
bz
    b f
b f
bb
m  m

m  m

q p rm,

x p rn, a p rm  n, b p rm

p rm  m

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

29

xzfa,b  gradients of a single-layer neural network (1)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
bbloomoon
bzloomoon
bz
    b f
b f
bb
m  m
baloomoon
bzloomoon
    b f
b f
ba
m  m

m  m
bz

m  pm  nq

p rm  m

p rm  pm  nq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

29

xzfa,b  gradients of a single-layer neural network (2)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
bbloomoon
bzloomoon
bz
    b f
b f
bb
m  m

m  m

p rm  m

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

30

gradients of a single-layer neural network (2)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
bbloomoon
bzloomoon
bz
    b f
b f
bb
m  m

m  m
b f
bz

p rm  m

    diagp1    tanh2pzqq p rm  m

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

30

gradients of a single-layer neural network (2)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
bbloomoon
bzloomoon
bz
    b f
b f
bb
m  m

p rm  m

m  m
b f
bz
bz
bb

ri, js     m  

l   1

b f
bb

    diagp1    tanh2pzqq p rm  m
    i p rm  m

b f
bz

ri, lsbz
bb

rl, js

(5)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

30

gradients of a single-layer neural network (2)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
bbloomoon
bzloomoon
bz
    b f
b f
bb
m  m

p rm  m

m  m
b f
bz
bz
bb

ri, js     m  

l   1

b f
bb

    diagp1    tanh2pzqq p rm  m
    i p rm  m

b f
bz

ri, lsbz
bb

rl, js

(5)

dfdb = np.einsum(   il, lj   , dfdz, dzdb)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

30

gradients of a single-layer neural network (3)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
baloomoon
bzloomoon
    b f
b f
bz
ba
m  m

m  pm  nq

p rm  pm  nq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

31

gradients of a single-layer neural network (3)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
baloomoon
bzloomoon
    b f
b f
bz
ba
m  m

m  pm  nq

p rm  pm  nq

b f
bz

    diagp1    tanh2pzqq p rm  m

(6)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

31

gradients of a single-layer neural network (3)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
baloomoon
bzloomoon
    b f
b f
bz
ba
m  m

m  pm  nq

p rm  pm  nq

    diagp1    tanh2pzqq p rm  m

(6)

b f
bz
bz
ba

ri, j, ks     m  

l   1

see (4)
b f
bz

ri, ls bz
ba

b f
ba

rl, j, ks

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

31

gradients of a single-layer neural network (3)

q p rm,

x p rn, a p rm  n, b p rm

loomoon
f     tanhpax ` b
   :zprm
baloomoon
bzloomoon
    b f
b f
bz
ba
m  m

m  pm  nq

p rm  pm  nq

    diagp1    tanh2pzqq p rm  m

(6)

b f
bz
bz
ba

ri, j, ks     m  

l   1

see (4)
b f
bz

ri, ls bz
ba

b f
ba

rl, j, ks

dfda = np.einsum(   il, ljk   , dfdz, dzda)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

31

putting things together

   inputs x, observed outputs y     fpz,   q `   ,       n

  

`

0,   

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

32

putting things together

   inputs x, observed outputs y     fpz,   q `   ,       n
   train single-layer neural network with

  

`

0,   

fpz,   q     tanhpzq ,

z     ax ` b ,

       ta, bu

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

32

putting things together

   inputs x, observed outputs y     fpz,   q `   ,       n
   train single-layer neural network with

  

`

0,   

fpz,   q     tanhpzq ,

z     ax ` b ,

       ta, bu

   find a, b, such that the squared loss

lp  q     1

2}e}2 ,

e     y    fpz,   q

is minimized

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

32

putting things together

   inputs x, observed outputs y     fpz,   q `   ,       n
   train single-layer neural network with

  

`

0,   

fpz,   q     tanhpzq ,

z     ax ` b ,

       ta, bu

   find a, b, such that the squared loss

lp  q     1

2}e}2 ,

e     y    fpz,   q

is minimized

   partial derivatives:
b f
bz
b f
bz

    bl
be
    bl
be

bl
ba
bl
bb

be
b f
be
b f

,//.//- bl

be
bz
ba

bz
ba
bz
bb

(1)

(4)

be
b f
bz
bb

(2), (3)

(5)

b f
bz

(6)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

32

gradients of a multi-layer neural network

   inputs x, observed outputs y
   train multi-layer neural network with

f 0     x
f i       ipai  1 f i  1 ` bi  1q ,

i     1, . . . , l

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

33

xfla1,b1al   1,bl   1lfl   1al   2,bl   2f1a2,b2gradients of a multi-layer neural network

   inputs x, observed outputs y
   train multi-layer neural network with

f 0     x
f i       ipai  1 f i  1 ` bi  1q ,

i     1, . . . , l

   find aj, bj for j     0, . . . , l    1, such that the squared loss

lp  q     }y    f lp  , xq}2

is minimized, where        taj, bju ,

j     0, . . . , l    1

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

33

xfla1,b1al   1,bl   1lfl   1al   2,bl   2f1a2,b2gradients of a multi-layer neural network

bl
b  l  1

    bl
b f l

b f l
b  l  1

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

34

xfla1,b1al   1,bl   1lfl   1al   2,bl   2f1a2,b2gradients of a multi-layer neural network

bl
b  l  1
bl
b  l  2

    bl
b f l
    bl
b f l

b f l
b  l  1
b f l
b f l  1

b f l  1
b  l  2

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

34

xfla1,b1al   1,bl   1lfl   1al   2,bl   2f1a2,b2gradients of a multi-layer neural network

bl
b  l  1
bl
b  l  2
bl
b  l  3

    bl
b f l
    bl
b f l
    bl
b f l

b f l
b  l  1
b f l
b f l  1
b f l
b f l  1

b f l  1
b  l  2
b f l  1
b f l  2

b f l  2
b  l  3

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

34

xfla1,b1al   1,bl   1lfl   1al   2,bl   2f1a2,b2gradients of a multi-layer neural network

bl
b  l  1
bl
b  l  2
bl
b  l  3
bl
b  i

    bl
b f l
    bl
b f l
    bl
b f l
    bl
b f l

b f l
b  l  1
b f l
b f l  1
b f l
b f l  1
b f l
b f l  1

b f l  1
b  l  2
b f l  1
b f l  2
       b f i`2
b f i`1

b f l  2
b  l  3
b f i`1b  i

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

34

xfla1,b1al   1,bl   1lfl   1al   2,bl   2f1a2,b2gradients of a multi-layer neural network

bl
b  l  1
bl
b  l  2
bl
b  l  3
bl
b  i

    bl
b f l
    bl
b f l
    bl
b f l
    bl
b f l

b f l
b  l  1
b f l
b f l  1
b f l
b f l  1
b f l
b f l  1

b f l  1
b  l  2
b f l  1
b f l  2
       b f i`2
b f i`1

b f l  2
b  l  3
b f i`1b  i

more details (including ef   cient implementation) later this week

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

34

xfla1,b1al   1,bl   1lfl   1al   2,bl   2f1a2,b2training neural networks as maximum likelihood
estimation

   training a neural network in the above way corresponds to

id113:
      n

   if y     nnpx,   q `  ,

`

  
0, i
then the log-likelihood is
2}y    nnpx,   q}2
log ppy|x,   q        1

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

35

training neural networks as maximum likelihood
estimation

   training a neural network in the above way corresponds to

id113:
      n

   if y     nnpx,   q `  ,

`

  
0, i
then the log-likelihood is
2}y    nnpx,   q}2
log ppy|x,   q        1

   find      by minimizing the negative log-likelihood:

  

         arg min
    arg min
    arg min

  

   log ppy|x,   q
2}y    nnpx,   q}2
lp  q

1

  

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

35

training neural networks as maximum likelihood
estimation

   training a neural network in the above way corresponds to

id113:
      n

   if y     nnpx,   q `  ,

`

  
0, i
then the log-likelihood is
2}y    nnpx,   q}2
log ppy|x,   q        1

   find      by minimizing the negative log-likelihood:

  

         arg min
    arg min
    arg min

  

   log ppy|x,   q
2}y    nnpx,   q}2
lp  q

1

  

   id113 can lead to over   tting (interpret

noise as signal)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

35

example: id75 (1)

   id75 with a polynomial of order m:

y     fpx,   q `   ,

fpx,   q       0 `   1x `   2x2 `        `   mxm     m  

      n

0,   2
 

  ixi

i   0

`

  

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

36

example: id75 (1)

   id75 with a polynomial of order m:

y     fpx,   q `   ,

fpx,   q       0 `   1x `   2x2 `        `   mxm     m  

      n

0,   2
 

  ixi

i   0

`

  

   given inputs xi and corresponding (noisy) observations yi,
i     1, . . . , n,    nd parameters        r  0, . . . ,   msj, that minimize the
squared loss (equivalently: maximize the likelihood)

lp  q     n  

i   1

pyi    fpxi,   qq2

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

36

example: id75 (2)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

37

x-505f(x)-3-2-10123polynomial of degree 16datamaximum likelihood estimateexample: id75 (2)

   id173, model selection etc. can address over   tting

tutorials later this week

   alternative approach based on integration

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

37

x-505f(x)-3-2-10123polynomial of degree 16datamaximum likelihood estimateoverview

introduction

differentiation

integration

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

38

integration: outline

1. motivation
2. monte-carlo estimation
3. basic sampling algorithms

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

39

bayesian integration to avoid over   tting

   instead of    tting a single set
of parameters     , we can
average over all plausible
parameters

bayesian integration:
ppy|xq    

ppy|x,   qpp  qd  

  

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

40

-505x-3-2-10123f(x)polynomial of degree 1695% predictive confidence bounddatamaximum likelihood estimatemap estimatebayesian integration to avoid over   tting

   instead of    tting a single set
of parameters     , we can
average over all plausible
parameters

bayesian integration:
ppy|xq    

ppy|x,   qpp  qd  

  

   more details on what pp  q is

tutorials later this week

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

40

-505x-3-2-10123f(x)polynomial of degree 1695% predictive confidence bounddatamaximum likelihood estimatemap estimatebayesian integration to avoid over   tting

   instead of    tting a single set
of parameters     , we can
average over all plausible
parameters

bayesian integration:
ppy|xq    

ppy|x,   qpp  qd  

  

   more details on what pp  q is
   for neural networks this integration is intractable

tutorials later this week

approximations

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

40

-505x-3-2-10123f(x)polynomial of degree 1695% predictive confidence bounddatamaximum likelihood estimatemap estimatecomputing statistics of random variables

   computing means/(co)variances also requires solving integrals:

  
  
xppxqdx    :   x
  
px      xqpx      xqjdx
px      xqpy      yqjdxdy

exrxs    
vxrxs    
covrx, ys    

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

41

computing statistics of random variables

   computing means/(co)variances also requires solving integrals:

  
  
xppxqdx    :   x
  
px      xqpx      xqjdx
px      xqpy      yqjdxdy

exrxs    
vxrxs    
covrx, ys    

   these integrals can often not be computed in closed form

approximations

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

41

approximate integration

   numerical integration (low-dimensional problems)
   bayesian quadrature, e.g., o   hagan (1987, 1991); rasmussen &

ghahramani (2003)

   id58, e.g., jordan et al. (1999)
   expectation propagation, opper & winther (2001); minka (2001)
   monte-carlo methods, e.g., gilks et al. (1996), robert & casella

(2004), bishop (2006)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

42

monte carlo methods   motivation

   monte carlo methods are computational techniques that make

use of random numbers

   two typical problems:

1. problem 1: generate samples txpsqu from a given id203
distribution ppxq, e.g., for simulation or representations of data
distributions

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

43

monte carlo methods   motivation

   monte carlo methods are computational techniques that make

use of random numbers

   two typical problems:

1. problem 1: generate samples txpsqu from a given id203
distribution ppxq, e.g., for simulation or representations of data
distributions

2. problem 2: compute expectations of functions under that

distribution:

er fpxqs    

fpxqppxqdx

  

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

43

monte carlo methods   motivation

   monte carlo methods are computational techniques that make

use of random numbers

   two typical problems:

1. problem 1: generate samples txpsqu from a given id203
distribution ppxq, e.g., for simulation or representations of data
distributions

2. problem 2: compute expectations of functions under that

distribution:

er fpxqs    

fpxqppxqdx

  

example: means/variances of distributions, predictions

complication: integral cannot be evaluated analytically

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

43

problem 2: monte carlo estimation

   computing expectations via statistical sampling:

  

er fpxqs    
   1
s

  s

fpxqppxqdx
fpxpsqq,

s   1

xpsq     ppxq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

44

problem 2: monte carlo estimation

   computing expectations via statistical sampling:

  

er fpxqs    
   1
s
  

ppy|xq    

  s

fpxqppxqdx
fpxpsqq,

s   1

xpsq     ppxq

loomoon
ppy|  , xq pp  q
ppy|  psq, xq ,

  s

d  

parameter distribution

s   1

   1
s

  psq     pp  q

   making predictions (e.g., bayesian regression with inputs x and

targets y)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

44

problem 2: monte carlo estimation

   computing expectations via statistical sampling:

  

er fpxqs    
   1
s
  

ppy|xq    

  s

fpxqppxqdx
fpxpsqq,

s   1

xpsq     ppxq

loomoon
ppy|  , xq pp  q
ppy|  psq, xq ,

  s

d  

parameter distribution

   making predictions (e.g., bayesian regression with inputs x and

targets y)

  psq     pp  q
   key problem: generating samples from ppxq or pp  q

   1
s

s   1

need to solve problem 1

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

44

sampling discrete values

   u     ur0, 1s, where u is the uniform distribution
   u     0.55    x     c

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

45

p=0.3p=0.2p=0.5abcu=0.55continuous variables

   more complicated
   geometrically, we wish to sample uniformly from the area under

the curve

   two algorithms here:
   rejection sampling
   importance sampling

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

46

p(z)z~rejection sampling: setting

   assume:

   sampling from ppzq is dif   cult
   evaluating   ppzq     zppzq is easy (and z may be unknown)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

47

p(z)z~rejection sampling: setting

   assume:

   sampling from ppzq is dif   cult
   evaluating   ppzq     zppzq is easy (and z may be unknown)

   find a simpler distribution (proposal distribution) qpzq from
which we can easily draw samples (e.g., gaussian, laplace)
   find an upper bound kqpzq      ppzq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

47

p(z)z~rejection sampling: algorithm

adapted from prml (bishop, 2006)

1. generate z0     qpzq
2. generate u0     ur0, kqpz0qs
3. if u0      ppz0q, reject the sample. otherwise, retain z0

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

48

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areaproperties

   accepted pairs pz, uq are uniformly distributed under   ppzq

adapted from prml (bishop, 2006)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

49

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areaproperties

adapted from prml (bishop, 2006)

   accepted pairs pz, uq are uniformly distributed under   ppzq
   id203 density of the z-coordiantes of accepted points must
be proportional to ppzq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

49

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areaproperties

adapted from prml (bishop, 2006)

   accepted pairs pz, uq are uniformly distributed under   ppzq
   id203 density of the z-coordiantes of accepted points must
be proportional to ppzq
   samples are independent samples from ppzq

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

49

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areashortcomings

   finding the upper bound k is tricky

adapted from prml (bishop, 2006)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

50

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areashortcomings

adapted from prml (bishop, 2006)

   finding the upper bound k is tricky
   in high dimensions the factor k is probably huge

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

50

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areashortcomings

adapted from prml (bishop, 2006)

   finding the upper bound k is tricky
   in high dimensions the factor k is probably huge
   low acceptance rate/high rejection rate of samples

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

50

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areaimportance sampling

key idea: do not throw away all rejected samples, but give them
lower weight by rewriting the integral as an expectation under a
simpler distribution q (proposal distribution):

  

epr fpxqs    

fpxqppxqdx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

51

importance sampling

key idea: do not throw away all rejected samples, but give them
lower weight by rewriting the integral as an expectation under a
simpler distribution q (proposal distribution):

  
  

epr fpxqs    
   

fpxqppxqdx
fpxqppxqqpxq
qpxqdx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

51

importance sampling

key idea: do not throw away all rejected samples, but give them
lower weight by rewriting the integral as an expectation under a
simpler distribution q (proposal distribution):

epr fpxqs    
   

  

fpxqppxqdx
fpxqppxqqpxq

qpxqdx    

fpxq ppxq

qpxq qpxqdx

  
  

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

51

importance sampling

key idea: do not throw away all rejected samples, but give them
lower weight by rewriting the integral as an expectation under a
simpler distribution q (proposal distribution):

  
  

epr fpxqs    
   
    eq

fpxqppxqdx
   
   
fpxqppxqqpxq
qpxqdx    
fpxq ppxq
qpxq

  

fpxq ppxq

qpxq qpxqdx

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

51

importance sampling

key idea: do not throw away all rejected samples, but give them
lower weight by rewriting the integral as an expectation under a
simpler distribution q (proposal distribution):

  

fpxq ppxq

qpxq qpxqdx

fpxqppxqdx
   
   
fpxqppxqqpxq
qpxqdx    
fpxq ppxq
qpxq

  
  

epr fpxqs    
   
    eq
s  

   

   1
s

s   1

   

fpxq ppxq
qpxq

eq

if we choose q in a way that we can easily sample from it, we can
approximate this last expectation by monte carlo:

fpxpsqq ppxpsqq
qpxpsqq

xpsq     qpxq

,

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

51

importance sampling

key idea: do not throw away all rejected samples, but give them
lower weight by rewriting the integral as an expectation under a
simpler distribution q (proposal distribution):

  

fpxq ppxq

qpxq qpxqdx

fpxqppxqdx
   
   
fpxqppxqqpxq
qpxqdx    
fpxq ppxq
qpxq

  
  

epr fpxqs    
   
    eq
s  

   

   1
s

s   1

   

fpxq ppxq
qpxq

eq

if we choose q in a way that we can easily sample from it, we can
approximate this last expectation by monte carlo:

fpxpsqq ppxpsqq

qpxpsqq     1

s

ws fpxpsqq,

xpsq     qpxq

s  

s   1

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

51

properties

   unbiased if q    0 where p    0 and if we can evaluate p

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

52

properties

   unbiased if q    0 where p    0 and if we can evaluate p
   breaks down if we do not have enough samples (puts nearly all

weight on a single sample)

degeneracy, see also id143ing and smc
(thrun et al., 2005; doucet et al., 2000)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

52

properties

   unbiased if q    0 where p    0 and if we can evaluate p
   breaks down if we do not have enough samples (puts nearly all

weight on a single sample)

degeneracy, see also id143ing and smc
(thrun et al., 2005; doucet et al., 2000)

   many draws from proposal density q required, especially in high

dimensions

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

52

properties

   unbiased if q    0 where p    0 and if we can evaluate p
   breaks down if we do not have enough samples (puts nearly all

weight on a single sample)

degeneracy, see also id143ing and smc
(thrun et al., 2005; doucet et al., 2000)

   many draws from proposal density q required, especially in high

dimensions

   requires to be able to evaluate true p. generalization exists for   p.

this generalization is biased (but consistent).

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

52

properties

   unbiased if q    0 where p    0 and if we can evaluate p
   breaks down if we do not have enough samples (puts nearly all

weight on a single sample)

degeneracy, see also id143ing and smc
(thrun et al., 2005; doucet et al., 2000)

   many draws from proposal density q required, especially in high

dimensions

   requires to be able to evaluate true p. generalization exists for   p.

this generalization is biased (but consistent).

   does not scale to interesting (high-dimensional) problems

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

52

properties

   unbiased if q    0 where p    0 and if we can evaluate p
   breaks down if we do not have enough samples (puts nearly all

weight on a single sample)

degeneracy, see also id143ing and smc
(thrun et al., 2005; doucet et al., 2000)

   many draws from proposal density q required, especially in high

dimensions

   requires to be able to evaluate true p. generalization exists for   p.

this generalization is biased (but consistent).

   does not scale to interesting (high-dimensional) problems
different approach to sample from complicated (high-dimensional)
distributions: id115 (e.g., gilks et al., 1996)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

52

summary

   two mathematical challenges in machine learning

   differentiation for optimizing parameters of machine learning

models

vector calculus and chain rule

   integration for computing statistics (e.g., means, variances) and as

a principled way to address over   tting issue

monte-carlo integration to solve intractable integrals

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

53

z0zu0kq(z0)kq(z)p(z)~acceptance arearejection areasome application areas

   image/speech/text/language processing using deep neural

networks (e.g., krizhevsky et al., 2012 or overview in goodfellow
et al., 2016)

   data-ef   cient id23 and robot learning using

gaussian processes (e.g., deisenroth & rasmussen, 2011)

   high-energy physics using deep neural networks or gaussian

processes (e.g., sadowski et al. 2014; bertone et al., 2016)

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

54

references i

[1] g. bertone, m. p. deisenroth, j. s. kim, s. liem, r. r. de austri, and m. welling. accelerating the bsm interpretation of

lhc data with machine learning. arxiv preprint arxiv:1611.02704, 2016.

[2] c. m. bishop. pattern recognition and machine learning. information science and statistics. springer-verlag, 2006.
[3] m. p. deisenroth, d. fox, and c. e. rasmussen. gaussian processes for data-ef   cient learning in robotics and control.

ieee transactions on pattern analysis and machine intelligence, 37(2):408   423, feb. 2015.

[4] m. p. deisenroth and c. e. rasmussen. pilco: a model-based and data-ef   cient approach to policy search. in

proceedings of the international conference on machine learning, pages 465   472. acm, june 2011.

[5] a. doucet, s. j. godsill, and c. andrieu. on sequential monte carlo sampling methods for bayesian filtering. statistics

and computing, 10:197   208, 2000.

[6] w. r. gilks, s. richardson, and d. j. spiegelhalter, editors. id115 in practice: interdisciplinary statistics.

chapman & hall, 1996.
i. goodfellow, y. bengio, and a. courville. deep learning. mit press, 2016.

[7]
[8] m. i. jordan, z. ghahramani, t. s. jaakkola, and l. k. saul. an introduction to variational methods for id114.

machine learning, 37:183   233, 1999.

[9] s. kamthe and m. p. deisenroth. data-ef   cient id23 with probabilistic model predictive control.

arxiv:1706.06491, abs/1706.06491, 2017.

[10] a. krizhevsky, i. sutskever, and g. e. hinton. id163 classi   cation with deep convolutional neural networks. in

advances in neural information processing systems, pages 1097   1105, 2012.

[11] t. p. minka. a family of algorithms for approximate bayesian id136. phd thesis, massachusetts institute of technology,

cambridge, ma, usa, jan. 2001.

[12] r. m. neal. bayesian learning for neural networks. phd thesis, department of computer science, university of toronto,

1996.

[13] a. o   hagan. monte carlo is fundamentally unsound. the statistician, 36(2/3):247   249, 1987.

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

55

references ii

[14] a. o   hagan. bayes-hermite quadrature. journal of statistical planning and id136, 29:245   260, 1991.
[15] m. opper and o. winther. adaptive and self-averaging thouless-anderson-palmer mean-   eld theory for probabilistic

modeling. physical review e: statistical, nonlinear, and soft matter physics, 64:056131, 2001.

[16] c. e. rasmussen and z. ghahramani. bayesian monte carlo. in s. becker, s. thrun, and k. obermayer, editors, advances

in neural information processing systems 15, pages 489   496. the mit press, cambridge, ma, usa, 2003.

[17] c. p. robert and g. casella. monte carlo methods. wiley online library, 2004.
[18] p. sadowski, j. collado, d. whiteson, and p. baldi. deep learning, dark knowledge, and dark matter. in nips workshop

on high-energy physics and machine learning, pages 81   87, 2014.

[19] s. thrun, w. burgard, and d. fox. probabilistic robotics. the mit press, cambridge, ma, usa, 2005.

mathematics for machine learning

marc deisenroth

@deep learning indaba, september 10, 2017

56

