bayesian reasoning and machine learning

david barber c(cid:13)2007,2008,2009,2010,2011,2012,2013,2014,2015,2016

notation list
v
dom(x)

x = x

p(x = tr)

p(x = fa)

p(x, y)

p(x     y)
p(x     y)
p(x|y)
x       y|z
x(cid:62)(cid:62)y|z
x f (x)
i [s]

(cid:82)

pa (x)

ch (x)

ne (x)

dim (x)

(cid:104)f (x)(cid:105)p(x)
  (a, b)

dim x

a calligraphic symbol typically denotes a set of random variables . . . . . . . . 7

domain of a variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

the variable x is in the state x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

id203 of event/variable x being in the state true . . . . . . . . . . . . . . . . . . . 7

id203 of event/variable x being in the state false . . . . . . . . . . . . . . . . . . . 7

id203 of x and y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

id203 of x and y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

id203 of x or y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

the id203 of x conditioned on y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

variables x are independent of variables y conditioned on variables z . 11
variables x are dependent on variables y conditioned on variables z . . 11

for continuous variables this is shorthand for(cid:82) f (x)dx and for discrete vari-
ables means summation over the states of x,(cid:80)

x f (x) . . . . . . . . . . . . . . . . . . 18
indicator : has value 1 if the statement s is true, 0 otherwise . . . . . . . . . . 19

the parents of node x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

the children of node x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

neighbours of node x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

for a discrete variable x, this denotes the number of states x can take . . 40

the average of the function f (x) with respect to the distribution p(x) . 170

delta function. for discrete a, b, this is the kronecker delta,   a,b and for
continuous a, b the dirac delta function   (a     b) . . . . . . . . . . . . . . . . . . . . . . 172
the dimension of the vector/matrix x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183

(cid:93) (x = s, y = t)

the number of times x is in state s and y in state t simultaneously . . . 209

(cid:93)x
y

d
n

n

s

  (x)

erf(x)

xa:b

i     j
im

ii

the number of times variable x is in state y . . . . . . . . . . . . . . . . . . . . . . . . . . 290

dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303

data index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303

number of dataset training points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303

sample covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327

the logistic sigmoid 1/(1 + exp(   x)) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
the (gaussian) error function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367

xa, xa+1, . . . , xb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469

the set of unique neighbouring edges on a graph . . . . . . . . . . . . . . . . . . . . . . 601

the m    m identity matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623

draft november 9, 2017

preface

the data explosion

we live in a world that is rich in data, ever increasing in scale. this data comes from many di   erent
sources in science (bioinformatics, astronomy, physics, environmental monitoring) and commerce (customer
databases,    nancial transactions, engine monitoring, id103, surveillance, search). possessing
the knowledge as to how to process and extract value from such data is therefore a key and increasingly
important skill. our society also expects ultimately to be able to engage with computers in a natural manner
so that computers can    talk    to humans,    understand    what they say and    comprehend    the visual world
around them. these are di   cult large-scale information processing tasks and represent grand challenges
for computer science and related    elds. similarly, there is a desire to control increasingly complex systems,
possibly containing many interacting parts, such as in robotics and autonomous navigation. successfully
mastering such systems requires an understanding of the processes underlying their behaviour. processing
and making sense of such large amounts of data from complex systems is therefore a pressing modern day
concern and will likely remain so for the foreseeable future.

machine learning

machine learning is the study of data-driven methods capable of mimicking, understanding and aiding
human and biological information processing tasks. in this pursuit, many related issues arise such as how
to compress data, interpret and process it. often these methods are not necessarily directed to mimicking
directly human processing but rather to enhance it, such as in predicting the stock market or retrieving
information rapidly. in this id203 theory is key since inevitably our limited data and understanding
of the problem forces us to address uncertainty. in the broadest sense, machine learning and related    elds
aim to    learn something useful    about the environment within which the agent operates. machine learning
is also closely allied with arti   cial intelligence, with machine learning placing more emphasis on using data
to drive and adapt the model.

in the early stages of machine learning and related areas, similar techniques were discovered in relatively
isolated research communities. this book presents a uni   ed treatment via id114, a marriage
between graph and id203 theory, facilitating the transference of machine learning concepts between
di   erent branches of the mathematical and computational sciences.

whom this book is for

the book is designed to appeal to students with only a modest mathematical background in undergraduate
calculus and id202. no formal computer science or statistical background is required to follow the
book, although a basic familiarity with id203, calculus and id202 would be useful. the book
should appeal to students from a variety of backgrounds, including computer science, engineering, applied
statistics, physics, and bioinformatics that wish to gain an entry to probabilistic approaches in machine
learning. in order to engage with students, the book introduces fundamental concepts in id136 using

iii

only minimal reference to algebra and calculus. more mathematical techniques are postponed until as and
when required, always with the concept as primary and the mathematics secondary.

the concepts and algorithms are described with the aid of many worked examples. the exercises and
demonstrations, together with an accompanying matlab toolbox, enable the reader to experiment and
more deeply understand the material. the ultimate aim of the book is to enable the reader to construct
novel algorithms. the book therefore places an emphasis on skill learning, rather than being a collection of
recipes. this is a key aspect since modern applications are often so specialised as to require novel methods.
the approach taken throughout is to describe the problem as a graphical model, which is then translated
into a mathematical framework, ultimately leading to an algorithmic implementation in the brmltoolbox.

the book is primarily aimed at    nal year undergraduates and graduates without signi   cant experience in
mathematics. on completion, the reader should have a good understanding of the techniques, practicalities
and philosophies of probabilistic aspects of machine learning and be well equipped to understand more
advanced research level material.

the structure of the book

the book begins with the basic concepts of id114 and id136. for the independent reader
chapters 1,2,3,4,5,9,10,13,14,15,16,17,21 and 23 would form a good introduction to probabilistic reasoning,
modelling and machine learning. the material in chapters 19, 24, 25 and 28 is more advanced, with the
remaining material being of more specialised interest. note that in each chapter the level of material is of
varying di   culty, typically with the more challenging material placed towards the end of each chapter. as
an introduction to the area of probabilistic modelling, a course can be constructed from the material as
indicated in the chart.

the material from parts i and ii has been successfully used for courses on id114. i have also
taught an introduction to probabilistic machine learning using material largely from part iii, as indicated.
these two courses can be taught separately and a useful approach would be to teach    rst the graphical
models course, followed by a separate probabilistic machine learning course.

a short course on approximate id136 can be constructed from introductory material in part i and the
more advanced material in part v, as indicated. the exact id136 methods in part i can be covered
relatively quickly with the material in part v considered in more in depth.

a timeseries course can be made by using primarily the material in part iv, possibly combined with material
from part i for students that are unfamiliar with probabilistic modelling approaches. some of this material,
particularly in chapter 25 is more advanced and can be deferred until the end of the course, or considered
for a more advanced course.

the references are generally to works at a level consistent with the book material and which are in the most
part readily available.

accompanying code

the brmltoolbox is provided to help readers see how mathematical models translate into actual mat-
lab code. there are a large number of demos that a lecturer may wish to use or adapt to help illustrate
the material. in addition many of the exercises make use of the code, helping the reader gain con   dence
in the concepts and their application. along with complete routines for many machine learning methods,
the philosophy is to provide low level routines whose composition intuitively follows the mathematical de-
scription of the algorithm. in this way students may easily match the mathematics with the corresponding
algorithmic implementation.

iv

draft november 9, 2017

e
s
r
u
o
c
g
n

i

n
r
a
e
l

e
n

i

h
c
a
m

c
i
t
s
i
l
i

b
a
b
o
r
p

e
s
r
u
o
c
t
r
o
h
s

e
c
n
e
r
e
f
n
i

e
t
a
m
i
x
o
r
p
p
a

e
s
r
u
o
c
s
l
e
d
o
m

l
a
c
i

h
p
a
r
g

e
s
r
u
o
c
g
n

i
l
l
e
d
o
m

c
i
t
s
i
l
i

b
a
b
o
r
p

e
s
r
u
o
c
t
r
o
h
s

s
e
i
r
e
s
-
e
m
t

i

part i:
id136 in probabilistic models

part ii:
learning in probabilistic models

part iii:
machine learning

part iv:
dynamical models

1: probabilistic reasoning
2: basic graph concepts
3: belief networks
4: id114
5: e   cient id136 in trees
6: the junction tree algorithm
7: making decisions

8: statistics for machine learning
9: learning as id136
10: naive bayes
11: learning with hidden variables
12: bayesian model selection

13: machine learning concepts
14: nearest neighbour classi   cation
15: unsupervised linear dimension reduction
16: supervised linear dimension reduction
17: linear models
18: bayesian linear models
19: gaussian processes
20: mixture models
21: latent linear models
22: latent ability models

23: discrete-state markov models
24: continuous-state markov models
25: switching linear dynamical systems
26: distributed computation

part v:
approximate id136

27: sampling
28: deterministic approximate id136

website

the brmltoolbox along with an electronic version of the book is available from

www.cs.ucl.ac.uk/staff/d.barber/brml

instructors seeking solutions to the exercises can    nd information at the website, along with additional
teaching materials.

draft november 9, 2017

v

other books in this area

the literature on machine learning is vast with much relevant literature also contained in statistics, en-
gineering and other physical sciences. a small list of more specialised books that may be referred to for
deeper treatments of speci   c topics is:

    id114

    id114 by s. lauritzen, oxford university press, 1996.
    id110s and decision graphs by f. jensen and t. d. nielsen, springer verlag, 2007.
    probabilistic networks and id109 by r. g. cowell, a. p. dawid, s. l. lauritzen and d.

j. spiegelhalter, springer verlag, 1999.

    probabilistic reasoning in intelligent systems by j. pearl, morgan kaufmann, 1988.
    id114 in applied multivariate statistics by j. whittaker, wiley, 1990.
    probabilistic id114: principles and techniques by d. koller and n. friedman, mit

press, 2009.

    machine learning and information processing

    id205, id136 and learning algorithms by d. j. c. mackay, cambridge uni-

versity press, 2003.

    pattern recognition and machine learning by c. m. bishop, springer verlag, 2006.
    an introduction to support vector machines, n. cristianini and j. shawe-taylor, cambridge

university press, 2000.

    gaussian processes for machine learning by c. e. rasmussen and c. k. i. williams, mit press,

2006.

acknowledgements

many people have helped this book along the way either in terms of reading, feedback, general insights,
allowing me to present their work, or just plain motivation. amongst these i would like to thank dan
cornford, massimiliano pontil, mark herbster, john shawe-taylor, vladimir kolmogorov, yuri boykov,
tom minka, simon prince, silvia chiappa, bertrand mesot, robert cowell, ali taylan cemgil, david blei,
je    bilmes, david cohn, david page, peter sollich, chris williams, marc toussaint, amos storkey, za-
kria hussain, le chen, seraf    n moral, milan studen  y, luc de raedt, tristan fletcher, chris vryonides,
yannis haralambous (and particularly for his help with example 1.5), tom furmston, ed challis and chris
bracegirdle. i would also like to thank the many students that have helped improve the material during
lectures over the years. i   m particularly grateful to taylan cemgil for allowing his graphlayout package to
be bundled with the brmltoolbox.

++

the sta    at cambridge university press have been a delight to work with and i would especially like to
thank heather bergman for her initial endeavors and the wonderful diana gillooly for her continued enthu-
siasm.

a heartfelt thankyou to my parents and sister     i hope this small token will make them proud. i   m also
fortunate to be able to acknowledge the support and generosity of friends throughout. finally, i   d like to
thank silvia who made it all worthwhile.

vi

draft november 9, 2017

brmltoolbox

the brmltoolbox is a lightweight set of routines that enables the reader to experiment with concepts in
id207, id203 theory and machine learning. the code contains basic routines for manipulating
discrete variable distributions, along with more limited support for continuous variables. in addition there
are many hard-coded standard machine learning algorithms. the website contains also a complete list of
all the teaching demos and related exercise material.

brmltoolkit

id207

ancestors
ancestralorder
descendents
children
edges
elimtri
connectedcomponents
istree
neigh
noselfpath
parents
spantree
triangulate
triangulateporder

- return the ancestors of nodes x in dag a
- return the ancestral order or the dag a (oldest first)
- return the descendents of nodes x in dag a
- return the children of variable x given adjacency matrix a
- return edge list from adjacency matrix a
- return a variable elimination sequence for a triangulated graph
- find the connected components of an adjacency matrix
- check if graph is singly-connected
- find the neighbours of vertex v on a graph with adjacency matrix g
- return a path excluding self transitions
- return the parents of variable x given adjacency matrix a
- find a spanning tree from an edge list
- triangulate adjacency matrix a
- triangulate adjacency matrix a according to a partial ordering

potential manipulation

condpot
changevar
dag
deltapot
disptable
divpots
drawfg
drawid
drawjtree
drawnet
evalpot
exppot
eyepot
grouppot
groupstate
logpot
markov
maxpot
maxsumpot
multpots
numstates

- return a potential conditioned on another variable
- change variable names in a potential
- return the adjacency matrix (zeros on diagonal) for a belief network
- a delta function potential
- print the table of a potential
- divide potential pota by potb
- draw the factor graph a
- plot an influence diagram
- plot a junction tree
- plot network
- evaluate the table of a potential when variables are set
- exponential of a potential
- return a unit potential
- form a potential based on grouping variables together
- find the state of the group variables corresponding to a given ungrouped state
- logarithm of the potential
- return a symmetric adjacency matrix of markov network in pot
- maximise a potential over variables
- maximise or sum a potential over variables
- multiply potentials into a single potential
- number of states of the variables in a potential

vii

orderpot
orderpotfields
potsample
potscontainingonly
potvariables
setevpot
setpot
setstate
squeezepots
sumpot
sumpotid
sumpots
table
ungrouppot
uniquepots
whichpot

- return potential with variables reordered according to order
- order the fields of the potential, creating blank entries where necessary
- draw sample from a single potential
- returns those potential numbers that contain only the required variables
- returns information about all variables in a set of potentials
- sets variables in a potential into evidential states
- sets potential variables to specified states
- set a potential   s specified joint state to a specified value
- eliminate redundant potentials (those contained wholly within another)
- sum potential pot over variables
- return the summed id203 and utility tables from an id
- sum a set of potentials
- return the potential table
- form a potential based on ungrouping variables
- eliminate redundant potentials (those contained wholly within another)
- returns potentials that contain a set of variables

routines also extend the toolbox to deal with gaussian potentials:
multpotsgaussianmoment.m, sumpotgaussiancanonical.m, sumpotgaussianmoment.m, multpotsgaussiancanonical.m
see demosumprodgausscanon.m, demosumprodgausscanonlds.m, demosumprodgaussmoment.m

id136

- update potentials in absorption message passing on a junction tree
- perform full round of absorption on a junction tree
- perform full round of absorption on an influence diagram
- ancestral sampling from a belief network
- get the map assignment for a binary mrf with positive w
- bucket elimination on a set of potentials
- conditional independence check using graph of variable interactions
- compute the empirical log bayes factor and mi for independence/dependence
- numerical conditional independence measure
- conditional mutual information i(x,y|z) of a potential.

absorb
absorption
absorptionid
ancestralsample
binarymrfmap
bucketelim
condindep
condindepemp
condindeppot
condmi
factorconnectingvariable - factor nodes connecting to a set of variables
factorgraph
idvars
jtassignpot
jtree
jtreeid
loopybp
maxflow
maxnpot
maxnprodfg
maxprodfg
mdpemdeterministicpolicy - solve mdp using em with deterministic policy
mdpsolve
messtofact
metropolis
mostprobablepath
mostprobablepathmult
sumprodfg

- returns a factor graph adjacency matrix based on potentials
- id203 and decision variables from a partial order
- assign potentials to cliques in a junction tree
- setup a junction tree based on a set of potentials
- setup a junction tree based on an influence diagram
- loopy belief propagation using sum-product algorithm
- ford fulkerson max flow - min cut algorithm (breadth first search)
- find the n most probable values and states in a potential
- n-max-product algorithm on a factor graph (returns the nmax most probable states)
- max-product algorithm on a factor graph

- solve a markov decision process
- returns the message numbers that connect into factor potential
- metropolis sample
- find the most probable path in a markov chain
- find the all source all sink most probable paths in a markov chain
- sum-product algorithm on a factor graph represented by a

speci   c models

arlds
artrain
bayeslinreg
bayeslogregressionrvm
canonvar
cca
covfnge
embeliefnet
emminimizekl
emqtranmarginal
emqutilmarginal
emtotalbetamessage
emvaluetable
fa

viii

- learn ar coefficients using a linear dynamical system
- fit autoregressive (ar) coefficients of order l to v.
- bayesian id75 training using basis functions phi(x)
- bayesian id28 with the relevance vector machine
- canonical variates (no post rotation of variates)
- canonical correlation analysis
- gamma exponential covariance function
- train a belief network using expectation maximisation
- mdp deterministic policy solver. finds optimal actions
- em marginal transition in mdp
- returns term proportional to the q marginal for the utility term
- backward information needed to solve the mdp process using message passing
- mdp solver calculates the value function of the mdp with the current policy
- factor analysis

draft november 9, 2017

- fit a mixture of gaussian to the data x using em
- gaussian process binary classification
- gaussian process regression
- learn a sequence for a hopfield network
- id48 backward pass
- backward pass (beta method) for the switching autoregressive id48
- em algorithm for id48
- id48 forward pass
- switching autoregressive id48 with switches updated only every tskip timesteps
- id48 posterior smoothing using the rauch-tung-striebel correction method
- smoothing for a hidden markov model (id48)
- switching autoregressive id48 smoothing
- viterbi most likely joint hidden state of a id48
- a kernel evaluated at two points
- id116 id91 algorithm
- full backward pass for a latent linear dynamical system (rts correction method)
- single backward update for a latent linear dynamical system (rts smoothing update)
- full forward pass for a latent linear dynamical system (kalman filter)
- single forward update for a latent linear dynamical system (kalman filter)
- linear dynamical system : filtering and smoothing
- subspace method for identifying linear dynamical system
- learning logistic id75 using gradient ascent (batch version)
- em training of a mixture of a product of bernoulli distributions
- em training for a mixture of markov models
- naive bayes prediction having used a dirichlet prior for training

gmmem
gpclass
gpreg
hebbml
id48backward
id48backwardsar
id48em
id48forward
id48forwardsar
id48gamma
id48smooth
id48smoothsar
id48viterbi
kernel
kmeans
ldsbackward
ldsbackwardupdate
ldsforward
ldsforwardupdate
ldssmooth
ldssubspace
logreg
mixprodbern
mixmarkov
naivebayesdirichlettest
naivebayesdirichlettrain - naive bayes training using a dirichlet prior
naivebayestest
naivebayestrain
nearneigh
pca
plsa
plsacond
rbf
sarlearn
sldsbackward
sldsforward
sldsmarggauss
softloss
svdm
id166train

- test naive bayes bernoulli distribution after max likelihood training
- train naive bayes bernoulli distribution using max likelihood
- nearest neighbour classification
- principal components analysis
- probabilistic latent semantic analysis
- conditional plsa (probabilistic latent semantic analysis)
- radial basis function output
- em training of a switching ar model
- backward pass using a mixture of gaussians
- switching latent linear dynamical system gaussian sum forward pass
- compute the single gaussian from a weighted slds mixture
- soft id168
- singular value decomposition with missing values
- train a support vector machine

general

argmax
assign
betaxbiggery
bar3zcolor
avsigmagauss
cap
chi2test
count
condexp
condp
dirrnd
field2cell
gausscond
hinton
ind2subv
ismember_sorted
lengthcell
logdet
logeps
loggaussgamma
logsumexp
logzdirichlet
majority
maxarray
maxnarray

- performs argmax returning the index and value
- assigns values to variables
- p(x>y) for x~beta(a,b), y~beta(c,d)
- plot a 3d bar plot of the matrix z
- average of a logistic sigmoid under a gaussian
- cap x at absolute value c
- inverse of the chi square cumulative density
- for a data matrix (each column is a datapoint), return the state counts
- compute normalised p proportional to exp(logp);
- make a conditional distribution from the matrix
- samples from a dirichlet distribution
- place the field of a structure in a cell
- return the mean and covariance of a conditioned gaussian
- plot a hinton diagram
- subscript vector from linear index
- true for member of sorted set
- length of each cell entry
- log determinant of a positive definite matrix computed in a numerically stable manner
- log(x+eps)
- unnormalised log of the gauss-gamma distribution
- compute log(sum(exp(a).*b)) valid for large a
- log normalisation constant of a dirichlet distribution with parameter u
- return majority values in each column on a matrix
- maximise a multi-dimensional array over a set of dimensions
- find the highest values and states of an array over a set of dimensions

draft november 9, 2017

ix

mix2mix
mvrandn
mygamrnd
mynanmean
mynansum
mynchoosek
myones
myrand
myzeros
normp
randgen
replace
sigma
sigmoid
sqdist
subv2ind
sumlog

miscellaneous

compat
logp
placeobject
plotcov
pointscov
setup
validgridposition

- fit a mixture of gaussians with another mixture of gaussians
- samples from a multi-variate normal(gaussian) distribution
- gamma random variate generator
- mean of values that are not nan
- sum of values that are not nan
- binomial coefficient v choose k
- same as ones(x), but if x is a scalar, interprets as ones([x 1])
- same as rand(x) but if x is a scalar interprets as rand([x 1])
- same as zeros(x) but if x is a scalar interprets as zeros([x 1])
- make a normalised distribution from an array
- generates discrete random variables given the pdf
- replace instances of a value with another value
- 1./(1+exp(-x))
- 1./(1+exp(-beta*x))
- square distance between vectors in x and y
- linear index from subscript vector.
- sum(log(x)) with a cutoff at 10e-200

- compatibility of object f being in position h for image v on grid gx,gy
- the logarithm of a specific non-gaussian distribution
- place the object f at position h in grid gx,gy
- return points for plotting an ellipse of a covariance
- unit variance contours of a 2d gaussian with mean m and covariance s
- run me at initialisation -- checks for bugs in matlab and initialises path
- returns 1 if point is on a defined grid

x

draft november 9, 2017

contents

front matter

i
ii
notation list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
preface
ii
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
brml toolbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii
contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi

i

id136 in probabilistic models

1

1 probabilistic reasoning

1.1 id203 refresher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
interpreting id155 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7
7
1.1.1
9
1.1.2 id203 tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.2 probabilistic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.3 prior, likelihood and posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.3.1 two dice : what were the individual scores? . . . . . . . . . . . . . . . . . . . . . . . . 19
1.4 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.5 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.5.1 basic id203 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.5.2 general utilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
1.5.3 an example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

1.6 exercises

2 basic graph concepts

29
2.1 graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.2 numerically encoding graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.2.1 edge list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.2.2 adjacency matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.2.3 clique matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.3 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.4 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.4.1 utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

2.5 exercises

3 belief networks

37
3.1 the bene   ts of structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.1.1 modelling independencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.1.2 reducing the burden of speci   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.2 uncertain and unreliable evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2.1 uncertain evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

xi

contents

contents

3.2.2 unreliable evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.3 belief networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.3.1 conditional independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.3.2 the impact of collisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.3.3 graphical path manipulations for independence . . . . . . . . . . . . . . . . . . . . . . 49
3.3.4
d-separation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.3.5 graphical and distributional in/dependence . . . . . . . . . . . . . . . . . . . . . . . . 49
3.3.6 markov equivalence in belief networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.3.7 belief networks have limited expressibility . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.4 causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.4.1
simpson   s paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.4.2 the do-calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.4.3
in   uence diagrams and the do-calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.5 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.6 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.6.1 naive id136 demo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.6.2 conditional independence demo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.6.3 utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

3.7 exercises

4 id114

65
4.1 id114 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.2 markov networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.2.1 markov properties
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.2.2 markov random    elds
4.2.3 hammersley-cli   ord theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.2.4 conditional independence using markov networks . . . . . . . . . . . . . . . . . . . . . 71
4.2.5 lattice models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.3 chain id114 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.4 factor graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.4.1 conditional independence in factor graphs . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.5 expressiveness of id114 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.7 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.8 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

5 e   cient id136 in trees

5.2 other forms of id136

83
5.1 marginal id136 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.1.1 variable elimination in a markov chain and message passing . . . . . . . . . . . . . . . 83
5.1.2 the sum-product algorithm on factor graphs
. . . . . . . . . . . . . . . . . . . . . . . 86
5.1.3 dealing with evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.1.4 computing the marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.1.5 the problem with loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
5.2.1 max-product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
5.2.2 finding the n most probable states
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.2.3 most probable path and shortest path . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
5.2.4 mixed id136 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
id136 in multiply connected graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.3.1 bucket elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
5.3.2 loop-cut conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.4 message passing for continuous distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5.5 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5.6 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.6.1 factor graph examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.6.2 most probable and shortest path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

5.3

xii

draft november 9, 2017

contents

contents

5.6.3 bucket elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
5.6.4 message passing on gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

5.7 exercises

6 the junction tree algorithm

6.2 clique graphs

6.3 junction trees

6.1 id91 variables

107
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.1.1 reparameterisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
6.2.1 absorption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.2.2 absorption schedule on clique trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.3.1 the running intersection property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.4 constructing a junction tree for singly-connected distributions . . . . . . . . . . . . . . . . 114
6.4.1 moralisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
6.4.2 forming the clique graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
6.4.3 forming a junction tree from a clique graph . . . . . . . . . . . . . . . . . . . . . . . . 114
6.4.4 assigning potentials to cliques
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.5 junction trees for multiply-connected distributions . . . . . . . . . . . . . . . . . . . . . . . 115
6.5.1 triangulation algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6.6 the junction tree algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
6.6.1 remarks on the jta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.6.2 computing the normalisation constant of a distribution . . . . . . . . . . . . . . . . . 120
6.6.3 the marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
some small jta examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.6.4
6.6.5
shafer-shenoy propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.7 finding the most likely state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.8 reabsorption : converting a junction tree to a directed network . . . . . . . . . . . . . . . 124
6.9 the need for approximations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
6.9.1 bounded width junction trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
6.10 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
6.11 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
6.11.1 utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127

6.12 exercises

7 making decisions

7.3.1

syntax of in   uence diagrams

7.4 solving in   uence diagrams

131
7.1 expected utility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.1.1 utility of money . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.2 id90 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
7.3 extending id110s for decisions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
7.4.1 messages on an id . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
7.4.2 using a junction tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
7.5 id100 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.5.1 maximising expected utility by message passing . . . . . . . . . . . . . . . . . . . . . . 144
7.5.2 bellman   s equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
7.6.1 value iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
7.6.2 policy iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
7.6.3 a curse of dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
7.7 variational id136 and planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.8 financial matters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
7.8.1 options pricing and expected utility . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
7.8.2 binomial options pricing model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
7.8.3 optimal investment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154

7.6 temporally unbounded mdps

7.9 further topics

draft november 9, 2017

xiii

contents

contents

7.9.1 partially observable mdps
7.9.2 id23

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
7.10 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
7.11 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
7.11.1 sum/max under a partial order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
7.11.2 junction trees for in   uence diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
7.11.3 party-friend example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
7.11.4 chest clinic with decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
7.11.5 id100 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

7.12 exercises

ii learning in probabilistic models

165

8 statistics for machine learning

8.2 distributions

169
8.1 representing data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.1.1 categorical
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.1.2 ordinal
8.1.3 numerical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.2.1 the id181 kl(q|p) . . . . . . . . . . . . . . . . . . . . . . . . . 173
8.2.2 id178 and information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
8.3 classical distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
8.4 multivariate gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
8.4.1 completing the square . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
8.4.2 conditioning as system reversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
8.4.3 whitening and centering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
8.5 exponential family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
8.5.1 conjugate priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
8.6 learning distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
8.7 properties of maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
8.7.1 training assuming the correct model class . . . . . . . . . . . . . . . . . . . . . . . . . 187
8.7.2 training when the assumed model is incorrect . . . . . . . . . . . . . . . . . . . . . . . 187
8.7.3 maximum likelihood and the empirical distribution . . . . . . . . . . . . . . . . . . . . 188
8.8 learning a gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
8.8.1 maximum likelihood training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
8.8.2 bayesian id136 of the mean and variance . . . . . . . . . . . . . . . . . . . . . . . . 189
8.8.3 gauss-gamma distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
8.9 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
8.10 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
8.11 exercises

9 learning as id136

203
9.1 learning as id136 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
9.1.1 learning the bias of a coin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
9.1.2 making decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
9.1.3 a continuum of parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
9.1.4 decisions based on continuous intervals
. . . . . . . . . . . . . . . . . . . . . . . . . . 206
9.2 bayesian methods and ml-ii
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
9.3 maximum likelihood training of belief networks . . . . . . . . . . . . . . . . . . . . . . . . . 208
9.4 bayesian belief network training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
9.4.1 global and local parameter independence . . . . . . . . . . . . . . . . . . . . . . . . . 211
9.4.2 learning binary variable tables using a beta prior
. . . . . . . . . . . . . . . . . . . . 212
9.4.3 learning multivariate discrete tables using a dirichlet prior . . . . . . . . . . . . . . . 214
9.5 structure learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
9.5.1 pc algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218

xiv

draft november 9, 2017

contents

contents

9.5.2 empirical independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
9.5.3 network scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
9.5.4 chow-liu trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
9.6 maximum likelihood for undirected models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
9.6.1 the likelihood gradient
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
9.6.2 general tabular clique potentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
9.6.3 decomposable markov networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
9.6.4 exponential form potentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
9.6.5 conditional random    elds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
9.6.6 pseudo likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
9.6.7 learning the structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
9.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
9.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
9.8.1 pc algorithm using an oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
9.8.2 demo of empirical conditional independence . . . . . . . . . . . . . . . . . . . . . . . . 237
9.8.3 bayes dirichlet structure learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238

9.9 exercises

10 naive bayes

241
10.1 naive bayes and conditional independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
10.2 estimation using maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
10.2.1 binary attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
10.2.2 multi-state variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
10.2.3 text classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
10.3 bayesian naive bayes
10.4 tree augmented naive bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.4.1 learning tree augmented naive bayes networks . . . . . . . . . . . . . . . . . . . . . . 248
10.5 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
10.6 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
10.7 exercises

11 learning with hidden variables

253
11.1 hidden variables and missing data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
11.1.1 why hidden/missing variables can complicate proceedings . . . . . . . . . . . . . . . . 253
11.1.2 the missing at random assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
11.1.3 maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
11.1.4 identi   ability issues
11.2 expectation maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
11.2.1 variational em . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
11.2.2 classical em . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
11.2.3 application to belief networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
11.2.4 general case
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.2.5 convergence
11.2.6 application to markov networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.3 extensions of em . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.3.1 partial m step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.3.2 partial e-step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.4 a failure case for em . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
11.5 id58 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
11.5.1 em is a special case of id58 . . . . . . . . . . . . . . . . . . . . . . . . . . 270
11.5.2 an example: vb for the asbestos-smoking-cancer network . . . . . . . . . . . . . . . 270
. . . . . . . . . . . . . . . . . . . . . . . . . 273
11.6.1 undirected models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
11.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
11.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
11.9 exercises

11.6 optimising the likelihood by gradient methods

draft november 9, 2017

xv

contents

contents

12 bayesian model selection

279
12.1 comparing models the bayesian way . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
12.2 illustrations : coin tossing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
12.2.1 a discrete parameter space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
12.2.2 a continuous parameter space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
12.3 occam   s razor and bayesian complexity penalisation . . . . . . . . . . . . . . . . . . . . . . 282
12.4 a continuous example : curve    tting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
12.5 approximating the model likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
12.5.1 laplace   s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
12.5.2 bayes information criterion (bic)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
12.6 bayesian hypothesis testing for outcome analysis . . . . . . . . . . . . . . . . . . . . . . . . 288
12.6.1 outcome analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
12.6.2 hindep : model likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
12.6.3 hsame : model likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
12.6.4 dependent outcome analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
12.6.5 is classi   er a better than b? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
12.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
12.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
12.9 exercises

iii machine learning

299

13 machine learning concepts

13.1 styles of learning

303
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
13.1.1 supervised learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
13.1.2 unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
13.1.3 anomaly detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
13.1.4 online (sequential) learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
13.1.5 interacting with the environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
13.1.6 semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
13.2 supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
13.2.1 utility and loss
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
13.2.2 using the empirical distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
13.2.3 bayesian decision approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
13.3 bayes versus empirical decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
13.4 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
13.5 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

14 nearest neighbour classi   cation

317
14.1 do as your neighbour does . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
14.2 k-nearest neighbours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
14.3 a probabilistic interpretation of nearest neighbours . . . . . . . . . . . . . . . . . . . . . . . 320
14.3.1 when your nearest neighbour is far away . . . . . . . . . . . . . . . . . . . . . . . . . 321
14.4 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
14.5 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
14.6 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321

15 unsupervised linear dimension reduction

323
15.1 high-dimensional spaces     low dimensional manifolds
. . . . . . . . . . . . . . . . . . . . . 323
15.2 principal components analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
15.2.1 deriving the optimal linear reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . 324
15.2.2 maximum variance criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
15.2.3 pca algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
15.2.4 pca and nearest neighbours classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . 328
15.2.5 comments on pca . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328

xvi

draft november 9, 2017

contents

contents

15.4 latent semantic analysis

15.3 high dimensional data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
15.3.1 eigen-decomposition for n < d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
15.3.2 pca via singular value decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
15.4.1 information retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
15.5 pca with missing data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
15.5.1 finding the principal directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
15.5.2 collaborative    ltering using pca with missing data . . . . . . . . . . . . . . . . . . . 335
15.6 matrix decomposition methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
15.6.1 probabilistic latent semantic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
15.6.2 extensions and variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
15.6.3 applications of plsa/nmf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
15.7 kernel pca . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
15.8 canonical correlation analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
15.8.1 svd formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
15.9 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
15.10code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
15.11exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347

16 supervised linear dimension reduction

351
16.1 supervised linear projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
16.2 fisher   s linear discriminant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
16.3 canonical variates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
16.3.1 dealing with the nullspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
16.4 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
16.5 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
16.6 exercises

17 linear models

17.3 the dual representation and kernels

359
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
17.1 introduction: fitting a straight line
17.2 linear parameter models for regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
17.2.1 vector outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
17.2.2 regularisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
17.2.3 radial basis functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
17.3.1 regression in the dual-space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
17.4 linear parameter models for classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
17.4.1 id28 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
17.4.2 beyond    rst order gradient ascent
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
17.4.3 avoiding overcon   dent classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
17.4.4 multiple classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
17.4.5 the kernel trick for classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
17.5.1 maximum margin linear classi   er . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
17.5.2 using kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
17.5.3 performing the optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
17.5.4 probabilistic interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
17.6 soft zero-one loss for outlier robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
17.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
17.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
17.9 exercises

17.5 support vector machines

draft november 9, 2017

xvii

contents

contents

18 bayesian linear models

381
18.1 regression with additive gaussian noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
18.1.1 bayesian linear parameter models
18.1.2 determining hyperparameters: ml-ii
. . . . . . . . . . . . . . . . . . . . . . . . . . . 383
18.1.3 learning the hyperparameters using em . . . . . . . . . . . . . . . . . . . . . . . . . . 384
18.1.4 hyperparameter optimisation : using the gradient
. . . . . . . . . . . . . . . . . . . . 385
18.1.5 validation likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
18.1.6 prediction and model averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
18.1.7 sparse linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
18.2 classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
18.2.1 hyperparameter optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
18.2.2 laplace approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
18.2.3 variational gaussian approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
18.2.4 local variational approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
18.2.5 relevance vector machine for classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . 395
18.2.6 multi-class case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
18.3 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
18.4 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
18.5 exercises

19 gaussian processes

399
19.1 non-parametric prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
19.1.1 from parametric to non-parametric
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
19.1.2 from bayesian linear models to gaussian processes . . . . . . . . . . . . . . . . . . . . 400
19.1.3 a prior on functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
19.2 gaussian process prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
19.2.1 regression with noisy training outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
19.3 covariance functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
19.3.1 making new covariance functions from old . . . . . . . . . . . . . . . . . . . . . . . . . 405
19.3.2 stationary covariance functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
19.3.3 non-stationary covariance functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
19.4 analysis of covariance functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
19.4.1 smoothness of the functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
19.4.2 mercer kernels
19.4.3 fourier analysis for stationary kernels
. . . . . . . . . . . . . . . . . . . . . . . . . . . 409
19.5 gaussian processes for classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
19.5.1 binary classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
19.5.2 laplace   s approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
19.5.3 hyperparameter optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
19.5.4 multiple classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
19.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
19.7 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
19.8 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415

20 mixture models

20.3 the gaussian mixture model

417
20.1 density estimation using mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
20.2 expectation maximisation for mixture models
. . . . . . . . . . . . . . . . . . . . . . . . . . 418
20.2.1 unconstrained discrete tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
20.2.2 mixture of product of bernoulli distributions . . . . . . . . . . . . . . . . . . . . . . . 420
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
20.3.1 em algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
20.3.2 practical issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
20.3.3 classi   cation using gaussian mixture models . . . . . . . . . . . . . . . . . . . . . . . 427
20.3.4 the parzen estimator
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
20.3.5 id116
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
20.3.6 bayesian mixture models

xviii

draft november 9, 2017

contents

contents

20.6 mixed membership models

20.5.1 joint indicator approach: factorised prior
20.5.2 polya prior

20.3.7 semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
20.4 mixture of experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
20.5 indicator models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
. . . . . . . . . . . . . . . . . . . . . . . . . 431
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
20.6.1 id44 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
20.6.2 graph based representations of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
20.6.3 dyadic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
20.6.4 monadic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
20.6.5 cliques and adjacency matrices for monadic binary data . . . . . . . . . . . . . . . . . 437
20.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
20.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
20.9 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441

21 latent linear models

443
21.1 factor analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
21.1.1 finding the optimal bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
21.2 factor analysis : maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
21.2.1 eigen-approach likelihood optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
21.2.2 expectation maximisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
21.3 interlude: modelling faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
. . . . . . . . . . . . . . . . . . . . . . . . . . . 452
21.4 probabilistic principal components analysis
21.5 canonical correlation analysis and factor analysis
. . . . . . . . . . . . . . . . . . . . . . . 453
21.6 independent components analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
21.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
21.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
21.9 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456

22 latent ability models

459
22.1 the rasch model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
22.1.1 maximum likelihood training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
22.1.2 bayesian rasch models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
22.2 competition models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
22.2.1 bradley-terry-luce model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
22.2.2 elo ranking model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
22.2.3 glicko and trueskill
22.3 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
22.4 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
22.5 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463

iv dynamical models

465

23 discrete-state markov models

23.2 id48

469
23.1 markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
23.1.1 equilibrium and stationary distribution of a markov chain . . . . . . . . . . . . . . . . 470
23.1.2 fitting markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
23.1.3 mixture of markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474
23.2.1 the classical id136 problems
23.2.2 filtering p(ht|v1:t)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475
23.2.3 parallel smoothing p(ht|v1:t )
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
23.2.4 correction smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
23.2.5 sampling from p(h1:t|v1:t )
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
23.2.6 most likely joint state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478

draft november 9, 2017

xix

contents

contents

23.3 learning id48s

23.2.7 prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
23.2.8 self localisation and kidnapped robots . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
23.2.9 natural language models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
23.3.1 em algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
23.3.2 mixture emission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
23.3.3 the id48-gmm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
23.3.4 discriminative training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
23.4 related models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
23.4.1 explicit duration model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
23.4.2 input-output id48 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
23.4.3 linear chain crfs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
23.4.4 dynamic id110s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
23.5 applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
23.5.1 object tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
23.5.2 automatic id103 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
23.5.3 bioinformatics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
23.5.4 part-of-speech tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
23.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
23.7 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
23.8 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490

24 continuous-state markov models

24.1.1 stationary distribution with noise

497
24.1 observed linear dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498
24.2 auto-regressive models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
24.2.1 training an ar model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
24.2.2 ar model as an olds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
24.2.3 time-varying ar model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
24.2.4 time-varying variance ar models
24.3 latent linear dynamical systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
24.4 id136 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
24.4.1 filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
24.4.2 smoothing : rauch-tung-striebel correction method . . . . . . . . . . . . . . . . . . . 508
24.4.3 the likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509
24.4.4 most likely state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
24.4.5 time independence and riccati equations . . . . . . . . . . . . . . . . . . . . . . . . . 510
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
24.5.1 identi   ability issues
24.5.2 em algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
24.5.3 subspace methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
24.5.4 structured ldss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
24.5.5 bayesian ldss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
24.6.1 id136
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
24.6.2 maximum likelihood learning using em . . . . . . . . . . . . . . . . . . . . . . . . . . 515
24.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
24.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
24.8.1 autoregressive models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518

24.5 learning linear dynamical systems

24.6 switching auto-regressive models

24.9 exercises

xx

draft november 9, 2017

contents

contents

25 switching linear dynamical systems

521
25.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
25.2 the switching lds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
25.2.1 exact id136 is computationally intractable . . . . . . . . . . . . . . . . . . . . . . . 522
25.3 gaussian sum filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522
25.3.1 continuous    ltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
25.3.2 discrete    ltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
25.3.3 the likelihood p(v1:t ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
25.3.4 collapsing gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
25.3.5 relation to other methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526
25.4 gaussian sum smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526
25.4.1 continuous smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
25.4.2 discrete smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
25.4.3 collapsing the mixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
25.4.4 using mixtures in smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529
25.4.5 relation to other methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
25.5 reset models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
25.5.1 a poisson reset model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
25.5.2 reset-id48-lds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
25.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
25.7 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
25.8 exercises

26 distributed computation

539
26.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
26.2 stochastic hop   eld networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
26.3 learning sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540
26.3.1 a single sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540
26.3.2 multiple sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
26.3.3 boolean networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
26.3.4 sequence disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
26.4 tractable continuous latent variable models . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
26.4.1 deterministic latent variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
26.4.2 an augmented hop   eld network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
26.5.1 stochastically spiking neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
26.5.2 hop   eld membrane potential
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
26.5.3 dynamic synapses
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550
26.5.4 leaky integrate and    re models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
26.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
26.7 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
26.8 exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552

26.5 neural models

v approximate id136

553

27 sampling

557
27.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
27.1.1 univariate sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558
27.1.2 rejection sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
27.1.3 multivariate sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
27.2 ancestral sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562
27.2.1 dealing with evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562
27.2.2 perfect sampling for a markov network . . . . . . . . . . . . . . . . . . . . . . . . . . 563
27.3 id150 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
27.3.1 id150 as a markov chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564

draft november 9, 2017

xxi

contents

contents

27.4 id115 (mcmc)

27.3.2 structured id150 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
27.3.3 remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
27.4.1 markov chains
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
27.4.2 metropolis-hastings sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
27.5 auxiliary variable methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
27.5.1 hybrid monte carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
27.5.2 swendson-wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
27.5.3 slice sampling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
27.6 importance sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574
27.6.1 sequential importance sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576
27.6.2 particle    ltering as an approximate forward pass . . . . . . . . . . . . . . . . . . . . . 577
27.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
27.8 code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
27.9 exercises

28 deterministic approximate id136

585
28.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
28.2 the laplace approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
28.3 properties of kullback-leibler variational id136
. . . . . . . . . . . . . . . . . . . . . . . 586
28.3.1 bounding the normalisation constant . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
28.3.2 bounding the marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
28.3.3 bounding marginal quantities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587
. . . . . . . . . . . . . . . . . . . . . . 587
28.3.4 gaussian approximations using kl divergence
28.3.5 marginal and moment matching properties of minimising kl(p|q)
. . . . . . . . . . . 588
28.4 variational bounding using kl(q|p) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
28.4.1 pairwise markov random    eld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
28.4.2 general mean    eld equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592
28.4.3 asynchronous updating guarantees approximation improvement
. . . . . . . . . . . . 592
28.4.4 structured variational approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
28.5 local and kl variational approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
28.5.1 local approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596
28.5.2 kl variational approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596
28.6 mutual information maximisation : a kl variational approach . . . . . . . . . . . . . . . . 597
28.6.1 the information maximisation algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 598
28.6.2 linear gaussian decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599
28.7 loopy belief propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
28.7.1 classical bp on an undirected graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
28.7.2 loopy bp as a variational procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601
28.8 expectation propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603
28.9 map for markov networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
28.9.1 pairwise markov networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608
28.9.2 attractive binary markov networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609
28.9.3 potts model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611
28.10further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612
28.11summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612
28.12code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613
28.13exercises

end matter

vi appendix

a background mathematics

xxii

619

619

621

draft november 9, 2017

contents

contents

a.2 multivariate calculus

a.1 id202 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621
a.1.1 vector algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621
a.1.2 the scalar product as a projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
a.1.3 lines in space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
a.1.4 planes and hyperplanes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
a.1.5 matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623
a.1.6 linear transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
a.1.7 determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
a.1.8 matrix inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 625
a.1.9 computing the matrix inverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
a.1.10 eigenvalues and eigenvectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
a.1.11 matrix decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 628
a.2.1 interpreting the gradient vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
a.2.2 higher derivatives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
a.2.3 matrix calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630
a.3 inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630
a.3.1 convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630
a.3.2 jensen   s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631
a.4 optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631
a.5 multivariate optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631
a.5.1 id119 with    xed stepsize . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
a.5.2 id119 with line searches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
a.5.3 minimising quadratic functions using line search . . . . . . . . . . . . . . . . . . . . . 633
a.5.4 gram-schmidt construction of conjugate vectors
. . . . . . . . . . . . . . . . . . . . . 633
a.5.5 the conjugate vectors algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634
a.5.6 the conjugate gradients algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634
a.5.7 newton   s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635
a.6 constrained optimisation using lagrange multipliers . . . . . . . . . . . . . . . . . . . . . . . 637
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637

a.6.1 lagrange dual

bibliography

index

639

655

draft november 9, 2017

xxiii

contents

contents

xxiv

draft november 9, 2017

part i

id136 in probabilistic models

1

introduction to part i

probabilistic models explicitly take into account uncertainty and deal with our
imperfect knowledge of the world. such models are of fundamental signi   cance in
machine learning since our understanding of the world will always be limited by our
observations and understanding. we will focus initially on using probabilistic models
as a kind of expert system.

in part i, we assume that the model is fully speci   ed. that is, given a model of the
environment, how can we use it to answer questions of interest. we will relate the
complexity of inferring quantities of interest to the structure of the graph describing
the model.
in addition, we will describe operations in terms of manipulations on
the corresponding graphs. as we will see, provided the graphs are simple tree-like
structures, most quantities of interest can be computed e   ciently.

part i deals with manipulating mainly discrete variable distributions and forms the
background to all the later material in the book.

draft november 9, 2017

3

some members of the id114 family and their uses. nodes further from the id114
root node are loosely speaking specialised versions of their parents. we discuss many of these models in
part i, although some of the more specialised models are deferred to later parts of the book.

4

draft november 9, 2017

graphicalmodelsdirecteddirectedfactorgraphbayesiannetworksdynamicbayesnetsmarkovchainsid48ldslatentvariablemodelsdiscretemixturemodelscluster-ingcontinuousdimen-reductover-completerepres.in   uencediagramsstrongjtdecisiontheorychaingraphsundirectedgraphsmarkovnetworkinputdependentcrfpairwiseboltz.machine(disc.)gauss.process(cont)cliquegraphsjunctiontreecliquetreefactorgraphsid114 and associated (marginal) id136 methods. speci   c id136 methods are highlighted
in red. loosely speaking, provided the graph corresponding to the model is singly-connected most of
the standard (marginal) id136 methods are tractable. multiply-connected graphs are generally more
problematic, although there are special cases which remain tractable.

draft november 9, 2017

5

graphicalmodelmultiplyconnecteddecomposablecliquessmallmessages-tractablemessage-passingmessages-intractableapprox-requiredcliqueslargeapprox-requirednon-decomposablejtacliquessmallabsorptionshafer-shenoycliquesbig(ormess.intract)approx-requiredcutsetconditioning(inef   cient)tractable-special-casesgaussianattractive-binary-mrf-mapplanar-binary-pure-interaction-mrfsinglyconnectedmessageupdatestractablesum/maxproductmessageupdatesintractableapproxrequired(ep)bucketelimination(inef   cient)6

draft november 9, 2017

chapter 1

probabilistic reasoning

we have intuition about how uncertainty works in simple cases. to reach sensible conclusions in complicated
situations, however     where there may be many (possibly) related events and many possible outcomes    
we need a formal    calculus    that extends our intuitive notions. the concepts, mathematical language and
rules of id203 give us the formal framework we need.
in this chapter we review basic concepts in
id203     in particular, id155 and bayes    rule, the workhorses of machine learning.
another strength of the language of id203 is that it structures problems in a form consistent for
computer implementation. we also introduce basic features of the brmltoolbox that support manipulating
id203 distributions.

1.1 id203 refresher

variables, states and notational shortcuts

variables will be denoted using either upper case x or lower case x and a set of variables will typically be
denoted by a calligraphic symbol, for example v = {a, b, c} .
the domain of a variable x is written dom(x), and denotes the states x can take. states will typically
be represented using sans-serif font. for example, for a coin c, dom(c) = {heads, tails} and p(c = heads)
represents the id203 that variable c is in state heads.
the meaning of p(state) will often be clear,
without speci   c reference to a variable. for example, if we are discussing an experiment about a coin c,
the meaning of p(heads) is clear from the context, being shorthand for p(c = heads). when summing over a
s   dom(x) f (x = s).
given a variable, x, its domain dom(x) and a full speci   cation of the id203 values for each of the
variable states, p(x), we have a distribution for x. sometimes we will not fully specify the distribution, only
certain properties, such as for variables x, y, p(x, y) = p(x)p(y) for some unspeci   ed p(x) and p(y). when
clarity on this is required we will say distributions with structure p(x)p(y), or a distribution class p(x)p(y).

x f (x), the interpretation is that all states of x are included, i.e. (cid:80)

variable(cid:80)

x f (x)    

(cid:80)

for our purposes, events are expressions about random variables, such as two heads in 6 coin tosses. two
events are mutually exclusive if they cannot both be true. for example the events the coin is heads and
the coin is tails are mutually exclusive. one can think of de   ning a new variable named by the event so,
for example, p(the coin is tails) can be interpreted as p(the coin is tails = true). we use the shorthand
p(x = tr) for the id203 of event/variable x being in the state true and p(x = fa) for the id203 of
variable x being in the state false.

de   nition 1.1 (rules of id203 for discrete variables).

7

the id203 p(x = x) of variable x being in state x is represented by a value between 0 and 1.
p(x = x) = 1 means that we are certain x is in state x. conversely, p(x = x) = 0 means that we are certain
x is not in state x. values between 0 and 1 represent the degree of certainty of state occupancy.

id203 refresher

the summation of the id203 over all the states is 1:

(cid:88)

p(x = x) = 1

x   dom(x)

this is called the normalisation condition. we will usually more conveniently write(cid:80)

x p(x) = 1.

two variables x and y can interact through

p(x = a or y = b) = p(x = a) + p(y = b)     p(x = a and y = b)

or, more generally, we can write

p(x or y) = p(x) + p(y)     p(x and y)

(1.1.1)

(1.1.2)

(1.1.3)

we will use the shorthand p(x, y) for p(x and y). note that p(y, x) = p(x, y) and p(x or y) = p(y or x).

de   nition 1.2 (set notation). an alternative notation in terms of set theory is to write

p(x or y)     p(x     y),

p(x, y)     p(x     y)

(1.1.4)

de   nition 1.3 (marginals). given a joint distribution p(x, y) the distribution of a single variable is given
by

p(x) =

p(x, y)

(1.1.5)

(cid:88)

y

(cid:88)

here p(x) is termed a marginal of the joint id203 distribution p(x, y). the process of computing a
marginal from a joint distribution is called marginalisation. more generally, one has

p(x1, . . . , xi   1, xi+1, . . . , xn) =

p(x1, . . . , xn)

(1.1.6)

xi

de   nition 1.4 (id155 / bayes    rule). the id203 of event x conditioned on knowing
event y (or more shortly, the id203 of x given y) is de   ned as

p(x|y)    

p(x, y)

p(y)

(1.1.7)

if p(y) = 0 then p(x|y) is not de   ned. from this de   nition and p(x, y) = p(y, x) we immediately arrive at
bayes    rule

p(x|y) =

p(y|x)p(x)

p(y)

(1.1.8)

since bayes    rule trivially follows from the de   nition of id155, we will sometimes be loose
in our language and use the terms bayes    rule and id155 as synonymous.

as we shall see throughout this book, bayes    rule plays a central role in probabilistic reasoning since it helps

8

draft november 9, 2017

(1.1.9)

(1.1.10)

id203 refresher

us    invert    probabilistic relationships, translating between p(y|x) and p(x|y).

de   nition 1.5 (id203 density functions). for a continuous variable x, the id203 density f (x)
is de   ned such that

f (x)     0,

f (x)dx = 1

      

and the id203 that x falls in an interval [a, b] is given by

(cid:90)    

(cid:90) b

p(a     x     b) =

as shorthand we will sometimes write(cid:82)

f (x)dx

a

x f (x), particularly when we want an expression to be valid for either
continuous or discrete variables. the multivariate case is analogous with integration over all real space, and
the id203 that x belongs to a region of the space de   ned accordingly. unlike probabilities, id203
densities can take positive values greater than 1.

strange, the nervous reader may simply replace our p(x) notation for(cid:82)

formally speaking, for a continuous variable, one should not speak of the id203 that x = 0.2 since the
id203 of a single value is always zero. however, we shall often write p(x) for continuous variables, thus
not distinguishing between probabilities and id203 density function values. whilst this may appear
x       f (x)dx, where     is a small region
centred on x. this is well de   ned in a probabilistic sense and, in the limit     being very small, this would
give approximately    f (x). if we consistently use the same     for all occurrences of pdfs, then we will simply
have a common prefactor     in all expressions. our strategy is to simply ignore these values (since in the end
only relative probabilities will be relevant) and write p(x). in this way, all the standard rules of id203
carry over, including bayes    rule.

remark 1.1 (subjective id203). id203 is a contentious topic and we do not wish to get bogged
down by the debate here, apart from pointing out that it is not necessarily the rules of id203 that
are contentious, rather what interpretation we should place on them. in some cases potential repetitions
of an experiment can be envisaged so that the    long run    (or frequentist) de   nition of id203 in which
probabilities are de   ned with respect to a potentially in   nite repetition of experiments makes sense. for
example, in coin tossing, the id203 of heads might be interpreted as    if i were to repeat the experiment
of    ipping a coin (at    random   ), the limit of the number of heads that occurred over the number of tosses
is de   ned as the id203 of a head occurring.   

here   s a problem that is typical of the kind of scenario one might face in a machine learning situation. a
   lm enthusiast joins a new online    lm service. based on expressing a few    lms a user likes and dislikes,
the online company tries to estimate the id203 that the user will like each of the 10000    lms in their
database. if we were to de   ne id203 as a limiting case of in   nite repetitions of the same experiment,
this wouldn   t make much sense in this case since we can   t repeat the experiment. however, if we assume
that the user behaves in a manner consistent with other users, we should be able to exploit the large amount
of data from other users    ratings to make a reasonable    guess    as to what this consumer likes. this degree
of belief or bayesian subjective interpretation of id203 sidesteps non-repeatability issues     it   s just a
framework for manipulating real values consistent with our intuition about id203[159].

1.1.1 interpreting id155

id155 matches our intuitive understanding of uncertainty. for example, imagine a circular
dart board, split into 20 equal sections, labelled from 1 to 20. randy, a dart thrower, hits any one of the 20
sections uniformly at random. hence the id203 that a dart thrown by randy occurs in any one of the
20 regions is p(region i) = 1/20. a friend of randy tells him that he hasn   t hit the 20 region. what is the
id203 that randy has hit the 5 region? conditioned on this information, only regions 1 to 19 remain
possible and, since there is no preference for randy to hit any of these regions, the id203 is 1/19. the

draft november 9, 2017

9

conditioning means that certain states are now inaccessible, and the original id203 is subsequently
distributed over the remaining accessible states. from the rules of id203 :

p(region 5|not region 20) =

p(region 5, not region 20)

p(not region 20)

=

p(region 5)

p(not region 20)

=

1/20
19/20

=

1
19

id203 refresher

giving the intuitive result. an important point to clarify is that p(a = a|b = b) should not be interpreted
as    given the event b = b has occurred, p(a = a|b = b) is the id203 of the event a = a occurring   .
in most contexts, no such explicit temporal causality is implied1 and the correct interpretation should be    
p(a = a|b = b) is the id203 of a being in state a under the constraint that b is in state b   .
the relation between the conditional p(a = a|b = b) and the joint p(a = a, b = b) is just a normalisation
a p(a = a, b = b) (cid:54)= 1. to
a p(a = a, b = b) which, when summed over

constant since p(a = a, b = b) is not a distribution in a     in other words, (cid:80)
make it a distribution we need to divide : p(a = a, b = b)/(cid:80)

a does sum to 1. indeed, this is just the de   nition of p(a = a|b = b).

de   nition 1.6 (independence).

variables x and y are independent if knowing the state (or value in the continuous case) of one variable
gives no extra information about the other variable. mathematically, this is expressed by

p(x, y) = p(x)p(y)

provided that p(x) (cid:54)= 0 and p(y) (cid:54)= 0 independence of x and y is equivalent to

p(x|y) = p(x)     p(y|x) = p(y)

if p(x|y) = p(x) for all states of x and y, then the variables x and y are said to be independent. if

p(x, y) = kf (x)g(y)

(1.1.11)

(1.1.12)

(1.1.13)

for some constant k, and positive functions f (  ) and g(  ) then x and y are independent and we write x       y.

example 1.1 (independence). let x denote the day of the week in which females are born, and y denote
the day in which males are born, with dom(x) = dom(y) = {1, . . . , 7}. it is reasonable to expect that x
is independent of y. we randomly select a woman from the phone book, alice, and    nd out that she was
born on a tuesday. we also randomly select a male at random, bob. before phoning bob and asking him,
what does knowing alice   s birth day add to which day we think bob is born on? under the independence
assumption, the answer is nothing. note that this doesn   t mean that the distribution of bob   s birthday is
necessarily uniform     it just means that knowing when alice was born doesn   t provide any extra information
than we already knew about bob   s birthday, p(y|x) = p(y). indeed, the distribution of birthdays p(y) and
p(x) are non-uniform (statistically fewer babies are born on weekends), though there is nothing to suggest
that x are y are dependent.

deterministic dependencies

sometimes the concept of independence is perhaps a little strange. consider the following : variables x and
y are both binary (their domains consist of two states). we de   ne the distribution such that x and y are
always both in a certain joint state:

p(x = a, y = 1) = 1, p(x = a, y = 2) = 0, p(x = b, y = 2) = 0, p(x = b, y = 1) = 0

are x and y dependent? the reader may show that p(x = a) = 1, p(x = b) = 0 and p(y = 1) = 1,
p(y = 2) = 0. hence p(x)p(y) = p(x, y) for all states of x and y, and x and y are therefore independent.

1we will discuss issues related to causality further in section(3.4).

10

draft november 9, 2017

id203 refresher

this may seem strange     we know for sure the relation between x and y, namely that they are always in the
same joint state, yet they are independent. since the distribution is trivially concentrated in a single joint
state, knowing the state of x tells you nothing that you didn   t anyway know about the state of y, and vice
versa. this potential confusion comes from using the term    independent    which may suggest that there is no
relation between objects discussed. the best way to think about statistical independence is to ask whether
or not knowing the state of variable y tells you something more than you knew before about variable x,
where    knew before    means working with the joint distribution of p(x, y) to    gure out what we can know
about x, namely p(x).

de   nition 1.7 (conditional independence).

x       y|z

(1.1.14)

denotes that the two sets of variables x and y are independent of each other provided we know the state
of the set of variables z. for conditional independence, x and y must be independent given all states of
z. formally, this means that

p(x ,y|z) = p(x|z)p(y|z)

(1.1.15)

for all states of x ,y,z. in case the conditioning set is empty we may also write x        y for x        y|    , in
which case x is (unconditionally) independent of y.
if x and y are not conditionally independent, they are conditionally dependent. this is written

x(cid:62)(cid:62)y|z

similarly x(cid:62)(cid:62)y|    can be written as x(cid:62)(cid:62)y.

(1.1.16)

(cid:48)

intuitively, if x is conditionally independent of y given z, this means that, given z, y contains no additional
information about x. similarly, given z, knowing x does not tell me anything more about y. note that
x       y|z     x
remark 1.2 (independence implications). it   s tempting to think that if a is independent of b and b is
independent of c then a must be independent of c:

(cid:48)
|z for x

    x and y

    y.

      y

(cid:48)

(cid:48)

{a       b, b       c}     a       c

however, this does not follow. consider for example a distribution of the form

p(a, b, c) = p(b)p(a, c)

from this

p(a, b) =

(cid:88)

c

p(a, b, c) = p(b)

(cid:88)

c

p(a, c)

(1.1.17)

(1.1.18)

(1.1.19)

hence p(a, b) is a function of b multiplied by a function of a so that a and b are independent. similarly, one
can show that b and c are independent. however, a is not necessarily independent of c since the distribution
p(a, c) can be set arbitrarily.

similarly, it   s tempting to think that if a and b are dependent, and b and c are dependent, then a and c
must be dependent:

{a(cid:62)(cid:62)b, b(cid:62)(cid:62)c}     a(cid:62)(cid:62)c

(1.1.20)

however, this also does not follow. we give an explicit numerical example in exercise(3.17).

finally, note that conditional independence x       y| z does not imply marginal independence x       y. see also
exercise(3.20).

draft november 9, 2017

11

1.1.2 id203 tables

probabilistic reasoning

based on the populations 60776238, 5116900 and 2980700 of england (e), scotland (s) and wales (w),
the a priori id203 that a randomly selected person from the combined three countries would live in
england, scotland or wales, is approximately 0.88, 0.08 and 0.04 respectively. we can write this as a vector
(or id203 table) :

       p(cnt = e)

p(cnt = s)
p(cnt = w)

       =

       0.88

0.08
0.04

      

(1.1.21)

whose component values sum to 1. the ordering of the components in this vector is arbitrary, as long as it
is consistently applied.

for the sake of simplicity, we assume that only three mother tongue languages exist : english (eng),
scottish (scot) and welsh (wel), with conditional probabilities given the country of residence, england (e),
scotland (s) and wales (w). we write a (   ctitious) id155 table

p(m t = eng|cnt = w) = 0.6
p(m t = eng|cnt = e) = 0.95
p(m t = scot|cnt = e) = 0.04 p(m t = scot|cnt = s) = 0.3 p(m t = scot|cnt = w) = 0.0
p(m t = wel|cnt = e) = 0.01
p(m t = wel|cnt = w) = 0.4

p(m t = eng|cnt = s) = 0.7
p(m t = wel|cnt = s) = 0.0

(1.1.22)

from this we can form a joint distribution p(cnt, m t ) = p(m t|cnt)p(cnt). this could be written as a
3    3 matrix with columns indexed by country and rows indexed by mother tongue:

       0.95    0.88 0.7    0.08 0.6    0.04

0.04    0.88 0.3    0.08 0.0    0.04
0.01    0.88 0.0    0.08 0.4    0.04

       =

       0.836

0.056 0.024

0.0352 0.024
0.0088

0

0

0.016

      

(1.1.23)

the joint distribution contains all the information about the model of this environment. by summing the
columns of this table, we have the marginal p(cnt). summing the rows gives the marginal p(m t ). similarly,
one could easily infer p(cnt|m t )     p(m t|cnt)p(cnt) from this joint distribution by dividing an entry of
equation (1.1.23) by its row sum.

states, the table describing the joint distribution is an array with(cid:81)d

for joint distributions over a larger number of variables, xi, i = 1, . . . , d, with each variable xi taking ki
i=1 ki entries. explicitly storing tables
therefore requires space exponential in the number of variables, which rapidly becomes impractical for a
large number of variables. we discuss how to deal with this issue in chapter(3) and chapter(4).

a id203 distribution assigns a value to each of the joint states of the variables. for this reason,
p(t, j, r, s) is considered equivalent to p(j, s, r, t ) (or any such reordering of the variables), since in each
case the joint setting of the variables is simply a di   erent index to the same id203. this situation is
more clear in the set theoretic notation p(j     s     t     r). we abbreviate this set theoretic notation by using
the commas     however, one should be careful not to confuse the use of this indexing type notation with
functions f (x, y) which are in general dependent on the variable order. whilst the variables to the left of the
conditioning bar may be written in any order, and equally those to the right of the conditioning bar may be
written in any order, moving variables across the bar is not generally equivalent, so that p(x1|x2) (cid:54)= p(x2|x1).

1.2 probabilistic reasoning

the central paradigm of probabilistic reasoning is to identify all relevant variables x1, . . . , xn in the envi-
ronment, and make a probabilistic model p(x1, . . . , xn ) of their interaction. reasoning (id136) is then
performed by introducing evidence that sets variables in known states, and subsequently computing proba-
bilities of interest, conditioned on this evidence. the rules of id203, combined with bayes    rule make
for a complete reasoning system, one which includes traditional deductive logic as a special case[159]. in
the examples below, the number of variables in the environment is very small. in chapter(3) we will discuss

12

draft november 9, 2017

probabilistic reasoning

reasoning in networks containing many variables, for which the graphical notations of chapter(2) will play
a central role.

example 1.2 (hamburgers). consider the following    ctitious scienti   c information: doctors    nd that
people with kreuzfeld-jacob disease (kj) almost invariably ate hamburgers, thus p(hamburger eater|kj ) =
0.9. the id203 of an individual having kj is currently rather low, about one in 100,000.

1. assuming eating lots of hamburgers is rather widespread, say p(hamburger eater) = 0.5, what is the

id203 that a hamburger eater will have kreuzfeld-jacob disease?

this may be computed as

p(kj |hamburger eater) =

p(hamburger eater, kj )

p(hamburger eater)

=

p(hamburger eater|kj )p(kj )

p(hamburger eater)

=

9

10    1

100000
1
2

   5

= 1.8    10

(1.2.1)

(1.2.2)

2. if the fraction of people eating hamburgers was rather small, p(hamburger eater) = 0.001, what is the
id203 that a regular hamburger eater will have kreuzfeld-jacob disease? repeating the above
calculation, this is given by

9

10    1

100000
1

1000

    1/100

(1.2.3)

this is much higher than in scenario (1) since here we can be more sure that eating hamburgers is
related to the illness.

example 1.3 (inspector clouseau). inspector clouseau arrives at the scene of a crime. the victim lies dead
in the room alongside the possible murder weapon, a knife. the butler (b) and maid (m ) are the inspector   s
main suspects and the inspector has a prior belief of 0.6 that the butler is the murderer, and a prior belief
of 0.2 that the maid is the murderer. these beliefs are independent in the sense that p(b, m ) = p(b)p(m ).
(it is possible that both the butler and the maid murdered the victim or neither). the inspector   s prior
criminal knowledge can be formulated mathematically as follows:

dom(b) = dom(m ) = {murderer, not murderer} , dom(k) = {knife used, knife not used}

(1.2.4)

p(b = murderer) = 0.6,

p(m = murderer) = 0.2

p(knife used|b = not murderer, m = not murderer) = 0.3
p(knife used|b = not murderer, m = murderer)
= 0.2
m = not murderer) = 0.6
p(knife used|b = murderer,
m = murderer)
p(knife used|b = murderer,
= 0.1

(1.2.5)

(1.2.6)

in addition p(k, b, m ) = p(k|b, m )p(b)p(m ). assuming that the knife is the murder weapon, what is
the id203 that the butler is the murderer? (remember that it might be that neither is the murderer).
using b for the two states of b and m for the two states of m ,

(cid:88)

m

(cid:88)

(cid:80)
(cid:80)

p(b|k) =

p(b, m|k) =

p(b, m, k)

=

p(k)

m

m p(k|b, m)p(b, m)
m,b p(k|b, m)p(b, m)

=

m p(k|b, m)p(m)
m p(k|b, m)p(m)

p(b)(cid:80)
(cid:80)
b p(b)(cid:80)

(1.2.7)

13

draft november 9, 2017

probabilistic reasoning

where we used the fact that in our model p(b, m ) = p(b)p(m ). plugging in the values we have (see also
democlouseau.m)

(cid:0) 2
10    1
10    6

10

(cid:1) + 4

10 + 8

10

(cid:1)
(cid:0) 2
10    6
10    2

10

6
10

10 + 8

(cid:0) 2
10    1

(cid:1) =

10 + 8

10    3

10

300
412     0.73

(1.2.8)

p(b = murderer|knife used) =

6
10

hence knowing that the knife was the murder weapon strengthens our belief that the butler did it.

remark 1.3. the role of p(knife used) in the inspector clouseau example can cause some confusion. in
the above,

p(knife used) =

p(b)

p(knife used|b, m)p(m)

(1.2.9)

(cid:88)

(cid:88)

b

m

0.412. but surely, p(knife used) = 1, since this is given in the question! note that the
is computed to be
quantity p(knife used) relates to the prior id203 the model assigns to the knife being used (in the
absence of any other information). if we know that the knife is used, then the posterior

@@

p(knife used|knife used) =

p(knife used, knife used)

p(knife used)

=

p(knife used)
p(knife used)

= 1

(1.2.10)

which, naturally, must be the case.

example 1.4 (who   s in the bathroom?). consider a household of three people, alice, bob and cecil.
cecil wants to go to the bathroom but    nds it occupied. he then goes to alice   s room and sees she is there.
since cecil knows that only either alice or bob can be in the bathroom, from this he infers that bob must
be in the bathroom.

to arrive at the same conclusion in a mathematical framework, we de   ne the following events

a = alice is in her bedroom, b = bob is in his bedroom, o = bathroom occupied

(1.2.11)

we can encode the information that if either alice or bob are not in their bedrooms, then they must be in
the bathroom (they might both be in the bathroom) as

p(o = tr|a = fa, b) = 1,

p(o = tr|a, b = fa) = 1

(1.2.12)

the    rst term expresses that the bathroom is occupied if alice is not in her bedroom, wherever bob is.
similarly, the second term expresses bathroom occupancy as long as bob is not in his bedroom. then

p(b = fa|o = tr, a = tr) =
where

p(b = fa, o = tr, a = tr)

p(o = tr, a = tr)

=

p(o = tr|a = tr, b = fa)p(a = tr, b = fa)

p(o = tr, a = tr)

(1.2.13)

p(o = tr, a = tr) = p(o = tr|a = tr, b = fa)p(a = tr, b = fa)

(1.2.14)
using the fact p(o = tr|a = tr, b = fa) = 1 and p(o = tr|a = tr, b = tr) = 0, which encodes that if alice
is in her room and bob is not, the bathroom must be occupied, and similarly, if both alice and bob are in
their rooms, the bathroom cannot be occupied,

+ p(o = tr|a = tr, b = tr)p(a = tr, b = tr)

p(b = fa|o = tr, a = tr) =

p(a = tr, b = fa)
p(a = tr, b = fa)

= 1

(1.2.15)

this example is interesting since we are not required to make a full probabilistic model in this case thanks
to the limiting nature of the probabilities (we don   t need to specify p(a, b)). the situation is common in
limiting situations of probabilities being either 0 or 1, corresponding to traditional logic systems.

14

draft november 9, 2017

probabilistic reasoning

@@

example 1.5 (aristotle : modus ponens). according to logic, the statements    all apples are fruit    and
   all fruits grow on trees    lead to the conclusion that    all apples grow on trees   . this kind of reasoning is a
form of transitivity : from the statements a     f and f     t we can infer a     t .
to see how this might be deduced using bayesian, we assume that    all apples are fruit    corresponds to
p(f = tr|a = tr) = 1 and    all fruit grows on trees    corresponds to p(t = tr|f = tr) = 1. we then want to
show that this implies p(t = tr|a = tr) = 1. showing this is equivalent to showing p(t = fa|a = tr) = 0
which (assuming p(a = tr) > 0) is in turn equivalent to showing that p(t = fa, a = tr) = 0. consider

p(t = fa, a = tr) = p(t = fa, a = tr, f = tr) + p(t = fa, a = tr, f = fa)

(1.2.16)

we can show that both terms on the right are zero. first, consider

p(t = fa, a = tr, f = tr)     p(t = fa, f = tr) = p(t = fa|f = tr)p(f = tr)

this is zero since, by assumption, p(t = fa|f = tr) = 1     p(t = tr|f = tr) = 1     1 = 0. similarly,

p(t = fa, a = tr, f = fa)     p(a = tr, f = fa) = p(f = fa|a = tr)p(a = tr)

(1.2.17)

(1.2.18)

where again, by assumption, p(f = fa|a = tr) = 0.

example 1.6 (aristotle : inverse modus ponens). according to logic, from the statement :    if a is true
then b is true   , one may deduce that    if b is false then a is false   . to see how this    ts in with a probabilistic
reasoning system we can    rst express the statement :    if a is true then b is true    as p(b = tr|a = tr) = 1.
then we may infer

p(a = fa|b = fa) = 1     p(a = tr|b = fa)

= 1    

p(b = fa|a = tr)p(a = tr)

p(b = fa|a = tr)p(a = tr) + p(b = fa|a = fa)p(a = fa)

= 1

(1.2.19)

this follows since p(b = fa|a = tr) = 1     p(b = tr|a = tr) = 1     1 = 0, annihilating the second term.

both the above examples are intuitive expressions of deductive logic. the standard rules of aristotelian
logic are therefore seen to be limiting cases of probabilistic reasoning.

example 1.7 (soft xor gate).
a standard xor logic gate is given by the table on the right. if we
observe that the output of the xor gate is 0, what can we say about
a and b? in this case, either a and b were both 0, or a and b were
both 1. this means we don   t know which state a was in     it could
equally likely have been 1 or 0.

a b a xor b
0
0
1
1

0
1
0
1

0
1
1
0

draft november 9, 2017

15

probabilistic reasoning

a b p(c = 1|a, b)
0
0
1
1

0.1
0.99
0.8
0.25

0
1
0
1

++

consider a    soft    version of the xor gate given on the right,
so that the gate stochastically outputs c = 1 depending on its
inputs, with additionally a        b and p(a = 1) = 0.65, p(b =
1) = 0.77. what is p(a = 1|c = 0)?

(cid:88)

(cid:88)

(cid:88)

(cid:88)

p(a = 1, c = 0) =

p(a = 1, b, c = 0) =

b

b

p(c = 0|a = 1, b)p(a = 1)p(b)

= p(a = 1) (p(c = 0|a = 1, b = 0)p(b = 0) + p(c = 0|a = 1, b = 1)p(b = 1))
= 0.65    (0.2    0.23 + 0.75    0.77) = 0.405275

(1.2.20)

p(a = 0, c = 0) =

p(a = 0, b, c = 0) =

b

b

p(c = 0|a = 0, b)p(a = 0)p(b)

= p(a = 0) (p(c = 0|a = 0, b = 0)p(b = 0) + p(c = 0|a = 0, b = 1)p(b = 1))
= 0.35    (0.9    0.23 + 0.01    0.77) = 0.075145

then

p(a = 1|c = 0) =

p(a = 1, c = 0)

p(a = 1, c = 0) + p(a = 0, c = 0)

=

0.405275

0.405275 + 0.075145

= 0.8436

(1.2.21)

example 1.8 (larry). larry is typically late for school. if larry is late, we denote this with l = late,
otherwise, l = not late. when his mother asks whether or not he was late for school he never admits to
being late. the response larry gives rl is represented as follows

p(rl = not late|l = not late) = 1,

p(rl = late|l = late) = 0

the remaining two values are determined by normalisation and are

p(rl = late|l = not late) = 0,

p(rl = not late|l = late) = 1

given that rl = not late, what is the id203 that larry was late, i.e. p(l = late|rl = not late)?
using bayes    we have

p(l = late|rl = not late) =

p(l = late, rl = not late)

p(rl = not late)

=

p(l = late, rl = not late)

p(l = late, rl = not late) + p(l = not late, rl = not late)

in the above

and

p(l = late, rl = not late) = p(rl = not late|l = late)

p(l = late)

(cid:124)

(cid:124)

(cid:123)(cid:122)

=1

(cid:125)

(cid:123)(cid:122)

=1

(cid:125)

p(l = not late, rl = not late) = p(rl = not late|l = not late)

p(l = not late)

(1.2.26)

hence

p(l = late|rl = not late) =

p(l = late)

p(l = late) + p(l = not late)

= p(l = late)

(1.2.27)

where we used normalisation in the last step, p(l = late) + p(l = not late) = 1. this result is intuitive    
larry   s mother knows that he never admits to being late, so her belief about whether or not he really was
late is unchanged, regardless of what larry actually says.

16

draft november 9, 2017

(1.2.22)

(1.2.23)

(1.2.24)

(1.2.25)

probabilistic reasoning

example 1.9 (larry and sue). continuing the example above, larry   s sister sue always tells the truth to
her mother as to whether or not larry was late for school.

p(rs = not late|l = not late) = 1,

p(rs = late|l = late) = 1
the remaining two values are determined by normalisation and are

p(rs = late|l = not late) = 0,

p(rs = not late|l = late) = 0
we also assume p(rs, rl|l) = p(rs|l)p(rl|l). we can then write

p(rl, rs, l) = p(rl|l)p(rs|l)p(l)

given that rs = late and rl = not late, what is the id203 that larry was late?

using bayes    rule, we have

(1.2.28)

(1.2.29)

(1.2.30)

p(l = late|rl = not late,rs = late)

=

1
z

p(rs = late|l = late)p(rl = not late|l = late)p(l = late)

(1.2.31)

where the normalisation z is given by

p(rs = late|l = late)p(rl = not late|l = late)p(l = late)

+ p(rs = late|l = not late)p(rl = not late|l = not late)p(l = not late)

(1.2.32)

hence

p(l = late|rl = not late, rs = late) =

1    1    p(l = late)

1    1    p(l = late) + 0    1    p(l = not late)

= 1

(1.2.33)

this result is also intuitive     since larry   s mother knows that sue always tells the truth, no matter what
larry says, she knows he was late.

example 1.10 (luke). luke has been told he   s lucky and has won a prize in the lottery. there are 5
prizes available of value   10,   100,   1000,   10000,   1000000. the prior probabilities of winning these 5
prizes are p1, p2, p3, p4, p5, with p0 being the prior id203 of winning no prize. luke asks eagerly    did i
win   1000000?!   .
   did i win   10000?!   
asks luke.    again, i   m afraid not sir   . what is the id203 that luke has won   1000?

   i   m afraid not sir   , is the response of the lottery phone operator.

note    rst that p0 + p1 + p2 + p3 + p4 + p5 = 1. we denote w = 1 for the    rst prize of   10, and w = 2, . . . , 5
for the remaining prizes and w = 0 for no prize. we need to compute

p(w = 3|w (cid:54)= 5, w (cid:54)= 4, w (cid:54)= 0) =

=

p(w = 3, w (cid:54)= 5, w (cid:54)= 4, w (cid:54)= 0)

p(w (cid:54)= 5, w (cid:54)= 4, w (cid:54)= 0)

p(w = 3)

p(w = 1 or w = 2 or w = 3)

=

p3

p1 + p2 + p3

(1.2.34)

where the term in the denominator is computed using the fact that the events w are mutually exclusive
(one can only win one prize). this result makes intuitive sense : once we have removed the impossible states
of w , the id203 that luke wins the prize is proportional to the prior id203 of that prize, with
the normalisation being simply the total set of possible id203 remaining.

draft november 9, 2017

17

prior, likelihood and posterior

(a)

(b)

(c)

figure 1.1:
on 5 possible values of   . (c): the posterior belief on   .

(a): noisy observations of displacements x1, . . . , x100 for a pendulum.

(b): the prior belief

1.3 prior, likelihood and posterior

(cid:82)

much of science deals with problems of the form : tell me something about the variable    given that i have
observed data d and have some knowledge of the underlying data generating mechanism. our interest is
then the quantity

p(  |d) =

p(d|  )p(  )

p(d)

=

p(d|  )p(  )
   p(d|  )p(  )

(1.3.1)

this shows how from a forward or generative model p(d|  ) of the dataset, and coupled with a prior belief
p(  ) about which variable values are appropriate, we can infer the posterior distribution p(  |d) of the vari-
able in light of the observed data. the most probable a posteriori (map ) setting is that which maximises
the posterior,       = arg max   p(  |d). for a       at prior   , p(  ) being a constant, not changing with   , the map
solution is equivalent to the maximum likelihood , namely that    that maximises the likelihood p(d|  ) of the
model generating the observed data. we will return to a discussion of such summaries of the posterior and
parameter learning in chapter(9).

this use of a generative model sits well with physical models of the world which typically postulate how
to generate observed phenomena, assuming we know the model. for example, one might postulate how
to generate a time-series of displacements for a swinging pendulum but with unknown mass, length and
damping constant. using this generative model, and given only the displacements, we could infer the
unknown physical properties of the pendulum.

t(cid:89)

t=1

example 1.11 (pendulum). as a prelude to scienti   c id136 and the use of continuous variables, we
consider an idealised pendulum for which xt is the angular displacement of the pendulum at time t. assuming
that the measurements are independent, given the knowledge of the parameter of the problem,   , we have
that the likelihood of a sequence of observations x1, . . . , xt is given by

p(x1, . . . , xt|  ) =

p(xt|  )

(1.3.2)

if the model is correct and our measurement of the displacements x is perfect, then the physical model is

xt = sin(  t)

(1.3.3)

where    represents the unknown physical constants of the pendulum (
g/l, where g is the gravitational
attraction and l the length of the pendulum). if, however, we assume that we have a rather poor instrument
to measure the displacements, with a known variance of   2 (see chapter(8)), then

xt = sin(  t) +  t

18

(1.3.4)

draft november 9, 2017

(cid:112)

020406080100   4   2024   0.100.10.20.30.40.50.600.050.10.150.20.250.30.35   0.100.10.20.30.40.50.600.20.40.60.81prior, likelihood and posterior

t(cid:89)

t=1

where  t is zero mean gaussian noise with variance   2. we can also consider a set of possible parameters
   and place a prior p(  ) over them, expressing our prior belief (before seeing the measurements) in the
appropriateness of the di   erent values of   . the posterior distribution is then given by

p(  |x1, . . . , xt )     p(  )

    1
2  2 (xt   sin(  t))2
e

1

   2    2

(1.3.5)

despite noisy measurements, the posterior over the assumed possible values for    becomes strongly peaked
for a large number of measurements, see    g(1.1).

1.3.1 two dice : what were the individual scores?

two fair dice are rolled. someone tells you that the sum of the two scores is 9. what is the posterior
distribution of the dice scores2?

the score of die a is denoted sa with dom(sa) = {1, 2, 3, 4, 5, 6} and similarly for sb. the three variables
involved are then sa, sb and the total score, t = sa + sb. a model of these three variables naturally takes
the form

p(t, sa, sb) = p(t|sa, sb)

(cid:124)

(cid:123)(cid:122)

likelihood

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

p(sa, sb)

prior

the prior p(sa, sb) is the joint id203 of score sa
and score sb without knowing anything else. assuming
no dependency in the rolling mechanism,

p(sa, sb) = p(sa)p(sb)

(1.3.7)

since the dice are fair both p(sa) and p(sb) are uniform
distributions, p(sa) = p(sb) = 1/6.

here the likelihood term is

p(t|sa, sb) = i [t = sa + sb]

(1.3.8)

which states that the total score is given by sa + sb.
here i [a] is the indicator function de   ned as i [a] = 1
if the statement a is true and 0 otherwise.

hence, our complete model is

p(t, sa, sb) = p(t|sa, sb)p(sa)p(sb)

(1.3.9)

where the terms on the right are explicitly de   ned.

the posterior is then given by,

p(sa, sb|t = 9) =

where

p(t = 9) =

p(t = 9|sa, sb)p(sa)p(sb)

p(t = 9)

(1.3.10)

p(t = 9|sa, sb)p(sa)p(sb) (1.3.11)

(cid:88)

sa,sb

2this example is due to taylan cemgil.

draft november 9, 2017

(1.3.6)

sa = 5
1/36
1/36
1/36
1/36
1/36
1/36

sa = 6
1/36
1/36
1/36
1/36
1/36
1/36

sa = 5

sa = 6

p(sa)p(sb):

sa = 2
1/36
1/36
1/36
1/36
1/36
1/36

sa = 3
1/36
1/36
1/36
1/36
1/36
1/36

sa = 4
1/36
1/36
1/36
1/36
1/36
1/36

p(t = 9|sa, sb):

sa = 3

sa = 2

sa = 4

0
0
0
0
0
0

0
0
0
0
0
1

0
0
0
0
1
0

0
0
0
1
0
0

0
0
1
0
0
0

sa = 1
1/36
1/36
1/36
1/36
1/36
1/36

sa = 1

0
0
0
0
0
0

p(t = 9|sa, sb)p(sa)p(sb):

sa = 2

sa = 3

sa = 4

sa = 1

sa = 5

0
0
0
0
0
0

0
0
0
0
0
0

0
0
0
0
0

1/36

0
0
0
0

1/36

0

0
0
0

1/36

0
0

sa = 6

0
0

1/36

0
0
0

p(sa, sb|t = 9):

sa = 3

sa = 2

sa = 4

0
0
0
0
0
0

0
0
0
0
0
1/4

0
0
0
0
1/4
0

sa = 1

0
0
0
0
0
0

sa = 5

sa = 6

0
0
0
1/4
0
0

0
0
1/4
0
0
0

19

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

sb = 1
sb = 2
sb = 3
sb = 4
sb = 5
sb = 6

prior, likelihood and posterior

(a)

(b)

figure 1.2: the posterior distribution for the location of the explosion (darker corresponds to a higher
id203). this is based on a spiral coordinate system (see earthquake.jl) with    = 0.2. the red points
are the locations of the surface sensors. the observed (noisy) measurements at each sensor are represented
by the magenta lines, and the true (unknown) blast values are denoted by the red lines. (a) using 10 sensors.
(b) using 5 sensors. note how the uncertainty in the posterior is larger in the case of having fewer sensors.

mass in only 4 non-zero elements, as shown.

sa,sb

p(t = 9|sa, sb)p(sa)p(sb) = 4   1/36 = 1/9. hence the posterior is given by equal

the term p(t = 9) =(cid:80)

example 1.12 (explosions). we consider a modi   ed and simpli   ed form of stuart russell   s    earth-
quake/nuclear blast    detection problem [9]. we assume that there is an    explosion event    somewhere within
the earth and we wish to estimate where the explosion happened based on surface measurements of the
explosion. for simplicity we assume a two dimensional earth.

there are n sensors evenly spread out on the surface of the earth with locations (xi, yi), i = 1, . . . , n ,
indicated by the dots below:

y

x

the explosion happens at some (unknown) location (ex, ey) within the earth, from which a wave of energy
propagates outwards and reaches the sensors on the surface. the blast signal that each sensor receives is

20

draft november 9, 2017

prior, likelihood and posterior

given by

1

d2
i + 0.1

where d2

i is the squared distance from the explosion to sensor i and
d2
i = (xi     ex)2 + (yi     ey)2

this means that the signal strength of the explosion decreases as the distance from the explosion to a
sensor increases.

the sensors are not able to perfectly detect the strength of the signal and the signal is measured with
gaussian noise with a standard deviation of   . this means that the observed value vi at sensor i follows a
gaussian distribution:

(cid:18)

(cid:19)2

p(vi|di) =

    1
2  2
e

1

   2    2

vi    1
d2
i

+0.1

assuming that the observed sensor values are independent (given the explosion location), our simple gener-
ative model is

p(v1, . . . , vn , ex, ey) = p(ex, ey)

p(vi|di)

n(cid:89)

i=1

which has the belief network description below (see chapter(3))

ex, ey

v1

v2

v3

. . .

vn

given an observed set of values v1, . . . , vn , our interest is then the posterior distribution

p(ex, ey|v1, . . . , vn )

for a uniform prior p(ex, ey) = const., then

p(ex, ey|v1, . . . , vn )    

p(vi|di)

n(cid:89)

i=1

in    g(1.2) we plot a representation of the posterior and also plot the most likely point

one can also extend the method to deal with multiple explosions, see    g(1.3) for a two-sources example.

arg max p(ex, ey|v1, . . . , vn )

1.4 summary

draft november 9, 2017

21

code

figure 1.3: the posterior distribution for the location of two explosions (darker corresponds to a higher
id203). this is based on a spiral coordinate system (see earthquake.jl) with    = 0.2. the red points
are the locations of the surface sensors. the observed (noisy) measurements at each sensor are represented
by the magenta lines, and the two true (unknown) blast values are denoted by the red lines.

    the standard rules of id203 are a consistent, logical way to reason with uncertainty.
    bayes    rule mathematically encodes the process of id136.

a useful introduction to id203 is given in [293]. the interpretation of id203 is contentious and
we refer the reader to [159, 198, 194] for detailed discussions. the website understandinguncertainty.org
contains entertaining discussions on reasoning with uncertainty.

1.5 code

the brmltoolbox code accompanying this book is intended to give the reader some insight into repre-
senting discrete id203 tables and performing simple id136. we provide here only the briefest of
descriptions of the code and the reader is encouraged to experiment with the demos to understand better
the routines and their purposes.

1.5.1 basic id203 code

at the simplest level, we only need two basic routines. one for multiplying id203 tables together
(called potentials in the code), and one for summing a id203 table. potentials are represented using a
structure. for example, in the code corresponding to the inspector clouseau example democlouseau.m, we
de   ne a id203 table as

>> pot(1)
ans =

variables: [1 3 2]

table: [2x2x2 double]

this says that the potential depends on the variables 1, 3, 2 and the entries are stored in the array given
by the table    eld. the size of the array informs how many states each variable takes in the order given by
variables. the order in which the variables are de   ned in a potential is irrelevant provided that one indexes
the array consistently. a routine that can help with setting table entries is setstate.m. for example,

>> pot(1) = setstate(pot(1),[2 1 3],[2 1 1],0.3)

22

draft november 9, 2017

code

means that for potential 1, the table entry for variable 2 being in state 2, variable 1 being in state 1 and
variable 3 being in state 1 should be set to value 0.3.

the philosophy of the code is to keep the information required to perform computations to a minimum.
additional information about the labels of variables and their domains can be useful to interpret results,
but is not actually required to carry out computations. one may also specify the name and domain of each
variable, for example

>>variable(3)
ans =

domain: {   murderer       not murderer   }

name:    butler   

the variable name and domain information in the clouseau example is stored in the structure variable,
which can be helpful to display the potential table:

>> disptable(pot(1),variable);
knife
knife
knife
knife
knife
knife
knife
knife

used
not used
used
not used
used
not used
used
not used

maid
maid
maid
maid
maid
maid
maid
maid

=
=
=
=
=
=
=
=

= murderer
= murderer
= not murderer
= not murderer
= murderer
= murderer
= not murderer
= not murderer

butler
butler
butler
butler
butler
butler
butler
butler

=
=
=
=
=
=
=
=

murderer
murderer
murderer
murderer
not murderer
not murderer
not murderer
not murderer

0.100000
0.900000
0.600000
0.400000
0.200000
0.800000
0.300000
0.700000

multiplying potentials

in order to multiply potentials, (as for arrays) the tables of each potential must be dimensionally consistent
    that is the number of states of variable i must be the same for all potentials. this can be checked us-
ing potvariables.m. this consistency is also required for other basic operations such as summing potentials.

multpots.m: multiplying two or more potentials
divpots.m: dividing a potential by another

summing a potential

sumpot.m: sum (marginalise) a potential over a set of variables
sumpots.m: sum a set of potentials together

making a conditional potential

condpot.m: make a potential conditioned on variables

setting a potential

setpot.m: set variables in a potential to given states
setevpot.m: set variables in a potential to given states and return also an identity potential on the given
states

the philosophy of brmltoolbox is that all information about variables is local and is read o    from a
potential. using setevpot.m enables one to set variables in a state whilst maintaining information about
the number of states of a variable.

maximising a potential

maxpot.m: maximise a potential over a set of variables
see also maxnarray.m and maxnpot.m which return the n -highest values and associated states.

draft november 9, 2017

23

exercises

other potential utilities

setstate.m: set a potential state to a given value
table.m: return a table from a potential
whichpot.m: return potentials which contain a set of variables
potvariables.m: variables and their number of states in a set of potentials
orderpotfields.m: order the    elds of a potential structure
uniquepots.m: merge redundant potentials by multiplication and return only unique ones
numstates.m: number of states of a variable in a domain
squeezepots.m: find unique potentials and rename the variables 1,2,...
normpot.m: normalise a potential to form a distribution

1.5.2 general utilities

condp.m: return a table p(x|y) from p(x, y)
condexp.m: form a conditional distribution from a log value
logsumexp.m: compute the log of a sum of exponentials in a numerically precise way
normp.m: return a normalised table from an unnormalised table
assign.m: assign values to multiple variables
maxarray.m: maximize a multi-dimensional array over a subset

1.5.3 an example

the following code highlights the use of the above routines in solving the inspector clouseau, example(1.3),
and the reader is invited to examine the code to become familiar with how to numerically represent proba-
bility tables.
democlouseau.m: solving the inspector clouseau example

1.6 exercises

exercise 1.1. prove

p(x, y|z) = p(x|z)p(y|x, z)

and also

p(x|y, z) =

p(y|x, z)p(x|z)

p(y|z)

exercise 1.2. prove the bonferroni inequality

p(a, b)     p(a) + p(b)     1

(1.6.1)

(1.6.2)

(1.6.3)

exercise 1.3 (adapted from [182]). there are two boxes. box 1 contains three red and    ve white balls and
box 2 contains two red and    ve white balls. a box is chosen at random p(box = 1) = p(box = 2) = 0.5 and
a ball chosen at random from this box turns out to be red. what is the posterior id203 that the red ball
came from box 1?

exercise 1.4 (adapted from [182]). two balls are placed in a box as follows: a fair coin is tossed and a
white ball is placed in the box if a head occurs, otherwise a red ball is placed in the box. the coin is tossed
again and a red ball is placed in the box if a tail occurs, otherwise a white ball is placed in the box. balls
are drawn from the box three times in succession (always with replacing the drawn ball back in the box). it
is found that on all three occasions a red ball is drawn. what is the id203 that both balls in the box are
red?

exercise 1.5 (adapted from david spiegelhalter understandinguncertainty.org). a secret government
agency has developed a scanner which determines whether a person is a terrorist. the scanner is fairly
reliable; 95% of all scanned terrorists are identi   ed as terrorists, and 95% of all upstanding citizens are

24

draft november 9, 2017

exercises

@@

identi   ed as such. an informant tells the agency that exactly one passenger of 100 aboard an aeroplane in
which you are seated is a terrorist.
the police haul o    the plane the    rst person for which the scanner tests
positive. what is the id203 that this person is a terrorist?

exercise 1.6. consider three variable distributions which admit the factorisation

p(a, b, c) = p(a|b)p(b|c)p(c)

(1.6.4)

where all variables are binary. how many parameters are needed to specify distributions of this form?

exercise 1.7. repeat the inspector clouseau scenario, example(1.3), but with the restriction that either the
maid or the butler is the murderer, but not both. explicitly, the id203 of the maid being the murderer
and not the butler is 0.04, the id203 of the butler being the murderer and not the maid is 0.64. modify
democlouseau.m to implement this.

exercise 1.8. prove

p(a, (b or c)) = p(a, b) + p(a, c)     p(a, b, c)

exercise 1.9. prove

(cid:88)

y

(cid:88)

y,w

p(x|z) =

p(x|y, z)p(y|z) =

p(x|w, y, z)p(w|y, z)p(y|z)

(1.6.5)

(1.6.6)

exercise 1.10. as a young man mr gott visits berlin in 1969. he   s surprised that he cannot cross into
east berlin since there is a wall separating the two halves of the city. he   s told that the wall was erected 8
years previously. he reasons that : the wall will have a    nite lifespan; his ignorance means that he arrives
uniformly at random at some time in the lifespan of the wall. since only 5% of the time one would arrive
in the    rst or last 2.5% of the lifespan of the wall he asserts that with 95% con   dence the wall will survive
between 8/0.975     8.2 and 8/0.025 = 320 years. in 1989 the now professor gott is pleased to    nd that
his prediction was correct and promotes his prediction method in prestigious journals. this    delta-t    method
is widely adopted and used to form predictions in a range of scenarios about which researchers are    totally
ignorant   . would you    buy    a prediction from prof. gott? explain carefully your reasoning.

exercise 1.11. implement the soft xor gate, example(1.7) using brmltoolbox. you may    nd condpot.m
of use.

exercise 1.12. implement the hamburgers, example(1.2) (both scenarios) using brmltoolbox. to do so
you will need to de   ne the joint distribution p(hamburgers, kj) in which dom(hamburgers) = dom(kj) =
{tr, fa}.
exercise 1.13. implement the two-dice example, section(1.3.1) using brmltoolbox.

exercise 1.14. a redistribution lottery involves picking the correct four numbers from 1 to 9 (without
replacement, so 3,4,4,1 for example is not possible). the order of the picked numbers is irrelevant. every
week a million people play this game, each paying   1 to enter, with the numbers 3,5,7,9 being the most
popular (1 in every 100 people chooses these numbers). given that the million pounds prize money is split
equally between winners, and that any four (di   erent) numbers come up at random, what is the expected
amount of money each of the players choosing 3,5,7,9 will win each week? the least popular set of numbers
is 1,2,3,4 with only 1 in 10,000 people choosing this. how much do they pro   t each week, on average? do
you think there is any    skill    involved in playing this lottery?

exercise 1.15. in a test of    psychometry    the car keys and wrist watches of 5 people are given to a medium.
the medium then attempts to match the wrist watch with the car key of each person. what is the expected
number of correct matches that the medium will make (by chance)? what is the id203 that the medium
will obtain at least 1 correct match?

exercise 1.16.

1. show that for any function f

(cid:88)

x

p(x|y)f (y) = f (y)

draft november 9, 2017

(1.6.7)

25

2. explain why, in general,

p(x|y)f (x, y) (cid:54)=

(cid:88)

x

(cid:88)

x

f (x, y)

exercises

(1.6.8)

exercise 1.17 (inspired by singingbanana.com). seven friends decide to order pizzas by telephone from
pizza4u based on a    yer pushed through their letterbox. pizza4u has only 4 kinds of pizza, and each person
chooses a pizza independently. bob phones pizza4u and places the combined pizza order, simply stating
how many pizzas of each kind are required. unfortunately, the precise order is lost, so the chef makes seven
randomly chosen pizzas and then passes them to the delivery boy.

1. how many di   erent combined orders are possible?

2. what is the id203 that the delivery boy has the right order?

exercise 1.18. sally is new to the area and listens to some friends discussing about another female friend.
sally knows that they are talking about either alice or bella but doesn   t know which. from previous conver-
sations sally knows some independent pieces of information: she   s 90% sure that alice has a white car, but
doesn   t know if bella   s car is white or black. similarly, she   s 90% sure that bella likes sushi, but doesn   t know
if alice likes sushi. sally hears from the conversation that the person being discussed hates sushi and drives
a white car. what is the id203 that the friends are talking about alice?
assume maximal uncertainty
in the absence of any knowledge of the probabilities.

@@

exercise 1.19. the weather in london can be summarised as: if it rains one day there   s a 70% chance it
will rain the following day; if it   s sunny one day there   s a 40% chance it will be sunny the following day.

1. assuming that the prior id203 it rained yesterday is 0.5, what is the id203 that it was raining

yesterday given that it   s sunny today?

2. if the weather follows the same pattern as above, day after day, what is the id203 that it will rain

on any day (based on an e   ectively in   nite number of days of observing the weather)?

3. use the result from part 2 above as a new prior id203 of rain yesterday and recompute the proba-

bility that it was raining yesterday given that it   s sunny today.

a game of battleships is played on a 10    10 pixel grid. there are two 5-pixel length
exercise 1.20.
ships placed uniformly at random on the grid, subject to the constraints that (i) the ships cannot over-
lap and (ii) one ship is vertical and the other horizontal. after 10 unsuccessful    misses    in locations
(1, 10), (2, 2), (3, 8), (4, 4), (5, 6), (6, 5), (7, 4), (7, 7), (9, 2), (9, 9) calculate which pixel has the highest proba-
bility of containing a ship. state this pixel and the value of the highest id203.

++

exercise 1.21.
a game of battleships is played on a 8    8 grid. there are two 5-pixel length ships placed
horizontally and two 5-pixel length ships placed vertically, subject to the same constraints as in the previous
question. given    misses    in locations (1, 1), (2, 2) and a    hit    in location (5, 5), which pixel most likely contains
a ship and what is that id203?

++

exercise 1.22.
sions at locations s1 and s2 and the observed value at sensor i is

we consider an extension of the explosion example. in this extension there are two explo-

++

vi =

1

d2
i (1) + 0.1

+

1

d2
i (2) + 0.1

+    i

where di(1), di(2) is the distance from explosion 1, 2 to the sensor respectively;    is the standard deviation of
the gaussian sensor noise and the noise  i is drawn from a zero mean unit variance gaussian independently
for each sensor. the data in the    le earthquakeexercisedata.txt represents the observed sensor values
vi and the coordinate system setup is given in earthquakeexercisesetup.jl. assuming that the prior
locations of the explosions are independent and uniform (according to the spiral coordinate system):

1. calculate the posterior p(s1|v) and draw an image similar to    g(1.3) that visualises the posterior.
2. writing h2 for the hypothesis that there are two explosions and h1 for the hypothesis that there is

only one explosion, report the value of log p(v|h2)     log p(v|h1).

26

draft november 9, 2017

exercises

3. assuming we have no prior preference, that is p(h1) = p(h2) = const., explain why log p(v|h2)    

log p(v|h1) relates to the id203 that there are two explosions compared to only 1.

4. if we assumed that there were k explosions, explain the computational complexity of calculating

log p(v|hk).

++

exercise 1.23.
incoming explosions, so that

similar to the above exercise, a di   erent kind of explosion sensor measures the mean of two

vi =

0.5

d2
i (1) + 0.1

+

0.5

d2
i (2) + 0.1

+    i

the observed sensor data is given in earthquakeexercisemeandata.txt.

1. calculate the posterior p(s1|v) and draw an image similar to    g(1.3) that visualises the posterior.
2. writing h2 for the hypothesis that there are two explosions and h1 for the hypothesis that there is

only one explosion, report the value of log p(v|h2)     log p(v|h1).

3. explain why in this case it will generally be more di   cult (compared to the previous exercise) to

accurately estimate the location of the two explosions and determine the number of explosions.

draft november 9, 2017

27

exercises

28

draft november 9, 2017

chapter 2

basic graph concepts

often we have good reason to believe that one event a   ects another, or conversely that some events are
independent. incorporating such knowledge can produce models that are better speci   ed and computation-
ally more e   cient. graphs describe how objects are linked and provide a convenient picture for describing
related objects. we will ultimately introduce a graph structure among the variables of a probabilistic model
to produce a    graphical model    that captures relations among the variables as well as their uncertainties. in
this chapter, we introduce the required basic concepts from id207.

2.1 graphs

de   nition 2.1 (graph). a graph g consists of nodes (also called vertices) and edges (also called links)
between the nodes. edges may be directed (they have an arrow in a single direction) or undirected. edges
can also have associated weights. a graph with all edges directed is called a directed graph, and one with
all edges undirected is called an undirected graph.

a

d

a

d

b

c

b

c

e

e

an directed graph g consists of directed edges between nodes.

an undirected graph g consists of undirected edges between nodes.

graphs with edge weights are often used to model networks and    ows along    pipes   , or distances between
cities, where each node represents a city. we will also make use of these concepts in chapter(5) and chap-
ter(28). our main use of graphs though will be to endow them with a probabilistic interpretation and we
develop a connection between directed graphs and id203 in chapter(3). undirected graphs also play a
central role in modelling and reasoning with uncertainty. essentially, two variables will be independent if
they are not linked by a path on the graph. we will discuss this in more detail when we consider markov
networks in chapter(4).

de   nition 2.2 (path, ancestors, descendants). a path a (cid:55)    b from node a to node b is a sequence of
nodes that connects a to b. that is, a path is of the form a0, a1, . . . , an   1, an, with a0 = a and an = b

29

graphs

and each edge (ak   1, ak), k = 1, . . . , n being in the graph. a directed path is a sequence of nodes which
when we follow the direction of the arrows leads us from a to b. in directed graphs, the nodes a such that
(cid:55)    a are the descendants
a (cid:55)    b and b (cid:54)
of a.

(cid:55)    a are the ancestors of b. the nodes b such that a (cid:55)    b and b (cid:54)

de   nition 2.3 (cycle, loop and chord). a cycle is a directed path that starts and returns to the same
node a     b     . . .     z     a. a loop is a path containing more than two nodes, irrespective of edge direction,
that starts and returns to the same node. for example in    g(2.2b) 1     2     4     3     1 forms a loop, but the
graph is acyclic (contains no cycles). a chord is an edge that connects two non-adjacent nodes in a loop    
for example, the 2     3 edge is a chord in the 1     2     4     3     1 loop of    g(2.2a).

de   nition 2.4 (directed acyclic graph (dag)). a dag is a graph g with directed edges (arrows on each
link) between the nodes such that by following a path of nodes from one node to another along the direction
of each edge no path will revisit a node. in a dag the ancestors of b are those nodes who have a directed
path ending at b. conversely, the descendants of a are those nodes who have a directed path starting at
a.

de   nition 2.5 (relationships in a dag).

x1

x8

x2

x4

x3

x7

x5

x6

the parents of x4 are pa (x4) = {x1, x2, x3}. the children of x4 are
the family of a node is itself and its parents.
ch (x4) = {x5, x6}.
the markov blanket of a node is its parents, children and the parents
of its children
(excluding itself). in this case, the markov blanket of
x4 is x1, x2, x3, x5, x6, x7.

@@

dags will play a central role in modelling environments with many variables, in particular they are used
for the belief networks that we describe in the following chapter. one can view the directed links on a graph
as    direct dependencies    between parent and child variables. naively, the acyclic condition prevents circular
reasoning. these connections are discussed in detail in chapter(3).

de   nition 2.6 (neighbour). for an undirected graph g the neighbours of x, ne (x) are those nodes directly
connected to x.

de   nition 2.7 (clique).

a

d

b

c

e

given an undirected graph, a clique is a fully connected subset of nodes.
all the members of the clique are neighbours; for a maximal clique there is
no larger clique that contains the clique. for example this graph has two
maximal cliques, c1 = {a, b, c, d} and c2 = {b, c, e}. whilst a, b, c
are fully connected, this is a non-maximal clique since there is a larger
fully connected set, a, b, c, d that contains this. a non-maximal clique is
sometimes called a cliquo.

cliques play a central role in both modelling and id136.
in modelling they will describe variables
that are all dependent on each other, see chapter(4). in id136 they describe sets of variables with no
simpler structure describing the relationship between them and hence for which no simpler e   cient id136
procedure is likely to exist. we will discuss this issue in detail in chapter(5) and chapter(6).

30

draft november 9, 2017

numerically encoding graphs

c

a

f

b

g

d

(a)

e

c

b

g

e

a

f

d

(b)

figure 2.1:
(b): multiply-connected graph.

(a): singly-connected graph.

de   nition 2.8 (connected graph). an undirected graph is connected if there is a path between every pair
of nodes (i.e. there are no isolated islands). for a graph which is not connected, the connected components
are those subgraphs which are connected.

de   nition 2.9 (singly connected graph). a graph is singly connected if there is only one path from any
node a to any other node b. otherwise the graph is multiply connected . this de   nition applies regardless
of whether or not the edges in the graph are directed. an alternative name for a singly connected graph is
a tree. a multiply-connected graph is also called loopy.

de   nition 2.10 (spanning tree).

a spanning tree of an undirected graph g is a singly-connected
subset of the existing edges such that the resulting singly-
connected graph covers all nodes of g. on the right is a graph
and an associated spanning tree. a maximum weight spanning
tree is a spanning tree such that the sum of all weights on the
edges of the tree is at least as large as any other spanning tree of
g.

procedure 2.1 (finding a maximal weight spanning tree). an algorithm to    nd a spanning tree with
maximal weight is as follows: start by picking the edge with the largest weight and add this to the edge
set. then pick the next candidate edge which has the largest weight and add this to the edge set     if this
results in an edge set with cycles, then reject the candidate edge and propose the next largest edge weight.
note that there may be more than one maximal weight spanning tree.

2.2 numerically encoding graphs

our ultimate goal is to make computational implementations of id136. therefore, if we want to incorpo-
rate graph structure into our models, we need to express graphs in a way that a computer can understand
and manipulate. there are several equivalent possibilities.

2.2.1 edge list

as the name suggests, an edge list simply lists which node-node pairs are in the graph. for    g(2.2a), an
edge list is l = {(1, 2), (2, 1), (1, 3), (3, 1), (2, 3), (3, 2), (2, 4), (4, 2), (3, 4), (4, 3)}. undirected edges are listed
twice, once for each direction.

draft november 9, 2017

31

numerically encoding graphs

2

2

1

4

1

4

3

(a)

3

(b)

(a): an undirected graph can be represented
figure 2.2:
as a symmetric adjacency matrix. (b): a directed acyclic
graph with nodes labelled in ancestral order corresponds to
a triangular adjacency matrix.

2.2.2 adjacency matrix

an alternative is to use an adjacency matrix

             0 1 1 0

1 0 1 1
1 1 0 1
0 1 1 0

            

a =

(2.2.1)

where aij = 1 if there is an edge from node i to node j in the graph, and 0 otherwise. some authors include
self-connections and place 1   s on the diagonal in this de   nition. an undirected graph has a symmetric
adjacency matrix.

provided that the nodes are labelled in ancestral order (parents always come before children) a directed
graph    g(2.2b) can be represented as a triangular adjacency matrix:

             0 1 1 0

0 0 1 1
0 0 0 1
0 0 0 0

            

t =

adjacency matrix powers

adjacency matrices may seem wasteful since many of the entries are zero. however, they have a useful
property that more than redeems them. for an n    n adjacency matrix a, powers of the adjacency
ij specify how many paths there are from node i to node j in k edge hops. if we include 1   s
ij is non-zero when there is a path connecting i to j in the graph. if a

matrix (cid:2)ak(cid:3)
on the diagonal of a then(cid:2)an   1(cid:3)
corresponds to a dag the non-zero entries of the jth row of(cid:2)an   1(cid:3) correspond to the descendants of node

j.

2.2.3 clique matrix

for an undirected graph with n nodes and maximal cliques c1, . . . ,ck a clique matrix is an n    k matrix
in which each column ck has zeros except for ones on entries describing the clique. for example

1 1
1 1
0 1

             1 0
            
             1 1 0 0 0

1 0 1 1 0
0 1 1 0 1
0 0 0 1 1

            

c =

cinc =

is a clique matrix for    g(2.2a). a cliquo matrix relaxes the constraint that cliques are required to be
maximal1. a cliquo matrix containing only two-node cliques is called an incidence matrix . for example

1the term    cliquo    for a non-maximal clique is attributed to julian besag.

32

draft november 9, 2017

(2.2.2)

(2.2.3)

(2.2.4)

code

is an incidence matrix for    g(2.2a). it is straightforward to show that cincct
inc is equal to the adjacency
matrix except that the diagonals now contain the degree of each node (the number of edges it touches).
similarly, for any cliquo matrix the diagonal entry of [cct]ii expresses the number of cliquos (columns)
that node i occurs in. o    diagonal elements [cct]ij contain the number of cliquos that nodes i and j
jointly inhabit.

remark 2.1 (graph confusions). graphs are widely used, but di   er markedly in what they represent.
two potential pitfalls are described below.

state transition diagrams such representations are used in markov chains and    nite state automata.
each state is a node and a directed edge between node i and node j (with an associated weight pij)
represents that a transition from state i to state j can occur with id203 pij. from the graphical
models perspective we use a directed graph x(t)     x(t + 1) to represent this markov chain. the
state-transition diagram provides a more detailed graphical description of the id155
table p(x(t + 1)|x(t)).

neural networks neural networks also have nodes and edges. in general, however, neural networks are
graphical representations of functions, whereas id114 are representations of distributions.

2.3 summary

    a graph is made of nodes and edges, which we will use to represent variables and relations between them.
    a dag is an acyclic graph and will be useful for representing    causal    relationships between variables.
    neighbouring nodes on an undirected graph will represent dependent variables.
    a graph is singly-connected if there is only one path from any node to any other     otherwise the graph is

multiply-connected.

    a clique is group of nodes all of which are connected to each other.
    the adjacency matrix is a machine-readable description of a graph. powers of the adjacency matrix give

information on the paths between nodes.

good reference for graphs, associated theories and their uses are [87, 122].

2.4 code

2.4.1 utility routines

drawnet.m: draw a graph based on an adjacency matrix
ancestors.m: find the ancestors of a node in a dag
edges.m: edge list from an adjacency matrix
ancestralorder.m: ancestral order from a dag
connectedcomponents.m: connected components
parents.m: parents of a node given an adjacency matrix
children.m: children of a node given an adjacency matrix
neigh.m: neighbours of a node given an adjacency matrix

a connected graph is a tree if the number of edges plus 1 is equal to the number of nodes. however, for a
disconnected graph this is not the case. the code istree.m below deals with the possibly disconnected case.
the routine is based on the observation that any singly-connected graph must always possess a simplicial

draft november 9, 2017

33

node (a leaf node) which can be eliminated to reveal a smaller singly-connected graph.

istree.m: if graph is singly connected return 1 and elimination sequence
spantree.m: return a spanning tree from an ordered edge list
singleparenttree.m: find a directed tree with at most one parent from an undirected tree

additional routines for basic graph manipulations are given at the end of chapter(6).

exercises

2.5 exercises

j in one timestep, and 0 otherwise. show that the matrix (cid:2)ak(cid:3)

exercise 2.1. consider an adjacency matrix a with elements [a]ij = 1 if one can reach state i from state
ij represents the number of paths that lead
from state j to i in k timesteps. hence derive an algorithm that will    nd the minimum number of steps to
get from state j to state i.

exercise 2.2. for an n    n symmetric adjacency matrix a, describe an algorithm to    nd the connected
components. you may wish to examine connectedcomponents.m.

exercise 2.3. show that for a connected graph that is singly-connected, the number of edges e must be
equal to the number of nodes minus 1, e = v     1. give an example graph with e = v     1 that is not
singly-connected. hence the condition e = v     1 is a necessary but not su   cient condition for a graph to
be singly-connected.

exercise 2.4. describe a procedure to determine if a graph is singly-connected.

exercise 2.5. describe a procedure to determine all the ancestors of a set of nodes in a dag.

exercise 2.6. wikiadjsmall.mat contains a random selection of 1000 wiki authors, with a link between
two authors if they    know    each other (see snap.stanford.edu/data/wiki-vote.html).
assume that if i
   knows    j, then j    knows    i. plot a histogram of the separation (the length of the
shortest path between two
users on the graph corresponding to the adjacency matrix) between all users based on separations from 1 to
20. that is the bin n(s) in the histogram contains the number of pairs with separation s.

exercise 2.7. the    le cliques.mat contains a list of
100 cliques de   ned on a graph of 10 nodes. your task
is to return a set of unique maximal cliques, eliminating cliques that are wholly contained within another.
once you have found a clique, you can represent it in binary form as, for example

++
++

@@

(1110011110)

which says that this clique contains variables 1, 2, 3, 6, 7, 8, 9, reading from left to right. converting this
binary representation to decimal (with the rightmost bit being the units and the leftmost 29) this corresponds
to the number 926. using this decimal representation, write the list of unique cliques, ordered from lowest
decimal representation to highest. describe fully the stages of the algorithm you use to    nd these unique
cliques. hint: you may    nd examining uniquepots.m useful.

exercise 2.8. explain how to construct a graph with n nodes, where n is even, that contains at least
(n/2)2 maximal cliques.

exercise 2.9. let n be divisible by 3. construct a graph with n nodes by partitioning the nodes into n/3
subsets, each subset containing 3 nodes. then connect all nodes, provided they are not in the same subset.
show that such a graph has 3n/3 maximal cliques. this shows that a graph can have an exponentially large
number of maximal cliques[218].

exercise 2.10.
the room, stays for some time, and leaves. the testimonies they gave to the police are:

a jewel was stolen from a room during a party2. each of the guests (a,b,c,d,e,f ) enters

++

2this question is a modi   ed version of a well known problem, disguised here to make looking up the solution harder.

apologies to the original author for the lack of a citation.

34

draft november 9, 2017

exercises

1. a: i was present in the room with e, b

2. b: i was present in the room with a, f and e

3. c: i was present in the room with f and d

4. d: i was present in the room with a and f

5. e: i was present in the room with c

6. f: i was present in the room with c and e

one of the statements in the testimonies is false. which is it?

notes:

if any two people are present in the room at the same time, then at least one of them will report the fact that
he was present in the room with the other person. each above testimony should be interpreted as, for example:

a: i was present in the room with e. i was present in the room with b. it does not necessarily mean that
a was simultaneously present in the room with both e and b (so a and b could have been present in the
room at some time t1, and a and e present in the room at some other time t2). the other testimonies have
a similar interpretation.

the testimonies could be partially false. if x says that he was present in the room with y and z, it could
be that the statement x was present in the room with y is true, but x was present in the room with z is
false.

draft november 9, 2017

35

exercises

36

draft november 9, 2017

chapter 3

belief networks

we can now make a    rst connection between id203 and id207. a belief network introduces
structure into a probabilistic model by using graphs to represent independence assumptions among the vari-
ables. id203 operations such as marginalizing and conditioning then correspond to simple operations
on the graph, and details about the model can be    read    from the graph. there is also a bene   t in terms of
computational e   ciency. belief networks cannot capture all possible relations among variables. however,
they are natural for representing    causal    relations, and they are a part of the family of id114 we
study further in chapter(4).

3.1 the bene   ts of structure

it   s tempting to think of feeding a mass of undigested data and id203 distributions into a computer
and getting back good predictions and useful insights in extremely complex environments. however, unfor-
tunately, such a naive approach is likely to fail. the possible ways variables can interact is extremely large,
so that without some sensible assumptions we are unlikely to make a useful model. independently specifying
all the entries of a table p(x1, . . . , xn ) over binary variables xi takes o(2n ) space, which is impractical for
more than a handful of variables. this is clearly infeasible in many machine learning and related application
areas where we need to deal with distributions on potentially hundreds if not millions of variables. structure
is also important for computational tractability of inferring quantities of interest. given a distribution on n
binary variables, p(x1, . . . , xn ), computing a marginal such as p(x1) requires summing over the 2n   1 states
of the other variables. even on the most optimistically fast supercomputer this would take far too long,
even for a n = 100 variable system.

joint id203 distribution. for a distribution on a chain, p(x1, . . . , x100) =(cid:81)99

the only way to deal with such large distributions is to constrain the nature of the variable interactions
in some manner, both to render speci   cation and ultimately id136 in such systems tractable. the key
idea is to specify which variables are independent of others, leading to a structured factorisation of the
i=1   (xi, xi+1), computing
a marginal p(x1) can be computed in the blink of an eye on modern computers. belief networks are a con-
venient framework for representing such independence assumptions. we will discuss belief networks more
formally in section(3.3),    rst discussing their natural role as    causal    models.

belief networks (also called bayes    networks or bayesian belief networks) are a way to depict the indepen-
dence assumptions made in a distribution [162, 183]. their application domain is widespread, ranging from
troubleshooting[54] and expert reasoning under uncertainty to machine learning. before we more formally
de   ne a belief network (bn), an example will help motivate the development1.

1the scenario is adapted from [237].

37

r

s

b

e

j

t

(a)

r

a

(b)

the bene   ts of structure

figure 3.1: (a): belief network structure for the
   wet grass    example. each node in the graph rep-
resents a variable in the joint distribution, and
the variables which feed in (the parents) to an-
other variable represent which variables are to
(b): belief
the right of the conditioning bar.
network for the burglar model.

3.1.1 modelling independencies

one morning tracey leaves her house and realises that her grass is wet. is it due to overnight rain or did
she forget to turn o    the sprinkler last night? next she notices that the grass of her neighbour, jack, is also
wet. this explains away to some extent the possibility that her sprinkler was left on, and she concludes
therefore that it has probably been raining.

we can model the above situation by    rst de   ning the variables we wish to include in our model. in the
above situation, the natural variables are

r     {0, 1} r = 1 means that it has been raining, and 0 otherwise
s     {0, 1} s = 1 means that tracey has forgotten to turn o    the sprinkler, and 0 otherwise
j     {0, 1}
j = 1 means that jack   s grass is wet, and 0 otherwise
t     {0, 1} t = 1 means that tracey   s grass is wet, and 0 otherwise
a model of tracey   s world then corresponds to a id203 distribution on the joint set of the variables of
interest p(t, j, r, s) (the order of the variables is irrelevant).

since each of the variables in this example can take one of two states, it would appear that we naively
have to specify the values for each of the 24 = 16 states, e.g.
p(t = 1, j = 0, r = 0, s = 1) = 0.057 etc.
however, since there are normalisation conditions for probabilities, we do not need to specify all the state
probabilities. to see how many states need to be speci   ed, consider the following decomposition. without
loss of generality and repeatedly using the de   nition of id155, we may write

@@

p(t, j, r, s) = p(t|j, r, s)p(j, r, s)

= p(t|j, r, s)p(j|r, s)p(r, s)
= p(t|j, r, s)p(j|r, s)p(r|s)p(s)

(3.1.1)

(3.1.2)

(3.1.3)

that is, we may write the joint distribution as a product of conditional distributions. the    rst term
p(t|j, r, s) requires us to specify 23 = 8 values     we need p(t = 1|j, r, s) for the 8 joint states of j, r, s.
the other value p(t = 0|j, r, s) is given by normalisation : p(t = 0|j, r, s) = 1     p(t = 1|j, r, s).
similarly, we need 4 + 2 + 1 values for the other factors, making a total of 15 values in all. in general, for a
distribution on n binary variables, we need to specify 2n     1 values in the range [0, 1]. the important point
here is that the number of values that need to be speci   ed in general scales exponentially with the number
of variables in the model     this is impractical in general and motivates simpli   cations.

conditional independence

the modeler often knows constraints on the system. for example, in the scenario above, we may assume
that tracey   s grass is wet depends only directly on whether or not it has been raining and whether or not
her sprinkler was on. that is, we make a conditional independence assumption

p(t|j, r, s) = p(t|r, s)

(3.1.4)

similarly, we assume that jack   s grass is wet is in   uenced only directly by whether or not it has been raining,
and write

p(j|r, s) = p(j|r)

38

(3.1.5)

draft november 9, 2017

the bene   ts of structure

furthermore, we assume the rain is not directly in   uenced by the sprinkler,

p(r|s) = p(r)

which means that our model equation (3.1.3) now becomes

p(t, j, r, s) = p(t|r, s)p(j|r)p(r)p(s)

(3.1.6)

(3.1.7)

we can represent these conditional independencies graphically, as in    g(3.1a). this reduces the number of
values that we need to specify to 4 + 2 + 1 + 1 = 8, a saving over the previous 15 values in the case where
no conditional independencies had been assumed.

to complete the model, we need to numerically specify the values of each id155 table
(cpt). let the prior probabilities for r and s be p(r = 1) = 0.2 and p(s = 1) = 0.1. we set the
remaining probabilities to p(j = 1|r = 1) = 1, p(j = 1|r = 0) = 0.2 (sometimes jack   s grass is wet
due to unknown e   ects, other than rain), p(t = 1|r = 1, s = 0) = 1, p(t = 1|r = 1, s = 1) = 1,
p(t = 1|r = 0, s = 1) = 0.9 (there   s a small chance that even though the sprinkler was left on, it didn   t
wet the grass noticeably), p(t = 1|r = 0, s = 0) = 0.
id136

now that we   ve made a model of an environment, we can perform id136. let   s calculate the id203
that the sprinkler was on overnight, given that tracey   s grass is wet: p(s = 1|t = 1). to do this we use:

(cid:80)
(cid:80)

p(s = 1|t = 1) =

=

=

=

p(s = 1, t = 1)

j,r p(t = 1, j, r, s = 1)
j,r,s p(t = 1, j, r, s)
p(t = 1)
j,r p(j|r)p(t = 1|r, s = 1)p(r)p(s = 1)

=

j,r,s p(j|r)p(t = 1|r, s)p(r)p(s)

(cid:80)
(cid:80)

r p(t = 1|r, s = 1)p(r)p(s = 1)

r,s p(t = 1|r, s)p(r)p(s)

(cid:80)
(cid:80)

0.9    0.8    0.1 + 1    0.2    0.1

(3.1.8)

(3.1.9)

(3.1.10)

= 0.3382

(3.1.11)

0.9    0.8    0.1 + 1    0.2    0.1 + 0    0.8    0.9 + 1    0.2    0.9

any function f (r), a summation of the form(cid:80)

so the (posterior) belief that the sprinkler is on increases above the prior id203 0.1, due to the evidence
that the grass is wet. note that in equation (3.1.9), the summation over j in the numerator is unity since, for
j p(j|r)f (r) equals f (r). this follows from the de   nition
that a distribution p(j|r) must sum to one, and the fact that f (r) does not depend on j. a similar e   ect
occurs for the summation over j in the denominator.

let us now calculate the id203 that tracey   s sprinkler was on overnight, given that her grass is wet
and that jack   s grass is also wet, p(s = 1|t = 1, j = 1). we use id155 again:

p(s = 1, t = 1, j = 1)

p(s = 1|t = 1, j = 1) =

=

=

=

(cid:80)
(cid:80)
(cid:80)
(cid:80)

0.0344
0.2144

p(t = 1, j = 1)
r p(t = 1, j = 1, r, s = 1)
r,s p(t = 1, j = 1, r, s)
r p(j = 1|r)p(t = 1|r, s = 1)p(r)p(s = 1)

r,s p(j = 1|r)p(t = 1|r, s)p(r)p(s)
= 0.1604

(3.1.12)

(3.1.13)

(3.1.14)

(3.1.15)

the id203 that the sprinkler is on, given the extra evidence that jack   s grass is wet, is lower than the
id203 that the grass is wet given only that tracey   s grass is wet. this occurs since the fact that jack   s
grass is also wet increases the chance that the rain has played a role in making tracey   s grass wet.

naturally, we don   t wish to carry out such id136 calculations by hand all the time. general purpose
algorithms exist for this, such as the junction tree algorithm, chapter(6).

draft november 9, 2017

39

the bene   ts of structure

example 3.1 (was it the burglar?). here   s another example using binary variables, adapted from [237].
sally comes home to    nd that the burglar alarm is sounding (a = 1). has she been burgled (b = 1), or
was the alarm triggered by an earthquake (e = 1)? she turns the car radio on for news of earthquakes and
   nds that the radio broadcasts an earthquake alert (r = 1).

using bayes    rule, we can write, without loss of generality,

p(b, e, a, r) = p(a|b, e, r)p(b, e, r)

we can repeat this for p(b, e, r), and continue

p(b, e, a, r) = p(a|b, e, r)p(r|b, e)p(e|b)p(b)

(3.1.16)

(3.1.17)

however, the alarm is surely not directly in   uenced by any report on the radio     that is, p(a|b, e, r) =
p(a|b, e). similarly, we can make other conditional independence assumptions such that

p(b, e, a, r) = p(a|b, e)p(r|e)p(e)p(b)

as depicted in    g(3.1b).

specifying id155 tables

alarm = 1 burglar earthquake

0.9999
0.99
0.99
0.0001

1
1
0
0

1
0
1
0

(3.1.18)

radio = 1 earthquake

1
0

1
0

the remaining tables are p(b = 1) = 0.01 and p(e = 1) = 0.000001. the tables and graphical structure
fully specify the distribution. now consider what happens as we observe evidence.

(cid:80)
(cid:80)
(cid:80)
(cid:80)

initial evidence: the alarm is sounding

p(b = 1|a = 1) =

=

e,r p(b = 1, e, a = 1, r)
b,e,r p(b, e, a = 1, r)
e,r p(a = 1|b = 1, e)p(b = 1)p(e)p(r|e)

b,e,r p(a = 1|b, e)p(b)p(e)p(r|e)

    0.99

(3.1.19)

(3.1.20)

additional evidence: the radio broadcasts an earthquake warning: a similar calculation gives
p(b = 1|a = 1, r = 1)     0.01. thus, initially, because the alarm sounds, sally thinks that she   s been
burgled. however, this id203 drops dramatically when she hears that there has been an earthquake.
that is, the earthquake    explains away    to an extent the fact that the alarm is ringing. see demoburglar.m.

remark 3.1 (causal intuitions). belief networks as we   ve de   ned them are ways to express independence
statements. nevertheless, in expressing these independencies it can be useful (though also potentially
misleading) to think of    what causes what   .
in example(3.1) we chose the ordering of the variables as
(reading from right to left) b, e, r, a in equation (3.1.17) since b and e can be considered root    causes   
and a and r as    e   ects   .

3.1.2 reducing the burden of speci   cation

consider a discrete variable y with many discrete parental variables x1, . . . , xn,    g(3.2a). formally, the
structure of the graph implies nothing about the form of the parameterisation of the table p(y|x1, . . . , xn).
if each parent xi has dim (xi) states, and there is no constraint on the table, then the table p(y|x1, . . . , xn)
draft november 9, 2017
40

uncertain and unreliable evidence

x1

x2

x3

x4

x5

x1

x2

x3

x4

x5

x1

x2

x3

x4

x5

z1

z2

z1

z2

z3

z4

z5

y

(a)

y

(b)

y

(c)

figure 3.2:
here only 16 states are required. (c): noisy logic gates.

(a): if all variables are binary 25 = 32 states are required to specify p(y|x1, . . . , x5).

(b):

contains (dim (y)     1)(cid:81)
(cid:88)

p(y|x1, . . . , x5) =

z1,z2

i dim (xi) entries. if stored explicitly for each state, this would require potentially
huge storage. an alternative is to constrain the table to have a simpler parametric form. for example, one
might write a decomposition in which only a limited number of parental interactions are required (this is
called divorcing parents in [162]). for example, in    g(3.2b), we have

p(y|z1, z2)p(z1|x1, x2, x3)p(z2|x4, x5)

(3.1.21)

assuming all variables are binary, the number of states requiring speci   cation is 23 + 22 + 22 = 16, compared
to the 25 = 32 states in the unconstrained case.

logic gates

another technique to constrain tables uses simple classes of conditional tables. for example, in    g(3.2c),
one could use a logical or gate on binary zi, say

(cid:26) 1 if at least one of the zi is in state 1

(3.1.22)

p(y|z1, . . . , z5) =

0 otherwise

we can then make a table p(y|x1, . . . , x5) by including the additional terms p(zi = 1|xi). when each xi
is binary there are in total only 2 + 2 + 2 + 2 + 2 = 10 quantities required for specifying p(y|x). in this
case,    g(3.2c) can be used to represent any noisy logic gate, such as the noisy or or noisy and, where the
number of parameters required to specify the noisy gate is linear in the number of parents.

the noisy-or is particularly common in disease-symptom networks in which many diseases x can give rise
to the same symptom y    provided that at least one of the diseases is present, the id203 that the
symptom will be present is high.

3.2 uncertain and unreliable evidence

in the following we make a distinction between evidence that is uncertain, and evidence that is unreliable.

3.2.1 uncertain evidence

in soft or uncertain evidence, the evidence variable is in more than one state, with the strength of our belief
about each state being given by probabilities. for example, if x has the states dom(x) = {red, blue, green}
the vector (0.6, 0.1, 0.3) represents the belief in the respective states. in contrast, for hard evidence we are
certain that a variable is in a particular state. in this case, all the id203 mass is in one of the vector
components, for example (0, 0, 1).

performing id136 with soft-evidence is straightforward and can be achieved using bayes    rule. for
example, for a model p(x, y), consider that we have some soft evidence   y about the variable y, and wish to

draft november 9, 2017

41

uncertain and unreliable evidence

know what e   ect this has on the variable x     that is we wish to compute p(x|  y). from bayes    rule, and the
assumption p(x|y,   y) = p(x|y), we have

p(x|  y) =

p(x, y|  y) =

p(x|y,   y)p(y|  y) =

p(x|y)p(y|  y)

(3.2.1)

(cid:88)

y

(cid:88)

y

(cid:88)

y

where p(y = i|  y) represents the id203 that y is in state i under the soft-evidence.
the assumption
here is that, if we know the certain evidence y, we don   t need to know the uncertain evidence   y, hence
p(x|y,   t) = p(x|y). this is a generalisation of hard-evidence in which the vector p(y|  y) has all zero compo-
nent values, except for a single component. this procedure in which we    rst de   ne the model conditioned
on the evidence, and then average over the distribution of the evidence is also known as je   rey   s rule.

++

in the bn we use a dashed circle to represent that a variable is in a soft-evidence
state.

x

y

example 3.2 (soft-evidence). revisiting the burglar scenario, example(3.1), imagine that we are only 70%
sure we heard the burglar alarm sounding. for this binary variable case we represent this soft-evidence for
the states (1, 0) as   a = (0.7, 0.3). what is the id203 of a burglary under this soft-evidence?

p(b = 1|   a) =

p(b = 1|a)p(a|   a) = p(b = 1|a = 1)    0.7 + p(b = 1|a = 0)    0.3

(3.2.2)

(cid:88)

a

the probabilities p(b = 1|a = 1)     0.99 and p(b = 1|a = 0)     0.0001 are calculated using bayes    rule as
before to give

p(b = 1|   a)     0.6930

(3.2.3)

this is lower than 0.99, the id203 of having been burgled when we are sure we heard the alarm.

holmes, watson and mrs gibbon

an entertaining example of uncertain evidence is given by pearl[237] that we adapt for our purposes here.
the environment contains four variables

b     {tr, fa} b = tr means that holmes    house has been burgled
a     {tr, fa} a = tr means that holmes    house alarm went o   
w     {tr, fa} w = tr means that watson heard the alarm
g     {tr, fa} g = tr means that mrs gibbon heard the alarm
the bn below for this scenario is depicted in    g(3.3a)

p(b, a, g, w ) = p(a|b)p(b)p(w|a)p(g|a).

(3.2.4)

watson states that he heard the alarm is sounding. mrs gibbon is a little deaf and cannot be sure herself
that she heard the alarm, being 80% sure she heard it. this can be dealt with using the soft evidence
technique,    g(3.3b). from je   rey   s rule, one uses the original model equation (3.2.4) to    rst compute the
model conditioned on the evidence

(cid:80)

(cid:80)

=

a p(g|a)p(w = tr|a)p(a|b = tr)p(b = tr)

b,a p(g|a)p(w = tr|a)p(a|b)p(b)

p(b = tr|w = tr, g) =

p(b = tr, w = tr, g)

p(w = tr, g)

and then uses the soft-evidence

(cid:26) 0.8 g = tr

0.2 g = fa

p(g|   g) =

(3.2.5)

(3.2.6)

to compute
p(b = tr|w = tr,   g) = p(b = tr|w = tr, g = tr)p(g = tr|   g)+p(b = tr|w = tr, g = fa)p(g = fa|   g) (3.2.7)
a full calculation requires us to numerically specify all the terms in equation (3.2.4); see for example
exercise(3.8).

42

draft november 9, 2017

uncertain and unreliable evidence

b

a

g

w

b

a

g

w

b

a

h

w

b

a

g

h

(a)

(b)

(c)

(d)

(a): mr holmes    burglary worries as given in [237]: (b)urglar, (a)larm, (w)atson, mrs
figure 3.3:
(g)ibbon. (b): mrs gibbon   s uncertain evidence represented by a dashed circle. (c): virtual evidence or
the replacement of unreliable evidence can be represented by a dashed line. (d): mrs gibbon is uncertain
in her evidence. holmes also replaces the unreliable watson with his own interpretation.

3.2.2 unreliable evidence

@@

holmes telephones mrs gibbon and realises that he doesn   t trust her evidence (he suspects that she   s been
drinking) and
his interpretation is that if the alarm sounded it was 80% probable to have resulted in mrs
gibbon stating that she heard it; if the alarm didn   t sound, there is a 20% chance that mrs gibbon would
have stated that she heard it. note that this is not the same as mrs gibbon being 80% sure herself that
she heard the alarm     this would be soft evidence whose e   ect on our calculations would also contain the
term p(g|a), as in equation (3.2.5). holmes rather wishes to discard all of this and simply replace it with
his own interpretation of events. mr holmes can achieve this by replacing the term p(g|a) by a so-called
virtual evidence term

p(g|a)     p(h|a),

where p(h|a) =

(cid:26) 0.8 a = tr

0.2 a = fa

here the state h is arbitrary and    xed. this is used to modify the joint distribution to

p(b, a, h, w ) = p(a|b)p(b)p(w|a)p(h|a),

see    g(3.3c). when we then compute p(b = tr|w = tr, h) the e   ect of mr holmes    judgement will count
for a factor of 4 times more in favour of the alarm sounding than not. the values of the table entries are
irrelevant up to normalisation since any constants can be absorbed into the proportionality constant. note
also that p(h|a) is not a distribution in a, and hence no normalisation is required. this form of evidence
is also called likelihood evidence.

uncertain and unreliable evidence

to demonstrate how to combine such e   ects as unreliable and uncertain evidence, consider the situation in
which mrs gibbon is uncertain in her evidence, and mr holmes feels that watson   s evidence is unreliable
and wishes to replaces it with his own interpretation, see    g(3.3d). to account for this we    rst deal with
the unreliable evidence

p(b, a, w, g)     p(b, a, h, g) = p(b)p(a|b)p(g|a)p(h|a)

(3.2.10)

using this modi   ed model, we can now use je   rey   s rule to compute the model conditioned on the evidence

p(b, a|h, g) =

(cid:80)
(cid:88)
p(b, a|h,   g) =
(cid:88)

g

p(b|h,   g) =

a

we now include the uncertain evidence   g to form the    nal model

p(b)p(a|b)p(g|a)p(h|a)
a,b p(b)p(a|b)p(g|a)p(h|a)

p(b, a|h, g)p(g|   g)

from which we may compute the marginal p(b|h,   g)

p(b, a|h,   g)

draft november 9, 2017

(3.2.8)

(3.2.9)

(3.2.11)

(3.2.12)

(3.2.13)

43

3.3 belief networks

belief networks

d(cid:89)

i=1

de   nition 3.1 (belief network). a belief network is a distribution of the form

p(x1, . . . , xd) =

p(xi|pa (xi))

(3.3.1)

where pa (xi) represent the parental variables of variable xi. represented as a directed graph, with an arrow
pointing from a parent variable to child variable, a belief network corresponds to a directed acyclic graph
(dag), with the ith node in the graph corresponding to the factor p(xi|pa (xi)).

remark 3.2 (graphs and distributions). a somewhat subtle point is whether or not a belief network
corresponds to a speci   c instance of a distribution (as given in de   nition(3.1)) requiring also the numerical
speci   cation of the id155 tables, or whether or not it refers to any distribution which is
consistent with the speci   ed structure.
in this one can potentially distinguish between a belief network
distribution (containing a numerical speci   cation) and a belief network graph (which contains no numerical
speci   cation). normally this issue will not arise much throughout the book, but is potentially important in
clarifying the scope of independence/dependence statements.

in the wet grass and burglar examples, we had a choice as to how we recursively used bayes    rule. in a
general 4 variable case we could choose the factorisation,

p(x1, x2, x3, x4) = p(x1|x2, x3, x4)p(x2|x3, x4)p(x3|x4)p(x4)

an equally valid choice is (see    g(3.4))

p(x1, x2, x3, x4) = p(x3|x4, x1, x2)p(x4|x1, x2)p(x1|x2)p(x2).

(3.3.2)

(3.3.3)

in general, two di   erent graphs may represent the same independence assumptions, as we will discuss further
in section(3.3.1). if one wishes to make independence assumptions, then the choice of factorisation becomes
signi   cant.

the observation that any distribution may be written in the cascade form,    g(3.4), gives an algorithm for
constructing a bn on variables x1, . . . , xn : write down the n   node cascade graph; label the nodes with
the variables in any order; now each successive independence statement corresponds to deleting one of the
edges. more formally, this corresponds to an ordering of the variables which, without loss of generality, we
may write as x1, . . . , xn. then, from bayes    rule, we have

p(x1, . . . , xn) = p(x1|x2, . . . , xn)p(x2, . . . , xn)

= p(x1|x2, . . . , xn)p(x2|x3, . . . , xn)p(x3, . . . , xn)
= p(xn)

p(xi|xi+1, . . . , xn)

n   1(cid:89)

i=1

(3.3.4)

(3.3.5)

(3.3.6)

the representation of any bn is therefore a directed acyclic graph (dag).

every id203 distribution can be written as a bn, even though it may correspond to a fully connected
   cascade    dag. the particular role of a bn is that the structure of the dag corresponds to a set of
conditional independence assumptions, namely which ancestral parental variables are su   cient to specify
each id155 table. note that this does not mean that non-parental variables have no in   u-
ence. for example, for distribution p(x1|x2)p(x2|x3)p(x3) with dag x1     x2     x3, this does not imply
p(x2|x1, x3) = p(x2|x3). the dag speci   es conditional independence statements of variables on their an-
cestors     namely which ancestors are direct    causes    for the variable. the    e   ects   , given by the descendants
of the variable, will generally be dependent on the variable. see also remark(3.3).

44

draft november 9, 2017

belief networks

x1

x2

x3

x4

x3

x4

x1

x2

(a)

(b)

figure 3.4: two bns for a 4 variable distribution. both graphs (a) and (b) represent the same distribution
p(x1, x2, x3, x4). strictly speaking they represent the same (lack of) independence assumptions     the graphs
say nothing about the content of the tables. the extension of this    cascade    to many variables is clear and
always results in a directed acyclic graph.

remark 3.3 (dependencies and the markov blanket). consider a distribution on a set of variables x . for
a variable xi     x and corresponding belief network represented by a dag g, let m b(xi) be the variables
in the markov blanket of xi. then for any other variable y that is also not in the markov blanket of xi
(y     x\{xi     m b(xi)}), then xi        y| m b(xi). that is, the markov blanket of xi carries all information
about xi. as an example, for    g(3.2b), m b(z1) = {x1, x2, x3, y, z2} and z1       x4| m b(z1).

the dag corresponds to a statement of conditional independencies in the model. to complete the speci-
   cation of the bn we need to de   ne all elements of the id155 tables p(xi|pa (xi)). once
the graphical structure is de   ned, the entries of the id155 tables (cpts) p(xi|pa (xi)) can
be expressed. for every possible state of the parental variables pa (xi), a value for each of the states of xi
needs to be speci   ed (except one, since this is determined by normalisation). for a large number of parents,
writing out a table of values is intractable, and the tables are usually parameterised in a low dimensional
manner. this will be a central topic of our discussion on the application of bns in machine learning.

3.3.1 conditional independence

whilst a bn corresponds to a set of conditional independence assumptions, it is not always immediately
clear from the dag whether a set of variables is conditionally independent of a set of other variables (see
de   nition(1.7)). for example, in    g(3.5) are x1 and x2 independent, given the state of x4? the answer is
yes, since we have

(cid:88)
(cid:88)

x3

x3

(cid:88)

1

p(x4)

x3

p(x1, x2|x4) =

1

p(x4)

p(x1, x2, x3, x4) =

p(x1|x4)p(x2|x3, x4)p(x3)p(x4)

= p(x1|x4)

p(x2|x3, x4)p(x3)

(cid:88)

x1,x3

p(x1, x2, x3, x4) =

(cid:88)

1

p(x4)

x1,x3

p(x1|x4)p(x2|x3, x4)p(x3)p(x4)

now

p(x2|x4) =

=

p(x4)

1

(cid:88)

x3

p(x2|x3, x4)p(x3)

combining the two results above we have

p(x1, x2|x4) = p(x1|x4)p(x2|x4)

(3.3.7)

(3.3.8)

(3.3.9)

(3.3.10)

(3.3.11)

x1

x2

x3

x4

figure 3.5: p(x1, x2, x3, x4) = p(x1|x4)p(x2|x3, x4)p(x3)p(x4).

draft november 9, 2017

45

belief networks

x1

x2

x1

x2

x1

x2

x1

x2

x3

(a)

x3

(b)

x3

(c)

x3

(d)

figure 3.6: by dropping say the connection between variables x1 and x2, we reduce the 6 possible bn graphs
amongst three variables to 4. (the 6 fully connected    cascade    graphs correspond to (a) with x1     x2, (a)
with x2     x1, (b) with x1     x2, (b) with x2     x1, (c) with x1     x2 and (d) with x2     x1. any other
graphs would be cyclic and therefore not distributions).

so that x1 and x2 are indeed independent conditioned on x4.

we would like to have a general algorithm that will allow us to avoid doing such tedious manipulations by
reading the result directly from the graph. to help develop intuition towards constructing such an algorithm,
consider the three variable distribution p(x1, x2, x3). we may write this in any of the 6 ways

p(x1, x2, x3) = p(xi1|xi2, xi3)p(xi2|xi3)p(xi3)

(3.3.12)

where (i1, i2, i3) is any of the 6 permutations of (1, 2, 3). whilst each factorisation produces a di   erent dag,
all represent the same distribution, namely one that makes no independence statements. if the dags are of
the cascade form, no independence assumptions have been made. the minimal independence assumptions
then correspond to dropping a single link in the cascade graph. this gives rise to the 4 dags in    g(3.6).
are any of these graphs equivalent, in the sense that they represent the same distribution? applying bayes   
rule gives :

(cid:124)

(cid:123)(cid:122)

graph(c)

p(x2|x3)p(x3|x1)p(x1)

= p(x2, x3)p(x3, x1)/p(x3) = p(x1|x3)p(x2, x3)

(cid:123)(cid:122)

(cid:125)

(cid:124)

= p(x1|x3)p(x3|x2)p(x2)

= p(x1|x3)p(x2|x3)p(x3)

graph(d)

graph(b)

(cid:123)(cid:122)

(cid:125)

(cid:125)

(cid:124)

(3.3.13)

(3.3.14)

so that dags (b), (c) and (d) represent the same conditional independence (ci) assumptions     given the
state of variable x3, variables x1 and x2 are independent, x1       x2| x3.
however, graph (a) represents something fundamentally di   erent, namely: p(x1, x2) = p(x1)p(x2). there is
no way to transform the distribution p(x3|x1, x2)p(x1)p(x2) into any of the others.

remark 3.4 (graphical dependence). belief network (graphs) are good for encoding conditional indepen-
dence but are not well suited for encoding dependence. for example, consider the graph a     b. this may
appear to encode the relation that a and b are dependent. however, a speci   c numerical instance of a belief
network distribution could be such that p(b|a) = p(b), for which a        b. the lesson is that even when the
dag appears to show    graphical    dependence, there can be instances of the distributions for which depen-
dence does not follow. the same caveat holds for markov networks, section(4.2). we discuss this issue in
more depth in section(3.3.5).

3.3.2 the impact of collisions

de   nition 3.2. given a path p, a
a     c     b. note that a collider is path speci   c, see    g(3.8).

collider is a node c on p with neighbours a and b on p such that

@@

46

draft november 9, 2017

belief networks

x

y

x

y

x

y

z

(a)

z

(b)

z

(c)

x

y

w

z

(d)

(c): variable z is a collider. graphs
figure 3.7: in graphs (a) and (b), variable z is not a collider.
(a) and (b) represent conditional independence x        y| z. in graphs (c) and (d), x and y are    graphically   
conditionally dependent given variable z.

in a general bn, how can we check if x       y| z? in    g(3.7a), x and y are independent when conditioned on
z since

p(x, y|z) = p(x|z)p(y|z)

similarly, for    g(3.7b), x and y are independent conditioned on z since

p(x, y|z)     p(z|x)p(x)p(y|z)

(3.3.15)

(3.3.16)

@@

which is a function of x multiplied by a function of y. in    g(3.7c), however, x and y are graphically dependent
since p(x, y|z)     p(z|x, y)p(x)p(y); in this situation, variable z
is a collider     the arrows of its neighbours
are pointing towards it. what about    g(3.7d)? in (d), when we condition on z, x and y will be graphically
dependent, since

p(x, y|z) =

p(x, y, z)

p(z)

=

1

p(z)

p(z|w)p(w|x, y)p(x)p(y) (cid:54)= p(x|z)p(y|z)

(3.3.17)

(cid:88)

w

@@ the above inequality holds due to the term p(w|x, y)     only in special cases such as p(w|x, y) = const. would
x and y be independent. intuitively, variable w becomes dependent on the value of z, and since x and y are
conditionally dependent on w, they are also conditionally dependent on z.

if there is a non-collider z which is conditioned along the path between x and y (as in    g(3.7)(a,b)), then
this path cannot induce dependence between x and y. similarly, if there is a path between x and y which
contains a collider, provided that this collider is not in the conditioning set (and neither are any of its
descendants) then this path does not make x and y dependent. if there is a path between x and y which
contains no colliders and no conditioning variables, then this path    d-connects    x and y. note that a collider
is de   ned relative to a path. in    g(3.8a), the variable d is a collider along the path a     b     d     c, but not
along the path a     b     d     e (since, relative to this path, the two arrows do not point inwards to d).
consider the bn: a     b     c. here a and c are (unconditionally) independent. however, conditioning
of b makes them    graphically    dependent. intuitively, whilst we believe the root causes are independent,
given the value of the observation, this tells us something about the state of both the causes, coupling them
and making them (generally) dependent.
in de   nition(3.3) below we describe the e   ect that condition-
ing/marginalisation has on the graph of the remaining variables.

de   nition 3.3 (some properties of belief networks). it is useful to understand what e   ect conditioning or
marginalising a variable has on a belief network. we state here how these operations e   ect the remaining
variables in the graph and use this intuition to develop a more complete description in section(3.3.4).

a

b

p(a, b, c) = p(c|a, b)p(a)p(b)

(3.3.18)

c

from a    causal    perspective, this models the    causes    a and b as a priori independent,
both determining the e   ect c.

draft november 9, 2017

47

belief networks

a

b

e

a

e

c

d

(a): the variable d is a collider along the path a     b     d     c,
is a        e| b? a and e
but not along the path a     b     d     e.
are not d-connected since there are no colliders on the only path
between a and e, and since there is a non-collider b which is in the
conditioning set. hence a and e are d-separated by b,     a       e| b.

b

d

c

(b): the variable d is a collider along the path a    d    e, but not
along the path a     b     c     d     e. is a       e| c? there are two paths
between a and e, namely a     d     e and a     b     c     d     e. the
path a    d    e is not blocked since although d is a collider on this
path and d is not in the conditioning set, we have a descendant
of the collider d in the conditioning set, namely c. for the path
a     b     c     d     e, the node c is a collider on this path and c is
in the conditioning set. for this path d is not a collider. hence
this path is not blocked and a and e are (graphically) dependent
given c.

figure 3.8: collider examples for d-separation and d-connection.

b

b

b

marginalising over c makes a and b independent. a
and b are (unconditionally) independent : p(a, b) =
p(a)p(b).
in the absence of any information about
the e   ect c, we retain this belief.

conditioning on c makes a and b (graphically) de-
pendent     in general p(a, b|c) (cid:54)= p(a|c)p(b|c). al-
though the causes are a priori independent, knowing
the e   ect c in general tells us something about how
the causes colluded to bring about the e   ect observed.

conditioning on d, a descendent of a collider c,
makes a and b (graphically) dependent     in general
p(a, b|d) (cid:54)= p(a|d)p(b|d).

p(a, b, c) = p(a|c)p(b|c)p(c)

(3.3.19)

here there is a    cause    c and independent    e   ects    a and b.

b

    a

b

marginalising over c makes a and b (graphically) de-
pendent. in general, p(a, b) (cid:54)= p(a)p(b). although
we don   t know the    cause   , the    e   ects    will neverthe-
less be dependent.

draft november 9, 2017

a

b

c

    a

a

b

c

    a

    a

a

b

c

d

a

b

c

c

a

48

belief networks

a

b

c

c

a

    a

b

conditioning on c makes a and b independent:
p(a, b|c) = p(a|c)p(b|c). if you know the    cause   
c, you know everything about how each e   ect occurs,
independent of the other e   ect. this is also true for
reversing the arrow from a to c     in this case a would
   cause    c and then c    cause    b. conditioning on c
blocks the ability of a to in   uence b.

b

= a
=

b

= a

b

c

c

these graphs all express the same
conditional
assump-
tions.

independence

3.3.3 graphical path manipulations for independence

intuitively, we now have all the tools we need to understand when x is independent of y conditioned on z.
examining the rules in de   nition(3.3), we need to look at each path between x and y. colouring x as red
and y as green and the conditioning node z as yellow, we need to examine each path between x and y and
adjust the edges, following the intuitive rules in    g(3.9).

3.3.4 d-separation

the above description is intuitive. a more formal treatment that is amenable to computational implemen-
tation is straightforward to obtain from these intuitions. first we de   ne the dag concepts of d-separation
and d-connection that are central to determining conditional independence in any bn with structure given
by the dag[305].

de   nition 3.4 (d-connection, d-separation). if g is a directed graph in which x , y and z are disjoint
sets of vertices, then x and y are d-connected by z in g if and only if there exists an undirected path
u between some vertex in x and some vertex in y such that for every collider c on u , either c or a
descendent of c is in z, and no non-collider on u is in z.
x and y are d-separated by z in g if and only if they are not d-connected by z in g.
one may also phrase this as follows. for every variable x     x and y     y, check every path u between x
and y. a path u is said to be blocked if there is a node w on u such that either

1. w is a collider and neither w nor any of its descendants is in z, or
2. w is not a collider on u and w is in z.

if all such paths are blocked then x and y are d-separated by z. if the variable sets x and y are d-separated
by z, they are independent conditional on z in all id203 distributions such a graph can represent.

remark 3.5 (bayes ball). the bayes ball algorithm[259] provides a linear time complexity algorithm
which given a set of nodes x and z determines the set of nodes y such that x       y|z. y is called the set
of irrelevant nodes for x given z.

3.3.5 graphical and distributional in/dependence

we have shown

x and y d-separated by z     x       y|z in all distributions consistent with the belief network structure.

draft november 9, 2017

49

x

x

x

x

x

x

u

z

u

w

z

u

z

u

z

u

z

z

y

y

y

y

y

u

   

   

   

   

   

x

x

x

x

x

u

z

u

w

z

u

z

u

z

u

z

y

y

y

y

y

belief networks

if z is a collider (bottom path) keep undirected links
between the neighbours of the collider.

if z is a descendant of a collider, this could induce
dependence so we retain the links (making them undi-
rected)

if there is a collider not in the conditioning set (upper
path) we cut the links to the collider variable. in this
case the upper path between x and y is    blocked   .

if there is a non-collider which is in the conditioning
set (bottom path), we cut the link between the neigh-
bours of this non-collider which cannot induce depen-
dence between x and y. the bottom path is    blocked   .

in this case, neither path contributes to dependence
and hence x       y| z. both paths are    blocked   .

y

x

u

y

   

w

z

w

whilst z is a collider in the conditioning
set, w is a collider that is not in the con-
ditioning set. this means that there is no
path between x and y, and hence x and y
are independent given z.

figure 3.9: graphical manipulations to determine independence x        y | z. after these manipulations,
if there is no undirected path between x and y, then x and y are independent, conditioned on z. note
that the graphical rules here di   er from those in de   nition(3.3) which considered the e   ect on the graph
having eliminated a variable (via conditioning or marginalisation). here we consider rules for determining
independence based on a graphical representation in which the variables remain in the graph.

in other words, if one takes any instance of a distribution p which factorises according to the belief network
structure and then writes down a list lp of all the conditional independence statements that can be obtained
from p , if x and y are d-separated by z then this list must contain the statement x       y|z. note that
the list lp could contain more statements than those obtained from the graph. for example for the belief
network graph

p(a, b, c) = p(c|a, b)p(a)p(b)

50

(3.3.20)

draft november 9, 2017

belief networks

b

t

g

(a)

f

s

b

t

f

s

g

u

(b)

figure 3.10: (a): t and f are d-connected by g. (b):
b and f are d-separated by u.

which is representable by the dag a     c     b, then a        b is the only graphical independence statement
we can make. consider a distribution consistent with equation (3.3.20), for example, on binary variables
dom(a) = dom(b) = dom(c) = {0, 1}

p[1](c = 1|a, b) = (a     b)2, p[1](a = 1) = 0.3, p[1](b = 1) = 0.4

(3.3.21)

then numerically we must have a       b for this distribution p[1]. indeed the list l[1] contains only the statement
a       b. on the other hand, we can also consider the distribution
p[2](c = 1|a, b) = 0.5, p[2](a = 1) = 0.3, p[2](b = 1) = 0.4

(3.3.22)

from which l[2] = {a       b, a       c, b       c}. in this case l[2] contains more statements than a       b.
an interesting question is whether or not d-connection similarly implies dependence? that is, do all distri-
butions p consistent with the belief network possess the dependencies implied by the graph? if we consider
the belief network structure equation (3.3.20) above, a and b are d-connected by c, so that graphically a and
b are dependent, conditioned on c. for the speci   c instance p[1] we have numerically a(cid:62)(cid:62)b| c so that the list
of dependence statements for p[1] contains the graphical dependence statement. now consider p[2]. the list
of dependence statements for p[2] is empty. hence the graphical dependence statements are not necessarily
found in all distributions consistent with the belief network. hence

x and y d-connected by z (cid:54)    x(cid:62)(cid:62)y|z in all distributions consistent with the belief network structure.
see also exercise(3.17). this shows that belief networks are powerful in ensuring that distributions necessarily
obey the independence assumptions we expect from the graph. however, belief networks are not suitable
for ensuring that distributions obey desired dependency statements.

example 3.3. consider the graph in    g(3.10a).

1. are the variables t and f unconditionally independent, i.e. t        f|    ? here there are two colliders,
namely g and s     however, these are not in the conditioning set (which is empty), and hence t and f
are d-separated and therefore unconditionally independent.

2. what about t       f| g? there is a path between t and f for which all colliders are in the conditioning
set. hence t and f are d-connected by g, and therefore t and f are graphically dependent conditioned
on g.

example 3.4. is {b, f}       u|    in    g(3.10b)? since the conditioning set is empty and every path from either
b or f to u contains a collider, b and f are unconditionally independent of u.

3.3.6 markov equivalence in belief networks

we have invested a lot of e   ort in learning how to read conditional independence relations from a dag.
happily, we can determine whether two dags represent the same set of conditional independence statements
(even when we don   t know what they are) by using a relatively simple rule.

draft november 9, 2017

51

belief networks

t1

y1

t2

y2

h

(a)

t2

y2

t1

y1

(b)

figure 3.11: (a): two treatments t1, t2 and corresponding
outcomes y1, y2. the health of a patient is represented by
h. this dag embodies the conditional independence state-
ments t1        t2, y2|    , t2        t1, y1|    , namely that the treat-
ments have no e   ect on each other. (b): one could repre-
sent the e   ect of marginalising over h using a bi-directional
edge.

de   nition 3.5 (markov equivalence). two graphs are markov equivalent if they both represent the same
set of conditional independence statements. this de   nition holds for both directed and undirected graphs.

example 3.5. consider the belief network with edges a     c     b, from which the set of conditional
independence statements is a       b|   . for another belief network with edges a     c     b and a     b, the
set of conditional independence statements is empty. in this case, the two belief networks are not markov
equivalent.

procedure 3.1 (determining markov equivalence). de   ne an immorality in a dag as a con   guration of
three nodes, a, b, c such that c is a child of both a and b, with a and b not directly connected. de   ne
the skeleton of a graph by removing the directions on the arrows. two dags represent the same set of
independence assumptions (they are markov equivalent) if and only if they have the same skeleton and the
same set of immoralities [79].

using procedure(3.1) we see that in    g(3.6), bns (b,c,d) have the same skeleton with no immoralities and
are therefore equivalent. however bn (a) has an immorality and is therefore not equivalent to bns (b,c,d).
note that whether unmarried parents having children should be considered immoral is beyond the scope of
this book!

++

3.3.7 belief networks have limited expressibility

belief networks    t well with our intuitive notion of modelling    causal    independencies. however, formally
speaking they cannot necessarily graphically represent all the independence properties of a given distribu-
tion.

consider the dag in    g(3.11a), (from [250]). this dag could be used to represent two successive ex-
periments where t1 and t2 are two treatments and y1 and y2 represent two outcomes of interest; h is the
underlying health status of the patient; the    rst treatment has no e   ect on the second outcome hence
there is no edge from y1 to y2. now consider the implied independencies in the marginal distribution
p(t1, t2, y1, y2), obtained by marginalising the full distribution over h. there is no dag containing only
the vertices t1, y1, t2, y2 which represents the independence relations and does not also imply some other
independence relation that is not implied by    g(3.11a). consequently, any dag on vertices t1, y1, t2, y2
alone will either fail to represent an independence relation of p(t1, t2, y1, y2), or will impose some additional
independence restriction that is not implied by the dag. in the above example

p(t1, t2, y1, y2) = p(t1)p(t2)

p(y1|t1, h)p(y2|t2, h)p(h)

(3.3.23)

(cid:88)

h

cannot in general be expressed as a product of functions de   ned on a limited set of the variables. however,
it is the case that the conditional independence conditions t1       (t2, y2), t2       (t1, y1) hold in p(t1, t2, y1, y2)    
draft november 9, 2017
52

causality

they are there, encoded in the form of the id155 tables. it is just that we cannot    see    this
independence since it is not present in the structure of the marginalised graph (though one can naturally
infer this in the larger graph p(t1, t2, y1, y2, h)). for example, for the bn with link from y2 to y1, we have
@@ y1        t2| y2, which is not true for the distribution in (3.3.23). similarly, for the bn with link from y1 to y2,
@@

the implied statement

y2       t1| y1 is also not true for (3.3.23).

this example demonstrates that bns cannot express all the conditional independence statements that could
be made on that set of variables (the set of conditional independence statements can be increased by consid-
ering additional variables however). this situation is rather general in the sense that any graphical model
has limited expressibility in terms of independence statements[282]. it is worth bearing in mind that bns
may not always be the most appropriate framework to express one   s independence assumptions and intu-
itions.

a natural consideration is to use a bi-directional arrow when a variable is marginalised. for    g(3.11a), one
could depict the marginal distribution using a bi-directional edge,    g(3.11b). for a discussion of extensions
of bns using bi-directional edges see [250].

3.4 causality

causality is a contentious topic and the purpose of this section is to make the reader aware of some pitfalls
that can occur and which may give rise to erroneous id136s. the reader is referred to [238] and [79] for
further details.

the word    causal    is contentious particularly in cases where the model of the data contains no explicit tem-
poral information, so that formally only correlations or dependencies can be inferred. for a distribution
p(a, b), we could write this as either (i) p(a|b)p(b) or (ii) p(b|a)p(a). in (i) we might think that b    causes   
a, and in (ii) a    causes    b. clearly, this is not very meaningful since they both represent exactly the same
distribution, see    g(3.12). formally bns only make independence statements, not causal ones. neverthe-
less, in constructing bns, it can be helpful to think about dependencies in terms of causation since our
intuitive understanding is usually framed in how one variable    in   uences    another. first we discuss a classic
conundrum that highlights potential pitfalls that can arise.

3.4.1 simpson   s paradox

simpson   s    paradox    is a cautionary tale in causal reasoning in bns. consider a medical trial in which
patient treatment and outcome are recovered. two trials were conducted, one with 40 females and one with
40 males. the data is summarised in table(3.1). the question is : does the drug cause increased recovery?
according to the table for males, the answer is no, since more males recovered when they were not given the
drug than when they were. similarly, more females recovered when not given the drug than recovered when
given the drug. the conclusion appears that the drug cannot be bene   cial since it aids neither subpopulation.

however, ignoring the gender information, and collating both the male and female data into one combined
table, we    nd that more people recovered when given the drug than when not. hence, even though the
drug doesn   t seem to work for either males or females, it does seem to work overall! should we therefore
recommend the drug or not?

a

b

a

b

r

w

r

w

(a)

(b)

(c)

(d)

figure 3.12: both (a) and (b) represent the same distribution p(a, b) = p(a|b)p(b) = p(b|a)p(a).
the graph represents p(rain, grasswet) = p(grasswet|rain)p(rain).
p(rain|grasswet)p(grasswet), although this appears to be causally non-sensical.

(c):
(d): we could equally have written

draft november 9, 2017

53

causality

males

recovered not recovered rec. rate

given drug

not given drug

18
7

12
3

60%
70%

females

recovered not recovered rec. rate

given drug

not given drug

2
9

8
21

20%
30%

combined
given drug

not given drug

recovered not recovered rec. rate

20
16

20
24

50%
40%

table 3.1: table for simpson   s paradox (from [238])

g

d

r

fd

(a)

g

d

(b)

resolution of the paradox

(a): a dag for the relation be-
figure 3.13:
tween gender (g), drug (d) and recovery (r), see
table(3.1). (b): in   uence diagram. no decision vari-
able is required for g since g has no parents.

r

the    paradox    occurs because we are asking a causal (interventional) question     if we give someone the drug,
what happens?     but we are performing an observational calculation. pearl[238] would remind us that there
is a di   erence between    given that we see    (observational evidence) and    given that we do    (interventional
evidence). we want to model a causal experiment in which we    rst intervene, setting the drug state, and
then observe what e   ect this has on recovery.

a model of the gender, drug and recovery data (which makes no conditional independence assumptions)
is,    g(3.13a),

p(g, d, r) = p(r|g, d)p(d|g)p(g)

(3.4.1)

in a causal interpretation, however, if we intervene and give the drug, then the term p(d|g) in equation
(3.4.1) should play no role in the experiment     we decide to give the drug or not independent of gender.
the term p(d|g) therefore needs to be replaced by a term that re   ects the set-up of the experiment. we
use the idea of an atomic intervention, in which a single variable is set in a particular state. in our atomic
causal intervention, where we set d, we deal with the modi   ed distribution

  p(g, r|d) = p(r|g, d)p(g)

(3.4.2)

where the terms on the right hand side of this equation are taken from the original bn of the data. to
denote an intervention we use ||:
p(r||g, d)       p(r|g, d) =

= p(r|g, d)

(cid:80)

(3.4.3)

p(r|g, d)p(g)
r p(r|g, d)p(g)

(one can also consider here g as being interventional     in this case it doesn   t matter since the fact that the
variable g has no parents means that for any distribution conditional on g, the prior factor p(g) will not
be present). using equation (3.4.3), for the males given the drug 60% recover, versus 70% recovery when
not given the drug. for the females given the drug 20% recover, versus 30% recovery when not given the drug.

similarly,

p(r||d)       p(r|d) =

(cid:80)
(cid:80)

g p(r|g, d)p(g)
r,g p(r|g, d)p(g)

(cid:88)

g

=

p(r|g, d)p(g)

(3.4.4)

54

draft november 9, 2017

causality

using the post intervention distribution, equation (3.4.4), we have

p(recovery|drug) = 0.6    0.5 + 0.2    0.5 = 0.4
p(recovery|no drug) = 0.7    0.5 + 0.3    0.5 = 0.5

(3.4.5)

(3.4.6)

hence we infer that the drug is overall not helpful, as we intuitively expect, and is consistent with the
results from both subpopulations.

++

summarising the above argument, p(g, d, r) = p(r|g, d)p(g)p(d) means that we choose either a male or
female patient and give them the drug or not independent of their gender, hence the absence of the term
p(d|g) from the joint distribution.
if we were to do a purely observation calculation p(r|d), the condi-
tioning on d induces a dependency on gender (through the term p(d|g) and hence on the recovery. this
setup would correspond to the doctor determining    rst whether or not to give the drug, and then selecting
a gender at random according to p(g|d), which is not how we would expect a    drug trial    to proceed. doing
a random trial in which the drug is simply given (or not) at random, according to p(d) would break the
dependency on the gender and correspond to a sensible interventional experiment. one way to think about
such models is to consider how to draw a sample from the joint distribution of the random variables     in
most cases this should clarify the role of causality in the experiment.

in contrast to the interventional calculation, the observational calculation makes no conditional independence
assumptions. this means that, for example, the term p(d|g) plays a role in the calculation (the reader
might wish to verify that the result given in the combined data in table(3.1) is equivalent to inferring with
the full distribution equation (3.4.1)).

3.4.2 the do-calculus

in making causal id136s we   ve seen above that we must adjust the model to re   ect any causal experi-
mental conditions. in setting any variable into a particular state we need to surgically remove all parental
links of that variable. pearl calls this the do operator , and contrasts an observational (   see   ) id136 p(x|y)
with a causal (   make    or    do   ) id136 p(x|do(y)).

de   nition 3.6 (pearl   s do operator).

intervention variables x   c. for a belief network p(x ) = (cid:81)

@@ let all the variables x = xc     x   c be written in terms of the intervention variables xc and the non-
i p(xi|pa (xi)), inferring the e   ect of setting
variables xc1, . . . , xck , ck     c, in states xc1, . . . , xck , is equivalent to standard evidential id136 in the
post intervention distribution:

@@

p(x   c|do(xc1 = xc1), . . . , do(xck = xck )) =

p (xj|pa (xj))

(3.4.7)

(cid:89)

j      c

@@ where any parental variable included in the intervention set is set to its intervention state. an alternative
@@

notation is p(

x   c||xc1, . . . , xck ).

@@

in words, for those variables for which we causally intervene and set in a particular state, the corresponding
terms p(xci|pa (xci)) are removed from the original belief network.
graphically, the e   ect is to consider each
intervention variable, cut the connections to its parents and set the intervention variable to its intervention
state. for variables which are evidential but non-causal, the corresponding factors are not removed from
the distribution. the interpretation is that the post intervention distribution corresponds to an experiment
in which the causal variables are    rst set and non-causal variables are subsequently observed.

++ for a belief network to have a causal interpretation, it means that the ancestral order of the variables must
correspond to the temporal order. this means therefore that if we start with the variables that have no
parents, these must come    rst in time, with their children coming later, etc. ancestral sampling from a
causal belief network therefore corresponds to the temporal evolution of a physical experiment.

draft november 9, 2017

55

causality

3.4.3 in   uence diagrams and the do-calculus

another way to represent intervention is to modify the basic bn by appending a parental decision variable
fx to any variable x on which an intervention can be made, giving rise to a so-called in   uence diagram[79].
for example2, for the simpson   s paradox example, we may use,    g(3.13b),

  p(d, g, r, fd) = p(d|fd, g)p(g)p(r|g, d)p(fd)

(3.4.8)

where

p(d|fd =    , g)     p(d|pa (d)), p(d|fd = d, g) = 1 for d = d and 0 otherwise

hence, if the decision variable fd is set to the empty state, the variable d is determined by the standard
observational term p(d|pa (d)). if the decision variable is set to a state of d, then the variable puts all its
id203 in that single state of d = d. this has the e   ect of replacing the id155 term by
a unit factor and any instances of d set to the variable in its interventional state3. a potential advantage
of this in   uence diagram approach over the do-calculus is that conditional independence statements can be
derived using standard techniques for the augmented bn. additionally, for learning, standard techniques
apply in which the decision variables are set to the condition under which each data sample was collected
(a causal or non-causal sample).

remark 3.6 (learning the edge directions). in the absence of data from causal experiments, one should
be justi   ably sceptical about learning    causal    networks. nevertheless, one might prefer a certain direction
of a link based on assumptions of the    simplicity    of the cpts. this preference may come from a physical
intuition that whilst root causes may be uncertain, the relationship from cause to e   ect is
relatively simple.
in this sense a measure of the complexity of a cpt is required, such as id178.
similarly, a useful heuristic
is that root causes are independent and, amongst equally likely belief networks, the one with the smallest
number of edges will correspond to the    causal    one. such heuristics can be numerically encoded and the
edge directions

see also exercise(12.6).

learned.

@@
++

@@

3.5 summary

    we can reason with certain or uncertain evidence using repeated application of bayes    rule.
    a belief network represents a factorisation of a distribution into conditional probabilities of variables de-

pendent on parental variables.

    belief networks correspond to directed acyclic graphs.
    variables are conditionally independent x       y| z if p(x, y|z) = p(x|z)p(y|z); the absence of a link in a belief

network corresponds to a conditional independence statement.

    if in the graph representing the belief network, two variables are independent, then they are independent

in any distribution consistent with the belief network structure.
    belief networks are natural for representing    causal    in   uences.
    causal questions must be addressed by an appropriate causal model.

2here the in   uence diagram is a distribution over variables including decision variables, in contrast to the application of

ids in chapter(7).

3more general cases can be considered in which the variables are placed in a distribution of states [79].

56

draft november 9, 2017

exercises

d

u

h

p

a

figure 3.14: party animal. here all variables are binary.
p = been to
party, h = got a headache, d = demotivated at work, u = underperform
at work, a =boss angry. shaded variables are observed in the true state.

3.6 code

3.6.1 naive id136 demo

demoburglar.m: was it the burglar demo
demochestclinic.m: naive id136 on chest clinic. see exercise(3.4).

3.6.2 conditional independence demo

the following demo determines whether x        y | z for the chest clinic network,    g(3.15), and checks
the result numerically4. the independence test is based on the markov method of section(4.2.4). this
is an alternative to the d-separation method and also more general in that it deals also with conditional
independence in markov networks as well as belief networks. running the demo code below, it may happen
that the numerical dependence is very small     that is

p(x ,y|z)     p(x|z)p(y|z)

(3.6.1)

even though x(cid:62)(cid:62)y|z. this highlights the di   erence between    structural    and    numerical    independence.
condindeppot.m: numerical measure of conditional independence
democondindep.m: demo of conditional independence (using markov method)

3.6.3 utility routines

dag.m: find the dag structure for a belief network

3.7 exercises

exercise 3.1 (party animal). the party animal problem corresponds to the network in    g(3.14). the boss
is angry and the worker has a headache     what is the id203 the worker has been to a party? to complete
the speci   cations, the probabilities are given as follows:
p(u = tr|p = tr, d = tr) = 0.999
p(u = tr|p = tr, d = fa) = 0.9
p(a = tr|u = tr) = 0.95
exercise 3.2. consider the distribution p(a, b, c) = p(c|a, b)p(a)p(b). (i) is a       b|   ? (ii) is a       b| c?
exercise 3.3. the chest clinic network[185] concerns the diagnosis of lung disease (tuberculosis, lung
cancer, or both, or neither), see    g(3.15). in this model a visit to asia is assumed to increase the id203
of tuberculosis. state if the following conditional independence relationships are true or false

p(u = tr|p = fa, d = tr) = 0.9
p(u = tr|p = fa, d = fa) = 0.01
p(a = tr|u = fa) = 0.5

p(h = tr|p = tr) = 0.9
p(h = tr|p = fa) = 0.2
p(p = tr) = 0.2, p(d = tr) = 0.4

1. tuberculosis       smoking| shortness of breath
2. lung cancer       bronchitis| smoking
3. visit to asia       smoking| lung cancer
4. visit to asia       smoking| lung cancer, shortness of breath
4the code for graphical conditional independence is given in chapter(4).

draft november 9, 2017

57

exercises

a

t

x

l

e

b

s

d

x = positive x-ray

d = dyspnea (shortness of breath)

e = either tuberculosis or lung cancer

t = tuberculosis

l = lung cancer

b = bronchitis

a = visited asia

s = smoker

figure 3.15: belief network structure for the chest clinic example.

battery

fuel

gauge

figure 3.16: belief network of car starting, see exercise(3.6).

turn over

start

exercise 3.4. consider the chest clinic belief network in    g(3.15)[185]. calculate by hand the values for
p(d), p(d|s = tr), p(d|s = fa). the table values are:

p(a = tr)
p(t = tr|a = tr)
p(l = tr|s = tr)
p(b = tr|s = tr)
p(x = tr|e = tr)
p(d = tr|e = tr, b = tr) = 0.9
p(d = tr|e = fa, b = tr) = 0.8

= 0.01 p(s = tr)
= 0.5
= 0.05 p(t = tr|a = fa)
= 0.01
p(l = tr|s = fa)
= 0.1
= 0.01
p(b = tr|s = fa)
= 0.6
= 0.3
= 0.98 p(x = tr|e = fa)
= 0.05
p(d = tr|e = tr, b = fa) = 0.7
p(d = tr|e = fa, b = fa) = 0.1

p(e = tr|t, l) = 0 only if both t and l are fa, 1 otherwise.
exercise 3.5. if we interpret the chest clinic network exercise(3.4) causally, how can we help a doctor
answer the question    if i could cure my patients of bronchitis, how would this a   ect my patients    chance of
being short of breath?   . how does this compare with p(d = tr|b = fa) in a non-causal interpretation, and
what does this mean?

exercise 3.6 ([141]). the network in    g(3.16) concerns the id203 of a car starting, with

p(f = empty) = 0.05

p(b = bad) = 0.02
p(g = empty|b = good, f = not empty) = 0.04 p(g = empty|b = good, f = empty) = 0.97
p(g = empty|b = bad, f = empty) = 0.99
p(g = empty|b = bad, f = not empty) = 0.1
p(t = fa|b = bad) = 0.98
p(t = fa|b = good) = 0.03
p(s = fa|t = tr, f = not empty) = 0.01
p(s = fa|t = tr, f = empty) = 0.92
p(s = fa|t = fa, f = not empty) = 1.0
p(s = fa|t = fa, f = empty) = 0.99

calculate p (f = empty|s = no), the id203 of the fuel tank being empty conditioned on the observation
that the car does not start.

exercise 3.7. there is a synergistic relationship between asbestos (a) exposure, smoking (s) and cancer
(c). a model describing this relationship is given by

p(a, s, c) = p(c|a, s)p(a)p(s)

1. is a       s|   ?

58

(3.7.1)

draft november 9, 2017

exercises

2. is a       s| c?
3. how could you adjust the model to account for the fact that people who work in the building industry

have a higher likelihood to also be smokers and also a higher likelihood to asbestos exposure?

exercise 3.8.

consider the belief network on the right which represents mr
holmes    burglary worries as given in    g(3.3a) :
(b)urglar,
(a)larm, (w)atson, mrs (g)ibbon.

all variables are binary with states {tr, fa}. the table entries are

= 0.01

p(b = tr)
p(a = tr|b = tr) = 0.99 p(a = tr|b = fa) = 0.05
p(w = tr|a = fa) = 0.5
p(w = tr|a = tr) = 0.9
p(g = tr|a = tr) = 0.7
p(g = tr|a = fa) = 0.2

1. compute    by hand    (i.e. show your working) :

(a) p(b = tr|w = tr)
(b) p(b = tr|w = tr, g = fa)

b

a

g

w

(3.7.2)

2. consider the same situation as above, except that now the evidence is uncertain. mrs gibbon thinks
that the state is g = fa with id203 0.9. similarly, dr watson believes in the state w = fa with
value 0.7. compute    by hand    the posteriors under these uncertain (soft) evidences:
(a) p(b = tr|   w )
(b) p(b = tr|   w ,   g)

exercise 3.9. a doctor gives a patient a (d)rug (drug or no drug) dependent on their (a)ge (old or young)
and (g)ender (male or female). whether or not the patient (r)ecovers (recovers or doesn   t recover) depends
on all d, a, g. in addition a       g|   .

1. write down the belief network for the above situation.

2. explain how to compute p(recover|drug).
3. explain how to compute p(recover|do(drug), young).

exercise 3.10. implement the wet grass scenario in section(3.1.1) using the brmltoolbox.

exercise 3.11 (la burglar). consider the burglar scenario, example(3.1). we now wish to model the
fact that in los angeles the id203 of being burgled increases if there is an earthquake. explain how to
include this e   ect in the model.

exercise 3.12. given two belief networks represented as dags with associated adjacency matrices a and
b, write a matlab function markovequiv(a,b).m that returns 1 if a and b are markov equivalent, and
zero otherwise.

exercise 3.13. the adjacency matrices of two belief networks are given below (see abmatrices.mat). state
if they are markov equivalent.

                                 

0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0

1
1
0
0
1
0
0
0
0

1
0
0
0
0
1
0
0
0

0
1
0
0
0
0
0
0
0

1
0
0
0
0
0
0
0
0

0
0
1
0
1
0
0
0
0

0
0
0
1
0
1
0
0
0

0
0
0
1
0
0
1
0
0

                                  ,

                                 

                                 

0
0
0
0
0
1
0
0
0

0
0
0
0
1
0
0
0
0

1
1
0
0
1
0
0
0
0

1
0
0
0
0
1
0
0
0

0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0

0
0
1
0
1
0
0
0
0

0
0
0
1
0
1
0
0
0

0
0
0
1
0
0
1
0
0

a =

b =

(3.7.3)

59

draft november 9, 2017

exercises

exercise 3.14. there are three computers indexed by i     {1, 2, 3}. computer i can send a message in
one timestep to computer j if cij = 1, otherwise cij = 0. there is a fault in the network and the task is
to    nd out some information about the communication matrix c (c is not necessarily symmetric). to do
this, thomas, the engineer, will run some tests that reveal whether or not computer i can send a message
to computer j in t timesteps, t     {1, 2}. this is expressed as cij(t), with cij(1)     cij. for example, he
might know that c13(2) = 1, meaning that according to his test, a message sent from computer 1 will arrive
at computer 3 in at most 2 timesteps. note that this message could go via di   erent routes     it might go
directly from 1 to 3 in one timestep, or indirectly from 1 to 2 and then from 2 to 3, or both. you may
assume cii = 1. a priori thomas thinks there is a 10% id203 that cij = 1, i (cid:54)= j, and assumes that
each such connection is independent of the rest. given the test information c = {c12(2) = 1, c23(2) = 0},
compute the a posteriori id203 vector

[p(c12 = 1|c), p(c13 = 1|c), p(c23 = 1|c), p(c32 = 1|c), p(c21 = 1|c), p(c31 = 1|c)]

(3.7.4)

exercise 3.15. a belief network models the relation between the variables oil, inf, eh, bp, rt which stand for
the price of oil, in   ation rate, economy health, british petroleum stock price, retailer stock price. each
variable takes the states low, high, except for bp which has states low, high, normal. the belief network model
for these variables has tables

p(eh=low)=0.2
p(bp=low|oil=low)=0.9
p(bp=low|oil=high)=0.1
p(oil=low|eh=low)=0.9
p(rt=low|inf=low,eh=low)=0.9
p(rt=low|inf=high,eh=low)=0.1
p(inf=low|oil=low,eh=low)=0.9
p(inf=low|oil=high,eh=low)=0.1

p(bp=normal|oil=low)=0.1
p(bp=normal|oil=high)=0.4
p(oil=low|eh=high)=0.05
p(rt=low|inf=low,eh=high)=0.1
p(rt=low|inf=high,eh=high)=0.01
p(inf=low|oil=low,eh=high)=0.1
p(inf=low|oil=high,eh=high)=0.01

1. draw a belief network for this distribution.

2. given that the bp stock price is normal and the retailer stock price is high, what is the id203 that

in   ation is high?

exercise 3.16. there are a set of c potentials with potential c de   ned on a subset of variables xc. if
xc     xd we can merge (multiply) potentials c and d since the variables in potential c are contained within
potential d. with reference to suitable graph structures, describe an e   cient algorithm to merge a set of
potentials so that for the new set of potentials no potential is contained within the other.

exercise 3.17. this exercise explores the distinction between d-connection and dependence. consider the
distribution class

p(a, b, c) = p(c|b)p(b|a)p(a)

(3.7.5)

for which a is d-connected to c. one might expect that this means that a and c are dependent, a(cid:62)(cid:62)c. our
interest is to show that there are non-trivial distributions for which a       c.

1. consider dom(a) = dom(c) = {1, 2} and dom(b) = {1, 2, 3}. for

(cid:19)

(cid:18) 3/5

2/5

p(a) =

,

p(b|a) =

       1/4

1/12
2/3

       ,

15/40
1/8
1/2

(cid:18) 1/3 1/2 15/40

(cid:19)

2/3 1/2

5/8

p(c|b) =

show that a       c.

2. consider

p(a, b, c) =

for positive function   ,    and z =(cid:80)

  (a, b)  (b, c)

1
z

a,b,c   (a, b)  (b, c). de   ning matrices m and n with elements

mij =   (a = i, b = j),

nkj =   (b = j, c = k)

(3.7.8)

60

draft november 9, 2017

(3.7.6)

(3.7.7)

exercises

show that the marginal distribution p(a = i, c = k) is represented by the matrix elements

(cid:104)
mnt(cid:105)

ik

p(a = i, c = k) =

1
z

3. show that if

mnt = m0nt
0

for some vectors m0 and n0, then a       c.

4. writing

m = [m1 m2 m3] ,

n = [n1 n2 n3]

for two dimensional vectors mi, ni, i = 1, . . . , 3, show that

mnt = m1nt

1 + m2nt

2 + m3nt
3

5. show that by setting

m2 =   m1, n3 =    (n1 +   n2)

for scalar   ,    then mnt can be written as m0nt

0 where

m0     m1 +   m3,

n0     n1 +   n2

(3.7.9)

(3.7.10)

(3.7.11)

(3.7.12)

(3.7.13)

(3.7.14)

@@

6. hence construct example tables

(di   erent from those speci   ed in part 1 above) p(a), p(b|a), p(c|b) for

which a       c. verify your examples explicitly using brmltoolbox.

exercise 3.18. alice and bob share a bank account which contains an a priori unknown total amount
of money t . whenever alice goes to the cash machine, the available amount for withdrawal a for alice is
always 10% of the total t . similarly, when bob goes to the cash machine the available amount for withdrawal
b for bob is 10% of the total t . whatever the amount in the bank, alice and bob check their available
amounts for withdrawal independently. draw a belief network that expresses this situation and show that
a(cid:62)(cid:62)b.
exercise 3.19. assume that the day of the week that females are born on, x, is independent of the day of
the week, y, on which males are born. assume, however, that the old rhyme is true and that personality
is dependent on the day of the week you   re born on. if a represents the female personality type and b the
male personality type, then a(cid:62)(cid:62)x and b(cid:62)(cid:62)y, but a       b. whether or not a male and a female are married, m,
depends strongly on their personality types, m(cid:62)(cid:62){a, b}, but is independent of x and y if we know a and b.
draw a belief network that can represent this setting. what can we say about the (graphical) dependency
between the days of the week that john and jane are born on, given that they are not married?

++

exercise 3.20.
a survey of households in which the husband and wife each own their own car is made.
the survey also states whether each household income (inc) is high or low. there are 4 car types, dom(h) =
dom(w) = {1, 2, 3, 4}, the    rst two being    cheap    and the last two being    expensive   . the survey    nds that,
given their household income, the types of cars owned by a husband and wife are independent:

wife   s car type       husband   s car type| family income

speci   cally, p(inc = low) = 0.8 and

             0.7
             0.2

0.3
0
0

0.8
0
0

             , p(w|inc = high) =
             0.2
             0
             , p(h|inc = high) =

0.1
0.4
0.3

            
            

0
0.3
0.7

p(w|inc = low) =

p(h|inc = low) =

use brmltoolbox to    nd the marginal p(w, h) and show that whilst h       w| inc, it is not the case that h       w.
draft november 9, 2017
61

exercises

exercise 3.21.
a, b, c that have been competing in a tournament, with the following outcomes:

bingame is a two-player game with only a    win    or    lose    outcome. there are three players,

++

    a beat b 2 times.
    b beat c 2 times.
    a beat c 2 times.
    c beat a 1 time.
    player d enters the tournament and loses twice to player c.

we assume that each player has a skill level from 1, . . . , 10 and that, given the skill levels of two players, sa,
sb, the id203 that player a beats player b is 1/(1 + exp(sb     sa)). assume that a priori the skill levels
of the players are independent and that we place equal id203 to each of the 10 skills levels. additionally,
the outcome of all games, given the skill levels are independent. using the above tournament information :

1. draw a belief network that represents the independence assumptions above.

2. are the skill levels of the players a posteriori (i.e. given the game outcomes) independent?

3. calculate the id203 that player d will beat player a in a game of bingame.

4. calculate the expected skill level of each of the 4 players.

before watching a video for free on their website, the daily fail forces you to watch a
exercise 3.22.
video advert. there are in fact 10 adverts available and the daily fail randomly selects three advert links
to present to each viewer. after clicking on one of the three advert links, the advert plays, after which the
desired news video can be watched. the data from the daily fail from 20 website visitors is as follows:

++

(1, 2, 3)(2, 5, 3)(4, 7, 10)(6, 3, 4)(6, 8, 5)(9, 3, 7)(10, 2, 4)(7, 1, 2)(8, 7, 2)(8, 3, 6)

(8, 6, 4)(4, 3, 9)(5, 4, 1)(9, 5, 1)(6, 7, 8)(4, 9, 7)(10, 8, 6)(5, 4, 3)(6, 3, 2)(1, 4, 2)

where (ai, aj, ak) means that advert i was clicked on in preference to adverts j and k.

assume that each advert has an implicit    interest    value s     {1, . . . , 5} which a   ects the id203 that
viewers will click on the video. if sa, sb, sc are the interest levels of the three randomly chosen adverts
a, b, c, then the id203 that the viewer clicks on advert a is assumed to be proportional to exp(sa    
max(sb, sc)). assuming that the interest levels are a priori independent and uniform and that each viewer
clicks on adverts independently of the others (given the advert interest levels), calculate the expected interest
levels in each advert.

exercise 3.23.
is played by player1 and player2. the sequences of moves x1
r, paper = p, scissors = s) by both players is

the game rockpaperscissors https: // en. wikipedia. org/ wiki/ rock-paper-scissors
t being one of rock =

1:t (each move xi

1:t , x2

++

player1 = [r p r p r s p r s p p r r r r p r s r p p s r]

player2 = [s p r s p s p s r p s r p p r r s p r s s p r]

assume that player1 plays according to a    rst order markov chain, de   ned by

p(x1

2:t|x2

1:t   1) =

p(x1

t|x1

t   1, x2

t   1)

(3.7.15)

where x1

t     {r, p, s} and x2

t     {r, p, s} are the moves at time t by player 1 and player 2 respectively.

1. calculate the id203 given in equation (3.7.15) for the data above.

2. how much more likely is it that player 1 is playing according to this strategy, compared to playing

random moves?

62

draft november 9, 2017

t(cid:89)

t=2

exercises

3. calculate the id203 that player 1 plays each of rock, paper, scissors at the next time step t = t +1.

4. what would be the best move for player 2 to make at time t = t + 1?

where t = 23.

++

exercise 3.24.

consider the belief network

p1(x1, y1, x2, y2, x3, y3)     p(x1|y1)p(y1)p(x2|x1, y2)p(y2)p(x3|x2, y3)p(y3)

1. show that p1 can be written as

p2(x1, y1, x2, y2, x3, y3)     p(x3, y3)p(x2, y2|x3, y3)p(x1, y1|x2, y2)

2. show that this can be further simpli   ed to

p3(x1, y1, x2, y2, x3, y3) = p(y1|x1)p(x1|x2, y2)p(y2|x2)p(x2|x3, y3)p(x3, y3)

(3.7.16)

(3.7.17)

(3.7.18)

3. draw the belief networks for p1 and p3 and show that the number of edges in p3 is larger than in p1
(even though they represent the same distribution). this can be considered a form of causal heuristic.
representation p1 is the causal one in which root causes are independent. in representation p3, the
number of edges is greater than in p1. a heuristic for    nding the    right    causal graph is to select the
graph (amongst all those that are equally likely) that has the least number of edges.

++

exercise 3.25.
consider a    physical    system on a collection of variables xt = x1,t, . . . , x4,t where xi,t     {0, 1}
is state of spacial variable i     {1, 2, 3, 4} at time t. according to the laws of physics, only    nearest    spatial
and temporal variables are relevant for the dynamics. the dynamics is therefore encoded as the    causal   
distribution

p(x1:t ) = p(x1,1)p(x2,1)p(x3,1)p(x4,1)

t(cid:89)

t=2

  

p(x1,t|x1,t   1, x2,t   1)p(x2,t|x1,t   1, x2,t   1, x3,t   1)p(x3,t|x2,t   1, x3,t   1, x4,t   1)p(x4,t|x3,t   1, x4,t   1)
(3.7.19)

1. draw a belief network for p(x1:t ).

2. show that p(x1:t ) can also be written in the    time reversed    form

p(xt )

p(xt|xt+1)

(3.7.20)

and draw a belief network on the collection of variables {x1,t, x2,t, x3,t, x4,t}, t = 1, . . . , t for this
alternative representation of p(x1:t ).

3. show that, in general, the number of edges in the    causal    representation is 10    (t     1), so that in
the limit t        , the number of edges per timestep is 10. show that in the time reversed form, the
average number of edges per timestep approaches 15 as t        .

++

exercise 3.26.
students who take the machine learning masters degree have to take the project module
and then select 8 additional modules from the list: id114, research methods, applied machine
learning, natural language modelling, bioinformatics, information retrieval, machine vision, advanced
topics, a   ective computing, computational modelling. each student has, a priori, certain skills in maths,
programming, organisation, biology and writing. these skills are a priori independent and the id203
that a student is skilled in maths is 0.95, programming 0.8, organisation 0.7, biology 0.3, writing 0.95.

the id203 of passing a module depends on certain skills. the id203 of passing some modules de-
pends on two skills, whereas others depend on only one. for example, the id203 of passing graphical

draft november 9, 2017

63

t   1(cid:89)

t=1

exercises

models is 0.99 if the student is skilled in maths and programming, 0.75 is she is skilled in only one of these,
and 0.01 if she is skilled in neither. the id203 of passing information retrieval is 0.995 if the student
is skilled in programming and 0.1 otherwise. other two-skill modules are research methods (depends on
programming and organisation), bioinformatics (biology and writing), applied machine learning (maths
and programming), natural language modelling (maths and programming) and    nally the project (writ-
ing and organisation). the remaining modules depend on only one skill: information retrieval (depends
on programming), machine vision (maths), advances topics (maths), a   ective computing (writing) and
computational modelling (maths). assume that the id203 of passing the modules are all independent,
given the skills of the student. additionally, assume that each two-skill module has the same id203
values as for id114 and each one-skill module has the same id203 values as for information
retrieval.

provided that the student passes the project and each of the 8 modules she selects, the student will pass the
masters.

1. if we have no additional information about a student   s skills, which modules (in addition to the project)
should she take in order to maximise her chance of passing the masters? state the id203 of passing
the masters with this set of modules.

2. if we know the student is skilled in maths and organisation, which would be the best set of modules to

take and the associated id203 of passing the masters?

3. if we know the student is skilled in biology and writing, which would be the best set of modules to take

and the associated id203 of passing the masters?

64

draft november 9, 2017

chapter 4

id114

in chapter(3) we saw how belief networks are used to represent statements about independence of variables in
a probabilistic model. belief networks are simply one way to unite id203 and graphical representation.
many others exist, all under the general heading of    id114   . each has speci   c strengths and
weaknesses. broadly, id114 fall into two classes: those useful for modelling, such as belief
networks, and those useful for id136. this chapter will survey the most popular models from each class.

4.1 id114

id114 (gms) are depictions of independence/dependence relationships for distributions. each
class of gm is a particular union of graph and id203 constructs and details the form of independence
assumptions represented. gms are useful since they provide a framework for studying a wide class of prob-
abilistic models and associated algorithms. in particular they help to clarify modelling assumptions and
provide a uni   ed framework under which id136 algorithms in di   erent communities can be related.

it needs to be emphasised that all forms of gm have a limited ability to graphically express conditional
(in)dependence statements[282]. as we   ve seen, belief networks are useful for modelling ancestral condi-
tional independence. in this chapter we   ll introduce other types of gm that are more suited to representing
di   erent assumptions. here we   ll focus on markov networks, chain graphs (which marry belief and markov
networks) and factor graphs. there are many more inhabitants of the zoo of id114, see [74, 315].

the general viewpoint we adopt is to describe the problem environment using a probabilistic model, after
which reasoning corresponds to performing probabilistic id136. this is therefore a two part process :

modelling after identifying all potentially relevant variables of a problem environment, our task is to
describe how these variables can interact. this is achieved using structural assumptions as to the
form of the joint id203 distribution of all the variables, typically corresponding to assumptions
of independence of variables. each class of graphical model corresponds to a factorisation property of
the joint distribution.

id136 once the basic assumptions as to how variables interact with each other is formed (i.e. the
probabilistic model is constructed) all questions of interest are answered by performing id136 on
the distribution. this can be a computationally non-trivial step so that coupling gms with accurate
id136 algorithms is central to successful graphical modelling.

whilst not a strict separation, gms tend to fall into two broad classes     those useful in modelling, and those
useful in representing id136 algorithms. for modelling, belief networks, markov networks, chain graphs
and in   uence diagrams are some of the most popular. for id136 one typically    compiles    a model into a

65

markov networks

x2

x2

x1

x3

x1

x3

x1

x4

(a)

x4

(b)

x5

x6

x3

x2

x4

(c)

figure 4.1:
  (x1, x2, x4)  (x2, x3, x4)  (x3, x5)  (x3, x6)/zc.

(a):

  (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)/za

(b):

  (x1, x2, x3, x4)/zb

(c):

suitable gm for which an algorithm can be readily applied. such id136 gms include factor graphs and
junction trees.

4.2 markov networks

belief networks correspond to a special kind of factorisation of the joint id203 distribution in which
each of the factors is itself a distribution. an alternative factorisation is, for example

p(a, b, c) =

1
z

  (a, b)  (b, c)

(4.2.1)

where   (a, b) and   (b, c) are potentials (see below) and z is a constant which ensures normalisation, called
the partition function

z =

  (a, b)  (b, c)

(4.2.2)

a,b,c

of a potential satisfying normalisation, (cid:80)

de   nition 4.1 (potential). a potential   (x) is a non-negative function of the variable x,   (x)     0. a joint
potential   (x1, . . . , xn) is a non-negative function of the set of variables. a distribution is a special case
x   (x) = 1. this holds similarly for continuous variables, with

summation replaced by integration.

we will typically use the convention that the ordering of the variables in the potential is not relevant (as
for a distribution)     the joint variables simply index an element of the potential table. markov networks
are de   ned as products of potentials de   ned on maximal cliques of an undirected graph     see below and
   g(4.1).

(cid:88)

p(x1, . . . , xn) =

1
z

c(cid:89)

c=1

de   nition 4.2 (markov network). for a set of variables x = {x1, . . . , xn} a markov network is de   ned as
a product of potentials on subsets of the variables xc     x :

  c(xc)

(4.2.3)

the constant z ensures the distribution is normalised. graphically this is represented by an undirected
graph g with xc, c = 1, . . . , c being the maximal cliques of g. for the case in which clique potentials are
strictly positive, this is called a gibbs distribution.

de   nition 4.3 (pairwise markov network ). in the special case that the graph contains cliques of only size
2, the distribution is called a pairwise markov network , with potentials de   ned on each link between two
variables.

66

draft november 9, 2017

markov networks

1

2

3

5

6

4

(a)

7

1

5

6

7

2

3

4

(b)

(a):   (1, 2, 3)  (2, 3, 4)  (4, 5, 6)  (5, 6, 7).
figure 4.2:
(b): by the global markov property, since every path
from 1 to 7 passes through 4, then 1       7| 4.

whilst a markov network is formally de   ned on maximal cliques, in practice
authors often use the term to refer to non-maximal cliques. for example, in the
graph on the right, the maximal cliques are x1, x2, x3 and x2, x3, x4, so that the
graph describes a distribution p(x1, x2, x3, x4) =   (x1, x2, x3)  (x2, x3, x4)/z. in
a pairwise network though the potentials are assumed to be over two-cliques,
giving p(x1, x2, x3, x4) =   (x1, x2)  (x1, x3)  (x2, x3)  (x2, x4)  (x3, x4)/z.

x2

x1

x3

x4

example 4.1 (id82). a id82 is a mn on binary variables dom(xi) = {0, 1}
of the form

i bixi

(4.2.4)

(cid:80)

i<j wij xixj +(cid:80)

p(x) =

1

z(w, b)

e

@@

where the interactions wij are the    weights    and the bi the    biases   . this model has been studied in the
machine learning community as a basic model of distributed memory and computation[2]. the graphical
id82 is an undirected graph with a link between nodes i and j for wij (cid:54)= 0.
model of the
consequently, for all but specially constrained w, the graph is multiply-connected and id136 will be
typically intractable.

de   nition 4.4 (properties of markov networks).

b

b

b

a

a

a

c

c

c

p(a, b, c) =   ac(a, c)  bc(b, c)/z

(4.2.5)

    a

    a

b

b

marginalising over c makes a and b (graphically)
dependent. in general p(a, b) (cid:54)= p(a)p(b).

conditioning on c makes a and b independent:
p(a, b|c) = p(a|c)p(b|c).

4.2.1 markov properties

we consider here informally the properties of markov networks and the reader is referred to [183] for detailed
proofs. consider the mn in    g(4.2a) in which we use the shorthand p(1)     p(x1),   (1, 2, 3)       (x1, x2, x3)
etc. we will use this undirected graph to demonstrate conditional independence properties. note that
throughout we will be often dividing by potentials and, in order to ensure this is well de   ned, we assume the
potentials are positive. for positive potentials the following local, pairwise and global markov properties
are all equivalent.

draft november 9, 2017

67

x1

x3

x2

x1

x2

x1

x4

x3

x4

x3

x4

x3

x4

x3

(a)

(b)

(c)

(d)

(e)

markov networks

x2

x4

(a-d): local conditional distributions. note that no distribution is implied for the parents of
figure 4.3:
each variable. that is, in (a) we are given the conditional p(x4|x1, x3)     one should not read from the graph
(e): the markov network consistent with the local
that we imply x1 and x3 are marginally independent.
distributions. if the local distributions are positive, by the hammersley-cli   ord theorem, the only joint
distribution that can be consistent with the local distributions must be a gibbs distribution with structure
given by (e).

de   nition 4.5 (separation). a subset s separates a subset a from a subset b (for disjoint a and b) if
every path from any member of a to any member of b passes through s. if there is no path from a member
of a to a member of b then a is separated from b. if s =     then provided no path exists from a to b, a
and b are separated.

de   nition 4.6 (global markov property). for disjoint sets of variables, (a,b,s) where s separates a
from b in g, then a      b|s.

as an example of the global markov property,

consider    g(4.2a) for which

@@

2,3,5,6

(cid:88)
(cid:88)
         (cid:88)

2,3,5,6

2,3

p(1, 7|4)    

=

=

p(1, 2, 3, 4, 5, 6, 7)

  (1, 2, 3)  (2, 3, 4)  (4, 5, 6)  (5, 6, 7)

         

         (cid:88)

5,6

         

  (1, 2, 3)  (2, 3, 4)

  (4, 5, 6)  (5, 6, 7)

(4.2.6)

(4.2.7)

(4.2.8)

this implies that p(1, 7|4) = p(1|4)p(7|4). this can be inferred since all paths from 1 to 7 pass

through 4.

@@

procedure 4.1 (an algorithm for independence). the separation property implies a simple algorithm for
deciding a        b| s. we simply remove all links that neighbour the set of variables s. if there is no path
from any member of a to any member of b, then a      b|s is true     see also section(4.2.4).

for positive potentials, the so-called local markov property holds

p(x|x\x) = p(x|ne (x)).

(4.2.9)

that is, when conditioned on its neighbours, x is independent of the remaining variables of the graph. in
addition, the so-called pairwise markov property holds that for any non-adjacent vertices x and y

x       y|x\{x, y} .

(4.2.10)

4.2.2 markov random    elds

a mrf is a set of conditional distributions, one for each indexed    location   .

68

draft november 9, 2017

markov networks

de   nition 4.7 (markov random field). a mrf is de   ned by a set of distributions p(xi|ne (xi)) where
i     {1, . . . , n} indexes the distributions and ne (xi) are the neighbours of variable xi, namely that subset of
the variables x1, . . . , xn that the distribution of variable xi depends on. the term markov indicates that
this is a proper subset of the variables. a distribution is an mrf with respect to an undirected graph g if

p(xi|x\i) = p(xi|ne (xi))

(4.2.11)

where ne (xi) are the neighbouring variables of variable xi, according to the undirected graph g. the
notation x\i is shorthand for the set of all variables x excluding variable xi, namely x\xi in set notation.

4.2.3 hammersley-cli   ord theorem

an undirected graph g speci   es a set of independence statements. an interesting challenge is to    nd
the most general functional form of a distribution that satis   es these independence statements. a trivial
example is the graph x1     x2     x3, from which we have x1       x3| x2. from this requirement we must have
(4.2.12)

p(x1|x2, x3) = p(x1|x2)

hence

p(x1, x2, x3) = p(x1|x2, x3)p(x2, x3) = p(x1|x2)p(x2, x3) =   12(x1, x2)  23(x2, x3)

(4.2.13)

where the    are potentials.

more generally, for any decomposable graph g, see de   nition(6.8), we can start at the edge and work inwards
to reveal that the functional form must be a product of potentials on the cliques of g. for example, for
x1       x4, x5, x6, x7|
   g(4.2a), we can start with the variable x1 and the corresponding local markov statement
x2, x3 to write

@@

p(x1, . . . , x7) = p(x1|x2, x3)p(x2, x3, x4, x5, x6, x7)

(4.2.14)

now we consider x1 eliminated and move to the neighbours of x1, namely x2, x3. the graph speci   es that
x1, x2, x3 are independent of x5, x6, x7 given x4:

p(x1, x2, x3|x4, x5, x6, x7) = p(x1, x2, x3|x4)

by summing both sides above over x1 we have that p(x2, x3|x4, x5, x6, x7) = p(x2, x3|x4). hence
p(x2, x3, x4, x5, x6, x7) = p(x2, x3|x4, x5, x6, x7)p(x4, x5, x6, x7) = p(x2, x3|x4)p(x4, x5, x6, x7)

and

p(x1, . . . , x7) = p(x1|x2, x3)p(x2, x3|x4)p(x4, x5, x6, x7)

(4.2.15)

(4.2.16)

(4.2.17)

having eliminated x2, x3, we now move to their neighbour(s) on the remaining graph, namely x4. continuing
in this way, we necessarily end up with a distribution of the form

p(x1, . . . , x7) = p(x1|x2, x3)p(x2, x3|x4)p(x4|x5, x6)p(x5, x6|x7)p(x7)

(4.2.18)

the pattern here is clear and shows that the markov conditions mean that the distribution is expressible
as a product of potentials de   ned on the cliques of the graph. that is g     f where f is a factorisation
into clique potentials on g. the converse is easily shown, namely that given a factorisation into clique
potentials, the markov conditions on g are implied. hence g     f . it is clear that for any decomposable
g, this always holds since we can always work inwards from the edges of the graph.

the hammersley-cli   ord theorem is a stronger result and shows that this factorisation property holds for
any undirected graph, provided that the potentials are positive. for a formal proof, the reader is referred
to [183, 37, 220]. an informal argument can be made by considering a speci   c example, and we take the

draft november 9, 2017

69

markov networks

4-cycle x1     x2     x3     x4     x1 from    g(4.1a). the theorem states that for positive potentials   , the markov
conditions implied by the graph mean that the distribution must be of the form

p(x1, x2, x3, x4) =   12(x1, x2)  23(x2, x3)  34(x3, x4)  41(x4, x1)

(4.2.19)

one may readily verify that for any distribution of this form x1       x3| x2, x4. consider including an additional
term that links x1 to a variable not a member of the cliques that x1 inhabits. that is we include a term
  13(x1, x3). our aim is to show that a distribution of the form

p(x1, x2, x3, x4) =   12(x1, x2)  23(x2, x3)  34(x3, x4)  41(x4, x1)  13(x1, x3)

(4.2.20)

(4.2.21)

(4.2.22)

(4.2.23)

(cid:33)   1

cannot satisfy the markov property x1       x3| x2, x4. to do so we examine

p(x1|x2, x3, x4) =

=

  12(x1, x2)  23(x2, x3)  34(x3, x4)  41(x4, x1)  13(x1, x3)
x1   12(x1, x2)  23(x2, x3)  34(x3, x4)  41(x4, x1)  13(x1, x3)
  12(x1, x2)  41(x4, x1)  13(x1, x3)
x1   12(x1, x2)  41(x4, x1)  13(x1, x3)

(cid:80)
(cid:80)

if we assume that the potential   13 is weakly dependent on x1 and x3,

  13(x1, x3) = 1 +    (x1, x3)

where   (cid:28) 1, then p(x1|x2, x3, x4) is given by

(cid:80)

(cid:32)

(cid:80)

(cid:80)

x1   12(x1, x2)  41(x4, x1)  (x1, x3)

  12(x1, x2)  41(x4, x1)
x1   12(x1, x2)  41(x4, x1)

1 +  

(1 +    (x1, x3))

by expanding (1 +  f )   1 = 1      f + o(cid:0) 2(cid:1) and retaining only terms that are    rst order in  , we obtain
+ o(cid:0) 2(cid:1) (4.2.25)

  12(x1, x2)  41(x4, x1)
x1   12(x1, x2)  41(x4, x1)

x1   12(x1, x2)  41(x4, x1)  (x1, x3)

p(x1|x2, x3, x4) =

x1   12(x1, x2)  41(x4, x1)

(cid:35)(cid:33)

(cid:80)

(cid:80)

(4.2.24)

(cid:32)

(cid:34)

1 +  

(cid:80)

  

  (x1, x3)    

x1   12(x1, x2)  41(x4, x1)

the    rst factor above is independent of x3, as required by the markov condition. however, for   (cid:54)= 0, the
second term varies as a function of x3. the reason for this is that one can always    nd a function   (x1, x3)
for which

(cid:80)

(cid:80)

  (x1, x3) (cid:54)=

x1   12(x1, x2)  41(x4, x1)  (x1, x3)

x1   12(x1, x2)  41(x4, x1)

(4.2.26)

since the term   (x1, x3) on the left is functionally dependent on x1 whereas the term on the right is not a
function of x1. hence, the only way we can ensure the markov condition holds is if   = 0, namely that there
is no connection between x1 and x3.

one can generalise this argument to show that if the graph of potentials in the distribution contains a
link which is not present in g, then there is some distribution for which a corresponding markov condition
cannot hold. informally, therefore, g     f . the converse f     g is trivial.
the hammersley-cli   ord theorem also helps resolve questions as to when a set of positive local conditional
distributions p(xi|pa (xi)) could ever form a consistent joint distribution p(x1, . . . , xn). each local conditional
distribution p(xi|pa (xi)) corresponds to a factor on the set of variables {xi, pa (xi)}, so we must include such
a term in the joint distribution. the mn can form a joint distribution consistent with the local conditional
distributions if and only if p(x1, . . . , xn) factorises according to

p(x1, . . . , xn) =

1
z

70

vc(xc)

(4.2.27)

draft november 9, 2017

(cid:32)

(cid:88)

c

exp

   

(cid:33)

markov networks

a

c

e

h

i

j

(a)

b

d

f

k

a

c

e

b

d

f

a

c

e

b

d

f

g

i

(b)

i

(c)

(a): belief network for
figure 4.4:
which we are interested in checking
conditional independence a       b|{d, i}.
(b): ancestral graph.
(c): ances-
tral, moralised and separated graph
for a        b | {d, i}. there is no path
from a red to green node so a and b
are independent given d, i.

indexed by c. equation (4.2.27) is equivalent to (cid:81)

where the sum is over all cliques and vc(xc) is a real function de   ned over the variables in the clique
c   (xc), namely a mn on positive clique potentials.
the graph over which the cliques are de   ned is an undirected graph constructed by taking each local
conditional distribution p(xi|pa (xi)) and drawing a clique on {xi, pa (xi)}. this is then repeated over all
the local conditional distributions, see    g(4.3). note that the hc theorem does not mean that, given a set
of conditional distributions, we can always form a consistent joint distribution from them     rather it states
what the functional form of a joint distribution has to be for the conditionals to be consistent with the joint,
see exercise(4.8).

4.2.4 conditional independence using markov networks

for x , y, z each being collections of variables, in section(3.3.4) we discussed an algorithm to determine
if x        y| z for belief networks. an alternative and more general method (since it handles directed and
undirected graphs) uses the procedure below (see [79, 184]). see    g(4.4) for an example.

procedure 4.2 (ascertaining independence in markov and belief networks). for markov networks only
the    nal separation criterion needs to be applied:

ancestral graph identify the ancestors a of the nodes x     y     z. retain the nodes x     y     z but

remove all other nodes which are not in a, together with any edges in or out of such nodes.

moralisation add a link between any two remaining nodes which have a common child, but are not already

connected by an arrow. then remove remaining arrowheads.

separation remove links neighbouring z. in the undirected graph so constructed, look for a path which

joins a node in x to one in y. if there is no such path deduce that x       y|z.

note that the ancestral step in procedure(4.2) for belief networks is intuitive since, given a set of nodes
x and their ancestors a, the remaining nodes d form a contribution to the distribution of the form
p(d|x ,a)p(x ,a), so that summing over d simply has the e   ect of removing these variables from the
dag.

4.2.5 lattice models

undirected models have a long history in di   erent branches of science, especially statistical mechanics on
lattices and more recently as models in visual processing in which the models encourage neighbouring vari-
ables to be in the same states[37, 38, 117].

draft november 9, 2017

71

consider a model in which our desire is that states of the binary valued variables
x1, . . . , x9, arranged on a lattice (right) should prefer their neighbouring variables
to be in the same state

  ij(xi, xj)

(4.2.28)

p(x1, . . . , x9) =

1
z

(cid:89)

i   j

where i     j denotes the set of indices where i and j are neighbours in the
undirected graph.

the ising model

markov networks

x1

x4

x7

x2

x5

x8

x3

x6

x9

a set of potentials for equation (4.2.28) that encourages neighbouring variables to have the same state is

  ij(xi, xj) = e

    1
2t (xi   xj )2

,

xi     {   1, +1}

(4.2.29)

this corresponds to a well-known model of the physics of magnetic systems, called the ising model which
consists of    mini-magnets    which prefer to be aligned in the same state, depending on the temperature
t . for high t the variables behave independently so that no global magnetisation appears. for low t ,
there is a strong preference for neighbouring mini-magnets to become aligned, generating a strong macro-
magnet. remarkably, one can show that, in a very large two-dimensional lattice, below the so-called curie
temperature, tc     2.269 (for   1 variables), the system admits a phase change in that a large fraction of the
variables become aligned     above tc, on average, the variables are unaligned. this is depicted in    g(4.5)
where m =
non-zero temperature has driven considerable research in this and related areas[42]. global coherence e   ects
such as this that arise from weak local constraints are present in systems that admit emergent behaviour .
similar local constraints are popular in image restoration algorithms to clean up noise, under the assumption
that noise will not show any local spatial coherence, whilst    signal    will.

(cid:12)(cid:12)(cid:12) /n is the average alignment of the variables. that this phase change happens for

(cid:12)(cid:12)(cid:12)(cid:80)n

i=1 xi

example 4.2 (cleaning up images).

consider a binary image de   ned on a set of pixels xi     {   1, +1}, i = 1, . . . , d. we
observe a noise corrupted version yi of each pixel xi, in which the state of yi     {   1, +1}
is opposite to xi with some id203. here the    lled nodes indicate observed noisy
pixels and the unshaded nodes the latent clean pixels. our interest is to    clean up    the
observed dirty image y, and    nd the most likely joint clean image x .
a model for this situation is

(cid:34) d(cid:89)

i=1

p(x ,y) =

1
z

(cid:35)      (cid:89)

i   j

       ,

  (xi, yi)

  (xi, xj)

  (xi, yi) = e  xiyi,   (xi, xj) = e  xixj

(4.2.30)

here i     j indicates the set of latent variables that are neighbours. the potential    encourages the noisy
and clean pixel to be in the same state. similarly, the potential   (xi, xj) encourages neighbouring pixels to
be in the same state. to    nd the most likely clean image, we need to compute

argmaxx

p(x|y) = argmaxx

p(x ,y)

(4.2.31)

this is a computationally di   cult task but can be approximated using iterative methods, see section(28.9).

figure 4.5: onsager magnetisation. as the temperature t decreases
towards the critical temperature tc a phase transition occurs in which
a large fraction of the variables become aligned in the same state.

72

draft november 9, 2017

00.511.5200.51t/tcmchain id114

a

c

(a)

b

d

a

b

cd

(b)

a

h

b

e

g

d

f

c

c

aedf h

bg

(c)

(d)

figure 4.6: chain graphs. the chain components are identi   ed by deleting the directed edges and identifying
the remaining connected components. (a): chain components are (a),(b),(c, d), which can be written as a
bn on the cluster variables in (b). (c): chain components are (a, e, d, f, h), (b, g), (c), which has the cluster
bn representation (d).

on the left is the clean image, from which a noisy corrupted image y is formed (middle). the most likely
restored image is given on the right. see demomrfclean.m. note that the parameter    is straightforward
to set, given knowledge of the corruption id203 pcorrupt, since p(yi (cid:54)= xi|xi) =
   (   2  ), so that    =
2      1(pcorrupt). setting    is more complex since relating p(xi = xj) to    is not straightforward, see
section(28.4.1). in the demonstration we set    = 10, pcorrupt = 0.15.

@@
@@     1

4.3 chain id114

chain graphs (cgs) contain both directed and undirected links. to develop the intuition, consider    g(4.6a).
the only terms that we can unambiguously specify from this depiction are p(a) and p(b) since there is no
mixed interaction of directed and undirected edges at the a and b vertices. by id203, therefore, we
must have

p(a, b, c, d) = p(a)p(b)p(c, d|a, b)

looking at the graph, we might expect the interpretation to be

p(c, d|a, b) =   (c, d)p(c|a)p(d|b)

however, to ensure normalisation, and also to retain generality, we interpret this as

p(c, d|a, b) =   (c, d)p(c|a)p(d|b)  (a, b), with   (a, b)    

  (c, d)p(c|a)p(d|b)

      (cid:88)

c,d

         1

(4.3.1)

(4.3.2)

(4.3.3)

this leads to the interpretation of a cg as a dag over the chain components see below.

de   nition 4.8 (chain component). the chain components of a graph g are obtained by :

1. forming a graph g(cid:48) with directed edges removed from g.
2. then each connected component in g(cid:48) constitutes a chain component.

draft november 9, 2017

73

each chain component represents a distribution over the variables of the component, conditioned on the
parental components. the conditional distribution is itself a product over the cliques of the undirected
component and moralised parental components, including also a factor to ensure normalisation over the
chain component.

chain id114

de   nition 4.9 (chain graph distribution). the distribution associated with a chain graph g is found by
   rst identifying the chain components,   

and their associated variables x   . then

@@

(cid:89)

p(x) =

p (x  |pa (x   ))

  

and

p (x  |pa (x   ))    

(cid:89)

d   d  

(cid:89)

c   c  

   (xc)

p(xd|pa (xd))

(4.3.4)

(4.3.5)

@@

where c   denotes the union of the cliques in component   
with    being the associated functions de   ned on
each clique; d   is the set of variables in component    that correspond to directed terms p(xd|pa (xd)). the
proportionality factor is determined implicitly by the constraint that the distribution sums to 1.

@@

bns are cgs in which the connected components are singletons. mns are cgs in which the chain components
are simply the connected components of the undirected graph. cgs can be useful since they are more
expressive of ci statements than either belief networks or markov networks alone. the reader is referred to
[183] and [107] for further details.

example 4.3 (chain graphs are more expressive than belief or markov networks). consider the chain
graph in    g(4.7a), which has chain component decomposition

p(a, b, c, d, e, f ) = p(a)p(b)p(c, d, e, f|a, b)

where

p(c, d, e, f|a, b) =

p(c|a)  (c, e)  (e, f )  (d, f )

p(d|b)  (a, b)

with the normalisation requirement

       (cid:88)

c,d,e,f

  (a, b)    

p(c|a)  (c, e)  (e, f )  (d, f )

p(d|b)

the marginal p(c, d, e, f ) is given by

  (c, e)  (e, f )  (d, f )

  (a, b)p(a)p(b)

(cid:88)
(cid:124)

a,b

(cid:123)(cid:122)

p(c|a)

p(d|b)

(cid:125)

  (c,d)

         1

(4.3.6)

(4.3.7)

@@
@@

(4.3.8)

@@
@@

(4.3.9)

@@
@@

since the marginal distribution of p(c, d, e, f ) is an undirected 4-cycle, no dag can express the ci statements
contained in the marginal p(c, d, e, f ). similarly no undirected distribution on the same skeleton as    g(4.7a)
could express that a and b are independent (unconditionally), i.e. p(a, b) = p(a)p(b).

74

draft november 9, 2017

factor graphs

a

c

e

b

d

f

c

e

d

f

c

e

d

f

(a)

(b)

(c)

figure 4.7: the cg (a) expresses a        b|     and d       
e| (c, f ). no directed graph could express both these
conditions since the marginal distribution p(c, d, e, f )
is an undirected four cycle, (b). any dag on a 4 cycle
must contain a collider, as in (c) and therefore express
a di   erent set of ci statements than (b). similarly, no
connected markov network can express unconditional
independence and hence (a) expresses ci statements
that no belief network or markov network alone can
express.

4.4 factor graphs

factor graphs (fgs) are mainly used as part of id136 algorithms1.

(cid:89)

i

(cid:89)

1
z

de   nition 4.10 (factor graph). given a function

f (x1, . . . , xn) =

  i (xi)

(4.4.1)

the fg has a node (represented by a square) for each factor   i, and a variable node (represented by
a circle) for each variable xj. for each xj     xi an undirected link is made between factor   i and variable xj.
when used to represent a distribution

p(x1, . . . , xn) =

a normalisation constant z =(cid:80)x(cid:81)

  i (xi)

i

++

i   i (xi) is assumed.

here x represents all variables in the distribution.

(4.4.2)

for a factor   i (xi) which is a conditional distribution p(xi|pa (xi)), we may use directed links from the
parents to the factor node, and a directed link from the factor node to the child xi. this has the same
structure as an (undirected) fg, but preserves the information that the factors are distributions.

factor graphs are useful since they can preserve more information about the form of the distribution than
either a belief network or a markov network (or chain graph) can do alone. consider the distribution

p(a, b, c) =   (a, b)  (a, c)  (b, c)

(4.4.3)

represented as a mn, this must have a single clique, as given in    g(4.8c). however,    g(4.8c) could equally
represent some unfactored clique potential   (a, b, c) so that the factorised structure within the clique is lost.
in this sense, the fg representation in    g(4.8b) more precisely conveys the form of distribution equation
(4.4.3). an unfactored clique potential   (a, b, c) is represented by the fg    g(4.8a). hence di   erent fgs
can have the same mn since information regarding the structure of the clique potential is lost in the mn.
similarly, for a belief network, as in    g(4.8d) one can represent this using a standard undirected fg, although
more information about the independence is preserved by using a directed fg representation, as in    g(4.8e).
one can also consider partially directed fgs which contain both directed and undirected edges; this requires
a speci   cation of how the structure is normalised, one such being to use an approach analogous to the chain
graph     see [104] for details.

1formally a fg is an alternative graphical depiction of a hypergraph[87] in which the vertices represent variables, and a
hyperedge a factor as a function of the variables associated with the hyperedge. a fg is therefore a hypergraph with the
additional interpretation that the graph represents a function de   ned as products over the associated hyperedges. many thanks
to robert cowell for this observation.

draft november 9, 2017

75

expressiveness of id114

a

a

a

a

a

c

b

c

b

c

b

c

b

c

b

(a)

(b)

(c)

(d)

(e)

@@

@@
@@
@@

@@
@@

@@
@@
@@
@@
@@
@@

(a):   (a, b, c).

(b):   (a, b)  (b, c)  (c, a).

(c):   (a, b, c). both (a) and (b) have the same
figure 4.8:
undirected graphical model, (c). (d): (a) is an undirected fg of (d). (e): directed fg of the bn in (d).
a directed factor represents a term p(children|parents). the advantage of (e) over (a) is that information
regarding the marginal independence of variables b and c is clear from graph (e), whereas one could only
ascertain this by examination of the numerical entries of the factors in graph (a).

4.4.1 conditional independence in factor graphs

conditional independence questions can be addressed using a rule which works with directed, undirected
and partially directed fgs[104]. to determine whether two variables are independent given a set of condi-
tioned variables, consider all paths connecting the two variables. if all paths are blocked, the variables are
conditionally independent. a path is blocked if one or more of the following conditions is satis   ed:

    one of the variables in the path is in the conditioning set.
    one of the variables or factors in the path has two incoming edges that are part of the path (variable
or factor collider), and neither the variable or factor nor any of its descendants are in the conditioning
set.

4.5 expressiveness of id114

it is clear that directed distributions can be represented as undirected distributions since one can asso-
ciate each (normalised) factor of the joint distribution with a potential. for example, the distribution
p(a|b)p(b|c)p(c) can be factored as   (a, b)  (b, c), where   (a, b) = p(a|b) and   (b, c) = p(b|c)p(c), with z = 1.
hence every belief network can be represented as some mn by simple identi   cation of the factors in the
distributions. however, in general, the associated undirected graph (which corresponds to the moralised
directed graph) will contain additional links and independence information can be lost. for example, the
mn of p(c|a, b)p(a)p(b) is a single clique   (a, b, c) from which one cannot graphically infer that a       b.

the converse question is whether every undirected model can be represented by a bn with a readily derived
link structure. consider the example in    g(4.9). in this case, there is no directed model with the same
link structure that can express the (in)dependencies in the undirected graph. naturally, every id203
distribution can be represented by some bn though it may not necessarily have a simple structure and
be a    fully connected    cascade style graph. in this sense the dag cannot always graphically represent the
independence properties that hold for the undirected distribution.

de   nition 4.11 (independence maps). a graph is an independence map (i-map) of a given distribution p
if every conditional independence statement that one can derive from the graph g is true in the distribution
p . that is

x       y|z g     x       y|z p

for all disjoint sets x , y, z.

(4.5.1)

76

draft november 9, 2017

expressiveness of id114

b

b

a

c

a

c

d

(a)

d

(b)

figure 4.9: (a): an undirected model for which we wish to    nd a
directed equivalent. (b): every dag with the same structure as
the undirected model must have a situation where two arrows will
point to a node, such as node d (otherwise one would have a cyclic
graph). summing over the states of variable d will leave a dag
on the variables a, b, c with no link between a and c. this cannot
represent the undirected model since when one marginalises over
d this adds a link between a and c.

similarly, a graph is a dependence map (d-map) of a given distribution p if every conditional independence
statement that one can derive from p is true in the graph g. that is

x       y|z g     x       y|z p

for all disjoint sets x , y, z.
a graph g which is both an i-map and a d-map for p is called a perfect map and

x       y|z g     x       y|z p

(4.5.2)

(4.5.3)

for all disjoint sets x ,y,z. in this case, the set of all conditional independence and dependence statements
expressible in the graph g are consistent with p and vice versa.

note that by contraposition, a dependence map is equivalent to

x(cid:62)(cid:62)y|z g     x(cid:62)(cid:62)y|z p

(4.5.4)

meaning that if x and y are graphically dependent given z, then they are dependent in the distribution.

one way to think about this is to take a distribution p and write out a list lp of all the independence
statements. for a graph g, one writes a list of all the possible independence statements lg. then:

lp     lg dependence map (d-map)
lp     lg independence map (i-map)
lp = lg perfect map

(4.5.5)

in the above we assume the statement l is contained in l if it is consistent with (can be derived from) the
independence statements in l.
one can also discuss whether or not a distribution class has an associated map. that is whether or not all
numerical instances of distributions consistent with the speci   ed form obey the constraints required for the
map. to do so we take any numerical instance of a distribution pi consistent with a given class p and write
out a list lpi of all the independence statements. one then takes the intersection lp =    ilpi of all the
lists from all possible distribution instances. this list is then used in equation (4.5.5) to determine if there
is an associated map. for example the distribution class

p(x, y, z) = p(z|x, y)p(x)p(y)

(4.5.6)
has a directed perfect map x     z     y. however, the undirected graph for the class equation (4.5.6) is
fully connected so that lg is empty. for any distribution consistent with equation (4.5.6) x and y are
independent, a statement which is not contained in lg     hence there is no undirected d-map and hence no
perfect undirected map for the class represented in equation (4.5.6).

(cid:88)

example 4.4. consider the distribution (class) de   ned on variables t1, t2, y1, y2[250]:

p(t1, t2, y1, y2) = p(t1)p(t2)

h

draft november 9, 2017

p(y1|t1, h)p(y2|t2, h)p(h)

(4.5.7)

77

in this case the list of all independence statements (for all distribution instances consistent with p) is

expressiveness of id114

lp =

{y1       t2| t1, y2       t1| t2, t2       t1}

consider the graph of the bn

p(y2|y1, t1, t2)p(y1|t1)p(t1)p(t2)

for this we have

lg = {y1       t2| t1, t2       t1}

(4.5.8)

@@

(4.5.9)

@@

(4.5.10)

@@

hence lg     lp so that the bn is an i-map for (4.5.7) since every independence statement in the bn is
true for the distribution class in equation (4.5.7). however, it is not a d-map since lp (cid:54)    lg. in this case
no perfect map (a bn or a mn) can represent (4.5.7).

remark 4.1 (forcing dependencies?). whilst id114 as we have de   ned them ensure speci   ed
independencies, they seem to be inappropriate for ensuring speci   ed dependencies. consider the undirected
graph x     y     z. graphically this expresses that x and z are dependent. however, there are numerical
instances of distributions for which this does not hold, for example

p(x, y, z) =   (x, y)  (y, z)/z1

(4.5.11)

with   (x, y) = const. one might complain that this is a pathological case since any graphical representation
of this particular instance contains no link between x and y. maybe one should therefore    force    potentials
to be non-trivial functions of their arguments and thereby ensure dependency? consider

  (x, y) =

x
y

,

  (y, z) = yz

(4.5.12)

in this case both potentials are non-trivial in the sense that they are truly functionally dependent on their
arguments. hence, the undirected network contains    genuine    links x     y and y     z. nevertheless,

p2(x, y, z) =   (x, y)  (y, z)/z2    

x
y

yz = xz

(4.5.13)

hence p2(x, z)     xz     x        z. so    forcing    local non-trivial functions does not guarantee dependence of
path-connected variables. in this case, the algebraic cancellation is clear and the problem is again rather
trivial since for p2, x       y and y       z, so one might assume that x       z (see however, remark(1.2)). however,
there may be cases where such algebraic simpli   cations are highly non-trivial, though nevertheless true.
see, for example, exercise(3.17) in which we construct p(x, y, z)       (x, y)  (y, z) for which x(cid:62)(cid:62)y and y(cid:62)(cid:62)z, yet
x       z.

4.6 summary

    graphical modelling is the discipline of representing id203 models graphically.
    belief networks intuitively describe which variables    causally    in   uence others and are represented using

directed graphs.

    a markov network is represented by an undirected graph.
    intuitively, linked variables in a markov network are graphically dependent, describing local cliques of

graphically dependent variables.

78

draft november 9, 2017

exercises

    markov networks are historically important in physics and may be used to understand how global collabo-

rative phenomena can emerge from only local dependencies.

    id114 are generally limited in their ability to represent all the possible logical consequences of

a probabilistic model.

    some special probabilistic models can be    perfectly    mapped graphically.
    factor graphs describe the factorisation of functions and are not necessarily related to id203 distribu-

tions.

a detailed discussion of the axiomatic and logical basis of conditional independence is given in [48] and
[281].

4.7 code

condindep.m: conditional independence test p(x, y |z) = p(x|z)p(y |z)?
4.8 exercises

exercise 4.1.

1. consider the pairwise markov network,

p(x) =   (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)

express in terms of    the following:

p(x1|x2, x4),

p(x2|x1, x3),

p(x3|x2, x4),

p(x4|x1, x3)

2. for a set of local distributions de   ned as

p1(x1|x2, x4),

p2(x2|x1, x3),

p3(x3|x2, x4),

p4(x4|x1, x3)

(4.8.1)

(4.8.2)

(4.8.3)

is it always possible to    nd a joint distribution p(x1, x2, x3, x4) consistent with these local conditional
distributions?

exercise 4.2. consider the markov network

p(a, b, c) =   ab(a, b)  bc(b, c)

(4.8.4)

nominally, by summing over b, the variables a and c are dependent. for binary b, explain a situation in
which this is not the case, so that marginally, a and c are independent.

exercise 4.3. show that for the id82 de   ned on binary variables xi with

(cid:16)

(cid:17)

p(x) =

1

z(w, b)

exp

xtwx + xtb

one may assume, without loss of generality, w = wt.

exercise 4.4.

the restricted id82 (or harmonium[270]) is a constrained
id82 on a bipartite graph, consisting of a layer of visible
variables v = (v1, . . . , vv ) and hidden variables h = (h1, . . . , hh ):

h1

h2

(cid:16)

(cid:17)

p(v, h) =

1

z(w, a, b)

exp

vtwh + atv + bth

(4.8.6)

v1

v2

v3

all variables are binary taking states 0, 1.

draft november 9, 2017

(4.8.5)

79

1. show that the distribution of hidden units conditional on the visible units factorises as

(cid:89)

i

      bi +

(cid:88)

j

      

p(h|v) =

p(hi|v),

with p(hi = 1|v) =   

wjivj

where   (x) = ex/(1 + ex).

is p(h) =(cid:81)

2. by symmetry arguments, write down the form of the conditional p(v|h).
3.

i p(hi)?

4. can the partition function z(w, a, b) be computed e   ciently for the rbm?

exercise 4.5. you are given that

x       y| (z, u) ,

u       z|   

exercises

(4.8.7)

@@

(4.8.8)

derive the most general form of id203 distribution p(x, y, z, u) consistent with these statements. does
this distribution have a simple graphical model?

exercise 4.6. the undirected graph
clockwise around the pentagon with potentials   (xi, xj). show that the joint distribution can be written as

represents a markov network with nodes x1, x2, x3, x4, x5, counting

p(x1, x2, x3, x4, x5) =

p(x1, x2, x5)p(x2, x4, x5)p(x2, x3, x4)

p(x2, x5)p(x2, x4)

(4.8.9)

and express the marginal id203 tables explicitly as functions of the potentials   (xi, xj).

exercise 4.7.

consider the belief network on the right.

1. write down a markov network of p(x1, x2, x3).

2. is your markov network a perfect map of p(x1, x2, x3)?

h1

h2

x1

x2

x3

exercise 4.8. two research labs work independently on the relationship between discrete variables x and
y. lab a proudly announces that they have ascertained distribution pa(x|y) from data. lab b proudly
announces that they have ascertained pb(y|x) from data.

1. is it always possible to    nd a joint distribution p(x, y) consistent with the results of both labs?

2. is it possible to de   ne consistent marginals p(x) and p(y), in the sense that p(x) = (cid:80)
and p(y) =(cid:80)

x pb(y|x)p(x)? if so, explain how to    nd such marginals. if not, explain why not.

y pa(x|y)p(y)

exercise 4.9. research lab a states its    ndings about a set of variables x1, . . . , xn as a list la of conditional
independence statements. lab b similarly provides a list of conditional independence statements lb.

1. is it always possible to    nd a distribution which is consistent with la and lb?

2. if the lists also contain dependence statements, how could one attempt to    nd a distribution that is

consistent with both lists?

exercise 4.10.
consider the distribution

p(x, y, w, z) = p(z|w)p(w|x, y)p(x)p(y)

(4.8.10)

1. write p(x|z) using a formula involving (all or some of ) p(z|w), p(w|x, y), p(x), p(y).
2. write p(y|z) using a formula involving (all or some of ) p(z|w), p(w|x, y), p(x), p(y).

80

draft november 9, 2017

exercises

3. using the above results, derive an explicit condition for x       y| z and explain if this is satis   ed for this

distribution.

exercise 4.11. consider the distribution

p(t1, t2, y1, y2, h) = p(y1|y2, t1, t2, h)p(y2|t2, h)p(t1)p(t2)p(h)

1. draw a belief network for this distribution.

2. does the distribution

p(t1, t2, y1, y2) =

(cid:88)

h

p(y1|y2, t1, t2, h)p(y2|t2, h)p(t1)p(t2)p(h)

have a perfect map belief network?

3. show that for p(t1, t2, y1, y2) as de   ned above t1       y2|   .

exercise 4.12. consider the distribution

(4.8.11)

(4.8.12)

p(a, b, c, d) =   ab(a, b)  bc(b, c)  cd(c, d)  da(d, a)

(4.8.13)

where the    are potentials.

1. draw a markov network for this distribution.

2. explain if the distribution can be represented as a (   non-complete   ) belief network.

3. derive explicitly if a       c|   .

exercise 4.13. show how for any singly-connected markov network, one may construct a markov equivalent
belief network.

with p(s) =(cid:81)

exercise 4.14. consider a pairwise binary markov network de   ned on variables si     {0, 1}, i = 1, . . . , n ,
ij   e   ij(si, sj), where e is a given edge set and the potentials   ij are arbitrary. explain how
to translate such a markov network into a id82.

++

exercise 4.15.
would give rise to a set of speci   ed consistent distributions {q} on subsets of the variables.

our interest here is to show that it is not always possible to    nd a joint distribution p that

1. we    rst wish to construct a set of distributions q12(x1, x2), q13(x1, x3), q23(x2, x3) on binary variables

xi     {0, 1} that are    marginally    consistent; that is

(cid:88)
(cid:88)
(cid:88)

x1

x2

(cid:88)
(cid:88)
(cid:88)

x3

x3

q12(x1, x2) =

q12(x1, x2) =

q13(x1, x3) =

q13(x1, x3)

q23(x2, x3)

q23(x2, x3)

x1

x2

with all the q being distributions (non negative and summing to 1). by writing q12(x1 = 0, x2 = 0) = y1,
q12(x1 = 0, x2 = 1) = y2, etc. , show that the above equations can be represented as a linear system

my = c,

0     yi     1

where m is a suitably de   ned 9    12 matrix and c is a suitably de   ned 0 dimensional vector. hence
by solving this system using id135 (or otherwise),    nd a set of marginally consistent
distributions q.

draft november 9, 2017

81

exercises

(cid:88)

2. given a set of marginally consistent q, our interest is to see if we can    nd a joint distribution p which

would give rise to these marginals q. that is

p(x1, x2, x3) = q23(x2, x3),

p(x1, x2, x3) = q13(x1, x3),

p(x1, x2, x3) = q12(x1, x2)

x1

x2

x3

show that, by writing the 8 states of p as z1, . . . , z8, the above can be expressed as the linear system

(cid:88)

(cid:88)

(cid:88)

i

az = y,

0     zi     1,

zi = 1

where a is a suitably de   ned 12    8 matrix. for a marginally consistent y show numerically that it is
not always possible to    nd a joint distribution z that would give rise to these marginals.

the set of marginals consistent with a distribution p is called the marginal polytope of p. the question we
are therefore asking here is whether a given marginal set q is in the marginal polytope of p. whilst this is
straightforward to solve (using linear-programming for example), for an n-variable system the size of the
linear system will be exponential in n, thus resulting in a computationally hard problem.

this issue is important for at least two reasons: (i) imagine that di   erent research labs are asked to examine
di   erent aspects of a problem, each lab summarising their results as a distribution on a subset of the variables.
the question is whether there exists a single joint distribution that is consistent with all these marginals.
(ii) in certain deterministic approximation schemes (see section(28.7.2)) the objective depends only on a set
of marginals and the question is how to characterise the marginal polytope. as this question intimates, this
is generally very di   cult. however, as we will see in chapter(6), as least for singly-connected structures,
representing a distribution in terms of locally consistent marginals is straightforward.

exercise 4.16. the question concerns cleaning up the binary image given in the    le xnoisy.mat. the
objective function to maximise is

(cid:88)

(cid:88)

wi,ji [xi = xj] +

i,j

i

bixi,

xi     {0, 1}

++

where wi,j = 10 for neighbouring pixels (up, down, left, right) in the image and wi,j = 0 otherwise. using y
to represent the noisy image, bi = 2yi     1. plot the    nal cleaned up image x and give the maximum objective
function found for this cleaned up image.

82

draft november 9, 2017

chapter 5

e   cient id136 in trees

in previous chapters we discussed how to set up models. id136 then corresponds to operations such as
summing over subsets of variables. in machine learning and related areas we will often deal with distributions
containing hundreds of variables. in general id136 is computationally very expensive and it is useful to
understand for which graphical structures this could be cheap in order that we may make models which we
can subsequently compute with. in this chapter we discuss id136 in a cheap case, namely trees, which
has links to classical algorithms in many di   erent    elds from computer science (id145) to
physics (transfer matrix methods).

5.1 marginal id136

given a distribution p(x1, . . . , xn), id136 is the process of computing functions of the distribution.
marginal id136 is concerned with the computation of the distribution of a subset of variables, possibly
conditioned on another subset. for example, given a joint distribution p(x1, x2, x3, x4, x5) and evidence
x1 = tr, a marginal id136 calculation is

p(x1 = tr, x2, x3, x4, x5).

(5.1.1)

(cid:88)

p(x5|x1 = tr)    

x2,x3,x4

marginal id136 for discrete models involves summation and will be the focus of our development. in
principle the algorithms carry over to continuous variable models although the lack of closure of most
continuous distributions under marginalisation (the gaussian being a notable exception) can make the
direct transference of these algorithms to the continuous domain problematic. the focus here is on e   cient
id136 algorithms for marginal id136 in singly connected structures. an e   cient algorithm for multiply
connected graphs will be considered in chapter(6).

5.1.1 variable elimination in a markov chain and message passing

a key concept in e   cient id136 is message passing in which information from the graph is summarised
by local edge information. to develop this idea, consider the four variable markov chain (markov chains are

a

b

c

d

figure 5.1: a markov chain is of the form p(xt )(cid:81)t   1

t=1 p(xt|xt+1)
for some assignment of the variables to labels xt. variable elimi-
nation can be carried out in time linear in the number of variables
in the chain.

83

discussed in more depth in section(23.1))
p(a, b, c, d) = p(a|b)p(b|c)p(c|d)p(d)

marginal id136

(5.1.2)

as given in    g(5.1), for which our task is to calculate the marginal p(a). for simplicity, we assume that each
of the variables has domain {0, 1}. then

p(a = 0) =

p(a = 0, b, c, d) =

b   {0,1},c   {0,1},d   {0,1}

b   {0,1},c   {0,1},d   {0,1}

p(a = 0|b)p(b|c)p(c|d)p(d) (5.1.3)

(cid:88)

(cid:88)

we could carry out this computation by simply summing each of the probabilities for the 2    2    2 = 8
states of the variables b, c and d. this would therefore require 7 addition-of-two-numbers calls.

(cid:88)

a more e   cient approach is to push the summation over d as far to the right as possible:

p(a = 0) =

b   {0,1},c   {0,1}

p(a = 0|b)p(b|c)

(cid:124)

d   {0,1}

p(c|d)p(d)

(cid:123)(cid:122)

  d(c)

(cid:125)

(5.1.4)

where   d (c) is a (two state) potential. de   ning   d (c) requires two addition-of-two-numbers calls, one call
for each state of c. similarly, we can distribute the summation over c as far to the right as possible:

(cid:88)

(cid:88)

b   {0,1}

(cid:88)

b   {0,1}

p(a = 0) =

then,    nally,

p(a = 0) =

(cid:88)

(cid:124)

p(a = 0|b)

c   {0,1}

p(b|c)  d (c)

(cid:123)(cid:122)

  c(b)

(cid:125)

p(a = 0|b)  c (b)

(5.1.5)

(5.1.6)

3    2     1 = 5 addition-of-two-numbers calls, compared to
by distributing the summations we have made
23     1 = 7 from the naive approach. whilst this saving may not appear much, the important point is that
the number of computations for a chain of length t + 1 would be linear, 2t , as opposed to exponential,
2t     1 for the naive approach.
this procedure is called variable elimination since each time we sum over the states of a variable we elimi-
nate it from the distribution. we can always perform variable elimination in a chain e   ciently since there is
a natural way to distribute the summations, working inwards from the edges. note that in the above case,
the potentials are in fact always distributions     we are just recursively computing the marginal distribution
of the right leaf of the chain.

@@

one can view the elimination of a variable as passing a message (information) to a neighbouring node on
the graph. we can calculate a univariate-marginal of any tree (singly connected graph) by starting at a
leaf of the tree, eliminating the variable there, and then working inwards, nibbling o    each time a leaf of
the remaining tree. provided we perform elimination from the leaves inwards, then the structure of the
remaining graph is simply a subtree of the original tree, albeit with the id155 table entries
modi   ed. this is guaranteed to enable us to calculate any marginal p(xi) using a number of summations
which scales linearly with the number of variables in the tree.

finding conditional marginals for a chain

consider the following id136 problem,    g(5.1) : given

p(a, b, c, d) = p(a|b)p(b|c)p(c|d)p(d),
   nd p(d|a). this can be computed using

(cid:88)

(cid:88)

p(a, b, c, d) =

b,c

b,c

p(a|b)p(b|c)p(c|d)p(d) =

p(d|a)    

84

(cid:88)

c

(cid:88)
(cid:124)

b

p(a|b)p(b|c)

(cid:123)(cid:122)

  b(c)

(cid:125)

(5.1.7)

p(c|d)p(d)       c (d)

(5.1.8)

draft november 9, 2017

marginal id136

the missing proportionality constant is found by repeating the computation for all states of variable d.
since we know that p(d|a) = k  c (d), where   c (d) is the unnormalised result of the summation, we can use

the fact that(cid:80)

d p(d|a) = 1 to infer that k = 1/(cid:80)

d   c (d).

in this example, the potential   b (c) is not a distribution in c, nor is   c (d). in general, one may view variable
elimination as the passing of messages in the form of potentials from nodes to their neighbours. for belief
networks variable elimination passes messages that are distributions when following the direction of the
edge, and non-normalised potentials when passing messages against the direction of the edge.

remark 5.1 (variable elimination in trees as id127). variable elimination is related to the
associativity of id127. for equation (5.1.2) above, we can de   ne matrices

[mab]i,j = p(a = i|b = j),
[mcd]i,j = p(c = i|d = j),

[mbc]i,j = p(b = i|c = j),
[md]i = p(d = i),

[ma]i = p(a = i)

then the marginal ma can be written

ma = mabmbcmcdmd = mab(mbc(mcdmd))

(5.1.9)

(5.1.10)

since id127 is associative. this matrix formulation of calculating marginals is called the
transfer matrix method, and is particularly popular in the physics literature[27].

example 5.1 (where will the    y be?).

you live in a house with three rooms, labelled 1, 2, 3. there is a door between rooms 1 and 2 and another
between rooms 2 and 3. one cannot directly pass between rooms 1 and 3 in one time-step. an annoying    y
is buzzing from one room to another and there is some smelly cheese in room 1 which seems to attract the
   y more. using xt to indicate which room the    y is in at time t, with dom(xt) = {1, 2, 3}, the movement of
the    y can be described by a transition

the matrix m is called    stochastic    meaning that, as required of a id155 table, its columns
i=1 mij = 1. given that the    y is in room 1 at time t = 1, what is the id203 of room

occupancy at time t = 5? assume a markov chain which is de   ned by the joint distribution

where mij is an element of the transition matrix

p(xt+1 = i|xt = j) = mij

       0.7 0.5

0
0.3 0.3 0.5
0
0.2 0.5

      

m =

sum to 1, (cid:80)3

t   1(cid:89)

t=1

p(x1, . . . , xt ) = p(x1)

p(xt+1|xt)

we are asked to compute p(x5|x1 = 1) which is given by

p(x5|x4)p(x4|x3)p(x3|x2)p(x2|x1 = 1)

(cid:88)

x4,x3,x2

(5.1.11)

(5.1.12)

(5.1.13)

(5.1.14)

since the graph of the distribution is a markov chain, we can easily distribute the summation over the terms.
this is most easily done using the transfer matrix method, giving

p(x5 = i|x1 = 1) = [m4v]i

draft november 9, 2017

(5.1.15)

85

where v is a vector with components (1, 0, 0)t, re   ecting the evidence that at time t = 1 the    y is in room
1. computing this we have (to 4 decimal places of accuracy)

       0.5746

0.3180
0.1074

      

m4v =

marginal id136

(5.1.16)

similarly, at time t = 6, the occupancy probabilities are (0.5612, 0.3215, 0.1173). the room occupancy
id203 is converging to a particular distribution     the stationary distribution of the markov chain. one
might ask where the    y is after an in   nite number of time-steps. that is, we are interested in the large t
behaviour of

p(xt+1) =

p(xt+1|xt)p(xt)

(5.1.17)

(cid:88)

xt

at convergence p(xt+1) = p(xt). writing p for the vector describing the stationary distribution, this means

p = mp

(5.1.18)

distribution is (0.5435, 0.3261, 0.1304). note that software packages usually return eigenvectors with(cid:80)
1     the unit eigenvector therefore will usually require normalisation to make this a id203 with(cid:80)

in other words, p is the eigenvector of m with eigenvalue 1[135]. computing this numerically, the stationary
i e2
i =
i ei =

1.

5.1.2 the sum-product algorithm on factor graphs

both markov and belief networks can be represented using factor graphs. for this reason it is convenient to
derive a marginal id136 algorithm for fgs since this then applies to both markov and belief networks.
this is termed the sum-product algorithm since to compute marginals we need to distribute the sum over
variable states over the product of factors. in other texts, this is also referred to as belief propagation.

non-branching graphs : variable to variable messages

consider the distribution

p(a, b, c, d) = f1 (a, b) f2 (b, c) f3 (c, d) f4 (d)

(5.1.19)

which has the factor graph represented in    g(5.2). to compute the marginal p(a, b, c), since the variable d
only occurs locally, we use

p(a, b, c) =

p(a, b, c, d) =

f1 (a, b) f2 (b, c) f3 (c, d) f4 (d) = f1 (a, b) f2 (b, c)

f3 (c, d) f4 (d)

(5.1.20)

(cid:88)
(cid:124)

d

(cid:123)(cid:122)

  d   c(c)

(cid:125)

here   d   c (c) de   nes a message from node d to node c and is a function of the variable c. similarly,

(cid:88)

d

d

(cid:88)

(cid:88)

p(a, b) =

p(a, b, c) = f1 (a, b)

f2 (b, c)   d   c (c)

(cid:88)
(cid:124)

c

(cid:123)(cid:122)

  c   b(b)

(cid:125)

c

(cid:88)

c

hence

  c   b (b) =

f2 (b, c)   d   c (c)

(5.1.21)

(5.1.22)

it is clear how one can recurse this de   nition of messages so that for a chain of n variables the marginal
of the    rst node can be computed in time linear in n. the term   c   b (b) can be interpreted as carrying

86

draft november 9, 2017

marginal id136

f1

a

b

f2

f3

c

d

f4

figure 5.2: for singly connected structures without
branches, simple messages from one variable to its
neighbour may be de   ned to form an e   cient marginal
id136 scheme.

marginal information from the graph beyond c. for simple linear structures with no branching, messages
from variables to variables are su   cient. however, as we will see below in more general structures with
branching, it is useful to consider two types of messages, namely those from variables to factors and vice
versa.

general singly connected factor graphs

the slightly more complex example,

p(a|b)p(b|c, d)p(c)p(d)p(e|d)

has the factor graph depicted in    g(5.3)

f1 (a, b) f2 (b, c, d) f3 (c) f4 (d, e) f5 (d)

the marginal p(a, b) can be represented by an amputated graph with a message, since

p(a, b) = f1 (a, b)

f2 (b, c, d) f3 (c) f5 (d)

(cid:88)
(cid:124)

c,d

(cid:88)

e

f4 (d, e)

(cid:125)

(cid:123)(cid:122)

  f2   b(b)

where   f2   b (b) is a message from a factor to a variable. this message can be constructed from messages
arriving from the two branches through c and d, namely

  f2   b (b) =

f2 (b, c, d) f3 (c)

f5 (d)

f4 (d, e)

(cid:88)

c,d

(cid:124)(cid:123)(cid:122)(cid:125)

  c   f2 (c)

(cid:124)

(cid:88)
(cid:123)(cid:122)

e

  d   f2 (d)

(cid:125)

similarly, we can interpret

(cid:124)(cid:123)(cid:122)(cid:125)

  d   f2 (d) = f5 (d)
  f5   d(d)

(cid:88)
(cid:124)

e

f4 (d, e)

(cid:123)(cid:122)

(cid:125)

  f4   d(d)

(cid:88)
(cid:124)

b

p(a) =

f1 (a, b)   f2   b (b)

(cid:123)(cid:122)

  f1   a(a)

(cid:125)

to complete the interpretation we identify   c   f2 (c)       f3   c (c). in a non-branching link, one can more
simply use a variable to variable message. to compute the marginal p(a), we then have

a

f1

b

f2

f3

e

f4

c

d

f5

figure 5.3: for a branching singly connected graph,
it is useful to de   ne messages from both factors to
variables, and variables to factors.

draft november 9, 2017

87

(5.1.23)

(5.1.24)

(5.1.25)

(5.1.26)

(5.1.27)

(5.1.28)

for consistency of interpretation, one also can view the above as

  f1   a (a) =

(cid:88)

b

(cid:124)

(cid:123)(cid:122)

f1 (a, b)   f2   b (b)
  b   f1 (b)

(cid:125)

marginal id136

(5.1.29)

we can now see how a message from a factor to a node is formed from summing the product of incoming
node-to-factor messages. similarly, a message from a node to a factor is given by the product of incoming
factor-to-node messages.

a convenience of this approach is that the messages can be reused to evaluate other marginal id136s.
for example, it is clear that p(b) is given by

p(b) =

f1 (a, b)

  f2   b (b)

(cid:88)
(cid:124)

a

  f1   b(b)

(cid:125)

(cid:123)(cid:122)
(cid:88)

(5.1.30)

(5.1.31)

if we additionally desire p(c), we need to de   ne the message from f2 to c,

  f2   c (c) =

f2 (b, c, d)   b   f2 (b)   d   f2 (d)

b,d

where   b   f2 (b)       f1   b (b). this demonstrates the reuse of already computed message from d to f2 to
compute the marginal p(c).

de   nition 5.1 (message schedule). a message schedule is a speci   ed sequence of message updates. a valid
schedule is that a message can be sent from a node only when that node has received all requisite messages
from its neighbours. in general, there is more than one valid updating schedule.

sum-product algorithm

the sum-product algorithm is described below in which messages are updated as a function of incoming
messages. one then proceeds by computing the messages in a schedule that allows the computation of a
new message based on previously computed messages, until all messages from all factors to variables and
vice-versa have been computed.

(cid:81)

procedure 5.1 (sum-product messages on factor graphs).

given a distribution de   ned as a product on subsets of the variables, p(x ) = 1
factor graph is singly connected we can carry out summation over the variables e   ciently.

z

f   f (xf ), provided the

initialisation messages from leaf node factors are initialised to the factor. messages from leaf variable

nodes are set to unity.

variable to factor message

  x   f (x) =

g   {ne(x)\f}

(cid:89)

  g   x (x)

f2

f1

f3

  
f
1   

x(x)

  f2   x (x)

x ( x )

   

f

3

  

  x   f (x)

f

x

88

draft november 9, 2017

marginal id136

factor to variable message

(cid:88)

(cid:89)

  f   x (x) =

  f (xf )

we write(cid:80)xf\x to denote summation over all states in the set

y   {ne(f )\x}

xf\x

  y   f (y)

of variables xf\x.

marginal

(cid:89)

f   ne(x)

p(x)    

  f   x (x)

y1

  
y
1   

f(y

1)

  y2   f (y2)
( y

   

f

y

3

  

y3

  f1   x(x)

( x )

   f 2     x

y2

f1

f2

f

  f   x (x)

x

3 )

x

  x   f3(x)

f3

for marginal id136, the important information is the relative size of the message states so that we may
renormalise messages as we wish. since the marginal will be proportional to the incoming messages for that
variable, the normalisation constant is trivially obtained using the fact that the marginal must sum to 1.
however, if we wish to also compute any normalisation constant using these messages, we cannot normalise
the messages since this global information will then be lost.

5.1.3 dealing with evidence

for a distribution which splits into evidential and non-evidential variables, x = xe     xn, the marginal of a
non-evidential variable p(xi,xe) is given by summing over all the variables in xn (except for xi) with xe set
into their evidential states. there are two ways to reconcile this with the factor graph formalism. either
we can simply say that by setting the variables xe, we de   ne a new factor graph on xn, and then pass
messages on this new factor graph. alternatively, we can de   ne the potentials which contain the variables
xe by multiplying each potential that contains an evidential variable by a delta function (an indicator) that
is zero unless the variable
xe is in the speci   ed evidential state. when we then perform a factor to variable
message, the sum of this modi   ed potential over any of the evidential variable states will be zero except
for that state which corresponds to the evidential setting. another way to view this is that the sum in
the factor to variable message is only over the non-evidential variables, with any evidential variables in the
potential set into their evidential states.

@@

5.1.4 computing the marginal likelihood

for a distribution de   ned as products over potentials   f (xf )

p(x ) =

  f (xf )

the normalisation is given by

z =

  f (xf )

x

f

(cid:88)

(cid:88)

1
z

(cid:89)
(cid:89)

f

(cid:89)

(cid:88)

a,c

x

f   ne(x)

p(b, d) =

p(a|b)p(b|c)p(c|d)p(d).

draft november 9, 2017

to compute this summation e   ciently we take the product of all incoming messages to an arbitrarily chosen
variable x and then sum over the states of that variable:

z =

  f   x (x)

(5.1.34)

if the factor graph is derived from setting a subset of variables of a bn in evidential states then the
summation over all non-evidential variables will yield the marginal on the visible (evidential) variables. for
example

(5.1.32)

(5.1.33)

(5.1.35)

89

marginal id136

this can be interpreted as requiring the sum over a product of suitably de   ned factors. hence one can
readily    nd the marginal likelihood of the evidential variables for singly connected bns.

log messages

for the above method to work, the absolute (not relative) values of the messages are required, which
prohibits renormalisation at each stage of the message passing procedure. however, without normalisation
the numerical value of messages can become very small, particularly for large graphs, and numerical precision
issues can occur. a remedy in this situation is to work with log messages,

for this, the variable to factor messages

   = log   

  x   f (x) =

become simply

  x   f (x) =

  g   x (x)

  g   x (x)

(cid:89)

(cid:89)

g   {ne(x)\f}

(cid:88)

g   {ne(x)\f}

xf\x

(cid:88)
      (cid:88)

  f   x (x) =

  f (xf )

y   {ne(f )\x}

  y   f (y)

naively, one may write

  f   x (x) = log

  f (xf )exp

xf\x

       (cid:88)

y   {ne(f )\x}

            

  y   f (y)

more care is required for the factors to variable messages, which are de   ned by

(5.1.36)

(5.1.37)

(5.1.38)

(5.1.39)

(5.1.40)

however, the exponentiation of the log messages will cause potential numerical precision problems. a
solution to this numerical di   culty is obtained by    nding the largest value of the incoming log messages,

   
y   f = max
  

y   {ne(f )\x}   y   f (y)

then

   
  f   x (x) =   
y   f + log

       (cid:88)

      (cid:88)
(cid:16)(cid:80)
y   {ne(f )\x}   y   f (y)          
y   f

  f (xf )exp

y   {ne(f )\x}

xf\x

  y   f (y)       
(cid:17)

            

   
y   f

(5.1.41)

(5.1.42)

will be     1, with at least one term being
by construction the terms exp
equal to 1. this ensures that the dominant numerical contributions to the summation are computed accu-
rately.

log marginals are readily found using

(cid:88)

f   ne(x)

log p(x) =

  f   x (x)

(5.1.43)

90

draft november 9, 2017

other forms of id136

f4

a

d

f1

f3

(a)

b

c

f2

a

f1

f5

(b)

b

c

f2

5.1.5 the problem with loops

(a): factor graph with a loop.

(b): elimi-
figure 5.4:
nating the variable d adds an edge between a and c, demon-
strating that, in general, one cannot perform marginal in-
ference in loopy graphs by simply passing messages along
existing edges in the original graph.

loops cause a problem with variable elimination (or message passing) techniques since once a variable is
eliminated the structure of the    amputated    graph in general changes. for example, consider the fg

p(a, b, c, d) = f1 (a, b) f2 (b, c) f3 (c, d) f4 (a, d)

depicted in    g(5.4a). the marginal p(a, b, c) is given by

p(a, b, c) = f1 (a, b) f2 (b, c)

f3 (c, d) f4 (a, d)

(cid:88)
(cid:124)

d

(cid:123)(cid:122)

f5(a,c)

(cid:125)

(5.1.44)

(5.1.45)

which adds a link ac in the amputated graph, see    g(5.4b). this means that one cannot account for
information from variable d by simply updating potentials on links in the original graph     one needs to
account for the fact that the structure of the graph changes. the junction tree algorithm, chapter(6) deals
with this by combining variables to make a new singly connected graph for which the graph structure remains
singly connected under variable elimination.

5.2 other forms of id136

5.2.1 max-product

a common interest is the most likely state of distribution. that is

argmax
x1,x2,...,xn

p (x1, x2, . . . , xn)

(5.2.1)

to compute this e   ciently for trees we exploit any factorisation structure of the distribution, analogous to
the sum-product algorithm. that is, we aim to distribute the maximization so that only local computations
are required. to develop the algorithm, consider a function which can be represented as an undirected chain,

f (x1, x2, x3, x4) =   (x1, x2)  (x2, x3)  (x3, x4)
2, x   
3, x   

for which we wish to    nd the joint state x   
value of f . since potentials are non-negative, we may write

1, x   

4 which maximises f . firstly, we calculate the maximum

(5.2.2)

max

x

f (x) = max

x1,x2,x3,x4

  (x1, x2)  (x2, x3)  (x3, x4) = max
x1,x2,x3

  (x1, x2)  (x2, x3) max

  (x3, x4)

= max
x1,x2

  (x1, x2) max

  (x2, x3)  4(x3)

= max
x1,x2

  (x1, x2)  3(x2) = max

x1

x2

max

  (x1, x2)  3(x2)

(cid:124)

x3

(cid:123)(cid:122)

  3(x2)

(cid:125)

x4

(cid:124)
(cid:124)

(cid:123)(cid:122)

  4(x3)

(cid:125)

(cid:123)(cid:122)

  2(x1)

(cid:125)

the    nal equation corresponds to solving a single variable optimisation and determines both the optimal
value of the function f and also the optimal state x   
1, the optimal x2 is given
by x   
3, x4). this
procedure is called backtracking. note that we could have equally started at the other end of the chain by

  2(x1). given x   
2, x3)  4(x3), x   

1, x2)  3(x2), and similarly x   

1 = argmax
  (x   

x1
3 = argmax

2 = argmax

4 = argmax

  (x   

  (x   

x3

x4

x2

draft november 9, 2017

91

de   ning messages    that pass information from xi to xi+1. the chain structure of the function ensures that
the maximal value (and its state) can be computed in time which scales linearly with the number of factors
in the function. there is no requirement here that the function f corresponds to a id203 distribution
(though the factors must be non-negative).

other forms of id136

example 5.2. consider a distribution de   ned over binary variables:

p(a, b, c)     p(a|b)p(b|c)p(c)

with

p(a = tr|b = tr) = 0.3, p(a = tr|b = fa) = 0.2, p(b = tr|c = tr) = 0.75
p(b = tr|c = fa) = 0.1, p(c = tr) = 0.4

what is the most likely joint con   guration, argmax

p(a, b, c)?

a,b,c

(5.2.3)

naively, we could evaluate p(a, b, c) over all the 8 joint states of a, b, c and select that states with highest
id203. an alternative message passing approach is to de   ne

  c(b)     max

c

p(b|c)p(c)

for the state b = tr,

p(b = tr|c = tr)p(c = tr) = 0.75    0.4,

p(b = tr|c = fa)p(c = fa) = 0.1    0.6

hence,   c(b = tr) = 0.75    0.4 = 0.3. similarly, for b = fa,

p(b = fa|c = tr)p(c = tr) = 0.25    0.4

p(b = fa|c = fa)p(c = fa) = 0.9    0.6

hence,   c(b = fa) = 0.9    0.6 = 0.54.
we now consider

  b(a)     max

b

p(a|b)  c(b)

for a = tr, the state b = tr has value

p(a = tr|b = tr)  c(b = tr) = 0.3    0.3 = 0.09

and state b = fa has value

p(a = tr|b = fa)  c(b = fa) = 0.2    0.54 = 0.108

hence   b(a = tr) = 0.108. similarly, for a = fa, the state b = tr has value

p(a = fa|b = tr)  c(b = tr) = 0.7    0.3 = 0.21

and state b = fa has value

p(a = fa|b = fa)  c(b = fa) = 0.8    0.54 = 0.432

giving   b(a = fa) = 0.432. now we can compute the optimal state

   
a

= argmax

a

  b(a) = fa

given this optimal state, we can backtrack, giving

   
b

= argmax

b

p(a = fa|b)  c(b) = fa, c

   

= argmax

c

p(b = fa|c)p(c) = fa

(5.2.4)

(5.2.5)

(5.2.6)

(5.2.7)

(5.2.8)

(5.2.9)

(5.2.10)

(5.2.11)

(5.2.12)

(5.2.13)

note that in the backtracking process, we already have all the information required from the computation
of the messages   .

92

draft november 9, 2017

other forms of id136

if we want to    nd the most likely state for a variable in the centre of the chain we can therefore pass
messages from one end to the other followed by backtracking. this is the approach for example taken by
the viterbi algorithm for id48s, section(23.2). alternatively, we can send messages (carrying the result
of maximisations) simultaneously from both ends of the chain and then read o    the maximal state of
the variable from the state which maximises the product of incoming messages. the    rst is a sequential
procedure since we must have passed messages along the chain before we can backtrack. the second is
a parallel procedure in which messages can be sent concurrently. the latter approach can be represented
using a factor graph as described below.

using a factor graph

one can also use the factor graph to compute the joint most probable state. provided that a full schedule
of message passing has occurred, the product of messages into a variable equals the maximum value of the
joint function with respect to all other variables. one can then simply read o    the most probable state by
maximising this local potential.

procedure 5.2 (max-product messages on factor graphs).

given a distribution de   ned as a product on subsets of the variables, p(x ) = 1
factor graph is singly connected we can carry out maximisation over the variables e   ciently.

f   f (xf ), provided the

z

initialisation messages from leaf node factors are initialised to the factor. messages from leaf variable

nodes are set to unity.

(cid:81)

variable to factor message

(cid:89)

  x   f (x) =

  g   x (x)

g   {ne(x)\f}

factor to variable message

  f   x (x) = maxxf\x

  f (xf )

(cid:89)

y   {ne(f )\x}

  y   f (y)

maximal state

   

x

= argmax

x

(cid:89)

f   ne(x)

  f   x (x)

this algorithm is also called belief revision.

5.2.2 finding the n most probable states

f1

  
f
1   

x(x)

f2

  f2   x (x)

  x   f (x)

f

x

f3

y1

x ( x )

   

f

3

  

  
y
1   

f(y

1)

f

  f   x (x)

x

3 )

y2

f1

f2

  y2   f (y2)
( y

   

f

y

3

  

y3

  f1   x(x)

( x )

   f 2     x

x

  x   f3(x)

f3

it is often of interest to calculate not just the most likely joint state, but the n most probable states,
particularly in cases where the optimal state is only slightly more probable than other states. this is an
interesting problem in itself and can be tackled with a variety of methods. a general technique is given by
nilson[227] which is based on the junction tree formalism, chapter(6), and the construction of candidate

draft november 9, 2017

93

other forms of id136

lists, see for example [73].

for singly connected structures, several approaches have been developed [228, 320, 286, 273]. for the
hidden markov model, section(23.2), a simple algorithm is the n -viterbi approach which stores the n -most
probable messages at each stage of the propagation. for more general singly connected graphs one can
extend the max-product algorithm to an n -max-product algorithm by retaining at each stage the n most
probable messages, see below.

n -max-product

the algorithm for n -max-product is a minor modi   cation of the standard max-product algorithm. compu-
tationally, a straightforward way to accomplish this is to introduce an additional variable for each message
that is used to index the most likely messages. we will    rst develop this for message passing on an undi-
rected graph. consider the distribution

e

a

b

c

d

p(a, b, c, d, e) =   (e, a)  (a, b)  (b, c)  (b, d)

(5.2.14)

for which we wish to    nd the two most probable values. using the notation

max

f (x)

i

x

for the ith highest value of f (x), the maximisation over d can be expressed using the message

  d(b, 1) =

max

1

d

  (b, d),

  d(b, 2) =

max

  (b, d)

2

d

de   ning messages similarly, the 2 most likely values of p(a, b, c, d, e) can be computed using

1:2
max
a,b,c,d,e

  (e, a)  (a, b)  (b, c)  (b, d) =

1:2
max
e,ma

1:2
max
a,mb

  (e, a)

(cid:124)

1:2
max
b,mc,md

(cid:124)

  (a, b)

  (b, c)

  (b, d)

(cid:125)

(cid:124)

1:2
max

d

(cid:123)(cid:122)

  c(b,mc)

  d(b,md)

1:2
max

c

(cid:124)

(cid:123)(cid:122)
(cid:123)(cid:122)

  b(a,mb)

(cid:123)(cid:122)

  a(e,ma)

(cid:125)
(cid:125)
(cid:125)

(5.2.15)

(5.2.16)

(5.2.17)

where ma, mb, mc and md index the two highest values. at the    nal stage we now have a table with dim(e)  2
entries, from which we compute the highest two joint states of e, ma. given these    rst most likely joint
states, e   , m   
a,mb   (e   , a)  b(a, mb).
one continues backtracking, then    nding the most likely state of b, mc, md and    nally c and d. one may
then restart the backtracking using the second most likely state of e, ma and continue to    nd the most likely
states to have given rise to this, leading to the second most likely joint state.

a one then backtracks to    nd the most likely state of a, mb using arg max1:2

the translation of this to the factor graph formalism is straightforward and contained in maxnprodfg.m.
essentially the only modi   cation required is to de   ne extended messages which contain the n -most likely
messages computed at each stage. a variable to factor message consists of the product of extended mes-
sages. for a factor to variable message, all extended messages from the neighbours are multiplied together
into a large table. the n -most probable messages are retained, de   ning a new extended message. the
n -most probable states for each variable can then be read o    by    nding the variable state that maximises
the product of incoming extended messages.

branches are the bottleneck in the above computation. consider a term as part of a larger system with

z

a

b

c

  (z, a)  (a, b)  (a, c)

(5.2.18)

94

draft november 9, 2017

(5.2.19)

(5.2.20)

other forms of id136

3

5

4

1

2

6

8

7

9

figure 5.5: state transition diagram (weights not shown). the shortest
(unweighted) path from state 1 to state 7 is 1     2     7. considered as a
markov chain (random walk), the most probable path from state 1 to state
7 is 1     8     9     7. the latter path is longer but more probable since for
the path 1     2     7, the id203 of exiting from state 2 into state 7 is
1/5 (assuming each transition is equally likely). note that in this example,
states 3,4,5,6,7 are    absorbing        one can enter, but not exit these states.
this would correspond to adding self-loops on nodes 3,4,5,6,7; these are not
drawn here to improve clarity. see demomostprobablepath.m

we would like to pass a message along the branch from b to a and then from c to a and then from a to the
rest of the graph. to compute the most likely value we can    nd this from

max

a

  (z, a)

max

b

  (a, b)

max

  (a, c)

(cid:26)

(cid:26)

(cid:27)(cid:110)

(cid:27)(cid:110)

c

c

(cid:111)

(cid:111)

max

2

a

  (z, a)

max

b

  (a, b)

max

  (a, c)

which represents an e   cient approach since the maximisation can be carried out over each branch separately.
however, to    nd the second most likely value, we cannot write

for a    xed z, this would erroneously always force a to be in a di   erent state than the state corresponding to
the most likely value. similarly, we cannot assume that the second most likely state corresponds to    nding
the second most likely state of each factor. unlike the single most likely state case, therefore, we cannot
distribute the maximisations over each branch but have to search over all branch contributions concurrently.
this therefore corresponds to an exponentially complex search for the n -highest joint branch states. whilst
non branching links are non-problematic, the degree d of a variable   s node (in either the fg or undirected
representation) contributes an additional exponential term n d to the computational complexity.

5.2.3 most probable path and shortest path

what is the most likely path from state a to state b for an n state markov chain? note that this is not
necessarily the same as the shortest path, as explained in    g(5.5). if we consider a path of length t , this
has id203

p(s2|s1 = a)p(s3|s2) . . . p(st = b|st   1)

(5.2.21)

finding the most probable path can then be readily solved using the max-product (or max-sum algorithm
for the log-transitions) on a simple serial factor graph. to deal with the issue that we don   t know the optimal
t , one approach is to rede   ne the id203 transitions such that the desired state b is an absorbing state
of the chain (that is, one can enter this state but not leave it). with this rede   nition, the most probable
joint state will correspond to the most probable state on the product of n     1 transitions. this approach
is demonstrated in demomostprobablepath.m, along with the more direct approaches described below.

an alternative, cleaner approach is as follows: for the markov chain we can dispense with variable-to-factor
and factor-to-variable messages and use only variable-to-variable messages.
if we want to    nd the most
likely set of states a, s2, . . . , st   1, b to get us there, then this can be computed by de   ning the maximal path
id203 e (a     b, t ) to get from a to b in t -timesteps:

e (a     b, t ) = max
s2,...,st   1
= max

s3,...,st   1

p(s2|s1 = a)p(s3|s2)p(s4|s3) . . . p(st = b|st   1)
(cid:124)
max

p(s2|s1 = a)p(s3|s2)

p(s4|s3) . . . p(st = b|st   1)

(cid:123)(cid:122)

(cid:125)

s2

  2   3(s3)

to compute this e   ciently we de   ne messages
  t   1   t (st) p(st+1|st),

  t   t+1 (st+1) = max

st

draft november 9, 2017

t     2,

  1   2 (s2) = p(s2|s1 = a)

(5.2.22)

(5.2.23)

(5.2.24)

95

until the point

e (a     b, t ) = max
st   1

  t   2   t   1 (st   1) p(st = b|st   1) =   t   1   t (st = b)

(5.2.25)

other forms of id136

we can now proceed to    nd the maximal path id203 for timestep t + 1. since the messages up to time
t     1 will be the same as before, we need only compute one additional message,   t   1   t (st ), from which
(5.2.26)

  t   1   t (st ) p(st +1 = b|st ) =   t   t +1 (st +1 = b)

e (a     b, t + 1) = max

st

we can proceed in this manner until we reach e (a     b, n ) where n is the number of nodes in the graph.
we don   t need to go beyond this number of steps since those that do must necessarily contain non-simple
paths. (a simple path is one that does not include the same state more than once.) the optimal time t   
is then given by which of e (a     b, 2) , . . . , e (a     b, n ) is maximal. given t    one can begin to backtrack1.

since

   
e (a     b, t

) = max
st      1

  t      2   t      1 (st      1) p(st    = b|st      1)

we know the optimal state

s   
t      1 = argmax
st      1

  t      2   t      1 (st      1) p(st    = b|st      1)

we can then continue to backtrack:

s   
t      2 = argmax
st      2

  t      3   t      2 (st      2) p(s   

t      1|st      2)

and so on. see mostprobablepath.m.

(5.2.27)

(5.2.28)

(5.2.29)

    in the above derivation we do not use any properties of id203, except that p must be non-negative
(otherwise sign changes can    ip a whole sequence    id203    and the local    message recursion no
longer applies). one can consider the algorithm as    nding the optimal    product    path from a to b.

    it is straightforward to modify the algorithm to solve the (single-source, single-sink) shortest weighted
path problem. one way to do this is to replace the markov transition probabilities with exp(   u(st|st   1)),
where u(st|st   1) is the edge weight and is in   nite if there is no edge from st   1 to st. this approach
is taken in shortestpath.m which is able to deal with either positive or negative edge weights. this
method is therefore more general than the well-known dijkstra   s algorithm [122] which requires weights
to be positive. if a negative edge cycle exists, the code returns the shortest weighted length n path,
where n is the number of nodes in the graph. see demoshortestpath.m.

    the above algorithm is e   cient for the single-source, single-sink scenario, since the messages contain

only n states, meaning that the overall storage is o(n 2).

    as it stands, the algorithm is numerically impractical since the messages are recursively multiplied by
values usually less than 1 (at least for the case of probabilities). one will therefore quickly run into
numerical under   ow (or possibly over   ow in the case of non-probabilities) with this method.

to    x the    nal point above, it is best to work by de   ning the logarithm of e. since this is a monotonic
transformation, the most probable path de   ned through log e is the same as that obtained from e. in this
case

log [p(s2|s1 = a)p(s3|s2)p(s4|s3) . . . p(st = b|st   1)]

l (a     b, t ) = max
s2,...,st   1

= max

s2,...,st   1

         log p(s2|s1 = a) +

t   1(cid:88)

t=3

log p(st|st   1) + log p(st = b|st   1)

         

(5.2.30)

(5.2.31)

@@

1an alternative to    nding t    is to de   ne self-transitions with id203 1, and then use a    xed time t = n . once the
desired state b is reached, the self-transition then preserves the chain in state b for the remaining timesteps. this procedure is
used in mostprobablepathmult.m

96

draft november 9, 2017

id136 in multiply connected graphs

we can therefore de   ne new messages

  t   t+1 (st+1) = max

st

[  t   1   t (st) + log p(st+1|st)]

one then proceeds as before by    nding the most probable t    de   ned on l, and backtracks.

(5.2.32)

corresponding to this simple markov chain is the belief network (cid:81)

remark 5.2. a possible confusion is that optimal paths can be e   ciently found    when the graph is loopy   .
note that the graph in    g(5.5) is a state-transition diagram, not a graphical model. the graphical model
t p(st|st   1), a linear serial structure.

hence the underlying graphical model is a simple chain, which explains why computation is e   cient.

most probable path (multiple-source, multiple-sink)

if we need the most probable path between all states a and b, one could re-run the above single-source-
single-sink algorithm for all a and b. a computationally more e   cient approach is to observe that one can
de   ne a message for each starting state a:

  t   t+1 (st+1|a) = max

st

  t   1   t (st|a) p(st+1|st)

(5.2.33)

and continue until we    nd the maximal path id203 matrix for getting from any state a to any state b
in t timesteps:

e (a     b, t ) = max
st   1

  t   2   t   1 (st   1|a) p(st = b|st   1)

(5.2.34)

since we know the message   t   2   t   1 (st   1|a) for all states a, we can readily compute the most probable
path from all starting states a to all states b after t steps. this requires passing an n    n matrix message
  . we can then proceed to the next timestep t + 1. since the messages up to time t     1 will be the same
as before, we need only compute one additional message,   t   1   t (st ), from which

e (a     b, t + 1) = max

st

  t   1   t (st|a) p(st +1 = b|st )

(5.2.35)

in this way one can then e   ciently compute the optimal path probabilities for all starting states a and
end states b after t timesteps. to    nd the optimal corresponding path, backtracking proceeds as before,
see mostprobablepathmult.m and demomostprobablepathmult.m. one can also use the same algorithm to
solve the multiple-source, multiple-sink shortest weighted path problem using exponentiated negative edge
weights, as before. this is a variant of the floyd-warshall-roy algorithm[122].

5.2.4 mixed id136

an often encountered situation is to infer the most likely state of a joint marginal, possibly given some
evidence. for example, given a distribution p(x1, . . . , xn),    nd

argmax
x1,x2,...,xm

p(x1, x2, . . . , xm) = argmax
x1,x2,...,xm

xm+1,...,xn

p(x1, . . . , xn)

(5.2.36)

in general, even for tree structured p(x1, . . . , xn), the optimal marginal state cannot be computed e   ciently.
one way to see this is that due to the summation the resulting joint marginal does not have a structured
factored form as products of simpler functions of the marginal variables. finding the most probable joint
marginal then requires a search over all the joint marginal states     a task exponential in m. an approximate
solution is provided by the em algorithm (see section(11.2) and exercise(5.7)).

5.3 id136 in multiply connected graphs

we brie   y discuss here some relatively straightforward approaches to dealing with multiply connected graphs
that are conceptually straightforward, or build on the repeated use of singly connected structures. we discuss
a more general algorithm in chapter(6).

draft november 9, 2017

97

(cid:88)

algorithm 5.1 compute marginal p(x1|evidence) from distribution p(x) = (cid:81)
1: procedure bucket elimination(p(x) =(cid:81)

evidential variables are ordered x1, . . . , xn.

f   f ({x}f ).)

initialize all bucket potentials to unity.
while there are potentials left in the distribution do

for each potential   f ,    nd its highest variable xj (according to the ordering).
multiply   f with the potential in bucket j and remove   f the distribution.

id136 in multiply connected graphs

f   f ({x}f ). assumes non-

(cid:46) fill buckets

(cid:46) empty buckets

end while
for i = bucket n to 1 do

for bucket i sum over the states of variable xi and call this potential   i
identify the highest variable xh of potential   i
multiply the existing potential in bucket h by   i

end for
the marginal p(x1|evidence) is proportional to   1.
return p(x1|evidence)

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end procedure

(cid:46) the conditional marginal.

5.3.1 bucket elimination

we consider here a general conditional marginal variable elimination method that works for any distribu-
tion (including multiply connected graphs). bucket elimination is presented in algorithm(5.1) and can be
considered a way to organise the distributed summation[84]. the algorithm is perhaps best explained by a
simple example, as given below.

example 5.3 (bucket elimination). consider the problem of calculating the marginal p(f ) of

p(a, b, c, d, e, f, g) = p(f|d)p(g|d, e)p(c|a)p(d|a, b)p(a)p(b)p(e),

see    g(2.1a). whilst this is singly-connected, this serves to explain the general procedure.

(cid:88)

(cid:88)

p(f ) =

p(a, b, c, d, e, f, g) =

a,b,c,d,e,g

a,b,c,d,e,g

p(f|d)p(g|d, e)p(c|a)p(d|a, b)p(a)p(b)p(e)

(5.3.1)

(5.3.2)

p(f ) =

(cid:88)

(cid:32)(cid:88)

we can distribute the summation over the various terms as follows: e, b and c are end nodes, so that we
can sum over their values:

(cid:33)(cid:32)(cid:88)
b p(d|a, b)p(b)       b (a, d), (cid:80)
for convenience, let   s write the terms in the brackets as (cid:80)
  e (d, g). the term(cid:80)

e p(g|d, e)p(e)    
c p(c|a) is equal to unity, and we therefore eliminate this node directly. rearranging

(cid:33)(cid:32)(cid:88)

p(g|d, e)p(e)

p(d|a, b)p(b)

p(f|d)p(a)

p(c|a)

(cid:33)

(5.3.3)

d,a,g

e

c

b

terms, we can write

p(f ) =

p(f|d)p(a)  b (a, d)   e (d, g)

(5.3.4)

if we think of this graphically, the e   ect of summing over b, c, e is e   ectively to remove or    eliminate    those
variables. we can now carry on summing over a and g since these are end points of the new graph:

(cid:32)(cid:88)

(cid:33)(cid:32)(cid:88)

(cid:33)

p(f ) =

p(f|d)

p(a)  b (a, d)

  e (d, g)

a

g

again, this de   nes new potentials   a (d),   g (d), so that the    nal answer can be found from

(5.3.5)

(5.3.6)

(cid:88)

d,a,g

(cid:88)
(cid:88)

d

d

p(f|d)  a (d)   g (d)

p(f ) =

98

draft november 9, 2017

id136 in multiply connected graphs

e

c

b

g

a

d

f

p(e)p(g|d, e)

p(c|a)

p(b)p(d|a, b)

p(b)p(d|a, b)

  e (d, g)

  e (d, g)

p(a)

p(a)

p(a)  b (d, a)

p(a)  b (d, a)

p(f|d)

p(f|d)

p(f|d)

p(f|d)  g (d)

p(f|d)  g (d)   a (d)

  d (f )

node is eliminated from the graph. the second stage of eliminating c is trivial since(cid:80)

figure 5.6: the bucket elimination algorithm applied to the graph    g(2.1). at each stage, at least one
c p(c|a) = 1 and has

therefore been skipped over since this bucket does not send any message.

we illustrate this in    g(5.6). initially, we de   ne an ordering of the variables, beginning with the one that
we wish to    nd the marginal for     a suitable ordering is therefore, f, d, a, g, b, c, e. then starting with the
highest bucket e (according to our ordering f, d, a, g, b, c, e), we put all the potentials that mention e in the
e bucket. continuing with the next highest bucket, c, we put all the remaining potentials that mention c
in this c bucket, etc. the result of this initialisation procedure is that terms (conditional distributions) in
the dag are distributed over the buckets, as shown in the left most column of    g(5.6). eliminating then
the highest bucket e, we pass a message to node g. immediately, we can also eliminate bucket c since this
sums to unity. in the next column, we have now two fewer buckets, and we eliminate the highest remaining
bucket, this time b, passing a message to bucket a, and so on.

there are some important observations we can make about bucket elimination:

1. to compute say p(x2|evidence) we need to re-order the variables (so that the required marginal variable
is labelled x1) and repeat bucket elimination. hence each query (calculation of a marginal in this
it would be more e   cient to reuse messages, rather than
case) requires re-running the algorithm.
recalculating them each time.

2. in general, bucket elimination constructs multi-variable messages    from bucket to bucket. the storage

requirements of a multi-variable message are exponential in the number of variables of the message.

3. for trees we can always choose a variable ordering to render the computational complexity to be
linear in the number of variables. such an ordering is called perfect, de   nition(6.9), and indeed it can
be shown that a perfect ordering can always easily be found for singly connected graphs (see [93]).
however, orderings exist for which bucket elimination will be extremely ine   cient.

5.3.2 loop-cut conditioning

for multiply connected distributions we run into some di   culty with the message passing routines such as
the sum-product algorithm which are designed to work on singly connected graphs only. one way to solve
the di   culties of multiply connected (loopy) graphs is to identify nodes that, when removed, would reveal
a singly connected subgraph[237]. consider the example of    g(5.7). imagine that we wish to calculate a

draft november 9, 2017

99

message passing for continuous distributions

c

a

f

b

g

d

(a)

e

c

b

g

e

a

f

d

(b)

figure 5.7: a multiply con-
nected graph (a) reduced to a
singly connected graph (b) by
conditioning on the variable c.

marginal, say p(d). then

(cid:88)

(cid:88)

c

a,b,e,f,g

p(d) =

(cid:124)

(cid:123)(cid:122)

p(c|a)p(a)

p   (a)

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

p(d|a, b)p(b) p(f|c, d)
p   (f|d)

p(g|d, e)

(5.3.7)

where the p    potentials are not necessarily distributions. for each state of c, the product of the new po-
tentials on the variables a, b, e, f, g is singly connected, so that standard singly connected message passing
can be used to perform id136. we will need to perform id136 for each state of variable c, each state
de   ning a new singly connected graph (with the same structure) but with modi   ed potentials.

more generally, we can de   ne a set of variables c, called the loop cut set and run singly connected id136
for each joint state of the cut-set variables c. this can also be used for    nding the most likely state of a
multiply connected joint distribution as well. hence, for a computational price exponential in the loop-cut
size, we can calculate the marginals (or the most likely state) for a multiply connected distribution. however,
determining a small cut set is in general di   cult, and there is no guarantee that this will anyway be small for
a given graph. whilst this method is able to handle loops in a general manner, it is not particularly elegant
since the concept of messages now only applies conditioned on the cut set variables, and how to re-use
messages for id136 of additional quantities of interest becomes unclear. we will discuss an alternative
method for handling multiply connected distributions in chapter(6).

5.4 message passing for continuous distributions

for parametric continuous distributions p(x|  x), message passing corresponds to passing parameters    of
the distributions. for the sum-product algorithm, this requires that the operations of multiplication and
integration over the variables are closed with respect to the family of distributions. this is the case, for
example, for the gaussian distribution     the marginal (integral) of a gaussian is another gaussian, and
the product of two gaussians is a gaussian, see section(8.4). this means that we can then implement
the sum-product algorithm based on passing mean and covariance parameters. to implement this requires
some tedious algebra to compute the appropriate message parameter updates. at this stage, the complex-
ities from performing such calculations are a potential distraction, though the interested reader may refer
to demosumprodgaussmoment.m, demosumprodgausscanon.m and demosumprodgausscanonlds.m and also
chapter(24) for examples of message passing with gaussians. for more general exponential family distri-
butions, message passing is essentially straightforward, though again the speci   cs of the updates may be
tedious to work out. in cases where the operations of marginalisation and products are not closed within the
family, the distributions need to be projected back to the chosen message family. expectation propagation,
section(28.8), is relevant in this case.

5.5 summary

    for a tree-structured factor graph, non-mixed id136 is essentially linear in the number of nodes in the

graph (provided the variables are discrete or the id136 operations form a tractable closed family).

    computation on trees can be achieved using local    message passing    algorithms, analogous to dynamic

programming.

100

draft november 9, 2017

code

    the sum-product and max-product algorithm are particularly useful for computing marginal and most likely

id136s respectively.

    message-passing also holds for continuous variables based on passing messages that update the parameters

of the distribution.

    shortest-path problems can be solved using such message-passing approaches.
    id136 in non-trees (multiply connected distributions) is more complex since there is a    ll-in e   ect when

variables are eliminated that adds additional links to the graph.

    id136 in multiply connected graphs can be achieved using techniques such as cut-set conditioning which,
by conditioning on a subset of variables, reveal a singly connected structure. however, this is generally
ine   cient since the messages cannot be readily re-used.

a take-home message from this chapter is that (non-mixed) id136 in singly connected structures is usu-
ally computationally tractable. notable exceptions are when the message passing operations are not closed
within the message family, or representing messages explicitly requires an exponential amount of space. this
happens for example when the distribution can contain both discrete and continuous variables, such as the
switching linear dynamical system, which we discuss in chapter(25).

broadly speaking, id136 in multiply connected structures is more complex and may be intractable.
however, we do not want to give the impression that this is always the case. notable exceptions are:
   nding the most likely state in an attractive pairwise mn, section(28.9);    nding the most likely state and
marginals in a binary planar mn with pure interactions, see for example [128, 261]. for n variables in the
graph, a naive use of general purpose routines such as the junction tree algorithm for these id136s would

result in an o(cid:0)2n(cid:1) computation, whereas clever algorithms are able to return the exact results in o(cid:0)n 3(cid:1)

operations. of interest is bond propagation[192] which is an intuitive node elimination method to perform
marginal id136 in pure-interaction ising models.

5.6 code

the code below implements message passing on a tree structured factor graph. the fg is stored as an
adjacency matrix with the message between fg node i and fg node j given in ai,j.
factorgraph.m: return a factor graph adjacency matrix and message numbers
sumprodfg.m: sum-product algorithm on a factor graph

in general it is recommended to work in log-space in the max-product case, particularly for large graphs
since the product of messages can become very small. the code provided does not work in log space and
as such may not work on large graphs; writing this using log-messages is straightforward but leads to less
readable code. an implementation based on log-messages is left as an exercise for the interested reader.
maxprodfg.m: max-product algorithm on a factor graph
maxnprodfg.m: n -max-product algorithm on a factor graph

5.6.1 factor graph examples

for the distribution from    g(5.3), the following code    nds the marginals and most likely joint states. the
number of states of each variable is chosen at random.
demosumprod.m: test the sum-product algorithm
demomaxprod.m: test the max-product algorithm
demomaxnprod.m: test the max-n -product algorithm

5.6.2 most probable and shortest path

mostprobablepath.m: most probable path
demomostprobablepath.m: most probable versus shortest path demo

draft november 9, 2017

101

exercises

demoshortestpath.m: the shortest path demo works for both positive and negative edge weights.
negative weight cycles exist, the code    nds the best length n shortest path.
mostprobablepathmult.m: most probable path     multi-source, multi-sink
demomostprobablepathmult.m: demo of most probable path     multi-source, multi-sink

if

5.6.3 bucket elimination

the e   ciency of bucket elimination depends critically on the elimination sequence chosen. in the demon-
stration below we    nd the marginal of a variable in the chest clinic exercise using a randomly chosen
elimination order. the desired marginal variable is speci   ed as the last to be eliminated. for comparison
we use an elimination sequence based on decimating a triangulated graph of the model, as discussed in
section(6.5.1), again under the constraint that the last variable to be    decimated    is the marginal variable of
interest. for this smarter choice of elimination sequence, the complexity of computing this single marginal
is roughly the same as that for the junction tree algorithm, using the same triangulation.
bucketelim.m: bucket elimination
demobucketelim.m: demo bucket elimination

5.6.4 message passing on gaussians

the following code hints at how message passing may be implemented for continuous distributions. the
reader is referred to the brmltoolbox for further details and also section(8.4) for the algebraic manipu-
lations required to perform marginalisation and products of gaussians. the same principle holds for any
family of distributions which is closed under products and marginalisation, and the reader may wish to
implement speci   c families following the method outlined for gaussians.
demosumprodgaussmoment.m: sum-product message passing based on gaussian moment parameterisation

5.7 exercises

exercise 5.1. given a pairwise singly connected markov network of the form

   (xi, xj)

(5.7.1)

(cid:89)

i   j

p(x) =

1
z

explain how to e   ciently compute the normalisation factor (also called the partition function) z as a function
of the potentials   .

exercise 5.2. consider a pairwise markov network de   ned on binary variables:

p(x) =   (x1, x100)

  (xi, xi+1)

(5.7.2)

i=1

is it possible to compute argmax
x1,...,x100

p(x) e   ciently?

exercise 5.3. you are employed by a web start up company that designs virtual environments, in which
players can move between rooms. the rooms which are accessible from another in one time step is given by
the 100    100 matrix m, stored in virtualworlds.mat, where mij = 1 means that there is a door between
rooms i and j (mij = mji). mij = 0 means that there is no door between rooms i and j. mii = 1 meaning
that in one time step, one can stay in the same room. you can visualise this matrix by typing imagesc(m).

1. write a list of rooms which cannot be reached from room 2 after 10 time steps.

2. the manager complains that takes at least 13 time steps to get from room 1 to room 100. is this true?

3. find the most likely path (sequence of rooms) to get from room 1 to room 100.

4. if a single player were to jump randomly from one room to another (or stay in the same room), with no
preference between rooms, what is the id203 at time t (cid:29) 1 the player will be in room 1? assume
that e   ectively an in   nite amount of time has passed and the player began in room 1 at t = 1.

102

draft november 9, 2017

99(cid:89)

t(cid:89)

t=2

exercises

5. if two players are jumping randomly between rooms (or staying in the same room), explain how to
compute the id203 that, after an in   nite amount of time, at least one of them will be in room 1?
assume that both players begin in room 1.

exercise 5.4. consider the hidden markov model:

p(v1, . . . , vt , h1, . . . , ht ) = p(h1)p(v1|h1)

p(vt|ht)p(ht|ht   1)

(5.7.3)

in which dom(ht) = {1, . . . , h} and dom(vt) = {1, . . . , v } for all t = 1, . . . , t .

1. draw a belief network representation of the above distribution.

2. draw a factor graph representation of the above distribution.

3. use the factor graph to derive a sum-product algorithm to compute marginals p(ht|v1, . . . , vt ). explain

the sequence order of messages passed on your factor graph.

4. explain how to compute p(ht, ht+1|v1, . . . , vt ).
5. show that the belief network for p(h1, . . . , ht ) is a simple linear chain, whilst p(v1, . . . , vt ) is a fully

connected cascade belief network.

exercise 5.5. for a singly connected markov network, p(x) = p(x1, . . . , xn), the computation of a marginal
p(xi) can be carried out e   ciently. similarly, the most likely joint state x    = arg maxx1,...,xn p(x) can be
computed e   ciently. explain when the most likely joint state of a marginal can be computed e   ciently, i.e.
under what circumstances could one e   ciently (in o (m) time) compute argmax
p(x1, . . . , xm) for m < n?
x1,x2,...,xm

exercise 5.6. consider the internet with webpages labelled 1, . . . , n . if webpage j has a link to webpage i,
then we place an element of the matrix lij = 1, otherwise lij = 0. by considering a random jump from
webpage j to webpage i to be given by the transition id203

(5.7.4)

lij(cid:80)

i lij

mij =

t(cid:89)

t=2

what is the id203 that after an in   nite amount of random sur   ng, one ends up on webpage i? how
could you relate this to the potential    relevance    of a webpage in terms of a search engine?

exercise 5.7. a special time-homogeneous hidden markov model is given by

p(x1, . . . , xt , y1, . . . , yt , h1, . . . , ht ) = p(x1|h1)p(y1|h1)p(h1)

p(ht|ht   1)p(xt|ht)p(yt|ht)

(5.7.5)

the variable xt has 4 states, dom(xt) = {a, c, g, t} (numerically labelled as states 1,2,3,4). the variable
yt has 4 states, dom(yt) = {a, c, g, t}. the hidden or latent variable ht has 5 states, dom(ht) = {1, . . . , 5}.
the id48 models the following (   ctitious) process:

in humans, z-factor proteins are a sequence on states of the variables x1, x2, . . . , xt . in bananas z-factor
proteins are also present, but represented by a di   erent sequence y1, y2, . . . , yt . given a sequence x1, . . . , xt
from a human, the task is to    nd the corresponding sequence y1, . . . , yt in the banana by    rst    nding the
most likely joint latent sequence, and then the most likely banana sequence given this optimal latent sequence.
that is, we require

argmax
y1,...,yt

   
p(y1, . . . , yt|h
1, . . . , h

   
t )

where

   
1, . . . , h
h

   
t = argmax
h1,...,ht

p(h1, . . . , ht|x1, . . . , xt )

(5.7.6)

(5.7.7)

the    le banana.mat contains the emission distributions pxgh (p(x|h)), pygh (p(y|h)) and transition phtghtm
(p(ht|ht   1)). the initial hidden distribution is given in ph1 (p(h1)). the observed x sequence is given in x.

draft november 9, 2017

103

exercises

1. explain mathematically and in detail how to compute the optimal y-sequence, using the two-stage

procedure as stated above.

2. write a matlab routine that computes and displays the optimal y-sequence, given the observed

x-sequence. your routine must make use of the factor graph formalism.

3. explain whether or not it is computationally tractable to compute

argmax
y1,...,yt

p(y1, . . . , yt|x1, . . . , xt )

(5.7.8)

4. bonus question: by considering y1, . . . , yt as parameters, explain how the em algorithm, section(11.2),
p(y1, . . . , yt|x1, . . . , xt ). implement this approach with a suitable initiali-

may be used to    nd argmax
y1,...,yt

sation for the optimal parameters y1, . . . , yt .

++

exercise 5.8.
there are a set of    rstnames: david, anton, fred, jim, barry which are numbered 1
(david) to 5 (barry) and a set of surnames: barber, ilsung, fox, chain, fitzwilliam, quinceadams,
grafvonunterhosen, which are numbered 1 (barber) to 7 (grafvonunterhosen). a string is generated by
   rst randomly sampling a character from a to z. then we either continue this random character sampling
with id203 0.8, or we start to generate a    rstname. a    rstname is chosen uniformly at random.
after generating a    rstname, we generate a character at random. we continue to generate another letter at
random with id203 0.8, or start to generate a surname (chosen uniformly from the set of surnames). we
continue until the last letter of the surname is generated. the process then goes back to start (unless we are
at the end timepoint t = 10000). for example, we might generate then dtyjimdfilsungffdavidmjfox....
the catch is that the process of character generation is very noisy. the character we want to generate is only
generated correctly with id203 0.3, with id203 0.7 that another character (uniformly at random)
will be generated. given the 10000 character sequence in the    le noisystring.mat, you must decode the
noisystring to    nd the most likely intended    clean    sequence. once you have this sequence, you will be able to
   nd a set of (   rstname,surname) pairs as you traverse along the clean sequence. construct a matrix m(i, j)
with the counts of the number of occurrences of the pair (f irstname(i), surname(j)) in your clean sequence
and display this matrix.

a security company is employed to keep watch on the behaviour of people moving through
exercise 5.9.
a train station. using video cameras they are able to track the x, y position of 500 people. the matrix in
drunkproblemx.mat contains the position of the 500 people through time, with a 1 in the matrix representing
that a person is occupying that position. this matrix is generated using the program drunkmover.m which
you may wish to examine. if a person moves out of the grid, they are moved back into a random position in
the grid, as described in drunkmover.m. all but one of the people move only to a single neighbouring point
on the x-y grid in one time step. however, person 1 is a fast and dangerous drunk that we want to track.
the drunk can move from (x, y) to (x    2, y    2) at the next time step.
devise a method to track where you think the dangerous drunk is and give a list of the (x, y) coordinates of
the single most likely path the drunk took through the station.

++

exercise 5.10.
the    le bearbulldata contains the price of an asset through t = 200 timepoints. the
price takes values from 1 to 100. if the market is in a    bear    state, the price changes from time t     1 to t
with id203 transition matrix pbear(t, t     1). if the market is in the    bull    state, the price changes from
time t     1 to t with transition matrix pbull(t, t     1). if the market is in a    bear    state it will remain so with
id203 0.8. if the market is in a bull state it will remain so with id203 0.7. you may assume that
at timestep 1, the market is uniformly in either a bear or bull state and also that the price distribution at
timestep 1 is uniform. use this model to compute the id203 of the price at time t + 1 given all the
observed prices from time 1 to t; that is p(price(t + 1)|price(1 : t )). using this id203, compute the
expected gain price(t + 1)     price(t ) in the price of the asset, and also the standard deviation in this price
gain price(t + 1)     price(t ).
exercise 5.11.
you    nd yourself adrift in outer space after a space station disaster. your rocket suit has
only very simple controls that allow you to accelerate at time t an amount ai(t)     {   1, 0, 1} in each of three
draft november 9, 2017
104

++

++

exercises

       a1(t)

a2(t)
a3(t)

       ,

at =

dimensions of space i     {1, 2, 3} independently. at each time t, you can therefore command your spacesuit
to provide an acceleration vector

ai(t)     {   1, 0, +1}

(5.7.9)

according to discrete time newton   s laws, this changes the velocity vt and position xt according to the
markovian updates

vt+1 = vt +   at
xt+1 = xt +   vt

(5.7.10)

(5.7.11)

where    = 0.1 is a given value corresponding to a unit of real time. each time an acceleration +1 or    1
is applied a unit amount of fuel is used. your task is to get from the origin x1 = (0, 0, 0)t at time 1 to
rendezvous with the rescue station at x102 = (4.71,   6.97, 8.59)t at time 102 using the minimum amount
of fuel (it doesn   t matter what speed you have, as long as you rendezvous with the rescue station). at time

t = 1, you are stationary, v1 = (0, 0, 0)t. the total amount of fuel used is(cid:80)100

(cid:80)3

t=1

i=1 |ai(t)|.

1. for each of the three spatial dimensions i     {1, 2, 3}, write down a single equation that relates ai(t),

t     {1, 2, . . . , 100} to the rescue station at x102.

2. what is the minimum amount of fuel that can be used to rendezvous with the rescue station at time

102?

3. assuming that there is a sequence a1, . . . , a100 that will obtain a rendezvous with the rescue station at
time 102, explain if you believe there is an e   cient way to calculate the minimum amount of fuel. if
there is an e   cient algorithm, state it. otherwise explain why no e   cient algorithm is available.

++

++

an algorithm exists (for example dijkstra   s algorithm) that    nds the minimum weighted
exercise 5.12.
path between two speci   ed nodes on a graph; the algorithm however only generally works if all weights are
non-negative. you have a minimum weighted path problem but with weights uij that can be negative. a
friend suggests that you can still use dijkstra   s algorithm by making a new set of edge weights that are all

non-negative, wij = uij     u    where u    is the minimum value of all the uij weights. explain why this will not

generally give the correct minimum weight path.

exercise 5.13.
it   s the year 2515 and the tax collector simo hurtta needs to go on a trip starting from
planet 1 and ending on planet 1725. interplanetary travel is only possible between certain planets, with the

(cid:113)
(xi     xj)2 minus
cost of going from planet i to planet j equal to the euclidian distance between the planets
the tax that he will collect on planet j. given the set of planet positions x, interplanetary travel possibilities
(the matrix aij = 1 if it is possible to go from planet i to planet j and collectable taxes for each planet (t)
(see simohurtta.mat), what is the minimum cost of travelling from planet 1 to planet 1725? (assume that
hurrta does not collect taxes from planet 1.)

draft november 9, 2017

105

exercises

106

draft november 9, 2017

chapter 6

the junction tree algorithm

when the distribution is multiply-connected it would be useful to have a generic id136 approach that is
e   cient in its reuse of messages. in this chapter we discuss an important structure, the junction tree, that by
id91 variables enables one to perform message-passing e   ciently (although the structure on which the
message-passing occurs may consist of intractably large clusters). the most important thing is the junction
tree itself, based on which di   erent message-passing procedures can be considered. the junction tree helps
forge links with the computational complexity of id136 in    elds from computer science to statistics and
physics.

6.1 id91 variables

in chapter(5) we discussed e   cient id136 for singly-connected graphs, for which variable elimination and
message passing schemes are appropriate. in the multiply connected case, however, one cannot in general
perform id136 by passing messages only along existing links in the graph. the idea behind the junction
tree algorithm (jta) is to form a new representation of the graph in which variables are clustered together,
resulting in a singly-connected graph in the cluster variables (albeit on a di   erent graph). the main focus
of the development will be on marginal id136, though similar techniques apply to di   erent id136s,
such as    nding the most probable state of the distribution.

at this stage it is important to point out that the jta is not a magic method to deal with intractabilities
resulting from multiply connected graphs; it is simply a way to perform correct id136 on a multiply
connected graph by transforming to a singly connected structure. carrying out the id136 on the resulting
junction tree may still be computationally intractable. for example, the junction tree representation of a
general two-dimensional ising model is a single supernode containing all the variables. id136 in this case
is exponentially complex in the number of variables. nevertheless, even in cases where implementing the
jta may be intractable, the jta provides useful insight into the representation of distributions that can
form the basis for approximate id136. in this sense the jta is key to understanding issues related to
representations and complexity of id136 and is central to the development of e   cient id136 algorithms.

6.1.1 reparameterisation

consider the chain

p(a, b, c, d) = p(a|b)p(b|c)p(c|d)p(d)

from the de   nition of id155, we can reexpress this as

p(a, b, c, d) =

p(a, b)
p(b)

p(b, c)
p(c)

p(c, d)
p(d)

p(d) =

p(a, b)p(b, c)p(c, d)

p(b)p(c)

107

(6.1.1)

(6.1.2)

a

b

c

(a)

d

a, b, c

b, c

(b)

b, c, d

6.1:

figure
  (a, b, c)  (b, c, d).
representation of (a).

(a) markov

(b) clique

clique graphs

network
graph

a useful insight is that the distribution can therefore be written as a product of marginal distributions, di-
vided by a product of the intersection of the marginal distributions: looking at the numerator p(a, b)p(b, c)p(c, d)
this cannot be a distribution over a, b, c, d since we are overcounting b and c, where this overcounting of
b arises from the overlap between the sets {a, b} and {b, c}, which have b as their intersection. similarly,
the overcounting of c arises from the overlap between the sets {b, c} and {c, d}.
intuitively, we need to
correct for this overcounting by dividing by the distribution on the intersections. given the transformed
representation as a product of marginals divided by a product of their intersection (the right equation in
(6.1.2)), a marginal such as p(a, b) can be read o    directly from the factors in the new expression. the aim
of the junction tree algorithm is to form just such a representation of the distribution which contains the
marginals explicitly. we want to do this in a way that works for belief and markov networks, and also deals
with the multiply connected case. in order to do so, an appropriate way to parameterise the distribution is
in terms of a clique graph, as described in the next section.

6.2 clique graphs

de   nition 6.1 (clique graph). a clique graph consists of a set of potentials,   1(x 1), . . . ,   n(x n) each
de   ned on a set of variables x i. for neighbouring cliques on the graph, de   ned on sets of variables x i and
x j, the intersection x s = x i     x j is called the separator and has a corresponding potential   s(x s). a
clique graph represents the function

(6.2.1)

(cid:81)
(cid:81)

c   c(x c)
s   s(x s)

for notational simplicity we will usually drop the clique potential index c. graphically clique potentials are
represented by circles/ovals, and separator potentials by rectangles.

x 1

x 1     x 2

x 2

the graph on the left represents   (x 1)  (x 2)/  (x 1     x 2).

clique graphs translate markov networks into structures convenient for carrying out id136. consider
the markov network in    g(6.1a)

p(a, b, c, d) =

  (a, b, c)  (b, c, d)

z

(6.2.2)

an equivalent representation is given by the clique graph in    g(6.1b), de   ned as the product of the numerator
clique potentials, divided by the product of the separator potentials. in this case the separator potential
may be set to the normalisation constant z. by summing we have

zp(a, b, c) =   (a, b, c)

  (b, c, d),

zp(b, c, d) =   (b, c, d)

  (a, b, c)

(6.2.3)

multiplying the two expressions, we have

z2p(a, b, c)p(b, c, d) =

  (a, b, c)

(cid:33)(cid:32)

  (b, c, d)

  (b, c, d)

(cid:88)

a

a

(cid:33)

  (a, b, c)

= z2p(a, b, c, d)

(cid:88)

a,d

p(a, b, c, d)

(cid:88)

d

108

draft november 9, 2017

(cid:88)

d

(cid:32)

(cid:88)

clique graphs

in other words

p(a, b, c, d) =

p(a, b, c)p(b, c, d)

p(c, b)

(6.2.4)

(6.2.5)

the important observation is that the distribution can be written in terms of its marginals on the variables
in the original cliques and that, as a clique graph, it has the same structure as before. all that has changed is
that the original clique potentials have been replaced by the marginals of the distribution and the separator
by the marginal de   ned on the separator variables   (a, b, c)     p(a, b, c),   (b, c, d)     p(b, c, d), z     p(c, b).
the usefulness of this representation is that if we are interested in the marginal p(a, b, c), this can be read o   
from the transformed clique potential. to make use of this representation, we require a systematic way of
transforming the clique graph potentials so that at the end of the transformation the new potentials contain
the marginals of the distribution.

remark 6.1. note that, whilst visually similar, a factor graph and a clique graph are di   erent rep-
resentations. in a clique graph the nodes contain sets of variables, which may share variables with other
nodes.

6.2.1 absorption

consider neighbouring cliques v and w, sharing the variables s in common. in this case, the distribution
on the variables x = v     w is

p(x ) =

   (v)   (w)

   (s)

and our aim is to    nd a new representation

p(x ) =

     (v)      (w)

     (s)

in which the potentials are given by

(6.2.6)

   (v)

   (s)

   (w)

(6.2.7)

     (v)

     (s)

     (w)

     (v) = p(v),      (w) = p(w),      (s) = p(s)

(6.2.8)

@@ in this example, we can explicitly work out the new potentials as function of the old potentials by computing

(cid:88)

v\s

the marginals as follows:

p(w) =

p(x ) =

and

(cid:88)

w\s

p(v) =

p(x ) =

(cid:88)

v\s

(cid:88)

w\s

   (v)   (w)

   (s)

=    (w)

   (v)   (w)

   (s)

=    (v)

(cid:80)v\s    (v)
(cid:80)w\s    (w)

   (s)

   (s)

there is a symmetry present in the two equations above     they are the same under interchanging v and
w. one way to describe these equations is through    absorption   . we say that the cluster w    absorbs   
information from cluster v by the following updating procedure. first we de   ne a new separator

(cid:88)

v\s

   

  

(s) =

   (v)

and re   ne the w potential using

   

  

      (s)
   (s)
draft november 9, 2017

(w) =    (w)

(6.2.9)

(6.2.10)

(6.2.11)

(6.2.12)

109

clique graphs

figure 6.2: an example absorption schedule on a
clique tree. many valid schedules exist under the con-
straint that messages can only be passed to a neigh-
bour when all other messages have been received.

a

4

3

       

b

6

   

1

    2    

7    

   

5

d

8

      9

f

e

c

   

1

0

the advantage of this interpretation is that the new representation is still a valid clique graph representation
of the distribution since

   (v)      (w)

      (s)

=

   (v)   (w)      (s)

  (s)

      (s)

   (v)   (w)

=

   (s)

= p(x )

(6.2.13)

for this simple two clique graph, we see that after w absorbs information from v then       (w) = p(w),
which can be veri   ed by comparing the right of equation (6.2.9) with equation (6.2.12). with these new
updated potentials, we now go back along the graph, from w to v. after v absorbs information from w
then       (v) contains the marginal p(v). after the separator s has participated in absorption along both
directions, then the separator potential will contain p(s) (this is not the case after only a single absorption).
to see this, consider absorbing from w to v using the updated potentials       (w) and       (s)

(cid:88)

w\s

      

  

(s) =

   

  

(w) =

   

  

(v) =

   (v)         (s)

      (s)

=

w\s

(cid:88)

(cid:88)
   (w)      (s)
   (v)(cid:80)w\s    (w)      (s)/   (s)

{w   v}\s

   (s)

=

      (s)

   (w)   (v)

   (s)

(cid:80)w\s    (v)   (w)

=

   (s)

continuing, we have the new potential       (v) given by

= p(s)

(6.2.14)

= p(v)

(6.2.15)

hence, in terms of equation (6.2.7), the new representation is      (v) =       (v),      (s) =          (s),      (w) =
      (w).

de   nition 6.2 (absorption).

   (v)

      (s)

      (w)

let v and w be neighbours in a clique graph, let s be their
separator, and let    (v),    (w) and    (s) be their potentials.
absorption from v to w through s replaces the tables    (s) and
   (w) with

   

  

(s) =

   (v)

   

  

(w) =    (w)

(6.2.16)

(cid:88)

v\s

      (s)
   (s)

we say that clique w absorbs information from clique v. the
potentials are written on the clique graph (left) to highlight the
potential updating.

6.2.2 absorption schedule on clique trees

having de   ned the local message propagation approach, we need to de   ne an update ordering for absorption.
in general, a node v can send exactly one message to a neighbour w, and it may only be sent when v has
received a message from each of its other neighbours. we continue this sequence of absorptions until a
message has been passed in both directions along every link. see, for example,    g(6.2). note that there are
many valid message passing schemes in this case.

110

draft november 9, 2017

junction trees

x1

x4

x1, x4

x1, x4

x4

x4

x4

x4

x3

x2

x3, x4

(a)

x4

(b)

x2, x4

x3, x4

x2, x4

(c)

figure 6.3: (a): singly connected markov network. (b): clique graph. (c): clique tree.

de   nition 6.3 (absorption schedule). a clique can send a message to a neighbour, provided it has already
received messages from all other neighbours.

6.3 junction trees

there are a few stages we need to go through in order to transform a distribution into an appropriate
structure for id136. initially we explain how to do this for singly connected structures before moving
onto the multiply connected case.

consider the singly connected markov network,    g(6.3a)

p(x1, x2, x3, x4) =   (x1, x4)  (x2, x4)  (x3, x4)

(6.3.1)

the clique graph of this singly connected markov network is multiply connected,    g(6.3b), where the sep-
arator potentials are all set to unity. nevertheless, let   s try to reexpress the markov network in terms of
marginals. first we have the relations

p(x1, x4) =

p(x2, x4) =

p(x3, x4) =

p(x1, x2, x3, x4) =   (x1, x4)

p(x1, x2, x3, x4) =   (x2, x4)

p(x1, x2, x3, x4) =   (x3, x4)

  (x2, x4)

  (x1, x4)

  (x1, x4)

  (x3, x4)

  (x3, x4)

  (x2, x4)

(cid:88)
(cid:88)
(cid:88)

x2

x1

(cid:88)
(cid:88)
(cid:88)

x3

x3

(cid:88)
(cid:88)
(cid:88)

x1,x3

x2,x3

x1,x2

x1

x2

taking the product of the three marginals, we have

p(x1, x4)p(x2, x4)p(x3, x4) =   (x1, x4)  (x2, x4)  (x3, x4)

(cid:32)(cid:88)
(cid:124)

x1

(cid:88)

x2

  (x1, x4)

  (x2, x4)

(cid:123)(cid:122)

p(x4)2

this means that the markov network can be expressed in terms of marginals as

p(x1, x2, x3, x4) =

p(x1, x4)p(x2, x4)p(x3, x4)

p(x4)p(x4)

(6.3.2)

(6.3.3)

(6.3.4)

(6.3.5)

(6.3.6)

(cid:88)

x3

  (x3, x4)

(cid:33)2
(cid:125)

hence a valid clique graph is also given by the representation    g(6.3c). indeed, if a variable (here x4) occurs
on every separator in a clique graph loop, one can remove that variable from an arbitrarily chosen separator
in the loop. if this leaves an empty separator, we may then simply remove it. this shows that in such cases
we can transform the clique graph into a clique tree (i.e. a singly connected clique graph). provided that
the original markov network is singly connected, one can always form a clique tree in this manner.

draft november 9, 2017

111

6.3.1 the running intersection property

sticking with the above example, consider the clique tree in    g(6.3)

  (x3, x4)  (x1, x4)  (x2, x4)

  1(x4)  2(x4)

junction trees

(6.3.7)

as a representation of the distribution (6.3.1) where we set   1(x4) =   2(x4) = 1 to make this match. now
perform absorption on this clique tree:
we absorb (x3, x4) (cid:32) (x1, x4). the new separator is

(cid:88)

(cid:88)

   
1(x4) =

  

  (x3, x4)

x3

and the new potential is

   

  

(x1, x4) =   (x1, x4)

     
1(x4)
  1(x4)

=   (x1, x4)  

   
1(x4)

now (x1, x4) (cid:32) (x2, x4). the new separator is

   
2(x4) =

  

   

  

(x1, x4)

x1

and the new potential is

   

  

(x2, x4) =   (x2, x4)

     
2(x4)
  2(x4)

=   (x2, x4)  

   
2(x4)

since we   ve    hit the bu   ers    in terms of message passing, the potential   (x2, x4) cannot be updated further.
let   s examine more carefully the value of this new potential,

   

  

(x2, x4) =   (x2, x4)  

   
2(x4) =   (x2, x4)

   

  

(x1, x4)

(cid:88)

(cid:88)

x1

  (x3, x4) =

(cid:88)

p(x1, x2, x3, x4) = p(x2, x4)

(6.3.13)

(6.3.12)

=   (x2, x4)

  (x1, x4)

(cid:88)

hence the new potential      (x2, x4) contains the marginal p(x2, x4).

x1

x3

x1,x3

to complete a full round of message passing we need to have passed messages in a valid schedule along both
directions of each separator. to do so, we continue as follows:
we absorb (x2, x4) (cid:32) (x1, x4). the new separator is

(cid:88)

x2

      
2 (x4) =

  

   

  

(x2, x4)

and

      

  

(x1, x4) =   

(x1, x4)

        
2 (x4)
     
2(x4)

x2      (x2, x4) = (cid:80)

   

2 (x4) = (cid:80)
(cid:88)

      

      
1 (x4) =

x1

  

112

  

(x1, x4) = p(x4)

note that         
directions, the separator contains the marginal p(x4). the reader may show that         (x1, x4) = p(x1, x4).
finally, we absorb (x1, x4) (cid:32) (x3, x4). the new separator is

x2 p(x2, x4) = p(x4) so that now, after absorbing through both

(6.3.16)

draft november 9, 2017

(6.3.8)

(6.3.9)

(6.3.10)

(6.3.11)

(6.3.14)

(6.3.15)

junction trees

and

   

  

(x3, x4) =   (x3, x4)

        
1 (x4)
     
1(x4)

= p(x3, x4)

(6.3.17)

hence, after a full round of message passing, the new potentials all contain the correct marginals.

the new representation is consistent in the sense that for any (not necessarily neighbouring) cliques v and
w with intersection i, and corresponding potentials    (v) and    (w),

   (v) =

   (w)

(6.3.18)

(cid:88)

v\i

(cid:88)

w\i

note that bidirectional absorption following a valid schedule guarantees local consistency for neighbouring
cliques, as in the example above, provided that we started with a clique tree which is a correct representation
of the distribution. to ensure global consistency, if a variable occurs in two cliques it must be present in all
cliques on any path connecting these cliques. an extreme example would be if we removed the link between
cliques (x3, x4) and (x1, x4). in this case this is still a clique tree
(forest); however global consistency could
not be guaranteed since the information required to make clique (x3, x4) consistent with the rest of the
graph cannot reach this clique.

@@

formally, the requirement for the propagation of local to global consistency is that the clique tree is a
junction tree, as de   ned below.

see also exercise(6.12).

++

de   nition 6.4 (junction tree). a clique tree is a junction tree if, for each pair of nodes, v and w, all nodes
on the path between v and w contain the intersection v     w. this is also called the running intersection
property.

from this de   nition local consistency will be passed on to any neighbours and the distribution will be
globally consistent. proofs for these results are contained in [162].

example 6.1 (a consistent junction tree). to gain some intuition about the meaning of consistency,
consider the junction tree in    g(6.4d). after a full round of message passing on this tree, each link is
consistent, and the product of the potentials divided by the product of the separator potentials is just the
original distribution itself. imagine that we are interested in calculating the marginal for the node abc. that
requires summing over all the other variables, def gh. if we consider summing over h then, because the link

(e, h) =   

   

(e)

(6.3.19)

is consistent,(cid:88)
so that the ratio(cid:80)

  

   

h

     (e,h)
     (e)

h

is unity, and the e   ect of summing over node h is that the link between eh and
dce can be removed, along with the separator. the same happens for the link between node eg and dce,
and also for cf to abc. the only nodes remaining are now dce and abc and their separator c, which have so
far been una   ected by the summations. we still need to sum out over d and e. again, because the link is

   

   

de

  

(c)

(d, c, e) =   

consistent,(cid:88)
so that the ratio(cid:80)
other variables in that potential, for example p(f ) =(cid:80)

     (d,c,e)
     (c) = 1. the result of the summation of all variables not in abc therefore produces
unity for the cliques and their separators, and the summed potential representation reduces simply to the
potential       (a, b, c) which is the marginal p(a, b, c). it is clear that a similar e   ect will happen for other
nodes. we can then obtain the marginals for individual variables by simple brute force summation over the

de

(6.3.20)

c       (c, f ).

draft november 9, 2017

113

constructing a junction tree for singly-connected distributions

b

f

d

g

a

e

c

h

(b)

a

e

(a)

abc

c

h

c

c

cf

dce

e

d

g

c

e

dce

e

eg

e

eh

eg

(c)

b

f

c

e

(d)

abc

c

cf

eh

(a): belief network.

(d):
figure 6.4:
a junction tree. this satis   es the running intersection property that for any two nodes which contain a
variable in common, any clique on the path linking the two nodes also contains that variable.

(b): moralised version of (a).

(c): clique graph of (b).

6.4 constructing a junction tree for singly-connected distributions

6.4.1 moralisation

for belief networks an initial step is required, which is not required in the case of undirected graphs.

de   nition 6.5 (moralisation). for each variable x add an undirected link between all parents of x and
replace the directed link
to x from its parents by undirected links. this creates a    moralised    markov
network.

@@

6.4.2 forming the clique graph

the clique graph is formed by identifying the cliques in the markov network and adding a link between
cliques that have a non-empty intersection. add a separator between the intersecting cliques.

6.4.3 forming a junction tree from a clique graph

for a singly connected distribution, any maximal weight spanning tree of a clique graph is a junction tree.

de   nition 6.6 (junction tree). a junction tree is obtained by    nding a maximal weight spanning tree of
the clique graph. the weight of the tree is de   ned as the sum of all the separator weights of the tree, where
the separator weight is the number of variables in the separator.

if the clique graph contains loops, then all separators on the loop contain the same variable. by continuing
to remove loop links until a tree is revealed, we obtain a junction tree.

114

draft november 9, 2017

junction trees for multiply-connected distributions

a

d

a

b

c

b

c

a

d

b

c

a

d

b

c

(a)

(b)

(c)

(d)

abc

ac

(e)

acd

(a): an undirected graph with a loop.

figure 6.5:
and c in the subgraph.
representation. (e): junction tree for (a).

(c): the induced representation for the graph in (a).

(b): eliminating node d adds a link between a
(d): equivalent induced

example 6.2 (forming a junction tree). consider the belief network in    g(6.4a). the moralisation
procedure gives    g(6.4b). identifying the cliques in this graph, and linking them together gives the clique
graph in    g(6.4c). there are several possible junction trees one could obtain from this clique graph, and
one is given in    g(6.4d).

6.4.4 assigning potentials to cliques

of a set of potentials   (cid:0)

de   nition 6.7 (clique potential assignment). given a junction tree and a function de   ned as the product

x 1(cid:1), . . . ,    (x n), a valid clique potential assignment places potentials in jt cliques

whose variables can contain them such that the product of the jt clique potentials, divided by the jt
separator potentials, is equal to the function.

a simple way to achieve this assignment is to list all the potentials and order the jt cliques arbitrarily.
then, for each potential, search through the jt cliques until the    rst is encountered for which the potential
variables are a subset of the jt clique variables. subsequently the potential on each jt clique is taken as
the product of all clique potentials assigned to the jt clique. lastly, we assign all jt separators to unity.
this approach is taken in jtassignpot.m. note that in some instances it can be that a junction tree clique
is assigned to unity.

example 6.3. for the belief network of    g(6.4a), we wish to assign its potentials to the junction tree
   g(6.4d). in this case the assignment is unique and is given by

   (abc) = p(a)p(b)p(c|a, b)
   (dce) = p(d)p(e|d, c)
   (cf ) = p(f|c)
   (eg) = p(g|e)
   (eh) = p(h|e)

all separator potentials are initialised to unity.

(6.4.1)

6.5 junction trees for multiply-connected distributions

when the distribution contains loops, the construction outlined in section(6.4) does not result in a junction
tree. the reason is that, due to the loops, variable elimination changes the structure of the remaining graph.

draft november 9, 2017

115

junction trees for multiply-connected distributions

to see this, consider the following distribution,

p(a, b, c, d) =   (a, b)  (b, c)  (c, d)  (d, a)

(6.5.1)

as shown in    g(6.5a). let   s    rst try to make a clique graph. we have a choice about which variable    rst to
marginalise over. let   s choose d:

p(a, b, c) =   (a, b)  (b, c)

  (c, d)  (d, a)

(6.5.2)

the remaining subgraph therefore has an extra connection between a and c, see    g(6.5b). we can express
the joint in terms of the marginals using

p(a, b, c, d) =

  (c, d)  (d, a)

(6.5.3)

to continue the transformation into marginal form, let   s try to replace the numerator terms with probabil-
ities. we can do this by considering

(cid:80)

p(a, b, c)

d   (c, d)  (d, a)

(cid:88)

d

(cid:88)

b

p(a, c, d) =   (c, d)  (d, a)

  (a, b)  (b, c)

plugging this into the above equation, we have

(cid:80)
d   (c, d)  (d, a)(cid:80)

p(a, b, c)p(a, c, d)

b   (a, b)  (b, c)

p(a, b, c, d) =

we recognise that the denominator is simply p(a, c), hence

p(a, b, c, d) =

p(a, b, c)p(a, c, d)

p(a, c)

.

(6.5.4)

(6.5.5)

(6.5.6)

this means that a valid clique graph for the distribution    g(6.5a) must contain cliques larger than those
in the original distribution. to form a jt based on products of cliques divided by products of separators,
we could start from the induced representation    g(6.5c). alternatively, we could have marginalised over
variables a and c, and ended up with the equivalent representation    g(6.5d).

generally, the result from variable elimination and re-representation in terms of the induced graph is that a
link is added between any two variables on a loop (of length 4 or more) which does not have a chord. this
is called triangulation. a markov network on a triangulated graph can always be written in terms of the
product of marginals divided by the product of separators. armed with this new induced representation,
we can form a junction tree.

example 6.4. a slightly more complex loopy distribution is depicted in    g(6.6a),

p(a, b, c, d, e, f ) =   (a, b)  (b, c)  (c, d)  (d, e)  (e, f )  (a, f )  (b, e)

(6.5.7)

there are di   erent induced representations depending on which variables we decide to eliminate. the reader
may convince herself that one such induced representation is given by    g(6.6b).

a

f

b

e

(a)

c

a

d

f

b

e

(b)

c

d

figure 6.6:
markov network.
resentation.

(a): loopy    ladder   
(b): induced rep-

116

draft november 9, 2017

junction trees for multiply-connected distributions

de   nition 6.8 (triangulated (decomposable) graph). an undirected graph is triangulated if every loop
of length 4 or more has a chord. an equivalent term is that the graph is decomposable or chordal . from
this de   nition, one may show that an undirected graph is triangulated if and only if its clique graph has a
junction tree.

6.5.1 triangulation algorithms

when a variable is eliminated from a graph, links are added between all the neighbours of the eliminated
variable. a triangulation algorithm is one that produces a graph for which there exists a variable elimination
order that introduces no extra links in the graph.

for discrete variables the complexity of id136 scales exponentially with clique sizes in the triangulated
graph since absorption requires computing tables on the cliques.
it is therefore of some interest to    nd
a triangulated graph with small clique sizes. however,    nding the triangulated graph with the smallest
maximal clique is a computationally hard problem for a general graph, and heuristics are unavoidable.
below we describe two simple algorithms that are generically reasonable, although there may be cases where
an alternative algorithm may be considerably more e   cient[57, 29, 207].

remark 6.2 (   triangles   ). note that a triangulated graph is not one in which    squares in the original graph
have triangles within them in the triangulated graph   . whilst this is the case for    g(6.6b), this is not true
for    g(6.10d). the term triangulation refers to the fact that every    square    (i.e. loop of length 4) must have
a    triangle   , with edges added until this criterion is satis   ed. see also    g(6.7).

greedy variable elimination

an intuitive way to think of triangulation is to    rst start with simplicial nodes, namely those which, when
eliminated do not introduce any extra links in the remaining graph. next consider a non-simplicial node of
the remaining graph that has the minimal number of neighbours. then add a link between all neighbours
of this node and then eliminate this node from the graph. continue until all nodes have been eliminated.
(this procedure corresponds to rose-tarjan elimination[251] with a particular node elimination choice).
by labelling the nodes eliminated in sequence, we obtain a perfect ordering (see below). in the case that
(discrete) variables have di   erent numbers of states, a more re   ned version is to choose the non-simplicial
node i which, when eliminated, leaves the smallest clique table size (the product of the size of all the state
dimensions of the neighbours of node i). see    g(6.8) for an example.

procedure 6.1 (variable elimination). in variable elimination, one simply picks any non-deleted node x
in the graph, and then adds links to all the neighbours of x. node x is then deleted. one repeats this until
all nodes have been deleted[251].

de   nition 6.9 (perfect elimination order). let the n variables in a markov network be ordered from 1 to
n. the ordering is perfect if, for each node i, the neighbours of i that are later in the ordering, and i itself,
form a (maximal) clique. this means that when we eliminate the variables in sequence from 1 to n, no
additional links are induced in the remaining marginal graph. a graph which admits a perfect elimination
order is decomposable, and vice versa.

d

a

e

c

b

figure 6.7: this graph is not triangulated, despite its    triangular   
appearance. the loop a     b     c     d     a does not have a chord.

draft november 9, 2017

117

the junction tree algorithm

a

a

f

f

b

j

b

j

b

j

b

j

c

d

e

a

g

h

i

f

k

l

(a)

c

d

e

a

g

h

i

f

k

l

(d)

c

d

e

a

g

h

i

f

k

l

(b)

c

d

e

a
a

g

h

i

f
f

k

l

(e)

b

j

b
b

j
j

c

d

e

g

h

i

k

l

(c)

c
c

d
d

e
e

g
g

h
h

i
i

k
k

l
l

(f)

(a): markov network for which we seek a triangulation via greedy variable elimination. we
figure 6.8:
(b): we then eliminate variables b, d since these only add a
   rst eliminate the simplicial nodes a, e, l.
(c): there are no simplicial nodes at this stage, and we choose to
single extra link to the induced graph.
(d): we eliminate g and h since these are
eliminate f and i, each elimination adding only a single link.
simplicial. (e): the remaining variables {c, j, k} may be eliminated in any order. (f ): final triangulation.
the variable elimination (partial) order is {a, e, l} ,{b, d} ,{f, i} ,{g, h} ,{c, j, k} where the brackets indicate
that the order in which the variables inside the bracket are eliminated is irrelevant. compared with the
triangulation produced by the max-cardinality checking approach in    g(6.10d), this triangulation is more
parsimonious.

whilst this variable elimination guarantees a triangulated graph, its e   ciency depends heavily on the se-
quence of nodes chosen to be eliminated. several heuristics for this have been proposed, including the one
below, which corresponds to choosing x to be the node with the minimal number of neighbours.

maximum cardinality checking

algorithm(6.1) terminates with success if the graph is triangulated. not only is this a su   cient condition
for a graph to be triangulated, but it is also necessary [288]. it processes each node and the time to process
a node is quadratic in the number of adjacent nodes. this triangulation checking algorithm also suggests
a triangulation construction algorithm     we simply add a link between the two neighbours that caused the
algorithm to fail, and then restart the algorithm. the algorithm is restarted from the beginning, not just
continued from the current node. this is important since the new link may change the connectivity between
previously labelled nodes. see    g(6.10) for an example1.

6.6 the junction tree algorithm

we now have all the steps required for id136 in multiply connected graphs, given in the procedure below.

procedure 6.2 (junction tree algorithm).

1this example is due to david page www.cs.wisc.edu/   dpage/cs731

118

draft november 9, 2017

the junction tree algorithm

abf

bf

bcf g

cdhi

di

dei

cf g

cf gj

chi

chik

cj

ck

cjk

jk

jkl

figure 6.9: junction tree formed from the triangulation    g(6.8f).
one may verify that this satis   es the running intersection prop-
erty.

algorithm 6.1 a check if a graph is decomposable (triangulated). the graph is triangulated if, after
cycling through all the n nodes in the graph, the fail criterion is not encountered.

1: choose any node in the graph and label it 1.
2: for i = 2 to n do
3:
4:
5: end for

choose the node with the most labeled neighbours and label it i.
if any two labeled neighbours of i are not adjacent to each other, fail.

where there is more than one node with the most labeled neighbours, the tie may be broken arbitrarily.

moralisation marry the parents. this is required only for directed distributions. note that all the parents

of a variable are married together     a common error is to marry only the    neighbouring    parents.

triangulation ensure that every loop of length 4 or more has a chord.

junction tree form a junction tree from cliques of the triangulated graph, removing any unnecessary
links in a loop on the cluster graph. algorithmically, this can be achieved by    nding a tree with
maximal spanning weight with weight wij given by the number of variables in the separator between
cliques i and j. alternatively, given a clique elimination order (with the lowest cliques eliminated
   rst), one may connect each clique i to the single neighbouring clique j > i with greatest edge weight
wij.

potential assignment assign potentials to junction tree cliques and set the separator potentials to unity.

message propagation carry out absorption until updates have been passed along both directions of every

link on the jt. the clique marginals can then be read o    from the jt.

an example is given in    g(6.11).

6.6.1 remarks on the jta

    the algorithm provides an upper bound on the computation required to calculate marginals in the
graph. there may exist more e   cient algorithms in particular cases, although generally it is believed
that there cannot be much more e   cient approaches than the jta since every other approach must
perform a triangulation[161, 188]. one particular special case is that of marginal id136 for a binary
variable markov network on a two-dimensional lattice containing only pure quadratic interactions. in

this case the complexity of computing a marginal id136 is o(cid:0)n3(cid:1) where n is the number of variables

in the distribution. this is in contrast to the pessimistic exponential complexity suggested by the jta.
    one might think that the only class of distributions for which essentially a linear time algorithm is
available are singly connected distributions. however, there are decomposable graphs for which the
cliques have limited size meaning that id136 is tractable. for example an extended version of the
   ladder    in    g(6.6a) has a simple induced decomposable representation    g(6.6b), for which marginal
id136 would be linear in the number of rungs in the ladder. e   ectively these structures are hyper
trees in which the complexity is then related to the tree width of the graph[87].

draft november 9, 2017

119

the junction tree algorithm

1

3

2

6

5

7

10

1

2

5

7

10

1

2

5

7

10

4

8

9

3

4

8

9

3

4

8

9

11

6

11

6

11

(a)

(b)

(c)

(d)

figure 6.10: starting with the markov network in (a), the maximum cardinality check algorithm proceeds
until (b), where an additional link is required, see (c). one continues until the fully triangulated graph (d)
is found.

a

c

f

b

e

d

g

b

e

a

c

f

d

g

h

i

h

i

(a)

(b)

(a): original loopy belief network.
figure 6.11:
(b): the moralisation links (dashed) are between
nodes e and f and between nodes f and g. the other
additional links come from triangulation. the clique
size of the resulting clique tree (not shown) is four.

    ideally, we would like to    nd a triangulated graph which has minimal clique size. however, it can be
shown to be a computationally hard problem to    nd the most e   cient triangulation. in practice, most
general purpose triangulation algorithms are chosen to provide reasonable, but clearly not always
optimal, generic performance.

    numerical over/under    ow issues can occur under repeated multiplication of potentials. if we only
care about marginals we can avoid numerical di   culties by normalising potentials at each step; these
missing normalisation constants can always be found under the normalisation constraint. if required
one can always store the values of these local renormalisations, should, for example, the global nor-
malisation constant of a distribution be required, see section(6.6.2).

    after clamping variables in evidential states, running the jta returns the joint distribution on the
non-evidential variables xc in a clique with all the evidential variables clamped in their evidential
states, p(xc, evidence). from this conditionals are straightforward to calculate.

    representing the marginal distribution of a set of variables x which are not contained within a single
clique is in general computationally di   cult. whilst the id203 of any state of p(x ) may be
computed e   ciently, there are in general an exponential number of such states. a classical example in
this regard is the id48, section(23.2) which has a singly connected joint distribution p(v,h). however
the marginal distribution p(v) is fully connected. this means that for example whilst the id178 of
p(v,h) is straightforward to compute, the id178 of the marginal p(v) is intractable.

6.6.2 computing the normalisation constant of a distribution

p(x ) =

how can we    nd z e   ciently? if we used the jta on the unnormalised distribution (cid:81)

  (xi)

i

for a markov network

(cid:89)

1
z

have the equivalent representation:

(cid:81)
(cid:81)

c   (xc)
s   (xs)

p(x ) =

1
z

120

(6.6.1)

i   (xi), we would

(6.6.2)

draft november 9, 2017

the junction tree algorithm

(cid:81)
(cid:81)

(cid:88)

x

c   (xc)
s   (xs)

z =

where s and c are the separator and clique indices. since the distribution must normalise, we can obtain z
from

(6.6.3)

for a consistent jt, summing    rst over the variables of a simplicial jt clique (not including the separator
variables), the marginal clique will cancel with the corresponding separator to give a unity term so that the
clique and separator can be removed. this forms a new jt for which we then eliminate another simplicial
clique. continuing in this manner we will be left with a single numerator potential so that

z =

  (xc)

(6.6.4)

(cid:88)

xc

this is true for any clique c, so it makes sense to choose one with a small number of states so that the
resulting raw summation is e   cient. hence in order to compute the normalisation constant of a distribution
one runs the jt algorithm on an unnormalised distribution and the global normalisation is then given by
the local normalisation of any clique. note that if the graph is disconnected (there are isolated cliques), the
normalisation is the product of the connected component normalisation constants.

6.6.3 the marginal likelihood

our interest here is the computation of p(v) where v     x is a subset of the full variable set x . naively,
one could carry out this computation by summing over all the non-evidential variables (hidden variables
h = x\v) explicitly. in cases where this is computationally impractical an alternative is to use

p(h|v) =

p(v,h)
p(v)

(6.6.5)

one can view this as a product of clique potentials divided by the normalisation p(v), for which the general
method of section(6.6.2) may be directly applied. see demojtree.m.

6.6.4 some small jta examples

example 6.5 (a simple example of the jta).
consider running the jta on the simple graph

p(a, b, c) = p(a|b)p(b|c)p(c)

(6.6.6)

the moralisation and triangulation steps are trivial, and the jta is given
immediately by the    gure on the right. a valid assignment is

a

ab

b

b

c

bc

   (a, b) = p(a|b),    (b) = 1,    (b, c) = p(b|c)p(c)

to    nd a marginal p(b) we    rst run the jta:

    absorbing from ab through b, the new separator is       (b) =(cid:80)

(6.6.7)

a    (a, b) =(cid:80)

a p(a|b) = 1.

    the new potential on (b, c) is given by

   

  

(b, c) =

   (b, c)      (b)

   (b)

=

p(b|c)p(c)    1

1

    absorbing from bc through b, the new separator is

(cid:88)

   

(cid:88)

      

  

(b) =

  

(b, c) =

p(b|c)p(c)

c

c

draft november 9, 2017

(6.6.8)

(6.6.9)

121

the junction tree algorithm

    the new potential on (a, b) is given by

   (a, b)         (b)

   

  

(a, b) =

this is therefore indeed equal to the marginal since(cid:80)

      (b)

c p(b|c)p(c)
1

=

the new separator          (b) contains the marginal p(b) since

p(a|b)(cid:80)

(cid:88)

c

      

  

(b) =

(cid:88)

c

p(b|c)p(c) =

p(b, c) = p(b)

c p(a, b, c) = p(a, b).

(6.6.10)

(6.6.11)

example 6.6 (finding a conditional marginal). continuing with the distribution in example(6.5), we
consider how to compute p(b|a = 1, c = 1). first we clamp the evidential variables in their states. then we
claim that the e   ect of running the jta is to produce on a set of clique variables x the marginals on the
cliques p(x ,v). we demonstrate this below:

a p(a|b) = 1. however, since
a is clamped in state a = 1, then the summation is not carried out over a, and we have instead

    in general, the new separator is given by       (b) = (cid:80)
      (b) = p(a = 1|b).
    the new potential on the (b, c) clique is given by

   

  

(b, c) =

   (b, c)      (b)

   (b)

=

p(b|c = 1)p(c = 1)p(a = 1|b)

1

a    (a, b) = (cid:80)

    the new separator is normally given by

(cid:88)

   

      

  

(b) =

  

(b, c)

c

however, since c is clamped in state 1, we have instead

      

  

(b) = p(b|c = 1)p(c = 1)p(a = 1|b)

(6.6.12)

(6.6.13)

@@

(6.6.14)

    the new potential on (a, b) is given by

   

  

(a, b) =

   (a, b)         (b)

      (b)

=

p(a = 1|b)p(b|c = 1)p(c = 1)p(a = 1|b)

p(a = 1|b)

= p(a = 1|b)p(b|c = 1)p(c = 1)
(6.6.15)

which is the joint distribution p(a = 1, b, c = 1).

@@

the e   ect of clamping a set of variables v in their evidential states and running the jta is that, for a clique
i which contains the set of non-evidential variables hi, the consistent potential from the jta contains the
marginal p(hi,v). finding a conditional marginal is then straightforward by ensuring normalisation.

example 6.7 (   nding the likelihood p(a = 1, c = 1)). one may also use the jta to compute the marginal
likelihood for variables not in the same clique since the e   ect of clamping the variables in their evidential
states and running the jta produces the joint marginals, such as       (a, b) = p(a = 1, b, c = 1). then
calculating the likelihood is easy since we just sum out over the non-evidential variables of any converged

potential : p(a = 1, c = 1) =(cid:80)

b       (a, b) =(cid:80)

b p(a = 1, b, c = 1).

122

draft november 9, 2017

finding the most likely state

6.6.5 shafer-shenoy propagation

(6.6.16)

(6.6.17)

consider the markov network in    g(6.12a) for which a junction tree is given in    g(6.12b). we use the
obvious notation shortcut of writing the variable indices alone. in the absorption procedure, we essentially
store the result of message passing in the potentials and separators. an alternative message passing scheme
for the junction tree can be derived as follows: consider computing the marginal on the variables 2, 3, 4,
which involves summing over variables 1, 5, 6:

p(2, 3, 4) =

=

  (1, 2, 5)  (1, 3, 6)  (1, 2, 3)  (2, 3, 4)

(cid:88)
(cid:124)
(cid:123)(cid:122)

6

(cid:125)

  123   234

  (1, 2, 5)

  (1, 3, 6)

  125   123

  136   123

(cid:123)(cid:122)

(cid:125)

  (2, 3, 4)

  (1, 2, 3)

(cid:125)

1,5,6

(cid:88)
(cid:88)
(cid:124)

1

(cid:88)
(cid:124)

5

(cid:123)(cid:122)

(cid:89)

k(cid:54)=j

(cid:88)

vi\vj

  i   j =

  (vi)

in general, for a clique i with potential   (vi), and neighbouring clique j with potential   (vj), provided we
have received messages from the other neighbours of i, we can send a message

  k   i

(6.6.18)

once a full round of message passing has been completed, the marginal of any clique is given by the product
of incoming messages.

this message passing scheme is called shafer-shenoy propagation and has the property that no division of
potentials is required, unlike absorption. on the other hand, to compute a message we need to take the
product of all incoming messages; in absorption this is not required since the e   ect of the message passing
is stored in the clique potentials. the separators are not required in the shafer-shenoy approach and we
use them here only to indicate which variables the messages depend on. both absorption and shafer-shenoy
propagation are valid message passing schemes on the junction tree and the relative e   cacy of the approaches
depends on the topology of the junction tree[188].

6.7 finding the most likely state

it is often of interest to compute the most likely joint state of a distribution:

argmax
x1,...,xn

p(x1, . . . , xn)

(6.7.1)

4

5

1

6

2

3

(a)

125

136

12

13

123

23

234

125

136

12

13

123

23

234

(b)

(c)

(a): markov network.

(b): junction tree. under absorption, once we have absorbed from
figure 6.12:
125 to 123 and 136 to 123, the result of these absorptions is stored in the new potential on 123. after this
we can absorb from 123 to 234 by operating on the 123 potential and then sending this information to 234.
(c): in shafer-shenoy updating, we send a message from a clique to a neighbouring clique based on the
product of all incoming messages.

draft november 9, 2017

123

reabsorption : converting a junction tree to a directed network

since the development of the jta is based around a variable elimination procedure and the max operator
distributes over the distribution as well, eliminating a variable by maximising over that variable will have
the same e   ect on the graph structure as summation did. this means that a junction tree is again an
appropriate structure on which to perform max operations. once a jt has been constructed, one then uses
the max absorption procedure (see below), to perform maximisation over the variables. after a full round
of absorption has been carried out, the cliques contain the distribution on the variables of the clique with
all remaining variables set to their optimal states. the optimal local states can then be found by explicit
optimisation of each clique potential separately.

note that this procedure holds also for non-distributions     in this sense this is an example of a more general
id145 procedure applied in a case where the underlying graph is multiply connected. this
demonstrates how to e   ciently compute the optimum of a multiply connected function de   ned as the product
on potentials.

de   nition 6.10 (max absorption).

   (v)

      (s)

      (w)

let v and w be neighbours in a clique graph, let s be their
separator, and let    (v),    (w) and    (s) be their potentials.
absorption replaces the tables    (s) and    (w) with
      (s)
   (s)

(s) = maxv\s    (v)

(w) =    (w)

  

  

   

   

once messages have been passed in both directions over all separators, according to a valid schedule, the
most-likely joint state can be read o    from maximising the state of the clique potentials. this is implemented
in absorb.m and absorption.m where a    ag is used to switch between either sum or max absorption.

6.8 reabsorption : converting a junction tree to a directed network

it is sometimes useful to be able to convert
a consistent jt (in which a full round of message passing has
occurred) back to a bn of a desired form. for example, if one wishes to draw samples from a markov
network, this can be achieved by ancestral sampling on an equivalent directed structure, see section(27.2.2).

@@

de   nition 6.11 (reabsorption).

v

s

w

   

v

w\s

let v and w be neighbouring cliques in a directed
consistent jt in which each clique in the tree has at
most one parent. furthermore, let s be their separator, and    (v),    (w) and    (s) be the potentials.
reabsorption into w removes the separator and forms a (set) conditional distribution

@@

p(w\s|v) =

   (w)
   (s)

we say that clique w reabsorbs the separator s.

(6.8.1)

revisiting the example from    g(6.4), we have the jt given in    g(6.13a). to    nd a valid directed represen-
tation we    rst orient the jt edges consistently away from a chosen root node (see singleparenttree.m),
thereby forming a directed jt which has the property that each clique has at most one parent clique.
consider    g(6.13a) which represents

p(a, b, c, d, e, f, g, h) =

p(e, g)p(d, c, e)p(a, b, c)p(c, f )p(e, h)

p(e)p(c)p(c)p(e)

124

(6.8.2)

draft november 9, 2017

the need for approximations

dce

c

e

(a)

e

eg

abc

c

cf

dce

e

eh

eg

abc

c

c

e

cf

a b c

e d

f

eh

(b)

g

h

(c)

(a): junction tree.

figure 6.13:
away from the clique (abc).
into its child clique.

(b): directed junction tree in which all edges are consistently oriented
(c): a set chain formed from the junction tree by reabsorbing each separator

we now have many choices as to which clique re-absorbs a separator. one such choice would give

@@

p(a, b, c, d, e, f, g, h) = p(g|e)p(d, e|c)

p(a, b, c)p(f|c)p(h|e)

(6.8.3)

this can be represented using a so-called set chain[185] in    g(6.13c) (set chains generalise belief networks
to a product of clusters of variables conditioned on parents). by writing each of the set conditional proba-
bilities as local conditional bns, one may also form a bn. for example, one such would be given from the
decomposition

p(c|a, b)p(b|a)p(a)p(g|e)p(f|c)p(h|e)p(d|e, c)p(e|c)

(6.8.4)

6.9 the need for approximations

the jta provides an upper bound on the complexity of (marginal/max) id136 and attempts to exploit
the structure of the graph to reduce computations. however, in a great deal of interesting applications the
use of the jta algorithm would result in clique-sizes in the triangulated graph that are prohibitively large.
a classical situation in which this can arise are disease-symptom networks. for example, for the graph in
   g(6.14), the triangulated graph of the diseases is fully connected, meaning that no simpli   cation can occur
in general. this situation is common in such bipartite networks, even when the children only have a small
number of parents. intuitively, as one eliminates each parent, links are added between other parents, me-
diated via the common children. unless the graph is highly regular, analogous to a form of hidden markov
model, this    ll-in e   ect rapidly results in large cliques and intractable computations.

dealing with large cliques in the triangulated graph is an active research topic and we   ll discuss strategies
for approximate id136 in chapter(28).

6.9.1 bounded width junction trees

in some applications we may be at liberty to choose the structure of the markov network. for example,
if we wish to    t a markov network to data, we may wish to use as complex a markov network as we
can computationally a   ord.
in such cases we desire that the clique sizes of the resulting triangulated
markov network are smaller than a speci   ed    tree width    (considering the corresponding junction tree as
a hypertree). this results in a    thin    junction tree. a simple way to do this is to start with a graph and
include a randomly chosen edge provided that the size of all cliques in the resulting triangulated graph is
below a speci   ed maximal width. see demothinjt.m and makethinjt.m which assumes an initial graph g
and a graph of candidate edges c, iteratively expanding g until a maximal tree width limit is reached. see
also [11] for a discussion on learning an appropriate markov structure based on data.

draft november 9, 2017

125

code

figure 6.14: 5 diseases giving rise to 3 symptoms. the triangulated graph
contains a 5 clique of all the diseases.

d1

d2

d3

d4

d5

s1

s2

s3

6.10 summary

    the junction tree is a structure on clusters of variables such that, under id136 operations such as
marginalisation, the junction-tree structure remains invariant. this resolves the    ll-in issue when using
message-passing on a multiply connected graph.

    the key stages are moralisation, triangulation, potential assignment, and message-passing.
    there are di   erent propagation algorithms, including absorption and shafer-shenoy. these are both valid
message-passing algorithms on the junction tree and di   er in their e   ciency depending on the branch-
structure of the junction tree.

    the junction tree algorithm does not make a di   cult id136 problem necessarily any easier. it is simply
a way to organise the computations required to correctly carry out message-passing. the computational
complexity is dominated by the clique-size and there is no guarantee that one can    nd cliques with small
sizes in general.

    the junction tree algorithm is clever, but not clairvoyant.

it provides only an upper bound on the com-
putational complexity of id136. it may be that there are problems which possess additional structure,
not immediately apparent, that can be exploited to reduce the computational complexity of id136 much
below that suggested by the junction-tree approach.

6.11 code

absorb.m: absorption update v     s     w
absorption.m: full absorption schedule over tree
jtree.m: form a junction tree
triangulate.m: triangulation based on simple node elimination

6.11.1 utility routines

knowing if an undirected graph is a tree, and returning a valid elimination sequence is useful. a connected
graph is a tree if the number of edges plus 1 is equal to the number of nodes. however, for a possibly discon-
nected graph this is not the case. the code istree.m deals with the possibly disconnected case, returning a
valid elimination sequence if the graph is singly connected. the routine is based on the observation that any
singly connected graph must always possess a simplicial node which can be eliminated to reveal a smaller
singly connected graph.

istree.m: if graph is singly connected return 1 and elimination sequence
elimtri.m: node elimination on a triangulated graph, with given end node
demojtree.m: chest clinic demo

126

draft november 9, 2017

exercises

6.12 exercises

exercise 6.1. show that the markov network 1
elimination labelling for this graph.

4

2

exercise 6.2. consider the following distribution:

3

is not perfect elimination ordered and give a perfect

p(x1, x2, x3, x4) =   (x1, x2)  (x2, x3)  (x3, x4)

(6.12.1)

1. draw a clique graph that represents this distribution and indicate the separators on the graph.

2. write down an alternative formula for the distribution p(x1, x2, x3, x4) in terms of the marginal prob-

abilities p(x1, x2), p(x2, x3), p(x3, x4), p(x2), p(x3)

exercise 6.3. consider the distribution

p(x1, x2, x3, x4) =   (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)

(6.12.2)

1. write down a junction tree for the above distribution.

2. carry out the absorption procedure and demonstrate that this gives the correct result for the marginal

p(x1).

exercise 6.4. consider the distribution

p(a, b, c, d, e, f, g, h, i) = p(a)p(b|a)p(c|a)p(d|a)p(e|b)p(f|c)p(g|d)p(h|e, f )p(i|f, g)

(6.12.3)

1. draw the belief network for this distribution.

2. draw the moralised graph.

3. draw the triangulated graph. your triangulated graph should contain cliques of the smallest size pos-

sible.

4. draw a junction tree for the above graph and verify that it satis   es the running intersection property.

5. describe a suitable initialisation of clique potentials.

6. describe the absorption procedure and write down an appropriate message updating schedule.

exercise 6.5. this question concerns the distribution

p(a, b, c, d, e, f ) = p(a)p(b|a)p(c|b)p(d|c)p(e|d)p(f|a, e)

1. draw the belief network for this distribution.

2. draw the moralised graph.

(6.12.4)

3. draw the triangulated graph. your triangulated graph should contain cliques of the smallest size pos-

sible.

4. draw a junction tree for the above graph and verify that it satis   es the running intersection property.

5. describe a suitable initialisation of clique potentials.

6. describe the absorption procedure and an appropriate message updating schedule.

7. show that the distribution can be expressed in the form

p(a|f )p(b|a, c)p(c|a, d)p(d|a, e)p(e|a, f )p(f )

draft november 9, 2017

(6.12.5)

127

exercise 6.6.

for the undirected graph on the square lattice as shown, draw a triangulated graph with
the smallest clique sizes possible.

consider a binary variable markov random field p(x) = z   1(cid:81)

exercise 6.7.

i>j   (xi, xj), de   ned
i[xi=xj ] for i a neighbour of j on the lattice and
on the n    n lattice with   (xi, xj) = e
i > j. a naive way to perform id136 is to    rst stack all the variables in the tth
column and call this cluster variable xt, as shown. the resulting graph is then singly
connected. what is the complexity of computing the normalisation constant based on
this cluster representation? compute log z for n = 10.

exercises

x1

x2

x3

exercise 6.8. given a consistent junction tree on which a full round of message passing has occurred,
explain how to form a belief network from the junction tree.

exercise 6.9. the    le diseasenet.mat contains the potentials for a disease bi-partite belief network, with
20 diseases d1, . . . , d20 and 40 symptoms, s1, . . . , s40. the disease variables are numbered from 1 to 20 and
the symptoms from 21 to 60. each disease and symptom is a binary variable, and each symptom connects
to 3 parent diseases.

1. using the brmltoolbox, construct a junction tree for this distribution and use it to compute all the

marginals of the symptoms, p(si = 1).

2. explain how to compute the marginals p(si = 1) in a way more e   cient than using the junction tree
formalism. by implementing this method, compare it with the results from the junction tree algorithm.

3. symptoms 1 to 5 are present (state 1), symptoms 6 to 10 not present (state 2), and the rest not known.

compute the marginal p(di = 1|s1:10) for all diseases.

exercise 6.10. consider the distribution

p(y|x1, . . . , xt )p(x1)

p(xt|xt   1)

t(cid:89)

t=2

where all variables are binary.

1. draw a junction tree for this distribution and explain the computational complexity of computing p(xt ),

as suggested by the junction tree algorithm.

2. by using an approach di   erent from the plain jta above, explain how p(xt ) can be computed in time

that scales linearly with t .

exercise 6.11. analogous to jtpot=absorption(jtpot,jtsep,infostruct), write a routine
[jtpot jtmess]=shafershenoy(jtpot,infostruct) that returns the clique marginals and messages for a
junction tree under shafer-shenoy updating. modify demojtree.m to additionally output your results for
marginals and conditional marginals alongside those obtained using absorption.

exercise 6.12 (clique elimination). since a junction tree is indeed a tree, it must have an extremal (   edge   )
clique x1 that connects to only one other clique through their separator s1. more generally, we must be able
to label the cliques in an elimination order, 1, 2, . . . , n such that by eliminating clique i we only have cliques
left with label greater than i. for a separator connected to two cliques j and k, we then number this separator
the lower of the two values, min (j, k). using this elimination ordering of the cliques we can write the jt in
the form (using   (x ) to represent clique potentials over clique variables x and   (s) to represent separator
potentials over separator variables s)

(cid:81)
(cid:81)

c   2   c(xc)
s   2   s(ss)

p(x ) =

  1(x1)
  1(s1)

128

(6.12.6)

draft november 9, 2017

1. now eliminate clique 1 by summing over all variables in clique 1 that are not in the separator s1.

show that this gives a new jt on cliques c     2 and separators s     2 of the form

exercises

c   2,c(cid:54)=d   c(xc)
s   2   s(ss)

(cid:81)
(cid:81)
  d(xd)(cid:80)x1\s1   1(x1)

  1(s1)

  

(cid:48)
d(xd)

  

(cid:48)
d(xd) =

where d is the clique neighbouring clique 1 and

(6.12.7)

(6.12.8)

furthermore, by considering that

p(x ) = p(x1\s1|s1)p(x\{x1\s1})

show that by updating

(cid:48)
1(s1) =
  

  1(x1)

(cid:88)

x1\s1

the updated jt on p(x ) still represents the original distribution and relate this to the absorption
procedure.

2. show that by continuing in this manner, eliminating cliques one by one, the last clique must contain

the marginal p(xn).

3. explain how by now reversing the elimination schedule, and eliminating cliques one by one, updating
their potentials in a similar manner to equation(6.12.8), the updated cliques   j(xj) will contain the
marginals p(xj).

4. hence explain why, after updating cliques according to the forward and reversed elimination schedules,

the cliques must be globally consistent.

draft november 9, 2017

129

exercises

130

draft november 9, 2017

chapter 7

making decisions

so far we   ve considered modelling and id136 of distributions. in cases where we need to make decisions
under uncertainty, we need to additionally express how useful making the right decision is. in this chapter
we are particularly interested in the case when a sequence of decisions need to be taken. the corresponding
sequential decision theory problems can be solved using either a general decision tree approach or by ex-
ploiting structure in the problem based on extending the belief network framework and the corresponding
id136 routines. the framework is related to problems in control theory and id23.

7.1 expected utility

this chapter concerns situations in which decisions need to be taken under uncertainty. consider the
following scenario: you are asked if you wish to take a bet on the outcome of tossing a fair coin. if you
bet and win, you gain   100.
if you don   t bet, the cost to you is
zero. we can set this up using a two state variable x, with dom(x) = {win, lose}, a decision variable d with
dom(d) = {bet, no bet} and utilities as follows:

if you bet and lose, you lose   200.

u (win, bet) = 100, u (lose, bet) =    200, u (win, no bet) = 0, u (lose, no bet) = 0

(7.1.1)

since we don   t know the state of x, in order to make a decision about whether or not to bet, arguably the
best we can do is work out our expected winnings/losses under the situations of betting and not betting[258].
if we bet, we would expect to gain

u (bet) = p(win)    u (win, bet) + p(lose)    u (lose, bet) = 0.5    100     0.5    200 =    50

if we don   t bet, the expected gain is zero, u (no bet) = 0. based on taking the decision which maximises
expected utility, we would therefore be advised not to bet.

de   nition 7.1 (subjective expected utility). the utility of a decision is

u (d) = (cid:104)u (d, x)(cid:105)p(x)

(7.1.2)

where p(x) is the distribution of the outcome x and d represents the decision.

7.1.1 utility of money

you are a wealthy individual, with   1, 000, 000 in your bank account. you are asked if you would like to
participate in a fair coin tossing bet in which, if you win, your bank account will become   1, 000, 000, 000.

131

id90

y e s

( 0 . 6 )

   100

rain

(0.4) no

yes

p arty

n

o

y e s

( 0 . 6 )

(0.4) no

rain

500

0

50

figure 7.1: a decision tree containing chance nodes (denoted
with ovals), decision nodes (denoted with rectangles) and utility
nodes (denoted with diamonds). note that a decision tree is not
a graphical representation of a belief network with additional
nodes. rather, a decision tree is an explicit enumeration of the
possible choices that can be made, beginning with the leftmost
decision node, with probabilities on the links out of chance nodes.

however, if you lose, your bank account will contain only   1000. assuming the coin is fair, should you take
the bet? if we take the bet our expected bank balance would be

u (bet) = 0.5    1, 000, 000, 000 + 0.5    1000 = 500, 000, 500.00

(7.1.3)

if we don   t bet, our bank balance will remain at   1, 000, 000. based on expected utility, we are therefore
advised to take the bet. (note that if one considers instead the amount one will win or lose, one may show
that the di   erence in expected utility between betting and not betting is the same, exercise(7.7)).

whilst the above makes mathematical sense, few people who are millionaires are likely to be willing to risk
losing almost everything in order to become a billionaire. this means that the subjective utility of money is
not simply the quantity of money. in order to better re   ect the situation, the utility of money would need
to be a non-linear function of money, growing slowly for large quantities of money and decreasing rapidly
for small quantities of money, exercise(7.2).

7.2 id90

id90 (dts) are a way to graphically organise a sequential decision process. a decision tree contains
decision nodes, each with branches for each of the alternative decisions. chance nodes (random variables)
also appear in the tree, with the utility of each branch computed at the leaf of each branch. the expected
utility of any decision can then be computed on the basis of the weighted summation of all branches from
the decision to all leaves from that branch.

example 7.1 (party). consider the decision problem as to whether or not to go ahead with a fund-raising
garden party. if we go ahead with the party and it subsequently rains, then we will lose money (since very
few people will show up); on the other hand, if we don   t go ahead with the party and it doesn   t rain we   re
free to go and do something else fun. to characterise this numerically, we use:

p(rain = rain) = 0.6, p(rain = no rain) = 0.4

(7.2.1)

the utility is de   ned as

u (party, rain) =    100, u (party, no rain) = 500, u (no party, rain) = 0, u (no party, no rain) = 50 (7.2.2)
we represent this situation in    g(7.1). the question is, should we go ahead with the party? since we don   t
know what will actually happen to the weather, we compute the expected utility of each decision:

u (party) =

u (party, rain)p(rain) =    100    0.6 + 500    0.4 = 140

u (no party) =

u (no party, rain)p(rain) = 0    0.6 + 50    0.4 = 20

(7.2.3)

(7.2.4)

(cid:88)
(cid:88)

rain

rain

132

draft november 9, 2017

id90

(cid:88)

rain

max
p arty

based on expected utility, we are therefore advised to go ahead with the party. the maximal expected
utility is given by (see demodecparty.m)

p(rain)u (p arty, rain) = 140

(7.2.5)

example 7.2 (party-friend). an extension of the party problem is that if we decide not to go ahead with
the party, we have the opportunity to visit a friend. however, we   re not sure if this friend will be in. the
question is should we still go ahead with the party?

we need to quantify all the uncertainties and utilities. if we go ahead with the party, the utilities are as
before:

uparty (party, rain) =    100, uparty (party, no rain) = 500

with

p(rain = rain) = 0.6, p(rain = no rain) = 0.4

(7.2.6)

(7.2.7)

if we decide not to go ahead with the party, we will consider going to visit a friend. in making the decision
not to go ahead with the party we have utilities

uparty (no party, rain) = 0, uparty (no party, no rain) = 50

the id203 that the friend is in depends on the weather according to

p(f riend = in|rain) = 0.8, p(f riend = in|no rain) = 0.1,

the other probabilities are determined by normalisation. we additionally have

uvisit (friend in, visit) = 200, uvisit (friend out, visit) =    100

(7.2.8)

(7.2.9)

(7.2.10)

with the remaining utilities zero. the two sets of utilities add up so that the overall utility of any decision
sequence is uparty + uvisit. the decision tree for the party-friend problem is shown is    g(7.2). for each
decision sequence the utility of that sequence is given at the corresponding leaf of the dt. note that the
leaves contain the total utility uparty + uvisit. solving the dt corresponds to    nding for each decision
node the maximal expected utility possible (by optimising over future decisions). at any point in the tree
choosing that action which leads to the child with highest expected utility will lead to the optimal strategy.
using this, we    nd that the optimal expected utility has value 140 and is given by going ahead with the
party, see demodecpartyfriend.m.

(cid:88)

rain

(cid:88)

f riend

mathematically, we can express the optimal expected utility for the party-friend example by summing over
un-revealed variables and optimising over future decisions:

max
p arty

p(rain) max
v isit

p(f riend|rain) [uparty(p arty, rain) + uvisit(v isit, f riend)i [p arty = no]]

(7.2.11)
where the term i [p arty = no] has the e   ect of curtailing the dt if the party goes ahead. to answer the
question as to whether or not to go ahead with the party, we take that state of p arty that corresponds to
the maximal expected utility above. the way to read equation (7.2.11) is to start from the last decision
that needs to be taken, in this case v isit. when we are at the v isit stage we assume that we will have
previously made a decision about p arty and also will have observed whether or not it is raining. however,

draft november 9, 2017

133

s
e
y

p arty

n

o

-100

-100

s
e
y

s
e
y
)

0.6

(

rain

rain(140)

(

0

.

4

)

n

o

500

s
e
y

n

o

500

200

(0.8)in

200

in

f riend

(

0
.
2

)

o

u

t

   100

yes

v isit

n

o

0

(0.8)in

f riend

s
e
y
)

0.6

(

(

0
.
2

)

o

rain

(

0

.

4

)

n

o

yes

v isit

u

t

0

250

(0.1)in

f riend

(

0
.
9

)

o

u

t

   50

n

o

50

(0.1)in

f riend

(

0
.
9

)

o

u

t

50

p arty(140)

f riend(140)

o

u

t

   100

yes

v isit(140)

n

o

s
e
y

rain(104)

n

o

in

f riend(0)

o

u

t

0

0

250

in

n

o

f riend(   20)

o

u

t

   50

yes

v isit(50)

n

o

50

in

f riend(50)

o

u

t

50

id90

(a):
figure 7.2: solving a decision tree.
decision tree for the party-friend problem,
(b): solving the dt cor-
example(7.2).
responds to making the decision with the
highest expected future utility. this can be
achieved by starting at the leaves (utilities).
for a chance parent node x, the utility of
the parent is the expected utility of that
variable. for example, at the top of the dt
we have the rain variable with the children
   100 (id203 0.6) and 500 (probabil-
ity 0.4). hence the expected utility of the
rain node is    100   0.6 + 500   0.4 = 140.
for a decision node, the value of the node
is the optimum of its child values. one re-
curses thus backwards from the leaves to
the root. for example, the value of the
rain chance node in the lower branch is
given by 140  0.6+50  0.4 = 104. the opti-
mal decision sequence is then given at each
decision node by    nding which child node
has the maximal value. hence the overall
best decision is to decide to go ahead with
the party. if we decided not to do so, and
it does not rain, then the best decision we
could take would be to not visit the friend
(which has an expected utility of 50). a
more compact description of this problem
is given by the in   uence diagram,    g(7.4).
see also demodecpartyfriend.m.

(a)

(b)

we don   t know whether or not our friend will be in, so we compute the expected utility by averaging over
this unknown. we then take the optimal decision by maximising over v isit. subsequently we move to the
next-to-last decision, assuming that what we will do in the future is optimal. since in the future we will
have taken a decision under the uncertain f riend variable, the current decision can then be taken under
uncertainty about rain and maximising this expected optimal utility over p arty. note that the sequence
of maximisations and summations matters     changing the order will in general result in a di   erent problem
with a di   erent expected utility1.

for the party-friend example the dt is asymmetric since if we decide to go ahead with the party we will not
visit the friend, curtailing the further decisions present in the lower half of the tree. whilst the dt approach
is    exible and can handle decision problems with arbitrary structure, a drawback is that the same nodes are
often repeated throughout the decision tree. for a longer sequence of decisions, the number of branches in
the tree can grow exponentially with the number of decisions, making this representation impractical.

1if one only had a sequence of summations, the order of the summations is irrelevant     likewise for the case of all maximi-

sations. however, summation and maximisation operators do not in general commute.

134

draft november 9, 2017

extending id110s for decisions

p arty

rain

u tility

figure 7.3: an in   uence diagram which contains random vari-
ables (denoted with ovals/circles) decision nodes (denoted with
rectangles) and utility nodes (denoted with diamonds). con-
trasted with    g(7.1) this is a more compact representation of the
structure of the problem. the diagram represents the expres-
sion p(rain)u(party, rain). in addition the diagram denotes an
ordering of the variables with party     rain (according to the
convention given by equation (7.3.1)).

7.3 extending id110s for decisions

an in   uence diagram is a id110 with additional decision nodes and utility nodes [150, 162, 176].
the decision nodes have no associated distribution and the utility nodes are deterministic functions of their
parents. the utility and decision nodes can be either continuous or discrete; for simplicity, in the examples
here the decisions will be discrete.

a bene   t of id90 is that they are general and explicitly encode the utilities and probabilities
associated with each decision and event. in addition, we can readily solve small decision problems using
id90. however, when the sequence of decisions increases, the number of leaves in the decision tree
grows and representing the tree can become an exponentially complex problem. in such cases it can be
useful to use an in   uence diagram (id). an id states which information is required in order to make each
decision, and the order in which these decisions are to be made. the details of the probabilities and utilities
are not speci   ed in the id, and this can enable a more compact description of the decision problem.

7.3.1 syntax of in   uence diagrams

information links an information link from a random variable into a decision node

x

d

d

indicates that the state of the variable x will be known before decision d is taken. information links
from another decision node d in to d similarly indicate that decision d is known before decision d is
taken. we use a dashed link to denote that decision d is not functionally related to its parents.

random variables random variables may depend on the states of parental random variables (as in belief

networks), but also decision node states:

d

x

y

as decisions are taken, the states of some random variables will be revealed. to emphasise this we
typically shade a node to denote that its state will be revealed during the sequential decision process.

utilities a utility node is a deterministic function of its parents. the parents can be either random

variables or decision nodes.

d

u

x

in the party example, the bn trivially consists of a single node, and the in   uence diagram is given in    g(7.3).
the more complex party-friend problem is depicted in    g(7.4). the id generally provides a more compact
representation of the structure of problem than a dt, although details about the speci   c probabilities and
utilities are not present in the id.

draft november 9, 2017

135

extending id110s for decisions

figure 7.4: an in   uence diagram for the party-friend problem,
example(7.2). the partial ordering is p arty   
   
f riend. the dashed-link from party to visit is not strictly nec-
essary but retained in order to satisfy the convention that there
is a directed path connecting all decision nodes.

    rain     v isit   

p arty

v isit

rain

f riend

uparty

uvisit

partial ordering

an id de   nes a partial ordering of the nodes. we begin by writing those variables x0 whose states are
known (evidential variables) before the    rst decision d1. we then    nd that set of variables x1 whose states
are revealed before the second decision d2. subsequently the set of variables xt is revealed before decision
dt+1. the remaining fully-unobserved variables are placed at the end of the ordering:

x0     d1     x1     d2, . . . ,    xn   1     dn     xn

(7.3.1)

with xk being the variables revealed between decision dk and dk+1. the term    partial    refers to the fact
that there is no order implied amongst the variables within the set xn. for notational clarity, at points
below we will indicate decision variables with     to reinforce that we maximise over these variables, and sum
over the non-starred variables. where the sets are empty we omit writing them.
in    g(7.5a) we consider
an oil exploration situation in which there is    rst a decision to be taken as to whether or not to carry out
a seismic test, with an associated utility (cost) u1. the result of this test is represented by the variable
seismic, and depends on whether there is oil present. based on this seismic result, a subsequent decision
is to be taken as to whether or not to drill for oil, which has an associated utility u2. the ordering is
   
t est

   

    seismic     drill

    oil.

the optimal    rst decision d1 is determined by computing

(cid:88)

x1

(cid:88)

xn   1

(cid:88)

(cid:89)

xn

i   i

u (d1|x0)    

max
d2

. . .

max
dn

p (xi|pa (xi))

(cid:88)

j   j

uj (pa (uj))

(7.3.2)

for each state of the decision d1, given x0. in equation (7.3.2) above i denotes the set of indices for the
random variables, and j the indices for the utility nodes. for each state of the conditioning variables, the
optimal decision d1 is found using

argmax

d1

u (d1|x0)

(7.3.3)

remark 7.1 (reading o    the partial ordering). sometimes it can be tricky to read the partial ordering from
the id. a method is to identify the    rst decision d1 and then any variables x0 that need to be observed to
make that decision. then identify the next decision d2 and the variables x1 that are revealed after decision
d1 is taken and before decision d2 is taken, etc. this gives the partial ordering x0     d1     x1     d2, . . ..
place any unrevealed variables at the end of the ordering.

implicit and explicit information links

the information links are a potential source of confusion. an information link speci   es explicitly which
quantities are known before that decision is taken2. we also implicitly assume the no forgetting assumption
that all past decisions and revealed variables are available at the current decision (the revealed variables

2some authors prefer to write all information links where possible, and others prefer to leave them implicit. here we largely
take the implicit approach. for the purposes of computation, all that is required is a partial ordering; one can therefore view
this as    basic    and the information links as super   cial (see [73]).

136

draft november 9, 2017

@@

extending id110s for decisions

t est

oil

u2

t est

oil

u2

u1

seismic

drill

u1

seismic

drill

(a)

(b)

   
(a): the partial ordering is t est

    oil. the explicit information links
figure 7.5:
from t est to seismic and from seismic to drill are both fundamental in the sense that removing either
results in a di   erent partial ordering. the shaded node emphasises that the state of this variable will be
revealed during the sequential decision process. conversely, the non-shaded node will never be observed.
(b): based on the id in (a), there is an implicit link from t est to drill since the decision about t est is
taken before seismic is revealed.

    seismic     drill

   

@@

are necessarily the parents of all
future decision nodes). if we were to include all such information links,
ids would get potentially rather messy. in    g(7.5), both explicit and implicit information links are demon-
strated. we call an information link fundamental if its removal would alter the partial ordering.

causal consistency

@@ for an in   uence diagram to be consistent a current decision cannot a   ect the past. this means that any

random variable descendants of a decision d in the id must come later in the partial ordering.

asymmetry

ids are most convenient when the corresponding dt is symmetric. however, some forms of asymmetry
are relatively straightforward to deal with in the id framework. for our party-friend example, the dt is
asymmetric. however, this is easily dealt with in the id by using a link from p arty to uvisit which removes
the contribution from uvisit when the party goes ahead.

more complex issues arise when the set of variables that can be observed depends on the decision sequence
taken. in this case the dt is asymmetric. in general, in   uence diagrams are not well suited to modelling
such asymmetries, although some e   ects can be mediated either by careful use of additional variables, or
extending the id notation. see [73] and [162] for further details of these issues and possible resolutions.

example 7.3 (should i do a phd?). consider a decision whether or not to do phd as part of our education
(e). taking a phd incurs costs, uc both in terms of fees, but also in terms of lost income. however, if
we have a phd, we are more likely to win a nobel prize (p ), which would certainly be likely to boost our
income (i), subsequently bene   tting our    nances (ub). this setup is depicted in    g(7.6a). the ordering is
(excluding empty sets)

    {i, p}

(7.3.4)

   

e

and

dom(e) = (do phd, no phd) , dom(i) = (low, average, high) , dom(p ) = (prize, no prize)

(7.3.5)

the probabilities are

p(win nobel prize|no phd) = 0.0000001

p(win nobel prize|do phd) = 0.001

p(low|do phd, no prize) = 0.1 p(average|do phd, no prize) = 0.5 p(high|do phd, no prize) = 0.4
p(low|no phd, no prize) = 0.2 p(average|no phd, no prize) = 0.6 p(high|no phd, no prize) = 0.2
p(low|do phd, prize) = 0.01
p(high|do phd, prize) = 0.95
p(high|no phd, prize) = 0.95
p(low|no phd, prize) = 0.01

p(average|do phd, prize) = 0.04
p(average|no phd, prize) = 0.04

draft november 9, 2017

(7.3.6)

(7.3.7)

137

s

us

p

i

ub

e

uc

(a)

e

uc

p

i

ub

(b)

extending id110s for decisions

figure 7.6: (a): education e incurs some
cost, but also gives a chance to win a pres-
tigious science prize. both of these a   ect
our likely incomes, with corresponding long
term    nancial bene   ts. (b): the start-up
scenario.

the utilities are

uc (do phd) =    50000, uc (no phd) = 0,
ub (low) = 100000, ub (average) = 200000, ub (high) = 500000

the expected utility of education is

u (e) =

p(i|e, p )p(p|e) [uc(e) + ub(i)]

(cid:88)

i,p

(7.3.8)

(7.3.9)

(7.3.10)

so that u (do phd) = 260174.000, whilst not taking a phd is u (no phd) = 240000.0244, making it on average
bene   cial to do a phd. see demodecphd.m.

example 7.4 (phds and start-up companies). in   uence diagrams are particularly useful when a sequence
of decisions is taken. for example, in    g(7.6b) we model a new situation
(now dropping the link from
e to i simply to reduce the table size below) in which someone has    rst decided whether or not to take
a phd. ten years later in their career they decide whether or not to make a start-up company. this
decision is based on whether or not they won the nobel prize. the start-up decision is modelled by s with
dom(s) = (tr, fa). if we make a start-up, this will cost some money in terms of investment. however, the
potential bene   t in terms of our income could be high.

++

we model this with (the other required table entries being taken from example(7.3)):

p(low|start up, no prize) = 0.1
p(low|no start up, no prize) = 0.2 p(average|no start up, no prize) = 0.6 p(high|no start up, no prize) = 0.2
p(low|start up, prize) = 0.005
p(low|no start up, prize) = 0.05

p(average|start up, no prize) = 0.5
p(average|start up, prize) = 0.005
p(average|no start up, prize) = 0.15

p(high|start up, no prize) = 0.4
p(high|start up, prize) = 0.99
p(high|no start up, prize) = 0.8

(7.3.11)

and

us (start up) =    200000,

us (no start up) = 0

(7.3.12)

our interest is to advise whether or not it is desirable (in terms of expected utility) to take a phd, now
bearing in mind that later one may or may not win the nobel prize, and may or may not make a start-up
company.

the ordering is (eliding empty sets)

   

e

    p     s

   

    i

138

(7.3.13)

draft november 9, 2017

solving in   uence diagrams

the expected optimal utility for any state of e is

u (e) =

max

s

p(i|s, p )p(p|e) [us(s) + uc(e) + ub(i)]

(cid:88)

p

(cid:88)

i

where we assume that the optimal decisions are taken in the future. computing the above, we    nd

u (do phd) = 190195.00,

u (no phd) = 240000.02

hence, we are better o    not doing a phd. see demodecphd.m.

7.4 solving in   uence diagrams

(7.3.14)

(7.3.15)

solving an in   uence diagram means computing the optimal decision or sequence of decisions. the direct
variable elimination approach is to take equation (7.3.2) and perform the required sequence of summations
and maximisations explicitly. due to the causal consistency requirement the future cannot in   uence the
past. to help matters with notation, we order the variables and decisions such that we may write the belief
network of the in   uence diagram as

@@

for a general utility u(x1:t , d1:t ), solving the id

then corresponds to carrying out the operations

let   s look at eliminating    rst xt and then dt . our aim is to write a new id on the reduced variables
x1:t   1, d1:t   1. since xt and dt only appear in the    nal factor of the belief network, we can write

max

d1

. . . max
dt   1

p(xt|x1:t   1, d1:t) max

dt

p(xt|x1:t   1, d1:t )u(x1:t , d1:t )

(cid:88)

xt

p(x1:t , d1:t ) =

t(cid:89)

t=1

p(xt|x1:t   1, d1:t)
t(cid:89)
(cid:88)

max

d1

. . . max

dt

xt

t=1

p(xt|x1:t   1, d1:t)u(x1:t , d1:t )

(cid:88)

x1

(cid:88)

x1

(cid:88)

x1

(cid:88)

t   1(cid:89)

xt   1

t=1

(cid:88)

t   1(cid:89)

xt   1

t=1

which is then a new id

max

d1

. . . max
dt   1

with modi   ed potential

  u(x1:t   1, d1:t   1)     max

dt

p(xt|x1:t   1, d1:t)  u(x1:t   1, d1:t   1)
(cid:88)

p(xt|x1:t   1, d1:t )u(x1:t , d1:t )

xt

(7.4.1)

(7.4.2)

(7.4.3)

(7.4.4)

(7.4.5)

this however doesn   t exploit the fact that the utilities will typically have structure. without loss of gen-
erality, we may also write the utility as one that is independent of xt , dt and one that depends on xt , dt :

u(x1:t , d1:t ) = ua(x1:t   1, d1:t   1) + ub(x1:t , d1:t )

then eliminating xt , dt updates the utility to

  u(x1:t   1, d1:t   1) = ua(x1:t   1, d1:t   1) + max
dt

(cid:88)

xt

draft november 9, 2017

p(xt|x1:t   1, d1:t )ub(x1:t , d1:t )

(7.4.6)

(7.4.7)

139

7.4.1 messages on an id

for an id with two sets of variables x1,x2 and associated decision sets d1 and d2, we can write the belief
network as

solving in   uence diagrams

p(x|d) = p(x2|x1,d1,d2)p(x1|d1)

where d1     x1     d2     x2, and corresponding utilities

u(x ,d) = u(x1,d) + u(x1,x2,d)

the optimal utility is given by

uopt = maxd1

p(x|d)u(x ,d)

after eliminating x2 and d2, we obtain the id

x2

x1

maxd2

(cid:88)
(cid:88)
      u(x1,d) +
(cid:88)
            u(x1,d) +

maxd2

x2

p(x|d)

p(x1|d1)

          (cid:88)
where (cid:80)   

(x ,d)2

      

p(x2|x1,d1,d2)u(x1,x2,d)

(cid:80)   

1
(x ,d)2

   (cid:88)

p(x|d)

(x ,d)2

p(x|d)u(x1,x2,d)

      

which we can express in terms of the original distribution p(x|d) as

(7.4.8)

(7.4.9)

(7.4.10)

(7.4.11)

@@

(7.4.12)

y refers to summing    rst over the chance variables in y and then maximising over the decision
variables in y. these updates then de   ne an id on a reduced set of variables and can be viewed as messages.
the potential usefulness of equation (7.4.12) is that it may be applied to ids that are causally consistent
(future decisions cannot a   ect the past) but which are not expressed directly in a causal form.

7.4.2 using a junction tree

in complex ids computational e   ciency in carrying out the series of summations and maximisations may
be an issue and one therefore seeks to exploit structure in the id. it is intuitive that some form of junction
tree style algorithm is applicable. the treatment here is inspired by [160]; a related approach which deals
with more general chain graphs is given in [73]. we can    rst represent an id using decision potentials which
consist of two parts, as de   ned below.

de   nition 7.2 (decision potential). a decision potential on a clique c contains two potentials: a id203
potential   c and a utility potential   c. the joint potentials for the junction tree are de   ned as

   =

  c,

   =

  c

(7.4.13)

(cid:89)

c   c

(cid:88)

c   c

with the junction tree representing the term     .

(inverse of the) partial ordering which
in this case there are constraints on the triangulation, imposed by the
restricts the variables elimination sequence. this results in a so-called strong junction tree. the sequence
of steps required to construct a jt for an id is given by the following procedure:

@@

procedure 7.1 (making a strong junction tree).

remove information edges parental links of decision nodes are removed.

moralization marry all parents of the remaining nodes.

140

draft november 9, 2017

solving in   uence diagrams

remove utility nodes remove the utility nodes and their parental links.

strong triangulation form a triangulation based on an elimination order which obeys the partial ordering

of the variables.

strong junction tree from the strongly triangulated graph, form a junction tree and orient the edges

towards the strong root (the clique that appears last in the elimination sequence).

the cliques are then ordered according to the sequence in which they are eliminated. the separator prob-
ability cliques are initialised to the identity, with the separator utilities initialised to zero. the id203
cliques are then initialised by placing id155 factors into the lowest available clique (that is
the id203 factors are placed in the cliques closest to the leaves of the tree, furthest from the root) that
can contain them, and similarly for the utilities. remaining id203 cliques are set to the identity and
utility cliques to zero.

example 7.5 (junction tree). an example of a junction tree for an id is given in    g(7.7a). the
moralisation and triangulation links are given in    g(7.7b). the orientation of the edges follows the
partial ordering with the leaf cliques being the    rst to disappear under the sequence of summations and
maximisations.

a by-product of the above steps is that the cliques describe the fundamental dependencies on previous
decisions and observations. in    g(7.7a), for example, the information link from f to d2 is not present in the
moralised-triangulated graph    g(7.7b), nor in the associated cliques of    g(7.7c). this is because once e is
revealed, the utility u4 is independent of f , giving rise to the two-branch structure in    g(7.7b). nevertheless,
the information link from f to d2 is fundamental since it speci   es that f will be revealed     removing this
link would therefore change the partial ordering.

absorption

by analogy with the de   nition of messages in section(7.4.1), for two neighbouring cliques c1 and c2, where
c1 is closer to the strong root of the jt (the last clique de   ned through the elimination order), we de   ne

   (cid:88)

c2\s

   (cid:88)

c2\s

  s =

  c2,

  s =

  c2  c2

  new
c1 =   c1  s,

in the above(cid:80)   

  new
c1 =   c1 +

  s
  s

c is a    generalised marginalisation    operation     it sums over those elements of clique c which
are random variables and maximises over the decision variables in the clique. the order of this sequence of
sums and maximisations follows the partial ordering de   ned by    .
absorption is then carried out from the leaves inwards to the root of the strong jt. the optimal setting
of a decision d1 can then be computed from the root clique. subsequently backtracking may be applied to
infer the optimal decision trajectory. the optimal decision for d can be obtained by working with the clique
containing d which is closest to the strong root and setting any previously taken decisions and revealed
observations into their evidential states. see demodecasia.m for an example.

draft november 9, 2017

141

(7.4.14)

(7.4.15)

l

u4

u3

a

b

d1

c

d

d4

i

h

d3

j

k

u2

g

d2

(a)

a

b

d1

u1

c

d

e

f

b, c, a

b, c

solving in   uence diagrams

d4

i

h

d3

l

j

k

g

d2

e

f

(b)

b, e, d, c

e, d2, g

d2, g

d2, g, d4, i

d4, i

d4, i, l

b, e, d

b, d1, e, f, d

e

f

f, d3, h

d3, h

d3, h, k

h, k

h, k, j

(c)

(a): in   uence diagram, adapted from [160]. causal consistency is satis   ed since there is a
figure 7.7:
directed path linking all decisions in sequence. the partial ordering is b     d1     (e, f )     d2     (  )     d3    
g     d4     (a, c, d, h, i, j, k, l). (b): moralised and strongly triangulated graph. moralisation links are in
(c): strong junction tree. absorption passes information
green, strong triangulation links are in red.
from the leaves of the tree towards the root.

example 7.6 (absorption on a chain). for the id of    g(7.8), the moralisation and triangulation steps are
trivial and give the jt:

3: x1, x2, d1

x2

2: x2, x3, d2

x3

1: x3, x4, d3

where the cliques are indexed according the elimination order. the id203 and utility cliques are
initialised to

  3 (x1, x2, d1) = p(x2|x1, d1)   3 (x1, x2, d1) = 0
  2 (x2, x3, d2) = p(x3|x2, d2)   2 (x2, x3, d2) = u(x2)
  1 (x3, x4, d3) = p(x4|x3, d3)   1 (x3, x4, d3) = u(x3) + u(x4)

with the separator cliques initialised to

updating the separator we have the new id203 potential

  1   2 (x3) = 1   1   2 (x3) = 0
  2   3 (x2) = 1   2   3 (x2) = 0

(cid:88)

x4

   

= max

d3

  1 (x3, x4, d3) = 1

  1   2 (x3)

142

(7.4.16)

(7.4.17)

(7.4.18)

draft november 9, 2017

id100

and utility potential

  1   2 (x3)

   

= max

d3

(cid:88)
(cid:32)

x4

  1 (x3, x4, d3)   1 (x3, x4, d3) = max

(cid:33)

d3

(cid:88)

x4

= max

d3

u(x3) +

p(x4|x3, d3)u(x4)

at the next step we update the id203 potential

@@

   
  2 (x2, x3, d2)

   
=   2 (x2, x3, d2)   1   2 (x3)

=

p(x3|x2, d2)

(cid:88)

x4

p(x4|x3, d3) (u(x3) + u(x4))

(7.4.19)

(7.4.20)

(7.4.21)

(cid:33)

p(x4|x3, d3)u(x4)

(7.4.22)

(cid:88)

x4

and utility potential

   
  2 (x2, x3, d2)

=   2 (x2, x3, d2) +

   
  1   2 (x3)
  1   2 (x3)

the next separator decision potential is

(cid:32)

= u(x2) + max

d3

u(x3) +

   
  2   3 (x2)

= max

d2

   
  2   3 (x2)

= max

d2

= max

d2

   
  2 (x2, x3, d2)

= 1

   
  2 (x2, x3, d2)   2 (x2, x3, d2)

(cid:32)

(cid:32)

p(x3|x2, d2)

u(x2) + max

d3

u(x3) +

(cid:33)(cid:33)

p(x4|x3, d3)u(x4)

(cid:88)

x4

x3

(cid:88)
(cid:88)
(cid:88)

x3

x3

finally we end up with the root decision potential

  3 (x1, x2, d1)

   

   
=   3 (x1, x2, d1)   2   3 (x2)

= p(x2|x1, d1)

and

  3 (x1, x2, d1)

   

=   3 (x2, x1, d1) +

   
  2   3 (x2)
   
  2   3 (x2)

(cid:32)

(cid:32)

= max

d2

p(x3|x2, d2)

u(x2) + max

d3

u(x3) +

(cid:88)

x3

from the    nal decision potential we have the expression

   
  3 (x1, x2, d1)

  3 (x1, x2, d1)

   

(cid:33)(cid:33)

p(x4|x3, d3)u(x4)

(cid:88)

x4

(7.4.23)

(7.4.24)

(7.4.25)

(7.4.26)

(7.4.27)

(7.4.28)

(7.4.29)

which is equivalent to that which would be obtained by simply distributing the summations and maximi-
sations over the original id. at least for this special case, we therefore have veri   ed that the jt approach
yields the correct root clique potentials.

7.5 id100

consider a markov chain with transition probabilities p(xt+1 = i|xt = j). at each time t we consider an
action (decision), which a   ects the state at time t + 1. we describe this by

p(xt+1 = i|xt = j, dt = k)

(7.5.1)

associated with each state xt is a utility u(xt), and is schematically depicted in    g(7.8). more generally
one could consider utilities that depend on transitions and decisions, u(xt+1 = i, xt = j, dt = k) and also

draft november 9, 2017

143

d1

x1

d2

x2

u2

d3

x3

u3

x4

u4

id100

figure 7.8: markov decision process. these can be
used to model planning problems of the form    how
do i get to where i want to be incurring the lowest
total cost?   . they are readily solvable using a message
passing algorithm.

time dependent versions of all of these, pt(xt+1 = i|xt = j, dt = k), ut(xt+1 = i, xt = j, dt = k). we   ll stick
with the time-independent (stationary) case here since the generalisations are conceptually straightforward
at the expense of notational complexity. id100 (mdps) can be used to solve planning
tasks such as how to get to a desired goal state as quickly as possible.

for positive utilities, the total utility of any state-decision path x1:t , d1:t is de   ned as (assuming we know
the initial state x1)

u (x1:t )    

u(xt)

and the id203 with which this happens is given by

p(x2:t|x1, d1:t   1) =

p(xt+1|xt, dt)

t(cid:88)

t=2

(cid:88)

x2

t   1(cid:89)
(cid:88)

t=1

x3

at time t = 1 we want to make that decision d1 that will lead to maximal expected total utility

u (d1|x1)    

max

d2

max

d3

. . . max
dt   1

p(x2:t|x1, d1:t   1)u (x1:t )

(cid:88)

x4

(cid:88)

xt

our task is to compute u (d1|x1) for each state of d1 and then choose that state with maximal expected
total utility. to carry out the summations and maximisations e   ciently, we could use the junction tree
approach, as described in the previous section. however, in this case, the id is su   ciently simple that a
direct message passing approach can be used to compute the expected utility.

7.5.1 maximising expected utility by message passing

consider the time-dependent decisions (non-stationary policy) mdp

t   1(cid:89)

t=1

t(cid:88)

t=2

p(xt+1|xt, dt)

u(xt)

for the speci   c example in    g(7.8) the joint model of the bn and utility is

p(x4|x3, d3)p(x3|x2, d2)p(x2|x1, d1) (u(x2) + u(x3) + u(x4))

to decide on how to take the    rst optimal decision, we need to compute

(cid:88)

x4

(cid:88)

x2

(cid:88)

x2

(cid:88)

x3

(cid:88)

x3

u (d1|x1) =

max

d2

max

d3

p(x4|x3, d3)p(x3|x2, d2)p(x2|x1, d1) (u(x2) + u(x3) + u(x4))

(7.5.7)

since only u(x4) depends on x4 explicitly, we can write

(cid:32)

max

d2

p(x3|x2, d2)p(x2|x1, d1)

u(x2) + u(x3) + max

d3

u (d1|x1) =

144

(cid:88)

x4

(cid:33)

p(x4|x3, d3)u(x4)

(7.5.8)

draft november 9, 2017

(7.5.2)

(7.5.3)

(7.5.4)

(7.5.5)

(7.5.6)

id100

de   ning a message and corresponding value

(cid:88)

x4

we can write

d3

u3   4(x3)     max
(cid:88)

u (d1|x1) =

x2

max

d2

p(x4|x3, d3)u(x4),

v(x3)     u(x3) + u3   4(x3)

(cid:88)

x3

p(x3|x2, d2)p(x2|x1, d1) (u(x2) + v(x3))

in a similar manner, only the last term depends on x3 and hence

u (d1|x1) =

p(x2|x1, d1)

u(x2) + max

d2

p(x3|x2, d2)v(x3)

(cid:33)

(cid:88)

x3

(cid:88)

x2

(cid:32)

(cid:88)

x3

de   ning similarly the value

v(x2)     u(x2) + max

d2

p(x3|x2, d2)v(x3)

then

(cid:88)

x2

u (d1|x1) =

p(x2|x1, d1)v(x2)

given u (d1|x1) above, we can then    nd the optimal decision d1 by

   
d
1(x1) = argmax

d1

u (d1|x1)

7.5.2 bellman   s equation

in a markov decision process, as above, we can de   ne utility messages recursively as

ut   1   t(xt   1)     max

dt   1

p(xt|xt   1, dt   1) [u(xt) + ut   t+1(xt)]

(cid:88)

xt

it is more common to de   ne the value of being in state xt as

vt(xt)     u(xt) + ut   t+1(xt),

vt (xt ) = u(xt )

and write then the equivalent recursion

vt   1(xt   1) = u(xt   1) + max
dt   1

p(xt|xt   1, dt   1)vt(xt)

(cid:88)

xt

the optimal decision d   

t is then given by

   
t (xt) = argmax
d

dt

p(xt+1|xt, dt)vt+1(xt+1)

(cid:88)

xt+1

equation(7.5.17) is called bellman   s equation[31]3.

3the continuous-time analog has a long history in physics and is called the hamilton-jacobi equation.

draft november 9, 2017

(7.5.9)

(7.5.10)

(7.5.11)

(7.5.12)

(7.5.13)

(7.5.14)

(7.5.15)

(7.5.16)

(7.5.17)

(7.5.18)

145

temporally unbounded mdps

figure 7.9: states de   ned on a two dimensional grid. in each square the top
left value is the state number, and the bottom right is the utility of being
in that state. an    agent    can move from a state to a neighbouring state, as
indicated. the task is to solve this problem such that for any position (state)
one knows how to move optimally to maximise the expected utility. this
means that we need to move towards the goal states (states with non-zero
utility). see demomdp.

7.6 temporally unbounded mdps

in the previous discussion about mdps we assumed a given end time, t , from which one can propagate
messages back from the end of the chain. the in   nite t case would appear to be ill-de   ned since the sum
of utilities

u(x1) + u(x2) + . . . + u(xt )

(7.6.1)
will in general be unbounded. there is a simple way to avoid this di   culty. if we let u    = maxs u(s) be the
largest value of the utility and consider the sum of modi   ed utilities for a chosen discount factor 0 <    < 1

t(cid:88)

t=1

    t(cid:88)

t=1

  tu(xt)     u

  t =   u

    1       t
1       

(7.6.2)

where we used the result for a geometric series. in the limit t         this means that the summed modi   ed
utility   tu(xt) is    nite. the only modi   cation required to our previous discussion is to include a factor   
in the message de   nition. assuming that we are at convergence, we de   ne a value v(xt = s) dependent
only on the state s, and not the time. this means we replace the time-dependent bellman   s value recursion
equation (7.5.17) with the time-independent equation

v(s)     u(s) +    max

d

p(xt = s(cid:48)

|xt   1 = s, dt   1 = d)v(s(cid:48)

)

(7.6.3)

(cid:88)

s(cid:48)

we then need to solve equation (7.6.3) for the value v(s) for all states s. the optimal decision policy when
one is in state xt = s is then given by
p(xt+1 = s(cid:48)

(s) = argmax

(cid:88)

(7.6.4)

d   

)

|xt = s, dt = d)v(s(cid:48)

d

s(cid:48)

for a deterministic transition p (i.e. for each decision d, only one state s(cid:48) is available), this means that the
best decision is the one that takes us to the accessible state with highest value.

equation(7.6.3) seems straightforward to solve. however, the max operation means that the equations
are non-linear in the value v and no closed form solution is available. two popular techniques for solving
equation (7.6.3), are value and policy iteration, which we describe below. when the number of states s
is very large, approximate solutions are required. sampling and state-dimension reduction techniques are
described in [62].

7.6.1 value iteration

a naive procedure is to iterate equation (7.6.3) until convergence, assuming some initial guess for the values
(say uniform). one can show that this value iteration procedure is guaranteed to converge to a unique
optimum[36]. the convergence rate depends on        the smaller    is, the faster is the convergence. an
example of value iteration is given in    g(7.10).

146

draft november 9, 2017

1020314050607080901001101201301401501601701811902002102202302402502602702802903003103203303403503603703803904004104204304404504614704804905005115205305405505605705805906006106206306406506606706806907017107207317407507607707807908008108208308408508608708808909019109219309409509609709809901000temporally unbounded mdps

figure 7.10: value iteration on a set of 225 states,
corresponding to a 15    15 two dimensional grid. de-
terministic transitions are allowed to neighbours on
the grid, {stay, left, right, up, down}. there are three
goal states, each with utility 1     all other states have
utility 0. plotted is the value v(s) for    = 0.9 after
30 updates of value iteration, where the states index
a point on the x     y grid. the optimal decision for
any state on the grid is to go to the neighbouring state
with highest value. see demomdp.

7.6.2 policy iteration
in policy iteration we    rst assume we know the optimal decision d   
equation (7.6.3) to give

(s) for any state s. we may use this in

v(s) = u(s) +   

p(xt = s(cid:48)

|xt   1 = s, d   

(s))v(s(cid:48)

)

(7.6.5)

(cid:88)

s(cid:48)

the maximisation over d has disappeared since we have assumed we already know the optimal decision for
each state s. for    xed d   
(s), equation (7.6.5) is now linear in the value. de   ning the value v and utility u
vectors and transition matrix p,

in matrix notation, equation (7.6.5) becomes

[v]s = v(s),

[u]s = u(s),

(cid:16)

i       pt(cid:17)

[p]s(cid:48),s = p(s   |s, d   

(s))

(cid:16)

i       pt(cid:17)   1

v = u +   ptv    

v = u     v =

u

(7.6.6)

(7.6.7)

these linear equations are readily solved with gaussian elimination. using this, the optimal policy is
recomputed using equation (7.6.4). the two steps of solving for the value, and recomputing the policy are
iterated until convergence. the procedure may be initialised by guessing an initial d   
(s), then solve the
linear equations (7.6.5) for the value, or alternatively guessing the initial values and solving for the initial
policy.

@@
@@

@@

example 7.7 (a grid-world mdp). we de   ne a set of states on
an n    n grid with corresponding utilities
for each state, as given
for example in    g(7.9) for n = 8. the agent is able to deterministically move to
a neighbouring grid state at each time step. after initialising the value of each grid state to unity, the
obtained. an example on a 15    15 grid is given in    g(7.10) in which
converged value for each state is
the utilities are zero everywhere except for 3 states. the optimal policy is then given by moving to the
neighbouring grid state with highest value.

7.6.3 a curse of dimensionality

consider the following tower of hanoi problem. there are 4 pegs a, b, c, d and 10 disks numbered from 1
to 10. you may move a single disk from one peg to another     however, you are not allowed to put a bigger
numbered disk on top of a smaller numbered disk. starting with all disks on peg a, how can you move them
all to peg d in the minimal number of moves?

this would appear to be a straightforward markov decision process in which the transitions are allowed disk
moves. if we use x to represent the state of the disks on the 4 pegs, naively this has 410 = 1048576 states

draft november 9, 2017

147

123456789101112131415123456789101112131415024variational id136 and planning

(some are equivalent up to permutation of the pegs, which reduces this by a factor of 2). this large number
of states renders this naive approach computationally problematic.

many interesting real-world problems su   er from this large number of states issue so that a naive approach
to    nd the best decision is computationally infeasible. finding e   cient exact and also approximate state
representations is a key aspect to solving large scale mdps, see for example [209].

7.7 variational id136 and planning

for the    nite-horizon stationary policy mdp, learning the optimal policy can be addressed by a variety of
methods. two popular approaches are policy gradients and em style procedures     see for example [110]
and section(11.2).
for many mdps of interest the optimal policy is deterministic[284], so that methods which explicitly seek
for deterministic policies are of interest. for this reason and to remain close to our discussion on policy
and value iteration, which involved deterministic policies, we focus on this case in our brief discussion here,
referring the reader to other texts [82, 299, 108, 109] for the details on the non-deterministic case. for a
time-independent deterministic policy d(s) that maps a state s to a decision d (which we write as    for
short), we have the expected utility

t(cid:88)

(cid:88)

(cid:88)

t(cid:89)

u (  ) =

ut(xt)

t=1

xt

x1:t   1

   =1

p(x  |x     1, d(x     1))

(7.7.1)

with the convention p(x1|x0, d(x0)) = p(x1). viewed as a factor graph, this is simply a set of chains, so
that for any policy   , the expected utility can be computed easily. in principle one could then attempt to
optimise u with respect to the policy directly. an alternative is to use an em style procedure[108]. to do
this we de   ne a (trans-dimensional) distribution

t(cid:89)

   =1

  p(x1:t, t) =

ut(xt)
z(  )

t(cid:88)

(cid:88)

t(cid:89)

p(x  |x     1, d(x     1))
t(cid:88)

(cid:88)

the normalisation constant z(  ) of this distribution is

ut(xt)

p(x  |x     1, d(x     1)) =

t=1

x1:t

   =1

t=1

x1:t

ut(xt)

t(cid:89)

   =1

p(x  |x     1, d(x     1)) = u (  )

if we now de   ne a variational distribution q(x1:t, t), and consider

kl(q(x1:t, t)|  p(x1:t, t))     0

this gives the lower bound

(cid:42)

log u (  )        h(q(x1:t, t)) +

log ut(xt)

(cid:43)
p(x  |x     1, d(x     1))

q(x1:t,t)

t(cid:89)

   =1

where h(q(x1:t, t)) is the id178 of the distribution q(x1:t, t). in terms of an em algorithm, the m-step
requires the dependency on    alone, which is

e(  ) =

=

t(cid:88)
t(cid:88)

t=1

t=1

(cid:88)

   =1

t(cid:88)
t(cid:88)
(cid:40) t(cid:88)

   =1

t(cid:88)

s(cid:48)

t=1

   =1

  e(d|s) =

148

(cid:104)log p(x  |x     1, d(x     1))(cid:105)q(x   ,x     1,t)

q(x   = s(cid:48)

, x     1 = s, t) log p(x   = s(cid:48)

|x     1 = s, d(x     1) = d)

(cid:41)

q(x   = s(cid:48)

, x     1 = s, t)

log p(s(cid:48)

|s, d)

(7.7.8)

draft november 9, 2017

(7.7.2)

(7.7.3)

(7.7.4)

(7.7.5)

(7.7.6)

(7.7.7)

for each given state s we now attempt to    nd the optimal decision d, which corresponds to maximising

financial matters

de   ning

q(s(cid:48)

|s)    

t(cid:88)

t(cid:88)

t=1

   =1

q(x   = s(cid:48)

, x     1 = s, t)

(7.7.9)

we see that for given s, up to a constant,   e(d|s) is the id181 between q(s(cid:48)
p(s(cid:48)
|s, d) so that the optimal decision d is given by the index of the distribution p(s(cid:48)
with q(s(cid:48)
d   

|s):
(s) = argmin

kl(cid:0)q(s

|s) and
|s, d) most closely aligned

(7.7.10)

(cid:48)

(cid:48)

d

|s)|p(s

|s, d)(cid:1)

the e-step concerns the computation of the marginal distributions required in the m-step. the optimal q
distribution is proportional to   p evaluated at the previous decision function d:

t(cid:89)

   =1

q(x1:t, t)     ut(xt)

p(x  |x     1, d(x     1))

for a constant discount factor    at each time-step and an otherwise time-independent utility4

ut(xt) =   tu(xt)

using this

q(x1:t, t)       tu(xt)

t(cid:89)

   =1

p(x  |x     1, d(x     1))

(7.7.11)

(7.7.12)

(7.7.13)

for each t this is a simple markov chain for which the pairwise transition marginals required for the m-step,
equation (7.7.9) are straightforward. this requires id136 in a series of markov models of di   erent lengths.
this can be done e   ciently using a single forward and backward pass [299, 110].

em and related methods follow closely the spirit of id136 in id114, but can exhibit disappoint-
ingly slow convergence. more recently, an alternative method using lagrange duality shows very promising
performance and the reader is referred to [111] for details. note that this em algorithm formally fails in
the case of a deterministic environment (the transition p(xt|xt   1, dt   1) is deterministic)     see exercise(7.8)
for an explanation and exercise(7.9) for a possible resolution.

remark 7.2 (solving an mdp     easy or hard?). the discussion in section(7.5.1) highlights that solving a
linear-chain in   uence diagram (   nding the optimal decision at each timestep) is straightforward, and can
be achieved using a simple message passing algorithm, scaling linearly with the length of the chain.
in
contrast,    nding the optimal time-independent policy    typically is much more complex     hence the reason
for many di   erent algorithms that attempt to    nd optimal policies in areas such as time-independent control,
id23 and games. mathematically, the reason for this di   erence is that the constraint in the
time-independent case that the policy must be the same over all time steps, leads to a graphical structure
that is no longer a chain, with all timepoints connected to a single   . in this case, in general, no simple
linear time message passing algorithm is available to optimise the resulting objective.

7.8 financial matters

utility and decision theory play a major role in    nance, both in terms of setting prices based on expected
future gains, but also in determining optimal investments. in the following sections we brie   y outline two
such basic applications.

4in the standard mdp framework it is more common to de   ne ut(xt) =   t   1u(xt) so that for comparison with the standard

policy/value routines one needs to divide the expected utility by   .

draft november 9, 2017

149

financial matters

su

s   

sd

t

figure 7.11: options pricing: an asset has market value   s at time
t = 1. we assume that the asset will have market value either   su
or   sd at time t . the owner and    options buyer    (client) agree that
if the market value is greater than the    strike price      s    at time t ,
the client has the right to purchase the asset for   s   . the question
is: how much should the owner of the asset charge the client for the
privilege of having the right to purchase the asset?

s

1

7.8.1 options pricing and expected utility

an owner has an asset currently priced by the market at   s. the owner wants to give us the opportunity
to purchase this asset at time t for an agreed price of   s   . at time t , if the market price goes    up    beyond
the strike price to   su we will decide to purchase the asset from the owner for   s    and sell the asset at
this increased value, see    g(7.11). if, however, the price remains    down    below the strike price at   sd, we
will walk away, leaving the owner with the asset. the question is, how much should the owner charge,   c
for this option of us being able to buy the asset for the agreed price at time t ? to help answer this, we also
need to know how much a risk-free investment would make over the same time period (i.e. how much we
would get from putting money in a safe bank). we assume the interest rate r is known for the time period
t , say 0.06. we call the two people the owner and the client (who may or may not buy the asset).

two possibilities case

for simplicity, we assume the asset can take only the prices su or sd at time t . we also assume we know
the probabilities of these events (see below), namely    and 1       . let   s work out the expected utilities for
both parties:

asset goes up:

u (up, client) =

u (up, owner) =

immediate pro   t from selling

option cost

lost interest on option cost

(cid:124) (cid:123)(cid:122) (cid:125)
su     s   
(cid:124) (cid:123)(cid:122) (cid:125)
s        s

   

   

c(cid:124)(cid:123)(cid:122)(cid:125)
c(cid:124)(cid:123)(cid:122)(cid:125)

+

+

cr(cid:124)(cid:123)(cid:122)(cid:125)
cr(cid:124)(cid:123)(cid:122)(cid:125)

    sr(cid:124)(cid:123)(cid:122)(cid:125)

immediate loss from sale

option cost

gained interest on option cost

lost interest

the    nal sr term above comes from entering into the option deal     otherwise the owner could have just
sold the asset at time 1, and then put the resulting sum in the bank.

asset goes down:

u (down, client) =    

option cost

lost interest on option cost

   

c(cid:124)(cid:123)(cid:122)(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)

sd     s

cr(cid:124)(cid:123)(cid:122)(cid:125)
c(cid:124)(cid:123)(cid:122)(cid:125)

the above follows since in this case the client doesn   t act on his option to sell the asset.

u (down, owner) =

+

+

change in asset value

option cost

gained interest on option cost

lost interest

cr(cid:124)(cid:123)(cid:122)(cid:125)

    sr(cid:124)(cid:123)(cid:122)(cid:125)

the expected utility for the client is

u (client) =       u (up, client) + (1       )    u (down, client)
=    (su     s        c     cr) + (1       ) (   c     cr)
=    (su     s   )     c(1 + r)

the expected utility for the owner is

u (owner) =       u (up, owner) + (1       )    u (down, owner)

=    (s        s + c + cr     sr) + (1       ) (sd     s + c + cr     sr)
=    (s        sd) + sd     s + c(1 + r)     sr

150

draft november 9, 2017

(7.8.1)

(7.8.2)

(7.8.3)

(7.8.4)

(7.8.5)

(7.8.6)

(7.8.7)

(7.8.8)

(7.8.9)

(7.8.10)

financial matters

it seems reasonable to assume that both the client and the owner should have the same expected bene   t,
u (client) = u (owner). hence

   (su     s   )     c(1 + r) =    (s        sd) + sd     s + c(1 + r)     sr

solving for c, we    nd

c =

   (su     2s    + sd)     sd + s(1 + r)

2(1 + r)

(7.8.11)

(7.8.12)

all the quantities required to price the option are assumed known, except for   . one way to set this is
described below.

setting   

it seems reasonable to expect    to be set by some process which describes the true id203 of the price
increase. however, given a value for    and knowing the two possible prices su and sd, the owner can compute
the expected utility of just holding on to the asset. this is

  su + (1       )sd     s

(7.8.13)

alternatively, the owner could sell the asset for   s and put his money in the bank, collecting the interest
rs at time t . in a fair market we must have that the expected reward for holding on to the asset is the
same as the risk-free return on the asset:

  su + (1       )sd     s = rs        =

s(1 + r)     sd

su     sd

(7.8.14)

using this value to price the option in equation (7.8.12) ensures that the expected gain from o   ering the
option and not o   ering the option is the same to the owner and, furthermore that if the option is available,
the expected reward for both parties is the same.

7.8.2 binomial options pricing model

if we have more than two timepoints, we can readily extend the above. it   s easiest to assume that at each
time t we have two price change possibilities     either the price can go up by a factor u > 1 or down by
a factor d < 1. for t timesteps, we then have a set of possible values the asset can take at time t . for
some of them the client will sell the asset (when st > s   ) otherwise not. we need to work out the expected
gains for both the owner and client as before, assuming    rst that we know    (which is the id203 of the
price increasing by a factor u in one time step). for a sequence of n ups and t     n downs, we have a price
st = sundt   n. if this is greater than s   , then the client will sell the asset and have utility

i(cid:2)sundt   n > s   (cid:3) (sundt   n     s        c(1 + r))

the id203 of n ups and t     n downs is

(cid:18)t

(cid:19)

where(cid:0)t

n

  (t, n,   )    

n

(cid:1) is the binomial coe   cient. hence the total expected utility for the client on the upside is

  n(1       )t   n
t(cid:88)
  (t, n,   )i(cid:2)sundt   n > s   (cid:3) (sundt   n     s        c(1 + r))

u (client, up) =

similarly,

n=0

u (client, down) =    c(1 + r)

draft november 9, 2017

  (t, n,   )i(cid:2)sundt   n < s   (cid:3)

t(cid:88)

n=0

(7.8.15)

(7.8.16)

(7.8.17)

(7.8.18)

151

financial matters

the total expected utility for the client is then

u (client) = u (client, up) + u (client, down)

similarly, for the owner

u (owner, up) = (s        s(1 + r) + c(1 + r))

  (t, n,   )i(cid:2)sundt   n > s   (cid:3)

t(cid:88)

n=0

u (owner, down) =

t(cid:88)

n=0

  (t, n,   )i(cid:2)sundt   n < s   (cid:3)(cid:0)sundt   n     s(1 + r) + c(1 + r)(cid:1)

and

and

u (owner) = u (owner, up) + u (owner, down)

setting

u (client) = u (owner)

results in a simple linear equation for c.

setting   

(7.8.19)

(7.8.20)

(7.8.21)

@@

(7.8.22)

(7.8.23)

to set   , we can use a similar logic as in the two timestep case. first we compute the expected value of the
asset at time t , which is

  (t, n,   )sundt   n

t(cid:88)

n=0

t(cid:88)

n=0

(7.8.24)

(7.8.25)

and equate the expected gain equal to the gain from a risk free investment in the asset:

  (t, n,   )sundt   n     s = rs    

  (t, n,   )undt   n = r + 1

t(cid:88)

n=0

knowing u and d, we can solve the above for   , and then use this in the equation for c. we can learn u and
d from past observed data (in the literature they are often related to the observed variance in the prices[75]).

the binomial options pricing approach is a relatively simplistic way to price options. the celebrated black-
scholes method [46] is essentially a limiting case in which the number of timepoints becomes in   nite[151].

7.8.3 optimal investment

another example of utility in    nance, and which is related to id100, is the issue of
how best to invest your wealth in order to maximise some future criterion. we   ll consider here a very
simple setup, but one which can be readily extended to more complex scenarios. we assume that we have
two assets, a and b with prices at time t given by sa
t. the prices are assumed to independently follow
markovian updating:

t , sb

sa
t = sa

t   1(1 +  a

t )     p(sa

t |sa

t   1,  a

t     (1 +  a

t )sa

t   1

t ) =   (cid:0)sa

(cid:1)

where   (  ) is the dirac delta function. the price increments follow a markov transition

p( a

t ,  b

t   1,  b

t   1) = p( a

t   1)p( b

t | a

t   1)

t| b

t| a

(7.8.26)

(7.8.27)

using this one can model e   ects such as price increments being likely to stay the same (as in bank interest)
or more variable (as in the stock market).

152

draft november 9, 2017

financial matters

(a)

(b)

(c)

figure 7.12: (a): two assets through time     a risky asset whose value    uctuates wildly, and a stable asset
that grows slowly. (b): the wealth of our portfolio over time, based on investing with the hope to achieve
(c): the optimal investment decisions through time, with 1 corresponding to
a wealth of 1.5 at t = 40.
placing all wealth in the safe asset and 0 placing all the money on the risky asset.

we have an investment decision 0     dt     1 that says what fraction of our current wealth wt we buy of asset
a at time t. we will invest the rest of our wealth in asset b. if asset a is priced at sa
t and we decide to use
a fraction dt of our current wealth wt to purchase a quantity qa
t of asset b, these
are given by

t of asset a and quantity qb

qa
t =

dtwt
sa
t

,

qb
t =

wt(1     dt)

sb
t

(7.8.28)

at time step t + 1, the prices of assets a and b will have changed to sa
be

t+1, sb

t+1, so that our new wealth will

wt+1 = qa

t sa

t+1 + qb

t sb

t+1 =

t+1

= wt

dt(1 +  a

t+1) + (1     dt)(1 +  b

t+1)

this can be expressed as a transition

p(wt+1|wt,  a

t+1,  b

t+1, dt) =   

wt+1     wt

t+1) + (1     dt)(1 +  b

t+1)

(cid:17)(cid:17)

t+1

dtwtsa
sa
t

+

(cid:16)

sb
t

wt(1     dt)sb
(cid:16)

dt(1 +  a

(cid:17)

(7.8.29)

(7.8.30)

(cid:16)

at an end time t we have a utility u(wt ) that expresses our satisfaction with our wealth. given that we
start with a wealth w1 and assume we know  a
1, we want to    nd the best decision d1 that will maximise
our expected utility at time t . to do so, we also bear in mind that at any intermediate time 1 < t < t we
can adjust the fraction dt of wealth in asset a. the markov chain is given by (see also section(23.1))

1,  b

1:t , w2:t| a
(cid:88)
(cid:111)

 a
2 , b

2,w2

 a
2,  b

2, w2

(cid:110)

p( a

1:t ,  b

1,  b

1, w1, d1:t   1) =

p( a

t | a

t   1)p( b

t| b

t   1)p(wt|wt   1,  a

t ,  b

t, dt   1)

(7.8.31)

(cid:88)
(cid:111)

1:t , w2:t| a
(cid:110)

(cid:111)

the expected utility of a decision d1 is
u (d1| a

. . . max
dt   2

1, w1) =

1,  b

max
dt   1

 a
t   1, b

t   1,wt   1

 a
t , b

t ,wt

p( a

1:t ,  b

1,  b

1, w1, d1:t   1)u(wt ) (7.8.32)

which, for the corresponding in   uence diagram, corresponds to the ordering

d1    

to compute u (d1| a
t   1,  b

  t   1   t ( a

and, generally,

 a
t ,  b

 a
t   1,  b

t   1, wt   1

    dt   1    

t , wt

(cid:88)

    d2     . . .    
1,  b
1, w1), we    rst can carry out the operations at t to give a message
t   1, wt   1)     max
(cid:88)

t   1)p(wt|wt   1,  a

t   1)p( b

t| a

t| b

t ,  b

p( a

t , b
 a

dt   1

t ,wt

(7.8.33)

t , dt   1)u(wt ) (7.8.34)

  t   1   t( a

t   1,  b

t   1, wt   1)     max

dt   1

p( a

t | a

t   1)p( b

t| b

t   1)p(wt|wt   1,  a

t ,  b

t, dt   1)  t   t+1( a

t ,  b

t, wt) (7.8.35)

 a
t , b

t ,wt

draft november 9, 2017

153

t(cid:89)

t=2

(cid:88)
(cid:110)

05101520253035400.511.522.5305101520253035400.811.21.41.61.8051015202530354000.20.40.60.81d1

h1

v1

d2

h2

v2

u2

d3

h3

v3

u3

h4

v4

u4

further topics

figure 7.13: an example partially observable markov
decision process (pomdp). the    hidden    variables h
are never observed. in solving the in   uence diagram
we are required to    rst sum over variables that are
never observed; doing so will couple together all past
observed variables and decisions; any decision at time
t will then depend on all previous decisions. note that
the no-forgetting principle means that we do not need
to explicitly write that each decision depends on all
previous observations     this is implicitly assumed.

so that

u (d1| a

1,  b

1, w1) =

(cid:88)

 a
2 , b

2,w2

p( a

2| a

1)p( b

2| b

1)p(w2|w1,  a

2,  b

2, d1)  2   3( a

2,  b

2, w2)

(7.8.36)

note that this process is not equivalent to a    myopic    strategy which would make an investment decision to
maximise the expected next-period wealth.

for a continuous wealth wt the above messages are di   cult to represent. a simple strategy, therefore, is to
discretise all wealth values and also the price changes  a
t and investment decisions dt, see exercise(7.13).
in this case, one needs to approximate the delta function   (x) by the distribution which is zero for every
state except that discrete state which is closest to the real value x.

t ,  b

example 7.8 (optimal investment). we demonstrate a simple optimal portfolio investment problem in
   g(7.12), in which there is a safe bank asset and a risky    stock market    asset. we start with a unit wealth
and wish to obtain a wealth of 1.5 at time t = 40. if we place all our money in the bank, we will not be
able to reach this desired amount, so we must place some of our wealth at least in the risky asset. in the
beginning the stock market does poorly, and our wealth is correspondingly poor. the stock market picks
up su   ciently so that after around t = 20, we no longer need to take many risks and may place most of our
money in the bank, con   dent that we will reach our investment objective.

7.9 further topics

7.9.1 partially observable mdps

in a pomdp there are states that are not observed. this seemingly innocuous extension of the mdp case
can lead however to computational di   culties. let   s consider the situation in    g(7.13), and attempt to
compute the optimal expected utility based on the sequence of summations and maximisations. the sum
over the hidden variables couples all the decisions and observations, meaning that we no longer have a simple
chain structure for the remaining maximisations. for a pomdp of length t, this leads to an intractable
problem with complexity exponential in t. an alternative view is to recognise that all past decisions and
observations v1:t, d1:t   1, can be summarised in terms of a belief in the current latent state, p(ht|v1:t, d1:t   1).
this suggests that instead of having an actual state, as in the mdp case, we need to use a distribution
over states to represent our current knowledge. one can therefore write down an e   ective mdp albeit over
belief distributions, as opposed to    nite states. approximate techniques are required to solve the resulting
   in   nite    state mdps, and the reader is referred to more specialised texts for a study of approximation
procedures. see for example [162, 165].

7.9.2 id23

id23 deals mainly with time-independent id100. the added twist
is that the transition p(s(cid:48)
|s, d) (and possibly the utility) is unknown. initially an    agent    begins to explore
draft november 9, 2017

154

further topics

the set of states and utilities (rewards) associated with taking decisions. the set of accessible states and
their rewards populates as the agent traverses its environment. consider for example a maze problem with a
given start and goal state, though with an unknown maze structure. the task is to get from the start to the
goal in the minimum number of moves on the maze. clearly there is a balance required between curiosity
and acting to maximise the expected reward. if we are too curious (don   t take optimal decisions given the
currently available information about the maze structure) and continue exploring the possible maze routes,
this may be bad. on the other hand, if we don   t explore the possible maze states, we might never realise
that there is a much more optimal short-cut to follow than that based on our current knowledge. this
exploration-exploitation tradeo    is central to the di   culties of rl. see [284] for an extensive discussion of
id23.

from model based to model free learning

@@
@@

consider a mdp with state transitions p(xt+1|xt, dt) and policy p(dt|xt). for simplicity we consider utilities
u (dt|xt), can be
that depend only on the state xt. the expected utility of taking decision dt in state xt,
derived using a similar argument as in section(7.5.2), for a discount factor   .
analogous to equation (7.5.10),
we have (now including discounting)

u (dt|xt) =

p(xt+1|xt, dt)

u(xt+1) +    max

d

p(x

(cid:48)

(cid:48)
|xt+1, d)v(x

)

(cid:33)

(cid:88)

x(cid:48)

(cid:88)

xt+1

(cid:88)

xt+1

using the value recursion

v(xt+1) = u(xt+1) +    max

d

(cid:88)

x(cid:48)

(cid:48)

p(x

|xt+1, d)v(x

(cid:48)

)

we can write

u (dt|xt) =

(cid:88)

xt+1

p(xt+1|xt, dt)v(xt+1)

substituting this into equation (7.9.2) we have

v(xt+1) = u(xt+1) +    max

d

u (d|xt+1)

substituting equation (7.9.4) into equation (7.9.3) we obtain

u (dt|xt) =

p(xt+1|xt, dt)

u(xt+1) +    max

d

u (d|xt+1)

(cid:19)

(cid:32)

(cid:18)

if we know the model p(xt+1|xt, dt) we can solve equation (7.9.5) for u (d|x). given this solution, when
we are in state x, the optimal policy is to take the decision d = arg maxd u (d|x). in the case that we do
not wish to explicitly store or describe a model p(xt+1|xt, dt) we can use a sample from this transition to
approximate equation (7.9.5); if we are in state xt and take decision dt the environment returns for us a
sample xt+1. this gives the one-sample estimate to equation (7.9.5):

  u (dt|xt) = u(xt+1) +    max

d

this gives a highly stochastic update and to ensure convergence it is preferable to use (see below)

  u (d|xt+1)
(cid:18)

(cid:18)

  ut+1(dt|xt) = (1       t)   ut(dt|xt) +   t

u(xt+1) +    max

d

  ut(d|xt+1)

which can be written

  ut+1(dt|xt) =   ut(dt|xt) +   t

draft november 9, 2017

u(xt+1) +    max

d

  ut(d|xt+1)       ut(dt|xt)

(cid:19)

(cid:19)

(7.9.1)

(7.9.2)

(7.9.3)

(7.9.4)

(7.9.5)

(7.9.6)

(7.9.7)

(7.9.8)

155

(cid:80)

where the learning rate satis   es 0        < 1,(cid:80)

t   t =    ,(cid:80)

t   2

t <    . (for example   t = 1/t.) this gives a
procedure called id24 for updating the approximation to u based on samples from the environment5.
this is a simple and powerful scheme and as such is one of the most popular model-free methods in re-
inforcement learning. a complicating factor is that if we select a decision based on d = arg maxd   u (d|x)
this in   uences the sample that will be next drawn. nevertheless, under certain conditions (essentially all
decisions are repeatedly sampled for each state), the sample estimate   ut(d|x) converges to the exact u (d|x)
in the limit t         [312].

further topics

moving average estimator of the mean

in order to motivate the above id24 approach, we consider a way to estimate the average       
x xp(x)
of a distribution p(x) from a sequence of single samples x1, . . . , xt. based on the sequence, a naive approxi-
mation is to take a single sample

    t     xt

(7.9.9)

whilst this is an unbiased estimate, it is not consistent. as t increases, the approximation does not improve
towards the correct answer   . a better approximation is to consider the moving average. de   ning

    t    

1
t

(x1 + . . . + xt)

we have

    t+1 =

1

t + 1

(x1 + . . . + xt + xt+1)

from these we can easily derive

  mut+1 = (1       t)    t +   txt

(7.9.10)

(7.9.11)

(7.9.12)

where   t     1/(t + 1). this is an unbiased estimator of    and in the limit t         converges to the
exact mean   . this is the procedure that is used in id24 in which xt is replaced by the quantity
u(xt) +    maxd u (d|xt). that is, id24 uses a moving average to estimate the expectation.

bayesian id23

for a given set of environment data x (observed transitions and utilities) one aspect of the rl problem
can be considered as    nding the policy that maximises expected reward, given only prior belief about the
environment and observed decisions and states.
if we assume we know the utility function but not the
transition, we may write

u (  |x ) = (cid:104)u (  |  )(cid:105)p(  |x )

where    represents the environment state transition,

   = p(xt+1|xt, dt)

given a set of observed states and decisions,

p(  |x )     p(x|  )p(  )

(7.9.13)

(7.9.14)

(7.9.15)

where p(  ) is a prior on the transition. similar techniques to the em style training can be carried through
in this case as well[82, 299, 109]. rather than the policy being a function of the state and the environment
  , optimally one needs to consider a policy p(dt|xt, b(  )) as a function of the state and the belief in the
environment, b(  )     p(  |x ). this means that, for example, if the belief in the environment has high
id178, the agent can recognise this and explicitly carry out decisions/actions to explore the environment.
a further complication in rl is that the data collected x depends on the policy   . if we write t for an

5the standard notation in the id24 literature uses q(x, d) in place of u (d|x).

156

draft november 9, 2017

further topics

a

t

x

dx

ux

l

e

s

d

b

dh

uh

s = smoking

x = positive x-ray

d = dyspnea (shortness of breath)

e = either tuberculosis or lung cancer

t = tuberculosis

l = lung cancer

b = bronchitis

a = visited asia
dh = hospitalise?
dx = take x-ray?

figure 7.14: in   uence diagram for the    chest clinic    decision example.

   episode    in which policy   t is followed and data xt collected, then the utility of the policy    given all the
historical information is

u (  |  1:t,x1:t) = (cid:104)u (  |  )(cid:105)p(  |x1:t,  1:t)

(7.9.16)

depending on the prior on the environment, and also on how long each episode is, we will have di   erent
posteriors for the environment parameters. if we then set

  t+1 = argmax

  

u (  |  1:t,x1:t)

(7.9.17)

this a   ects the data we collect at the next episode xt+1. in this way, the trajectory of policies   1,   2, . . . can
be very di   erent depending on the episodes and priors.

7.10 summary

    one way to take decisions is to take that decision that maximises the expected utility of the decision.
    sequential decision problems can be modelled using id90. these are powerful but unwieldy in long

decision sequences.

    in   uence diagrams extend belief networks to the decision arena. e   cient id136 approaches carry over

to this case as well, including extensions using the strong junction tree formalism.

    the sequence in which information is revealed and decisions are taken is speci   ed in the in   uence diagram.

the optimal utility is not invariant to the corresponding partial-ordering.

    id100 correspond to a simple chain-like in   uence diagram, for which id136 is

straightforward, and corresponds to the classical bellman equations.

    id23 can be considered an extension of the markov decision framework when the model

of the environment in which the agent acts needs to be learned on the basis of experience.

in this chapter we discussed planning and control as an id136 problem with particular attention to
discrete variables. see example(28.2) for an application of approximate id136 to continuous control.

draft november 9, 2017

157

code

7.11 code

7.11.1 sum/max under a partial order

maxsumpot.m: generalised elimination operation according to a partial ordering
sumpotid.m: sum/max an id with id203 and decision potentials
demodecparty.m: demo of summing/maxing an id

7.11.2 junction trees for in   uence diagrams

in the code
there is no need to specify the information links provided that a partial ordering is given.
jtreeid.m no check is made that the partial ordering is consistent with the in   uence diagram.
in this
case, the    rst step of the junction tree formulation in section(7.4.2) is not required. also the moralisation
and removal of utility nodes is easily dealt with by de   ning utility potentials and including them in the
moralisation process.

the strong triangulation is found by a simple variable elimination scheme which seeks to eliminate a variable
with the least number of neighbours, provided that the variable may be eliminated according to the speci   ed
partial ordering. the junction tree is constructed based only on the elimination clique sequence c1, . . . ,cn .
obtained from the triangulation routine. the junction tree is then obtained by connecting a clique ci to
the    rst clique j > i that is connected to this clique. clique ci is then eliminated from the graph. in this
manner a junction tree of connected cliques is formed. we do not require the separators for the in   uence
diagram absorption since these can be computed and discarded on the    y.

note that the code only computes messages from the leaves to the root of the junction tree, which is
su   cient for taking decisions at the root. if one desires an optimal decision at a non-root, one would need
to absorb probabilities into a clique which contains the decision required. these extra forward id203
absorptions are required because information about any unobserved variables can be a   ected by decisions
and observations in the past. this extra forward id203 schedule is not given in the code and left as an
exercise for the interested reader.

jtreeid.m: junction tree for an in   uence diagram
absorptionid.m: absorption on an in   uence diagram
triangulateporder.m: triangulation based on a partial ordering
demodecphd.m: demo for utility of doing phd and startup

7.11.3 party-friend example

the code below implements the party-friend example in the text. to deal with the asymmetry the v isit
utility is zero if p arty is in state yes.
demodecpartyfriend.m: demo for party-friend

7.11.4 chest clinic with decisions

the table for the chest clinic decision network,    g(7.14) is taken from exercise(3.4), see [132, 73]. there is
a slight modi   cation however to the p(x|e) table. if an x-ray is taken, then information about x is available.
however, if the decision is not to take an x-ray no information about x is available. this is a form of
asymmetry. a straightforward approach in this case is to make dx a parent of the x variable and set the

158

draft november 9, 2017

exercises

distribution of x to be uninformative if dx = fa.

p(a = tr) = 0.01
p(s = tr) = 0.5
p(t = tr|a = tr) = 0.05
p(t = tr|a = fa) = 0.01
p(l = tr|s = tr) = 0.1
p(l = tr|s = fa) = 0.01
p(b = tr|s = tr) = 0.6
p(b = tr|s = fa) = 0.3
p(x = tr|e = tr, dx = tr) = 0.98 p(x = tr|e = fa, dx = tr) = 0.05
p(x = tr|e = fa, dx = fa) = 0.5
p(x = tr|e = tr, dx = fa) = 0.5
p(d = tr|e = tr, b = tr) = 0.9
p(d = tr|e = tr, b = fa) = 0.3
p(d = tr|e = fa, b = tr) = 0.2
p(d = tr|e = fa, b = fa) = 0.1

(7.11.1)

the two utilities are designed to re   ect the costs and bene   ts of taking an x-ray and hospitalising a patient:

dh = tr
t = tr
l = tr
180
dh = tr
t = tr
l = fa 120
dh = tr
t = fa l = tr
160
dh = tr
t = fa l = fa 15
dh = fa t = tr
l = tr
2
dh = fa t = tr
l = fa 4
dh = fa t = fa l = tr
0
dh = fa t = fa l = fa 40

(7.11.2)

t = tr
dx = tr
0
t = fa 1
dx = tr
dx = fa t = tr
10
dx = fa t = fa 10

(7.11.3)

we assume that we know whether or not the patient has been to asia, before deciding on taking an x-ray.
the partial ordering is then

a     dx     {d, x}     dh     {b, e, l, s, t}

the demo demodecasia.m produces the results:

utility table:
asia = yes takexray = yes 49.976202
asia = no takexray = yes 46.989441
48.433043
asia = yes takexray = no
asia = no takexray = no
47.460900

(7.11.4)

which shows that optimally one should take an x-ray only if the patient has been to asia.
demodecasia.m: junction tree in   uence diagram demo

7.11.5 id100

in demomdp.m we consider a simple two dimensional grid in which an    agent    can move to a grid square either
above, below, left, right of the current square, or stay in the current square. we de   ned goal states (grid
squares) that have high utility, with others having zero utility.
demomdpclean.m: demo of value and policy iteration for a simple mdp
mdpsolve.m: mdp solver using value or policy iteration
routines for e   cient mdp variational solvers are available from the book website. there is also code for
fast lagrange duality techniques, which are beyond the scope of our discussion here.

7.12 exercises

exercise 7.1. you play a game in which you have a id203 p of winning. if you win the game you gain
an amount   s and if you lose the game you lose an amount   s. show that the expected gain from playing
the game is   (2p     1)s.
exercise 7.2. it is suggested that the utility of money is based, not on the amount, but rather how much we
have relative to other people. assume a distribution p(i), i = 1, . . . , 10 of incomes using a histogram with 10

draft november 9, 2017

159

exercises

bins, each bin representing an income range. use a histogram to roughly re   ect the distribution of incomes
in society, namely that most incomes are around the average with few very wealthy and few extremely poor
people. now de   ne the utility of an income x as the chance that income x will be higher than a randomly
chosen income y (under the distribution you de   ned) and relate this to the cumulative distribution of p.
write a program to compute this id203 and plot the resulting utility as a function of income. now
repeat the coin tossing bet of section(7.1.1) so that if one wins the bet one   s new income will be placed in the
top histogram bin, whilst if one loses one   s new income is in the lowest bin. compare the optimal expected
utility decisions under the situations in which one   s original income is (i) average, and (ii) much higher
than average.

exercise 7.3.

derive a partial ordering for the id on the right, and
explain how this id di   ers from that of    g(7.5).

t est

oil

u2

u1

seismic

drill

exercise 7.4. this question follows closely demomdp.m, and represents a problem in which a pilot wishes
to land an airplane. the matrix u (x, y) in the    le airplane.mat contains the utilities of being in position
x, y and is a very crude model of a runway and taxiing area. the airspace is represented by an 18   15 grid
(gx = 18, gy = 15 in the notation employed in demomdp.m). the matrix u (8, 4) = 2 represents that position
(8, 4) is the desired parking bay of the airplane (the vertical height of the airplane is not taken in to account).
the positive values in u represent runway and areas where the airplane is allowed. zero utilities represent
neutral positions. the negative values represent unfavourable positions for the airplane. by examining the
matrix u you will see that the airplane should preferably not veer o    the runway, and also should avoid two
small villages close to the airport.

at each timestep the plane can perform one of the following actions stay up down left right:
for stay, the airplane stays in the same x, y position.
for up, the airplane moves to the x, y + 1 position.
for down, the airplane moves to the x, y     1 position.
for left, the airplane moves to the x     1, y position.
for right, the airplane moves to the x + 1, y position.

a move that takes the airplane out of the airspace is not allowed
, and the plane remains in its current
position x, y. for example, if we issue the right action, then we stay at x, y if x + 1, y is out of the airspace.

@@

1. the airplane begins in at point x = 1, y = 13. assuming that an action deterministically results in the
intended grid move,    nd the optimal xt, yt sequence for times t = 1, . . . , for the position of the aircraft.

2.

the pilot tells you that there is a fault with the airplane for the right action. provided x + 1, y is in
the airspace for current position x, y:

@@

if x, y + 1 is out of the airspace, then we go right to x + 1, y with id203 1.

if x, y +1 is in the airspace, we go right to x+1, y with id203 0.9 and up to x, y +1 with id203
0.1.

assuming again that the airplane begins at point x = 1, y = 13, return the
t = 1, . . . , the aircraft would ideally reach.

sequence of positions xt, yt,

@@

exercise 7.5.

160

draft november 9, 2017

exercises

the in   uence diagram depicted describes the    rst stage of a game. the decision
variable dom(d1) = {play, not play}, indicates the decision to either play the    rst
stage or not. if you decide to play, there is a cost c1(play) = c1, but no cost oth-
erwise, c1(no play) = 0. the variable x1 describes if you win or lose the game,
dom(x1) = {win, lose}, with probabilities:

p(x1 = win|d1 = play) = p1,
the utility of winning/losing is

p(x1 = win|d1 = no play) = 0

(7.12.1)

u1(x1 = win) = w1,

u1(x1 = lose) = 0

(7.12.2)

c1

d1

x1

u1

show that the expected utility gain of playing this game is

u (d1 = play) = p1w1     c1

(7.12.3)

exercise 7.6. exercise(7.5) above describes the    rst stage of a new two-stage game. if you win the    rst
stage x1 = win, you have to make a decision d2 as to whether or not play in the second stage dom(d2) =
{play, not play}. if you do not win the    rst stage, you cannot enter the second stage. if you decide to play
the second stage, you win with id203 p2:

p(x2 = win|x1 = win, d2 = play) = p2

if you decide not to play the second stage there is no chance to win:

p(x2 = win|x1 = win, d2 = not play) = 0

the cost of playing the second stage is

c2(d2 = play) = c2,

c2(d2 = no play) = 0

and the utility of winning/losing the second stage is

u2(x2 = win) = w2,

u2(x2 = lose) = 0

1. draw an in   uence diagram that describes this two-stage game.

(7.12.4)

(7.12.5)

(7.12.6)

(7.12.7)

2. a gambler needs to decide if he should even enter the    rst stage of this two-stage game. show that

based on taking the optimal future decision d2 the expected utility based on the    rst decision is:

(cid:26) p1(p2w2     c2) + p1w1     c1

p1w1     c1

u (d1 = play) =

if p2w2     c2     0
if p2w2     c2     0

(7.12.8)

exercise 7.7. you have   b in your bank account. you are asked if you would like to participate in a bet in
which, if you win, your bank account will become   w . however, if you lose, your bank account will contain
only   l. you win the bet with id203 pw.

1. assuming that the utility is given by the number of pounds in your bank account, write down a formula
for the expected utility of taking the bet, u (bet) and also the expected utility of not taking the bet,
u (no bet).

2. the above situation can be formulated di   erently. if you win the bet you gain   (w     b). if you lose
the bet you lose   (b     l). compute the expected amount of money you gain if you bet ugain(bet) and
if you don   t bet ugain(no bet).

3. show that u (bet)     u (no bet) = ugain(bet)     ugain(no bet).

draft november 9, 2017

161

exercise 7.8. consider an objective

(cid:88)

x

f (  ) =

u (x)p(x|  )

exercises

(7.12.9)

for a positive function u (x) and that our task is to maximise f with respect to   . an expectation-
maximisation style bounding approach (see section(11.2)) can be derived by de   ning the auxiliary distribution

  p(x|  ) =

u (x)p(x|  )

f (  )

so that by considering kl(q(x)|  p(x)) for some variational distribution q(x) we obtain the bound

log f (  )        (cid:104)log q(x)(cid:105)q(x) + (cid:104)log u (x)(cid:105)q(x) + (cid:104)log p(x|  )(cid:105)q(x)

the m-step states that the optimal q distribution is given by

q(x) =   p(x|  old)

(7.12.10)

(7.12.11)

(7.12.12)

at the e-step of the algorithm the new parameters   new are given by maximising the    energy    term

  new = argmax

  

(cid:104)log p(x|  )(cid:105)  p(x|  old)

show that for a deterministic distribution

p(x|  ) =    (x, f (  ))

the e-step fails, giving   new =   old.

exercise 7.9. consider an objective

(cid:88)

x

f (  ) =

u (x)p (x|  )

for a positive function u (x) and

p (x|  ) = (1      )   (x, f (  )) +  n(x), 0           1

(7.12.13)

(7.12.14)

(7.12.15)

(7.12.16)

and an arbitrary distribution n(x). our task is to maximise f with respect to   . as the previous exercise
showed, if we attempt an em algorithm in the limit of a deterministic model   = 0, then no-updating occurs
and the em algorithm fails to    nd    that optimises f0(  ).

1. show that

f (  ) = (1      )f0(  ) +  

and hence

(cid:88)

x

n(x)u (x)

f (  new)     f (  old) = (1      ) [f0(  new)     f0(  old)]

(7.12.17)

(7.12.18)

2. show that if for   > 0 we can    nd a   new such that f (  new) > f (  old), then necessarily f0(  new) >

f0(  old).

3. using this result, derive an em-style algorithm that guarantees to increase f (  ) (unless we are already

at an optimum) for   > 0 and therefore guarantees to increase f0(  ). hint: use

  p(x|  ) =

u (x)p (x|  )

f (  )

(7.12.19)

and consider kl(q(x)|  p(x)) for some variational distribution q(x).

162

draft november 9, 2017

exercises

exercise 7.10. the    le idjensen.mat contains id203 and utility tables for the in   uence diagram of
   g(7.7a). using brmltoolbox, write a program that returns the maximal expected utility for this id using a
strong junction tree approach, and check the result by explicit summation and maximisation. similarly, your
program should output the maximal expected utility for both states of d1, and check that the computation
using the strong junction tree agrees with the result from explicit summation and maximisation.

exercise 7.11. for a pomdp, explain the structure of the strong junction tree, and relate this to the
complexity of id136 in the pomdp.

exercise 7.12.

(i) de   ne a partial order for the id depicted. (ii) draw a (strong) junction
tree for this id.

i

b

f

u2

d

g

a

e

u1

c

h

exercise 7.13. exerciseinvest.m contains the parameters for a simple investment problem in which the
prices of two assets, a and b follow markovian updating, as in section(7.8.3). the transition matrices of
these are given, as is the end time t , initial wealth w1, and initial price movements  a
1, wealth and
investment states. write a function of the form

1,  b

[d1 val]=optdec(epsilona1,epsilonb1,desired,t,w1,pars)

where desired is the desired wealth level at time t . the end utility is de   ned by

(cid:26) 10000 wt     1.5w1

wt < 1.5w1

0

u(wt ) =

(7.12.20)

@@

++

using your routine, compute the optimal expected utility and decision at time 1.
(round wealth values to the
nearest possible discrete values given in exerciseinvest.m.) draw also an in   uence diagram that describes
this markov decision problem.

exercise 7.14.
tom writes two cheques, one of which has a value of twice the other. you can   t see the
value of either cheque and are asked to pick one. the one you select has a value of   100. you can cash this
cheque, or exchange it for the other cheque. your friend randy thinks that it doesn   t really matter what you
do since you have a 50% chance of having in your hand the higher value cheque. what do you think you
should do? what if you don   t even look at the value of the cheque you    rst select?

++

exercise 7.15.
you   re planning on going on a summer trip and are contemplating the best route to take.
you start at location a and want to get to location d. the graph of locations and (one-way) roads between
them is shown below

b

c

a

d

each segment has an associated road length cost. for example, cab is the cost of going from a to b. however,
we don   t know if the road from b to d is going to be open; historically, it is open with id203 p. whilst
we don   t know when we start out at a whether the road from b to d will be open, if we decide to go along the
route a     b, when we get to b we will know if the road b     d is open. if it   s open, we can decide whether to
go along b     d, or the alternative b     c     d. if b     d is not open, then we must take the route b     c     d.
1. using the above graph, the road costs cab, cbc, cbd, ccd, cad and id203 p, explicitly show how to
decide whether it is better (in terms of minimal expected total cost) to initially take the road a     b or
go directly a     d.

draft november 9, 2017

163

exercises

2. for the same graph as above, a friend suggests a way to decide which route to take. assuming cbd <
cbc + ccd, and that b     d is open, it   s clear that (provided you go from a to b) you would prefer then
to go from b to d directly. otherwise, if b     d is not open, you will go along b     c     d.
you look at the possible routes, namely a     d, a     b     d, a     b     c     d and calculate the expected
cost of each route. you should then select the route with the minimal expected cost and decide to go
from a to b if the route with the minimal cost includes going from a to b. is there any di   erence
between this approach and the one in question part (1) above?

exercise 7.16.

1. consider that, for any function f (x, y) of two variables x and y

++

max

x

f (x, y)     f (x, y)

show that(cid:88)

max

x

f (x, y)     max

x

y

(cid:88)

y

f (x, y)

2. for a    nite t , a markov decision process speci   es a distribution over a set of states s2:t , given actions

a1:t as follows:

p(s2:t|a1:t   1) =

t(cid:89)

t=2

p(st|st   1, at   1)

once an action at is taken, the new state st+1 will be revealed. given the initial state s1, the optimal
id203 that st is in state 1 is given by

(cid:88)

s2

(cid:88)

st   1

   

p

(s1)     max

a1

max

a2

. . .

max
at   1

p(s2:t   1, st = 1|a1:t   1, s1)

derive an o(t ) algorithm that will calculate p    (s1) and decide which action to take at timestep 1.

3. de   ne

      

p

(s1)     max
a1:t   1

p(st = 1|a1:t   1, s1)

and show that p    (s1)     p       (s1), giving also an intuitive explanation why this must be the case.

exercise 7.17.
at timestep 1, you start out at position s1 = 0. at each time step there are 3 actions you
can take: down, same, up. the down action means st+1 = st     1, same means st+1 = st and up means
st+1 = st + 1. your task is to hit one of two possible targets at time t = 50. one of the targets is at
position +25 and the other is at position    25. if you hit a target, you receive a reward of 1, otherwise 0.
unfortunately, the noise in the environment means that when you take an action this is only carried out with
id203 0.8, with id203 0.1 of each the other two possible actions being carried out instead. what   s
the optimal expected reward of taking the actions down, same, up at timestep 1?

++

164

draft november 9, 2017

part ii

learning in probabilistic models

165

introduction to part ii

in part ii we address how to learn a model from data. in particular we will discuss
learning a model as a form of id136 on an extended distribution, now taking into
account the parameters of the model.

learning a model or model parameters from data forces us to deal with uncertainty
since with only limited data we can never be certain which is the    correct    model. we
also address how the structure of a model, not just its parameters, can in principle be
learned.

in part ii we show how learning can be achieved under simplifying assumptions, such
as maximum likelihood that set parameters by those that would most likely reproduce
the observed data. we also discuss the problems that arise when, as is often the case,
there is missing data.

together with part i, part ii prepares the basic material required to embark on under-
standing models in machine learning, having the tools required to learn models from
data and subsequently query them to answer questions of interest.

draft november 9, 2017

167

maximum likelihood algorithms for learning in id114. the leaf nodes denote speci   c algorithms

described in part ii.

168

draft november 9, 2017

graphicalmodelundirectedcompletedatadecomposaid7nconstr.countingconstr.limitedcliquesipfexp.formisgradientnon-decomposablegradient(dif   cult)incompletedatadecomposableexp.formisgradientnondecomposableipf(dif   cult)gradient(dif   cult)gradientdirectedcompletedatacountingincompletedatavariationalemchapter 8

statistics for machine learning

in this chapter we discuss some classical distributions and their manipulations. in previous chapters we   ve
assumed that we know the distributions and have concentrated on the id136 problem.
in machine
learning we will typically not fully know the distributions and need to learn them from available data. this
means we need familiarity with standard distributions, for which the data will later be used to set the
parameters.

8.1 representing data

the numeric encoding of data can have a signi   cant e   ect on performance and an understanding of the
options for representing data is therefore of considerable importance. we brie   y outline three central
encodings below.

8.1.1 categorical

for categorical (or nominal) data, the observed value belongs to one of a number of classes, with no intrinsic
ordering, and can be represented simply by an integer. an example of a categorical variable would be the
description of the type of job that someone does, e.g. healthcare, education,    nancial services, transport,
homeworker, unemployed, engineering etc. which could be represented by the values 1, 2, . . . , 7. another
way to transform such data into numerical values would be to use 1-of-m encoding. for example, if there
are four kinds of jobs: soldier, sailor, tinker, spy, we could represent a soldier as (1,0,0,0), a sailor as (0,1,0,0),
a tinker as (0,0,1,0) and a spy as (0,0,0,1). in this encoding the distance between the vectors representing
two di   erent professions is constant. note that 1-of-m encoding induces dependencies in the profession
attributes since if one of the attributes is 1, the others must be zero.

8.1.2 ordinal

an ordinal variable consists of categories with an ordering or ranking of the categories, e.g. cold, cool, warm,
hot. in this case, to preserve the ordering, we could use say -1 for cold, 0 for cool, +1 for warm and +2 for
hot. this choice is somewhat arbitrary, and one should bear in mind that results may be dependent on the
numerical coding used.

8.1.3 numerical

numerical data takes on values that are real numbers, e.g. a temperature measured by a thermometer, or
the salary that someone earns.

169

8.2 distributions

distributions over discrete variables, section(1.1) have been the focus of much of the book up to this point.
here we discuss also distributions over continuous variables, for which the concepts of marginalisation and
conditioning carry over from the discrete case, simply on replacing summation over the discrete states with
integration over the continuous domain of the variable.

distributions

de   nition 8.1 (id203 density functions). for a continuous variable x, the id203 density p(x)
is de   ned such that

(cid:90)    

(cid:90) b

p(x)     0,

p(x)dx = 1,

      

p(a     x     b) =

p(x)dx

a

we will also refer to continuous id203 densities as distributions.

de   nition 8.2 (averages and expectation).

(cid:104)f (x)(cid:105)p(x)

denotes the average or expectation of f (x) with respect to the distribution p(x). a common alternative
notation is

e (f (x))

when the context is clear, one may drop the notational dependency on p(x). the notation

(cid:104)f (x)|y(cid:105)

(8.2.3)

(8.2.4)

is shorthand for the average of f (x) conditioned on knowing the state of variable y, i.e. the average of f (x)
with respect to the distribution p(x|y).
an advantage of the expectation notations is that they hold whether the distribution is over continuous or
discrete variables. in the discrete case

(cid:88)
(cid:90)    

x

(cid:104)f (x)(cid:105)    

f (x = x)p(x = x)

and for continuous variables,

(cid:104)f (x)(cid:105)    

f (x)p(x)dx

      

the reader might wonder what (cid:104)x(cid:105) means when x is discrete. for example, if dom(x) = {apple, orange, pear},
with associated probabilities p(x) for each of the states, what does (cid:104)x(cid:105) refer to? clearly, (cid:104)f (x)(cid:105) makes sense
if f (x = x) maps the state x to a numerical value. for example f (x = apple) = 1, f (x = orange) = 2,
f (x = pear) = 3 for which (cid:104)f (x)(cid:105) is meaningful. unless the states of the discrete variable are associated
with a numerical value, then (cid:104)x(cid:105) has no meaning.

result 8.1 (change of variables). for a univariate continuous random variable x with distribution p(x)
the transformation y = f (x), where f (x) is a monotonic function, has distribution

(cid:18) df

(cid:19)   1

dx

p(y) = p(x)

p(y) = p(x = f   1(y))

, x = f

   1(y)

(cid:12)(cid:12)(cid:12)(cid:12)det

(cid:18)    f

   x

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)   1

for multivariate x and bijection f (x), then y = f (x) has distribution

170

draft november 9, 2017

(8.2.1)

(8.2.2)

(8.2.5)

(8.2.6)

(8.2.7)

(8.2.8)

distributions

(cid:20)    f

(cid:21)

   x

ij

where the jacobian matrix has elements

=

   fi(x)

   xj

(8.2.9)

sometimes one needs to consider transformations between di   erent dimensions. for example, if z has lower
dimension than x, then one may introduce additional variables z(cid:48) to de   ne a new multivariate y = (z, z(cid:48))
with the same dimension as x. then one applies the above transformation to give the distribution on the
joint variables y, from which p(z) can be obtained by marginalisation.

de   nition 8.3 (moments). the kth moment of a distribution is given by the average of xk under the
distribution:

(cid:68)

xk(cid:69)

p(x)

for k = 1, we have the mean, typically denoted by   ,

       (cid:104)x(cid:105)

de   nition 8.4 (cumulative distribution function). for a univariate distribution p(x), the cdf is de   ned
as

cdf (y)     p(x     y) = (cid:104)i [x     y](cid:105)p(x)

for an unbounded domain, cdf (      ) = 0 and cdf (   ) = 1.

(8.2.12)

de   nition 8.5 (moment generating function). for a distribution p(x), we de   ne the moment generating
function g(t) as

g(t) =(cid:10)etx(cid:11)

p(x)

the usefulness of this is that by di   erentiating g(t), we    generate    the moments,

(cid:68)

xk(cid:69)

p(x)

dk
dtk g(t) =

lim
t   0

de   nition 8.6 (mode). the mode x    of a distribution p(x) is the state of x at which the distribution takes
its highest value, x    = arg maxx p(x). a distribution could have more than one mode (be multi-modal). a
widespread abuse of terminology is to refer to any isolated local maximum of p(x) to be a mode.

de   nition 8.7 (variance and correlation).

(cid:68)
(x     (cid:104)x(cid:105))2(cid:69)

  2    

p(x)

(8.2.15)

the variance measures the    spread    of a distribution around the mean. the square root of the variance,   
is called the standard deviation and is a natural length scale suggesting how far typical values drawn from
p(x) might be from the mean. the notation var(x) is also used to emphasise for which variable the variance
is computed. the reader may show that an equivalent expression is

(8.2.10)

(8.2.11)

(8.2.13)

(8.2.14)

(8.2.16)

171

(cid:10)x2(cid:11)

  2    

    (cid:104)x(cid:105)2

draft november 9, 2017

for a multivariate distribution the matrix with elements

  ij = (cid:104)(xi       i) (xj       j)(cid:105)

distributions

(8.2.17)

where   i = (cid:104)xi(cid:105) is called the covariance matrix . the diagonal entries of the covariance matrix contain the
variance of each variable. an equivalent expression is

the correlation matrix has elements

  ij = (cid:104)xixj(cid:105)     (cid:104)xi(cid:105)(cid:104)xj(cid:105)

(cid:28) (xi       i)

  i

  ij =

(xj       j)

  j

(cid:29)

(8.2.18)

(8.2.19)

where   i is the deviation of variable xi. the correlation is a normalised form of the covariance so that each
element is bounded    1       ij     1. the reader will note a resemblance of the correlation coe   cient and the
scalar product, equation (a.1.3). see also exercise(8.40).

for independent variables xi and xj, xi        xj the covariance   ij is zero. similarly independent variables
have zero correlation     they are    uncorrelated   . note however that the converse is not generally true     two
variables can be uncorrelated but dependent. a special case is for when xi and xj are gaussian distributed
for which independence is equivalent to being uncorrelated, see exercise(8.2).

de   nition 8.8 (skewness and kurtosis). the skewness is a measure of the asymmetry of a distribution:

where   2 is the variance of x with respect to p(x). a positive skewness means the distribution has a heavy
tail to the right. similarly, a negative skewness means the distribution has a heavy tail to the left.

the kurtosis is a measure of how peaked around the mean a distribution is:

(8.2.20)

(cid:68)

(x     (cid:104)x(cid:105))3(cid:69)

  3

p(x)

  1    

(cid:68)

(x     (cid:104)x(cid:105))4(cid:69)

  4

p(x)

  2    

    3

(8.2.21)

a distribution with positive kurtosis has more mass around its mean than would a gaussian with the same
mean and variance. these are also called super gaussian. similarly a negative kurtosis (sub gaussian)
distribution has less mass around its mean than the corresponding gaussian. the kurtosis is de   ned such
that a gaussian has zero kurtosis (which accounts for the -3 term in the de   nition).

de   nition 8.9 (delta function). for continuous x, we de   ne the dirac delta function

except at x0, where there is a spike. (cid:82)    

         (x     x0)dx = 1 and

which is zero everywhere

  (x     x0)

(cid:90)    

  (x     x0)f (x)dx = f (x0)

      

one can view the dirac delta function as an in   nitely narrow gaussian:   (x     x0) = lim     0 n
the kronecker delta,

(8.2.22)

@@

(8.2.23)

(cid:0)x x0,   2(cid:1).

  x,x0

(8.2.24)
is similarly zero everywhere, except for   x0,x0 = 1. the kronecker delta is equivalent to   x,x0 = i [x = x0].
we use the expression    (x, x0) to denote either the dirac or kronecker delta, depending on the context.

172

draft november 9, 2017

distributions

de   nition 8.10 (empirical distribution). for a set of datapoints x1, . . . , xn , which are states of a random
variable x, the empirical distribution has id203 mass distributed evenly over the datapoints, and zero
elsewhere.

for a discrete variable x the empirical distribution is, see    g(8.1),

p(x) =

1
n

i [x = xn]

where n is the number of datapoints.

for a continuous distribution we have

p(x) =

1
n

   (x     xn)

n(cid:88)

n=1

n(cid:88)

n=1

where    (x) is the dirac delta function.

the mean of the empirical distribution is given by the sample mean of the datapoints

n(cid:88)

n=1

xn

     =

1
n

similarly, the variance of the empirical distribution is given by the sample variance

    2 =

1
n

(xn         )2

n(cid:88)

n=1

n(cid:88)

n=1

    i =

1
n

xn
i

for vectors the sample mean vector has elements

and sample covariance matrix has elements

n(cid:88)

n=1

i         i)(cid:0)xn

(xn

j         j

(cid:1)

    ij =

1
n

(8.2.25)

(8.2.26)

(8.2.27)

(8.2.28)

(8.2.29)

(8.2.30)

8.2.1 the id181 kl(q|p)
the id181 kl(q|p) measures the    di   erence    between distributions q and p[72].

de   nition 8.11 (kl divergence). for two distributions q(x) and p(x)

kl(q|p)     (cid:104)log q(x)     log p(x)(cid:105)q(x)     0

(8.2.31)

1

2

3

4

figure 8.1: empirical distribution over a discrete variable with
4 states. the empirical samples consist of n samples at each
of states 1, 2, 4 and 2n samples at state 3 where n > 0. on
normalising this gives a distribution with values 0.2, 0.2, 0.4, 0.2
over the 4 states.

draft november 9, 2017

173

the kl divergence is     0
the kl divergence is widely used and it is therefore important to understand why the divergence is positive.

distributions

to see this, consider the following linear bound on the function log(x)

log(x)     x     1

(8.2.32)

as plotted in the    gure on the right. replacing x by p(x)/q(x) in the above bound

p(x)
q(x)     1     log

p(x)
q(x)

(8.2.33)

since probabilities are non-negative, we can multiply both sides by q(x) to obtain

we now integrate (or sum in the case of discrete variables) both sides. using(cid:82) p(x)dx = 1,(cid:82) q(x)dx = 1,

p(x)     q(x)     q(x) log p(x)     q(x) log q(x)

(8.2.34)

1     1     (cid:104)log p(x)     log q(x)(cid:105)q(x)

rearranging gives

(cid:104)log q(x)     log p(x)(cid:105)q(x)     kl(q|p)     0

the kl divergence is zero if and only if the two distributions are exactly the same.

(8.2.35)

(8.2.36)

(cid:68) p     1(x)

(cid:69)

q     1(x)
  (1       )

1    

d  (p|q)    

de   nition 8.12 (  -divergence). for two distributions q(x) and p(x) and real    the   -divergence is de   ned
as

p(x)

    0

(8.2.37)

the id181 kl(p|q) corresponds to d1(p|q) and kl(q|p) = d0(p|q), which is readily
veri   ed using l   h  opital   s rule.

8.2.2 id178 and information

for both discrete and continuous variables, the id178 is de   ned as

h(p)        (cid:104)log p(x)(cid:105)p(x)

(8.2.38)

for continuous variables, this is also called the di   erential id178, see also exercise(8.34). the id178 is
a measure of the uncertainty in a distribution. one way to see this is that

h(p) =    kl(p|u) + const.

(8.2.39)

where u is a uniform distribution. since kl(p|u)     0, the less like a uniform distribution p is, the smaller
will be the id178. or, vice versa, the more similar p is to a uniform distribution, the greater will be the
id178. since the uniform distribution contains the least information a priori about which state p(x) is
in, the id178 is therefore a measure of the a priori uncertainty in the state occupancy. for a discrete
distribution we can permute the state labels without changing the id178. for a discrete distribution the
id178 is positive, whereas the di   erential id178 can be negative.
the mutual information is a measure of dependence between (sets of) variables x and y, conditioned on
variables z.

174

draft november 9, 2017

1234   4   3   2   10123classical distributions

de   nition 8.13 (mutual information).

mi(x ;y|z)     (cid:104)kl(p(x ,y|z)|p(x|z)p(y|z))(cid:105)p(z)     0

(8.2.40)

if x       y|z is true, then mi(x ;y|z) is zero, and vice versa. when z =    , the average over p(z) is absent
and one writes mi(x ;y).

8.3 classical distributions

de   nition 8.14 (bernoulli distribution). the bernoulli distribution concerns a discrete binary variable x,
with dom(x) = {0, 1}. the states are not merely symbolic, but real values 0 and 1.

p(x = 1) =   

from normalisation, it follows that p(x = 0) = 1       . from this

(cid:104)x(cid:105) = 0    p(x = 0) + 1    p(x = 1) =   
the variance is given by var(x) =    (1       ).

(8.3.1)

(8.3.2)

de   nition 8.15 (categorical distribution). the categorical distribution generalises the bernoulli distribu-
tion to more than two (symbolic) states. for a discrete variable x, with symbolic states dom(x) = {1, . . . , c},

p(x = c) =   c,

  c = 1

(8.3.3)

the dirichlet is conjugate to the categorical distribution.

c

(cid:88)

de   nition 8.16 (binomial distribution). the binomial describes the distribution of a discrete two-state
variable x, with dom(x) = {1, 0} where the states are symbolic. the id203 that in n bernoulli trials
(independent samples), x1, . . . , xn there will be k    success    states 1 observed is

(cid:18)n

k

(cid:19)
  k (1       )n   k ,

i(cid:2)xi = 1(cid:3)

n(cid:88)

i=1

y    

p(y = k|  ) =

where(cid:0)n
(cid:1)

k

    n!/(k!(n     k)!) is the binomial coe   cient. the mean and variance are

(cid:104)y(cid:105) = n  ,

var(y) = n   (1       )

the beta distribution is the conjugate prior for the binomial distribution.

de   nition 8.17 (multinomial distribution). consider a multi-state variable x, with dom(x) = {1, . . . , k},
with corresponding state probabilities   1, . . . ,   k. we then draw n samples from this distribution. the
id203 of observing the state 1 y1 times, state 2 y2 times, . . . , state k yk times in the n samples is

k(cid:89)

  yi
i

n!

y1! . . . yk!

i=1

@@

p(y1, . . . , yk|  ) =

(cid:80)k

@@

where n =

i=1yi.

(cid:104)yi(cid:105) = n  i, var(yi) = n  i (1       i) ,

(cid:104)yiyj(cid:105)     (cid:104)yi(cid:105)(cid:104)yj(cid:105) =    n  i  j (i (cid:54)= j)

draft november 9, 2017

(8.3.4)

(8.3.5)

(8.3.6)

(8.3.7)

175

classical distributions

(a)

(b)

figure 8.2: (a): exponential distribution. (b): laplace (double exponential) distribution.

the dirichlet distribution is the conjugate prior for the multinomial distribution.

de   nition 8.18 (poisson distribution). the poisson distribution can be used to model situations in which
the expected number of events scales with the length of the interval within which the events can occur. if   
is the expected number of events per unit interval, then the distribution of the number of events x within
an interval t   is

p(x = k|  ) =

     t (  t)k ,
e

1
k!

k = 0, 1, 2, . . .

for a unit length interval (t = 1),

(cid:104)x(cid:105) =   , var(x) =   

(8.3.8)

(8.3.9)

the poisson distribution can be derived as a limiting case of a binomial distribution in which the success
id203 scales as    =   /n, in the limit n        .

de   nition 8.19 (uniform distribution). for a variable x, the distribution is uniform if p(x) = const. over
the domain of the variable.

de   nition 8.20 (exponential distribution). for x     0, see    g(8.2a),

     x

p(x|  )       e

one can show that for rate   

(cid:104)x(cid:105) =

1
  

,

var(x) =

1
  2

the alternative parameterisation b = 1/   is called the scale.

de   nition 8.21 (gamma distribution).

(cid:18) x

(cid:19)     1

gam (x|  ,   ) =

    (  )

  

1

    x
   ,

e

x     0,    > 0,    > 0

(cid:90)    

ta   1e

   tdt

  (a) =

0

176

(8.3.10)

(8.3.11)

(8.3.12)

(8.3.13)

draft november 9, 2017

   is called the shape parameter,    is the scale parameter and the gamma function is de   ned as

   101234500.511.5    =0.2  =0.5  =1  =1.5   50500.20.40.60.81    =0.2  =0.5  =1  =1.5classical distributions

(a)

(b)

figure 8.3: gamma distribution. (a): varying    for    xed   . (b): varying    for    xed   .

(cid:16)   

(cid:17)2

s

   =

the parameters are related to the mean and variance through

,

   =

s2
  

(8.3.14)

where    is the mean of the distribution and s is the standard deviation. the mode is given by (       1)   ,
for        1, see    g(8.3).
an alternative parameterisation uses the inverse scale

gamis (x|  ,   ) = gam (x|  , 1/  )     x     1e

     x

de   nition 8.22 (inverse gamma distribution).

invgam (x|  ,   ) =

    
   (  )

1
x  +1 e

     /x

this has mean   /(       1) for    > 1 and variance

  2

(     1)2(     2)

for    > 2.

de   nition 8.23 (beta distribution).

p(x|  ,   ) = b (x|  ,   ) =

1

b(  ,   )

x     1 (1     x)     1 , 0     x     1

where the beta function is de   ned as

b(  ,   ) =

  (  )  (  )
  (   +   )

(8.3.15)

(8.3.16)

(8.3.17)

(8.3.18)

(a)

(b)

figure 8.4: beta distribution. the parameters    and    can also be written in terms of the mean and
variance, leading to an alternative parameterisation, see exercise(8.16).

draft november 9, 2017

177

012345012345    =1   =0.2  =2   =0.2  =5   =0.2  =10   =0.2012345012345    =2   =0.1  =2   =0.5  =2   =1  =2   =200.20.40.60.8100.511.522.53    =0.1   =0.1  =1   =1  =2   =2  =5   =500.20.40.60.8100.511.522.53    =0.1   =2  =1   =2  =2   =2  =5   =2classical distributions

figure 8.5: top: 200 datapoints x1, . . . , x200 drawn from a gaussian distribution.
each vertical line denotes a datapoint at the corresponding x value on the hori-
zontal axis. middle: histogram using 10 equally spaced bins of the datapoints.
bottom: gaussian distribution n (x    = 5,    = 3) from which the datapoints
were drawn. in the limit of an in   nite amount of data, and limitingly small bin
size, the normalised histogram tends to the gaussian id203 density function.

and   (x) is the gamma function. note that the distribution can be    ipped by interchanging x for 1     x,
which is equivalent to interchanging    and   . see    g(8.4).

the mean and variance are given by

(cid:104)x(cid:105) =

  

   +   

,

var(x) =

    

(   +   )2 (   +    + 1)

de   nition 8.24 (laplace distribution).

    1
b |x     |

p(x|  )       e

for scale b

(cid:104)x(cid:105) =   ,

var(x) = 2b2

the laplace distribution is also known as the double exponential distribution,    g(8.2b).

de   nition 8.25 (univariate gaussian distribution).

p(x|  ,   2) = n

1

   2    2

e

   

2  2 (x     )2
    1

(cid:0)x   ,   2(cid:1)

(8.3.19)

(8.3.20)

(8.3.21)

(8.3.22)

where    is the mean of the distribution, and   2 the variance. this is also called the normal distribution.

(cid:68)

(x       )2(cid:69)

one can show that the parameters indeed correspond to

   = (cid:104)x(cid:105)n (x   ,  2) ,

  2 =

n (x   ,  2)

(8.3.23)

for    = 0 and    = 1, the gaussian is called the standard normal distribution. see    g(8.5) for a depiction of
the univariate gaussian and samples therefrom.

de   nition 8.26 (student   s t-distribution).

p(x|  ,   ,   ) = student (x|  ,   ,   ) =

  (   +1
2 )
  (   
2 )

(cid:34)

(cid:19) 1

2

(cid:18)   

    

1 +

   (x       )2

  

(cid:35)      +1

2

(8.3.24)

where    is the mean,    the degrees of freedom, and    scales the distribution. the variance is given by

var(x) =

  

   (       2)

, for    > 2

(8.3.25)

for            the distribution tends to a gaussian with mean    and variance 1/  . as    decreases the tails of
the distribution become fatter.

178

draft november 9, 2017

   5051015   50510150102030   505101500.10.2classical distributions

(a)

(b)

(c)

(d)

figure 8.6: dirichlet distribution with parameter (u1, u2, u3) displayed on the simplex x1, x2, x3     0, x1 +
(c):
x2 + x3 = 1. black denotes low id203 and white high id203. (a): (3, 3, 3) (b): (0.1, 1, 1).
(4, 3, 2). (d): (0.05, 0.05, 0.05).

the t-distribution can be derived from a scaled mixture

p(x|  , a, b) =

   =0 n

(cid:90)    
(cid:90)    

=

=

   =0
ba
  (a)

(cid:0)x   ,   
(cid:16)   
(cid:17) 1

2 e

2  

  (a + 1
2 )
   2  

      
2 (x     )2

   1(cid:1) gamis (  |a, b) d  
2 (x       )2(cid:17)a+ 1
(cid:16)

b + 1

bae

1

2

   b      a   1 1
  (a)

d  

(8.3.26)

(8.3.27)

(8.3.28)

this matches equation (8.3.24) on setting    = 2a and    = a/b.

de   nition 8.27 (dirichlet distribution). the dirichlet distribution is a distribution on id203 distri-

butions,    = (  1, . . . ,   q),   i     0,(cid:80)
(cid:33) q(cid:89)

(cid:32) q(cid:88)

1

p(  ) =

  

z(u)

  i     1

i=1

q=1

where

z(u) =

(cid:81)q
(cid:16)(cid:80)q

q=1   (uq)

  

q=1 uq

(cid:17)

i   i = 1:

uq   1
q

  

i [  q     0]

it is conventional to denote the distribution as

dirichlet (  |u)

(8.3.29)

(8.3.30)

(8.3.31)

the parameter u controls how strongly the mass of the distribution is pushed to the corners of the simplex.
setting uq = 1 for all q corresponds to a uniform distribution,    g(8.6). in the binary case q = 2, this is
equivalent to a beta distribution.

the product of two dirichlet distributions is another dirichlet distribution

dirichlet (  |u1) dirichlet (  |u2) = dirichlet (  |u1 + u2)

the marginal of a dirichlet is also dirichlet:

dirichlet (  |u) = dirichlet(cid:0)  \j|u\j

(cid:1)

(cid:90)

  j

draft november 9, 2017

(8.3.32)

(8.3.33)

179

00.5100.5100.51x3x2x100.5100.5100.51x3x2x100.5100.5100.51x3x2x100.5100.5100.51x3x2x1multivariate gaussian

(a)

(b)

(a): bivariate gaussian with mean (0, 0) and covariance [1, 0.5; 0.5, 1.75]. plotted on the
figure 8.7:
vertical axis is the id203 density value p(x). (b): id203 density contours for the same bivariate
gaussian. plotted are the unit eigenvectors scaled by the square root of their eigenvalues,      i.

the marginal of a single component   i is a beta distribution:

        i|ui,

(cid:88)

j(cid:54)=i

      

uj

p(  i) = b

(8.3.34)

8.4 multivariate gaussian

the multivariate gaussian plays a central role in data analysis and as such we discuss its properties in some
detail.

de   nition 8.28 (multivariate gaussian distribution).

p(x|  ,   ) = n (x   ,   )    

1(cid:112)

det (2    )

    1
2 (x     )t     1(x     )
e

(8.4.1)

where    is the mean vector of the distribution, and    the covariance matrix. the inverse covariance      1
is called the precision.

one may show

   = (cid:104)x(cid:105)n (x   ,  ) ,

   =

(cid:68)

(x       ) (x       )t(cid:69)

n (x   ,  )

(8.4.2)

note that det (  m) =   ddet (m), where m is a d    d matrix, which explains the dimension independent
notation in the normalisation constant of de   nition(8.28).

the moment representation uses    and    to parameterise the gaussian. the alternative canonical repre-
sentation

p(x|b, m, c) = ce

    1

2 xtmx+xtb

is related to the moment representation via

   = m   1,

   = m   1b,

1(cid:112)

det (2    )

1

2 btm   1b

= ce

(8.4.3)

(8.4.4)

180

draft november 9, 2017

   4   2024   4   202400.020.040.060.080.10.120.14   4   3   2   101234   4   3   2   101234  0.020.040.060.080.10.12multivariate gaussian

the multivariate gaussian is widely used and it is instructive to understand the geometric picture. this
can be achieved by viewing the distribution in a di   erent co-ordinate system. first we use the fact that
every real symmetric matrix d    d has an eigen-decomposition

   = e  et

(8.4.5)

where ete = i and    = diag (  1, . . . ,   d). in the case of a covariance matrix, all the eigenvalues   i are
positive. this means that one can use the transformation

y =       1

2 et (x       )

so that

(x       )t      1 (x       ) = (x       )t e     1et (x       ) = yty

(8.4.6)

(8.4.7)

under this transformation, the multivariate gaussian reduces to a product of d univariate zero-mean unit
variance gaussians (since the jacobian of the transformation is a constant). this means that we can view a
multivariate gaussian as a shifted, scaled and rotated version of a    standard    (zero mean, unit covariance)
gaussian in which the centre is given by the mean, the rotation by the eigenvectors, and the scaling by the
square root of the eigenvalues, as depicted in    g(8.7b). a gaussian with covariance    =   i for some scalar
   is an example of an isotropic meaning    same under rotation   . for any isotropic distribution, contours of
equal id203 are spherical around the origin.

(8.4.8)

(8.4.9)

result 8.2 (product of two gaussians). the product of two gaussians is another gaussian, with a multi-
plicative factor, exercise(8.35):

(cid:16)

(cid:112)

2 (  1       2)t s   1 (  1       2)
    1

det (2  s)

(cid:17)

n (x   1,   1)n (x   2,   2) = n (x   ,   )

exp

where s       1 +   2 and the mean and covariance are given by

   =   1s   1  2 +   2s   1  1

   =   1s   1  2

8.4.1 completing the square

(cid:18)

(cid:19)

exp

   

xtax + btx

1
2

a useful technique in manipulating gaussians is completing the square. for example, the expression

can be transformed as follows. first we complete the square:

1
2

hence

xtax     btx =
(cid:18)

@@

exp

   

xtax

+btx

from this one can derive

1
2

(cid:18)

(cid:90)

1
2

(cid:0)x     a   1b(cid:1)t
(cid:19)

   

a(cid:0)x     a   1b(cid:1)
(cid:0)x a   1b, a   1(cid:1)(cid:113)
(cid:113)
det(cid:0)2  a   1(cid:1)exp

bta   1b

1
2

det(cid:0)2  a   1(cid:1)exp
(cid:18) 1

bta   1b

2

= n

(cid:19)

exp

1
2

   

xtax + btx

dx =

draft november 9, 2017

(cid:19)

bta   1b

(cid:18) 1
(cid:19)

2

(8.4.10)

(8.4.11)

(8.4.12)

(8.4.13)

181

multivariate gaussian

result 8.3 (linear transform of a gaussian). let y be linearly related to x through

y = mx +   

where x         ,        n (  ,   ), and x     n (  x,   x). then the marginal p(y) =(cid:82)

(cid:16)

(cid:17)

p(y) = n

y m  x +   , m  xmt +   

(8.4.14)

x p(y|x)p(x) is a gaussian

(8.4.15)

result 8.4 (partitioned gaussian). consider a distribution n (z   ,   ) de   ned jointly over two vectors x
and y of potentially di   ering dimensions,

z =

(cid:19)
(cid:18) x
(cid:18)   x

y

  y

(cid:19)

   =

   =

(cid:18)   xx   xy

  yx   yy

(cid:19)

with corresponding mean and partitioned covariance

where   yx       t

xy. the marginal distribution is given by

p(x) = n (x   x,   xx)

and conditional

p(x|y) = n

(cid:0)x   x +   xy     1

yy

(cid:0)y       y

(cid:1),   xx       xy     1

yy   yx

(cid:1)

result 8.5 (gaussian average of a quadratic function).

(cid:68)

(cid:69)

xtax

n (x   ,  )

=   ta   + trace (a  )

(8.4.16)

(8.4.17)

(8.4.18)

(8.4.19)

(8.4.20)

(cid:16)

8.4.2 conditioning as system reversal
for a joint gaussian distribution p(x, y), consider the conditional p(x|y). the formula for this gaussian is
given in equation (8.4.19). an equivalent and useful way to write this result is to consider a    reversed    linear
system of the form
x =       ay +          ,
(cid:90)
(cid:16)

(8.4.21)
and show that the marginal over the    reverse    noise          is equivalent to conditioning. that is, for a gaussian

(cid:17)
                  ,        

where              n

(8.4.22)
for suitably de   ned       a,          ,         . to show this, we need to make the statistics of x under this linear system
match those given by the conditioning operation, (8.4.19). the mean and covariance of the linear system
equation (8.4.21) are given by

(cid:17)
x           ay             

(cid:17)
                  ,        

p(         ) = n

p(x|y) =

p(         ),

(cid:16)

  

  x =       ay +          ,

  xx =         .

we can make these match equation (8.4.19) by setting

      a =   xy     1
yy ,

         =   xx       xy     1

yy   yx,

         =   x       xy     1

yy   y

(8.4.23)

(8.4.24)

this means that we can write an explicit linear system of the form equation (8.4.21) where the parameters
are given in terms of the statistics of the original system. this is particularly useful in deriving results in
id136 with linear dynamical systems, section(24.3).

182

draft november 9, 2017

exponential family

8.4.3 whitening and centering

for a set of data x1, . . . , xn , with dim xn = d, we can transform this data to y1, . . . , yn with zero mean
using centering:

where the mean m of the data is given by

yn = xn     m
n(cid:88)

m =

xn

1
n

n=1

(8.4.25)

(8.4.26)

furthermore, we can transform to values z1, . . . , zn that have zero mean and unit covariance using whitening

(8.4.27)

(8.4.28)

(8.4.29)

(8.4.30)

where the covariance s of the data is given by

zn = s    1

2 (xn     m)
n(cid:88)

s =

1
n

n=1

(xn     m) (xn     m)t

y =(cid:2)y1, . . . , yn(cid:3)

usvt = y,

an equivalent approach is to compute the svd decomposition of the matrix of centered datapoints

then for the d    n matrix

z =    n diag (1/s1,1, . . . , 1/sd,d) uty

the columns of z =(cid:0)z1, . . . , zn(cid:1) have zero mean and unit covariance, see exercise(8.32).

result 8.6 (id178 of a gaussian). the di   erential id178 of a multivariate gaussian p(x) = n (x   ,   )
is

h(x)        (cid:104)log p(x)(cid:105)p(x) =

1
2

log det (2    ) +

d
2

where d = dim x. note that the id178 is independent of the mean   .

(8.4.31)

8.5 exponential family

a theoretically convenient class of distributions are the exponential family, which contains many standard
distributions, including the gaussian, gamma, poisson, dirichlet, wishart, multinomial.

de   nition 8.29 (exponential family). for a distribution on a (possibly multidimensional) variable x
(continuous or discrete) an exponential family model is of the form

p(x|  ) = h(x)exp

  i (  ) ti(x)       (  )

(8.5.1)

   are the parameters, ti(x) the test statistics, and    (  ) is the log partition function that ensures normali-
sation

  i (  ) ti(x)

.

(8.5.2)

(cid:32)(cid:88)

i

(cid:33)

(cid:33)

(cid:90)

(cid:32)(cid:88)

i

   (  ) = log

h(x)exp

x

(cid:16)

(cid:17)
  tt(x)        (  )

p(x|  ) = h(x)exp

draft november 9, 2017

one can always transform the parameters to the form    (  ) =    in which case the distribution is in canonical
form:

(8.5.3)

183

(cid:18)

for example the univariate gaussian can be written

1

   2    2

exp

1

   

2  2 (x       )2

= exp

1
2  2 x2 +

   

  
  2 x    

  2
2  2    

(cid:19)
de   ning t1(x) = x, t2(x) =    x2/2 and ,   1 =   ,   2 =   2, h(x) = 1, then

(cid:18)

  1
  2

(cid:18)   2

1
  2

1
2

(cid:19)

1
  2

  1(  ) =

,

  2(  ) =

,

  (  ) =

+

log 2    2

(8.5.5)

@@

learning distributions

(8.5.4)

@@

(cid:19)

.

log 2    2

1
2

note that the parameterisation is not necessarily unique     we can for example rescale the functions ti(x)
and inversely scale   i by the same amount to arrive at an equivalent representation.

8.5.1 conjugate priors

for an exponential family likelihood

p(x|  ) = h(x)exp

and prior with hyperparameters   ,   ,

(cid:16)

(cid:17)
  tt(x)        (  )
(cid:17)

  t            (  )

(cid:16)

p(  |  ,   )     exp

the posterior is

p(  |x,   ,   )     p(x|  )p(  |  ,   )     exp
= p(  |t(x) +   , 1 +   )

(cid:16)

(cid:17)
  t [t(x) +   ]     [   + 1]    (  )

(8.5.6)

(8.5.7)

(8.5.8)

(8.5.9)

so that the prior, equation (8.5.7), is conjugate for the exponential family likelihood equation (8.5.6); that
is, the posterior is of the same form as the prior, but with modi   ed hyperparameters. whilst the likelihood
is in the exponential family, the conjugate prior is not necessarily in the exponential family.

8.6 learning distributions

for a distribution p(x|  ), parameterised by   , and data x =(cid:8)x1, . . . , xn(cid:9), learning corresponds to inferring

the    that best explains the data x . there are various criteria for de   ning this:
bayesian methods in this one examines the posterior p(  |x )     p(x|  )p(  ). this gives rise to a distribu-

tion over   . the bayesian method itself says nothing about how to best summarise this posterior.

maximum a posteriori this is a summarisation of the posterior, that is

  m ap = argmax

  

p(  |x )

(8.6.1)

maximum likelihood under a    at prior, p(  ) = const., the map solution is equivalent to setting    to

that value that maximises the likelihood of observing the data

  m l = argmax

  

p(x|  )

(8.6.2)

moment matching based on an empirical estimate of a moment (say the mean),    is set such that the

moment (or moments) of the distribution matches the empirical moment.

pseudo likelihood for multivariate x = (x1, . . . , xn ), one sets the parameters based on

   = argmax

  

n=1

i=1

log p(xn

i |xn\i,   )

(8.6.3)

the pseudo-likelihood method is sometimes used when the full likelihood p(x|  ) is di   cult to compute.
draft november 9, 2017

184

n(cid:88)

d(cid:88)

learning distributions

in seeking the    best    single parameter   , we are often required to carry out a numerical optimisation. this
is not necessarily a trivial step, and considerable e   ort is often spent either in attempting to de   ne models
for which the resulting computational di   culties are minimal, or in    nding good approximations that    nd
useful optima of complex objective functions, see section(a.5).

in this book we focus on the bayesian methods and maximum likelihood. we    rst reiterate some of the
basic ground covered in section(1.3), in which the bayesian and maximum likelihood methods are related.

de   nition 8.30. prior, likelihood and posterior

for data x and variable   , bayes    rule tells us how to update our prior beliefs about the variable    in light
of the data to a posterior belief:

p(  )

p(x|  )

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124)(cid:123)(cid:122)(cid:125)
(cid:124)(cid:123)(cid:122)(cid:125)

p(x )

prior

likelihood

evidence

(cid:124) (cid:123)(cid:122) (cid:125)

p(  |x )

posterior

=

(8.6.4)

the evidence is also called the marginal likelihood . note that the term    evidence    is (rather unfortunately)
used for both the marginal likelihood of observations and the observations themselves.

the term    likelihood    is used for the id203 that a model generates observed data. more fully, if we
condition on the model m , we have

p(  |x , m ) =

p(x|  , m )p(  |m )

p(x|m )

where we see the role of the likelihood p(x|  , m ) and model likelihood p(x|m ).
the most probable a posteriori (map ) setting is that which maximises the posterior,

  m ap = argmax

  

p(  |x , m )

(8.6.5)

for a       at prior   , p(  |m ) being a constant, the map solution is equivalent to maximum likelihood namely
that    that maximises p(x|  , m ),

  m l = argmax

  

p(x|  , m )

(8.6.6)

de   nition 8.31 (conjugacy). if the posterior is of the same parametric form as the prior, then we call
the prior the conjugate distribution for the likelihood distribution. that is, for a prior on parameter   ,
with hyperparameter   , p(  |  ), the posterior given data d is the same form as the prior, but with updated
hyperparameters, p(  |d,   ) = p(  |  (cid:48)).

de   nition 8.32 (independent and identically distributed). for a variable x, and a set of i.i.d. observations,
x1, . . . , xn , conditioned on   , we assume there is no dependence between the observations

p(x1, . . . , xn|  ) =

p(xn|  )

(8.6.7)

n(cid:89)

n=1

for non-bayesian methods which return a single value for   , based on the data x , it is interesting to know
how    good    the procedure is. concepts that can help in this case are    bias    and    consistency   . the bias

draft november 9, 2017

185

properties of maximum likelihood

measures if the estimate of    is correct    on average   . the property of an estimator such that the parameter
   converges to the true model parameter   0 as the sequence of data increases is termed consistency.

distribution p(x|  ) we can use the data x to estimate the parameter    that was used to generate the data.
the estimator is a function of the data, which we write     (x ). for an unbiased (parameter) estimator

de   nition 8.33 (unbiased estimator). given data x = (cid:8)x1, . . . , xn(cid:9), formed from i.i.d. samples of a
(cid:68)    (x )
(cid:69)
(cid:68)    (x )
(cid:69)

more generally, one can consider any function of the distribution p(x), with scalar value   , for example the
mean    = (cid:104)x(cid:105)p(x). then     (x ) is an unbiased estimator of    with respect to the data distribution   p(x ) if

p(x|  )

(8.6.8)

  p(x )

=   .

=   

a classical example for estimator bias are those of the mean and variance. let

n(cid:88)

n=1

xn

    (x ) =

1
n

this is an unbiased estimator of the mean (cid:104)x(cid:105)p(x) since, for iid data

n(cid:88)

n=1

(cid:104)    (x )(cid:105)p(x ) =

1
n

(cid:104)xn(cid:105)p(xn) =

1
n

n (cid:104)x(cid:105)p(x) = (cid:104)x(cid:105)p(x)

on the other hand, consider the estimator of the variance,

n(cid:88)

n=1

    2(x ) =

1
n

(cid:10)    2(x )(cid:11)

p(x ) =

1
n

(xn         (x ))2
n(cid:88)

(cid:68)
(xn         (x ))2(cid:69)

n=1

=

n     1
n

  2

this is biased since (omitting a few lines of algebra)

(8.6.9)

(8.6.10)

(8.6.11)

(8.6.12)

8.7 properties of maximum likelihood

  (cid:0)  ,   m ap(cid:1), see de   nition(8.30) . in making such an approximation, potentially useful information concern-

a crude summary of the posterior is given by a distribution with all its mass in a single most likely state,

ing the reliability of the parameter estimate is lost. in contrast the full posterior re   ects our beliefs about
the range of possibilities and their associated credibilities.

the term    maximum likelihood    refers to the parameter    for which the observed data is most likely to be
generated by the model. one can motivate map from a decision theoretic perspective. if we assume a
utility that is zero for all but the correct   ,

u (  true,   ) = i [  true =   ]

then the expected utility of    is

(cid:88)

  true

u (  ) =

i [  true =   ] p(  true|x ) = p(  |x )

(8.7.1)

(8.7.2)

this means that the maximum utility decision is to return that    with the highest posterior value.

186

draft november 9, 2017

properties of maximum likelihood

when a       at    prior p(  ) = const. is used the map parameter assignment is equivalent to the maximum
likelihood setting

  m l = argmax

  

p(x|  )

since the logarithm is a strictly increasing function, then for a positive function f (  )

  opt = argmax

  

f (  )       opt = argmax

  

log f (  )

(8.7.3)

(8.7.4)

so that the map parameters can be found either by optimising the map objective or, equivalently, its
logarithm,

log p(  |x ) = log p(x|  ) + log p(  )     log p(x )

(8.7.5)

where the normalisation constant, p(x ), is not a function of   . the log likelihood is convenient since, under
the i.i.d. assumption, it is a summation of data terms,

log p(  |x ) =

log p(xn|  ) + log p(  )     log p(x )

(8.7.6)

(cid:88)

n

so that quantities such as derivatives of the log-likelihood w.r.t.    are straightforward to compute.

8.7.1 training assuming the correct model class
consider a dataset x = {xn, n = 1, . . . , n} generated from an underlying parametric model p(x|  0). our
interest is to    t a model p(x|  ) of the same form as the correct underlying model p(x|  0) and examine if,
in the limit of a large amount of data, the parameter    learned by maximum likelihood matches the correct
parameter   0. our derivation below is non-rigorous, but highlights the essence of the argument.

assuming the data is i.i.d., the scaled log likelihood is

l(  )   

1
n

log p(x|  ) =

1
n

log p(xn|  )

(8.7.7)

in the limit n        , the sample average can be replaced by an average with respect to the distribution
generating the data

(cid:104)log p(x|  )(cid:105)p(x|  0) =    kl(cid:0)p(x|  0)|p(x|  )(cid:1) +(cid:10)log p(x|  0)(cid:11)

l(  ) =n      

p(x|  0)

(8.7.8)

up to a negligible constant, this is the id181 between two distributions in x, just
with di   erent parameter settings. the    that maximises l(  ) is that which minimises the kullback-leibler
divergence, namely    =   0. in the limit of a large amount of data we can therefore, in principle, learn the
correct parameters (assuming we know the correct model class). that is, maximum likelihood is a consistent
estimator.

8.7.2 training when the assumed model is incorrect

we write q(x|  ) for the assumed model, and p(x|  ) for the correct generating model. repeating the above
calculations in the case of the assumed model being correct, we have that, in the limit of a large amount of
data, the scaled log likelihood is

l(  ) = (cid:104)log q(x|  )(cid:105)p(x|  ) =    kl(p(x|  )|q(x|  )) + (cid:104)log p(x|  )(cid:105)p(x|  )

(8.7.9)

since q and p are not of the same form, setting    to    does not necessarily minimise kl(p(x|  )|q(x|  )), and
therefore does not necessarily optimize l(  ).

draft november 9, 2017

187

n(cid:88)

n=1

learning a gaussian

8.7.3 maximum likelihood and the empirical distribution

given a dataset of discrete variables x =(cid:8)x1, . . . , xn(cid:9) we de   ne the empirical distribution as

n(cid:88)
(cid:89)

n=1

q(x) =

1
n

i [x = xn]

in the case that x is a vector of variables,

i [x = xn] =

i [xi = xn
i ]

i

the id181 between the empirical distribution q(x) and a distribution p(x) is

kl(q|p) = (cid:104)log q(x)(cid:105)q(x)     (cid:104)log p(x)(cid:105)q(x)

our interest is the functional dependence of kl(q|p) on p. since the entropic term (cid:104)log q(x)(cid:105)q(x) is indepen-
dent of p(x) we may consider this constant and focus on the second term alone. hence

(8.7.10)

(8.7.11)

(8.7.12)

log p(xn) + const.

(8.7.13)

kl(q|p) =    (cid:104)log p(x)(cid:105)q(x) + const. =    

we recognise(cid:80)n

n=1 log p(xn) as the log likelihood under the model p(x), assuming that the data is i.i.d. this
means that setting parameters by maximum likelihood is equivalent to setting parameters by minimising the
id181 between the empirical distribution and the parameterised distribution. in the
case that p(x) is unconstrained, the optimal choice is to set p(x) = q(x), namely the maximum likelihood
optimal distribution corresponds to the empirical distribution.

8.8 learning a gaussian

given the importance of the gaussian distribution, it is instructive to explicitly consider maximum likelihood
and bayesian methods for    tting a gaussian to data.

8.8.1 maximum likelihood training

given a set of training data x =(cid:8)x1, . . . , xn(cid:9), drawn from a gaussian n (x   ,   ) with unknown mean   

and covariance   , how can we    nd these parameters? assuming the data are drawn i.i.d. the log likelihood
is

log p(x|  ,   ) =    

1
2

(xn       )t      1 (xn       )    

n
2

log det (2    )

(8.8.1)

n(cid:88)

n=1

n(cid:88)

n=1

1
n

taking the partial derivative with respect to the vector    we obtain the vector derivative

l(  ,   )    

optimal   

n(cid:88)

n=1

n(cid:88)

n=1

n(cid:88)

xn

   =

1
n

188

     l(  ,   ) =

     1 (xn       )

equating to zero gives that at the optimum of the log likelihood,

n(cid:88)

n=1

     1xn = n        1

and therefore, optimally    is given by the sample mean

n=1

draft november 9, 2017

(8.8.2)

(8.8.3)

(8.8.4)

learning a gaussian

(a) prior

(b) posterior

figure 8.8: bayesian approach to inferring the mean and precision (inverse variance) of a gaussian based
(a): a gauss-gamma prior with   0 = 0,    = 2,    = 1,    = 1.
on n = 10 randomly drawn datapoints.
(b): gauss-gamma posterior conditional on the data. for comparison, the sample mean of the data is 1.87
and maximum likelihood optimal variance is 1.16 (computed using the n normalisation). the 10 datapoints
were drawn from a gaussian with mean 2 and variance 1. see demogaussbayes.m.

optimal   

the derivative of l with respect to the matrix    requires more work.
dependence on the covariance, and also parameterise using the inverse covariance,      1,

it is convenient to isolate the

                       1

n(cid:88)
(cid:124)

n=1

l =    

1
2

trace

(cid:125)
(xn       ) (xn       )t

(cid:123)(cid:122)

   m

                   +

log det(cid:0)2       1(cid:1)

n
2

using m = mt, we obtain

   

        1 l =    

1
2

m +

n
2

  

equating the derivative to the zero matrix and solving for    gives the sample covariance

n(cid:88)

n=1

   =

1
n

(xn       ) (xn       )t

equations (8.8.4) and (8.8.7) de   ne the maximum likelihood solution mean and covariance for training data
x . consistent with our previous results, in fact these equations simply set the parameters to their sample
statistics of the empirical distribution. that is, the mean is set to the sample mean of the data and the
covariance to the sample covariance.

for simplicity we here deal only with the univariate case. assuming i.i.d. data the likelihood is

8.8.2 bayesian id136 of the mean and variance

p(x|  ,   2) =

1

(2    2)n/2 exp

1
2  2

   

(xn       )2

(cid:32)

n(cid:88)

n=1

(cid:33)

for a bayesian treatment, we require the posterior of the parameters

p(  ,   2|x )     p(x|  ,   2)p(  ,   2) = p(x|  ,   2)p(  |  2)p(  2)

our aim is to    nd conjugate priors for the mean and variance. a convenient choice for a prior on the mean
   is that it is a gaussian centred on   0:

(cid:18)

1(cid:112)

p(  |  0,   2

0) =

exp

   

2    2
0

1
2  2
0

(  0       )2

(cid:19)

draft november 9, 2017

(8.8.5)

(8.8.6)

(8.8.7)

(8.8.8)

(8.8.9)

(8.8.10)

189

the posterior is then

p(  ,   2|x )    

1
  0

1
  n exp

(cid:32)

   

1
2  2
0

(  0       )2    

1
2  2

(cid:33)

(xn       )2

p(  2)

(cid:88)

n

it is convenient to write this in the form

learning a gaussian

(8.8.11)

(8.8.12)
since equation (8.8.11) has quadratic contributions in    in the exponent, the conditional posterior p(  |  2,x )
is gaussian. to identify this gaussian we multiply out the terms in the exponent to arrive at

p(  ,   2|x ) = p(  |  2,x )p(  2|x )
(cid:18)

1
2

(cid:0)a  2     2b   + c(cid:1)(cid:19)
(cid:80)

with

exp

   

a =

1
  2
0

+

n
  2 , b =

  0
  2
0

+

n xn
  2

, c =

  2
0
  2
0

+

(cid:88)

n

(xn)2
  2

using the identity

a  2     2b   + c = a

we can write

(cid:19)

(cid:18)

+

b2
a

c    

(cid:18)

      

(cid:32)

b
a

(cid:19)2
(cid:18)
(cid:123)(cid:122)

a

(cid:19)2(cid:33)
(cid:125)

(cid:18)

(cid:18)

exp

   

1
2

1
   a

(cid:124)

(cid:123)(cid:122)

b2
c    
a
p(  2|x )

(cid:19)(cid:19) 1

  0

p(  ,   2|x )        aexp

(cid:124)

b
a

1
2

      

   
p(  |x ,  2)

(8.8.13)

(8.8.14)

(8.8.15)

(8.8.16)

(cid:125)
1
  n p(  2)

we encounter a di   culty in attempting to    nd a conjugate prior for   2 because the term b2/a is not a simple
expression of   2. for this reason we constrain

(8.8.17)

(8.8.18)

(8.8.19)

(8.8.20)

0         2
  2

for some    xed hyperparameter   . de   ning the constants

+ n,   b =

  0
  

xn,   c =

  2
0
  

+

(xn)2

(cid:88)

n

(cid:88)

n

+

(cid:33)

  a =

1
  

we have

(cid:32)

b2
a

=

1
  2

c    

  b2
  a

  c    

using this expression in equation (8.8.16) we obtain

p(  2)

(cid:32)

(cid:32)

(cid:33)(cid:33)

   

exp

  b2
  a

  c    

1
2  2

(cid:0)  2(cid:1)   n/2
(cid:0)     0,     2(cid:1) invgam(cid:0)  2|  ,   (cid:1)
(cid:32)
(cid:32)
  2|   +

invgam

(cid:33)

  2
  a

  b
  a

  

,

p(  2|x )    

p(  ,   2) = n

p(  ,   2|x ) = n

the posterior is also gauss-inverse-gamma with

an inverse gamma distribution for the prior p(  2) is therefore conjugate. for a gauss-inverse-gamma
prior:

(cid:32)

  c    

(cid:33)(cid:33)

  b2
  a

n
2

,    +

1
2

(8.8.21)

(8.8.22)

190

draft november 9, 2017

(8.8.23)

(8.8.24)

(8.8.25)

(8.8.26)

(8.8.27)

(8.8.28)

learning a gaussian

8.8.3 gauss-gamma distribution

it is common to use a prior on the precision, de   ned as the inverse variance

1
  2

      

if we then use a gamma prior

p(  |  ,   ) = gam (  |  ,   ) =

1

      (  )

       1e

     /  

(cid:17)
  |   + n/2,     

the posterior will be

p(  |x ,   ,   ) = gam

where

1
    

=

1
  

+

1
2

(cid:32)

  b2
  a

  c    

(cid:16)
(cid:33)

the gauss-gamma prior distribution

p(  ,   |  0,   ,   ,   ) = n

(cid:0)     0,     
(cid:32)

   1(cid:1) gam (  |  ,   )
(cid:33)
(cid:16)

  b
  a

,

1
  a  

gam

(cid:17)
  |   + n/2,     

p(  ,   |x ,   0,   ,   ,   ) = n

  

is therefore the conjugate prior for a gaussian with unknown mean    and precision   . the posterior for
this prior is a gauss-gamma distribution with parameters

the marginal p(  |x ,   0,   ,   ,   ) is a student   s t-distribution. an example of a gauss-gamma prior/posterior
is given in    g(8.8). the maximum likelihood solution is recovered in the limit of a       at    prior   0 = 0,       
   ,    = 1/2,           , see exercise(8.23). the unbiased estimators for the mean and variance are given using
the prior   0 = 0,           ,    = 1,           , exercise(8.24).
for the multivariate case, the extension of these techniques uses a multivariate gaussian distribution for
the conjugate prior on the mean, and an inverse wishart distribution for the conjugate prior on the
covariance[137].

8.9 summary

    classical univariate distributions include the exponential, gamma, beta, gaussian and poisson.
    a classical distribution of distributions is the dirichlet distribution.
    multivariate distributions are often di   cult to deal with computationally. a special case is the multivariate
gaussian, for which marginals and normalisation constants can be computed in time cubic in the number
of variables in the model.

    a useful measure of the di   erence between distributions is the id181.
    bayes    rule enables us to achieve parameter learning by translating a priori parameter belief into a posterior

parameter belief based on observed data.

    bayes    rule itself says nothing about how best to summarise the posterior distribution.
    conjugate distributions are such that the prior and posterior are from the same distribution, just with

di   erent parameters.

    maximum likelihood corresponds to a simple summarisation of the posterior under a    at prior.

draft november 9, 2017

191

    provided that we are using the correct model class, maximum likelihood will learn the optimal parameters

in the limit of a large amount of data     otherwise there is no such guarantee.

exercises

8.10 code

demogaussbayes.m: bayesian    tting of a univariate gaussian
loggaussgamma.m: plotting routine for a gauss-gamma distribution

8.11 exercises

exercise 8.1. in a public lecture, the following phrase was uttered by a professor of experimental psy-
chology:    in a recent data survey, 90% of people claim to have above average intelligence, which is clearly
nonsense!   
[audience laughs]. is it theoretically possible for 90% of people to have above average intelli-
gence? if so, give an example, otherwise explain why not. what about above median intelligence?

exercise 8.2. consider the distribution de   ned on real variables x, y:

p(x, y)     (x2 + y2)2e

   x2   y2

,

dom(x) = dom(y) = {       . . .   }

(8.11.1)

show that (cid:104)x(cid:105) = (cid:104)y(cid:105) = 0. furthermore show that x and y are uncorrelated, (cid:104)xy(cid:105) = (cid:104)x(cid:105)(cid:104)y(cid:105). whilst x and y
are uncorrelated, show that they are nevertheless dependent.

exercise 8.3. for a variable x with dom(x) = {0, 1}, and p(x = 1) =   , show that in n independent draws
x1, . . . , xn from this distribution, the id203 of observing k states 1 is the binomial distribution

(cid:18)n

k

(cid:19)
  k (1       )n   k
(cid:90)    

    1

2 x2

dx

i =

e

      

(8.11.2)

(8.11.3)

(8.11.4)

exercise 8.4 (normalisation constant of a gaussian). the normalisation constant of a gaussian distribu-
tion is related to the integral

by considering

(cid:90)    

      

(cid:90)    

      

    1
e

2 x2

dx

    1
e

2 y2

dy =

(cid:90)    

(cid:90)    

      

      

    1

2 (x2+y2)dxdy

e

i 2 =

and transforming to polar coordinates,

x = r cos   , y = r sin   , dxdy     rdrd  , r = 0, . . . ,   ,    = 0, . . . , 2  

show that

1. i =    2  
    1
2  2 (x     )2

2. (cid:82)    

       e

dx =    2    2

exercise 8.5. for a univariate gaussian distribution, show that

(cid:68)

1.    = (cid:104)x(cid:105)n (x   ,  2)
2.   2 =

(x       )2(cid:69)

n (x   ,  2)

192

draft november 9, 2017

exercises

exercise 8.6. using

xtax = trace

(cid:16)

axxt(cid:17)

derive result(8.5).

(cid:68)

xk(cid:69)

p(x)

dk
dtk g(t) =

lim
t   0

(cid:18) df

(cid:19)   1

dx

p(y) = p(x)

p(y) = p(x = f   1(y))

by using the transformation

(cid:20)    f

(cid:21)

   x

ij

(cid:90)    

=

   fi(x)

   xj

i =

exp

      

z =       1

2 (x       )

det (2    )

(cid:112)
(cid:18) a b

(cid:19)

c d

m =

show that

i =

(cid:18) p q

(cid:19)

r s

m   1 =

draft november 9, 2017

exercise 8.7. show that the marginal of a dirichlet distribution is another dirichlet distribution:

(cid:90)

(cid:68)

(cid:1)

  j

dirichlet (  |u) = dirichlet(cid:0)  \j|u\j
xk(cid:69)

b(   + k,   )

=

=

b(  ,   )

exercise 8.8. for a beta distribution, show that

where we used   (x + 1) = x  (x).

(   + k     1)(   + k     2) . . . (  )

(   +    + k     1)(   +    + k) . . . (   +   )

(cid:10)etx(cid:11)

exercise 8.9. for the moment generating function g(t)    

p(x), show that

exercise 8.10 (change of variables). consider a one dimensional continuous random variable x with
corresponding p(x). for a variable y = f (x), where f (x) is a monotonic function, show that the distribution
of y is

more generally, for vector variables, and y = f (x), we have:

, x = f

   1(y)

(cid:12)(cid:12)(cid:12)(cid:12)det

(cid:18)    f

   x

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)   1

where the jacobian matrix has elements

exercise 8.11 (normalisation of a multivariate gaussian). consider

(cid:18)

1
2

   

(cid:19)
(x       )t      1 (x       )

dx

exercise 8.12. consider the partitioned matrix

for which we wish to    nd the inverse m   1. we assume that a is m   m and invertible, and d is n   n and

invertible. by de   nition, the partitioned inverse

(8.11.5)

(8.11.6)

(8.11.7)

(8.11.8)

(8.11.9)

(8.11.10)

(8.11.11)

(8.11.12)

(8.11.13)

(8.11.14)

(8.11.15)

(8.11.16)

193

must satisfy

(cid:18) a b

(cid:19)(cid:18) p q

(cid:19)

c d

r s

(cid:19)

(cid:18) im 0

0

in

=

exercises

(8.11.17)

where im is the m   m identity matrix, and 0 the zero matrix of the same dimension as d. using the above,
derive the results

p =(cid:0)a     bd   1c(cid:1)   1
r =    d   1c(cid:0)a     bd   1c(cid:1)   1 s =(cid:0)d     ca   1b(cid:1)   1

q =    a   1b(cid:0)d     ca   1b(cid:1)   1

(cid:0)x   ,   2(cid:1) the skewness and kurtosis are both

(8.11.18)

exercise 8.13. show that for gaussian distribution p(x) = n
zero.

exercise 8.14. consider a small interval of time   t and let the id203 of an event occurring in this
small interval be     t. derive a distribution that expresses the id203 of at least one event in an interval
from 0 to t.

exercise 8.15. consider a vector variable x = (x1, . . . , xn)t and set of functions de   ned on each component
of x,   i(xi). for example for x = (x1, x2)t we might have

  1(x1) =    |x1| ,   2(x2) =    x2

2

consider the distribution

(cid:16)

(cid:17)

p(x|  ) =

1
z

exp

  t  (x)

(8.11.19)

(8.11.20)

where   (x) is a vector function with ith component   i(xi), and    is a parameter vector. each component is
tractably integrable in the sense that

(cid:90)    

exp (  i  i(xi))dxi

      

can be computed either analytically or to an acceptable numerical accuracy. show that

1. xi       xj.
2. the normalisation constant z can be tractably computed.

3. consider the transformation

x = my

(8.11.21)

(8.11.22)

for an invertible matrix m. show that the distribution p(y|m,   ) is tractable (its normalisation con-
stant is known), and that, in general, yi(cid:62)(cid:62)yj. explain the signi   cance of this in deriving tractable
multivariate distributions.

exercise 8.16. show that we may reparameterise the beta distribution, de   nition(8.23) by writing the
parameters    and    as functions of the mean m and variance s using

   =     ,

   =

1

1 +   

(cid:18)

       m/(1     m)

(cid:19)

  

s (1 +   )2     1

exercise 8.17. consider the function

f (   +   ,   ,   )         +     1 (1       )     1

194

(8.11.23)

(8.11.24)

(8.11.25)

draft november 9, 2017

exercises

show that

and hence that

(cid:90)

   
     

f (   +   ,   ,   ) =        1 (1       )     1 log   

lim
     0

(cid:90)

       1 (1       )     1 log   d   = lim

     0

   
     

f (   +   ,   ,   )d   =

(8.11.26)

(cid:90)

   
     

f (  ,   ,   )d  

(8.11.27)

using this result, show therefore that

(cid:104)log   (cid:105)b(  |  ,  ) =

   
     

log b(  ,   )

where b(  ,   ) is the beta function. show additionally that

(cid:104)log (1       )(cid:105)b(  |  ,  ) =

   
     

log b(  ,   )

using the fact that

b(  ,   ) =

  (  )  (  )
  (   +   )

where   (x) is the gamma function, relate the above averages to the digamma function, de   ned as

  (x) =

d
dx

log   (x)

(8.11.28)

(8.11.29)

(8.11.30)

(8.11.31)

exercise 8.18. using a similar    generating function    approach as in exercise(8.17), explain how to compute

(cid:104)log   i(cid:105)dirichlet(  |u)

f (x) =

(cid:90)    

exercise 8.19. consider the function

(cid:33)(cid:89)
(cid:27)
show that the laplace transform   f (s)    

(cid:32) n(cid:88)
(cid:26)(cid:90)    

  i     x

i=1

  

0

i

i

  ui   1

   s  i  ui   1

e

i

d  i

=

  f (s) =

0

i=1

n(cid:89)
(cid:81)n
   ((cid:80)

(cid:80)

i ui

i=1    (ui)
i ui)

x

f (x) =

(cid:81)n
   ((cid:80)

i=1    (ui)
i ui)

d  1 . . . d  n

(cid:82)    
0 e   sxf (x)dx is
(cid:80)

n(cid:89)

   (ui)

1
i ui

s

i=1

by using that the inverse laplace transform of 1/s1+q is xq/  (1 + q), show that

exercise 8.20. derive the formula for the di   erential id178 of a multi-variate gaussian.

   

exercise 8.21. show that for a gamma distribution gam (x|  ,   ) the mode is given by

x

= (       1)   
provided that        1.
exercise 8.22. consider a distribution p(x|  ) and a distribution p(x|   +   ) for small   .
draft november 9, 2017

hence show that the normalisation constant of a dirichlet distribution with parameters u is given by

(8.11.32)

(8.11.33)

(8.11.34)

(8.11.35)

(8.11.36)

(8.11.37)

195

exercises

(8.11.38)

(8.11.39)

(8.11.40)

(8.11.41)

(8.11.42)

(8.11.44)

2. more generally for a distribution parameterised by a vector with elements   i +   i, show that a small

change in the parameter results in

1. take the taylor expansion of
kl(p(x|  )|p(x|   +   ))

(cid:28)    2

(cid:29)

     2 log p(x|  )

p(x|  )

for small    and show that this is equal to

  2
2

   

(cid:88)

i,j

  i  j
2

fij

(cid:28)    2
(cid:28)    

where the fisher information matrix is de   ned as

fij =    

     i     j

log p(x|  )

p(x|  )

(cid:29)

fij =

log p(x|  )

     i

log p(x|  )

exercise 8.23. consider the joint prior distribution

(cid:29)

   
     j

(cid:0)     0,     

p(x|  )

   1(cid:1) gam (  |  ,   )
(cid:88)

(xn          )2

n

3. show that the fisher information matrix is positive semide   nite by expressing it equivalently as

p(  ,   |  0,   ,   ,   ) = n

(8.11.43)
show that for   0 = 0,           ,           , then the prior distribution becomes       at    (independent of    and   )
for    = 1/2. show that for these settings the mean and variance that jointly maximise the posterior equation
(8.8.28) are given by the standard maximum likelihood settings

(cid:88)

n

      =

1
n

xn,

  2    =

1
n

exercise 8.24. show that for equation (8.8.28) in the limit   0 = 0,           ,    = 1,           , the jointly
optimal mean and variance obtained from

p(  ,   |x ,   0,  ,   ,   )

argmax

  ,  

is given by

      =

1
n

(cid:88)

n

xn,

  2    =

1

n + 1

(cid:88)

n

(xn          )2

(8.11.45)

(8.11.46)

where   2    = 1/     . note that these correspond to the standard    unbiased    estimators of the mean and variance.
exercise 8.25. for the gauss-gamma posterior p(  ,   |  0,   ,   ,x ) given in equation (8.8.28) compute the
marginal posterior p(  |  0,   ,   ,x ). what is the mean of this distribution?
exercise 8.26. this exercise concerns the derivation of equation (8.4.15).

1. by considering

(cid:90)
(cid:90)

p(y) =

   

show that

p(y|x)p(x)dx

(cid:18)

exp

   

1
2

(y     mx       )t      1 (y     mx       )    

1
2

(cid:18)

(cid:19)(cid:90)

(cid:18)

1
2

   

exp

yt     1y

1
2

p(y)     exp

   

xtax + xt (by + c)

x (x       x)

(x       x)t      1
(cid:19)

(cid:19)

(8.11.47)

dx

(8.11.48)

(8.11.49)

for suitably de   ned a, b, c.

196

draft november 9, 2017

(8.11.51)

exercises

2. using equation (8.4.13), show that

p(y)     exp

   

1
2

yt     1y    

1
2

(cid:18)

(cid:19)

(by + c)t a   1 (by + c)

3. this establishes that p(y) is gaussian. we now need to    nd the mean and covariance of this gaussian.

we can do this by the lengthly process of completing the square. alternatively, we can appeal to

(cid:104)y(cid:105) = m(cid:104)x(cid:105) + (cid:104)  (cid:105) = m  x +   

by considering

(cid:68)

(y     (cid:104)y(cid:105)) (y     (cid:104)y(cid:105))t(cid:69)

=

(cid:68)
(mx +        m  x       ) (mx +        m  x       )t(cid:69)

and the independence of x and   , derive the formula for the covariance of p(y).

(8.11.50)

exercise 8.27. consider the multivariate gaussian distribution p(x)     n (x   ,   ) on the vector x with
components x1, . . . , xn:

p(x) =

    1
2 (x     )t     1(x     )
e

det (2    )

1(cid:112)
(cid:1) and p(yi|x)     n
(cid:0)x 0,   2

calculate p(xi|x1, . . . , xi   1, xi+1, . . . , xn). hint: make use of equation (8.4.19).
exercise 8.28. observations y0, . . . , yn   1 are noisy i.i.d. measurements of an underlying variable x with
p(x)     n
with mean

(cid:0)yi x,   2(cid:1) for i = 0, . . . , n     1. show that p(x|y0, . . . , yn   1) is gaussian

0

   =

n  2
0
0 +   2 y

n  2

where y = (y0 + y1 + . . . + yn   1)/n and variance   2

n such that

1
  2
n

=

n
  2 +

1
  2
0

.

(8.11.52)

(8.11.53)

exercise 8.29. consider a set of data x = x1, . . . , xn where each xn is independently drawn from a
gaussian with known mean    and unknown variance   2. assume a gamma distribution prior on    = 1/  2,

p(   ) = gamis (  |a, b)

1. show that the posterior distribution is

(cid:32)

p(  |x ) = gamis

  |a +

n
2

, b +

1
2

2. show that the distribution for x is

(cid:90)

n(cid:88)

n=1

(cid:33)

(8.11.54)

(8.11.55)

(8.11.56)

(cid:48)(cid:19)

a(cid:48)
b(cid:48) ,    = 2a

(xn       )2
(cid:18)

x|  ,    =

p(x|x ) =
where a(cid:48) = a + 1

(cid:80)n
p(x|   )p(  |x )d   = student
n=1 (xn       )2.

2 n , b(cid:48) = b + 1

2

exercise 8.30. the poisson distribution is a discrete distribution on the non-negative integers, with

p(x) =

e       x
x!

x = 0, 1, 2, . . .

(8.11.57)

you are given a sample of n observations x1, . . . , xn independently drawn from this distribution. determine
the maximum likelihood estimator of the poisson parameter   .

draft november 9, 2017

197

n(cid:88)

i=1

h =    

(cid:88)

i

exercise 8.31. for a gaussian mixture model

p(x) =

pin (x   i,   i) ,

pi > 0,

show that p(x) has mean

(cid:104)x(cid:105) =

(cid:88)
(cid:16)
and covariance(cid:88)

i

pi  i

pi

  i +   i  t
i

i

(cid:17)

(cid:88)

i

   

(cid:88)

j

pi  i

pj  t
j

(cid:88)

i

pi = 1

exercises

(8.11.58)

(8.11.59)

(8.11.60)

exercise 8.32. show that for the whitened data matrix, given in equation (8.4.30), zzt = n i.

exercise 8.33. consider a uniform distribution pi = 1/n de   ned on states i = 1, . . . , n . show that the
id178 of this distribution is

pi log pi = log n

(8.11.61)

and that therefore as the number of states n increases to in   nity, the id178 diverges to in   nity.

exercise 8.34. consider a continuous distribution p(x), x     [0, 1]. we can form a discrete approximation
with probabilities pi to this continuous distribution by identifying a continuous value i/n for each state
i = 1, . . . , n . with this

(cid:80)

pi =

p(i/n )
i p(i/n )

1(cid:80)

(cid:80)

(cid:88)

show that the id178 h =    

i pi log pi is given by

h =    

i p(i/n )

p(i/n ) log p(i/n ) + log

i

i

(8.11.62)

p(i/n )

(8.11.63)

(cid:88)

a discrete approximation of this integral into bins of size 1/n gives

since for a continuous distribution

(cid:90) 1

p(x)dx = 1

0

n(cid:88)

i=1

1
n

p(i/n ) = 1

(cid:90) 1

hence show that for large n ,

h        

0

p(x) log p(x)dx + const.

(8.11.64)

(8.11.65)

(8.11.66)

where the constant tends to in   nity as n        . note that this result says that as a continuous distribu-
tion has essentially an in   nite number of states, the amount of uncertainty in the distribution is in   nite
(alternatively, we would need an in   nite number of bits to specify a continuous value). this motivates the
de   nition of the di   erential id178, which neglects the in   nite constant of the limiting case of the discrete
id178.

exercise 8.35. consider two multivariate gaussians n (x   1,   1) and n (x   2,   2).
198

draft november 9, 2017

exercises

2

1
2

1 +      1

   
2. de   ning a =      1

xt(cid:0)     1
(cid:0)x     a   1b(cid:1)t

(cid:1) x+xt(cid:0)     1
a(cid:0)x     a   1b(cid:1)+

1 +      1

   

1. show that the log product of the two gaussians is given by

(cid:16)

(cid:1)

1
2

   

1   1 +      1

2   2

1      1
  t

1   1 +   t

2      1

2   2

(cid:17)

1
2

   

log det (2    1) det (2    2)

2 and b =      1

1   1 +      1
(cid:16)

2   2 we can write the above as

(cid:17)

1
2
writing    = a   1 and    = a   1b show that the product of two gaussians is a gaussian with covariance

log det (2    1) det (2    2)

1   1 +   t

bta   1b   

1      1
  t

2      1

2   2

   

1
2

1
2

1
2

   =   1 (  1 +   2)

   1   2

mean

   =   1 (  1 +   2)

and log prefactor

1
2

bta   1b    

1
2

(cid:16)

   1   2 +   2 (  1 +   2)
   1   1
(cid:17)

1      1
  t

1   1 +   t

2      1

2   2

1
2

   

log det (2    1) det (2    2) +

(8.11.67)

(8.11.68)

1
2

log det (2    )

3. show that this can be written as

n (x   1,   1)n (x   2,   2) = n (x   ,   )

(cid:16)

exp

where s =   1 +   2.

exercise 8.36. show that

(cid:17)
2 (  1       2)t s   1 (  1       2)
    1

(cid:112)

det (2  s)

   
      (cid:104)log p(x|  )(cid:105)p(x|  0) |  =  0 = 0

exercise 8.37. using(cid:10)f 2(x)(cid:11)

(cid:28) p(x)

(cid:29)

q(x)

p(x)     1

    (cid:104)f (x)(cid:105)2     0, and a suitably chosen function f (x), show that

(8.11.69)

(8.11.70)

(8.11.71)

for distributions q(x) and p(x). this is related to the    = 2 divergence.

exercise 8.38. show that for any   , d  (p|p) = 0 and that d  (p|q)     0 for any distributions p(x), q(x).
exercise 8.39. show that for two d dimensional gaussians,

2kl(n (x   1,   1)|n (x   2,   2)) = trace(cid:0)     1

(cid:1) + (  1       2)t      1

2

(  1       2) + log det(cid:0)  2     1

1

(cid:1)

    d
exercise 8.40. for data pairs (xn, yn), n = 1, . . . , n , the correlation can be considered a measure of the
extent to which a linear relationship holds between x and y. for simplicity, we consider that both x and y
have zero mean, and wish to consider the validity of the linear relation

2   1

a measure of the discrepancy of this linear assumption is given by

x =   y

n(cid:88)

n=1

e(  )    

(xn       yn)2

draft november 9, 2017

(8.11.72)

(8.11.73)

199

1. show that the    that minimises e(  ) is given by

where

   

  

=

c
  2
y

(cid:88)

n

c =

1
n

xnyn,

  2
y =

1
n

(cid:88)

n

(yn)2

2. a measure of the    linearity    between x and y is then

e(     )
e(0)

exercises

(8.11.74)

(8.11.75)

(8.11.76)

(cid:115)

which is 1 when there is no linear relation (   = 0) and 0 when there is a perfect linear relation (since
then e(     ) = 0). show that the correlation coe   cient, de   nition(8.7), is given by

   =

1    

e(     )
e(0)

(8.11.77)

3. de   ning the vectors x = (x1, . . . , xn )t, y = (y1, . . . , yn )t, show that the correlation coe   cient is the

cosine of the angle between x and y,

   =

xty
|x||y|

and hence show that    1            1.

4. show that for the more general relation

x =   y +   

for constant o   set   , setting    to minimise

n(cid:88)

n=1

e(  ,   )    

(xn       yn       )2

(8.11.78)

(8.11.79)

(8.11.80)

has the e   ect of simply replacing xn by xn       x and yn by yn       y, where   x and   y are the mean of the x
and y data respectively.

exercise 8.41. for variables x, y, and z = x + y, show that the correlation coe   cients are related by
  x,z       x,y. with reference to the correlation coe   cient as the angle between two vectors, explain why
  x,z       x,y is geometrically obvious.
exercise 8.42. consider a    boltzman machine    distribution on binary variables xi     {0, 1}, i = 1, . . . , d

p(x|w) =

1

zp(w)

exp

xtwx

(cid:17)

(cid:16)

(cid:16)

(cid:17)

xtux

q(x|u) =

1

zq(u)

exp

1. show that

and that we wish to    t another distribution q of the same form to p

argmin

u

kl(p|q) = argmax

u

trace (uc)     log zq(u),

cij     (cid:104)xixj(cid:105)p

2. hence show that knowing the    cross moment    matrix c of p is su   cient to fully specify p.

200

draft november 9, 2017

(8.11.81)

(8.11.82)

(8.11.83)

exercises

3. generalise the above result to all models in the exponential family.

++

exercise 8.43. consider a distribution n (z   ,   ) de   ned jointly over two vectors x and y of potentially
di   ering dimensions,

z =

(cid:19)
(cid:18) x
(cid:18)   x

y

(cid:19)

   =

  y
where   yx       t
xy.

with corresponding mean and partitioned covariance

(cid:18)   xx   xy

  yx   yy

(cid:19)

   =

1. by considering p(x|y)     p(x, y), show that

(cid:1)(cid:111)

1
2

(cid:110)
(x       x)t p (x       x) + 2 (x       x)t q(cid:0)y       y
(cid:18) p q
(cid:19)   1
(cid:110)(cid:0)x       x + p   1q(cid:0)y       y

(cid:1)(cid:1)t

qt s

(cid:19)

1
2

=

p(cid:0)x       x + p   1q(cid:0)y       y

p(x|y)     exp   

where(cid:18)   xx   xy

  yx   yy

p(x|y)     exp   

2. by completing the square, show that

(cid:0)y       y

(cid:1),   xx       xy     1

yy   yx

(cid:1)

exercise 8.44. as for the question above, consider the joint gaussian distribution p(z). consider

3. using partitioned matrix inversion, show that

  yxp +   yyqt = 0

and that therefore

show also that

yy

p   1q =      xy     1
p =(cid:0)  xx       xy     1

4. hence show that

yy   yx

(cid:1)   1
(cid:0)x   x +   xy     1
(cid:90)

yy

p(x|y) = n

(cid:90)

p(x) =

p(x, y)dy =

p(z)dy

1. show that

p(x)     exp   

(cid:110)
(x       x)t p (x       x)

1
2

(cid:111)(cid:90)

(cid:16)

exp   

1
2

2. by completing the square, show that

(cid:1) +(cid:0)y       y
2 (x       x)t q(cid:0)y       y
(cid:16)
(cid:17)t
y       y + s   1qt (x       x)

=

s

draft november 9, 2017

2 (x       x)t q(cid:0)y       y
(cid:1)

(cid:1)t s(cid:0)y       y
(cid:16)
(cid:17)
y       y + s   1qt (x       x)

(8.11.93)

(cid:1) +(cid:0)y       y

(cid:1)t s(cid:0)y       y

(cid:1)(cid:17)

dy

(8.11.94)

(8.11.95)

    (x       x)t qs   1qt (x       x)

(8.11.96)

201

(cid:1)(cid:1)(cid:111)

(8.11.84)

(8.11.85)

(8.11.86)

(8.11.87)

(8.11.88)

(8.11.89)

(8.11.90)

(8.11.91)

(8.11.92)

3. using the fact that

y       y + s   1qt (x       x)

(cid:17)t

(cid:16)

(cid:17)(cid:27)
y       y + s   1qt (x       x)

s

(cid:113)

det(cid:0)2  s   1(cid:1)

dy =

exercises

(cid:26)(cid:16)

(cid:90)

exp   

1
2

p     qs   1qt(cid:17)
(cid:19)   1

show that

1
2

(x       x)t(cid:16)
(cid:18) p q
(cid:19)
p     qs   1qt(cid:17)   1

qt s

=

p(x)     exp   

4. since(cid:18)   xx   xy
(cid:16)

  yx   yy

  xx =

5. hence show that

p(x) = n (x   x,   xx)

use partitioned matrix inversion to show that

(x       x)

(8.11.97)

(8.11.98)

(8.11.99)

(8.11.100)

(8.11.101)

exercise 8.45. consider a not necessarily square matrix t and gaussian distributed x, with p(x) =
n (x   ,   ). we are interested in the distribution of the variable y = tx, where dim y     dim x.
de   ne

z =

=

x

(8.11.102)

(cid:19)

(cid:18) y

  x

(cid:19)
(cid:18) t
(cid:124) (cid:123)(cid:122) (cid:125)

  i

m

where   i is any matrix such that m is square and invertible.

(cid:90)

   (z     mx) p(x)dx     exp   

(cid:0)m   1z       (cid:1)t

     1(cid:0)m   1z       (cid:1) (8.11.103)

1
2

p(z) = n

3. using p(y) = (cid:82) p(z)d  x and the result that the marginal of a gaussian is a gaussian (see above

(8.11.104)

1. show that

(cid:90)

2. show that

p(z) =

p(z|x)p(x)dx =

(cid:16)

z m  , m  mt(cid:17)
(cid:16)
y t  , t  tt(cid:17)

exercise), show that

p(y) = n

(8.11.105)

202

draft november 9, 2017

chapter 9

learning as id136

in previous chapters we largely assumed that all distributions are fully speci   ed for the id136 tasks.
in machine learning and related    elds, however, the distributions need to be learned on the basis of data.
learning is then the problem of integrating data with domain knowledge of the model environment. in this
chapter we discuss how learning can be phrased as an id136 problem.

9.1 learning as id136

9.1.1 learning the bias of a coin

consider data expressing the results of tossing a coin. we write vn = 1 if on toss n the coin comes up heads,
and vn = 0 if it is tails. our aim is to estimate the id203    that the coin will be a head, p(vn = 1|  ) =   
    called the    bias    of the coin. for a fair coin,    = 0.5. the variables in this environment are v1, . . . , vn and
   and we require a model of the probabilistic interaction of the variables, p(v1, . . . , vn ,   ). assuming there
is no dependence between the observed tosses, except through   , we have the belief network

p(v1, . . . , vn ,   ) = p(  )

p(vn|  )

(9.1.1)

which is depicted in    g(9.1). the assumption that each observation is independent and identically dis-
tributed is called the i.i.d. assumption.

learning refers to using the observations v1, . . . , vn to infer   . in this context, our interest is

p(  |v1, . . . , vn ) =

p(v1, . . . , vn ,   )
p(v1, . . . , vn )

=

p(v1, . . . , vn|  )p(  )

p(v1, . . . , vn )

(9.1.2)

we still need to fully specify the prior p(  ). to avoid complexities resulting from continuous variables, we   ll
consider a discrete    with only three possible states,        {0.1, 0.5, 0.8}. speci   cally, we assume

p(   = 0.1) = 0.15,

p(   = 0.5) = 0.8,

p(   = 0.8) = 0.05

(9.1.3)

as shown in    g(9.2a). this prior expresses that we have 80% belief that the coin is    fair   , 5% belief the coin
is biased to land heads (with    = 0.8), and 15% belief the coin is biased to land tails (with    = 0.1). the
distribution of    given the data and our beliefs is

n(cid:89)

n=1

p(  |v1, . . . , vn )     p(  )

  

i[vn=1] (1       )

i[vn=0]

n(cid:89)
n(cid:89)
p(vn|  ) = p(  )
(cid:80)n
(cid:80)n
i[vn=1] (1       )

n=1

n=1

n=1

    p(  )  

i[vn=0]

n=1

203

(9.1.4)

(9.1.5)

  

v3

(a)

      

vn

  

vn

(b)

n

learning as id136

(a): belief network for coin
figure 9.1:
tossing model. (b): plate notation equiv-
alent of (a). a plate replicates the quanti-
ties inside the plate a number of times as
speci   ed in the plate.

i [vn = 1] is the number of occurrences of heads, which we more conveniently denote as
i [vn = 0] is the number of tails, nt . hence

v1

v2

in the above(cid:80)n
nh . likewise,(cid:80)n

n=1

n=1

p(  |v1, . . . , vn )     p(  )  nh (1       )nt

for an experiment with nh = 2, nt = 8, the posterior distribution is

   4
p(   = 0.1|v) = k    0.15    0.12    0.98 = k    6.46    10
   4
p(   = 0.5|v) = k    0.8    0.52    0.58 = k    7.81    10
   8
p(   = 0.8|v) = k    0.05    0.82    0.28 = k    8.19    10

(9.1.6)

(9.1.7)

(9.1.8)

(9.1.9)

where v is shorthand for v1, . . . , vn . from the normalisation requirement we have 1/k = 6.46    10   4 +
7.81    10   4 + 8.19    10   8 = 0.0014, so that

p(   = 0.1|v) = 0.4525,

p(   = 0.5|v) = 0.5475,

p(   = 0.8|v) = 0.0001

(9.1.10)

as shown in    g(9.2b). these are the    posterior    parameter beliefs. in this case, if we were asked to choose a
single a posteriori most likely value for   , it would be    = 0.5, although our con   dence in this is low since
the posterior belief that    = 0.1 is also appreciable. this result is intuitive since, even though we observed
more tails than heads, our prior belief was that it was more likely the coin is fair.

repeating the above with nh = 20, nt = 80, the posterior changes to

p(   = 0.1|v)   1     1.93    10

   6,

p(   = 0.5|v)   1.93    10

   6,

p(   = 0.8|v)   2.13    10

   35 (9.1.11)

   g(9.2c), so that the posterior belief in    = 0.1 dominates. this is reasonable since in this situation, there
are so many more tails than heads that this is unlikely to occur from a fair coin. even though we a priori
thought that the coin was fair, a posteriori we have enough evidence to change our minds.

@@

9.1.2 making decisions

in itself, the bayesian posterior merely represents our beliefs and says nothing about how best to summarise
these beliefs. in situations in which decisions need to be taken under uncertainty we need to additionally
specify what the utility of any decision is, as in chapter(7).

0.8

0.1

0.1

0.5
  

(a)

0.8

0.1

0.5
  

(b)

0.8

0.5
  

(c)

(a): prior encoding our be-
figure 9.2:
liefs about the amount the coin is biased to
heads. (b): posterior having seen nh = 2
(c): posterior
heads and nt = 8 tails.
having seen nh = 20 heads and nt = 80
tails. assuming any value of 0            1 is
possible the ml setting is    = 0.2 in both
nh = 2, nt = 8 and nh = 20, nt = 80.

204

draft november 9, 2017

learning as id136

in the coin tossing scenario where    is assumed to be either 0.1, 0.5 or 0.8, we setup a decision problem as
follows: if we correctly state the bias of the coin we gain 10 points; being incorrect, however, loses 20 points.
we can write this using

u (  ,   0) = 10i(cid:2)   =   0(cid:3)

    20i(cid:2)   (cid:54)=   0(cid:3)

where   0 is the true value for the bias. the expected utility of the decision that the coin is    = 0.1 is

u (   = 0.1) = u (   = 0.1,   0 = 0.1)p(  0 = 0.1|v)

+ u (   = 0.1,   0 = 0.5)p(  0 = 0.5|v) + u (   = 0.1,   0 = 0.8)p(  0 = 0.8|v)

plugging in the numbers from equation (9.1.10), we obtain

u (   = 0.1) = 10    0.4525     20    0.5475     20    0.0001 =    6.4270

similarly

u (   = 0.5) = 10    0.5475     20    0.4525     20    0.0001 =    3.5770

and

u (   = 0.8) = 10    0.0001     20    0.4525     20    0.5475 =    19.999

the best (that with the highest utility) is to say that the coin is unbiased,    = 0.5.

repeating the above calculations for nh = 20, nt = 80, we arrive at

u (   = 0.1) = 10    (1     1.93    10

u (   = 0.5) = 10    1.93    10

u (   = 0.8) = 10    2.13    10

   6)     20(cid:0)1.93    10
   6     20(cid:0)1     1.93    10
   35     20(cid:0)1     1.93    10

   35(cid:1) = 9.9999
   6 + 2.13    10
   35(cid:1)
   6 + 2.13    10
   6(cid:1)
   6 + 1.93    10

       20.0

       20.0

so that the best decision in this case is to choose    = 0.1.

as more information about the distribution p(v,   ) becomes available the posterior p(  |v) becomes increas-
ingly peaked, aiding our decision making process.

9.1.3 a continuum of parameters

in section(9.1.1) we considered only three possible values for   . here we discuss a continuum of parameters.

using a    at prior

we    rst examine the case of a       at    or uniform prior p(  ) = k for some constant k. for continuous variables,
normalisation requires

(cid:90)
(cid:90) 1

p(  )d   = 1

since    represents a id203 we must have 0            1,

p(  )d   = k = 1

0

repeating the previous calculations with this    at continuous prior, we have

p(  |v) =

1
c

  nh (1       )nt

(cid:90) 1

where c is a constant to be determined by normalisation,

c =

0

  nh (1       )nt d       b(nh + 1, nt + 1)

where b(  ,   ) is the beta function. see    g(9.3) for an example.

draft november 9, 2017

(9.1.12)

(9.1.13)

(9.1.14)

(9.1.15)

(9.1.16)

(9.1.17)

(9.1.18)

(9.1.19)

(9.1.20)

(9.1.21)

(9.1.22)

(9.1.23)

205

learning as id136

figure 9.3: posterior p(  |v) assuming a    at prior on   . (blue)
nh = 2, nt = 8 and (red) nh = 20, nt = 80. in both cases,
the most probable state of the posterior (maximum a posteriori)
is 0.2, which makes intuitive sense, since the fraction of heads to
tails in both cases is 0.2. where there is more data, the posterior
is more certain and sharpens around the most probable value.

using a conjugate prior

determining the normalisation constant of a continuous distribution requires that the integral of the unnor-
malised posterior can be carried out. for the coin tossing case, it is clear that if the prior is of the form of
a beta distribution, then the posterior will be of the same parametric form. for prior

p(  ) =

1

b(  ,   )

       1 (1       )     1

the posterior is

p(  |v)            1 (1       )     1   nh (1       )nt

so that

p(  |v) = b(  |   + nh ,    + nt )

(9.1.24)

(9.1.25)

(9.1.26)

the prior and posterior are of the same form (both beta distributions) but simply with di   erent parameters.
hence the beta distribution is    conjugate    to the binomial distribution.

9.1.4 decisions based on continuous intervals

to illustrate the use of continuous variables in decision making, we consider a simple decision problem.
the result of a coin tossing experiment is nh = 2 heads and nt = 8 tails. you now need to make a
decision: you win 10 dollars if you correctly guess which way the coin is biased     towards heads or tails.
if your guess is incorrect, you lose a million dollars. what is your decision? (assume an uninformative prior).

we need two quantities,    for our guess and   0 for the truth. then the utility of saying heads is

u (   > 0.5,   0 > 0.5)p(  0 > 0.5|v) + u (   > 0.5,   0 < 0.5)p(  0 < 0.5|v)

in the above,

p(  0 < 0.5|v) =

(cid:90) 0.5

0

p(  0|v)d  0

1

(cid:90) 0.5

=

b(   + nh ,    + nt )
    i0.5(   + nh ,    + nt )

0

    +nh   1 (1       )  +nt    1 d  

(9.1.27)

(9.1.28)

(9.1.29)

(9.1.30)

where ix(a, b) is the regularised incomplete beta function. for the case of nh = 2, nt = 8, under a    at
prior,

p(  0 < 0.5|v) = i0.5(nh + 1, nt + 1) = 0.9673

(9.1.31)
since the events are exclusive, p(  0     0.5|v) = 1     0.9673 = 0.0327. hence the expected utility of saying
heads is more likely is

10    0.0327     1000000    0.9673 =    9.673    105.

similarly, the utility of saying tails is more likely can be computed to be

10    0.9673     1000000    0.0327 =    3.269    104.

(9.1.32)

(9.1.33)

206

draft november 9, 2017

00.20.40.60.810510  bayesian methods and ml-ii

a

s

c

(a)

  

v

(a)

  (cid:48)

  

v

(b)

  a

an

  s

sn

cn

n = 1 : n

  c

(b)

figure 9.4: (a): a model for the relationship between
(b):
lung cancer, asbestos exposure and smoking.
plate notation replicating the observed n datapoints
with the cpts tied across all datapoints.

(a): standard ml learning. the best parameter    is found
figure 9.5:
by maximising the id203 that the model generates the observed data
(b): ml-ii learning. in cases where we have a
  opt = arg max   p(v|  ).
prior preference for the parameters   , but with unspeci   ed hyperparameter
  (cid:48), we can    nd   (cid:48) by   (cid:48)

opt = arg max  (cid:48) p(v|  (cid:48)) = arg max  (cid:48) (cid:104)p(v|  )(cid:105)p(  |  (cid:48)).

since the expected utility of deciding    tails    is highest, we are better o    taking the decision that the coin is
more likely to come up tails.

if we modify the above so that we lose 100 million dollars if we guess tails when in fact it is heads, the
expected utility of saying tails would be    3.27    106. in this case, even though we are more con   dent that
the coin is likely to come up tails, we would pay such a penalty of making a mistake in saying tails, that it
is in fact better to say heads.

9.2 bayesian methods and ml-ii

consider a parameterised distribution p(v|  ), for which we wish to the learn the optimal parameters    given
some data. the model p(v|  ) is depicted in    g(9.5a), where a dot indicates that no distribution is present
on that variable. for a single observed datapoint v, setting    by maximum likelihood corresponds to    nding
the parameter    that maximises p(v|  ).

in some cases we may have an idea about which parameters    are more appropriate and can express this prior
preference using a distribution p(  ). if the prior were fully speci   ed, then there is nothing to    learn    since
p(  |v) is now fully known. however, in many cases in practice, we are unsure of the exact parameter settings
of the prior, and hence specify a parametersised prior using a distribution p(  |  (cid:48)) with hyperparameter   (cid:48).
this is depicted in    g(9.5b). learning then corresponds to    nding the optimal   (cid:48) that maximises the
   p(v|  )p(  |  (cid:48)). this is known as an ml-ii procedure since it corresponds to maximum

likelihood p(v|  (cid:48)) =(cid:82)

likelihood, but at the higher, hyperparameter level[34, 198]. by treating the parameters    as variables, one
can view this then as learning under hidden variables, for which the methods of chapter(11) are applicable.
we will encounter examples of this ml-ii procedure later, for example in section(18.1.2).

draft november 9, 2017

207

maximum likelihood training of belief networks

a
1
1
0
0
1
0
1

s
1
0
1
1
1
0
0

c
1
0
1
0
1
0
1

figure 9.6: a database containing information about the asbestos exposure
(1 signi   es exposure), being a smoker (1 signi   es the individual is a smoker),
and lung cancer (1 signi   es the individual has lung cancer). each row
contains the information for each of the seven individuals in the database.

9.3 maximum likelihood training of belief networks

consider the following model of the relationship between exposure to asbestos (a), being a smoker (s) and
the incidence of lung cancer (c)

p(a, s, c) = p(c|a, s)p(a)p(s)

(9.3.1)

which is depicted in    g(9.4a). each variable is binary, dom(a) = {0, 1}, dom(s) = {0, 1}, dom(c) = {0, 1}.
we assume that there is no direct relationship between smoking and exposure to asbestos. this is the
kind of assumption that we may be able to elicit from medical experts. furthermore, we assume that we
have a list of patient records,    g(9.6), where each row represents a patient   s data. to learn the table entries
p(c|a, s) we can do so by counting the number of times variable c is in state 1 for each of the 4 parental
states of a and s:

p(c = 1|a = 0, s = 0) = 0,
p(c = 1|a = 0, s = 1) = 0.5
p(c = 1|a = 1, s = 0) = 0.5 p(c = 1|a = 1, s = 1) = 1

(9.3.2)

similarly, based on counting, p(a = 1) = 4/7, and p(s = 1) = 4/7. these three cpts then complete the full
distribution speci   cation.

setting the cpt entries in this way by counting the relative number of occurrences corresponds mathemat-
ically to maximum likelihood learning under the i.i.d. assumption, as we show below.

maximum likelihood corresponds to counting

for a bn there is a constraint on the form of p(x), namely

p(x) =

p(xi|pa (xi))

(9.3.3)

to compute the maximum likelihood setting of each term p(xi|pa (xi)), as shown in section(8.7.3), we can
equivalently minimise the id181 between the empirical distribution q(x) and p(x). for
the bn p(x), and empirical distribution q(x) we have

(cid:42) k(cid:88)

(cid:43)

kl(q|p) =    

log p (xi|pa (xi))

+ const. =    

q(x)

i=1

k(cid:88)

i=1

(cid:104)log p (xi|pa (xi))(cid:105)q(xi,pa(xi)) + const.

(9.3.4)

(9.3.5)

this follows using the general result

(cid:104)f (xi)(cid:105)q(x ) = (cid:104)f (xi)(cid:105)q(xi)

which says that if the function f only depends on a subset of the variables, we only need to know the
marginal distribution of this subset of variables in order to carry out the average.

since q(x) is    xed, we can add on entropic terms in q and equivalently mimimize

k(cid:89)

i=1

(cid:105)

(cid:104)

k(cid:88)
k(cid:88)

i=1

kl(q|p) =

=

(cid:104)log q(xi|pa (xi))(cid:105)q(xi,pa(xi))     (cid:104)log p (xi|pa (xi))(cid:105)q(xi,pa(xi))

+const.

(9.3.6)

@@

i=1

208

(cid:104)kl(q(xi|pa (xi))|p(xi|pa (xi)))(cid:105)q(pa(xi))

+const.

(9.3.7)

@@

draft november 9, 2017

maximum likelihood training of belief networks

the    nal line is a positive weighted sum of individual id181s. the minimal kullback-
leibler setting, and that which corresponds to maximum likelihood, is therefore

p(xi|pa (xi)) = q(xi|pa (xi))

in terms of the original data, this is

p(xi = s|pa (xi) = t)    

i [xn

i = s, pa (xn

i ) = t]

n(cid:88)

n=1

(9.3.8)

(9.3.9)

this expression corresponds to the intuition that the table entry p(xi|pa (xi)) can be set by counting the
number of times the state {xi = s, pa (xi) = t} occurs in the dataset (where t is a vector of parental states).
the table is then given by the relative number of counts of being in state s compared to the other states s(cid:48),
for    xed joint parental state t.

an alternative method to derive this intuitive result is to use lagrange multipliers, see exercise(9.4). for
reader less comfortable with the above kullback-leibler derivation, a more direct example is given below
which makes use of the notation

(cid:93) (x1 = s1, x2 = s2, x3 = s3, . . .)

(9.3.10)

to denote the number of times that states x1 = s1, x2 = s2, x3 = s3, . . . occur together in the training data.
see also section(10.1) for further examples.

example 9.1. we wish to learn the table entries of the distribution p(x1, x2, x3) = p(x1|x2, x3)p(x2)p(x3).
we address here how to    nd the cpt entry p(x1 = 1|x2 = 1, x3 = 0) using maximum likelihood. for i.i.d.
data, the contribution from p(x1|x2, x3) to the log likelihood is

(cid:88)

n

log p(xn

2 , xn
3 )

1|xn

the number of times p(x1 = 1|x2 = 1, x3 = 0) occurs in the log likelihood is (cid:93) (x1 = 1, x2 = 1, x3 = 0),
the number of such occurrences in the training set. since (by the normalisation constraint) p(x1 = 0|x2 =
1, x3 = 0) = 1     p(x1 = 1|x2 = 1, x3 = 0), the total contribution of p(x1 = 1|x2 = 1, x3 = 0) to the log
likelihood is

(cid:93) (x1 = 1, x2 = 1, x3 = 0) log p(x1 = 1|x2 = 1, x3 = 0)

+ (cid:93) (x1 = 0, x2 = 1, x3 = 0) log (1     p(x1 = 1|x2 = 1, x3 = 0))

using        p(x1 = 1|x2 = 1, x3 = 0) we have

(cid:93) (x1 = 1, x2 = 1, x3 = 0) log    + (cid:93) (x1 = 0, x2 = 1, x3 = 0) log (1       )
di   erentiating the above expression w.r.t.    and equating to zero gives

(cid:93) (x1 = 1, x2 = 1, x3 = 0)

   
the solution for optimal    is then

  

(cid:93) (x1 = 0, x2 = 1, x3 = 0)

= 0

1       

(9.3.11)

(9.3.12)

(9.3.13)

p(x1 = 1|x2 = 1, x3 = 0) =

(cid:93) (x1 = 1, x2 = 1, x3 = 0)

(cid:93) (x1 = 1, x2 = 1, x3 = 0) + (cid:93) (x1 = 0, x2 = 1, x3 = 0)

,

(9.3.14)

corresponding to the intuitive counting procedure.

draft november 9, 2017

209

maximum likelihood training of belief networks

xn   1

xn

x1

x2

      

y

figure 9.7: a variable y with a large number of parents
x1, . . . , xn requires the speci   cation of an exponen-
tially large number of entries in the conditional prob-
ability p(y|x1, . . . , xn). one solution to this di   culty
is to parameterise the conditional, p(y|x1, . . . , xn,   ).

id155 functions

consider a binary variable y with n binary parental variables, x = (x1, . . . , xn), see    g(9.7). there are 2n
entries in the cpt of p(y|x) so that it is infeasible to explicitly store these entries for even moderate values
of n. to reduce the complexity of this cpt we may constrain the form of the table. for example, one could
use a function

p(y = 1|x, w) =

1

1 + e   wtx

(9.3.15)

where we only need to specify the n-dimensional parameter vector w.

in this case, rather than using maximum likelihood to learn the entries of the cpts directly, we instead
learn the value of the parameter w. since the number of parameters in w is small (n, compared with 2n
in the unconstrained case), we also have some hope that with a small number of training examples we can
learn a reliable value for w.

example 9.2. consider the following 3 variable model p(x1, x2, x3) = p(x1|x2, x3)p(x2)p(x3), where xi    
{0, 1} , i = 1, 2, 3. we assume that the cpt is parameterised using    = (  1,   2) with

p(x1 = 1|x2, x3,   )     e

     2

1     2

2(x2   x3)2

(9.3.16)

one may verify that the above id203 is always positive and lies between 0 and 1. due to normalisation,
we must have

p(x1 = 0|x2, x3) = 1     p(x1 = 1|x2, x3)

(9.3.17)

for unrestricted p(x2) and p(x3), the maximum likelihood setting is p(x2 = 1)     (cid:93) (x2 = 1), and p(x3 =
1)     (cid:93) (x3 = 1). the contribution to the log likelihood from the term p(x1|x2, x3,   ), assuming i.i.d. data, is

1 = 1](cid:0)

i [xn

n(cid:88)

n=1

3 )2(cid:1) + i [xn

(cid:16)

3 )2(cid:17)

     2

1     2

2    xn

2(xn

     2

1       2

2(xn

2     xn

1 = 0] log

1     e

(9.3.18)

this objective function needs to be optimised numerically to    nd the best   1 and   2. the gradient is

   2i [xn

1 = 1]   1 + 2i [xn

1 = 0]

1     2
  1e     2
1     e     2
1     2

2    xn
3 )2
2    xn
3 )2

2(xn
2(xn

   2i [xn

1 = 1]   2 (xn

2     xn

3 )2 + 2  2i [xn

1 = 0]

(xn

3 )2e     2
1     2

2     xn
1     e     2

1     2
2(xn

2    xn
2(xn
3 )2
2    xn
3 )2

(9.3.19)

(9.3.20)

the gradient can be used as part of a standard optimisation procedure (such as conjugate gradients, see
section(a.5)) to    nd the maximum likelihood parameters   1,   2.

210

draft november 9, 2017

l(  1,   2) =

n(cid:88)
n(cid:88)

n=1

n=1

dl
d  1

=

dl
d  2

=

bayesian belief network training

9.4 bayesian belief network training

an alternative to maximum likelihood training of a bn is to use a bayesian approach in which we maintain
a distribution over parameters. we continue with the asbestos, smoking, cancer scenario,

p(a, c, s) = p(c|a, s)p(a)p(s)

(9.4.1)

as represented in    g(9.4a). so far we   ve only speci   ed the independence structure, but not the entries of the
tables p(c|a, s), p(a), p(s). given a set of visible observations, v = {(an, sn, cn) , n = 1, . . . , n}, we would
like to learn appropriate distributions for the table entries.

to begin we need a notation for the table entries. with all variables binary we have parameters such as

p(a = 1|  a) =   a, p(c = 1|a = 0, s = 1,   c) =   0,1
,   1,0

and similarly for the remaining parameters   1,1

,   0,0

c

c

c

c

  a,   s,   0,0

c

,   0,1

c

,   1,0

c

,   1,1

c

(cid:124)

(cid:123)(cid:122)

  c

(cid:125)

. for our example, the parameters are

(9.4.2)

(9.4.3)

in the following section, section(9.4.1), we describe    rst useful independence assumptions on the general
form of the prior variables, before making a speci   c numerical prior speci   cation in section(9.4.2).

9.4.1 global and local parameter independence

in bayesian learning of bns, we need to specify a prior on the joint table entries. since in general dealing
with multi-dimensional continuous distributions is computationally problematic, it is useful to specify only
uni-variate distributions in the prior. as we show below, this has a pleasing consequence that for i.i.d. data
the posterior also factorises into uni-variate distributions.

global parameter independence

a convenient assumption is that the prior factorises over parameters. for our asbestos, smoking, cancer
example, we assume

p(  a,   s,   c) = p(  a)p(  s)p(  c)

assuming the data is i.i.d., we then have the joint model

p(  a,   s,   c,v) = p(  a)p(  s)p(  c)

p(an|  a)p(sn|  s)p(cn|sn, an,   c)

(cid:89)

n

(9.4.4)

(9.4.5)

the belief network for which is given in    g(9.8.) a convenience of the factorised prior for a bn is that the
posterior also factorises, since

p(  a,   s,   c|v)     p(  a,   s,   c,v)

(cid:40)

=

p(  a)

(cid:41)(cid:40)
p(an|  a)

(cid:89)

n

(cid:89)

n

(cid:41)(cid:40)

(cid:41)

(cid:89)

n

p(cn|sn, an,   c)

p(  s)

p(sn|  s)

p(  c)

    p(  a|va)p(  s|vs)p(  c|vc)

(9.4.6)

so that one can consider each parameter posterior separately. in this case,    learning    involves computing the
posterior distributions p(  i|vi) where vi is the set of training data restricted to the family of variable i.
the global independence assumption conveniently results in a posterior distribution that factorises over the
conditional tables. however, the parameter   c is itself 4 dimensional. to simplify this we need to make a
further assumption as to the structure of each local table.

draft november 9, 2017

211

bayesian belief network training

figure 9.8: a bayesian parameter model for the relationship between
lung cancer, asbestos exposure and smoking with factorised parameter
priors. the global parameter independence assumption means that the
prior over tables factorises into priors over each id155 ta-
ble. the local independence assumption, which in this case comes into
c ), where

e   ect only for p(c|a, s), means that p(  c) factorises in(cid:81)

a,s   p p(  a,s

p = {(0, 0), (0, 1), (1, 0), (1, 1)}.

  a

an

  s

sn

cn

n = 1 : n

  a,s
c

(a, s)     p

local parameter independence

if we further assume that the prior for the table factorises over all states a, c:

p(  c) = p(  0,0

c )p(  1,0

c )p(  0,1

c )p(  1,1
c )

then the posterior is given by

(9.4.7)

c

c )p(  1,0

=(cid:2)  0,0
p(  c|vc)     p(vc|  c)p(  0,0
(cid:124)
(cid:2)  1,0
(cid:124)

(cid:3)(cid:93)(a=0,s=0,c=1)(cid:2)1       0,0
(cid:123)(cid:122)
(cid:3)(cid:93)(a=1,s=0,c=1)(cid:2)1       1,0
(cid:123)(cid:122)

c
   p(  0,0
|vc)

  

c )p(  0,1

c )p(  1,1
c )

(cid:3)(cid:93)(a=0,s=0,c=0)
(cid:3)(cid:93)(a=1,s=0,c=0)

c

c

c
   p(  1,0
|vc)

c

c

(cid:3)(cid:93)(a=0,s=1,c=1)(cid:2)1       0,1
(cid:2)  0,1
(cid:124)
(cid:123)(cid:122)
(cid:3)(cid:93)(a=1,s=1,c=1)(cid:2)1       1,1
(cid:2)  1,1
(cid:124)
(cid:123)(cid:122)

c
   p(  0,1
|vc)

c
   p(  1,1
|vc)

c

c

c

(cid:3)(cid:93)(a=0,s=1,c=0)
(cid:3)(cid:93)(a=1,s=1,c=0)

(cid:125)
(cid:125)

p(  0,0
c )

p(  1,0
c )

so that the posterior also factorises over the parental states of the local conditional table.

(cid:90)
(cid:90)

posterior marginal table

a marginal id203 table is given by, for example,

p(c = 1|a = 1, s = 0,v) =

p(c = 1|a = 1, s = 0,   1,0

c )p(  c|vc)

  c

the integral over all the other tables in equation (9.4.9) is unity, and we are left with

(cid:90)

p(c = 1|a = 1, s = 0,v) =

p(c = 1|a = 1, s = 0,   1,0

c )p(  1,0
c

|vc) =

c p(  1,0
  1,0
c

|vc)

(9.4.10)

  1,0
c

  1,0
c

9.4.2 learning binary variable tables using a beta prior

we continue the example of section(9.4.1) where all variables are binary, but using a continuous valued table
prior. the simplest case is to start with p(a|  a) since this requires only a univariate prior distribution p(  a).
the likelihood depends on the table variable via

p(a = 1|  a) =   a

so that the total likelihood term is

  (cid:93)(a=1)
a

(1       a)(cid:93)(a=0)

212

(9.4.11)

(9.4.12)

draft november 9, 2017

p(  0,1
c )

p(  1,1
c )

(cid:125)
(cid:125)

(9.4.8)

(9.4.9)

bayesian belief network training

the posterior is therefore

p(  a|va)     p(  a)  (cid:93)(a=1)

a

(1       a)(cid:93)(a=0)

(9.4.13)

this means that if the prior is also of the form     
of integration will be straightforward. this suggests that the most convenient choice is a beta distribution,

a (1       a)   then conjugacy will hold, and the mathematics

p(  a) = b (  a|  a,   a) =

1

b(  a,   a)

    a   1

a

(1       a)  a   1

for which the posterior is also a beta distribution:

p(  a|va) = b (  a|  a + (cid:93) (a = 1) ,   a + (cid:93) (a = 0))

(cid:90)

the marginal table is given by (following similar reasoning as for equation (9.4.10))

p(a = 1|va) =

  a

p(  a|va)  a =

  a + (cid:93) (a = 1)

  a + (cid:93) (a = 1) +   a + (cid:93) (a = 0)

(9.4.14)

(9.4.15)

(9.4.16)

using the result for the mean of a beta distribution, de   nition(8.23).

the situation for the table p(c|a, s) is slightly more complex since we need to specify a prior for each of
the parental tables. as above, this is most convenient if we specify a beta prior, one for each of the (four)
parental states. let   s look at a speci   c table

p(c = 1|a = 1, s = 0)

assuming the local independence property, we have p(  1,0

|  c(a = 1, s = 0) + (cid:93) (c = 1, a = 1, s = 0) ,   c(a = 1, s = 0) + (cid:93) (c = 0, a = 1, s = 0)(cid:1)

|vc) given by

b(cid:0)  1,0

c

c

as before, the marginal id203 table is then given by

p(c = 1|a = 1, s = 0,vc) =

  c(a = 1, s = 0) + (cid:93) (c = 1, a = 1, s = 0)

  c(a = 1, s = 0) +   c(a = 1, s = 0) + (cid:93) (a = 1, s = 0)

since (cid:93) (a = 1, s = 0) = (cid:93) (c = 0, a = 1, s = 0) + (cid:93) (c = 1, a = 1, s = 0).

(9.4.17)

(9.4.18)

(9.4.19)

the prior parameters   c(a, s) are called hyperparameters. a complete ignorance prior would correspond
to setting    =    = 1, see    g(8.4).

it is instructive to examine this bayesian solution under various conditions:

no data limit n     0 in the limit of no data, the marginal id203 table corresponds to the prior,

which is given in this case by

p(c = 1|a = 1, s = 0) =

  c(a = 1, s = 0)

  c(a = 1, s = 0) +   c(a = 1, s = 0)

(9.4.20)

for a    at prior    =    = 1 for all states a, c, this would give a prior id203 of p(c = 1|a = 1, s =
0) = 0.5.

in   nite data limit n         in this limit the marginal id203 tables are dominated by the data counts,
since these will typically grow in proportion to the size of the dataset. this means that in the in   nite
(or very large) data limit,

p(c = 1|a = 1, s = 0,v)    

(cid:93) (c = 1, a = 1, s = 0)

(cid:93) (c = 1, a = 1, s = 0) + (cid:93) (c = 0, a = 1, s = 0)

(9.4.21)

which corresponds to the maximum likelihood solution.

this e   ect that the large data limit of a bayesian procedure corresponds to the maximum likelihood
solution is general unless the prior has a pathologically strong e   ect.

draft november 9, 2017

213

bayesian belief network training

zero hyperparameter limit when   c =   c = 0, the marginal table equation (9.4.19) corresponds to the
maximum likelihood table setting for any amount of data. when   c =   c = 0, the beta distribution
places mass 0.5 at 0 and mass 0.5 at 1. note that this equivalence of the maximum likelihood solution
with the marginal table under zero hyperparameter values contrasts with the equivalence of the map
table under uniform hyperparameter values.

example 9.3 (asbestos-smoking-cancer).

consider the binary variable network

p(c, a, s) = p(c|a, s)p(a)p(s)

(9.4.22)

the data v is given in    g(9.6). using a    at beta prior    =    = 1 for all id155 tables, the
marginal posterior tables are given by

p(a = 1|v) =

1 + (cid:93) (a = 1)

2 + n

=

1 + 4
2 + 7

=

5
9     0.556

(9.4.23)

by comparison, the maximum likelihood setting is 4/7 = 0.571. the bayesian result is a little more
cautious, which squares with our prior belief that any setting of the id203 is equally likely, pulling the
posterior towards 0.5.

similarly,

p(s = 1|v) =

1 + (cid:93) (s = 1)

2 + n

=

1 + 4
2 + 7

=

5
9     0.556

and

p(c = 1|a = 1, s = 1,v) =

1 + (cid:93) (c = 1, a = 1, s = 1)

2 + (cid:93) (c = 1, a = 1, s = 1) + (cid:93) (c = 0, a = 1, s = 1)

p(c = 1|a = 1, s = 0,v) =

1 + (cid:93) (c = 1, a = 1, s = 0)

2 + (cid:93) (c = 1, a = 1, s = 0) + (cid:93) (c = 0, a = 1, s = 0)

p(c = 1|a = 0, s = 1,v) =

1 + (cid:93) (c = 1, a = 0, s = 1)

2 + (cid:93) (c = 1, a = 0, s = 1) + (cid:93) (c = 0, a = 0, s = 1)

p(c = 1|a = 0, s = 0,v) =

1 + (cid:93) (c = 1, a = 0, s = 0)

2 + (cid:93) (c = 1, a = 0, s = 0) + (cid:93) (c = 0, a = 0, s = 0)

(9.4.24)

(9.4.25)

(9.4.26)

@@

(9.4.27)

(9.4.28)

=

1 + 2
2 + 2

=

=

1 + 1
2 + 2

=

=

1 + 1
2 + 2

=

=

1 + 0
2 + 1

=

3
4

1
2

1
2

1
3

9.4.3 learning multivariate discrete tables using a dirichlet prior

the natural generalisation to our discussion of bayesian learning of bns is to consider variables that can
take more than two states. in this case the natural conjugate prior is given by the dirichlet distribution,
which generalises the beta distribution to more than two states. again we assume throughout i.i.d. data
and the local and global parameter prior independencies. since under the global parameter independence
assumption the posterior factorises over variables (as in equation (9.4.6)), we can concentrate on the posterior
of a single variable.

214

draft november 9, 2017

let   s consider a variable v with dom(v) = {1, . . . , i}. if we denote the id203 of v being in state i by
  i, i.e. p(v = i|  ) =   i, the contribution to the posterior from a datapoint vn is

bayesian belief network training

no parents

  

p(vn|  ) =

so that the posterior for    given a dataset v =(cid:8)v1, . . . , vn(cid:9)

i=1

i=1

  i = 1

,

i(cid:88)

i(cid:89)

i[vn=i]
i

n(cid:89)

i(cid:89)

i(cid:89)

(cid:80)n

n=1

  

i

i[vn=i]

p(  |v)    p(  )

i[vn=i]
i

  

= p(  )

n=1

i=1

i=1

it is convenient to use a dirichlet prior distribution with hyperparameters u

p(  )= dirichlet (  |u)    

i(cid:89)

i=1

  ui   1

i

using this prior the posterior becomes

i(cid:89)

i(cid:89)

(cid:80)n

p(  |v)    

i=1

i=1

  ui   1

i

i[vn=i]

=

n=1

  

i

ui   1+(cid:80)n

i[vn=i]

n=1

i(cid:89)

i=1

  

i

which means that the posterior is given by

p(  |v) = dirichlet (  |u + c)

where c is a count vector with components

n(cid:88)

ci =

i [vn = i]

n=1

being the number of times state i was observed in the training data.

the single-variable marginal distribution of a dirichlet is a beta distribution,

the marginal table is given by integrating

p(v = i|v) =

p(v = i|  )p(  |v) =

  ip(  i|v)

  i

(cid:90)
       .

(cid:88)

j(cid:54)=i

uj + cj

  

(cid:90)
        i|ui + ci,
(cid:80)

ui + ci
j uj + cj

p(  i|v) = b

p(v = i|v) =

the marginal table is then given by the mean of the beta distribution

(9.4.29)

(9.4.30)

(9.4.31)

(9.4.32)

(9.4.33)

(9.4.34)

(9.4.35)

(9.4.36)

(9.4.37)

which generalises the binary state formula equation (9.4.16).

parents

to deal with the general case of a variable v with parents pa (v) we denote the id203 of v being in
state i, conditioned on the parents being in state j as

p(v = i|pa (v) = j,   ) =   i(v; j)

draft november 9, 2017

(9.4.38)

215

where (cid:80)

bayesian belief network training

s
1
0
1
1
1
0
0

c
2
0
1
0
2
0
1

figure 9.9: a database of patient records about asbestos exposure (1 signi   es ex-
posure), being a smoker (1 signi   es the individual is a smoker), and lung cancer (0
signi   es no cancer, 1 signi   es early stage cancer, 2 signi   es late state cancer). each
row contains the information for each of the seven individuals in the database.

a
1
1
0
0
1
0
1

i   i(v; j) = 1. this forms the components of a vector   (v; j). note that if v has k parents then

the number of parental states s will be exponential in k.

writing   (v) = [  (v; 1), . . . ,   (v; s)], local (parental state) independence means

(cid:89)

j

(cid:89)

p(  (v)) =

p(  (v; j))

and global independence means

p(  ) =

p(  (v))

(9.4.39)

(9.4.40)

v

where    = (  (v), v = 1, . . . , v ) represents the combined table of all the variables.

parameter posterior

thanks to the global parameter independence assumption the posterior factorises, with one posterior table
per variable. each posterior table for a variable v depends only on the data d(v) of the family of the
variable. assuming a dirichlet distribution prior

p(  (v; j)) = dirichlet (  (v; j)|u(v; j))

the posterior is also dirichlet

(cid:89)

j

dirichlet(cid:0)  (v; j)|u(cid:48)

(v; j)(cid:1)

p(  (v)|d(v)) =

where the hyperparameter prior term is updated by the observed counts,

(cid:48)
i(v; j)     ui(v; j) + (cid:93) (v = i, pa (v) = j) .
u

by analogy with the no-parents case, the marginal table is given by

(cid:48)
p(v = i|pa (v) = j,d(v))     u
i(v; j).

(9.4.41)

(9.4.42)

(9.4.43)

(9.4.44)

(cid:80)

example 9.4. consider the p(c|a, s)p(s)p(a) asbestos example with dom(a) = dom(s) = {0, 1}, except
now with the variable c taking three states, dom(c) = {0, 1, 2}, accounting for di   erent kinds of cancer, see
   g(9.9). the marginal table under a dirichlet prior is then given by, for example

p(c = 0|a = 1, s = 1,v) =

u0(a = 1, s = 1) + (cid:93) (c = 0, a = 1, s = 1)

i   {0,1,2} ui(a = 1, s = 1) + (cid:93) (c = i, a = 1, s = 1)

assuming a    at dirichlet prior, which corresponds to setting all components of u to 1, this gives

p(c = 0|a = 1, s = 1,v) =

p(c = 1|a = 1, s = 1,v) =

p(c = 2|a = 1, s = 1,v) =

1 + 0
3 + 2

=

1 + 0
3 + 2

=

1 + 2
3 + 2

=

1
5

1
5

3
5

(9.4.45)

(9.4.46)

(9.4.47)

(9.4.48)

and similarly for the other three tables p(c|a = 1, s = 0), p(c|a = 0, s = 1), p(c|a = 1, s = 1).

216

draft november 9, 2017

structure learning

algorithm 9.1 pc algorithm for skeleton learning.

1: start with a complete undirected graph g on the set v of all vertices.
2: i = 0
3: repeat
4:
5:

for x     v do

for y     adj {x} do

6:

7:
8:
9:

end for

end for
i = i + 1.

10: until all nodes have     i neighbours.

determine if there is a subset s of size i of the neighbours of x (not including y) for which
x       y|s. if this set exists remove the x     y link from the graph g and set sxy = s.

model likelihood
for a variable v, and i.i.d. data d(v) = {(vn|pa (vn)), n = 1, . . . , n} for the family of this variable,

(cid:89)

n

p(vn|pa (vn)) =

=

=

=

(cid:89)

n

1

(cid:90)

(cid:90)
(cid:90)
(cid:89)
(cid:89)

j

j

  (v)

p(  (v))

         (cid:89)

j

1

z(u(v; j))
z(u(cid:48)(v; j))
z(u(v; j))

  (v)

z(u(v; j))

(cid:89)
(cid:89)

i

  (v;j)

i

p(vn|pa (vn) ,   (v))

  i(v; j)ui(v;j)   1

         (cid:89)

n

(cid:89)

(cid:89)

j

i

(9.4.49)

i[vn=i,pa(vn)=j]

  i(v; j)

(9.4.50)

  i(v; j)ui(v;j)   1+(cid:93)(v=i,pa(v)=j)

(9.4.51)

(9.4.52)

where z(u) is the normalisation constant of a dirichlet distribution with hyperparameters u; u(cid:48) is as given
in equation (9.4.43).

for a belief network on variables v = (v1, . . . , vd) the joint id203 of all variables factorises into the
local probabilities of each variable conditioned on its parents. the likelihood of a complete set of i.i.d. data

d =(cid:8)v1, . . . , vn(cid:9) is then given by:

p(d) =

p(vn

k|pa (vn

k )) =

(cid:89)

(cid:89)

k

n

(cid:89)

(cid:89)

k

j

z(u(cid:48)(vk; j))
z(u(vk; j))

(9.4.53)

where u(cid:48) is given by equation (9.4.43). expression (9.4.53) can be written explicitly in terms of gamma
functions, see exercise(9.9). in the above expression in general the number of parental states di   ers for each
variable vk, so that implicit in the above formula is that the state product over j goes from 1 to the number
of parental states of variable vk. due to the local and global parameter independence assumptions, the
logarithm of the model likelihood is a
sum of terms, one for each variable vk and parental con   guration j.
this is called the likelihood decomposable property.

@@

9.5 structure learning

up to this point we have assumed that we are given both the structure of the distribution and a dataset
d. a more complex task is when we need to learn the structure of the network as well. we   ll consider the
case in which the data is complete (i.e. there are no missing observations). since for d variables, there is
an exponentially large number (in d) of bn structures, it   s clear that we cannot search over all possible
structures. for this reason structure learning is a computationally challenging problem and we must rely on
constraints and heuristics to help guide the search. whilst in general structure learning is intractable, a cele-
brated tractable special case is when the network is constrained to have at most one parent, see section(9.5.4).

draft november 9, 2017

217

structure learning

for all but the sparsest networks, estimating the dependencies to any accuracy requires a large amount of
data, making testing of dependencies di   cult. consider the following simple situation of two independent
variables, p(x, y) = p(x)p(y). based on a    nite sample from this joint distribution d = {(xn, yn), n = 1, . . . , n},
we want to try to understand if x is independent of y. one way to do this is to compute the empirical mutual
information i(x, y); if this is zero then, empirically, x and y are independent. however, for a    nite amount
of data, two variables will typically have non-zero mutual information, so that a threshold needs to be set
to decide if the measured dependence is signi   cant under the    nite sample, see section(9.5.2).

other complexities arise from the concern that a belief or markov network on the visible variables alone
may not be a parsimonious way to represent the observed data if, for example, there may be latent variables
which are driving the observed dependencies. we will not enter into such issues in our discussion here
and limit the presentation to two central approaches, one which attempts to make a network structure
consistent with local empirical dependencies (the pc algorithm), and one which builds a structure that is
most probable for the global data (network scoring).

9.5.1 pc algorithm

the pc algorithm[275]    rst learns the skeleton of a graph, after which edges may be oriented to form a
(partially oriented) dag. the procedure to learn the skeleton is based on using the empirical data to test if
two variables are independent. a variety of approaches can be used to ascertain independence, as described
in section(9.5.2).

the pc algorithm begins at the    rst round with a complete skeleton g and attempts to remove as many
links as possible. at the    rst step we test all pairs x        y|    . if an x and y pair are deemed independent
then the link x     y is removed from the complete graph. one repeats this for all the pairwise links. in the
second round, for the remaining graph, one examines each x     y link and conditions on a single neighbour
z of x. if x       y| z then remove the link x     y. one repeats in this way through all the variables. at each
round the number of neighbours in the conditioning set is increased by one. see algorithm(9.1),    g(9.10)1
and demopcoracle.m. a re   nement of this algorithm, known as npc for necessary path pc[277] limits the
number of independence checks to remove inconsistencies resulting from the empirical estimates of condi-
tional mutual information.

given a learned skeleton, a partial dag can be constructed using algorithm(9.2). note that this is necessary
since the undirected graph g is a skeleton     not a belief network of the independence assumptions discovered.
for example, we may have a graph g with x     z     y in which the x     y link was removed on the basis
x       y|        sxy =    . as a mn the graph x     z     y (graphically) implies x(cid:62)(cid:62)y, although this is inconsistent
with the discovery in the    rst round x       y. this is the reason for the orientation part: for consistency, we
must have x     z     y, for which x       y and x(cid:62)(cid:62)y| z, see example(9.5). see also    g(9.11).

example 9.5 (skeleton orienting).

z

z

x

x

y

y

x       y|       

x       y| z    

z

z

x

x

y

y

if x is (unconditionally) independent of y, it must
be that z is a collider since otherwise marginalis-
ing over z would introduce a dependence between
x and y.

if x is independent of y conditioned on z, z must
not be a collider. any other orientation is ap-
propriate.

1this example appears in [162] and [224]     thanks also to seraf    n moral for his online notes.

218

draft november 9, 2017

structure learning

x

z

x

z

t

(a)

t

(j)

y

x

y

x

y

x

y

x

y

x

y

x

y

x

y

x

w

z

w

z

w

z

w

z

w

z

w

z

w

z

w

z

t

(b)

t

(c)

t

(d)

t

(e)

t

(f)

t

(g)

t

(h)

y

x

y

x

y

x

y

x

y

x

y

x

y

x

y

x

w

z

w

z

w

z

w

z

w

z

w

z

w

z

w

z

t

(k)

t

(l)

t

(m)

t

(n)

t

(o)

t

(p)

t

(q)

y

w

y

w

t

(i)

t

(r)

(a): the bn from which data is assumed generated and against which
figure 9.10: pc algorithm.
(c-l):
conditional independence tests will be performed.
in the    rst round (i = 0) all the pairwise mutual informations x        y|     are checked, and the link between
(m-o): i = 1. we now look at connected subsets
x and y removed if deemed independent (green line).
on three variables x, y, z of the remaining graph, removing the link x     y if x       y| z is true. not all steps
(p,q): i = 2. we now examine all x        y| {a, b}. the algorithm terminates after this round
are shown.
(r): final skeleton.
(when i gets incremented to 3) since there are no nodes with 3 or more neighbours.
during this process the sets sx,y =    , sx,w =    , sz,w = y, sx,t = {z, w} , sy,t = {z, w} were found. see also
demopcoracle.m

(b): the initial skeleton is fully connected.

algorithm 9.2 skeleton orientation algorithm (returns a dag).

1: unmarried collider: examine all undirected links x     z     y. if z (cid:54)    sxy set x     z     y.
2: repeat
3:
4:
5:
6: until no more edges can be oriented.
7: the remaining edges can be arbitrarily oriented provided that the graph remains a dag and no additional

x     z     y     x     z     y
for x     y, if there is a directed path from x to y orient x     y
if for x     z     y there is a w such that x     w, y     w, z     w then orient z     w

colliders are introduced.

example 9.6. in    g(9.10) we describe the processes of the pc algorithm in learning the structure for a
belief network on the variables x, y, z, w, t. in this case, rather than using data to assess independence, we
assume that we have access to an    oracle    that can correctly answer any independence question put to it.
in practice, of course, we will not be so fortunate! once the skeleton has been found, we then orient the
skeleton, as in    g(9.11).

9.5.2 empirical independence

mutual information test

given data we can obtain an estimate of the conditional mutual information by using the empirical distri-
bution p(x, y, z) estimated by simply counting occurrences in the data. in practice, however, we only have
a    nite amount of data to estimate the empirical distribution. this means that for data sampled from a
distribution for which the variables truly are independent, the empirical mutual information will neverthe-
less typically be greater than zero. an issue therefore is what threshold to use for the empirical conditional
mutual information to decide if this is su   ciently far from zero to be caused by dependence. a frequen-
tist approach is to compute the distribution of the conditional mutual information and then see where the

draft november 9, 2017

219

structure learning

sample value is compared to the distribution. according to [179], under the null hypothesis that the vari-
ables are independent, 2n mi(x; y|z) is chi-square distributed with (x     1)(y     1)z degrees of freedom with
dim (x) = x, dim (y) = y , dim (z) = z. this can then be used to form a hypothesis test; if the sample value
of the empirical mutual information is    signi   cantly    in the tails of the chi-square distribution, we deem that
the variables are conditionally dependent. this classical approach can work well for large amounts of data,
but is less e   ective in the case of small amounts of data. an alternative pragmatic approach is to estimate
the threshold based on empirical samples of the mi under controlled independent/dependent conditions    
see democondindepemp.m for a comparison of these approaches.

x

z

y

x

y

x

y

x

w

z

w

z

w

z

y

w

t

(a)

t

(b)

t

(c)

t

(d)

(a):
figure 9.11: skeleton orientation algorithm.
the skeleton along with sx,y =    , sx,w =    , sz,w =
y, sx,t = {z, w} , sy,t = {z, w}. (b): z (cid:54)    sx,y, so form
(d): final
collider.
partially oriented dag. the remaining edge may be
oriented as desired, without violating the dag condi-
tion. see also demopcoracle.m.

(c): t (cid:54)    sz,w, so form collider.

bayesian conditional independence test

a bayesian approach to testing for independence can be made by comparing the likelihood of the data under
the independence hypothesis, versus the likelihood under the dependent hypothesis. for the independence
hypothesis,    g(9.12a), we have a joint distribution over variables and parameters:

p(x, y, z,   |hindep) = p(x|z,   x|z)p(y|z,   y|z)p(z|  z)p(  x|z)p(  y|z)p(  z)

(9.5.1)

for categorical distributions, it is convenient to use a prior dirichlet (  |u) on the parameters   , assuming also
local as well as global parameter independence. for a set of assumed i.i.d. data (x ,y,z) = (xn, yn, zn) , n =
1, . . . , n , the likelihood is then given by integrating over the parameters   :

(cid:90)

(cid:89)

n

(cid:89)

p(x ,y,z|hindep) =

p(  |hindep)

  

p(xn, yn, zn|  ,hindep)

thanks to conjugacy, this is straightforward and gives the expression

p(x ,y,z|hindep) =

z(uz + (cid:93) (z))

z(ux|z + (cid:93) (x, z))

z(uy|z + (cid:93) (y, z))

z(uz)

z

z(ux|z)

z(uy|z)

@@

(9.5.2)

where ux|z is a hyperparameter matrix of pseudo counts for each state of x given each state of z. z(v) is
the normalisation constant of a dirichlet distribution with vector parameter v.

for the dependent hypothesis,    g(9.12b), we have

p(x, y, z,   |hdep) = p(x, y, z|  x,y,z)p(  x,y,z)

the likelihood is then

p(x ,y,z|hdep) =

z(ux,y,z + (cid:93) (x, y, z))

z(ux,y,z)

assuming each hypothesis is equally likely, for a bayes    factor

p(x ,y,z|hindep)
p(x ,y,z|hdep)

(9.5.3)

(9.5.4)

(9.5.5)

greater than 1, we assume that conditional independence holds; otherwise we assume the variables are con-
ditionally dependent. democondindepemp.m suggests that the bayesian hypothesis test tends to outperform
the conditional mutual information approach, particularly in the small sample size case, see    g(9.13).

220

draft november 9, 2017

structure learning

  z

zn

xn

yn

n

  x|z

  y|z

  x,y,z

xn, yn, zn

n

figure 9.12: bayesian conditional independence test
(a): a model
using dirichlet priors on the tables.
(b):
hindep for conditional independence x        y| z.
a model hdep for conditional dependence x(cid:62)(cid:62)y | z.
by computing the likelihood of the data under each
model, a numerical score for the validity of the con-
ditional independence assumption can be formed. see
democondindepemp.m.

(a)

(b)

x

z1

z2

y

figure 9.13: conditional independence test of x        y| z1, z2 with x, y, z1, z2 having
3, 2, 4, 2 states respectively. from the oracle belief network shown, in each experiment
the tables are drawn at random and 20 examples are sampled to form a dataset. for
each dataset a test is carried out to determine if x and y are independent conditioned
on z1, z2 (the correct answer being that they are independent). over 500 experiments,
the bayesian conditional independence test correctly states that the variables are con-
ditionally independent 74% of the time, compared with only 50% accuracy using the
chi-square mutual information test. see democondindepemp.m.

9.5.3 network scoring

p(v) = (cid:81)

an alternative to local methods such as the pc algorithm is to evaluate the whole network structure on
the set of variables v. that is we wish to ascertain how well a belief network with a particular structure
k p(vk|pa (vk))    ts the data. in a probabilistic context, given a model structure m , we wish to
compute p(m|d)     p(d|m )p(m ). some care is needed here since we have to    rst       t    each model with
parameters   , p(v|  , m ) to the data d. if we do this using maximum likelihood alone, with no constraints on
  , we will always end up favouring that model m with the most complex structure (assuming p(m ) = const.).
this can be remedied by using the bayesian technique

(cid:90)

p(d|m ) =

  

p(d|  , m )p(  |m )

(9.5.6)

in the case of directed networks, as we saw in section(9.4), the assumptions of local and global parameter
independence make the integrals tractable. for a discrete state network and dirichlet priors, we have p(d|m )
given explicitly by the bayesian dirichlet score equation (9.4.53). first we specify the hyperparameters
u(v; j), and then search over structures m , to    nd the one with the best score p(d|m ). the simplest
setting for the hyperparameters is set them all to unity[70]. another setting is the    uninformative prior   [56]

ui(v; j) =

  

dim (v) dim (pa (v))

(9.5.7)

where dim (x) is the number of states of the variable(s) x, giving rise to the bdeu score, for an    equivalent
sample size    parameter   . a discussion of these settings is given in [142] under the concept of likelihood
equivalence, namely that two networks which are markov equivalent should have the same score. how dense
the resulting network is can be sensitive to   [279, 267, 278]. including an explicit prior p(m ) on the networks
to favour those with sparse connections is also a sensible idea, for which one considers the modi   ed score
p(d|m )p(m ).
searching over structures is a computationally demanding task. however, since the log-score decomposes
into additive terms involving only the family of each variable v, we can compare two networks di   ering in a
single edge e   ciently since when we make an adjustment within a family, no other terms outside the family

draft november 9, 2017

221

structure learning

x1

x3

x6

x2

x4

x7

x8

x5

x1

x3

x6

x8

x2

x4

x7

x5

x5

x1

x3

x6

x2

x4

x7

x8

(a)

(b)

(c)

(a): the correct structure in which all
figure 9.14: learning the structure of a id110.
variables are binary. the ancestral order is x2, x1, x5, x4, x3, x8, x7, x6. the dataset is formed from 1000
(b): the learned structure based on the pc algorithm using the bayesian
samples from this network.
empirical conditional independence test. undirected edges may be oriented arbitrarily (provided the graph
(c): the learned structure based on the bayes dirichlet network scoring method. see
remains acyclic).
demopcdata.m and demobdscore.m.

are a   ected. this means that, given a candidate family, we can    nd which parents in this family should
connect to the child; this computation can be carried out for all families independently. to help    nd the
best families, search heuristics based on local addition/removal/reversal of edges [70, 142] that increase the
score are popular[142]. in learnbayesnet.m we simplify the problem for demonstration purposes in which
we assume we know the ancestral order of the variables, and also the maximal number of parents of each
variable. in practice, it is unlikely that a large number of parents will in   uence a variable; even if this were
the case, we would in general require an amount of data exponential in the number of parents to ascertain
this. one could in principle approach this by assuming parametric forms for such large tables, although this
is not common in practice.

example 9.7 (pc algorithm versus network scoring). in    g(9.14) we compare the pc algorithm with
network scoring based (with dirichlet hyperparameters set to unity) on 1000 samples from a known belief
network. the pc algorithm conditional independence test is based on the bayesian factor (9.5.5) in which
dirichlet priors with u = 0.1 were used throughout.

in the network scoring approach, in general there is no need to assume an ancestral ordering. however,
here we assume that we know the correct ancestral ordering, and also limit the number of parents of each
variable to be at most two.
in this case, we can then easily search over all possible graph structures,
choosing that with the highest posterior score. this proceeds by for example looking at the contribution of
variable x7 and its family. since, according to the given ancestral ordering, x1, x2, x3, x4, x5, x8 are possible
parents of x7, in principle we need to search over all the 26 parental con   gurations for this family. however,

since we assume that there are only maximally two parents, this reduces to 1 +(cid:0)6

(cid:1) = 22 parental

(cid:1) +(cid:0)6

1

2

con   gurations. we can perform this optimisation for the parental structure of variable x7 independently of
the parental structure of the other variables, thanks to the likelihood decomposable property of the network
score. similarly, we carry out this optimisation for the other variables separately, based on their possible
parents according to the ancestral order.

in    g(9.14) the network scoring technique outperforms the pc algorithm. this is partly explained by the
network scoring technique being provided with the correct ancestral order and the constraint that each
variable has maximally two parents.

222

draft november 9, 2017

d(cid:89)

i=1

structure learning

algorithm 9.3 chow-liu trees

end for

for j = 1 to d do

compute the mutual information for the pair of variables xi, xj: wij = mi(xi; xj).

1: for i = 1 to d do
2:
3:
4:
5: end for
6: for the undirected graph g with edge weights w,    nd a maximum weight undirected spanning tree t .
7: choose an arbitrary variable as the root node of the tree t .
8: form a directed tree by orienting all edges away from the root node.

9.5.4 chow-liu trees

consider a multivariate distribution p(x) that we wish to approximate with a distribution q(x). furthermore,
we constrain the approximation q(x) to be a belief network in which each node has at most one parent,
see    g(9.15). first we assume that we have chosen a particular labelling of the d variables so that children
have higher parent indices than their parents. the dag single parent constraint then means

d(cid:88)

(cid:10)log q(xi|xpa(i))(cid:11)

q(x) =

q(xi|xpa(i)),

pa(i) < i, or pa(i) =    

(9.5.8)

where pa(i) is the single parent index of node i. to    nd the best approximating distribution q in this
constrained class, we may minimise the id181

kl(p|q) = (cid:104)log p(x)(cid:105)p(x)    

since p(x) is    xed, the    rst term is constant. by adding a term(cid:10)log p(xi|xpa(i))(cid:11)

p(xi,xpa(i))

i=1

p(x) alone, we can write

(9.5.9)

p(xi,xpa(i)) that depends on

(cid:68)(cid:10)log q(xi|xpa(i))(cid:11)

d(cid:88)

i=1

(cid:10)log p(xi|xpa(i))(cid:11)

(cid:69)

kl(p|q) = const.    

p(xi|xpa(i))    

p(xi|xpa(i))

p(xpa(i))

(9.5.10)

this enables us to recognise that, up to a negligible constant, the overall id181 is a
positive sum of individual id181s so that the optimal setting is therefore

q(xi|xpa(i)) = p(xi|xpa(i))

(9.5.11)

plugging this solution into equation (9.5.9) and using log p(xi|xpa(i)) = log p(xi, xpa(i))     log p(xpa(i)) we
obtain

(cid:10)log p(xi, xpa(i))(cid:11)

d(cid:88)

i=1

(cid:10)log p(xpa(i))(cid:11)

d(cid:88)

i=1

kl(p|q) = const.    

p(xi,xpa(i)) +

p(xpa(i))

(9.5.12)

we still need to    nd the optimal parental structure pa(i) that minimises the above expression. if we add
and subtract an id178 term we can write

(cid:10)log p(xi, xpa(i))(cid:11)

d(cid:88)

i=1

(cid:10)log p(xpa(i))(cid:11)

d(cid:88)

i=1

d(cid:88)

i=1

kl(p|q) =    

p(xi,xpa(i)) +

p(xpa(i)) +

(cid:104)log p(xi)(cid:105)p(xi)

d(cid:88)

i=1

   

(cid:104)log p(xi)(cid:105)p(xi) + const.

(9.5.13)

x1

x2

x3

x4

figure 9.15: a chow-liu tree in which each variable xi has at most one
parent. the variables may be indexed such that 1     i     d.

draft november 9, 2017

223

structure learning

d(cid:88)

mi(cid:0)xi; xpa(i)

(cid:1)

for two variables xi and xj and distribution p(xi, xj), the mutual information de   nition(8.13) can be written
as

p(xi,xj )

(9.5.14)

which can be seen as the id181 kl(p(xi, xj)|p(xi)p(xj)) and is therefore non-negative.
using this, equation (9.5.13) is

(cid:28)

(cid:29)

mi(xi; xj) =

log

p(xi, xj)
p(xi)p(xj)

d(cid:88)

mi(cid:0)xi; xpa(i)

(cid:1)

d(cid:88)

i=1

kl(p|q) =    

since our task is to    nd the optimal parental indices pa(i), and the entropic term (cid:80)

i (cid:104)log p(xi)(cid:105)p(xi) of
the    xed distribution p(x) is independent of this mapping,    nding the optimal mapping is equivalent to
maximising the summed mutual informations

(cid:104)log p(xi)(cid:105)p(xi) + const.

(9.5.15)

   

i=1

(9.5.16)

i=1

under the constraint that pa(i) < i. since we also need to choose the optimal initial labelling of the variables
as well, the problem is equivalent to computing all the pairwise mutual informations

wij = mi(xi; xj)

(9.5.17)

and then    nding a maximal spanning tree for the graph with edge weights w (see spantree.m)[65]. once
found, we need to identify a directed tree with at most one parent. this is achieved by choosing any node
and then orienting edges consistently away from this node.

maximum likelihood chow-liu trees

if p(x) is the empirical distribution

n(cid:88)

n=1

   (x, xn)

p(x) =

1
n

then

(cid:88)

n

1
n

log q(xn)

kl(p|q) = const.    

(9.5.18)

(9.5.19)

hence the distribution q that minimises kl(p|q) is equivalent to that which maximises the likelihood of the
data. this means that if we use the mutual information found from the empirical distribution, with

p(xi = a, xj = b)     (cid:93) (xi = a, xj = b)

(9.5.20)

then the chow-liu tree produced corresponds to the maximum likelihood solution amongst all single-parent
trees. an outline of the procedure is given in algorithm(9.3). an e   cient algorithm for sparse data is also
available[206].

remark 9.1 (learning tree structured belief networks). the chow-liu algorithm pertains to the discus-
sion in section(9.5) on learning the structure of belief networks from data. under the special constraint that
each variable has at most one parent, the chow-liu algorithm returns the maximum likelihood structure to
   t the data.

224

draft november 9, 2017

maximum likelihood for undirected models

9.6 maximum likelihood for undirected models

consider a markov network p(x ) de   ned on (not necessarily maximal) cliques xc     x , c = 1, . . . , c with
clique parameters    = (  1, . . . ,   c)

  c(xc|  c)

(9.6.1)

(cid:89)

p(x|  ) =

1

z(  )

c

the term

(cid:88)

(cid:89)

z(  ) =

  c(xc|  c)

ensures normalisation, with the notation(cid:80)x indicating a summation over all states of the set of variables

x

c

(9.6.2)

x . given a set of data, {x n, n = 1, . . . , n}, and assuming i.i.d. data, the log likelihood is

l(  )=

log p(x n|  ) =

log   c(x n

c |  c)     n log z(  )

(9.6.3)

(cid:88)

(cid:88)

n

c

(cid:88)

n

our interest is to    nd the parameters that maximise the log likelihood l(  ). in general learning the optimal
parameters   c, c = 1, . . . , c is awkward since they are coupled via z(  ). unlike the bn, the objective
function does not split into a set of isolated parameter terms and in general we need to resort to numerical
methods. in special cases, however, exact results still apply, in particular when the mn is decomposable
and no constraints are placed on the form of the clique potentials, as we discuss in section(9.6.3). more
generally, however, gradient based techniques may be used and also give insight into properties of the
maximum likelihood solution.

9.6.1 the likelihood gradient

the gradient of the log likelihood with respect to a clique parameter   c is given by

(cid:28)    

(cid:29)

c |  c)     n

     c

log   c(xc|  c)

.

p(xc|  )

(9.6.4)

this is obtained by using the result

   
     c

log z(  ) =

1

z(  )

x

  c(xc|  c)

(cid:89)

c(cid:48)(cid:54)=c

  c(cid:48)(xc(cid:48)|  c(cid:48)) =

(cid:28)    

     c

(cid:29)

log   c(xc|  c)

.

p(xc|  )

(9.6.5)

log   c(x n
(cid:88)

   
     c

(cid:88)

n

   
     c

l(  ) =

   
     c

the gradient can then be used as part of a standard numerical optimisation package.

exponential form potentials

a common form of parameterisation is to use an exponential form

(cid:16)

(cid:17)

  c(xc) = exp

  t
c   c(xc)

where   c are the vector parameters and   c(xc) is a    xed    feature function    de   ned on the variables of clique
c. from equation (9.6.4) to    nd the gradient, we need

   
     c

log   c(xc|  c) =

   
     c

  t
c   c(xc) =   c(xc).

using this in equation (9.6.4), we    nd that the l(  ) has a zero derivative when

(cid:88)

n

1
n

  c(x n

c ) = (cid:104)  c(xc)(cid:105)p(xc) .

draft november 9, 2017

(9.6.6)

(9.6.7)

(9.6.8)

225

(9.6.9)

(9.6.10)

(9.6.12)

(9.6.13)

(9.6.14)

maximum likelihood for undirected models

hence the maximum likelihood solution satis   es that the empirical average of a feature function matches
the average of the feature function with respect to the model. by de   ning the empirical distribution on the
clique variables xc as

n(cid:88)

n=1

 (xc)    

1
n

i [xc = x n

c ]

we can write equation (9.6.8) more compactly as

(cid:104)  c(xc)(cid:105) (xc) = (cid:104)  c(xc)(cid:105)p(xc) .

an example of learning such an exponential form is given in example(9.8). we return to learning the
parameters of these models in section(9.6.4).

example 9.8 (id82 learning). we de   ne the bm as

1

1

2 vtwv,

e

z(w)

p(v|w) =

for symmetric w and binary variables dom(vi) = {0, 1}. given a set of training data, d =(cid:8)v1, . . . , vn(cid:9),

z(w) =

(9.6.11)

e

v

the log likelihood is

(cid:88)

1

2 vtwv

n(cid:88)

n=1

n(cid:88)

(cid:16)

n=1

l(w) =

1
2

(vn)t wvn     n log z(w)

di   erentiating w.r.t. wij, i (cid:54)= j and wii we have the gradients

   l
   wij

=

vn
i vn

j     (cid:104)vivj(cid:105)p(v|w)

,

   l
   wii

=

1
2

n(cid:88)

n=1

(cid:16)

(cid:17)

.

vn
i     (cid:104)vi(cid:105)p(v|w)

(cid:17)

a simple algorithm to optimise the weight matrix w is to use gradient ascent,

wnew

ij = wold

ij +   1

wnew

ii = wold

ii +   2

   l
   wij

,

   l
   wii

the second order statistics of the model (cid:104)vivj(cid:105)p(v|w) match those of the empirical distribution,(cid:80)

for learning rates   1,   2 > 0. the intuitive interpretation is that learning will stop (the gradient is zero) when
j /n .
bm learning however is di   cult since (cid:104)vivj(cid:105)p(v|w) is typically computationally intractable for an arbitrary
interaction matrix w and therefore needs to be approximated. indeed, one cannot compute the likelihood
l(w) exactly for a general matrix w so that monitoring performance is also di   cult.

n vn

i vn

9.6.2 general tabular clique potentials

for unconstrained clique potentials we have a separate table for each of the states de   ned on the clique. in
writing the log likelihood, it is convenient to use the identity

  c(x n

c ) =

i[yc=x n
c ]

  c(yc)

(9.6.15)

(cid:89)

yc

where the product is over all states of potential c. this expression follows since the indicator is zero for all
but the single observed state x n
i [yc = x n

c ] log   c(yc)     n log z(  )

c . the log likelihood is then

(cid:88)

(cid:88)

(cid:88)

(9.6.16)

l(  ) =

c

yc

n

226

draft november 9, 2017

maximum likelihood for undirected models

where

z(  ) =

(cid:88)

(cid:89)

yc

c

  c(yc)

di   erentiating the log likelihood with respect to a speci   c table entry   c(yc) we obtain

(cid:88)

n

   

     c(yc)

l(  ) =

i [yc = x n

c ]

1

  c(yc)     n

p(yc)
  c(yc)

(9.6.17)

(9.6.18)

equating to zero, and rewriting in terms of the variables x , the maximum likelihood solution is obtained
when

p(xc) =  (xc)

(9.6.19)

where the empirical distribution is de   ned in equation (9.6.9). that is, the unconstrained optimal maximum
likelihood solution is given by setting the clique potentials such that the marginal distribution on each clique
p(xc) matches the empirical distribution on each clique  (xc). note that this only describes the form that
the optimal maximum likelihood solution should take, and doesn   t give us a closed form expression for
setting the tables. to    nd the optimal tables in this case would still require a numerical procedure, such as
gradient based methods based on equation (9.6.18), or the ipf method described below.

iterative proportional    tting

according to the general result of equation (9.6.19) the maximum likelihood solution is such that the clique
marginals match the empirical marginals. assuming that we can absorb the normalisation constant into an
arbitrarily chosen clique, we can drop explicitly representing the normalisation constant. for a clique c, the
requirement that the marginal of p matches the empirical marginal on the variables in the clique is

  (xc)

  (xd) =  (xc)

(9.6.20)

(cid:88)

(cid:89)

x\c

d(cid:54)=c

given an initial setting for the potentials we can then update   (xc) to satisfy the above marginal requirement,

  new(xc) =

(cid:80)x\c

(cid:81)

 (xc)
d(cid:54)=c   (xd)

which is required for each of the states of xc. by multiplying and dividing the right hand side by   (xc) this
is equivalent to ascertaining if

  new(xc) =

  (xc) (xc)

p(xc)

(9.6.22)

this is a so-called iterative proportional fitting (ipf) update and corresponds to a coordinate-wise optimi-
sation of the log likelihood in which the coordinate corresponds to   c(xc), with all other parameters    xed.
in this case this conditional optimum is analytically given by the above setting. one proceeds by selecting
another potential to update, and continues updating until some convergence criterion is met. note that in
general, with each update, the marginal p(xc) needs to be recomputed; computing these marginals may be
expensive unless the width of the junction tree formed from the graph is suitably limited.

9.6.3 decomposable markov networks

whilst for general markov networks we require numerical methods to    nd the maximum likelihood solution,
there is an important special case for which we can    nd the optimal tables very easily. if the mn corre-
sponding is decomposable, then we know (from the junction tree representation) that we can express the
distribution in the form of a product of local marginals divided by the separator distributions

(9.6.21)

(9.6.23)

227

(cid:81)
(cid:81)

p(x ) =

c p(xc)
s p(xs)

draft november 9, 2017

maximum likelihood for undirected models

   (x1, x2)

   (x2)

   (x2, x3, x5)

   (x5)

   (x5, x6)

x1

x2

x3

x4

(a)

x5

x6

   (x2, x5)

   (x2, x4, x5)

(b)

x1

x2, x3, x5

x6

x4

(c)

(a): a decomposable markov network.

(c): set chain for
figure 9.16:
(a) formed by choosing clique x2, x3, x5 as root and orienting edges consistently away from the root. each
separator is absorbed into its child clique to form the set chain.

(b): a junction tree for (a).

algorithm 9.4 learning of an unconstrained decomposable markov network using maximum likelihood.
we have a triangulated (decomposable) markov network on cliques   c(xc), c = 1, . . . , c and the empirical
marginal distributions on all cliques and separators,  (xc),  (xs)
1: form a junction tree from the cliques.
2: initialise each clique   c(xc) to  (xc) and each separator   s(xs) to  (xs).
3: choose a root clique on the junction tree and orient edges consistently away from this root.
4: for this oriented junction tree, divide each clique by its parent separator.
5: return the new potentials on each clique as the maximum likelihood solution.

by reabsorbing the separators into the numerator terms, we can form a set chain distribution, section(6.8)

p(x ) =

p(xc|x\c)

(9.6.24)

(cid:89)

c

since this is directed, and provided no constraint is placed on the tables, the maximum likelihood solution
to learning the tables is given by assigning each set chain factor p(xc|x\c) based on counting the instances
in the dataset[183], see learnmarkovdecom.m. the procedure is perhaps best explained by an example, as
given below. see algorithm(9.4) for a general description.

example 9.9. given a dataset {x n, n = 1, . . . , n}, with corresponding empirical distribution  (x ), we
wish to    t by maximum likelihood a mn of the form

p(x1, . . . , x6) =

1
z

  (x1, x2)  (x2, x3, x5)  (x2, x4, x5)  (x5, x6)

(9.6.25)

where the potentials are unconstrained tables, see    g(9.16a). since the graph is decomposable, we know it
admits a factorisation of clique potentials divided by the separators:

p(x1, . . . , x6) =

p(x1, x2)p(x2, x3, x5)p(x2, x4, x5)p(x5, x6)

p(x2)p(x2, x5)p(x5)

(9.6.26)

we can convert this to a set chain by reabsorbing the denominators into numerator terms, see section(6.8).
for example, by choosing the clique x2, x3, x5 as root, we can write

p(x1, . . . , x6) = p(x1|x2)

p(x2, x3, x5)

p(x4|x2, x5)

(cid:124) (cid:123)(cid:122) (cid:125)

  (x1,x2)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

  (x2,x3,x5)

  (x2,x4,x5)

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

p(x6|x5)

  (x5,x6)

(9.6.27)

(cid:88)

n

where we identi   ed the factors with clique potentials, and the normalisation constant z is unity, see
   g(9.16b). the advantage is that in this representation, the clique potentials are independent since the
distribution is a bn on cluster variables. the log likelihood for an i.i.d. dataset is

l =

228

log p(xn

1|xn

2 ) + log p(xn

2 , xn

3 , xn

5 ) + log p(xn

2 , xn

5 ) + log p(xn

4|xn

6|xn
5 )

(9.6.28)

draft november 9, 2017

maximum likelihood for undirected models

x3

x2

x4

x1

(a)

x5

  4,5  1,5

x1,4

  1,2  1,4

x2,4

  2,3  2,4  3,4

(b)

(a):

9.17:

interpreted as

the dis-
figure
represents
tribution   (x1, x4, x5)  (x1, x2, x4)  (x2, x4, x3).
a
(b): a junction tree for the pairwise
  (x4, x5)  (x1, x4)  (x4, x5)  (x1, x2)  (x2, x4)  (x2, x3)  (x3, x4).
mn in (a). we have a choice where to place the pairwise cliques, and this is one valid choice, using the
shorthand   a,b =   a,b(xa, xb) and xa,b = {xa, xb}.

a markov network,

graph represents

pairwise mn,

graph

the

the

as

where each of the terms is an independent parameter of the model. the maximum likelihood solution then
corresponds (as for the bn case) to simply setting each factor to the empirical distribution

  (x1, x2) =  (x1|x2),   (x2, x3, x5) =  (x2, x3, x5),   (x2, x4, x5) =  (x4|x2, x5),   (x5, x6) =  (x6|x5)
(9.6.29)

constrained decomposable markov networks

if there are no constraints on the forms of the maximal clique potentials of the markov network, as we   ve
seen, learning is straightforward. here our interest is when the functional form of the maximal clique is
constrained to be a product of potentials on smaller cliques2:

  c(xc) =

  i
c(x i
c )

(9.6.30)

(cid:89)

i

with no constraint being placed on the non-maximal clique potentials   i
cannot write down directly the maximum likelihood solution for the non-maximal clique potentials   i

c(x i

c ). in general, in this case one
c(x i
c ).

consider the graph in    g(9.17) which we consider a pairwise mn. in this case, the clique potentials are con-
strained, so that we cannot simply write down the solution, as we did in the unconstrained decomposable
case. since the graph is decomposable, there are however, computational savings that can be made in this
case[11]. for an empirical distribution  , maximum likelihood requires that all the pairwise marginals of the
mn match the corresponding marginals obtained from  . as explained in    g(9.17) we have a choice as to
which junction tree clique each potential is assigned to, with one valid choice being given in    g(9.17b).

let   s consider updating the potentials within the 1, 2, 4 clique. keeping the potentials of the other cliques
  4,5  1,5 and   2,3  2,4  3,4    xed we can update the potentials   1,2,   1,4. using a bar to denote    xed potentials,
the marginal requirement that the mn marginal p(x1, x2, x4) matches the empirical marginal  (x1, x2, x4)
can be written in shorthand as

p1,2,4 =  1,2,4

(9.6.31)

we can express this requirement in terms of the pairs of variables x1, x2 and x1, x4 within the   1,2  1,4 clique
as

p1,2 =  1,2,

p1,4 =  1,4

(9.6.32)

2a id82 is of this form since any unconstrained binary pairwise potentials can be converted into a bm. for

other cases in which the   i

c are constrained, then iterative scaling may be used in place of ipf.

draft november 9, 2017

229

algorithm 9.5 e   cient iterative proportional fitting. given a set of   i, i = 1, . . . , i and a corresponding
set of reference (empirical) marginal distributions on the variables of each potential,  i, we aim to set all   
such that all marginals of the markov network match the given empirical marginals.

maximum likelihood for undirected models

1: given a markov network on potentials   i, i = 1, . . . , i, triangulate the graph and form the cliques c1, . . . ,cc.
2: assign potentials to cliques. thus each clique has a set of associated potentials fc
3: initialise all potentials (for example to unity).
4: repeat
5:
6:
7:
8:
9:
10:
11: until all markov network marginals converge to the reference marginals.

choose a clique c as root.
propagate messages towards the root and compute the separators on the boundary of the root.
repeat

choose a potential   i in clique c, i     fc.
perform an ipf update for   i, given    xed boundary separators and other potentials in c.

until potentials in clique c converge.

taking the    rst marginal requirement, this means

p1,2 =

    1,5     4,5  1,4  1,2     2,4     2,3     3,4 =  1,2

(cid:88)

x3,x4,x5

which can be expressed as

(cid:88)

x4

(cid:32)(cid:88)
(cid:124)

x5

    1,5     4,5

(cid:123)(cid:122)

  1,4

  1,4  1,2

(cid:32)(cid:88)
(cid:124)

x3

(cid:33)
(cid:125)

=  1,2

    2,4     2,3     3,4

(cid:123)(cid:122)

  2,4

  new
1,2 =

 1,2

x4   1,4  1,4  2,4

(cid:80)
(cid:32)(cid:88)
(cid:124)

x5

(cid:123)(cid:122)

  1,4

(cid:88)

x2

so that

(cid:80)

 1,4

x2   1,4  1,2  2,4

  new
1,4 =

    1,5     4,5

  1,4  1,2

=  1,4

(cid:32)(cid:88)
(cid:124)

x3

(cid:33)
(cid:125)

    2,4     2,3     3,4

(cid:123)(cid:122)

  2,4

(cid:33)
(cid:125)

(cid:33)
(cid:125)

the    messages      1,4 and   1,2 are the boundary separator tables when we choose the central clique as root
and carry out absorption towards the root. given these    xed messages we can then perform ipf updates
of the root clique using

after making this update, we can subsequently update   1,4 similarly using the constraint

(9.6.33)

(9.6.34)

(9.6.35)

(9.6.36)

(9.6.37)

we then iterate these updates until convergence within this 1, 2, 4 clique. given converged updates for this
clique, we can choose another clique as root, propagate towards the root and compute the separator cliques
on the boundary of the root. given these    xed boundary clique potentials we again perform ipf within the
clique.

this    e   cient    ipf procedure is described more generally in algorithm(9.5) for an empirical distribution  .
more generally, ipf minimises the id181 between a given reference distribution   and
the markov network. see demoipfeff.m and ipf.m.

230

draft november 9, 2017

maximum likelihood for undirected models

figure 9.18: learning digits (from simon lucas    algoval system) using a markov network. top row: the
36 training examples. each example is a binary image on 18    14 pixels. second row: the training data
with 50% missing pixels (grey represents a missing pixel). third row: reconstructions from the missing
data using a thin-junction-tree mn with maximum clique size 15. bottom row: reconstructions using a
thin-junction-tree id82 with maximum clique size 15, trained using e   cient ipf.

(a): based on the pair-
figure 9.19:
wise empirical entropies h(xi, xj) edges are
ordered, high id178 edges    rst. shown
is the adjacency matrix of the resulting
markov network whose junction tree has
cliques     15 in size (white represents an
(b): indicated are the number of
edge).
cliques that each pixel is a member of, in-
dicating a degree of importance. note that
the lowest clique membership value is 1, so
that each pixel is a member of at least one
clique.

(a)

(b)

example 9.10 (learning with a structured markov network). in this example we aim to    t a markov
network to data, constrained so that id136 in the markov network is computationally cheap by ensuring
that the junction tree of the markov network has limited clique sizes.

in    g(9.18) 36 examples of 18    14 = 252 binary pixel handwritten twos are presented,
forming
the training set from which we wish to    t a markov network. first all pairwise empirical entropies
h(xi, xj), i, j = 1, . . . , 252 were computed and used to rank edges, with highest id178 edges ranked    rst.
edges were included in a graph g, highest ranked    rst, provided the triangulated g had all cliques less than
size 15. this resulted in 238 unique cliques and an adjacency matrix for the triangulated g as presented in
   g(9.19a). in    g(9.19b) the number of times that a pixel appears in the 238 cliques is shown, and indicates
the degree of importance of each pixel in distinguishing between the 36 examples. two models were then
trained and used to compute the most likely reconstruction based on missing data p(xmissing|xvisible).
the    rst model was a markov network on the maximal cliques of the graph, for which essentially no training
is required, and the settings for each clique potential can be obtained as explained in algorithm(9.4).
the model makes 3.8% errors in reconstruction of the missing pixels. note that the unfortunate e   ect of
reconstructing a white pixel surrounded by black pixels is an e   ect of the limited training data. with larger
amounts of data the model would recognise that such e   ects do not occur.

in the second model, the same maximal cliques were used, but the maximal clique potentials restricted to be
the product of all pairwise two-cliques within the maximal clique. this is equivalent to using a structured
id82, and was trained using the e   cient ipf approach of algorithm(9.5). the corresponding
reconstruction error is 20%. this performance is worse than the    rst model since the id82 is a
more constrained markov network and struggles to represent the data well. see demolearnthinmndigit.m.

draft november 9, 2017

231

5010015020025050100150200250  24681012142468101214161820406080100120140160180200220(cid:89)

p(x|  ) =

1

z(  )

c

(cid:88)

(cid:89)

x

c

9.6.4 exponential form potentials

for exponential form potentials

(cid:16)

(cid:17)

  c(xc) = exp

  t
c   c(xc)

maximum likelihood for undirected models

(9.6.38)

we saw in section(9.6.1) how to compute the derivatives for use in standard numerical optimisation proce-
dures. in the following section we outline another popular numerical technique.

iterative scaling

we consider markov networks of the exponential form

e  cfc(xc)

(9.6.39)

where the    feature functions    fc(xc)     0 and c ranges of the non-maximal cliques xc     x (note that
equation (9.6.38) can be written in this form by having multiple potentials within the same clique.). the
normalisation requirement is

z(  ) =

exp (  cfc(xc))

(9.6.40)

a maximum likelihood training algorithm for a markov network, somewhat analogous to the em approach
of section(11.2) can be derived as follows[33]. consider the bound, for positive x:

log x     x     1         log x     1     x

hence

    log

z(  )
z(  old)     1    

z(  )
z(  old)         log z(  )         log z(  old) + 1    

z(  )
z(  old)

then we can write a bound on the log likelihood

1
n

l(  )    

1
n

  cfc(x n

c )     log z(  old) + 1    

z(  )
z(  old)

(cid:88)

c,n

(9.6.41)

(9.6.42)

(9.6.43)

as it stands, the bound (9.6.43) is in general not straightforward to optimise since the parameters of each
potential are coupled through the z(  ) term. for convenience it is useful to    rst reparmameterise and write

+  old

c

(9.6.44)

one can decouple this using an additional bound derived by    rst considering:

(cid:33)

fc(xc)  c

=

(cid:32)(cid:88)

(cid:88)
(cid:34)

x

(cid:32)(cid:88)

c

exp

(cid:88)

c

fc(xc)  old
(cid:35)(cid:33)

  cfc(xc)

c

= exp

pc

  c

c

d

then

c

  c

(cid:124)

(cid:123)(cid:122)
(cid:125)
  c =   c       old
(cid:88)
(cid:32)(cid:88)

z(  ) =

x

exp

exp

(cid:32)(cid:88)
(cid:33)

c

where

pc    

232

(cid:80)

fc(xc)
d fd(xd)

fd(xd)

(9.6.46)

(9.6.47)

draft november 9, 2017

(cid:33)

exp

(cid:32)(cid:88)

c

(cid:33)

fc(xc)  c

(9.6.45)

c

(cid:32)(cid:88)
          1
(cid:124)

n

n

(cid:88)

n

(cid:88)

n

maximum likelihood for undirected models

since pc     0 and(cid:80)
(cid:32)(cid:88)

exp

hence

z(  )    

(cid:88)

x

exp

(cid:88)

c

1
n

l(  )    

  cfc(xc)

   

c

pc exp

fd(xd)  c

c pc = 1 we may apply jensen   s inequality to give

(cid:33)

(cid:88)

c

(cid:32)(cid:88)
(cid:33)(cid:88)

d

pcexp

c

fc(xc)  old
(cid:88)

c

(cid:42)

fc(x n

c )  c    

pcexp

(cid:33)

(cid:88)

f

        c
(cid:32)
(cid:88)
(cid:123)(cid:122)
(cid:32)(cid:16)

  c

d

lb(  c)

fd(xc)

      
(cid:33)(cid:43)

fd(xc)

p(x|  old)

(cid:17)(cid:88)

(cid:33)(cid:43)

(cid:42)

plugging this bound into (9.6.43) we have

(9.6.48)

(9.6.49)

         
(cid:125)

+1     log z(  old)

(9.6.50)

the term in curly brackets contains the potential parameters   c in an uncoupled fashion. di   erentiating
with respect to   c the gradient of each lower bound is given by

   lb(  c)

     c

=

1
n

fc(x n

c )    

fc(xc)exp

  c       old

c

fd(xd)

p(x|  old)

(9.6.51)

d

intuitively,
this can be used as part of a gradient based optimisation procedure to learn the parameters   c.
the parameters converge when the empirical average of the functions f match the average of the functions
with respect to samples drawn from the distribution, in line with our general condition for maximum likeli-
hood optimal solution.

approach in section(9.6.1) [212]. however, in the special case that the functions sum to 1, (cid:80)

in general, there would appear to be little advantage in using the above procedure over the general gradient
c fc(xc) = 1,

the zero of the gradient (9.6.51) can be found analytically, giving the iterative scaling (is) update

  c =   old

c + log

1
n

fc(x n

c )     log (cid:104)fc(xc)(cid:105)p(xc|  old)

(9.6.52)

the constraint that the features fc need to be non-negative can be relaxed at the expense of additional
variational parameters, see exercise(9.12).

if the junction tree formed from this exponential form markov network has limited tree width, computational
savings can be made by performing ipf over the cliques of the junction tree and updating the parameters
   within each clique using is[11]. this is a modi   ed version of the constrained decomposable case. see also
[291] for a uni   ed treatment of propagation and scaling on junction trees.

9.6.5 conditional random    elds

for an input x and output y, a crf is de   ned by a conditional distribution [283, 181]

  k(y, x)

(9.6.53)

(cid:89)

p(y|x) =

1

z(x)

k

for (positive) potentials   k(y, x). to make learning more straightforward, the potentials are usually de   ned
as exp (  kfk(y, x)) for    xed functions f (y, x) and parameters   k. in this case the distribution of the output
conditioned on the input is

p(y|x,   ) =

1

z(x,   )

k

draft november 9, 2017

exp (  kfk(y, x))

(9.6.54)

233

(cid:89)

n(cid:88)

(cid:88)

n=1

k

n(cid:88)

n=1

(cid:88)

(cid:16)

n

maximum likelihood for undirected models

crfs can also be viewed simply as markov networks with exponential form potentials, as in section(9.6.4).
equation(9.6.54) is equivalent to equation (9.6.39) where the parameters    are here denoted by    and the
variables x are here denoted by y. in the crf case the inputs x simply have the e   ect of determining the
feature fk(y, x).

for an i.i.d. dataset of input-outputs, d = {(xn, yn), n = 1, . . . , n}, training based on conditional maximum
likelihood requires the maximisation of

(cid:88)

n

   2

     i     j

l =

l(  )    

log p(yn|xn,   ) =

  kfk(yn, xn)     log z(xn,   )

(9.6.55)

in general no closed form solution for the optimal    exists and this needs to be determined numerically.
the methods we   ve discussed, such as iterative scaling may be readily adapted to this optimisation problem,
although in practice gradient based techniques are to be preferred[212]. for completeness we describe
gradient based training. the gradient has components

   
     i

l =

fi(yn, xn)     (cid:104)fi(y, xn)(cid:105)p(y|xn,  )

(cid:17)

(9.6.56)

the terms (cid:104)fi(y, xn)(cid:105)p(y|xn,  ) can be problematic and their tractability depends on the structure of the poten-
tials. for a multivariate y, provided the structure of the cliques de   ned on subsets of y is singly-connected,
then computing the average is generally tractable. more generally, provided the cliques of the resulting
junction tree have limited width, then exact marginals are available. an example of this is given for a
linear-chain crf in section(23.4.3)     see also example(9.11) below.

another quantity often useful for numerical optimisation is the hessian which has components

((cid:104)fi(y, xn)(cid:105)(cid:104)fj(y, xn)(cid:105)     (cid:104)fi(y, xn)fj(y, xn)(cid:105))

(9.6.57)

where the averages above are with respect to p(y|xn,   ). this expression is a (negated) sum of covariance
elements, and is therefore negative (semi) de   nite. hence the function l(  ) is concave and has only a
single global optimum. in practice crfs often have many thousands if not millions of parameters    so that
computing the newton update associated with the inverse of the hessian may be prohibitively expensive.
in this case alternative optimisation methods such as conjugate gradients are preferable.

in practice regularisation terms are often added to prevent over   tting (see section(13.2.2) for a discussion
of regularisation) . using a term

c2
k  2
k

(9.6.58)

(cid:88)

k

   

for positive regularisation constants c2
negative de   nite and hence the overall objective function remains concave.
once trained a crf can be used for predicting the output distribution for a novel input x   . the most likely
output y    is equivalently given by

k discourages the weights    from being too large. this term is also

   

y

= argmax

y

log p(y|x

   

) = argmax

y

   

  kfk(y, x

)     log z(x

   

,   )

(cid:88)

k

since the normalisation term is independent of y,    nding the most likely output is equivalent to

(9.6.59)

(9.6.60)

(cid:88)

   
  kfk(y, x

)

   

y

= argmax

y

234

k

draft november 9, 2017

maximum likelihood for undirected models

(a)

(b)

figure 9.20: (a): training results for a linear chain crf. there are 5 training sequences, one per subpanel.
in each subpanel the top row corresponds to the input sequence x1:20, xt     {1, . . . , 5} (each state represented
by a di   erent colour). the middle row is the correct output sequence y1:20, yt     {1, 2, 3} (each state
represented by a di   erent colour). together the input and output sequences make the training data d. the
bottom row contains the most likely output sequence given the trained crf, arg maxy1:20 p(y1:20|x1:20,d).
(b): five additional test input sequences (top), correct output sequence (middle) and predicted output
sequence (bottom).

natural language processing

in a natural language processing application, xt might represent a word and yt a corresponding linguistic
tag (   noun   ,   verb   , etc. ). a more suitable form in this case is to constrain the crf to be of the form

(cid:32)(cid:88)

(cid:88)

(cid:33)

exp

  kgk(yt, yt   1) +

  lhl(yt, xt)

(9.6.61)

k

l

for binary functions gk and hl and parameters   k and   l. the grammatical structure of tag-tag transitions
is encoded in gk(yt, yt   1) and linguistic tag information in hk(yt, xt), with the importance of these being
determined by the corresponding parameters[181]. in this case id136 of the marginals (cid:104)ytyt   1|x1:t(cid:105) is
straightforward since the factor graph corresponding to the id136 problem is a linear chain.

variants of the linear chain crf are used heavily in natural language processing, including part-of-speech
tagging and machine translation (in which the input sequence x represents a sentence say in english and
the output sequence y the corresponding translation into french). see, for example, [230].

example 9.11 (linear chain crf). we consider a crf with x = 5 input states and y = 3 output states
of the form

t(cid:89)

t=2

e

p(y1:t|x1:t ) =

(cid:80)

k   kgk(yt,yt   1)+(cid:80)

l   lhl(yt,xt)

(9.6.62)

here the binary functions gk(yt, yt   1) = i [yt = ak] i [yt   1 = bk], k = 1, . . . , 9 , ak     {1, 2, 3}, bk    
{1, 2, 3}, simply index the transitions between two consecutive outputs. the binary functions hl(yt, xt) =
i [yt = al] i [xt = cl], l = 1, . . . , 15, al     {1, 2, 3}, cl     {1, 2, 3, 4, 5}, index the translation of the input to the
output. there are therefore 9 + 15 = 24 parameters in total. in    g(9.20) we plot the training and test
results based on a small set of data. as we see, the model learns to predict the output well, both for the
training and test data. the training of the crf is obtained using 50 iterations of gradient ascent with a
learning rate of 0.1. see demolinearcrf.m.

draft november 9, 2017

235

2468101214161820123246810121416182012324681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123(cid:89)

c

p(x|  ) =

1
z

n(cid:88)

d(cid:88)

n=1

i=1

(cid:48)
l

(  ) =

9.6.6 pseudo likelihood

maximum likelihood for undirected models

consider a mn on variables x with dim (x) = d of the form

  c(xc|  c)

(9.6.63)

for all but specially constrained   c, the partition function z will be intractable and the likelihood of a set
of i.i.d. data intractable as well. a surrogate is to use the pseudo likelihood of each variable conditioned on
all other variables (which is equivalent to conditioning on only the variable   s neighbours for a mn)

log p(xn

i |xn\i|  )

(9.6.64)

the terms p(xn
i |xn\i|  ) are usually straightforward to work out since they require    nding the normalisation
of a univariate distribution only. in this case the gradient can be computed exactly, and learning of the
parameters    carried out. in general, the solution found does not correspond to the maximum likelihood
solution. however, at least for some special cases, such as the id82, this forms a consistent
estimator[153].

9.6.7 learning the structure

learning the structure of a markov network can also be based on independence tests, as for belief networks
in section(9.5). a criterion for    nding a mn on a set of nodes x is to use the fact that no edge exits between
x and y if, conditioned on all other nodes, x and y are deemed independent. this is the pairwise markov
property described in section(4.2.1). by checking x       y|x\{x, y} for every pair of variables x and y, this
edge deletion approach in principle reveals the structure of the network[237]. for learning the structure
from an oracle, this method is sound. however, a practical di   culty in the case where the independencies
are determined from data is that checking if x        y| x\{x, y} requires in principle enormous amounts of
data. the reason for this is that the conditioning selects only those parts of the dataset consistent with
the conditioning. in practice this will result in very small numbers of remaining datapoints, and estimating
independencies on this basis is unreliable.

the markov boundary criterion[237] uses the local markov property, section(4.2.1), namely that conditioned
on its neighbours, a variable is independent of all other variables in the graph. by starting with a variable x
and an empty neighbourhood set, one can progressively include neighbours, testing if their inclusion renders
the remaining non-neighbours independent of x. a di   cultly with this is that, if one doesn   t have the correct
markov boundary, then including a variable in the neighbourhood set may be deemed necessary. to see
this, consider a network which corresponds to a linear chain and that x is at the edge of the chain. in this
case, only the nearest neighbour of x is in the markov boundary of x. however, if this nearest neighbour
were not currently in the set, then any other non-nearest neighbour would be included, even though this is
not strictly required. to counter this, the neighbourhood variables included in the neighbourhood of x may
be later removed if they are deemed super   uous to the boundary[113].

in cases where speci   c constraints are imposed, such as learning structures whose resulting triangulation
has a bounded tree-width, whilst still formally di   cult, approximate procedures are available[276].

in terms of network scoring methods for undirected networks, computing a score is hampered by the fact
that the parameters of each clique become coupled in the normalisation constant of the distribution. this
issue can be addressed using hyper markov priors[80].

9.7 summary

    for discrete belief networks, it is particularly convenient to use a dirichlet parameter prior since this is

conjugate to the categorical distribution.

236

draft november 9, 2017

code

    provided we assume local and global parameter independence, the posterior over belief network tables

factorises.

    learning the structure of a belief network is more complex. the pc algorithm uses local independence
tests to decide if two variables should be linked. a global alternative is based on using a network scoring
method such as the model likelihood of a network structure under a dirichlet prior.

    learning the maximum likelihood parameters of a decomposable markov network is straightforward and

can be achieved by counting.

    for non-decomposable markov networks, no closed form solution exists. the maximum likelihood criterion
is equivalent to ensuring that clique marginals match empirical marginals. the iterative proportional    tting
algorithm is a technique to set the tables to ensure these marginals match.

    for markov networks parameterised using feature functions, iterative scaling is a maximum likelihood
technique that enables individual parameter updates to be made. gradient based approaches are also
straightforward and popular in conditional random    elds.

9.8 code

condindepemp.m: bayes test and mutual information for empirical conditional independence
condmi.m: conditional mutual information
condmiemp.m: conditional mutual information of empirical distribution
miemp.m: mutual information of empirical distribution

9.8.1 pc algorithm using an oracle

this demo uses an oracle to determine x       y| z, rather than using data to determine the empirical depen-
dence. the oracle is itself a belief network. for the partial orientation only the    rst    unmarried collider   
rule is implemented.
demopcoracle.m: demo of pc algorithm with an oracle
pcskeletonoracle.m: pc algorithm using an oracle
pcorient.m: orient a skeleton

9.8.2 demo of empirical conditional independence

for half of the experiments, the data is drawn from a distribution for which x       y| z is true. for the other
half of the experiments, the data is drawn from a random distribution for which x       y| z is false. we then
measure the fraction of experiments for which the bayes test correctly decides x       y| z. we also measure the
fraction of experiments for which the mutual information test correctly decides x       y| z, based on setting
the threshold equal to the median of all the empirical conditional mutual information values. a similar
empirical threshold can also be obtained for the bayes    factor (although this is not strictly kosher in the
pure bayesian spirit since one should in principle set the threshold to zero). the test based on the assumed
chi-squared distributed mi is included for comparison, although it seems to be impractical in these small
data cases.
democondindepemp.m: demo of empirical conditional independence based on data

9.8.3 bayes dirichlet structure learning

it is interesting to compare the result of demopcdata.m with demobdscore.m.
pcskeletondata.m: pc algorithm using empirical conditional independence
demopcdata.m: demo of pc algorithm with data
bdscore.m: bayes dirichlet (bd) score for a node given parents
learnbayesnet.m: given an ancestral order and maximal parents, learn the network
demobdscore.m: demo of structure learning

draft november 9, 2017

237

fuse

drum

toner

paper

roller

burning

quality

wrinkled

mult. pages

paper jam

exercises

figure 9.21: printer nightmare belief network. all variables are binary. the upper variables without parents
are possible problems (diagnoses), and the lower variables consequences of problems (faults).

9.9 exercises

exercise 9.1 (printer nightmare). cheapco is, quite honestly, a pain in the neck. not only did they buy
a dodgy old laser printer from stoppress and use it mercilessly, but try to get away with using substandard
components and materials. unfortunately for stoppress, they have a contract to maintain cheapco   s old
warhorse, and end up frequently sending the mechanic out to repair the printer. they decide to make a
statistical model of cheapco   s printer, so that they will have a reasonable idea of the fault based only on the
information that cheapco   s secretary tells them on the phone. in that way, stoppress hopes to be able to
send out to cheapco only a junior repair mechanic, having most likely diagnosed the fault over the phone.

based on the manufacturer   s information, stoppress has a good idea of the dependencies in the printer, and
what is likely to directly a   ect other printer components. the belief network in    g(9.21) represents these
assumptions. however, the speci   c way that cheapco abuse their printer is a mystery, so that the exact
probabilistic relationships between the faults and problems is idiosyncratic to cheapco. stoppress has the
following table of faults in which each column represents a visit.

fuse assembly malfunction

drum unit
toner out

poor paper quality

worn roller

burning smell

poor print quality

wrinkled pages

multiple pages fed

paper jam

0
0
1
1
0
0
1
0
0
0

0
0
1
0
0
0
1
0
0
0

0
0
0
1
0
0
1
1
1
1

1
0
0
0
0
1
0
0
0
1

0
1
0
1
0
0
1
0
0
0

0
0
1
0
0
0
1
0
0
0

0
0
0
1
1
0
0
0
1
1

0
1
1
0
0
0
1
0
0
1

0
0
0
1
0
0
0
1
1
1

0
0
0
1
0
0
0
0
0
1

0
1
0
0
0
0
1
0
0
0

0
1
1
1
0
0
1
0
0
0

1
0
0
1
0
1
0
1
0
0

0
0
0
0
1
0
0
1
0
1

1
0
0
0
1
0
0
1
1
0

1. the above table is contained in printer.mat. learn all table entries on the basis of maximum likeli-

hood.

2. program the belief network using the tables maximum likelihood tables and brmltoolbox. compute
the id203 that there is a fuse assembly malfunction given that the secretary complains there is a
burning smell and that the paper is jammed, and that there are no other problems.

3. repeat the above calculation using a bayesian method in which a    at beta prior is used on all tables.

continue to use these tables for the remainder of this question.

++

4. given the above information from the secretary, what is the most likely joint diagnosis over the diag-
nostic variables     that is the joint most likely p(f use, drum, t oner, p aper, roller|evidence)? use the
max-absorption method on the associated junction tree.

5. compute the joint most likely state of the distribution

p(f use, drum, t oner, p aper, roller|burning smell, paper jammed)

explain how to compute this e   ciently using the max-absorption method.

exercise 9.2. consider data xn, n = 1, . . . , n . show that for a gaussian distribution, the maximum
likelihood estimator of the mean is   m = 1
n

n=1 xn and variance is     2 = 1
n

238

draft november 9, 2017

(cid:80)n
n=1(xn       m)2.

(cid:80)n

exercises

exercise 9.3. a training set consists of one dimensional examples from two classes. the training examples
from class 1 are

0.5, 0.1, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.35, 0.25

and from class 2 are

0.9, 0.8, 0.75, 1.0

(9.9.1)

(9.9.2)

fit a (one dimensional) gaussian using maximum likelihood to each of these two classes. also estimate the
class probabilities p1 and p2 using maximum likelihood. what is the id203 that the test point x = 0.6
belongs to class 1?

exercise 9.4. for a set of n observations (training data), x =(cid:8)x1, . . . , xn(cid:9), and independently gathered

observations, the log likelihood for a belief network to generate x is

representing the id203 that variable xi is in state s given the parents of variable xi are in the vector of
states t. using a lagrangian

n(cid:88)

k(cid:88)

n=1

i=1

log p(x ) =

log p (xn

i |pa (xn
i ))

we de   ne the notation

  i
s(t) = p(xi = s|pa (xi) = t)

n(cid:88)

k(cid:88)

n=1

i=1

l    

log p (xn

i |pa (xn

i )) +

show that the maximum likelihood setting of   i

(cid:80)n
(cid:80)n

n=1

i(cid:104)
i(cid:104)
(cid:80)

s

n=1

xn
j = s

xn
j = s

(cid:16)

(cid:105) i(cid:104)
(cid:105) i(cid:104)

pa

pa

xn
j

(cid:16)

  j
s(tj) =

k(cid:88)

i=1

1    

(cid:32)

ti

  i
ti

(cid:88)
= tj(cid:105)
= tj(cid:105)
(cid:17)

(cid:17)

xn
j

s(t) is

(cid:0)ti(cid:1)(cid:33)

(cid:88)

s

  i
s

(9.9.3)

(9.9.4)

(9.9.5)

(9.9.6)

n(cid:88)

n=1

cl(  ) =

1
n

exercise 9.5 (conditional likelihood training). consider a situation in which we partition observable
variables into disjoint sets x and y and that we want to    nd the parameters that maximize the conditional
likelihood,

log p(yn|xn,   ),

(9.9.7)

for a set of training data {(xn, yn) , n = 1, . . . , n}. all data is assumed generated from the same distribu-
tion p(x, y|  0) = p(y|x,   0)p(x|  0) for some unknown parameter   0. in the limit of a large amount of i.i.d.
training data, does cl(  ) have an optimum at   0?

exercise 9.6 (moment matching). one way to set parameters of a distribution is to match the moments
of the distribution to the empirical moments. this sometimes corresponds to maximum likelihood (for the
gaussian distribution for example), though generally this is not consistent with maximum likelihood.
for data with mean m and variance s, show that to    t a beta distribution b (x|  ,   ) by moment matching,
we use

   =

m(m     m2     s)

s

,    =   

1     m
m

does this correspond to maximum likelihood?

draft november 9, 2017

(9.9.8)

239

exercises

n(cid:88)

n=1

n(cid:88)

n=1

exercise 9.7. for i.i.d. data 0     xn     1, n = 1, . . . , n , generated from a beta distribution b (x|a, b), show
that the log likelihood is given by

n(cid:88)

n=1

n(cid:88)

n=1

(cid:21)

l(a, b)     (a     1)

log xn + (b     1)

log(1     xn)     n log b(a, b)

(9.9.9)

where b(a, b) is the beta function. show that the derivatives are

   
   a

l =

log xn     n   (a)

+n   (a + b),

   
   b

l =

log(1     xn)     n   (b)

+n   (a + b)

(9.9.10)

@@
@@

where   (x)     d log   (x)/dx is the digamma function, and suggest a method to learn the parameters a, b.
exercise 9.8. consider the id82 as de   ned in example(9.8). write down the pseudo likeli-
hood for a set of i.i.d. data v1, . . . , vn and derive the gradient of this with respect to wij, i (cid:54)= j.
exercise 9.9. show that the model likelihood equation (9.4.53) can be written explicitly as

(cid:89)

(cid:89)

k

j

   ((cid:80)
   ((cid:80)

(cid:20)    (u(cid:48)

(cid:89)

i ui(vk; j))
i u(cid:48)
i(vk; j))

i

i(vk; j))
   (ui(vk; j))

p(d|m ) =

(9.9.11)

exercise 9.10. de   ne the set n as consisting of 8 node belief networks in which each node has at most 2
parents. for a given ancestral order a, the restricted set is written na

1. how many belief networks are in na?
2. what is the computational time to    nd the optimal member of na using the bayesian dirichlet score,
assuming that computing the bd score of any member of na takes 1 second and bearing in mind the
decomposability of the bd score.

3.

estimate the time required to    nd the optimal member of n ?

exercise 9.11. for the markov network

p(x, y, z) =

1
z

  1(x, y)  2(y, z)

@@

(9.9.12)

derive an iterative scaling algorithm to learn the unconstrained tables   1(x, y) and   2(x, y) based on a set of
i.i.d. data x ,y,z.
exercise 9.12. in section(9.6.4) we considered maximum likelihood learning of a markov network p(x )    

(cid:81)

c   c(xc) with parameters   c and potentials of the form

  c(xc) = exp (  cfc(xc))

(9.9.13)
with the constraint fc(xc)     0. our interest here is to drop this positive constraint on fc(xc). by considering

(cid:88)

  cfc(xc) =

for auxiliary variables pc > 0 such that(cid:80)

  cfc(xc)

pc

pc

c

c

(9.9.14)

c pc = 1, explain how to derive a form of iterative scaling training

algorithm for general fc in which each parameter   c can be updated separately.
exercise 9.13. write a matlab routine a = chowliu(x) where x is a d    n data matrix containing
a multivariate datapoint on each column that returns a chow-liu maximum likelihood tree for x. the tree
structure is to be returned in the sparse matrix a. you may    nd the routine spantree.m useful. the    le
chowliudata.mat contains a data matrix for 10 variables. use your routine to    nd the maximum likelihood
chow liu tree, and draw a picture of the resulting dag with edges oriented away from variable 1.

exercise 9.14.

show that for graph with n > 1 nodes, there are at least

++

2n   1 = 2n (n   1)/2

n=1

and less than n !2n (n   1)/2 valid dags.

240

draft november 9, 2017

(cid:88)

n(cid:89)

chapter 10

naive bayes

so far we   ve discussed methods in some generality without touching much on how we might use the methods
in a practical setting. here we discuss one of the simplest methods that is widely used in practice to classify
data. this is a useful junction since it enables us to discuss the issues of parameter learning from data and
also (constrained) structure learning.

10.1 naive bayes and conditional independence

we shall discuss machine learning concepts in some detail in chapter(13). here, we require only the intuitive
concept of classi   cation, which means giving a discrete label to an input. for example, one might wish to
classify an input image into one of two classes - male or female. naive bayes (nb) is a popular classi   cation
method and aids our discussion of conditional independence, over   tting and bayesian methods. in nb, we
form a joint model of a d-dimensional attribute (input) vector x and the corresponding class label c

p(x, c) = p(c)

p(xi|c)

(10.1.1)

whose belief network is depicted in    g(10.1a). coupled with a suitable choice for each conditional distribution

p(xi|c), we can then use bayes    rule to form a classi   er for a novel input vector x   :

p(c|x   

) =

p(x   

|c)p(c)
p(x   )

=

(cid:80)

p(x   
c p(x   

|c)p(c)
|c)p(c)

(10.1.2)

in practice it is common to consider only two classes dom(c) = {0, 1}. the theory we describe below is
valid for any number of classes c, though our examples are restricted to the binary class case. also, the
attributes xi are often taken to be binary, as we shall do initially below as well. the extension to more than
two attribute states, or continuous attributes is straightforward.

example 10.1. ezsurvey.org partitions radio station listeners into two groups     the    young    and    old   . they
assume that, given the knowledge that a customer is either    young    or    old   , this is su   cient to determine
whether or not a customer will like a particular radio station, independent of their likes or dislikes for any
other stations:

p(r1, r2, r3, r4|age) = p(r1|age)p(r2|age)p(r3|age)p(r4|age)

(10.1.3)

where each of the variables r1, r2, r3, r4 can take the states
like or dislike, and the    age    variable can take
the value young or old. thus the information about the age of the customer determines the individual

241

d(cid:89)

i=1

estimation using maximum likelihood

radio station preferences without needing to know anything else. to complete the speci   cation, given that
a customer is young, she has a 95% chance to like radio1, a 5% chance to like radio2, a 2% chance to like
radio3 and a 20% chance to like radio4. similarly, an old listener has a 3% chance to like radio1, an 82%
chance to like radio2, a 34% chance to like radio3 and a 92% chance to like radio4. they know that 90%
of the listeners are old.

given this model, and the fact that a new customer likes radio1, and radio3, but dislikes radio2 and
radio4, what is the id203 that the new customer is young? this is given by

p(young|r1 = like, r2 = dislike, r3 = like, r4 = dislike)

(cid:80)

=

p(r1 = like, r2 = dislike, r3 = like, r4 = dislike|young)p(young)
age p(r1 = like, r2 = dislike, r3 = like, r4 = dislike|age)p(age)

(10.1.4)

using the naive bayes structure, the numerator above is given by

p(r1 = like|young)p(r2 = dislike|young)p(r3 = like|young)p(r4 = dislike|young)p(young)

(10.1.5)

plugging in the values we obtain

0.95    0.95    0.02    0.8    0.1 = 0.0014

the denominator is given by this value plus the corresponding term evaluated assuming the customer is old,

0.03    0.18    0.34    0.08    0.9 = 1.3219    10

   4

which gives

p(young|r1 = like, r2 = dislike, r3 = like, r4 = dislike) =

0.0014

0.0014 + 1.3219    10   4 = 0.9161

(10.1.6)

10.2 estimation using maximum likelihood

learning the table entries for nb is a straightforward application of the more general bn learning discussed
in section(9.3). for a fully observed dataset, maximum likelihood learning of the table entries corresponds
to counting the number of occurrences in the training data, as we show below. this is a useful exercise to
reinforce and make more concrete the general theory of section(9.3).

10.2.1 binary attributes
consider a dataset {(xn, cn), n = 1, . . . , n} of binary attributes, xn
i     {0, 1}, i = 1, . . . , d and associated
class label cn. the number of datapoints from class c = 0 is denoted n0 and the number from class c = 1
is denoted n1. for each attribute of the two classes, we need to estimate the values p(xi = 1|c)       c
i . the

c

cn

xn
i

  c

  i,c

i = 1 : d

x1

x2

x3

(a)

n = 1 : n

(b)

(a): the central
figure 10.1: naive bayes classi   er.
assumption is that given the class c, the attributes xi
(b): assuming the data is i.i.d.,
are independent.
maximum likelihood learns the optimal parameters   c
of the distribution p(c) and the parameters   i,c of the
class-dependent attribute distributions p(xi|c).

242

draft november 9, 2017

estimation using maximum likelihood

other id203, p(xi = 0|c) is given by the normalisation requirement, p(xi = 0|c) = 1   p(xi = 1|c) = 1     c
i .
based on the nb conditional independence assumption the id203 of observing a vector x can be
compactly written1

p(x|c) =

p(xi|c) =

i )xi(1       c
(  c

i )1   xi

(10.2.1)

in the above expression, xi is either 0 or 1 and hence each i term contributes a factor   c
i if xi = 1 or 1       c
i
if xi = 0. together with the assumption that the training data is i.i.d. generated, the log likelihood of the
attributes and class labels is

d(cid:89)

i=1

(cid:88)

(cid:89)

l =

=

log p(xn, cn) =

log p(cn)

p(xn

n

i

i log   cn
xn

i + (1     xn

i ) log(1       cn
i )

i |cn)

          + n0 log p(c = 0) + n1 log p(c = 1)

this can be written more explicitly in terms of the parameters as

l =

i = 1, cn = 0] log   0

i + i [xn

i = 0, cn = 0] log(1       0

i ) + i [xn

i = 1, cn = 1] log   1
i

i )(cid:9) + n0 log p(c = 0) + n1 log p(c = 1)

+ i [xn

i = 0, cn = 1] log(1       1

we can    nd the maximum likelihood optimal   c

i by di   erentiating w.r.t.   c

i and equating to zero, giving

d(cid:89)

i=1

n

(cid:88)
         (cid:88)
(cid:88)
(cid:8)i [xn

i,n

i,n

(cid:80)

(cid:80)

  c
i = p(xi = 1|c) =

i [xn

n

i = 1, cn = c]

i [xn

i = 0, cn = c] + i [xn

n

i = 1, cn = c]

=

number of times xi = 1 for class c

number of datapoints in class c

similarly, optimising equation (10.2.3) with respect to p(c) gives

p(c) =

number of times class c occurs

total number of data points

(10.2.2)

(10.2.3)

(10.2.4)

(10.2.5)

(10.2.6)

(10.2.7)

these results are consistent with our general theory in section(9.3) that maximum likelihood corresponds
to setting tables by counting.

classi   cation boundary
we classify a novel input x    as class 1 if

p(c = 1|x   

) > p(c = 0|x   

)

using bayes    rule and writing the log of the above expression, this is equivalent to

from the de   nition of the classi   er, this is equivalent to (the normalisation constant     log p(x   ) can be

) > log p(x   

|c = 0) + log p(c = 0)     log p(x   

dropped from both sides)

log p(x   

|c = 1) + log p(c = 1)     log p(x   
(cid:88)

   
i|c = 1) + log p(c = 1) >

i

log p(x

(cid:88)

i

   
i|c = 0) + log p(c = 0)
log p(x

(10.2.10)

1this makes use of the general notation that any quantity raised to the power 0 is 1, i.e. x0     1. unfortunately, there is a
potential general mathematical notational confusion with xy meaning possibly x raised to the power y, or alternatively that y
is simply indexing a set of x variables. this potential con   ict will not hopefully arise too often, and can most often be resolved
by reference to the meaning of the symbols involved.

draft november 9, 2017

243

(10.2.8)

)

(10.2.9)

estimation using maximum likelihood

(cid:8)x

   
i log   1

using the binary encoding xi     {0, 1}, we therefore classify x    as class 1 if
(cid:88)
this decision rule can be expressed in the form: classify x    as class 1 if(cid:80)

i )(cid:9) + log p(c = 1) >

   
i ) log(1       1
i + (1     x

   
i log   0

(cid:88)

(cid:8)x

i

i

i + (1     x

i wix   

i )(cid:9) + log p(c = 0)

   
i ) log(1       0

(10.2.11)

i + a > 0 for some suitable
choice of weights wi and constant a, see exercise(10.4). the interpretation is that w speci   es a hyperplane
in the attribute space and x    is classi   ed as 1 if it lies on the positive side of the hyperplane.

example 10.2 (are they scottish?). consider the following vector of binary attributes:

(shortbread, lager, whiskey, porridge, football)

(10.2.12)

a vector x = (1, 0, 1, 1, 0)t would describe that a person likes shortbread, does not like lager, drinks
whiskey, eats porridge, and has not watched england play football. together with each vector x, there is a
label nat describing the nationality of the person, dom(nat) = {scottish, english}, see    g(10.2).
we wish to classify the vector x = (1, 0, 1, 1, 0)t as either scottish or english. using bayes    rule:

p(scottish|x) =

p(x|scottish)p(scottish)

p(x)

=

p(x|scottish)p(scottish)

p(x|scottish)p(scottish) + p(x|english)p(english)

(10.2.13)

by maximum likelihood the    prior    class id203 p(scottish) is given by the fraction of people in the
database that are scottish, and similarly p(english) is given as the fraction of people in the database that
are english. this gives p(scottish) = 7/13 and p(english) = 6/13.

for p(x|nat) under the naive bayes assumption:

p(x|nat) = p(x1|nat)p(x2|nat)p(x3|nat)p(x4|nat)p(x5|nat)

(10.2.14)

so that knowing whether not someone is scottish, we don   t need to know anything else to calculate the
id203 of their likes and dislikes. based on the table in    g(10.2) and using maximum likelihood we
have:

p(x1 = 1|english) = 1/2
p(x2 = 1|english) = 1/2
p(x3 = 1|english) = 1/3
p(x4 = 1|english) = 1/2
p(x5 = 1|english) = 1/2
for x = (1, 0, 1, 1, 0)t, we get

p(x1 = 1|scottish) = 1
p(x2 = 1|scottish) = 4/7
p(x3 = 1|scottish) = 3/7
p(x4 = 1|scottish) = 5/7
p(x5 = 1|scottish) = 3/7

p(scottish|x) =

1    3

7    3

7    5

1    3
7    4

7    3
7    7

7    5
13 + 1

7    4
2    1

7    7
2    1

13

3    1

2    1

2    6

13

(10.2.15)

= 0.8076

(10.2.16)

since this is greater than 0.5, we would classify this person as being scottish.

small data counts

in example(10.2), consider trying to classify the vector x = (0, 1, 1, 1, 1)t. in the training data, all scottish
people say they like shortbread. this means that for this particular x, p(x, scottish) = 0, and therefore
that we make the extremely con   dent classi   cation p(scottish|x) = 0. this demonstrates a di   culty using
maximum likelihood with sparse data. one way to ameliorate this is to smooth the probabilities, for
example by adding a small number to the frequency counts of each attribute. this ensures that there are

244

draft november 9, 2017

estimation using maximum likelihood

0
0
1
1
1

1
0
1
1
0

1
1
0
0
0

1
1
0
0
1
(a)

0
1
0
0
1

0
0
0
1
0

1
0
0
1
1

1
1
0
0
1

1
1
1
1
0

1
1
0
1
0
(b)

1
1
0
1
1

1
0
1
1
0

1
0
1
0
0

6

10.2:
for

figure
english
tastes
attributes
(shortbread, lager, whiskey, porridge, f ootball).
each column represents
tastes of an
individual. (b): scottish tastes for 7 people.

(a):
over

people

the

no zero probabilities in the model. an alternative is to use a bayesian approach that discourages extreme
probabilities, as discussed in section(10.3).

potential pitfalls with encoding

in practice,
in many o   -the-shelf packages implementing naive bayes, binary attributes are assumed.
however, the case of non-binary attributes often occurs. consider the following attribute : age. in a survey,
a person   s age is marked down using the variable a     1, 2, 3. a = 1 means the person is between 0 and 10
years old, a = 2 means the person is between 10 and 20 years old, a = 3 means the person is older than
20. one way to transform the variable a into a binary representation would be to use three binary variables
(a1, a2, a3) with (1, 0, 0), (0, 1, 0), (0, 0, 1) representing a = 1, a = 2, a = 3 respectively. this is called
1     of     m coding since only 1 of the binary variables is active in encoding the m states. by construction,
this means that the variables a1, a2, a3 are dependent     for example, if we know that a1 = 1, we know that
a2 = 0 and a3 = 0. regardless of any class conditioning, these variables will always be dependent, contrary
to the assumption of naive bayes. a correct approach is to use variables with more than two states, as
explained in section(10.2.2).

10.2.2 multi-state variables

the extension of the above method to class variables c with more than two states is straightforward. we
concentrate here therefore on extending the attribute variables to having more than two states. for a
variable xi with states, dom(xi) = {1, . . . , s}, the likelihood of observing a state xi = s is denoted

with(cid:80)
p(xi = s|c) =   i
s p(xi = s|c) = 1. the class conditional likelihood of generating the i.i.d. data d = (xn, cn), n =
n(cid:89)
n(cid:89)

d(cid:89)

s(cid:89)

c(cid:89)

1, . . . , n is

(10.2.17)

s(c)

p(xn|cn) =

n=1

n=1

i=1

s=1

c=1

  i
s(c)

i[xn

i =s]i[cn=c].

(10.2.18)

the e   ect of the indicators is that only those terms   i
for class c. this then gives the class conditional log-likelihood

s(c) survive for which there are attributes i in state s

l(  ) =

i [xn

i = s] i [cn = c] log   i

s(c)

(10.2.19)

we can optimize with respect to the parameters    using a lagrange multiplier (one for each of the attributes
i and classes c) to ensure normalisation. this gives the lagrangian

i [xn

i = s] i [cn = c] log   i

s(c) +

  i
s(c)

(10.2.20)

n=1

i=1

s=1

c=1

c=1

i=1

to    nd the optimum of this function we may di   erentiate with respect to   i
the resulting equation we obtain

s(c) and equate to zero. solving

(cid:32)

c(cid:88)

d(cid:88)

  c
i

1    

s(cid:88)

s=1

(cid:33)

n(cid:88)

d(cid:88)

s(cid:88)

c(cid:88)

n=1

i=1

s=1

c=1

n(cid:88)

d(cid:88)

s(cid:88)

c(cid:88)

l(  ,   ) =

n(cid:88)

i [xn

i = s] i [cn = c]

n=1

  i
s(c)

draft november 9, 2017

=   c
i

(10.2.21)

245

bayesian naive bayes

figure 10.3: bayesian naive bayes with a factorised prior on the class con-
ditional attribute probabilities p(xi = s|c). for simplicity we assume that
the class id203   c     p(c) is learned with maximum likelihood, so that
no distribution is placed over this parameter.

  c

n = 1 : n

cn

xn
i

  i,c

c = 1 : c

i = 1 : d

hence, by normalisation,

  i
s(c) = p(xi = s|c) =

(cid:80)
s(cid:48),n(cid:48) i(cid:2)xn(cid:48)
(cid:80)

i [xn

n

i = s] i [cn = c]

i = s(cid:48)(cid:3) i [cn(cid:48)

= c]

(10.2.22)

the maximum likelihood setting for the parameter p(xi = s|c) equals the relative number of times that
attribute i is in state s for class c.

10.2.3 text classi   cation

consider a set of documents about politics, and another set about sport. our interest is to make a method
that can automatically classify a new document as pertaining to either sport or politics. we search through
both sets of documents to    nd say the 100 most commonly occurring words (not including so-called    stop
words    such as    a    or    the   ). each document is then represented by a 100 dimensional vector representing the
number of times that each of the words occurs in that document     the so called bag of words representation
(this is a crude representation of the document since it discards word order). a naive bayes model speci   es
a distribution of these number of occurrences p(xi|c), where xi is the count of the number of times word i
appears in documents of type c. one can achieve this using either a multistate representation (as discussed
in section(10.2.2)) or using a continuous xi to represent the relative frequency of word i in the document.
in the latter case p(xi|c) could be conveniently modelled using for example a beta distribution.
despite the simplicity of naive bayes, it can classify novel documents surprisingly well[138] (although nat-
urally the real practical methods work with many more attributes and choose them wisely). intuitively a
potential justi   cation for the conditional independence assumption is that if we know that a document is
about politics, this is a good indication of the kinds of other words we will    nd in the document. because
naive bayes is a reasonable classi   er in this sense, and has minimal storage and fast training, it has been
applied to time and storage critical applications, such as automatically classifying webpages into types[310],
and spam    ltering[8]. it also forms one of the simplest yet most commonly used basic machine learning
classi   cation routines.

10.3 bayesian naive bayes

as we saw in the previous section, naive bayes can be a powerful method for classi   cation, yet can be overly
zealous in the case of small counts. if a single attribute i has no counts for class c then, irrespective of
the other attributes, the classi   er will say that x cannot be from class c. this happens because the prod-
uct of 0 with anything else remains 0. to counter overcon   dence e   ect, we can use a simple bayesian method.

given a dataset d = {(xn, cn), n = 1, . . . , n}, we predict the class c of an input x using

p(c|x,d)    

p(x,d, c)     p(x|d, c)p(c|d)

246

(10.3.1)

@@

draft november 9, 2017

bayesian naive bayes

(cid:88)

n

p(c|d) =

1
n

for convenience we will simply set p(c|d) using maximum likelihood

i [cn = c]

(10.3.2)

however, as we   ve seen, setting the parameters of p(x|d, c) using maximum likelihood training can yield
over-con   dent predictions in the case of sparse data. a bayesian approach that addresses this di   culty
uses priors on the probabilities p(xi = s|c)       i
s(c) to discourage extreme values. the model is depicted in
   g(10.3).

s(c)(cid:1) as the vector of probabilities and    =(cid:8)  i(c), i = 1, . . . , d, c = 1, . . . , c(cid:9),

1(c), . . . ,   i

we make the global factorisation assumption (see section(9.4)) and use a prior

the prior

writing   i(c) =(cid:0)  i
(cid:89)

p(  ) =

p(  i(c))

i,c

we consider discrete xi each of which take states from 1, . . . , s. in this case p(xi = s|c) corresponds to a
categorical distribution, for which the conjugate prior is a dirichlet distribution. under the factorised prior
assumption (10.3.3) we de   ne a prior for each attribute i and class c,

p(  i(c)) = dirichlet(cid:0)  i(c)|ui(c)(cid:1)

where ui(c) is the hyperparameter vector of the dirichlet distribution for table p(xi|c).
the posterior

consistent with our general bayesian bn training result in section(9.4), the parameter posterior factorises

   
p(  (c

where

   
p(  i(c

(cid:89)

)|d) =

   
p(  i(c

)|d)

n:cn=c   

i

))

   
)|d)     p(  i(c

(cid:89)
)|d) = dirichlet(cid:0)  i(c
(cid:88)
)(cid:3)

   
s(c

s = ui

) +

n:cn=c   

   
p(  i(c

   
)|  ui(c
where the vector   ui(c   ) has components

   

(cid:2)  ui(c

   

i [xn

i = s]

p(xn

   
i |  i(c

))

)(cid:1)

by conjugacy, the posterior for class c    is a dirichlet distribution,

for dirichlet hyperparameters ui(c   ) the above equation updates the hyperparameter by the number of
times variable i is in state s for class c    data. a common default setting is to take all components of u to
be 1.

classi   cation
the posterior class distribution for a novel input x    is given by

   
p(c

|x   

   
,d)     p(c

, x   

   
,d)     p(c

|d)p(x   

   
|d, c

   
) = p(c

|d)

p(x

   
   
i|d, c

)

(cid:89)

i

(cid:90)

p(x

   
   
i = s,   i(c

   
)|d, c

) =

p(x

   
   
i = s|  i(c

   
))p(  i(c

)|d)

  i(c   )

   
  i
s(c

   
)p(  i(c

)|d)

(cid:90)
(cid:90)

to compute p(x   

i|d, c   ) we use
   
   
i = s|d, c

  i(c   )

) =

p(x

=

  i(c   )

draft november 9, 2017

(10.3.3)

(10.3.4)

(10.3.5)

(10.3.6)

(10.3.7)

(10.3.8)

(10.3.9)

(10.3.10)

(10.3.11)

247

tree augmented naive bayes

c

x1

x2

x3

x4

figure 10.4: tree augmented naive (tan) bayes. each variable xi has
at most one parent. the maximum likelihood optimal tan structure is
computed using a modi   ed chow-liu algorithm in which the conditional
mutual information mi(xi; xj|c) is computed for all i, j. a maximum weight
spanning tree is then found and turned into a directed graph by orienting
the edges outwards from a chosen root node. the table entries can then be
read o    using the usual maximum likelihood counting argument.

using the general identity

  sdirichlet (  |u) d   =

1

z(u)

(cid:90) (cid:89)

s(cid:48)

  s(cid:48) us(cid:48)   1+i[s(cid:48)=s]d   =

z(u(cid:48))
z(u)

where z(u) is the normalisation constant of the distribution dirichlet (  |u) and

(cid:90)

(cid:26) us

(cid:48)
u
s =

s (cid:54)= s(cid:48)
us + 1 s = s(cid:48)
(cid:89)

   
,d)     p(c

|d)

i

z(u   i(c   ))
z(  ui(c   ))

we obtain

   
p(c

|x   

where

   i
   
s (c
u

) =   ui

   
s(c

) + i [x

   
i = s]

(10.3.12)

(10.3.13)

(10.3.14)

(10.3.15)

example 10.3 (bayesian naive bayes). repeating the previous analysis for the    are they scottish?    data
from example(10.2), the id203 under a uniform dirichlet prior for all the tables, gives a value of 0.764
for the id203 that (1, 0, 1, 1, 0) is scottish, compared with a value of
0.8076 under the standard naive
bayes assumption. see demonaivebayes.m.

@@

10.4 tree augmented naive bayes

a natural extension of naive bayes is to relax the assumption that the attributes are independent given the
class:

p(x|c) (cid:54)=

p(xi|c)

(10.4.1)

the question then arises     which structure should we choose for p(x|c)? as we saw in section(9.5), learning
a structure is computationally infeasible for all but very small numbers of attributes. a practical algorithm
therefore requires a speci   c form of constraint on the structure. in section(9.5.4) we saw that we can learn
single-parent tree-structured networks e   ciently. below we extend this to learning class dependent tree
networks for classi   cation.

10.4.1 learning tree augmented naive bayes networks

for a distribution p(x1, . . . , xd|c) of the form of a tree structure with a single-parent constraint, see    g(10.4),
we can readily    nd the class conditional maximum likelihood solution by computing the chow-liu tree for
each class. one then adds links from the class node c to each variable and learns the class conditional
probabilities from c to x, which can be read o    for maximum likelihood using the usual counting argument.
note that this would generally result in a di   erent chow-liu tree for each class.

248

draft november 9, 2017

d(cid:89)

i=1

exercises

practitioners typically constrain the network to have the same structure for all classes. the maximum
likelihood objective under the tan constraint then corresponds to maximising the conditional mutual in-
formation[105]

mi(xi; xj|c) = (cid:104)kl(p(xi, xj|c)|p(xi|c)p(xj|c))(cid:105)p(c)

(10.4.2)

see exercise(10.7). once the structure is learned one subsequently sets parameters by maximum likelihood
counting. techniques to prevent over   tting are discussed in [105] and can be addressed using dirichlet
priors, as for the simpler naive bayes structure.

one can readily consider less restrictive structures than single-parent belief networks. however,    nding
optimal bn structures is generally computationally infeasible and heuristics are required to limit the search
space.

10.5 summary

    naive bayes is a simple class-conditional generative model of data that can be used to form a simple

classi   er.

    bayesian training of the parameters is straightforward.
    an extension of the standard naive bayes    model is to consider attributes with at most a single parent
attribute (in addition to the class label). finding the maximum likelihood optimal tree augmented structure
is straightforward and corresponds to a maximum spanning tree problem with weights given by the class
conditional mutual information.

10.6 code

naivebayestrain.m: naive bayes trained with maximum likelihood
naivebayestest.m: naive bayes test
naivebayesdirichlettrain.m: naive bayes trained with bayesian dirichlet
naivebayesdirichlettest.m: naive bayes testing with bayesian dirichlet
demonaivebayes.m: demo of naive bayes

10.7 exercises

exercise 10.1. a local supermarket specializing in breakfast cereals decides to analyze the buying patterns
of its customers. they make a small survey asking 6 randomly chosen people their age (older or younger
than 60 years) and which of the breakfast cereals (corn   akes, frosties, sugar pu   s, bran   akes) they like.
each respondent provides a vector with entries 1 or 0 corresponding to whether they like or dislike the cereal.
thus a respondent with (1101) would like corn   akes, frosties and bran   akes, but not sugar pu   s. the
older than 60 years respondents provide the following data (1000), (1001), (1111), (0001). the younger than
60 years old respondents responded (0110), (1110). a novel customer comes into the supermarket and says
she only likes frosties and sugar pu   s. using naive bayes trained with maximum likelihood, what is the
id203 that she is younger than 60?

exercise 10.2. a psychologist does a small survey on    happiness   . each respondent provides a vector with
entries 1 or 0 corresponding to whether they answer    yes    to a question or    no   , respectively. the question
vector has attributes

x = (rich, married, healthy)

draft november 9, 2017

(10.7.1)

249

exercises

thus, a response (1, 0, 1) would indicate that the respondent was    rich   ,    unmarried   ,    healthy   . in addition,
each respondent gives a value c = 1 if they are content with their lifestyle, and c = 0 if they are not. the fol-
lowing responses were obtained from people who claimed also to be    content    : (1, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 1)
and for    not content   : (0, 0, 0), (1, 0, 0), (0, 0, 1), (0, 1, 0), (0, 0, 0).

1. using naive bayes, what is the id203 that a person who is    not rich   ,    married    and    healthy    is

   content    ?

2. what is the id203 that a person who is    not rich    and    married    is    content    ? (that is, we do not

know whether or not they are    healthy   ).

3. consider the following vector of attributes :

x1 = 1 if customer is younger than 20 ; x1 = 0 otherwise

(10.7.2)

x2 = 1 if customer is between 20 and 30 years old ; x2 = 0 otherwise

(10.7.3)

x3 = 1 if customer is older than 30 ; x3 = 0 otherwise

x4 = 1 if customer walks to work ; x4 = 0 otherwise

(10.7.4)

(10.7.5)

each vector of attributes has an associated class label    rich    or    poor   . point out any potential di   culties
with using your previously described approach to training using naive bayes. hence describe how to
extend your previous naive bayes method to deal with this dataset.

exercise 10.3. whizzco decide to make a text classi   er. to begin with they attempt to classify documents
as either sport or politics. they decide to represent each document as a (row) vector of attributes describing
the presence or absence of words.

x = (goal, football, golf, defence, o   ence, wicket, o   ce, strategy)

(10.7.6)

training data from sport documents and from politics documents is represented below in matlab using a
matrix in which each row represents the 8 attributes.

xp=[1 0 1 1 1 0 1 1; % politics

0 0 0 1 0 0 1 1;
1 0 0 1 1 0 1 0;
0 1 0 0 1 1 0 1;
0 0 0 1 1 0 1 1;
0 0 0 1 1 0 0 1]

xs=[1 1 0 0 0 0 0 0; % sport

0 0 1 0 0 0 0 0;
1 1 0 1 0 0 0 0;
1 1 0 1 0 0 0 1;
1 1 0 1 1 0 0 0;
0 0 0 1 0 1 0 0;
1 1 1 1 1 0 1 0]

using a maximum likelihood naive bayes classi   er, what is the id203 that the document x = (1, 0, 0, 1, 1, 1, 1, 0)
is about politics?
exercise 10.4. a naive bayes classi   er for binary attributes xi     {0, 1} is parameterised by   1
i = p(xi =
1|class = 1),   0
i = p(xi = 1|class = 0), and p1 = p(class = 1) and p0 = p(class = 0). show that the decision
to classify a datapoint x as class 1 holds if wtx + b > 0 for some w and b, and state explicitly w and b as
a function of   1,   0, p1, p0.

exercise 10.5. this question concerns spam    ltering. each email is represented by a vector

x = (x1, . . . , xd)

(10.7.7)

where xi     {0, 1}. each entry of the vector indicates if a particular symbol or word appears in the email.
the symbols/words are

money, cash, !!!, viagra, . . . , etc.

250

(10.7.8)

draft november 9, 2017

d(cid:89)

i=1

(cid:89)

i

(10.7.10)

(10.7.11)

(10.7.12)

(10.7.13)

exercises

so that for example x2 = 1 if the word    cash    appears in the email. the training dataset consists of a set of
vectors along with the class label c, where c = 1 indicates the email is spam, and c = 0 not spam. hence,
the training set consists of a set of pairs (xn, cn), n = 1, . . . , n . the naive bayes model is given by

p(c, x) = p(c)

p(xi|c)

(10.7.9)

1. derive expressions for the parameters of this model in terms of the training data using maximum

likelihood. assume that the data is independent and identically distributed

n(cid:89)

p(c1, . . . , cn , x1, . . . , xn ) =

p(cn, xn)

explicitly, the parameters are

n=1

p(c = 1), p(xi = 1|c = 1), p(xi = 1|c = 0), i = 1, . . . , d

2. given a trained model p(x, c), explain how to form a classi   er p(c|x).
3. if    viagra    never appears in the spam training data, discuss what e   ect this will have on the classi   cation
for a new email that contains the word    viagra   . explain how you might counter this e   ect. explain
how a spammer might try to fool a naive bayes spam    lter.

exercise 10.6. for a distribution p(x, c) and an approximation q(x, c), show that when p(x, c) corresponds
to the empirical distribution,    nding q(x, c) that minimises the id181

kl(p(x, c)|q(x, c))

corresponds to maximum likelihood training of q(x, c) assuming i.i.d. data.

exercise 10.7. consider a distribution p(x, c) and a tree augmented approximation

q(x, c) = q(c)

q(xi|xpa(i), c),

pa(i) < i or pa(i) =    

(cid:29)

(cid:28)

(cid:88)

i

show that for the optimal q(x, c) constrained as above, the solution q(x, c) that minimises kl(p(x, c)|q(x, c))
when plugged back into the kullback-leibler expression gives, as a function of the parental structure,

kl(p(x, c)|q(x, c)) =    

log

p(xi, xpa(i)|c)
p(xpa(i)|c)p(xi|c)

+ const.

p(xi,xpa(i),c)

(10.7.14)

this shows that under the single-parent constraint and that each tree q(x|c) has the same structure, minimis-
ing the id181 is equivalent to maximising the sum of conditional mutual information
terms. from exercise(10.6), we know therefore that this also corresponds to the maximum likelihood setting
for the parental structure. this can then be achieved by    nding a maximal weight spanning tree, as in the
case of the chow-liu tree.

draft november 9, 2017

251

exercises

252

draft november 9, 2017

chapter 11

learning with hidden variables

in many models some variables are not directly observed, but are latent or    hidden   . this can also occur when
some data are not observed. in this chapter we discuss methods for learning in the presence of such missing
information, and in particular develop the expectation-maximisation algorithm and related variants.

11.1 hidden variables and missing data

in practice data entries are often missing resulting in incomplete information to specify a likelihood. obser-
vational variables may be split into visible (those for which we actually know the state) and missing (those
whose states would nominally be known but are missing for a particular datapoint).

another scenario in which not all variables in the model are observed are the so-called hidden or latent
variable models. in this case there are variables which are essential for the model description but never
observed. for example, the underlying physics of a model may contain latent processes which are essential
to describe the model, but cannot be directly measured.

11.1.1 why hidden/missing variables can complicate proceedings

in learning the parameters of models as previously described in chapter(9), we assumed we have complete
information to de   ne all variables of the joint model of the data p(x|  ). consider the asbestos-smoking-
cancer network of section(9.3). using the multivariate variable x = (a, s, c), if patient n has a complete
record, the likelihood of this record is

p(xn|  ) = p(an, sn, cn|  ) = p(cn|an, sn,   c)p(an|  a)p(sn|  s)

(11.1.1)

which is factorised in terms of the table entry parameters. we exploited this property to show that table
entries    can be learned by considering only local information, both in the maximum likelihood and bayesian
frameworks.

now consider the case that for some of the patients, only partial information is available. for example,
for patient n with incomplete record xn = (c = 1, s = 1) it is known that the patient has cancer and is a
smoker, but whether or not they had exposure to asbestos is unknown. since we can only use the    visible   
available information it would seem reasonable (see section(11.1.2)) to assess parameters using the marginal
likelihood

(cid:88)

a

p(xn|  ) =

p(a, sn, cn|  ) =

(cid:88)

a

p(cn|a, sn,   c)p(a|  a)p(sn|  s)

253

(11.1.2)

hidden variables and missing data

  

  

xvis

xinv

xvis

xinv

minv

(a)

minv

(b)

(a): missing at random assumption. the
figure 11.1:
mechanism that generates the missing data does not depend
on either the parameter    of the model, or on the value of
(b): missing completely at random
the missing data.
assumption. the mechanism generating the missing data
is completely independent of the model. note that in both
cases the direction of the arrow between xvis and xinv is
irrelevant.

using the marginal likelihood, however, may result in computational di   culties since the likelihood equation
(11.1.2) cannot now be factorised into a product of the separate table parameters of the form fs(  s)fa(  a)fc(  c).
in this case the maximisation of the likelihood is more complex since the parameters of di   erent tables are
coupled.

a similar complication holds for bayesian learning. as we saw in section(9.4.1), under a prior factorised
over each cpt   , the posterior is also factorised. however, the missing variable introduces dependencies in
the posterior parameter distribution, making the posterior more complex. in both the maximum likelihood
and bayesian cases, one has a well de   ned likelihood function of the table parameters/posterior.
note that missing data does not always make the parameter posterior non-factorised. for example, if
the cancer state is unobserved above, because cancer is a collider with no descendants, the conditional
distribution simply sums to 1, and one is left with a factor dependent on a and another on s.

11.1.2 the missing at random assumption

under what circumstances is it valid to use the marginal likelihood to assess parameters? we partition
the variables x into those that are    visible   , xvis and    invisible   , xinv, so that the set of all variables can be
written x = [xvis, xinv]. for the visible variables we have an observed state xvis = v, whereas the state of
the invisible variables is unknown. to complete the picture we also need to model the process that informs
when data will be missing. we use an indicator minv = 1 to denote that the state of the invisible variables
is unknown and require then a model p(minv|xvis, xinv,   ). then for a datapoint which contains both visible
and invisible information,

p(xvis = v, minv = 1|  ) =

=

p(xvis = v, xinv, minv = 1|  )

p(minv = 1|xvis = v, xinv,   )p(xvis = v, xinv|  )

(11.1.3)

(11.1.4)

(cid:88)
(cid:88)

xinv

xinv

if we assume that the mechanism which generates invisible data is independent of the parameter and the
missing value xinv

p(minv = 1|xvis = v, xinv,   ) = p(minv = 1|xvis = v)

then

p(xvis = v, minv = 1|  ) = p(minv = 1|xvis = v)

(cid:88)

xinv

p(xvis = v, xinv|  )

= p(minv = 1|xvis = v)p(xvis = v|  )

(11.1.5)

(11.1.6)

(11.1.7)

only the term p(xvis = v|  ) conveys information about the parameter    of the model. therefore, provided
the mechanism by which the data is missing depends only on the visible states, we may simply use the
marginal likelihood to assess parameters. this is called the missing at random (mar) assumption, see
   g(11.1).

254

draft november 9, 2017

hidden variables and missing data

example 11.1 (not missing at random). ezsurvey.org stop men on the street and ask them their favourite
colour. all men whose favourite colour is pink decline to respond to the question     for any other colour,
all men respond to the question. based on the data, ezsurvey.org produce a histogram of men   s favourite
colour, based on the likelihood of the visible data alone, con   dently stating that none of them likes pink.
for simplicity, we assume there are only three colours, blue, green and pink. ezsurvey.org attempts to    nd
the histogram with probabilities   b,   g,   p with   b +   g +   p = 1. each respondent produces a visible response
x with dom(x) = {blue, green, pink}, otherwise m = 1 if there is no response. three men are asked their
favourite colour, giving data

(cid:8)x1, x2, x3(cid:9) = {blue, missing, green}

based on the likelihood of the visible data alone we have the log likelihood for i.i.d. data

l(  b,   g,   p) = log   b + log   g +    (1       b       g       p)

where the last lagrange term ensures normalisation. maximising the expression we arrive at

  b =

1
2

,   g =

1
2

,   p = 0

(11.1.8)

(11.1.9)

(11.1.10)

hence ezsurvey.org arrive at the erroneous conclusion that no men like pink, even though the reality is that
some men like pink     they just don   t want to say so. the unreasonable result that ezsurvey.org produce
is due to not accounting correctly for the mechanism which produces the data. in this case the data is not
mar since whether or not the data is missing depends on the state of the missing variable.

the correct mechanism that generates the data (including the missing data is)

p(x1 = blue|  )p(m2 = 1|  )p(x3 = green|  ) =   b  p  g =   b (1       b       g)   g

(11.1.11)

where we used p(m2 = 1|  ) =   p since the id203 that a datapoint is missing is the same as the
id203 that the favourite colour is pink. maximising the likelihood, we arrive at

  b =

1
3

,   g =

1
3

,   p =

1
3

(11.1.12)

as we would expect. on the other hand if there is another visible variable, t, denoting the time of day,
and the id203 that men respond to the question depends only on the time t alone (for example the
id203 of having missing data is high during rush hour), then we may indeed treat the missing data as
missing at random.

a stronger assumption than mar is that the missing data mechanism is completely independent of all other
model processes

p(minv = 1|xvis = v, xinv,   ) = p(minv = 1)

(11.1.13)

which is called missing completely at random. this applies for example to latent variable models in which
the variable state is always missing, independent of anything else.

11.1.3 maximum likelihood

throughout the remaining discussion we will assume any missing data is mar or missing completely at
random. we partition the variables into    visible variables v for which we know the states and    hidden   
variables h whose states will not be observed. for maximum likelihood we may then learn model parameters
   by optimising the on the visible variables alone

(cid:88)

h

p(v|  ) =

p(v, h|  )

with respect to   .

draft november 9, 2017

(11.1.14)

255

11.1.4 identi   ability issues

the marginal likelihood objective function depends on the parameters only through p(v|  ), so that equivalent
parameter solutions may exist. for example, consider a latent variable model with distribution

expectation maximisation

(11.1.15)

p(x1, x2|  ) =   x1,x2

p(x1|  ) = (cid:80)
(cid:88)

(cid:88)

in which variable x2 is never observed. this means that the marginal likelihood only depends on the entry
x2   x1,x2. given a maximum likelihood solution      , we can then always    nd an equivalent
maximum likelihood solution   (cid:48) provided (see exercise(11.9))

(cid:48)
x1,x2 =

  

   
x1,x2

  

(11.1.16)

x2

x2

in other cases there is an inherent symmetry in the parameter space of the marginal likelihood. for example,
consider the network over binary variables

p(c, a, s) = p(c|a, s)p(a)p(s)

(11.1.17)

we assume we know the table for p(s) and that our aim is to learn the asbestos table   p(a = 1) and cancer
tables

  p(c = 1|a = 1, s = 1),   p(c = 1|a = 1, s = 0),   p(c = 1|a = 0, s = 1),   p(c = 1|a = 0, s = 0)

(11.1.18)

where we used a          to denote that these are parameter estimates.

we assume that we have missing data such that the states of variable a are never observed. in this case
an equivalent solution (in the sense that it has the same marginal likelihood) is given by interchanging the
states of a:

(cid:48)
  p

(a = 0) =   p(a = 1)

and the four tables

(11.1.19)

(cid:48)
  p
(cid:48)
  p

(c = 1|a = 0, s = 1) =   p(c = 1|a = 1, s = 1),
(c = 1|a = 1, s = 1) =   p(c = 1|a = 0, s = 1),

(cid:48)
  p
(cid:48)
  p

(c = 1|a = 0, s = 0) =   p(c = 1|a = 1, s = 0)
(c = 1|a = 1, s = 0) =   p(c = 1|a = 0, s = 0)

(11.1.20)

a similar situation occurs in a more general setting in which the state of a variable is consistently unobserved
(mixture models are a case in point) yielding an inherent symmetry in the solution space. a well known
characteristic of maximum likelihood algorithms is that    jostling    occurs in the initial stages of training in
which these symmetric solutions compete.

11.2 expectation maximisation

the em algorithm is a convenient and general purpose iterative approach to maximising the likelihood
under missing data/hidden variables[85, 203]. it is generally straightforward to implement and can achieve
large jumps in parameter space, particularly in the initial iterations.

11.2.1 variational em

the key feature of the em algorithm is to form an alternative objective function for which the parameter
coupling e   ect discussed in section(11.1.1) is removed, meaning that individual parameter updates can be
achieved, akin to the case of fully observed data. the way this works is to replace the marginal likelihood
with a lower bound     it is this lower bound that has the useful decoupled form.

we    rst consider a single variable pair (v, h), where v stands for    visible    and h for    hidden   . the model of the
data is then p(v, h|  ) and our interest is to set    by maximising the marginal likelihood p(v|  ). to derive the
draft november 9, 2017
256

expectation maximisation

algorithm 11.1 expectation maximisation. compute maximum likelihood value for data with hidden
variables. input: a distribution p(x|  ) and dataset v. returns ml candidate   .
1: t = 0
2: choose an initial setting for the parameters   0.
3: while    not converged (or likelihood not converged) do
4:
5:
6:
7:
8:
9: end while
10: return   t

(cid:80)n
n=1 (cid:104)log p(hn, vn|  )(cid:105)qn

t (hn|vn) = p(hn|vn,   t   1)

(cid:46) run over all datapoints
(cid:46) e step

(cid:46) the max likelihood parameter estimate.

(cid:46) iteration counter
(cid:46) initialisation

t     t + 1
for n = 1 to n do

end for
  t = arg max  

t (hn|vn)

(cid:46) m step

qn

bound on the marginal likelihood, consider the id181 (which is always non-negative)
between a    variational    distribution q(h|v) and the parametric model p(h|v,   ):

kl(q(h|v)|p(h|v,   ))     (cid:104)log q(h|v)     log p(h|v,   )(cid:105)q(h|v)     0

(11.2.1)

the term    variational    refers to the fact that this distribution will be a parameter of an optimisation problem.
using p(h|v,   ) = p(h, v|  )/p(v|  ) and the fact that p(v|  ) does not depend on h,
kl(q(h|v)|p(h|v,   )) = (cid:104)log q(h|v)(cid:105)q(h|v)     (cid:104)log p(h, v|  )(cid:105)q(h|v) + log p(v|  )     0

(11.2.2)

rearranging, we obtain a bound on the marginal likelihood1

log p(v|  )        (cid:104)log q(h|v)(cid:105)q(h|v)

+(cid:104)log p(h, v|  )(cid:105)q(h|v)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

id178

energy

(11.2.3)

the energy term is also called the    expected complete data log likelihood   . the bound is potentially useful
since the    dependent energy term is similar in form to the fully observed case, except that terms with
missing data have their log likelihood weighted by a prefactor. equation(11.2.3) is a marginal likelihood
bound for a single training example. under the i.i.d. assumption, the log likelihood of all training data

v =(cid:8)v1, . . . , vn(cid:9) is the sum of the individual log likelihoods:

n(cid:88)

n=1

log p(v|  ) =

log p(vn|  )

summing over the training data, we obtain a bound on the log (marginal) likelihood

(cid:124)
log p(v|  )       l({q} ,   )        

(cid:125)
(cid:104)log q(hn|vn)(cid:105)q(hn|vn)

(cid:123)(cid:122)

id178

+

(cid:125)
(cid:104)log p(hn, vn|  )(cid:105)q(hn|vn)

(cid:123)(cid:122)

energy

n(cid:88)
(cid:124)

n=1

n(cid:88)

n=1

(11.2.4)

(11.2.5)

note that the bound   l({q} ,   ) is exact (that is, the right hand side is equal to the log likelihood) when we
set q(hn|vn) = p(hn|vn,   ), n = 1, . . . , n .
the bound depends both on    and the set of variational distributions {q}. our aim is then to try to optimise
this bound w.r.t.    and {q}; in doing so we will push up the lower bound, and hopefully increase thereby
the likelihood itself. a simple iterative procedure to optimise the bound is to    rst    x    and then optimise
w.r.t. {q}, and then    x {q} and optimise the bound w.r.t.   . these are known as the    e    and    m    steps and
are repeated until convergence:
e-step for    xed   ,    nd the distributions q(hn|vn), n = 1, . . . , n that maximise equation (11.2.5).
m-step for    xed q(hn|vn), n = 1, . . . , n ,    nd the parameters    that maximise equation (11.2.5).

1this is analogous to a standard partition function bound in statistical physics, from where the terminology    energy    and

   id178    hails.

draft november 9, 2017

257

11.2.2 classical em

in the variational e-step above, the fully optimal setting is

q(hn|vn) = p(hn|vn,   )

expectation maximisation

(11.2.6)

since q is    xed during the m-step, performing the m-step optimisation is equivalent to maximising the energy
term alone, see algorithm(11.1). from here onwards we shall use the term    em    to refer to the classical
em algorithm, unless otherwise stated. it is important to note that the em algorithm cannot generally
guarantee to    nd the fully optimal maximum likelihood solution and can get trapped in local optima, as
discussed in example(11.2).

example 11.2 (em for a one-parameter model). we consider a model small enough that we can plot fully
the evolution of the em algorithm. the model is on a single visible variable v with dom(v) = r and single
two-state hidden variable h with dom(h) = {1, 2}. we de   ne a model p(v, h|  ) = p(v|h,   )p(h) with

p(v|h,   ) =

1
     

e

   (v     h)2

(11.2.7)

and p(h = 1) = p(h = 2) = 0.5. for an observation v = 2.75 our interest is to    nd the parameter    that
optimises the likelihood

(cid:16)

   (2.75   2  )2(cid:17)

p(v = 2.75|  ) =

p(v = 2.75|h,   )p(h) =

1
2     

   (2.75     )2
e

+ e

(cid:88)

h=1,2

the log likelihood is plotted in    g(11.2a) with optimum at    = 1.325. if the state of h were given, the log
likelihood would be a single bump, rather than the more complex double bump in the case of missing data.
to use the em approach to    nd the optimum   , we need to work out the energy

(cid:104)log p(v, h|  )(cid:105)q(h|v) = (cid:104)log p(v|h,   )(cid:105)q(h|v) + (cid:104)log p(h)(cid:105)q(h|v)

(cid:68)

(v       h)2(cid:69)

=    

+ const.

q(h|v)

(11.2.8)

(11.2.9)

(11.2.10)

there are two h states of the q distribution and using q(h) as shorthand for q(h|v), normalisation requires
q(1) + q(2) = 1. due to the normalisation, we can fully parameterise q using q(2) alone. the em procedure
then iteratively optimises the lower bound

log p(v = 2.75|  )       l(q(2),   )

       q(1) log q(1)     q(2) log q(2)    

(cid:88)

h=1,2

q(h) (2.75       h)2 + const.

(11.2.11)

from an initial starting   , the em algorithm    nds the q distribution that optimises   l(q(2),   ) (e-step) and
then updates    (m-step). depending on the initial   , the solution found is either a global or local optimum
of the likelihood, see    g(11.2).

the m-step is easy to work out analytically in this case with   new = v (cid:104)h(cid:105)q(h) /(cid:10)h2(cid:11)

sets qnew(h) = p(h|v,   ) so that

qnew(h = 2) =

p(v = 2.75|h = 2,   )p(h = 2)

p(v = 2.75)

=

where we used

e   (2.75   2  )2

e   (2.75   2  )2 + e   (2.75     )2

p(v = 2.75) = p(v = 2.75|h = 1,   )p(h = 1) + p(v = 2.75|h = 2,   )p(h = 2)

q(h). similarly, the e-step

(11.2.12)

(11.2.13)

258

draft november 9, 2017

expectation maximisation

(a)

(b)

(c)

(a): the log likelihood for the model described in example(11.2).

(b): contours of the
figure 11.2:
lower bound   l(q(h = 2),   ). for an initial choice    = 1.9, successive updates of the e (vertical) and m
(horizontal) steps are plotted, with the algorithm converging to the global optimum maximum likelihood
setting for   . (c): starting at    = 1.95, the em algorithm converges to a local optimum.

example 11.3. consider a simple model

p(x1, x2|  )

where dom(x1) = dom(x2) = {1, 2}. assuming an unconstrained distribution

p(x1, x2|  ) =   x1,x2,

  1,1 +   1,2 +   2,1 +   2,2 = 1

(11.2.14)

(11.2.15)

our aim is to learn    from the data x1 = (1, 1) , x2 = (1, ?) , x3 = (?, 2). the energy term for the classical
em is

log p(x1 = 1, x2 = 1|  ) + (cid:104)log p(x1 = 1, x2|  )(cid:105)p(x2|x1=1,  old) + (cid:104)log p(x1, x2 = 2|  )(cid:105)p(x1|x2=2,  old)

writing out fully each of the above terms on a separate line gives the energy

log   1,1
+p(x2 = 1|x1 = 1,   old) log   1,1 + p(x2 = 2|x1 = 1,   old) log   1,2
+p(x1 = 1|x2 = 2,   old) log   1,2 + p(x1 = 2|x2 = 2,   old) log   2,2

(11.2.16)

(11.2.17)

(11.2.18)

(11.2.19)

this expression resembles the standard log likelihood of fully observed data except that terms with missing
data have their weighted log parameters. the parameters are conveniently decoupled in this bound (apart
from the trivial normalisation constraint) so that    nding the optimal parameters is straightforward. this is
achieved by the m-step update which gives

  1,1     1 + p(x2 = 1|x1 = 1,   old)   1,2     p(x2 = 2|x1 = 1,   old) + p(x1 = 1|x2 = 2,   old)
  2,1 = 0

  2,2     p(x1 = 2|x2 = 2,   old)

(11.2.20)

where p(x2|x1,   old)       old

x1,x2 (e-step) etc. the e and m-steps are iterated till convergence.

the em algorithm increases the likelihood

whilst, by construction, the em algorithm cannot decrease the lower bound on the likelihood, an important
question is whether or not the log likelihood itself is necessarily increased by this procedure.

draft november 9, 2017

259

11.522.53   1.55   1.5   1.45   1.4   1.35   1.3   1.25   1.2   1.15   1.1   1.05  logp(v|  )  q(h=2)11.522.530.10.20.30.40.50.60.70.80.9  q(h=2)11.522.530.10.20.30.40.50.60.70.80.9expectation maximisation

s
1
0
1
1
1
0
0

c
1
0
1
0
1
0
1

figure 11.3: a database containing information about being a smoker (1 signi   es the
individual is a smoker), and lung cancer (1 signi   es the individual has lung cancer).
each row contains the information for an individual, so that there are 7 individuals in
the database.

(11.2.21)

(11.2.22)

we use   (cid:48) for the new parameters, and    for the previous parameters in two consecutive iterations. using
q(hn|vn) = p(hn|vn,   ) we see that as a function of the parameters, the lower bound for a single variable
pair (v, h) depends on    and   (cid:48):

|  )        (cid:104)log p(h|v,   )(cid:105)p(h|v,  ) +(cid:10)log p(h, v|  

)(cid:11)

(cid:48)

(cid:48)

lb(  

p(h|v,  ) .

from the de   nition of the lower bound, equation (11.2.3), we have

(cid:48)

log p(v|  

) = lb(  

(cid:48)

|  ) + kl(p(h|v,   )|p(h|v,   

(cid:48)

)).

that is, the id181 is the di   erence between the lower bound and the true likelihood.
we may then also write

log p(v|  ) = lb(  |  ) + kl(p(h|v,   )|p(h|v,   ))

(cid:124)

0

(cid:123)(cid:122)
(cid:123)(cid:122)

   0

(cid:125)

(cid:125)

(cid:124)

hence

(cid:48)

log p(v|  

)     log p(v|  ) = lb(  

(cid:124)

(cid:48)

|  )     lb(  |  )

+ kl(p(h|v,   )|p(h|v,   

(cid:123)(cid:122)

   0

(11.2.23)

(11.2.24)

(cid:48)

(cid:125)

))

the    rst assertion is true since, by de   nition of the m-step, we search for a   (cid:48) which has a higher value
for the bound than our starting value   . the second assertion is true by the non-negative property of the
id181.

for more than a single datapoint, we simply sum each individual bound for log p(vn|  ). hence we reach the
important conclusion that the em algorithm increases, not only the lower bound on the marginal likelihood,
but the marginal likelihood itself (more correctly, em cannot decrease these quantities).

shared parameters and tables

it is often the case in models that parameters are shared between components of the model. the application
of em to this shared parameter case is essentially straightforward. according to the energy term, we need
to identify all those terms in which the shared parameter occurs. the objective for the shared parameter is
then the sum over all energy terms containing the shared parameter.

11.2.3 application to belief networks

conceptually, the application of em to training belief networks with missing data is straightforward. the
battle is more notational than conceptual. we begin the development with an example, from which intuition
about the general case can be gleaned.

example 11.4. consider the network.

p(a, c, s) = p(c|a, s)p(a)p(s)

(11.2.25)

for which we have a set of data, but that the states of variable a are never observed, see    g(11.3). our goal
is to learn the cpts p(c|a, s) and p(a) and p(s). to apply em, algorithm(11.1) to this case, we    rst assume
initial parameters   0

a,   0

s ,   0
c .

260

draft november 9, 2017

7(cid:88)
7(cid:88)

n=1

n=1

(cid:88)
n {qn

expectation maximisation

the    rst e-step, for iteration t = 1 then de   nes a set of distributions on the hidden variables (here the
hidden variable is a). for notational convenience, we write qn

t (a) in place of qn

t (a|vn). then

qn=1
t=1 (a) = p(a|c = 1, s = 1,   0),

qn=2
t=1 (a) = p(a|c = 0, s = 0,   0)

(11.2.26)

and so on for the 7 training examples, n = 2, . . . , 7.

we now move to the    rst m-step. the energy term for any iteration t is:

e(  ) =

=

(cid:104)log p(cn|an, sn) + log p(an) + log p(sn)(cid:105)qn
(cid:110)
(cid:104)log p(cn|an, sn)(cid:105)qn

t (a) + (cid:104)log p(an)(cid:105)qn

t (a)

t (a) + log p(sn)

(cid:111)

(11.2.27)

(11.2.28)

the    nal term is the log likelihood of the variable s, and p(s) appears explicitly only in this term. hence,
the usual maximum likelihood rule applies, and p(s = 1) is simply given by the relative number of times
that s = 1 occurs in the database, giving p(s = 1) = 4/7, p(s = 0) = 3/7.

the contribution of parameter p(a = 1) to the energy occurs in the terms

t (a = 0) log p(a = 0) + qn

t (a = 1) log p(a = 1)}

which, using the normalisation constraint is

log p(a = 0)

qn
t (a = 0) + log(1     p(a = 0))

(cid:88)

n

(cid:88)

n

qn
t (a = 1)

(11.2.29)

(11.2.30)

(cid:88)

n for which s = 1(cid:88)

n:cn=1,sn=1

di   erentiating with respect to p(a = 0) and solving for the zero derivative we get the m-step update for
p(a = 0)

(cid:80)
t (a = 0) +(cid:80)

n qn

t (a = 0)
n qn

n qn

(cid:80)

=

1
n

t (a = 1)

(cid:88)

n

p(a = 0) =

qn
t (a = 0)

(11.2.31)

that is, whereas in the standard maximum likelihood estimate, we would have the real counts of the data
in the above formula, here they have been replaced with our guessed values qn

t (a = 0) and qn

t (a = 1).

a similar story holds for p(c = 1|a = 0, s = 1). again we need to consider that normalisation means that
p(c = 0|a = 0, s = 1) also contributes. the contribution of this term to the energy is from those data indices

qn
t (a = 0) log p(c = 1|a = 0, s = 1) +

qn
t (a = 0) log (1     p(c = 1|a = 0, s = 1))

n:cn=0,sn=1

optimising with respect to p(c = 1|a = 0, s = 1) gives

(cid:80)
(cid:80)

(cid:80)
(cid:80)

n

n

n

i [cn = 1] i [sn = 1] qn

t (a = 0) +(cid:80)
i [cn = 1] i [sn = 1] i [an = 0] +(cid:80)

n

p(c = 1|a = 0, s = 1) =

p(c = 1|a = 0, s = 1) =

for comparison, the setting in the complete data case is

i [cn = 1] i [sn = 1] qn

t (a = 0)

i [cn = 0] i [sn = 1] qn

n

t (a = 0)

i [cn = 1] i [sn = 1] i [an = 0]

i [cn = 0] i [sn = 1] i [an = 0]

n

(11.2.32)

(11.2.33)

there is an intuitive relationship between these updates: in the missing data case we replace the indicators
by the assumed distributions q.
iterating the e and m steps, these parameters will converge to a local
likelihood optimum.

draft november 9, 2017

261

expectation maximisation

(11.2.34)

a belief network on a multivariate variable x takes the general form

11.2.4 general case

(cid:89)

p(x) =

p(xi|pa (xi)).

i

some of the variables will be observed, and others hidden. we therefore partition the variable x into visible

and hidden multivariate parts x = (v, h). given an i.i.d. dataset v =(cid:8)v1, . . . , vn(cid:9) our interest is to learn

the tables of p(x) to maximise the likelihood on the visible data v. for each datapoint index n, each mul-
tivariate variable xn = (vn, hn) decomposes into its visible and hidden parts; an upper variable index will
typically the datapoint, and a lower index the variable number.

from equation (11.2.5), the form of the energy term for belief networks is

(cid:88)
n (cid:104)log p(xn)(cid:105)qt(hn|vn) =

(cid:88)

(cid:88)

n

i

(cid:104)log p(xn

i |pa (xn

i ))(cid:105)qt(hn|vn)

(11.2.35)

here t indexes the iteration count for the em algorithm. it is useful to de   ne the following notation:

qn
t (x) = qt(hn|vn)  (v, vn)

(11.2.36)

this means that qn
on the unobserved variables. we then de   ne the mixture distribution

t (x) sets the visible variables in the observed state, and de   nes a conditional distribution

qn
t (x)

(11.2.37)

the energy term in the left hand side of equation (11.2.35) can be written more compactly using this notation

n(cid:88)

n=1

qt(x) =

1
n

as (cid:88)

using this compact notation for the energy, and the structure of the belief network, we can decompose the
energy as

to see this consider the right hand side of the above

n (cid:104)log p(xn)(cid:105)qt(hn|vn) = n (cid:104)log p(x)(cid:105)qt(x)
(cid:88)

(cid:88)

n (cid:104)log p(x)(cid:105)qt(x) = n

x

[log p(x)]

1
n

n

qt(hn|vn)  (v, vn) =
(cid:88)

(cid:68)

i

(cid:88)
n (cid:104)log p(xn)(cid:105)qt(hn|vn)
(cid:69)

(cid:69)

qt(pa(xi))

(cid:104)log p(x)(cid:105)qt(x) =

(cid:104)log p(xi|pa (xi))(cid:105)qt(x) =

(cid:104)log p(xi|pa (xi))(cid:105)qt(xi|pa(xi))

qt(pa(xi))

this means that maximising the energy is equivalent to minimising

(cid:104)log qt(xi|pa (xi))(cid:105)qt(xi|pa(xi))     (cid:104)log p(xi|pa (xi))(cid:105)qt(xi|pa(xi))

(cid:88)

(cid:68)

i

(cid:88)

i

(11.2.38)

(11.2.39)

(11.2.40)

(11.2.41)

where we added the constant    rst term to make this into the form of a id181. since
this is a sum of independent id181s, optimally the m-step is given by setting

pnew(xi|pa (xi)) = qt(xi|pa (xi))

(11.2.42)

in practice, storing the qt(x) over the states of all variables x is prohibitively expensive. fortunately, since
the m-step only requires the distribution on the family of each variable xi, one only requires the local
distributions qn

t (xi, pa (xi)). we may therefore dispense with the global qt(x) and equivalently use

(cid:80)
(cid:80)

n qn
t (xi, pa (xi))
n(cid:48) qn(cid:48)
t (pa (xi))

pnew(xi|pa (xi)) =

262

(11.2.43)

draft november 9, 2017

expectation maximisation

algorithm 11.2 em for belief networks. input: a bn dag and dataset on the visible variables v. returns
the maximum likelihood estimate of the tables p(xi|pa (xi)), i = 1, . . . , k.
1: t = 1
2: set pt (xi|pa (xi)) to initial values.
3: while p (xi|pa (xi)) not converged (or likelihood not converged) do

(cid:46) iteration counter
(cid:46) initialisation

4:
5:
6:
7:
8:
9:

t     t + 1
for n = 1 to n do

qn
t (x) = pt (hn|vn)   (v, vn)

end for
for i = 1 to k do

(cid:80)n
(cid:80)n
n=1 qn
n(cid:48)=1 qn(cid:48)

t (xi,pa(xi))
t (pa(xi))

@@

pt+1(xi|pa (xi)) =

end for
10:
11: end while
12: return pt(xi|pa (xi))

(cid:46) run over all datapoints
(cid:46) e step

(cid:46) run over all variables
(cid:46) m step

(cid:46) the max likelihood parameter estimate.

using the em algorithm, the optimal setting for the e-step is to use qt(hn|vn) = pold(hn|vn). with this
notation, the em algorithm can be compactly stated as in algorithm(11.2). see also embeliefnet.m. an
illustration of the evolution of the log likelihood under em iterations is given in    g(11.4). for readers less
comfortable with the above kl-based derivation, we describe a more classical approach based on lagrange
multipliers for a speci   c case in example(11.5).

example 11.5 (another belief network example). consider a    ve variable distribution with discrete vari-
ables,

p(x1, x2, x3, x4, x5) = p(x1|x2)p(x2|x3)p(x3|x4)p(x4|x5)p(x5)

(11.2.44)

in which the variables x2 and x4 are consistently hidden in the training data, and training data for x1, x3, x5
are always present. the distribution can be represented as a belief network

x1

x2

x3

x4

x5

the m-step is given by maximising the energy. according to the general form for the energy, equation
(11.2.35), we need to consider a variational distribution q(h|v) on the hidden variables h = (x2, x4) condi-
tioned on the visible variables v = (x1, x3, x5). using n as the datapoint index and t as the em iteration
counter, we require variational distributions for each datapoint

qt(xn

2 , xn

4|xn

1 , xn

3 , xn
5 )

(11.2.45)

to keep the notation more compact, we drop here the iteration counter index and write the above as simply
qn(x2, x4). in this case, the contributions to the energy have the form

which may be written as

3 )p(xn

1|x2)p(x2|xn

(cid:88)
n (cid:104)log p(xn
(cid:88)
(cid:88)
n (cid:104)log p(xn
1|x2)(cid:105)qn(x2,x4) +
n (cid:104)log p(xn

+

5 )(cid:105)qn(x2,x4)

5 )p(xn

3|x4)p(x4|xn
(cid:88)
(cid:88)
n (cid:104)log p(x2|xn
3 )(cid:105)qn(x2,x4)
n (cid:104)log p(x4|xn

3|x4)(cid:105)qn(x2,x4) +

5 )(cid:105)qn(x2,x4) +

(11.2.46)

(cid:88)

n

log p(xn
5 )

(11.2.47)

draft november 9, 2017

263

expectation maximisation

a useful property can now be exploited, namely that each term depends on only those hidden variables in
the family that that term represents. thus we may write

(cid:88)
(cid:88)
n (cid:104)log p(xn
1|x2)(cid:105)qn(x2) +
n (cid:104)log p(xn

+

(cid:88)
(cid:88)
n (cid:104)log p(x2|xn
3 )(cid:105)qn(x2)
n (cid:104)log p(x4|xn

3|x4)(cid:105)qn(x4) +

(cid:88)

n

log p(xn
5 )

(11.2.48)

5 )(cid:105)qn(x4) +

the    nal term can be set using maximum likelihood. let us consider therefore a more di   cult table,
p(x1|x2). when will the table entry p(x1 = i|x2 = j) occur in the energy? this happens whenever xn
1 is in
state i. since there is a summation over all the states of variables x2 (due to the average), there is also a term
with variable x2 in state j. hence the contribution to the energy from terms of the form p(x1 = i|x2 = j) is
(11.2.49)

(cid:88)

i [xn

1 = i] qn(x2 = j) log p(x1 = i|x2 = j)

n

where the indicator function i [xn
sation of the table, we add a lagrange term:

1 = i] equals 1 if xn

1 is in state i and is zero otherwise. to ensure normali-

i [xn

1 = i] qn(x2 = j) log p(x1 = i|x2 = j) +   

1    

p(x1 = k|x2 = j)

(11.2.50)

(cid:40)

(cid:88)

k

(cid:41)

di   erentiating with respect to p(x1 = i|x2 = j) and equating to zero we get

(cid:88)

n

(cid:88)

i [xn

1 = i]

qn(x2 = j)

p(x1 = i|x2 = j)

=   

n

or

(cid:88)
(cid:80)
(cid:80)

n

i [xn

1 = i] qn(x2 = j).

i [xn
i [xn

1 = i] qn(x2 = j)
1 = k] qn(x2 = j)

n

n,k

p(x1 = i|x2 = j)    

hence

p(x1 = i|x2 = j) =

from the e-step we have

1 , xn

3 , xn
5 )

qn(x2 = j) = pold(x2 = j|xn
(cid:80)
(cid:80)
(cid:88)

pnew(x1 = i|x2 = j) =

pnew(x1 = i|x2 = j)    

n

n,k

n

i [xn

1 = i] i [xn

2 = j]

i [xn
i [xn

1 = i] pold(x2 = j|xn
1 = k] pold(x2 = j|xn

1 , xn
1 , xn

3 , xn
5 )
3 , xn
5 )

if there were no hidden data, equation (11.2.55) would read

(11.2.51)

(11.2.52)

(11.2.53)

(11.2.54)

(11.2.55)

(11.2.56)

this optimal distribution is easy to compute since this is the marginal on the family, given some evidential
variables. hence, the m-step update for the table is

all that we do, therefore, in the general em case, is to replace those deterministic functions such as i [xn
by their missing variable equivalents pold(x2 = i|xn

1 , xn

3 , xn

5 ).

2 = i]

264

draft november 9, 2017

extensions of em

figure 11.4: evolution of the log-likelihood versus iterations un-
der the em training procedure (from solving the printer night-
mare with missing data, exercise(11.1)). note how rapid progress
is made at the beginning, but convergence can be slow.

11.2.5 convergence

convergence of em can be slow, particularly when the number of missing observations is greater than the
number of visible observations. in practice, one often combines the em with gradient based procedures to
improve convergence, see section(11.6). note also that the log likelihood is typically a non-convex function
of the parameters. this means that there may be multiple local optima and the solution found often depends
on the initialisation.

11.2.6 application to markov networks

whilst our examples have been for belief networks, we may also apply the em application to learning the
parameters of markov networks that have missing data. for a mn de   ned over visible and hidden variables
with separate parameters   c for each clique c

(cid:89)

p(v, h|  ) =

1

z(  )

c

  c(h, v|  c)

the em variational bound is

log p(v|  )     h(q) +

(cid:88)
c (cid:104)log   c(h, v|  c)(cid:105)q(h)     log z(  )

(11.2.57)

(11.2.58)

(cid:88)

c(cid:89)

v,h

c=1

where h(p) is the id178 function of a distribution, h(p)        (cid:104)log p(x)(cid:105)p(x). whilst the bound decouples
the clique parameters in the second term, the parameters are nevertheless coupled in the normalisation

z(  ) =

  c(h, v|  c),

   = (  1, . . . ,   c)

(11.2.59)

because of this we cannot directly optimise the above bound on a parameter by parameter basis. one
approach is to use an additional bound log z(  ) from above, as for iterative scaling, section(9.6.4) to decouple
the clique parameters in z; we leave the details as an exercise for the interested reader.

11.3 extensions of em

11.3.1 partial m step

it is not necessary to    nd the full optimum of the energy term at each iteration. as long as one    nds a
parameter   (cid:48) which has a higher energy than that of the current parameter   , then the conditions required
in section(11.2.2) still hold, and the likelihood cannot decrease at each iteration.

11.3.2 partial e-step

n(cid:88)

n=1

the e-step requires us to    nd the optimum of

log p(v|  )        

(cid:104)log q(hn|vn)(cid:105)q(hn|vn) +

with respect to q(hn|vn). the fully optimal setting is

q(hn|vn) = p(hn|vn)
draft november 9, 2017

n(cid:88)

n=1

(cid:104)log p(hn, vn|  )(cid:105)q(hn|vn)

(11.3.1)

(11.3.2)

265

024681012   120   110   100   90   80   70   60   50log likelihoodfor a guaranteed increase in likelihood at each iteration, from section(11.2.2) we required that this fully
optimal setting of q is used. unfortunately, therefore, one cannot in general guarantee that a partial e-step
(in which one would only partially optimsise the lower bound with respect to q for    xed   ) would always
increase the likelihood. of course, it is guaranteed to increase the lower bound on the likelihood, though
not the likelihood itself. we discuss some partial e-step scenarios below.

extensions of em

intractable energy

the em algorithm assumes that we can calculate

(cid:104)log p(h, v|  )(cid:105)q(h|v)

(11.3.3)

tractable is the factorised distributions q(h|v) = (cid:81)

however, there may be cases in which we cannot computationally carry out the averages with respect to the
fully optimal form of q. in this case one may consider a restricted class q of q-distributions for which the
averages can be carried out. for example, one class for which the averages may become computationally
j q(hj|v); another popular class are gaussian q distri-
butions. we can then    nd the best distribution in the class q by using a numerical optimisation routine:

qopt = argmin

q   q kl(q(h)|p(h|v,   ))

(11.3.4)

alternatively, one can assume a certain structured form for the q distribution, and learn the optimal factors
of the distribution by free form functional calculus. this approach is taken for example in section(28.4.2).

viterbi training
an extreme case of a partial e-step is to restrict q(hn|vn) to a delta-function. in this case, the entropic
term (cid:104)log q(hn|vn)(cid:105)q(hn|vn) is constant (zero for discrete h), so that the optimal delta function q is to set

when used in the energy, the average with respect to q is trivial and the energy becomes simply

q(hn|vn) =    (hn, hn    )

where

hn    = argmax

h

p(h, vn|  )

n(cid:88)

n=1

log p(hn    , vn|  )
n(cid:88)

the corresponding bound on the log-likelihood is then

log p(v|  )     h +

log p(hn    , vn|  )

n=1

(11.3.5)

(11.3.6)

(11.3.7)

(11.3.8)

where h is the id178 of the delta function (zero for discrete h).

as a partial justi   cation for this technique, provided there is su   cient data, one might hope that the likeli-
hood as a function of the parameter    will be sharply peaked around the optimum value. this means that
at convergence the approximation of the posterior p(h|v,   opt) by a delta function will be reasonable, and an
update of em using viterbi training will produce a new    approximately the same as   opt. for any highly
suboptimal   , however, p(h|v,   ) may be far from a delta function, and therefore a viterbi update is less
reliable in terms of leading to an increase in the likelihood itself. this suggests that the initialisation of   
for viterbi training is more critical than for the standard em. note that since viterbi training corresponds
to a partial e-step, em training with this restricted class of q distribution is therefore only guaranteed to
increase the lower bound on the log likelihood, not the likelihood itself.

this technique is popular in the id103 community for training id48s, section(23.2), from where
the terminology viterbi training arises.

266

draft november 9, 2017

stochastic em
another approximate q(hn|vn) distribution that is popular is to use an empirical distribution formed by
samples from the fully optimal distribution p(hn|vn,   ). that is one draws samples (see chapter(27) for a
discussion on sampling) hn

l from p(hn|vn,   ) and forms a q distribution

1 , . . . , hn

a failure case for em

l(cid:88)

l=1

1
l

   (hn, hn
l )

q(hn|vn) =
l(cid:88)
n(cid:88)

n=1

l=1

log p(hn

l , vn|  )

the energy then becomes proportional to

(11.3.9)

(11.3.10)

(cid:90)

h

so that, as in viterbi training, the energy is always computationally tractable for this restricted q class.
provided that the samples from p(hn|vn) are reliable, stochastic training will produce an energy function
with (on average) the same characteristics as the true energy under the classical em algorithm. this means
that the solution obtained from stochastic em should tend to that from classical em as the number of
samples increases.

11.4 a failure case for em

whilst the em algorithm is very useful, there are some cases in which it does not work. consider a likelihood
of the form

@@

p(v|  ) =

p (v|h,   ) p(h),

with

p (v|h,   ) =    (v, f (h|  ))

(11.4.1)

if we attempt an em approach for this, this will fail (see also exercise(7.8)). to see why this happens, the
m-step sets

@@
@@

  new = argmax

  

(cid:104)log p(v, h|  )(cid:105)

p(h|v,  old)

= argmax

  

(cid:104)log p(v|h,   )(cid:105)

p(h|v,  old)

(11.4.2)

where we used the fact that for this model p(h) is independent of   . in the case that p (v|h,   ) =    (v, f (h|  ))
then

@@

@@

p(h|  old)        (v, f (h|  old)) p(h)

so that optimising the energy gives the update

  new = argmax

  

(cid:104)log    (v, f (h|  ))(cid:105)

p(h|v,  old)

(11.4.3)

(11.4.4)

except at that h    for which v = f (h   

|  old), f (h   

since p(h|  old) is zero everywhere
|  old), then the energy term becomes
log    (f (h   
|  )). this is e   ectively negative in   nity if    (cid:54)=   old and zero when    =   old. hence
   =   old is optimal and the em algorithm fails to produce a meaningful parameter update.2 this situation
occurs in practice, and has been noted in particular in the context of independent component analysis[240].
whilst using a delta-function for the output is clearly extreme, a similar slowing down of parameter updates
can occur when the term p(v|h,   ) becomes close to deterministic.
one can attempt to heal this behaviour by deriving an em algorithm based on the distribution

@@

p (v, h|  ) = (1      )   (v, f (h|  )) p(h) +  n(v, h), 0           1

(11.4.5)

2for discrete variables and the kronecker delta, the energy attains the maximal value of zero when    =   old. in the case
of continuous variables, however, the log of the dirac delta function is not well de   ned. considering the delta function as the
limit of a narrow width gaussian, for any small but    nite width, the energy is largest when    =   old.

draft november 9, 2017

267

id58

p0(v, h|  ). de   n-

@@
@@

(11.4.6)

@@
@@

n(v, h) is an arbitrary distribution. the original deterministic model corresponds to

where
ing

(cid:90)

p (v|  ) =

h

p (v, h|  ),

p (v|  ) = (1      )p0(v|  ) +  

n(v)

an em algorithm for p (v|  ), 0 <   < 1

cannot decrease the likelihood and therefore satis   es

@@

p (v|  new)     p (v|  old) = (1      ) (p0(v|  new)     p0(v|  old)) > 0

which implies

p0(v|  new)     p0(v|  old) > 0

(11.4.7)

(11.4.8)

this means that the em algorithm for the non-deterministic case 0 <   < 1 is guaranteed to increase the
likelihood under the deterministic model p0(v|  ) at each iteration (unless we are at convergence). see [108]
for an application of this    antifreeze    technique to learning id100 with em.

11.5 id58

id58 is analogous to em in that it helps us to deal with hidden variables; however it is a
bayesian method that returns a posterior distribution on parameters, rather than a single best    as given
by maximum likelihood. to keep the notation simple, we   ll initially assume only a single datapoint with
observation v. our interest is then the parameter posterior

p(  |v)     p(v|  )p(  )

=

p(v, h|  )p(  )

(11.5.1)

@@

(cid:88)

h

the vb approach assumes a factorised approximation of the joint hidden and parameter posterior, see
   g(11.5):

p(h,   |v)     q(h)q(  )

(11.5.2)

the optimal settings for the factors q(h) and q(  ) can be found by minimising the id181
between p(h,   |v) and q(h)q(  ) as discussed below.
a bound on the marginal likelihood

by minimising the kl divergence,

kl(q(h)q(  )|p(h,   |v)) = (cid:104)log q(h)(cid:105)q(h) + (cid:104)log q(  )(cid:105)q(  )     (cid:104)log p(h,   |v)(cid:105)q(h)q(  )     0

we arrive at the bound

log p(v)        (cid:104)log q(h)(cid:105)q(h)     (cid:104)log q(  )(cid:105)q(  ) + (cid:104)log p(v, h,   )(cid:105)q(h)q(  )

(11.5.3)

(11.5.4)

minimizing the id181 with respect to q(  ) and q(h) is equivalent to obtaining the
tightest lower bound on log p(v). a simple coordinate-wise procedure in which we    rst    x the q(  ) and solve
for q(h) and then vice versa is analogous to the e and m step of the em algorithm:

e-step

m-step

qnew(h) = argmin

q(h)

(cid:16)

(cid:17)

kl

q(h)qold(  )|p(h,   |v)

qnew(  ) = argmin

q(  )

kl(qnew(h)q(  )|p(h,   |v))

(11.5.5)

(11.5.6)

for a set of observations v and hidden variables h, the procedure is described in algorithm(11.3). for
distributions q(h) and q(  ) which are parameterised or otherwise constrained, the best distributions in the
minimal kl sense are returned. in general, each iteration of vb is guaranteed to increase the bound on
the marginal likelihood, but not the marginal likelihood itself. like the em algorithm, vb can (and often
does) su   er from local maxima issues. this means that the converged solution can be dependent on the
initialisation.

268

draft november 9, 2017

id58

algorithm 11.3 id58.

1: t = 0
2: choose an initial distribution q0(  ).
3: while    not converged (or likelihood bound not converged) do
4:
5:
6:
7: end while
8: return qt(  )

t     t + 1
qt(h) = arg minq(h) kl(q(h)qt   1(  )|p(h,   |v))
qt(  ) = arg minq(  ) kl(qt(h)q(  )|p(h,   |v))

hn

vn

n

(a)

  

hn

n

  

(b)

(cid:46) iteration counter
(cid:46) initialisation

(cid:46) e step
(cid:46) m step

(cid:46) the posterior parameter approximation.

(a): generic form of a
figure 11.5:
(b): a
model with hidden variables.
factorised posterior approximation used in
id58.

unconstrained approximations

for    xed q(  ) the contribution to the kl divergence equation (11.5.3) from q(h) is

(cid:104)log q(h)(cid:105)q(h)     (cid:104)log p(v, h,   )(cid:105)q(h)q(  ) = kl(q(h)|  p(h)) + const.

where

  p(h)    

1
  z

exp

(cid:16)

(cid:104)log p(v, h,   )(cid:105)q(  )

(cid:17)

and   z is a normalising constant. hence, for    xed q(  ), the e-step sets q(h) to   p,

q(h)     exp(cid:104)log p(v, h,   )(cid:105)q(  )     exp(cid:104)log p(v, h|  )(cid:105)q(  )

similarly, for    xed q(h), the m-step sets

q(  )     exp(cid:104)log p(v, h,   )(cid:105)q(h) = p(  ) exp(cid:104)log p(v, h|  )(cid:105)q(h)

these e and m-step updates are iterated to convergence.

(11.5.7)

(11.5.8)

(11.5.9)

(11.5.10)

i.i.d. data

(cid:8)v1, . . . , vn(cid:9):

@@

log p(v)    

under the i.i.d. assumption, we obtain a bound on the marginal likelihood for the whole dataset v =

(cid:88)

n

(cid:110)
   (cid:104)log q(hn)(cid:105)q(hn)     (cid:104)log q(  )(cid:105)q(  ) + (cid:104)log p(vn, hn,   )(cid:105)q(hn)q(  )

(cid:111)

the bound holds for any q(hn) and q(  ) but is tightest for the converged estimates from the vb procedure.
for an i.i.d. dataset, it is straightforward to show that without loss of generality we may assume

(cid:89)

q(h1, . . . , hn ) =

q(hn)

n

under this we arrive at algorithm(11.4).

draft november 9, 2017

(11.5.11)

(11.5.12)

269

algorithm 11.4 id58 (i.i.d. data).

1: t = 0
2: choose an initial distribution q0(  ).
3: while    not converged (or likelihood bound not converged) do
4:
5:

t     t + 1
for n = 1 to n do
qn
t (hn)     exp
end for
qt(  )     p(  ) exp

(cid:16)
(cid:16)(cid:80)

6:

7:
8:
9: end while
10: return qn

t (  )

(cid:17)

(cid:17)

(cid:104)log p(vn, hn|  )(cid:105)qt   1(  )
n (cid:104)log p(vn, hn|  )(cid:105)qn

t (hn)

id58

(cid:46) iteration counter
(cid:46) initialisation

(cid:46) run over all datapoints
(cid:46) e step

(cid:46) m step

(cid:46) the posterior parameter approximation.

11.5.1 em is a special case of id58

if we wish to    nd a summary of the parameter posterior corresponding to only the most likely point      ,
then we may use a restricted q(  ) of the form

q(  ) =    (  ,      )

(11.5.13)

where       is the single optimal value of the parameter. if we plug this assumption into equation (11.5.4) we
obtain the bound

log p(v|     )        (cid:104)log q(h)(cid:105)q(h) + (cid:104)log p(v, h,      )(cid:105)q(h) + const.

the m-step is then given by

(cid:16)

(cid:17)

      = argmax

  

(cid:104)log p(v, h|  )(cid:105)q(h) + log p(  )

(11.5.14)

(11.5.15)

for a    at prior p(  ) = const., this is therefore equivalent to energy maximisation in the em algorithm.
using this single optimal value in the vb e-step update for q(hn) we have

qn
t (h)     p(v, h|     )     p(h|v,      )

(11.5.16)

which is the standard e-step of em. hence em is a special case of vb under a    at prior p(  ) = const. and
delta function approximation of the parameter posterior.

11.5.2 an example: vb for the asbestos-smoking-cancer network

in section(9.4) we showed how to apply bayesian methods to train a belief network, giving rise to a posterior
distribution over parameter. in our previous discussion, the data was fully observed. here, however, we wish
to revisit this case, now assuming however that some of the observations can be missing. this complicates
the bayesian analysis and motivates approximate methods such as vb.
let   s reconsider bayesian learning in the binary variable asbestos-smoking-cancer network, as described in
section(9.4)

p(a, c, s) = p(c|a, s)p(a)p(s)

in which we use a factorised parameter prior

p(  c)p(  a)p(  s)

(11.5.17)

(11.5.18)

when all the data v is i.i.d. and observed, the parameter posterior factorises. however, as we discussed in
section(11.1.1) if the state of a (asbestos) is not observed, the parameter posterior no longer factorises:

p(  a,   s,   c|v)     p(  a)p(  s)p(  c)p(v|  a,   s,   c)

    p(  a)p(  s)p(  c)

    p(  a)p(  s)p(  c)

(cid:88)
p(vn|  a,   s,   c)
p(sn|  s)

an

(cid:89)
(cid:89)

n

n

p(cn|sn, an,   c)p(an|  a)

(11.5.19)

(11.5.20)

(11.5.21)

270

draft november 9, 2017

id58

  s

s

n

  a

a

c

  c

(a)

  s

  a

a

n

  c

(b)

figure 11.6: (a): a model for the relation-
ship between lung cancer, asbestos expo-
sure and smoking with factorised parame-
ter priors. variables c and s are observed,
but variable a is consistently missing. (b):
a factorised parameter posterior approxi-
mation.

where the summation over a prevents the factorisation into a product of the individual table parameters.
this means that it becomes more awkward to represent the posterior since we cannot exactly do this by
using posterior distributions on each parameter table alone. this is a situation in which vb can be useful
since it enables us to impose factorisations on the posterior. in vb we consider an approximation to the
posterior over the parameters and latent variables q(h)q(  ).
in this case    = (  a,   s,   c) and the latent
variables are asbestos, an (one for each datapoint). for this example the visible variables are smoking and
cancer: v = {sn, cn} , n = 1, . . . , n . the exact joint distribution over all these variables is then

(cid:125)
p(  a,   s,   c, a1, . . . , an|v)     p(  a)p(  s)p(  c)

(cid:123)(cid:122)

(cid:124)

prior

(cid:89)
(cid:124)

n

(cid:125)
p(cn|sn, an,   c)p(sn|  s)p(an|  a)

(cid:123)(cid:122)

posterior

in vb we make a factorised assumption, splitting the parameters and latent variables:

p(  , a1:n|v)     q(  )q(a1:n )

from the general results in equation (11.5.9), we have (ignoring terms that are independent of a in the
exponent)

(cid:32)(cid:88)
n (cid:104)log p(cn|sn, an,   c)(cid:105)q(  c)

(cid:88)
n (cid:104)log p(an|  a)(cid:105)q(  a)

+

(cid:33)

from this we see immediately that our approximation automatically factorises

(11.5.22)

(11.5.23)

(11.5.24)

(11.5.25)

@@

q(a1:n )     exp
(cid:89)

q(a1:n ) =

q(an)

n

similarly, from equation (11.5.10) we have

(cid:32)(cid:88)
(cid:33)
(cid:32)(cid:88)
(cid:41)
n (cid:104)log p(cn|sn, an,   c)p(sn|  s)p(an|  a)(cid:105)q(an)
p(sn|  s)
n (cid:104)log p(cn|sn, an,   c)(cid:105)q(an)

(cid:33)(cid:40)(cid:89)

n

q(  )     p(  ) exp

= p(  ) exp

(cid:32)(cid:88)
n (cid:104)log p(an|  a)(cid:105)q(an)

(cid:33)

(11.5.26)

(11.5.27)

exp

since the parameter prior is factorised, we can collect terms in   a,   s and   c and see that the vb assumption
automatically results in a factorised parameter posterior approximation. hence, our vb approximation is
of the form, see    g(11.6),

p(  a,   s,   c, a1, . . . , an|v)     q(  a)q(  c)q(  s)

q(an)

(11.5.28)

(cid:89)

n

all that remains is to form e-step (q(an) updates) and m-step (q(  ) updates), as described below.

draft november 9, 2017

271

m-step: q(  ) updates

from above we have

q(  a)     p(  a)

where

(cid:89)

n

exp(cid:104)log p(an|  a)(cid:105)q(an)

(cid:104)log p(an|  a)(cid:105)q(an) = q(an = 1) log   a + q(an = 0) log (1       a)

hence

exp(cid:104)log p(an|  a)(cid:105)q(an) =   q(an=1)

a

(1       a)q(an=0)

it is convenient to use a beta distribution prior,

since the posterior approximation is then also a beta distribution:

a

p(  a) = b (  a|  ,   )            1
(cid:88)

(cid:32)

(1       a)     1
(cid:88)

q(an = 1),    +

q(  a) = b

  a|   +

a similar calculation gives

(cid:32)

q(  s) = b

  s|   +

n

(cid:88)

n

n

(cid:88)

n

i [sn = 1] ,    +

(cid:32)

(cid:88)

(cid:33)

q(an = 0)

(cid:33)

i [sn = 0]

(cid:88)

finally, we have a table for each of the parental states of c. for example

q(  c(a = 0, s = 1)) = b

  c|   +

i [sn = 1] q(an = 0),    +

i [sn = 0] q(an = 1)

n

n

id58

(11.5.29)

(11.5.30)

(11.5.31)

(11.5.32)

(11.5.33)

(11.5.34)

(11.5.35)

(cid:33)

these are reminiscent of the standard bayesian equations, equation (9.4.15), except that the missing data
counts have been replaced by q   s.

e-step: q(an) updates

from equation (11.5.24), we have

(cid:16)

q(an)     exp

(cid:104)log p(cn|sn, an,   c)(cid:105)q(  c) + (cid:104)log p(an|  a)(cid:105)q(  a)

(cid:17)

for example, if assume that for datapoint n, s is in state 1 and c in state 0, then

q(an = 1)     exp(cid:104)log (1       c(s = 1, a = 1))(cid:105)q(  c(s=1,a=1)) + (cid:104)log   a(cid:105)q(  a)

and

q(an = 0)     exp

(cid:16)

(cid:104)log (1       c(s = 1, a = 0))(cid:105)q(  c(s=1,a=1)) + (cid:104)log (1       a)(cid:105)q(  a)

these updates require the beta distribution averages (cid:104)log   (cid:105)b(  |  ,  ) and (cid:104)log (1       )(cid:105)b(  |  ,  ); these are
straightforward to compute, see exercise(8.17).

the complete vb procedure is then given by iterating equations (11.5.33,11.5.34,11.5.35) and (11.5.37,11.5.38)
until convergence.

272

draft november 9, 2017

(11.5.36)

(11.5.37)

(11.5.38)

(cid:17)

optimising the likelihood by gradient methods

given a converged factorised approximation, computing a marginal table such as p(a = 1|v) is then straight-
forward under the approximation

(cid:90)

   +(cid:80)
n q(an = 0) +    +(cid:80)

n q(an = 1)

   +(cid:80)

n q(an = 1)

p(a = 1|v)    

q(a = 1|  a)q(  a) =

  a

(11.5.39)

the application of vb to learning the tables in arbitrarily structured bns is a straightforward extension
of the technique outlined here. under the factorised approximation, q(h,   ) = q(h)q(  ), one will always
obtain a simple updating equation analogous to the full data case, but with the missing data replaced by
variational approximations. nevertheless, if a variable has many missing parents, the number of states in
the average with respect to the q distribution can become intractable, and further constraints on the form
of the approximation, or additional bounds are required.

one may readily extend the above to the case of dirichlet distributions on multinomial variables, see
exercise(11.5). indeed, the extension to the exponential family is straightforward.

11.6 optimising the likelihood by gradient methods

the em algorithm typically works well when the amount of missing information is small compared to the
complete information.
in this case em exhibits approximately the same convergence as newton based
gradient method[256]. however, if the fraction of missing information approaches unity, em can converge
very slowly.
in the case of continuous parameters   , an alternative is to compute the gradient of the
likelihood directly and use this as part of a standard continuous variable optimisation routine. the gradient
is straightforward to compute using the following identity. consider the log likelihood

(cid:90)

     

h

p(v, h|  )

(11.6.1)

(11.6.2)

l(  ) = log p(v|  )

the derivative can be written

     l(  ) =

1

p(v|  )

     p(v|  ) =

1

p(v|  )

(cid:90)

     l(  ) =

1

p(v|  )

h

(cid:90)

at this point, we take the derivative inside the integral

     p(v, h|  ) =

h

p(h|v,   )      log p(v, h|  ) = (cid:104)      log p(v, h|  )(cid:105)p(h|v,  )

(11.6.3)

where we used     log f (x) = (1/f (x))   f (x). the right hand side is the average of the derivative of the
log complete likelihood. this is closely related to the derivative of the energy term in the em algorithm,
though note that the average here is performed with respect the current distribution parameters    and not
  old as in the em case. used in this way, computing the derivatives of latent variable models is relatively
straightforward. these derivatives may then be used as part of a standard optimisation routine such as
conjugate gradients[256].

11.6.1 undirected models

whilst equation (11.6.3) represents the general case, it is not always possible to easily compute the required
averages. consider an undirected model which contains both hidden and visible variables

p(v, h|  ) =

1

z(  )

exp (  (v, h|  ))

for i.i.d. data, the log likelihood on the visible variables is (assuming discrete v and h)

(cid:88)

      log

(cid:88)

l(  ) =

n

h

draft november 9, 2017

(cid:88)

h,v

      

exp   (vn, h|  )     log

exp   (v, h|  )

(11.6.4)

(11.6.5)

273

which has gradient

                  

(cid:88)

n

(cid:28)    
(cid:124)

     

(cid:29)
(cid:123)(cid:122)
  (vn, h|  )

   
     

l =

(cid:28)    
(cid:124)

     

   

(cid:125)

(cid:29)

(cid:123)(cid:122)

p(h|vn,  )

  (v, h|  )

p(h,v|  )

                  

(cid:125)

clamped average

free average

exercises

(11.6.6)

for a markov network p that is intractable (the partition function z cannot be computed e   ciently and
averages w.r.t. p cannot be computed exactly), the gradient is particularly di   cult to estimate since it is
the di   erence of two averages, each of which needs to be estimated. even getting the sign of the gradient
correct can therefore be computationally di   cult. for this reason learning in general unstructured markov
networks is particularly di   cult (the unstructured id82 with hidden units being a particular
case in point).

11.7 summary

    provided the data is missing at random, we can safely learn parameters by maximising the likelihood of the

observed data.

    variational expectation maximisation is a general-purpose algorithm for maximum likelihood learning under

missing information.

    the classical em algorithm is a special case of v-em and guarantees an improvement (non-decrease) in

the likelihood at each iteration.

    bayesian learning in the case of missing information is potentially problematic since the posterior is typically
not factored according to the prior assumptions. in this case approximations are useful, such as variational
bayes, which assumes a factorisation between the parameters and the latent/missing variables.

    the gradient can be easily computed for latent variable models and may be used as part of an optimisation

routine. this provides an alternative training approach in cases when em is slow to converge.

11.8 code

demoemchestclinic.m: demo of em in learning the chest clinic tables

in the demo code we take the original chest clinic network [185] and draw data samples from this network.
our interest is then to see if we can use the em algorithm to estimate the tables based on the data (with
some parts of the data missing at random). we assume that we know the correct bn structure, only that
the cpts are unknown. we assume the logic gate table is known, so we do not need to learn this.

embeliefnet.m: em training of a belief network

code implements maximum likelihood learning of bn tables based on data with possibly missing

the
values.

11.9 exercises

exercise 11.1 (printer nightmare continued). continuing with the bn given in    g(9.21), the following
table represents data gathered on the printer, where ? indicates that the entry is missing. each column
represents a datapoint. use the em algorithm to learn all cpts of the network.

274

draft november 9, 2017

exercises

fuse assembly malfunction

drum unit
toner out

poor paper quality

worn roller

burning smell

poor print quality

wrinkled pages

multiple pages fed

paper jam

?
?
1
1
0
0
1
0
0
?

?
0
1
0
0
?
1
0
?
0

?
?
0
1
?
?
1
1
1
1

1
0
?
0
?
1
0
0
0
1

0
1
?
1
?
0
1
0
?
?

0
0
1
?
0
0
1
0
0
0

?
0
0
1
1
0
0
?
1
1

0
1
1
0
?
0
1
0
0
1

?
?
0
1
0
0
0
1
1
1

0
?
?
1
0
?
0
?
?
1

0
1
0
?
?
0
1
0
0
0

?
1
1
1
0
?
1
0
0
?

1
?
?
1
?
1
?
1
?
0

?
0
0
?
1
0
?
1
0
1

1
0
?
0
1
?
0
1
1
?

the table is contained in emprinter.mat, using states 1, 2, nan in place of 0, 1, ? (since brmltoolbox requires
states to be numbered 1,2,....). given no wrinkled pages, no burning smell and poor print quality, what is
the id203 there is a drum unit problem?

exercise 11.2. consider the following distribution over discrete variables,

p(x1, x2, x3, x4, x5) = p(x1|x2, x4)p(x2|x3)p(x3|x4)p(x4|x5)p(x5),

(11.9.1)

@@

in which the variables x2 and x4 are consistently hidden in the training data, and training data for x1, x3, x5
are always present. derive the em update for the table

p(x1|x2, x4).

exercise 11.3. consider a simple two variable bn

p(y, x) = p(y|x)p(x)

(11.9.2)

where both y and x are binary variables, dom(x) = {1, 2}, dom(y) = {1, 2}. you have a set of training
data {(yn, xn) , n = 1, . . . , n}, in which for some cases xn may be missing. we are speci   cally interested
in learning the table p(x) from this data. a colleague suggests that one can set p(x) by simply looking at
datapoints where x is observed, and then setting p(x = 1) to be the fraction of observed x that is in state 1.
explain how this suggested procedure relates to maximum likelihood and em.

exercise 11.4. assume that a sequence v1, . . . , vt , vt     {1, . . . , v } is generated by a markov chain. for a
single chain of length t , we have

p(v1, . . . , vt ) = p(v1)

p(vt+1|vt)

for simplicity, we denote the sequence of visible variables as

v = (v1, . . . , vt )

for a single markov chain labelled by h,

p(v|h) = p(v1|h)

p(vt+1|vt, h)

t   1(cid:89)

t=1

t   1(cid:89)

t=1

(11.9.3)

(11.9.4)

(11.9.5)

h(cid:88)

h=1

in total there are a set of h such markov chains (h = 1, . . . , h). the distribution on the visible variables is
therefore

p(v) =

p(v|h)p(h)

(11.9.6)

1. there are a set of training sequences, vn, n = 1, . . . , n . assuming that each sequence vn is inde-
pendently and identically drawn from a markov chain mixture model with h components, derive the
expectation maximisation algorithm for training this model.

2. the    le sequences.mat contains a set of    ctitious bio-sequence in a cell array sequences{\acup{n}}(t).

thus sequences{3}(:) is the third sequence, gtctcctgccctctctgaac which consists of 20 timesteps.
there are 20 such sequences in total. your task is to cluster these sequences into two clusters, assum-
ing that each cluster is modelled by a markov chain. state which of the sequences belong together by
assigning a sequence vn to that state for which p(h|vn) is highest. you may wish to use mixmarkov.m.
275

draft november 9, 2017

exercise 11.5. write a general purpose routine vbbeliefnet(pot,x,pars) along the lines of embeliefnet.m
that performs id58 under a dirichlet prior, using a factorised parameter approximation. as-
sume both global and local parameter independence for the prior and the approximation q, section(9.4.1).

exercise 11.6. consider a 3    layered    id82 which has the form

exercises

p(v, h1, h2, h3|  ) =

  (v, h1|  1)  (h1, h2|  2)  (h2, h3|  3)

where dim v = dim h1 = dim h2 = dim h3 = v

1
z

       v(cid:88)

      

  (x, y|  ) = exp

all variables are binary with states 0, 1 and the parameters for each layer l are   l =(cid:8)wl, al, bl(cid:9).

i,j=1

wijxiyj + aijxixj + bijyiyj

1. in terms of    tting the model to visible data v1, . . . , vn , is the 3 layered model above any more powerful

than    tting a two-layered model (the factor   (h2, h3|  3) is not present in the two-layer case)?

2. if we use a restricted potential

      (cid:88)

i,j

      

  (x, y|  ) = exp

wijxiyj

is the three layered model more powerful in being able to    t the visible data than the two-layered model?

exercise 11.7. the sigmoid belief network is de   ned by the layered network

p(xl)

p(xl   1|xl)

(11.9.10)

where vector variables have binary components xl     {0, 1}wl and the width of layer l is given by wl. in

addition

l(cid:89)

l=1

wl(cid:89)

i=1

p(xl   1|xl) =

p(xl   1

i

|xl)

and

p(xl   1

i = 1|xl) =   

(cid:16)

i,lxl(cid:17)

wt

,

  (x) = 1/(1 + e

   x)

for a weight vector wi,l describing the interaction from the parental layer. the top layer, p(xl) describes a
factorised distribution p(xl

1 ), . . . , p(xl

wl).

1. draw the belief network structure of this distribution.

2. for the layer x0, what is the computational complexity of computing the likelihood p(x0), assuming

that all layers have equal width w?

3. assuming a fully factorised approximation for an equal width network,

p(x1, . . . , xl|x0)    

q(xl
i)

(11.9.13)

write down the energy term of the variational em procedure for a single data observation x0, and
discuss the tractability of computing the energy.

exercise 11.8. show how to    nd the components 0     (  b,   g,   p)     1 that maximise equation (11.1.9).
276

draft november 9, 2017

l(cid:89)

w(cid:89)

l=1

i=1

(11.9.7)

(11.9.8)

(11.9.9)

(11.9.11)

(11.9.12)

exercises

exercise 11.9. a 2    2 id203 table, p(x1 = i, x2 = j) =   i,j, with 0       i,j     1,(cid:80)2

learned using maximal marginal likelihood in which x2 is never observed. show that if

i=1

(cid:18) 0.3 0.3
(cid:18) 0.2 0.4

0.2 0.2

0.4

0

(cid:19)
(cid:19)

   =

   =

is given as a maximal marginal likelihood solution, then

(cid:80)2

j=1   i,j = 1 is

(11.9.14)

(11.9.15)

has the same marginal likelihood score.

draft november 9, 2017

277

exercises

278

draft november 9, 2017

chapter 12

bayesian model selection

so far we   ve mostly used bayes    rule for id136 at the parameter level. applied at the model level, bayes   
rule gives a method for evaluating competing models. this provides an alternative to classical statistical
hypothesis testing techniques.

12.1 comparing models the bayesian way

given two models m1 and m2 with parameters   1,   2 and associated parameter priors,

p(x,   1|m1) = p(x|  1, m1)p(  1|m1),

(12.1.1)
how can we compare the performance of the models in    tting a set of data d = {x1, . . . , xn}? the
application of bayes    rule to models gives a framework for answering questions like this     a form of bayesian
hypothesis testing, applied at the model level. more generally, given an indexed set of models m1, . . . , mm,
and associated prior beliefs in the appropriateness of each model p(mi), our interest is the model posterior
id203

p(x,   2|m2) = p(x|  2, m2)p(  2|m2)

p(mi|d) =

p(d|mi)p(mi)

p(d)

where

p(d) =

p(d|mi)p(mi)

m(cid:88)

i=1

(cid:90)

p(d|mi) =

p(d|  i, mi)p(  i|mi)d  i

model mi is parameterised by   i, and the model likelihood is given by

(12.1.2)

(12.1.3)

(12.1.4)

in discrete parameter spaces, the integral is replaced with summation. note that the number of parameters
dim (  i) need not be the same for each model.

a point of caution here is that p(mi|d) only refers to the id203 relative to the set of models speci   ed
m1, . . . , mm. this is not the absolute id203 that model m    ts    well   . to compute such a quantity
would require one to specify all possible models. whilst interpreting the posterior p(mi|d) requires some
care, comparing two competing model hypotheses mi and mj is straightforward and only requires the bayes   
factor

(cid:124)

p(mi|d)
p(mj|d)

(cid:123)(cid:122)

(cid:125)

posterior odds

=

(cid:124)

p(d|mi)
p(d|mj)

(cid:123)(cid:122)

(cid:125)

bayes    factor

p(mi)
p(mj)

(cid:124) (cid:123)(cid:122) (cid:125)

prior odds

(12.1.5)

279

illustrations : coin tossing

(a)

(b)

(a): discrete prior p(  |mf air) model of a    fair    coin. a perfectly unbiased coin has    = 0.5,
figure 12.1:
which would corresponds to a prior   (  , 0.5)     however, we assume a more general form here to illustrate
how richer prior assumptions can be used. (b): prior p(  |mbiased) for a biased    unfair    coin. in both cases
we are making explicit choices here about what we consider to be    fair    and    unfair   .

which does not require integration/summation over all possible models. we also call the posterior odds the
posterior bayes    factor.

12.2 illustrations : coin tossing

we   ll consider two illustrations for testing whether a coin is biased or not. the    rst uses a discrete parameter
space to keep the mathematics simple. in the second we use a continuous parameter space. in both cases
we have a dataset d that consists of a sequence x1, . . . , xn of outcomes dom(xn) = {heads, tails}.
12.2.1 a discrete parameter space

we consider two competing models, one corresponding to a fair coin, and the other a biased coin. the
bias of the coin, namely the id203 that the coin will land heads, is speci   ed by   , so that a truly
fair coin has    = 0.5. for simplicity we assume dom(  ) = {0.1, 0.2, . . . , 0.9}. for the fair coin we use the
distribution p(  |mf air) in    g(12.1a) and for the biased coin the distribution p(  |mbiased) in    g(12.1b). note
that these priors essentially encode our subjective beliefs about what we mean for a coin to be biased, or
not. we are free to choose any prior distributions we wish. a strength of the bayesian framework is that in
doing so, we spell out what we mean by (in this case)    biased    or    unbiased   . this is the    subjective    beauty
of the framework; if others disagree with the subjective choice of the prior, they are free to declare their
own competing assumptions and carry through the corresponding analysis, adding to the possible models
available.

(12.2.1)

(12.2.2)

for each model m , the likelihood for the model to generate the data d, which contains nh heads and nt
tails, is given by

(cid:88)

  

(cid:88)

  

p(d|m ) =

p(d|  , m )p(  |m ) =

  nh (1       )nt p(  |m )

= 0.1nh (1     0.1)nt p(   = 0.1|m ) + . . . + 0.9nh (1     0.9)nt p(   = 0.9|m )

assuming that p(mf air) = p(mbiased) the posterior odds is given by the ratio of the two model likelihoods.

example 12.1 (discrete parameter space).

5 heads and 2 tails using nh = 5, nt = 2 in equation (12.2.2) we obtain p(d|mf air) = 0.00786 and

p(d|mbiased) = 0.0072. the posterior odds is

p(mf air|d)
p(mbiased|d)

= 1.09

indicating that there is little to choose between the two models.

(12.2.3)

280

draft november 9, 2017

00.10.20.30.40.50.60.70.80.9100.20.40.60.800.10.20.30.40.50.60.70.80.9100.050.10.15illustrations : coin tossing

(a)

(b)

figure 12.2: id203 density priors on the id203 of a head p(  ).
p(  |mf air) = b (  |50, 50).
note the di   erent vertical scales in the two cases.

(a): for a fair coin we choose
(b): for an biased coin we choose p(  |mbiased) = 0.5 (b (  |3, 10) + b (  |10, 3)).

50 heads and 20 tails for this case, repeating the above calculation, we obtain p(d|mf air) = 1.5  10   20

and p(d|mbiased) = 1.4    10   19. the posterior odds is

p(mf air|d)
p(mbiased|d)

= 0.109

(12.2.4)

indicating that we have around 10 times the belief in the biased model as opposed to the fair model.

12.2.2 a continuous parameter space

here we repeat the above calculation but for continuous parameter spaces. as for the discrete case, we are
free to choose any prior we wish; below we consider some simple priors for which the integrations required
are straightforward.

fair coin

for the fair coin, a uni-modal prior is appropriate. we use beta distribution

@@

p(  |mf air) = b (  |a, b) ,

b (  |a, b)    

1

b(a, b)

for convenience since this is conjugate to the binomial distribution and the required integrations are trivial.
for the fair coin prior we chose a = 50, b = 50, as shown in    g(12.2a). the likelihood is then given by

  a   1 (1       )b   1
(cid:90)

p(  |mf air)  nh (1       )nt =

1

b(a, b)

  

  a   1 (1       )b   1   nh (1       )nt

  nh +a   1 (1       )nt +b   1 =

b(nh + a, nt + b)

b(a, b)

(12.2.5)

(12.2.6)

(12.2.7)

(cid:90)

(cid:90)

  

1

b(a, b)

  

@@

p(d|mf air) =

=

biased coin

for the biased coin, we use a bimodal distribution formed, for convenience, as a mixture of two beta
distributions:

p(  |mbiased) =

[b (  |a1, b1) + b (  |a2, b2)]

as shown in    g(12.2b). the model likelihood p(d|mbiased) is given by

@@

p(d|mbiased) =

p(  |mbiased)  nh (1       )nt =

1
2

  

(cid:26) b(nh + a1, nt + b1)

b(a1, b1)

1
2

(cid:90)

draft november 9, 2017

(12.2.8)

(cid:27)

+

b(nh + a2, nt + b2)

b(a2, b2)

(12.2.9)

281

00.20.40.60.810246800.20.40.60.8100.511.5occam   s razor and bayesian complexity penalisation

figure 12.3: the likelihood of the total dice score, p(t|n) for n = 1 (top) to n = 5 (bottom) dice. plotted
along the horizontal axis is the total score t. the vertical line marks the comparison for p(t = 9|n) for the
di   erent number of die. the more complex models, which can reach more states, have lower likelihood, due
to normalisation over t.

assuming no prior preference for either a fair or biased coin p(m ) = const., and repeating the above scenario
in the discrete parameter case:

example 12.2 (continuous parameter space).

5 heads and 2 tails here p(d|mf air) = 0.0079 and p(d|mbiased) = 0.00622. the posterior odds is

p(mf air|d)
p(mbiased|d)

= 1.27

(12.2.10)

indicating that there is little to choose between the two models.

50 heads and 20 tails here p(d|mf air) = 9.4    10   21 and p(d|mbiased) = 1.09    10   19. the posterior

odds is

p(mf air|d)
p(mbiased|d)

= 0.087

(12.2.11)

indicating that we have around 11 times the belief in the biased model as opposed to the fair model.

12.3 occam   s razor and bayesian complexity penalisation

we return to the dice scenario of section(1.3.1). there we assumed there are two dice whose scores s1 and
s2 are not known. only the sum of the two scores t = s1 + s2 is known. we then computed the posterior
joint score distribution p(s1, s2|t = 9) for the two die. we repeat the calculation here but now for multiple
dice and with the twist that we don   t know how many dice there are1, only that the sum of the scores is 9.
i=1 si = 9 but are not told the number of dice involved n. assuming a priori that any

that is, we know(cid:80)n

number n is equally likely, what is the posterior distribution over n?

from bayes    rule, we need to compute the posterior distribution over models

p(n|t) =

p(t|n)p(n)

p(t)

1this description of occam   s razor is due to taylan cemgil.

(12.3.1)

282

draft november 9, 2017

000.10.2000.10.2000.10.2000.10.212345678910111213141516171819202122232425262728293000.10.2(cid:88)

(cid:90)

(cid:82)

occam   s razor and bayesian complexity penalisation

0.5

0

1

2

3

n

4

5

6

in the above the likelihood term is given by

p(t|n) =

p(t, s1, . . . , sn|n) =

s1,...,sn

s1,...,sn

figure 12.4: the posterior distribution p(n|t = 9) of
the number of die given the observed summed score of
9.

(cid:89)

i

p(si) =

(cid:88)

(cid:34)

i

s1,...,sn

t =

(cid:35)(cid:89)

i

si

n(cid:88)

i=1

p(si)

p(t|s1, . . . , sn)

(12.3.2)

(cid:88)

where p(si) = 1/6 for all scores si. by enumerating all 6n states, we can explicitly compute p(t|n), as
displayed in    g(12.3). the important observation is that as the models explaining the data become more
   complex    (n increases), more states become accessible. assuming p(n) = const., the posterior p(n|t = 9)
is plotted in    g(12.4). a posteriori, there are only 3 plausible models, namely n = 2, 3, 4 since the rest are
either too complex, or impossible.

++

occam   s razor

for a model m with parameters   , the likelihood of generating the data is given by

p(d|m ) =

p(d|  , m )p(  |m )

  

to simplify the argument, we place    at priors on the parameter space,

where v is the volume (number of states in the discrete case) of the parameter spaces. then

p(  |m ) = 1/v

p(d|m ) =

   p(d|  , m )

v

p(d|  , m )    

0

(cid:26) l    p(d|  , m )      
(cid:90)

p(d|  , m ) <  

p(d|m ) = l

    v  
v

,

v      

  :p(d|  ,m )    

1

we can approximate the likelihood p(d|  , m ) by thresholding it at a value  :

that is, when the likelihood is appreciable (bigger than  ) we give it value l   , otherwise we give the value
0. then

one can then interpret the model likelihood p(d|m ) as approximately the high likelihood value l    multi-

plied by the fraction of the parameter volume for which the likelihood is high.

(a): the likelihood p(d|  , mcomplex) has a higher
figure 12.5:
maximum value than then simpler model, but the likelihood
drops quickly as we move away from regions of high likelihood.
(b): the likelihood p(d|  , msimple) has a lower maximum value
than then complex model, but the likelihood changes less quickly
as we move away from regions of high likelihood. the corre-
sponding volume of parameter space in which the model    ts well
can then be higher for the simpler model.

283

(a)

(b)

draft november 9, 2017

occam   s razor and bayesian complexity penalisation

(a)

(b)

(c)

figure 12.6:

(a): data for which we wish to    t a regression model.

(b): the best    simple    model    t
(c): the best    complex    model    t y = ax + cos(bx) has

y = ax has maximum likelihood 5.9    10   11.
maximum likelihood 1.1    10   10.

consider two models msimple and mcomplex with corresponding parameters    and   . then, for    at parameter
priors, we can approximate

   
p(d|msimple) = l
simple

v  
simple
vsimple

,

p(d|mcomplex) = l

   
complex

v  
complex
vcomplex

at this point it is useful to re   ect on what constitutes a    complex    model. this can be characterised by a
relative    exibility in the data generating process, compared to a    simple    model. consequently, this means
that as we move in the space of the complex model, we will generate very di   erent datasets compared to
our given dataset d. this is a form of parameter sensitivity meaning that the likelihood of generating
the observed data d will typically drop dramatically as we move away from regions of parameter space in
which the model    ts well. hence a simple model msimple that has a similar maximum likelihood to a complex
model, l   
complex, typically the fraction of the parameter space in which the likelihood is appreciable
will be smaller for the complex model than the simple model, meaning that p(d|msimple) > p(d|mcomplex),
see    g(12.5). if we have no prior preference for either model p(msimple) = p(msimple) the bayes factor is
given by

simple     l   

p(msimple|d)
p(mcomplex|d)

=

p(d|msimple)
p(d|mcomplex)

and the bayes factor will typically prefer the simpler of two competing models with similar maximum
likelihood values. this demonstrates the occam   s razor e   ect of bayesian model id136 which penalises
models which are over complex.

as an example of this e   ect, consider the regression problem in    g(12.6) for which we consider the following
two models of the    clean    underlying regression function:

msimple : y0 = ax

mcomplex : y0 = ax + cos(bx)

to account for noise in the observations, y = y0 +  ,       n

p(y|x, a, msimple) = n

(cid:0)  0,   2(cid:1), we use a mode

the likelihood of a collection y of independent observations for a set of inputs x is then

(cid:0)y ax,   2(cid:1)

(cid:90)

n(cid:89)

n=1

p(y|x , msimple) =

284

p(a|msimple)

a

p(yn|xn, a, msimple)

draft november 9, 2017

05101520   2024681012xy05101520   2024681012xy05101520   2024681012xya continuous example : curve    tting

(a)

(b)

(a): the likelihood p(y|x , a, b, mcomplex) (b):
figure 12.7: likelihood plots for the problem in    g(12.6).
the likelihood p(y|x , a, msimple) plotted on the same scale as (a). this displays the characteristic of a
   complex    model in the likelihood drops dramatically as we move only a small distance in parameter space
from a point with high likelihood. whilst the maximum likelihood of the complex model is higher than the
simpler model, the volume of parameter space in which the complex model    ts the data well is smaller than
for the simpler model, giving rise to p(y|x , msimple) > p(y|x , mcomplex).

similarly,

p(y|x, a, b, mcomplex) = n

and

p(y|x , mcomplex) =

(cid:90)

(cid:0)y ax + cos(bx),   2(cid:1)
n(cid:89)

p(a, b|mcomplex)

a,b

n=1

p(yn|xn, a, b, mcomplex)

using a discrete set of 21 values for a, evenly spaced from 0.4 to 0.6, and 121 discrete values for b, evenly
spaced from -15 to 15, we can compute the corresponding likelihoods p(y|x , a, msimple) and
p(y|x , a, b, mcomplex). for this data, the maximum likelihoods are

max

a

p(y|x , a, msimple) = 5.9    10

max
a,b

p(y|x , a, b, mcomplex) = 1.1    10

   11,

   10

so that the more complex model has a higher maximum likelihood. however, as we can see in    g(12.7), the
fraction of the parameter space in which the complex model    ts the data well is relatively small. using a
   at prior for the parameter spaces of both models, we obtain

p(y|x , msimple) = 1.36    10

   11,

p(y|x , mcomplex) = 1.87e     12    10

   11

whilst the more complex model has a higher maximum likelihood value by a factor 2, it is roughly 7 times
less likely to be the correct model, compared to the simpler model.

12.4 a continuous example : curve    tting

consider an additive set of periodic functions

y0(x)     w0 + w1 cos(x) + w2 cos(2x) + . . . + wk cos(kx)

this can be conveniently written in vector form

y0(x)     wt  (x)

draft november 9, 2017

(12.4.1)

(12.4.2)

285

ab   15   10   50510150.40.420.440.460.480.50.520.540.560.580.6ba00.40.420.440.460.480.50.520.540.560.580.6where   (x) is a k + 1 dimensional vector with elements

  (x)     (1, cos(x), cos(2x), . . . , cos(kx))t

and the vector w contains the weights of the additive function. we are given a set of data d = {(xn, yn), n = 1, . . . , n}
drawn from this distribution, where yn is the clean y0(xn) corrupted with additive zero mean gaussian noise
with variance   2,

approximating the model likelihood

(12.4.3)

see    g(12.8) and    g(12.9). assuming i.i.d. data, we are interested in the posterior id203 of the number
of coe   cients, given the observed data:

yn = y0(xn) +  n,

 n     n

p(k|d) =

p(d|k)p(k)

=

p(d)

n p(xn)

(cid:0) n 0,   2(cid:1)
p(k)(cid:81)
(cid:90)

p(d)

p(y1, . . . , yn|x1, . . . , xn , k) =

we will assume that a priori we have no preference for the number of frequency components in the model,
p(k) = const. the likelihood term above is given by the integral

p(y1, . . . , yn|x1, . . . , xn , k)
n(cid:89)

w

n=1

p(w|k)

p(yn|xn, w, k)
n(cid:88)
   n log(cid:0)2    2(cid:1)
n(cid:88)

   

n=1

yn  (xn)

b    

1
  2

n=1

(yn)2
  2 +bta   1b

for p(w|k) = n (w 0, ik/  ), the integrand is a gaussian in w for which it is straightforward to evaluate
the integral, (see section(8.4) and exercise(12.3))

2 log p(y1, . . . , yn|x1, . . . , xn , k) =
where

a       i +

1
  2

  (xn)  t(xn),

n(cid:88)

n=1

    log det (a) + k log (  ) (12.4.7)

@@
@@

(12.4.8)

assuming    = 1 and    = 0.5, we sampled some data from a model with k = 5 components,    g(12.9a).
given then this data, and assuming that we know the correct noise level    and prior precision    = 1, the
task is to infer the number of components k that were used to generate this data. the posterior p(k|d)
plotted in    g(12.9b) is sharply peaked at k = 5, which is the value used to generate the data. the clean
posterior mean reconstructions (cid:104)yn

0|d(cid:105) for k = 5 are plotted in    g(12.9c).

12.5 approximating the model likelihood

for a model with continuous parameter vector   , dim (  ) = k and data d, the model likelihood is

(cid:90)

p(d|m ) =

p(d|  , m )p(  |m )d  

for a generic expression

p(d|  , m )p(  |m ) = exp (   f (  ))

unless f is of a particularly simple form (quadratic in    for example), for large k, the integral in (12.5.1)
is high dimensional and cannot be exactly evaluated. in order to implement bayesian model comparison
methods in practice, therefore, typically we need to use some form of approximation for the model likelihood.

(12.4.4)

(12.4.5)

(12.4.6)

(12.5.1)

(12.5.2)

k

w

xn

yn
0

yn

n

(cid:82)

for regression under the i.i.d.

figure 12.8: belief network representation of a hierarchical bayesian
model
data assumption. note that
the intermediate nodes on yn
0 are included to highlight the role of the
p(y|y0)p(y0|w, x) =
   clean    underlying model.
y0 n
with the intermediate node y0 and place directly arrows from w and xn to
yn.

since p(y|w, x) = (cid:82)
(cid:0)y wtx,   2(cid:1), we can if desired do away

(cid:0)y y0,   2(cid:1)   (cid:0)y0     wtx(cid:1) = n

y0

286

draft november 9, 2017

approximating the model likelihood

(a)

(b)

(c)

(a): the data generated with additive gaussian noise    = 0.5 from a k = 5 component
figure 12.9:
(c): the reconstruction of the data using (cid:104)w(cid:105)t   (x) where (cid:104)w(cid:105) is
model.
the mean posterior vector of the optimal dimensional model p(w|d, k = 5). plotted in the continuous line
is the reconstruction. plotted in dots is the true underlying clean data.

(b): the posterior p(k|d).

12.5.1 laplace   s method

a simple approximation of (12.5.1) is given by laplace   s method, section(28.2), which    nds the optimum
     

of the posterior and then    ts a gaussian at this point based on the local curvature, giving

log det(cid:0)2  h   1(cid:1)

log p(d|m )     log p(d|     

, m ) + log p(     

|m ) +

1
2

where      
     

is the map solution

= argmax

  

p(d|  , m )p(  |m )

and h is the hessian of f (  )         log p(d|  , m )p(  |m ) evaluated at      
for data d =(cid:8)x1, . . . , xn(cid:9) that is i.i.d. generated the above specialises to

.

(cid:90)

p(d|m ) =

p(  |m )

   f (  ) = log p(  |m ) +

n(cid:89)

n=1

p(xn|  , m )d  
n(cid:88)

log p(xn|  , m )

n=1

in this case laplace   s method computes the optimum of the function

(12.5.3)

(12.5.4)

(12.5.5)

(12.5.6)

a partial justi   cation for laplace   s approximation is that as the number of datapoints n increases, the
posterior typically becomes increasingly peaked around a single most likely explanation for the data. this
means that a narrow width gaussian will tend to be a good approximation in the large data limit. quantify-
ing how well an approximation works though is typically rather di   cult. the laplace method   s popularity
stems from it simplicity although clearly there
will be cases where the posterior is known to be strongly
non-gaussian, in which case laplace   s method should be used with some caution.

@@

12.5.2 bayes information criterion (bic)

the bayes information criterion is a simpler version of laplace   s method that replaces the exact hessian
with a crude approximation. for i.i.d. data the hessian scales with the number of training examples, n ,
and a somewhat severe approximation is to set h     n ik where k = dim (  ). continuing with the notation
from laplace   s method, this gives the approximation

log p(d|m )     log p(d|     

, m ) + log p(     

|m ) +

k
2

log 2      

k
2

log n

draft november 9, 2017

(12.5.7)

287

   10   50510   3   2   101231234567891000.20.40.60.81   10   50510   3   2   10123for a simple prior that penalises the length of the parameter vector, p(  |m ) = n (   0, i), the above reduces
to

bayesian hypothesis testing for outcome analysis

log p(d|m )     log p(d|     

, m )    

1
2

(     

)t      

k
2

   

log n

the bayes information criterion[262] approximates (12.5.8) by ignoring the penalty term, giving

bic = log p(d|     

, m )    

k
2

log n

typically the bic criterion is used when no speci   c prior is speci   ed on   , in which case      
is given by the
maximum likelihood setting. the bic criterion may be used as an approximate way to compare models,
where the term     k
in general, the laplace approximation, equation
(12.5.3), is to be preferred to the bic criterion since it more correctly accounts for the uncertainty in the
posterior parameter estimate. other techniques that aim to improve on the laplace method are discussed
in section(28.3) and section(28.8).

2 log n penalises model complexity.

(12.5.8)

(12.5.9)

12.6 bayesian hypothesis testing for outcome analysis

in outcome analysis we wish to analyse the results of some experimental data. we assume however, that we
make no detailed model of the data generating mechanism, asking rather generic questions such as whether
the results support some basic hypothesis such as whether two classi   ers are performing di   erently. the
techniques we discuss are quite general, but we phrase them in terms of classi   er analysis for concreteness.
the central question we consider here therefore is how to assess whether two classi   ers are performing dif-
ferently. for techniques which are based on bayesian classi   ers there will always be, in principle, a direct
   p(d|  , m )p(  |d, m ). we
consider here the less fortunate situation where the only information presumed available is the test perfor-
mance of the two classi   ers.

way to estimate the suitability of the model m by computing p(m|d)     p(m )(cid:82)

to outline the basic issue, we consider two classi   ers a and b which predict the class of 55 test examples.
classi   er a makes 20 errors, and 35 correct classi   cations, whereas classi   er b makes 23 errors and 32 correct
classi   cations. is classi   er a better than classi   er b? our lack of con   dence in pronouncing that a is better
than b results from the small number of test examples. on the other hand if classi   er a makes 200 errors
and 350 correct classi   cations, whilst classi   er b makes 230 errors and 320 correct classi   cations, intuitively,
we would be more con   dent that classi   er a is better than classi   er b. perhaps the most practically relevant
question from a machine learning perspective is the id203 that classi   er a outperforms classi   er b,
given the available test information. whilst this question can be addressed using a bayesian procedure,
section(12.6.5), we    rst focus on a simpler question, namely whether classi   er a and b are the same[16].

12.6.1 outcome analysis

consider a situation where two classi   ers a and b have been tested on some data, so that we have, for each
example in the test set, an outcome pair

(oa(n), ob(n)) , n = 1, . . . , n

(12.6.1)
where n is the number of test data points, and oa     {1, . . . , q} (and similarly for ob). that is, there are q
possible types of outcomes that can occur. for example, for binary classi   cation we will typically have the
four cases

dom(o) = {truepositive, falsepositive, truenegative, falsenegative}

(12.6.2)

if the classi   er predicts class c     {true, false} and the truth is class t     {true, false} these are de   ned as

truepositive
falsepositive
truenegative
falsenegative

c = true
c = true
c = false
c = false

t = true
t = false
t = false
t = true

(12.6.3)

we call oa = {oa(n), n = 1, . . . , n}, the outcomes for classi   er a, and similarly ob = {ob(n), n = 1, . . . , n}
for classi   er b. to be speci   c we have two hypotheses we wish to test:

288

draft november 9, 2017

bayesian hypothesis testing for outcome analysis

  

oa

  

  

ob

oa

ob

(a)

(b)

p

oa, ob

(c)

(a): hindep : corresponds to the out-
figure 12.10:
comes for the two classi   ers being independently gen-
(b): hsame: both outcomes are generated
erated.
(c): hdep : the out-
from the same distribution.
comes are dependent.

1. hindep : oa and ob are from di   erent categorical distributions.

2. hsame : oa and ob are from the same categorical distribution.

q, with unknown parameters   c. hypothesis
in both cases we will use categorical models p(oc = q|  , h) =   c
2 will correspond to using the same parameters   a =   b for both classi   ers, and hypothesis 1 to using
di   erent parameters, as we will discuss below. in the bayesian framework we want to    nd how likely it is
that a model/hypothesis is responsible for generating the data. for any hypothesis h this is given by

p(h|oa, ob) =

p(oa, ob|h)p(h)

p(oa, ob)

(12.6.4)

where p(h) is the prior belief that h is the correct hypothesis. note that the normalising constant p(oa, ob)
does not depend on the hypothesis. for all hypotheses we make the independence of trials assumption

n(cid:89)

n=1

p(oa, ob|h) =

p(oa(n), ob(n)|h).

to make further progress we need to clarify the meaning of the hypotheses.

12.6.2 hindep : model likelihood

from bayes    rule we may write the posterior hypothesis id203 as

p(hindep|oa, ob) =

p(oa, ob, hindep)

p(oa, ob)

=

p(oa, ob|hindep)p(hindep)

p(oa, ob)

the outcome model for classi   er a is speci   ed using continuous parameters,   , giving p(oa|  , hindep), and
similarly we use    for classi   er b. the    nite amount of data means that we are uncertain as to these
parameter values and therefore the joint term in the numerator above is

p(oa, ob, hindep) =

p(oa, ob|  ,   , hindep)p(  ,   |hindep)p(hindep)d  d  

(12.6.7)

= p(hindep)

where we assumed

p(oa|  , hindep)p(  |hindep)d  

p(ob|  , hindep)p(  |hindep)d  

(12.6.8)

(cid:90)

(cid:90)

(cid:90)

p(  ,   |hindep) = p(  |hindep)p(  |hindep) and p(oa, ob|  ,   , hindep) = p(oa|  , hindep)p(ob|  , hindep)

(12.6.9)

see    g(12.10a) for a depiction of these independence assumptions. note that one might expect there to be
a speci   c constraint that the two models a and b are di   erent. however since the models are assumed
independent and each has parameters sampled from an e   ectively in   nite set (   and    are continuous), the
a priori id203 that values of randomly sampled    and    are the same is zero, and we may consider
hindep as equivalent to hdi   erent.

since we are dealing with categorical distributions, it is convenient to use the dirichlet prior, which is
conjugate to the categorical distribution:

p(  |hindep) =

uq   1
  
q

,

z(u) =

(cid:89)

1

z(u)

q

(cid:81)q
(cid:16)(cid:80)q

q=1   (uq)

  

q=1 uq

(cid:17)

(12.6.10)

289

draft november 9, 2017

(12.6.5)

(12.6.6)

bayesian hypothesis testing for outcome analysis

the prior hyperparameter u controls how strongly the mass of the distribution is pushed to the corners of
the simplex, see    g(8.6). setting uq = 1 for all q corresponds to a uniform prior. the likelihood of observing
oa is given by

p(oa|  , hindep)p(  |hindep)d   =

1

(cid:93)a
q
  
q

q

z(u)

q

uq   1
q

  

d   =

z(u + (cid:93)a)

z(u)

(12.6.11)

(cid:90) (cid:89)

(cid:89)

(cid:90)

where (cid:93)a is a vector with components (cid:93)a
hence

q being the number of times that variable a is in state q in the data.

p(oa, ob, hindep) = p(hindep)

z(u + (cid:93)a)

z(u + (cid:93)b)

z(u)

z(u)

where z(u) is given by equation (12.6.10).

12.6.3 hsame : model likelihood

(12.6.12)

in hsame, the hypothesis is that the outcomes for the two classi   ers are generated from the same categorical
distribution, see    g(12.10b). hence

(cid:90)

p(oa, ob, hsame) = p(hsame)

p(oa|  , hsame)p(ob|  , hsame)p(  |hsame)d  

= p(hsame)

z(u + (cid:93)a + (cid:93)b)

z(u)

bayes    factor

if we assume no prior preference for either hypothesis, p(hindep) = p(hsame), then

p(hindep|oa, ob)
p(hsame|oa, ob)

=

z(u + (cid:93)a)z(u + (cid:93)b)
z(u)z(u + (cid:93)a + (cid:93)b)

(12.6.13)

(12.6.14)

(12.6.15)

the higher this ratio is, the more we believe that the data were generated by two di   erent categorical
distributions.

example 12.3. two people classify the expression of each image into happy, sad or normal, using states
1, 2, 3 respectively. each column of the data below represents an image classi   ed by the two people (person
1 is the top row and person 2 the second row). are the two people essentially in agreement?

1
1

3
3

1
1

3
2

1
2

1
3

3
3

2
3

2
2

3
3

1
3

1
2

1
2

1
2

1
2

1
1

1
2

1
1

1
3

2
2

to help answer this question, we perform a hindep versus hsame test. from this data, the count vector for
person 1 is [13, 3, 4] and for person 2, [4, 9, 7]. based on a    at prior for the categorical distribution and
assuming no prior preference for either hypothesis, we have the posterior bayes    factor

p(persons 1 and 2 classify di   erently)
p(persons 1 and 2 classify the same)

=

z([14, 4, 5])z([5, 10, 8])
z([1, 1, 1])z([18, 13, 12])

= 12.87

(12.6.16)

where the z function is given in equation (12.6.10). this is strong evidence the two people are classifying
the images di   erently.

below we discuss some further examples for the hindep versus hsame test. as above, the only quantities we
need for this test are the vector counts from the data. we assume that there are three kinds of outcomes,
q = 3, for example dom(o) = {good, bad, ugly}, and we want to test if two classi   ers are essentially producing
the same outcome distributions, or di   erent. throughout we assume a    at prior on the table entries, u = 1.

290

draft november 9, 2017

bayesian hypothesis testing for outcome analysis

example 12.4 (hindep versus hsame).

equation (12.6.15) is 20.7     strong evidence in favour of the two classi   ers being di   erent.

    we have the two outcome counts (cid:93)a = [39, 26, 35] and (cid:93)b = [63, 12, 25]. the posterior bayes    factor
    alternatively, consider the two outcome counts (cid:93)a = [52, 20, 28] and (cid:93)b = [44, 14, 42]. then, the
posterior bayes    factor equation (12.6.15) is 0.38     weak evidence against the two classi   ers being
di   erent.

    as a    nal example, consider counts (cid:93)a = [459, 191, 350] and (cid:93)b = [465, 206, 329]. this gives a posterior
bayes    factor equation (12.6.15) of 0.008     strong evidence that the two classi   ers are statistically the
same.

in all cases the results are consistent with the actual model in fact used to generate the count data.

12.6.4 dependent outcome analysis

here we consider the case that outcomes are dependent. for example, it may be the case that when classi   er
a works well, classi   er b will also work well. our interest is to evaluate the hypothesis:

hdep : the outcomes that the two classi   ers make are dependent

(12.6.17)

to do so we assume a categorical distribution over the joint states, see    g(12.10c):

p(oa(n), ob(n)|p, hdep)

here p is a q    q matrix of probabilities:

[p ]ij = p(oa = i, ob = j)

(12.6.18)

(12.6.19)

so [p ]ij is the id203 that a makes outcome i and b makes outcome j. then,

(cid:90)

(cid:90)

p(o|hdep) =

p(o, p|hdep)dp =

p(o|p, hdep)p(p|hdep)dp

where, for convenience, we write o = (oa, ob). assuming a dirichlet prior on p, with hyperparameters u,
we have

p(o, hdep) = p(hdep)

z(vec (u + (cid:93)))

z(vec(u))

(12.6.20)

where vec(d) is a vector formed from concatenating the rows of the matrix d. here (cid:93) is the count matrix,
with [(cid:93)]ij equal to the number of times that joint outcome (oa = i, ob = j) occurred in the n datapoints.
we assume the uniform prior, [u ]ij = 1,   i, j.

testing for dependencies in the outcomes: hdep versus hindep

to test whether or not the outcomes of the classi   ers are dependent hdep against the hypothesis that they
are independent hindep we may use, assuming p(hindep) = p(hdep),

p(hindep|o)
p(hdep|o)

=

z(u + (cid:93)a)

z(u + (cid:93)b)

z(vec(u))

z(u)

z(u)

z(vec (u + (cid:93)))

example 12.5 (hdep versus hindep).

draft november 9, 2017

(12.6.21)

291

bayesian hypothesis testing for outcome analysis

    consider the outcome count matrix (cid:93)

       98

7

93
168 13 163
245 12 201

so that (cid:93)a = [511, 32, 457], and (cid:93)b = [198, 344, 458]. then

p(hindep|o)
p(hdep|o)

= 3020

- strong evidence that the classi   ers perform independently.

    consider the outcome count matrix (cid:93)

       82

120 83
107 162
4
170 203 70

      

      

so that (cid:93)a = [359, 485, 156], and (cid:93)b = [284, 273, 443]. then

p(hindep|o)
p(hdep|o)

   18

= 2    10

- strong evidence that the classi   ers perform dependently.

these results are in fact consistent with the way the data was generated in each case.

(12.6.22)

(12.6.23)

(12.6.24)

(12.6.25)

12.6.5 is classi   er a better than b?

we return to the question with which we began this outcome analysis. given the common scenario of observ-
ing a number of (binary) errors for classi   er a on a test set and a number for b, can we say which classi   er
is better? this corresponds to the special case of binary classes q = 2 with dom(e) = {correct, incorrect}.
then for   a being the id203 that classi   er a generates a correct label, and similarly for   b, one way to
judge this is to compare the hypotheses:

1. ha>b : oa and ob are from di   erent binomial distributions with corresponding probabilities   a,   b and

  a >   b.

2. hsame : oa and ob are from the same binomial distribution.

writing d = {oa, ob}, and assuming independent parameter priors, we have

(cid:90)

p(d|ha>b) =

using

p(oa|  a)p(ob|  b)p(  a)p(  b)

  a>  b

p(oa|  a) =   

(cid:93)a
correct
a

(1       a)(cid:93)a

incorrect ,

p(ob|  b) =   

(cid:93)b
correct
b

(1       b)(cid:93)b

incorrect

and beta distribution priors

p(  a) = b (  a|u1, u2) ,

p(  b) = b (  b|u1, u2)

then one may readily show, using the beta function b(x, y):

b(u1 + (cid:93)a

correct, u2 + (cid:93)a

correct, u2 + (cid:93)b

incorrect)

incorrect)b(u1 + (cid:93)b
(b(u1, u2))2

p(d|ha>b) =

292

(cid:90)

  a>  b

p(  a >   b|d, ha>b) (12.6.29)

draft november 9, 2017

(12.6.26)

(12.6.27)

(12.6.28)

bayesian hypothesis testing for outcome analysis

where

p(  a >   b|d, ha>b) =

(cid:90)

  a>  b

b (  a|u1 + (cid:93)a

correct, u2 + (cid:93)a

incorrect) b

(cid:16)

  b|u1 + (cid:93)b

correct, u2 + (cid:93)b

incorrect

(cid:17)

(12.6.30)

the integral above needs to be computed numerically     see betaxbiggery.m. for the hsame hypothesis,
using    =   a =   b:

(cid:90)

p(d|hsame) =

=

1

  u1+(cid:93)a

correct+(cid:93)b

b(u1, u2)
b(u1 + (cid:93)a

  

correct + (cid:93)b

incorrect+(cid:93)b

incorrect

correct(1       )u2+(cid:93)a
incorrect + (cid:93)b

incorrect)

correct, u2 + (cid:93)a
b(u1, u2)

(12.6.31)

(12.6.32)

the question of whether a is better than b can then be addressed by computing

p(d|ha>b)
p(d|hsame)

=

correct, u2 + (cid:93)a

b(u1 + (cid:93)a
b(u1, u2)b(u1 + (cid:93)a

incorrect)b(u1 + (cid:93)b

correct + (cid:93)b

correct, u2 + (cid:93)a

correct, u2 + (cid:93)b
incorrect + (cid:93)b

incorrect)
incorrect)

p(  a >   b|d, ha>b)

(12.6.33)

examining the quotient term, we see that this is related to the hindep hypothesis via

p(d|ha>b)
p(d|hsame)

=

p(d|hindep)
p(d|hsame)

p(  a >   b|d, ha>b)

(12.6.34)

where the second term decreases the hindep versus hsame bayes    factor. this is intuitive since ha>b places
more constraints on the parameter space than hindep.

example 12.6. classi   er a makes 20 errors, and 35 correct classi   cations, whereas classi   er b makes 27
errors and 28 correct classi   cations. using a    at prior u1 = u2 = 1 this gives

p(  a >   b|oa, ob, hindep) = betaxbiggery(1+35,1+20,1+28,1+27)= 0.909

the bayes    factor is then

p(d|ha>b)
p(d|hsame)

=

b(1 + 35, 1 + 20)b(1 + 28, 1 + 27)
b(1, 1)b(1 + 35 + 28, 1 + 20 + 27)

0.909 = 0.516

(12.6.35)

(12.6.36)

on the other hand if classi   er a makes 200 errors and 350 correct classi   cations, whilst classi   er b makes

270 errors and 280 correct classi   cations, we have

p(  a >   b|oa, ob, hsame) =betaxbiggery(1+350,1+200,1+280,1+270)= 1.0

and

p(d|ha>b)
p(d|hsame)

=

b(1 + 350, 1 + 200)b(1 + 280, 1 + 270)
b(1, 1)b(1 + 350 + 280, 1 + 200 + 270)

1.0 = 676

(12.6.37)

(12.6.38)

this demonstrates the intuitive e   ect that even though the proportion of correct/incorrect classi   cations
doesn   t change for the two scenarios, our con   dence in determining which is the better classi   er increases
with the amount of data. see also    g(12.11).

12.7 summary

    bayes    rule enables us to evaluate models based on how well they    t the data via the model likelihood.
    there is no need to explicitly penalise    complex    models in the bayesian approach since it automatically

incorporates an occam   s razor e   ect due to the integral over the posterior parameter distribution.

draft november 9, 2017

293

exercises

(a)

(b)

figure 12.11: two classi   ers a and b and their posterior distributions of the id203 that they
(a): for a with 35 correct and 20 incorrect labels,
classify correctly (using a uniform beta prior).
b (x|1 + 35, 1 + 20) (solid curve); b with 28 correct 27 incorrect b (y|1 + 28, 1 + 27) (dashed curve). also
plotted is the posterior assuming that both classi   ers are the same, b (x|1 + 35 + 28, 1 + 20 + 27), (dot-
ted curve). whilst the posterior overlap of the two classi   ers a and b is small, they both overlap with
the posterior for the assumption that the classi   ers are the same. for this reason, there is no signi   -
(b): for a with 350 correct and 200 incorrect labels (solid
cant evidence that a is better than b.
curve), b (x|1 + 350, 1 + 200); b with 280 correct 270 incorrect b (y|1 + 280, 1 + 270) (dashed curve). as
the amount of data increases the overlap between the distributions decreases and the certainty that one
classi   er is better than the other correspondingly increases.

    computing the model likelihood can be a complex task. in continuous parameter models, laplace   s method

provides a simple approximation, the bic being a cruder version of laplace   s approximation.

    assessing performance on the basis of a limited amount of data can be achieved using simple bayesian

hypothesis testing.

12.8 code

demobayeserroranalysis.m: demo for bayesian error analysis
betaxbiggery.m: p(x > y) for x     b (x|a, b), y     b (y|c, d)

12.9 exercises

exercise 12.1. write a program to implement the fair/biased coin tossing model selection example of
section(12.2.1) using a discrete domain for   . explain how to overcome potential numerical issues in dealing
with large nh and nt (of the order of 1000).

exercise 12.2. you work at dodder   s hedge fund and the manager wants to model next day    returns    yt+1
based on current day information xt. the vector of    factors    each day, xt captures essential aspects of the
market. he argues that a simple linear model

yt+1 =

wkxkt

(12.9.1)

should be reasonable and asks you to    nd the weight vector w, based on historical information
d = {(xt, yt+1), t = 1, . . . , t     1}. in addition he also gives you a measure of the    volatility      2
294

draft november 9, 2017

t for each day.

k(cid:88)

k=1

00.10.20.30.40.50.60.70.80.91012345678900.10.20.30.40.50.60.70.80.91051015202530exercises

1. under the assumption that the returns are i.i.d. gaussian distributed

t(cid:89)

t=2

(cid:16)

t(cid:89)

t=2

(cid:17)

p(y1:t|x1:t , w) =

p(yt|xt   1, w) =

n

yt wtxt   1,   2
t

(12.9.2)

explain how to set the weight vector w by maximum likelihood.

2. your hedge fund manager is however convinced that some of the factors are useless for prediction and
wishes to remove as many as possible. to do this you decide to use a bayesian model selection method
in which you use a prior

p(w|m ) = n (w 0, i)

(12.9.3)
where m = 1, . . . , 2k     1 indexes the model. each model uses only a subset of the factors. by
translating the integer m into a binary vector representation, the model describes which factors are to
be used. for example if k = 3, there would be 7 models

{0, 0, 1} ,{0, 1, 0} ,{1, 0, 0} ,{0, 1, 1} ,{1, 0, 1} ,{1, 1, 0} ,{1, 1, 1}

(12.9.4)

where the    rst model is yt = w3x3 with weight prior p(w3) = n (w3 0, 1). similarly model 7 would be
yt = w1x1 + w2x2 + w3x3 with p(w1, w2, w3) = n ((w1, w2, w3) (0, 0, 0), i3). you decide to use a    at
prior p(m ) = const. draw the hierarchical id110 for this model and explain how to    nd
the best model for the data using bayesian model selection by suitably adapting equation (12.4.7).

3. using the data dodder.mat, perform bayesian model selection as above for k = 6 and    nd which of

the factors x1, . . . , x6 are most likely to explain the data.

exercise 12.3. here we will derive the expression (12.4.7) and also an alternative form.

1. starting from

n(cid:89)

n=1

p(w)

p(yn|w, xn, k) = n (w 0, i/  )

(cid:16)

(cid:89)
n n
      
2 wtw

yn wt  (xn),   2(cid:17)
(cid:80)

1

    1
2  2

(2    2)n/2 e

@@

@@

=

1

(2       1)k/2 e
(cid:80)

n(yn)2

    1

e

    1
2  2

show that this can be expressed as

1

(2       1)k/2

where

a =   i +

1

(2    2)n/2 e
(cid:88)

1
  2

n

  (xn)  t(xn)

2 wtaw+btw

(cid:88)

n

b =

1
  2

yn  (xn)

n(yn   wt  (xn))2

(12.9.5)

(12.9.6)

(12.9.7)

(12.9.8)

2. by completing the square (see section(8.4.1)), derive (12.4.7).

3. since each yn, n = 1, . . . , n is linearly related through w and w is gaussian distributed, the joint vector
y1, . . . , yn is gaussian distributed. using gaussian propagation, result(8.3), derive an alternative
expression for log p(y1, . . . , yn|x1, . . . , xn ).

exercise 12.4. similar to example(12.3), three people classify images into 1 of three categories. each col-
umn in the table below represents the classi   cations of each image, with the top row being the class from per-
son 1, the middle from person 2 and the bottom from person 3. you wish to estimate p(class of image|person =
i), assuming that for person i each image class is drawn independently from this distribution.

1
1
1

3
3
2

1
1
1

3
2
1

1
2
1

1
3
3

3
3
2

2
3
2

2
2
2

3
3
3

1
3
1

1
2
2

1
2
1

1
2
2

1
2
1

1
1
1

1
2
2

1
1
3

1
3
3

2
2
2

draft november 9, 2017

295

assuming no prior preference amongst hypotheses and a uniform prior on counts, compute

p(persons 1, 2 and 3 classify di   erently)
p(persons 1, 2 and 3 classify the same)

exercises

(12.9.9)

exercise 12.5. consider a classi   er that makes r correct classi   cations and w wrong classi   cations. is
the classi   er better than random guessing? let d represent the fact that there are r right and w wrong
answers. assume also that the classi   cations are i.i.d.

1. show that under the hypothesis that the data is generated purely at random, the likelihood is

p(d|hrandom) = 0.5r+w

2. de   ne    to be the id203 that the classi   er makes an error. then

now consider

p(d|  ) =   r (1       )w
(cid:90)

p(d|hnon random) =

p(d|  )p(  )

  

show that for a beta prior, p(  ) = b (  |a, b)

p(d|hnon random) =

b(r + a, w + b)

b(a, b)

where b(a, b) is the beta-function.

3. considering the random and non-random hypotheses as a priori equally likely, show that

p(hrandom|d) =

0.5r+w

0.5r+w + b(r+a,w +b)

b(a,b)

(12.9.10)

(12.9.11)

(12.9.12)

(12.9.13)

(12.9.14)

4. for a    at prior a = b = 1 compute the id203 that for 10 correct and 12 incorrect classi   cations,
the data is from a purely random distribution (according to equation (12.9.14)). repeat this for 100
correct and 120 incorrect classi   cations.

5. show that the standard deviation in the number of errors of a random classi   er is 0.5   r + w and

relate this to the above computation.

exercise 12.6.
consider a distribution

our interest here is to discuss a method to learn the direction of an edge in a belief network.

++

p(x, y|  , my   x) = p(x|y,   x|y)p(y|  y)
(cid:89)

(cid:90)

where    are the parameters of the id155 tables. for a prior p(  ) = p(  x|y)p(  y) and i.i.d.
data d = {xn, yn, n = 1, . . . , n}, the likelihood of the data is
p(xn|yn,   x|y)p(yn|  y)

p(d|my   x) =

p(  x|y)p(  y)

(12.9.16)

(12.9.15)

  

n

for binary variables, x     {0, 1}, y     {0, 1},

p(y = 1|  y) =   y,

and beta distribution priors

p(  y) = b (  y|  ,   ) ,

p(x = 1|y,   x|y) =   1|y

p(  1|y) = b(cid:0)  1|y|  1|y,   1|y

(cid:1) ,

296

(12.9.17)

p(  1|1,   1|0) = p(  1|1)p(  1|0)

(12.9.18)

draft november 9, 2017

exercises

1. show that p(d|my   x) is given by

b((cid:93) (x = 1, y = 0) +   1|0, (cid:93) (x = 0, y = 0) +   1|0)

b((cid:93) (x = 1, y = 1) +   1|1, (cid:93) (x = 0, y = 1) +   1|1)

b(  1|0,   1|0)

b((cid:93) (y = 1) +   , (cid:93) (y = 0) +   )

b(  ,   )

  

b(  1|1,   1|1)

(12.9.19)

2. now derive a similar expression for the model with the edge direction reversed, p(d|mx   y), assuming

the same values for all hyperparameters of this reverse model.

3. using the above, derive a simple expression for the bayes    factor

p(d|my   x)
p(d|mx   y)

(12.9.20)

4. by choosing appropriate hyperparameters, give a numerical example that illustrates how to encode the
heuristic that if the table p(x|y)p(y) is    more deterministic    than p(y|x)p(x), then we should prefer the
model mx   y. show results for the situation

(cid:93) (x = 0, y = 0) = 10, (cid:93) (x = 0, y = 1) = 10, (cid:93) (x = 1, y = 0) = 0, (cid:93) (x = 1, y = 1) = 0

draft november 9, 2017

297

exercises

298

draft november 9, 2017

part iii

machine learning

299

introduction to part iii

machine learning is fundamentally about extracting value from large datasets. often
the motivation is ultimately to produce an algorithm that can either mimic or enhance
human/biological performance.

in part iii we begin by discussing some of the basic concepts in machine learning,
namely supervised and unsupervised learning. we then move on to discuss some
standard models in machine learning and related areas.

draft november 9, 2017

301

machine learning is a large    eld and the diagram denotes loose associations between subjects.
in part
iii we discuss some of the standard machine learning concepts including probabilistic variants of classical
algorithms.

302

draft november 9, 2017

machinelearningsupervisedlearningreinforce-mentlearningdimensionreductionpartialleastsquarescanonicalvariatesfisher   slineardisc.classi   cationdiscriminativegaussianprocesslogisticregressionsupportvectormachinegenerative(classconditional)naivebayesmixturemodelsfactoranalysismixturesnearestneighbourmixtureofexpertsregressiongaussianprocessparametricmodellinearnon-lineartime-seriespredictionsemi-supervisedlearningunsupervisedlearningtime-seriesmodellingmarkovchainarcontinuouscrfundirectedlatentmarkovchainid48discreteldscontinuoussldsmixeddimensionreductionlinearfactoranalysispcaplsanmfnonlinearvisualisationlatentvariablemodelsundercompleteovercompletesparsebasismixedmembershiplatentdirichletallocationcliquedecomp.anomalydetectionchapter 13

machine learning concepts

machine learning is the body of research related to automated large-scale data analysis. historically, the
   eld was centred around biologically inspired models and the long term goals of much of the community
are oriented to producing models and algorithms that can process information as well as biological systems.
the    eld also encompasses many of the traditional areas of statistics with, however, a strong focus on
mathematical models and also prediction. machine learning is now central to many areas of interest in
computer science and related large-scale information processing domains.

13.1 styles of learning

broadly speaking the main two sub   elds of machine learning are supervised learning and unsupervised
learning. in supervised learning the focus is on accurate prediction, whereas in unsupervised learning the
aim is to    nd compact descriptions of the data. in both cases, one is interested in methods that generalise
well to previously unseen data. in this sense, one distinguishes between data that is used to train a model
and data that is used to test the performance of the trained model, see    g(13.1). we discuss    rst the basic
characteristics of some learning frameworks before discussing supervised learning in more detail.

13.1.1 supervised learning

consider a database of face images, each represented by a vector1 x. along with each image x is an output
class y     {male, female} that states if the image is of a male or female. a database of 10000 such image-class
pairs is available, d = {(xn, yn) , n = 1, . . . , 10000}. the task is to make an accurate predictor y(x   ) of the
sex of a novel image x   . this is an example application that would be hard to program in a traditional
manner since formally specifying a rule that di   erentiates male from female faces is di   cult. an alternative
is to give example faces and their gender labels and let a machine automatically    learn    such a rule.

de   nition 13.1 (supervised learning). given a set of data d = {(xn, yn) , n = 1, . . . , n} the task is
to learn the relationship between the input x and output y such that, when given a novel input x    the
predicted output y    is accurate. the pair (x   , y   ) is not in d but assumed to be generated by the same
unknown process that generated d. to specify explicitly what accuracy means one de   nes a id168
l(ypred, ytrue) or, conversely, a utility function u =    l.
in supervised learning our interest is describing y conditioned on knowing x. from a probabilistic modelling
perspective, we are therefore concerned primarily with the conditional distribution p(y|x,d). the term
   supervised    indicates that there is a notional    supervisor    specifying the output y for each input x in the
available data d. the output is also called a    label   , particularly when discussing classi   cation.

1for an m    n face image with elements fmn we can form a vector by stacking the entries of the matrix.

303

train

test

styles of learning

figure 13.1: in training and evaluating a model, conceptually
there are two sources of data. the parameters of the model are
set on the basis of the train data only. if the test data is generated
from the same underlying process that generated the train data,
an unbiased estimate of the generalisation performance can be
obtained by measuring the test data performance of the trained
model. importantly, the test performance should not be used to
adjust the model parameters since we would then no longer have
an independent measure of the performance of the model.

predicting tomorrow   s stock price y(t + 1) based on past observations y(1), . . . , y(t ) is a form of supervised
learning. we have a collection of times and prices d = {(t, y(t)) , t = 1, . . . , t} where time t is the input
and the price y(t) is the output.

example 13.1. a father decides to teach his young son what a sports car is. finding it di   cult to explain
in words, he decides to give some examples. they stand on a motorway bridge and, as each car passes
underneath, the father cries out    that   s a sports car!    when a sports car passes by. after ten minutes, the
father asks his son if he   s understood what a sports car is. the son says,    sure, it   s easy   . an old red vw
beetle passes by, and the son shouts        that   s a sports car!   . dejected, the father asks        why do you say
that?   .    because all sports cars are red!   , replies the son.

this is an example scenario for supervised learning. here the father plays the role of the supervisor, and his
son is the    student    (or    learner   ). it   s indicative of the kinds of problems encountered in machine learning
in that it is not easy to formally specify what a sports car is     if we knew that, then we wouldn   t need
to go through the process of learning. this example also highlights the issue that there is a di   erence
between performing well on training data and performing well on novel test data. the main interest in
supervised learning is to discover an underlying rule that will generalise well, leading to accurate prediction
on new inputs. if there is insu   cient train data then, as in this scenario, generalisation performance may
be disappointing.

if the output is one of a discrete number of possible    classes   , this is called a classi   cation problem.
in
if the output is continuous, this is
classi   cation problems we will generally use c for the output.
called a regression problem. for example, based on historical information of demand for sun-cream in your
supermarket, you are asked to predict the demand for the next month.
in some cases it is possible to
discretise a continuous output and then consider a corresponding classi   cation problem. however, in other
cases it is impractical or unnatural to do this, for example if the output y is a high dimensional continuous
valued vector.

13.1.2 unsupervised learning

de   nition 13.2 (unsupervised learning). given a set of data d = {xn, n = 1, . . . , n} in unsupervised
learning we aim to    nd a plausible compact description of the data. an objective is used to quantify the
accuracy of the description. in unsupervised learning there is no special prediction variable so that, from a
probabilistic perspective, we are interested in modelling the distribution p(x). the likelihood of the model
to generate the data is a popular measure of the accuracy of the description.

example 13.2. a supermarket chain wishes to discover how many di   erent basic consumer buying
behaviours there are based on a large database of supermarket checkout data. items brought by a customer

304

draft november 9, 2017

styles of learning

on a visit to a checkout are represented by a (very sparse) 10000 dimensional vector x which contains a 1
in the ith element if the customer bought product i and 0 otherwise. based on 10 million such checkout

vectors from stores across the country, d =(cid:8)xn, n = 1, . . . , 107(cid:9) the supermarket chain wishes to discover

patterns of buying behaviour.

in the table each column represents the products bought by a
customer (7 customer records with the    rst 6 of the 10,000 prod-
ucts are shown). a 1 indicates that the customer bought that
item. we wish to    nd common patterns in the data, such as if
someone buys diapers they are also likely to buy aspirin.

co   ee

tea
milk
beer

diapers
aspirin

1
0
1
0
0
0

0
0
0
0
0
1

0
1
1
0
1
1

1
0
1
1
0
0

0
0
0
1
1
1

0
0
1
0
0
0

0
0
1
1
1
1

     
     
     
     
     
     

example 13.3 (id91).

the table represents a collection of unlabelled two-dimensional points.
by simply eye-balling the data, we can see that there are two apparent
clusters, one centred around (0,5) and the other around (35,45). a
reasonable compact description of the data is that is has two clusters,
one centred at (0,0) and one at (35,45), each with a standard deviation
of 10.

x1
x2

-2
7

-6
22

-1
1

11
1

-1
-8

46
52

33
40

42
33

32
54

45
39

13.1.3 anomaly detection

a baby processes a mass of initially confusing sensory data. after a while the baby begins to understand
her environment so that sensory data from the same environment is familiar or expected. when a strange
face presents itself, the baby recognises that this is not familiar and becomes upset. the baby has learned a
representation of the environment and can distinguish the expected from the unexpected; this is an example
of unsupervised learning. detecting anomalous events in industrial processes (plant monitoring), engine
monitoring and unexpected buying behaviour patterns in customers all fall under the area of anomaly
detection. this is also known as    novelty    detection.

13.1.4 online (sequential) learning
in the above situations we assumed that the data d was given beforehand. in online learning data arrives
sequentially and we continually update our model as new data becomes available. online learning may occur
in either a supervised or unsupervised context.

13.1.5 interacting with the environment

in certain situations, an agent may be able to interact in some manner with its environment. this interaction
can complicate but also enrich the potential for learning.

query (active) learning here the agent has the ability to request data from the environment. for
example, a predictor might recognise that it is less con   dently able to predict in certain regions of
the space x and therefore requests more training data in this region. active learning can also be
considered in an unsupervised context in which the agent might request information in regions where
p(x) is currently uninformative.

id23 in id23 an agent inhabits an environment in which it may
take actions. some actions may eventually be bene   cial (lead to food for example), whilst others may

draft november 9, 2017

305

   100102030405001020304050supervised learning

be disastrous (lead to being eaten for example). based on accumulated experience, the agent needs
to learn which action to take in a given situation in order to maximise the id203 of obtaining
a desired long term goal (long term survival, for example). actions that lead to long term rewards
need to be reinforced. id23 has connections with control theory, markov decision
processes and game theory. whilst we discussed mdps and brie   y mentioned how an environment
can be learned based on delayed rewards in section(7.9.2), we will not discuss this topic further and
refer the reader to specialised texts, for example [284].

13.1.6 semi-supervised learning

in machine learning, a common scenario is to have a small amount of labelled and a large amount of
unlabelled data. for example, it may be that we have access to many images of faces; however, only a small
number of them may have been labelled as instances of known faces. in semi-supervised learning, one tries
to use the unlabelled data to make a better classi   er than that based on the labelled data alone. this is
a common issue in many examples since often gathering unlabelled data is cheap (taking photographs, for
example). however, typically the labels are assigned by humans, which is expensive.

13.2 supervised learning

supervised and unsupervised learning are mature    elds with a wide range of practical tools and associated
theoretical analyses. our aim here is to give a brief introduction to the issues and    philosophies    behind the
approaches. we focus here on supervised learning and classi   cation in particular.

13.2.1 utility and loss
given a new input x   , the optimal prediction depends on how costly making an error is. this can be
in forming a decision function c(x   ) that will
quanti   ed using a id168 (or conversely a utility).
produce a class label for the new input x   , we don   t know the true class, only our surrogate for this, the
predictive distribution p(c|x   ). if u (ctrue, cpred) represents the utility of making a decision cpred when the

truth is ctrue, the expected utility for the decision function is

(cid:88)

ctrue

u (c(x

   

)) =

u (ctrue, c(x

   

))p(ctrue|x

   

)

and the optimal decision function c(x   ) is that which maximises the expected utility,

(13.2.1)

(13.2.2)

   

c(x

) = argmax
c(x   )

u (c(x

   

))

one may also consider equivalently a loss l(ctrue, c(x)), for which the expected loss with respect to p(c, x)
is then termed the risk . the optimal decision function is then that which minimises the risk with respect
to   .

zero-one loss/utility

) =

0 if c   

(cid:54)= ctrue

   
u (ctrue, c

for the two class case, we then have the expected utility equation (13.2.1) given by

a    count the correct predictions    measure of prediction performance is based on the zero-one utility (or
conversely the zero-one loss):

(cid:26) 1 if c    = ctrue
(cid:26) p(ctrue = 1|x   ) for c(x   ) = 1
p(ctrue = 2|x   ) for c(x   ) = 2
(13.2.4)
hence, in order to have the highest expected utility, the decision function c(x   ) should correspond to selecting
(cid:26) 1 if p(c = 1|x   ) > 0.5
the highest class id203 p(c|x   ):
2 if p(c = 2|x   ) > 0.5

(13.2.3)

(13.2.5)

u (c(x

)) =

) =

c(x

   

   

in the case of a tie, either class is selected at random with equal id203.

306

draft november 9, 2017

supervised learning

general loss/utility functions

in general, for a two-class problem, we have

u (c(x

   

)) =

(cid:26) u (ctrue = 1, c    = 1)p(ctrue = 1|x   ) + u (ctrue = 2, c    = 1)p(ctrue = 2|x   ) for c(x   ) = 1
u (ctrue = 1, c    = 2)p(ctrue = 1|x   ) + u (ctrue = 2, c    = 2)p(ctrue = 2|x   ) for c(x   ) = 2

(13.2.6)
and the optimal decision function c(x   ) chooses that class with highest expected utility. one can readily
generalise this to multiple-class situations using a utility matrix with elements

uij = u (ctrue = i, cpred = j)

(13.2.7)

where the i, j element of the matrix contains the utility of predicting class j when the true class is i. con-
versely one could think of a loss-matrix with entries lij =    uij. in some applications the utility matrix
is highly non-symmetric. consider a medical scenario in which we are asked to predict whether or not the
patient has cancer dom(c) = {cancer, benign}. if the true class is cancer yet we predict benign, this could
have terrible consequences for the patient. on the other hand, if the class is benign yet we predict cancer,
this may be less disastrous for the patient. such asymmetric utilities can favour conservative decisions    
in the cancer case, we would be more inclined to decide the sample is cancerous than benign, even if the
predictive id203 of the two classes is equal.
in solving for the optimal decision function c(x   ) in equation (13.2.5) we are assuming that the model p(c, x)
is correct. however, in practice we typically don   t know the correct model underlying the data     all we have
is a dataset of examples d = {(xn, cn) , n = 1, . . . , n} and our domain knowledge. we therefore need to
form a distribution p(c, x|d) which should ideally be close to the true but unknown joint data distribution
ptrue(c, x). only then can our decisions be expected to generalise well to examples outside of the train
data. communities of researchers in machine learning form around di   erent strategies to address the lack
of knowledge about ptrue(c, x).

squared loss/utilty

in regression problems, for a real-valued prediction ypred and truth ytrue, a common id168 is the
squared loss

(cid:16)

ytrue     ypred(cid:17)2

l(ytrue, ypred) =

(13.2.8)

the above decision framework then follows through, replacing summation with integration for the continuous
variables.

13.2.2 using the empirical distribution

a direct approach to not knowing the correct model ptrue(c, x) is to replace it with the empirical distribution

n(cid:88)

n=1

p(c, x|d) =

1
n

   (c, cn)    (x, xn)

(13.2.9)

(cid:88)

n

that is, we assume that the underlying distribution is approximated by placing equal mass on each of the
points (xn, cn) in the dataset. using this gives the empirical expected utility

(cid:104)u (c, c(x))(cid:105)p(c,x|d) =

1
n

or conversely the empirical risk

r =

1
n

l(cn, c(xn))

(cid:88)

n

draft november 9, 2017

u (cn, c(xn))

(13.2.10)

(13.2.11)

307

x ,c

p(x, c)

  

x   

c    = c(x   

|  )

(cid:104)l(c, c(x|  ))(cid:105)p(x,c) +   p (  )

supervised learning

figure 13.2: empirical risk approach. given the
dataset x ,c, a model of the data p(x, c) is made,
usually using the empirical distribution. for a
classi   er c(x|  ), the parameter    is learned by
minimising the penalised empirical risk with re-
spect to   . the penalty parameter    is set by
validation. a novel input x    is then assigned to
class c(x   

|  ), given this optimal   .

assuming the loss is minimal when the correct class is predicted, the optimal decision c(x) for any input in

the train set is given by c(xn) = cn. however, for any new x    not contained in d then c(x   ) is unde   ned.
in order to de   ne the class of a novel input, one may use a parametric function c(x|  ). for example for a
two class problem dom(c) = {1, 2}, a linear decision function is given by

if the vector input x is on the positive side of a hyperplane de   ned by the vector    and bias   0, we assign
it to class 1, otherwise to class 2. (we return to the geometric interpretation of this in chapter(17)). the
empirical risk then becomes a function of the parameters    = {  ,   0},

(cid:26) 1 if   tx +   0     0

2 if   tx +   0 < 0

c(x|  ) =

(cid:88)

n

r(  |d) =

1
n

l(cn, c(xn|  ))

the optimal parameters    are given by minimising the empirical risk with respect to   ,

  opt = argmin

  

r(  |d)

the decision for a new datapoint x    is then given by c(x   

|  opt).

in this empirical risk minimisation approach, as we make the decision function c(x|  ) more    exible, the
empirical risk goes down. however, if we make c(x|  ) too    exible we will have little con   dence that c(x|  )
will perform well on a novel input x   . the reason for this is that a    exible decision function c(x|  ) is one
for which the class label can change for only a small change in x. such    exibility seems good since it means
that we will be able to    nd a parameter setting    so that the train data is    tted well. however, since we are
only constraining the decision function on the known training points, a    exible c(x|  ) may change rapidly
as we move away from the train data, leading to poor generalisation. to constrain the complexity of c(x|  )
we may minimise the penalised empirical risk

(cid:48)

r

(  |d) = r(  |d) +   p (  )

(13.2.15)

where p (  ) is a function that penalises complex functions c(x|  ). the regularisation constant,   , determines
the strength of this penalty and is typically set by validation     see below. the empirical risk approach is
summarised in    g(13.2).

class label. the squared di   erence in   tx +   0 for two inputs x1 and x2 is(cid:0)  t   x(cid:1)2
(cid:68)(cid:0)  t   x(cid:1)2(cid:69)

for the linear decision function above, it is reasonable to penalise wildly changing classi   cations in the
sense that if we change the input x by only a small amount we expect (on average) minimal change in the
where    x     x2     x1.
by constraining the length of    to be small we limit the ability of the classi   er to change class for only a
small change in the input space. assuming the distance between two datapoints is distributed according
to an isotropic multivariate gaussian with zero mean and covariance   2i, the average squared change is
=   2  t  , motivating the choice of the euclidean squared length of the parameter    as the

penalty term, p (  ) =   t  .

308

draft november 9, 2017

(13.2.12)

(13.2.13)

(13.2.14)

supervised learning

train

validate

test

figure 13.3: models can be trained using the train data based on
di   erent regularisation parameters. the optimal regularisation
parameter is determined by the empirical performance on the
validation data. an independent measure of the generalisation
performance is obtained by using a separate test set.

algorithm 13.1 setting regularisation parameters using cross-validation.

2: choose a set of training and validation set splits(cid:8)

1: choose a set of regularisation parameters   1, . . . ,   a
di
train,di

(cid:9) , i = 1, . . . , k

validate

3: for a = 1 to a do
4:
5:

for i = 1 to k do

  i
a = argmin

(cid:2)r(  |di

train) +   ap (  )(cid:3)

  

(cid:80)k

l(  a)

end for
l(  a) = 1
k

6:
7:
8: end for
9:   opt = argmin

  a

i=1 r(  i

a|di

validate)

validation

in penalised empirical risk minimisation we need to set the regularisation constant   . this can be achieved
by evaluating the performance of the learned classi   er c(x|  ) on validation data dvalidate for several di   erent
   values, and choosing the    which gave rise to the classi   er with the best performance. it   s important that
the validation data is not the data on which the model was trained since we know that the optimal setting
for    in that case is zero, and again we will have little con   dence in the generalisation ability.

given a dataset d we split this into disjoint parts, dtrain,dvalidate, where the size of the validation set is
usually chosen to be smaller than the train set, see    g(13.3). for each parameter   a one then    nds the
minimal empirical risk parameter   a. the optimal    is chosen as that which gives rise to the model with
the minimal validation risk. using the optimal regularisation parameter   , many practitioners retrain    on
the basis of the whole dataset d.

in cross-validation the dataset is partitioned into training and validation sets multiple times with validation
results obtained for each partition. each partition produces a di   erent training di
validate
set, along with an optimal penalised empirical risk parameter   i
a and associated (unregularised) validation
performance r(  i
validate). the performance of regularisation parameter   a is taken as the average of the
validation performances over i. the best regularisation parameter is then given as that with the minimal
average validation error, see algorithm(13.1). more speci   cally, in k-fold cross-validation the data d is split
into k equal sized disjoint parts d1, . . . ,dk. then di
validate. this gives a
total of k di   erent training-validation sets over which performance is averaged, see    g(13.4). in practice

train and validation di

validate = di and di

train = d\di

a|di

train

validate

train

validate

train

validate

train

test

draft november 9, 2017

figure 13.4: in cross-validation the dataset is split into several
train-validation sets. depicted is 3-fold cross-validation. for
a range of regularisation parameters, the optimal regularisation
parameter is found based on the empirical validation performance
averaged across the di   erent splits.

309

supervised learning

(a)

(b)

figure 13.5: the true function which generated the noisy data is the dashed line; the function learned from
the data is given by the solid line. (a): the unregularised    t (   = 0) to training given by   . whilst
(b): the
the training data is well    tted, the error on the validation examples, denoted by +, is high.
regularised    t (   = 0.5). whilst the train error is high, the validation error is low.

10-fold cross-validation is popular, as is leave-one-out cross-validation in which the validation sets consist of
only a single example.

example 13.4 (finding a good regularisation parameter). in    g(13.5), we    t the function a sin(wx)
to
the plotted data, learning the parameters a, w based on minimising the squared loss. the unregularised
solution    g(13.5a) badly over   ts the data, and has a high validation error. to encourage a smoother solution,
a regularisation term w2 is used. the validation error based on several di   erent values of the regularisation
parameter    was computed, and    = 0.5 gives the lowest validation error. the resulting    t to the data,
found by retraining a and w using the validation-optimal   , is reasonable, see    g(13.5b).

bene   ts of the empirical risk approach

    in the limit of a large amount of training data the empirical distribution tends to the correct distri-

bution.

    the discriminant function is chosen on the basis of minimal risk, which is the quantity we are ultimately

interested in.

    the procedure is conceptually straightforward.

drawbacks of the empirical risk approach

    it seems extreme to assume that the data follows the empirical distribution, particularly for small
amounts of training data. more reasonable assumptions for p(x) would take into account likely x that
could arise, not just those in the train data.

    if the id168 changes, the discriminant function needs to be retrained.
    some problems require an estimate of the con   dence of the prediction. whilst there may be heuristic

ways to evaluating con   dence in the prediction, this is not inherent in the framework.

    when there are many penalty parameters, performing cross-validation in a discretised grid of the

parameters becomes infeasible.

    during validation, many models are trained, and all but one subsequently discarded.

13.2.3 bayesian decision approach

an alternative to using the empirical distribution is to    rst    t a model p(c, x|  ) to the train data d. given
this model, the decision function c(x) is automatically determined from the maximal expected utility (or
minimal risk) with respect to this model, as in equation (13.2.6), in which the unknown p(ctrue|x) is replaced
draft november 9, 2017
310

   3   2   10123   1.5   1   0.500.511.5   3   2   10123   1.5   1   0.500.511.5supervised learning

x ,c

  

p(x, c|  )

x   

p(c|x   ,   )

(cid:104)l(c, c   )(cid:105)p(c|x   ,  )

c   

figure 13.6: bayesian decision approach. a
model p(x, c|  ) is    tted to the data. after learn-
ing the optimal model parameters   , we compute
p(c|x,   ). for a novel x   , the distribution of the
assumed    truth    is p(c|x   ,   ). the prediction (de-
cision) is then given by that c    which minimises
the expected risk (cid:104)l(c, c   )(cid:105)p(c|x   ,  ).

with p(c|x,   ). see    g(13.6) for a schematic depiction of the bayesian decision approach.
there are two main approaches to    tting p(c, x|  ) to data d, see    g(13.7). we could parameterise the joint
distribution using

p(c, x|  ) = p(c|x,   c|x)p(x|  x)

discriminative approach

or

p(c, x|  ) = p(x|c,   x|c)p(c|  c)

generative approach

(13.2.16)

(13.2.17)

we   ll consider these two approaches below in the context of trying to make a system that can distinguish
between a male and female face. the setup is that we have a database of face images in which each image
is represented as a real-valued vector xn, n = 1, . . . , n ), along with a label cn     {0, 1} stating if the image
is male or female.

generative approach p(x, c|  ) = p(x|c,   x|c)p(c|  c)
for simplicity we use maximum likelihood training for the parameters   . assuming the data d is i.i.d., we
have a log likelihood

log p(d|  ) =

log p(xn|cn,   x|c) +

log p(cn|  c)

(13.2.18)

(cid:88)

n

(cid:88)

n

as we see the dependence on   x|c occurs only in the    rst term, and   c only occurs in the second. this
means that learning the optimal parameters is equivalent to isolating the data for the male-class and    t-
ting a model p(x|c = male,   x|male). we may similarly isolate the female data and    t a separate model
p(x|c = female,   x|female). the class distribution p(c|  c) is set according to the ratio of males/females in the
set of training data.
to make a classi   cation of a new image x    as either male or female, we use bayes    rule:

p(c = male|x   

) =

p(x   , c = male|  x|male)

p(x   , c = male|  x|male) + p(x   , c = female|  x|female)

(13.2.19)

  c

cn

  x|c

  c|x

  x

xn

n

cn

xn

n

(a)

(b)

figure 13.7: two generic strategies for probabilistic
classi   cation. (a): class dependent generative model
of x. after learning parameters, classi   cation is ob-
tained by making x evidential and inferring p(c|x).
(b): a discriminative classi   cation method p(c|x).

draft november 9, 2017

311

supervised learning

based on zero-one loss, if this id203 is greater than 0.5 we classify x    as male, otherwise female. for a
general id168, we use this id203 as part of a decision process, as in equation (13.2.6).

advantages prior information about the structure of the data is often most naturally speci   ed through
a generative model p(x|c). for example, for male faces, we would expect to see heavier eyebrows, a
squarer jaw, etc.

disadvantages the generative approach does not directly target the classi   cation model p(c|x) since the
goal of generative training is rather to model p(x|c).
if the data x is complex,    nding a suitable
generative data model p(x|c) is a di   cult task. furthermore, since each generative model is separately
trained for each class, there is no competition amongst the models to explain the x data. on the
other hand it might be that making a model of p(c|x) is simpler, particularly if the decision boundary
between the classes has a simple form, even if the data distribution of each class is complex, see
   g(13.8).

discriminative approach p(x, c|  ) = p(c|x,   c|x)p(x|  x)
assuming i.i.d. data, the log likelihood is
log p(cn|xn,   c|x) +

log p(xn|  x)

log p(d|  ) =

(cid:88)

(cid:88)

n

n

(13.2.20)

the parameters are isolated in the two terms so that maximum likelihood training is equivalent to    nding
the parameters of   c|x that will best predict the class c for a given training input x. the parameters   x
for modelling the data occur only in the second term above, and setting them can therefore be treated
as a separate unsupervised learning problem. this approach consequently isolates modelling the decision
boundary from modelling the input distribution, see    g(13.8).
classi   cation of a new point x    is based on

p(c|x,   opt
c|x )

(13.2.21)

as for the generative case, this approach still learns a joint distribution p(c, x) = p(c|x)p(x) which can be
used as part of a decision process if required, equation (13.2.6).

advantages the discriminative approach directly addresses    nding an accurate classi   er p(c|x) based on
modelling the decision boundary, as opposed to the class conditional data distribution in the generative
approach. whilst the data from each class may be distributed in a complex way, it could be that the
decision boundary between them is relatively easy to model.

disadvantages discriminative approaches are usually trained as    black-box    classi   ers, with little prior
knowledge built used to describe how data for a given class is distributed. domain knowledge is often
more easily expressed using the generative framework.

hybrid generative-discriminative approaches

one could use a generative description, p(x|c), building in prior information, and use this to form a joint
distribution p(x, c), from which a discriminative model p(c|x) may be formed, using bayes    rule. speci   cally,
we can use

(cid:80)
p(x|c,   x|c)p(c|  c)
c p(x|c,   x|c)p(c|  c)

subsequently the parameters    =(cid:0)  x|c,   c

p(c|x,   ) =

(cid:1), for this hybrid model can be found by maximising the id203

(13.2.22)

of being in the correct class. a separate model is learned for p(x|  x). this approach would appear to leverage
the advantages of both the discriminative and generative frameworks since we can more readily incorporate
domain knowledge in the generative model p(x|c,   x|c) yet train this in a discriminative way. this approach
is rarely taken in practice since the resulting functional form of the likelihood depends in a complex manner
on the parameters. in this case no parameter separation between   c and   x|c occurs (as was previously the
case for the generative and discriminative approaches).

312

draft november 9, 2017

supervised learning

figure 13.8: each point represents a high dimensional vector with an
associated class label, either male or female. the point x    is a new
point for which we would like to predict whether this should be male or
female. in the generative approach, a male model p(x|male) generates
data similar to the    m    points. similarly, the female model p(x|female)
generates points that are similar to the    f    points above. we then use
bayes    rule to calculate the id203 p(male|x   ) using the two    tted
make a model of p(male|x   ), which cares less about how the points    m   

models, as given in the text. in the discriminative approach, we directly

or    f    are distributed, but more about describing the boundary which
can separate the two classes, as given by the line. in this case, modelling
the distribution of the data is not required since our decisions are only
related to which side of the decision boundary a point lies.

features and preprocessing

it is often the case that in discriminative training, transforming the raw input x into a form that more
directly captures the relevant label information can greatly improve performance. for example, in the male-
female classi   cation case, it might be that building a classi   er directly in terms of the elements of the face
vector x is di   cult. however, using    features    which contain geometric information such as the distance
between eyes, width of mouth, etc. may make    nding a classi   er easier.
in practice data is also often
preprocessed to remove noise, centre an image etc.

learning lower-dimensional representations in semi-supervised learning

one way to exploit a large amount of unlabelled training data to improve classi   cation is to    rst    nd a
lower dimensional representation h of the data x. based on this, the mapping from h to c may be rather
simpler to learn than a mapping from x to c directly. we use cn =     to indicate that the class for datapoint
n is missing. we can then form the likelihood on the visible data using, see    g(13.9),

@@

p(c,x ,h|  ) =

p(xn|hn,   x|h)

p(hn|  h)

and set any parameters for example by using maximum likelihood

(cid:89)
(cid:8)p(cn|hn,   c|h)(cid:9)i[cn(cid:54)=   ]
(cid:88)

n

p(c,x ,h|  )

h

  opt = argmax

  

(13.2.23)

(13.2.24)

bene   ts of the bayesian decision approach

    this is a conceptually    clean    approach, in which one tries ones best to model the environment (using
either a generative or discriminative approach), independent of the subsequent decision process. in
this case learning the environment is separated from the e   ect this will have on the expected utility.

    the decision c    for a novel input x    can be a highly complex function of x    due to the maximisation
    if p(x, c|  ) is the    true    model of the data, this approach is optimal.

operation.

drawbacks of the bayesian decision approach

the environment is divorced from prediction.

    if the environment model p(c, x|  ) is poor, the prediction c    could be highly inaccurate since modelling
    to avoid fully divorcing the learning of the model p(c, x|  ) from its e   ect on decisions, in practice
one often includes regularisation terms in the environment model p(c, x|  ) which are set by validation
based on an empirical loss.

draft november 9, 2017

313

*mmmmmmmffffffffffxbayes versus empirical decisions

  c|h

  h

  x|h

cn

hn

xn

n

figure 13.9: a strategy for semi-supervised learning. when cn is miss-
ing, the term p(cn|hn) is absent. the large amount of training data
helps the model learn a good lower dimension/compressed representa-
tion h of the data x. fitting then a classi   cation model p(c|h) using
this lower dimensional representation may be much easier than    tting
a model directly from the complex data to the class, p(c|x).

13.3 bayes versus empirical decisions

the empirical risk and bayesian approaches are at the extremes of the philosophical spectrum.
in the
empirical risk approach one makes a seemingly over-simplistic data generating assumption. however decision
function parameters are set based on the task of making decisions. on the other hand, the bayesian approach
attempts to learn meaningful p(c, x) without regard to its ultimate use as part of a larger decision process.
what    objective    criterion can we use to learn p(c, x), particularly if we are only interested in classi   cation
with a low test-risk? the following example is intended to recapitulate the two generic bayes and empirical
risk approaches we   ve been considering. note that we   ve previously written utilities suggesting that they
are both for example class labels; however the theory applies more generally, for example to utilities u(c, d)
where d is some decision (which is not necessarily of the form of the class label c).

example 13.5 (the two generic decision strategies). consider a situation in which, based on patient
information x, we need to take a decision d as whether or not to operate. the utility of operating u(c, d)
depends on whether or not the patient has cancer, c. for example

u(cancer, operate) = 100
u(cancer, don   t operate) = 0 u(benign, don   t operate) = 70

u(benign, operate) = 30

(13.3.1)

we have independent true assessments of whether or not a patient had cancer, giving rise to a set of
historical records d = {(xn, cn), n = 1, . . . , n}. faced with a new patient with information ,x, we need to
make a decision whether or not to operate.

in the bayesian decision approach one would    rst make a model p(c|x,d) (for example using a discriminative
model such as id28, section(17.4.1)). using this model the decision is given by that which
maximises the expected utility

d = argmax

d

[p(cancer|x,d)u(cancer, d) + p(benign|x,d)u(benign, d)]

(13.3.2)

in this approach learning the model p(c|x,d) is divorced from the ultimate use of the model in the decision
making process. an advantage of this approach is that, from the viewpoint of expected utility, it is optimal
    provided the model p(c|x,d) is    correct   . unfortunately, this is rarely the case. given the limited model
resources, it might make sense to focus on ensuring the prediction of cancer is correct since this has a more
signi   cant e   ect on the utility. however, formally, this would require a corruption of this framework.

the alternative empirical utility approach recognises that the task can be stated as to translate patient
information x into an operation decision d. to do so one could parameterise this as d(x) = f (x|  ) and then
learn    under maximising the empirical utility

u(  ) =

u(f (xn|  ), cn)

(13.3.3)

(cid:88)

n

for example, if x is a vector representing the patient information and    the parameter, we might use a
linear decision function such as

(cid:26)   tx     0 d = operate

  tx < 0 d = don   t operate

f (x|  ) =

314

(13.3.4)

draft november 9, 2017

exercises

the advantage of this approach is that the parameters of the decision are directly related to the utility of
making the decision. however, it may be that we have a good model of p(c|x) and would wish to make
use of this. a disadvantage is that we cannot easily incorporate such domain knowledge into the decision
function.

both approaches are heavily used in practice and which is to be preferred depends very much on the problem.
whilst the bayesian approach appears formally optimal, it is prone to model mis-speci   cation. a pragmatic
alternative bayesian approach is to    t a parameterised distribution p(c, x|  ) to the data d, where    penalises
complexity of the    tted distribution, setting    using validation on the risk. this has the potential advantage
of allowing one to incorporate sensible prior information about p(c, x) whilst assessing competing models
in the light of their actual predictive risk. similarly, for the empirical risk approach, one can modify the
extreme empirical distribution assumption by using a more plausible model p(c, x) of the data.

13.4 summary

    supervised and unsupervised learning are the two main branches of machine learning considered in this

book.

    the two classical approaches in supervised learning are empirical risk minimisation and bayesian decision

theory.

    in the empirical risk minimisation, there is typically no explicit model of the data, and the focus is on the

end use of the predictor.

    in the bayesian decision approach, an explicit model of the data is made, with the end decision/classi   cation

being computed independently of the    tting of the model to the data.

a general introduction to machine learning is given in [216]. an excellent reference for bayesian decision
theory is [34]. approaches based on empirical risk are discussed in [303].

13.5 exercises

exercise 13.1. given the distributions p(x|class 1) = n
corresponding prior occurrence of classes p1 and p2 (p1+p2 = 1), calculate the decision boundary p(class1|x) =
0.5 explicitly as a function of   1,   2,   2
2, p1, p2. how many solutions are there to the decision boundary
and are they all reasonable?

1,   2

1

2

(cid:0)x   1,   2

(cid:1) and p(x|class 2) = n

(cid:0)x   2,   2

(cid:1), with

exercise 13.2. under zero-one loss, the bayes    decision rule chooses class k if p(class k|x) > p(class j|x)
imagine instead we use a randomized decision rule, choosing class j with id203
for all j (cid:54)= k.
q(class j|x). calculate the error for this decision rule, and show that the error is minimized by using bayes   
decision rule.

exercise 13.3. for a novel input x, a predictive model of the class c is given by p(c = 1|x) = 0.7,
p(c = 2|x) = 0.2, p(c = 3|x) = 0.1. the corresponding utility matrix u (ctrue, cpred) has elements

       5

0
   3 0

3
1
4    2
10

      
(cid:0)x m1,   2(cid:1) and class 2 has the distribution p(x|c = 2)     n

in terms of maximal expected utility, which is the best decision to take?

exercise 13.4. consider datapoints generated from two di   erent classes. class 1 has the distribution
p(x|c = 1)     n
ities of each class are p(c = 1) = p(c = 2) = 1/2. show that the posterior id203 p(c = 1|x) is of the
draft november 9, 2017
315

(cid:0)x m2,   2(cid:1). the prior probabil-

(13.5.1)

form

p(c = 1|x) =

1

1 + exp   (ax + b)

and determine a and b in terms of m1, m2 and   2.

exercises

(13.5.2)

exercise 13.5. wowco.com is a new startup prediction company. after years of failures, they eventually
   nd a neural network with a trillion hidden units that achieves zero test error on every learning problem
posted on the internet up to last week. each learning problem included a train and test set. proud of their
achievement, they market their product aggressively with the claim that it    predicts perfectly on all known
problems   . discuss whether or not these claims would justify buying this product.
exercise 13.6. for a prediction model   p(y|x) and true data generating distribution p(x, y), one may de   ne
a measure of the accuracy as

(cid:90)

a =

x,y

p(x, y)  p(y|x)

(13.5.3)

(13.5.4)

(13.5.5)

(13.5.6)

(13.5.7)

(13.5.8)

which is the average overlap between the true and predicting distributions.

1. by de   ning

  p(x, y)    
and considering

p(x, y)  p(y|x)

a

kl(q(x, y)|  p(x, y))     0

show that for any distribution q(x, y),

log a     (cid:104)log   p(y|x)(cid:105)q(x,y)     kl(q(x, y)|p(x, y))

1
n

n(cid:88)
n(cid:88)

n=1

n=1

q(x, y) =

show that

   (x, xn)    (y, yn)

log a    

1
n

log   p(yn|xn)     kl(q(x, y)|p(x, y))

2. consider a set of train data d = {(xn, yn), n = 1, . . . , n} and de   ne the empirical distribution

this shows that the log prediction accuracy is lower bounded by the training accuracy and the    gap   
between the empirical distribution and the unknown true data generating mechanism. according to
this naive bound (which doesn   t account for possible over   tting), the best thing to do to increase the
prediction accuracy is to increase the training accuracy (since the kullback-leibler term is indepen-
dent of the predictor). as n increases, the empirical distribution tends to the true distribution and
the kullback-leibler term becomes small, justifying minimising the train error.

n(cid:88)

n=1

log a    

1
n

assuming that the train outputs are drawn from a distribution p(y|x) =    (y, f (x)) which is determin-
istic, show that

log   p(yn|xn)     kl(q(x)|p(x))

(13.5.9)

and hence that, provided the train data is correctly predicted with full certainty, the accuracy can be
related to the empirical and true input distribution by

a     exp (   kl(q(x)|p(x)))

(13.5.10)

exercise 13.7. you wish to make a classi   er for a variable c based on two kinds of inputs, x and y. you
have a generative model p(y|c) and a discriminative model p(c|x). explain how to combine these to make a
model p(c|x, y).

316

draft november 9, 2017

chapter 14

nearest neighbour classi   cation

often when faced with a classi   cation problem it is useful to    rst employ a simple method to produce a
baseline against which more complex methods can be compared. in this chapter we discuss the simple nearest
neighbour method. the nearest neighbour methods are extremely popular and can perform surprisingly well.
we also discuss how these methods are related to probabilistic mixture models.

14.1 do as your neighbour does

successful prediction typically relies on smoothness in the data     if the class label can change as we move
a small amount in the input space, the problem is essentially random and no algorithm will generalise well.
in machine learning one constructs appropriate measures of smoothness for the problem at hand and hopes
to exploit this to obtain good generalisation. nearest neighbour methods are a useful starting point since
they readily encode basic smoothness intuitions and are easy to program.

in a classi   cation problem each input vector x has a corresponding class label, cn     {1, . . . , c}. given a
dataset of n train examples, d = {xn, cn} , n = 1, . . . , n , and a novel x, we aim to return the correct class
c(x). a simple, but often e   ective, strategy for this supervised learning problem can be stated as: for novel
x,    nd the nearest input in the train set and use the class of this nearest input, algorithm(14.1). for vectors
x and x(cid:48) representing two di   erent datapoints, we measure    nearness    using a dissimilarity function d(x, x(cid:48)).
a common dissimilarity is the squared euclidean distance

d(x, x(cid:48)

) = (x     x(cid:48)

)t(x     x(cid:48)

)

(14.1.1)

which can be more conveniently written (x     x(cid:48))2. based on the squared euclidean distance, the decision

boundary is determined by the perpendicular bisectors of the closest training points with di   erent training
labels, see    g(14.1). this partitions the input space into regions classi   ed equally and is called a voronoi
tessellation.

the nearest neighbour algorithm is simple and intuitive. there are, however, some issues:

    how should we measure the distance between points? whilst the euclidean square distance is popular,
this may not always be appropriate. a fundamental limitation of the euclidean distance is that it does
not take into account how the data is distributed. for example if the length scales of the components of
the vector x vary greatly, the largest length scale will dominate the squared distance, with potentially
useful class-speci   c information in other components of x lost. the mahalanobis distance

) =(cid:0)x     x(cid:48)(cid:1)t      1(cid:0)x     x(cid:48)(cid:1)

d(x, x(cid:48)

(14.1.2)

where    is the covariance matrix of the inputs (from all classes) can overcome some of these problems
since it e   ectively rescales the input vector components.

317

k-nearest neighbours

figure 14.1: in nearest neighbour classi   cation a new
vector is assigned the label of the nearest vector in the
training set. here there are three classes, with training
points given by the circles, along with their class. the
dots indicate the class of the nearest training vector.
the decision boundary is piecewise linear with each
segment corresponding to the perpendicular bisector
between two datapoints belonging to di   erent classes,
giving rise to a voronoi tessellation of the input space.

algorithm 14.1 nearest neighbour algorithm to classify a vector x, given train data d =
{(xn, cn), n = 1, . . . , n}:
1: calculate the dissimilarity of the test point x to each of the train points, dn = d (x, xn), n = 1, . . . , n .
2: find the train point xn   

which is nearest to x :

   

n

= argmin

n

d (x, xn)

3: assign the class label c(x) = cn   
4: in the case that there are two or more nearest neighbours with di   erent class labels, the most numerous class

.

is chosen. if there is no one single most numerous class, we use the k-nearest-neighbours.

    the whole dataset needs to be stored to make a classi   cation since the novel point must be compared
to all of the train points. this can be partially addressed by a method called data editing in which
datapoints which have little or no e   ect on the decision boundary are removed from the training
dataset. depending on the geometry of the training points,    nding the nearest neighbour can also be
accelerated by examining the values of each of the components xi of x in turn. such an axis-aligned
space-split is called a kd-tree[219] and can reduce the possible set of candidate nearest neighbours in
the training set to the novel x   , particularly in low dimensions.

    each distance calculation can be expensive if the datapoints are high dimensional. principal com-
ponents analysis, see chapter(15), is one way to address this and replaces x with a low dimensional
is then approximately given by
, see section(15.2.4). this is both faster to compute and can also improve classi   cation

projection p. the euclidean distance of two datapoints (cid:0)xa     xb(cid:1)2
(cid:0)pa     pb(cid:1)2

accuracy since only the large scale characteristics of the data are retained in the pca projections.

    it is not clear how to deal with missing data or incorporate prior beliefs and domain knowledge.

14.2 k-nearest neighbours

if your neighbour is simply mistaken (has an incorrect training class label), or is not a particularly repre-
sentative example of his class, then these situations will typically result in an incorrect classi   cation. by
including more than the single nearest neighbour, we hope to make a more robust classi   er with a smoother
decision boundary (less swayed by single neighbour opinions). if we assume the euclidean distance as the
dissimilarity measure, the k-nearest neighbour algorithm considers a hypersphere centred on the test point
x. the radius of the hypersphere is increased until it contains exactly k train inputs. the class label c(x)
is then given by the most numerous class within the hypersphere, see    g(14.2).

318

draft november 9, 2017

k-nearest neighbours

choosing k

figure 14.2: in k-nearest neighbours, we centre a hypersphere around the
point we wish to classify (here the central dot). the inner circle corre-
sponds to the nearest neighbour, a square. however, using the 3 near-
est neighbours, we    nd that there are two round-class neighbours and one
square-class neighbour    and we would therefore classify the central point as
round-class. in the case of a tie, one may increase k until the tie is broken.

whilst there is some sense in making k > 1, there is certainly little sense in making k = n (n being the
number of training points). for k very large, all classi   cations will become the same     simply assign each
novel x to the most numerous class in the train data. this suggests that there is an optimal intermediate
setting of k which gives the best generalisation performance. this can be determined using cross-validation,
as described in section(13.2.2).

(a)

(b)

(c)

figure 14.3: some of the train examples of the digit zero (a), one (b) and seven (c). there are 300 train
examples of each of these three digit classes.

example 14.1 (handwritten digit example). consider two classes of handwritten digits, zeros and ones.
each digit contains 28    28 = 784 pixels. the train data consists of 300 zeros, and 300 ones, a subset
of which are plotted in    g(14.3a,b). to test the performance of the nearest neighbour method (based on
euclidean distance) we use an independent test set containing a further 600 digits. the nearest neighbour
method, applied to this data, correctly predicts the class label of all 600 test points. the reason for the high
success rate is that examples of zeros and ones are su   ciently di   erent that they can be easily distinguished.

a more di   cult task is to distinguish between ones and sevens. we repeat the above experiment, now
using 300 training examples of ones, and 300 training examples of sevens,    g(14.3b,c). again, 600 new test
examples (containing 300 ones and 300 sevens) were used to assess the performance. this time, 18 errors
are found using nearest neighbour classi   cation     a 3% error rate for this two class problem. the 18 test
points on which the nearest neighbour method makes errors are plotted in    g(14.4). if we use k = 3 nearest
neighbours, the classi   cation error reduces to 14     a slight improvement. as an aside, the best machine
learning methods classify real world digits (over all 10 classes) to an error of less than 1 per cent     better
than the performance of an    average    human.

draft november 9, 2017

319

a probabilistic interpretation of nearest neighbours

figure 14.4:    1    versus    7    classi   cation us-
ing the nn method. (top) the 18 out of
600 test examples that are incorrectly clas-
si   ed; (bottom) the nearest neighbours in
the training set corresponding to each test-
point above.

14.3 a probabilistic interpretation of nearest neighbours

(cid:88)

(cid:88)

consider the situation where we have data from two classes     class 0 and class 1. we make the following
mixture model for data from class 0 which places a gaussian on each datapoint:

p(x|c = 0) =

1
n0

n

n    class 0

1

1
n0

(2    2)d/2

n    class 0

   (x   xn)2/(2  2)
e

(14.3.1)

(cid:0)x xn,   2i(cid:1) =

(cid:88)

where d is the dimension of a datapoint x and n0 are the number of train points of class 0, and   2 is
the variance. this is a parzen estimator , and models the data as a uniform weighted sum of gaussian
distributions centred on the training points,    g(14.5).

similarly, for data from class 1:

p(x|c = 1) =

1
n1
to classify a new datapoint x   , we use bayes    rule

n    class 1

1
n1

n

(cid:0)x xn,   2i(cid:1) =

(cid:88)

1

(2    2)d/2

n    class 1

   (x   xn)2/(2  2)
e

(14.3.2)

p(c = 0|x   

) =

p(x   

p(x   

|c = 0)p(c = 0)

|c = 0)p(c = 0) + p(x   

|c = 1)p(c = 1)

(14.3.3)

the maximum likelihood setting of p(c = 0) is n0/(n0 + n1), and p(c = 1) = n1/(n0 + n1). an analogous

expression to equation (14.3.3) holds for p(c = 1|x   ). to see which class is most likely we may use the ratio

p(c = 0|x   )
p(c = 1|x   )

p(x   
p(x   

=

|c = 0)p(c = 0)
|c = 1)p(c = 1)

(14.3.4)

if this ratio is greater than one, we classify x    as 0, otherwise 1.
equation(14.3.4) is a complicated function of x   . however, if   2 is very small, the numerator, which is a
sum of exponential terms, will be dominated by that term for which datapoint xn0 in class 0 is closest to
the point x   . similarly, the denominator will be dominated by that datapoint xn1 in class 1 which is closest
to x   . in this case, therefore,

p(c = 0|x   )
p(c = 1|x   )    

e   (x      xn0 )2/(2  2)p(c = 0)/n0
e   (x      xn1 )2/(2  2)p(c = 1)/n1

e   (x      xn0 )2/(2  2)
e   (x      xn1 )2/(2  2)

=

(14.3.5)

taking the limit   2     0, with certainty we classify x    as class 0 if x    is closer to xn0 than to xn1. the

nearest (single) neighbour method is therefore recovered as the limiting case of a probabilistic generative
model, see    g(14.5).

figure 14.5: a probabilistic interpretation of nearest neighbours. for each
class we use a mixture of gaussians to model the data from that class p(x|c),
placing at each training point an isotropic gaussian of width   2. the width
of each gaussian is represented by the circle. in the limit   2     0 a novel
point (small black dot) is assigned the class of its nearest neighbour. for
   nite   2 > 0 the in   uence of non-nearest neighbours has an e   ect, resulting
in a soft version of nearest neighbours.

320

draft november 9, 2017

exercises

the motivation for k nearest neighbours is to produce a classi   cation that is robust against unrepresen-
tative single nearest neighbours. to ensure a similar kind of robustness in the probabilistic interpretation,
we may use a    nite value   2 > 0. this smoothes the extreme probabilities of classi   cation and means
that more points (not just the nearest) will have an e   ective contribution in equation (14.3.4). the exten-
sion to more than two classes is straightforward, requiring a class conditional generative model for each class.

by using a richer generative model of the data we may go beyond the parzen estimator approach. we will
examine such cases in some detail in later chapters, in particular chapter(20).

14.3.1 when your nearest neighbour is far away
for a novel input x    that is far from all training points, nearest neighbours, and its soft probabilistic variant
will con   dently classify x    as belonging to the class of the nearest training point. this is arguably opposite
to what we would like, namely that the classi   cation should tend to the prior probabilities of the class based
on the number of training data per class. a way to avoid this problem is, for each class, to include a    ctitious
large-variance mixture component at the mean of all the data, one for each class. for novel inputs close to
the training data, this extra    ctitious component will have no appreciable e   ect. however, as we move away
from the high density regions of the training data, this additional    ctitious component will dominate since
it has larger variance than any of the other components. as the distance from x    to each    ctitious class
point is the same, in the limit that x    is far from the training data, the e   ect is that no class information
from the position of x    occurs. see section(20.3.3) for an example.

14.4 summary

    nearest neighbour methods are general classi   cation methods.
    the nn method can be understood as a class conditional mixture of gaussians in the limit of a vanishingly

small covariance for each mixture component model.

14.5 code

nearneigh.m: k nearest neighbour
majority.m: find the majority entry in each column of a matrix
demonearneigh.m: k nearest neighbour demo

14.6 exercises

exercise 14.1. the    le nndata.mat contains training and test data for the handwritten digits 5 and 9.
using leave one out cross-validation,    nd the optimal k in k-nearest neighours, and use this to compute
the classi   cation accuracy of the method on the test data.

exercise 14.2. write a routine softnearneigh(xtrain,xtest,trainlabels,sigma) to implement soft
nearest neighbours, analogous to nearneigh.m. here sigma is the variance   2 in equation (14.3.1). the
   le nndata.mat contains training and test data for the handwritten digits 5 and 9. using leave one out
cross-validation,    nd the optimal   2 and use this to compute the classi   cation accuracy of the method on
the test data. hint: you may have numerical di   culty with this method. to avoid this, consider using the

logarithm, and how to numerically compute log(cid:0)ea + eb(cid:1) for large (negative) a and b. see also logsumexp.m.

exercise 14.3. the editor at yoman! (a    mens    magazine) has just had a great idea. based on the success
of a recent national poll to test iq, she decides to make a    beauty quotient    (bq) test. she collects as

draft november 9, 2017

321

exercises

many images of male faces as she can, taking care to make sure that all the images are scaled to roughly
the same size and under the same lighting conditions. she then gives each male face a bq score from 0
(   severely aesthetically challenged   ) to 100 (   generously aesthetically gifted   ). thus, for each real-valued
d-dimensional image x, there is an associated value b in the range 0 to 100. in total she collects n images
and associated scores, {(xn, bn) , n = 1, . . . , n}. one morning, she bounces into your o   ce and tells you the
good news : it is your task to make a test for the male nation to determine their beauty quotient. the
idea, she explains, is that a man can send online an image of their face x   , to yoman! and will immediately
receive an automatic bq response b   .

1. as a    rst step, you decide to use the k nearest neighbour method (knn) to assign a bq score b    to

a novel test image x   . describe how to determine the optimal number of neighbours k to use.

2. your line manager is pleased with your algorithm but is disappointed that it does not provide any simple
explanation of beauty that she can present in a future version of yoman! magazine. to address this,
you decide to make a model based on id75. that is

b = wtx

(14.6.1)

where w is a parameter vector chosen to minimise

(cid:88)

n

(cid:16)

bn     wtxn(cid:17)2

e(w) =

after training (   nding a suitable w), how can yoman! explain to its readership in a simple way what
facial features are important for determining one   s bq?

322

draft november 9, 2017

chapter 15

unsupervised linear dimension reduction

high-dimensional data is prevalent in machine learning and related areas. indeed, there often arises the
situation in which there are more data dimensions than there are data examples. in such cases we seek a
lower dimensional representation of the data. in this chapter we discuss some standard methods which can
also improve the prediction performance by removing    noise    from the representation.

15.1 high-dimensional spaces     low dimensional manifolds

in machine learning problems data is often high dimensional     images, bag-of-word descriptions, gene-
expressions etc. in such cases we cannot expect the training data to densely populate the space, meaning
that there will be large parts in which little is known about the data. for the hand-written digits from
chapter(14), the data is 784 dimensional and for binary valued pixels the number of possible images is
2784     10236. nevertheless, we would expect that only a handful of examples of a digit should be su   cient
(for a human) to understand how to recognise a 7. digit-like images must therefore occupy a highly
constrained volume in the 784 dimensions and we expect only a small number of degrees of freedom to
be required to describe the data to a reasonable accuracy. whilst the data vectors may be very high
dimensional, they will therefore typically lie close to a much lower dimensional    manifold    (informally, a
two-dimensional manifold corresponds to a warped sheet of paper embedded in a high dimensional space),
meaning that the distribution of the data is heavily constrained. here we concentrate on computationally
e   cient linear dimension reduction techniques in which a high dimensional datapoint x is projected down
to a lower dimensional vector y by

y = fx + const.

(15.1.1)

the non-square matrix f has dimensions dim (y)    dim (x), with dim (y) < dim (x). the methods in this
chapter are largely non-probabilistic, although many have natural probabilistic interpretations. for example,
pca is closely related to factor analysis, as described in chapter(21).

15.2 principal components analysis

if data lies close to a linear subspace, as in    g(15.1) we can accurately approximate each data point by using
vectors that span the linear subspace alone. in such cases we aim to discover a low dimensional co-ordinate
system in which we can approximately represent the data. we express the approximation for datapoint xn
as

j bj       xn
yn

(15.2.1)

323

m(cid:88)

j=1

xn     c +

n(cid:88)

d(cid:88)

principal components analysis

figure 15.1:
in linear dimension reduction a linear
subspace is    tted such that the average squared dis-
tance between datapoints (red rings) and their projec-
tions onto the plane (black dots) is minimal.

here the vector c is a constant and de   nes a point in the linear subspace and the bj are    basis    vectors that
span the linear subspace (also known as    principal component coe   cients    or    loadings   ). note that some
i are the
low dimensional coordinates of the data, forming a lower dimension yn for each datapoint n; collectively we

authors de   ne pca without the constant c. collectively we can write b =(cid:2)b1, . . . , bm(cid:3). the yn
can write these lower dimensional vectors as y =(cid:2)y1, . . . , yn(cid:3). equation(15.2.1) expresses how to    nd the

reconstruction   xn given the lower dimensional representation yn (which has components yn
i , i = 1, . . . , m ).
for a data space of dimension dim (x) = d, we hope to accurately describe the data using only a small
number m (cid:28) d of co-ordinates y.

to determine the best lower dimensional representation it is convenient to use the squared distance error
between x and its reconstruction   x:

e(b, y, c) =

i ]2
i       xn
[xn

n=1

it is straightforward to show that the optimal bias c is given by the mean of the data(cid:80)
we therefore assume that the data has been centred (has zero mean(cid:80)

i=1

n xn/n , exercise(15.1).
n xn = 0), so that we can set c to

(15.2.2)

zero, and concentrate on    nding the optimal basis b below.

15.2.1 deriving the optimal linear reconstruction

to    nd the best basis vectors b (de   ning [b]i,j = bj
wish to minimize the sum of squared di   erences between each vector x and its reconstruction   x:

i ) and corresponding low dimensional coordinates y, we

(cid:16)

(cid:17)

i    

j bj
yn

i

= trace

(x     by)t (x     by)

(15.2.3)

      xn
d(cid:88)
n(cid:88)
where x =(cid:2)x1, . . . , xn(cid:3).

e(b, y) =

n=1

i=1

      2

m(cid:88)

j=1

an important observation is that the optimal solution for b and y is not unique since the reconstruction
error e(b, y) depends only on the product by. indeed, without loss of generality, we may constrain b
to be an orthonormal matrix. to see this, consider an invertible transformation q of the basis b so that
  b     bq is an orthonormal matrix,   bt   b = i. since q is invertible, we may write by =   b   y on de   ning
  y = q   1y. since   y is unconstrained (because y is unconstrained), without loss of generality, we may
consider equation (15.2.3) under the orthonormality constraint btb = i, namely that the basis vectors are
mutually orthogonal and of unit length.

by di   erentiating equation (15.2.3) with respect to yn

k we obtain (using the orthonormality constraint on

324

draft november 9, 2017

   2024   2024   10123principal components analysis

b)

      xn

i    

(cid:88)

i

(cid:88)

j

       bk

i =

(cid:88)

i

j bj
yn

i

(cid:88)

j

yn
j

xn
i bk

i    

1
2

   
   yn
k

   

e(b, y) =

(cid:88)

i

=

i bk
xn
i     yn

k

(cid:88)
(cid:124) (cid:123)(cid:122) (cid:125)

bj
i bk
i

i

  jk

the squared error e(b, y) therefore has zero derivative when

(cid:88)

i

yn
k =

i xn
bk
i ,

which can be written as y = btx

(15.2.4)

we now substitute this solution into equation (15.2.3) to write the squared error as a function of b alone.
using

(x     by)t (x     by) = xtx     xtbbtx     xtbbtx + xtb btb(cid:124)(cid:123)(cid:122)(cid:125)

i

btx

(15.2.5)

the last two terms above therefore cancel. using then trace (abc) = trace (cab) we obtain

e(b) = trace

hence the objective becomes

i     bbt(cid:17)(cid:17)

(cid:16)

xxt(cid:16)
(cid:104)

e(b) = (n     1)

trace (s)     trace

n(cid:88)

(cid:16)

sbbt(cid:17)(cid:105)

(15.2.6)

(15.2.7)

where s is the sample covariance matrix of the data1. since we assumed the data is zero mean, this is

s =

1

n     1

xxt =

1

n     1

n=1

xn(xn)t

more generally, for non-zero mean data, we have

n(cid:88)

n=1

s =

1

n     1

(xn     m)(xn     m)t,

m =

1
n

n(cid:88)

n=1

xn

(15.2.8)

(15.2.9)

to minimise equation (15.2.7) under the constraint btb = i we use a set of lagrange multipliers l, so that
the objective is to minimize

(cid:16)

sbbt(cid:17)

(cid:16)

(cid:16)

(cid:17)(cid:17)

   trace

+ trace

l

btb     i

(15.2.10)

(neglecting the constant prefactor n     1 and the trace (s) term). since the constraint is symmetric, we can
assume that l is also symmetric. di   erentiating with respect to b and equating to zero we obtain that at
the optimum

sb = bl

(15.2.11)

we need to    nd matrices b and l that satisfy this equation. one solution is given when l is diagonal, in
which case this is a form of eigen-equation and the columns of b are the corresponding eigenvectors of s. in

this case, trace(cid:0)sbbt(cid:1) = trace (l), which is the sum of the eigenvalues corresponding to the eigenvectors

forming b. for this eigen solution, therefore

1

n     1

e(b) =    trace (l) + trace (s) =    

m(cid:88)

i=1

  i + const.

(15.2.12)

1here we use the unbiased sample covariance, simply because this is standard in the literature. if we were to replace this
with the sample covariance as de   ned in chapter(8), the only change required is to replace n     1 by n throughout, which has
no e   ect on the form of the solutions found by pca.

draft november 9, 2017

325

principal components analysis

figure 15.2: projection of two dimensional data using
one dimensional pca. plotted are the original dat-
apoints x (larger rings) and their reconstructions   x
(small dots) using 1 dimensional pca. the lines rep-
resent the orthogonal projection of the original data-
point onto the    rst eigenvector. the arrows are the
two eigenvectors scaled by the square root of their cor-
responding eigenvalues. the data has been centred to
have zero mean. for each    high dimensional    datapoint
x, the    low dimensional    representation y is given in
this case by the distance (possibly negative) from the
origin along the    rst eigenvector direction to the cor-
responding orthogonal projection point.

since we wish to minimise e(b), we therefore de   ne the basis using the eigenvectors with largest corre-
if we order the eigenvalues   1       2, . . ., the squared error is then given by, from
sponding eigenvalues.
equation (15.2.7)

1

n     1

e(b) = trace (s)     trace (l) =

m(cid:88)

d(cid:88)

i=1

  i    

d(cid:88)

i=1

i=m +1

  i =

  i

(15.2.13)

whilst the solution to this eigen-problem is unique, this only serves to de   ne the solution subspace since
one may rotate and scale b and y such that the value of the squared loss is exactly the same (since the
least squares objective depends only on the product by). the justi   cation for choosing the non-rotated
eigen solution is given by the additional requirement that the basis (columns of b) correspond to directions
of maximal variance, as explained in section(15.2.2).

15.2.2 maximum variance criterion

to break the invariance of least squares projection with respect to rotations and rescaling, we need an
additional criterion. one such is given by    rst searching for the single direction b such that the variance of
the data projected onto this direction is maximal amongst all possible such projections. this makes sense
since we are looking for    interesting    directions along which the data varies a lot. using equation (15.2.4)
for a single vector b we have

yn =

bixn
i

(15.2.14)

(cid:88)

i

the projection of a datapoint onto a direction b is btxn for a unit length vector b. hence the sum of
squared projections is

(cid:88)

(cid:16)

btxn(cid:17)2

(cid:34)(cid:88)

(cid:35)

= bt

xn (xn)t

b = (n     1)btsb

(15.2.15)

n

n

ignoring constants, this is the negative of equation (15.2.7) for a single basis vector b. since, optimally, b is
an eigenvector with sb =   b,
the squared projection becomes   (n     1). hence the optimal single b which
maximises the projection variance is given by the eigenvector corresponding to the largest eigenvalue of s.
under the criterion that the next optimal direction b(2) should be orthonormal to the    rst, one can readily
show that b(2) is given by the second largest eigenvector, and so on. this explains why, despite the squared
loss equation (15.2.7) being invariant with respect to arbitrary rotation (and scaling) of the basis vectors,
the ones given by the eigen-decomposition have the additional property that they correspond to directions
of maximal variance. these maximal variance directions found by pca are called the principal directions.

@@

15.2.3 pca algorithm

the routine for pca is presented in algorithm(15.1). in the notation of y = fx, the projection matrix
f corresponds to et. similarly for the reconstruction equation (15.2.1), the coordinate yn corresponds to

326

draft november 9, 2017

   2   10123   2   1.5   1   0.500.511.52principal components analysis

algorithm 15.1 principal components analysis to form an m -dimensional approximation of a dataset
{xn, n = 1, . . . , n}, with dim (xn) = d.
n(cid:88)
1: find the d    1 sample mean vector and d    d covariance matrix

n(cid:88)

1

m =

1
n

n=1

xn, s =

n     1

n=1

(xn     m)(xn     m)t

2: find the eigenvectors e1, . . . , ed of the covariance matrix s, sorted so that the eigenvalue of ei is larger than

ej for i < j. form the matrix e = [e1, . . . , em ].

3: the lower dimensional representation of each data point xn is given by

yn = et(xn     m)

4: the approximate reconstruction of the original datapoint xn is

5: the total squared error over all the training data made by the approximation is

xn     m + eyn
n(cid:88)

d(cid:88)

(xn       xn)2 = (n     1)

  j

j=m +1

n=1

(15.2.16)

(15.2.17)

(15.2.18)

@@

where   m +1 . . .

  d are the eigenvalues discarded in the projection.

etxn and bi corresponds to ei. the pca reconstructions are orthogonal projections of the data onto the
subspace spanned by the eigenvectors corresponding to the m largest eigenvalues of the covariance matrix,
see    g(15.2).

example 15.1 (reducing the dimension of digits). we have 892 examples of handwritten 5   s, where each
image consists of 28    28 real-values pixels, see    g(15.3). each image matrix is stacked to form a 784
dimensional vector, giving a 784    892 dimensional data matrix x. the covariance matrix of this data has
eigenvalue spectrum as plotted in    g(15.4), where we plot only the 100 largest eigenvalues. note how after
around 40 components, the mean squared reconstruction error is small, indicating that the data lies close
to a 40 dimensional linear subspace. the eigenvalues are computed using pca.m. the reconstructions using
di   erent numbers of eigenvectors (100, 30 and 5) are plotted in    g(15.3). note how using only a small
number of eigenvectors, the reconstruction more closely resembles the mean image.

example 15.2 (eigenfaces). in    g(15.5) we present example images for which we wish to    nd a lower
dimensional representation. using pca the mean and    rst 48    eigenfaces    are presented along with recon-

figure 15.3: top row : a selection of the
digit 5 taken from the database of 892 ex-
amples. plotted beneath each digit is the
reconstruction using 100, 30 and 5 eigen-
vectors (from top to bottom). note how
the reconstructions for fewer eigenvectors
express less variability from each other, and
resemble more a mean 5 digit.

draft november 9, 2017

327

structions of the original data using these eigenfaces, see    g(15.6). the pca respresentation was found
using the svd techniques, as described in section(15.3.2).

principal components analysis

15.2.4 pca and nearest neighbours classi   cation

in nearest neighbours classi   cation, chapter(14), we need to compute the distance between datapoints. for
high-dimensional data computing the squared euclidean distance between vectors can be expensive, and
also sensitive to noise. it is therefore often useful to project the data to a lower dimensional representation
   rst. for example, in making a classi   er to distinguish between the digit 1 and the digit 7, example(14.1),
we can form a lower dimensional representation and use this as a more robust representation of the data.
to do so we    rst ignore the class label to make a dataset of 1200 training points. each of the training points
xn is then projected to a lower dimensional pca representation yn. subsequently, any distance calculations
(xa     xb)2 are replaced by (ya     yb)2. to justify this, consider

(xa     xb)t(xa     xb)     (eya + m     eyb     m)t(eya + m     eyb     m)

= (ya     yb)tete(ya     yb)
= (ya     yb)t(ya     yb)

where the last equality is due to the orthonormality of eigenvectors, ete = i.

(15.2.19)

using 19 principal components (see example(15.3) as to why this number was chosen) and the nearest
neighbour rule to classify 1   s and 7   s gave a test-set error of 14 in 600 examples, compared to 18 from
the standard method on the non-projected data. a plausible explanation for this improvement is that the
new pca representation of the data is more robust since only the    interesting    directions in the space are
retained, with low variance directions discarded.

example 15.3 (finding the best pca dimension). there are 600 examples of the digit 1 and 600 examples
of the digit 7. we will use half the data for training and the other half for testing. the 600 training examples
were further split into a training set of 400 examples and a separate validation set of 200 examples. pca
was used to reduce the dimensionality of the inputs, and then nearest neighbours used to classify the 200
validation examples. di   erent reduced dimensions were investigated and, based on the validation results,
19 was selected as the optimal number of pca components retained, see    g(15.7). the independent test
error on 600 independent examples using 19 dimensions is 14.

15.2.5 comments on pca

the    intrinsic    dimension of data

how many dimensions should the linear subspace have? from equation (15.2.13), the reconstruction error
is proportional to the sum of the discarded eigenvalues.
if we plot the eigenvalue spectrum (the set of
eigenvalues ordered by decreasing value), we might hope to see a few large values and many small values. if
the data does lie close to an m dimensional linear subspace, we would see m large eigenvalues with the rest

figure 15.4: the digits data consists of 892 examples of the digit
5, each image being represented by a 784 dimensional vector.
plotted are 100 largest eigenvalues (scaled so that the largest
eigenvalue is 1) of the sample covariance matrix.

328

draft november 9, 2017

02040608010000.20.40.60.81eigenvalue numbereigenvaluehigh dimensional data

figure 15.5: 100 training images. each image consists of 92   
112 = 10304 greyscale pixels. the train data is scaled so that,
represented as an image, the components of each image sum to 1.

the average value of each pixel across all images is 9.70    10   5.

this is a subset of the 400 images in the full olivetti research
face database.

(a): svd reconstruc-
figure 15.6:
tion of the images in    g(15.5) using
a combination of the 49 eigen-images.
(b): the eigen-images are found us-
ing svd of the images in    g(15.5)
and taking the mean and 48 eigenvec-
tors with largest corresponding eigen-
value. the images corresponding to
the largest eigenvalues are contained
in the    rst row, and the next 7 in the
row below, etc. the root mean square

reconstruction error is 1.121    10   5,

a small improvement over plsa (see
   g(15.16)).

(a)

(b)

being very small. this gives an indication of the number of degrees of freedom in the data, or the intrinsic
dimensionality. directions corresponding to the small eigenvalues are then interpreted as    noise   .

non-linear dimension reduction

in pca we are presupposing that the data lies close to a linear subspace. is this really a good description?
more generally, we would expect data to lie on low dimensional non-linear subspace. also, data is often
clustered     examples of handwritten 4   s look similar to each other and form a cluster, separate from the 8   s
cluster. nevertheless, since linear dimension reduction is computationally relatively straightforward, this is
one of the most common id84 techniques.

15.3 high dimensional data

the computational complexity of computing the eigen-decomposition of a d    d matrix is o(cid:0)d3(cid:1). you
cannot exceed 500. one can exploit this fact to reduce the o(cid:0)d3(cid:1) complexity of the naive approach, as

might be wondering therefore how it is possible to perform pca on high dimensional data. for example, if
we have 500 images each of 1000  1000 = 106 pixels, the covariance matrix will be a 106  106 square matrix.
it would appear to be a signi   cant computational challenge to    nd the eigen-decomposition of this matrix
directly. in this case, however, since there are only 500 such vectors, the number of non-zero eigenvalues

described below.

draft november 9, 2017

329

high dimensional data

figure 15.7: finding the optimal pca dimension to use for clas-
sifying hand-written digits using nearest neighbours. 400 train-
ing examples are used, and the validation error plotted on 200
further examples. based on the validation error, we see that a
dimension of 19 is reasonable.

15.3.1 eigen-decomposition for n < d

first note that for zero mean data, the sample covariance matrix can be expressed as

n(cid:88)

1

[s]ij =

xn
i xn
j

n     1

n=1

in matrix notation this can be written

1

xxt

s =

n     1

x =(cid:2)x1, . . . , xn(cid:3)

where the d    n matrix x contains all the data vectors:

(15.3.1)

(15.3.2)

(15.3.3)

since the eigenvectors of a matrix m are equal to those of   m for scalar   , one can consider more simply
the eigenvectors of xxt. writing the d    n matrix of eigenvectors as e and the eigenvalues as an n    n
diagonal matrix   , the eigen-decomposition of the scaled covariance s satis   es

xxte = e       xtxxte = xte       xtx   e =   e  

(15.3.4)
where we de   ned   e = xte. the    nal expression above represents the eigenvector equation for xtx.

this is a matrix of dimensions n    n so that calculating the eigen-decomposition takes o(cid:0)n 3(cid:1) operations,
compared with o(cid:0)d3(cid:1) operations in the original high-dimensional space. we can therefore calculate the

eigenvectors   e and eigenvalues    of this matrix more easily. once found, we use the fact that the eigenvalues
of s are given by the diagonal entries of    and the eigenvectors by

e = x   e     1

(15.3.5)

15.3.2 pca via singular value decomposition

x = udvt

an alternative to using an eigen-decomposition routine to    nd the pca solution is to make use of the
singular value decomposition (svd) of an d    n dimensional matrix x. this is given by

(15.3.6)
where utu = id and vtv = in and d is a d    n diagonal matrix of the (positive) singular values. we
assume that the decomposition has ordered the singular values so that the upper left diagonal element of d
contains the largest singular value. the matrix xxt can then be written as

= u   dut

xxt = udvtvdtut

(15.3.7)
where   d     ddt is a d    d diagonal matrix with the n squared singular values on the    rst diagonal
entries, and zero elsewhere. since u   dut is in the form of an eigen-decomposition, the pca solution is
equivalently given by performing the svd decomposition of x, for which the eigenvectors are then given by
u, and corresponding eigenvalues by the square of the singular values.

@@

equation(15.3.6) shows that pca is a form of matrix decomposition method:

x = udvt     um dm vt

m

where um , dm , vm correspond to taking only the    rst m singular values of the full matrices.

(15.3.8)

330

draft november 9, 2017

020406080100024681012number of eigenvaluesnumber of errorslatent semantic analysis

figure 15.8: (top) document data for a dictionary containing 10 words and 2000 documents. black indicates
that a word was present in a document. the data consists of two distinct topics and a random background
topic. the    rst topic contains two sub-topics which di   er only in their usage of the    rst two words,    in   uenza   
and       u   . (bottom) the projections of each datapoint onto the two principal components.

15.4 latent semantic analysis

in the document analysis literature pca is also called latent semantic analysis and is concerned with
analysing a set of n documents. each document is represented by a vector

xn = (xn

1 , . . . , xn

d)t

(15.4.1)

of word occurrences. for example the    rst element xn
1 might count how many times the word    cat    appears
2 the number of occurrences of    dog   , etc. this bag of words 2 is formed by    rst choosing
in document n, xn
a dictionary of d words. the vector element xn
is the (possibly normalised) number of occurrences of the
i
word i in the document n. typically d will be large, of the order 106, and x will be very sparse since any
document contains only a small fraction of the available words in the dictionary. using (cid:93)i,n to represent the
number of times that term i occurs in document n, the term-frequency is de   ned as

an issue with this representation is that frequently occurring words such as    the    will dominate. to counter
this one can measure how unique a term i is by seeing how many documents contain the term, and de   ne
the inverse-document frequency

idfi     log

number of documents that contain term i

n

(15.4.3)

an alternative to the above term-frequency representation is the term frequency   inverse document frequency
(tf-idf) representation which is given by

xn
i = tfn

i    idfi

(15.4.4)

which gives high weight to terms that appear often in a document, but rarely amongst documents.

given a set of documents d, the aim in lsa is to form a lower dimensional representation of each document.
the whole document database is represented by the so-called term-document matrix

x =(cid:2)x1, . . . , xn(cid:3)

(15.4.2)

(15.4.5)

which has dimension d    n , see for example    g(15.8), with entries typically de   ned by either the term-
frequency, or tf-idf representation. an interpretation of pca in this case is that the principal directions
de   ne    topics   . pca is arguably suboptimal for document analysis since we would expect the presence
of a latent topic to contribute only positive counts to the data. a related version of pca in which the
decomposition is constrained to have positive elements is called plsa, and discussed in section(15.6).

2more generally one can consider term-counts, in which terms can be single words, or sets of words, or even sub-words.

draft november 9, 2017

331

(cid:93)i,n(cid:80)

i (cid:93)i,n

tfn
i    

200400600800100012001400160018002000246810200400600800100012001400160018002000123latent semantic analysis

example 15.4 (latent topic). we have a small dictionary containing the words in   uenza,    u, headache,
nose, temperature, bed, cat, dog, rabbit, pet. the database contains a large number of articles that discuss
ailments, and articles which seem to talk about the e   ects of in   uenza, in addition to some background
documents that are not speci   c to ailments; other documents also discuss pet related issues. some of the
more formal documents exclusively use the term in   uenza, whereas the other more    tabloid    documents use
the informal term    u. each document is represented by a 10-dimensional vector in which element i of that
vector is set to 1 if word i occurs in the document, and 0 otherwise. the data is represented in    g(15.8).
the data is generated using the arti   cial mechanism described in demolsi.m.

the result of using pca on this data is represented in    g(15.9) where we plot the eigenvectors, scaled by
their eigenvalue. to aid interpretability, we do not use the bias term c in equation (15.2.1). the    rst
eigenvector groups all the    ailment    words together, the second the    pet    words and the third deals with
the di   erent usage of the terms in   uenza and    u. note that unlike id91 models (see section(20.3) for
example) in pca (and related methods such as plsa) a datapoint can in principle be constructed from
many basis vectors, so that a document could represent a mixture of di   erent topics.

rescaling

in lsa it is common to scale the transformation so that the projected vectors have approximately unit
covariance (assuming centred data). using

y =    n     1d   1

m ut

m x

(15.4.6)

the covariance of the projections is obtained from

(cid:88)

1

n     1

n

(cid:88)
(cid:124)

n

yn (yn)t = d   1

m ut

m

um d   1

m = d   1

m ut

m uddtutum d   1

m     i

xn (xn)t

(cid:123)(cid:122)

xxt

(cid:125)

given y, the approximate reconstruction   x is

the euclidean distance between two points xa and xb is then approximately

  x =

1

   n     1
  xa,   xb(cid:17)

=

(cid:16)

d

um dm y

(cid:16)

ya     yb(cid:17)t

1

n     1

(15.4.7)

(cid:16)

ya     yb(cid:17)

d2
m

(cid:16)

ya     yb(cid:17)t

(cid:16)

ya     yb(cid:17)

   

1

n     1

dm ut

m um dm

it is common to ignore the d2
the projected space to be the euclidean distance between the y vectors.

m term (and 1/ (n     1) factor), and to consider a measure of dissimilarity in

15.4.1 information retrieval

consider a large collection of documents from the web, creating a database d. our interest it to    nd
the most similar document to a speci   ed query document. using a bag-of-words style representation for
document n, xn, and similarly for the query document, x    we address this task by    rst de   ning a measure
of dissimilarity between documents, for example

d(xn, xm) = (xn     xm)t (xn     xm)

one then searches for the document that minimises this dissimilarity:

nopt = argmin

n

d(xn, x   

)

332

(15.4.8)

(15.4.9)

draft november 9, 2017

pca with missing data

figure 15.9: hinton diagram of the eigenvector matrix e where each eigen-
vector column is scaled by the corresponding eigenvalue. the dark shaded
squares indicates positive and light shaded squares negative values (the area
of each square corresponds to the magnitude), showing that there are only
a few large eigenvalues. note that the overall sign of any eigenvector is
irrelevant. the    rst eigenvector corresponds to a topic in which the words
in   uenza,    u, headache, nose, temperature, bed are prevalent. the second
eigenvector denotes the    pet    topic words. the third eigenvector shows that
there is negative correlation between the occurrence of in   uenza and    u.
the interpretation of the eigenvectors as    topics    can be awkward since the
basis eigenvectors are by de   nition orthogonal. contrast this basis with
that found using psla with two components,    g(15.14).

and returns document xnopt as the result of the search query. the squared di   erence between two documents
can also be written

(cid:0)x     x(cid:48)(cid:1)t(cid:0)x     x(cid:48)(cid:1) = xtx + x(cid:48)tx(cid:48)

    2xtx(cid:48)

if, as is commonly done, the bag-of-words representations are scaled to have unit length,

  x =

x

   xtx

so that   xt  x = 1, the distance is

(cid:0)  x       x(cid:48)(cid:1)t(cid:0)  x       x(cid:48)(cid:1) = 2

(cid:16)

1       xt  x(cid:48)(cid:17)

and one may equivalently consider the cosine similarity

s(  x,   x(cid:48)

) =   xt  x(cid:48)

= cos (  )

where    is the angle between the unit vectors   x and   x(cid:48),    g(15.10).

(15.4.10)

(15.4.11)

(15.4.12)

(15.4.13)

a di   culty with using a bag-of-words representation is that the representation will have mostly zeros.
hence di   erences may be due to    noise    rather than any real similarity between the query and database
document. lsa helps alleviate this problem somewhat by using a lower dimensional representation y of
the high-dimensional x. the y capture the main variations in the data and are less sensitive to random
uncorrelated noise. using the dissimilarity de   ned in terms of the lower dimensional y is therefore more
robust and likely to result in the retrieval of more useful documents.

example 15.5. continuing the in   uenza example, someone who uploads a query document which uses the
term       u    might also be interested in documents about    in   uenza   . however, the search query term       u    does
not contain the word    in   uenza   , so how can one retrieve such documents? since the    rst component using
pca (lsa) groups all    in   uenza    terms together, if we use only the    rst component of the representation y
to compare documents, this will retrieve documents independent of whether the term       u    or    in   uenza    is
used.

15.5 pca with missing data

when values of the data matrix x are missing, the standard pca algorithm as described cannot be applied.
unfortunately, there is no    quick    x    pca solution when some of the xn
i are missing and more complex

draft november 9, 2017

333

1234567891010987654321pca with missing data

(a): two bag-of-word vectors. the euclidean distance
figure 15.10:
between the two is large. (b): normalised vectors. the euclidean distance
is now related directly to the angle between the vectors. in this case two
documents which have the same relative frequency of words will both have
the same dissimilarity, even though the number of occurrences of the words
is di   erent.

(a)

(b)

numerical procedures need to invoked. a simple approach in this case is to require the squared reconstruction
error to be small only for the existing elements of x. that is3

n(cid:88)

d(cid:88)

n=1

i=1

      xn

(cid:88)

j

  n
i

i    

j bj
yn

i

      2

e(b, y) =

(15.5.1)

di   erentiating with respect

@@

i = 1 if the ith entry of the nth vector is available, and is zero otherwise.

k , we    nd that the optimal weights satisfy

where   n
to ym

(cid:88)

(cid:88)

  m
i ym

j bj

i bk

i =

i,j

i

  m
i xm

i bk
i

(15.5.2)

@@

one then substitutes this expression into the squared error, and minimises the error with respect to b under
the orthonormality constraint. an alternative iterative optimisation procedure is as follows: first select a
random d    m matrix   b. then iterate until convergence the following two steps:
optimize y for    xed b

for    xed   b the above e(   b, y) is a quadratic function of the matrix y, which can be optimised
directly. by di   erentiating and equating to zero, one obtains the    xed point condition

n(cid:88)

d(cid:88)

n=1

i=1

      2

(cid:88)

j

e(   b, y) =

  n
i

i    

yn
j

  bj
i

      xn
(cid:33)
m(n)(cid:105)

  n
i

(cid:88)
de   ning(cid:104)
y(n)(cid:105)

i

(cid:32)

xn
i    

= yl
n,

l

(cid:88)

l

(cid:104)

yn
l

  bl
i

  bk
i = 0

(cid:88)

i

=

  bl
i

  bk
i   n
i ,

[cn]k =

(cid:88)

i

  n
i xn
i

  bk
i ,

kl

in matrix notation, we then have a set of linear systems:

c(n) = m(n)y(n),

n = 1, . . . , n

one may solve each linear system for yn using gaussian elimination4. it can be that one or more of
the above linear systems is underdetermined    this can occur when there are less observed values in
the nth data column of x than there are components m . in this case one may use the pseudo-inverse
to provide a minimal length solution.

optimize b for    xed y

one now freezes   y and considers the function

n(cid:88)

d(cid:88)

n=1

i=1

      xn

(cid:88)

j

e(b,   y) =

  n
i

i    

j bj
  yn

i

      2

(15.5.7)

3for simplicity we assume that there is no mean term included.
4one can avoid explicit matrix inversion by using the \ operator in matlab

334

draft november 9, 2017

(15.5.3)

(15.5.4)

(15.5.5)

(15.5.6)

pca with missing data

figure 15.11: top: original data matrix x. black is
missing, white present. the data is constructed from
a set of only 5 basis vectors. middle : x with missing
data (80% sparsity). bottom : reconstruction found
using svdm.m, svd for missing data. this problem is
essentially easy since, despite there being many miss-
ing elements, the data is indeed constructed from a
model for which svd is appropriate. such techniques
have application in collaborative    ltering and recom-
mender systems where one wishes to       ll in    missing
values in a matrix.

for    xed   y the above expression is quadratic in the matrix b, which can again be optimised using
id202. this corresponds to solving a set of linear systems for the ith row of b:

m(i) = f(i)b(i)

where(cid:104)

m(i)(cid:105)

(cid:88)

n

=

k

  n
i xn

i   yn
k ,

f(i)(cid:105)
(cid:104)

kj

(cid:88)

n

=

  n
i   yn

j   yn
k

(15.5.8)

(15.5.9)

mathematically, this is b(i) = f(i)   1

m(i).

in this manner one is guaranteed to iteratively decrease the value of the squared error loss until a minimum
is reached. this technique is implemented in svdm.m     see also    g(15.11). e   cient techniques based on
updating the solution as a new column of x arrives one at a time (   online    updating) are also available, see
for example [53].

15.5.1 finding the principal directions

for the missing data case the basis b found using the above technique is based only on minimising the
squared reconstruction error and therefore does not necessarily satisfy the maximal variance (or principal
directions) criterion, namely that the columns of b point along the eigen-directions. for a given b, y with
approximate decomposition x     by we can return a new orthonormal basis u by performing svd on the
completed data, by = usvt to return an orthonormal basis u.

remaining text in 15.5.1 deleted.

@@

15.5.2 collaborative    ltering using pca with missing data

in the vector xn speci   es the rating the user n gives to the ith    lm. the matrix x = (cid:2)x1, . . . , xn(cid:3)

a database contains a set of vectors, each describing the    lm ratings for a user in the database. the entry
xn
i
contains the ratings for all the n users and has many missing values since any single user will only have
given a rating for a small selection of the possible d    lms. in a practical example one might have d = 104
   lms and n = 106 users. for any user n the task is to predict reasonable values for the missing entries of
their rating vector xn, thereby providing a suggestion as to which    lms they might like to view. viewed
as a missing data problem, one can    t b and y using svdm.m as above. given b and y we can form a
reconstruction on all the entries of x, by using

  x = by

giving therefore a prediction for the missing values.

draft november 9, 2017

(15.5.10)

335

501001502002503002040608010050100150200250300204060801005010015020025030020406080100matrix decomposition methods

figure 15.12: (a): under-complete representation. there are too few basis
vectors to represent the datapoints exactly. (b): over-complete represen-
tation. there are too many basis vectors to form a unique representation
of a datapoint in terms of a linear combination of the basis vectors.

(a)

(b)

15.6 matrix decomposition methods

given a data matrix x for which each column represents a datapoint, an approximate matrix decomposition
is of the form x     by into a basis matrix b and weight (or coordinate) matrix y. symbolically, matrix
decompositions are of the form

                  

(cid:124)

x : data

(cid:123)(cid:122)

d  n

                  

(cid:125)

                   b : basis

(cid:123)(cid:122)

d  m

   

(cid:124)

                  

(cid:125)

      
(cid:124)

y : weights/components

(cid:123)(cid:122)

m  n

      
(cid:125)

(15.6.1)

in this section we will
by considering the svd of the data matrix, we see that pca is in this class.
consider some further common matrix decomposition methods. the form of the decomposition we have
previously considered has been under-complete, although over-complete decompositions are also of interest,
as described below.

under-complete decompositions

when m < d, there are fewer basis vectors than dimensions,    g(15.12a). the matrix b is then called    tall   
or    thin   . in this case the matrix y forms a lower dimensional approximate representation of the data x,
pca being a classic example.

over-complete decompositions

for m > d the basis is over-complete, there being more basis vectors than dimensions,    g(15.12b). in such
cases additional constraints are placed on either the basis or components. for example, one might require
that only a small number of the large number of available basis vectors is used to form the representation
for any given x. other popular constraints are that the basis vectors themselves should be sparse. such
sparse-representations are common in theoretical neurobiology where issues of energy e   ciency, rapidity of
processing and robustness are of interest[231, 169, 269].

below we discuss some popular constrained matrix factorisation methods, in particular including positivity
constraints on both the basis vectors and the components.

15.6.1 probabilistic latent semantic analysis

consider two objects, x and y, where dom(x) = {1, . . . , i}, dom(y) = {1, . . . , j} and a dataset
{(xn, yn) , n = 1, . . . , n}. we have a count matrix with elements cij which describes the number of times
the joint state x = i, y = j was observed in the dataset. we can transform this count matrix into a frequency
matrix p with elements

ij cij

cij(cid:80)
(cid:88)
(cid:124)

k

p(x = i, y = j) =

(cid:124)

(cid:123)(cid:122)

xij

p(x = i, y = j)

(cid:125)

   

our interest is to    nd a decomposition of this frequency matrix of the form in    g(15.13a)

(cid:123)(cid:122)

bik

(cid:125)

(cid:124)

(cid:123)(cid:122)

ykj

(cid:125)

  p(x = i|z = k)

  p(y = j|z = k)  p(z = k)

      p(x = i, y = j)

(15.6.2)

(15.6.3)

336

draft november 9, 2017

matrix decomposition methods

z

(a)

x

y

x

y

z

(b)

figure 15.13: (a): joint plsa. (b): conditional plsa.

where all quantities   p are distributions. this is then a form of matrix decomposition into a positive basis b
and positive coordinates y. this has the interpretation of discovering latent features/topics z that describe
the joint behaviour of x and y. after training, the term   p(z) can be used to rank the basis vectors or    topics   
(the columns on b) in terms of importance, if desired. note that plsa is not a sequential solver in the
sense that the optimal two basis vectors solution is not generally equivalent to the taking the highest ranked
(according to   p(z)) columns of the three basis vector solution     see    g(15.14).

an em style training algorithm

in order to    nd the approximate decomposition we    rst need a measure of di   erence between the matrix
with elements pij and the approximation with elements   pij. since all elements are bounded between 0 and 1
and sum to 1, we may interpret p as a joint id203 and   p as an approximation to this. for probabilities,
a useful measure of discrepancy is the id181

kl(p|  p) = (cid:104)log p(cid:105)p     (cid:104)log   p(cid:105)p

(15.6.4)

since p is    xed, minimising the id181 with respect to the approximation   p is equivalent
to maximising the    likelihood    term (cid:104)log   p(cid:105)p. this is

p(x, y) log   p(x, y)

x,y

@@ where the general notation (cid:80)z implies summation over all states of the variables z. it   s convenient to

derive an em style algorithm to learn   p(x|z),   p(y|z) and   p(z). to do this, consider

(15.6.5)

kl(q(z|x, y)|  p(z|x, y)) =

q(z|x, y) log q(z|x, y)    

q(z|x, y) log   p(z|x, y)     0

(15.6.6)

(cid:88)

z

(cid:88)

z

using

  p(z|x, y) =

  p(x, y, z)
  p(x, y)

and rearranging, this gives the bound,

log   p(x, y)        

q(z|x, y) log q(z|x, y) +

(cid:88)

z

(cid:88)

z

q(z|x, y) log   p(z, x, y)

(15.6.7)

(15.6.8)

plugging this into the    likelihood    term above, we have the bound

p(x, y) log   p(x, y)        

q(z|x, y) log q(z|x, y)

(cid:88)

p(x, y)

x,y

+

(cid:88)
(cid:88)

z

(cid:88)

p(x, y)

x,y

z

q(z|x, y) [log   p(x|z) + log   p(y|z) + log   p(z)]

(15.6.9)

(cid:88)

(cid:88)

x,y

for    xed   p(x|z),   p(y|z), the contribution to the bound from   p(z) is

m-step

(cid:88)

(cid:88)

p(x, y)

x,y

z

q(z|x, y) log   p(z)

draft november 9, 2017

(15.6.10)

337

algorithm 15.2 plsa: given a frequency matrix p(x = i, y = j), return a decomposition(cid:80)

matrix decomposition methods

it is straightforward to see that the optimal setting of   p(z) is

since equation (15.6.10) is, up to a constant, kl
the contribution to the bound from   p(x|z) is

x,y q(z|x, y)p(x, y)|  p(z)

(cid:16)(cid:80)

(cid:17)

(15.6.11)

. similarly, for    xed   p(y|z),   p(z),

p(x, y)

q(z|x, y)

log   p(x|z)

(15.6.12)

@@

k)  p(y = j|z = k)  p(z = k). see plsa.m
1: initialise   p(z),   p(x|z),   p(y|z).
2: while not converged do
set q(z|x, y) =   p(z|x, y)
3:
set   p(x|z)    
4:
set   p(y|z)    
5:
6: end while

y p(x, y)q(z|x, y)
x p(x, y)q(z|x, y)

(cid:80)
(cid:80)

x,y p(x, y)q(z|x, y)

7: set   p(z) =(cid:80)
(cid:88)

  p(z) =

q(z|x, y)p(x, y)

x,y

(cid:88)

x,y

z

(cid:88)
(cid:88)
(cid:88)

y

x

therefore, optimally

p(x, y)q(z|x, y)

p(x, y)q(z|x, y)

  p(x|z)    

and similarly,

  p(y|z)    

e-step

the optimal setting for the q distribution at each iteration is

q(z|x, y) =   p(z|x, y)

which is    xed throughout the m-step.

the    likelihood    equation (15.6.5) is guaranteed to increase (and the id181 equation
(15.6.4) decrease) under iterating between the e and m-steps, since the method is analogous to an em pro-
cedure. the procedure is given in algorithm(15.2) and a demonstration is in demoplsa.m. generalisations,
such as using simpler q distributions, (corresponding to generalised em procedures) are immediate based
on modifying the above derivation.

algorithm 15.3 conditional plsa: given a frequency matrix p(x = i|y = j), return a decomposition

(cid:80)

k   p(x = i|z = k)  p(z = k|y = j). see plsacond.m
1: initialise   p(x|z),   p(z|y).
2: while not converged do
3:
4:
5:
6: end while

set q(z|x, y) =   p(z|x, y)
set   p(x|z)    
set   p(z|y)    

y p(x|y)q(z|x, y)
x p(x|y)q(z|x, y)

(cid:80)
(cid:80)

338

draft november 9, 2017

k   p(x = i|z =

(cid:46) e-step
(cid:46) m-steps

(15.6.13)

(15.6.14)

(15.6.15)

(cid:46) e-step
(cid:46) m-steps

matrix decomposition methods

(a)

(b)

(c)

(b):
figure 15.14: plsa for the document data in    g(15.8).
hinton diagram for three basis vectors. (c): the projections for the three basis vectors case. the solution
is quite satisfactory since the    rst 1000 documents are clearly considered to be from the similar    ailments   
topics, the next 500 from some other non-speci   c    background    topic, and the last 500 from a separate    pet   
topic.

(a): hinton diagram for two basis vectors.

example 15.6 (plsa for documents). we repeated the analysis of the toy document data in    g(15.8),
now using plsa. as we see from    g(15.14), the basis found is interpretable and intuitive. also, the
   projections    onto these basis vectors is also reasonable, and corresponds to what we would expect. the
limited expressibility of the two-basis model causes the two    in   uenza    and       u    terms to be placed in the
same basis vector and each basis vector clearly represents two non-overlapping topics. note that there is
no requirement in general for the topics to be non-overlapping; this just happens to be the case for this
example only. for the richer three-basis vector model, the model distinguishes clearly two similar topics,
di   ering in only a single word.

conditional plsa

in some cases it is more natural to consider a conditional frequency matrix

p(x = i|y = j)

and seek an approximate decomposition

(cid:124)

(cid:123)(cid:122)

xij

p(x = i|y = j)

   

  p(x = i|z = k)

  p(z = k|y = j)

(cid:123)(cid:122)

bik

(cid:125)

(cid:124)

(cid:123)(cid:122)

ykj

(cid:125)

(cid:88)

k

(cid:124)

(cid:125)

(15.6.16)

(15.6.17)

as depicted in    g(15.13b). deriving an em style algorithm for this is straightforward, exercise(15.9), and is
presented in algorithm(15.3).

unity,(cid:80)

example 15.7 (discovering the basis). a set of images is given in    g(15.15a). these were created by    rst
de   ning 4 base images    g(15.15b). each base image is positive and scaled so that the sum of the pixels is
i p(x = i|z = k) = 1, where k = 1, . . . , 4 and x indexes the pixels, see    g(15.15). we then summed
each of these images using a randomly chosen positive set of 4 weights (under the constraint that the weights
sum to 1) to generate a training image with elements p(x = i|y = j) and j indexes the training image. this
was repeated 144 times to form the full train set,    g(15.15a). the task is, given only the training set images,
to reconstruct the basis from which the images were formed. we assume that we know the correct number
of base images, namely 4. the results of using conditional plsa on this task are presented in    g(15.15c)
and using svd in    g(15.15d). in this case plsa    nds the correct    natural    basis, corresponding to the way
the images were generated. the eigenbasis is better in terms of mean squared reconstruction error of the
training images, but in this case does not correspond to the constraints under which the data was generated.

draft november 9, 2017

339

121098765432112310987654321200400600800100012001400160018002000123matrix decomposition methods

(a)

(b)

(c)

(d)

(a): training data, consisting of a positive (convex) combination of the base images.

(b):
figure 15.15:
the chosen base images from which the training data is derived. (c): basis learned using conditional plsa
(d): eigenbasis (sometimes
on the training data. this is virtually indistinguishable from the true basis.
called    eigenfaces   ).

(a): conditional
figure 15.16:
plsa reconstruction of the images in
   g(15.5) using a positive convex com-
bination of the 49 positive base im-
ages in (b). the root mean square

reconstruction error is 1.391    10   5.

the base images tend to be more    lo-
calised    than the corresponding eigen-
images    g(15.6b). here one sees local
structure such as foreheads, chins, etc.

(a)

(b)

example 15.8 (eigenfaces verus plsa faces). we return to example(15.2) and now rerun the experiment,
seeking a plsa basis, rather than an eigenbases. in    g(15.16) we show the reconstruction of the original
100 images and the corresponding plsa basis. the reconstruction error is necessarily higher for the plsa
solution since the optimal mean squared error solution is given by pca (plsa has more constraints on the
form of the solution than pca). however, the basis found by plsa is arguably more interpretable since
the positive features are positively summed to produce an image. because of this, the plsa basis vectors
tend to be rather sparse.

15.6.2 extensions and variations

plsa and id44

++

k     1,(cid:80)l

an alternative viewpoint of plsa is as a form of generative model. we will phrase this in the same language
as id44, section(20.6.1), since the two models are very similar. we will describe a
generative process for document n. the document is described by a distribution over k latent topics   n
with 0       n
k = 1. these topic distributions will form parameters of the model. document n
is a list of words which we generate as follows. for word position w in the document, we    rst draw a topic
k . given the topic for this word position, we now generate a word vn
zw for this word from p(zw = k|n) =   n
w
340

draft november 9, 2017

k=1   n

w n(cid:89)

(cid:88)

w=1

zn
w

w n(cid:89)

(cid:88)

w=1

k

(cid:32)(cid:88)

d(cid:89)

i=1

k

(15.6.19)

(15.6.20)

(15.6.21)

(15.6.23)

matrix decomposition methods

from the word distribution for topic k

p(vn

w = i|zn

w = k) =   i|k

(15.6.18)

we repeat this process for each of the wn word positions in the document. this describes a distribution

w n(cid:89)

w=1

p(vn

1 , . . . , vn

wn, zn

1 , . . . , zn

wn) =

p(vn

w|zn

w)p(zn

w|n)

the marginal distribution over the observed data is then

p(vn

1 , . . . , vn

wn) =

p(vn

w|zn

w)p(zn

w|n)

since the zn

w is simply a summation variable, we can simplify the above to

p(vn

1 , . . . , vn

wn) =

obtain a factor(cid:80)

p(vn

w|k)p(k|n)

consider a speci   c word i in our dictionary of d words. each time word i appears in the document, we will

k p(i|k)p(k|n). an equivalent expression for the likelihood is therefore

(cid:33)fi,n

p(vn

1 , . . . , vn

wn) =

p(vn

w = i|k)p(k|n)

(15.6.22)

where fi,n is the frequency of occurrence of word i in document n. assuming that we would generate each
document in the same way, the likelihood of a collection of documents is the product of the above likelihoods,
one for each document. the parameters of this model are then the word distributions for each topic,   i|k and
the topic distributions for each document   n
k . maximising the likelihood with respect to these parameters
is equivalent to maximising the log likelihood which is

d(cid:88)

n(cid:88)

k(cid:88)
counts p(i, n) = f(i, n)/(cid:80)

fi,n log

n=1

k=1

i=1

where p(i|k) =   k|i and p(k|n) =   n

p(i|k)p(k|n)

(cid:88)

k

k . this is then exactly in the form of plsa. if we normalise the frequency

i,n fi,n, this is plsa with the factorisation

p(i, n)    

  p(i|k)  p(k|n)

(15.6.24)

for which a very e   cient em style algorithm is available, see exercise(15.10). the di   erence with latent
dirichlet allocation is that, rather than   p(k|n) being a parameter, these topic distributions are rather
sampled from a dirichlet distribution. an alternative probabilistic interpretation of plsa can be made via
poisson processes[59].

non-negative matrix factorisation

non-negative matrix factorisation (nmf) considers a decomposition in which both the basis and weight
matrices have non-negative entries; this can be considered as constrained factor analysis[234]. closely
related works are [186] which is a generalisation of plsa (with no requirement that the basis or components
sum to unity). in all cases em-style training algorithms exist, although their convergence can be slow. we
will encounter similar models in the discussion on independent component analysis, section(21.6).

gradient based training

em style algorithms are easy to derive and implement but can exhibit poor convergence. gradient based
methods to simultaneously optimize with respect to the basis and the components have been developed, but
require a parameterisation that ensures positivity of the solutions[234].

draft november 9, 2017

341

array decompositions

matrix decomposition methods

it is straightforward to extend plsa to the decomposition of multidimensional arrays. for example

p(s, t, u)    

  p(s, t, u|v, w)  p(v, w) =

  p(s, t|u, v)  p(u|w)  p(v)  p(w)

(15.6.25)

(cid:88)

v,w

(cid:88)

v,w

such extensions require only additional bookkeeping.

15.6.3 applications of plsa/nmf

modelling citations

we have a collection of research documents which cite other documents. for example, document 1 might
cite documents 3, 2, 10, etc. given only the list of citations for each document, can we identify key research
papers and the communities that cite them? note that this is not the same question as    nding the most
cited documents     rather we want to identify documents with communities and    nd their relevance for a
community.

we use the variable d     {1, . . . , d} to index documents and c     {1, . . . , d} to index citations (both d and c
have the same domain, namely the index of a research article). if document d = i cites article c = j then we
set the entry of the matrix cij = 1. if there is no citation, cij is set to zero. we can form a    distribution   
over documents and citations using

p(d = i, c = j) =

cij(cid:80)

ij cij

(15.6.26)

and use plsa to decompose this matrix into citation-topics, see the example below.

example 15.9 (modelling citations). the cora corpus [202] contains an archive of around 30,000 computer
science research papers. from this archive the authors in [67] extracted the papers in the machine learning
category, consisting of 4220 documents and 38,372 citations. using these the distribution equation (15.6.26)
was formed. the documents have additionally been categorised by hand into 7 topics: case-based reasoning,
id107, neural networks, probabilistic methods, id23, rule learning and
theory. in [67] the joint plsa method is    tted to the data using dim (z) = 7 topics. from the trained
model the expression p(c = j|z = k) de   nes how authoritative paper j is according to community z = k.
the method discovers intuitively meaningful topics, as presented in table(15.1).

modelling the web

consider a collection of websites, indexed by i. if website j points to website i, set cij = 1, otherwise set
cij = 0. this gives a directed graph of website-to-website links. since a website will discuss usually only a
small number of    topics    we might be able to explain why there is a link between two websites using a plsa
decomposition. these algorithms have proved useful for internet search for example to determine the latent
topics of websites and identify the most authoritative websites. see [68] for a discussion.

physical models

non-negative decompositions can arise naturally in physical situations. for example, in acoustics, positive
amounts of energy combine linearly from di   erent signal sources to form the observed signal. if we consider
that two kinds of signals are present in an acoustic signal, say a piano and a singer then using nmf one
can learn two separate bases, one for each    instrument   , and then reconstruct a given signal using only one
of the bases. this means that one could potentially remove the singer from a recording, leaving only the
piano. this is analogous to reconstructing the images in    g(15.15a) using say only one of the learned basis
images, see example(15.7). see [306] for a related model in acoustics.

342

draft november 9, 2017

kernel pca

(id23)
learning to predict by the methods of temporal di   erences. sutton.
neuronlike adaptive elements that can solve di   cult learning control problems. barto et al.
practical issues in temporal di   erence learning. tesauro.
(rule learning)
explanation-based generalization: a unifying view. mitchell et al.
learning internal representations by error propagation. rumelhart et al.
explanation-based learning: an alternative view. dejong et al.
(neural networks)
learning internal representations by error propagation. rumelhart et al.
neural networks and the bias-variance dilemma. geman et al.
the cascade-correlation learning architecture. fahlman et al.
(theory)
classi   cation and regression trees. breiman et al.
learnability and the vapnik-chervonenkis dimension. blumer et al.
learning quickly when irrelevant attributes abound. littlestone.
(probabilistic reasoning)
probabilistic reasoning in intelligent systems: networks of plausible id136. pearl.

factor 1
0.0108
0.0066
0.0065
factor 2
0.0038
0.0037
0.0036
factor 3
0.0120
0.0061
0.0049
factor 4
0.0093
0.0066
0.0055
factor 5
0.0118
0.0094 maximum likelihood from incomplete data via the em algorithm. dempster et al.
0.0056
factor 6
0.0157
0.0132
0.0096
factor 7
0.0063
0.0054
0.0033

local computations with probabilities on graphical structures. lauritzen et al.
(id107)
id107 in search, optimization, and machine learning. goldberg.
adaptation in natural and arti   cial systems. holland.
genetic programming: on the programming of computers by means of natural selection. koza.
(logic)
e   cient induction of logic programs. muggleton et al.
learning logical de   nitions from relations. quinlan.
inductive logic programming techniques and applications. lavrac et al.

table 15.1: highest ranked documents according to p(c|z). the factor topic labels are manual assignments
based on similarity to the cora topics. reproduced from [67].

15.7 kernel pca

kernel pca is a non-linear extension of pca designed to discover non-linear subspace. here we only brie   y
in kernel pca, we replace each x by a
describe the approach and refer the reader to [260] for details.
   feature    vector   x       (x). note that the use of   x here does not have the interpretation we used before as the
approximate reconstruction. rather, the feature map    takes a vector x and produces a higher dimensional
vector   x. for example we could map a two dimensional vector x = [x1, x2]t using

  (x) =(cid:2)x1, x2, x2

1, . . .(cid:3)t

1, x2

2, x1x2, x3

(15.7.1)

the idea is then to perform pca on these higher dimensional feature vectors, subsequently mapping back
the eigenvectors to the original space x. the main challenge is to write this without explicitly computing
pca in the potentially very high dimensional feature vector space. as a reminder, in standard pca, for
zero mean data, one forms an eigen-decomposition of the sample matrix5

s =

1
n

  x   xt

(15.7.2)

for simplicity, we concentrate here on    nding the    rst principal component   e which satis   es

(15.7.3)
for corresponding eigenvalue    (writing   (cid:48) = n   ). the    dual    representation is obtained by pre-multiplying
by   xt, so that in terms of   f       xt  e, the standard pca eigen-problem reduces to solving:

  x   xt  e =   

  e

(cid:48)

  xt   x  f =   

(cid:48)  f

the feature eigenvector   e is then recovered using

  x  f =   

(cid:48)

  e

5we use the normalisation n as opposed to n     1 just for notational convenience     in practice, there is little di   erence.

draft november 9, 2017

343

(15.7.4)

(15.7.5)

and recognise this as the scalar product between vectors. this means that the matrix is positive semide   nite
and we may equivalently use a covariance function kernel, see section(19.3),

we note that matrix   xt   x has elements

  xt   x

=   (xm)t  (xn)

mn

(cid:104)

(cid:104)

(cid:105)

(cid:105)

  xt   x

= k(xm, xn) = kmn

mn

then equation (15.7.4) can be written as

k  f =   

(cid:48)  f

kernel pca

(15.7.6)

(15.7.7)

(15.7.8)

one then solves this eigen-equation to    nd the n dimensional principal dual feature vector   f . the projection
of the feature   x is given by

y =   xt  e =

  xt   x  f

1
  

(15.7.9)

more generally, for a larger number of components, the ith kernel pca projection yi can be expressed in
terms of the kernel directly as

k(x, xn)   f i
n

(15.7.10)

n(cid:88)

n=1

yi =

1

n   i

(cid:88)

(cid:88)

  x   

=

yi  ei =

i

i

yi

1
  i

(cid:88)

n

where i is the eigenvalue label and   f i

n is the nth component of the ith eigenvector of k.

the above derivation implicitly assumed zero mean features   x. even if the original data x is zero mean,
due to the non-linear mapping, however, the features may not be zero mean. to correct for this one may
show that the only modi   cation required is to replace the matrix k in equation (15.7.8) above with

(cid:48)
mn = k(xm, xn)    
k

1
n

k(xd, xn)    

1
n

k(xm, xd) +

1
n 2

k(xd(cid:48)

, xd)

(15.7.11)

n(cid:88)

d=1

n(cid:88)

d=1

n(cid:88)

d=1,d(cid:48)=1

finding the reconstructions

through equation (15.7.10), the above gives a procedure for    nding the kpca projection y. however, in
many cases we would also like to have an approximate reconstruction x(cid:48) using the lower dimensional y.
this is not straightforward since the mapping from y to x is in general highly non-linear. here we outline
a procedure for achieving this. first we    nd the reconstruction   x    of the feature space vector   x, using

  f n
i   (xn)

(15.7.12)

)       x   (cid:1)2
(cid:88)

)     2

(cid:88)

yi
  i

i

n

given   x    we try to    nd that point x(cid:48) in the original data space that maps to   x   . this can be found by
minimising
e(x(cid:48)

) =(cid:0)  (x(cid:48)

(15.7.13)

up to negligable constants this is

e(x(cid:48)

) = k(x(cid:48)

, x(cid:48)

i k(xn, x(cid:48)
  f n

)

(15.7.14)

one then    nds x(cid:48) by minimising e(x(cid:48)) numerically. the resulting procedure gives a way to form a non-linear
extension of pca; however it is computationally relatively demanding and results in a complex optimisation
problem. see [260] for details and example applications.

344

draft november 9, 2017

canonical correlation analysis

15.8 canonical correlation analysis

consider variables x and y being    di   erent views    of the same underlying object. for example x might rep-
resent a segment of video and y the corresponding audio. given then a collection {(xn, yn) , n = 1, . . . , n},
an interesting challenge is to identify which parts of the audio and video are strongly correlated. one might
expect, for example, that the mouth region of the video is strongly correlated with the audio.

one way to achieve this is to project each x and y to one dimension using atx and bty such that the
correlation between the projections is maximal. the unnormalised correlation between the projections atx

and bty is(cid:88)

(cid:34)(cid:88)

(cid:35)

atxnbtyn = at

xnynt

b.

n

de   ning

sxy    

1
n

n

(cid:88)

n

xnynt

and similarly for syx, sxx, syy, the normalised correlation is

(cid:112)
atsxxa(cid:112)btsyyb

atsxyb

since equation (15.8.3) is invariant with respect to recaling a and also b, we can consider the equivalent
objective

e(a, b) = atsxyb

(15.8.4)

subject to atsxxa = 1 and btsyyb = 1. to    nd the optimal projections a, b, under the constraints, we use
the lagrangian,

(cid:16)

  a
2

(cid:17)

(cid:16)

(cid:17)
1     btsyyb

+

  b
2

l (a, b,   a,   b)     atsxyb +

1     atsxxa
from which we obtain the zero derivative criteria

sxyb =   asxxa,

syxa =   bsyyb

hence

atsxyb =   aatsxxa =   a,

btsyxa =   bbtsyyb =   b

since atsxyb = btsyxa we must have   a =   b =    at the optimum. if we assume that syy is invertible we
can write,

b =

s   1
yy syxa

1
  

and use this to eliminate b in equation (15.8.6), giving

sxys   1

yy syxa =   2sxxa

which is a generalised eigen-problem. assuming that sxx is invertible we can equivalently write

xx sxys   1
s   1

yy syxa =   2a

(15.8.8)

(15.8.9)

(15.8.10)

which is a standard eigen-problem (albeit with   2 as the eigenvalue). once this is solved we can    nd b using
equation (15.8.8).

draft november 9, 2017

345

(15.8.1)

(15.8.2)

(15.8.3)

(15.8.5)

(15.8.6)

(15.8.7)

canonical correlation analysis

(a)

(b)

(c)

(a): training data. the top panel contains the x matrix
figure 15.17: canonical correlation analysis.
of 100, 15 dimensional points, and the bottom the corresponding 30 dimensional y matrix. (b): the data
in (a) was produced using x = ah, y = bh where a is a 15    1 matrix, and b is a 30    1 matrix. the
(c): matrices a and b learned by
underlying latent h is a 1    100 dimensional randomly chosen vector.
cca. note that they are close to the true a and b up to rescaling and sign changes. see democca.m.

15.8.1 svd formulation

it is straightforward to show that we can    nd a (the optimal projection of x) by    rst computing the svd
of

    1
xx sxys

2

s

    1
2
yy

(15.8.11)

in the form udvt and extracting the maximal singular vector u1 of u (the    rst column of u). then a is
    1
yy v1, where v1 is the    rst column of v. in this way, the
optimally s

extension to    nding m multiple directions a =(cid:2)a1, . . . , am(cid:3) and b =(cid:2)b1, . . . , bm(cid:3) is clear     one takes the

    1
xx u1, and similarly, b is optimally s

2

2

corresponding    rst m singular values accordingly. doing so maximises the criterion

trace(cid:0)atsxyb(cid:1)

(cid:112)

(cid:112)

trace (atsxxa)

trace (btsyyb)

(15.8.12)

this approach is taken in cca.m     see    g(15.17) for a demonstration. one can also show that cca corre-
sponds to factor analysis under a block restriction on the form of the factor loadings, see section(21.2.1).

cca and related kernel extensions have been applied in machine learning contexts, for example to model
the correlation between images and text in order to improve id162 from text queries, see [139].

15.9 summary

    pca is a classical linear dimension reduction method and assumes that the data lies close to a linear

subspace.

    the pca representation can be found by an eigen-decomposition of the data covariance matrix, or alter-

natively using an svd decomposition of the data matrix.

    more generally, pca is special case of a matrix decomposition method. other standard methods include
plsa and non-negative matrix factorisation, which can be considered as constrained forms of pca (posi-
tivity constraints).

    canonical correlation analysis attempts to    nd a low dimensional representation that jointly models the

two related data spaces. cca is a special case of the probabilistic factor analysis model.

346

draft november 9, 2017

2040608010051015202530training data2040608010051015051015202530   2   10123051015   2   1012true model051015202530   0.2   0.100.10.20.3051015   0.4   0.200.20.40.6learned modelexercises

pca appears in many di   erent research communities and can be referred to di   erently. for example it is
also known as the karhunen-lo`eve decomposition and proper orthogonal decomposition.

another important use for dimension reduction is in data visualisation. methods such as pca can be used
in this context, though are not designed to produce visually interpretable results. see [253] and [301] for
discussions of work in this area.

15.10 code

pca.m: principal components analysis
demolsi.m: demo of id45/analysis
svdm.m: singular value decomposition with missing data
demosvdmissing.m: demo svd with missing data
plsa.m: probabilistic latent semantic analysis
plsacond.m: conditional probabilistic latent semantic analysis
demoplsa.m: demo of plsa
cca.m: canonical correlation analysis (cca)
democca.m: demo of canonical correlation analysis

15.11 exercises

exercise 15.1. as described in section(15.2), we wish to show that the optimal bias c is equal to the sample
mean of the data.

1. explain why, when considering a bias c, and carrying out the derivation in section(15.2.1), we will

obtain, analogously to equation (15.2.1):

(cid:88)

n

(xn     c)t(cid:16)

i     bbt(cid:17)

(xn     c)

e(b, c) =

2. show that for a matrix m

   
   c

ctmc = mc + mtc

(cid:16)

i     bbt(cid:17)(cid:88)

3. using this derivative result, show that the optimal bias c satis   es

0 =

and hence determine that the optimal bias is given by the mean of the data, c =(cid:80)n

n

(xn     c)

n=1 xn/n .

exercise 15.2. consider a dataset in two dimensions where the data lies on the circumference of a circle
of unit radius. what would be the e   ect of using pca on this dataset, in which we attempt to reduce the
dimensionality to 1? suggest an alternative one dimensional representation of the data.

exercise 15.3. consider two vectors xa and xb and their corresponding pca approximations c +(cid:80)m
and c+(cid:80)m

i=1 aiei
i=1 biei, where the eigenvectors ei, i = 1, . . . , m are mutually orthogonal and have unit length. the
eigenvector ei has corresponding eigenvalue   i. approximate (xa     xb)2 by using the pca representations
of the data, and show that this is equal to (a     b)2.
exercise 15.4. show how the solution for a to the cca problem in equation (15.8.9) can be transformed
into the form expressed by equation (15.8.11), as claimed in the text.

draft november 9, 2017

347

(15.11.1)

(15.11.2)

(15.11.3)

exercises

(cid:16)

xa     xb(cid:17)t

s   1(cid:16)

xa     xb(cid:17)

exercise 15.5. let s be the covariance matrix of the data. the mahalanobis distance between xa and xb
is de   ned as

.

(15.11.4)

explain how to approximate this distance using m -dimensional pca.

exercise 15.6 (pca with external inputs). in some applications, one may suspect that certain external
variables v have a strong in   uence on how the data x is distributed. for example, if x represents an image,
it might be that we know the lighting condition v under which the image was made     this will have a large
e   ect on the image. it would make sense therefore to include the known lighting condition in forming a lower
dimensional representation of the image. note that we don   t want to form a lower dimensional representation
of the joint (x, v), rather we want to form a lower dimensional representation of x alone, bearing in mind
that some of the variability observed may be due to v. we therefore assume an approximation

yn
j bj +

vn
k ck

(15.11.5)

(cid:88)

xn    

(cid:88)

j

k

where the coe   cients yn
i , i = 1, . . . , n , n = 1, . . . , n , basis vectors bj, j = 1, . . . , j and ck, k = 1, . . . , k
are to be determined. the external inputs v1, . . . , vn are given. the sum squared error loss between the xn
and their linear reconstruction equation (15.11.5) is

(cid:88)

      xn

(cid:88)

(cid:88)

      2

e =

i    

j bj
yn

i    

k ck
vn
i

find the parameters(cid:8)bj, ck(cid:9) , j = 1, . . . , j, k = 1, . . . , k that minimise e.

n,i

k

j

(15.11.6)

exercise 15.7. consider the following 3-dimensional datapoints:

(1.3, 1.6, 2.8)(4.3,   1.4, 5.8)(   0.6, 3.7, 0.7)(   0.4, 3.2, 5.8)(3.3,   0.4, 4.3)(   0.4, 3.1, 0.9)

(15.11.7)

perform principal components analysis by:

1. calculating the mean, c, of the data.

2. calculating the covariance matrix s = 1
6

(cid:80)6
n=1 xn(xn)t     cct of the data.

3. finding the eigenvalues and eigenvectors ei of the covariance matrix.

4. you should    nd that only two eigenvalues are large, and therefore that the data can be well represented
using two components only. let e1 and e2 be the two eigenvectors with largest eigenvalues. calculate
the two dimensional representation of each datapoint (et

1 (xn     c), et

2 (xn     c)), n = 1, . . . , 6.

5. calculate the reconstruction of each datapoint c + (et

1 (xn     c))e1 + (et

2 (xn     c))e2, n = 1, . . . , 6.

exercise 15.8. show that for the missing data case, the transformed solution   b given in equation (??)
satis   es   bt   b = i.

exercise 15.9. consider a    conditional frequency matrix   

p(x = i|y = j)

(15.11.8)

following section(15.6.1), show how to derive an em style algorithm for an approximate decomposition of
this matrix in the form

p(x = i|y = j)    

  p(x = i|z = k)  p(z = k|y = j)

(15.11.9)

(cid:88)

k

where k = 1, . . . , z, i = 1, . . . , x, j = 1, . . . , y .

348

draft november 9, 2017

++

exercises

exercise 15.10. consider a id203 matrix with elements p(i, n), 0     p(i, n)     1, (cid:80)

we wish to approximate by a factoriation

i,n p(i, n) = 1 that

p(i, n)    

  p(i|k)  p(k|n)

(15.11.10)

(cid:88)

k

if we    nd the factors   p by the requirement of minimal kl(p|  p), show that we obtain the update equations

  pnew(i|k)       p(i|k)

  pnew(k|n)       p(k|n)

(cid:88)
(cid:80)
(cid:88)
(cid:80)

n

i

  p(i, n)  p(k|n)
k   p(i|k)  p(k|n)

  p(i, n)  p(k|n)
k   p(i|k)  p(k|n)

(15.11.11)

(15.11.12)

draft november 9, 2017

349

exercises

350

draft november 9, 2017

chapter 16

supervised linear dimension reduction

pca is a popular and very useful method. however, if we subsequently use the projected data in a classi   ca-
tion problem, by not making use of the class labels of the data, we are potentially forming lower dimensional
representations that are suboptimal in terms of how well separated they are amongst the di   erent classes.
in this chapter we discuss some classical methods that reduce the data dimension such that the resulting
data is well separated between the classes.

16.1 supervised linear projections

in cases where class
in chapter(15) we discussed dimension reduction using an unsupervised procedure.
information is available, and our ultimate interest is to reduce dimensionality for improved classi   cation, it
makes sense to use the available class information in forming the projection. we consider data from two
di   erent classes. for class 1, we have a set of n1 datapoints,

(cid:110)
(cid:110)

x1 =

1, . . . , xn1
x1
1

x2 =

2, . . . , xn2
x1
2

(cid:111)
(cid:111)

and similarly for class 2, we have a set of n2 datapoints

(16.1.1)

(16.1.2)

our interest is then to    nd a linear projection,

y = wtx

(16.1.3)
where dim w = d    l, l < d, such that for two datapoints xi and xj in the same class, the distance
between their projections yi and yj should be small. conversely, for datapoints in di   erent classes, the
distance between their projections should be large. this may be useful for classi   cation purposes since for
a novel point x   , if its projection

y   

= wtx   

(16.1.4)
is close to class 1 projected data, we would expect x    to belong to class 1. in forming the supervised pro-
jection, only the class discriminative parts of the data are retained, so that the procedure can be considered
a form of supervised feature extraction.

16.2 fisher   s linear discriminant

we    rst restrict our attention to binary class data. also, for simplicity, we project the data down to one
dimension. the canonical variates algorithm of section(16.3) deals with the generalisations.

351

fisher   s linear discriminant

(a)

(b)

figure 16.1: the large crosses represent data from class 1, and the large circles from class 2. their projections
onto 1 dimension are represented by their small counterparts. (a): fisher   s id156.
here there is little class overlap in the projections. (b): unsupervised dimension reduction using principal
components analysis for comparison. there is considerable class overlap in the projection. in both (a) and
(b) the one dimensional projection is the distance along the line, measured from an arbitrary chosen    xed
point on the line.

gaussian assumption

we model the data from each class with a gaussian. that is

p(x1) = n (x1 m1, s1) ,

p(x2) = n (x2 m2, s2)

(16.2.1)

where m1 is the sample mean of class 1 data, and s1 the sample covariance; similarly for class 2. the
projections of the points from the two classes are then given by

because the projections are linear, the projected distributions are also gaussian,

1 = wtxn
yn
1 ,

2 = wtxn
yn
2

(cid:0)y1   1,   2
(cid:0)y2   2,   2

1

2

(cid:1) ,
(cid:1) ,

p(y1) = n
p(y2) = n

  1 = wtm1,
  2 = wtm2,

1 = wts1w
  2
2 = wts2w
  2

(16.2.2)

(16.2.3)

(16.2.4)

we search for a projection w such that the projected distributions have minimal overlap. this can be
achieved if the projected gaussian means are maximally separated, that is (  1       2)2 is large. however, if
1,   2
the variances   2
2 are also large, there could still be a large overlap still in the classes. a useful objective
function therefore is

(  1       2)2
  1  2
1 +   2  2
2

(16.2.5)

where   i represents the fraction of the dataset in class i. in terms of the projection w, the objective equation
(16.2.5) is

f (w) =

wt (m1     m2) (m1     m2)t w

wt (  1s1 +   2s2) w

=

wtaw
wtbw

where

the optimal w can be found by di   erentiating equation (16.2.6) with respect to w. this gives

a = (m1     m2) (m1     m2)t ,
(cid:104)(cid:16)

2

   
   w

wtaw
wtbw

=

(wtbw)2

b =   1s1 +   2s2

(cid:17)

(cid:16)

(cid:17)

(cid:105)

wtbw

aw    

wtaw

bw

(16.2.6)

(16.2.7)

(16.2.8)

352

draft november 9, 2017

   202468   5   4   3   2   1012345   202468   5   4   3   2   1012345canonical variates

and therefore the zero derivative requirement is

(cid:16)

(cid:17)

(cid:16)

(cid:17)

wtbw

aw =

wtaw

bw

multiplying by the inverse of b we have

b   1 (m1     m2) (m1     m2)t w =

wtaw
wtbw

w

since (m1     m2)t w is a scalar, the optimal projection is explicitly given by

w     b   1 (m1     m2)

(16.2.9)

(16.2.10)

(16.2.11)

although the proportionality factor depends on w, we may take it to be constant since the objective function
f (w) in equation (16.2.6) is invariant to rescaling of w. we may therefore take

it is common to rescale w to have unit length, wtw = 1, such that

w = kb   1 (m1     m2)

k =

(cid:113)
(m1     m2)t b   2 (m1     m2)

1

(16.2.12)

(16.2.13)

an illustration of the method is given in    g(16.1), which demonstrates how supervised dimension reduction
can produce lower dimensional representations more suitable for subsequent classi   cation than an unsuper-
vised method such as pca.

one can also arrive at equation (16.2.12) from a di   erent starting objective. by treating the projection as
a regression problem y = wtx + b in which the outputs y are de   ned as y1 and y2 for classes 1 and class 2
respectively, one may show that, for suitably chosen y1 and y2, the solution using a least squares criterion is
given by equation (16.2.12) [89, 44]. this also suggests a way to regularise lda, see exercise(16.3). kernel
extensions of lda are possible, see for example [83, 265].

when the naive method breaks down

the above derivation relied on the existence of the inverse of b.
in practice, however, b may not be
invertible, and the above procedure requires modi   cation. a case where b is not invertible is when there
are fewer datapoints n1 + n2 than dimensions d. a related problematic case is when there are elements
of the input vectors that never vary. for example, in the hand-written digits case, the pixels at the corner
let   s call such a pixel z. the matrix b will then have a zero entry for
edges are actually always zero.
[b]z,z (indeed the whole zth row and column of b will be zero) so that for any vector of the form

wt = (0, 0, . . . , wz, 0, 0, . . . , 0)     wtbw = 0

(16.2.14)

this shows that the denominator of fisher   s objective can become zero, and the objective ill de   ned. we
will address these issues in section(16.3.1).

16.3 canonical variates

canonical variates generalises fisher   s method to projections of more than one dimension and more than
two classes. the projection of any point is given by

y = wtx

where w is a d    l matrix. assuming that the data x from class c is gaussian distributed,

p(x) = n (x mc, sc)

the projections y are also gaussian

(cid:16)

p(y) = n

y wtmc, wtscw

(cid:17)

by analogy with equation (16.2.7), we de   ne the following matrices:

draft november 9, 2017

(16.3.1)

(16.3.2)

(16.3.3)

353

between class scatter find the mean m of the whole dataset and mc, the mean of the each class c.

canonical variates

form

c(cid:88)

c=1

a    

nc (mc     m) (mc     m)t

where nc is the number of datapoints in class c, c = 1, . . . , c.

within class scatter compute the covariance matrix sc of the data for each class c. de   ne

c(cid:88)

c=1

b    

ncsc

assuming b is invertible (see section(16.3.1) otherwise), we can de   ne the cholesky factor   b, with

  bt   b = b

a natural objective is then to maximise the raleigh quotient

(cid:17)

trace

f (w)    

wt   b   ta   b   1w
trace (wtw)

(cid:17)

wtcw

, subject to wtw = i

f (w)     trace

where

if we assume an orthonormality constraint on w, then we equivalently require the maximisation of

c    

1
d

  b   ta   b   1

since c is symmetric and positive semide   nite, it has a real eigen-decomposition

c = e  et

where    = diag (  1,   2, . . . ,   d) is diagonal with non-negative entries containing the eigenvalues, sorted by
decreasing order,   1       2     . . . and ete = i. hence

f (w) = trace

wte  etw

(cid:17)

by setting w = [e1, . . . , el], where el is the lth eigenvector, the objective f (w) becomes the sum of the
   rst l eigenvalues. this setting maximises the objective function since forming w from any other columns
of e would give a lower sum. the procedure is outlined in algorithm(16.1). note that since a has rank c,
there can be no more than c     1 non-zero eigenvalues and corresponding directions.

q1

q2

figure 16.2: each three dimensional datapoint lies in a two-
dimensional plane, meaning that the matrix b is not full
rank, and therefore not invertible. a solution is given by
   nding vectors q1, q2 that span the plane, and expressing
the canonical variates solution in terms of these vectors
alone.

354

draft november 9, 2017

(16.3.4)

(16.3.5)

(16.3.6)

(16.3.7)

(16.3.8)

(16.3.9)

(16.3.10)

(16.3.11)

(cid:16)

(cid:16)

(cid:16)

canonical variates

algorithm 16.1 canonical variates

1: compute the between and within class scatter matrices a, equation (16.3.4) and b, equation (16.3.5).
2: compute the cholesky factor   b of b.
3: compute the l principal eigenvectors [e1, . . . , el] of   b   ta   b   1.
4: return w = [e1, . . . , el] as the projection matrix.

(a)

(b)

(a): canonical variates projection of examples of handwritten digits 3(   +   ), 5(   o   ) and
figure 16.3:
7(diamond). there are 800 examples from each digit class. plotted are the projections down to 2 dimensions.
(b): pca projections for comparison.

16.3.1 dealing with the nullspace

the above derivation of canonical variates (and also fisher   s lda) requires the invertibility of the matrix
b. however, as we discussed in section(16.2), one may encounter situations where b is not invertible.
a solution is to require that w lies only in the subspace spanned by the data (that is there can be no
contribution from the nullspace). to do this we    rst concatenate the training data from all classes into one
large matrix x. a basis for x can be found using, for example, the thin-svd technique which returns an
orthonormal non-square basis matrix q. we then require the solution w to be expressed in this basis, see
   g(16.2):

w = qw(cid:48)

(16.3.12)

for some matrix w(cid:48). substituting this in the canonical variates objective equation (16.3.7), we obtain

(cid:16)

w(cid:48)tqtaqw(cid:48)(cid:17)
trace(cid:0)w(cid:48)tqtbqw(cid:48)(cid:1)

trace

f (w(cid:48)

)    

(16.3.13)

this is of the same form as the standard quotient, equation (16.3.7), on replacing the between-scatter a
with

a(cid:48)

    qtaq

and the within-scatter b with

(16.3.14)

b(cid:48)

    qtbq

(16.3.15)
in this case b(cid:48) is guaranteed invertible since b is projected down to the basis that spans the data. one may
then carry out canonical variates, as in section(16.3) above, which returns the matrix w(cid:48). transforming,
back, w is then given by equation (16.3.12). see also canonvar.m.

draft november 9, 2017

355

   0.1   0.0500.050.10.15   0.06   0.04   0.0200.020.040.060.080.10.120.14   3000   2500   2000   1500   1000   500   1000   50005001000exercises

example 16.1 (using canonical variates on the digit data). we apply canonical variates to project the digit
data onto two dimensions, see    g(16.3). there are 800 examples of a three, 800 examples of a    ve and 800
examples of a seven. thus, overall, there are 2400 examples lying in a 784 (28  28 pixels) dimensional space.
the canonical variates projected data onto two dimensions has very little class overlap, see    g(16.3a). in
comparison the projections formed from pca, which discards the class information, displays a high degree
of class overlap. the di   erent scales of the canonical variates and pca projections is due to the di   erent
in pca w is unitary; in canonical variates wtbw = i,
constraints on the projection matrices w.
meaning that w will scale with the inverse square root of the largest eigenvalues of the within class scatter
matrix. since the canonical variates objective is independent of linear scaling, w can be rescaled with an
arbitrary scalar prefactor   w, as desired.

16.4 summary

    fisher   s linear discriminant seeks a scalar projection that is maximally di   erent for data from each of two

classes.

    canonical variates generalises fisher   s method to multiple classes and multiple projected dimensions.
    fisher   s method and canonical variates are related to standard eigen-problems.

the applicability of canonical variates depends on our assumption that a gaussian is a good description of
the data. clearly, if the data is multimodal, using a single gaussian to model the data in each class is a poor
assumption. this may result in projections with a large class overlap. in principle, there is no conceptual
di   culty in using more complex distributions, and a more general criteria such as maximal kullback-leibler
divergence between projected distributions. however, such criteria typically result in di   cult optimisation
problems. canonical variates is popular due to its simplicity and lack of local optima issues in constructing
the projection.

16.5 code

canonvar.m: canonical variates
democanonvardigits.m: demo for canonical variates

16.6 exercises

exercise 16.1. what happens to fisher   s linear discriminant if there are less datapoints than dimensions?

exercise 16.2. modify democanonvardigits.m to project and visualise the digits data in 3 dimensions.

exercise 16.3. consider n1 class 1 datapoints xn1, n1 = 1, . . . , n1 and n2 class 2 datapoints xn2, n2 =
1, . . . , n2. we de   ne a linear predictor for the data,

y = wtx + b

(16.6.1)

with the aim to predict value y1 for data from class 1 and y2 for data from class two. a measure of the    t
is given by

n1(cid:88)

n1=1

(cid:16)

(cid:17)2
y1     wtxn1     b

+

n2(cid:88)

n2=1

(cid:16)

(cid:17)2
y2     wtxn2     b

(16.6.2)

draft november 9, 2017

e(w, b|y1, y2) =

356

show that by setting y1 = (n1 + n2)/n1 and y2 = (n1 + n2)/n2 the w which minimises e corresponds to
fisher   s lda solution. hint:    rst show that the two zero derivative conditions are

exercises

(cid:88)
and (cid:88)

n1

(cid:16)
(cid:16)

n1

y1     b     wtxn1

y1     b     wtxn1
(cid:18)

(cid:17)
(cid:17)

(cid:88)

+

n2

xt

n1 +

(cid:17)

(cid:16)
y2     b     wtxn2
(cid:16)
(cid:88)

y2     b     wtxn2

n2

= 0

(cid:17)

xt

n2 = 0

which can be reduced to the single equation

n (m1     m2) =

n b +

n1n2

n

(m1     m2) (m1     m2)t

(cid:19)

w

(16.6.3)

(16.6.4)

(16.6.5)

where b is as de   ned for lda in the text, equation (16.2.7).

note that this suggests a way to regularise lda, namely by adding on a term   wtw to e(w, b|y1, y2). this
can be absorbed into rede   ning equation (16.3.5) as

b(cid:48)

= b +   i

(16.6.6)

that is, we increase the covariance b by an additive amount   i. the optimal regularising constant    may
be set by cross-validation.

exercise 16.4. consider the digit data of 892    ves digit5.mat and 1028 sevens digit7.mat. make a
training set which consists of the    rst 500 examples from each digit class. use canonical variates to    rst
project the data down to 10 dimensions and compute the nearest neighbour performance on the remaining
digits. alternatively, use pca to reduce the data to 10 dimensions, and compare the resulting nearest
neighbour classi   cation performance. visualise also the 10 directions found by canonical variates and the
10 principal directions of pca.

exercise 16.5. consider an objective function of the form

f (w)    

a(w)
b(w)

(16.6.7)

where a(w) and b(w) are positive functions, and our task is to maximise f (w) with respect to w. it may
be that this objective does not have a simple algebraic solution, even though a(w) and b(w) are simple
functions. we can consider an alternative objective, namely

j(w,   ) = a(w)       b(w)

where    is a constant scalar. choose an initial point wold at random and set

  old     a(wold)/b(wold)

in that case j(wold,   old) = 0. now choose a w such that

j(w,   old) = a(w)       oldb(w)     0

this is certainly possible since j(wold,   old) = 0. if we can    nd a w such that j(w,   old) > 0, then

a(w)       oldb(w) > 0

(16.6.8)

(16.6.9)

(16.6.10)

(16.6.11)

show that for such a w, f (w) > f (wold), and suggest an iterative optimisation procedure for objective
functions of the form f (w).

draft november 9, 2017

357

exercises

358

draft november 9, 2017

chapter 17

linear models

in this chapter we discuss some classical methods for prediction based on    tting simple linear models to data.
these include standard methods such as linear and id28, and also their kernelised variants.
we also discuss the popular support vector machine and related methods for ensuring good generalisation
performance.

17.1 introduction: fitting a straight line

given training data {(xn, yn) , n = 1, . . . , n}, for scalar input xn and scalar output yn, a id75
   t is

y(x) = a + bx

(17.1.1)

to determine the best parameters a, b, we use a measure of the discrepancy between the observed outputs
and the id75    t such as the sum squared training error. this is also called ordinary least squares
and minimises the average vertical projection of the points y to the    tted line,    g(17.1a):

e(a, b) =

[yn     y(xn)]2 =

(yn     a     bxn)2

n(cid:88)

n=1

our task is to    nd the parameters a and b that minimise e(a, b). di   erentiating with respect to a and b we
obtain

   
   a

e(a, b) =    2

(yn     a     bxn),

   
   b

e(a, b) =    2

(yn     a     bxn)xn

n(cid:88)

n=1

n(cid:88)

n=1

n(cid:88)

n=1

dividing by n and equating to zero, the optimal parameters are given from the solution to the two linear
equations

(cid:104)y(cid:105)     a     b(cid:104)x(cid:105) = 0,

(cid:104)xy(cid:105)     a(cid:104)x(cid:105)     b(cid:10)x2(cid:11) = 0
(cid:80)n

where we used the notation (cid:104)f (x, y)(cid:105) to denote 1
to determine a and b:

n

n=1 f (xn, yn). we can readily solve the equations(17.1.4)

a = (cid:104)y(cid:105)     b(cid:104)x(cid:105)

b(cid:10)x2(cid:11) = (cid:104)yx(cid:105)     (cid:104)x(cid:105) ((cid:104)y(cid:105)     b(cid:104)x(cid:105))     b

(cid:104)(cid:10)x2(cid:11)

    (cid:104)x(cid:105)2(cid:105)

= (cid:104)xy(cid:105)     (cid:104)x(cid:105)(cid:104)y(cid:105)

359

(17.1.5)

(17.1.6)

(17.1.2)

(17.1.3)

(17.1.4)

linear parameter models for regression

(a)

(b)

figure 17.1: data from singbats     the number of chirps per second, versus the temperature in fahrenheit.
(a): straight line regression    t to the singbat data.
(b): pca    t to the data. in regression we minimize
the residuals     the vertical distances from datapoints to the line. in pca the    t minimizes the orthogonal
projections to the line. both lines go through the mean of the data.

hence

b = (cid:104)xy(cid:105)     (cid:104)x(cid:105)(cid:104)y(cid:105)
(cid:104)x2(cid:105)     (cid:104)x(cid:105)2

and a is found by substituting this value for b into equation (17.1.5).

(17.1.7)

in contrast to ordinary least squares regression, pca from chapter(15) minimises the orthogonal projection
of y to the line and is known as orthogonal least squares     see example(17.1).

example 17.1. in    g(17.1) we plot the number of chirps c per second for singbats, versus the temperature
t in degrees fahrenheit. a biologist believes that there is a simple relation between the number of chirps
and the temperature of the form

c = a + bt

(17.1.8)

where she needs to determine the parameters a and b. for the singbat data, the    t is plotted in    g(17.1a).
for comparison we plot the    t from the pca,    g(17.1b), which minimises the sum of the squared orthogonal
projections from the data to the line.

17.2 linear parameter models for regression

we can generalise the above to    tting linear functions of vector inputs x. for a dataset {(xn, yn) , n = 1, . . . , n},
a linear parameter regression model (lpm) is de   ned by1

y(x) = wt  (x)

(17.2.1)

where   (x) is a vector valued function of the input vector x. for example, in the case of a straight line    t,
with scalar input and output, equation (17.1.1), we have

  (x) = (1, x)t,

w = (a, b)t,

(17.2.2)

we de   ne the train error as the sum of squared di   erences between the observed outputs and the predictions
under the linear model:

e(w) =

(yn     wt  n)2,

where   n        (xn)

(17.2.3)

1note that the model is linear in the parameter w     not necessarily linear in x.

360

draft november 9, 2017

n(cid:88)

n=1

607080901001106065707580859095100105110chirps per sectemperature (f)607080901001106065707580859095100105110chirps per sectemperature (f)linear parameter models for regression

figure 17.2: cubic polynomial    t to the singbat data.

we now wish to determine the parameter vector w that minimises e(w). in terms of the components of w,
the squared error is

di   erentiating with respect to wk, and equating to zero gives

e(w) =

wi  n

i )(yn    

wj  n
j )

(cid:88)

j

n(cid:88)

n=1

(cid:88)

i

n(cid:88)

(yn    
(cid:88)

wi

n(cid:88)

n(cid:88)

n(cid:88)

yn  n

k =

  n
i   n
k

n=1

i

n=1

or, in matrix notation,

yn  n =

  n(  n)tw

n=1

n=1

these are called the normal equations, for which the solution is

(cid:32) n(cid:88)

(cid:33)   1 n(cid:88)

w =

  n(  n)t

yn  n

(17.2.4)

(17.2.5)

(17.2.6)

(17.2.7)

n=1

n=1

although we write the solution using matrix inversion, in practice one    nds the numerical solution using
gaussian elimination[130] since this is faster and numerically more stable.

example 17.2 (a cubic polynomial    t).

a cubic polynomial is given by

y(x) = w1 + w2x + w3x2 + w4x3

as a lpm, this can be expressed using

  (x) =(cid:0)1, x, x2, x3(cid:1)t

(17.2.8)

(17.2.9)

the ordinary least squares solution has the form given in equation (17.2.7). the    tted cubic polynomial is
plotted in    g(17.2). see also democubicpoly.m.

example 17.3 (predicting return). in    g(17.3) we    t an lpm with vector inputs x to a scalar output y.
the vector x represents factors that are believed to a   ect the stock price of a company, with the stock price

draft november 9, 2017

361

607080901001106065707580859095100105110chirps per sectemperature (f)linear parameter models for regression

y21, . . . , y25 are the predictions based on yt = (cid:80)

figure 17.3: predicting stock return using a linear lpm. the top
   ve panels present the inputs x1, . . . , x5 for 20 train days (blue)
and 5 test days (red). the corresponding train output (stock re-
turn) y for each day is given in the bottom panel. the predictions
i wixit with w
trained using ordinary least squares. with a regularisation term
0.01wtw, the ols learned w is [1.42, 0.62, 0.27,   0.26, 1.54]. de-
spite the simplicity of these models, their application in the    -
nance industry is widespread, with signi   cant investment made
on    nding factors x that may be indicative of future return. see
demolpmhedge.m.

5(cid:88)

return given by the scalar y. a hedge fund manager believes that the returns may be linearly related to the
factors:

yt =

wixit

(17.2.10)

i=1

and wishes to    t the parameters w in order to use the model to predict future stock returns. this is
straightforward using ordinary least squares, this being simply an lpm with a linear    function. see
   g(17.3) for an example. such models also form the basis for more complex models in    nance, see for
example [210].

17.2.1 vector outputs

it is straightforward to generalise the above framework to vector outputs y. using a separate weight vector
wi for each output component yi, we have

the mathematics follows similarly to before, and we may de   ne a train error per output as

yi(x) = wt

i   (x)

(cid:88)

(cid:16)

n

i   n(cid:17)2

e(wi) =

i     wt
yn

,

e(w) =

e(wi)

(cid:88)

i

(17.2.11)

(17.2.12)

since the training error decomposes into individual terms, one for each output, the weights for each output
can be trained separately. in other words, the problem decomposes into a set of independent scalar output
problems. in case the parameters w are tied or shared amongst the outputs, the training is still straight-
forward since the objective function remains linear in the parameters, and this is left as an exercise for the
interested reader.

17.2.2 regularisation

for most purposes, our interest is not just to    nd the function that best    ts the train data but one that
that will generalise well. to control the complexity of the    tted function we may add an extra regularising
term r(w) to the train error to penalise rapid changes in the output

(cid:48)

e

(w) = e(w) +   r(w)

362

(17.2.13)

draft november 9, 2017

051015202500.510510152025   0.500.50510152025   0.500.50510152025   2.5   2   1.50510152025   1010510152025024linear parameter models for regression

(a)

(b)

(cid:16)

    1

(cid:0)x     mi(cid:1)2(cid:17)

figure 17.4: (a): a set of    xed-width (   = 1) radial basis functions, exp
, with the centres
mi evenly spaced. by taking a linear combination of these functions we can form a    exible function class.
(b): the    are the training points, and the + are the validation points. the solid line is the correct
underlying function sin(10x) which is corrupted with a small amount of additive noise to form the train
data. the dashed line is the best predictor based on the validation set.

2

where    is a scalar amount that adjust the strength of the regularisation term. for example a regularising
term that can be added to equation (17.2.3) is

penalises large di   erences in the outputs corresponding to two inputs. the

has the e   ect of weighting more heavily terms for which two input vectors xn

are close together;    is a    xed length-scale parameter. since y = wt  (x), expression (17.2.14) can

(cid:105)2

y(xn)     y(xn(cid:48)

)

r(w) =

e

n=1

(cid:16)

     

n(cid:48)=1

xn   xn(cid:48)(cid:17)2(cid:104)
n(cid:88)
n(cid:88)
(cid:105)2
xn     xn(cid:48)(cid:17)2(cid:19)

(cid:104)
(cid:16)
y(xn)     y(xn(cid:48)
     

)

(cid:18)

the factor

factor exp
and xn(cid:48)
be written as

wtrw

where

r    

n(cid:88)

n=1

n(cid:88)

     
e
n(cid:48)=1

xn   xn(cid:48)(cid:17)2(cid:16)
(cid:16)

  n       n(cid:48)(cid:17)(cid:16)

  n       n(cid:48)(cid:17)t

the regularised train error is then

n(cid:88)
(cid:32)(cid:88)

n=1

n

(cid:48)

e

(w) =

(yn     wt  n)2 +   wtrw

(cid:33)   1 n(cid:88)

n=1

w =

  n(  n)t +   r

yn  n

(cid:88)

r(w) = wtw =

w2
i

by di   erentiating the regularised training error and equating to zero, we    nd the optimal w is given by

in practice it is common to use a regulariser that penalises the sum squared length of the weights

i

which corresponds to setting r = i. this is known as ridge regression. regularising pararameters such as
  ,    may be determined using a validation set, section(13.2.2).

draft november 9, 2017

363

(17.2.14)

(17.2.15)

(17.2.16)

(17.2.17)

(17.2.18)

(17.2.19)

00.20.40.60.8100.10.20.30.40.50.60.70.80.91x00.20.40.60.81   1.5   1   0.500.511.517.2.3 radial basis functions

linear parameter models for regression

a popular lpm is given by the non-linear function   (x) with components

(cid:18)

(cid:0)x     mi(cid:1)2(cid:19)

  i(x) = exp

1
2  2

   

(17.2.20)

these basis functions are bump shaped, with the centre of bump i being given by mi and the width by   .
an example is given in    g(17.4) in which several rbfs are plotted with di   erent centres. in lpm regression
we can then use a linear combination of these bumps to    t the data. one can apply the same approach
using vector inputs. for vector x and centre m, the radial basis function depends on the distance between
x and the centre m, giving a bump in input space,    g(17.6).

example 17.4 (setting   ). consider    tting the data in    g(17.4b) using 16 radial basis functions uniformly
spread over the input space, with width parameter    and regularising term   wtw. the generalisation
performance on the test data depends heavily on the width and regularising parameter   . in order to    nd
reasonable values for these parameters we may use a validation set. for simplicity we set the regularisation
parameter to    = 0.0001 and use the validation set to determine a suitable   .
in    g(17.5) we plot the
validation error e(w) as a function of   , choosing then the    with the lowest validation error. the predictions
for this optimal    are also given in    g(17.4b).

a curse of dimensionality

if the data has non-trivial behaviour over some input region, then we need to cover this region input space
fairly densely with bump type functions. in the above case, we used 16 basis functions for a one dimensional
input space. in 2 dimensions if we wish to cover each dimension to the same discretisation level, we would
need 162 = 256 basis functions. similarly, for 10 dimensions we would need 1610     1012 functions. to
   t such an lpm would require solving a linear system in more than 1012 variables. this explosion in the
number of basis functions with the input dimension is a    curse of dimensionality   .

a possible remedy is to make the basis functions very broad so that each covers more of the high dimensional
space. however, this will mean a lack of    exibility of the    tted function since it is constrained to be smooth.
another approach is to place basis functions centred on the training input points and add some more basis
functions randomly placed close to the training inputs. the rationale behind this is that when we come to
do prediction, we will most likely see novel x that are close to the training points     we do not need to make
accurate predictions over all the space. a further approach is to make the positions of the basis functions
adaptive, allowing them to be moved around in the space to minimise the error. this approach is used in
neural network models[43]. an alternative is to reexpress the problem of    tting an lpm by reparameterising
the problem, as discussed below.

figure 17.5: the validation error as a function of the basis func-
tion width for the validation data in    g(17.4b) and rbfs in
   g(17.4a). based on the validation error, the optimal setting
of the basis function width parameter is    = 0.25.

364

draft november 9, 2017

00.20.40.60.81012345678  validation errorthe dual representation and kernels

(cid:0)x     m1(cid:1)2

(a): the output of an rbf
figure 17.6:
function exp(    1
/  2). here m1 =
(0, 0.3)t and    = 0.25.
(b): the combined
output for two rbfs with m1 as above and
m2 = (0.5,   0.5)t.

2

(a)

(b)

17.3 the dual representation and kernels

consider a set of training data with inputs, x = {xn, n = 1, . . . , n} and corresponding outputs yn, n =
1, . . . , n . for an lpm of the form

f (x) = wtx

(17.3.1)

our interest is to    nd the    best    t    parameters w. we assume that we have found an optimal parameter w   .
the nullspace of x are those x    which are orthogonal to all the inputs in x . that is, x    is in the nullspace

for all n = 1, . . . , n

(17.3.2)

if we then consider the vector w    with an additional component in the nullspace,

xn = wt    xn

(17.3.3)
this means that adding a contribution to w    outside of the space spanned by x , has no e   ect on the
predictions on the train data. if the training criterion depends only on how well the lpm predicts the train
data, there is therefore no need to consider contributions to w from outside of x . that is, without loss of
generality we may consider the representation

w =

amxm

(17.3.4)

if

xn = 0,

(cid:16)
(cid:16)

x   (cid:17)t
w    + x   (cid:17)t

the parameters a = (a1, . . . , an ) are called the dual parameters. we can then write the output of the lpm
directly in terms of the dual parameters,

wtxn =

am (xm)t xn

(17.3.5)

more generally, for a vector function   (x), the solution will lie in the space spanned by   (x1), . . . ,   (xn ),

n(cid:88)

m=1

n(cid:88)

n(cid:88)

m=1

w =

an   (xn)

n=1

and we may write

wt  (xn) =

n(cid:88)

m=1

am   (xm)t    (xn) =

n(cid:88)

m=1

amk (xm, xn)

where we have de   ned a id81

k (xm, xn)        (xm)t    (xn)     [k]m,n

in matrix form, the output of the lpm on a training input x is then

wt  (xn) = [ka]n = atkn

(17.3.6)

(17.3.7)

(17.3.8)

(17.3.9)

where kn is the nth column of the gram matrix k. by construction, the gram matrix must be positive
semide   nite, and the kernel a covariance function, see section(19.3).

draft november 9, 2017

365

   1   0.500.51   1   0.8   0.6   0.4   0.200.20.40.60.8100.51x(1)x(2)   1   0.500.51   1   0.500.5100.511.5x(1)x(2)17.3.1 regression in the dual-space

linear parameter models for classi   cation

for ordinary least squares regression, using equation (17.3.9), we have a train error

n(cid:88)

n=1

(cid:16)

yn     atkn(cid:17)2

e(a) =

(17.3.10)

equation(17.3.10) is analogous to the standard regression equation (17.2.3) on interchanging a for w and
kn for   (xn). similarly, the regularisation term can be expressed as

(17.3.11)

(17.3.12)

(17.3.13)

n,m=1

n(cid:88)
(cid:32) n(cid:88)
(cid:32) n(cid:88)

n=1

wtw =

anam   (xn)    (xm) = atka

by direct analogy the optimal solution for a is therefore

a =

kn (kn)t +   k

ynkn

(cid:33)   1 n(cid:88)
(cid:33)   1 n(cid:88)

n=1

we can express the above solution more conveniently by    rst writing

a =

k   1kn (kn)t +   i

ynk   1kn

n=1

n=1

since kn is the nth column of k then k   1kn is the nth column of the identity matrix. with a little
manipulation, we can therefore rewrite equation (17.3.13) more simply as

a = (k +   i)

   1 y

(17.3.14)

where y is the vector with components formed from the training inputs y1, . . . , yn . using this, the prediction
for a new input x    is given by
   1 y

(17.3.15)

y(x   

) = kt    (k +   i)

where the vector k    has components

[k   ]m = k (x   

, xm)

(17.3.16)
this dual space solution shows that predictions can be expressed purely in terms of the kernel k (x, x(cid:48)).
this means that we may dispense with de   ning the vector functions   (x) and de   ne a id81
directly. this approach is also used in gaussian processes, chapter(19) and enables us to use e   ectively
very large (even in   nite) dimensional vectors    without ever explicitly needing to compute them. note that
the gram matrix k has dimension n    n , which means that the computational complexity of performing
will be prohibitively expensive, and numerical approximations are required. this is in contrast to the
computational complexity of solving the normal equations in the original weight space viewpoint which
is o
. the dual parameterisation therefore helps us with the curse of dimensionality since the
complexity of learning in the dual parameterisation scales cubically with the number of training points    
not cubically with the dimension of the    vector.

the matrix inversion in equation (17.3.16) is o(cid:0)n 3(cid:1). for moderate to large n (greater than 5000), this
(cid:16)

dim (  )3(cid:17)

17.4 linear parameter models for classi   cation

in a binary classi   cation problem we are given train data, d = {(xn, cn), n = 1 . . . , n}, where the targets
c     {0, 1}. inspired by the lpm regression model, we can assign the id203 that a novel input x belongs
to class 1 using

p(c = 1|x) = f (xtw)

366

(17.4.1)

draft november 9, 2017

linear parameter models for classi   cation

figure 17.7: the logistic sigmoid function     (x) = 1/(1 + e     x).
the parameter    determines the steepness of the sigmoid. the
full (blue) line is for    = 1 and the dashed (red) for    = 10.
as           , the logistic sigmoid tends to a heaviside step func-
tion. the dotted curve (magenta) is the error function (probit)
0.5 (1 + erf(  x)) for    =      /4, which closely matches the stan-
dard logistic sigmoid with    = 1.

where 0     f (x)     1. in the statistics literature, f (x) is termed a mean function     the inverse function
f   1(x) is the link function2. two popular choices for the function f (x) are the logit and probit functions.
the logit is given by

f (x) =

ex

1 + ex =

1

1 + e   x

(17.4.2)

which is also called the logistic sigmoid and written   (x),    g(17.7). the scaled version is de   ned as

    (x) =   (  x)

(17.4.3)

a closely related model is probit regression which uses in place of the logistic sigmoid the cumulative
distribution of the standard normal distribution

(cid:90) x
(cid:90) x

      

0

f (x) =

1
   2  

    1
e

2 t2

dt =

1
2

(1 + erf(x))

where the standard error function,

is

erf(x)    

2
     

   t2

e

dt

(17.4.4)

(17.4.5)

the shape of the probit and logistic functions are similar under rescaling, see    g(17.7). we focus below on
the logit function.

17.4.1 id28

id28 corresponds to the model

p(c = 1|x) =   (b + xtw)

where b is a scalar, and w is a vector.

the decision boundary

(17.4.6)

the decision boundary is de   ned as that set of x for which p(c = 1|x) = p(c = 0|x) = 0.5. this is given by
the hyperplane

b + xtw = 0

(17.4.7)

on the side of the hyperplane for which b + xtw > 0, inputs x are classi   ed as 1   s, and on the other side
they are classi   ed as 0   s. the    bias    parameter b simply shifts the decision boundary by a constant amount.
the orientation of the decision boundary is determined by w, the normal to the hyperplane, see    g(17.8).
to clarify the geometric interpretation, let x be a point on the decision boundary and consider a new point
x    = x + w   , where w    is a vector perpendicular to w, so that wtw    = 0. then

= b + wt(cid:16)

x + w   (cid:17)

b + wtx   

= b + wtx + wtw   

= b + wtx = 0

(17.4.8)

thus if x is on the decision boundary, so is x plus any vector perpendicular to w. in d dimensions, the space
of vectors that are perpendicular to w occupy a d     1 dimensional hyperplane. for example, if the data is
two dimensional, the decision boundary is a one dimensional hyperplane, a line, as depicted in    g(17.8).

2these models are part of the    generalised linear models    class in the statistics literature which includes the regression model

and classi   cation models as special cases.

draft november 9, 2017

367

   5   4   3   2   101234500.10.20.30.40.50.60.70.80.91linear parameter models for classi   cation

w

figure 17.8: the decision boundary p(c = 1|x) = 0.5 (solid line).
for two dimensional data, the decision boundary is a line. if all
the train data for class 1 (   lled circles) lie on one side of the line,
and for class 0 (open circles) on the other, the data is said to
be linearly separable. more generally, w de   nes the normal to a
hyperplane and data is linearly separable if data from each of the
two classes lies on opposite sides of the hyperplane.

de   nition 17.1 (linear separability). if all the train data for class 1 lies on one side of a hyperplane, and
for class 0 on the other, the data is said to be linearly separable.

for d dimensional data, provided there are no more than d training points, then these are linearly separable
if they are linearly independent. to see this, let cn = +1 if xn is in class 1, and cn =    1 if xn is in class 0.
for the data to be linearly separable we require

wtxn + b =

 ncn,

n = 1, . . . , n

(17.4.9)

@@

 n are arbitrary positive constants. the above equations state that each input

where
the decision boundary. if there are n = d datapoints, the above can be written in matrix form as

is the correct side of

@@
@@

xw + b =

d

where x is a square matrix whose nth column contains xn and [b]i = b.
provided that x is invertible the solution is

w = x   1(cid:0)

d     b(cid:1)

(17.4.10)

@@

the vector d has element [d]n =  ncn.

@@

(17.4.11)

@@

the bias b can be set arbitrarily. this shows that provided the xn are linearly independent, we can always
   nd a hyperplane that linearly separates the data. provided the data are not-collinear (all occupying the
same d    1 dimensional subspace) the additional bias enables d + 1 arbitrarily labelled points to be linearly
separated in d dimensions.

an example dataset that is not linearly separable is given by the following four training points and class
labels

{([0, 0], 0), ([0, 1], 1), ([1, 0], 1), ([1, 1], 0)}

(17.4.12)

this data represents the xor function, and is plotted in    g(17.9). this function is not linearly separable
since no straight line has all inputs from one class on one side and the other class on the other. classifying
data which is not linearly separable can only be achieved using a non-linear decision boundary. it might be
that data is non-linearly separable in the original data space. an alternative is to map the data to a higher
dimension using a non-linear vector function; this creates a set of non-linearly dependent high-dimensional
vectors which can then be separated using a high-dimensional hyperplane. we discuss this in section(17.4.5).

the id88

we brie   y describe here the id88, an important early model in the    eld of ai, see for example [43].
the id88 deterministically assigns x to class 1 if b + wtx     0, and to class 0 otherwise. that is

p(c = 1|x) =   (b + xtw)

(17.4.13)

figure 17.9: the xor problem. this is not linearly separable.

368

draft november 9, 2017

linear parameter models for classi   cation

where the step function is de   ned as

  (x) =

if we consider the id28 model

p(c = 1|x) =     

b + xtw

(cid:17)

0 x     0

(cid:26) 1 x > 0
(cid:16)
          1

p(c = 1|x) =

b + xtw > 0
0.5 b + xtw = 0
b + xtw < 0
0

and take the limit           , we have the id88 like classi   er

(17.4.14)

(17.4.15)

(17.4.16)

the only di   erence between this    probabilistic id88    and the standard id88 is in the technical
de   nition of the value of the step function at 0. the id88 may therefore essentially be viewed as a
limiting case of id28.

maximum likelihood training

for this class discriminative model, we do not model the input distribution p(x) so that we may equivalently
consider the likelihood of the set of output class variables c conditioned on the set of training inputs x . if
we assume that each data point has been drawn independently from the same distribution that generates
the data (the standard i.i.d. assumption), the likelihood is (writing explicitly the conditional dependence
on the parameters b, w)

@@
@@

p(c|b, w,x ) =

p(cn|xn, b, w)

   
p(xn) =

p(c = 1|xn, b, w)cn

(1     p(c = 1|xn, b, w))1   cn

   
p(xn) (17.4.17)

where we have used the fact that cn     {0, 1}. for id28 this gives the log likelihood as

n(cid:89)

n=1

(cid:16)

(cid:17)
1       (b + wtxn)

cn log   (b + wtxn) + (1     cn) log

l(w, b) =

n=1

gradient ascent

there is no closed form solution to the maximisation of l(w, b) which needs to be carried out numerically.
one of the simplest methods is gradient ascent for which the gradient is given by

   wl =

(cn       (wtxn + b))xn

here we made use of the derivative relation for the logistic sigmoid

d  (x)/dx =   (x)(1       (x))

the derivative with respect to the bias is

dl
db

=

(cn       (wtxn + b))

the gradient ascent procedure then corresponds to updating the weights and bias using

wnew = w +      wl,

bnew = b +   

dl
db

where   , the learning rate is a scalar chosen small enough to ensure convergence3. the application of the
above rule will lead to a gradual increase in the log likelihood.

3in principle one may use a di   erent learning rate for each parameter.

draft november 9, 2017

369

n(cid:89)

n=1

n(cid:88)

n(cid:88)

n=1

n(cid:88)

n=1

(17.4.18)

(17.4.19)

(17.4.20)

(17.4.21)

(17.4.22)

linear parameter models for classi   cation

figure 17.10: the decision boundary p(c = 1|x) = 0.5
(solid line) and con   dence boundaries p(c = 1|x) =
0.9 and p(c = 1|x) = 0.1 after 10000 iterations of
(a): linearly
batch gradient ascent with    = 0.1.
(b): non-linearly separable data.
separable data.
note how the con   dence interval remains broad, see
demologreg.m.

(a)

(b)

batch training

writing the updates (17.4.22) explicitly gives

wnew = w +   

(cn       (wtxn + b))xn,

n(cid:88)

n=1

n(cid:88)

bnew = b +   

(cn       (wtxn + b))

n=1

(17.4.23)

this is called a batch update since the parameters w and b are updated only after passing through the whole
(batch) of train data.
for linearly separable data, we can also show that the weights must become in   nite at convergence. taking
the scalar product of equation(17.4.19) with w, we have the zero gradient requirement

(cn       n) wtxn = 0

n=1

where   n       (cid:0)wtxn + b(cid:1). for simplicity we assume b = 0. for linearly separable data we have

n(cid:88)

(cid:26) > 0 if cn = 1

< 0 if cn = 0

wtxn

then, using the fact that 0       n     1, we have

(cn       n) wtxn

    0 if cn = 1
    0 if cn = 0

(cid:26)

(17.4.24)

(17.4.25)

(17.4.26)

each term (cn       n) wtxn is therefore non-negative and the zero gradient condition requires the sum of
these terms to be zero. this can only happen if all the terms are zero, implying that cn =   n, requiring the
sigmoid to saturate and the weights to be in   nite.

online training

in practice it is common to update the parameters after each training example pair (xn, cn) has been
considered:

wnew = w +

  
n

(cn       (wtxn + b))xn,

bnew = b +

  
n

(cn       (wtxn + b))

(17.4.27)

an advantage of online training is that the dataset need not be stored since only the performance on the
current input is required. provided that the data is linearly separable, the above online procedure converges
(provided    is not too large). however, if the data is not linearly separable, the online version will not
converge since the opposing class labels will continually pull the weights one way and then the other as each
con   icting example is used to form an update. for the limiting case of the id88 (replacing   (x) with
  (x)) and linearly separable data, online updating converges in a    nite number of steps[229, 43], but does
not converge for non-linearly separable data.

370

draft november 9, 2017

00.20.40.60.8100.10.20.30.40.50.60.70.80.9100.20.40.60.8100.10.20.30.40.50.60.70.80.91linear parameter models for classi   cation

figure 17.11: id28 for classifying handwritten digits 1 and 7.
displayed is a hinton diagram of the 784 learned weight vector w, plotted as
a 28   28 image for visual interpretation. light squares are positive weights
and an input x with a (positive) value in this component will tend to increase
the id203 that the input is classed as a 7. similarly, inputs with
positive contributions in the dark regions tend to increase the id203
as being classed as a 1 digit. note that the elements of each input x are
either positive or zero.

geometry of the error surface

the hessian of the log likelihood l(w) is the matrix with elements4

hij    

   2l
   wiwj

=    

xn
i xn

j   n(1       n)

this is negative semide   nite since, for any z,

zihijzj =    

zixn

i zjxn

j   n(1       n)        

(cid:88)

ij

(cid:32)(cid:88)

(cid:88)

n

i

(cid:33)2

zixn
i

    0

(17.4.28)

(17.4.29)

(cid:88)

n

(cid:88)

i,j,n

this means that the error surface is concave (an upside down bowl) and batch gradient ascent converges to
the optimal solution, provided the learning rate    is small enough.

example 17.5 (classifying handwritten digits). we apply id28 to the 600 handwritten digits
of example(14.1), in which there are 300 ones and 300 sevens in the train data. using gradient ascent training
with a suitably chosen stopping criterion, the number of errors made on the 600 test points is 12, compared
with 14 errors using nearest neighbour methods. see    g(17.11) for a visualisation of the learned w.

17.4.2 beyond    rst order gradient ascent

since the surface has a single optimum, a newton update

wnew = wold +   h   1wold

(17.4.30)

where h is the hessian matrix as above and 0 <    < 1, will typically converge much faster than gradient
ascent. however, for large scale problems with dim (w) (cid:29) 1, the inversion of the hessian is computationally
demanding and limited memory bfgs or conjugate gradient methods are more practical alternatives, see
section(a.4).

17.4.3 avoiding overcon   dent classi   cation

provided the data is linearly separable the weights will continue to increase and the classi   cations will
become extreme. this is undesirable since the resulting classi   cations will be over-con   dent. one way
to prevent this is early stopping in which only a limited number of gradient updates are performed. an
alternative method is to add a penalty term to the objective function

(cid:48)
l

(w, b) = l(w, b)       wtw.

(17.4.31)

the scalar constant    > 0 encourages smaller values of w (remember that we wish to maximise the log
likelihood). an appropriate value for    can be determined using validation data.

1 in the d + 1 component. then for a d + 1 dimensional   w =(cid:0)wt, wd+1

(cid:1)t

4for simplicity we ignore the bias b. this can readily be dealt with by extending x to a d + 1 dimensional vector   x with a

, we have   wt   x = wtx + wd+1.

draft november 9, 2017

371

linear parameter models for classi   cation

figure 17.12: id28 p(c = 1|x) =   (wt  (x)) us-
2, x1x2)t. 1000
ing a quadratic function   (x) = (1, x1, x2, x2
iterations of gradient ascent training were performed with a
learning rate    = 0.1. plotted are the datapoints for the
two classes (cross) and (circle) and the equal id203 con-
tours. the decision boundary is the 0.5-id203 contour. see
demologregnonlinear.m.

1, x2

17.4.4 multiple classes

for more than two classes, one may use the softmax function

p(c = i|x) =

(cid:80)c

ewt
i x+bi
j=1 ewt

j x+bj

(17.4.32)

where c is the number of classes. when c = 2 this can be reduced to the logistic sigmoid model. one can
show that the likelihood for this case is also concave, see exercise(17.3) and [319]. gradient based training
methods then can be applied to training these models, as a straightforward extension of the two-class case.

17.4.5 the kernel trick for classi   cation

a drawback of id28 as described above is the simplicity of the decision surface     a hyperplane.
analogous to the regression case, one way to achieve more complex non-linear decision boundaries is to map
the inputs x in a non-linear way to a higher dimensional   (x) and use

(cid:16)

(cid:17)

p(c = 1|x) =   

wt  (x) + b

(17.4.33)

mapping into a higher dimensional space makes it easier to    nd a separating hyperplane since any set of
points that are linearly independent can be linearly separated provided we have as many dimensions as
datapoints. for the maximum likelihood criterion, we may use exactly the same algorithm as before on
replacing x with   (x). see    g(17.12) for a demonstration using a quadratic function. since only the scalar
product between the    vectors plays a role the dual representation section(17.3) may again be used in which
we assume the weight can be expressed in the form

w =

  n  (xn)

(17.4.34)

@@

(cid:88)

n

we then subsequently    nd a solution in terms of the dual parameters   n. this is potentially advantageous
since there may be less training points than dimensions of   . the classi   er depends only on the scalar
n   n  (xn)t  (x) and we can write more generally, using a positive de   nite kernel,
k(x, x(cid:48))

product wt  (x) = (cid:80)
(cid:32)(cid:88)
(cid:16)

p(c = 1|x) =   

n

p(c = 1|x) =   

atk(x)

(cid:17)

(cid:33)

ank(x, xn)

for convenience, we can write the above as

(17.4.35)

(17.4.36)

where the n dimensional vector k(x) has elements [k(x)]m = k(x, xm). then the above is of exactly the
same form as the original speci   cation of id28, namely as a function of a linear combination of
vectors. hence the same training algorithm to maximise the likelihood can be employed, simply on replacing
xn with k(xn). the details are left to the interested reader and follow closely the treatment of gaussian
processes for classi   cation, section(19.5).

372

draft november 9, 2017

0.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.40.40.40.40.40.40.50.50.50.50.50.60.60.60.60.60.70.70.70.70.70.80.80.80.80.80.90.90.90.90.9   0.500.511.5   0.500.511.5(cid:26)

(cid:26)

support vector machines

17.5 support vector machines

like kernel id28, id166s are a form of kernel linear classi   er. however, the id166 uses an
objective which more explicitly encourages good generalisation performance. id166s do not    t comfortably
within a probabilistic framework and as such we describe them here only brie   y, referring the reader to the
wealth of excellent literature on this topic5. the description here is inspired largely by [76].

17.5.1 maximum margin linear classi   er

in the id166 literature it is common to use +1 and    1 to denote the two classes. for a hyperplane de   ned
by weight w and bias b, a linear discriminant is given by

wtx + b

    0 class +1
< 0 class -1

(17.5.1)

for a point x that is close to the decision boundary at wtx + b = 0, a small change in x can lead to a
change in classi   cation. to make the classi   er more robust we therefore impose that for the train data at
least, the decision boundary should be separated from the data by some    nite amount  2 (assuming in the
   rst instance that the data is linearly separable):

wtx + b

     2
<     2

class +1
class -1

(17.5.2)

since w, b and  2 can all be rescaled arbitrary, we need to    x the scale of the above to break this invariance.
it is convenient to set   = 1 so that a point x+ from class +1 that is closest to the decision boundary satis   es

wtx+ + b = 1

and a point x    from class -1 that is closest to the decision boundary satis   es

wtx    + b =    1

(17.5.3)

(17.5.4)

from vector algebra,    g(17.13), the distance from the origin along the direction w to a point x is given by

wtx
   wtw

(17.5.5)

the margin between the hyperplanes for the two classes is then the di   erence between the two distances
along the direction w which is

wt
   wtw

(x+     x   ) =

2

   wtw

(17.5.6)

to set the distance between the two hyperplanes to be maximal, we therefore need to minimise the length
wtw. given that for each xn we have a corresponding label yn     {+1,   1}, in order to classify the training
labels correctly and maximise the margin, the optimisation problem is equivalent to:

subject to yn(cid:16)

(cid:17)

wtxn + b

    1,

n = 1, . . . , n

(17.5.7)

this is a quadratic programming problem. the factor 0.5 is just for convenience.

to account for potentially mislabelled training points (or for data that is not linearly separable), we relax
the exact classi   cation constraint and use instead

wtxn + b

    1       n

(17.5.8)

where the    slack variables    are   n     0. here each   n measures how far xn is from the correct margin, see
   g(17.14). for 0 <   n < 1 datapoint xn is on the correct side of the decision boundary. however for   n > 1,
the datapoint is assigned the opposite class to its training label. ideally we want to limit the size of these
   violations      n. here we brie   y describe two standard approaches.

draft november 9, 2017

373

minimise

wtw

1
2

yn(cid:16)

(cid:17)

support vector machines

figure 17.13: id166 classi   cation of data from two
classes (open circles and    lled circles). the decision
boundary wtx+b = 0 (solid line). for linearly separa-
ble data the maximum margin hyperplane is equidis-
tant from the closest opposite class points. these sup-
port vectors are highlighted in blue and the margin
in red. the distance of the decision boundary from

the origin is    b/   wtw, and the distance of a gen-
eral point x from the origin along the direction w is
xtw/   wtw.

figure 17.14: slack margin. the term   n measures
how far a variable is from the correct side of the mar-
if   n > 1 then the point will be
gin for its class.
misclassi   ed and treated as an outlier.

subject to yn(cid:16)

(cid:17)

wtxn + b

    1       n, n = 1, . . . , n

(17.5.9)

w

origin

  n

w

origin

2-norm soft-margin

the 2-norm soft-margin objective is

minimise

wtw +

1
2

c
2

(  n)2

(cid:88)

n

where c controls the number of mislabellings of the train data. the constant c needs to be determined
empirically using a validation set. the optimisation problem expressed by (17.5.9) can be formulated using
the lagrangian

(cid:17)

    1 +   n(cid:105)

wtxn + b

,

  n     0,   n     0 (17.5.10)

(cid:88)

l(w, b,   ,   ) =

yn(cid:16)
  n(cid:104)
   correct    side of the decision boundary yn(cid:0)wtxn + b(cid:1)

which is to be minimised with respect to

(  n)2    

(cid:88)

wtw +

c
2

1
2

n

n

w, b,    and maximised with respect to   . for points xn on the
    1 +   n > 0 so that maximising l with respect to
   requires the corresponding   n to be set to zero. only training points that are support vectors lying on
the decision boundary have non-zero   n. di   erentiating the lagrangian and equating to zero, we have the
conditions

@@

(cid:88)
(cid:88)

n

n

   
   wi

   
   b

l(w, b,   ,   ) = wi    

  nynxn

i = 0

l(w, b,   ,   ) =    

  nyn = 0

   
     n l(w, b,   ,   ) = c  n       n = 0

from this we see that the solution for w is given by

(cid:88)

w =

  nynxn

(17.5.11)

(17.5.12)

(17.5.13)

(17.5.14)

n

5http://www.support-vector.net

374

draft november 9, 2017

(17.5.15)

(17.5.16)

(17.5.17)

(17.5.18)

support vector machines

since only the support vectors have non-zero   n, the solution for w will typically depend on only a small
number of the training data. using these conditions and substituting back into the original problem, the
objective is equivalent to minimising

(cid:88)

n

l(  ) =

  n    

1
2

subject to

(cid:88)
1
(cid:88)
ynym  n  m (xn)t xm    
2c
  n     0
yn  n = 0,

n,m

n

(cid:88)

n

(  n)2

if we de   ne

k(xn, xm) = (xn)t xm

the optimisation problem is

(cid:88)

(cid:88)
(cid:88)
  n    
n,m
yn  n = 0,

1
2

n

n

(cid:18)

k(xn, xm) +

(cid:19)

1
c

  n,m

ynym  n  m

  n     0

optimising this objective is discussed in section(17.5.3).

1-norm soft-margin (box constraint)

in the 1-norm soft-margin version, one uses a 1-norm penalty

maximize

subject to

(cid:88)

n

c

  n

(cid:88)

n

  n

(cid:88)

n

to give the optimisation problem:

minimise

wtw +c

1
2

subject to yn(cid:16)

(cid:17)

wtxn + b

    1     n,   n     0,

n = 1, . . . , n (17.5.19)

where c is an empirically determined penalty factor that controls the number of mislabellings of the train
data. to reformulate the optimisation problem we use the lagrangian

l(w, b,   ) =

wtw + c

1
2

(cid:88)

  n(cid:104)

yn(cid:16)

  n   

n

(cid:17)

    1 +   n(cid:105)

   

(cid:88)

n

wtxn + b

rn  n,

  n     0,   n     0, rn     0
(17.5.20)

the variables rn are introduced in order to give a non-trivial solution (otherwise   n = c). following a
similar argument as for the 2-norm case, by di   erentiating the lagrangian and equating to zero, we arrive
at the optimisation problem

maximize

subject to

(cid:88)

(cid:88)
(cid:88)
  n    
n,m
yn  n = 0,

1
2

n

n

ynym  n  mk(xn, xm)

0       n     c

(17.5.21)

which is closely related to the 2-norm problem except that we now have the box-constraint 0       n     c.
17.5.2 using kernels
the    nal objectives (17.5.17) and (17.5.21) depend on the inputs xn only via the scalar product (xn)t xn.
if we map x to a vector function of x, then we can write

k(xn, xm) =   (xn)t  (xm)

(17.5.22)

this means that we can use any positive semide   nite kernel k and make a non-linear classi   er. see also
section(19.3).

draft november 9, 2017

375

soft zero-one loss for outlier robustness

figure 17.15: id166 training. the solid red and solid blue circles represent train
data from di   erent classes. the support vectors are highlighted in green. for
the un   lled test points, the class assigned to them by the id166 is given by the
colour. see demoid166.m

17.5.3 performing the optimisation

both of the above soft-margin id166 optimisation problems (17.5.17) and (17.5.21) are quadratic programs

for which the exact computational cost scales as o(cid:0)n 3(cid:1). whilst these can be solved with general purpose
optimisation algorithm[242], whose practical performance is typically o(cid:0)n 2(cid:1) or better. a variant of this

routines, speci   cally tailored routines that exploit the structure of the problem are preferred in practice. of
particular practical interest are    chunking    techniques that optimise over a subset of the   . in the limit of
updating only two components of   , this can be achieved analytically, resulting in the sequential minimal

algorithm [100] is provided in id166train.m. see    g(17.15) for a demonstration.

once the optimal solution   1   , . . . ,   n    is found the decision function for a new point x is

(cid:26) > 0

< 0

  n    ynk(xn, x) + b   

assign to class 1
assign to class -1

the optimal b    is determined using the maximum margin condition, equations(17.5.3,17.5.4):

b    =

1
2

min
yn=1

  m    ymk(xm, xn)     max
yn=   1

  m    ymk(xm, xn)

(cid:35)

(cid:88)

m

(cid:88)

n

(cid:34)

(cid:88)

m

(17.5.23)

(17.5.24)

17.5.4 probabilistic interpretation

kernelised logistic-regression has some of the characteristics of the id166 but does not express the large
margin requirement. also the sparse data usage of the id166 is similar to that of the relevance vector
machine we discuss in section(18.2.5). however, a probabilistic model whose map assignment matches
exactly the id166 is hampered by the normalisation requirement for a id203 distribution. whilst,
arguably, no fully satisfactory direct match between the id166 and a related probabilistic model has been
achieved, approximate matches have been obtained[272].

17.6 soft zero-one loss for outlier robustness

both the support vector machine and id28 are potentially misled by outliers. for the id166,
a mislabelled datapoint that is far from the correct side of the decision boundary would require a large
slack   . however, since exactly such large    are discouraged, it is unlikely that the id166 would admit such
a solution. for id28, the id203 of generating a mislabelled point far from the correct
side of the decision boundary is so exponentially small that this will never happen in practice. this means
that the model trained with maximum likelihood will never present such a solution. in both cases therefore
mislabelled points (or outliers) potentially have a signi   cant impact on the location of the decision boundary.

a robust technique to deal with outliers is to use the zero-one loss in which a mislabeled point contributes
only a relatively small loss. soft variants of this are obtained by using the objective

+   wtw

(17.6.1)

n(cid:88)

n=1

(cid:104)
    (b + wtxn)     cn(cid:105)2

which is to be minimised with respect to w and b. for            the    rst term above tends to the zero-one
loss. the second term represents a penalty on the length of w and prevents over   tting. kernel extensions

376

draft november 9, 2017

   2   1012   2.5   2   1.5   1   0.500.511.52soft zero-one loss for outlier robustness

figure 17.16: soft zero-one loss decision boundary (solid line) versus logistic
regression (dotted line). the number of mis-classi   ed training points using
the soft zero-one loss is 2, compared to 3 for id28. the penalty
   = 0.01 was used for the soft-loss, with    = 10. for id28, no
penalty term was used. the outliers have a signi   cant impact on the decision
boundary for id28, whilst the soft zero-one loss essentially
gives up on the outliers and    ts a classi   er to the remaining points. see
demosoftloss.m.

of this soft zero-one loss are straightforward.

unfortunately, the objective (17.6.1) is non-convex and    nding the optimal w, b is computationally dif-
   cult. a simple-minded scheme is to    x all components of w except one and then perform a numerical
one-dimensional optimisation over this single parameter wi. at the next step, another parameter wj is cho-
sen, and the procedure repeated until convergence. as usual,    can be set using validation. the practical
di   culties of minimising non-convex high-dimensional objective functions means that these approaches are
rarely used in practice. a discussion of practical attempts in this area is given in [307].

an illustration of the di   erence between id28 and this soft zero-one loss is given in    g(17.16),
which demonstrates how id28 is in   uenced by the mass of the data points, whereas the zero-one
loss attempts to minimise the number of mis-classi   cations whilst maintaining a large margin.

17.7 summary

    fitting id75 models based on least-squares is straightforward and requires only the solution of

a linear system.

    in classi   cation, the maximum likelihood criterion typically results in a simple concave function of the

parameters, for which simple gradient based learning methods are suitable for training.

    the well-known and historically important id88 can be viewed as a limiting case of id28.
    the kernel extensions of these linear models enable one to    nd non-linear decision boundaries in the

classi   cation case. these techniques are closely related to gaussian processes.

historical note

the id88 has a long history in arti   cial intelligence and machine learning. rosenblatt discussed the
id88 as a model for human learning, arguing that its distributive nature (the input-output    patterns   
are stored in the weight vector) is closely related to the kind of information storage believed to be present
in biological systems[252]. to deal with non-linear decision boundaries, the main thrust of research in the
ensuing neural network community was on the use of multilayered structures in which the outputs of per-
ceptrons are used as the inputs to other id88s, resulting in potentially highly non-linear discriminant
functions. this line of research was largely inspired by analogies to biological information processing in
which layered structures are prevalent. such multilayered arti   cial neural networks are fascinating and,
once trained, are extremely fast in forming their decisions. however, reliably training these systems is a
highly complex task and probabilistic generalisations in which priors are placed on the parameters lead to
computational di   culties. whilst perhaps less inspiring from a biological viewpoint, the alternative route
of using the kernel trick to boost the power of a linear classi   er has the advantage of ease of training and
generalisation to probabilistic variants. more recently, however, there has been a resurgence of interest in
multilayer systems, with new heuristics aimed at improving the di   culties in training, see for example [148].

draft november 9, 2017

377

   4   202468   4   2024681012exercises

17.8 code

democubicpoly.m: demo of    tting a cubic polynomial
demologreg.m: demo of id28
logreg.m: id28 gradient ascent training
demologregnonlinear.m: demo of id28 with a non-linear   (x)
id166train.m: id166 training using the smo algorithm
demoid166.m: id166 demo
demosoftloss.m: softloss demo
softloss.m: softid168

17.9 exercises

exercise 17.1.

1. give an example of a two-dimensional dataset for which the data are linearly separable, but not linearly

independent.

2. can you    nd a dataset which is linearly independent but not linearly separable?

1, . . . , n ) the    tted lines go through the point(cid:80)n

n=1(xn, yn)/n .

exercise 17.2. show that for both ordinary and orthogonal least squares regression    ts to data (xn, yn), n =

exercise 17.3. consider the softmax function for classifying an input vector x into one of c = 1, . . . , c
classes using

c x(cid:80)c

ewt
c(cid:48)=1 ewt
c(cid:48) x

p(c|x) =

(17.9.1)

a set of input-class examples is given by d = {(xn, cn), n = 1, . . . , n}.

1. relate this model to id28 when c = 2.

2. write down the log-likelihood l of the classes conditional on the inputs, assuming that the data is i.i.d.

3. compute the hessian with elements

hij =

   2l(d)
   wiwj

(cid:16)

w =

wt

1 , . . . , wt
c

(cid:17)t

where w is the stacked vector

(17.9.2)

(17.9.3)

and show that the hessian is negative semide   nite, that is zthz     0 for any z. hint: at some point
you need to use to use the result that the variance is non-negative.

exercise 17.4. derive from equation (17.5.9) the dual optimisation problem equation (17.5.15).

exercise 17.5. a datapoint x is projected to a lower dimensional vector   x using

  x = mx

(17.9.4)
where m is a given    fat    (short and wide) matrix. for a set of data {xn, n = 1, . . . , n} and corresponding
binary class labels yn     {0, 1}, using id28 on the projected datapoints   xn corresponds to a form
of constrained id28 in the original higher dimensional space x. explain if it is reasonable to
use an algorithm such as pca to    rst reduce the data dimensionality before using id28.

378

draft november 9, 2017

exercises

exercise 17.6. the logistic sigmoid function is de   ned as   (x) = ex/(1+ex). what is the inverse function,
     1(x)?
exercise 17.7. given a dataset d = {(xn, cn), n = 1, . . . , n}, where cn     {0, 1}, id28 uses the
model p(c = 1|x) =   (wtx + b). assuming the data is drawn independently and identically, show that the
derivative of the log likelihood l with respect to w is

wtxn + b

xn

(17.9.5)

n(cid:88)

(cid:16)

n=1

   wl =

cn       

(cid:16)

(cid:17)(cid:17)

exercise 17.8. consider a dataset d = {(xn, cn), n = 1, . . . , n}, where cn     {0, 1}, and x is a d dimen-
sional vector.

1. show that if the training data is linearly separable with the hyperplane wtx + b, the data is also

separable with the hyperplane   wtx +   b, where   w =   w,   b =   b for any scalar    > 0.

2. what consequence does the above result have for maximum likelihood training of id28 for

linearly separable data?

exercise 17.9. consider a dataset d = {(xn, cn), n = 1, . . . , n}, where cn     {0, 1}, and x is a n dimen-
sional vector. hence we have n datapoints in a n dimensional space. in the text we showed that to    nd
a hyperplane (parameterised by w and b) that linearly separates this data we need, for each datapoint xn,
wtxn + b =  n where  n > 0 for cn = 1 and  n < 0 for cn = 0. furthermore, we suggested an algorithm to
   nd such a hyperplane. comment on the relation between maximum likelihood training of id28
and the algorithm suggested above.
exercise 17.10. given training data d = {(xn, cn), n = 1, . . . , n}, cn     {0, 1}, where x are vector inputs,
a discriminative model is

p(c = 1|x) =   (b0 + v1g(wt

1 x + b1) + v2g(wt

2 x + b2))

(17.9.6)

where g(x) = exp(   0.5x2) and   (x) = ex/(1 + ex) (this is a neural network[43] with a single hidden layer
and two hidden units).

1. write down the log likelihood for the class conditioned on the inputs, based on the usual i.i.d. assump-

tion.

2. calculate the derivatives of the log likelihood as a function of the network parameters, w1, w2, b1, b2,

v0,v1, v2.

3. comment on the relationship between this model and id28.

4. comment on the decision boundary of this model.

draft november 9, 2017

379

exercises

380

draft november 9, 2017

chapter 18

bayesian linear models

the previous chapter discussed the use of linear models in classi   cation and regression.
in this chapter
we discuss using priors on the parameters and the resulting posterior distribution over parameters. this
represents a powerful extension since it enables us to specify in a principled way our prior knowledge and
takes into account that when data is limited there will be considerable uncertainty as to which is the    best   
parameter estimate.

18.1 regression with additive gaussian noise

the linear models in chapter(17) were trained under maximum likelihood and do not deal with the is-
sue that, from a probabilistic perspective, parameter estimates are inherently uncertain due to the lim-
ited available training data. regression refers to inferring a mapping on the basis of observed data
d = {(xn, yn), n = 1, . . . , n}, where (xn, yn) represents an input-output pair. we discuss here the scalar
output case (and vector inputs x) with the extension to the vector output case y being straightforward. we
assume that each (clean) output is generated from a model f (x; w) where the parameters w of the function
f are unknown. an observed output y is generated by the addition of noise    to the clean model output,

y = f (x; w) +   

if the noise is gaussian distributed,        n
id203

(cid:0)y f (x; w),   2(cid:1) =

p(y|w, x) = n

(cid:0)   0,   2(cid:1), the model generates an output y for input x with
(cid:18)

(18.1.1)

(cid:19)

(18.1.2)

1

   2    2

exp

   

1

2  2 [y     f (x; w)]2

here our interest is only in modelling this output distribution, and not the distribution of the inputs. as such
no parameters are used to model the inputs.
if we assume that each data input-output pair is generated
identically and independently, the likelihood the model generates the data is

p(d|w) =

p(yn|w, xn)p(xn)

(18.1.3)

we may use a prior weight distribution p(w) to quantify our a priori belief in the suitability of each parameter
setting. writing d = {dx,dy}, the posterior weight distribution is then given by

p(w|d)     p(d|w)p(w)     p(dy|w,dx)p(w)

using the gaussian noise assumption, and for convenience de   ning    = 1/  2, this gives

n(cid:89)

n=1

log p(w|d) =    

  
2

n(cid:88)

n=1

[yn     f (xn; w)]2 + log p (w) +

n
2

log    + const.

381

(18.1.4)

(18.1.5)

b(cid:88)

i=1

regression with additive gaussian noise

  

w

xn

yn

n

figure 18.1: belief network representation of a bayesian model for
regression under the i.i.d. data assumption. the hyperparameter   
acts as a form of regulariser, controlling the    exibility of the prior on
the weights w. the hyperparameter    controls the level of noise on
the observations.

  

note the similarity between equation (18.1.5) and the regularised training error equation (17.2.17). in the
probabilistic framework, we identify the choice of a sum squared error with the assumption of additive
gaussian noise. similarly, the regularising term is identi   ed with log p(w).

18.1.1 bayesian linear parameter models

linear parameter models, as discussed in chapter(17), have the form

n(cid:88)

n=1

(cid:104)

(cid:105)2
yn     wt  (xn)

f (x; w) =

wi  i(x)     wt  (x)

(18.1.6)

where the parameters wi are also called    weights    and the number of basis functions is dim (w) = b. such
models have a linear parameter dependence, but may represent a non-linear input-output mapping if the
basis functions   i(x) are non-linear in x. since the output scales linearly with w, we can discourage extreme
output values by penalising large weight values. a natural weight prior is thus

(cid:0)w 0,   

   1i(cid:1) =

(cid:16)   

(cid:17) b

2  

(cid:16)

2 exp

  
2

   

wtw

(cid:17)

p(w|  ) = n

(18.1.7)

where the precision    is the inverse variance. if    is large, the total squared length of the weight vector w
is encouraged to be small. the full model p(d, w|  ,   ) is then speci   ed, see    g(18.1). under the gaussian
noise assumption, the posterior distribution is

log p(w|  ,d) =    

  
2

  
2

   

wtw + const.

(18.1.8)

where    = {  ,   } represents the hyperparameter set. parameters that determine the functions    may also be
included in the hyperparameter set. completing the square, section(8.4.1), the weight posterior is therefore
a gaussian distribution,

p(w|  ,d) = n (w m, s)

where the covariance and mean are given by

(cid:33)   1

s =

  i +   

   (xn)   t (xn)

,

m =   s

n(cid:88)

n=1

yn   (xn)

the mean prediction for an input x is then given by

  f (x)    

f (x; w)p(w|d,   )dw = mt   (x) .

similarly, the variance of the underlying estimated clean function is

var(f (x)) =

wt  (x)

p(w|d,  )       f (x)2 =   t(x)s  (x)

(cid:105)2(cid:29)

(cid:32)

(cid:90)

n(cid:88)

n=1

(cid:28)(cid:104)

(18.1.9)

(18.1.10)

(18.1.11)

(18.1.12)

the output variance var(f (x)) depends only on the input variables and not on the training outputs y. since
the additive noise    is uncorrelated with the model outputs, the predictive variance is

var(y(x)) = var(f (x)) +   2

(18.1.13)

and represents the variance of the    noisy    output for an input x.

382

draft november 9, 2017

regression with additive gaussian noise

(a)

(b)

(c)

(a):
figure 18.2: along the horizontal axis we plot the input x and along the vertical axis the output y.
(b): prediction using regularised training
the raw input-output training data and basis functions   i(x).
and (poorly chosen)    xed hyperparameters. (c): prediction using ml-ii optimised hyperparameters. also
plotted are standard error bars on the clean underlying function,   f (x)   

var(f (x)).

(cid:112)

example 18.1. in    g(18.2b), we show the mean prediction on the data in    g(18.2a) using 15 gaussian
basis functions

  i(x) = exp(cid:0)

   0.5(x     ci)2/  2(cid:1)

(18.1.14)

with width    = 0.032 and centres ci spread out evenly over the 1-dimensional input space from    2 to 2. we
set the other hyperparameters by hand to    = 100 and    = 1. the prediction severely over   ts the data, a
result of a poor choice of hyperparameter settings. this is resolved in    g(18.2c) using the ml-ii parameters,
as described below.

18.1.2 determining hyperparameters: ml-ii

the hyperparameter posterior distribution is

p(  |d)     p(d|  )p(  )

(18.1.15)

a simple summarisation of the posterior is given by the map assignment which takes the single    optimal   
setting:
   
  

= argmax

(18.1.16)

if the prior belief about the hyperparameters is weak (p(  )     const.), this is equivalent to using the    that
maximises the marginal likelihood

p(  |d)

  

(cid:90)

p(d|  ) =

p(d|  , w)p(w|  )dw

(18.1.17)

this approach to setting hyperparameters is called    ml-ii    [34] or the evidence procedure[196].

in the case of bayesian linear parameter models under gaussian additive noise, computing the marginal
likelihood equation (18.1.17) involves only gaussian integration. a direct approach to deriving an expression
for the marginal likelihood is to consider

(cid:32)

n(cid:88)

n=1

  
2

(cid:104)
yn     wt  (xn)

(cid:105)2

  
2

   

(cid:33)

@@
@@

p(d|  , w)p(w) = exp

   

draft november 9, 2017

wtw

(  /2  )n/2 (  /2  )b/2

(18.1.18)

383

regression with additive gaussian noise

by collating terms in w (completing the square, section(8.4.1)), the above represents a gaussian in w with
additional factors. after integrating over this gaussian we have

2 log p(d|  ) =      

(yn)2 + dts   1d

+ log det (s) + b log    + n log        n log (2  )

(18.1.19)

@@

n(cid:88)

n=1

(cid:88)

where

d =   

  (xn)yn

(18.1.20)

n

see exercise(18.2) for an alternative expression.

example 18.2. we return to the example in    g(18.2) and try to    nd a more appropriate setting for the
hyperparameters. using the hyperparameters   ,   ,    that optimise expression (18.1.19) gives the results in
   g(18.2c) where we plot both the mean predictions and standard predictive error bars. this demonstrates
that an acceptable setting for the hyperparameters can be obtained by maximising the marginal likelihood.
generally speaking, provided the number of hyperparameters is low compared to the number of datapoints,
setting hyperparameters using ml-ii will not be at risk of over   tting.

18.1.3 learning the hyperparameters using em

as described above, we can set hyperparameters    by maximising the marginal likelihood equation (18.1.17).
a convenient computational procedure to achieve this is to interpret the w as latent variables and apply
the em algorithm, section(11.2). from equation (18.1.17), the energy term is

according to the general em procedure we need to maximise the energy term. for a hyperparameter    the
derivative of the energy is given by

for the bayesian lpm with gaussian weight and noise distributions, we obtain

p(w|  old,d)

trace

s

(cid:32)

(cid:33)

  (xn)  t(xn)

n(cid:88)

n=1

where s and m are given in equation (18.1.10). solving for the zero derivatives gives the m-step update

e     (cid:104)log p(d|w,   )p(w|  )(cid:105)p(w|d,  old)

(cid:28)    

(cid:29)

   
     

e    

log p(d|w,   )p(w|  )

     

p(w|d,  old)

   
     

e =

n
2      

=

n
2      

1
2

1
2

n=1

n(cid:88)
n(cid:88)
(cid:104)

n=1

(cid:28)(cid:104)
(cid:105)2(cid:29)
yn     wt  (xn)
(cid:104)
(cid:105)2
yn     mt  (xn)
(cid:105)2

   

1
2

(cid:16)

yn     mt  (xn)

+ trace

s  s

(cid:17)

n(cid:88)

n=1

1
  new =

1
n

n(cid:88)

where

  s    

1
n

  (xn)  t(xn)

n=1
similarly, for   ,
b
2      

   
     

e =

(cid:69)

(cid:68)

1
2

wtw

which, on equating to zero, gives the update

(cid:16)

p(w|  old,d)

(cid:17)

1
  new =

1
b

trace (s) + mtm

(cid:16)

=

b
2      

1
2

(cid:17)

trace (s) + mtm

(18.1.21)

(18.1.22)

(18.1.23)

(18.1.24)

(18.1.25)

(18.1.26)

(18.1.27)

an alternative    xed point procedure that can be more rapidly convergent than em is given in equation
(18.1.37). closed form updates for other hyperparameters, such as the width of the basis functions, are
generally not available, and the corresponding energy term needs to be optimised numerically.

384

draft november 9, 2017

regression with additive gaussian noise

figure 18.3: predictions for an rbf for di   erent widths   . for each    the ml-ii optimal   ,    are obtained
by running the em procedure to convergence and subsequently used to form the predictions. in each panel
the dots represent the training points, with x along the horizontal axis and y along the vertical axis. mean
predictions are plotted, along with predictive error bars of one standard deviation. according to ml-ii, the
best model corresponds to    = 0.37, see    g(18.4). the smaller values of    over   t the data, giving rise to too
   rough    functions. the largest values of    under   t, giving too    smooth    functions. see demobayeslinreg.m.

18.1.4 hyperparameter optimisation : using the gradient

to maximise equation (18.1.17) with respect to hyperparameters   , we can make use of the general identity
from equation (11.6.3) which, in this context, is

(cid:29)

(cid:28)    
(cid:28)    

   
     

log p(d|  ) =

log p(d|w,   )p(w|  )

p(w|d,  )

     

since the likelihood is independent of   ,

   
     

log p(d|  ) =

using

log p(w|  )

p(w|  ,d)

     

(cid:29)

log p(w|  ) =    

wtw +

  
2

b
2

log    + const.

(18.1.28)

(18.1.29)

(18.1.30)

figure 18.4: the log marginal likelihood log p(d|  ,      (  ),      (  ))

having found the optimal values of the hyperparameters    and
   using ml-ii. these optimal values are dependent on   . ac-
cording to ml-ii, the best model corresponds to    = 0.37. from
   g(18.3) we see that this gives a reasonable    t to the data.

draft november 9, 2017

385

   202   2   1012lambda=0.01   202   2   1012lambda=0.05   202   2   1012lambda=0.09   202   2   1012lambda=0.13   202   2   1012lambda=0.17   202   2   1012lambda=0.21   202   2   1012lambda=0.25   202   2   1012lambda=0.29   202   2   1012lambda=0.33   202   2   1012lambda=0.37   202   2   1012lambda=0.41   202   2   1012lambda=0.45   202   2   1012lambda=0.49   202   2   1012lambda=0.53   202   2   1012lambda=0.57   202   2   1012lambda=0.61   202   2   1012lambda=0.65   202   2   1012lambda=0.69   202   2   1012lambda=0.73   202   2   1012lambda=0.77   0.100.10.20.30.40.50.60.702468101214x 109log marginal likelihoodlambdawe obtain

   
     

log p(d|  ) =

1
2

(cid:28)

   wtw +

b
  

(cid:29)

p(w|  ,d)

setting the derivative to zero, the optimal    satis   es

(cid:69)

0 =    

wtw

p(w|  ,d)

+

b
  

one may now form a    xed point equation

  new =

b

(cid:104)wtw(cid:105)p(w|  ,d)

(cid:68)

(cid:69)

regression with additive gaussian noise

(18.1.31)

(18.1.32)

(18.1.33)

(cid:68)

which is equivalent to an em update, equation (18.1.27), for this model. for a gaussian posterior,
p(w|  ,d) = n (w m, s),

wtw

= trace

= trace (s) + mtm

(18.1.34)

(cid:16)(cid:68)

wwt(cid:69)

    (cid:104)w(cid:105)(cid:104)w(cid:105)t + (cid:104)w(cid:105)(cid:104)w(cid:105)t(cid:17)

  new =

b

trace (s) + mtm

(18.1.35)

one may similarly    nd the gradient and associated    xed point update for    which, again is equivalent to
the em update for this model.

gull-mackay    xed point iteration

from equation (18.1.32) we have

(cid:68)

(cid:69)

0 =      

wtw

p(w|  ,d)

+ b =      s       mtm + b

so that an alternative    xed point equation[136, 195] is

  new =

b       trace (s)

mtm

(18.1.36)

(18.1.37)

in practice this update converges more rapidly than equation (18.1.35). similarly, one can form an alternative
update for   

(cid:16)

(cid:17)

(cid:80)n
1       trace
n=1 [yn     mt  (xn)]2

s  s

  new =

1
n

  i(x) = exp(cid:0)

   0.5(x     ci)2/  2(cid:1)

(18.1.38)

(18.1.39)

example 18.3 (learning the basis function widths). in    g(18.3) we plot the training data for a regression
problem using a bayesian lpm. a set of 10 radial basis functions (rbfs) are used,

with ci, i = 1, . . . , 10 spread out evenly between    2 and 2. the hyperparameters    and    are learned by
ml-ii under em updating. for a    xed width    we then present the predictions, each time    nding the
optimal    and    for this width. the optimal joint   ,   ,    hyperparameter setting is obtained as described
in    g(18.4) which shows the marginal log likelihood for a range of widths. the    t resulting from the jointly
optimal   ,   ,    hyperparameter is reasonable.

386

draft november 9, 2017

regression with additive gaussian noise

18.1.5 validation likelihood

the hyperparameters found by ml-ii are those which are best at explaining the training data. in principle,
this is di   erent from those that are best for prediction and, in practice therefore, it is reasonable to set
hyperparameters also by validation techniques. one such method is to set hyperparameters by minimal
prediction error on a validation set. another common technique is to set hyperparameters    by their
likelihood on a validation set {xval,yval}     {(xm

val), m = 1, . . . , m}:

val, ym

(cid:90)

p(yval|  ,xtrain,ytrain,xval) =

w

p(yval|w,   )p(w|  ,xtrain,ytrain)

(18.1.40)

from which we obtain (see exercise(18.3))

log p(yval|  ,dtrain,xval) =    

1
2

log det (2  cval)    

1
2

(yval       valm)t c   1

val (yval       valm)

(18.1.41)

where yval =(cid:2)y1

(cid:3)t

val, . . . , ym
val

, covariance

and the design matrix of explanatory variables

cval       vals  t

val +   2im

val =(cid:2)  (x1

  t

val)(cid:3)

val), . . . ,   (xm

(18.1.42)

(18.1.43)

the optimal hyperparameters       can then be found by maximising (18.1.41) with respect to   .

18.1.6 prediction and model averaging

for    xed hyperparameters, the bayesian lpm de   nes a gaussian posterior distribution on the weights w.
the distribution can be used to compute the expected predictor and also the variance of the predictor.
more generally, however, we can place a prior distribution on the hyperparameters themselves and obtain a
corresponding posterior

p(  |d)     p(d|  )p(  )

(18.1.44)

the mean function predictor is then given by integrating over both the posterior weights and the hyperpa-
rameters

(cid:90) (cid:26)(cid:90)

(cid:27)

  f (x) =

f (x; w)p(w,   |d)dwd   =

f (x; w)p(w|  ,d)dw

p(  |d)d  

(18.1.45)

the term in curly brackets is the mean predictor for    xed hyperparameters. equation(18.1.45) then weights
each mean predictor by the posterior id203 of the hyperparameter p(  |d). this is a general recipe for
combining model predictions, where each model is weighted by its posterior id203. however, computing
the integral over the hyperparameter posterior is numerically challenging and approximations are usually
required. provided the hyperparameters are well determined by the data, we may instead approximate the
above hyperparameter integral by    nding the map hyperparameters and use

  f (x)    

f (x; w)p(w|  

   

,d)dw

(18.1.46)

under a    at prior p(  ) = const., this is equivalent to using the mean predictor with hyperparameters set by
ml-ii. an alternative is to draw samples   l, l = 1, . . . , l from the posterior distribution p(  |d) and form
then a predictor by averaging over the samples

(cid:90)

(cid:90)

(cid:90)

l(cid:88)

l=1

  f (x)    

1
l

f (x; w)p(w|  l,d)dw

draft november 9, 2017

(18.1.47)

387

regression with additive gaussian noise

18.1.7 sparse linear models

a common interest is to attempt to explain data using as few of the inputs as possible. more generally, we
can try to    nd the most parsimonious explanation using a limited number of features   i(x). from a bayesian
lpm perspective, we require a prior that strongly encourages only a limited number of weights wi to be
active, with the rest being set to zero. formally, this corresponds to a non-gaussian prior on the weights
which hampers the use of exact bayesian methods. non bayesian methods make use of penalty terms such
i |wi| for which the objective of the lpm remains concave [292]l; this is closely
related to using laplace priors in the bayesian setting. here we brie   y discuss some approximate methods
for sparse id75.

as l1 (lasso) regularisation(cid:80)

the relevance vector machine

the relevance vector machine assumes that only a small number of components of the basis function vector
are relevant in determining the solution for w. for a predictor,

f (x; w) =

wi  i(x)     wt  (x)

(18.1.48)

it is often the case that some basis functions will be redundant in the sense that a linear combination of the
other basis functions can reproduce the training outputs with insigni   cant loss in accuracy. to exploit this
e   ect and seek a parsimonious solution we may use a more re   ned prior that encourages each wi itself to be
small:

p(w|  ) =

p(wi|  i)

(18.1.49)

for computational convenience, in the rvm one typically chooses a gaussian

(cid:0)wi 0,   

(cid:1) =

   1
i

(cid:16)   i

(cid:17) 1

2  

2 exp

  i
2

   

w2
i

(cid:17)

p(wi|  i) = n

.

(18.1.50)

sparsity is then achieved by optimising over the hyperparameters   i, with large   i e   ectively forcing a
weight wi to be zero. the modi   cations required to the description of section(18.1.1) are to replace s with

(cid:16)

(cid:33)   1

s =

diag (  ) +   

   (xn)   t (xn)

(18.1.51)

b(cid:88)

i=1

(cid:89)

i

the marginal likelihood is then given by

2 log p(d|  ) =      

(yn)2 + dts   1d     log det (s) +

b(cid:88)

i=1

log   i + n log        n log (2  )

(18.1.52)

the em update for    is unchanged, and the em update for each   i is

1

  new

i

= [s]ii + m2
i

(18.1.53)

a potential di   culty with this approach is that there are as many hyperparameters as there are parameters
and    nding the optimal hyperparameters by ml-ii can be problematic, resulting in overly aggressive pruning
of the weights. this can be ameliorated by using a more fully bayesian approach [295].

spike and slab priors

a natural alternative approach to sparse id75 is to use binary indicators si     {0, 1}

b(cid:88)

f (x; w) =

388

siwi  i(x),

p(w) =

i=1

i

(cid:89)

n

(cid:0)wi 0,   2(cid:1)

(18.1.54)

draft november 9, 2017

(cid:32)

n(cid:88)

n=1

n(cid:88)

n=1

(18.1.55)

(18.1.56)

classi   cation

algorithm 18.1 evidence procedure for bayesian id28

1: initialise w and   .
2: while not converged do
3:
4:
5: end while

find optimal w    by iterating equation (18.2.16), equation (18.2.15) to convergence.
update    according to equation (18.2.9).

(cid:46) e-step
(cid:46) m-step

so that only those weights wi with si = 1 will contribute to the function. one can specify a level of sparsity
by choosing a prior on the joint set p(s1, . . . , sb) that encourages only a small number of the si to be 1, the
rest being zero. this can be achieved using a product of bernoulli distributions

b(cid:89)

i=1

p(s) =

f (x; w) =

p(w|s) =

  si (1       )1   si
b(cid:88)
(cid:8)sin

wi  i(x),

i=1

(cid:89)

i

where 0            1 speci   es the prior level of sparsity. this is equivalent to using the original lpm

with the    spike and slab    weight prior

(cid:0)wi 0,   2(cid:1) + (1     si)   (wi)(cid:9)

which places either a spike at 0 (when si = 0) or a broader gaussian    slab    n
two formulations are equivalent and result in a non-gaussian posterior for the weights, for which approx-
imation methods are required. popular approaches include id150, chapter(27), and variational
methods chapter(28).

(18.1.57)

(cid:0)wi 0,   2(cid:1) when si = 1. these

18.2 classi   cation

for the id28 model

(cid:33)

(cid:32) b(cid:88)

i=1

p(c = 1|w, x) =   

wi  i(x)

(18.2.1)

the maximum likelihood method returns only a single optimal w. to deal with the inevitable uncertainty
in estimating w we need to determine the posterior distribution. to do so we    rst de   ne a prior on the
weights p(w). as for the regression case, a convenient choice is a gaussian

(cid:0)w 0,   

   1i(cid:1) =

(cid:16)

(cid:17)

  b/2
(2  )b/2 exp

     wtw/2

p(w|  ) = n

(18.2.2)

where    is the inverse variance (precision). given a dataset of input-class labels, d = {(xn, cn) , n = 1, . . . , n},
the parameter posterior is (assuming we make no model of the input distribution)

p(w|  ,d) =

p(d|w,   )p(w|  )

p(d|  )

=

1

p(d|  )

p(w|  )

p(cn|xn, w)

(18.2.3)

unfortunately, this distribution is not of any standard form and exactly inferring statistics such as the mean
is formally computationally intractable.

draft november 9, 2017

389

n(cid:89)

n=1

18.2.1 hyperparameter optimisation

classi   cation

analogous to the regression case, hyperparameters such as    can be set by maximising the marginal likelihood

(cid:90)

(cid:90) n(cid:89)

n=1

(cid:16)   

(cid:17)b/2

(cid:16)

(cid:17)

p(cn|xn, w)

2  

exp

  
2

   

wtw

p(d|  ) =

p(d|w)p(w|  )dw =

dw.

(18.2.4)

there are several approaches one could take to approximate this integral and below we discuss the laplace
and a variational technique. common to all approaches, however, is the form of the gradient, di   ering only
in the statistics under an approximation to the posterior. for this reason we derive    rst generic hyperpa-
rameter update formulae that apply under both approximations.

to    nd the optimal   , we search for the zero derivative of log p(d|  ). this is equivalent to the linear
regression case, and we immediately obtain

(cid:28)

(cid:29)

   
     

log p(d|  ) =

1
2

   wtw +

b
  

p(w|  ,d)

setting the derivative to zero, an exact equation is that the optimal    satis   es

(cid:68)

(cid:69)

0 =    

wtw

p(w|  ,d)

+

b
  

one may now form a    xed point equation

  new =

b

(cid:104)wtw(cid:105)p(w|  ,d)

(18.2.5)

(18.2.6)

(18.2.7)

the averages in the above expression cannot be computed exactly and are replaced by averages with respect
to an approximation of the posterior q(w|  ,d). for a gaussian approximation of the posterior, q(w|  ,d) =
n (w m, s)
wtw

    (cid:104)w(cid:105)(cid:104)w(cid:105)t + (cid:104)w(cid:105)(cid:104)w(cid:105)t(cid:17)

wwt(cid:69)

= trace (s) + mtm

(cid:16)(cid:68)

(18.2.8)

= trace

(cid:69)

(cid:68)

  new =

b

trace (s) + mtm

in this case the gull-mackay alternative    xed point equation[136, 195] is

  new =

b       s
mtm

(18.2.9)

(18.2.10)

the hyperparameter updates (18.2.9) and (18.2.10) have the same form as for the regression model. the
mean m and covariance s of the posterior in the regression and classi   cation cases are however di   erent.
in the classi   cation case we need to approximate the mean and covariance, as discussed below.

18.2.2 laplace approximation

the laplace approximation, section(28.2) is a simple approximation made by    tting a gaussian locally
around the most probable point of the posterior. the weight posterior for the id28 model is
given by

p(w|  ,d)     exp (   e(w))

where

e(w) =

  
2

wtw    

(cid:16)

wthn(cid:17)

,

log   

n(cid:88)

n=1

hn     (2cn     1)  n

(18.2.11)

(18.2.12)

390

draft november 9, 2017

classi   cation

(18.2.13)

(18.2.14)

(18.2.15)

by approximating e(w) by a quadratic function in w, we obtain a gaussian approximation q(w|d,   ) to
p(w|d,   ). to do so we    rst    nd the minimum of e(w). di   erentiating, we obtain

n(cid:88)

n=1

   e =   w    

(1       n)hn,

  n       

(cid:16)

wthn(cid:17)

it is convenient to use a id77 to    nd the optimum. the hessian matrix with elements

hij    

   2

   wi   wj

e(w)

is given by

h =   i +

n(cid:88)
(cid:124)

n=1

(cid:125)
  n(1       n)  n (  n)t

(cid:123)(cid:122)

j

note that the hessian is positive semide   nite (see exercise(18.4)) so that the function e(w) is convex (bowl
shaped), and    nding a minimum of e(w) is numerically unproblematic. a newton update then is

wnew = w     h   1   e

given a converged w, the posterior approximation is given by

(18.2.16)

q(w|d,   ) = n (w m, s) ,

(18.2.17)
where m = w    is the converged estimate of the minimum point of e(w) and h is the hessian of e(w) at
this point.

s     h   1

approximating the marginal likelihood

using the laplace approximation, the marginal likelihood is given by

(cid:90)

p(d|  ) =

p(d|w)p(w|  ) =

w

(cid:90)

e

w

   e(w)

(18.2.18)

      
2 wtw    
e

for an optimum value m = w   , we approximate the marginal likelihood using (see section(28.2))

log p(d|  )     l(  )        

(w   

)tw   

  
2

1
2

   

log det (  i + j) +

b
2

log    (18.2.19)

given this approximation l(  ) to the marginal likelihood, an alternative strategy for hyperparameter opti-
misation is to optimise l(  ) with respect to   . by di   erentiating l(  ) directly, the reader may show that
the resulting updates are in fact equivalent to using the general condition equation (18.2.6) under a laplace
approximation to the posterior statistics.

making predictions

ultimately, our interest is to classify in novel situations, averaging over posterior weight uncertainty (as-
suming    is    xed to a suitable value),

(cid:90)

n(cid:89)

w

n=1

(cid:16)   

2  

(w   

(cid:17)b/2
)t hn(cid:17)

p(cn|xn, w)
(cid:16)
(cid:88)

log   

+

n

p(c = 1|x,d) =

p(c = 1|x, w)p(w|d)dw

(18.2.20)

the b dimensional integrals over w cannot be computed analytically and numerical approximation is re-
quired. using the laplace approximation, we replace the exact posterior p(w|d) with our laplace   s method
gaussian approximation q(w|d) = n (w m, s)

p(c = 1|x,d)    

p(c = 1|x, w)q(w|d)dw =

draft november 9, 2017

n (w m, s) dw

(18.2.21)

391

(cid:90)

(cid:16)

  

  xtw

(cid:17)

(cid:90)

(cid:90)

classi   cation

figure 18.5: bayesian id28 using rbf

functions   i(x) = exp(cid:0)

     (x     mi)2(cid:1), placing the cen-

tres mi on a subset of the training points. the green
points are training data from class 1, and the red
points are training data from class 0. the contours
represent the id203 of being in class 1. the op-
timal value of    found by ml-ii is 0.45 (   is set by
hand to 2). see demobayeslogregression.m

(cid:90)

(cid:16)

(cid:17)

need to carry out an integral in b dimensions. however, since the term   (cid:0)  xtw(cid:1) depends on w via the scalar

where, for notational convenience, we write   x       (x). to compute the predictions it would appear that we
product   xtw, we only require the integral over the one-dimensional projection h       xtw, see exercise(18.5).
that is

p(c = 1|x,d)    

   (h) q(h|x,d)dh

(18.2.22)

since under the laplace approximation w is gaussian distributed, then so is the linear projection h

q(h|x,d) = n

h   xtm,   xt    x

.

(18.2.23)

predictions may then be made by numerically evaluating the one-dimensional integral over the gaussian
distribution in h, equation (18.2.22). fast approximate methods for computing this are discussed below.

approximating the gaussian average of a logistic sigmoid

predictions under a gaussian posterior approximation require the computation of

i     (cid:104)  (x)(cid:105)n (x   ,  2)

(18.2.24)

gaussian quadrature is an obvious numerical candidate[245]. an alternative is to replace the logistic sigmoid
by a suitably transformed erf function[196], the reason being that the gaussian average of an erf function
is another erf function. using a single erf, an approximation is1

  (x)    

1
2

(1 + erf (  x))

(18.2.25)

these two functions agree at       , 0,   . a reasonable criterion is that the derivatives of these two should
agree at x = 0 since then they have locally the same slope around the origin and have globally similar shape.
using   (0) = 0.5 and that the derivative is   (0)(1       (0)), this requires

1
4

=

  
             =

     
4

(18.2.26)

a more accurate approximation can be obtained by taking a convex combination of scaled erf functions [23],
see logsigapp.m

(cid:82) x
1note that the de   nition of the erf function used here is taken to be consistent with matlab, namely that erf(x)    
0 e   t2

dt. other authors de   ne it to be the cumulative density function of a standard gaussian,

(cid:82) x
       e    1
2 t2

2   
  

dt.

2   
  

392

draft november 9, 2017

   6   4   20246   6   4   202460.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.50.50.50.50.50.50.60.60.60.60.70.70.70.70.80.80.80.90.9classi   cation

18.2.3 variational gaussian approximation

an alternative to laplace   s method is to use a so-called variational method. since the marginal likelihood is a
key quantity that can be used for example for hyperparameter selection, it is useful to have a lower bound on
the log marginal likelihood. like in em, one can then    nd the best hyperparameters by maximising this lower
bound, rather than the likelihood itself. to keep the notation reasonably simple, we drop the conditioning
on hyperparameters throughout and attempt to    nd a lower bound on the log marginal likelihood p(d).
one approach to obtaining a bound is based on the id181

kl(q(w)|p(w|d))     0

since p(w|d) = p(d|w)p(w)/p(d) we obtain

log p(d)     (cid:104)log p(d|w)p(w)(cid:105)q(w)     (cid:104)log q(w)(cid:105)q(w)

(18.2.27)

(18.2.28)

which holds for any distribution q(w). the angled brackets (cid:104)  (cid:105)q denote expectation with respect to q. using
the explicit form for the id28 model, the right hand side is given by2

(cid:16)

(cid:17)(cid:69)

n(cid:88)

(cid:68)

n=1

bkl    

log   

snwt  xn

q(w)     kl(q(w)|p(w))

to form a tractable lower bound, we need to choose a class of distributions q(w) for which the above expres-
sion can be evaluated, for example a gaussian q(w) = n (w m, s). for a gaussian prior p(w) = n (w 0,   )
then kl(q(w)|p(w)) is straightforward. for numerical convenience, we parameterise the covariance of the
approximation using a cholesky decomposition s = ctc for upper triangular c. this gives

(cid:16)

ctc     1(cid:17)

+ mt     1m + b

(18.2.30)

   2kl(q(w)|p(w)) = 4

log cii     log det (  ) + trace

where b = dim (  x). in computing the bound (18.2.29), the problematic remaining terms are

in    

log   

n (w m,s)

(cid:68)

snwt  xn

(cid:16)
(cid:0)an     n,     2

n

p(an) = n

so that

we de   ne the activation an     snwt  xn. since w is gaussian distributed, so is the activation,

    n = sn  x(cid:62)

n m,     2

n =   x(cid:62)

n c(cid:62)c  xn

in = (cid:104)log    (an)(cid:105)n (an     n,    2

n) = (cid:104)log    (    n + z    n)(cid:105)z

where (cid:104)  (cid:105)z denotes expectation with respect to the standard normal distribution z     n (z 0, 1).
in this
way in can be computed by any standard one-dimensional numerical integration method, such as gaussian
quadrature. since the bound (18.2.29) is therefore numerically accessible for any parameters m, c of the
approximating gaussian, we may proceed to    nd the optimal parameters by direct numerical maximisation
of the lower bound. the gradient of the bound with respect to m is

i

(cid:88)
(cid:17)(cid:69)
(cid:1) ,

n(cid:88)

n=1

   bkl
   m

=         1m +

sn  xn (1     (cid:104)   (    n + z    n)(cid:105)z)

similarly, one may show that

   bkl
   c

= c   t    c     1 + c

n(cid:88)

n=1

  xn  xt
n
    n (cid:104)z   (    n + z    n)(cid:105)z

(18.2.29)

(18.2.31)

(18.2.32)

(18.2.33)

(18.2.34)

with the understanding that only the upper triangular part of the matrix expression is to be taken. one
may show that the bound equation (18.2.29) is concave in the mean and covariance[61]. these gradients
may then be used as part of a general purpose optimisation routine to    nd the best m and c approximation
parameters.

2the generalisation to using   (x) in the below follows from simply replacing x with   (x) throughout.

draft november 9, 2017

393

classi   cation

figure 18.6: the logistic sigmoid   (x) = 1/(1 +
exp(   x)) (red) and a gaussian lower bound (blue
dashed curve) with operating point    = 2.5.

18.2.4 local variational approximation

an alternative to the kl bounding method above is to use a so-called local method that bounds each term
in the integrand. to lower bound

we may bound the logistic sigmoid function[156], see    g(18.6),

p(d) =

n

  

(cid:17)

p(w)dw

snwt  xn

(cid:90) (cid:89)
(cid:16)
(cid:18) 1
(x       )        (  )(cid:0)x2       2(cid:1)(cid:19)
(cid:17)

2

,

    log    (  n) +

1
2

snwt  xn    

snwt  xn

  (x)       (  )

hence

log   

(cid:16)

(cid:19)

(cid:18)

   (  )    

1
2

1
2  

(cid:20)(cid:16)

   (  )    

1
2

  n        (  n)

(cid:17)2

wt  xn

using this we can write

(cid:90)

p(d)    

dwn (w 0,   )

log   (  n)+ 1

2 snwt   xn    1

(cid:104)
2   n     (  n)

(wt   xn)2     2

n

(cid:89)

n

e

(cid:16)

(cid:17)

det

  s

det (  )

  mt   s   1   m +

+

1
2

n(cid:88)

n=1

(cid:20)

log p(d)    

1
2

log

log   (  n)    

  n
2

+    (  n)   2
n

where

a =      1 + 2

n(cid:88)

n=1

   (  n)   xn  xt

n , b =

1
2

sn  xn,   s     a   1,

  m     a   1b

n(cid:88)
(cid:16)

n=1

(cid:17)

(cid:21)

n

      2
(cid:105)

(cid:21)

(18.2.35)

(18.2.36)

(18.2.37)

(18.2.38)

(18.2.39)

(18.2.40)

the bound equation (18.2.39) may then be maximised with respect to the variational parameters   n. at
convergence, we may take the gaussian n

as an approximation to the posterior.

w   m,   s

relation to kl variational procedure

instead bound the problematic term(cid:10)log   (cid:0)snwt  xn

as an alternative to the numerical integration in the kl procedure described in section(18.2.3), we may
q(w) in equation (18.2.29) using (18.2.37). as we dis-
cuss in section(28.5), this enables us to relate the kl and the local bounding procedures [61], showing that
for unconstrained cholesky parameterisation c the kl approach results in a provably tighter lower bound
than the local approach.

(cid:1)(cid:11)

as an illustration of these approximations, for two-dimensional data, d = 2, we plot contours of the
posterior and approximations in    g(18.7) which again demonstrates the over-compact nature of the local
approximation. the laplace approximation, which does not yield a bound, is plotted for comparison.

394

draft november 9, 2017

for    xed   n, n = 1, . . . , n , the right hand side can be analytically integrated over w, resulting in the bound

   6   4   2024600.51(cid:89)

i

(cid:18)

(cid:19)

1
  i

classi   cation

(a)

(b)

(c)

(d)

figure 18.7: posterior approximations for a two-dimensional bayesian id28 posterior based on
n = 20 datapoints. (a) true posterior. (b) the gaussian from the jaakola-jordan local approximation. (c)
the laplace gaussian approximation. (d) the variational gaussian approximation from the kl approach.

18.2.5 relevance vector machine for classi   cation

it is straightforward to apply the sparse linear model methods of section(18.1.7) to classi   cation. for
example, in adopting the rvm prior to classi   cation, as before we encourage individual weights to be small
using

p(w|  ) =

p(wi|  i),

p(wi|  i) = n

wi 0,

(18.2.41)

the only alterations to the previous ml-ii approach is to replace equation (18.2.13) and equation (18.2.15)
with

h = diag (  ) + j

(18.2.42)

(cid:88)
(1       n)hn
i ,

n

[   e]i =   iwi    

these are used in the newton update formula as before. the update equation for the      s is given by

  new

i =

1

m2

i + sii

similarly, the gull-mackay update is given by

  new

i =

1       isii

m2
i

(18.2.43)

(18.2.44)

running this procedure, one typically    nds that many of the      s tend to in   nity and the corresponding
weights are pruned from the system. the remaining weights typically correspond to basis functions (in the
rbf case) in the centres of mass of clusters of datapoints of the same class, see    g(18.8). contrast this with
the situation in id166s, where the retained datapoints tend to be on the decision boundaries. the number
of training points retained by the rvm tends to be very small     smaller indeed than the number retained
in the id166 framework. whilst the rvm does not support large margins, and hence may be a less robust
classi   er, it does retain the advantages of a probabilistic framework[296]. a potential critique of the rvm,
coupled with an ml-ii procedure for learning the   i is that it is overly aggressive in terms of pruning.
indeed, as one may verify running demobayeslogregrvm.m it is common to    nd an instance of a problem
for which there exists a set of   i such that the training data can be classi   ed perfectly; however, after using
ml-ii, so many of the   i are set to zero that the training data can no longer be classi   ed perfectly. an
alternative is to apply the spike-and-slab prior technique which su   ers less from overly aggressive pruning,
although requires more sophisticated approximation methods.

18.2.6 multi-class case

we brie   y note that the multi-class case can be treated by using the softmax function under a one-of-m
class coding scheme. the class probabilities are

p(c = m|y) =

eym

  m(cid:48)eym(cid:48)

draft november 9, 2017

(18.2.45)

395

p(w|d)   1012   3   2.5   2   1.5   1   0.50jj   1012   3   2.5   2   1.5   1   0.50laplace   1012   3   2.5   2   1.5   1   0.50vg   1012   3   2.5   2   1.5   1   0.50classi   cation

(a)

(b)

figure 18.8: classi   cation using the rvm with rbf e     (x   m)2
, placing a basis function on a subset of the
training data points. the green points are training data from class 1, and the red points are training data
(b): the
from class 0. the contours represent the id203 of being in class 1.
training points weighted by their relevance value 1/  n. nearly all the points have a value so small that they
e   ectively vanish. see demobayeslogregrvm.m

which automatically enforces the constraint(cid:80)
the cost of the laplace approximation scales as o(cid:0)c3n 3(cid:1). however, one may show by careful implementa-
tion that the cost may be reduced to only o(cid:0)cn 3(cid:1), analogous to the cost savings possible in the gaussian

m p(c = m) = 1. naively it would appear that for c classes,

(a): training points.

process classi   cation model [319, 248].

18.3 summary

    a simple extension of id75 is achieved by using a gaussian prior on the parameters. coupled
with the assumption of additive gaussian noise on the output, the posterior distribution is gaussian, for
which predictions can be computed readily.

    in the case of classi   cation, no closed form bayesian solution is obtained by using simple gaussian priors
on the parameters, and approximations are required. the parameter posterior is nevertheless well-behaved
so that simple unimodal approximations may be adequate.

18.4 code

demobayeslinreg.m: demo of bayesian id75
bayeslinreg.m: bayesian id75
demobayeslogregrvm.m: demo of bayesian id28 (rvm)
bayeslogregressionrvm.m: bayesian id28 (rvm)
avsigmagauss.m: approximation of the gaussian average of a logistic sigmoid
logsigapp.m: approximation of the logistic sigmoid using mixture of erfs

396

draft november 9, 2017

   8   6   4   202468   8   6   4   2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.70.80.80.90.9   8   6   4   202468   8   6   4   2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.70.80.80.90.9exercises

18.5 exercises

exercise 18.1. this exercise concerns bayesian regression.

(cid:0)  0,   2(cid:1). what is p(f|t, x)?

1. show that for f = wtx and p(w)     n (w 0,   ), that p(f|x) is gaussian distributed. furthermore,

   nd the mean and covariance of this gaussian.

2. consider a target point t = f +  , where       n

exercise 18.2. a bayesian linear parameter regression model is given by

yn = wt  (xn) +   n

in vector notation y =(cid:0)y1, . . . , yn(cid:1)t this can be written
with   t =(cid:2)  (x1), . . . ,   (xn )(cid:3) and    is a zero mean gaussian distributed vector with covariance      1i. an

y =   w +   

(18.5.1)

(18.5.2)

expression for the marginal likelihood of a dataset is given in equation (18.1.19). we aim to    nd a more
compact expression for the likelihood given the hyperparameters   

p(y1, . . . , yn|x1, . . . , xn ,   )

since yn is linearly related to w and p(w) = n

(cid:0)w 0,      1i(cid:1), then y is gaussian distributed with mean

(18.5.3)

(cid:104)y(cid:105) =   (cid:104)w(cid:105) = 0
and covariance matrix

(cid:68)

yyt(cid:69)

c =

    (cid:104)y(cid:105)(cid:104)y(cid:105)t =

(cid:68)
(  w +   ) (  w +   )t(cid:69)

1. show that the covariance matrix can be expressed as

c =

1
  

i +

1
  

    t

2. hence show that the log marginal likelihood can we written as

log p(y1, . . . , yn|x1, . . . , xn ,   ) =    

1
2

log det (2  c)    

ytc   1y

1
2

(18.5.4)

(18.5.5)

(18.5.6)

(18.5.7)

exercise 18.3. using exercise(18.2) as a basis, derive expression (18.1.41) for the log likelihood on a
validation set.

exercise 18.4. consider the function e(w) as de   ned in equation (18.2.12).

1. show that

the hessian matrix which has elements,

hij    

   2

   wi   wj

e(w) =

  i +

  n (1       n)   n (  n)t

(cid:34)

n(cid:88)

n=1

2. show that the hessian is positive de   nite.

(cid:90)

exercise 18.5. show that for any function f (  ),

f (xtw)p(w)dw =

f (h)p(h)dh

(cid:90)

(cid:35)

ij

(18.5.8)

(18.5.9)

where p(h) is the distribution of the scalar xtw. the signi   cance is that any high-dimensional integral of
the above form can be reduced to a one-dimensional integral over the distribution of the       eld    h[23].

draft november 9, 2017

397

exercise 18.6. this exercise concerns bayesian id28. our interest is to derive the op-
timal regularisation parameter    based on the laplace approximation in section(18.2.2) to the marginal
log-likelihood given by

log p(d|  )     l(  )        

(w)tw +

  
2

log   

(cid:88)

n

(cid:16)

(w)t hn(cid:17)

1
2

   

log det (  i + j) +

b
2

log   

n log   (cid:0)wthn(cid:1), as in equation
(cid:80)

(18.5.10)

the laplace procedure    nds    rst an optimal w    that minimises   wtw/2   

(18.2.12) which will depend on the setting of   . formally, therefore, in    nding the    that optimises l(  )
we should make use of the total derivative formula

dl
d  

=

   l
     

+

   l
   wi

   wi
     

(18.5.11)

however, when evaluated at w = w   ,    l
   w = 0. this means that in order to compute the derivative with
respect to   , we only need consider the terms with an explicit    dependence. equating the derivative to zero
and using

(cid:88)

i

    log det (m) = trace(cid:0)m   1   m(cid:1)

show that the optimal    satis   es the    xed point equation

  new =

n
(w   )tw    + trace

(  i + j)

(cid:16)

   1(cid:17)

exercises

(18.5.12)

(18.5.13)

398

draft november 9, 2017

chapter 19

gaussian processes

in bayesian linear parameter models, we saw that the only relevant quantities are related to the scalar
product of data vectors. in gaussian processes we use this to motivate a prediction method that does not
necessarily correspond to any    parametric    model of the data. such models are    exible bayesian predictors.

19.1 non-parametric prediction

gaussian processes are    exible bayesian models that    t well within the probabilistic modelling framework.
in developing gps it is useful to    rst step back and see what information we need to form a predictor. given
a set of training data

d = {(xn, yn), n = 1, . . . , n} = x     y

(19.1.1)

where xn is the input for datapoint n and yn the corresponding output (a continuous variable in the regression
case and a discrete variable in the classi   cation case), our aim is to make a prediction y    for a new input x   .
in the discriminative framework no model of the inputs x is assumed and only the outputs are modelled,
conditioned on the inputs. given a joint model
   

   

p(y1, . . . , yn , y

   
|x1, . . . , xn , x

) = p(y, y

   
|x , x

)

we may subsequently use conditioning to form a predictor p(y   
much use of the i.i.d. assumption that each datapoint is independently sampled from the same generating
distribution. in this context, this might appear to suggest the assumption

|x   ,d). in previous chapters we   ve made

(19.1.2)

p(y1, . . . , yn , y

   

   
|x1, . . . , xn , x

) = p(y

   

   
|x , x

)

   
p(yn|x , x

)

(19.1.3)

(cid:89)

n

however, this is clearly of little use since the predictive conditional is simply p(y   
meaning the predictions make no use of the training outputs. for a non-trivial predictor we therefore need
to specify a joint non-factorised distribution over outputs.

|d, x   ) = p(y   

|x , x   )

19.1.1 from parametric to non-parametric

if we revisit our i.i.d. assumptions for parametric models, we used a parameter    to make a model of the
input-output distribution p(y|x,   ). for a parametric model predictions are formed using

p(y

   

   

|x

,d)     p(y

   

   
, x

,d) =

p(y

  

   

p(y

   

,y|  , x

,x )p(  |   
,x )

x

   

(19.1.4)

(cid:90)

  

(cid:90)

   

   
,y, x

,x ,   )    

399

  

yn

xn

(a)

y1

x1

y   

x   

y   

x   

y1

x1

yn

xn

(b)

non-parametric prediction

(a): a parametric model for predic-
figure 19.1:
(b): the form of the
tion assuming i.i.d. data.
model after integrating out the parameters   . our
non-parametric model will have this structure.

under the assumption that, given   , the data is i.i.d., we obtain

(cid:90)

,   )p(  )

p(yn|  , xn)    

   

   
|x

,   )p(  |d)

p(y

  

p(y

   

   

|x

,d)    

where

p(  |d)     p(  )

  

   

   

p(y

|x

(cid:90)
(cid:89)
p(yn|  , xn)
(cid:90)

n

   

   

(cid:89)

n

(cid:89)

n

after integrating over the parameters   , the joint data distribution is given by

   

p(y

   

,y|x

,x ) =

p(y

  

|x

,   )p(  )

p(yn|  , xn)

which does not in general factorise into individual datapoint terms, see    g(19.1). the idea of a non-

parametric approach is to specify the form of the dependencies p(y   ,y|x   ,x ) without reference to an

explicit parametric model. one route towards a non-parametric model is to start with a parametric model
and integrate out the parameters.
in order to make this tractable, we use a simple linear parameter
predictor with a gaussian parameter prior. for regression this leads to closed form expressions, although
the classi   cation case will require numerical approximation.

19.1.2 from bayesian linear models to gaussian processes

to develop the gp, we brie   y revisit the bayesian linear parameter model of section(18.1.1). for parameters
w and basis functions   i(x) the output is given by (assuming zero output noise)

(cid:88)

y =

wi  i(x)

i

if we stack all the y1, . . . , yn into a vector y, then we can write the predictors as

y =   w

where    =(cid:2)  (x1), . . . ,   (xn )(cid:3)t

is the design matrix. assuming a gaussian weight prior

p(w) = n (w 0,   w)

the joint output

(cid:90)

p(y|x) =

   (y       w) p(w)

w

is gaussian distributed with mean

and covariance

(cid:104)y(cid:105) =   (cid:104)w(cid:105)p(w) = 0

yyt(cid:69)
(cid:68)

(cid:68)

wwt(cid:69)

=   

  t =     w  t =

p(w)

(cid:16)

    w

1
2

(cid:17)(cid:16)

    w

(cid:17)t

1
2

.

400

draft november 9, 2017

(19.1.5)

(19.1.6)

(19.1.7)

(19.1.8)

(19.1.9)

(19.1.10)

(19.1.11)

(19.1.12)

(19.1.13)

non-parametric prediction

from this we see that the   w can be absorbed into    using its cholesky decomposition. in other words,
without loss of generality we may assume   w = i. hence, after integrating out the weights, the bayesian
id75 model induces a gaussian distribution on any set of outputs y as

p(y|x) = n (y 0, k)

where the covariance matrix k depends on the training inputs alone via

[k]n,n(cid:48) =   (xn)t  (xn(cid:48)

),

(cid:48)
n, n

= 1, . . . , n

(19.1.14)

(19.1.15)

since the matrix k is formed as the scalar product of vectors, it is by construction positive semide   nite,
see section(19.4.2). after integrating out the weights, the only thing the model directly depends on is the
covariance matrix k. in a gaussian process we directly specify the joint output covariance k as a function
of two inputs, rather than specifying a linear model with parameters w. speci   cally we need to de   ne the
n, n(cid:48) element of the covariance matrix for any two inputs xn and xn(cid:48)
. this is achieved using a covariance
function k(xn, xn(cid:48)

)

[k]n,n(cid:48) = k(xn, xn(cid:48)

)

(19.1.16)

the matrix k formed from a covariance function k is called the gram matrix. the required form of the
function k(xn, xn(cid:48)
) is very special     when applied to create the elements of the matrix k it must produce a
positive de   nite matrix. we discuss how to create such covariance functions in section(19.3). one explicit
straightforward construction is to form the covariance function from the scalar product of the basis vector
  (xn) and   (xn(cid:48)
). for    nite-dimensional    this is known as a    nite dimensional gaussian process. given
any covariance function we can always    nd a corresponding basis vector representation     that is, for any
gp, we can always relate this back to a parametric bayesian lpm. however, for many commonly used
covariance functions, the basis functions corresponds to in   nite dimensional vectors. it is in such cases that
the advantages of using the gp framework are particularly evident since we would not be able to compute
e   ciently with the corresponding in   nite dimensional parametric model.

19.1.3 a prior on functions

the nature of many machine learning applications is such that the knowledge about the true underlying
mechanism behind the data generation process is limited. instead one relies on generic    smoothness    assump-
tions; for example we might wish that for two inputs x and x(cid:48) that are close, the corresponding outputs
y and y(cid:48) should be similar. many generic techniques in machine learning can be viewed as di   erent char-
acterisations of smoothness. an advantage of the gp framework in this respect is that the mathematical
smoothness properties of the functions are well understood, giving con   dence in the procedure.

a vector y =(cid:0)y1, . . . , yn(cid:1) from the gaussian de   ned by equation (19.1.14). we can then plot the sampled

for a given covariance matrix k, equation (19.1.14) speci   es a distribution on functions1 in the following
sense: we specify a set of input points x = (x1, . . . , xn ) and a n    n covariance matrix k. then we draw
   function    at the    nite set of points (xn, yn), n = 1, . . . , n . what kind of function does a gp correspond
to?
in    g(19.2a) we show three sample functions drawn from a squared exponential covariance function
de   ned over 500 points uniformly spaced from    2 to 3. each sampled function looks reasonably smooth.
conversely, for the ornstein uhlenbeck covariance function, the sampled functions    g(19.2c) look locally
rough. these smoothness properties are related to the form of the covariance function, as discussed in
section(19.4.1).

consider two scalar inputs, xi and xj and corresponding sampled outputs yi and yj. for a covariance func-
tion with large k(xi, xj), we expect yi and yj to be very similar since they are highly correlated. conversely,
for a covariance function that has low k(xi, xj), we expect yi and yj to be e   ectively independent. in general,
we would expect the correlation between yi and yj to decrease the further apart xi and xj are2.

1the term    function    is potentially confusing since we do not have an explicit functional form for the input output-mapping.

for any    nite set of inputs x1, . . . , xn the values for the    function    are given by the outputs at those points y1, . . . , yn .

2for periodic functions one can have high correlation even if the inputs are far apart.

draft november 9, 2017

401

gaussian process prediction

the zero mean assumption implies that if we were to draw a large number of such    functions   , the mean
across these functions at a given point x tends to zero. similarly, for any two points x and x(cid:48) if we compute
the sample covariance between the corresponding y and y(cid:48) for all such sampled functions, this will tend
to the covariance function value k(x, x(cid:48)). the zero-mean assumption can be easily relaxed by de   ning a
mean function m(x) to give p(y|x) = n (y m, k). in many practical situations one typically deals with
   detrended    data in which such mean trends have been already removed. for this reason much of the
development of gps in the machine learning literature is for the zero mean case.

19.2 gaussian process prediction
for a dataset d = {x, y} and novel input x   , a zero mean gp makes a gaussian model of the joint outputs
y1, . . . , yn , y    given the joint inputs x1, . . . , xn , x   . for convenience we write this as

(cid:0)y, y

    0n +1, k+(cid:1)

p(y, y

   

   
|x, x

) = n

where 0n +1 is an n + 1 dimensional zero-vector. the covariance matrix k+ is a block matrix with elements

kx,x

kx,x   

k+    

kx   ,x

where kx,x is the covariance matrix of the training inputs x =(cid:0)x1, . . . , xn(cid:1)     that is

kx   ,x   

[kx,x]n,n(cid:48)     k(xn, xn(cid:48)

),

(cid:48)
n, n

= 1, . . . , n

the n    1 vector kx,x    has elements

   
[kx,x   ]n,        k(xn, x

)

n = 1, . . . , n

kx   ,x is the transpose of the above vector. the scalar covariance is given by

kx   ,x        k(x

   

   
, x

)

the predictive distribution p(y   
gaussian distribution

(cid:0)y

|x   , x, y) is obtained by gaussian conditioning using result(8.4), giving a
x,xy, kx   ,x        kx   ,xk   1

x,xkx,x   (cid:1)

(19.2.5)

   

p(y

   

|x

,d) = n

    kx   ,xk   1

gp regression is an exact method and there are no issues with local minima. furthermore, gps are attractive
since they automatically model uncertainty in the predictions. however, the computational complexity for

making a prediction is o(cid:0)n 3(cid:1) due to the requirement of performing the matrix inversion (or solving the

corresponding linear system by gaussian elimination). this can be prohibitively expensive for large datasets
and a large body of research on e   cient approximations exists. a discussion of these techniques is beyond
the scope of this book, and the reader is referred to [248].

19.2.1 regression with noisy training outputs

to prevent over   tting to noisy data it is useful to assume that a training output yn is the result of some
clean gaussian process f n corrupted by independent additive gaussian noise,

yn = f n +  n, where

 n     n

402

(cid:0) n 0,   2(cid:1)

(19.2.6)

draft november 9, 2017

(19.2.1)

(19.2.2)

(19.2.3)

(19.2.4)

gaussian process prediction

in this case our interest is to predict the clean signal f    for a novel input x   . then the distribution
p(y, f   

(cid:18) kx,x +   2i kx,x   
|x, x   ) is a zero mean gaussian with block covariance matrix

(cid:19)

(19.2.7)

kx   ,x

kx   ,x   

so that kx,x is replaced by kx,x +   2i in forming the prediction, equation (19.2.5). this follows since

(cid:104)yn(cid:105) = (cid:104)f n(cid:105) + (cid:104) n(cid:105) = 0 + 0

(19.2.8)

and, using the assumed independence of the noise with the clean signal, f m         n and the independence of
two noise components,  m        n, m (cid:54)= n, we have

(cid:104)ymyn(cid:105) = (cid:104)(f m +  m) (f n +  n)(cid:105)

(cid:124) (cid:123)(cid:122) (cid:125)
= (cid:104)f mf n(cid:105)

k(xm,xn)

(cid:124) (cid:123)(cid:122) (cid:125)
+ (cid:104)f m n(cid:105)
(cid:104)f m(cid:105)(cid:104) n(cid:105)

+(cid:104)f n m(cid:105)(cid:104)f n(cid:105)(cid:104) m(cid:105) + (cid:104) m n(cid:105)  2  m,n .

(19.2.9)

using (cid:104) m(cid:105) = 0 we obtain the form in equation (19.2.7).

example 19.1. training data from a one-dimensional input x and one dimensional output y are plotted
in    g(19.2b,d), along with the mean regression function    t, based on two di   erent covariance functions.
note how the smoothness of the prior translates into smoothness of the prediction. the smoothness of
the function space prior is a consequence of the choice of covariance function. naively, we can partially
understand this by the behaviour of the covariance function at the origin, section(19.4.1). an intuitive
way to think about gp regression is to sample an    in   nite    number of functions from the prior; these are
then       ltered    by how well they    t the data by the likelihood term, leaving a posterior distribution on the
functions. see demogpreg.m

the marginal likelihood and hyperparameter learning

matrix k de   ned on the inputs x =(cid:0)x1, . . . , xn(cid:1), the log marginal likelihood is

for a set of n one-dimensional training inputs represented by the n  1 dimensional vector y and a covariance

one can learn any free (hyper)parameters of the covariance function by maximising the marginal likelihood.
for example, a squared exponential covariance function may have parameters   , v0:

log p(y|x) =    

1
2

log det (2  k)

1
2

ytk   1y    
(cid:26)
  (cid:0)x     x

   

1
2

(cid:27)

(cid:48)(cid:1)2

(cid:48)
k(x, x

) = v0 exp

the    parameter in equation (19.2.11) speci   es the appropriate length-scale of the inputs, and v0 the
variance of the function. the dependence of the marginal likelihood (19.2.10) on the parameters is typically
complex and no closed form expression for the maximum likelihood optimum exists; in this case one resorts
to numerical optimisation techniques such as conjugate gradients.

vector inputs

for regression with vector inputs and scalar outputs we need to de   ne a covariance as a function of the two
vectors, k(x, x(cid:48)). using the multiplicative property of covariance functions, de   nition(19.3), a simple way
to do this is to de   ne

(19.2.10)

(19.2.11)

(19.2.12)

403

(cid:89)

k(x, x(cid:48)

) =

(cid:48)
i)
k(xi, x

i

draft november 9, 2017

covariance functions

(a)

(c)

(b)

(d)

figure 19.2: the input space from -2 to 3 is split evenly into 1000 points x1, . . . , x1000.
(a):
three samples from a gp prior with squared exponential (se) covariance function,    = 2. the
1000    1000 covariance matrix k is de   ned using the se kernel, from which the samples are drawn us-
(b): prediction based on training points. plotted is the posterior
ing mvrandn(zeros(1000,1),k,3).
predicted function based on the se covariance. the central line is the mean prediction, with standard errors
bars on either side. the log marginal likelihood is     70. (c): three samples from the ornstein-uhlenbeck
(d): posterior prediction for the ou covariance. the log marginal likelihood is
gp prior with    = 2.
    3, meaning that the se covariance is much more heavily supported by the data than the rougher ou
covariance.

for example, for the squared exponential covariance function this gives

k(x, x(cid:48)

   (x   x(cid:48))2

) = e

(19.2.13)

though    correlated    forms are possible as well, see exercise(19.6). we can generalise the above using param-
eters:

k(x, x(cid:48)

) = v0 exp

(cid:40)

d(cid:88)

l=1

(cid:1)2

(cid:0)xl     x

(cid:48)
l

  l

1
2

   

(cid:41)

(19.2.14)

where xl is the lth component of x and    = (v0,   1, . . . ,   d) are the parameters. the   l in equation (19.2.14)
allow a di   erent length scale on each input dimension and can be learned by numerically maximising the
marginal likelihood. for irrelevant inputs, the corresponding   l will become small, and the model will ignore
the lth input dimension.

19.3 covariance functions
covariance functions k(x, x(cid:48)) are special in that they de   ne elements of a positive de   nite matrix. these
functions are also referred to as    kernels   , particulary in the machine learning literature.

404

draft november 9, 2017

   2   1.5   1   0.500.511.522.53   3   2.5   2   1.5   1   0.500.511.52   2   1.5   1   0.500.511.522.53   1.5   1   0.500.511.52   2   1.5   1   0.500.511.522.53   2.5   2   1.5   1   0.500.511.522.5   2   1.5   1   0.500.511.522.53   1.5   1   0.500.511.5covariance functions

de   nition 19.1 (covariance function). given any collection of points x1, . . . , xm , a covariance function
k(xi, xj) de   nes the elements of an m    m matrix

[c]i,j = k(xi, xj)

such that c is positive semide   nite.

19.3.1 making new covariance functions from old

the following rules (see exercise(19.1)) generate new covariance functions from existing covariance functions
k1, k2 [197],[248].

de   nition 19.2 (sum).
) = k1(x, x(cid:48)

k(x, x(cid:48)

) + k2(x, x(cid:48)

)

(cid:18) x

y

(cid:19)

,

de   nition 19.3 (product).

k(x, x(cid:48)

) = k1(x, x(cid:48)

)k2(x, x(cid:48)

)

de   nition 19.4 (product spaces). for z =

k(z, z(cid:48)

) = k1(x, x(cid:48)

) + k2(y, y(cid:48)

)

and

k(z, z(cid:48)

) = k1(x, x(cid:48)

)k2(y, y(cid:48)

)

de   nition 19.5 (vertical rescaling).

k(x, x(cid:48)

) = a(x)k1(x, x(cid:48)

)a(x(cid:48)

)

for any function a(x).

de   nition 19.6 (warping and embedding).

(cid:0)u(x), u(x(cid:48)

)(cid:1)

k(x, x(cid:48)

) = k1

(19.3.1)

(19.3.2)

(19.3.3)

(19.3.4)

(19.3.5)

(19.3.6)

for any mapping x     u(x), where the mapping u(x) has arbitrary dimension.

a small collection of covariance functions commonly used in machine learning is given below. we refer the
reader to [248] and [118] for further popular covariance functions.

19.3.2 stationary covariance functions

de   nition 19.7 (stationary kernel). a kernel k(x, x(cid:48)) is stationary if the kernel depends only on the
separation x     x(cid:48). that is
) = k(x     x(cid:48)

k(x, x(cid:48)

(19.3.7)

)

draft november 9, 2017

405

(cid:16)

   |d|2(cid:17)
xn     xn(cid:48)(cid:17)t(cid:16)

(cid:16)

(cid:17)

   (cid:88)

i=1

(cid:18)

(cid:16)

for a stationary covariance function we may write k(d), where d = x     x(cid:48). this means that for functions

drawn from the gp, on average, the functions depend only on the distance between inputs and not on
the absolute position of an input. in other words, the functions are on average translation invariant. for
isotropic covariance functions, the covariance is de   ned as a function of the distance |d|. such covariance
functions are, by construction, rotationally invariant.

covariance functions

de   nition 19.8 (squared exponential).

k(d) = exp

(19.3.8)

the squared exponential is one of the most common covariance functions. there are many ways to show
that this is a covariance function. an elementary technique is to consider

xn     xn(cid:48)(cid:17)(cid:19)

(cid:18)

(cid:19)

(cid:18)

(cid:12)(cid:12)(cid:12)xn(cid:48)(cid:12)(cid:12)(cid:12)2(cid:19)

(cid:16)

(xn)t xn(cid:48)(cid:17)

exp

= exp

1

2 |xn|2

   

exp

   

1
2

exp

   

1
2

(19.3.9)

the    rst two factors on the right above form a kernel of the form   (xn)  (xn(cid:48)
(xn)t xn(cid:48)
nential, we have

) =
is the linear kernel. taking the exponential and writing the power series expansion of the expo-

). in the    nal term k1(xn, xn(cid:48)

exp

k1(xn, xn(cid:48)

)

=

1(xn, xn(cid:48)
ki

)

1
i!

(19.3.10)

this can be expressed as a series of integer powers of k1, with positive coe   cients. by the product (with
itself) and sum rules above, this is therefore a kernel as well. we then use the fact that equation (19.3.9) is
the product of two kernels, and hence also a kernel.

de   nition 19.9 (  -exponential).

k(d) = exp (   |d|  ),

0 <        2

(19.3.11)

when    = 2 we have the squared exponential covariance function. when    = 1 this is the ornstein-
uhlenbeck covariance function.

de   nition 19.10 (mat  ern).

k(d) = |d|   k   (|d|)

where k   is a modi   ed bessel function,    > 0.

de   nition 19.11 (rational quadratic).

k(d) =

(cid:16)

,

   > 0

1 + |d|2(cid:17)     
    (u(x)     u(x(cid:48)))2(cid:17)
(cid:16)
) = exp(cid:0)
      sin2(cid:0)  (x     x
)(cid:1)(cid:1),

(cid:48)

(cid:48)

k(x     x
see    g(19.4a).

de   nition 19.12 (periodic). for 1-dimensional x and x(cid:48), a stationary (and isotropic) covariance function
can be obtained by    rst mapping x to the two dimensional vector u(x) = (cos(x), sin(x)) and then using
the se covariance exp

[197]

   > 0

(19.3.14)

406

draft november 9, 2017

(19.3.12)

(19.3.13)

analysis of covariance functions

(a)

(b)

figure 19.3: (a): plots of the gamma-exponential covariance e   |x|  
versus x. the case    = 2 corresponds
to the se covariance function. the drop in the covariance is much more rapid as a function of the separation
x for small   , suggesting that the functions corresponding to smaller    will be locally rough (though possess
(b): as for (a) but zoomed in towards the origin. for the se
relatively higher long range correlation).
case,    = 2, the derivative of the covariance function is zero, whereas the ou covariance    = 1 has a    rst
order contribution to the drop in the covariance, suggesting that locally ou sampled functions will be much
rougher than se functions.

19.3.3 non-stationary covariance functions

de   nition 19.13 (linear).

k(x, x(cid:48)

) = xtx(cid:48)

(cid:32)

de   nition 19.14 (neural network).
2xtx(cid:48)

k(x, x(cid:48)

) = arcsin

(cid:112)

   1 + 2xtx

1 + 2x(cid:48)tx(cid:48)

(cid:33)

(19.3.15)

(19.3.16)

the functions de   ned by this covariance always go through the origin. to shift this, one may use the
embedding x     (1, x) where the 1 has the e   ect of a    bias    from the origin. to change the scale of the bias
and non-bias contributions one may use additional parameters x     (b,   x). the nn covariance function
can be derived as a limiting case of a neural network with in   nite hidden units[318], and making use of
exact integral results in [22]. see    g(19.4b).

de   nition 19.15 (gibbs).

(cid:18) ri(x)ri(x(cid:48))

(cid:89)

r2
i (x) + r2

i (x(cid:48))

i

(cid:19) 1

2

(cid:32)

exp

   

(xi     x(cid:48)
i)2
i (x(cid:48))
r2
i (x) + r2

(cid:33)

(19.3.17)

k(x, x(cid:48)

) =

for functions ri(x) > 0 [123].

19.4 analysis of covariance functions

19.4.1 smoothness of the functions

we examine local smoothness for a translation invariant kernel k(x, x(cid:48)) = k(x   x(cid:48)). for two one-dimensional
points x and x(cid:48), separated by a small amount    (cid:28) 1, x(cid:48) = x +   , the covariance between the outputs y and

draft november 9, 2017

407

00.511.522.5300.20.40.60.81  21.51.00.500.020.040.060.080.10.120.140.160.180.20.650.70.750.80.850.90.951  21.51.00.5analysis of covariance functions

(a)

(b)

figure 19.4: samples from a gp prior for 500 x points uniformly placed from -20 to 20.

from the periodic covariance function exp(cid:0)

covariance function with bias b = 5 and    = 1.

   2 sin2 0.5(x     x(cid:48))(cid:1).

(a): samples
(b): samples from the neural network

y(cid:48) is, by taylor expansion,

(cid:48)
k(x, x

)     k(0) +   

dx|x=0 + o(cid:0)  2(cid:1)

dk

(19.4.1)

so that the change in the covariance at the local level is dominated by the    rst derivative of the covariance
function. for the se covariance k(x) = e   x2

,

dk
dx

=    2xe

   x2

(19.4.2)

is zero at x = 0. this means that for the se covariance function, the    rst order change in the covariance is
zero, and only higher order   2 terms contribute.
for the ornstein-uhlenbeck covariance, k(x) = e   |x|, the right derivative at the origin is

lim
     0

k(  )     k(0)

  

= lim
     0

e          1

  

=    1

(19.4.3)

where this result is obtained using l   h  opital   s rule. hence for the ou covariance function, there is a    rst
order negative change in the covariance; at the local level, this decrease in the covariance is therefore much
more rapid than for the se covariance, see    g(19.3). since low covariance implies low dependence (in
gaussian distributions), locally the functions generated from the ou process are rough, whereas they are
smooth in the se case. a more formal treatment for the stationary case can be obtained by examining the
eigenvalue-frequency plot of the covariance function (spectral density), section(19.4.3). for rough functions
the density of eigenvalues for high frequency components is higher than for smooth functions.

19.4.2 mercer kernels

consider the function

(cid:48)
k(x, x

(cid:48)
) =   (x)t  (x

) =

b(cid:88)

s=1

  s(x)  s(x

(cid:48)

)

(19.4.4)

where   (x) is a vector with component functions   1(x),   2(x), . . . ,   b(x). then for a set of points x1, . . . , xp ,
we construct the matrix k with elements

[k]ij = k(xi, xj) =

  s(xi)  s(xj)

(19.4.5)

s=1

we claim that the matrix k so constructed is positive semide   nite and hence a valid covariance matrix.
recalling that a matrix is positive semide   nite if for any non zero vector z, ztkz     0. using the de   nition
draft november 9, 2017
408

b(cid:88)

   20   15   10   505101520   2   1.5   1   0.500.511.5   20   15   10   505101520   3   2   101234analysis of covariance functions

of k above we have

p(cid:88)

i,j=1

ztkz =

zikijzj =

zi  s(xi)

b(cid:88)

s=1

(cid:34) p(cid:88)
(cid:124)

i=1

(cid:123)(cid:122)

  s

(cid:35)
(cid:125)

       p(cid:88)
(cid:124)

j=1

  s(xj)zj

(cid:123)(cid:122)

  s

      
(cid:125)

b(cid:88)

s=1

=

  2
s     0

(19.4.6)

hence any function of the form equation (19.4.4) is a covariance function. we can generalise the mercer
kernel to complex functions   (x) using

(cid:48)
k(x, x

) =   (x)t     

(cid:48)

(19.4.7)
where     represents the complex conjugate. then the matrix k formed from inputs xi, i = 1, . . . , p is positive
semide   nite since for any real vector z,

(x

)

ztkz =

zi  s(xi)

b(cid:88)

s=1

(cid:34) p(cid:88)
(cid:124)

i=1

(cid:123)(cid:122)

  s

(cid:35)
(cid:125)

       p(cid:88)
(cid:124)

j=1

      
(cid:125)

b(cid:88)

s=1

   
s(xj)zj

  

(cid:123)(cid:122)

   
  
s

=

|  s|2     0

(19.4.8)

(cid:90)

(cid:90)

(cid:90)

where we made use of the general result for a complex variable xx    = |x|2. a further generalisation is to

write

(cid:48)
k(x, x

) =

f (s)  (x, s)  

   

(cid:48)

(x

, s)ds

(19.4.9)

for real f (s)     0, and scalar complex functions   (x, s). then replacing summations with integration (and
assuming we can interchange the sum over the components of z with the integral over s), we obtain

ztkz =

f (s)

zi  (xi, s)

(xj, s)zj

ds =

f (s)|  (s)|2 ds     0

(19.4.10)

(cid:34) p(cid:88)
(cid:124)

i=1

(cid:123)(cid:122)

  (s)

(cid:35)
(cid:125)

       p(cid:88)
(cid:124)

j=1

   

  

(cid:123)(cid:122)

     (s)

      
(cid:125)

(cid:90)

19.4.3 fourier analysis for stationary kernels

for a function g(x) with fourier transform   g(s), we may use the inverse fourier transform to write

g(x) =

1
2  

   ixsds

  g(s)e

(19.4.11)

(19.4.12)

where i           1. for a stationary kernel k(x) with fourier transform   k(s), we can therefore write

(cid:90)

(cid:90)

k(x     x

(cid:48)

) =

1
2  

  k(s)e

   i(x   x(cid:48))sds =

1
2  

  k(s)e

   ixseix(cid:48)sds

which is of the same form as equation (19.4.9) where the fourier transform   k(s) is identi   ed with f (s)
and   (x, s) = e   isx. hence, provided the fourier transform   k(s) is positive, the translation invariant kernel
k(x   x(cid:48)) is a covariance function. bochner   s theorem[248] asserts the converse that any translation invariant

covariance function must have such a fourier representation.

application to the squared exponential kernel

for the translation invariant squared exponential kernel, k(x) = e

  k(s) =

    1
e

2 x2+isxdx = e

    s2

2

    1

2 (x+is)2

e

dx =    2  e

    s2

2

    1

2 x2

, its fourier transform is

(19.4.13)

(cid:90)    

      

(cid:90)    

      

hence the fourier transform of the se kernel is a gaussian. since this is positive the se kernel is a covariance
function.

draft november 9, 2017

409

gaussian processes for classi   cation

c1

cn

y1

x1

yn

xn

c   

y   

x   

figure 19.5: gp classi   cation. the gp induces a gaussian distribution on
the latent activations y1, . . . , yn , y   , given the observed values of c1, . . . , cn .
the classi   cation of the new input x    is then given via the correlation
induced by the training points on the latent activation y   .

19.5 gaussian processes for classi   cation

(cid:90)

adapting the gp framework to classi   cation requires replacing the gaussian regression term p(y|x) with a
corresponding classi   cation term p(c|x) for a discrete label c. to do so we will use the gp to de   ne a latent
continuous space y which will then be mapped to a class id203 using

p(c|x) =

given training data inputs x = (cid:8)x1, . . . , xn(cid:9), corresponding class labels c = (cid:8)c1, . . . , cn(cid:9), and a novel

p(c|y, x)p(y|x)dy =

p(c|y)p(y|x)dy

(19.5.1)

(cid:90)

|x

,c,x ) =

   
p(c

   

|y

   

)p(y

|x ,c)dy

   

(19.5.2)

input x   , then
   

   
p(c

(cid:90)

where

   

p(y

|x ,c)     p(y

,c|x )
   
p(y

   

(cid:90)
(cid:90)
(cid:90) (cid:40) n(cid:89)
(cid:124)

n=1

   
,y,c|x , x

)dy
   
,y|x , x

   

(cid:41)
(cid:125)
(cid:123)(cid:122)
p(cn|yn)

(cid:124)

p(c|y)p(y

class mapping

=

=

=

)dy

p(y1, . . . , yn , y

dy1, . . . , dyn

(19.5.3)

   

(cid:123)(cid:122)
   
|x1, . . . , xn , x

(cid:125)

)

gaussian process

the graphical structure of the joint class and latent y distribution is depicted in    g(19.5.) the posterior
marginal p(y   
|x ,c) is the marginal of a gaussian process, multiplied by a set of non-gaussian maps from the
latent activations to the class probabilities. we can reformulate the prediction problem more conveniently
as follows:
   

   

   

   

p(y

,y|x

,x ,c)     p(y

   
,y,c|x

,x )     p(y

   
|y, x

,x )p(y|c,x )

(19.5.4)

where

(cid:40) n(cid:89)

(cid:41)

n=1

p(cn|yn)

p(y|c,x )    

p(y1, . . . , yn|x1, . . . , xn )
|y, x   ,x ) does not contain any class label information and is simply a
in equation (19.5.4) the term p(y   
conditional gaussian. the advantage of the above description is that we can therefore form an approximation
to p(y|c,x ) and then reuse this approximation in the prediction for many di   erent x    without needing to

(19.5.5)

rerun the approximation[319, 248].

19.5.1 binary classi   cation

for the binary class case we will use the convention that c     {1, 0}. we therefore need to specify p(c = 1|y)
for a real valued activation y. a convenient choice is the logistic transfer function3

  (x) =

1

1 + e   x

(19.5.6)

3we will also refer to this as    the sigmoid function   . more strictly a sigmoid function refers to any    s-shaped    function (from

the greek for    s   ).

410

draft november 9, 2017

gaussian processes for classi   cation

then

p(c|y) =    ((2c     1) y)

(19.5.7)

is a valid distribution since   (   x) = 1       (x), ensuring that the sum over the class states is 1. a di   culty
is that the non-linear class mapping term makes the computation of the posterior distribution equation
(19.5.3) di   cult since the integrals over y1, . . . , yn cannot be carried out analytically. there are many ap-
proximate techniques one could apply in this case, including variational methods analogous to that described
in section(18.2.3). below we describe the straightforward laplace method, leaving the more sophisticated
methods for further reading[248].

19.5.2 laplace   s approximation

in the laplace method, section(28.2), we approximate the non-gaussian distribution (19.5.5) by a gaussian4
q(y|c,x ),

p(y|c,x )     q(y|c,x )

(19.5.8)

from equation (19.5.4), approximate predictions can then be formed through the joint gaussian

   

   

p(y

,y|x

,x ,c)     p(y

(19.5.9)
we will then marginalise this gaussian to    nd the gaussian distribution on y    alone, and then use this to
form the prediction via p(c   
for compactness we de   ne the class label vector, and outputs

   

,x )q(y|c,x )

   
|y, x
|y   ).
y =(cid:0)y1, . . . , yn(cid:1)t

and notationally drop the (ever present) conditioning on the inputs x. also for convenience, we de   ne

c =(cid:0)c1, . . . , cn(cid:1)t
   =(cid:0)  (y1), . . . ,   (yn )(cid:1)t

,

(19.5.10)

(19.5.11)

finding the mode

the laplace approximation, section(28.2), corresponds to a second order expansion around the mode of the
distribution. our task is therefore to    nd the maximum of

p(y|c)     p(y, c) = exp (  (y))

where

  (y) = cty    

n(cid:88)

n=1

log(1 + eyn)    

1
2

ytk   1

x,xy    

1
2

log det (kx,x)    

n
2

log 2  

(19.5.12)

(19.5.13)

the maximum needs to be found numerically, and it is convenient to use the id77 [133, 319, 248].

ynew = y     (        )

   1      

di   erentiating equation 19.5.13 with respect to y we obtain the gradient and hessian

      = (c       )     k   1
         =    k   1

x,x     d

x,xy

where the    noise    matrix is given by

d = diag (  1(1       1), . . . ,   n (1       n ))

(19.5.14)

(19.5.15)

(19.5.16)

(19.5.17)

4some authors use the term laplace approximation solely for approximating an integral. here we use the term to refer to a

gaussian approximation of a non-gaussian distribution.

draft november 9, 2017

411

using these expressions in the newton update, (19.5.14) gives

ynew = y +(cid:0)k   1

x,x + d(cid:1)   1(cid:0)c            k   1
x,xy(cid:1)
   1 (dy + c       )

ynew = kx,x (i + dkx,x)

to avoid unnecessary inversions, one may rewrite this in the form

gaussian processes for classi   cation

(19.5.18)

(19.5.19)

for an initial guess of y, we repeatedly apply equation (19.5.19) until convergence. this is guaranteed to
converge since the hessian is negative de   nite and has a unique maximum.

making predictions

given a converged solution   y we have found a gaussian approximation

   
q(y|x , x

,c) = n

we now have gaussians for p(y   

p(y

   

   

|x

,x ,c)    

p(y

   

   

|x

(cid:16)

y   y,(cid:0)k   1

x,x + d(cid:1)   1(cid:17)
|y) and q(y|x , x   ,c) in equation (19.5.9). predictions are then made using
   
,x , y)q(y|x , x

,c)dy

(19.5.20)

(19.5.21)

(cid:90)

x,xkx,x   (cid:1). using equation (19.5.23) and equation (19.5.20) and averag-

(19.5.23)

x,xy, kx   ,x        kx   ,xk   1

x,xkx,x   (cid:1)

where, by conditioning, section(8.4.2),
    kx   ,xk   1

p(y

   

   
|y, x

,x ) = n

(cid:0)y

we can also write this as a linear system

   

y

= kx   ,xk   1

where        n
ing over y and the noise   , we obtain

x,xy +   

(cid:0)   0, kx   ,x        kx   ,xk   1
,x ,c(cid:105)     kx   ,xk   1

   

   

|x

(cid:104)y

similarly, the variance of the latent prediction is

var(y

   

   

|x

,x ,c)     kx   ,xk   1

x,x

k   1
x,xkx,x    + kx   ,x        kx   ,xk   1

x,xkx,x   

x,x   y = kx   ,x (c        (  y))

(cid:0)k   1
x,x + d(cid:1)   1
(cid:0)kx,x + d   1(cid:1)   1

kx,x   

= kx   ,x        kx   ,x

where the last line is obtained using the matrix inversion lemma, de   nition(a.11).
the class prediction for a new input x    is then given by

   
p(c

   

= 1|x

,x ,c)     (cid:104)  (y

   

)(cid:105)n (y    (cid:104)y   (cid:105),var(y   ))

(19.5.22)

(19.5.24)

(19.5.25)

(19.5.26)

(19.5.27)

in order to calculate the gaussian integral over the logistic sigmoid function, we use an approximation of
the sigmoid function based on the error function erf(x), see section(18.2.2) and avsigmagauss.m.

example 19.2. an example of binary classi   cation is given in    g(19.6) in which one-dimensional input
training data with binary class labels is plotted along with the class id203 predictions on a range
of input points.
square exponential covariance produces a smoother class prediction than the ornstein-uhlenbeck covariance
function. see demogpclass1d.m and demogpclass.m.

in both cases the covariance function is of the form 2exp (|xi     xj|  ) + 0.001  ij. the

412

draft november 9, 2017

gaussian processes for classi   cation

(a)

(b)

figure 19.6: gaussian process classi   cation. the x-axis are the inputs, and the class is the y-axis. green
points are training points from class 1 and red from class 0. the dots are the predictions p(c = 1|x   ) for
points x    ranging across the x axis.
(   = 1). see demogpclass1d.m.

(a): square exponential covariance (   = 2).

(b): ou covariance

marginal likelihood

the marginal likelihood is given by

p(c|x ) =

p(c|y)p(y|x )

y

(cid:90)

(cid:90)

under the laplace approximation, the marginal likelihood is approximated by

(cid:18)

(cid:19)

p(c|x )    

exp (  (  y))exp

   

y

1
2

(y       y)t a (y       y)

where a =            . integrating over y gives

log p(c|x )     log q(c|x )

where

log q(c|x ) =   (  y)    
=   (  y)    
= ct   y    

x,x + d(cid:1) +

1
2
1
2

log det (2  a)

log det(cid:0)k   1
n(cid:88)

log(1 + exp (  yn))    

1
2

n=1

log 2  

n
2
  ytk   1

x,x   y    

1
2

log det (i + kx,xd)

(19.5.28)

(19.5.29)

(19.5.30)

(19.5.31)

(19.5.32)

(19.5.33)

where   y is the converged iterate of equation 19.5.18. one can also simplify the above using that at conver-
gence k   1

x,x   y = c       (y).

19.5.3 hyperparameter optimisation

the approximate marginal likelihood can be used to assess hyperparameters    of the kernel. a little care is
required in computing derivatives of the approximate marginal likelihood since the optimum   y depends on
  . we use the total derivative formula [26]

d
d  

   
     

log q(c|x ) =

   
     

log q(c|x ) +

   
      yi

log q(c|x )

d
d  

  yi

log q(c|x ) =    

1
2

   
     

ytk   1

x,xy + log det (i + kx,xd)

draft november 9, 2017

(cid:105)

(19.5.34)

(19.5.35)

413

(cid:88)

i

(cid:104)

   10   8   6   4   2024681000.20.40.60.81   10   8   6   4   2024681000.20.40.60.81gaussian processes for classi   cation

which can be evaluated using the standard results for the derivative of a matrix determinant and inverse.
since the derivative of    is zero at   y, and noting that d depends explicitly on   y,

   
      yi

log q(c|x ) =    

1
2

   
      yi

log det (i + kx,xd)

the implicit derivative is obtained from using the fact that at convergence

  y = kx,x (c       (y))

to give

d
d  

  y = (i + kx,xd)

   1    
     

kx,x (c       )

(19.5.36)

(19.5.37)

(19.5.38)

these results are substituted into equation (19.5.34) to    nd an explicit expression for the derivative. see
exercise(19.7).

19.5.4 multiple classes

the extension of the preceding framework to multiple classes is essentially straightforward and may be
achieved using the softmax function

eym

  m(cid:48)eym(cid:48)

p(c = m|y) =

which automatically enforces the constraint(cid:80)
the cost of implementing the laplace approximation for the multiclass case scales as o(cid:0)c3n 3(cid:1). however,
one may show by careful implementation that the cost is only o(cid:0)cn 3(cid:1), and we refer the reader to [319, 248]

m p(c = m) = 1. naively it would appear that for c classes,

(19.5.39)

for details.

19.6 summary

    gaussian processes are powerful regression models and mathematically well understood.
    the computational complexity of the prediction is cubic in the number of datapoints. this can be prohibitive

for large datasets and approximate implementations are required.

    gaussian process classi   cation is analytically intractable and approximations are required. the posterior is

log concave so that simple unimodal approximation schemes can provide satisfactory results.

    many classical models in statistics and physics are related to gaussian processes. for example, one can

view linear dynamical systems, chapter(24), as specially constrained gps.

gaussian processes have been heavily developed within the machine learning community over recent years
and    nding e   cient approximations for both regression and classi   cation remains an active research topic.
we direct the interested reader to [263] and [248] for further discussion.

19.7 code

gpreg.m: gaussian process regression
demogpreg.m: demo gp regression
covfnge.m: gamma-exponential covariance function
gpclass.m: gaussian process classi   cation
demogpclass.m: demo gaussian process classi   cation

414

draft november 9, 2017

exercises

(cid:88)

19.8 exercises
exercise 19.1. the gram matrix k of a covariance function k(x, x(cid:48)) is positive semide   nite and hence
expressible as

kij =

uilujl

(19.8.1)

l

for suitable uil.

1. the gram matrix of the sum of two covariance functions, k+(x, x(cid:48)) = k1(x, x(cid:48)) + k2(x, x(cid:48)), is of the

form

k+ = k1 + k2

(19.8.2)

for suitable gram matrices k1 and k2. show that k+(x, x(cid:48)) is a covariance function.

2. consider the element-wise (hadamard) product of two positive semide   nite matrices. that is

ijk2
ij

k

   
ij = k1

ij =(cid:80)

ij =(cid:80)

(19.8.3)
m vimvjm, show that k    is positive semide   nite, and hence that

l uilujl and k2

using k1
the product of two covariance functions, k   (x, x(cid:48)) = k1(x, x(cid:48))k2(x, x(cid:48)), is a covariance function.

exercise 19.2. show that the sample covariance matrix with elements sij =(cid:80)n
  xi =(cid:80)n

i /n , is positive semide   nite.

n=1 xn

n=1 xn

i xn

j /n       xi   xj, where

exercise 19.3. show that

k(x     x

   
is a covariance function.

(cid:12)(cid:12)sin(cid:0)x     x

exercise 19.4. consider the function

(cid:48)(cid:1)(cid:12)(cid:12)(cid:1)
(cid:19)

f (xi, xj) = exp

(cid:48)

) = exp(cid:0)
(cid:18)
(cid:18)

1
2

   

(xi     xj)2
(cid:19)

for one dimensional inputs xi. show that

f (xi, xj) = exp

   

1
2

x2
i

exp (xixj) exp

(cid:18)

1
2

   

x2
j

(cid:19)
(cid:16)

2 (xi     xj)2(cid:17)

    1

by taylor expanding the central term, show that exp
sentation for the kernel f (xi, xj) as the scalar product of two in   nite dimensional vectors.
exercise 19.5. show that for a covariance function k1 (x, x(cid:48)) then

is a kernel and    nd an explicit repre-

) = f(cid:0)k1

(cid:0)x, x(cid:48)(cid:1)(cid:1)

k(x, x(cid:48)

is also a covariance function for any polynomial f (x) with positive coe   cients. show therefore that
exp (k1 (x, x(cid:48))) and tan (k1 (x, x(cid:48))) are covariance functions.
exercise 19.6. for a covariance function

k1(x, x(cid:48)

show that

k2(x, x(cid:48)

) = f ((cid:0)x     x(cid:48)(cid:1)t(cid:0)x     x(cid:48)(cid:1))
) = f ((cid:0)x     x(cid:48)(cid:1)t a(cid:0)x     x(cid:48)(cid:1))

is also a valid covariance function for a positive de   nite symmetric matrix a.

draft november 9, 2017

(19.8.4)

(19.8.5)

(19.8.6)

(19.8.7)

(19.8.8)

(19.8.9)

415

exercises

i (19.8.10)

d
d  

log q(c|x ) =

1
2

ytk   1

x,xk(cid:48)

x,xk   1

x,xy   

1
2

exercise 19.7. show that the derivative of the laplace approximation to the marginal likelihood equation
(19.5.34) is given by

trace(cid:0)l   1k(cid:48)

x,xd(cid:1)

1
2

   

miid

(cid:48)
ii

(cid:88)
(cid:18)    

i

(cid:2)l   1k(cid:48)

x,x (c       )(cid:3)
(cid:19)

where

k(cid:48)

x,x    

   
     

(cid:18)    

(cid:19)

kx,x, l     i + kx,xd, m     l   1kx,x, d(cid:48)

    diag

   y1

   
   y2

d11,

d22, . . .

(19.8.11)

(cid:19)

(cid:18)

a   1    
   x

hint: you will need to make use of the general derivative results

a   1,

   
   x

(19.8.12)
exercise 19.8 (string kernel). let x and x(cid:48) be two strings of characters and   s(x) be the number of times
that substring s appears in string x. then

log det (a) = trace

   x

a

a

ws  s(x)  s(x

(cid:48)

)

(19.8.13)

   
   x

a   1 =    a   1
(cid:88)

(cid:48)
k(x, x

) =

s

is a (string kernel) covariance function, provided the weight of each substring ws is positive.

1. given a collection of strings about politics and another collection about sport, explain how to form a

gp classi   er using a string kernel.

2. explain how the weights ws can be adjusted to improve the    t of the classi   er to the data and give an
explicit formula for the derivative with respect to ws of the log marginal likelihood under the laplace
approximation.

exercise 19.9 (vector regression). consider predicting a vector output y given training data x     y =
{xn, yn, n = 1, . . . , n}. to make a gp predictor

(19.8.14)

p(y   

|x   

,x ,y)

we need a gaussian model

p(y1, . . . , yn , y   

|x1, . . . , xn, x   

)

a gp requires then a speci   cation of the covariance c(ym
two di   erent input vectors. show that under the dimension independence assumption

i , yn

(19.8.15)
j |xn, xm) of the components of the outputs for

c(ym

j |xn, xm) = ci(ym
(19.8.16)
i |xn, xm) is a covariance function for the ith dimension, that separate gp predictors can be

where ci(ym
constructed independently, one for each output dimension i.

i |xn, xm)  i,j

i , yn
i , yn

i , yn

exercise 19.10. consider the markov update of a linear dynamical system, section(24.1),

where a is a given matrix and   t is zero mean gaussian noise with covariance(cid:10)  i,t  j,t(cid:48)(cid:11) =   2  i,j  t,t(cid:48). also,

xt = axt   1 +   t,

(19.8.17)

t     2

p(x1) = n (x1 0,   ).

1. show that x1, . . . , xt is gaussian distributed.

2. show that the covariance matrix of x1, . . . , xt has elements

(cid:69)

= at(cid:48)   1  (cid:0)at   1(cid:1)t

min(t,t(cid:48))(cid:88)

at(cid:48)     (cid:0)at     (cid:1)t

+   2

(cid:68)

xt(cid:48)xt
t

(19.8.18)

and explain why a linear dynamical system is a (constrained) gaussian process.

   =2

3. consider

where  t is zero mean gaussian noise with covariance(cid:10) i,t j,t(cid:48)(cid:11) =   2  i,j  t,t(cid:48). the vectors   are uncor-

yt = bxt +  t

(19.8.19)

related with the vectors   . show that the sequence of vectors y1, . . . , yt is a gaussian process with a
suitably de   ned covariance function.

416

draft november 9, 2017

chapter 20

mixture models

mixture models assume that the data is essentially clustered with each component in the mixture repre-
senting a cluster. in this chapter we view mixture models from the viewpoint of learning with missing data
and discuss some of the classical algorithms, such as em training of gaussian mixture models. we also
discuss more powerful models which allow for the possibility that an object can be a member of more than
one cluster. these models have applications in areas such as document modelling.

20.1 density estimation using mixtures

a mixture model is one in which a set of component models is combined to produce a richer model:

p(v) =

p(v|h)p(h)

(20.1.1)

the variable v is    visible    or    observable    and the discrete variable h with dom(h) = {1, . . . , h} indexes each
component model p(v|h), along with its weight p(h). the variable v can be either discrete or continuous.
mixture models have natural application in id91 data, where h indexes the cluster. this interpretation
can be gained from considering how to generate a sample datapoint v from the model equation (20.1.1).
first we sample a cluster h from p(h), and then draw a visible state v from p(v|h).
for a set of i.i.d. data v1, . . . , vn , a mixture model is of the form,    g(20.1),

h(cid:88)

h=1

n(cid:89)

n=1

@@

p(v1, . . . , vn , h1, . . . ,

hn ) =

p(vn|hn)p(hn)

from which the observation likelihood is given by

n(cid:89)

(cid:88)

n=1

hn

p(v1, . . . , vn ) =

p(vn|hn)p(hn)

finding the most likely assignment of datapoints to clusters is achieved by id136 of

argmax
h1,...,hn

p(h1, . . . , hn|v1, . . . , vn )

which, thanks to the factorised form of the distribution is equivalent to computing arg maxhn p(hn|vn) for
each datapoint.

417

(20.1.2)

(20.1.3)

(20.1.4)

expectation maximisation for mixture models

  h

  v|h

hn

vn

n

figure 20.1: a mixture model has a graphical representation as
a dag with a single hidden node, h which indexes the mixture
component; given a setting of h, we then generate an observation
v from p(v|h). independence of the n observations means the
model is replicated by the plate. the parameters are assumed
common across all datapoints.

in most applications, however, the    location    of the clusters is a priori unknown and the parameters    of
the model need to be learned to locate where these clusters are. explicitly writing the dependence on the
parameters, the model for a single datapoint v and its corresponding cluster index h is

p(v, h|  ) = p(v|h,   v|h)p(h|  h)

(20.1.5)

the optimal parameters   v|h,   h of a mixture model are then most commonly set by maximum likelihood,

  opt = argmax

  

p(v1, . . . , vn|  ) = argmax

  

p(vn|  )

(20.1.6)

(cid:89)

n

numerically this can be achieved using an optimisation procedure such as gradient based approaches. al-
ternatively, by treating the component indices as latent variables, one may apply the em algorithm, as
described in the following section, which in many classical models produces simple update formulae.

example 20.1. the data in    g(20.2) naturally has two clusters and can be modelled with a mixture of
two two-dimensional gaussians, each gaussian describing one of the clusters. here there is a clear visual
interpretation of the meaning of    cluster   , with the mixture model placing two datapoints in the same cluster
if they are both likely to be generated by the same model component. a priori we don   t know the location
of these two clusters and need to    nd the parameters; the gaussian mean and covariance for each cluster
mh, ch, h = 1, 2. this can be achieved using maximum likelihood.

20.2 expectation maximisation for mixture models

our task is to    nd the parameters    that maximise the likelihood of the observations v1, . . . , vn

(cid:40)(cid:88)

n(cid:89)

n=1

h

(cid:41)
p(vn|h,   )p(h|  )

p(v1, . . . , vn|  ) =

(20.2.1)

by treating the index h as a missing variable, mixture models can be trained using the em algorithm,
section(11.2). there are two sets of parameters       v|h for each component model p(v|h,   v|h) and   h for the
mixture weights p(h|  h). according to the general approach for i.i.d. data of section(11.2), for the m-step

figure 20.2: two dimensional data which displays clusters. in this case a gaussian
mixture model 1/2n (x m1, c1)+1/2n (x m2, c2) would    t the data well for suitable
means m1, m2 and covariances c1, c2. to the human eye, identifying these clusters
is an easy task; however, we will be interested in automatic methods that can cluster
potentially very high dimensional data, for which the cluster solutions may be far less
obvious.

418

draft november 9, 2017

   3   2   10123   3   2   101234expectation maximisation for mixture models

we need to consider the energy term:

n(cid:88)
n(cid:88)

n=1

n=1

e(  ) =

=

(cid:104)log p(vn, h|  )(cid:105)pold(h|vn)

(cid:10)log p(vn|h,   v|h)(cid:11)

pold(h|vn) +

n(cid:88)

n=1

(cid:104)log p(h|  h)(cid:105)pold(h|vn)

(20.2.2)

(20.2.3)

and maximise (20.2.3) with respect to the parameters   v|h,   h, h = 1, . . . , h. the e-step results in the
update

pnew(h|vn)     p(vn|h,   old

v|h)p(h|  old
h )

(20.2.4)

for initial parameters   , one then updates the e and m-steps until convergence. this is a general approach
for training mixture models. below we    esh out how these updates work for some speci   c models.

20.2.1 unconstrained discrete tables
here we consider training a simple belief network p(v|h,   v|h)p(h|  h), dom(v) = {1, . . . , v }, dom(h) = {1, . . . , h}
in which the tables are unconstrained. this is a special case of the more general framework discussed in
section(11.2), although it is instructive to see how the em algorithm can be derived for this speci   c case.

(cid:88)

n(cid:88)

h p(h) = 1. isolating the dependence of equation (20.2.3) on p(h) we obtain

m-step: p(h)
if no constraint is placed on p(h|  h) we may write the parameters as simply p(h), with the understanding

that 0     p(h)     1 and(cid:80)
we now wish to maximise equation (20.2.5) with respect to p(h) under the constraint that (cid:80)

hp(h) =
1. there are di   erent ways to perform this constrained optimisation. one approach is to use lagrange
multipliers, see exercise(20.4). another, arguably more elegant approach is to use the techniques described
in section(11.2) based on the similarity of the above to a id181. first we de   ne the
distribution

(cid:104)log p(h)(cid:105)pold(h|vn) =

pold(h|vn)

n(cid:88)

(20.2.5)

log p(h)

n=1

n=1

h

(cid:80)n
(cid:80)
(cid:80)n
n=1 pold(h|vn)
n=1 pold(h|vn)

h

  p(h)    

n(cid:88)

n=1

=

1
n

pold(h|vn)

then maximising equation (20.2.5) is equivalent to maximising

(cid:104)log p(h)(cid:105)  p(h) = (cid:104)log p(h)(cid:105)  p(h)     (cid:104)log   p(h)(cid:105)  p(h)

+(cid:104)log   p(h)(cid:105)  p(h)

(cid:123)(cid:122)

   kl(  p|p)

(cid:125)

(20.2.6)

(20.2.7)

since equation (20.2.5) is related to (cid:104)log p(h)(cid:105)  p(h) by the constant factor n . by subtracting the    independent
term (cid:104)log   p(h)(cid:105)  p(h) from equation (20.2.7), we obtain the negative id181 kl(  p|p). this
means that the optimal p(h) is that distribution which minimises the id181. optimally,
therefore p(h) =   p(h), so that the m-step is given by

pnew(h) =

1
n

pold(h|vn)

m-step : p(v|h)
the dependence of equation (20.2.3) on p(v|h) is

n(cid:88)

n=1

(cid:10)log p(cid:0)vn|h,   v|h

(cid:1)(cid:11)

pold(h|vn) =

(cid:88)

n(cid:88)

h(cid:88)

v

n=1

h=1

draft november 9, 2017

i [vn = v] pold(h|vn) log p (v|h)

(20.2.8)

(20.2.9)

419

(cid:124)

n(cid:88)

n=1

expectation maximisation for mixture models

  h

  vi|h

hn

vn
i

i = 1, . . . , d

figure 20.3: mixture of a product of bernoulli distributions. in
a bayesian treatment, a parameter prior is used. in the text we
simply set the parameters using maximum likelihood.

di   erentiating with respect to p(v = i|h = j) and equating to zero,

n = 1, . . . , n

if the distributions p(cid:0)v|h,   v|h

(cid:1) are not constrained, we can apply a similar kullback-leibler method, as

we did in section(11.2) and above for p(h). as an alternative to that approach, we describe here how the
lagrange method works in this case. we need to ensure that p(v|h) is a distribution for each of the mixture
states h = 1, . . . , h. this can be achieved using a set of lagrange multipliers, giving the lagrangian:

v(cid:88)

n(cid:88)

h(cid:88)

v=1

n=1

h=1

l    

   l

   p(v = i|h = j)
solving this, we have

i [vn = v] pold(h|vn) log p (v|h) +
n(cid:88)

p(v = i|h = j)

i [vn = i] pold(h = j|vn = i)
n(cid:88)

1

=

n=1

(cid:32)

v(cid:88)

v=1

  (h)

1    

h(cid:88)

h=1

(cid:33)

      (h = j) = 0

p(v|h)

(20.2.10)

(20.2.11)

(20.2.12)

p(v = i|h = j) =

using the normalisation requirement,(cid:80)

  (h = j)

n=1

i [vn = i] pold(h = j|vn = i)

the above equation, summed over v. hence the m-step update is given by

v p(v = i|h = j) = 1 shows that   (h = j) is just the numerator of

pnew(v = i|h = j) =

(cid:80)n
(cid:80)n
(cid:80)v
i [vn = i] pold(h = j|vn = i)

i [vn = i] pold(h = j|vn = i)

n=1

n=1

i=1

e-step
according to the general em procedure, section(11.2), optimally we set pnew(h|vn) = pold(h|vn):

pnew(h|vn) =

(cid:80)
pold(vn|h)pold(h)
h pold(vn|h)pold(h)

equations (20.2.8,20.2.13,20.2.14) are repeated until convergence and guarantee that the likelihood equation
(20.2.1) cannot decrease. the initialisation of the tables and mixture probabilities can severely a   ect the
quality of the solution found since the likelihood often has local optima. if random initialisations are used,
it is recommended to record the converged value of the likelihood itself, to see which parameters have the
higher likelihood. the solution with the highest likelihood is to be preferred.

20.2.2 mixture of product of bernoulli distributions

as an example mixture model, and one that can be used for practical id91, see example(20.2), we

describe a simple mixture model that can be used to cluster binary vectors, v = (v1, . . . , vd)t, vi     {0, 1}.
the mixture of bernoulli products1 model is given by

(20.2.13)

(20.2.14)

p(vi|h)

(20.2.15)

1this is similar to the naive bayes classi   er in which the class labels are always hidden.

420

draft november 9, 2017

h(cid:88)

d(cid:89)

p(v) =

p(h)

h=1

i=1

expectation maximisation for mixture models

figure 20.4: top: data from questionnaire responses. 150 people were each asked 5 questions, with    yes   
(white) and    no    (black) answers. gray denotes that the absence of a response (missing data). this training
data was generated by a two component product of bernouilli model. missing data was simulated by
randomly removing values from the dataset. middle: the    correct    h values sampled from the data. a    0   
denotes hn = 1 and    1    denotes hn = 2. bottom: the estimated p(hn

i = 2|vn) values.

where each term p(vi|h) is a bernoulli distribution. the model is depicted in    g(20.3) and has parameters
p(h) and p(vi = 1|h), dom(h) = {1 . . . , h}. one way to understand the model is to imagine drawing
samples. in this case, for each datapoint n, we draw a cluster index h     {1, . . . , h} from p(h). then for
each i = 1, . . . , d, we draw a state vi     {0, 1} from p(vi|h).
em training

to train the model under maximum likelihood it is convenient to use the em algorithm which, as usual,
may be derived by writing down the energy:

(cid:104)log p(vn

i |h)(cid:105)pold(h|vn) +

(20.2.16)

(cid:88)
n (cid:104)log p(h)(cid:105)pold(h|vn)

(cid:88)
n (cid:104)log p(vn, h)(cid:105)pold(h|vn) =

(cid:88)

(cid:88)

n

i

and then performing the maximisation over the table entries. from our general results, section(11.2),
we know that the m-step is equivalent to maximum likelihood when all variables are observed, and then
replacing the unobserved variables h by the conditional distribution p(h|v). using this we can write down
immediately the m-step as

(cid:80)

n

i [vn

i = 1] pold(h = j|vn) +(cid:80)

(cid:80)
(cid:80)
i [vn
(cid:80)
h(cid:48)(cid:80)
n pold(h = j|vn)
|vn)

n pold(h(cid:48)

n

i = 1] pold(h = j|vn)

i [vn

n

i = 0] pold(h = j|vn)

pnew(vi = 1|h = j) =
pnew(h = j) =

and the e-step by

pnew(h = j|vn)     pold(h = j)

pold(vn

i |h = j)

d(cid:89)

i=1

(20.2.17)

(20.2.18)

equations (20.2.17,20.2.18) are iterated until convergence.

if an attribute i is missing for datapoint n, one needs to sum over the states of the corresponding vn
e   ect of performing the summation for this model is simply to remove the corresponding factor p(vn
the algorithm, see exercise(20.1).

i . the
i |h) from

initialisation

the em algorithm can be very sensitive to initial conditions. consider the following initialisation: p(vi =
1|h = j) = 0.5, with p(h) set arbitrarily. this means that at the    rst iteration, pold(h = j|vn) = p(h = j).
the subsequent m-step updates are

pnew(h) = pold(h),

pnew(vi|h = j) = pnew(vi|h = j

(cid:48)

)

draft november 9, 2017

(20.2.19)

421

0501001501234505010015000.5105010015000.51the gaussian mixture model

for any j, j(cid:48). this means that the parameters p(v|h) immediately become independent of h and the model

is numerically trapped in a symmetric solution. it makes sense, therefore, to initialise the parameters in a
non-symmetric fashion.

the model is readily extendable to more than two output class, and this is left as an exercise for the
interested reader.

example 20.2 (questionnaire). a company sends out a questionnaire containing a set of d    yes/no   
questions to a set of customers. the binary responses of a customer are stored in a vector v = (v1, . . . , vd)t.
in total n customers send back their questionnaires, v1, . . . , vn , and the company wishes to perform an
analysis to    nd what kinds of customers it has. the company assumes there are h essential types of
customer for which the pro   le of responses is de   ned by only the customer type.

data from a questionnaire containing 5 questions, with 150 respondents is presented in    g(20.4). the data
has a large number of missing values. we assume there are h = 2 kinds of respondents and attempt to assign
each respondent into one of the two clusters. running the em algorithm on this data, with random initial
values for the tables, produces the results in    g(20.5). based on assigning each datapoint vn to the cluster
with maximal posterior id203 hn = arg maxh p(h|vn), given a trained model p(v|h)p(h), the model
assigns 90% of the data to the correct cluster (which is known in this simulated case). see mixprodbern.m.

example 20.3 (handwritten digits). we have a collection of 5000 handwritten digits which we wish to
cluster into 20 groups,    g(20.6). each digit is a 28   28 = 784 dimensional binary vector. using a mixture of
bernoulli products, trained with 50 iterations of em (with random initialisation), the clusters are presented
in    g(20.6). as we see, the method captures natural clusters in the data     for example, there are two kinds
of 1, one slightly more slanted than the other, two kinds of 4, etc.

20.3 the gaussian mixture model

we turn our attention now to modelling continuous vector observations x (x plays the role of the    visible   
variable v previously). gaussians are particularly convenient continuous mixture components since they
constitute    bumps    of id203 mass, aiding an intuitive interpretation of the model. as a reminder, a d
dimensional gaussian distribution for a continuous variable x is

(cid:26)

1(cid:112)

p (x|m, s) =

det (2  s)

exp

1
2

(x     m)t s   1 (x     m)

   

(cid:27)

where m is the mean and s is the covariance matrix. a mixture of gaussians is then

h(cid:88)

i=1

p (x) =

p(x|mi, si)p(i)

(20.3.1)

(20.3.2)

(a)

(b)

422

figure 20.5: em learning of a mixture of
(a): true p(h) (left)
bernoulli products.
and learned p(h) (right) for h = 1, 2. (b):
true p(v|h) (left) and learned p(v|h) (right)
for v = 1, . . . , 5. each column pair corre-
sponds to p(vi|h = 1) (right column) and
p(vi|h = 2) (left column) with i = 1, . . . , 5.
the learned probabilities are reasonably
close to the true values.

draft november 9, 2017

1200.10.20.30.40.50.60.70.80.911200.10.20.30.40.50.60.70.80.911234500.10.20.30.40.50.60.70.80.911234500.10.20.30.40.50.60.70.80.91the gaussian mixture model

figure 20.6: top: a selection of 200 of the 5000 handwritten digits in the training set. bottom: the trained
cluster outputs p(vi = 1|h) for h = 1, . . . , 20 mixtures. see demomixbernoullidigits.m.

where p(i) is the mixture weight for component i. for a set of data x =(cid:8)x1, . . . , xn(cid:9) and under the usual

i.i.d. assumption, the log likelihood is

(cid:26)

1(cid:112)

p(i)

det (2  si)

exp

1
2

(xn     mi)t s   1

i

   

(cid:27)
(xn     mi)

(20.3.3)

where the parameters are    = {mi, si, p(i), i = 1, . . . , h}. the optimal parameters    can be set using maxi-
mum likelihood, bearing in mind the constraint that the si must be symmetric positive de   nite matrices, in
i p(i) = 1. gradient based optimisation approaches are feasible under a parame-
terisation of the si (e.g. cholesky decomposition) and p(i) (e.g. softmax) that enforce the constraints. an
alternative is the em approach which in this case is particularly convenient since it automatically provides
parameter updates that ensure these constraints.

log p(x|  ) =

n(cid:88)
h(cid:88)
addition to 0     p(i)     1,(cid:80)

log

n=1

i=1

20.3.1 em algorithm

from the general approach, section(11.2), for the m-step we need to consider the energy. using the fact
that the component index i plays the role of the latent variable, the energy is given by

n(cid:88)

n=1

n(cid:88)

n=1

n(cid:88)

(cid:104)log p(xn, i)(cid:105)pold(i|xn) =
h(cid:88)

(cid:26)

(cid:104)log [p(xn|i)p(i)](cid:105)pold(i|xn)

(20.3.4)

plugging in the de   nition of the gaussian components, we have

pold(i|xn)

1
2

(xn     mi)t s   1

i

   

(xn     mi)    

1
2

n=1

i=1

log det (2  si) + log p(i)

(20.3.5)

(cid:27)

the m-step requires the maximisation of the above with respect to mi, si, p(i).

m-step : optimal mi

maximising equation (20.3.5) with respect to mi is equivalent to minimising

pold(i|xn) (xn     mi)t s   1

i

(xn     mi)

di   erentiating with respect to mi and equating to zero we have

n(cid:88)

h(cid:88)

n=1

i=1

n(cid:88)

n=1

   2

pold(i|xn)s   1

i

(xn     mi) = 0

draft november 9, 2017

(20.3.6)

(20.3.7)

423

hence, optimally,

(cid:80)n
(cid:80)n
n=1 pold(i|xn)xn
n=1 pold(i|xn)

mi =

by de   ning the membership distribution

pold(n|i)    

(cid:80)n
pold(i|xn)
n=1 pold(i|xn)

the gaussian mixture model

(20.3.8)

(20.3.9)

which quanti   es the membership of datapoints to cluster i, we can write equation (20.3.8) more compactly
as the update

mnew

i =

pold(n|i)xn

(20.3.10)

intuitively, this updates the mean for cluster i as the average over all datapoints weighted by their mem-
bership to cluster i.

m-step : optimal si

n(cid:88)

n=1

optimising equation (20.3.5) with respect to si is equivalent to minimising

i     log det(cid:0)s   1

i

pold(i|xn)

(cid:1)(cid:69)
(cid:33)

(   n

i    n

i )ts   1
n(cid:88)

s   1

where    n

(cid:32)
i     xn     mi. to aid the matrix calculus, we isolate the dependency on si to give

    log det(cid:0)s   1

i

(cid:1) n(cid:88)

n=1

pold(i|xn)

i

trace

pold(i|xn)   n
di   erentiating with respect to s   1

n=1

i

i (   n

i )t

and equating to zero, we obtain

n(cid:88)

(cid:68)

n=1

n(cid:88)

n=1

using the membership distribution pold(n|i), the resulting update is given by

n(cid:88)

n=1

i (   n

i )t     si

pold(i|xn) = 0

pold(i|xn)   n
n(cid:88)

snew

i =

pold(n|i) (xn     mi) (xn     mi)t

n=1

n(cid:88)

n=1

si =

pold(n|i)diag

(cid:16)

(xn     mi) (xn     mi)t(cid:17)

as for the mean, this essentially softly       lters    those datapoints that belong to cluster i and takes their
covariance. this update also ensures that si is symmetric positive semide   nite. a special case is to
constrain the covariances si to be diagonal for which the update is, see exercise(20.2),

(20.3.11)

(20.3.12)

(20.3.13)

(20.3.14)

(20.3.15)

where above diag (m) means forming a new matrix from the matrix m with zero entries except for the
diagonal entries of m. a more extreme case is that of isotropic gaussians si =   2
i i. the reader may show
that the optimal update for   2
in this case is given by taking the average of the diagonal entries of the
i
diagonally constrained covariance update,

n(cid:88)

n=1

  2
i =

1
d

424

pold(n|i)(xn     mi)2

(20.3.16)

draft november 9, 2017

the gaussian mixture model

5:

end for

algorithm 20.1 em training for the gmm

for n = 1, . . . n do

for i = 1, . . . , h do
p(i|xn) = p(i) exp

2: while likelihood not converged or termination criterion not reached do
3:
4:

1: initialise the centres mi, covariances si and weights p(i) > 0,(cid:80)
(cid:111)
(xn     mi)

(cid:110)
2 (xn     mi)t s   1
normalise p(i|xn) so that(cid:80)
    1
i p(i|xn) = 1
normalise p(n|i) so that(cid:80)
mi =(cid:80)n
si =(cid:80)n
(cid:80)n
n=1 p(n|i)xn
n=1 p(n|i) (xn     mi) (xn     mi)t
(cid:110)
n=1 log(cid:80)h
l =(cid:80)n
n=1 p(i|xn)
2 (xn     mi)t s   1
    1
i=1 p(i)

n p(n|i) = 1 for each i.

p(n|i) = p(i|xn)

for i = 1, . . . , h do

1   det(2  si)

6:
7:
8:
9:
10:
11:
12:

p(i) = 1
n

13:

14:
15:
16:

end for

end for

det (si)

exp

i

i

17: end while

i p(i) = 1 i = 1, . . . , h.

    1

2

(cid:46) responsibility

(cid:46) membership

(cid:111)
(xn     mi)

(cid:46) m-step for means
(cid:46) m-step for covariances
(cid:46) m-step for weights

(cid:46) log likelihood

if no constraint is placed on the weights, the update follows the general formula given in equation (20.2.8),

m-step : optimal mixture coe   cients

n(cid:88)

n=1

pold(i|xn)

pnew(i) =

1
n

e-step

from the general theory, section(11.2), the e-step is given by

explicitly, this is given by the responsibility

p(i|xn)     p(xn|i)p(i).

p(i|xn) =

(cid:80)

p(i) exp
i(cid:48) p(i(cid:48)) exp

(cid:110)
(cid:110)
2 (xn     mi)t s   1
    1
2 (xn     mi(cid:48))t s   1
    1

i

(cid:111)

(cid:111)

(xn     mi)
i(cid:48) (xn     mi(cid:48))

    1

2

det (si)

det (si(cid:48))

    1

2

the above equations (20.3.8,20.3.14,20.3.17,20.3.19) are iterated in sequence until convergence. note that
this means that the    new    means are used in the update for the covariance, see algorithm(20.1).

the performance of em for gaussian mixtures can be strongly dependent on the initialisation, which we
discuss below.
in addition, constraints on the covariance matrix are required in order to    nd sensible
solutions.

20.3.2 practical issues

in   nite troubles

a di   culty arises with using maximum likelihood to    t a gaussian mixture model. consider placing a
component p(x|mi, si) with mean mi set to one of the datapoints mi = xn. the contribution from that
gaussian for datapoint xn will be

p(xn|mi, si) =

1(cid:112)

det (2  si)

2 (xn   xn)ts   1
    1

i

(xn   xn) =

e

1(cid:112)

det (2  si)

draft november 9, 2017

(20.3.17)

(20.3.18)

(20.3.19)

(20.3.20)

425

(a) 1 iteration

(b) 50 iterations

the gaussian mixture model

figure 20.7: training a mixture of 10 isotropic
gaussians (a): if we start with large variances
for the gaussians, even after one iteration, the
gaussians are centred close to the mean of the
(b): the gaussians begin to separate
data.
(c): one by one, the gaussians move towards
appropriate parts of the data (d): the    nal con-
verged solution. the gaussians are constrained
to have variances greater than a set amount. see
demogmmem.m.

(c) 125 iterations

(d) 150 iterations

in the limit that the    width    of the covariance goes to zero (the eigenvalues of si tend to zero), this proba-
bility density becomes in   nite. this means that one can obtain a maximum likelihood solution by placing
zero-width gaussians on a selection of the datapoints, resulting in an in   nite likelihood. this is clearly
undesirable and arises because, in this case, the maximum likelihood solution does not constrain the param-
eters in a sensible way. note that this is not related to the em algorithm, but a property of the maximum
likelihood method itself. all computational methods which aim to    t unconstrained mixtures of gaussians
using maximum likelihood therefore succeed in    nding    reasonable    solutions merely by getting trapped in
favourable local maxima. a remedy is to include an additional constraint on the width of the gaussians,
ensuring that they cannot become too small. one approach is to monitor the eigenvalues of each covariance
matrix and if an update would result in a new eigenvalue smaller than a desired threshold, the update is
rejected.
in gmmem.m we use a similar approach in which we constrain the determinant (the product of
the eigenvalues) of the covariances to be greater than a desired speci   ed minimum value. one can view
the formal failure of maximum likelihood in the case of gaussian mixtures as a result of an inappropriate
prior. maximum likelihood is equivalent to map in which a    at prior is placed on each matrix si. this is
unreasonable since the matrices are required to be positive de   nite and of non-vanishing width. a bayesian
solution to this problem is possible, placing a prior on covariance matrices. the natural prior in this case is
the wishart distribution, or a gamma distribution in the case of a diagonal covariance.

initialisation

a useful initialisation strategy is to set the covariances to be diagonal with large variances. this gives the
components a chance to    sense    where data lies. an illustration of the performance of the algorithm is given
in    g(20.7).

symmetry breaking

if the covariances are initialised to large values, the em algorithm appears to make little progress in the
beginning as each component jostles with the others to try to explain the data. eventually one gaussian
component breaks away and takes responsibility for explaining the data in its vicinity, see    g(20.7). the
origin of this initial jostling is an inherent symmetry in the solution     it makes no di   erence to the likelihood
if we relabel what the components are called. the symmetries can severely handicap em in    tting a large
number of component models in the mixture since the number of permutations increases dramatically with
the number of components. a heuristic is to begin with a small number of components, say two, for which
symmetry breaking is less problematic. once a local broken solution has been found, more models are
included into the mixture, initialised close to the currently found solutions.
in this way, a hierarchical
scheme is envisaged. another popular method for initialisation is to center the means to those found by the

426

draft november 9, 2017

the gaussian mixture model

(a)

(b)

(c)

(a): a gaussian mixture model with h = 4 components. there is a component (purple)
figure 20.8:
with large variance and small weight that has little e   ect on the distribution close to where the other three
components have appreciable mass. as we move further away this additional component gains in in   uence.
(b): the gmm id203 density function from (a).
(c): plotted on a log scale, the in   uence of each
gaussian far from the origin becomes clearer.

id116 algorithm, see section(20.3.5)     however, this itself requires a heuristic initialisation.

20.3.3 classi   cation using gaussian mixture models

we can use gmms as part of a class conditional generative model, in order to make a powerful classi   er.
consider data drawn from two classes, c     {1, 2}. we can    t a gmm p(x|c = 1,x1) to the data x1 from
class 1, and another gmm p(x|c = 2,x2) to the data x2 from class 2. this gives rise to two class-conditional
gmms,

h(cid:88)

i=1

p(x|c,xc) =

p(i|c)n (x mc

i , sc
i )

for a novel point x   , the posterior class id203 is

p(c|x   

,x )     p(x   

|c,xc)p(c)

where p(c) is the prior class id203. the maximum likelihood setting is that p(c) is proportional to the
number of training points in class c.

overcon   dent classi   cation
consider a testpoint x    a long way from the training data for both classes. for such a point, the proba-
bility that either of the two class models generated the data is very low. nevertheless, one id203 will
be exponentially higher than the other (since the gaussians drop exponentially quickly at di   erent rates),
meaning that the posterior id203 will be con   dently close to 1 for that class which has a component
closest to x   . this is an unfortunate property since we would end up con   dently predicting the class of
novel data that is not similar to anything we   ve seen before. we would prefer the opposite e   ect that for
novel data far from the training data, the classi   cation con   dence drops and all classes become equally likely.

a remedy for this situation is to include an additional component in the gaussian mixture for each class
that is very broad. we    rst collect the input data from all classes into a dataset x , and let m be the mean
of all this data and s the covariance. then for the model of each class c data we include an additional
gaussian (dropping the notational dependency on x )

p(x|c) =

  pc
in (x mc

i , sc

i ) +   pc

h+1n (x m,   s)

h(cid:88)
(cid:26) pc

i=1

i

  

where

  pc
i    

i     h
i = h + 1

draft november 9, 2017

(20.3.21)

(20.3.22)

(20.3.23)

(20.3.24)

427

   10   8   6   4   2024681000.050.10.150.20.25   10   8   6   4   2024681000.050.10.150.20.25   10   8   6   4   20246810   180   160   140   120   100   80   60   40   200the gaussian mixture model

figure 20.9: class conditional gmm train-
ing and classi   cation. (a): data from two
di   erent classes. we    t a gmm with two
components to the data from each class.
the (magenta) diamond is a test point far
from the training data we wish to classify.
(b): upper subpanel are the class probabil-
ities p(c = 1|n) for the 40 training points,
and the 41st point, being the test point.
this shows how the test point is con   dently
placed in class 1. the lower subpanel are
the class probabilities but including the ad-
ditional large variance gaussian term. the
class label for the test point is now maxi-
mally uncertain. see demogmmclass.m.

(a)

(b)

where    is a small positive value and    in   ates the covariance (we take    = 0.0001 and    = 10 in
demogmmclass.m). the e   ect of the additional component on the training likelihood is negligible since
it has small weight and large variance compared to the other components, see    g(20.8). however, as we
move away from the region where the    rst h components have appreciable mass, the additional component
gains in in   uence since it has a higher variance. if we include the same additional component in the gmm
for each class c then the in   uence of this additional component will be the same for each class, dominating
as we move far from the in   uence of the other components. for a point far from the training data the
likelihood will be roughly equal for each class since in this region the additional broad component dominates
each class with equal measure. the posterior distribution will then tend to the prior class id203 p(c),
mitigating the deleterious e   ect of a single gmm dominating when a testpoint is far from the training data.

example 20.4. the data in    g(20.9a) has a cluster structure for each class. based on    tting a gmm to
each of the two classes, a test point (diamond) far from the training data is con   dently classi   ed as belonging
to class 1. this is an undesired e   ect since we would prefer that points far from the training data are not
classi   ed with any certainty. by including an additional large variance gaussian component for each class
this has little e   ect on the class probabilities of the training data, yet has the desired e   ect of making the
class id203 for the test point maximally uncertain,    g(20.9b).

20.3.4 the parzen estimator
the parzen density estimator is formed by placing a    bump of mass   ,   (x|xn), on each datapoint,

a popular choice is (for a d dimensional x)

n=1

1
n

p(x) =

  (x|xn)

n(cid:88)
(cid:0)x xn,   2id
  (x|xn) = n
n(cid:88)

1

p(x) =

(cid:1)

1
n

(2    2)d/2 exp

n=1

giving the mixture of gaussians

(cid:18)

   

1

2  2 (x     xn)2

(cid:19)

(20.3.25)

(20.3.26)

(20.3.27)

there is no training required for a parzen estimator     only the positions of the n datapoints need storing.
whilst the parzen technique is a reasonable and cheap way to form a density estimator, it does not enable
us to form any simpler description of the data. in particular, we cannot perform id91 since there is
no lower number of clusters assumed to underly the data generating process. this is in contrast to gmms
trained using maximum likelihood on a    xed number h     n of components.
428

draft november 9, 2017

   8   6   4   20246810   6   4   2024681012051015202530354000.20.40.60.81051015202530354000.20.40.60.81the gaussian mixture model

algorithm 20.2 id116

1: initialise the centres mi, i = 1, . . . , k.
2: while not converged do
3:
4:
5:

for each centre i,    nd all the xn for which i is the nearest (in euclidean sense) centre.
call this set of points ni. let ni be the number of datapoints in set ni.
update the means

(cid:88)

n   ni

xn

mnew

i =

1
ni

6: end while

(a): 550 datapoints
figure 20.10:
clustered using id116 with 3 com-
ponents. the means are given by the
(b): evolution of the
red crosses.
mean square distance to the nearest
centre against iterations of the algo-
rithm. the means were initialised to
be close to the overall mean of the
data. see demokmeans.m.

(a)

(b)

20.3.5 id116

with mixture weights pi     0,(cid:80)
(cid:0)x mi,   2i(cid:1)

k(cid:88)

p(x) =

pin

i=1

i pi = 1

consider a mixture of k isotropic gaussians in which each covariance is constrained to be equal to   2i,

(20.3.28)

whilst the em algorithm breaks down if a gaussian component is allowed to set mi equal to a datapoint
with   2     0, by constraining all components to have the same variance   2, the algorithm has a well de   ned
limit as   2     0. the reader may show, exercise(20.3), that in this case the membership distribution equation
(20.3.9) becomes deterministic

p(n|i)    

if mi is closest to xn
otherwise

(20.3.29)

(cid:26) 1

0

in this limit the em update (20.3.10) for the mean mi is given by taking the average of the points closest to
mi. this limiting and constrained gmm then reduces to the so-called id116 algorithm, algorithm(20.2).
despite its simplicity the id116 algorithm converges quickly and often gives a reasonable id91, pro-
vided the centres are initialised sensibly. see    g(20.10).

id116 is often used as a simple form of data compression. rather than sending the datapoint xn, one
sends instead the index of the centre to which it is associated. this is called vector quantisation and is
a form of lossy compression. to improve the quality, more information can be transmitted such as an
approximation of the di   erence between xn and the corresponding closest mean m, which can be used to
improve the reconstruction of the compressed datapoint.

20.3.6 bayesian mixture models

bayesian extensions include placing priors on the parameters of each model in the mixture, and also on
the component mixture weights. in most cases this will give rise to an intractable integral for the marginal
likelihood. methods that approximate the integral include sampling techniques [106]. see also [120, 71] for
an approximate variational treatment focussed on bayesian gaussian mixture models.

draft november 9, 2017

429

   8   6   4   202468   4   20246811.522.533.544.550510152025mixture of experts

figure 20.11: mixture of experts model. the prediction of the
output yn (real or continuous) given the input xn is averaged
over individual experts p(yn|xn, whn). the expert hn is selected
by the gating mechanism with id203 p(hn|xn, u), so that
some experts will be more responsible for predicting the output
for xn in    their    part of the input space. the parameters w, u
can be learned by maximum likelihood after marginalising over
the hidden expert indices h1, . . . , hn .

n = 1, . . . , n

xn

yn

hn

w

u

20.3.7 semi-supervised learning

in some cases we may know to which mixture component certain datapoints belong. for example, given a
collection of images that we wish to cluster, it may be that we already have cluster labels for a subset of
the images. given this information we want to    t a mixture model with a speci   ed number of components
h and parameters   . we write (vm    , hm    ), m = 1, . . . , m for the m known datapoints and corresponding
components, and (vn, hn), n = 1, . . . , n for the remaining datapoints whose components hn are unknown.
we aim then to maximise the likelihood

(cid:40)(cid:89)

m

(cid:41)(cid:40)(cid:89)

(cid:88)

n

hn

(cid:41)

p(v1:m   

, v1:n|h1:m   

,   ) =

p(vm    |hm    ,   )

p(vn|hn,   )p(hn)

(20.3.30)

if we were to lump all the datapoints together, this is essentially equivalent to the standard unsupervised
case, expect that some of the h are    xed into known states. the only e   ect on the em algorithm is therefore
in the terms p(h|vm    ) for the labelled datapoints which are delta functions p(h|vm    ) =   h,hm   
in the known
state, resulting in a minor modi   cation of the standard algorithm, exercise(20.6).

20.4 mixture of experts

the mixture of experts model[163] is an extension to input-dependent mixture weights. for an output y (a
discrete class or continuous regression variable) and input x, this has the general form, see    g(20.11),

p(y|x, w, u) =

p(y|x, wh)p(h|x, u),

(20.4.1)

here h indexes the mixture component. each expert h has parameters wh with w = [w1, . . . , wh ] and
corresponding gating parameters uh with u = [u1, . . . , uh ]. unlike a standard mixture model, the compo-
nent distribution p(h|x, u) is dependent on the input x. this so-called gating distribution is conventionally
taken to be of the softmax form

h(cid:88)

h=1

p(h|x, u) =

hx(cid:80)

eut
h eut
hx

(20.4.2)

the idea is that we have a set of h predictive models (experts), p(y|x, wh), each with a di   erent parameter
wh, h = 1, . . . , h. how suitable model h is for predicting the output for input x is determined by the
alignment of input x with the weight vector uh. in this way the input x is softly assigned to the appropriate
experts.

learning the parameters

maximum likelihood training can be achieved using a form of em. we will not derive the em algorithm
for the mixture of experts model in full, merely pointing the direction along which the derivation would
continue. for a single datapoint x, the em energy term is

(cid:104)log p(y|x, wh)p(h|x, u)(cid:105)p(h|x,wold,uold)

430

(20.4.3)

draft november 9, 2017

indicator models

(cid:16)

y xtwh,   2(cid:17)

for regression a simple choice is

p(y|x, wh) = n

and for (binary) classi   cation

p(y = 1|x, wh) =   (xtwh)

(20.4.4)

(20.4.5)

in both cases computing the derivatives of the energy with respect to the parameters w is straightforward,
so that an em algorithm is readily available. an alternative to em is to compute the gradient of the
likelihood directly using the standard approach discussed in section(11.6).

a bayesian treatment is to consider

p(y, w, u, h|x) = p(y|x, wh)p(h|x, u)p(w)p(u)

where it is conventional to assume p(w) =(cid:81)

h p(wh), p(u) =(cid:81)

h p(uh). the integrals required to calculate
the marginal likelihood are generally intractable and approximations are required. see [311] for a variational
treatment for regression and [45] for a variational treatment of classi   cation. an extension to bayesian model
selection in which the number of experts is estimated is considered in [157].

(20.4.6)

20.5 indicator models

indicator models generalise our previous mixture models by allowing a more general prior distribution on
cluster assignments. for consistency with the literature we use an indicator z, as opposed to a hidden
variable h, although they play the same role. a id91 model with parameters    on the component
models and joint indicator prior p(z1:n ) takes the form, see    g(20.12a),

(cid:88)

z1:n

(cid:88)

p(v1:n|  ) =

p(v1:n|z1:n ,   )p(z1:n )

since the zn indicate cluster membership,

n(cid:89)

p(v1:n|  ) =

p(z1:n )

p(vn|zn,   )

z1:n

n=1

below we discuss the role of di   erent indicator priors p(z1:n ) in id91.

20.5.1 joint indicator approach: factorised prior

assuming prior independence of indicators,

n(cid:89)

n=1

(cid:88)

n(cid:89)

z1:n

n=1

p(z1:n ) =

p(zn),

we obtain from equation (20.5.2)

zn     {1, . . . , k}
n(cid:89)

(cid:88)

p(v1:n|  ) =

p(vn|zn,   )p(zn) =

p(vn|zn,   )p(zn)

n=1

zn

(20.5.1)

(20.5.2)

(20.5.3)

(20.5.4)

which recovers the standard mixture model equation (20.1.3). as we discuss below, more sophisticated joint
indicator priors can be used to explicitly control the complexity of the indicator assignments and open the
path to essentially    in   nite dimensional    models.

draft november 9, 2017

431

indicator models

figure 20.12: (a): a generic mixture model for
data v1:n . each zn indicates the cluster of each
datapoint.    is a set of parameters and zn = k
selects parameter   k for datapoint vn. (b): for
a potentially large number of clusters one way
to control complexity is to constrain the joint
(c): plate notation of
indicator distribution.
(b).

z1

v1

zn

vn

z1

v1

  

(a)

zn

vn

  

  

(b)

  

zn

vn

  

(c)

n

20.5.2 polya prior

for a large number of available clusters (mixture components) k (cid:29) 1, using a factorised joint indicator
distribution could potentially lead to over   tting, resulting in little or no meaningful id91. one way to
control the e   ective number of components that are used is via a parameter    that regulates the complexity,
see    g(20.12b,c),

(20.5.5)

(20.5.6)

(20.5.7)

(20.5.8)

(cid:41)

(cid:90)

(cid:40)(cid:89)

  

n

p(z1:n ) =

p(zn|  )

p(  )

where p(z|  ) is a categorical distribution,

p(zn = k|  ) =   k

p(  ) = dirichlet (  |  )    

k(cid:89)

k=1

  /k   1
k

  

  k is low, we are therefore unlikely to select cluster k for any datapoint n. a
this means, for example, that if
convenient choice for p(  ) is the dirichlet distribution (since this is conjugate to the categorical distribution),

@@

the integral over    in equation (20.5.5) can be performed analytically to give a polya distribution:

k(cid:89)

(cid:88)
the number of unique clusters used is then given by u =(cid:80)

  (nk +   /k)

p(z1:n ) =

  (n +   )

nk    

  (  /k)

  (  )

k=1

n

,

i [zn = k]

i [nk > 0]. the distribution over likely cluster
numbers is controlled by the parameter   . the scaling   /k in equation (20.5.7) ensures a sensible limit
as k        , see    g(20.13), in which limit the models are known as dirichlet process mixture models. this
approach means that we do not need to explicitly constrain the number of possible components k since the
number of active components u remains limited even for very large k.

k

id91 is achieved by considering argmax

z1:n

p(z1:n|v1:n ). in practice it is common to consider

argmax

zn

p(zn|v1:n )

(20.5.9)

unfortunately, posterior id136 of p(zn|v1:n ) for this class of models is formally computationally in-
tractable and approximate id136 techniques are required. a detailed discussion of these techniques is
beyond the scope of this book and we refer the reader to [180] for a deterministic (variational) approach
and [222] for a discussion of sampling approaches.

432

draft november 9, 2017

mixed membership models

(a)

(b)

(c)

figure 20.13: the distribution of the number of unique clusters u when indicators are sampled from a polya
(a): k = 50, (b): k = 100, (c):
distribution equation (20.5.8), with    = 2, and n = 50 datapoints.
k = 1000. even though the number of available clusters k is larger than the number of datapoints, the
number of used clusters u remains constrained. see demopolya.m.

  

  n

zn
w

vn
w

wn

n

  

figure 20.14: id44. for document n we    rst sample
a distribution of topics   n. then for each word position w = 1, . . . , wn in
the document we sample a topic zn
w from the topic distribution. given the
topic we then sample a word from the word distribution of that topic. the
parameters of the model are the word distributions for each topic   , and the
parameters of the topic distribution   .

20.6 mixed membership models

unlike standard mixture models in which each object is assumed to have been generated from a single
cluster, in mixed membership models an object may be a member of more than one group. latent dirichlet
allocation discussed below is an example of such a mixed membership model, and is one of a number of
models developed in recent years [4, 99].

20.6.1 id44

id44[47] considers that each datapoint may belong to more than a single cluster. a
typical application is to identify topic clusters in a collection of documents. a single document contains a
sequence of words, for example

v = (the, cat, sat, on, the, mat)

(20.6.1)

vn =(cid:0)vn

(cid:1) ,

if each word in the available dictionary of d words is assigned to a unique state (say dog = 1, tree = 2, cat =
3, . . .), we can represent then the nth document as a vector of word indices

1 , . . . , vn
wn

vn
i     {1, . . . , d}

(20.6.2)

where wn is the number of words in the nth document. the number of words wn in each document can
vary although the overall dictionary from which they came is    xed.

(cid:80)k

the aim is to    nd common topics in documents, assuming that any document could potentially contain
more than one topic. it is useful to think    rst of an underlying generative model of words, including latent
topics (which we will later integrate out). for each document n we have a distribution of topics   n with
k = 1 which gives a latent description of the document in terms of its topic membership. for example,

k=1   n

draft november 9, 2017

433

051015200501001502002500510152005010015020025005101520050100150200250mixed membership models

document n (which discusses issues related to wildlife conservation) might have a topic distribution with
high mass on the latent    animals    and    environment   , topics. note that the topics are indeed latent     the
name    animal    would be given post-hoc based on the kinds of words that the latent topic would generate,
  i|k. as in section(20.5.2), to control complexity one may use a dirichlet prior to limit the number of topics
active in any particular document:
p(  n|  ) = dirichlet (  n|  )

(20.6.3)

where    is a vector of length the number of topics.

we    rst sample a id203 distribution (histogram)   n that represents the topics likely to occur for this
document. then, for each word-position in the document, sample a topic and subsequently a word from the
distribution of words for that topic. for document n and the wth word-position in the document, vn
w, we
use zn
w     {1, . . . , k} to indicate to which of the k possible topics that word belongs. for each topic k, one
then has a categorical distribution over all the words i = 1, . . . , d, in the dictionary:

p(vn

w = i|zn

w = k,   ) =   i|k

(20.6.4)

for example, the    animal    topic has high id203 to emit animal-like words, etc.

a generative model for sampling a document vn with wn word positions is then given by, see    g(20.14),:

1. choose   n     dirichlet (  n|  )
2. for each of word position vn

(a) choose a topic zn
(b) choose a word vn

w, w = 1, . . . , wn :

w     p (zn

w     p(cid:0)vn

w|  n)
w|    |zn

w

(cid:1)

training the lda model corresponds to learning the parameters   , which relates to the number of topics,
and   , which describes the distribution of words within each topic. unfortunately,    nding the requisite
marginals for learning from the posterior is formally computationally intractable. e   cient approximate in-
ference for this class of models is a topic of research interest and both variational and sampling approaches
have recently been developed[47, 290, 243].

there are close similarities between lda and plsa[126], section(15.6.1), both of which describe a document
in terms of a distribution over latent topics. lda is a probabilistic model for which issues such as setting
hyperparameters can be addressed using maximum likelihood. plsa on the other hand is essentially
a matrix decomposition technique (such as pca). issues such as hyperparameters setting for plsa are
therefore addressed using validation data. whilst plsa is a description only of the training data, lda is
a generative data model and can in principle be used to synthesise new documents.

example 20.5. an illustration of the use of lda is given in    g(20.15)[47]. the documents are taken from
the trec associated press corpus containing 16,333 newswire articles with 23,075 unique terms. after
removing a standard list of stop words (frequent words such as    the   ,   a    etc. that would otherwise dominate
the statistics), the em algorithm (with variational approximate id136) was used to    nd the dirichlet and
conditional categorical parameters for a 100-topic lda model. the top words from four resulting categorical
distributions   i|k are illustrated    g(20.15a). these distributions capture some of the underlying topics in
the corpus. an example document from the corpus is presented along with the words coloured by the most
probable latent topic they correspond to.

20.6.2 graph based representations of data

mixed membership models are used in a variety of contexts and are distinguished also by the form of data
available. here we focus on analysing a representation of the interactions amongst a collection of objects;

434

draft november 9, 2017

mixed membership models

arts
new
   lm
show
music
movie
play
musical
best
actor
   rst
york
opera
theater
actress
love

budgets
million
tax
program
budget
billion
federal
year
spending
new
state
plan
money
programs
government
congress

children education
children
women
people
child
years
families
work
parents
says
family
welfare
men
percent
care
life

school
students
schools
education
teachers
high
public
teacher
bennett
manigat
namphy
state
president
elementary
haiti

(a)

the william randolph hearst foundation will give $ 1.25 million to lin-
coln center, metropolitan opera co., new york philharmonic and juilliard
school. our board felt that we had a real opportunity to make a mark on
the future of the performing arts with these grants an act every bit as
important as our traditional areas of support in health, medical research,
education and the social services, hearst foundation president randolph
a. hearst said monday in announcing the grants. lincoln centers share
will be $200,000 for its new building, which will house young artists and
provide new public facilities. the metropolitan opera co. and new york
philharmonic will receive $400,000 each. the juilliard school, where music
and the performing arts are taught, will get $250,000. the hearst foun-
dation, a leading supporter of the lincoln center consolidated corporate
fund, will make its usual annual $100,000 donation, too.

(b)

(a): a subset of the latent topics discovered by lda and the high id203 words
figure 20.15:
associated with each topic. each column represents a topic, with the topic name such as    arts    assigned by
(b): a document from the training
hand after viewing the most likely words corresponding to the topic.
data in which the words are coloured according to the most likely latent topic. this demonstrates the
mixed-membership nature of the model, assigning the datapoint (document in this case) to several clusters
(topics). reproduced from [47].

1

2

3

(a)

4

5

1

2

3

(b)

4

5

figure 20.16: (a) the social network of a set of 5 individuals, repre-
sented as an undirected graph. here individual 3 belongs to the group
(1, 2, 3) and also (3, 4, 5). (b) by contrast, in graph partitioning, one
breaks the graph into roughly equally sized disjoint partitions such
that each node is a member of only a single partition, with a minimal
number of edges between partitions.

in particular, the data has been processed such that all the information of interest is characterised by an
interaction matrix. for graph based representations of data, two objects are similar if they are neighbours
on a graph representing the data objects. in the    eld of social-networks, for example, each individual is
represented as a node in a graph, with a link between two nodes if the individuals are friends. given a
graph one might wish to identify communities of closely linked friends. interpreted as a social network, in
   g(20.16a), individual 3 is a member of his work group (1, 2, 3) and also the poker group (3, 4, 5). these two
groups of individuals are otherwise disjoint. discovering such groupings contrasts with graph partitioning
in which each node is assigned to only one of a set of subgraphs,    g(20.16b), for which a typical criterion
is that each subgraph should be roughly of the same size and that there are few connections between the
subgraphs[170].

another example is that nodes in the graph represent products and a link between nodes i and j indicates
that customers who buy product i frequently also buy product j. the aim is to decompose the graph into
groups, each corresponding to products that are commonly co-bought by customers[129]. a growing area of
application of graph based representations is in bioinformatics in which nodes represent genes, and a link
between them representing that the two genes have similar activity pro   les. the task is then to identify
groups of similarly behaving genes[5].

20.6.3 dyadic data

consider two kinds of objects, for example,    lms and customers. each    lm is indexed by f = 1, . . . , f and
each user by u = 1, . . . , u . the interaction of user u with    lm f can be described by the element of a matrix
muf representing the rating a user gives to a    lm. a dyadic dataset consists of such a matrix and the aim
is to decompose this matrix to explain the ratings by    nding types of    lms and types of user.

another example is to consider a collection of documents, summarised by an interaction matrix in which
mwd is 1 if word w appears in document d and zero otherwise. this matrix can be represented as a bipartite
upper nodes words, with a link
graph, as in    g(20.17a). the

lower nodes represent documents, and the

@@
@@

draft november 9, 2017

435

mixed membership models

(a)

(b)

(a): there are 6 documents and 13 words. a link
figure 20.17: graphical representation of dyadic data.
represents that a particular word-document pair occurs in the dataset. (b): a latent decomposition of (a)
using 3    topics   . a topic corresponds to a collection of words, and each document a collection of topics. the
open nodes indicate latent variables.

between them if that word occurs in that document. one then seeks assignments of documents to groups
or latent    topics    to succinctly explain the link structure of the bipartite graph via a small number of latent
nodes, as schematically depicted in    g(20.17b). one may view this as a form of matrix factorisation, as in
plsa section(15.6.1) [149, 205]

uwtv t
td

(20.6.5)

(cid:88)

t

mwd    

where t indexes the topics and the feature matrices u and v control the word-to-topic mapping and the topic-
to-document mapping. this di   ers from id44 which has a probabilistic interpretation
of    rst generating a topic and then a word, conditional on the chosen topic. here the interaction between
document-topic matrix v and word-topic matrix u is non-probabilistic.
in [205], real-valued data is modelled using

(cid:16)

(cid:17)

p(m|u, w, v) = n

m uwvt,   2i

(20.6.6)

where u and v are assumed binary and the real-valued w is a topic-interaction matrix. in this viewpoint
learning then consists of inferring u,w,v, given the dyadic observation matrix m. assuming factorised
priors, the posterior over the matrices is

p(u, w, v|m)     p(m|u, w, v)p(u)p(w)p(v)

(20.6.7)

a convenient choice is a gaussian prior distribution for w, with the feature matrices u and v sampled
from beta-bernoulli priors. the resulting posterior distribution is formally computationally intractable, and
in [205] this is addressed using a sampling approximation.

20.6.4 monadic data

in monadic data there is only one type of object and the interaction between the objects is represented by
a square interaction matrix. for example one might have a matrix with elements aij = 1 if proteins i and j
can bind to each other and 0 otherwise. a depiction of the interaction matrix is given by a graph in which an
edge represents an interaction, for example    g(20.18). in the following section we discuss a particular mixed
membership model and highlight potential applications. the method is based on clique decompositions of
graphs and as such we require a short digression into clique-based graph representations.

1

2

3

4

436

figure 20.18: the minimal clique cover is (1, 2, 3), (2, 3, 4).

draft november 9, 2017

mixed membership models

1

2

3

4

1

2

3

4

(a)

(b)

figure 20.19: bipartite representations of the decomposi-
tions of    g(20.18). shaded nodes represent observed vari-
ables, and open nodes latent variables. (a) incidence matrix
representation. (b) minimal clique decomposition.

20.6.5 cliques and adjacency matrices for monadic binary data

a symmetric adjacency matrix has elements aij     {0, 1}, with a 1 indicating a link between nodes i and j.
for the graph in    g(20.18), the adjacency matrix is

where we include self connections on the diagonal. given a, our aim is to    nd a    simpler    description that
reveals the underlying cluster structure, such as (1, 2, 3) and (2, 3, 4) in    g(20.18). given the undirected
graph in    g(20.18), the incidence matrix finc is an alternative description of the adjacency structure[87].
given the v nodes in the graph, we construct finc as follows: for each link i     j in the graph, form a
column of the matrix finc with zero entries except for a 1 in the ith and jth row. the column ordering is
arbitrary. for example, for the graph in    g(20.18) an incidence matrix is

the incidence matrix has the property that the adjacency structure of the original graph is given by the
outer product of the incidence matrix with itself. the diagonal entries contain the degree (number of links)
of each node. for our example, this gives

          1 1

1 1
1 1
0 1

         

1
1
1
1

0
1
1
1

a =

          1

1
0
0

         

1 0 0
0 1 1
1 1 0
0 0 1

0
0
1
1

finc =

         

1
1
3
1

0
1
1
2

          2 1
(cid:17)

1 3
1 1
0 1

fincft
inc

fincft

inc =

(cid:16)

so that

a = h

here h(  ) is the element-wise heaviside step function, [h(m)]ij = 1 if mij > 0 and is 0 otherwise. a useful
viewpoint of the incidence matrix is that it identi   es two-cliques in the graph (here we are using the term
   clique    in the non-maximal sense). there are    ve 2-cliques in    g(20.18), and each column of finc speci   es
which elements are in each 2-clique. graphically we can depict this incidence decomposition as a bipartite
graph, as in    g(20.19a) where the open nodes represent the    ve 2-cliques. the incidence matrix can be
generalised to describe larger cliques. consider the following matrix as a decomposition for    g(20.18), and
its outer-product:

          1

1
1
0

         ,

0
1
1
1

f =

fft =

          1 1

1 2
1 2
0 1

         

1
2
2
1

0
1
1
1

the interpretation is that f represents a decomposition into two 3-cliques. as in the incidence matrix, each
column represents a clique, and the rows containing a    1    express which elements are in the clique de   ned
by that column. this decomposition can be represented as the bipartite graph of    g(20.19b). for the graph
of    g(20.18), both finc and f satisfy

(cid:16)

fft(cid:17)

(cid:16)

a = h

= h

fincft
inc

(cid:17)

draft november 9, 2017

(20.6.8)

(20.6.9)

(20.6.10)

(20.6.11)

(20.6.12)

(20.6.13)

437

mixed membership models

figure 20.20: the function   (x)    
   increases, this sigmoid function tends to a step function.

for    = 1, 10, 100. as

(cid:0)1 + e  (0.5   x)(cid:1)   1

one can view equation (20.6.13) as a form of matrix factorisation of the binary square (symmetric) matrix a
into non-square binary matrices. for our id91 purposes, the decomposition using f is to be preferred
to the incidence decomposition since f decomposes the graph into a smaller number of larger cliques. a
formal speci   cation of the problem of    nding a minimum number of maximal fully-connected subsets is the
computationally hard problem min clique cover[114, 268].

(cid:2)fft(cid:3)

ii express the number of cliques/columns that node i occurs in. o   -diagonal elements (cid:2)fft(cid:3)

de   nition 20.1 (clique matrix). given an adjacency matrix [a]ij , i, j = 1, . . . , v (aii = 1), a clique
matrix f has elements fic     {0, 1} , i = 1, . . . , v, c = 1, . . . , c such that a = h(fft). diagonal elements
contain the number of cliques/columns that nodes i and j jointly inhabit [19].

ij

whilst    nding a clique decomposition f is easy (use the incidence matrix for example),    nding a clique
decomposition with the minimal number of columns, i.e. solving min clique cover, is np-hard[114, 10].
in [131] the cliques are required to be maximal, although in our de   nition the cliques may be non-maximal.

a generative model of adjacency matrices

given an adjacency matrix a and a prior on clique matrices f, our interest is the posterior

p(f|a)     p(a|f)p(f)

(20.6.14)
we    rst concentrate on the generative term p(a|f). to    nd    well-connected    clusters, we relax the constraint
that the decomposition is in the form of perfect cliques in the original graph and view the absence of links
as statistical    uctuations away from a perfect clique. given a v    c matrix f, we desire that the higher
the overlap between rows2 fi and fj is, the greater the id203 of a link between i and j. this may be
achieved using, for example,

p(aij = 1|f) =   

(cid:16)

(cid:17)
1 + e  (0.5   x)(cid:17)   1

fif t
j

(cid:16)

with

  (x)    

(20.6.15)

(20.6.16)

where    controls the steepness of the function, see    g(20.20). the 0.5 shift in equation (20.6.16) ensures
that    approximates the step-function since the argument of    is an integer. under equation (20.6.15), if
fi and fj have at least one    1    in the same position, fif t
j     0.5 > 0 and p(aij = 1|f) is high. absent links
contribute p(aij = 0|f) = 1     p(aij = 1|f). the parameter    controls how strictly   (fft) matches a; for
large   , very little    exibility is allowed and only cliques will be identi   ed. for small   , subsets that would
be cliques if it were not for a small number of missing links, are clustered together. the setting of    is user
and problem dependent.

assuming each element of the adjacency matrix is sampled independently from the generating process, the
joint id203 of observing a is (neglecting its diagonal elements),

(cid:17)(cid:105)aij(cid:104)

(cid:16)

(cid:89)

(cid:104)

i,j

  

fif t
j

(cid:16)

(cid:17)(cid:105)1   aij

1       

fif t
j

p(a|f) =

(20.6.17)

2we use lower indices fi to denote the ith row of f.

438

draft november 9, 2017

   2   1.5   1   0.500.511.5200.20.40.60.81xmixed membership models

(a)

(b)

(c)

(a): adjacency ma-
figure 20.21:
trix of 105 political books (black=1).
(b): clique matrix: 521 non-zero en-
(c): adjacency reconstruction
tries.
using an approximate clique matrix
with 10 cliques     see also    g(20.22)
and democliquedecomp.m.

the ultimate quantity of interest is the posterior distribution of clique structure, equation (20.6.14), for
which we now specify a prior p(f) over clique matrices.

clique matrix prior p(f)

since we are interested in id91, ideally we want to place as many nodes in the graph as possible in a
cluster. this means that we wish to bias the contributions to the adjacency matrix a to occur from a small
number of columns of f. to achieve this we    rst reparameterise f as

f =(cid:0)  1f 1, . . . ,   cmaxf cmax(cid:1)

where   c     {0, 1} play the role of indicators and f c is column c of f. cmax is an assumed maximal number
of clusters. ideally, we would like to    nd an f with a low number of indicators   1, . . . ,   cmax in state 1. to
achieve this we de   ne a prior distribution on the binary hypercube    = (  1, . . . ,   cmax),

p(  |  ) =

    c (1       )1     c

(20.6.19)

to encourage a small number of the   (cid:48)
that    is less than 0.5. this gives rise to a beta-bernoulli distribution

cs to be 1, we use a beta prior p(  ) with suitable parameters to ensure

(cid:89)

c

(cid:90)

(20.6.18)

(20.6.20)

p(  ) =

where b(a, b) is the beta function and n =(cid:80)cmax

p(  |  )p(  ) =

b(a, b)

b(a + n, b + cmax     n )

  

c=1   c is the number of indicators in state 1. to encour-
age that only a small number of components should be active, we set a = 1, b = 3. the distribution
(20.6.20) is on the vertices of the binary hypercube {0, 1}cmax with a bias towards vertices close to the ori-
gin (0, . . . , 0). through equation (20.6.18), the prior on    induces a prior on f. the resulting distribution
p(f,   |a)     p(f|  )p(  ) is formally intractable and in [19] this is addressed using a variational technique.
clique matrices also play a natural role in the parameterisation of positive de   nite matrices under the
constraint of speci   ed zeros in the matrix, see exercise(20.7)[19].

example 20.6 (political books id91). the data consists of 105 books on us politics sold by the
online bookseller amazon. the adjacency matrix with element aij = 1    g(20.21a), represents frequent
co-purchasing of books i and j (from valdis krebs). additionally, books are labelled    liberal   ,    neutral   ,
or    conservative    according to the judgement of a politically astute reader. the interest is to assign books
to clusters, using a alone, and then see if these clusters correspond in some way to the ascribed political
leanings of each book. note that the information here is minimal     all that is known to the id91
algorithm is which books were co-bought (matrix a); no other information on the content or title of the
books are exploited by the algorithm. with an initial cmax = 200 cliques, beta parameters a = 1, b = 3 and
steepness    = 10, the most probable posterior marginal solution contains 142 cliques    g(20.21b), giving a
perfect reconstruction of the adjacency a. for comparison, the incidence matrix has 441 2-cliques. however,
this clique matrix is too large to provide a compact interpretation of the data     indeed there are more clusters
than books. to cluster the data more aggressively, we    x cmax = 10 and re-run the algorithm. this results
only in an approximate clique decomposition, a     h(fft), as plotted in    g(20.21c). the resulting 105  10

draft november 9, 2017

439

204060801002040608010020406080100120140204060801002040608010020406080100mixed membership models

figure 20.22: political books. 105    10 dimensional clique matrix broken into 3 groups by a politically
astute reader. a black square indicates q(fic) > 0.5. liberal books (red), conservative books (green),
neutral books(yellow). by inspection, cliques 5,6,7,8,9 largely correspond to    conservative    books.

approximate clique matrix is plotted in    g(20.22) and demonstrates how individual books are present in
more than one cluster. interestingly, the clusters found only on the basis of the adjacency matrix have some
correspondence with the ascribed political leanings of each book; cliques 5, 6, 7, 8, 9 correspond to largely
   conservative    books. most books belong to more than a single clique/cluster, suggesting that they are not
single topic books, consistent with the assumption of a mixed membership model.

20.7 summary

    mixture models are discrete latent variable models and can be trained using maximum likelihood.
    a classical approach to training is to use the em algorithm, though gradient based approaches are also

possible.

    standard mixture models assume that a priori each object (datapoint) can be a member of only a single

cluster.

    in mixed-membership models, an object may a priori belong to more than a single cluster. models such as
id44 have interesting application for example to text modelling including the automatic
discovery of latent topics.

    mixed-membership models may also be considered for monadic or dyadic data.

the literature on mixture modelling is extensive, and a good overview and entrance to the literature is
contained in [204].

20.8 code

mixprodbern.m: em training of a mixture of product bernoulli distributions
demomixbernoulli.m: demo of a mixture of product bernoulli distributions

gmmem.m: em training of a mixture of gaussians
gmmloglik.m: gmm log likelihood
demogmmem.m: demo of a em for mixture of gaussians

440

draft november 9, 2017

05101000 years for revenge  bush vs. the beltway  charlie wilson   s war  losing bin laden  sleeping with the devil  the man who warned america  why america slept  ghost wars  a national party no more  bush country  dereliction of duty  legacy  off with their heads  persecution  rumsfeld   s war  breakdown  betrayal  shut up and sing  meant to be  the right man  ten minutes from normal  hillary   s scheme  the french betrayal of america  tales from the left coast  hating america  the third terrorist  endgame  spin sisters  all the shah   s men  dangerous dimplomacy  the price of loyalty  house of bush, house of saud  the death of right and wrong  useful idiots  the o   reilly factor  let freedom ring  those who trespass  bias  slander  the savage nation  deliver us from evil  give me a break  the enemy within  the real america  who   s looking out for you?  the official handbook vast right wing conspiracy  power plays  arrogance  the perfect wife  the bushes  things worth fighting for  surprise, security, the american experience  allies  why courage matters  hollywood interrupted  fighting back  we will prevail  the faith of george w bush  rise of the vulcans  downsize this!  stupid white men  rush limbaugh is a big fat idiot  the best democracy money can buy  the culture of fear  america unbound  the choice  the great unraveling  rogue nation  soft power  colossus  the sorrows of empire  against all enemies  american dynasty  big lies  the lies of george w. bush  worse than watergate  plan of attack  bush at war  the new pearl harbor  bushwomen  the bubble of american supremacy  living history  the politics of truth  fanatics and fools  bushwhacked  disarming iraq  lies and the lying liars who tell them  moveon   s 50 ways to love your country  the buying of the president 2004  perfectly legal  hegemony or survival  the exception to the rulers  freethinkers  had enough?  it   s still the economy, stupid!  we   re right they   re wrong  what liberal media?  the clinton wars  weapons of mass deception  dude, where   s my country?  thieves in high places  shrub  buck up suck up  the future of freedom  empire  exercises

demogmmclass.m: demo gmm for classi   cation

kmeans.m: id116
demokmeans.m: demo of id116

demopolya.m: demo of the number of active clusters from a polya distribution
dirrnd.m: dirichlet random distribution generator

cliquedecomp.m: clique matrix decomposition
cliquedecomp.c: clique matrix decomposition (c-code)
democliquedecomp.m: demo clique matrix decomposition

20.9 exercises

exercise 20.1. consider a mixture of factorised models for vector observations v

(cid:88)

(cid:89)

p(v) =

p(h)

h

i

p(vi|h)

(20.9.1)

for assumed i.i.d. data vn, n = 1, . . . , n , some observation components may be missing so that, for example
the third component of the    fth datapoint, v5
3 is unknown. show that maximum likelihood training on the
observed data corresponds to ignoring components vn

i that are missing.

exercise 20.2. derive the optimal em update for    tting a mixture of gaussians under the constraint that
the covariances are diagonal.

exercise 20.3. consider a mixture of k isotropic gaussians, each with the same covariance, si =   2i. in
the limit   2     0 show that the em algorithm tends to the id116 id91 algorithm.
exercise 20.4. consider the term

(cid:104)log p(h)(cid:105)pold(h|vn)

(20.9.2)

we wish to optimise the above with respect to the distribution p(h). this can be achieved by de   ning the
lagrangian

n(cid:88)

n=1

n(cid:88)

(cid:32)

(cid:88)

(cid:33)

l =

(cid:104)log p(h)(cid:105)pold(h|vn) +   

by di   erentiating the lagrangian with respect to p(h) and using the normalisation constraint(cid:80)

1    

p(h)

n=1

h

(20.9.3)

h p(h) = 1,

(20.9.4)

show that, optimally

n(cid:88)

n=1

p(h) =

1
n

pold(h|vn)

exercise 20.5. we showed that    tting an unconstrained mixture of gaussians using maximum likelihood is
problematic since, by placing one of the gaussians over a datapoint and letting the covariance determinant
go to zero, we obtain an in   nite likelihood. in contrast, when    tting a single gaussian n (x   ,   ) to i.i.d.
data x1, x2, . . . , xn show that the maximum likelihood optimum for    has non-zero determinant, and that
the optimal likelihood remains    nite.

exercise 20.6. modify gmmem.m suitably so that it can deal with the semi-supervised scenario in which the
mixture component h of some of the observations v is known.

draft november 9, 2017

441

exercise 20.7. you wish to parameterise covariance matrices s under the constraint that speci   ed elements
are zero. the constraints are speci   ed using a matrix a with elements aij = 0 if sij = 0 and aij = 1
otherwise. consider a clique matrix z, for which

exercises

a = h(zzt)

and matrix

s    = z   zt   

with

[z   ]ij =

(cid:26) 0

  ij

if zij = 0
if zij = 1

(20.9.5)

(20.9.6)

(20.9.7)

for parameters   . show that for any   , s    is positive semide   nite and parameterises covariance matrices
under the zero constraints speci   ed by a.

442

draft november 9, 2017

chapter 21

latent linear models

in this chapter we discuss some simple continuous latent variable models. the factor analysis model is
a classical statistical model and is essentially a probabilistic version of pca. such models can be used
to form simple low-dimensional generative models of data. as an example we consider an application to
face recognition. by extending the model to use non-gaussian priors on the latent variables, independent
dimensions underlying the data can be discovered, and can give rise to radically di   erent low dimensional
representations of the data than a   orded by pca.

21.1 factor analysis

in chapter(15) we discussed principal components analysis which forms lower dimensional representations
of data based on assuming that the data lies close to a linear subspace. here we describe a related prob-
abilistic model for which extensions to bayesian methods can be envisaged. any probabilistic model may
also be used as a component of a larger more complex model, such as a mixture model, enabling natural
generalisations.

we use v to describe a real data vector to emphasise that this is a visible (observable) quantity. the dataset
is then given by a set of vectors,

(21.1.1)

where dim (v) = d. our interest is to    nd a lower dimensional probabilistic description of this data. if
data lies close to a h-dimensional linear subspace we may accurately approximate each datapoint by a low
h-dimensional coordinate system. in general, datapoints will not lie exactly on the linear subspace and
we model this discrepancy with gaussian noise. mathematically, the fa model generates an observation v
according to, see    g(21.1),

v = fh + c +  

where the noise   is gaussian distributed, with zero mean and covariance   

      n (  0,   )

(21.1.2)

(21.1.3)

the constant bias c sets the origin of the coordinate system1. the d    h factor loading matrix f plays a
similar role as the basis matrix in pca. similarly, the hidden coordinates h plays the role of the components
we used in section(15.2). the di   erence between pca and factor analysis is in the choice of   :

1depending on the application, it can be sometimes useful to force the origin to be zero to aid the interpretation of the

factors; this results in only a minor modi   cation of the framework.

443

v =(cid:8)v1, . . . , vn(cid:9)

h1

h2

h3

v1

v2

v3

v4

v5

figure 21.1: factor analysis. the visible vector variable v is related
to the vector hidden variable h by a linear mapping, with independent
additive gaussian noise on each visible variable. the prior on the
hidden variable may be taken to be an isotropic gaussian, thus being
independent across its components.

factor analysis

probabilistic pca

   =   2i

factor analysis

   = diag (  1, . . . ,   d)

(21.1.4)

(21.1.5)

factor analysis therefore di   ers from probabilistic pca in that it has a richer description for the o   -subspace
noise   .

a probabilistic description

from equation (21.1.2) and equation (21.1.3), given h, the data is gaussian distributed with mean fh + c
and covariance   

(cid:19)
(v     fh     c)t      1 (v     fh     c)

1
2

to complete the model, we need to specify the hidden distribution p(h). a convenient choice is a gaussian

(cid:18)

(cid:17)

(cid:16)

p (v|h) = n (v fh + c,   )     exp

   

p (h) = n (h 0, i)     exp

   hth/2

under this prior the coordinates h will be preferentially concentrated around values close to 0. if we sample
a h vector from p(h) and then draw a value for v using p(v|h), the sampled v vectors would produce a saucer
or    pancake    of points in the v space, see    g(21.2). using a correlated gaussian prior p(h) = n (h 0,   h ) has
no e   ect on the    exibility of the model since   h can be absorbed into f, exercise(21.3). since v is linearly
related to h through equation (21.1.2) and both   and h are gaussian, then v is gaussian distributed. the
mean and covariance can be computed using propagation, result(8.3):

(cid:90)

(cid:16)

(cid:17)

p (v) =

p (v|h) p (h) dh = n

v c, fft +   

what this says is that our model for the data is a gaussian centred on c with covariance matrix constrained
to be of the form fft +   . the number of free parameters for factor analysis is therefore d(h + 1).
compared to an unconstrained covariance on v which has d(d + 1)/2 free parameters, by choosing h (cid:28) d,
we have signi   cantly fewer parameters to estimate in factor analysis than in unconstrained covariance case.

invariance of the likelihood under factor rotation

since the matrix f only appears in the    nal model p(v) through fft +   , the likelihood is unchanged if
we rotate f using fr, with rrt = i:

fr(fr)t +    = frrtft +    = fft +   

(21.1.9)

the solution space for f is therefore not unique     we can arbitrarily rotate the matrix f and produce an
equally likely model of the data. some care is therefore required when interpreting the entries of f. varimax
provides a more interpretable f by using a suitable rotation matrix r. the aim is to produce a rotated
f for which each column has only a small number of large values. finding a suitable rotation results in a
non-linear optimisation problem and needs to be solved numerically. see [200] for details.

444

draft november 9, 2017

(21.1.6)

(21.1.7)

(21.1.8)

factor analysis : maximum likelihood

(a)

(b)

(a): 1000 latent two-dimensional
figure 21.2: factor analysis: 1000 points generated from the model.
points hn sampled from n (h 0, i). these are transformed to a point on the three-dimensional plane by
0 = c + fhn. the covariance of x0 is degenerate, with covariance matrix fft.
(b): for each point xn
xn
0
on the plane a random noise vector is drawn from n (  0,   ) and added to the in-plane vector to form a
sample xn, plotted in red. the distribution of points forms a    pancake    in space. points    underneath    the
plane are not shown.

21.1.1 finding the optimal bias

for a set of data v and using the usual i.i.d. assumption, the log likelihood is

n(cid:88)

n=1

log p(vn) =    

1
2

n(cid:88)

n=1

(vn     c)t      1

d (vn     c)    

n
2

log det (2    d)

(21.1.10)

(21.1.11)

di   erentiating equation (21.1.10) with respect to c and equating to zero, we arrive at the maximum likelihood
optimal setting that the bias c is the mean of the data,

vn       v

(21.1.12)

we will use this setting throughout. with this setting the log likelihood equation (21.1.10) can be written

log p(v|f,   ) =    

n
2

(cid:0)trace(cid:0)     1

d s(cid:1) + log det (2    d)(cid:1)

log p(v|f,   , c) =

where

  d     fft +   

n(cid:88)

n=1

c =

1
n

where s is the sample covariance matrix

n(cid:88)

n=1

s =

1
n

(v       v) (v       v)t

(21.1.13)

(21.1.14)

21.2 factor analysis : maximum likelihood

we now specialise to the assumption that    = diag (  1, . . . ,   d). we consider two methods for learning
the factor loadings f: the    eigen    approach2 of section(21.2.1) and the em approach, section(21.2.2). the
eigen-approach is common in statistics and software packages, whilst the em approach is more common in
machine learning.

2the presentation here follows closely that of [324].

draft november 9, 2017

445

   4   20246   4   20246   2   101234   4   20246   4   20246   2   101234factor analysis : maximum likelihood

21.2.1 eigen-approach likelihood optimisation

as we will show, if the noise matrix    is given, we can    nd the optimal factor matrix f by solving an
eigen-problem. if the    is unknown, from a starting guess for   , we can    nd the optimal f and use this to
reestimate the noise   , iterating this two-step process until convergence. this section is necessarily rather
technical and can be skipped on    rst reading.

optimal f for    xed   

to    nd the maximum likelihood setting of f we di   erentiate the log likelihood equation (21.1.13) with
respect to f and equate to zero. this gives

0 = trace(cid:0)     1

d (   f  d)     1

d s(cid:1)

    trace(cid:0)     1

d    f  d

(cid:1)

using

   f(  d)=    f(fft) = f(   fft) + (   ff)ft

a stationary point is given when

     1
d f =      1

d s     1
d f

the optimal f therefore satis   es

f = s     1
d f

(cid:16)
(cid:17)

     1
d f =      1f
(cid:16)

i + ft     1f

f

(cid:17)   1

i + ft     1f

= s     1f

  s =       1

2 s      1

2

2 f,

  f           1
(cid:16)

(cid:17)

  f

i +   ft   f

=   s   f

using the reparameterisations

equation (21.2.6) can be written in the    isotropic    form

using the de   nition of   d, equation (21.1.11), one can rewrite      1

d f as (see exercise(21.4))

plugging this into the zero derivative condition, equation (21.2.4) can be rearranged to

we assume that the transformed factor matrix   f has a thin svd decomposition

  f = uh lwt

where dim (uh ) = d    h, dim (l) = h    h, dim (w) = h    h and

ut

h uh = ih ,

wtw = ih

and l = diag (l1, . . . , lh ) are the singular values of   f. plugging this assumption into equation (21.2.8) we
obtain

uh lwt(cid:16)
ih + wl2wt(cid:17)
(cid:0)ih + l2(cid:1) =   suh ,

uh

which gives

=   suh lwt

l2 = diag(cid:0)l2

1, . . . , l2
h

(cid:1)

(21.2.11)

(21.2.12)

equation(21.2.12) is then an eigen-equation for uh . intuitively, it   s clear that we need to    nd then the
eigen-decomposition of   s and then set the columns of uh to those eigenvectors corresponding to the largest
eigenvalues. this is derived more formally below.

446

draft november 9, 2017

(21.2.1)

(21.2.2)

(21.2.3)

(21.2.4)

(21.2.5)

(21.2.6)

(21.2.7)

(21.2.8)

(21.2.9)

(21.2.10)

(21.2.14)

(21.2.15)

+ log det (2    )

(21.2.16)

factor analysis : maximum likelihood

determining the appropriate eigenvalues

we can relate the form of the solution to the eigen-decomposition of   s

  s = u  ut,

u = [uh|ur]

(21.2.13)

where ur are arbitrary additional columns chosen to complete uh to form an orthogonal u, utu = uut =
i =   i, or li =      i     1, i = 1, . . . , h.
i. using    = diag (  1, . . . ,   d), equation (21.2.12) stipulates 1 + l2
given the solution for   f, the solution for f is found from equation (21.2.7). to determine the optimal   i
we write the log likelihood in terms of the   i as follows. using the new parameterisation,

(cid:16)

(cid:17)

  d =   

1
2

  f   ft + i

and s =   

1

2   s  

trace(cid:0)     1

1
2 , we have

d s(cid:1) = trace

1
2

  

(cid:18)(cid:16)

(cid:17)   1

(cid:19)

  s

  f   ft + id

the log likelihood equation (21.1.10) in this new parameterisation is

(cid:18)(cid:16)

id +   f   ft(cid:17)   1

(cid:19)

  s

+ log det

(cid:16)

id +   f   ft(cid:17)

2
n

   

log p(v|f,   ) = trace

using   i = 1 + l2

i , and equation (21.2.9) we can write

h = udiag (  1, . . . ,   h , 1, . . . , 1) ut

id +   f   ft = id + uh l2ut

so that the inverse of this matrix is given by udiag(cid:0)  
(cid:26)   i

id +   f   ft(cid:17)   1

(cid:88)

(cid:18)(cid:16)

(cid:19)

trace

=

  s

  

(cid:48)
i =

,

h , 1, . . . , 1(cid:1) ut and hence

   1

   1
1 , . . . ,   

i     h
i > h

1

  i
  (cid:48)

i

i

similarly

log det

(cid:16)

id +   f   ft(cid:17)

=

h(cid:88)

i=1

log   i

(21.2.17)

(21.2.18)

(21.2.19)

using this we can write the log likelihood as a function of the eigenvalues (for    xed   ) as

h(cid:88)

i=1

d(cid:88)

i=h+1

2
n

   

log p(v|f,   ) =

place the largest h eigenvalues in the(cid:80)

to maximise the likelihood we need to minimise the right hand side of the above. since log    <    we should

i log   i term. a solution for    xed    is therefore

f =   

1

2 uh (  h     ih )

1
2 r

where

  h     diag (  1, . . . ,   h )

(21.2.21)

(21.2.22)

are the h largest eigenvalues of       1
r is an arbitrary orthogonal matrix.

2 s      1

2 , with uh being the matrix of the corresponding eigenvectors.

draft november 9, 2017

447

log   i + h +

  i + log det (2    )

(21.2.20)

algorithm 21.1 factor analysis training using svd for n d-dimensional datapoints v1, . . . , vn . h is the
latent number of factors required.

factor analysis : maximum likelihood

1: initialise the diagonal noise   
2: find the mean   v of the data v1, . . . , vn
3: find the variance   2

4: compute the centred matrix x =(cid:2)v1       v, . . . , xn       v(cid:3)

i for each component i of the data v1

i , . . . , vn
i

5: while likelihood not converged or termination criterion not reached do
6:

2 x/   n
form the scaled data matrix   x =       1
2
perform svd for   x = u        wt and set    =     
set uh to the    rst h columns of u and set   h to contain the    rst h diagonal entries of   .
f =   
l =     n

(cid:110)(cid:80)h
i=1 log   i + h +(cid:80)d
    diag(cid:0)fft(cid:1)
   = diag(cid:0)  2(cid:1)

i=h+1   i + log det (2    )

2 uh (  h     ih )

(cid:111)

7:
8:

9:

10:

1
2

2

1

11:
12: end while

(cid:46) factor update
(cid:46) log likelihood

(cid:46) noise update

svd based approach
rather than    nding the eigen-decomposition of       1
considering the thin svd decomposition of

2 s      1

2 we can avoid forming the covariance matrix by

  x =

      1

2 x

1
   n

where the centred data matrix is

(cid:2)v1       v, . . . , xn       v(cid:3)

x    

given a thin decomposition

  x = uh        wt

(21.2.23)

(21.2.24)

(21.2.25)

we obtain the eigenvalues   i =     2
are available[53].

ii. when the matrix x is too large to store in memory, online svd methods

finding the optimal   

the zero derivative of the log likelihood equation (21.1.13) with respect to    occurs when

where f is given by equation (21.2.21). there is no closed form solution to equations(21.2.26, 21.2.21). a
simple iterative scheme is to    rst guess values for the diagonal entries of    and then    nd the optimal f
using equation (21.2.21). subsequently    is updated using

(cid:16)

s     fft(cid:17)
(cid:16)
s     fft(cid:17)

   = diag

  new = diag

(21.2.26)

(21.2.27)

we update f using equation (21.2.21) and    using equation (21.2.27) until convergence, see algorithm(21.1).

alternative schemes for updating the noise matrix    can improve convergence considerably. for example
updating only a single component of    with the rest    xed can be achieved using a closed form expression[324].

21.2.2 expectation maximisation

an alternative way way to train factor analysis that is popular in machine learning is to use em, sec-
tion(11.2). we assume that the bias c has been optimally set to the data mean   v.

448

draft november 9, 2017

factor analysis : maximum likelihood

m-step

as usual, we need to consider the energy which, neglecting constants, is

n(cid:88)

(cid:28) 1

2

n=1

(cid:29)

e (f,   ) =    

(dn     fh)t      1 (dn     fh)

q(h|vn)    

n
2

log det (  )

(21.2.28)

where dn     vn       v. the optimal variational distribution q(h|vn) is determined by the e-step below.
maximising e (f,   ) with respect to f gives

f = ah   1

where

a    

1
n

finally

   =

1
n

(cid:88)

n

(cid:88)

n

1
n

n

h    

q(h|vn) ,

dn (cid:104)h(cid:105)t
(cid:18)(cid:68)
(dn     fh) (dn     fh)t(cid:69)

diag

(cid:88)

(cid:68)
hht(cid:69)
(cid:19)

q(h|vn)

q(h|vn)

(cid:32)

(cid:88)

n

1
n

= diag

(21.2.29)

(21.2.30)

(cid:33)

dn(dn)t     2fat + fhft

(21.2.31)

note that the new f is used in the above update for the covariance.

e-step

the above updates depend on the statistics (cid:104)h(cid:105)q(h|vn) and(cid:10)hht(cid:11)

the e-step we have

q(h|vn). using the em optimal choice for

with

q(h|vn)     p(vn|h)p(h) = n (h mn,   )
(cid:17)   1

(cid:16)

i + ft     1f

ft     1dn,

(cid:16)

   =

i + ft     1f

(cid:17)   1

using these results we can express the statistics in equation (21.2.30) as

mn = (cid:104)h(cid:105)q(h|vn) =
(cid:88)

h =    +

1
n

n

mn(mn)t

(21.2.32)

(21.2.33)

(21.2.34)

equations (21.2.29,21.2.31,21.2.33) are iterated till convergence. as for any em algorithm, the likelihood
equation (21.1.10) (under the diagonal constraint on   ) increases at each iteration. convergence using this
em technique can be slower than that of the eigen-approach of section(21.2.1). provided however that a
reasonable initialisation is used, the performance of the two training algorithms can be similar. a useful
initialisation is to use pca and then set f to the principal directions.

mixtures of fa

an advantage of probabilistic models is that they may be used as components in more complex models, such
as mixtures of fa[294]. training can then be achieved using em or gradient based approaches. bayesian
extensions are clearly of interest; whilst formally intractable they can be addressed using approximate
methods, for example [106, 193, 120].

draft november 9, 2017

449

g
  x12

  x13

f1

  x11

g

  x21

f2

  

  x22

origin

interlude: modelling faces

f

figure 21.3: latent identity model. the mean    represents
the mean of the faces. the subspace f represents the direc-
tions of variation of di   erent faces so that f1 =    + fh1 is a
mean face for individual 1, and similarly for f2 =    + fh2.
the subspace g denotes the directions of variability for
any individual face, caused by pose, lighting etc. this vari-
ability is assumed the same for each person. a particular
mean face is then given by the mean face of the person plus
pose/illumination variation, for example   x12 = f1 + gw12.
a sample face is then given by a mean face   xij plus gaussian
noise from n ( ij 0,   ).

21.3 interlude: modelling faces

factor analysis has widespread application in statistics and machine learning. as an inventive application
of fa, highlighting the probabilistic nature of the model, we describe a face modelling technique that has as
its heart a latent linear model[246]. consider a gallery of face images x = {xij, i = 1, . . . , i; j = 1, . . . , j} so
that the vector xij represents the jth image of the ith person. as a latent linear model of faces we consider

xij =    + fhi + gwij +  ij

(21.3.1)

here f (dim f = d    f ) is used to model variability between people, and g (dim g = d    g) models
variability related to pose, illumination etc. within the di   erent images of each person. the contribution

fi        + fhi

(21.3.2)

accounts for variability between di   erent people, being constant for individual i. for    xed i, the contribution

gwij +  ij,

with  ij     n ( ij 0,   )

(21.3.3)

accounts for the variability over the images of person i, explaining why two images of the same person do not
look identical. see    g(21.3) for a graphical representation. as a probabilistic linear latent variable model,
we have for an image xij:

p(xij|hi, wij,   ) = n (xij    + fhi + gwij,   )

p(hi) = n (hi 0, i) ,

p(wij) = n (wij 0, i)

the parameters are    = {f, g,   ,   }. for the collection of images, assuming i.i.d. data,

          j(cid:89)

j=1

i(cid:89)

i=1

          p(hi)

p(x , w, h|  ) =

p(xij|hi, wij,   )p(wij)

(21.3.4)

(21.3.5)

(21.3.6)

wij

hi

  

xij

j

i

figure 21.4: the jth image of the ith person, xij, is modelled
using a linear latent model with parameters   .

450

draft november 9, 2017

interlude: modelling faces

(a) mean

(b) variance

(c)

(d)

(e)

(f)

(g)

(h)

figure 21.5: latent identity model of face images. each image is represented by a 70    70    3 vector (the 3
comes from the rgb colour coding). there are i = 195 individuals in the database and j = 4 images per
person. (a): mean of the data. (b): per pixel standard deviation     black is low, white is high. (c,d,e):
three directions from the between-individual subspace f.
(f,g,h): three samples from the model with h
   xed and drawing randomly from w in the within-individual subspace g. reproduced from [246].

for which the graphical model is depicted in    g(21.4). the task of learning is then to maximise the likelihood

p(x|  ) =

w,h

p(x , w, h|  )

(21.3.7)

this model can be seem as a constrained version of factor analysis by using stacked vectors (here for only
a single individual, i = 1)

                x11

x12
...
x1j

                  

  
...
  

                +

                f g 0 . . . 0

f 0 g . . . 0
...
...
...
...
0 . . . g
f 0

. . .

               

                     

h1
w11
w12
...
w1j

                      +

                 11

 12
...
 1j

               

(21.3.8)

the generalisation to multiple individuals i > 1 is straightforward. the model can be trained using either a
constrained form of the eigen method, or em as described in [246]. example images from the trained model
are presented in    g(21.5).

recognition

using the above constrained factor analysis models for faces, one can extend their application to classi   ca-
tion, as we now brie   y describe. in closed set face recognition a new    probe    face x    is to be matched to
a person n in the gallery of training faces. in model mn the nth gallery face is forced to share its latent
identity variable hn with the test face, indicating that these faces belong to the same person3, see    g(21.6).
assuming a single exemplar per person (j = 1),

(cid:90)
                =

i(cid:89)

p(x1, . . . , xi , x   |mn) = p(xn, x   )

p(xi)

i=1,i(cid:54)=n

bayes    rule then gives the posterior class assignment

p(mn|x1, . . . , xi , x   )     p(x1, . . . , xi , x   |mn)p(mn)

(21.3.9)

(21.3.10)

3this is analogous to bayesian outcome analysis in section(12.6) in which the hypotheses assume that either the errors were

generated from the same or a di   erent model.

draft november 9, 2017

451

h1

x1

h2

x2

h1

x   

x1

h2

x2

w1

w2

w   

w1

(a) m1

w2

(b) m2

probabilistic principal components analysis

x   

w   

(a):

figure 21.6: face recognition model (de-
picted only for a single exemplar per per-
in model m1 the
son, j = 1).
test image (or    probe   ) x    is assumed to
be from person 1, albeit with a di   erent
(b): for model m2
pose/illumination.
the test image is assumed to be from per-
son 2. one calculates p(x1, x2, x   |m1) and
p(x1, x2, x   |m2) and then uses bayes    rule
to infer to which person the test image x   
most likely belongs.

for a uniform prior, the term p(mn) is constant and can be neglected. all these marginal quantities are
straightforward to derive since they are simply marginals of a gaussian.
in practice, the best results are obtained using a between-individual subspace dimension f and within-
individual subspace dimension g both equal to 128. this model then has performance competitive with the
state-of-the-art[246]. a bene   t of the probabilistic model is that the extension to mixtures of this model
is essentially straightforward, which boosts performance further. related models can also be used for the
   open set    face recognition problem in which the probe face may or may not belong to one of the individuals
in the database[246].

21.4 probabilistic principal components analysis

ppca[297] corresponds to factor analysis under the restriction    =   2id. plugging this assumption into
the eigen-solution equation (21.2.21) gives

f =   uh (  h     ih )

1
2 r

(21.4.1)

where the eigenvalues (diagonal entries of   h ) and corresponding eigenvectors (columns of uh ) are the
largest eigenvalues of      2s. since the eigenvalues of      2s are those of s simply scaled by      2 (and the
eigenvectors are unchanged), we can equivalently write

(cid:0)  h       2ih

(cid:1) 1

2 r

f = uh

where r is an arbitrary orthogonal matrix with rtr = i and uh ,   h are the eigenvectors and correspond-
ing eigenvalues of the sample covariance s. classical pca, section(15.2), is recovered in the limit   2     0.
note that for a full correspondence with pca, one needs to set r = i, which points f along the principal
directions.

a particular convenience of ppca is that the optimal noise   2 can be found immediately. we order the
eigenvalues of s so that   1       2, ...       d. in equation (21.2.20) an expression for the log likelihood is given
in which the eigenvalues are those      2s. on replacing   i with   i/  2 we can therefore write an explicit
expression for the log likelihood in terms of   2 and the eigenvalues of the sample covariance s,

l(  2) =    

n
2

d log(2  ) +

log   i +

1
  2

  i + (d     h) log   2 + h

h(cid:88)

i=1

d(cid:88)

i=h+1

(cid:33)

by di   erentiating l(  2) and equating to zero, the maximum likelihood optimal setting for   2 is

optimal   2

(cid:32)

d(cid:88)

  2 =

1

d     h

j=h+1

  j

in summary ppca is obtained by taking the h principal eigenvalues and corresponding eigenvectors of the
sample covariance matrix s, and setting the variance by equation (21.4.4). the single-shot training nature
of ppca makes it an attractive algorithm and also gives a useful initialisation for factor analysis.

452

draft november 9, 2017

(21.4.2)

(21.4.3)

(21.4.4)

canonical correlation analysis and factor analysis

figure 21.7: for a 5 hidden unit model, here are plotted the
results of training ppca and fa on 100 examples of the hand-
written digit seven. the top row contains the 5 factor analysis
factors and the bottom row the 5 largest eigenvectors from ppca
are plotted.

example 21.1 (a comparison of fa and ppca). we trained both ppca and fa to model handwritten
digits of the number 7. from a database of 100 such images, we    tted both ppca and fa (100 iterations
of em in each case from the same random initialisation) using 5 hidden units. the learned factors for these
models are in    g(21.7). to get a feeling for how well each of these models the data, we drew 25 samples from
each model, as given in    g(21.8a). compared with ppca, in fa the individual noise on each observation
pixel enables a cleaner representation of the regions of zero sample variance.

21.5 canonical correlation analysis and factor analysis

we outline how cca, as discussed in section(15.8), is related to a constrained form of fa. as a brief re-
minder, cca considers two spaces x and y where, for example, x might represent an audio sequence of a
person speaking and y the corresponding video sequence of the face of the person speaking. the two streams
of data are dependent since we would expect the parts around the mouth region to be correlated with the
speech signal. the aim in cca is to    nd a low dimensional representation that explains the correlation
between the x and y spaces.

a model that achieves a similar e   ect to cca is to use a latent factor h to underlie the data in both the x
and y spaces, see    g(21.5). that is

(cid:90)

p(x, y) =

where

p(x|h)p(y|h)p(h)dh

(cid:18) x

y

z =

=

(cid:19)
(cid:18) x

y

(cid:18) a
(cid:19)

b

,

(cid:19)

h +

(cid:18)  x
(cid:18) a

 y

(cid:19)
(cid:19)

,

,

b

f =

by using the stacked vectors

p(x|h) = n (x ha,   x) ,

p(y|h) = n (y hb,   y) ,

p(h) = n (h 0, 1)

we can express equation (21.5.2) as a form of factor analysis by writing

 x     n ( x 0,   x) ,  y     n ( y 0,   y)

(21.5.1)

(21.5.2)

(21.5.3)

(21.5.4)

(a):

figure 21.8:
25 sam-
ples from the learned fa model.
note how the noise variance de-
pends on the pixel, being zero
for pixels on the boundary of the
(b): 25 samples from
image.
the learned ppca model with
the same noise variance on each
pixel.

(a) factor analysis

(b) ppca

draft november 9, 2017

453

fafafafafapcapcapcapcapcaindependent components analysis

h

x

y

figure 21.9: canonical correlation analysis. cca corresponds to the
latent variable model in which a common latent variable generates
both the observed x and y variables. this is therefore a formed of
constrained factor analysis.

and integrating out the latent variable h, we obtain

p(z) = n (z 0,   ) ,

   =     t +   ,

   =

(cid:18)   x

0
0   y

(cid:19)

(21.5.5)

(cid:17)

this is therefore clearly simply a factor analysis model, in this case with a single latent factor. we can learn
the best a and b by maximising the likelihood on a set of data. from the fa results equation (21.2.6) the
optimal f is given by (with s being the sample covariance matrix)

1 + f t     1f

f

= s     1f     f     s     1f

(21.5.6)

so that optimally f is given by the principal eigenvector of s     1. by imposing   x =   2
above equation can be expressed as the coupled equations

xi,   y =   2

yi the

a    

1
  2
x

sxxa +

1
  2
y

sxyb,

b    

1
  2
x

syxa +

1
  2
y

syyb

eliminating b we have, for an arbitrary proportionality constant   ,

(cid:19)

(cid:18)

(cid:19)   1

i    

  
  2
x

sxx

a =

  2
  2
x  2
y

sxy

i    

  
  2
y

syy

syxa

(21.5.7)

(21.5.8)

(cid:16)

(cid:18)

x,   2

in the limit   2
y     0, this tends to the zero derivative condition equation (15.8.9) so that cca can be
seen as in fact a limiting form of fa (see [12] for a more thorough correspondence). by viewing cca in
this manner extensions to using more than a single latent dimension h become clear, see exercise(21.2), in
addition to the bene   ts of a probabilistic interpretation.

as we   ve indicated, cca corresponds to training a form of fa by maximising the joint likelihood p(x, y|w, u).
if the x represents rather an input and y the output, we are more interested in    nding a good predictive
representation. in this case, training based on maximising the conditional p(y|x, w, u) corresponds to a
special case of a technique called partial least squares, see for example [83]. this correspondence is left as
an exercise for the interested reader.

21.6 independent components analysis

independent components analysis (ica) seeks a representation of data v using a coordinate system h in
which the components hi are independent[239, 152]. such independent coordinate systems arguably form a
natural representation of the data. in ica it is common to assume that the observations are linearly related
to the latent variables h. for technical reasons, the most convenient practical choice is to use4

v = ah

(cid:90)

where a is a square mixing matrix so that the likelihood of an observation v is

(cid:90)

(cid:89)

i

(cid:89)

i

p(v|a) =

p(v|h, a)

p(hi)dh =

   (v     ah)

p(hi)dh =

p((cid:2)a   1v(cid:3)

i)

(21.6.1)

(21.6.2)

(cid:89)

1

|det (a)|

i

4this treatment follows that presented in [198].

454

draft november 9, 2017

independent components analysis

(cid:112)

figure 21.10: latent data is sampled from the prior p(xi)    
|xi|) with the mixing matrix a shown in green to cre-
exp(   5
ate the observed two dimensional vectors y = ax. the red lines
are the mixing matrix estimated by ica.m based on the obser-
vations. for comparison, pca produces the blue (dashed) com-
ponents. note that the components have been scaled to improve
visualisation. as expected, pca    nds the orthogonal directions
of maximal variation. ica however, correctly estimates the di-
rections in which the components were independently generated.
see demoica.m.

for a given set of data v = (cid:0)v1, . . . , vn(cid:1) and prior p(h), our aim is to    nd a. for i.i.d. data, the log

likelihood is conveniently written in terms of b = a   1,

which is invariant with respect to an orthogonal rotation b     rb, with rtr = i. this means that for
a gaussian prior p(h), we cannot estimate uniquely the mixing matrix. to break this rotational invariance
we therefore need to use a non-gaussian prior. assuming we have a non-gaussian prior p(h), taking the
derivative w.r.t. bab we obtain

(cid:88)

(cid:88)

n

i

l(b) = n log det (b) +

log p([bvn]i)

note that for a gaussian prior

p(h)     exp(cid:0)

   h2(cid:1)

the log likelihood becomes

l(b) = n log det (b)    

(cid:88)

n

(vn)t btbvn + const.

   

   bab

l(b) = n aba +

  ([bv]a)vn
b

where

  (x)    

d
dx

log p(x) =

a simple gradient ascent learning rule for b is then

bnew = b +   

  (bvn) (vn)t

b   t +

1
n

(cid:88)

n

1

p(x)

d
dx

p(x)

(cid:88)

n

(cid:88)

n

1
n

(cid:32)

(cid:32)

(cid:33)

(cid:33)

an alternative    natural gradient    algorithm[7, 198] that approximates a newton update is given by multi-
plying the gradient by btb on the right to give the update

bnew = b +   

i +

  (bvn) (bvn)t

b

(21.6.9)

here    is a learning rate which in the code ica.m we nominally set to 0.5. the performance of the algorithm
is relatively insensitive to the choice of the prior function   (x). in ica.m we use the tanh function. an
example is given in    g(21.10) in which the data clearly has an underlying    x    shape, although the axes of
the    x    are not orthogonal. in this case, the ica and pca representations are very di   erent, see    g(21.10).
a natural extension is to consider noise on the outputs, exercise(21.6), for which an em algorithm is readily
available. however, in the limit of low output noise, the em formally fails, an e   ect which is related to the
general discussion in section(11.4).

draft november 9, 2017

455

(21.6.3)

(21.6.4)

(21.6.5)

(21.6.6)

(21.6.7)

(21.6.8)

   2   1.5   1   0.500.511.5   1.5   1   0.500.511.5a popular alternative estimation method is fastica5 and can be related to an iterative maximum likelihood
optimisation procedure. ica can also be motivated from several alternative directions, including information
theory[30]. we refer the reader to [152] for an in-depth discussion of ica and related extensions.

exercises

21.7 summary

    factor analysis is a classical probabilistic method for    nding low-dimensional representations of the data.
    there is no closed form solution in general and iterative procedures are typically used to    nd the maximum

likelihood parameters.

    canonical correlation analysis is a special case of factor analysis.
    under the assumption of non-gaussian latent variable priors, one is able to discover independent directions

in the data.

21.8 code

fa.m: factor analysis
demofa.m: demo of factor analysis
ica.m: independent components analysis
demoica.m: demo ica

21.9 exercises

exercise 21.1. factor analysis and scaling. assume that a h-factor model holds for x. now consider the
transformation y = cx, where c is a non-singular square diagonal matrix. show that factor analysis is
scale invariant, i.e. that the h-factor model also holds for y, with the factor loadings appropriately scaled.
how must the speci   c factors be scaled?

exercise 21.2. for the constrained factor analysis model

h +  ,

      n (  0, diag (  1, . . . ,   n)) ,

h     n (h 0, i)

(21.9.1)

derive a maximum likelihood em algorithm for the matrices a and b, assuming the datapoints x1, . . . , xn
are i.i.d.

exercise 21.3. an apparent extension of fa analysis is to consider a correlated prior

p(h) = n (h 0,   h )

(21.9.2)

show that, provided no constraints are placed on the factor loading matrix f, using a correlated prior p(h)
is an equivalent model to the original uncorrelated fa model.

exercise 21.4. using the woodbury identity and the de   nition of   d in equation (21.2.3), show that one
can rewrite      1

d f as
     1
d f =      1f

(cid:16)

i + ft     1f

(cid:17)   1

(21.9.3)

5see www.cis.hut.fi/projects/ica/fastica/

456

draft november 9, 2017

(cid:18) a 0

(cid:19)

0 b

x =

d(cid:88)

i=h+1

  i + (d     h) log   2 + h

(cid:33)

(21.9.4)

(21.9.5)

exercise 21.5. for the log likelihood function

l(  2) =    

n
2

d log(2  ) +

log   i +

1
  2

h(cid:88)

i=1

exercises

(cid:32)

j=h+1

d(cid:88)
(cid:89)

j

(cid:16)

show l(  2) is maximal for

  2 =

1

d     h

  j

p(y, x|w) =

with

(cid:89)
j x,   2(cid:17)

i

(21.9.6)

(21.9.7)

exercise 21.6. consider an ica model in which y represents the outputs and x the latent components

p(yj|x, w)

p(xi),

w = [w1, . . . , wj ]

p(yj|x, w) = n

yj wt

required statistics for the m-step are (cid:104)x(cid:105)p(x|yn,w) and(cid:10)xxt(cid:11)

p(x|yn,w).

1. for the above model derive an em algorithm for a set of i.i.d. data y1, . . . , yn and show that the

2. show that for a non-gaussian prior p(xi), the posterior

p(x|y, w)

(21.9.8)

is non-factorised, non-gaussian and generally intractable (its normalisation constant cannot be com-
puted e   ciently).

3. show that in the limit   2     0, the em algorithm fails.

draft november 9, 2017

457

exercises

458

draft november 9, 2017

chapter 22

latent ability models

in this chapter we discuss an application of latent variable models to ascertaining the ability of players
in games. these models are applicable in a variety of contexts from exam performance to football match
prediction to online gaming analysis.

22.1 the rasch model

consider an exam in which student s answers question q either correctly xqs = 1 or incorrectly xqs = 0.
for a set of n students and q questions, the performance of all students is given in the q    n binary
matrix x. based on this data alone we wish to evaluate the ability of each student. one approach is to
de   ne the ability as as the fraction of questions student s answered correctly. a more subtle analysis is to
accept that some questions are more di   cult than others so that a student who answered di   cult questions
should be awarded more highly than a student who answered the same number of easy questions. a priori,
however, we do not know which are the di   cult questions and this needs to be estimated based on x. to
account for inherent di   erences in question di   culty we may model the id203 that a student s gets
a question q correct based on the student   s latent ability   s and the latent di   culty of the question   q. a
simple generative model of the response is, see    g(22.2)

p(xqs = 1|  ,   ) =    (  s       q)

(22.1.1)
where    (x) = 1/(1 + e   x). under this model, the higher the latent ability is above the latent di   culty of
the question, the more likely it is that the student will answer the question correctly.

22.1.1 maximum likelihood training

we may use maximum likelihood to    nd the best parameters   ,   . making the i.i.d. assumption, the
likelihood of the data x under this model is

s(cid:89)

q(cid:89)

s=1

q=1

p(x|  ,   ) =

the log likelihood is then

   (  s       q)xqs (1        (  s       q))1   xqs
(cid:88)

l     log p(x|  ,   ) =

xqs log    (  s       q) + (1     xqs) log (1        (  s       q))

q,s

with derivatives

   l
     s

=

q(cid:88)

q=1

(xqs        (  s       q)) ,

s(cid:88)

s=1

   l
     q

=    

(xqs        (  s       q))

459

(22.1.2)

(22.1.3)

(22.1.4)

the rasch model

figure 22.1: the rasch model for analysing questions. each
element of the binary matrix x, with xqs = 1 if student s gets
question q correct, is generated using the latent ability of the
student   s and the latent di   culty of the question   q.

q

  q

xqs

  s

s

a simple way to learn the parameters is to use gradient ascent, see demorasch.m, with extensions to newton
methods being straightforward.

the generalisation to more than two responses xqs     {1, 2, . . .} can be achieved using a softmax-style
function. more generally, the rasch model falls under item response theory, a subject dealing with the
analysis of questionnaires[102].

missing data

assuming the data is missing at random, missing data can be treated by computing the likelihood of only
the observed elements of x. in rasch.m missing data is assumed to be coded as nan so that the likelihood
and gradients are straightforward to compute based on summing only over terms containing non nan entries.

example 22.1. we display an example of the use of the rasch model
in    g(22.2), estimat-
based on using the
ing the latent abilities of 20 students based on a set of 50 questions.
number of questions each student answered correctly,
the best students are (ranked from    rst)
8, 6, 1, 19, 4, 17, 20, 7, 15, 5, 12, 16, 2, 3, 18, 9, 11, 14, 10, 13. alternatively, ranking students according to the
latent ability gives 8, 6, 19, 1, 20, 4, 17, 7, 15, 12, 5, 16, 2, 3, 18, 9, 11, 14, 10, 13. this di   ers (only slightly in this
case) from the number-correct ranking since the rasch model takes into account the fact that some students
answered di   cult questions correctly. for example student 20 answered some di   cult questions correctly.

22.1.2 bayesian rasch models

the rasch model will potentially over   t the data especially when there is only a small amount of data. for
this case a natural extension is to use a bayesian technique, placing independent priors on the ability and
question di   culty, so that the posterior ability and question di   culty is given by

p(  ,   |x)     p(x|  ,   )p(  )p(  )

natural priors are those that discourage very large values of the parameters, such as gaussians

(cid:0)  s 0,   2(cid:1) ,

(cid:89)
s n

p(  ) =

p(  ) =

(cid:0)  q 0,    2(cid:1)

(cid:89)
q n

(22.1.5)

(22.1.6)

where   2 and    2 are hyperparameters that can be learned by maximising p(x|  2,    2). even using gaussian
priors, however, the posterior distribution p(  ,   |x) is not of a standard form and approximations are
required.
in this case however, the posterior is log concave so that approximation methods based on
variational or laplace techniques are potentially adequate, chapter(28); alternatively one may use sampling
approximations, chapter(27).

460

draft november 9, 2017

competition models

(a)

(c)

(b)

(d)

figure 22.2: rasch model.
the estimated latent di   culty of each question.
correctly. (d): the estimated latent ability.

(a): the data of correct answers (white) and incorrect answers (black).

(b):
(c): the fraction of questions each student answered

22.2 competition models

22.2.1 bradley-terry-luce model

the bradley-terry-luce model assesses the ability of players based on one-on-one matches. here we describe
games in which only win/lose outcomes arise, leaving aside the complicating possibility of draws. for this
win/lose scenario, the btl model is a straightforward modi   cation of the rasch model so that for latent
ability   i of player i and latent ability   j of player j, the id203 that i beats j is given by

where i (cid:66) j stands for player i beats player j. based on a matrix of games data x with

p(i (cid:66) j|  ) =    (  i       j)

(cid:26) 1

0

xn
ij =

if i (cid:66) j in game n
otherwise

the likelihood of the model is given by

[   (  i       j)]xn

ij =

(cid:89)

ij

[   (  i       j)]mij

(cid:89)

(cid:89)

n

ij

n xn

p(x|  ) =

where mij =(cid:80)

ij is the number of times player i beat player j. training using maximum likelihood or a

bayesian technique can then proceed as for the rasch model.

these models are also called pairwise comparison models and pioneered by thurstone in the 1920   s who
applied such models to a wide range of data [78].

draft november 9, 2017

461

(22.2.1)

(22.2.2)

(22.2.3)

questionstudent5101520253035404550246810121416182005101520253035404550   8   6   4   20246questionestimated difficulty0246810121416182000.10.20.30.40.50.60.7studentfraction of questions correct02468101214161820   2.5   2   1.5   1   0.500.511.5studentestimated abilitycompetition models

(a)

(b)

(a): the data m with mij being the number of times that competitor i beat
figure 22.3: btl model.
competitor j. (b): the true versus estimated ability of the 100 competitors (unlabelled). even though the
data is quite sparse, a reasonable estimate of the latent ability of each competitor is found.

example 22.2. an example application of the btl model is given in    g(22.3) in which a matrix m
containing the number of times that competitor i beat competitor j is given. the matrix entries m were
drawn from a btl model based on    true abilities   . using m alone the maximum likelihood estimate of these
latent abilities is in close agreement with the true abilities.

22.2.2 elo ranking model

the elo system [96] used in chess ranking is closely related to the btl model above, though there is the
added complication of the possibility of draws. in addition, the elo system takes into account a measure
of the variability in performance. for a given ability   i, the actual performance   i of player i in a game is
given by

where  i     n
variability in the performance. more formally the elo model modi   es the btl model to give

(cid:0) i 0,   2(cid:1). the variance   2 is    xed across all players and thus takes into account intrinsic
(cid:90)

  i =   i +  i

(22.2.4)

(cid:0)     ,   2i(cid:1)

(22.2.5)

p(x|  ) =

p(x|  )p(  |  ),

  

p(  |  ) = n

where p(x|  ) is given by equation (22.2.3) on replacing    with   .
22.2.3 glicko and trueskill

glicko[127] and trueskill[143] are essentially bayesian versions of the elo model with the re   nement that
the latent ability is modelled, not by a single number, but by a gaussian distribution

this can capture the fact that a player may be consistently reasonable (quite high   i and low   2
erratic genius (high   i but with large   2

i ). the parameters of the model are then

for a set of s players. the interaction model p(x|  ) is as for the win/lose elo model, equation (22.2.1).
the likelihood for the model given the parameters is

i

p(  i|  i) = n

(cid:0)  i   i,   2
(cid:1)
   =(cid:8)  i,   2
i , i = 1, . . . , s(cid:9)
(cid:90)

p(x|  ) =

p(x|  )p(  |  )

  

462

(22.2.6)

i ) or an

(22.2.7)

(22.2.8)

draft november 9, 2017

competitorcompetitor  2040608010010203040506070809010000.511.522.533.544.55   10   50510   10   50510true abilityestimated abilityexercises

this integral is formally intractable and numerical approximations are required. in this context expectation
propagation, section(28.8), has proven to be a useful technique[211]. the trueskillsystem is used for example
to assess the abilities of players in online gaming, also taking into account the abilities of teams of individuals
in tournaments. a temporal extension has recently been used to reevaluate the change in ability of chess
players with time[77].

22.3 summary

    the rasch model is a simple model of latent student ability and question di   culty. in principle it is better
able to assess student performance than simply counting the number of correct questions since it implicitly
takes into account the fact that some questions are more di   cult than others.
    related models can be used to assess the underlying ability of players in games.

22.4 code

rasch.m: rasch model training
demorasch.m: demo for the rasch model

22.5 exercises

exercise 22.1 (bucking bronco). bronco.mat contains information about a bucking bronco competition.
there are 500 competitors and 20 bucking broncos. a competitor j attempts to stay on a bucking bronco i
for a minute. if the competitor succeeds, the entry xij is 1, otherwise 0. each competitor gets to ride three
bucking broncos only (the missing data is coded as nan). having viewed all the 500 amateurs, desperate dan
enters the competition and bribes the organisers into letting him avoid having to ride the di   cult broncos.
based on using a rasch model, which are the top 10 most di   cult broncos, in order of the most di   cult
   rst?

exercise 22.2 (btl training).

(cid:88)

ij

1. show that the log likelihood for the bradley-terry-luce model is given by

l(  ) =

mij log    (  i       j)

(22.5.1)

where mij is the number of times that player i beats player j in a set of games.

2. compute the gradient of l(  ).

3. compute the hessian of the btl model and verify that it is negative semide   nite.

exercise 22.3 (la reine).

1. program a simple gradient ascent routine to learn the latent abilities of competitors based on a series

of win/lose outcomes.

2. in a modi   ed form of swiss cow       ghting   , a set of cows compete by pushing each other until submission.
at the end of the competition one cow is deemed to be    la reine   . based on the data in btl.mat (for
which xij contains the number of times cow i beat cow j),    t a btl model and return a ranked list of
the top ten best    ghting cows,    la reine       rst.

draft november 9, 2017

463

exercises

h(cid:88)

exercise 22.4. an extension of the btl model is to consider additional factors that describe the state
of the competitors when they play. for example, we have a set of s football teams, and a set of matrices
x1, . . . , xn , with x n
ij = 1 if team i beat team j in match n. in addition we have for each match n and team
i a vector of binary factors f n
h,i     {0, 1} , h = 1, . . . , h that describes the team. for example, for the team
i = 1 (madchester united), the factor f n
1,1 = 1 if bozo is playing in match n, 0 if not. it is suggested that
the ability of team i in game n is measured by

  n

i = di +

wh,if n

h,i.

(22.5.2)

h=1

here di is a default latent ability of the team which is assumed constant across all games. we have such a
set of factors for each match.

1. using the above de   nition of the latent ability in the btl model, our interest is to    nd the weights
w and abilities d that best describe the ability of the team, given that we have a set of historical plays
(xn, fn), n = 1, . . . , n . write down the likelihood for the btl model as a function of the set of all
team weights w, d.

2. compute the gradient of the log likelihood of this model.

3. explain how this model can be used to assess the importance of bozo   s contribution to madchester

united   s ability.

4. given learned w, d and the knowledge that madchester united (team 1) will play chelski (team 2)
tomorrow explain how, given the list of factors f for chelski (which includes issues such as who will
be playing in the team), one can select the best madchester united team to maximise the id203 of
winning the game.

464

draft november 9, 2017

part iv

dynamical models

465

introduction to part iv

natural organisms inhabit a dynamical environment and arguably a large part
intelligence is in modelling causal relations and consequences of ac-
of natural
tions.
in a
more arti   cial environment, there are many instances where predicting the future is
of interest, particularly in areas such as    nance and also in tracking of moving objects.

in this sense, modelling temporal data is of fundamental

interest.

in part iv, we discuss some of the classical models of timeseries that may be used to
represent temporal data and also to make predictions of the future. many of these
models are well known in di   erent branches of science from physics to engineering
and are heavily used in areas such as id103,    nancial prediction and
control. we also discuss some more sophisticated models in chapter 25, which may be
skipped at    rst reading.

as an allusion to the fact that natural organisms inhabit a temporal world, we also ad-
dress in chapter 26 some basic models of how information processing might be achieved
in distributed systems.

draft november 9, 2017

467

some dynamical id114 for timeseries. in part iv we cover id136 and learning for most of the
standard models above.

468

draft november 9, 2017

dynamicmodelsmarkovmodelscompletecont.oldsardiscretemarkov-chainlatentdbn(factorisedstructure)discretelatentid48cont.latentstate-spacemodelsldsswitchingmodels(egslds)non-markovmodelsgaussianprocesschapter 23

discrete-state markov models

timeseries require specialised models since the number of variables can be very large and typically increases
as new datapoints arrive. in this chapter we discuss models in which the process generating the observed
data is fundamentally discrete. these models give rise to classical models with interesting applications in
many    elds from    nance to speech processing and website ranking.

23.1 markov models

timeseries are datasets for which the constituent datapoints can be naturally ordered. this order often
corresponds to an underlying single physical dimension, typically time, though any other single dimension
may be used. the time-series models we consider are id203 models over a collection of random variables
v1, . . . , vt with individual variables vt indexed by discrete time t. a probabilistic time-series model requires
a speci   cation of the joint distribution p(v1, . . . , vt ). for the case in which the observed data vt are discrete,
the joint id203 table for p(v1, . . . , vt ) has exponentially many entries. we therefore cannot expect to
independently specify all the exponentially many entries and need to make simpli   ed models under which
these entries can be parameterised in a lower dimensional manner. such simpli   cations are at the heart of
timeseries modelling and we will discuss some classical models in the following sections.

de   nition 23.1 (time-series notation).

xa:b     xa, xa+1, . . . , xb,

with xa:b = xa for b     a

(23.1.1)

for timeseries data v1, . . . , vt , we need a model p(v1:t ). for consistency with the causal nature of time, it
is natural to consider the cascade decomposition

p(v1:t ) =

p(vt|v1:t   1)

(23.1.2)

with the convention p(vt|v1:t   1) = p(v1) for t = 1. it is often useful to assume that the in   uence of the
immediate past is more relevant than the remote past and in markov models only a limited number of
previous observations are required to predict the future, see    g(23.1).

de   nition 23.2 (markov chain). a markov chain de   ned on either discrete or continuous variables v1:t is
one in which the following conditional independence assumption holds:

p(vt|v1, . . . , vt   1) = p(vt|vt   l, . . . , vt   1)

(23.1.3)

469

t(cid:89)

t=1

markov models

v1

v2

v3

v4

v1

v2

v3

v4

(a)

(b)

figure 23.1: (a): first order markov chain. (b): second order markov chain.

where l     1 is the order of the markov chain and vt =     for t < 1. for a    rst order markov chain,

p(v1:t ) = p(v1)p(v2|v1)p(v3|v2) . . . p(vt|vt   1)

for a stationary markov chain the transitions p(vt = s(cid:48)
the chain is non-stationary, p(vt = s(cid:48)

|vt   1 = s) = f (s(cid:48), s, t).

|vt   1 = s) = f (s(cid:48), s) are time-independent. otherwise

(23.1.4)

for a discrete state    rst order time-independent markov chain one can visualise the transition p(vt|vt   1)
using a state-transition diagram, as in    g(23.2).

1

3

2

figure 23.2: a state transition diagram for a three state markov
chain. note that a state transition diagram is not a graphical
model     it simply displays the non-zero entries of the transition
matrix p(i|j). the absence of a link from j to i indicates that
p(i|j) = 0.

23.1.1 equilibrium and stationary distribution of a markov chain
for a transition p(xt|xt   1), it is interesting to know how the marginal p(xt) evolves through time. for a
discrete state system, this is given by

p(xt = i) =

p(xt   1 = j)

(23.1.5)

(cid:88)

j

(cid:124)
(cid:125)
p(xt = i|xt   1 = j)

(cid:123)(cid:122)

mij

for an initial distribution p(x1), the above recursively de   nes the marginal for all future timepoints. the
marginal p(xt = i) has the interpretation of the frequency that we visit state i at time t, given we started
with a sample from p(x1) and subsequently repeatedly drew samples from the transition p(x  |x     1). that
is, by drawing a state x1 = x1, from p(x1) we can draw samples x2, . . . , xt from the markov chain by    rst
drawing a sample from p(x2|x1 = x1), and then from p(x3|x2 = x2) etc. as we repeatedly sample a new
state from the chain, the marginal distribution at time t, represented by the vector [pt]i =
p(xt = i) and
initial distribution p1 is

@@

pt = mpt   1 = mt   1p1

(23.1.6)
if, for t        , p    is independent of the initial distribution p1, then p    is called the equilibrium distribution
of the chain. see exercise(23.2) for an example of a markov chain which does not have an equilibrium
distribution. the so-called stationary distribution of a markov chain is de   ned by the condition

(cid:88)

j

p   (i) =

p(xt = i|xt   1 = j)p   (j)

in matrix notation this can be written as the vector equation

p    = mp   

so that the stationary distribution is proportional to the eigenvector with unit eigenvalue of the transition
matrix. note that there may be more than one stationary distribution. see exercise(23.1) and [135].

470

draft november 9, 2017

(23.1.7)

(23.1.8)

markov models

h

v1

v2

v3

v4

(a)

the markov chain (cid:81)

figure 23.3: mixture of    rst order markov chains. the
discrete hidden variable dom(h) = {1, . . . , h} indexes
t p(vt|vt   1, h). such models can
be useful as simple sequence id91 tools.

example 23.1 (id95 ). despite their apparent simplicity, markov chains have been put to interesting
use in information retrieval and search-engines. de   ne the matrix

(cid:26) 1

0

aij =

if website j has a hyperlink to website i
otherwise

from this we can de   ne a markov transition matrix with elements

aij(cid:80)

i(cid:48) ai(cid:48)j

mij =

(23.1.9)

(23.1.10)

the equilibrium distribution of this markov chain has the interpretation : if we follow links at random,
jumping from website to website, the equilibrium distribution component p   (i) is the relative number of
times we will visit website i. this has a natural interpretation as the    importance    of website i; if a website
is isolated in the web, it will be visited infrequently by random hopping; if a website is linked by many
others it will be visited more frequently.

a crude search engine works as follows. for each website i a list of words associated with that website is
collected. after doing this for all websites, one can make an    inverse    list of which websites contain word
w. when a user searches for word w, the list of websites that contain that word is then returned, ranked
according to the importance of the site (as de   ned by the equilibrium distribution).

23.1.2 fitting markov models

given a sequence v1:t ,    tting a stationary    rst order markov chain by maximum likelihood corresponds to
setting the transitions by counting the number of observed (   rst-order) transitions in the sequence:

p(v   = i|v     1 = j)    

i [vt = i, vt   1 = j]

(23.1.11)

to show this, for convenience we write p(v   = i|v     1 = j)       i|j, so that the likelihood is (assuming v1 is
known):

t(cid:88)

t=2

t(cid:89)

t(cid:89)

(cid:89)

p(v2:t|  , v1) =

  vt|vt   1 =

t=2

t=2

i,j

i[vt=i,vt   1=j]
i|j

  

(23.1.12)

taking logs and adding the lagrange constraint for the normalisation,

t(cid:88)

(cid:88)

l(  ) =

i [vt = i, vt   1 = j] log   i|j +

t=2

i,j

j

(cid:32)

(cid:88)

  j

1    

(cid:88)

i

(cid:33)

  i|j

(23.1.13)

(cid:80)

di   erentiating with respect to   i|j and equating to zero, we immediately arrive at the intuitive setting, equa-
tion (23.1.11). for a set of timeseries, vn
1:tn, n = 1, . . . , n , the transition is given by counting all transitions
across time and datapoints. the maximum likelihood setting for the initial    rst timestep distribution is
p(v1 = i)    
draft november 9, 2017

1 = i].

i [vn

n

471

markov models

(23.1.14)

for simplicity, we assume a factorised prior on the transition

bayesian    tting

(cid:89)

p(  ) =

p(    |j)

j

p(    |j) = dirichlet(cid:0)    |j|uj
uij+(cid:80)t

where   uj =

t=2

p(  |v1:t )     p(v1:t|  )p(  )    

a convenient choice for each conditional transition is a dirichlet distribution with hyperparameters uj,

(cid:1), since this is conjugate to the categorical transition, giving
(cid:1)
dirichlet(cid:0)    |j|  uj

i[vt=i,vt   1=j]
i|j

(cid:89)

(cid:89)

(cid:89)

uij   1
i|j

=

  

  

t

i,j

j

i [vt   1 = j, vt = i], being the number of j     i transitions in the dataset.

(23.1.15)

@@

23.1.3 mixture of markov models
given a set of sequences v = {vn
1:t , n = 1, . . . , n}, how might we cluster them? to keep the notation less
cluttered, we assume that all sequences are of the same length t with the extension to di   ering lengths being
straightforward. one simple approach is to    t a mixture of markov models. assuming the data is i.i.d.,
1:t ), we de   ne a mixture model for a single sequence v1:t . here we assume each component

n p(vn

p(v) =(cid:81)

model is    rst order markov

h(cid:88)

h=1

h(cid:88)

t(cid:89)

p(v1:t ) =

p(h)p(v1:t|h) =

p(h)

h=1

t=1

p(vt|vt   1, h)

(23.1.16)

the graphical model is depicted in    g(23.3). id91 can then be achieved by    nding the maximum
likelihood parameters p(h), p(vt|vt   1, h) and subsequently assigning the clusters according to p(h|vn
em algorithm

1:t ).

the em algorithm, section(11.2), is particularly convenient for    nding the maximum likelihood solution in
this case since the m-step can be performed simply. under the i.i.d. data assumption, the log likelihood is

n(cid:88)

h(cid:88)

t(cid:89)

n=1

h=1

t=1

log p(v) =

log

p(h)

p(vn

t   1, h)

for the m-step, our task is to maximise the energy

t |vn
n(cid:88)

n=1

(cid:40)

(cid:104)log p(vn

1:t , h)(cid:105)pold(h|vn

1:t ) =

(cid:104)log p(h)(cid:105)pold(h|vn

1:t ) +

the contribution to the energy from the parameter p(h) is

t(cid:88)

t=1

(cid:104)log p(vt|vt   1, h)(cid:105)pold(h|vn

1:t )

n(cid:88)

n=1

e =

n(cid:88)

n=1

by de   ning

(cid:104)log p(h)(cid:105)pold(h|vn

1:t )

n(cid:88)

n=1

  pold(h)    
(cid:16)

kl

1:t )

pold(h|vn
(cid:17)

  pold(h)|p(h)
n(cid:88)

pnew(h)    

pold(h|vn

1:t )

n=1

one can view maximising (23.1.18) as equivalent to minimising

so that the optimal choice from the m-step is to set pnew =   pold, namely

(23.1.17)

(cid:41)

(23.1.18)

(23.1.19)

(23.1.20)

(23.1.21)

472

draft november 9, 2017

t(cid:88)

t=1

markov models

for those less comfortable with this argument, a direct maximisation including a lagrange term to ensure
normalisation of p(h) can be used to derive the same result.

similarly, the m-step for p(vt|vt   1, h) is
n(cid:88)

pnew(vt = i|vt   1 = j, h = k)    

n=1

t(cid:88)

t=2

pold(h = k|vn

1:t )

t = i] i(cid:2)vn

t   1 = j(cid:3)

i [vn

the initial term p(v1|h) is updated using

pnew(v1 = i|h = k)    

finally, the e-step sets

1:t )i [vn

1 = i]

n(cid:88)

n=1

pold(h = k|vn
t(cid:89)

pold(h|vn

1:t )     p(h)p(vn

1:t|h) = p(h)

p(vn

t |vn

t   1, h)

t=1

(23.1.22)

(23.1.23)

(23.1.24)

given an initialisation, the em algorithm then iterates (23.1.21),(23.1.22), (23.1.23) and (23.1.24) until
convergence.

for long sequences, explicitly computing the product of many terms may lead to numerical under   ow issues.
in practice it is therefore best to work with logs,

log pold(h|vn

1:t ) = log p(h) +

log p(vn

t |vn

t   1, h) + const.

(23.1.25)

in this way any large constants common to all h can be removed and the distribution may be computed
accurately. see mixmarkov.m.

example 23.2 (gene id91). consider the 20    ctitious gene sequences below presented in an arbitrarily
chosen order. each sequence consists of 20 symbols from the set {a, c, g, t}. the task is to try to cluster
these sequences into two groups, based on the assumption that gene sequences in the same cluster follow a
stationary markov chain.

cataggcattctatgtgctg
gtgcctggacctgaaaagcc
gttggtcagcacacggactg
taagtgtcctctgctcctaa
gccaagcagggtctcaactt

ccagttacggacgid35aaag
cggid35cgcctid35ggaacg
cctcccctcccctttcctgc
caccatcacccttgctaagg
catggactgctccacaaagg

tggaaccttaaaaaaaaaaa
aaagtgctctgaaaactcac
cactacggctacctgggcaa
aaagaactcccctccctgcc
aaaaaaacgaaaaacctaag

gtctcctgccctctctgaac
acatgaactacatagtataa
cggtid35tid35aggcactc
caaatgcctcacgcgtctca
gcgtaaaaaaagtcctgggt

(23.1.26)

a simple approach is to assume that the sequences are generated from a two-component h = 2 mixture of
markov models and train the model using maximum likelihood. the likelihood has local optima so that the
procedure needs to be run several times and the solution with the highest likelihood chosen. one can then
assign each of the sequences by examining p(h = 1|vn
1:t ). if this posterior id203 is greater than 0.5, we
assign it to cluster 1, otherwise to cluster 2. using this procedure, we    nd the following clusters:

cataggcattctatgtgctg
ccagttacggacgid35aaag
cggid35cgcctid35ggaacg
acatgaactacatagtataa
gttggtcagcacacggactg
cactacggctacctgggcaa
cggtid35tid35aggcactcg
caccatcacccttgctaagg
caaatgcctcacgcgtctca
gccaagcagggtctcaactt
catggactgctccacaaagg

tggaaccttaaaaaaaaaaa
gtctcctgccctctctgaac
gtgcctggacctgaaaagcc
aaagtgctctgaaaactcac
cctcccctcccctttcctgc
taagtgtcctctgctcctaa
aaagaactcccctccctgcc
aaaaaaacgaaaaacctaag
gcgtaaaaaaagtcctgggt

(23.1.27)

where sequences in the    rst column are assigned to cluster 1, and sequences in the second column to cluster
2. in this case the data in (23.1.26) were in fact generated by a two-component markov mixture and the
posterior assignment (23.1.27) is in agreement with the known clusters. see demomixmarkov.m

draft november 9, 2017

473

id48

h1

v1

h2

v2

h3

v3

h4

v4

figure 23.4: a    rst order hidden markov model with
   hidden    variables dom(ht) = {1, . . . , h}, t = 1 : t .
the    visible    variables vt can be either discrete or con-
tinuous.

23.2 id48

the hidden markov model (id48) de   nes a markov chain on hidden (or    latent   ) variables h1:t . the
observed (or    visible   ) variables are dependent on the hidden variables through an emission p(vt|ht). this
de   nes a joint distribution

p(h1:t , v1:t ) = p(v1|h1)p(h1)

p(vt|ht)p(ht|ht   1)

(23.2.1)

for which the graphical model is depicted in    g(23.4). for a stationary id48 the transition p(ht|ht   1) and
emission p(vt|ht) distributions are constant through time. the use of the id48 is widespread and a subset
of the many applications of id48s is given in section(23.5).

t(cid:89)

t=2

de   nition 23.3 (transition distribution). for a stationary id48 the transition distribution p(ht+1|ht) is
de   ned by the h    h transition matrix

(cid:48)
ai(cid:48),i = p(ht+1 = i

|ht = i)

and an initial distribution

ai = p(h1 = i).

(23.2.2)

(23.2.3)

de   nition 23.4 (emission distribution). for a stationary id48 and emission distribution p(vt|ht) with
discrete states vt     {1, . . . , v }, we de   ne a v    h emission matrix

bi,j = p(vt = i|ht = j)

(23.2.4)

for continuous outputs, ht selects one of h possible output distributions p(vt|ht), ht     {1, . . . , h}.

in the engineering and machine learning communities, the term id48 typically refers to the case of discrete
variables ht, a convention that we adopt here. in statistics the term id48 often refers to any model with
the independence structure in equation (23.2.1), regardless of the form of the variables ht (see for example
[58]).

23.2.1 the classical id136 problems

the common id136 problems in id48s are summarised below:

filtering
prediction
smoothing
likelihood
most likely hidden path (viterbi alignment)

(inferring the present)
(inferring the future)
(inferring the past)

p(ht|v1:t)
p(ht|v1:s)
p(ht|v1:u)
p(v1:t )
argmax

h1:t

p(h1:t|v1:t )

t > s
t < u

the most likely hidden path problem is termed viterbi alignment in the engineering and id103
literature. all these classical id136 problems are computationally straightforward since the distribution
is singly-connected, so that any standard id136 method can be adopted for these problems. the factor
graph and junction trees for the    rst order id48 are given in    g(23.5). in both cases, after suitable setting
of the factors and clique potentials,    ltering corresponds to passing messages from left to right and upwards;

474

draft november 9, 2017

id48

smoothing corresponds to a valid schedule of message passing/absorption both forwards and backwards along
all edges. it is also straightforward to derive appropriate recursions directly, as we will do below. this is
both instructive and also useful in constructing compact and numerically stable algorithms. the algorithms
we derive below only exploit the conditional independence statements in the belief network structure so
that analogous procedures hold for all models which have these conditional independencies; for example for
continuous states on replacing summation with integration.

23.2.2 filtering p(ht|v1:t)
filtering refers to ascertaining the distribution of the latent variable ht given all information up to the present
v1:t. to do so, we    rst compute the joint marginal p(ht, v1:t) from which the conditional marginal p(ht|v1:t)
can subsequently be obtained by normalisation. a recursion for p(ht, v1:t) is obtained by considering:

p(ht, v1:t) =

=

=

p(ht, ht   1, v1:t   1, vt)

ht   1)p(ht|   v1:t   1, ht   1)p(v1:t   1, ht   1)

p(vt|   v1:t   1, ht,   
p(vt|ht)p(ht|ht   1)p(ht   1, v1:t   1)

(23.2.5)

(23.2.6)

(23.2.7)

(cid:88)
(cid:88)
(cid:88)

ht   1

ht   1

ht   1

the cancellations follow from the conditional independence assumptions of the model. hence if we de   ne

  (ht) = p(ht, v1:t)

equation (23.2.7) above gives the   -recursion
(cid:125)
p(ht|ht   1)  (ht   1)

(cid:124) (cid:123)(cid:122) (cid:125)

  (ht) = p(vt|ht)

(cid:123)(cid:122)

corrector

ht   1

,

predictor

(cid:88)
(cid:124)

with

  (h1) = p(h1, v1) = p(v1|h1)p(h1)

t > 1

(23.2.8)

(23.2.9)

(23.2.10)

this recursion has the interpretation that the    ltered distribution   (ht   1) is propagated forwards by the
dynamics for one timestep to reveal a new    prior    distribution at time t. this distribution is then modulated
by the observation vt, incorporating the new evidence into the    ltered distribution (this is also referred to
as a predictor-corrector method). since each    is smaller than 1, and the recursion involves multiplication
by terms less than 1, the      s can become very small. to avoid numerical problems it is therefore advisable
to work with log   (ht), see id48forward.m.

normalisation gives the    ltered posterior

p(ht|v1:t)       (ht)

to working with log    messages is to work with normalised    messages so that(cid:80)

if we only require the    ltered posterior we are free to rescale the      s as we wish. in this case an alternative

(23.2.11)

ht   (ht) = 1.

h1

v1

h3

v3

h2

v2

(a)

h4

v4

v1, h1

h1

h1, h2

h2

h2, h3

h3

h3, h4

h2

v2, h2

h3

v3, h3

h4

v4, h4

(b)

figure 23.5: (a): factor graph for the    rst order id48 of    g(23.4). (b): junction tree for    g(23.4).

draft november 9, 2017

475

we can write equation (23.2.7) above directly as a recursion for the    ltered distribution

p(ht|v1:t)    

p(vt|ht)p(ht|ht   1)p(ht   1|v1:t   1)

t > 1

(23.2.12)

(cid:88)

ht   1

id48

intuitively, the term p(ht   1|v1:t   1) has the e   ect of removing all nodes in the graph before time t     1 and
replacing their in   uence by a modi   ed    prior    distribution on ht. one may interpret p(vt|ht)p(ht|ht   1) as a
likelihood, giving rise to the joint posterior p(ht, ht   1|v1:t) under bayesian updating. at the next timestep
the previous posterior becomes the new prior.

23.2.3 parallel smoothing p(ht|v1:t )
there are two main approaches to computing p(ht|v1:t ). perhaps the most common in the id48 literature
is the parallel method which is equivalent to message passing on factor graphs. in this one separates the
smoothed posterior into contributions from the past and future:

p(ht, v1:t ) = p(ht, v1:t, vt+1:t ) = p(ht, v1:t)

=   (ht)  (ht)

(23.2.13)

(cid:124)

(cid:123)(cid:122)

past

(cid:125)

(cid:124)
(cid:125)
p(vt+1:t|ht,  v1:t)

(cid:123)(cid:122)

future

the cancellation above occurs due to the fact that ht d-separates the past from the future. the term   (ht)
is obtained from the    forward       recursion, (23.2.9). the   (ht) term may be obtained using a    backward      
recursion as we show below. the forward and backward recursions are independent and may therefore be
run in parallel, with their results combined to obtain the smoothed posterior.

the    recursion

p(vt:t|ht   1) =

=

=

(cid:88)
(cid:88)
(cid:88)

ht

ht

ht

de   ning

p(vt, vt+1:t , ht|ht   1)
p(vt|    vt+1:t , ht,   
p(vt|ht)p(vt+1:t|ht,   

ht   1)p(vt+1:t , ht|ht   1)

ht   1)p(ht|ht   1)

  (ht)     p(vt+1:t|ht)

equation (23.2.16) above gives the   -recursion

  (ht   1) =

p(vt|ht)p(ht|ht   1)  (ht),

2     t     t

(cid:88)

ht

(23.2.14)

(23.2.15)

(23.2.16)

(23.2.17)

(23.2.18)

(cid:80)

with   (ht ) = 1. as for the forward pass, working in log space is recommended to avoid numerical di   culties.
if one only desires posterior distributions, one can also perform local normalisation at each stage to give
ht   (ht) = 1 since only the relative magnitude of the components of    are of importance. the smoothed

posterior is then given by

p(ht|v1:t )       (ht) =

(cid:80)

  (ht)  (ht)
ht   (ht)  (ht)

(23.2.19)

together the           recursions are called the forward-backward algorithm.
23.2.4 correction smoothing

an alternative to the parallel method is to form a recursion directly for the smoothed posterior. this can
be achieved by recognising that conditioning on the present makes the future redundant:

(cid:88)

ht+1

p(ht, ht+1|v1:t ) =

(cid:88)

ht+1

p(ht|v1:t ) =

476

p(ht|ht+1, v1:t,    vt+1:t )p(ht+1|v1:t )

(23.2.20)

draft november 9, 2017

id48

this gives a recursion for   (ht)     p(ht|v1:t ):

(cid:88)

ht+1

  (ht) =

p(ht|ht+1, v1:t)  (ht+1)

(23.2.21)

with   (ht )       (ht ). the term p(ht|ht+1, v1:t) may be computed using the    ltered results p(ht|v1:t):

p(ht|ht+1, v1:t) =

where the term p(ht+1|v1:t) = (cid:80)

p(ht+1, ht|v1:t)
p(ht+1|v1:t)

=

p(ht+1|ht)p(ht|v1:t)

p(ht+1|v1:t)

(23.2.22)

ht p(ht+1|ht)p(ht|v1:t) can be found by normalisation. this is a form of
dynamics reversal , as if we are reversing the direction of the hidden to hidden arrow in the id48. this
procedure, also termed the rauch-tung-striebel smoother1, is sequential since we need to    rst complete
the    recursions, after which the    recursion may begin. this is a so-called correction smoother since it
   corrects    the    ltered result. interestingly, once    ltering has been carried out, the evidential states v1:t are
not needed during the subsequent    recursion. the           and           recursions are related through

  (ht)       (ht)  (ht)

(23.2.23)

computing the pairwise marginal p(ht, ht+1|v1:t )
to implement the em algorithm for learning, section(23.3.1), we require terms such as p(ht, ht+1|v1:t ).
these can be obtained by message passing on either a factor graph or junction tree (for which the pairwise
marginals are contained in the cliques, see    g(23.4b)). alternatively, an explicit recursion is as follows:

p(ht, ht+1|v1:t )     p(v1:t, vt+1, vt+2:t , ht+1, ht)
= p(vt+2:t|((((((
= p(vt+2:t|ht+1)p(vt+1|    
= p(vt+2:t|ht+1)p(vt+1|ht+1)p(ht+1|  v1:t, ht)p(v1:t, ht)

v1:t, vt+1, ht, ht+1)p(v1:t, vt+1, ht+1, ht)

v1:t, ht, ht+1)p(v1:t, ht+1, ht)

rearranging, we therefore have

p(ht, ht+1|v1:t )       (ht)p(vt+1|ht+1)p(ht+1|ht)  (ht+1)

see id48smooth.m.

the likelihood p(v1:t )

the likelihood of a sequence of observations can be computed from

(cid:88)

p(v1:t ) =

p(ht , v1:t ) =

  (ht )

ht

ht

an alternative computation can be found by making use of the decomposition

(cid:88)

t(cid:89)

t=1

each factor can be computed using

p(v1:t ) =

p(vt|v1:t   1) =

p(vt|v1:t   1)
(cid:88)
(cid:88)
(cid:88)

ht

ht

=

=

ht

p(vt, ht|v1:t   1)
p(vt|ht,   v1:t   1)p(ht|v1:t   1)
(cid:88)
p(vt|ht)

ht   1

p(ht|ht   1,   v1:t   1)p(ht   1|v1:t   1)

(23.2.24)

(23.2.25)

(23.2.26)

(23.2.27)

(23.2.28)

(23.2.29)

(23.2.30)

1it is most common to use this terminology for the continuous variable case, though we adopt it here also for the discrete

variable case.

draft november 9, 2017

477

id48

(cid:88)

where the    nal term p(ht   1|v1:t   1) is the    ltered result.
in both approaches the likelihood of an output sequence requires only a forward computation (   ltering). if
required, one can also compute the likelihood using equation (23.2.13),

p(v1:t ) =

  (ht)  (ht)

(23.2.31)

ht

which is valid for any 1     t     t .
23.2.5 sampling from p(h1:t|v1:t )
sometimes one wishes to sample joint trajectories h1:t from the posterior p(h1:t|v1:t ). a general purpose
method is described in section(27.2.2) which can be applied in this case by noting that conditioned on the
observations v1:t , one can write the latent distribution as a markov network. this means that the posterior
distribution p(h1:t|v1:t ) is a simple linear markov chain and we can reexpress the distribution as

p(h1:t|v1:t ) = p(h1|h2, v1:t ) . . . p(ht   1|ht , v1:t )p(ht|v1:t )

where, from equation (23.2.25),

p(ht   1|ht, v1:t )     p(ht   1, ht|v1:t )       (ht   1)p(ht|ht   1)

(23.2.32)

(23.2.33)

sampling then begins by    rst drawing a state ht from p(ht|v1:t ). given this state, one then draws from
p(ht   t|ht , v1:t ) using equation (23.2.33). this procedure is known as forward-   ltering-backward-sampling
since one    rst needs to run    ltering to compute the required    time-reversed    transitions p(ht   1|ht, v1:t ).
23.2.6 most likely joint state

the most likely path h1:t of p(h1:t|v1:t ) is the same as the most likely state (for    xed v1:t ) of

p(h1:t , v1:t ) =

p(vt|ht)p(ht|ht   1)

(23.2.34)

(cid:89)

t

the most likely path can be found using the max-product version of the factor graph or max-absorption on
the junction tree. alternatively, an explicit derivation can be obtained by considering:

max
ht

p(vt|ht)p(ht|ht   1) =

p(vt|ht)p(ht|ht   1)

(cid:40)t   1(cid:89)

t=1

(cid:41)

t(cid:89)

t=1

(cid:124)

max
ht

(cid:125)
p(vt|ht )p(ht|ht   1)

(cid:123)(cid:122)

  (ht   1)

(23.2.35)

the message   (ht   1) conveys information from the end of the chain to the penultimate timestep. we can
continue in this manner, de   ning the recursion

  (ht   1) = max
ht

p(vt|ht)p(ht|ht   1)  (ht),

2     t     t

(23.2.36)

with   (ht ) = 1. this means that the e   ect of maximising over h2, . . . , ht is compressed into a message
  (h1) so that the most likely state h   

1 is given by

   
h
1 = argmax

h1

p(v1|h1)p(h1)  (h1)

once computed, backtracking gives

   
h
t = argmax

ht

p(vt|ht)p(ht|h

   
t   1)  (ht)

(23.2.37)

(23.2.38)

this special case of the max-product algorithm is called the viterbi algorithm. similarly, one may use the
n -max-product algorithm, section(5.2.1), to obtain the n -most likely hidden paths.

478

draft november 9, 2017

figure 23.6: localising the burglar. the latent variable ht    
{1, . . . , 25} denotes the positions, de   ned over the 5    5 grid
of the ground    oor of the house. (a): a representation of the
id203 that the       oor will creak    at each of the 25 positions,
p(vcreak|h). light squares represent id203 0.9 and dark
(b): a representation of the id203 p(vbump|h)
square 0.1.
that the burglar will bump into something in each of the 25 po-
sitions.

id48

(a)    creaks   

(b)    bumps   

(a) creaks and bumps

(b) filtering

(c) smoothing

(d) viterbi

(e) true burglar position

(cid:16)

(cid:17)

t

t

, vbump

, where vcreak

vcreak
t
= 2 otherwise) and vbump

(a): each panel represents the
= 1 means that there was a    creak in the    oorboard   

figure 23.7: localising the burglar through time for 10 time steps.
visible information vt =
(vcreak
= 1 meaning    bumped into something    (and is in state 2 otherwise). there
are 10 panels, one for each time t = 1, . . . , 10. the left half of the panel represents v1
t and the right half v2
t .
(b): the
the lighter shade represents the occurrence of a creak or bump, the darker shade the absence.
(c): the smoothed distribution
   ltered distribution p(ht|v1:t) representing where we think the burglar is.
p(ht|v1:10) that represents the distribution of the burglar   s position given that we know both the past and
future observations. (d): the most likely (viterbi) burglar path arg maxh1:10 p(h1:10|v1:10). (e): the actual
path of the burglar.

t

t

23.2.7 prediction

the one-step ahead predictive distribution is given by

(cid:88)

ht,ht+1

p(vt+1|v1:t) =

p(vt+1|ht+1)p(ht+1|ht)p(ht|v1:t)

(23.2.39)

example 23.3 (a localisation example). you   re asleep upstairs in your house and awoken by noises from
downstairs. you realise that a burglar is on the ground    oor and attempt to understand where he is from
listening to his movements. you mentally partition the ground    oor into a 5   5 grid. for each grid position
you know the id203 that if someone is in that position the    oorboard will creak,    g(23.6a). similarly
you know for each position the id203 that someone will bump into something in the dark,    g(23.6b).
the    oorboard creaking and bumping into objects can occur independently. in addition you assume that
the burglar will move only one grid square     forwards, backwards, left or right in a single timestep. based
on a series of bump/no bump and creak/no creak information,    g(23.7a), you try to    gure out based on
your knowledge of the ground    oor, where the burglar might be.

we can represent the scenario using a id48 where h     {1, . . . , 25} denotes the grid square. the visible
variable has a composite form v = vcreak     vbump where vcreak and vbump each take two states. to use our

draft november 9, 2017

479

m1

m2

m3

h1

v1

h2

v2

h3

v3

h4

v4

id48

figure 23.8: a model for robot self-localisation. at
each time the robot makes an intended movement, mt.
as a generative model, knowing the intended move-
ment mt and the current grid position ht, the robot
has an idea of where he should be at the next time-
step and what sensor reading vt+1 he would expect
there. based on only the sensor information v1:t and
the intended movements m1:t , the task is to infer a
distribution over robot locations p(h1:t|m1:t , v1:t ).

standard code we form a new visible variable v with four states using

p(v|h) = p(vcreak|h)p(vbump|h)

(23.2.40)

based on the past information, our belief as to where the burglar might be is represented by the    ltered
distribution p(ht|v1:t),    g(23.7). in the beginning the    ltered distribution has signi   cant mass in many states
since we don   t yet have su   cient information to establish where the burglar is. as time goes along and we
gather more information, the    ltered distribution becomes more concentrated in a small number of states.
after the burglar has left at t = 10, the police arrive and try to piece together where the burglar went,
based on the sequence of creaks and bumps you provide. at any time t, the information as to where the
burglar could have been is represented by the smoothed distribution p(ht|v1:10). the smoothed distribution
is generally more concentrated in a few states than the    ltered distribution since we have more information
(from the past and future) to help establish the burglar   s position at that time. the police   s single best-guess
for the trajectory the burglar took is provided by the most likely joint hidden state arg maxh1:10 p(h1:10|v1:10).
see demoid48burglar.m.

23.2.8 self localisation and kidnapped robots

a robot has an internal grid-based map of its environment and for each location h     {1, . . . , h} knows the
likely sensor readings he would expect in that location. the robot is    kidnapped    and placed somewhere in
the environment. the robot then starts to move, gathering sensor information. based on these readings
v1:t and intended movements m1:t, the robot attempts to    gure out his location by comparing his actual
sensor readings with his internal map of expected sensor readings for each location. due to wheel slippage
on the    oor an intended action by the robot, such as    move forwards   , might not be successful. given all
the information the robot has, he would like to infer p(ht|v1:t, m1:t). this problem di   ers from the burglar
scenario in that the robot now has knowledge of the intended movements he makes. this should give more
information as to where he could be. one can view this as extra    visible    information, though it is more
natural to think of this as additional input information. a model of this scenario is, see    g(23.9),

t(cid:89)

t=1

t(cid:89)

t=1

p(v1:t , m1:t , h1:t ) =

p(vt|ht)p(ht|ht   1, mt   1)p(mt)

(23.2.41)

the visible variables v1:t are known, as are the intended movements m1:t . the model expresses that the
movements selected by the robot are random (hence no decision making in terms of where to go next).
we assume that the robot has full knowledge of the conditional distributions de   ning the model (he knows
the    map    of his environment and all state transition and emission probabilities). if our interest is only in
localising the robot, since the inputs m are known, this model is in fact a form of time-dependent id48:

p(v1:t , h1:t ) =

p(vt|ht)p(ht|ht   1, t)

(23.2.42)

for a time-dependent transition p(ht|ht   1, t) de   ned by the intended movement mt   1. any id136 task
required then follows the standard stationary id48 algorithms, albeit on replacing the time-independent

480

draft november 9, 2017

id48

(a)

(b)

(c)

figure 23.9: filtering and smoothing for robot tracking using a id48 with s = 50. (a): a realisation from
the id48 example described in the text. the dots indicate the true latent locations of the robot, whilst
the open circles indicate the noisy measured locations. (b): the squares indicate the    ltering distribution
at each timestep t, p(ht|v1:t). this id203 is proportional to the grey level with black corresponding to
1 and white to 0. note that the posterior for the    rst timesteps is multimodal, therefore the true position
(c): the squares indicate the smoothing distribution at each timestep t,
cannot be accurately estimated.
p(t|v1:t ). note that, for t < t , we estimate the position retrospectively and the uncertainty is signi   cantly
lower when compared to the    ltered estimates.

transitions p(ht|ht   1) with the known time-dependent transitions.
in self localisation and mapping (slam) the robot does not know the map of his environment. this corre-
sponds to having to learn the transition and emission distributions on-the-   y as he explores the environment.

example 23.4 (robot localisation).

consider the following toy tracking problem (from taylan cemgil). a robot
is moving around a circular corridor and at any time occupies one of s
possible locations, as indicated. at each timestep t, the robot stays where
it is with id203  , or moves to the next point in a counter-clockwise
direction with id203 1      .
this can be conveniently represented by an s    s matrix a with elements aji = p(ht = j|ht   1 = i). for
example, for s = 3, we have

0 1 0
0 0 1

       1 0 0
       1 0 0

0 1 0
0 0 1

       + (1      )
       +

(1     w)

3

1 0 0
0 1 0

       0 0 1
       .
       .
       1 1 1

1 1 1
1 1 1

a =  

b = w

at each timestep t, the robot sensors measure its position, obtaining either the correct location with prob-
ability w or a uniformly random location with id203 1     w. for example, for s = 3, we have

a typical realisation y1:t from the process de   ned by this id48 with s = 50,   = 0.5, t = 30 and w = 0.3 is
depicted in    g(23.9a). we are interested in inferring the true locations of the robot from the noisy measured
locations. at each time t, the true location can be inferred from the    ltered posterior p(ht|v1:t),    g(23.9b),
which uses measurements up to t; or from the smoothed posterior p(ht|v1:t ),    g(23.9c), which uses both
past and future observations and is therefore generally more accurate.

draft november 9, 2017

481

(23.2.43)

(23.2.44)

23.2.9 natural language models

a simple generative model of language can be obtained from the letter-to-letter transitions (a so-called
bigram). in the example below, we use this in a id48 to clean up mis-typings.

learning id48s

example 23.5 (stubby    ngers). a    stubby    ngers    typist has the tendency to hit either the correct key
or a single neighbouring key. for simplicity we assume that there are 27 keys: lower case a to lower case z
and the space bar. to model this we use an emission distribution bij = p(v = i|h = j) where i = 1, . . . , 27,
j = 1, . . . , 27, as depicted in    g(23.10). a database of letter-to-next-letter frequencies, yields the transition
matrix aij = p(h(cid:48) = i|h = j) in english. for simplicity we assume that p(h1) is uniform. also we assume

that each intended key press results in a single press. given a typed sequence kezrninh what is the most
likely word that this corresponds to? by listing the 200 most likely hidden sequences (using the n -max-
product algorithm) and discarding those that are not in a standard english dictionary, the most likely word
that was intended is learning. see demoid48bigram.m.

(a)

(b)

(a): the letter-to-letter transition matrix for english p(h(cid:48) = i|h = j).

(b): the letter
figure 23.10:
emission matrix for a typist with    stubby    ngers    in which the key or a neighbour on the keyboard is likely
to be hit.

23.3 learning id48s

given a set of data v =(cid:8)v1, . . . , vn(cid:9) of n sequences, where sequence vn = vn

1:tn is of length tn, we seek
the id48 transition matrix a, emission matrix b, and initial vector a most likely to have have generated
v. we make the i.i.d. assumption so that each sequence is independently generated and assume that we
know the number of hidden states h. for simplicity we concentrate here on the case of discrete visible
variables, assuming also we know the number of states v . whilst implementing either em or gradient
based maximum likelihood is straightforward for the id48, the likelihood has many local optima, and care
needs to be taken with initialisation.

23.3.1 em algorithm

the application of em to the id48 model is called the baum-welch algorithm and follows the general
strategy outlined in section(11.2). the em algorithm is convenient in this case and leads to closed form
expressions for the m-step.

482

draft november 9, 2017

  abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz 00.10.20.30.40.50.60.70.80.9  abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz 0.050.10.150.20.250.30.350.40.450.50.55learning id48s

m-step

n(cid:88)

n=1

n(cid:88)

n=1

(cid:104)log p(vn
(cid:40)

assuming i.i.d. data, the m-step is given by maximising the    energy   :

1 , vn

2 . . . , vn

t n, hn

1 , hn

2 , . . . , hn

t n)(cid:105)pold(hn|vn)

(23.3.1)

(cid:41)

tn(cid:88)

t=1

tn   1(cid:88)

t=1

n(cid:88)

n=1

with respect to the parameters a, b, a; hn denotes h1:tn. using the form of the id48, we obtain

(cid:104)log p(h1)(cid:105)pold(h1|vn) +

(cid:104)log p(ht+1|ht)(cid:105)pold(ht,ht+1|vn) +

(cid:104)log p(vn

t |ht)(cid:105)pold(ht|vn)

(23.3.2)

where for compactness we drop the sequence index from the h variables. to avoid potential confusion, we
write pnew(h1 = i) to denote the (new) table entry for the id203 that the initial hidden variable is in
state i. optimising equation (23.3.2) with respect to p(h1), (and enforcing p(h1) to be a distribution) we
obtain

anew
i     pnew(h1 = i) =

1
n

pold(h1 = i|vn)

(23.3.3)

which is the average number of times (with respect to pold) that the    rst hidden variable is in state i.
similarly, the m-step for the transition is

(cid:48)
i(cid:48),i     pnew(ht+1 = i
anew

|ht = i)    

(cid:48)
pold(ht = i, ht+1 = i

|vn)

(23.3.4)

which is the number of times that a transition from hidden state i to hidden state i(cid:48) occurs, averaged over
all times (since we assumed stationarity) and training sequences. normalising, we obtain

n(cid:88)

tn   1(cid:88)

n=1

t=1

n(cid:88)

tn(cid:88)

n=1

t=1

(cid:80)n
(cid:80)
i(cid:48)(cid:80)n

n=1

(cid:80)tn   1
(cid:80)tn   1
t=1 pold(ht = i, ht+1 = i(cid:48)
t=1 pold(ht = i, ht+1 = i(cid:48)

n=1

|vn)
|vn)

anew

i(cid:48),i =

finally, the m-step update for the emission is

bnew
j,i     pnew(vt = j|ht = i)    

i [vn

t = j] pold(ht = i|vn)

(23.3.5)

(23.3.6)

which is the expected number of times that, for the observation being in state j, the hidden state is i. the
proportionality constant is determined by the normalisation requirement.

e-step

in computing the m-step above the quantities pold(h1 = i|vn), pold(ht = i, ht+1 = i(cid:48)

are obtained by smoothed id136 using the techniques described in section(23.2.1).

|vn) and pold(ht = i|vn)

equations (23.3.3,23.3.5,23.3.6) are repeated until convergence. see id48em.m and demoid48learn.m.

parameter initialisation

the em algorithm converges to a local maximum of the likelihood and, in general, there is no guarantee that
the algorithm will    nd the global maximum. how best to initialise the parameters is a thorny issue, with a
suitable initialisation of the emission distribution often being critical for success[247]. a practical strategy is
h p(v|h)p(h)

to initialise the emission p(v|h) based on    rst    tting a simpler non-temporal mixture model(cid:80)

to the data.

draft november 9, 2017

483

learning id48s

continuous observations

for a continuous vector observation vt, with dim (vt) = d, we require a model p(vt|ht) mapping the discrete
state ht to a distribution over outputs. using a continuous output does not change any of the standard
id136 message passing equations so that id136 can be carried out for essentially arbitrarily complex
emission distributions. indeed, for    ltering, smoothing and viterbi id136, the normalisation z of the
emission p(v|h) =   (v, h)/z is not required. for learning, however, the emission normalisation constant is
required since this is dependent on the parameters of the model.

23.3.2 mixture emission

to make a richer emission model (particularly for continuous observations), one approach is use a mixture

p(vt|ht) =

p(vt|kt, ht)p(kt|ht)

(23.3.7)

(cid:88)

kt

where kt is a discrete summation variable. for learning, it is useful to consider the kt as additional latent
variables; the em algorithm then carries through in a straightforward way. using the notation q as shorthand
for pold, the e-step sets:

t , kt)p(kt|ht)

t )     p(vn|hn
q(kt|hn
t(cid:88)
(cid:88)
the energy is given by

(cid:68)

e =

n

t=1

   (cid:104)log q(kt|hn

t )(cid:105)q(kt|hn

t ) + (cid:104)log p(vn

t |kt, hn

t )(cid:105)q(kt|hn

t ) + (cid:104)log p(kt|hn

t )(cid:105)q(kt|hn
t )

the contribution from each emission component p(v = v|h = h, k = k) is

(cid:88)

t(cid:88)

n

t=1

q(kt = k|hn

t = h)q(hn

t = h|vn

1:t ) log p(vn

t |h = h, k = k)

(cid:69)

q(hn

t |vn

1:t )

(23.3.8)

(23.3.9)

(23.3.10)

for    xed q(kt = k|hn
t = h), one needs to numerically optimise the above expression with respect to the
emission parameters. similarly, the contribution to the energy bound from the mixture weights is given by

log p(k = k|h = h)

q(kt = k|hn

t = h)q(hn

t = h|vn

1:t )

so that the m-step update for the mixture weights is,

p(k = k|h = h)    

q(kt = k|hn

t = h)q(hn

t = h|vn

1:t )

(cid:88)

t(cid:88)

n

t=1

(cid:88)

t(cid:88)

n

t=1

(23.3.11)

(23.3.12)

1:t ) are    xed, during which the emissions p(v|h, k) are learned, along with updating q(kt = k|hn

in this case the em algorithm is composed of an    emission    em loop in which the transitions and q(hn
h|vn
the    transition    em loop    xes the emission distribution p(v|h) and learns the best transition p(ht|ht   1).
23.3.3 the id48-gmm

t =
t = h).

a common continuous observation mixture emission model component is a gaussian

(cid:0)vt   kt,ht,   kt,ht

(cid:1)

p(vt|kt, ht) = n

(23.3.13)

so that kt, ht indexes the k    h mean vectors and covariance matrices. em updates for these means and
covariances are straightforward to derive from equation (23.3.9), see exercise(23.14). these models are
common in tracking applications and id103 (usually under the constraint that the covariances
are diagonal).

484

draft november 9, 2017

related models

c1

h1

v1

c2

h2

v2

c3

h3

v3

c4

h4

v4

23.3.4 discriminative training

figure 23.11: an explicit duration id48. the counter
variables ct deterministically count down to zero.
when they reach one, a h transition is allowed, and a
new value for the duration ct is sampled.

id48s can be used for supervised learning of sequences. that is, for each sequence vn
1:t , we have a cor-
responding class label cn. for example, we might associate a particular composer c     {1, . . . , c} with a
sequence v1:t and wish to make a model that will predict the composer for a novel music sequence. a
generative approach to using id48s for classi   cation is to train a separate id48 for each class, p(v1:t|c)
and subsequently use bayes    rule to form the classi   cation for a novel sequence v   

1:t using

if the data are noisy and di   cult to model, however, this generative approach may not work well since
much of the expressive power of each model is used to model the complex data, rather than focussing on
the decision boundary. in applications such as id103, improvements in performance are often
reported when the models are trained in a discriminative way. in discriminative training, see for example
[164], one de   nes a new single discriminative model, formed from the c id48s using

   
p(c

|v

   
1:t ) =

(cid:80)c
p(v   
c(cid:48)=1 p(v   

1:t|c   )p(c   )

1:t|c(cid:48))p(c(cid:48))

p(c|v1:t ) =

(cid:80)c
p(v1:t|c)p(c)
c(cid:48)=1 p(v1:t|c(cid:48))p(c(cid:48))

and then maximises the likelihood of a set of observed classes and corresponding observations v1:t . for a
single data pair, (cn, vn

1:t ), the log likelihood is

log p(cn|vn

1:t ) = log p(vn

+ log p(cn)     log

(cid:124)

(cid:125)
(cid:123)(cid:122)
1:t|cn)

generative likelihood

c(cid:88)

c(cid:48)=1

p(vn

(cid:48)
1:t|c

(cid:48)
)p(c

)

(23.3.16)

the    rst term above represents the generative likelihood term, with the last term accounting for the dis-
crimination. whilst deriving em style updates is hampered by the discriminative terms, computing the
gradient is straightforward using the technique described in section(11.6).

23.4 related models

23.4.1 explicit duration model
for a id48 with self-transition p(ht = i|ht   1 = i)       i, the id203 that the latent dynamics stays in
state i for    timesteps is     
i , which decays exponentially with time. in practice, however, we would often
like to constrain the dynamics to remain in the same state for a minimum number of timesteps, or to have
a speci   ed duration distribution. a way to enforce this is to use a latent counter variable ct which at the
beginning is initialised to a duration sampled from the duration distribution pdur(ct) with maximal duration
dmax. then at each timestep the counter decrements by 1, until it reaches 1, after which a new duration is
sampled:

(cid:26)    (ct, ct   1     1)
(cid:26)    (ht, ht   1)

pdur(ct)

p(ct|ct   1) =

ct   1 > 1
ct   1 = 1

the state ht can transition only when ct = 1:

p(ht|ht   1, ct) =

ptran(ht|ht   1)

ct > 1
ct = 1

draft november 9, 2017

(23.3.14)

(23.3.15)

(23.4.1)

(23.4.2)

485

related models

including the counter variable c de   nes a joint latent distribution p(c1:t , h1:t ) that ensures h remains in a
desired minimal number of timesteps, see    g(23.11). since dim (ct     ht) = dmaxh, naively the computa-
and backward recursions, the deterministic nature of the transitions means that this can be reduced to

tional complexity of id136 in this model scales as o(cid:0)t h 2d2
o(cid:0)t h 2dmax

(cid:1). however, when one runs the forward

(cid:1)[215]     see also exercise(23.15).

max

the hidden semi-markov model generalises the explicit duration model in that once a new duration ct is
sampled, the model emits a distribution p(vt:t+ct   1|ht) de   ned on a segment of the next ct observations[233].
23.4.2 input-output id48

the ioid48[32] is a id48 with additional input variables x1:t , see    g(23.12). each input can be continuous
or discrete and modulates the transitions

p(v1:t , h1:t|x1:t ) =

p(vt|ht, xt)p(ht|ht   1, xt)

(23.4.3)

(cid:89)

t

the ioid48 may be used as a conditional predictor, where the outputs vt represent the prediction at time
t. in the case of continuous inputs and discrete outputs, the tables p(vt|ht, xt) and p(ht|ht   1, xt) are usually
parameterised using a non-linear function, for example

p(vt = y|ht = h, xt = x, w)    

exp

wt

h,y  h,y(x)

(cid:16)

(cid:17)

(23.4.4)

@@

where   h,y(x) is a vector function of the input x. id136 then follows in a similar manner as for the
standard id48. de   ning

@@

(23.4.5)

(23.4.6)

(23.4.7)

(23.4.8)

p(vt|v1:t   1, x1:t, ht, ht   1)p(ht|v1:t   1, x1:t, ht   1)p(v1:t   1, ht   1|x1:t)

p(ht, ht+1|x1:t+1, xt+2:t , v1:t ) =

  (ht)     p(ht, v1:t|x1:t)
the forward pass is given by

(cid:88)
(cid:88)

ht   1

ht   1

  (ht) =

=

p(ht, ht   1, v1:t   1, vt|x1:t)

= p(vt|xt, ht)

p(ht|ht   1, xt)  (ht   1)

(cid:88)
(cid:88)

ht   1

ht+1

the    backward pass is

p(ht|x1:t , v1:t ) =

for which we need

p(ht|ht+1, x1:t+1, v1:t) =

the likelihood can be found from(cid:80)

p(ht+1, ht|x1:t+1, v1:t)
p(ht+1|x1:t+1, v1:t)

ht

x1

h1

v1

x2

h2

v2

x3

h3

v3

  (ht ).

x4

h4

v4

(cid:88)

ht+1

p(ht|ht+1, x1:t+1, v1:t)p(ht+1|x1:t , v1:t )

(cid:80)

p(ht+1|ht, xt+1)p(ht|x1:t, v1:t)
ht p(ht+1|ht, xt+1)p(ht|x1:t, v1:t)

=

(23.4.9)

(23.4.10)

figure 23.12: a    rst order input-output hidden
markov model. the input x and output v nodes are
shaded to emphasise that their states are known dur-
ing training. during testing, the inputs are known and
the outputs are predicted.

486

draft november 9, 2017

t(cid:89)

(cid:33)

p(y1:t|x,   ) =

1

z(x,   )

t=2

(cid:32) k(cid:88)

k=1

related models

x

y2

y3

y1

direction bias

figure 23.13: linear chain crf. since the input x is observed, the distribu-
tion is just a linear chain factor graph. the id136 of pairwise marginals
p(yt, yt   1|x) is therefore straightforward using message passing.

consider predicting the output distribution p(vt|x1:t ) given both past and future input information x1:t .
because the hidden states are unobserved we have p(vt|x1:t ) = p(vt|x1:t). thus the ioid48 prediction uses
only past information and discards any future contextual information. this    direction bias    is sometimes
considered problematic (particularly in natural language modelling) and motivates the use of undirected
models, such as conditional random    elds.

23.4.3 linear chain crfs

linear chain id49 (crfs) are an extension of the unstructured crfs we brie   y
discussed in section(9.6.5) and have application to modelling the distribution of a set of outputs y1:t given
an input vector x. for example, x might represent a sentence in english, and y1:t should represent the
translation into french. note that the vector x does not have to have dimension t . a    rst order linear
chain crf has the form

  t(yt, yt   1, x,   )

(23.4.11)

where    are the free parameters of the potentials. in practice it is common to use potentials of the form

exp

  kfk,t(yt, yt   1, x)

(23.4.12)

where fk,t(yt, yt   1, x) are    features   , see also section(9.6.5). given a set of input-output sequence pairs,
{(xn, yn
1:t ) , n = 1, . . . , n} (assuming all sequences have equal length t for simplicity), we can learn the
parameters    by maximum likelihood. under the standard i.i.d. data assumption, the log likelihood is

l(  ) =

  kfk(yn

t , yn

t   1, xn)    

log z(xn,   )

(23.4.13)

(cid:88)

n

the reader may readily check that the log likelihood is concave so that the objective function has no local
optima. the gradient is given by

   
     i

l(  ) =

fi(yn

t , yn

t   1, xn)     (cid:104)fi(yt, yt   1, xn)(cid:105)p(yt,yt   1|xn,  )

(23.4.14)

(cid:17)

(cid:88)

(cid:88)

t,n

k

(cid:88)

(cid:16)

n,t

learning therefore requires id136 of the marginal terms p(yt, yt   1|x,   ). since equation (23.4.11) cor-
responds to a linear chain factor graph, see    g(23.13), id136 of pairwise marginals is straightforward
using message passing. this can be achieved using either the standard factor graph message passing or by
deriving an explicit algorithm, see exercise(23.10). given the gradient, one may use any standard numeri-
cal optimisation routine to learn the parameters   . in some applications, particularly in natural language
processing, the dimension k of the vector of features f1, . . . , fk may be many hundreds of thousands. this
means that the storage of the hessian is not feasible for newton based training and either limited memory
methods or conjugate gradient techniques are typically preferred[309].
after training we can use the model to    nd the most likely output sequence for a novel input x   . this is
straightforward since

  t(yt, yt   1, x   

,   )

(23.4.15)

y

   
1:t = argmax

y1:t

(cid:89)

t

corresponds again to a simple linear chain, for which max-product id136 yields the required result, see
also exercise(23.9). see example(9.11) for a small example.

draft november 9, 2017

487

t(cid:89)

d(cid:89)

t=1

i=1

applications

figure 23.14: a dynamic id110. possible
transitions between variables at the same time-slice
have not been shown.

figure 23.15: a coupled id48. for example the up-
per id48 might model speech, and the lower the cor-
responding video sequence. the upper hidden units
then correspond to phonemes, and the lower to mouth
positions; this model therefore captures the expected
coupling between mouth positions and phonemes.

x3(t)

x2(t)

x1(t)

x3(t + 1)

x2(t + 1)

x1(t + 1)

v1(t)

v1(t + 1)

h1(t)

h1(t + 1)

h2(t)

h2(t + 1)

v2(t)

v2(t + 1)

23.4.4 dynamic id110s

a dbn is de   ned as a belief network replicated through time. for a multivariate xt, with dim (xt) = d,
the dbn de   nes a joint model

p(x1, . . . , xt ) =

p(xi(t)|x\i(t), x(t     1))

(23.4.16)

where x\i(t) denotes the set of variables at time t, except for xi(t). the form of each p(xi(t)|x\i(t), x(t    1))
is chosen such that the overall distribution remains acyclic. at each time-step t there is a set of variables
xi(t), i = 1, . . . , d, some of which may be observed. in a    rst order dbn, each variable xi(t) has parental
variables taken from the set of variables in the previous time-slice, xt   1, or from the present time-slice. in
most applications, the model is temporally homogeneous so that one may fully describe the distribution in
terms of a two-time-slice model,    g(23.14). the generalisation to higher-order models is straightforward. a
coupled id48 is a special dbn that may be used to model coupled    streams    of information, for example
video and audio, see    g(23.15)[225].

23.5 applications

23.5.1 object tracking

id48s are used to track moving objects, based on an understanding of the dynamics of the object (encoded
in the transition distribution) and an understanding of how an object with a known position would be
observed (encoded in the emission distribution). given an observed sequence, the hidden position can then
be inferred. the burglar, example(3.1) is a case in point. id48s have been applied in many tracking
contexts, including tracking people in videos, musical pitch, and many more[58, 247, 55].

23.5.2 automatic id103

many id103 systems make use of id48s[322]. roughly speaking, the raw scalar speech signal
x1:t is    rst translated into a stream of continuous acoustic vectors v1:t where at time t, vt represents which
frequencies are present in the speech signal in a small window around time t. these acoustic vectors are
typically formed from taking a discrete fourier transform of the speech signal over a small window around
time t, with additional transformations to mimic human auditory processing. alternatively, related forms
of linear coding of the observed acoustic waveform may be used[144].

the corresponding discrete latent state ht represents a phoneme     a basic unit of human speech (for which
there are 44 in standard english). training data is painstakingly constructed by a human linguist who

488

draft november 9, 2017

applications

determines the phoneme ht for each time t and many di   erent observed sequences x1:t . given then each
acoustic vector vt and an associated phoneme ht, one may use maximum likelihood to    t a mixture of
(usually isotropic) gaussians p(vt|ht) to vt. this forms the emission distribution for a id48.
using the database of labelled phonemes, the phoneme transition p(ht|ht   1) can be learned (by simple count-
ing) and forms the transition distribution for a id48. note that in this case, since the    hidden    variable
h and observation v are known during training, training the id48 is straightforward and boils down to
training the emission and transition distributions independently using the observed acoustic vectors and
associated phonemes.

for a new sequence of acoustic vectors v1:t we can then use the id48 to infer the most likely phoneme
sequence through time, arg maxh1:t p(h1:t|v1:t ), which takes into account both the way that phonemes
generate acoustic vectors, and also the prior language constraints of phoneme to phoneme transitions. the
fact that people speak at di   erent speeds can be addressed using time-warping in which the latent phoneme
remains in the same state for a number of timesteps. if the id48 is used to model a single word, it is natural
to constrain the hidden state sequence to only go    forwards    through the set of available phonemes (i.e. one
cannot revisit a state, except possibly the current state). in this case the structure of the transition matrices
corresponds to a dag state-transition diagram; under suitable labelling of the states, this can always be
represented by a triangular left-to-right transition matrix .

23.5.3 bioinformatics

in the    eld of bioinformatics id48s have been widely applied to modelling genetic sequences. multiple
sequence alignment using forms of constrained id48s have been particularly successful. other applications
involve gene    nding and protein family modelling[178, 91].

23.5.4 part-of-speech tagging

consider the sentence below in which each word has been linguistically tagged

hospitality_nn is_bez an_at excellent_jj virtue_nn ,_,
but_cc not_xnot when_wrb the_ati guests_nns have_hv
to_to sleep_vb in_in rows_nns in_in the_ati cellar_nn !_!

the linguistic tags are denoted at the end of each word, for example nn is the singular common noun
tag, ati is the article tag etc. given a training set of such tagged sentences, the task is to tag a novel
sentence. one approach is to use ht to be a tag, and vt to be a word and    t a id48 to this data. for the
training data, both the tags and words are observed so that maximum likelihood training of the transition
and emission distributions can be achieved by simple counting. given a new sequence of words, the most
likely tag sequence can be inferred using the viterbi algorithm.

more recent part-of-speech taggers tend to use conditional random    elds in which the input sequence x1:t
is the sentence and the output sequence y1:t is the tag sequence. one possible parameterisation of a linear
chain crf is to use a potential of the form   (yt   1, yt)  (yt, x), see section(9.6.5), in which the    rst factor
encodes the grammatical structure of the language and the second the a priori likely tag yt[181].

23.6 summary

    timeseries require simplifying assumptions such as the markov assumption which makes it feasible to specify

a model for the process.

    a discrete-state markov chain is a generalisation of deterministic    nite-state transitions to stochastic tran-

sitions between states.

    mixtures of markov models and extensions thereof can be used as simple timeseries id91 models.

draft november 9, 2017

489

exercises

    id48 are popular timeseries models in which the latent process is a discrete-state markov
chain. the observations can be either discrete or continuous and the classical id136 tasks of    ltering,
smoothing and computing the joint most likely latent sequence are computationally straightforward.

    dynamic bayes nets are essentially structured id48s with conditional independencies encoded in their

transition and emission distributions.

    id48s have widespread application in a variety of tracking scenarios, from id103 to genetic

sequence analysis.

    undirected linear chain structures such as the conditional random    eld are popular in areas such as natural

language processing for translating an input sequence to a structured output sequence.

23.7 code

demomixmarkov.m: demo for mixture of markov models
mixmarkov.m: mixture of markov models
demoid48id136.m: demo of id48 id136
id48forward.m: forward    recursion
id48backward.m: forward    recursion
id48gamma.m: rts       correction    recursion
id48smooth.m: single and pairwise           smoothing
id48viterbi.m: most likely state (viterbi) algorithm
demoid48burglar.m: demo of burglar localisation
demoid48bigram.m: demo of stubby    ngers typing
id48em.m: em algorithm for id48 (baum-welch)
demoid48learn.m: demo of em algorithm for id48 (baum-welch)
demolinearcrf.m: demo of learning a linear chain crf
linearcrfpotential.m: linear crf potential
linearcrfgrad.m: linear crf gradient
linearcrfloglik.m: linear crf log likelihood

23.8 exercises

exercise 23.1. a stochastic matrix mij has non-negative entries with(cid:80)
   and eigenvector e such(cid:80)

j mijej =   ei. by summing over i show that, provided(cid:80)

i mij = 1. consider an eigenvalue
i ei > 0, then    must

be equal to 1.

exercise 23.2. consider the markov chain with transition matrix

(cid:19)

(cid:18) 0 1

1 0

m =

(23.8.1)

show that this markov chain does not have an equilibrium distribution and state a stationary distribution
for this chain.

exercise 23.3. consider a id48 with 3 states (m = 3) and 2 output symbols, with a left-to-right state
transition matrix

       0.5 0.0 0.0

0.3 0.6 0.0
0.2 0.4 1.0

      

a =

490

(23.8.2)

draft november 9, 2017

exercises

where aij     p(ht+1 = i|ht = j), emission matrix bij     p(vt = i|ht = j)

(cid:18) 0.7 0.4 0.8

(cid:19)

0.3 0.6 0.2

b =

(23.8.3)

and initial state id203 vector a = (0.9 0.1 0.0)t. given the observed symbol sequence is v1:3 = (1, 2, 1):

1. compute p(v1:3).

2. compute p(h1|v1:3).
3. find the most probable hidden state sequence arg maxh1:3 p(h1:3|v1:3).

exercise 23.4. this exercise follows from example(23.5). given the 27 long character string
rgenmonleunosbpnntje vrancg typed with    stubby    ngers   , what is the most likely correct english sentence
intended? in the list of decoded sequences, what value is log p(h1:27|v1:27) for this sequence? you will need
to modify demoid48bigram.m suitably.

exercise 23.5. show that if a id48 transition matrix a and emission matrix b are initialised to uniformly
constant values, then the em algorithm fails to update the parameters meaningfully.

exercise 23.6. consider the problem of    nding the most likely joint output sequence v1:t for a id48. that
is,

v

   
1:t     argmax

v1:t

p(v1:t )

where

p(h1:t , v1:t ) =

t(cid:89)

t=1

p(vt|ht)p(ht|ht   1)

(23.8.4)

(23.8.5)

1. explain why a local message passing algorithm cannot, in general, be found for this problem and discuss

the computational complexity of    nding an exact solution.

2. explain how to adapt the expectation maximisation algorithm to form a recursive algorithm for    nding
1:t . explain why your approach guarantees an improved solution at each iteration.

an approximate v   
additionally, explain how the algorithm can be implemented using local message passing.

exercise 23.7. explain how to train a id48 using em (expectation maximisation), but with a constrained
transition matrix. in particular, explain how to learn a transition matrix with a triangular structure.

exercise 23.8.

using the correspondence a = 1, c = 2, g = 3, t = 4 de   ne a 4    4 transition matrix p that produces
sequences of the form

a, c, g, t, a, c, g, t, a, c, g, t, a, c, g, t, . . .

now de   ne a new transition matrix

pnew = 0.9*p + 0.1*ones(4)/4

de   ne a 4    4 transition matrix q that produces sequences of the form

t, g, c, a, t, g, c, a, t, g, c, a, t, g, c, a, . . .

now de   ne a new transition matrix

qnew = 0.9*q + 0.1*ones(4)/4

(23.8.6)

(23.8.7)

(23.8.8)

(23.8.9)

assume that the id203 of being in the initial state of the markov chain p(h1) is constant for all four
states a, c, g, t .

draft november 9, 2017

491

1. what is the id203 that the markov chain pnew generated the sequence s given by

s     a, a, g, t, a, c, t, t, a, c, c, t, a, c, g, c

exercises

(23.8.10)

2. similarly what is the id203 that s was generated by qnew? does it make sense that s has a higher

likelihood under pnew compared with qnew?

3. using the function randgen.m, generate 100 sequences of length 16 from the markov chain de   ned by
pnew. similarly, generate 100 sequences each of length 16 from the markov chain de   ned by qnew.
concatenate all these sequences into a cell array v so that v{1} contains the    rst sequence and v{200}
the last sequence. use mixmarkov.m to learn the maximum likelihood parameters that generated these
sequences. assume that there are h = 2 kinds of markov chain. the result returned in phgv indicates
the posterior id203 of sequence assignment. do you agree with the solution found?

4. take the sequence s as de   ned in equation (23.8.10). de   ne an emission distribution that has 4 output

states such that

p(v = i|h = j) =

(cid:26) 0.7 i = j

0.1 i (cid:54)= j

using this emission distribution and the transition given by pnew de   ned in equation (23.8.7), adapt
demoid48id136simple.m suitably to    nd the most likely hidden sequence hp
1:16 that generated the
observed sequence s. repeat this computation but for the transition qnew to give hq
1:16. which hidden
sequence     hp

1:16 is to be preferred? justify your answer.

1:16 or hq

exercise 23.9. derive an algorithm that will    nd the most likely joint state

t(cid:89)

t=2

t(cid:89)

t=2

argmax

h1:t

  t(ht   1, ht)

for arbitrarily de   ned potentials   t(ht   1, ht).

1. first consider

max
h1:t

  t(ht   1, ht)

  t   1   t (ht   1)

2. derive the recursion

  t   1   t(ht   1) = max
ht

  t(ht, ht   1)  t   t+1(ht)

3. explain how the above recursion enables the computation of

t(cid:89)

t=2

argmax

h1

  t(ht, ht   1)

show how the maximisation over ht may be pushed inside the product and that the result of the
maximisation can be interpreted as a message

4. explain how once the most likely state for h1 is computed, one may e   ciently compute the remaining

optimal states h2, . . . , ht .

exercise 23.10. derive an algorithm that will compute pairwise marginals

p(ht, ht   1)

from the joint distribution

t(cid:89)

t=2

p(h1:t )    

  t(ht   1, ht)

for arbitrarily de   ned potentials   t(ht   1, ht).

492

draft november 9, 2017

(23.8.11)

(23.8.12)

(23.8.13)

(23.8.14)

(23.8.15)

(23.8.16)

(23.8.17)

(23.8.18)

exercises

1. first consider(cid:88)
t(cid:89)

h1,...,ht

t=2

  t(ht, ht   1)

@@

show how the summation over h1 may be pushed inside the product and that the result of the
can be interpreted as a message

  1   2(h2) =

  2(h1, h2)

2. show that the summation over the variables h1:t   1 can be accomplished via the recursion

  t   1   t(ht) =

  t(ht   1, ht)  t   2   t   1(ht   1)

3. similarly, show that one can push the summation of ht inside the product to de   ne

  t   1   t (ht   1) =

  t (ht   1, ht ).

show that the summation over the variables ht :t+1 can be accomplished via the recursion

(cid:88)

h1

(cid:88)

ht   1

(cid:88)

ht

(cid:88)

  t   t+1(ht) =

  t+1(ht, ht+1)  t+1   t+2(ht+1)

ht+1

4. show that

@@

p(ht, ht   1)    

  t   2   t   1(ht   1)  (ht   1, ht)  t   t+1(ht)

exercise 23.11. a second order id48 is de   ned as

t(cid:89)

t=3

p(h1:t , v1:t ) = p(h1)p(v1|h1)p(h2|h1)p(v2|h2)

p(ht|ht   1, ht   2)p(vt|ht)

following a similar approach to the    rst order id48, derive explicitly a message passing algorithm to
compute the most likely joint state

argmax

h1:t

p(h1:t|v1:t )

(23.8.26)

exercise 23.12. derive an algorithm to e   ciently compute the gradient of the log likelihood for an id48;
assume unconstrained discrete transition and emission matrices.

exercise 23.13. consider the id48 de   ned on hidden variables h = {h1, . . . , ht} and observations v =
{v1, . . . , vt}

t(cid:89)

t=2

p(v,h) = p(h1)p(v1|h1)

p(ht|ht   1)p(vt|ht)

show that the posterior p(h|v) is a markov chain

t(cid:89)

t=2

p(h|v) =   p(h1)

  p(ht|ht   1)

where   p(ht|ht   1) and   p(h1) are suitably de   ned distributions.
draft november 9, 2017

(23.8.19)

summation

(23.8.20)

(23.8.21)

(23.8.22)

(23.8.23)

(23.8.24)

(23.8.25)

(23.8.27)

(23.8.28)

493

exercises

(23.8.29)

(23.8.30)

(23.8.31)

(23.8.32)

(23.8.33)

@@

(23.8.34)

@@
@@

@@
@@

exercise 23.14. for training a id48 with a gaussian mixture emission (the id48-gmm model) in sec-
tion(23.3.3), derive the following em update formulae for the means and covariances:

n(cid:88)

t(cid:88)

  k,h(t, n)vn
t

n=1

t=1

n(cid:88)

t(cid:88)

n=1

t=1

  k,h(t, n)(cid:0)vn

t       k,h

(cid:1)(cid:0)vn

t       k,h

(cid:1)t

  new

k,h =

  new

k,h =

and

where

  k,h(t, n) =

(cid:80)

n

(cid:80)
q(kt = k|hn
t q(kt = k|hn

t = h)q(hn

t = h)q(hn

t = h|vn

1:t )
t = h|vn

1:t )

exercise 23.15. consider the id48 duration model de   ned by equation (23.4.2) and equation (23.4.1)
with emission distribution p(vt|ht). our interest is to derive a recursion for the    ltered distribution

  t(ht, ct)     p(ht, ct, v1:t)

1. show that :

  t(ht, ct) = p(vt|ht)

(cid:88)

ht   1,ct   1

p(ht|ht   1, ct)p(ct|ct   1)  t   1(ht   1, ct   1)

2. using this derive

  t(ht, ct)
p(vt|ht)

=

(cid:88)

ht   1

p(ht|ht   1,

ct)p(ct|ct   1 = 1)  t   1(ht   1, ct   1 = 1)
dmax(cid:88)

(cid:88)

+

ht   1

p(ht|ht   1,

ct)

ct   1=2

ct|ct   1)  t   1(ht   1, ct   1)
p(

3. show that the right hand side of the above can be written as

(cid:88)

ht   1

p(ht|ht   1,

ct)p(

ct|ct   1 = 1)  t   1(ht   1, 1)

(cid:3)(cid:88)

ht   1

+ i(cid:2)

(cid:88)

4. show that the recursion for    is then given by

ct (cid:54)= dmax

p(ht|ht   1,

ct)  t   1(ht   1,

ct + 1)

(23.8.35)

@@
@@
@@

  t(h, 1) = p(vt|ht = h)pdur(1)

ht   1

(cid:88)
ptran(h|ht   1)  t   1(ht   1, 1)

+ i [dmax (cid:54)= 1] p(vt|ht = h)

ht   1

ptran(

h|ht   1)  t   1(ht   1, 2)

(23.8.36)

@@

and for c > 1

  t(h, c) = p(vt|ht = h){pdur(c)  t   1(h, 1) + i [c (cid:54)= dmax]   t   1(h, c + 1)}

5. explain why the computational complexity of    ltered id136 in the duration model is o(cid:0)t h 2dmax

(23.8.37)

(cid:1).

6. derive an e   cient smoothing algorithm for this duration model.

494

draft november 9, 2017

exercises

++

consider searching for a pattern s1, s2, . . . , sl in a longer text

exercise 23.16 (fuzzy string search).
v1, v2, . . . , vt . for example one might seek to    nd whether the pattern acgt aa occurs in the text aaacgt aat at .
we can model this naively using a id48 with l + 1 states. the idea is that we    rst de   ne a generating
mechanism which places the pattern s in the text (possibly multiple times). we can do this by de   ning a
latent state that either represents not emitting the pattern, or the position in the pattern. for notation here
we de   ne the state ht = 0 to mean that t is not the starting position of the pattern. we then de   ne

p(ht+1 = 0|ht = 0) =   ,

p(ht+1 = l|ht = 0) = 1       

to denote that the latent variable h either remains at the non-starting point with id203    or jumps to
the starting point of the pattern s with id203 1        . the remaining latent state transition is de   ned as

p(ht+1|ht) =    (ht+1, ht     1)

which decrements the latent variable deterministically. this is used as a counter for the position in the
pattern. given a position in the pattern, the emission is given by p(vt|ht). in the deterministic case,

p(vt|ht) =    (vt     sht+1   l)

and, more generally, one can de   ne a distribution p(vt|ht) to account for possible corruption.

1. show that id136 (   ltering, smoothing and viterbi) can be computed with o (t l) operations in both
the deterministic (perfect matching) and    fuzzy    matching case. (note that for the perfect matching
case, the reader is referred to classical approaches such as the aho-corasick and boyer-moore algo-
rithms which are generally much faster).

2. for the text 12132324233144241314223142331234, with st     {1, 2, 3, 4},    nd the smoothed id203
pt that the pattern 2314243 starts at position t in the sequence based on assuming    = 0.5 and p(vt =
i|ht = i) = 0.9, with uniform id203 on the mismatch states. assume also that when ht = 0 there
is a uniform emission distribution.

++

exercise 23.17.

as noted in section(23.2.2), a numerical issue with the standard   -recursion

  (ht) = p(vt|ht)

p(ht|ht   1)  (ht   1)

(23.8.38)

is that these    messages will typically become exponentially small in time, resulting in numerical under   ow.
whilst this can be addressed by computing log    messages, this would require special treatment of cases
in which the    messages have zero values. an alternative to using log    messages is to renormalise the
messages at each stage, resulting in messages that represent the    ltered distribution p(ht|v1:t). a drawback
of normalising the messages is that we are then unable to directly compute the likelihood term p(v1:t ) =
ht   (ht ). however, there is a way to de   ne normalised messages and still retain the information required

(cid:80)

to compute the log-likelihood. de   ne the local normalisation term

(cid:88)

ht   1

(cid:88)

ht   1

t(cid:89)

zt    

p(vt|ht)

and normalised message

    (ht) =

1
zt

p(vt|ht)

p(ht|ht   1)    (ht   1)
(cid:88)

p(ht|ht   1)    (ht   1)

ht   1

(cid:88)

ht

(cid:80)

  (ht) =     (ht)

and that therefore log p(v1:t ) =(cid:80)t

   =1

z  

t=1 log(zt).

draft november 9, 2017

where z1    

h1 p(v1|h1)p(h1) and     (h1) = p(v1|h1)p(h1)/z1. show that

(23.8.39)

(23.8.40)

(23.8.41)

495

exercises

496

draft november 9, 2017

chapter 24

continuous-state markov models

many physical systems can be understood as continuous variable models undergoing a transition that de-
pends only on the state of the system at the previous time. in this chapter we discuss some of the classical
models in this area which are highly specialised to retain tractability of id136. these models are found
in a wide variety of    elds from    nance to signal processing in engineering.

24.1 observed linear dynamical systems

in many practical timeseries applications the data is naturally continuous, particularly for models of the
physical environment. in contrast to discrete-state markov models, chapter(23), parametric continuous state
distributions are not automatically closed under operations such as products and marginalisation. to make
practical algorithms for which id136 and learning can be carried out e   ciently, we therefore are heavily
restricted in the form of the continuous transition p(vt|vt   1). a simple yet powerful class of such transitions
are the linear dynamical systems. a deterministic observed linear dynamical system1 (olds) de   nes the
temporal evolution of a vector vt according to the discrete-time update equation

vt = atvt   1

(24.1.1)

where at is the transition matrix at time t. for the case that at is invariant with t, the process is called
stationary or time-invariant, which we assume throughout unless explicitly stated otherwise.

a motivation for studying oldss is that many equations that describe the physical world can be written
if vt describes
as an olds. oldss are interesting since they may be used as simple prediction models:
the state of the environment at time t, then avt predicts the environment at time t + 1. as such, these
models, have widespread application in many branches of science, from engineering and physics to economics.

the olds equation (24.1.1) is deterministic so that if we specify v1, all future values v2, v3, . . . , are de   ned.
for a dim (v) = v dimensional vector, its evolution is described by (assuming a is diagonalisable)

vt = at   1v1 = p  t   1p   1v1

(24.1.2)

where    = diag (  1, . . . ,   v ), is the diagonal eigenvalue matrix, and p is the corresponding eigenvector
matrix of a. if any   i > 1 then for large t, vt will explode. on the other hand, if   i < 1, then   t   1
i will
tend to zero. for stable systems we require therefore no eigenvalues of magnitude greater than 1 and only
unit eigenvalues |  i = 1| will contribute in the long term. note that the eigenvalues may be complex which

1we use the terminology    observed    lds to di   erentiate from the more general lds state-space model.

in some texts,

however, the term lds is applied to the models under discussion in this chapter.

497

corresponds to rotational behaviour, see exercise(24.1).

more generally, we may consider additive noise on v and de   ne a stochastic olds, as de   ned below.

observed linear dynamical systems

de   nition 24.1 (observed linear dynamical system).

vt = atvt   1 +   t

where   t is a noise vector sampled from a gaussian distribution,

n (  t   t,   t)

this is equivalent to a    rst order markov model with transition

p(vt|vt   1) = n (vt atvt   1 +   t,   t)

(24.1.3)

(24.1.4)

(24.1.5)

at t = 1 we have an initial distribution p(v1) = n (v1   1,   1). for t > 1 if the parameters are time-
independent,   t       , at     a,   t       , the process is called time-invariant.

24.1.1 stationary distribution with noise

consider the one-dimensional linear system with independent additive noise

(cid:0)  t 0,   2

v

(cid:1)

if we start at some state v1, and then for t > 1 recursively sample according to vt = avt   1 +   t, what is the
distribution of vt? since the transitions are linear and the noise is gaussian, then vt must also be gaussian.
assuming that we can represent the distribution of vt   1 as a gaussian with mean   t   1 and variance   2
t   1,
vt   1     n

t   1

(cid:1), then using (cid:104)  t(cid:105) = 0 we have
= a2(cid:10)v2

(cid:11) + 2a(cid:104)vt   1(cid:105)(cid:104)  t(cid:105) +(cid:10)  2

t

(cid:11)

(cid:104)vt(cid:105) = a(cid:104)vt   1(cid:105) + (cid:104)  t(cid:105)       t = a  t   1
t   1

t

vt = avt   1 +   t,

  t     n

(cid:0)vt   1   t   1,   2
(cid:68)
(cid:11) =
(cid:10)v2

(avt   1 +   t)2(cid:69)
(cid:0)vt a  t   1, a2  2

t   1 +   2
v

      2

t = a2  2

so that

vt     n

(cid:1)

t   1 +   2
v

(24.1.6)

(24.1.7)

(24.1.8)

(24.1.9)

(24.1.10)

in   nite time limit

does the distribution of the vt, t (cid:29) 1 tend to a steady,    xed distribution? if a     1 the variance increases
inde   nitely with t, as does the mean value.

for a < 1, and assuming there is a    nite variance   2    for the in   nite time case, from equation (24.1.9), the
stationary distribution satis   es

  2
v
1     a2

v       2    =

  2    = a2  2    +   2

(24.1.11)
similarly, the mean is given by       = a     1. hence, for a < 1, the mean tends to zero yet the variance
remains    nite. even though the magnitude of vt   1 is decreased by a factor of a at each iteration, the
additive noise on average boosts the magnitude so that it remains steady in the long run. more generally
for a system updating a vector vt according to non-zero additive noise,

vt = avt   1 +   t

(24.1.12)

for the existence of a steady state we require that all eigenvalues of a must be < 1.

498

draft november 9, 2017

auto-regressive models

figure 24.1: fitting an order 3 ar model to the training points. the x
axis represents time, and the y axis the value of the timeseries. the dots
represent the 100 observations y1:100. the solid line indicates the mean
predictions (cid:104)y(cid:105)t , t > 100, and the dashed lines (cid:104)y(cid:105)t      . see demoartrain.m

24.2 auto-regressive models

where a = (a1, . . . , al)t are called the ar coe   cients and   2 is called the innovation noise. the model
predicts the future based on a linear combination of the previous l observations. as a belief network, the
ar model can be written as an lth order markov model:

a scalar time-invariant auto-regressive model is de   ned by

l(cid:88)

l=1

vt =

alvt   l +   t,

  t     n

(cid:0)  t   ,   2(cid:1)

t(cid:89)

p(v1:t ) =

t=1

with

p(vt|vt   1, . . . , vt   l),
l(cid:88)

(cid:32)

with vi =     for i     0

(cid:33)

alvt   l,   2

p(vt|vt   1, . . . , vt   l) = n

vt

l=1

introducing the vector of the l previous observations

we can write more compactly

  vt   1     [vt   1, vt   2, . . . , vt   l]t
(cid:16)

p(vt|vt   1, . . . , vt   l) = n

vt at   vt   1,   2(cid:17)

(24.2.1)

(24.2.2)

(24.2.3)

(24.2.4)

(24.2.5)

ar models are heavily used in    nancial time-series prediction (see for example [289]), being able to capture
simple trends in the data. another common application area is in speech processing whereby for a one-
dimensional speech signal partitioned into windows of length t (cid:29) l, the ar coe   cients best able to describe
the signal in each window are found[232]. these ar coe   cients then form a compressed representation of
the signal and subsequently transmitted for each window, rather than the original signal itself. the signal
can then be approximately reconstructed based on the ar coe   cients. this is known as a linear predictive
vocoder[274]. note that an interesting property of the ar representation of a signal is that if we scale the
signal by a constant amount   , then

  vt =

al  vt   l +     t,

(24.2.6)

l=1

so that provided we scale the noise suitably, the same ar coe   cients can be used to represent the signal;
the ar coe   cients are therefore to a degree amplitude invariant.

draft november 9, 2017

499

l(cid:88)

020406080100120140   5005010015020024.2.1 training an ar model

maximum likelihood training of the ar coe   cients is straightforward based on

log p(v1:t ) =

log p(vt|  vt   1) =    

1
2  2

vt       vt

t   1a

t
2

   

log(2    2)

t(cid:88)

(cid:16)

t=1

(cid:17)2

di   erentiating w.r.t. a and equating to zero we arrive at

t(cid:88)
(cid:17)

t=1

(cid:88)

t

(cid:16)
(cid:32)(cid:88)

so that optimally

vt       vt

t   1a

  vt   1 = 0

(cid:33)   1(cid:88)

vt   vt   1

a =

  vt   1   vt

t   1

t

t

t(cid:88)

(cid:16)

t=1

  2 =

1
t

vt       vt

t   1a

(cid:17)2

auto-regressive models

(24.2.7)

(24.2.8)

(24.2.9)

(24.2.10)

these equations can be solved by gaussian elimination. the linear system has a toeplitz form that can be
more e   ciently solved using the levinson-durbin method [90]. similarly, optimally,

above we assume that    negative    timesteps are available in order to keep the notation simple. if times before
the window over which we learn the coe   cients are not available, a minor adjustment is required to start
the summations from t = l + 1. given a trained a, future predictions can be made using vt+1 =   vt

t a.

example 24.1 (fitting a trend). we illustrate with a simple example how ar models can be used to
estimate trends underlying timeseries data. a third-order ar model was    t to the set of 100 observations
shown in    g(24.1) using maximum likelihood. a prediction for the mean (cid:104)y(cid:105)t was then recursively generated
as

(cid:26) (cid:80)3

(cid:104)y(cid:105)t =

yt

i=1 ai (cid:104)y(cid:105)t   i

for
for

t > 100,
t     100 .

as we can see (solid line in    g(24.1)), the predicted means for time t > 100 capture an underlying trend in
the data. whilst this example is very simple, ar models are quite powerful and can model complex signal
behaviour.

24.2.2 ar model as an olds

we can write equation (24.2.1) as an olds using

                vt

vt   1
...

vt   l+1

                =

                a1

1
...
0

               

                vt   1

vt   2
...
vt   l

                +

                  t

0
...
0

               

a2
0

1
. . .

. . . al
. . .
0

. . .
1

0
0

which in vector notation is

where we de   ne the block matrices

  vt = a  vt   1 +   t,

(cid:18) a1:l   1 al

(cid:19)

i

0

a =

,

   =

(cid:18)

  t     n (  t 0,   )

  2

01,1:l   1

01:l   1,1 01:l   1,1:l   1

(cid:19)

(24.2.11)

(24.2.12)

(24.2.13)

in this representation, the    rst component of the vector is updated according to the standard ar model,
with the remaining components being copies of the previous values.

500

draft november 9, 2017

auto-regressive models

a1

v1

a2

v2

a3

v3

a4

v4

figure 24.2: a time-varying ar model as a latent
lds. since the observations are known, this model is a
time-varying latent lds, for which smoothed id136
determines the time-varying ar coe   cients.

24.2.3 time-varying ar model

an alternative to maximum likelihood is to view learning the ar coe   cients as a problem in id136 in
a latent lds, a model which is discussed in detail in section(24.3). if at are the latent ar coe   cients, the
term

vt =   vt

t   1at +   t,

  t     n

at = at   1 +   a
t ,

  a
t     n

(cid:0)  t 0,   2(cid:1)
ai(cid:1)
(cid:0)  a

t 0,   2

p(v1:t , a1:t ) =

p(vt|at,   vt   1)p(at|at   1)

(cid:89)

t

can be viewed as the emission distribution of a latent lds in which the hidden variable is at and the
time-dependent emission matrix is given by   vt

t   1. by placing a simple latent transition

we encourage the ar coe   cients to change slowly with time. this de   nes a model, see    g(24.2)

(24.2.14)

(24.2.15)

(24.2.16)

our interest is then in the conditional p(a1:t|v1:t ) from which we can compute the a-posteriori most likely
sequence of ar coe   cients. standard smoothing algorithms can then be applied to yield the time-varying
ar coe   cients, see demoarlds.m.

n   1(cid:88)

n=0

de   nition 24.2 (discrete fourier transform). for a sequence x0:n   1 the dft f0:n   1 is de   ned as

fk =

    2  i

n kn,

xne

k = 0, . . . , n     1

(24.2.17)

fk is a (complex) representation as to how much frequency k is present in the sequence x0:n   1. the power
of component k is de   ned as the absolute length of the complex fk.

de   nition 24.3 (spectrogram). given a timeseries x1:t the spectrogram at time t is a representation of
the frequencies present in a window localised around t. for each window one computes the discrete fourier
transform, from which we obtain a vector of log power in each frequency. the window is then moved
(usually) one step forward and the dft recomputed. note that by taking the logarithm, small values in
the original signal can translate to visibly appreciable values in the spectrogram.

example 24.2 (nightingale). in    g(24.3a) we plot the raw acoustic recording for a 5 second fragment of a
nightingale song (freesound.org, sample 17185). the spectrogram is also plotted and gives an indication
of which frequencies are present in the signal as a function of time. the nightingale song is very complicated
but at least locally can be very repetitive. a crude way to    nd which segments repeat is to cluster the
time-slices of the spectrogram.
in    g(24.3c) we show the results of    tting a gaussian mixture model,
section(20.3), with 8 components, from which we see there is some repetition of components locally in time.
an alternative representation of the signal is given by the time-varying ar coe   cients, section(24.2.3), as
plotted in    g(24.3d). a gmm id91 with 8 components    g(24.3e) in this case produces a somewhat
clearer depiction of the di   erent phases of the nightingale singing than that a   orded by the spectrogram.

draft november 9, 2017

501

auto-regressive models

(a)

(b)

(c)

(d)

(e)

(b): spectrogram of (a) up to 20,000 hz.

(a): the raw recording of 5 seconds of a nightingale song (with additional background
figure 24.3:
(c): id91 of the results in panel (b) using an
birdsong).
8 component gaussian mixture model. plotted at each time vertically is the distribution over the cluster
indices (from 1 to 8), so that the darker the component, the more responsible that component is for the
spectrogram slice at that timepoint.
v = 0.001,
  2
(e): id91 the results in panel (d) using a gaussian mixture model with 8
h = 0.001, see arlds.m.
components. the ar components group roughly according to the di   erent song regimes.

(d): the 20 time-varying ar coe   cients learned using   2

24.2.4 time-varying variance ar models

in the standard ar model, equation (24.2.1), the variance   2 is assumed    xed throughout time. for some
applications, particular in    nance, this is undesirable since the    volatility    can change dramatically with
time. a simple extension of the ar model is to write

alvt   l +   t,

  t     n

(cid:0)  t   ,   2

t

(cid:1)

l(cid:88)
l(cid:88)

l=1

vt =

  vt =

l=1

  2
t =   0 +

alvt   l

q(cid:88)

i=1

  i (vt   i       vt   i)2

q(cid:88)

p(cid:88)

where   i     0. the motivation is that equation (24.2.20) represents an estimate of the variance of the
noise, based on a weighted sum of the squared discrepancies between the mean prediction   v and the actual
observation v over the previous q timesteps. this is called an autoregressive conditional heteroskedasticity
(arch) model [97]. a further extension is to the generalised arch (garch) model, for which

  2
t =   0 +

  i (vt   i       vt   i)2 +

i=1

502

  i  2

t   i

(24.2.21)

i=1

draft november 9, 2017

(24.2.18)

(24.2.19)

(24.2.20)

latent linear dynamical systems

  2
1

v1

  2
2

v2

(a)

  2
t

  2
1

vt

v1

  2
2

v2

(b)

  2
t

vt

(a): a    rst order l = 1, q = 1 arch model, in which the observations are dependent
figure 24.4:
on the previous observation, and the variance is dependent on the previous observations in a deterministic
(b): an l = 1, q = 1, p = 2 garch model, in which the observations are dependent on
manner.
the previous observation, and the variance is dependent on the previous observations and previous two
variances in a deterministic manner. these are special cases of the general deterministic latent variable
models, section(26.4).

where   i     0.
one can interpret these as deterministic latent variable models, see section(26.4), albeit with the extension
to a higher-order markov process.
in this sense, learning of the parameters by maximum likelihood is
straightforward since derivatives of the log likelihood can be computed by deterministic propagation, as
described in section(26.4). indeed, based on this insight, a whole range of possible non-linear    volatility   
models come into view.

24.3 latent linear dynamical systems

the latent lds de   nes a stochastic linear dynamical system in a latent (or    hidden   ) space on a sequence
of vectors h1:t . each observation vt is as linear function of the latent vector ht. this model is also called a
linear gaussian state space model 2. the model can also be considered a form of lds on the joint variables
xt = (vt, ht), with parts of the vector xt missing. for this reason we will also sometimes refer to this model
as a linear dynamical system (without the    latent    pre   x). these models are powerful models of timeseries
and their use is widespread. their latent nature means that we can use the latent variable ht to track and
explain the observation t. the formal de   nition of the model is given below.

de   nition 24.4 (latent linear dynamical system).

ht = atht   1 +   h
vt = btht +   v
t

t   h
t     n
  v
t     n (  v

  ht,   h
t
t
t   vt,   v
t )

(cid:1) transition model

emission model

t and   v

where   h
t are noise vectors. at is called the transition matrix and bt the emission matrix . the
terms   ht and   vt are the hidden and output bias respectively. the transition and emission models de   ne a
   rst order markov model

p(h1:t , v1:t ) = p(h1)p(v1|h1)

(cid:16)

p(ht|ht   1)p(vt|ht)
(cid:17)

with the transitions and emissions given by gaussian distributions

p(ht|ht   1) = n

ht atht   1 +   ht,   h
t

,

p(vt|ht) = n (vt btht +   vt,   v
t )

p(h1) = n (h1     ,     )

2these models are also often called kalman filters. we avoid this terminology here since the word       lter    refers to a speci   c

kind of id136 and runs the risk of confusing a    ltering algorithm with the model itself.

draft november 9, 2017

503

(24.3.1)

(24.3.2)

(24.3.3)

(24.3.4)

(cid:0)  h

t(cid:89)

t=2

h1

v1

h2

v2

h3

v3

h4

v4

id136

figure 24.5: a (latent) lds. both hidden and visible
variables are gaussian distributed.

figure 24.6: a single phasor plotted as a damped two
dimensional rotation ht+1 =   r  ht with a damping
factor 0 <    < 1. by taking a projection onto the y
axis, the phasor generates a damped sinusoid.

the model is represented as a belief network in    g(24.5) with the extension to higher orders being intuitive.
one may also include an external input ot at each time, which will add cot to the mean of the hidden
variable and dot to the mean of the observation.

(24.3.6)

(24.3.7)

explicit expressions for the transition and emission distributions are given below for the time-invariant
t       v with zero biases   vt = 0,   ht = 0. each hidden variable is a
case at     a, bt     b,   h
multidimensional gaussian distributed vector ht, with transition
1(cid:112)

(cid:19)
h (ht     aht   1)

(ht     aht   1)t      1

t       h,   v

(24.3.5)

(cid:18)

exp

   

1
2

which states that ht has a mean aht   1 and covariance   h. similarly,

|2    h|

p(ht|ht   1) =
1(cid:112)

p(vt|ht) =

|2    v|

(cid:18)

exp

1
2

(vt     bht)t      1

v

   

(vt     bht)

(cid:19)

describes an output vt with mean bht and covariance   v.

example 24.3. consider a dynamical system de   ned on two dimensional vectors ht:

ht+1 = r  ht,

with r   =

(cid:18) cos        sin   

sin   

cos   

(cid:19)

r   rotates the vector ht through angle    in one timestep. under this lds we trace out points on a circle,
h1, . . . , ht through time. by taking a scalar projection of ht, for example,

vt = [ht]1 = [1 0]tht,

(24.3.8)

the elements vt, t = 1, . . . , t describe a sinusoid through time, see    g(24.6). by using a block diagonal
r = blkdiag (r  1, . . . , r  m) and taking a scalar projection of the extended m   2 dimensional ht vector, one
can construct a representation of a signal in terms of m sinusoidal components. hence we see that ldss
can represent potentially highly complex period behaviour.

24.4 id136

given an observation sequence v1:t we here consider    ltering and smoothing, as we did for the id48,
section(23.2.1). for the id48, in deriving the various message passing recursions, we used only the inde-
pendence structure encoded by the belief network. since the lds has the same independence structure as
the id48, we can use the same independence assumptions in deriving the updates for the lds. however,
in implementing them we need to deal with the issue that we now have continuous hidden variables, rather
than discrete states. the fact that the distributions are gaussian means that we can deal with continuous

504

draft november 9, 2017

id136

(cid:90)

messages exactly.
integration. for example, the    ltering recursion (23.2.7) becomes

in translating the id48 message passing equations, we    rst replace summation with

p(ht|v1:t)    

ht   1

p(vt|ht)p(ht|ht   1)p(ht   1|v1:t   1),

t > 1

(24.4.1)

since the product of two gaussians is another gaussian, and the integral of a gaussian is another gaussian,
the resulting p(ht|v1:t) is also gaussian. this closure property of gaussians means that we may represent
p(ht   1|v1:t   1) = n (ht   1 ft   1, ft   1) with mean ft   1 and covariance ft   1. the e   ect of equation (24.4.1) is
equivalent to updating the mean ft   1 and covariance ft   1 into a mean ft and covariance ft for p(ht|v1:t).
our task below is to    nd explicit algebraic formulae for these updates.

numerical stability

translating the message passing id136 techniques we developed for the id48 into the lds is largely
straightforward.
indeed, one could simply run a standard sum-product algorithm (albeit for continuous
variables), see demosumprodgausscanonlds.m. in long timeseries, however, numerical instabilities can build
up and may result in grossly inaccurate results, depending on the transition and emission distribution
parameters and the method of implementing the message updates. for this reason specialised routines
have been developed that are numerically stable under certain parameter regimes[304]. for the id48
in section(23.2.1), we discussed two alternative methods for smoothing, the parallel    approach, and the
sequential    approach. the    recursion is suitable when the emission and transition covariance entries are
large, and the    recursion preferable in the more standard case of small covariance values.

analytical shortcuts

in deriving the id136 recursions we need to frequently multiply and integrate gaussians. whilst in prin-
ciple straightforward, this can be algebraically tedious and, wherever possible, it is useful to appeal to known
shortcuts. for example, one can exploit the general result that the linear transform of a gaussian random
variable is another gaussian random variable. similarly it is convenient to make use of the conditioning
formulae, as well as the dynamics reversal intuition. these results are stated in section(8.4), and below we
derive the most useful for our purposes here. this will also explain how the equations for    ltering can be
derived.

consider a linear transformation of a gaussian random variable:

y = mx +   ,

       n (     ,   ) ,

x     n (x   x,   x)

(24.4.2)

where x and    are assumed to be generated from independent processes. to    nd the distribution p(y), one
approach would be to write this formally as

p(y) =

n (y mx +   ,   )n (x   x,   x) dx

(24.4.3)

and carry out the integral (by completing the square). however, since a gaussian variable under linear
transformation is another gaussian, we can take a shortcut and just    nd the mean and covariance of the
transformed variable. its mean is given by

(cid:104)y(cid:105) = m(cid:104)x(cid:105) + (cid:104)  (cid:105) = m  x +   

(24.4.4)

to    nd the covariance of p(y), consider the displacement of a variable x from its mean, which we write as

   x     x     (cid:104)x(cid:105)

the covariance is, by de   nition,(cid:10)   x   xt(cid:11). for y, the displacement is

   y = m   x +      ,

draft november 9, 2017

(24.4.5)

(24.4.6)

505

(cid:90)

id136

parameters   t =(cid:8)a, b,   h,   v,   h,   v(cid:9)

algorithm 24.1 lds forward pass. compute the    ltered posteriors p(ht|v1:t)     n (ft, ft) for a lds with

t. the log-likelihood l = log p(v1:t ) is also returned.

{f1, f1, p1} = ldsforward(0, 0, v1;   1)
l     log p1
for t     2, t do

{ft, ft, pt} = ldsforward(ft   1, ft   1, vt;   t)
l     l + log pt

end for
function ldsforward(f , f, v;   )

  h     af +   h,
  v     b  h +   v
(cid:16)
  hh     afat +   h,   vv     b  hhbt +   v,   vh     b  hh
vh     1
f(cid:48)
      h +   t
vv (v       v),
2 (v       v)t      1
p(cid:48)
    1
    exp
return f(cid:48), f(cid:48), p(cid:48)

(cid:17)
      hh       t

vh     1
det (2    vv)

vv (v       v)

(cid:112)

vv   vh

f(cid:48)

/

end function

(cid:68)

so that the covariance is

(cid:68)
   y   yt(cid:69)

(m   x +      ) (m   x +      )t(cid:69)
(cid:68)
   x     t(cid:69)
(cid:68)
   x   xt(cid:69)

(cid:68)
since the noises    and x are assumed independent,(cid:10)        xt(cid:11) = 0 we have

        xt(cid:69)

mt + m

mt +

(cid:68)

= m

+

=

(cid:46) mean of p(ht, vt|v1:t   1)
(cid:46) covariance of p(ht, vt|v1:t   1)
(cid:46) find p(ht|v1:t) by conditioning
(cid:46) compute p(vt|v1:t   1)

          t(cid:69)

  y = m  xmt +   

24.4.1 filtering

we represent the    ltered distribution as a gaussian with mean ft and covariance ft,

p(ht|v1:t)     n (ht ft, ft)

(24.4.7)

(24.4.8)

this is called the moment representation. our task is then to    nd a recursion for ft, ft in terms of ft   1,
ft   1. a convenient approach is to    rst    nd the joint distribution p(ht, vt|v1:t   1) and then condition on
vt to    nd the distribution p(ht|v1:t). the term p(ht, vt|v1:t   1) is a gaussian whose statistics can be found
from the relations

using the above, and assuming time-invariance and zero biases, we readily    nd

(cid:68)
(cid:68)
(cid:68)

vt = bht +   v
t ,

ht = aht   1 +   h
t

(cid:69)
(cid:69)
(cid:69)

(cid:16)
(cid:16)

   ht   ht

   vt   ht

   vt   vt

= b

aft   1at +   h

= aft   1at +   h,

t |v1:t   1
t |v1:t   1
aft   1at +   h
t |v1:t   1
(cid:104)vt|v1:t   1(cid:105) = ba(cid:104)ht   1|v1:t   1(cid:105) ,
(cid:104)ht|v1:t   1(cid:105) = a(cid:104)ht   1|v1:t   1(cid:105)

= b

(cid:17)
(cid:17)

,

bt +   v,

(cid:68)

in the above, using our moment representation of the forward messages

(cid:69)
then, using conditioning3 p(ht|vt, v1:t   1) will have mean
(cid:1) and covariance   xx       xy     1
t |v1:t   1
(vt     (cid:104)vt|v1:t   1(cid:105))

(cid:104)ht   1|v1:t   1(cid:105)     ft   1,
(cid:68)

(cid:69)(cid:68)
(cid:0)y       y

3p(x|y) is a gaussian with mean   x +   xy     1

    ft   1
(cid:69)   1

ft     (cid:104)ht|v1:t   1(cid:105) +

t   1|v1:t   1

t |v1:t   1

   ht   1   ht

   ht   vt

   vt   vt

yy

yy   yx.

(24.4.9)

(24.4.10)

(24.4.11)

(24.4.12)

(24.4.13)

(24.4.14)

(24.4.15)

(24.4.16)

506

draft november 9, 2017

id136

and covariance

(cid:68)

(cid:69)

   

t |v1:t   1

ft    

   ht   ht

(cid:68)
ft = aft   1 + pbt(cid:16)
ft = p     pbt(cid:16)

   ht   vt

t |v1:t   1
(cid:17)   1

bpbt +   v

(cid:17)   1

bpbt +   v

bp

(vt     baft   1)

writing out the above explicitly we have for the mean and covariance:

(cid:69)(cid:68)

   vt   vt

t |v1:t   1

(cid:69)   1(cid:68)

   vt   ht

t |v1:t   1

where

p     aft   1at +   h

(cid:69)

(24.4.17)

(24.4.18)

(24.4.19)

(24.4.20)

the recursion is initialised with f0 = 0, f0 = 0. the    ltering procedure is presented in algorithm(24.1) with
a single update in ldsforwardupdate.m.

one can write the covariance update as

where we de   ne the kalman gain matrix

ft = (i     kb) p

k = pbt(cid:16)

  v + bpbt(cid:17)   1

symmetrising the updates

(24.4.21)

(24.4.22)

a potential numerical issue with the covariance update (24.4.21) is that it is the di   erence of two positive
de   nite matrices. if there are numerical errors, the ft may not be numerically positive de   nite, nor sym-
metric. using the woodbury identity, de   nition(a.11), equation (24.4.19) can be written more compactly
as

(cid:16)

ft =

p   1 + bt     1
v b

(cid:17)   1

(24.4.23)

whilst this is positive semide   nite, this is numerically expensive since it involves two matrix inversions. an
alternative is to use the de   nition of k, from which we can write

k  vkt = (i     kb) pbtkt

hence we arrive at joseph   s symmetrized update[115]

ft = (i     kb) p (i     kb)t + k  vkt

(24.4.24)

(24.4.25)

the right-hand side of the above is the addition of two positive de   nite matrices so that the resulting update
for the covariance is more numerically stable. a similar method can be used in the backward pass below.
an alternative is to avoid using covariance matrices directly and use their square root as the parameter,
deriving updates for these instead[244, 40].

prediction

from the dynamics, de   nition(24.4), the distribution of the future visible variable is given by propagating
the    ltered distribution forwards one timestep (written here for the time-independent case)

(cid:90)

= n

p(vt+1|v1:t) =

(cid:16)

ht,ht+1

p(vt+1|ht+1)p(ht+1|ht)p(ht|v1:t)

vt+1 b(cid:0)aft +   h(cid:1) +   v, b

aftat +   h

(cid:16)

(cid:17)

(cid:17)

bt +   v

(24.4.26)

(24.4.27)

507

draft november 9, 2017

24.4.2 smoothing : rauch-tung-striebel correction method
the smoothed posterior p(ht|v1:t ) is necessarily gaussian since it is the conditional marginal of a larger
gaussian. by representing the posterior as a gaussian with mean gt and covariance gt,

id136

(cid:90)
(cid:90)

p(ht|v1:t )     n (ht gt, gt)

we can form a recursion for gt and gt as follows:

p(ht|v1:t ) =

ht+1

p(ht, ht+1|v1:t )

p(ht|v1:t , ht+1)p(ht+1|v1:t ) =

(cid:90)

ht+1

p(ht|v1:t, ht+1)p(ht+1|v1:t )

=

ht+1

the term p(ht|v1:t, ht+1) can be found by conditioning the joint distribution

p(ht, ht+1|v1:t) = p(ht+1|ht,  v1:t)p(ht|v1:t)

which is obtained in the usual manner by    nding its mean and covariance: the term p(ht|v1:t) is a known
gaussian from    ltering with mean ft and covariance ft. hence the joint distribution p(ht, ht+1|v1:t) has
means

(cid:104)ht|v1:t(cid:105) = ft,

(cid:104)ht+1|v1:t(cid:105) = aft

and covariance elements

(cid:68)

(cid:69)

(cid:68)

   ht   ht

t |v1:t

= ft,

   ht   ht

t+1|v1:t

(cid:69)

= ftat,

(cid:68)

(cid:69)

   ht+1   ht

t+1|v1:t

(24.4.32)

= aftat +   h

(24.4.33)

to    nd p(ht|v1:t, ht+1) we may use the conditioned gaussian results, result(8.4). however, it will turn out to
be useful to use the system reversal result, section(8.4.2), which interprets p(ht|v1:t, ht+1) as an equivalent
linear system going backwards in time:

ht =       atht+1 +       mt +          t
(cid:69)(cid:68)

(cid:68)

where

      at    

   ht   ht

t+1|v1:t

   ht+1   ht

t+1|v1:t

(cid:69)   1

(cid:68)
(cid:17)

(cid:69)(cid:68)

(cid:69)   1

(cid:104)ht+1|v1:t(cid:105)

   ht   ht

t+1|v1:t

   ht+1   ht

t+1|v1:t

      mt     (cid:104)ht|v1:t(cid:105)    
         t 0,         t

(cid:16)
and          t     n
(cid:68)
         t    

   ht   ht

, with

(cid:69)

(cid:68)

t |v1:t

   

   ht   ht

t+1|v1:t

(cid:69)(cid:68)

   ht+1   ht

t+1|v1:t

(cid:69)   1(cid:68)

(cid:69)

   ht+1   ht

t |v1:t

using dynamics reversal, equation (24.4.34) and assuming that ht+1 is gaussian distributed, it is then
straightforward to work out the statistics of p(ht|v1:t ). the mean is given by

and covariance

gt     (cid:104)ht|v1:t(cid:105) =       at (cid:104)ht+1|v1:t(cid:105) +       mt =       atgt+1 +       mt
(cid:69)      at

   ht+1   ht

   ht   ht

=       at

(cid:68)

(cid:69)

(cid:68)

gt    

t |v1:t

t+1|v1:t

t +          t =       atgt+1      at

t +          t

this procedure is the rauch-tung-striebel kalman smoother[249]. this is called a    correction    method
since it takes the    ltered estimate p(ht|v1:t) and    corrects    it to form a smoothed estimate p(ht|v1:t ). the
procedure is outlined in algorithm(24.2) and is detailed in ldsbackwardupdate.m. see also ldssmooth.m.

508

draft november 9, 2017

(24.4.28)

(24.4.29)

(24.4.30)

(24.4.31)

(24.4.34)

(24.4.35)

(24.4.36)

(24.4.37)

(24.4.38)

(24.4.39)

id136

algorithm 24.2 lds backward pass. compute the smoothed posteriors p(ht|v1:t ). this requires the
   ltered results from algorithm(24.1).

{gt, gt} = ldsbackward(gt+1, gt+1, ft, ft;   t)

gt     ft , gt     ft
for t     t     1, 1 do
end for
function ldsbackward(g, g, f , f;   )
  h(cid:48)h(cid:48)     afat +   h,
h(cid:48)h     1
h(cid:48)h(cid:48)  h(cid:48)h,       a       t
          ag      at +         

  h     af +   h,
h(cid:48)h     1
             f       t
          ag +       m, g(cid:48)
g(cid:48)
return g(cid:48), g(cid:48)

  h(cid:48)h     af

h(cid:48)h(cid:48),       m     f           a  h

(cid:46) statistics of p(ht, ht+1|v1:t)
(cid:46) dynamics reversal p(ht|ht+1, v1:t)
(cid:46) backward propagation

end function

sampling a trajectory from p(h1:t|v1:t )
one may use equation (24.4.34) to draw a sample trajectory h1:t from the posterior p(h1:t|v1:t ). we start
by    rst drawing a sample ht from the gaussian posterior p(ht|v1:t ) = n (ht ft , ft ). given this value,
we write

ht   1 =       at   1ht +       mt   1 +          t   1

(cid:16)

(cid:17)

, we then have a value for ht   1. we
by drawing a sample from the zero mean gaussian n
continue in this way, reversing backwards in time, to build up the sample trajectory ht , ht   1, ht   2, . . . , h1.
this is equivalent to the forward-   ltering-backward-sampling approach described in section(23.2.5).

         t   1 0,         t   1

(24.4.40)

the cross moment

an advantage of the dynamics reversal interpretation given above is that the cross moment (which is required
for learning) is immediately obtained from

(cid:68)

(cid:69)

(cid:68)

(cid:69)

   ht   ht

t+1|v1:t

=       atgt+1    

htht

t+1|v1:t

=       atgt+1 + gtgt

t+1

(24.4.41)

for the discrete id48 in section(23.2), we showed how the likelihood can be computed directly from the
  (ht ). we cannot apply that technique here since the message distributions we are
passing are conditional distributions p(ht|v1:t), rather than joint distributions p(ht, v1:t). however, help is
at hand and we may compute the likelihood using the decomposition

ht

24.4.3 the likelihood

   ltered messages(cid:80)
t(cid:89)

p(v1:t ) =

p(vt|v1:t   1)

t=1

in which p(vt|v1:t   1) = n (vt   t,   t) with
  1     b  bt +     

  t     baft   1   t     b(cid:0)aft   1at +   h

  1     b    

(cid:1) bt +   v

t = 1
t > 1

the log likelihood is then given by

log p(v1:t ) =    

1
2

t(cid:88)

t=1

(cid:104)
(vt       t)t      1

t

(cid:105)

(vt       t) + log det (2    t)

draft november 9, 2017

(24.4.42)

(24.4.43)

(24.4.44)

509

24.4.4 most likely state

since the mode of a gaussian is equal to its mean, there is no di   erence between the most probable joint
posterior state

id136

argmax

h1:t

p(h1:t|v1:t )

and the set of most probable marginal states

ht = argmax

ht

p(ht|v1:t ),

t = 1, . . . , t

(24.4.45)

(24.4.46)

hence the most likely hidden state sequence is equivalent to the smoothed mean sequence.

24.4.5 time independence and riccati equations

both the    ltered ft and smoothed gt covariance recursions are independent of the observations v1:t ,
depending only on the parameters of the model. this is a general characteristic of linear gaussian systems.
typically the covariance recursions converge quickly to values that are constant throughout the dynamics,
with only appreciable di   erences close to the boundaries t = 1 and t = t .
in practice therefore one
often drops the time-dependence of the covariances and approximates them with a single time-independent
covariance. this approximation dramatically reduces storage requirements. the converged    ltered f satis   es
the recursion

(cid:17)

(cid:17)   1

(cid:16)

(cid:17)

afat +   h

b

afat +   h

bt +   v

b

afat +   h

(24.4.47)

(cid:17)

bt(cid:16)

(cid:16)

f = afat +   h    

which is a form of algebraic riccati equation. a technique to solve these equations is to initialise the
covariance f to   . with this, a new f is found using the right hand side of (24.4.47), and subsequently
recursively updated. alternatively, using the woodbury identity, the converged covariance satis   es

(cid:18)(cid:16)

f =

afat +   h

+ bt  v

   1b

(cid:19)   1

(cid:16)

(cid:17)   1

(24.4.48)

although this form is less numerically convenient in forming an iterative solver for f since it requires two
matrix inversions.

example 24.4 (newtonian trajectory analysis). a toy rocket with unknown mass and initial velocity
is launched in the air.
in addition, the constant accelerations from the rocket   s propulsion system are
unknown. it is known that newton   s laws apply and an instrument can make very noisy measurements
of the horizontal distance x(t) and vertical height y(t) of the rocket at each time t. based on these noisy
measurements, our task is to infer the position of the rocket at each time.

although this is perhaps most appropriately considered using continuous time dynamics, we will translate
this into a discrete time approximation. newton   s law states that

d2
dt2 x =

fx(t)

m

,

d2
dt2 y =

fy(t)

m

(24.4.49)

where m is the mass of the object and fx(t), fy(t) are the horizontal and vertical forces respectively. as
they stand, these equations are not in a form directly usable in the lds framework. a naive approach is to
reparameterise time to use the variable   t such that t       t   , where   t is integer and     is a unit of time. the
dynamics is then

(cid:48)

(cid:48)

x((  t + 1)   ) = x(  t   ) +    x

(  t   )
dt . we can write an update equation for the x(cid:48) and y(cid:48) as

(  t   ), y((  t + 1)   ) = y(  t   ) +    y

where y(cid:48)(t)     dy

((  t + 1)   ) = x

(cid:48)

(  t   ) + fx(  t   )   /m,

(cid:48)

y

((  t + 1)   ) = y

(cid:48)

(  t   ) + fx(  t   )   /m

(24.4.50)

(24.4.51)

(cid:48)

x

510

draft november 9, 2017

learning linear dynamical systems

figure 24.7: estimate of the trajectory of a newtonian
ballistic object based on noisy observations (small cir-
cles). all time labels are known but omitted in the
plot. the    x    points are the true positions of the ob-
ject, and the crosses    +    are the estimated smoothed
mean positions (cid:104)xt, yt|v1:t(cid:105) of the object plotted every
several time steps. see demoldstracking.m

these two sets of coupled discrete time di   erence equations approximate newton   s law equation (24.4.49).
for simplicity, we relabel ax(t) = fx(t)/m(t), ay(t) = fy(t)/m(t). these accelerations are unknown but
assumed to change slowly over time

ax((  t + 1)   ) = ax(  t   ) +   x, ay((  t + 1)   ) = ay(  t   ) +   y,

(24.4.52)

where   x and   y are small noise terms. the initial distributions for the accelerations are assumed vague,
using a zero mean gaussian with large variance. we describe the above model by de   ning

as the hidden variable, giving rise to a h = 6 dimensional lds with transition and emission matrices as
below:

(24.4.53)

(cid:2)x

                        

(t), y(t), ax(t), ay(t)(cid:3)t

(cid:48)

(t), x(t), y

(cid:48)

ht    

a =

0
0
1

0     0
1
0
0
0
0
    1
0    
0
0
0
0     1
0
0
0
0
1
0
0
0
0
0
0
0
1

0
0

                         ,

(cid:18) 0 1 0 0 0 0

(cid:19)

0 0 0 1 0 0

b =

(24.4.54)

we use large a covariance      to state that little is known about the latent state initial values. based
then on noisy observations vt = bht +   t we attempt to infer the unknown trajectory using smoothing. a
demonstration is given in    g(24.7). despite the signi   cant observation noise, the object trajectory can be
accurately inferred.

24.5 learning linear dynamical systems

whilst in many applications, particularly of underlying known physical processes, the parameters of the
lds are known, in many machine learning tasks we need to learn the parameters of the lds based on v1:t .
for simplicity we assume that we know the dimensionality h of the lds.

24.5.1 identi   ability issues

an interesting question is whether we can uniquely identify (learn) the parameters of an lds. there are
always trivial redundancies in the solution obtained by permuting the hidden variables arbitrarily and
   ipping their signs. to show that there are potentially many more equivalent solutions, consider the following
lds

vt = bht +   v
t ,

ht = aht   1 +   h
t

(24.5.1)

we now attempt to transform this original system to a new form which will produce exactly the same
outputs v1:t . for an invertible matrix r we consider

rht = rar   1rht   1 + r  h

t

draft november 9, 2017

(24.5.2)

511

   1000100200300400500600700800   150   100   50050100150200250300xywhich is representable as a new latent dynamics

  ht =   a  ht   1 +     h
t

learning linear dynamical systems

(24.5.3)

where   a     rar   1,   ht     rht,     h

the transformed h:

t     r  h

t . in addition, we can reexpress the outputs to be a function of

vt = br   1rht +   v

t =   b  ht +   v
t

(24.5.4)

hence, provided we place no constraints on a, b and   h there exists an in   nite space of equivalent solutions,
  a = ra r   1,   b = br   1,     h = r  hrt, all with the same likelihood value. this means that interpreting
the learned parameters needs to be done with some care.

24.5.2 em algorithm

for simplicity, we assume we have a single sequence v1:t , to which we wish to    t a lds using maximum
likelihood. since the lds contains latent variables one approach is to use the em algorithm. as usual, the
m-step of the em algorithm requires us to maximise the energy

(cid:104)log p(v1:t , h1:t )(cid:105)pold(h1:t |v1:t )

(24.5.5)

with respect to the parameters a, b, a,   ,   v,   h. thanks to the form of the lds the energy decomposes
as

t(cid:88)

t=2

t(cid:88)

t=1

(cid:104)log p(h1)(cid:105)pold(h1|v1:t ) +

(cid:104)log p(ht|ht   1)(cid:105)pold(ht,ht   1|v1:t ) +

(cid:104)log p(vt|ht)(cid:105)pold(ht|v1:t )

(24.5.6)

it is straightforward to derive that the m-step for the parameters is given by (angled brackets (cid:104)  (cid:105) denote
expectation with respect to the smoothed posterior pold(h1:t|v1:t )):

  new
   = (cid:104)h1(cid:105)
  new
   =

t=1

(cid:68)

(cid:69)

htht
t

h1ht
1

ht+1ht
t

(cid:69)(cid:33)   1
(cid:69)(cid:33)   1

(cid:69)(cid:32)t   1(cid:88)
(cid:68)
    (cid:104)h1(cid:105)(cid:104)h1(cid:105)t
(cid:32) t(cid:88)
(cid:68)

(cid:68)
t   1(cid:88)
t(cid:88)
vt (cid:104)ht(cid:105)t
t + bnew(cid:68)
(cid:16)
t(cid:88)
t     vt (cid:104)ht(cid:105)t bnewt     bnew (cid:104)ht(cid:105) vt
(cid:69)
(cid:68)
(cid:69)
(cid:16)(cid:68)
t   1(cid:88)

htht
t

vtvt

1
t

t=1

t=1

t=1

t=1

1

htht

t+1

ht+1ht
t

   

anew =

bnew =

  new

v =

  new

h =

t+1

(cid:69)

ht+1ht

    anew(cid:68)
t     vt (cid:104)ht(cid:105)t bnewt(cid:17)
    anew(cid:68)
(cid:16)(cid:68)

ht+1ht

(cid:69)

t+1

the above can be simpli   ed to

t     1

t=1

(cid:88)

(cid:16)

t

vtvt

  new

v =

1
t

similarly,

t   1(cid:88)

t=1

  new

h =

1

t     1

(cid:69)(cid:17)

htht

t+1

(cid:69)

htht
t

bnewt(cid:17)
anewt + anew(cid:68)

(24.5.7)

(24.5.8)

(24.5.9)

(24.5.10)

(cid:69)

htht
t

(24.5.11)

anewt(cid:17)

(24.5.12)

(24.5.13)

(24.5.14)

the statistics required therefore include smoothed means, covariances, and cross moments. the extension
to learning multiple timeseries is straightforward since the energy is simply summed over the individual

512

draft november 9, 2017

learning linear dynamical systems

sequences.

the performance of the em algorithm for the lds often depends heavily on the initialisation. if we remove
the hidden to hidden links, the model is closely related to factor analysis (the lds can be considered a
temporal extension of factor analysis). one initialisation technique therefore is to learn the b matrix using
factor analysis by treating the observations as temporally independent.

note that whilst the lds model in not identi   able, as described in section(24.5.1), the m-step for the em
algorithm is unique. this apparent contradiction is resolved when one considers that the em algorithm is
a conditional method, updating depending on the previous parameters and ultimately the initialisation. it
is this initialisation that breaks the invariance.

24.5.3 subspace methods

an alternative to maximum likelihood training is to use a subspace method[302, 266]. the chief bene   t
of these techniques is that they avoid the convergence di   culties of em. to motivate subspace techniques,
consider a deterministic lds

ht = aht   1,

vt = bht

(24.5.15)
under this assumption, vt = bht = baht   1 and, more generally, vt = bat   1h1. this means that a h-
dimensional system underlies all visible information since all points ath1 lie in a h-dimensional subspace,
which is then projected to form the observation. this suggests that some form of subspace identi   cation
technique will enable us to learn a and b.

given a set of observation vectors v1, . . . , vt, consider the block hankel matrix formed from stacking l
consecutive observation vectors. for example, for t = 6 and l = 3, this is

m =

      

v2 v3 v4 v5
v3 v4 v5 v6

       v1 v2 v3 v4
       bh1
(cid:124)(cid:123)(cid:122)(cid:125)

we now    nd the svd of m,

m =   u   s   vt
w

if the v are generated from a (noise free) lds, we can write

m =

bh2

bh4
bah1 bah2 bah3 bah4
ba2h1 ba2h2 ba2h3 ba2h4

bh3

       =

       b

ba
ba2

       (h1 h2 h3 h4)

(24.5.16)

(24.5.17)

(24.5.18)

where w is termed the extended observability matrix . the matrix   s will contain the singular values up to
the dimension of the hidden variables h, with the remaining singular values 0. from equation (24.5.17),
this means that the emission matrix b is contained in   u1:v,1:h . the estimated hidden variables are then
contained in the submatrix w1:h,1:t   l+1,

(h1 h2 h3 h4) = w1:h,1:t   l+1

(24.5.19)

based on the relation ht = aht   1 one can then    nd the best least squares estimate for a by minimising

t(cid:88)

t=2

(ht     aht   1)2

for which the optimal solution is

a = (h2 h3

. . . ht) (h1 h2

   
. . . ht   1)

draft november 9, 2017

(24.5.20)

(24.5.21)

513

switching auto-regressive models

s1

v1

s2

v2

s3

v3

s4

v4

in
figure 24.8: a    rst order switching ar model.
terms of id136, conditioned on v1:t , this is a id48.

where     denotes the pseudo inverse, see ldssubspace.m. estimates for the covariance matrices can also be
obtained from the residual errors in    tting the block hankel matrix and extended observability matrix.
whilst this derivation formally holds only for the noise free case one can nevertheless apply this in the case
of non-zero noise and hope to gain an estimate for a and b that is correct in the mean. in addition to
forming a solution in its own right, the subspace method forms a potentially useful way to initialise the em
algorithm.

24.5.4 structured ldss

many physical equations are local both in time and space. for example in weather models the atmosphere
is partitioned into cells hi(t) each containing the pressure at that location. the equations describing how
the pressure updates only depend on the pressure at the current cell and small number of neighbouring cells
at the previous time t    1. if we use a linear model and measure some aspects of the cells at each time, then
the weather is describable by a lds with a highly structured sparse transition matrix a. in practice, the
weather models are non-linear but local linear approximations are often employed[271]. a similar situation
arises in brain imaging in which voxels (local cubes of activity) depend only on their neighbours from the
previous timestep[112].

another application of structured ldss is in temporal independent component analysis. this is de   ned
as the discovery of a set of independent latent dynamical processes, from which the data is a projected
observation.
if each independent dynamical process can itself be described by a lds, this gives rise to
a structured lds with a block diagonal transition matrix a. such models can be used to extract inde-
pendent components under prior knowledge of the likely underlying frequencies in each of the temporal
compoments[63].

24.5.5 bayesian ldss

p(v1:t ) = (cid:82)

the extension to placing priors on the transition and emission parameters of the lds leads in general to
computational di   culties in computing the likelihood. for example, for a prior on a, the likelihood is
a p(v1:t|a)p(a) which is di   cult to evaluate since the dependence of the likelihood on the
matrix a is a complicated function. approximate treatments of this case are beyond the scope of this
book, although we brie   y note that sampling methods[58, 106] are popular in this context, in addition to
deterministic variational approximations[28, 24, 63].

24.6 switching auto-regressive models

whilst the linear dynamical models considered so far in this chapter are powerful, they nevertheless have
some inherent restrictions. for example, they cannot model abrupt changes in the observation. here we
describe an extension of the ar model. we consider a set of s di   erent ar models, each with associated
coe   cients a(s), s = 1, . . . ,
s, and allow the model to select one of these ar models at each time. for a
time-series of scalar values v1:t an lth order switching ar model can be written as

@@

(cid:0)  t 0,   2(st)(cid:1)
where we now have a set of ar coe   cients    =(cid:8)a(s),   2(s), s     {1, . . . , s}
themselves have a markov transition p(s1:t ) =(cid:81)

t   1a(st) +   t,

  t     n

vt =   vt

(cid:9). the discrete switch variables

(24.6.1)

t p(st|st   1) so that the full model is, see    g(24.7),

p(v1:t , s1:t|  ) =

514

p(vt|vt   1, . . . , vt   l, st,|  )p(st|st   1)

(24.6.2)

draft november 9, 2017

(cid:89)

t

switching auto-regressive models

24.6.1 id136

figure 24.9: learning a switching ar
model. the upper plot shows the train
data. the colour indicates which of the
two ar models is active at that time.
whilst this information is plotted here, this
is assumed unknown to the learning algo-
rithm, as are the coe   cients a(s). we
assume that the order l = 2 and num-
ber of switches s = 2 however is known.
in the bottom plot we show the time se-
ries again after training in which we colour
the points according to the most likely
smoothed ar model at each timestep. see
demosarlearn.m.

given an observed sequence v1:t and parameters    id136 is straightforward since this is a form of id48.
to make this more apparent we may write

(cid:89)

p(v1:t , s1:t ) =

t

where

  p(vt|st)p(st|st   1)

  p(vt|st)     p(vt|vt   1, . . . , vt   l, st) = n

(cid:16)

(cid:17)

vt   vt

t   1a(st),   2(st)

(cid:88)

st   1

  (st) =

  p(vt|st)p(st|st   1)  (st   1)

note that the emission distribution   p(vt|st) is time-dependent. the    ltering recursion is then

smoothing can be achieved using the standard recursions, modi   ed to use the time-dependent emissions,
see demosarid136.m.

with high frequency data it is unlikely that a change in the switch variable is reasonable at each time t. a
simple constraint to account for this is to use a modi   ed transition

(cid:26) p(st|st   1)

  p(st|st   1) =

mod (t, tskip) = 0

   (st     st   1) otherwise

24.6.2 maximum likelihood learning using em

to    t the set of ar coe   cients and innovation variances, a(s),   2(s), s = 1, . . . , s, using maximum likelihood
training for a set of data v1:t , we may make use of the em algorithm.

m-step

e =

up to negligible constants, the energy is given by

(cid:88)
t (cid:104)log p(vt|  vt   1, a(st))(cid:105)pold(st|v1:t ) +
(cid:17)2
(cid:88)

(cid:28) 1

(cid:16)

   2e =

  2(st)

t

vt       vt

t   1a(st)

+ log   2(st)

(cid:29)

(cid:88)
t (cid:104)log p(st|st   1)(cid:105)pold(st,st   1)

draft november 9, 2017

which we need to maximise with respect to the parameters   . using the de   nition of the emission and
isolating the dependency on a, we have

+ const.

pold(st|v1:t )

(24.6.8)

515

(24.6.3)

(24.6.4)

(24.6.5)

(24.6.6)

(24.6.7)

050100150200250300350400   100   50050100sample switches050100150200250300350400   100   50050100learned switchesswitching auto-regressive models

a(s)

(24.6.9)

on di   erentiating with respect to a(s) and equating to zero, the optimal a(s) satis   es the linear equation

(cid:35)

(cid:88)

t

pold(st = s|v1:t )

vt   vt   1
  2(s)

=

(cid:34)(cid:88)

  2(s) =

(cid:80)
1
t(cid:48) pold(s(cid:48)
t = s|v1:t )

t   1

  vt   1   vt
  2(s)

t

pold(st = s|v1:t )
(cid:104)
vt       vt

pold(st = s|v1:t )

(cid:88)

t

(cid:105)2

t   1a(s)

which may be solved using gaussian elimination. similarly one may show that updates that maximise the
energy with respect to   2 are

(24.6.10)

the update for p(st|st   1) follows the standard em for id48 rule, equation (23.3.5), see sarlearn.m. here
we don   t include an update for the prior p(s1) since there is insu   cient information at the start of the
sequence and assume p(s1) is    at.

e-step

the m-step requires the smoothed statistics pold(st = s|v1:t ) and pold(st = s, st   1 = s(cid:48)

obtained from id48 id136.

|v1:t ) which can be

example 24.5 (learning a switching ar model). in    g(24.9) the train data is generated by an switching
ar model so that we know the ground truth as to which model generated which parts of the data. based
on the train data (assuming the labels st are unknown), a switching ar model is    tted using em. in this
case the problem is straightforward so that a good estimate is obtained of both the sets of ar parameters
and which switches were used at which time.

example 24.6 (modelling parts of speech). in    g(24.10) a segment of a speech signal (corresponding to
the spoken digit    four   ) is shown. we model this data using a switching ar model with s = 10 states.
each of the 10 available ar models is responsible for modelling the dynamics of a basic subunit of speech
[98, 208]. the model was trained on many example clean spoken digit sequences using s = 10 states with a
left-to-right transition matrix. an additional complexity is that we wish to use the model to clean up a noisy
speech signal. a simple model for the noise is that it is gaussian and additive on the original speech signal
vt, forming now a new noisy observation   vt. based then on a sequence of noisy   v1:t we wish to infer the
switch states and the clean signal v1:t . unfortunately, this task is no longer in the form of an sar since we
now have both continuous and discrete latent states. formally this model falls in the class of the switching
linear dynamical systems, as described in the following chapter. the results using this more complex model
show how denoising of complex signals can be achieved and are a prelude to the more advanced material in
the following chapter.

24.7 summary

    continuous observations can be modelled using autoregressive models.
    observed linear dynamical systems are the vector versions of autoregressive models.
    latent continuous dynamical processes can be used to model many physical systems.

in order for these
to be computationally tractable, one needs to restrict the transition and emission distributions. the latent
linear dynamical system is such as restriction to linear gaussian transitions and emissions.

    the latent linear dynamical system is a powerful timeseries model with widespread application in tracking

and signal representation.

516

draft november 9, 2017

code

s1

v1

  v1

s2

v2

  v2

s3

v3

  v3

s4

v4

  v4

figure 24.10: (a): a latent switching (second order) ar model. here the st indicates which of a set of 10
available ar models is active at time t. the square nodes emphasise that these are discrete variables. the
   clean    ar signal vt, which is not observed, is corrupted by additive noise to form the noisy observations
  vt.
in terms of id136, conditioned on   v1:t , this can be expressed as a switching lds, chapter(25).
(b): signal reconstruction using the latent switching ar model in (a). top: noisy signal   v1:t ; bottom:
reconstructed clean signal v1:t . the dashed lines and the numbers show the most-likely state segmentation
arg maxs1:t p(s1:t|  v1:t ) of the 10 states, from left (1) to right (10).

24.8 code

in the linear dynamical system code below only the simplest form of the recursions is given. no attempt
has been made to ensure numerical stability.

ldsforwardupdate.m: lds forward
ldsbackwardupdate.m: lds backward
ldssmooth.m: linear dynamical system :    ltering and smoothing
ldsforward.m: alternative lds forward algorithm (see slds chapter)
ldsbackward.m: alternative lds backward algorithm (see slds chapter)
demosumprodgausscanonlds.m: sum-product algorithm for smoothed id136
demoldstracking.m: demo of tracking in a newtonian system
ldssubspace.m: subspace learning (hankel matrix method)
demoldssubspace.m: demo of subspace learning method

24.8.1 autoregressive models

note that in the code the autoregressive vector a has as its last entry the    rst ar coe   cient (i.e. in reverse
order to that presented in the text).

artrain.m: learn ar coe   cients (gaussian elimination)
demoartrain.m: demo of    tting an ar model to data
arlds.m: learn time-varying ar coe   cients using a lds
demoarlds.m: demo of learning ar coe   cients using an lds
demosarid136.m: demo for id136 in a switching autoregressive model
sarlearn.m: learning of a sar using em
demosarlearn.m: demo of sar learning
id48forwardsar.m: switching autoregressive id48 forward pass
id48backwardsar.m: switching autoregressive id48 backward pass

draft november 9, 2017

517

123456789101234567891024.9 exercises

exercise 24.1. consider the two-dimensional linear model

ht = r  ht   1,

r   =

(cid:18) cos        sin   

sin   

cos   

(cid:19)

exercises

(24.9.1)

where r   is a rotation matrix which rotates the vector ht through angle    in one timestep.

1. explain why the eigenvalues of a rotation matrix are (in general) imaginary.

2. explain how to model a sinusoid, rotating with angular velocity    using a two-dimensional latent lds.

3. by writing(cid:18) xt

(cid:19)

yt

(cid:18) r11 r12

r21 r22

(cid:19)(cid:18) xt   1

yt   1

(cid:19)

=

eliminate yt to write an equation for xt+1 in terms of xt and xt   1.

4. explain how to model a sinusoid using an ar model.

(24.9.2)

5. explain the relationship between the second order di   erential equation   x =      x, which describes
a harmonic oscillator, and the second order di   erence equation which approximates this di   erential
equation. is it possible to    nd a di   erence equation which exactly matches the solution of the di   erential
equation at particular points?

exercise 24.2. show that for any square anti-symmetric matrix m,

m =    mt

the matrix exponential (in matlab this is expm)

a = exp (m)

is orthogonal, namely

ata = i

(24.9.3)

(24.9.4)

(24.9.5)

explain how to construct random orthogonal matrices with some control over the angles of the complex
eigenvalues.

exercise 24.3. run demoldstracking.m which tracks a ballistic object using a linear dynamical system,
see example(24.4). modify demoldstracking.m so that in addition to the x and y positions, the x speed is
also observed. compare and contrast the accuracy of the tracking with and without this extra information.

exercise 24.4. nightsong.mat contains a small stereo segment nightingale song sampled at 44100 hertz.

1. plot the original waveform using plot(x(:,1))

2. plot the spectrogram using

y=myspecgram(x(:,1),1024,44100); imagesc(log(abs(y)))

3. the routine demogmmem.m demonstrates    tting a mixture of gaussians to data. the mixture assign-
ment probabilities are contained in phgn. write a routine to cluster the data v=log(abs(y)) using 8
gaussian components, and explain how one might segment the series x into di   erent regions.

4. examine demoarlds.m which    ts autoregressive coe   cients using an interpretation as a linear dy-
namical system. adapt the routine demoarlds.m to learn the ar coe   cients of the data x. you will
almost certainly need to subsample the data x     for example by taking every 4th datapoint. with the
learned ar coe   cients (use the smoothed results)    t a gaussian mixture with 8 components. compare
and contrast your results with those obtained from the gaussian mixture model    t to the spectrogram.

518

draft november 9, 2017

exercises

exercise 24.5. consider a supervised learning problem in which we make a linear model of the scalar output
yt based on vector input xt:

yt = wt

t xt +   y

t

(24.9.6)

where   y

t is zero mean gaussian noise with variance   2

y. train data d = {(xt, yt), t = 1, . . . , t} is available.
1. for a time-invariant weight vector wt     w, explain how to    nd the single weight vector w and the

noise variance   2

y by maximum likelihood.

2. extend the above model to include a transition

wt = wt   1 +   w
t

(24.9.7)

where   w
is zero mean gaussian noise with a given covariance   w; w1 has zero mean. explain
t
how to cast    nding (cid:104)wt|d(cid:105) as smoothing in a related linear dynamical system. write a routine w =
linpredar(x,y,sigmaw,sigmay) that takes an input data matrix x = [x1, . . . , xt ] where each column
contains an input, and vector y = [y1, . . . , yt ]t; sigmaw is the additive weight noise and sigmay is an
assumed known time-invariant output noise. the returned w contains the smoothed mean weights.

exercise 24.6. this exercise relates to forming an   -   style smoothing approach, as described in sec-
tion(23.2.3), but applied to the lds. note that the derivations in section(23.2.3) hold for continuous vari-
ables as well simply on replacing summation with integration.

1. by virtue of the fact that the smoothed posterior for a lds,   (ht)     p(ht|v1:t ) is gaussian, and using
the relation   (ht) =   (ht)  (ht), explain why the    message for an lds can be represented in the form

(cid:19)

(cid:18)

(cid:90)

  (ht) = ztexp

1
2

   

ht
t ztht + ht

t zt

where zt is a (not necessarily full rank) matrix.

2. based on the recursion

  (ht   1) =

p(vt|ht)p(ht|ht   1)  (ht)

ht

derive the recursion (ignoring the prefactor zt)

lt = q   1
zt   1 = at
t
zt   1 = at

t + bt

(cid:0)q   1
t r   1
(cid:16)
t     q   1
t l   1

t bt + zt
t l   1
t q   1
t r   1
bt
t + zt

(cid:1) at
(cid:17)

t q   1

t

t

with initialisation zt = 0, zt = 0. the notation qt       h
distribution p(ht|ht   1), and rt       v

t is the covariance of the emission p(vt|ht).

t is the covariance matrix of the transition

3. show that the posterior covariance and mean are given by

(cid:0)f   1

(cid:1)   1

(cid:0)f   1

(cid:1)   1(cid:0)f   1

t

(cid:1)

t + zt

,

t + zt

ft + zt

where ft and ft are the    ltered mean and covariance.

note that this parallel smoothing recursion is not appropriate in the case of small covariance since, due to
the explicit appearance of the inverse of the covariances, numerical stability issues can arise. however, it is
possible to reexpress the recursion without explicit reference to inverse noise covariances, see [17].

draft november 9, 2017

519

(24.9.8)

(24.9.9)

(24.9.10)

(24.9.11)

(24.9.12)

(24.9.13)

exercises

520

draft november 9, 2017

chapter 25

switching linear dynamical systems

id48 assume that the underlying process is discrete; linear dynamical systems that the
underlying process is continuous. however, there are scenarios in which the underlying system might jump
from one continuous regime to another. in this chapter we discuss a class of models that can be used in this
situation. unfortunately the technical demands of this class of models are somewhat more involved than in
previous chapters, although the models are correspondingly more powerful.

25.1 introduction

complex timeseries which are not well described globally by a single linear dynamical system may be
divided into segments, each modelled by a potentially di   erent lds. such models can handle situations
in which the underlying model    jumps    from one parameter setting to another. for example a single lds
might well represent the normal    ows in a chemical plant. when a break in a pipeline occurs, the dynamics
of the system changes from one set of linear    ow equations to another. this scenario can be modelled
using a set of two linear systems, each with di   erent parameters. the discrete latent variable at each time
st     {normal, pipe broken} indicates which of the ldss is most appropriate at the current time. this is
called a switching lds and is used in many disciplines, from econometrics to machine learning[13, 64, 60,
236, 325, 190].

25.2 the switching lds

at each time t, a switch variable st     1, . . . , s describes which of a set of ldss is to be used. the continuous
observation (or    visible   ) variable vt, dim (vt) = v is linearly related to the continuous hidden variable ht,
dim (ht) = h by

vt = b(st)ht +   v(st),

  v(st)     n (  v(st)   v(st),   v(st))

(25.2.1)

here st describes which one from the set of emission matrices {b(1), . . . , b(s)} is active at time t. the
observation noise   v(st) is drawn from a gaussian with mean   v(st) and covariance   v(st). the transition
dynamics of the continuous hidden state ht is linear,

(cid:16)

(cid:17)

ht = a(st)ht   1 +   h(st),

  h(st)     n

  h(st)   h(st),   h(st)

(25.2.2)

and the switch variable st selects a single transition matrix from the available set {a(1), . . . , a(s)}. the
gaussian transition noise   h(st) also depends on the switch variable. the dynamics of st itself is markovian,
with transition p(st|st   1). for the more general    augmented    aslds model the switch st is dependent on

521

gaussian sum filtering

s1

s2

s3

s4

h1

v1

h2

v2

h3

v3

h4

v4

figure 25.1: the independence structure of the aslds.
square nodes st denote discrete switch variables; ht are
continuous latent/hidden variables, and vt continuous ob-
served/visible variables. the discrete state st determines
which linear dynamical system from a    nite set of linear
dynamical systems is operational at time t. in the slds
links from h to s are not normally considered.

both the previous st   1 and ht   1. the model de   nes a joint distribution (see    g(25.1))

t(cid:89)

p(v1:t , h1:t , s1:t ) =

with

t=1

p(vt|ht, st)p(ht|ht   1, st)p(st|ht   1, st   1)

p(vt|ht, st) = n (vt   v(st) + b(st)ht,   v(st)) ,

p(ht|ht   1, st) = n

(cid:0)ht   h(st) + a(st)ht,   h(st)(cid:1)

(25.2.3)

(cid:90)

(cid:88)

st

ht

at time t = 1, p(s1|h0, s0) denotes the initial switch distribution p(s1), and p(h1|h0, s1) denotes the initial
gaussians p(h1|s1) = n (h1     (s1),     (s1)).
the slds can be thought of as a marriage between a hidden markov model and a linear dynamical system.
the slds is also called a jump markov model/process, switching kalman filter, switching linear gaussian
state space model, conditional linear gaussian model.

25.2.1 exact id136 is computationally intractable

both exact    ltered and smoothed id136 in the slds is intractable, scaling exponentially with time. as
an informal explanation, consider    ltered posterior id136 for which, by analogy with equation (23.2.9),
the forward pass is

p(st+1, ht+1|v1:t+1) =

p(st+1, ht+1|st, ht, vt+1)p(st, ht|v1:t)

(25.2.4)

at timestep 1, p(s1, h1|v1) = p(h1|s1, v1)p(s1|v1) is an indexed set of gaussians. at timestep 2, due to the
summation over the states s1, p(s2, h2|v1:2) will be an indexed set of s gaussians; similarly at timestep
3, it will be s2 and, in general, gives rise to st   1 gaussians at time t. even for small t, the number of
components required to exactly represent the    ltered distribution is therefore computationally intractable.
analogously, smoothing is also intractable. the origin of the intractability of the slds di   ers from    struc-
tural intractability    that we   ve previously encountered. in the slds, in terms of the cluster variables x1:t ,
with xt     (st, ht) and visible variables v1:t , the graph of the distribution is singly-connected. from a purely
graph theoretic viewpoint, one would therefore envisage little di   culty in carrying out id136. indeed, as
we saw above, the derivation of the    ltering algorithm is straightforward since the graph is singly-connected.
however, the numerical implementation of the algorithm is intractable since the description of the messages
requires an exponentially increasing number of terms.

in order to deal with this intractability, several approximation schemes have been introduced [106, 121,
189, 175, 174]. here we focus on techniques which approximate the switch conditional posteriors using a
limited mixture of gaussians. since the exact posterior distributions are mixtures of gaussians, albeit with
an exponentially large number of components, the aim is to drop low weight components such that the
resulting approximation accurately represents the posterior.

25.3 gaussian sum filtering

equation(25.2.4) describes the exact    ltering recursion and generates an exponentially increasing number of
components with time. in general, however, the in   uence of ancient observations will be much less relevant

522

draft november 9, 2017

gaussian sum filtering

i(cid:88)

it=1

(cid:90)

(cid:88)

st

ht

than that of recent observations. this suggests that the    e   ective time    is limited and that therefore a cor-
responding limited number of components in the gaussian mixture may su   ce to accurately represent the
   ltered posterior [6]. our aim is to form a recursion for p(st, ht|v1:t) based on a gaussian mixture approxi-
mation of p(ht|st, v1:t). given an approximation of the    ltered distribution p(st, ht|v1:t)     q(st, ht|v1:t), the
exact recursion equation (25.2.4) is approximated by

q(st+1, ht+1|v1:t+1) =

p(st+1, ht+1|st, ht, vt+1)q(st, ht|v1:t)

(25.3.1)

this approximation to the    ltered posterior at the next timestep will contain s times more components
than in the previous timestep and, to prevent an exponential explosion in mixture components, we will need
to subsequently collapse the mixture q(st+1, ht+1|v1:t+1) in a suitable way. it is useful to break the    ltered
approximation for equation (25.2.4) into continuous and discrete parts:

q(ht, st|v1:t) = q(ht|st, v1:t)q(st|v1:t)

and derive separate    ltered update formulae, as described below.

(25.3.2)

25.3.1 continuous    ltering

the exact representation of p(ht|st, v1:t) is a mixture with o(cid:0)st   1(cid:1) components. to retain computational

feasibility we therefore approximate this with a limited i-component mixture

q(ht|st, v1:t) =

q(ht|it, st, v1:t)q(it|st, v1:t)

(25.3.3)

where q(ht|it, st, v1:t) is a gaussian parameterised with mean f (it, st) and covariance f(it, st). strictly
speaking, we should use the notation ft(it, st) since, for each time t, we have a set of means indexed by it, st,
although we drop these dependencies in the notation used here.

state of st. naturally, this gives rise to a mixture of gaussians for p(ht|v1:t) =(cid:80)

an important remark is that many techniques approximate p(ht|st, v1:t) using a single gaussian, one for each
st p(ht|st, v1:t)p(st|v1:t).
however, in making a single gaussian approximation to p(ht|st, v1:t) the representation of the posterior
may be poor. our aim here is to maintain an accurate approximation to p(ht|st, v1:t) by using a mixture of
gaussians.

to    nd a recursion for the approximating distribution we    rst assume that we know the    ltered approxi-
mation q(ht, st|v1:t) and then propagate this forwards using the exact dynamics. to do so consider    rst the
relation

q(ht+1|st+1, v1:t+1) =

=

q(ht+1, st, it|st+1, v1:t+1)

q(ht+1|st, it, st+1, v1:t+1)q(st, it|st+1, v1:t+1)

(25.3.4)

(cid:88)
(cid:88)

st,it

st,it

wherever possible we now substitute the exact dynamics and evaluate each of the two factors above. the
usefulness of decomposing the update in this way is that the new    ltered approximation is of the form of
a gaussian mixture, where q(ht+1|st, it, st+1, v1:t+1) is gaussian and q(st, it|st+1, v1:t+1) are the weights or
mixing proportions of the components. we describe below how to compute these terms explicitly. equa-
tion(25.3.4) produces a new gaussian mixture with i    s components which we will collapse back to i
components at the end of the computation.

evaluating q(ht+1|st, it, st+1, v1:t+1)
we aim to    nd a    ltering recursion for q(ht+1|st, it, st+1, v1:t+1). since this is conditional on switch states
and components, this corresponds to a single lds forward step which can be evaluated by considering    rst
the joint distribution

q(ht+1, vt+1|st, it, st+1, v1:t) =

draft november 9, 2017

ht

p(ht+1, vt+1|ht,  st,   it, st+1, v1:t)q(ht|st, it,   st+1, v1:t)

(25.3.5)

523

(cid:90)

gaussian sum filtering

and subsequently conditioning on vt+1. in the above we use the exact dynamics where possible. to ease
the burden on notation we derive this for   ht,   vt     0 for all t. the exact forward dynamics is then given by
(25.3.6)

vt+1 = b(st+1)ht+1 +   v(st+1),

ht+1 = a(st+1)ht +   h(st+1),

given the mixture component index it,

q(ht|v1:t, it, st) = n (ht f (it, st), f(it, st))

(25.3.7)

we propagate this gaussian with the exact dynamics equation (25.3.6). then q(ht+1, vt+1|st, it, st+1, v1:t)
is a gaussian with covariance and mean elements

  hh = a(st+1)f(it, st)at(st+1) +   h(st+1),   vv = b(st+1)  hhbt(st+1) +   v(st+1)
  vh = b(st+1)  hh =   t

hv,   v = b(st+1)a(st+1)f (it, st),   h = a(st+1)f (it, st)

(25.3.8)

these results are obtained from integrating the forward dynamics, equations (25.2.1,25.2.2) over ht, using
result(8.3). to    nd q(ht+1|st, it, st+1, v1:t+1) we now condition q(ht+1, vt+1|st, it, st+1, v1:t) on vt+1 using
the standard gaussian conditioning formulae, result(8.4), to obtain

(cid:16)

(cid:17)

q(ht+1|st, it, st+1, v1:t+1) = n

with

ht+1   h|v,   h|v

  h|v =   h +   hv     1

vv (vt+1       v) ,

  h|v =   hh       hv     1

vv   vh

where the quantities required are de   ned in equation (25.3.8).

(25.3.9)

(25.3.10)

evaluating the mixture weights q(st, it|st+1, v1:t+1)
up to a normalisation constant the mixture weight in equation (25.3.4) can be found from

q(st, it|st+1, v1:t+1)     q(vt+1|it, st, st+1, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t)

(25.3.11)

the    rst factor in equation (25.3.11), q(vt+1|it, st, st+1, v1:t) is gaussian with mean   v and covariance   vv,
as given in equation (25.3.8). the last two factors q(it|st, v1:t) and q(st|v1:t) are given from the previous
   ltered iteration. finally, q(st+1|it, st, v1:t) is found from

q(st+1|it, st, v1:t) =

(cid:104)p(st+1|ht, st)(cid:105)q(ht|it,st,v1:t)
p(st+1|st)

augmented slds
standard slds

(25.3.12)

(cid:26)

in the aslds, the term in equation (25.3.12) will generally need to be computed numerically. a sim-
ple approximation is to evaluate equation (25.3.12) at the mean value of the distribution q(ht|it, st, v1:t).
to take covariance information into account an alternative would be to draw samples from the gaussian
q(ht|it, st, v1:t) and thus approximate the average of p(st+1|ht, st) by sampling. note that this does not
equate gaussian sum    ltering for the augmented slds with a sequential sampling procedure, such as
id143ing, section(27.6.2). the sampling here is exact, for which no convergence issues arise.

closing the recursion

we are now in a position to calculate equation (25.3.4). for each setting of the variable st+1, we have a
mixture of i    s gaussians. to prevent the number of components increasing exponentially with time, we
numerically collapse q(ht+1|st+1, v1:t+1) back to i gaussians to form

q(ht+1|st+1, v1:t+1)    

q(ht+1|it+1, st+1, v1:t+1)q(it+1|st+1, v1:t+1)

(25.3.13)

i(cid:88)

it+1=1

the numerical collapse then generates the new gaussian components and corresponding mixture weights.
any method of choice may be supplied to collapse a mixture to a smaller mixture. a straightforward
approach is to repeatedly merge low-weight components, as explained in section(25.3.4). in this way the
new mixture coe   cients q(it+1|st+1, v1:t+1), it+1     1, . . . , i are de   ned. this completes the description of
how to form a recursion for the continuous    ltered posterior approximation q(ht+1|st+1, v1:t+1) in equation
(25.3.2).

524

draft november 9, 2017

gaussian sum filtering

figure 25.2: gaussian sum filtering. the leftmost column depicts the
previous gaussian mixture approximation q(ht, it|v1:t) for two states s = 2
(red and blue) and three mixture components i = 3, with the mixture weight
represented by the area of each oval. there are s = 2 di   erent linear systems
which take each of the components of the mixture into a new    ltered state,
the colour of the arrow indicating which dynamic system is used. after one
time-step each mixture component branches into a further s components so
that the joint approximation q(ht+1, st+1|v1:t+1) contains s2i components
(middle column). to keep the representation computationally tractable the
mixture of gaussians for each state st+1 is collapsed back to i components.
this means that each coloured set of gaussians needs to be approximated
by a smaller i component mixture of gaussians. there are many ways to
achieve this. a naive but computationally e   cient approach is to simply
ignore the lowest weight components, as depicted on the right column, see
mix2mix.m.

25.3.2 discrete    ltering

a recursion for the switch variable distribution in equation (25.3.2) is

(cid:88)

it,st

q(st+1|v1:t+1)    

q(st+1, it, st, vt+1, v1:t)

the r.h.s. of the above equation is proportional to

q(vt+1|st+1, it, st, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t)

(cid:88)

st,it

(25.3.14)

(25.3.15)

for which all terms have been computed during the recursion for q(ht+1|st+1, v1:t+1). we therefore now have
all the quantities required to compute the gaussian sum approximation of the    ltering forward pass. a
schematic representation of gaussian sum    ltering is given in    g(25.2) and the pseudo code is presented in
algorithm(25.1). see also sldsforward.m.

25.3.3 the likelihood p(v1:t )

the likelihood p(v1:t ) may be found from

t   1(cid:89)

p(v1:t ) =

t=0

where

p(vt+1|v1:t)    

p(vt+1|v1:t)

(25.3.16)

(cid:88)

it,st,st+1

q(vt+1|it, st, st+1, v1:t)q(st+1|it, st, v1:t)q(it|st, v1:t)q(st|v1:t)

in the above expression, all terms have been computed in forming the recursion for the    ltered posterior
q(ht+1, st+1|v1:t+1).
25.3.4 collapsing gaussians

a central part of the above    ltering recursion is the collapse of a mixture of gaussians to a smaller number
of gaussians. that is, given a mixture of n gaussians

p(x) =

pin (x   i,   i)

(25.3.17)

we wish to collapse this to a smaller k < n mixture of gaussians. we describe a simple method which
has the advantage of computational e   ciency, but the disadvantage that no spatial information about the

draft november 9, 2017

525

n(cid:88)

i=1

t+1t(cid:80)
algorithm 25.1 aslds forward pass. approximate the    ltered posterior p(st|v1:t)       t, p(ht|st, v1:t)    
  (s) =(cid:8)a(s), b(s),   h(s),   v(s),   h(s),   v(s)(cid:9). the routine ldsforward is found in algorithm(24.1).
it wt(it, st)n (ht ft(it, st), ft(it, st)). also return the approximate log-likelihood l     log p(v1:t ). it are
the number of components in each gaussian mixture approximation. we require i1 = 1, i2     s, it     s  it   1.

gaussian sum smoothing

for s1     1 to s do

{f1(1, s1), f1(1, s1),   p} = ldsforward(0, 0, v1;   (s1))
  1     p(s1)  p

end for
for t     2 to t do

for st     1 to s do

for i     1 to it   1, and s     1 to s do

{  x|y(i, s),   x|y(i, s),   p} = ldsforward(ft   1(i, s), ft   1(i, s), vt;   (st))
p   (st|i, s)     (cid:104)p(st|ht   1, st   1 = s)(cid:105)p(ht   1|it   1=i,st   1=s,v1:t   1)
p(cid:48)(st, i, s)     wt   1(i, s)p   (st|i, s)  t   1(s)  p
end for
(cid:80)it
collapse the it   1    s mixture of gaussians de   ned by   x|y,  x|y,
and weights
p(i, s|st)     p(cid:48)(st, i, s)
p(ht|st, v1:t)    
it=1 p(it|st, v1:t)p(ht|st, it, v1:t). this de   nes the new means ft(it, st), covariances
ft(it, st) and mixture weights wt(it, st)     p(it|st, v1:t).
compute   t(st)    
l     l + log(cid:80)

end for
normalise   t

st,i,s p(cid:48)(st, i, s)

a gaussian with it

i,s p(cid:48)(st, i, s)

components,

(cid:80)

to

end for

mixture is used[298]. first we describe how to collapse a mixture to a single gaussian. this can be achieved
by    nding the mean and covariance of the mixture distribution (25.3.17). these are

(cid:88)

(cid:88)

(cid:16)

(cid:17)

   =

pi  i,

   =

pi

  i +   i  t
i

i

i

        t

(25.3.18)

to collapse a mixture then to a k-component mixture we may    rst retain the k     1 gaussians with the
largest mixture weights. the remaining n     k + 1 gaussians are simply merged to a single gaussian using
the above method. alternative heuristics such as recursively merging the two gaussians with the lowest
mixture weights are also reasonable.

more sophisticated methods which retain some spatial information would clearly be potentially useful. the
method presented in [189] is a suitable approach which considers removing gaussians which are spatially
similar (and not just low-weight components), thereby retaining a sense of diversity over the possible solu-
tions. in applications with many thousands of timesteps, speed can be a factor in determining which method
of collapsing gaussians is to be preferred.

25.3.5 relation to other methods

gaussian sum filtering can be considered a form of    analytical particle    ltering   , section(27.6.2), in which
instead of point distributions (delta functions) being propagated, gaussians are propagated. the collapse
operation to a smaller number of gaussians is analogous to resampling in id143ing. since a gaussian
is more expressive than a delta-function, the gaussian sum    lter is generally an improved approximation
technique over using point particles. see [18] for a numerical comparison.

25.4 gaussian sum smoothing

approximating the smoothed posterior p(ht, st|v1:t ) is more involved than    ltering and requires additional
approximations. for this reason smoothing is more prone to failure since there are more assumptions that
need to be satis   ed for the approximations to hold. the route we take here is to assume that a gaussian sum

526

draft november 9, 2017

gaussian sum smoothing

(cid:90)

(cid:88)

(cid:88)
(cid:88)

st+1

st+1

(cid:88)

st+1

   ltered approximation has been carried out, and then approximate the    backward pass, analogous to that
of section(23.2.4). by analogy with the rts smoothing recursion equation (23.2.20), the exact backward
pass for the slds reads

p(ht, st|v1:t ) =

st+1

ht+1

p(ht, st|ht+1, st+1, v1:t)p(ht+1, st+1|v1:t )

(25.4.1)

where p(ht+1, st+1|v1:t ) = p(st+1|v1:t )p(ht+1|st+1, v1:t ) is composed of the discrete and continuous com-
ponents of the smoothed posterior at the next time step. the recursion runs backwards in time, beginning
with the initialisation p(ht , st|v1:t ) set by the    ltered result (at time t = t , the    ltered and smoothed
posteriors coincide). apart from the fact that the number of mixture components will increase at each step,
computing the integral over ht+1 in equation (25.4.1) is problematic since the conditional distribution term
is non-gaussian in ht+1. for this reason it is more useful to derive an approximate recursion by beginning
with the exact relation

p(ht, st|v1:t ) =

p(st+1|v1:t )p(ht|st, st+1, v1:t )p(st|st+1, v1:t )

(25.4.2)

which can be expressed more directly in terms of the slds dynamics as

p(ht, st|v1:t ) =

p(st+1|v1:t )(cid:104)p(ht|ht+1, st, st+1, v1:t,    vt+1:t )(cid:105)p(ht+1|st,st+1,v1:t )

in forming the recursion we assume access to the distribution p(ht+1, st+1|v1:t ) from the future timestep.
however, we also require the distribution p(ht+1|st, st+1, v1:t ) which is not directly known and needs to be
inferred, in itself a computationally challenging task. in the expectation correction (ec) approach [18] one
assumes the approximation (see    g(25.3))

   (cid:104)p(st|ht+1, st+1, v1:t )(cid:105)p(ht+1|st+1,v1:t )

(25.4.3)

p(ht+1|st, st+1, v1:t )     p(ht+1|st+1, v1:t )

resulting in an approximate recursion for the smoothed posterior,

(25.4.4)

p(ht, st|v1:t )    

p(st+1|v1:t )(cid:104)p(ht|ht+1, st, st+1, v1:t)(cid:105)ht+1 (cid:104)p(st|ht+1, st+1, v1:t )(cid:105)ht+1

(25.4.5)

where (cid:104)  (cid:105)ht+1 represents averaging with respect to the distribution p(ht+1|st+1, v1:t ). in carrying out the
approximate recursion, (25.4.5) we will end up with a mixture of gaussians that grows at each timestep.
to avoid the exponential explosion problem, we use a    nite mixture approximation, q(ht+1, st+1|v1:t ):

p(ht+1, st+1|v1:t )     q(ht+1, st+1|v1:t ) = q(ht+1|st+1, v1:t )q(st+1|v1:t )

(25.4.6)

and plug this into the approximate recursion above. from equation (25.4.5) a recursion for the approximation
is given by

q(ht, st|v1:t ) =

q(st+1|v1:t )(cid:104)q(ht|ht+1, st, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:t )

(cid:104)q(st|ht+1, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:t )

(cid:123)(cid:122)

q(ht|st,st+1,v1:t )

(cid:125)

(cid:124)

(cid:123)(cid:122)

q(st|st+1,v1:t )

(cid:125)

(cid:88)

st+1

(cid:124)

as for    ltering, wherever possible, we replace approximate terms by their exact counterparts and parame-
terise the posterior using

q(ht+1, st+1|v1:t ) = q(ht+1|st+1, v1:t )q(st+1|v1:t )

(25.4.8)

to reduce the notational burden here we outline the method only for the case of using a single component
approximation in both the forward and backward passes. the extension to using a mixture to approximate
each p(ht+1|st+1, v1:t ) is conceptually straightforward and deferred to section(25.4.4). in the single gaussian
case we assume we have a gaussian approximation available for

(25.4.7)

q(ht+1|st+1, v1:t ) = n (ht+1 g(st+1), g(st+1))

draft november 9, 2017

(25.4.9)

527

(cid:90)

gaussian sum smoothing

figure 25.3:
the ec backpass approximates
p(ht+1|st+1, st, v1:t ) by p(ht+1|st+1, v1:t ). the moti-
vation for this is that st in   uences ht+1 only indirectly
through ht. however, ht will most likely be heavily in-
   uenced by v1:t, so that not knowing the state of st
is likely to be of secondary importance. the green
shaded node is the variable we wish to    nd the pos-
terior for. the values of the blue shaded nodes are
known, and the red shaded node indicates a known
variable which is assumed unknown in forming the ap-
proximation.

st   1

ht   1

vt   1

st

ht

vt

st+1

st+2

ht+1

ht+2

vt+1

vt+2

25.4.1 continuous smoothing

for given st, st+1, an rts style recursion for the smoothed continuous distribution is obtained from equation
(25.4.7), giving

q(ht|st, st+1, v1:t ) =

p(ht|ht+1, st, st+1, v1:t)q(ht+1|st+1, v1:t )

(25.4.10)

ht+1

to compute equation (25.4.10) we then perform a single update of the lds backward recursion, sec-
tion(24.4.2).

25.4.2 discrete smoothing

the second average in equation (25.4.7) corresponds to a recursion for the discrete variable and is given by

(cid:104)q(st|ht+1, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:t )     q(st|st+1, v1:t ).

(25.4.11)
the average of q(st|ht+1, st+1, v1:t) with respect to q(ht+1|st+1, v1:t ) cannot be achieved in closed form. a
simple approach is to approximate the average by evaluation at the mean1

(cid:104)q(st|ht+1, st+1v1:t)(cid:105)q(ht+1|st+1,v1:t )     q(st|ht+1, st+1, v1:t)

where (cid:104)ht+1|st+1, v1:t(cid:105) is the mean of ht+1 with respect to q(ht+1|st+1, v1:t ).
replacing ht+1 by its mean gives the approximation

(cid:104)q(st|ht+1, st+1, v1:t)(cid:105)q(ht+1|st+1,v1:t )    

e

1
z

where

(cid:112)

    1

2 zt

t+1(st,st+1)     1(st,st+1|v1:t)zt+1(st,st+1)

det (  (st, st+1|v1:t))

(25.4.12)

q(st|st+1, v1:t) (25.4.13)

(cid:12)(cid:12)ht+1=(cid:104)ht+1|st+1,v1:t (cid:105)

zt+1(st, st+1)     (cid:104)ht+1|st+1, v1:t(cid:105)     (cid:104)ht+1|st, st+1, v1:t(cid:105)

(25.4.14)
and z ensures normalisation over st.   (st, st+1|v1:t) is the    ltered covariance of ht+1 given st, st+1 and the
observations v1:t, which may be taken from   hh in equation (25.3.8). approximations which take covariance
information into account can also be considered, although the above simple (and fast) method may su   ce
in practice [18, 208].

25.4.3 collapsing the mixture

from section(25.4.1) and section(25.4.2) we now have all the terms in equation (25.4.8) to compute the
approximation to equation (25.4.7). due to the summation over st+1 in equation (25.4.7), the number of
mixture components is multiplied by s at each iteration. to prevent an exponential explosion of components,
the mixture equation (25.4.7) is then collapsed to a single gaussian

q(ht, st|v1:t )     q(ht|st, v1:t )q(st|v1:t )

the collapse to a mixture is discussed in section(25.4.4).

1in general this approximation has the form (cid:104)f (x)(cid:105)     f ((cid:104)x(cid:105)).

(25.4.15)

528

draft november 9, 2017

gaussian sum smoothing

(cid:80)jt

algorithm 25.2 aslds: ec backward pass.

routine needs the results from algorithm(25.1). the routine ldsbackward is found in algorithm(24.2).

approximates p(st|v1:t ) and p(ht|st, v1:t )    
jt=1 ut(jt, st)n (gt(jt, st), gt(jt, st)) using a mixture of gaussians. jt = it , jt     s    it    jt+1. this
gt     ft , gt     ft , ut     wt
for t     t     1 to 1 do
for s     1 to s, s(cid:48)

(  ,   )(i, s, j(cid:48), s(cid:48)) = ldsbackward(gt+1(j(cid:48), s(cid:48)), gt+1(j(cid:48), s(cid:48)), ft(i, s), ft(i, s),   (s(cid:48)))
p(it, st|jt+1, st+1, v1:t ) = (cid:104)p(st = s, it = i|ht+1, st+1 = s(cid:48), jt+1 = j(cid:48), v1:t)(cid:105)p(ht+1|st+1=s(cid:48),jt+1=j(cid:48),v1:t )
p(i, s, j(cid:48), s(cid:48)

    1 to s, i     1 to it, j(cid:48)

    1 to jt+1 do

|v1:t )ut+1(j(cid:48), s(cid:48))p(it, st|jt+1, st+1, v1:t )

|v1:t )     p(st+1 = s(cid:48)

end for
for st     1 to s do

collapse the mixture de   ned by weights p(it = i, st+1 = s(cid:48), jt+1 = j(cid:48)
|st, v1t )    
p(i, st, j(cid:48), s(cid:48)
|v1:t ), means   (it, st, jt+1, st+1) and covariances   (it, st, jt+1, st+1) to a mix-
ture with jt components. this de   nes the new means gt(jt, st), covariances gt(jt, st) and
mixture weights ut(jt, st).
p(st|v1:t )    

(cid:80)
it,j(cid:48),s(cid:48) p(it, st, j(cid:48), s(cid:48)

|v1:t )

end for

end for

25.4.4 using mixtures in smoothing

the extension to the mixture case is straightforward based on the representation

j(cid:88)

jt=1

(cid:88)

(cid:88)

(cid:88)

jt

p(ht|st, v1:t )    

q(jt|st, v1:t )q(ht|st, jt, v1:t ).

(25.4.16)

analogously to the case with a single component,

q(ht, st|v1:t ) =

it,jt+1,st+1

p(st+1|v1:t )p(jt+1|st+1, v1:t )q(ht|jt+1, st+1, it, st, v1:t )

   (cid:104)q(it, st|ht+1, jt+1, st+1, v1:t)(cid:105)q(ht+1|jt+1,st+1,v1:t )

(25.4.17)

the average in the last line of the above equation can be tackled using the same techniques as outlined in
the single gaussian case. to approximate q(ht|jt+1, st+1, it, st, v1:t ) we consider this as the marginal of the
joint distribution

q(ht, ht+1|it, st, jt+1, st+1, v1:t ) = q(ht|ht+1, it, st, jt+1, st+1, v1:t)q(ht+1|it, st, jt+1, st+1, v1:t ) (25.4.18)
as in the case of a single mixture, the problematic term is q(ht+1|it, st, jt+1, st+1, v1:t ). analogously to
equation (25.4.4), we make the assumption

q(ht+1|it, st, jt+1, st+1, v1:t )     q(ht+1|jt+1, st+1, v1:t )

meaning that information about the current switch state st, it is ignored. we can then form

p(ht|st, v1:t ) =

p(it, jt+1, st+1|st, v1:t )p(ht|it, st, jt+1, st+1, v1:t )

it,jt+1,st+1

this mixture can then be collapsed to a smaller mixture using any method of choice, to give

p(ht|st, v1:t )    

q(jt|st, v1:t )q(ht|jt, st, v1:t )

(25.4.19)

(25.4.20)

(25.4.21)

the resulting procedure is sketched in algorithm(25.2), including using mixtures in both the forward and
backward passes.

draft november 9, 2017

529

25.4.5 relation to other methods

gaussian sum smoothing

a classical smoothing approximation for the slds is generalised pseudo bayes
in gpb one starts from the exact recursion

(gpb) [13, 174, 173].

p(st|v1:t ) =

p(st, st+1|v1:t ) =

p(st|st+1, v1:t )p(st+1|v1:t )

the quantity p(st|st+1, v1:t ) is di   cult to obtain and gpb makes the approximation

(cid:88)

st+1

(cid:88)

st+1

(cid:88)
(cid:88)

st+1

st+1

p(st|st+1, v1:t )     p(st|st+1, v1:t)

plugging this into equation (25.4.22) we have

p(st|v1:t )    

=

(cid:80)

p(st|st+1, v1:t)p(st+1|v1:t )

p(st+1|st)p(st|v1:t)
st p(st+1|st)p(st|v1:t)

p(st+1|v1:t )

(25.4.22)

(25.4.23)

(25.4.24)

(25.4.25)

the recursion is initialised with the approximate    ltered p(st|v1:t ). computing the smoothed recursion
for the switch states in gpb is then equivalent to running the rts backward pass on a hidden markov
model, independently of the backward recursion for the continuous variables. the only information the
gpb method uses to form the smoothed distribution p(st|v1:t ) from the    ltered distribution p(st|v1:t)
is the markov switch transition p(st+1|st). this approximation drops information from the future since
information passed via the continuous variables is not taken into account.
in contrast to gpb, the ec
gaussian smoothing technique preserves future information passing through the continuous variables. gpb
forms an approximation for p(ht|st, v1:t ) by using the recursion (25.4.8) where q(st|st+1, v1:t ) is replaced
by q(st|st+1, v1:t). in sldsbackward.m one may choose to use either ec or gbp.

example 25.1 (tra   c flow). a illustration of modelling and id136 with a slds is to consider a simple
network of tra   c    ow,    g(25.4). here there are 4 junctions a, b, c, d and tra   c    ows along the roads in the
direction indicated. tra   c    ows into the junction at a and then goes via di   erent routes to d. flow out of
a junction must match the    ow into a junction (up to noise). there are tra   c light switches at junctions a
and b which, depending on their state, route tra   c di   erently along the roads. using    to denote the clean
(noise free)    ow, we model the    ows using the switching linear system:

                                              =

                                             

  a(t)
  a   d(t)
  a   b(t)
  b   d(t)
  b   c(t)
  c   d(t)

ht = a(st)ht   1 +   h
t

  a(t     1)
  a(t     1) (0.75    i [sa(t) = 1] + 1    i [sa(t) = 2])
  a(t     1) (0.25    i [sa(t) = 1] + 1    i [sa(t) = 3])
  a   b(t     1)0.5    i [sb(t) = 1]
  a   b(t     1) (0.5    i [sb(t) = 1] + 1    i [sb(t) = 2])
  b   c(t     1)

(25.4.26)

by identifying the    ows at time t with a 6 dimensional vector hidden variable ht, we can write the above
   ow equations as

(cid:78) sb, which takes 3   2 = 6

(25.4.27)

for a set of suitably de   ned matrices a(s) indexed by the switch variable s = sa
states. we additionally include noise terms to model cars parking or de-parking during a single timestep.
the covariance   h is diagonal with a larger variance at the in   ow point a to model that the total volume
of tra   c entering the system can vary.

noisy measurements of the    ow into the network are taken at a

v1,t =   a(t) +   v

1(t)

530

(25.4.28)

draft november 9, 2017

gaussian sum smoothing

a

d

b

c

figure 25.4: a representation of the tra   c    ow between junctions at a,b,c,d, with
tra   c lights at a and b. if sa = 1 a     d and a     b carry 0.75 and 0.25 of the    ow out
of a respectively. if sa = 2 all the    ow from a goes through a     d; for sa = 3, all the
   ow goes through a     b. for sb = 1 the    ow out of b is split equally between b     d
and b     c. for sb = 2 all    ow out of b goes along b     c.

figure 25.5: time evolution of the tra   c    ow measured at
two points in the network. sensors measure the total    ow
into the network (upper panel)   a(t)and the total    ow out
of the network (lower panel),   d(t) =   a   d(t) +   b   d(t) +
  c   d(t). the total in   ow at a undergoes a random walk.
note that the    ow measured at d can momentarily drop
to zero if all tra   c is routed through a     b     c in two
consecutive time steps.

along with a noisy measurement of the total    ow out of the system at d,

v2,t =   a   d(t) +   b   d(t) +   c   d(t) +   v

2(t)

(25.4.29)

the observation model can be represented by vt = bht +   v
t using a constant 2    6 projection matrix b.
the switch variables follow a simple markov transition p(st|st   1) which biases the switches to remain in
the same state in preference to jumping to another state. see demosldstraffic.m for details.

given the above system and a prior which initialises all    ow at a, we draw samples from the model using
forward (ancestral) sampling which form the observations v1:100,    g(25.5). using only the observations
and the known model structure we then attempt to infer the latent switch variables and tra   c    ows using
gaussian sum    ltering and smoothing (ec method) with 2 mixture components per switch state,    g(25.6).

we note that a naive id48 approximation based on discretising each continuous    ow into 20 bins would
contain 2    3    206 or 384 million states. even for modest size problems, a naive approximation based on
discretisation is therefore impractical.

example 25.2 (following the price trend). the following is a simple model of the price trend of a stock,
which assumes that the price tends to continue going up (or down) for a while before it reverses direction:

h1,t = h1,t   1 + h2,t   1 +   h
h2(t) = i [st = 1] h2,t   1 +   h

1 (st)
2 (st)

vt = h1,t +   v(st)

(25.4.30)

(25.4.31)

(25.4.32)

here h1 represents the    clean    price and h2 the direction. there is only a single observation variable at each
time, which is the clean price plus a small amount of noise. there are two switch states, dom(st) = {1, 2}.
when st = 1, the model functions normally, with the direction being equal to the previous direction plus
a small amount of noise   h
2 (st = 1). when st = 2 however, the direction is sampled from a gaussian with
a large variance. the transition p(st|st   1) is set so that normal dynamics is more likely, and when st = 2
it is likely to go back to normal dynamics the next timestep. full details are in sldspricemodel.mat.
in    g(25.7) we plot some samples from the model and also smoothed id136 of the switch distribution,
showing how we can analyse the series to infer the points of likely change in the stock price direction. see
also exercise(25.1).

draft november 9, 2017

531

0204060801000204002040608010002040reset models

(a)

(b)

(c)

figure 25.6: given the observations from    g(25.5) we infer the    ows and switch states of all the latent
variables. (a): the correct latent    ows through time along with the switch variable state used to generate
the data. the colours corresponds to the    ows at the corresponding coloured edges/nodes in    g(25.4). (b):
filtered    ows based on a i = 2 gaussian sum forward pass approximation. plotted are the 6 components
of the vector (cid:104)ht|v1:t(cid:105) with the posterior distribution of the sa and sb tra   c light states p(sa
t|v1:t)
plotted below. (c): smoothed    ows (cid:104)ht|v1:t(cid:105) and corresponding smoothed switch states p(st|v1:t ) using a
gaussian sum smoothing approximation (ec with j = 1).

t |v1:t),p(sb

figure 25.7: the top panel is a time series of    prices   . the prices
tend to keep going up or down with infrequent changes in the
direction. based on    tting a simple slds model to capture this
kind of behaviour, the id203 of a signi   cant change in the
price direction is given in the panel below based on the smoothed
distribution p(st = 2|v1:t ).

25.5 reset models

reset models are special switching models in which the switch state isolates the present from the past,
resetting the position of the latent dynamics (these are also known as changepoint models). whilst these
models are rather general, it can be helpful to consider a speci   c model, and here we consider the slds
changepoint model with two states. we use the state st = 0 to denote that the lds continues with the
standard dynamics. with st = 1, however, the continuous dynamics is reset to a prior:

st = 1

p1(ht)

(cid:26) p0(ht|ht   1) st = 0
(cid:0)ht aht   1 +   0,   0(cid:1) ,
(cid:26) p0(vt|ht) st = 0

p1(vt|ht) st = 1

p(ht|ht   1, st) =

where

p0(ht|ht   1) = n

similarly we write

p(vt|ht, st) =

(cid:0)ht   1,   1(cid:1)

p1(ht) = n

(25.5.1)

(25.5.2)

(25.5.3)

for simplicity we assume the switch dynamics are    rst order markov with transition p(st|st   1), see    g(25.8).
under this model the dynamics follows a standard lds, but when st = 1, ht is reset to a value drawn from

532

draft november 9, 2017

020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020204060801001232040608010012020406080100120140160180200   40   30   20   1001002040608010012014016018020000.20.40.60.81(25.5.4)

(25.5.5)

(25.5.6)

reset models

s1

s2

s3

s4

h1

v1

h2

v2

h3

v3

h4

v4

figure 25.8: the independence structure of a reset model.
square nodes st denote the binary reset variables and ht
the continuous state. the ht are continuous variables, and
vt continuous observations. if the dynamics resets, st = 1,
the dependence of the continuous ht on the past is cut.

a gaussian distribution, independent of the past. such models are of interest in prediction where the time-
series is following a trend but suddenly changes and the past is forgotten. whilst this may not seem like a
big change to the model, this model is computationally more tractable, exact    ltered id136 scaling with

o(cid:0)t 2(cid:1), compared to o(cid:0)t 2t(cid:1) in the general two-state slds. to see this, consider the    ltering recursion

(cid:90)

(cid:88)

  (ht, st)    

ht   1

st   1

(cid:90)

p(vt|ht, st)p(ht|ht   1, st)p(st|st   1)  (ht   1, st   1)
(cid:88)

p0(vt|ht)p0(ht|ht   1)p(st = 0|st   1)  (ht   1, st   1)

we now consider the two cases

  (ht, st = 0)    

ht   1

st   1

  (ht, st = 1)     p1(vt|ht)p1(ht)
    p1(vt|ht)p1(ht)
(cid:90)

(cid:88)

ht   1

st   1

(cid:90)
(cid:88)

st   1

(cid:90)

p(st = 1|st   1)  (ht   1, st   1)

p(st = 1|st   1)  (st   1)

equation(25.5.6) shows that p(ht, st = 1|v1:t) is not a mixture model in ht, but contains only a single
component proportional to p1(vt|ht)p1(ht). if we use this information in equation (25.5.5) we have

  (ht, st = 0)    

ht   1

p0(vt|ht)p0(ht|ht   1)p(st = 0|st   1 = 0)  (ht   1, st   1 = 0)

+

ht   1

p0(vt|ht)p0(ht|ht   1)p(st = 0|st   1 = 1)  (ht   1, st   1 = 1)

(25.5.7)

assuming   (ht   1, st   1 = 0) is a mixture distribution with k components, then   (ht, st = 0) will be a mixture
with k + 1 components. in general, therefore,   (ht, st = 0) will contain t components and   (ht, st = 1)
a single component. as opposed to the full slds case, the number of components therefore grows only
linearly with time, as opposed to exponentially. this means that the computational e   ort to perform exact

   ltering scales as o(cid:0)t 2(cid:1). smoothing can be achieved using the           approach, see exercise(25.3). despite

this reduction in complexity over the slds, for long timeseries t (cid:29) 1,    ltering and smoothing for these
reset models can still be computationally expensive. see [52] for approximations based on retaining only a
limited number of mixture components, reducing the complexity to linear in t .

run-length formalism

one may also describe reset models using a    run-length    formalism de   ning at each time t a latent variable
rt which describes the length of the current segment[3]. if there is a change, the run-length variable is reset
to zero, otherwise it is increased by 1:

p(rt|rt   1) =

1     pcp

rt = 0
rt = rt   1 + 1

where pcp is the id203 of a reset (or    changepoint   ). the joint distribution is given by

p(v1:t , r1:t ) =

p(rt|rt   1)p(vt|v1:t   1, rt),

p(vt|v1:t   1, rt) = p(vt|vt   rt:t   1)

draft november 9, 2017

(25.5.8)

(25.5.9)

533

(cid:26) pcp
(cid:89)

t

reset models

(25.5.10)

with the understanding that if rt = 0 then p(vt|vt   rt:t   1) = p(vt). the graphical model of this distribution
is awkward to draw since the number of links depends on the run-length rt. predictions can be made using

p(vt+1|v1:t) =

p(rt, v1:t) =

=

rt

(cid:88)
(cid:88)
(cid:88)
(cid:88)

rt   1

rt   1

p(vt+1|vt   rt:t)p(rt|v1:t)
(cid:88)

p(rt, rt   1, v1:t   1, vt) =

where the    ltered    run-length    p(rt|v1:t) is given by the forward recursion:

p(rt, vt|rt   1, v1:t   1)p(rt   1, v1:t   1)

rt   1

p(vt|rt,   rt   1, v1:t   1)p(rt|rt   1,   v1:t   1)p(rt   1, v1:t   1)
p(rt|rt   1)p(vt|vt   rt:t   1)p(rt   1, v1:t   1)

=

which shows that    ltered id136 scales with o(cid:0)t 2(cid:1).

rt   1

25.5.1 a poisson reset model

the changepoint structure is not limited to conditionally gaussian cases only. to illustrate this, we consider
the following model2: at each time t, we observe a count yt which we assume is poisson distributed with an
unknown positive intensity h. the intensity is constant, but at certain unknown times t, it jumps to a new
value. the indicator variable ct denotes whether time t is such a changepoint or not. mathematically, the
model is:

p(h0) = g(h0; a0, b0)
p(ct) = be(ct;   )

p(vt|ht) = po(vt; ht)

p(ht|ht   1, ct) = i [ct = 0]   (ht, ht   1) + i [ct = 1]g(ht;   , b)

(25.5.11)

(25.5.12)

(25.5.13)

(25.5.14)

the symbols g, be and po denote the gamma, bernoulli and the poisson distributions respectively:

g(h; a, b) = exp ((a     1) log h     bh     log   (a) + a log b)
be(c;   ) = exp (c log    + (1     c) log(1       ))
po(v; h) = exp (v log h     h     log   (v + 1))

(25.5.15)

(25.5.16)

(25.5.17)

given observed counts v1:t , the task is to    nd the posterior id203 of a change and the associated
intensity levels for each region between two consecutive changepoints. plugging the above de   nitions in the
generic updates equation (25.5.5) and equation (25.5.6), we see that   (ht, ct = 0) is a gamma potential,
and that   (gt, ct = 1) is a mixture of gamma potentials, where a gamma potential is de   ned as

  (h) = elg(h; a, b)

(25.5.18)

via the triple (a, b, l). for the corrector update step we need to calculate the product of a poisson term with
the observation model p(vt|ht) = po(vt; ht). a useful property of the poisson distribution is that, given the
observation, the latent variable is gamma distributed:

po(v; h) = v log h     h     log   (v + 1)

= (v + 1     1) log h     h     log   (v + 1)
= g(h; v + 1, 1)

(25.5.19)

(25.5.20)

(25.5.21)

hence, the update equation requires multiplication of two gamma potentials. a nice property of the gamma
density is that the product of two gamma densities is also a gamma potential:

(a1, b1, l1)    (a2, b2, l2) = (a1 + a2     1, b1 + b2, l1 + l2 + g(a1, b1, a2, b2))

(25.5.22)

2this example is due to taylan cemgil.

534

draft november 9, 2017

reset models

where

g(a1, b1, a2, b2)     log

  (a1 + a2     1)
  (a1)  (a2)

+ log(b1 + b2) + a1 log(b1/(b1 + b2)) + a2 log(b2/(b1 + b2)) (25.5.23)

the    recursions for this reset model are therefore closed in the space of a mixture of gamma potentials,
with an additional gamma potential in the mixture at each timestep. a similar approach can be used to
form the smoothing recursions, see exercise(25.3).

example 25.3 (coal mining disasters). we illustrate the poisson reset model on the coal mining disaster
dataset [158]. the data set consists of the number of deadly coal-mining disasters in england per year over
a time span of 112 years from 1851 to 1962. it is widely agreed in the statistical literature that a change in
the intensity (the expected value of the number of disasters) occurs around the year 1890, after new health
and safety regulations were introduced. in    g(25.9) we show the marginals p(ht|y1:t ) along with the    ltering
density. note that we are not constraining the number of changepoints and in principle allow any number.
the smoothed density suggests a sharp decrease around t = 1890.

estimation of change points.

figure 25.9:
(top)
coal mining disaster dataset. (middle) filtered esti-
mate of the marginal intensity p(ht|v1:t) and (bottom)
smoothed estimate p(ht|v1:t ).
here, darker color
means higher id203.

25.5.2 reset-id48-lds

the reset model de   ned by equations (25.5.1,25.5.3) above is useful in many applications, but is limited
since only a single dynamical model is considered. an extension is to consider a set of available dynamical
models, indexed by st     {1, . . . , s}, with a reset that cuts dependency of the continuous variable on the
past[101, 60]:

p(ht|ht   1, st, ct) =

ct = 0
ct = 1

(25.5.24)

the states st follow a markovian dynamics p(st|st   1, ct   1), see    g(25.10). a reset occurs if the state st
changes, otherwise, no reset occurs:

the computational complexity of    ltering for this model is o(cid:0)s2t 2(cid:1) which can be understood by analogy

p(ct = 1|st, st   1) = i [st (cid:54)= st   1]

(25.5.25)

with the reset    recursions, equations(25.5.5,25.5.6) on replacing ht by (ht, st). to see this we consider the
   ltering recursion for the two cases

(cid:26) p0(ht|ht   1, st)

p1(ht|st)

  (ht, st, ct = 0) =

  (ht, st, ct = 1) =

(cid:90)

(cid:88)

ht   1

(cid:90)

st   1,ct   1

(cid:88)

ht   1

st   1,ct   1

p0(vt|ht, st)p0(ht|ht   1, st)p(st|st   1, ct   1)p(ct = 0|st, st   1)  (ht   1, ct   1)
(25.5.26)

p1(vt|ht, st)p1(ht|st)p(st|st   1, ct)p(ct = 1|st, st   1)  (ht   1, st   1, ct   1)

= p1(vt|ht, st)p1(ht|st)

p(ct = 1|st, st   1)p(st|st   1, ct   1)  (st   1, ct   1)

(25.5.27)

(cid:88)

st   1,ct   1

draft november 9, 2017

535

186018701880189019001910192019301940195019600246# of accidentsfiltered intensity18601870188018901900191019201930194019501960246smoothed intensityyear18601870188018901900191019201930194019501960246c1

s1

h1

v1

c2

s2

h2

v2

c3

s3

h3

v3

c4

s4

h4

v4

exercises

figure 25.10: the independence structure of a reset-id48-
lds model. square nodes ct     {0, 1} denote reset vari-
ables; ht are continuous latent variables, and vt continuous
observations. the discrete state st     {1, . . . , s} determines
which linear dynamical system from a    nite set of linear
dynamical systems is operational at time t.

from equation (25.5.27) we see that   (ht, st, ct = 1) contains only a single component proportional to
p1(vt|ht, st)p1(ht|st). this is therefore exactly analogous to the standard reset model, except that we
need now to index a set of messages with st, therefore each message taking o (s) steps to compute. the

computational e   ort to perform exact    ltering scales as o(cid:0)s2t 2(cid:1).

25.6 summary

    the switching linear dynamical system is a marriage of a discrete state id48 with the continuous latent
it is able to model discrete jumps in an underlying continuous process,

state linear dynamical system.
   nding application in a large variety of domains from    nance to speech processing.

    the classical

requires an exponential amount of space.

id136 problems in the slds are formally intractable since representing the messages

    many approximation methods have been developed for the slds and in this chapter we described a robust

deterministic method based on a mixture of gaussians representation.

    reset models are such that the continuous variable forgets the past when reset. unlike the slds such
models are much more amenable to exact id136. extensions include the reset-id48 which allows for a
set of discrete and continuous states in which a special discrete state resets the continuous states.

25.7 code

sldsforward.m: slds forward
sldsbackward.m: slds backward (expectation correction)
mix2mix.m: collapse a mixture of gaussians to a smaller mixture of gaussians
sldsmarggauss.m: marginalise an slds gaussian mixture
logeps.m: logarithm with o   set to deal with log(0)
demosldstraffic.m: demo of tra   c    ow using a switching linear dynamical system

25.8 exercises

exercise 25.1. consider the setup described in example(25.2), for which the full slds model is given in
sldspricemodel.mat, following the notation used in demosldstraffic.m. given the data in the vector v
your task is to    t a prediction model to the data. to do so, approximate the    ltered distribution p(ht, st|v1:t)
using a mixture of i = 2 components. the prediction of the mean price at the next day is then

vpred
t+1 = (cid:104)h1,t + h2,t(cid:105)p(ht|v1:t)

536

(25.8.1)

draft november 9, 2017

exercises

figure 25.11: data from an intermittent mean-reverting process. see exercise(25.2).

where p(ht|v1:t) =(cid:80)

st p(ht, st|v1:t).

1. compute the mean prediction error

mean_abs_pred_error=mean(abs(vpred(2:200)-v(2:200)))

2. compute the mean naive prediction error

mean_abs_pred_error_naive=mean(abs(v(1:199)-v(2:200)))
which corresponds to saying that tomorrow   s price will be the same as today   s.

hint: you might    nd sldsmarggauss.m of interest.

exercise 25.2. the data in    g(25.11) are observed prices from an intermittent mean-reverting process,
contained in meanrev.mat. there are two states s = 2. there is a true (latent) price pt and an observed
price vt (which is plotted). when s = 1, the true underlying price reverts back towards the mean m = 10
with rate r = 0.9. otherwise the true price follows a random walk:

(cid:26) r(pt   1     m) + m +   p
(cid:26)

pt   1 +   p
t

t

n (  p
n (  p

t 0, 0.0001) st = 1
t 0, 0.01)
st = 2

st = 1
st = 2

where

pt =

  p
t    

the observed price vt is related to the unknown price pt by

vt     n (vt pt, 0.001)

(25.8.2)

(25.8.3)

(25.8.4)

it is known that 95% of the time st+1 is in the same state as at time t > 1 and that at time t = 1 either
state of s1 is equally likely. also at t = 1, p1     n (p1 m, 0.1). based on this information, and using
gaussian sum    ltering with i = 2 components (use sldsforward.m), what is the id203 at time t = 280
that the dynamics is following a random walk, p(s280 = 2|v1:280)? repeat this computation for smoothing
p(s280 = 2|v1:400) based on using expectation correction with i = j = 2 components.
exercise 25.3. we here derive a smoothing recursion based on the           approach for the reset-lds
described in section(25.5). smoothing can be achieved using

p(ht, st|v1:t )     p(ht, st|v1:t)

p(vt+1:t|ht, st)

  (ht,st)

  (ht,st)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

from the formal    recursion, we have

(cid:123)(cid:122)

(cid:90)

(cid:124)
(cid:88)
(cid:90)

st

  (ht   1, st   1) =

p(vt|ht, st)p(ht|ht   1, st)p(st|st   1)  (ht, st)

ht

1. show that the rhs of equation (25.8.6) can be written as

(cid:124)
p(st = 0|st   1)

(cid:125)
p0(vt|ht)p0(ht|ht   1)  (ht, st = 0)

(cid:123)(cid:122)

  0(ht   1,st   1)

(cid:124)
+ p(st = 1|st   1)

ht

draft november 9, 2017

(25.8.5)

(25.8.6)

(cid:90)

ht

(cid:123)(cid:122)
(cid:125)
p1(vt|ht)p1(ht)  (ht, st = 1)

  1(st   1)

(25.8.7)

537

0501001502002503003504009.51010.5112. writing

  (ht, st) =   0(ht, st) +   1(st)

derive then the recursions

and

  0(ht   1, st   1) = p(st = 0|st   1)
(cid:90)

  1(st   1) = p(st = 1|st   1)

ht

(cid:90)
p0(vt|ht)p0(ht|ht   1)(cid:2)  0(ht, st = 0) +   1(st = 0)(cid:3)
p1(vt|ht)p1(ht)(cid:2)  0(ht, st = 1) +   1(st = 1)(cid:3)

ht

exercises

(25.8.8)

(25.8.9)

(25.8.10)

the   1 contribution is simply a scalar, so that its complexity of representation is    xed. according to the
above   0 recursion, we include an additional component in the representation of   0 with each timestep that
we go backwards in time. thus the number of components to represent   0(ht, st) will be o (t     t), since
at time t , we may de   ne   (ht , st) = 1. this means that the term p(ht, st, v1:t ) =   (ht, st)  (ht, st) will

contain o (t(t     t)) components. to form a complete smoothing pass over all time therefore takes o(cid:0)t 3(cid:1)

time.

538

draft november 9, 2017

chapter 26

distributed computation

in this chapter we discuss from a probabilistic perspective models that are loosely based on a crude under-
standing of neural systems in biology. this is a fascinating area and we show how patterns can be stored
and recalled by reference to our standard tools from learning probabilistic models. we also discuss a general
model that enables us to consider non-linear latent continuous dynamics whilst retaining computational
tractability.

26.1 introduction

how natural organisms process information is a fascinating subject and one of the grand challenges of science.
whilst this subject is still in its early stages, loosely speaking, there are some generic properties that most
such systems are believed to possess: patterns are stored in a set of neurons; recall of patterns is robust to
noise; neural activity is essentially binary; information processing is distributed and highly modular. in this
chapter we discuss some of the classical toy models that have been developed as a test bed for analysing
such properties[66, 81, 69, 145].

26.2 stochastic hop   eld networks

hop   eld networks are models of biological memory in which a pattern is represented by the activity of a
set of v interconnected neurons. the term    network    here refers to the set of neurons, see    g(26.1), and not
the belief network representation of distribution of neural states unrolled through time,    g(26.2). at time
t neuron i    res vi(t) = +1 or is quiescent vi(t) =    1 (not    ring) depending on the states of the neurons at
the preceding time t     1. explicitly, neuron i    res depending on the potential

wijvj(t)

(26.2.1)

v(cid:88)

j=1

ai(t)     bi +

where wij characterizes the e   cacy with which neuron j transmits a binary signal to neuron i. the bias
bi relates to the neuron   s predisposition to    ring. writing the state of the network at time t as v(t)    
(v1(t), . . . , vv (t))t, the id203 that neuron i    res at time t + 1 is modelled as

p(vi(t + 1) = 1|v(t)) =      (ai(t))

(26.2.2)
where     (x) = 1/(1 + e     x) and    controls the level of stochastic behaviour of the neuron. the id203
of being in the quiescent state is given by id172

p(vi(t + 1) =    1|v(t)) = 1     p(vi(t + 1) = 1|v(t)) = 1          (ai(t))

(26.2.3)

539

learning sequences

v4

v4

v1

v3

v2

figure 26.1: a depiction of a hop   eld network (for 5 neurons).
the connectivity of the neurons is described by a weight matrix
with elements wij. the graph represents a snapshot of the state
of all neurons at time t which simultaneously update as functions
of the network at the previous time t     1.

these two rules can be compactly written as

p(vi(t + 1)|v(t)) =      (vi(t + 1)ai(t))

(26.2.4)

which follows directly from 1         (x) =     (   x). in the limit           , the neuron updates deterministically
(26.2.5)

vi(t + 1) = sgn (ai(t))

in a synchronous hop   eld network all neurons update independently and simultaneously, and we can rep-
resent the temporal evolution of the neurons as a dynamic belief network,    g(26.2)

p(v(t + 1)|v(t)) =

p(vi(t + 1)|v(t)).

(26.2.6)

given this description of how neurons update, we wish to use the network to do interesting things, for
example to store pattern sequences and recall them under some cue. the patterns will be stored in the
weights and biases and in the following section we address how to learn suitable settings based on simple
local learning rules.

26.3 learning sequences

26.3.1 a single sequence

given a sequence of network states, v = {v(1), . . . , v(t )}, we would like the network to store this sequence
such that it can be later recalled under some cue. that is, if the network is initialized in the correct starting
state of the training sequence v(t = 1), the remainder of the training sequence for t > 1 should be reproduced
under the deterministic dynamics equation (26.2.5), without error. two classical approaches to learning a
temporal sequence are the hebb and pseudo inverse rules[145]. in both the standard hebb and pi cases,
the biases bi are usually set to zero.

standard hebb rule

the standard hebb rule sets the weights according to1

v(cid:89)

i=1

vi(t + 1)vj(t)

(26.3.1)

1donald hebb, a neurobiologist actually stated[140]

let us assume that the persistence or repetition of a reverberatory activity (or    trace   ) tends to induce lasting
cellular changes that add to its stability. . . when an axon of cell a is near enough to excite a cell b and repeatedly
or persistently takes part in    ring it, some growth process or metabolic change takes place in one or both cells
such that a   s e   ciency, as one of the cells    ring b, is increased.

this statement is sometimes misinterpreted to mean that weights are exclusively of the correlation form equation (26.3.1)(see
[287] for a discussion). this can severely limit the performance and introduce adverse storage artifacts including local
minima[145].

540

draft november 9, 2017

t   1(cid:88)

t=1

wij =

1
v

learning sequences

v4(t)

v3(t)

v2(t)

v1(t)

v4(t + 1)

v3(t + 1)

v2(t + 1)

v1(t + 1)

figure 26.2: a dynamic belief network representation of a hop-
   eld network. the network operates by simultaneously gener-
ating a new set of neuron states from the previous set. equa-
tion(26.2.6) de   nes a markov transition matrix, modelling the
transition id203 v(t)     v(t + 1) and furthermore imposes
the constraint that the neurons are conditionally independent
given the previous state of the network.

the hebb rule can be motivated mathematically by considering

(cid:88)

j

t   1(cid:88)

   =1

vi(t + 1)

wijvj(t) =

=

1
v

1
v

vi(   + 1)

vj(   )vj(t)

v2
j (t) +

1
v

vi(   + 1)

(cid:88)

j

vj(   )vj(t)

(cid:88)

j

(cid:88)
t   1(cid:88)

j

1
v

  (cid:54)=t

t   1(cid:88)
(cid:88)

  (cid:54)=t

j

= vi(t + 1) +

vi(   + 1)

vj(   )vj(t)

if the patterns are uncorrelated then the    interference    term

t   1(cid:88)

  (cid:54)=t

       

1
v

(cid:88)

j

vi(   + 1)

vj(   )vj(t)

will be relatively small. to see this, we    rst note that for uniform randomly drawn patterns, the mean of    
is zero, since the patterns are randomly   1. the variance is therefore given by

for j (cid:54)= k, all the terms are independent and contribute zero on average. therefore

(cid:48)

)vk(t)(cid:11)
j (t)(cid:11)

(cid:10)   2(cid:11) =

t   1(cid:88)

(cid:88)

  ,  (cid:48)(cid:54)=t

j,k

1
v 2

(cid:10)vi(   + 1)vi(  

(cid:48)

j

  ,  (cid:48)=1

1
v 2

(cid:88)

t   1(cid:88)

(cid:10)   2(cid:11) =
(cid:10)   2(cid:11) =

(cid:10)vi(   + 1)vi(  
(cid:10)v2
meaning that the sign of(cid:80)

i (   + 1)v2

(cid:88)

(cid:88)

1
v 2

  (cid:54)=t

j

+ 1)vj(   )vj(t)vk(  

(cid:48)

+ 1)vj(   )vj(  

(cid:48)

)v2

j (t)(cid:11) =

j (   )v2

t     1
v

when    (cid:54)=   (cid:48) all the terms are independent zero mean and contribute zero. hence

provided that the number of neurons v is signi   cantly larger than the length of the sequence, t , the average
size of the interference will therefore be small. in this case the term vi(t + 1) in equation (26.3.4) dominates,
j wijvj(t) will be that of vi(t + 1), and the correct pattern sequence recalled. a
careful analysis beyond our scope shows that the hebb rule is capable of storing a random (uncorrelated)
temporal sequence of length 0.269v time steps[92]. however, the hebb rule performs poorly for the case of
correlated patterns since interference from the other patterns becomes signi   cant[145, 69].

the pi rule    nds a matrix [w]ij = wij that solves the linear equations

wijvj(t) = vi(t + 1),

t = 1, . . . , t     1

pseudo inverse rule

(cid:88)

j

draft november 9, 2017

(26.3.2)

(26.3.3)

(26.3.4)

(26.3.5)

(26.3.6)

(26.3.7)

(26.3.8)

(26.3.9)

541

(cid:16)(cid:80)

(cid:17)

= sgn (vi(t + 1)) = vi(t + 1) so that patterns will be correctly

learning sequences

under this condition sgn
j wijvj(t)
recalled. in matrix notation we require

wv =   v

where

[v]it = vi(t),

t = 1, . . . , t     1,

(cid:104)

  v

(cid:105)

it

(26.3.10)

= vi(t + 1),

t = 2, . . . , t

(26.3.11)

for t < v the problem is under-determined so that multiple solutions exist. one solution is given by the
pseudo inverse:

w =   v

vtv

vt

(26.3.12)

(cid:16)

(cid:17)   1

the pseudo inverse (pi) rule can store any sequence of v linearly independent patterns. whilst attractive
compared to the standard hebb rule in terms of its ability to store longer correlated sequences, this rule
su   ers from very small basins of attraction for temporally correlated patterns, see    g(26.3).

the maximum likelihood hebb rule

an alternative to the above classical algorithms is to view this as a problem of pattern storage in the dbn,
equation (26.2.6) [20]. first, we need to clarify what we mean by    store   . given that we initialize the network
in a state v(t = 1), we wish that the remaining sequence will be generated with high id203. that is,
we wish to adjust the network parameters such that the id203

p(v(t ), v(t     1), . . . , v(2)|v(1))

(26.3.13)

is maximal2. furthermore, we might hope that the sequence will be recalled with high id203 not just
when initialized in the correct state but also for states close (in hamming distance) to the correct initial
state v(1).

due to the markov nature of the dynamics, the conditional likelihood is

p(v(t ), v(t     1), . . . , v(2)|v(1)) =

p(v(t + 1)|v(t))

(26.3.14)

this is a product of transitions from given states to given states. since these transition probabilities are
known, equation (26.2.6) and equation (26.2.2), the conditional likelihood can be easily evaluated. the
sequence log (conditional) likelihood is

l(w, b)     log

p(v(t + 1)|v(t)) =

log p(v(t + 1)|v(t)) =

log      (vi(t + 1)ai(t))

(26.3.15)

our task is then to    nd weights w and biases b that maximise l(w, b). there is no closed form solution
and the parameters need to be determined numerically. nevertheless, this corresponds to a straightforward
computational problem since the log likelihood is a convex function. to show this, we compute the hessian
(neglecting b for expositional clarity):

t   1(cid:89)

t=1

t   1(cid:88)

t=1

t   1(cid:88)

v(cid:88)

t=1

i=1

t   1(cid:89)

t=1

t   1(cid:88)

t=1

d2l

dwijdwkl

=      2

where we de   ned

vi(t + 1)vj(t)  i(t)(1       i(t))vk(t + 1)vl(t)  ik

  i(t)     1          (vi(t + 1)ai(t)) .

2static patterns can also be considered in this framework as a set of patterns that map to each other.

542

draft november 9, 2017

(26.3.16)

(26.3.17)

learning sequences

figure 26.3: leftmost panel: the highly-correlated train-
ing sequence we desire to store. the other panels show the
temporal evolution of the network after initialization in the
correct starting state but corrupted with 30% noise. during
recall, deterministic updates    =     were used. the max-
imum likelihood rule was trained using 10 batch epochs
with    = 0.1. see also demohopfield.m

it is straightforward to show that the hessian is negative semide   nite (see exercise(26.4)) and hence the
likelihood has a single global maximum. to increase the likelihood of the sequence, we can use a simple
method such as gradient ascent3

wnew

ij = wij +   

dl
dwij

,

bnew
i = bi +   

dl
dbi

where

t   1(cid:88)

t=1

dl
dwij

=   

  i(t)vi(t + 1)vj(t),

t   1(cid:88)

t=1

dl
dbi

=   

(26.3.18)

  i(t)vi(t + 1)

(26.3.19)

the learning rate    is chosen empirically to be su   ciently small to ensure convergence. the maximum
likelihood learning rule equation (26.3.19) can be seen as a modi   ed hebb learning rule, the basic hebb rule
being given when   i(t)     1. as learning progresses, the factors   i(t) will typically tend to values close to
either 1 or 0, and hence the learning rule can be seen as asymptotically equivalent to making an update
only in the case of disagreement (ai(t) and vi(t + 1) are of di   erent signs). this batch training procedure
can be readily converted to an online process in which an update occurs immediately after the presentation
of two consecutive patterns.

storage capacity of the ml hebb rule

the ml hebb rule is capable of storing a sequence of v linearly independent patterns. to see this, we    rst
form an input-output training set for each neuron i, {(v(t), vi(t + 1)), t = 1, . . . , t     1}. each neuron has
an associated weight vector wi     wij, j = 1, . . . , v , which forms a logistic regressor or, in the limit    =    ,
a id88[145]. for perfect recall of the patterns, we therefore need only that the vectors constituting
the pattern sequence be linearly separable. this will be the case if the patterns are linearly independent,
regardless of the outputs vi(t + 1), t = 1, . . . , t     1, see section(17.4.1).
relation to the id88 rule

in the limit that the activation is large, |ai| (cid:29) 1

(cid:26) 1 vi(t + 1)ai < 0

0 vi(t + 1)ai     0

  i(t)    

provided the activation and desired next output are the same sign, no update is made for neuron i. in
this limit, equation (26.3.19) is called the id88 rule[145, 86]. for an activation a that is close to the
decision boundary, a small change can lead to a di   erent sign of the neural    ring. to guard against this it
is common to include a stability criterion

(cid:26) 1 vi(t + 1)ai < m

0 vi(t + 1)ai     m

  i(t) =

3naturally, one can use more sophisticated methods such as the id77, or conjugate gradients.
neurobiology the emphasis is on gradient style updates since these are deemed to be biologically more plausible.

in theoretical

draft november 9, 2017

543

(26.3.20)

(26.3.21)

training sequencetimeneuron number1020102030405060708090100max likelihood1020102030405060708090100hebb1020102030405060708090100pseudo inverse1020102030405060708090100learning sequences

figure 26.4: the fraction of neurons correct for the    -
nal state of the network t = 50 for a 100 neuron hop-
   eld network trained to store a length 50 sequence of
patterns. after initialization in the correct initial state
at t = 1, the hop   eld network is updated determinis-
tically, with a randomly chosen percentage of the neu-
rons    ipped after updating. the correlated sequence
of length t = 50 was produced by    ipping with prob-
ability 0.5, 20% of the previous state of the network.
a fraction correct value of 1 indicates perfect recall
of the    nal state, and a value of 0.5 indicates a per-
formance no better than random guessing of the    nal
state. for maximum likelihood 50 epochs of training
were used with    = 0.02. during recall, deterministic
updates    =     were used. the results presented are
averages over 5000 simulations, resulting in standard
errors of the order of the symbol sizes.

(a)

where m is an empirically chosen positive threshold.

example 26.1 (storing a correlated sequence). in    g(26.3) we consider storage of a temporal sequence
of length t = 20 of 100 neurons using the three learning rules: hebb, maximum likelihood and pseudo
inverse. the sequence is highly correlated and therefore represents a di   cult learning task. the biases bi
are set to zero throughout to facilitate comparison. the initial state of the training sequence, corrupted by
30% noise is presented to the trained networks, and we desire that the remaining training sequence will be
recalled from this initial noisy state. whilst the hebb rule is operating in a feasible limit for uncorrelated
patterns, the strong correlations in this training sequence give rise to poor results. the pi rule is capable
of storing a sequence of length 100 yet is not robust to perturbations from the correct initial state. the
maximum likelihood rule performs well after a small amount of training.

stochastic interpretation

by straightforward manipulations, the weight update rule in equation (26.3.19) can be written as

(cid:16)

t   1(cid:88)

t=1

1
2

dl
dwij

=

(cid:17)

vi(t + 1)     (cid:104)vi(t + 1)(cid:105)p(vi(t+1)|ai(t))

vj(t)

(26.3.22)

a stochastic, online learning rule is therefore

   wij(t) =    (vi(t + 1)       vi(t + 1)) vj(t)

(26.3.23)

where   vi(t+1) is sampled in state 1 with id203     (ai(t)), and    1 otherwise. provided that the learning
rate    is small, this stochastic updating will approximate the learning rule (26.3.18,26.3.19).

example 26.2 (recalling sequences under perpetual noise). we consider 50 neurons storing a t = 50
length sequence (with zero biases and compare the performance of the maximum likelihood learning rule
with the hebb, pseudo inverse, and id88 rule. the training sequences are produced by starting from
a random initial state, v(1), and then choosing at random 20% percent of the neurons to    ip, each of the
chosen neurons being    ipped with id203 0.5, giving a random training sequence with a high degree of

544

draft november 9, 2017

00.050.10.150.20.250.30.350.40.450.50.50.550.60.650.70.750.80.850.90.951flip id203fraction correctsequence length=50max likelihoodnoise trained max likelihoodid88 (m=10)id88 (m=0)hebbpseudo inverselearning sequences

(a)

(b)

(a): original t = 15 binary video sequence on a set of 81    111 = 8991 neurons.

(b): the
figure 26.5:
reconstructions beginning from a 20% noise perturbed initial state. every odd time reconstruction is also
randomly perturbed. despite the high level of noise the basin of attraction of the pattern sequence is very
broad and the patterns immediately fall back close to the pattern sequence even after a single timestep.

temporal correlation.

after training, the network is initialized to a noise corrupted version of the correct initial state v(t = 1)
from the training sequence. the dynamics is then run (at    =    ) for the same number of steps as the
length of the training sequence. the fraction of bits of the recalled    nal state which are the same as the
training sequence    nal state v(t ) is then measured,    g(26.4). at each stage in the dynamics (except the
last), the state of the network is corrupted with noise by    ipping each neuron state with the speci   ed    ip
id203.

the standard hebb rule performs relatively poorly, particularly for small    ip rates, whilst the other
methods perform relatively well, being robust at small    ip rates. as the    ip rate increases, the pseudo
inverse rule becomes unstable, especially for the longer temporal sequence which places more demands
on the network. the id88 rule can perform as well as the maximum likelihood rule, although its
performance is critically dependent on an appropriate choice of the threshold m . the results for m = 0
id88 training are poor for small    ip rates. an advantage of the maximum likelihood rule is that it
performs well without the need for    ne tuning of parameters.

a similar example for a larger network is given in    g(26.5) which consists of highly correlated sequences.
the weights are learned using the maximum likelihood procedure. for such short sequences the basin of
attraction is very large and the video sequence can be stored robustly.

26.3.2 multiple sequences

if we assume that the sequences are
we now address
independent, the log likelihood of a set of sequences is the sum of the individual sequences. the gradient is
given by

learning a set of sequences {v n, n = 1, . . . , n}.
t   1(cid:88)
t   1(cid:88)

n(cid:88)

  n
i (t)vn

i (t + 1)vn

j (t),

dl
dbi

=   

  n
i (t)vn

i (t + 1)

t=1

dl
dwij

=   

where

n=1

t=1

(26.3.24)

(26.3.25)

i (t)     1          (vn
  n

i (t + 1)an

i (t)) ,

an
i (t) = bi +

wijvn

j (t)

n(cid:88)
(cid:88)

n=1

j

the log likelihood remains convex since it is a sum of convex functions, so that the standard gradient based
learning algorithms can be used successfully here as well.

draft november 9, 2017

545

tractable continuous latent variable models

26.3.3 boolean networks
the hop   eld network is one particular parameterisation of the table p(vi(t + 1) = 1|v(t)). however, less
constrained parameters may be considered. in the fully unconstrained case each neuron i has an associated
2v parental states. however, to specify this exponentially large number of states is impractical and an
interesting restriction is to consider that each neuron has only k parents, so that each table contains 2k
entries. learning the table parameters by maximum likelihood is straightforward since the log likelihood is
a convex function of the table entries. hence, for any given sequence (or set of sequences) one may readily
   nd parameters that maximise the sequence reconstruction id203. the maximum likelihood method
also produces large basins of attraction for the associated stochastic dynamical system. such models are of
interest in arti   cial life and random boolean networks in which emergent macroscopic behaviour appears
from local update rules. such systems are also used to study the robustness of chemical and gene regulatory
networks[172].

26.3.4 sequence disambiguation

a limitation of time-independent    rst order networks de   ned on visible variables alone (such as the hop   eld
network) is that the observation transition p(vt+1|vt = v) is the same every time the joint state v is
encountered. this means that if the sequence contains a subsequence such as a, b, a, c this cannot be recalled
with high id203 since a joint state a transitions to di   erent states at di   erent times. whilst one could
attempt to resolve this sequence disambiguation problem using a higher order markov model to account
for a longer temporal context, or by using a time-dependent model, we would lose biological plausibility.
using latent variables is an alternative way to sequence disambiguation. in the hop   eld model the recall
capacity can be increased using latent variables by making a sequencing in the joint latent-visible space
that is linearly independent, even if the visible variable sequence alone is not. in section(26.4) we discuss a
general method that extends dynamic belief networks de   ned on visible variables alone, such as the hop   eld
network, to include non-linearly updating latent variables.

26.4 tractable continuous latent variable models

a dynamic belief network with hidden (latent) variables h and visible variables (observations) v takes the
form

t   1(cid:89)

t=1

p(v(1 : t ), h(1 : t )) = p(v(1))p(h(1)|v(1))

p(v(t + 1)|v(t), h(t))p(h(t + 1)|v(t), v(t + 1), h(t)) (26.4.1)

as we saw in chapter(23), provided all hidden variables are discrete, id136 in these models is straight-
forward. however, in many physical systems it is more natural to assume continuous h(t). in chapter(24)
we saw that one such tractable continuous h(t) model is given by linear gaussian transitions and emissions
- the lds. whilst this is useful, we cannot represent non-linear changes in the latent process using an lds
alone. the switching lds of chapter(25) is able to model non-linear continuous dynamics (via switching)
although we saw that this leads to computational di   culties. for computational reasons we therefore seem
limited to either purely discrete h (with no limitation on the discrete transitions) or purely continuous h
(but be forced to use simple linear dynamics). is there a way to have a continuous state with non-linear
dynamics for which posterior id136 remains tractable? the answer is yes, provided that we assume the
hidden transitions are deterministic[14]. when conditioned on the visible variables, this renders the hidden
unit distribution trivial. this allows the consideration of rich non-linear dynamics in the hidden space. note
that such models are
not limited to the distributed computation context of this chapter; for example the
arch and garch models of chapter(24) are special cases.

@@

26.4.1 deterministic latent variables

consider a belief network de   ned on a sequence of visible variables v(1 : t ). to enrich the model we
include additional continuous latent variables h(1 : t ) that follow a non-linear markov transition. to retain
tractability of id136, we constrain the latent dynamics to be deterministic, described by

p(h(t + 1)|v(t + 1), v(t), h(t)) =    (h(t + 1)     f (v(t + 1), v(t), h(t),   h))

(26.4.2)

546

draft november 9, 2017

tractable continuous latent variable models

h(1)

h(2)

h(t)

v(1)

v(2)

(a)

v(t)

h(1)

h(2)

h(t)

v(1)

(b)

v(t)

v(2)

(c)

(a): a    rst order dynamic belief network with deterministic hidden transitions (represented
figure 26.6:
(b):
by diamonds) that is, each hidden node is certainly in a single state, determined by its parents.
conditioning on the visible variables forms a directed chain in the hidden space which is deterministic.
hidden unit id136 can be achieved by forward propagation alone. (c): integrating out hidden variables
gives a cascade style directed visible graph so that each v(t) depends on all v(1 : t     1).

the (possibly non-linear) function f parameterises the id155 table. whilst the restric-
tion to deterministic transitions appears severe, the model retains some attractive features: the marginal
p(v(1 : t )) is non-markovian, coupling all the variables in the sequence, see    g(26.6c), whilst hidden unit
id136 p(h(1 : t )|v(1 : t )) is deterministic. see also    g(26.6).
the adjustable parameters of the hidden and visible distributions are represented by   h and   v respectively.
for learning, the log likelihood of a single training sequence v(1 : t ) is

to maximise the log likelihood using gradient techniques we need the derivatives with respect to the model
parameters. these can be calculated recursively as follows:

t   1(cid:88)

t=1

l = log p(v(1)|  v) +

log p(v(t + 1)|v(t), h(t),   v)

where the hidden unit values are calculated recursively using

h(t + 1) = f (v(t + 1), v(t), h(t),   h)

log p(v(1)|  v) +

   
     v

log p(v(t + 1)|v(t), h(t),   v)

t   1(cid:88)

t=1

   

   h(t)

log p(v(t + 1)|v(t), h(t),   v)

dh(t)
d  h

dl
d  v

=

dl
d  h

=

   
     v

t   1(cid:88)

t=1

dh(t)
d  h

=

   f (t)
     h

+

   f (t)
   h(t     1)

dh(t     1)

d  h

where we use the shorthand

f (t)     f (v(t), v(t     1), h(t     1),   h)

(26.4.3)

(26.4.4)

(26.4.5)

(26.4.6)

(26.4.7)

(26.4.8)

hence the derivatives can be calculated by deterministic forward propagation. the case of training multiple
independently generated sequences is a straightforward extension obtained by summing the above over the
individual sequences.

whilst the deterministic latent variable models are very general, in the context of this chapter it is interesting
to apply them to some simple neurobiological models, enriching the hop   eld model with more powerful
internal dynamics.

draft november 9, 2017

547

tractable continuous latent variable models

(a)

(b)

(a): the training sequence consists of a random set of vectors (v = 3)
figure 26.7:
(b): the reconstruction using h = 7 hidden units. the
over t = 10 time steps.
initial state v(t = 1) for the recalled sequence was set to the correct initial training
value albeit with one of the values    ipped. note that the method is capable of sequence
disambiguation in the sense that transitions of the form a, b, . . . , a, c can be recalled.

26.4.2 an augmented hop   eld network

to make the deterministic latent variable model more explicit, we consider the case of continuous vector
hidden variables h(t) and discrete, binary vector visible variables with components vi(t)     {   1, 1}.
in
particular, we restrict attention to the hop   eld model augmented with latent variables that have a simple
linear dynamics (see exercise(26.5) for a non-linear extension):

h(t + 1) = 2  (ah(t) + bv(t))     1 deterministic latent transition

p(v(t + 1)|v(t), h(t)) =

v(cid:89)

i=1

   (vi(t + 1)  i(t)) ,

  (t)     ch(t) + dv(t)

(26.4.9)

(26.4.10)

this model generalises a recurrent stochastic heteroassociative hop   eld network[145] to include deterministic
hidden units dependent on previous network states. the parameters of the model are a, b, c, d. for
gradient based training we require the derivatives with respect to each of these parameters. the derivative
of the log likelihood for a generic parameter    is

(cid:88)

i

d
d  

l =

this gives:

  i(t)

d
d  

  i(t),

  i(t)     (1       (vi(t + 1)  i(t))) vi(t + 1)

(cid:88)

j

cij

d

da    

hj(t),

d

db    

  i(t) =

(cid:88)

j

cij

d

db    

hj(t)

d

da    

  i(t) =

d

dd    

  i(t) =   i  v  (t)

aij

d

da    

hj(t) +   i  h  (t)

aij

d

db    

hj(t) +   i  v  (t)

d

dc    

d

da    

d

db    

  i(t) =   i  h  (t),

hi(t + 1) = 2  

(cid:48)
i(t + 1)

hi(t + 1) = 2  

(cid:48)
i(t + 1)

(cid:88)
(cid:88)

j

j

(cid:48)
i(t)       (hi(t)) (1       (hi(t)))
  

if we assume that h(1) is a given    xed value (say 0), we can compute the derivatives recursively by forward
propagation. gradient based training for this augmented hop   eld network is therefore straightforward
to implement. this model extends the power of the original hop   eld model, being capable of resolving
ambiguous transitions in sequences such as a, b, a, c, see example(26.3). in terms of a dynamic system, the
learned network is an attractor with the training sequence as a stable point and demonstrates that such
models are capable of learning attractor networks more powerful than those without hidden units.

548

draft november 9, 2017

(26.4.11)

(26.4.12)

(26.4.13)

(26.4.14)

(26.4.15)

(26.4.16)

neural models

example 26.3 (sequence disambiguation). the sequence in    g(26.7a) contains repeated patterns and
therefore cannot be reliably recalled with a    rst order model containing visible variables alone. to deal with
this we consider a hop   eld network with 3 visible units and 7 additional hidden units with deterministic
(linear) latent dynamics. the model was trained with gradient ascent to maximise the likelihood of the
binary sequence in    g(26.7a). see demohopfieldlatent.m. as shown in    g(26.7b), the learned network is
capable of recalling the sequence correctly, even when initialised in an incorrect state, having no di   culty
with the fact that the sequence transitions are ambiguous.

26.5 neural models

the tractable deterministic latent variable model introduced in section(26.4) presents an opportunity to
extend models such as the hop   eld network to include more biologically realistic processes without losing
computational tractability. first we discuss a general framework for learning in a class of neural models[15,
241], this being a special case of the deterministic latent variable models[14] and a generalisation of the
spike-response model of theoretical neurobiology[119].

26.5.1 stochastically spiking neurons

we assume that neuron i    res depending on the membrane potential ai(t) through

p(vi(t + 1) = 1|v(t), h(t)) = p(vi(t + 1) = 1|ai(t))

to be speci   c, we take throughout

p(vi(t + 1) = 1|ai(t)) =    (ai(t))

here we de   ne the quiescent state as vi(t + 1) = 0, so that

p(vi(t + 1)|ai(t)) =    ((2vi(t + 1)     1)ai(t))

(26.5.1)

(26.5.2)

(26.5.3)

the use of the sigmoid function   (x) is not fundamental and is chosen merely for analytical convenience.
the log-likelihood of a sequence of visible states v = {v(1), . . . , v(t )} is

l =

log    ((2vi(t + 1)     1)ai(t))

for which the gradient is

dl
dwij

=

(vi(t + 1)       (ai(t)))

dai(t)
dwij

(26.5.4)

(26.5.5)

where we used the fact that vi     {0, 1}. here wij are parameters of the membrane potential (see below).
we take equation (26.5.5) as common in the following models in which the membrane potential ai(t) is
described with increasing sophistication.

26.5.2 hop   eld membrane potential

as a    rst step, we show how the hop   eld maximum likelihood training rule, as described in section(26.3.1),
can be recovered as a special case of the above framework. the hop   eld membrane potential is

ai(t)    

wijvj(t)     bi

(26.5.6)

where wij characterizes the e   cacy of information transmission from neuron j to neuron i, and bi is a bias.
applying the maximum likelihood framework to this model to learn a temporal sequence v by adjustment of
draft november 9, 2017
549

t   1(cid:88)

v(cid:88)

t=1

i=1

t   1(cid:88)

t=1

v(cid:88)

j=1

neural models

figure 26.8: learning with depression : u = 0.5,
   = 5,   t = 1,    = 0.25. despite the apparent
complexity of the dynamics, learning appropriate neu-
ral connection weights is straightforward using max-
imum likelihood. the reconstruction using the stan-
dard hebb rule by contrast is poor[15].

t   1(cid:88)

t=1

the parameters wij (the bi are    xed for simplicity), we obtain the (batch) learning rule (using dai/dwij = vj(t)
from equation (26.5.5))

wnew

ij = wij +   

dl
dwij

,

dl
dwij

=

(vi(t + 1)       (ai(t))) vj(t),

(26.5.7)

where the learning rate    is chosen empirically to be su   ciently small to ensure convergence. equa-
tion(26.5.7) matches equation (26.3.19) (which uses the   1 encoding).

26.5.3 dynamic synapses

in more realistic synaptic models, neurotransmitter generation depends on a    nite rate of cell subcomponent
production and the quantity of vesicles released is a   ected by the history of    ring[1]. loosely speaking, when
a neuron    res it releases a chemical substance from a local reservoir. if the neuron    res several times in
short succession, its ability to continue    ring weakens since the reservoir of release chemical is depleted.
this phenomenon can be modelled by using a depression mechanism that reduces the membrane potential

ai(t) = wijxj(t)vj(t)

for depression factors xj(t)     [0, 1]. a simple dynamics for these depression factors is[300]

(cid:18) 1     xj(t)

  

(cid:19)

xj(t + 1) = xj(t) +   t

    u xj(t)vj(t)

(26.5.8)

(26.5.9)

where   t,    , and u represent time scales, recovery times and spiking e   ect parameters respectively. note that
these depression factor dynamics are exactly of the form of deterministic hidden variables. it is therefore
straightforward to include these dynamic synapses in a principled way using the maximum likelihood learning
framework. for the hop   eld potential, the learning dynamics is simply given by equations (26.5.5,26.5.9),
with

dai(t)
dwij

= xj(t)vj(t)

(26.5.10)

example 26.4 (learning with depression). in    g(26.8) we demonstrate learning a random temporal se-
quence of 20 timesteps for 50 neurons with dynamic depressive synapses. after learning wij the trained
network is initialised in the    rst state of the training sequence. the remaining states of the sequence were
then correctly recalled by forward sampling of the learned model. the corresponding generated factors xi(t)
are also plotted during the reconstruction; we see the characteristic drop in x after a neuron    res with grad-
ual recovery. for comparison, we plot the results of using the dynamics having set the wij using the temporal
hebb rule, equation (26.3.1). the poor performance of the correlation based hebb rule demonstrates the
necessity, in general, to tailor the learning rule to the dynamical system it attempts to control.

550

draft november 9, 2017

neuron numberoriginalt10205101520253035404550reconstructiont10205101520253035404550x valuest10205101520253035404550hebb reconstructiont10205101520253035404550neural models

26.5.4 leaky integrate and    re models

leaky integrate and    re models move a step further towards biological realism in which the membrane
potential increments if it receives an excitatory stimulus (wij > 0), and decrements if it receives an inhibitory
stimulus (wij < 0). after    ring, the membrane potential is reset to a low value below the    ring threshold,
and thereafter steadily increases to a resting level (see for example [66, 119]). a model that incorporates
such e   ects is

        ai(t     1) +

(cid:88)

j

ai(t) =

       (1     vi(t     1)) + vi(t     1)  f ired

wijvj(t) +   rest (1       )

(26.5.11)

since vi     {0, 1}, if neuron i    res at time t     1 the potential is reset to   f ired at time t. similarly, with no
synaptic input, the potential equilibrates to   rest with time constant    1/ log   [15].
despite the increase in complexity of the membrane potential over the hop   eld case, deriving appropriate
learning dynamics for this new system is straightforward since, as before, the hidden variables (here the
membrane potentials) update in a deterministic fashion. the membrane potential derivatives are

(cid:18)

(cid:19)

+ vj(t)

(26.5.12)

dai(t)
dwij

= (1     vi(t     1))

  

dai(t     1)

dwij

by initialising the derivative dai(t=1)
the gradient which can be used to adapt wij in the usual manner wnew
apply synaptic dynamics to this case by replacing the term vj(t) in equation (26.5.12) by xj(t)vj(t).

= 0, equations (26.5.5,26.5.11,26.5.12) de   ne a    rst order recursion for
ij = wij +   dl/dwij. we could also

dwij

although a detailed discussion of the properties of the neuronal responses for networks trained in this way
is beyond our scope here, an interesting consequence of the learning rule equation (26.5.12) is a spike-
time dependent learning window in qualitative agreement with experimental observation in real biological
systems[241, 201].

26.6 summary

    classical models such as the hop   eld network can be trained to learn temporal sequences using maximum
likelihood. this results in a robust storage mechanism in which the patterns have a large basin of attraction.
    we can form tractable non-linear continuous latent systems provided the latent dynamics is deterministic.

these deterministic latent variable models are powerful yet id136 is straightforward.

    we demonstrated how complex models in neurobiology can be considered within the deterministic latent
variable framework, and also how learning rules can be derived in a straightforward manner. it is important
that the learning rule is derived with the particular neural dynamics in mind     otherwise undesirable e   ects
in pattern storage can occur.

26.7 code

demohopfield.m: demo of hop   eld sequence learning
hebbml.m: gradient ascent training of a set of sequences using maximum likelihood
hopfieldhiddennl.m: hop   eld network with additional non-linear latent variables
demohopfieldlatent.m: demo of hop   eld net with deterministic latent variables
hopfieldhiddenliknl.m: hop   eld network with hidden variables sequence likelihood

draft november 9, 2017

551

t   1(cid:88)

26.8 exercises

exercises

exercise 26.1. consider a very large v (cid:29) 1 stochastic hop   eld network, section(26.2), used to store a
single temporal sequence v(1 : t ) of length t (cid:28) v . in this case the weight matrix with elements wij may
be computationally di   cult to store. explain how to justify the assumption

wij =

ui(t)vi(t + 1)vj(t)

(26.8.1)

t=1

where ui(t) are the dual parameters and derive a maximum likelihood update rule for the dual parameters
ui(t).

exercise 26.2. a hop   eld network is used to store a raw uncompressed binary video sequence. each image
in the sequence contains 106 binary pixels. at a rate of 10 frames per second, how many hours of video can
106 neurons store?

exercise 26.3. derive the update equation (26.3.22).

exercise 26.4. show that the hessian equation (26.3.16) is negative semide   nite. that is

(cid:88)

i,j,k,l

xijxkl

d2l

dwijdwkl     0

for any x (cid:54)= 0.
exercise 26.5. for the augmented hop   eld network of section(26.4.2), with latent dynamics

      (cid:88)

j

           1

hi(t + 1) = 2  

aijhj(t) + bijvj(t)

(26.8.2)

(26.8.3)

derive the derivative recursions described in section(26.4.2).

exercise 26.6. the storage of a static pattern v can be considered equivalent to the requirement that, under
temporal dynamics, the id203 p(v(t + 1) = v|v(t) = v) is high. based on this, estimate numerically
how many such static patterns a 100 neuron stochastic hop   eld network can robustly recover when 10% of
the elements of a pattern are    ipped.

552

draft november 9, 2017

part v

approximate id136

553

introduction to part v

in part i we discussed id136 and showed that for certain models this is com-
putationally tractable. however, for many models of interest, one cannot perform
id136 exactly and approximations are required.

in part v we discuss approximate id136 methods, beginning with sampling based
approaches. these are popular and well known in many branches of the mathematical
sciences, having their origins in chemistry and physics. we also discuss alternative
deterministic approximate id136 methods which in some cases can have remarkably
accurate performance.

it is important to bear in mind that no single algorithm is going to be best on all
id136 tasks. for this reason, we attempt throughout to explain the assumptions
behind the techniques so that one may select an appropriate technique for the problem
at hand.

draft november 9, 2017

555

approximate id136 methods and their loose associations. the leaf nodes denote speci   c methods. part
v discusses these methods and their application to models of interest in machine learning.

556

draft november 9, 2017

graphicalmodeldeterministicapprox.laplace(cont.variables)variationalboundskl(q|p)factorised(naivemf)structuredotherconvexboundsloopymessagepassingep(messagesintract.)samplingapprox.exactsampling(egan-cestral)mcmcauxilliaryvariablemethodshybridmcmcslicesamplingmetropolishastingsgibbssamplingstructuredgibbssamplingimportancesamplingchapter 27

sampling

in cases where exact results cannot be obtained, a general purpose approach is to draw samples from the
distribution. in this chapter we discuss classical exact sampling methods that are typically limited to a
small number of variables or models with a high degree of structure. when these can no longer be applied,
we discuss approximate sampling methods including id115.

27.1 introduction

sampling concerns drawing realisations (samples) x =(cid:8)x1, . . . , xl(cid:9) of a variable x from a distribution p(x).

for a discrete variable x, in the limit of a large number of samples, the fraction of samples in state x tends
to p(x = x). that is,

xl = x

= p(x = x)

(27.1.1)

l(cid:88)

i(cid:104)

l=1

lim
l      

1
l

(cid:105)

in the continuous case, one can consider a region r such that the id203 that the samples occupy r
tends to the integral of p(x) over r. given a    nite set of samples, one can then approximate expectations
using

(cid:104)f (x)(cid:105)p(x)    

1
l

f (xl)       fx

(27.1.2)

the subscript in   fx emphasises that the approximation is dependent on the set of samples drawn. this
sample approximation holds for both discrete and continuous variables.

l(cid:88)

l=1

a sampling procedure produces realisations of the set x and can itself be considered as generating a distri-
bution   p(x ). provided the marginals of the sampling distribution are equal to the marginals of the target
distribution,   p(xl) = p(xl), then the average of the approximation   fx with respect to draws of the sample

set x is(cid:68)   fx

(cid:69)

=

  p(x )

1
l

(cid:69)

f (xl)

l(cid:88)

(cid:68)

l=1

= (cid:104)f (x)(cid:105)p(x)

  p(xl)

(27.1.3)

hence the mean of the sample approximation is the exact mean of f provided only that the marginals of
  p(x ) correspond to the required marginals p(x); that is, using   p(x ) then   fx is an unbiased estimator for
(cid:104)f (x)(cid:105)p(x). note that this holds, even if the individual samples x1, . . . , xl are dependent, that is   p(x ) does

not factorise into(cid:81)

l   p(xl).

557

for any sampling method, an important issue is the variance of the sample estimate. if this is low then only
a small number of samples are required since the sample mean must be close to the true mean (assuming it
is unbiased). de   ning

(cid:69)

(cid:68)   fx

      fx =   fx    

(cid:28)(cid:104)

      fx

(cid:105)2(cid:29)

the variance of the approximation is (assuming   p(xl) = p(xl), for all l)

,

  p(x )

   f (x) = f (x)     (cid:104)f (x)(cid:105)p(x)

(cid:69)

   f (xl)   f (xl(cid:48)

)

(cid:88)
      l

l,l(cid:48)

(cid:68)
(cid:68)
[   f (x)]2(cid:69)

+

  p(x)

  p(x )

=

=

1
l2

1
l2

  p(xl,xl(cid:48)

)

(cid:68)

(cid:88)

l(cid:54)=l(cid:48)

   f (xl)   f (xl(cid:48)

      

(cid:69)

)

  p(xl,xl(cid:48)

)

introduction

(27.1.4)

(27.1.5)

(27.1.6)

provided the samples are independent

l(cid:89)

l=1

  p(x ) =

  p(xl)

(cid:69)

(cid:10)   f (xl)(cid:11)(cid:68)
(cid:28)(cid:104)

)

   f (xl(cid:48)

(cid:105)2(cid:29)

      fx

=

1
l

  p(x )

which is zero since (cid:104)   f (x)(cid:105) = 0. hence

(cid:68)
[   f (x)]2(cid:69)

and   p(x) = p(x), then   p(xl, xl(cid:48)

) = p(xl)p(xl(cid:48)

). the second term in equation (27.1.6) above is composed of

p(x)

(27.1.7)

and the variance of the approximation scales inversely with the number of samples. in principle, therefore,
provided the samples are independently drawn from p(x), only a small number of samples is required to
accurately estimate the expectation. importantly, this result is independent of the dimension of x. however,
the critical di   culty is in actually generating independent samples from p(x). drawing samples from high-
dimensional distributions is generally di   cult and few guarantees exist to ensure that in a practical timeframe
the samples produced are independent. whilst a dependent sampling scheme may be unbiased, the variance
of the resulting estimate can be high such that a large number of samples may be required for expectations to
be approximated accurately. there are many di   erent sampling algorithms, all of which work in principle,
but each working in practice only when the distribution satis   es particular properties[124]; for example
id115 methods do not produce independent samples and a large number of samples
may be necessary to produce a satisfactory approximation. before we develop schemes for multivariate
distributions, we consider the univariate case.

27.1.1 univariate sampling

in the following, we assume that a random number generator exists which is able to produce a value uniformly
at random from the unit interval [0, 1]. we will make use of this uniform sampler to draw samples from
non-uniform distributions.

consider the one dimensional discrete distribution p(x) where dom(x) = {1, 2, 3}, with

discrete case

          0.6 x = 1

0.1 x = 2
0.3 x = 3

p(x) =

(27.1.8)

this represents a partitioning of the unit interval [0, 1] in which the interval [0, 0.6] has been labelled as
if we were to drop a point    anywhere
state 1, (0.6, 0.7] as state 2, and (0.7, 1.0] as state 3,    g(27.1).
at random, uniformly in the interval [0, 1], the chance that    would land in interval 1 is 0.6, and the
chance that it would be in interval 2 is 0.1 and similarly, for interval 3, 0.3. this therefore de   nes a valid

558

draft november 9, 2017

introduction

1

  

2

3

figure 27.1: a representation of the discrete distribu-
tion equation (27.1.8). the unit interval from 0 to 1
is partitioned in parts whose lengths are equal to 0.6,
0.1 and 0.3.

sampling procedure for discrete one-dimensional distributions in which we draw from a uniform distribution
and then identify the partition of the unit interval in which it lies. this can easily be found using the
cumulant, see algorithm(27.1). sampling from a discrete univariate distribution is straightforward since
computing the cumulant takes only o (k) steps for a k state discrete variable. in our example, we have
(c0, c1, c2, c3) = (0, 0.6, 0.7, 1). we then draw a sample uniformly from [0, 1], say u = 0.66. then the sampled
state would be state 2, since this is in the interval (c1, c2]. since the samples are independent, the number
of samples required to accurately represent a marginal is reasonably small, see    g(27.2).

continuous case

intuitively, the generalisation from the discrete to the continuous case is clear. first we calculate the
cumulant density function

(cid:90) y

c(y) =

p(x)dx

      

(27.1.9)

then we sample u uniformly from [0, 1], and obtain the corresponding sample x by solving c(x) = u    
x = c   1(u). formally, therefore, sampling of a continuous univariate variable is straightforward provided
we can compute the integral of the corresponding id203 density function.

for special distributions, however, one may avoid explicit use of the cumulant of p(x) by using alternative
procedures based on coordinate transformations. for the gaussian, for example, one may avoid the use of
the cumulant, see exercise(27.1).

27.1.2 rejection sampling

consider that we have an e   cient sampling procedure for a distribution q(x). can we use this to help us
sample from another distribution p(x)? we assume p(x) is known only up to a normalisation constant z,

p(x) = p   (x)/z. one way to sample from p(x) is to use a binary auxiliary variable y     {0, 1} and de   ne
q(x, y) = q(x)q(y|x) with

q(x, y = 1) = q(x)q(y = 1|x)

(27.1.10)

we can use the term q(y = 1|x) to our advantage if we set q(y = 1|x)     p(x)/q(x) since then q(x, y = 1)    
p(x) and sampling from q(x, y) gives us a procedure for sampling from p(x). to achieve this we assume we
can    nd a positive m such that

q(y = 1|x) =

p   (x)
m q(x)     1    x

(27.1.11)

algorithm 27.1 sampling from a univariate discrete distribution p with k states.

1: label the k states as i = 1, . . . , k, with associated probabilities pi.
2: calculate the cumulant

(cid:88)

j   i

ci =

pj

and set c0 = 0.

3: draw a value u uniformly at random from the unit interval [0, 1].
4: find that i for which ci   1 < u     ci.
5: return state i as a sample from p.

draft november 9, 2017

559

algorithm 27.2 rejection sampling to draw l independent samples from p(x) = p   (x)/z.
1: given p   (x) and q(x),    nd m such that p   (x)/q(x)     m for all x.

2: for l = 1 to l do
3:
4:

repeat

5:

6:
7:
8:
9: end for

until u     a
xl = xcand

draw a candidate sample xcand from q(x).
let a = p   (xcand)
draw a value u uniformly between 0 and 1.

m q(xcand)

introduction

(cid:46) accept the candidate

to sample y given x we then draw a value u uniformly between 0 and 1. if this value is less then q(y = 1|x),
we set y = 1, otherwise we set y = 0. to draw a sample from p(x) we    rst draw a candidate xcand from
q(x), and then a y from q(y|xcand). if y = 1 we take xcand as an independent sample from p(x)     otherwise
no sample is made, see algorithm(27.2). note that the samples are drawn only proportionally to p(x)    
however, since this proportionality constant is common to all x, this is equivalent to drawing from p(x).

the expected rate that we accept a sample is

(cid:90)

q(y = 1) =

x

q(y = 1|x)q(x) =

z
m

(27.1.12)

so that, to increase the acceptance rate, we seek the minimal m subject to p   (x)     m q(x). in cases where
q(x) has free parameters q(x|  ), the parameter    can be adjusted to minimise m . if we set q(x) = p(x)
and m = z then q(y = 1|x) = 1 and sampling is e   cient. in general, however, q(y = 1|x) will be less than
in which the distributions p(x) and q(x) are factorised, p(x) =(cid:81)d
1 and sampling less than ideally e   cient. in high dimensions for vector x, q(y = 1|x) will often be much
less than 1 so that rejection sampling can be extremely ine   cient. to see this, consider a simple scenario

i=1 p(xi) and q(x) =(cid:81)d

i=1 q(xi). then

d(cid:89)

i=1

p   (xi)
miq(xi)

=

d(cid:89)

i=1

q(y = 1|xi) = o(cid:0)  d(cid:1)

q(y = 1|x) =

(27.1.13)

where 0            1 is the typical value of q(y = 1|xi) for each dimension. hence the id203 of accepting x
may decrease exponentially with the number of dimensions of x. rejection sampling is therefore a potentially
useful method of drawing independent samples in very low dimensions, but is likely to be impractical in
higher dimensions.

27.1.3 multivariate sampling

one way to generalise the one dimensional (univariate) discrete case to a higher dimensional (multivariate)
distribution p(x1, . . . , xn) is to translate this into an equivalent one-dimensional distribution. this can be
achieved by enumerating all the possible joint states (x1, . . . , xn), giving each a unique integer i from 1 to
the total number of states, and constructing a univariate distribution with id203 p(i). this then trans-
forms the multivariate distribution into an equivalent univariate distribution, and sampling can be achieved
as before. in general, of course, this procedure is impractical since the number of states grows exponentially
with the number of variables x1, . . . , xn.

(a)

(b)

figure 27.2: histograms of the samples from the three
state distribution p(x) = {0.6, 0.1, 0.3}. (a): 20 sam-
(b): 1000 samples. as the number of samples
ples.
increases, the relative frequency of the samples tends
to the distribution p(x).

560

draft november 9, 2017

123024681230200400600introduction

an alternative exact approach, that holds for both discrete and continuous variables is to capitalise on the
relation

p(x1, x2) = p(x2|x1)p(x1)

(27.1.14)

we can sample from the joint distribution p(x1, x2) by    rst sampling a state for x1 from the one-dimensional
p(x1), and then, with x1 clamped to this state, sampling a state for x2 from the one-dimensional p(x2|x1).
it is clear how to generalise this to more variables by using a cascade decomposition:

p(x1, . . . , xn) = p(xn|xn   1, . . . , x1)p(xn   1|xn   2, . . . , x1) . . . p(x2|x1)p(x1)

(27.1.15)
however, in order to apply this technique, we need to know the conditionals p(xi|xi   1, . . . , x1). unless these
are explicitly given we need to compute these from the joint distribution p(x1, . . . , xn). computing such
conditionals will, in general, require the summation over an exponential number of states and, except for
small n, generally also be impractical. for belief networks, however, by construction the conditionals are
speci   ed so that this technique becomes practical, as we discuss in section(27.2).

drawing samples from a multivariate distribution is in general therefore a complex task and one seeks to
exploit any structural properties of the distribution to make this computationally more feasible. a common
approach is to seek to transform the distribution into a product of lower dimensional distributions, for
which the general change of variables method for continuous variables, de   nition(8.1), is required. a classic
example of this is sampling from a multivariate gaussian, which can be reduced to sampling from a set of
univariate gaussians by a suitable coordinate transformation, as discussed in example(27.1). for much of
the remaining chapter we discuss methods that are appropriate for drawing from multivariate distributions
when no obvious transformation exists to reduce the problem to a univariate form. note that if there
nevertheless exist transformations that result even in approximately independent variables, this can still be
a very useful preprocessing step.

example 27.1 (sampling from a multivariate gaussian). our interest is to draw a sample from the mul-
tivariate gaussian p(x) = n (x m, s). for a general covariance matrix s, p(x) does not factorise into a
product of univariate distributions. however, consider the transformation

where c is chosen so that cct = s. since this is a linear transformation, y is also gaussian distributed
with mean

(cid:104)x(cid:105)p(x)     m

= c   1 (m     m) = 0

(27.1.17)

(27.1.16)

y = c   1 (x     m)
(cid:104)y(cid:105) =(cid:10)c   1 (x     m)(cid:11)
(cid:68)
= c   1(cid:68)
yyt(cid:69)

p(x)

(cid:17)

p(x) = c   1(cid:16)
(x     m) (x     m)t(cid:69)
(cid:89)

hence

p(y) = n (y 0, i) =

n (yi 0, 1)

i

since the mean of y is zero, the covariance is given by

c   t = c   1sc   t = c   1cctc   t = i

(27.1.18)

p(x)

(27.1.19)

a sample from y can then be obtained by independently drawing a sample from each of the univariate zero
mean unit variance gaussians. given a sample for y, a sample for x is obtained using

x = cy + m

(27.1.20)

drawing samples from a univariate gaussian is a well-studied topic, with a popular method being the
box-muller technique, exercise(27.1).

draft november 9, 2017

561

ancestral sampling

figure 27.3: an ancestral belief network without any
evidential variables. to sample from this distribution,
we draw a sample from variable 1, and then variables,
2,. . . ,6 in order.

x1

x2

x3

x4

x5

x6

27.2 ancestral sampling

belief networks take the general form:

(cid:89)

i

p(x) =

p(xi|pa (xi))

(27.2.1)

where we assume that each of the conditional distributions p(xi|pa (xi)) is speci   ed. provided that no
variables are evidential, we can sample from this distribution in a straightforward manner. for convenience,
we    rst rename the variable indices so that parent variables always come before their children (ancestral
ordering), for example (see    g(27.3))

p(x1, . . . , x6) = p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

(27.2.2)

one can sample    rst from those nodes that do not have any parents (here, x1 and x2). given these values,
one can then sample x3, and then x4 and x5 and    nally x6. despite the presence of loops in the graph, such
a forward sampling procedure is straightforward. this procedure holds for both discrete and continuous
variables. if one attempted to carry out an exact marginal id136 scheme in a complex multiply connected
graph, the moralisation and triangulation steps can result in very large cliques, so that exact id136
becomes intractable. however, regardless of the loop structure, ancestral sampling remains straightforward.
ancestral or    forward    sampling is a case of perfect sampling (also termed exact sampling) since each sample
is indeed independently drawn from the required distribution. this is in contrast to markov chain monte
carlo methods sections(27.3,27.4) for which (dependent) samples are drawn from p(x) only in the limit of
a large number of iterations.

27.2.1 dealing with evidence

how can we sample from a distribution in which a subset of variables xe are clamped to evidential states?
writing x = xe     x\e , formally we wish to sample from

p(x\e|xe ) =

p(x\e , xe )

p(xe )

(27.2.3)

if an evidential variable xi has no parents, then one can simply set the variable into this state and continue
forward sampling as before. for example, to compute a sample from p(x1, x3, x4, x5, x6|x2) de   ned in equa-
tion (27.2.2), one simply clamps x2 into its evidential state and continues forward sampling. the reason this
is straightforward is that conditioning on x2 merely de   nes a new distribution on a subset of the variables,
for which the belief network representation of the distribution is immediately known and remains ancestral.

(cid:80)

on the other hand, consider sampling from p(x1, x2, x3, x4, x5|x6). using bayes    rule, we have

p(x1, x2, x3, x4, x5|x6) =

p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

x1,x2,x3,x4,x5 p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

(27.2.4)

the conditioning on x6 means that the structure of the distribution on the non-evidential variables changes    
for example x4 and x5 become coupled (conditioned on x3, x4 and x5 are independent; however, conditioned
on x3 and x6, x4 and x5 are dependent). one could attempt to work out an equivalent new forward sam-
pling structure, see exercise(27.3), although generally this will be as complex as running an exact id136
approach.

an alternative is to proceed with forward sampling from the non-evidential distribution, and then discard
any samples which do not match the evidential states, see exercise(27.8) for an analogous situation that

562

draft november 9, 2017

id150

x2

x5

x1

x4

x3

x6

figure 27.4: the shaded nodes are the markov blanket of x4 for the belief
network of    g(27.3). to draw a sample from p(x4|x\4) we clamp x3, x5, x6
into their evidential states and draw a sample from p(x4|x3)p(x6|x4, x5)/z
where z is a normalisation constant.

from p(x) will be consistent with the evidence is roughly o (1/(cid:81)

justi   es this procedure. however, this is generally not recommended since the id203 that a sample
i is the number of
states of evidential variable i. in principle one can ease this e   ect by discarding the sample as soon as any
variable state is inconsistent with the evidence. nevertheless, the number of re-starts required to obtain a
valid sample would, on average, be very large. for this reason, alternative non-exact procedures are more
common, as we discuss in section(27.3).

i ) where dim xe

i dim xe

27.2.2 perfect sampling for a markov network

for a markov network we can draw exact samples by forming an equivalent directed representation of the
graph, see section(6.8), and subsequently using ancestral sampling on this directed graph. this is achieved
by    rst choosing a root clique and then consistently orienting edges away from this clique. an exact sample
can then be drawn from the markov network by    rst sampling from the root clique and then recursively
from the children of this clique. see potsample.m, jtsample.m and demojtreesample.m.

27.3 id150

the ine   ciency of methods such as ancestral sampling under evidence motivates alternative techniques. an
important and widespread technique is id150 which is generally straightforward to implement.

no evidence

assume we have a joint sample state x1 from the multivariate distribution p(x). we then consider a
particular variable, xi, for which we wish to draw a sample. using conditioning we may write

p(x) = p(xi|x1, . . . , xi   1, xi+1, . . . , xn)p(x1, . . . , xi   1, xi+1, . . . , xn)

(27.3.1)

given a joint initial state x1, from which we can read o    the    parental    state x1
then draw a sample x2

i from

1, . . . , x1

i   1, x1

i+1, . . . , x1

n, we

i   1, x1

1, . . . , x1

p(xi|x1

which only xi has been updated) x2 =(cid:0)x1

i+1, . . . , x1

n)     p(xi|x\i)

1, . . . , x1

i   1, x2

i , x1

i+1, . . . , x1
n

(cid:1). one then selects another variable xj

(27.3.2)

we assume this distribution is easy to sample from since it is univariate. we call this new joint sample (in

to sample and, by continuing this procedure, generates a set x1, . . . , xl of samples in which each xl+1 di   ers
from xl in only a single component. clearly, this is not an exact sampler since the resulting samples are
highly dependent. whilst id150 is therefore generally straightforward to implement, a drawback is
that the samples are strongly dependent. nevertheless, as discussed in section(27.1), provided the marginal
of the sampling distribution is correct, then this is still a valid sampler. we outline in section(27.3.1) why
in the limit of a large number of samples, this holds and the sampler becomes valid.

for a general distribution, the conditional p(xi|x\i) depends only on the markov blanket of the variable xi.
for a belief network, the markov blanket of xi is

p(xj|pa (xj))

(27.3.3)

see for example,    g(27.4). the normalisation constant for this univariate distribution is straightforward to
work out from the requirement:

p(xi|pa (xi))

xi

j   ch(i)

p(xj|pa (xj))

(27.3.4)

563

draft november 9, 2017

(cid:89)

j   ch(i)

1
z

p(xi|pa (xi))

(cid:89)

p(xi|x\i) =
(cid:88)

z =

id150

figure 27.5: a two dimensional distribution for which id150 fails.
the distribution has mass only in the shaded quadrants. id150
1, xl
proceeds from the lth sample state (xl
2) and then sampling from p(x2|xl
1),
) where xl+1
which we write (xl+1
, xl+1
1 = xl
1. one then continues with a
sample from p(x1|x2 = xl+1
), etc. if we start in the lower left quadrant and
proceed this way, the upper right region is never explored.

1

2

2

in the case of a continuous variable xi the summation above is replaced with integration. due to the local
structure of a belief network, only the parental and parents-of-children states are required in forming the
sample update.

evidence

evidence is readily dealt with by clamping for all samples the evidential variables into their evidential states.
there is also no need to sample for these variables, since their states are known. one then proceeds as before,
selecting a non-evidential variable and determining its distribution conditioned on its markov blanket, and
subsequently drawing a sample from this variable.

27.3.1 id150 as a markov chain

in id150 we have a sample of the joint variables xl at stage l. based on this we produce a new
joint sample xl+1. this means that we can write id150 as a procedure that draws from

for some distribution q(xl+1|xl). if we choose the variable to update, xi, at random from a distribution q(i),
then id150 corresponds to drawing samples using the markov transition

q(xl+1|xl, i)q(i),

i

q(xl+1|xl, i) = p(xl+1

i

|xl\i)

  

xl+1
j

, xl
j

(cid:16)

(cid:89)

j(cid:54)=i

(cid:17)

(27.3.5)

(27.3.6)

xl+1     q(xl+1|xl)
(cid:88)

q(xl+1|xl) =

with q(i) > 0,(cid:80)

i q(i) = 1. that is, we select a variable, then sample from its conditional, copying across
the states of the other variables from the previous sample. our interest is to show that the stationary
distribution of q(x(cid:48)
|x) is p(x). we carry this out assuming x is continuous     the discrete case is analogous:

(cid:90)
(cid:90)
(cid:90)

x

(cid:90)

x

(cid:48)
q(x

|x)p(x) =

=

=

=

i

(cid:88)
(cid:88)
(cid:88)
(cid:88)

i

i

i

q(i)

q(i)

(cid:48)
q(x

(cid:89)

  (cid:0)x

x

j(cid:54)=i

|x, i)p(x)

(cid:48)
j, xj

q(i)

p(x

xi

(cid:48)
(cid:48)
\i)p(xi, x
\i)

(cid:48)
i|x

q(i)p(x

(cid:48)
(cid:48)
\i)p(x
i|x

(cid:48)
\i) =

i

(cid:1) p(x
(cid:48)
i|x\i)p(xi, x\i)
(cid:88)

q(i)p(x

(cid:48)

(cid:48)
) = p(x

(27.3.7)

@@

(27.3.8)

(27.3.9)

(27.3.10)

)

hence, as long as we continue to draw samples according to the distribution q(x(cid:48)
|x), in the limit of a large
number of samples we will ultimately tend to draw (dependent) samples from p(x). any distribution q(i)
su   ces so that visiting all variables equally often is a valid choice. technically, we also require that q(x(cid:48)
|x)
has p(x) as its equilibrium distribution, so that no matter in which state we start, we always converge to
p(x); see    g(27.5) and section(27.3.3) for a discussion of this issue.

564

draft november 9, 2017

1x2xid150

27.3.2 structured id150

one can extend id150 by using conditioning to reveal a tractable distribution on the remaining
variables. for example, consider the distribution,    g(27.6a)

p(x1, x2, x3, x4) =   (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)  (x1, x3)

(27.3.11)

in single-site id150 we would condition on three of the four variables, and sample from the
remaining variable. for example, using x to emphasise known states,

p(x1|x2, x3, x4)       (x1, x2)  (x4, x1)  (x1, x3)

(27.3.12)

however, we may use more limited conditioning as long as the conditioned distribution is easy to sample
from. in the case of equation (27.3.11) we can condition on x3 alone to give

p(x1, x2, x4|x3)       (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)  (x1, x3)

this can be written as a modi   ed distribution,    g(27.6b)

p(x1, x2, x4|x3)       

(cid:48)

(x1, x2)  

(cid:48)

(x4, x1)

(27.3.13)

(27.3.14)

as a distribution on x1, x2, x4 this is a singly-connected linear chain markov network from which samples
can be drawn exactly, as explained in section(27.2.2). a simple approach is to compute the normalisation
constant by any of the standard techniques, for example, using the factor graph method. one may then
convert this undirected linear chain to a directed graph, and use ancestral sampling. these operations are
linear in the number of variables in the conditioned distribution. alternatively, one may form a junction
tree from a set of potentials, choose a root and then form a set chain by reabsorption on the junction tree.
ancestral sampling can then be performed on the resulting oriented clique tree. this is the approach taken
in gibbssample.m.

in the above example one can also reveal a tractable distribution by conditioning on x1,

p(x3, x2, x4|x1)       (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)  (x1, x3)

(27.3.15)

and then draw a sample of x2, x3, x4 from this distribution using another exact sampler. a structured gibbs
sampling procedure is then to    rst draw a sample x1, x2, x4 from equation (27.3.13) and then a sample
x3, x2, x4 from equation (27.3.15). these two steps (of drawing exact conditional samples) are then iterated.
note that x2 and x4 are not constrained to be equal to their values in the previous sample. this procedure
is generally to be preferred to the single-site gibbs updating since the samples are less dependent from one
sample to the next.

see demogibbssample.m for a comparison of unstructured and structured sampling from a set of potentials.

27.3.3 remarks

if the initial sample x1 is in a part of the state space that has very low id203, it may take some time
for the samples to become representative, as only a single component of x is updated at each iteration. this
motivates a so-called burn in stage in which the initial samples are discarded.

in single site id150 there will be a high degree of dependence in any two successive samples, since
only one variable (in the single-site updating version) is updated at each stage. this motivates subsampling

x1

x4

x2

x1

x2

x3

x4

(a)

(b)

(a): a toy    intractable    distribution.
figure 27.6:
id150 by conditioning on all variables ex-
cept one leads to a simple univariate conditional dis-
(b): conditioning on x3 yields a new
tribution.
distribution that is singly-connected, for which exact
sampling is straightforward.

draft november 9, 2017

565

id115 (mcmc)

(a)

(b)

figure 27.7: two hundred gibbs samples for a two dimensional gaussian. at each stage only a single
(a): for a gaussian with low correlation, id150 can move through the
component is updated.
likely regions e   ectively. (b): for a strongly correlated gaussian, id150 is less e   ective and does
not rapidly explore the likely regions, see demogibbsgauss.m.

in which, say, every 10th, sample xk, xk+10, xk+20, . . ., is taken, and the rest discarded.

due to its simplicity, id150 is one of the most popular sampling methods and is particularly con-
venient when applied to belief networks due to the markov blanket property1. id150 is a special
case of the mcmc framework and, as with all mcmc methods, one should bear in mind that convergence
can be an issue since it is generally unknown how many samples are needed to be reasonably sure that a
sample estimate is accurate.

id150 assumes that we can move throughout the space e   ectively by only single co-ordinate up-
dates. we also require that every state can be visited in   nitely often. in    g(27.5), we show a two dimensional
continuous distribution that has mass only in the lower left and upper right regions. in that case, if we
start in the lower left region, we will always remain there, and never explore the upper right region. this
problem occurs when two regions are not connected by a    probable    gibbs path.

a gibbs sampler is a perfect sampler (provided we subsample appropriately) when the distribution is
factorised     that is the variables are independent. this suggests that in general id150 will be less
e   ective when variables are strongly correlated. for example, if we consider id150 from a strongly
correlated two variable gaussian distribution, then updates will move very slowly in space, see    g(27.7). it
is useful therefore to    nd variable transformations that render the new variables approximately independent,
since then one may apply id150 more e   ectively.

27.4 id115 (mcmc)

we assume we have a multivariate distribution in the form

   
p

1
z

(x)

p(x) =

where p   (x) is the unnormalised distribution and z =(cid:82)

x p   (x) is the (computationally intractable) normal-
isation constant. we assume we are able to evaluate p   (x = x), for any state x, but not p(x = x) since
z is intractable. the idea in mcmc sampling is to sample, not directly from p(x), but from a di   erent
distribution such that, in the limit of a large number of samples, e   ectively the samples will be from p(x).
to achieve this we forward sample from a markov transition whose stationary distribution is equal to p(x).

(27.4.1)

1the bugs package www.mrc-bsu.cam.ac.uk/bugs is general purpose software for sampling from belief networks.

566

draft november 9, 2017

   4   3   2   101234   3   2   10123   3   2   10123   2   1.5   1   0.500.511.52id115 (mcmc)

27.4.1 markov chains
consider the conditional distribution q(xl+1|xl). if we are given an initial sample x1, then we can recursively
generate samples x1, x2, . . . , xl. after a long time l (cid:29) 1, (and provided the markov chain is    irreducible   ,
meaning that we can eventually get from any state to any other state
and    aperiodic   , meaning we don   t
unique stationary distribution q   (x) which is de   ned
periodically revisit any state) the samples are from the
as (for a continuous variable)

@@
@@

(cid:90)

x

(cid:48)

(cid:90)

q   (x

(cid:48)

) =

(cid:48)

q(x

|x)q   (x)

(27.4.2)

the condition for a discrete variable is analogous on replacing integration with summation. the idea in
mcmc is, for a given distribution p(x), to    nd a transition q(x(cid:48)
|x) which has p(x) as its stationary distri-
bution. if we can do so, then we can draw samples from the markov chain by forward sampling and take
these as samples from p(x) as the chain converges towards its stationary distribution.
note that for every distribution p(x) there will be more than one transition q(x(cid:48)
|x) with p(x) as its sta-
tionary distribution. this is why there are many di   erent mcmc sampling methods, each with di   erent
characteristics and varying suitability for the particular distribution at hand. we   ve already encountered
one class of samplers from the mcmc family, namely id150 which corresponds to a particular
transition q(x(cid:48)

|x). below we discuss more general members of this family.

(cid:18)

(cid:90)

(cid:90)

27.4.2 metropolis-hastings sampling

consider the following transition

(cid:19)

(cid:48)
q(x

(cid:48)

(cid:48)

(cid:48)(cid:48)

(cid:48)(cid:48)

, x) +   (x

|x)f (x

|x) =   q(x
|x) is a so-called proposal distribution and 0 < f (x(cid:48), x)     1 a positive function. this de   nes a

|x)f (x

(27.4.3)

1    

  q(x

, x)

, x)

x(cid:48)(cid:48)

where   q(x(cid:48)
valid distribution q(x(cid:48)

(cid:48)
q(x

|x) =

x(cid:48)

|x) since it is non-negative and
(cid:48)(cid:48)
  q(x

  q(x

(cid:48)

(cid:48)
|x)f (x

, x) + 1    

x(cid:48)(cid:48)

x(cid:48)

(cid:48)(cid:48)
|x)f (x

, x) = 1

(27.4.4)

(cid:90)

|x). that is

(cid:48)
p(x

) =

our interest is to set f (x(cid:48), x) such that the stationary distribution of q(x(cid:48)
  q(x(cid:48)

|x) is equal to p(x) for any proposal

(cid:18)

(cid:90)

(cid:19)

(cid:48)

q(x

|x)p(x) =

x

(cid:48)
  q(x

(cid:48)

|x)f (x

, x)p(x) + p(x

(cid:48)

)

x

1    

  q(x

x(cid:48)(cid:48)

(cid:48)(cid:48)

(cid:48)
, x

)

)f (x

(cid:48)(cid:48)

(cid:48)

|x

in order that this holds, we require (changing the integral variable from x(cid:48)(cid:48) to x)

(cid:90)

(cid:90)

(cid:90)
(cid:90)

(cid:48)

  q(x

x

|x)f (x

(cid:48)

, x)p(x) =

  q(x|x

x

(cid:48)

(cid:48)
)f (x, x

(cid:48)

)

)p(x

now consider the metropolis-hastings acceptance function

(cid:48)

f (x

, x) = min

(cid:48)

f (x

, x)  q(x

(cid:48)

1,

(cid:19)

  q(x|x(cid:48))p(x(cid:48))
  q(x(cid:48)
|x)p(x)

(cid:18)
|x)p(x) = min(cid:0)  q(x
= min(cid:0)  q(x|x

(cid:48)

(cid:18)

= min

1,

|x)p(x),   q(x|x
(cid:48)
(cid:48)
(cid:48)
),   q(x
)p(x

(cid:19)

  q(x|x(cid:48))p   (x(cid:48))
|x)p   (x)
  q(x(cid:48)
)(cid:1)

|x)p(x)(cid:1) = f (x, x

)p(x

(cid:48)

(cid:48)

which is de   ned for all x, x(cid:48) and has the    detailed balance    property

(cid:48)

(cid:48)
)  q(x|x

(cid:48)

)

)p(x

hence the function f (x(cid:48), x) as de   ned above ensures equation (27.4.6) holds and that q(x(cid:48)
stationary distribution.
how do we sample from q(x(cid:48)
proportional to   q(x(cid:48)

|x)f (x(cid:48), x) and the other   (x(cid:48), x) with mixture coe   cient 1    

|x)? equation(27.4.3) can be interpreted as a mixture of two distributions, one
|x)f (x(cid:48)(cid:48), x). to

x(cid:48)(cid:48)   q(x(cid:48)(cid:48)

(cid:82)

draft november 9, 2017

567

(27.4.5)

(27.4.6)

(27.4.7)

(27.4.8)

(27.4.9)

|x) has p(x) as its

algorithm 27.3 metropolis-hastings mcmc sampling.

id115 (mcmc)

1: choose a starting point x1.
2: for i = 2 to l do
3:

draw a candidate sample xcand from the proposal   q(x(cid:48)
let a =   q(xl   1|xcand)p(xcand)
  q(xcand|xl   1)p(xl   1)
if a     1 then xl = xcand
else

|xl   1).

draw a random value u uniformly from the unit interval [0, 1].
if u < a then xl = xcand
else

4:

5:
6:
7:
8:
9:
10:
11:
12:
13: end for

xl = xl   1

end if

end if

(cid:46) accept the candidate

(cid:46) accept the candidate

(cid:46) reject the candidate

|x) = n (x(cid:48) x, i).

figure 27.8: metropolis-hastings samples from a bi-variate
distribution p(x1, x2) using a proposal   q(x(cid:48)
we also plot the iso-id203 contours of p. although
p(x) is multi-modal, the dimensionality is low enough and
the modes su   ciently close such that a simple gaussian pro-
posal distribution is able to bridge the two modes. in higher
dimensions, such multi-modality is more problematic. see
demometropolis.m

draw a sample from this, we draw a sample from   q(x(cid:48)
|x) and accept this with id203 f (x(cid:48), x). since
drawing from   q(x(cid:48)
|x) and accepting are performed independently, the id203 of accepting the drawn
candidate is the product of these probabilities, namely   q(x(cid:48)
|x)f (x(cid:48), x). otherwise the candidate is rejected
and we take the sample x(cid:48) = x. using the properties of the acceptance function, equation (27.4.7), the
following is equivalent to the above procedure: when

   

(cid:48)

(cid:48)

   

(cid:48)
  q(x|x

)p

(x

|x)p

) >   q(x

(x)
we accept the sample from   q(x(cid:48)
  q(x|x(cid:48))p   (x(cid:48))/  q(x(cid:48)

|x). otherwise we accept the sample x(cid:48) from q(x(cid:48)

|x) with id203
if we reject the candidate we take x(cid:48) = x. note that if the candidate x(cid:48) is
rejected, we take the original x as the new sample. hence each iteration of the algorithm produces a sample
    either a copy of the current sample, or the candidate sample, see algorithm(27.3). a rough rule of thumb
is to choose a proposal distribution for which the acceptance rate is between 50% and 85%[116].

|x)p   (x).

(27.4.10)

gaussian proposal distribution

a common proposal distribution for vector x is

(cid:0)x(cid:48) x,   2i(cid:1)
(cid:18)
|x) =   q(x|x(cid:48)) and the acceptance criterion equation (27.4.7) becomes

    1
2  2 (x(cid:48)   x)2

(cid:19)

    e

  q(x(cid:48)

|x) = n

for which   q(x(cid:48)

f (x(cid:48)

p   (x(cid:48))
p   (x)

1,

, x) = min

(27.4.12)
if the unnormalised id203 p   (x(cid:48)) of the candidate state is higher than the current state, p   (x), we
therefore accept the candidate. otherwise, we accept the candidate only with id203 p   (x(cid:48))/p   (x). if

(27.4.11)

568

draft november 9, 2017

auxiliary variable methods

(a)

(b)

(c)

(a): multi-modal distribution p(x) for which we desire samples.

(b):
figure 27.9: hybrid monte carlo.
(c): this is a plot of (b) from above.
hmc forms the joint distribution p(x)p(y) where p(y) is gaussian.
starting from the point x, we    rst draw a y from the gaussian p(y), giving a point (x, y), given by the
green line. then we use hamiltonian dynamics (white line) to traverse the distribution at roughly constant
energy for a    xed number of steps, giving x(cid:48), y(cid:48). we accept this point if h(x(cid:48), y(cid:48)) > h(x, y(cid:48)) and make the
new sample x(cid:48) (red line). otherwise this candidate is accepted with id203 exp(h(x(cid:48), y(cid:48))    h(x, y(cid:48))). if
rejected the new sample x(cid:48) is taken as a copy of x.

the candidate is rejected, the new sample is taken to be a copy of the previous sample x. see    g(27.8) for
a demonstration.

in high dimensions it is unlikely that a random candidate sampled from a gaussian will result in a candidate
id203 higher than the current value, exercise(27.5). because of this, only very small jumps (  2 small)
are likely to be accepted. this limits the speed at which we explore the space x and increases the dependency
between samples. as an aside, the acceptance function (27.4.12) highlights that sampling is di   erent from
   nding the optimum. provided x(cid:48) has a higher id203 than x, we accept x(cid:48). however, we may also
accept candidates that have a lower id203 than the current sample.

27.5 auxiliary variable methods

a practical concern in mcmc methods is ensuring that one moves e   ectively through the signi   cant proba-
bility regions of the distribution. for methods such as metropolis-hastings with local proposal distributions
(local in the sense they are unlikely to propose a candidate far from the current sample), if the target distri-
bution has isolated islands of high density, then the chance that we would move from one island to the other
is very small. conversely, if we attempt to make the proposal less local by using one with a high variance
the chance then of landing at random on a high density island is remote. auxiliary variable methods use
additional dimensions to aid exploration and in certain cases to provide a bridge between isolated high
density islands. see also [214] for the use of auxiliary variables in perfect sampling.

consider drawing samples from p(x). for an auxiliary variable y we introduce a distribution p(y|x) to form
the joint distribution

p(x, y) = p(y|x)p(x)

(27.5.1)

if we draw samples (xl, yl) from this joint distribution then a valid set of samples from p(x) is given by
taking the xl alone. if we sampled x directly from p(x) and then y from p(y|x), introducing y is pointless
since there is no e   ect on the x sampling procedure. in order for this to be useful, therefore, the auxiliary
variable must in   uence how we sample x. below we discuss some of the common auxiliary variable schemes.

27.5.1 hybrid monte carlo

hybrid mc is a method for continuous variables that aims to make non-local jumps in the sample space
and, in so doing, to jump potentially from one mode to another. we de   ne the distribution from which we

draft november 9, 2017

569

   5   4   3   2   101234500.511.522.53x 10   3algorithm 27.4 hybrid monte carlo sampling

auxiliary variable methods

1: start from x1
2: for i = 1 to l do
3:
4:
5:
6:

draw a new sample y from p(y).
choose a random (forwards or backwards) trajectory direction.
starting from xi, y, follow hamiltonian dynamics for a    xed number of steps, giving a candidate x(cid:48), y(cid:48).
if h(x(cid:48), y(cid:48)) > h(x, y), otherwise accept it with id203
accept the candidate xi+1 = x(cid:48)

exp(h(x(cid:48), y(cid:48))     h(x, y)).

if rejected, we take the sample as xi+1 = xi.

7:
8: end for

wish to sample as

p(x) =

1
zx

ehx(x)

(27.5.2)

for some given    hamiltonian    hx(x). we then de   ne another,    easy    auxiliary distribution from which we
can readily generate samples,

p(y) =

1
zy

ehy(y)

so that the joint distribution is given by

p(x, y) = p(x)p(y) =

1
z

ehx(x)+hy(y) =

1
z

eh(x,y),

h(x, y)     h(x) + h(y)

(27.5.3)

(27.5.4)

in the standard form of the algorithm, a multi-dimensional gaussian is chosen for the auxiliary distribution
with dim (y) = dim (x), so that

hy(y) =    

yty

1
2

(27.5.5)

the hmc algorithm    rst draws from p(y) and subsequently from p(x, y). for a gaussian p(y), sampling
from this is straightforward. in the next    dynamic    step, a sample is drawn from p(x, y) using a metropolis
mcmc sampler. the idea is to go from one point of the space x, y to a new point x(cid:48), y(cid:48) that is a non-trivial
distance from x, y and which will be accepted with a high id203. the candidate (x(cid:48), y(cid:48)) will have a
good chance to be accepted if h(x(cid:48), y(cid:48)) is close to h(x, y)     this can be achieved by following a contour of
equal    energy    h, as described in the next section.

hamiltonian dynamics
we wish to make an update x(cid:48) = x +    x, y(cid:48) = y +    y for small    x and    y such that the hamiltonian
h(x, y)     hx(x) + hy(y) is conserved,

h(x(cid:48)

, y(cid:48)

)     h(x, y)

we can satisfy this (up to    rst order) by considering the taylor expansion

h(x(cid:48)

, y(cid:48)

) = h(x +    x, y +    y)

    h(x, y) +    xt   xh(x, y) +    yt   yh(x, y) + o(cid:0)

|   x|2(cid:1) + o(cid:0)

conservation, up to    rst order, therefore requires

   xt   xh(x, y) +    yt   yh(x, y) = 0

|   y|2(cid:1)

(27.5.6)

(27.5.7)

(27.5.8)

this is a single scalar requirement, and there are therefore many di   erent solutions for    x and    y that
satisfy this single condition. it is customary to use hamiltonian dynamics, which corresponds to the setting:

   x =     yh(x, y),

   y =        xh(x, y)

570

(27.5.9)

draft november 9, 2017

auxiliary variable methods

(a)

(c)

(b)

(cid:81)

(a): current sample of states
figure 27.10: swendson-wang updating for p(x)    
(here on a nearest neighbour lattice). (b): like coloured neighbours are bonded together with id203
1    e     , forming clusters of variables. (c): each cluster is given a random colour, forming the new sample.

i   j exp   i [xi = xj].

where   is a small value to ensure that the taylor expansion is accurate. hence

x(t + 1) = x(t) +     yhy(y),

y(t + 1) = y(t)         xhx(x)

(27.5.10)

for the hmc method,    xh(x, y) =    xhx(x) and    yh(x, y) =    yhy(y). for the gaussian case,
   yhy(y) =    y so that

x(t + 1) = x(t)      y,

y(t + 1) = y(t)         xh(x)

(27.5.11)

there are speci   c ways to implement the hamiltonian dynamics called leapfrog discretisation that are more
accurate than the simple time-discretisation used above and we refer the reader to [221] for details.

in order to make a symmetric proposal distribution, at the start of the dynamic step, we choose   = + 0
or   =     0 uniformly. we then follow the hamiltonian dynamics for many time steps (usually of the order
of several hundred) to reach a candidate point x(cid:48), y(cid:48). if the hamiltonian dynamics is numerically accurate,
h(x(cid:48), y(cid:48)) will have roughly the same value as h(x, y). we then do a metropolis step, and accept the point
x(cid:48), y(cid:48) if h(x(cid:48), y(cid:48)) > h(x, y) and otherwise accept it with id203 exp(h(x(cid:48), y(cid:48))     h(x, y)). if rejected,

we take the initial point x, y as the sample. combined with the p(y) sample step, we then have the general
procedure as described in algorithm(27.4).

in hmc we use not just the potential hx(x) to de   ne candidate samples, but the gradient of hx(x) as
well. an intuitive explanation for the success of the algorithm is that it is less myopic than straightforward
metropolis since the gradient enables the algorithm to feel its way to other regions of high id203 by
contouring around paths in the augmented space. one can also view the auxiliary variables as momentum
variables     it is as if the sample has now a momentum which can carry it through the low-density x-regions.
provided this momentum is high enough, we can escape local regions of signi   cant id203, see    g(27.9).

27.5.2 swendson-wang

originally, the sw method was introduced to alleviate the problems encountered in sampling from ising
models close to their critical temperature[285]. at this point large islands of same-state variables form so
that strong correlations appear in the distribution     the scenario under which, for example, id150
is not well suited. the method has since been generalised to other models[95], although here we outline the
procedure for the ising model only, referring the reader to more specialised text for the extensions [39].

the ising model with no external    elds is de   ned on variables x = (x1, . . . , xn), xi     {0, 1} and takes the
form

e  i[xi=xj ]

(27.5.12)

(cid:89)

i   j

p(x) =

1
z

which means that this is a pairwise markov network with a potential contribution e   if neighbouring nodes
i and j on a square lattice are in the same state, and a contribution 1 otherwise. we assume that    > 0
which encourages neighbours to be in the same state. the lattice based neighbourhood structure makes
this di   cult to sample from, and especially when        0.9 which encourages large scale islands of same-state
draft november 9, 2017
571

algorithm 27.5 swendson-wang sampling

auxiliary variable methods

1, . . . , x1
n.

if xl   1

for each i, j in the edge set do

1: start from a random con   guration of all x1
2: for l = 2 to l do
3:
4:
5:
6:
7:
8: end for

i = xl   1

j

, we bind variables xi and xj with id203 1     e     .

end for
for each cluster formed from the above bonds, set the state of the cluster uniformly at random.
this gives a new joint con   guration xl

1, . . . , xl
n.

variables to form.

the aim is to remove the problematic terms e  i[xi=xj ] by the use of auxiliary real-valued    bond    variables,
yij, one for each edge on the lattice, making the conditional p(x|y) easy to sample from. this is given by
(27.5.13)

e  i[xi=xj ]

(cid:89)

p(x|y)     p(y|x)p(x)     p(y|x)

i   j

(cid:89)

i(cid:104)
0 < yij < e  i[xi=xj ](cid:105)
using p(y|x) we can cancel the terms e  i[xi=xj ] by setting
where i(cid:2)0 < yij < e  i[xi=xj ](cid:3) denotes a uniform distribution between 0 and e  i[xi=xj ]; zij is the normalisation

p(yij|xi, xj) =

p(y|x) =

(cid:89)

(27.5.14)

1
zij

i   j

i   j

constant zij = e  i[xi=xj ]. hence

0 < yij < e  i[xi=xj ](cid:105)
i(cid:104)

p(x|y)     p(y|x)p(x)
1
e  i[xi=xj ]

   

i(cid:104)
0 < yij < e  i[xi=xj ](cid:105)

(cid:89)
(cid:89)

i   j

i   j

   

e  i[xi=xj ]

(27.5.15)

(27.5.16)

(27.5.17)

we    rst assume we have a sample for all the bond variables {yij}. if yij > 1, then to draw a sample from
p(x|y), we must have 1 < e  i[xi=xj ], which means that xi and xj are constrained to be in the same state.
otherwise, if yij < 1, then this introduces no constraint on xi and xj. hence, wherever yij > 1, we bind xi
and xj to be in the same state, otherwise not.
(cid:2)0, e  (cid:3)(cid:1). a bond will occur if yij > 1, which occurs with id203
0 < yij < e  (cid:105)
i(cid:104)

state. then p(yij|xi = xj) = u(cid:0)yij|
(cid:90)    

to sample from the bond variables p(yij|xi, xj) consider    rst the situation that xi and xj are in the same

(27.5.18)

     

=

p(yij > 1|xi = xj) =

1
zij

yij =1

e       1
e   = 1     e

hence, if xi = xj, we bind xi and xj to be in the same state with id203 1   e     . on the other hand if xi
and xj are in di   erent states, p(yij|xi (cid:54)= xj) = u (yij| [0, 1]), and yij is uniformly distributed between 0 and 1.

figure 27.11: ten successive samples from a 25    25 ising model p(x)     exp
, with    =
0.88, close to the critical temperature. the swendson-wang procedure is used. starting in a random initial
con   guration, the samples quickly move away from this initial state. the samples display the characteristic
long-range correlations close to the critical temperature.

i   j   i [xi = xj]

(cid:16)(cid:80)

(cid:17)

572

draft november 9, 2017

auxiliary variable methods

figure 27.12: the full slice for a given y.
ideally slice sampling would
draw an x sample from anywhere on the full slice (green). in general this
is intractable for a complex distribution and a local approximate slice is
formed instead, see    g(27.13).

(a)

(c)

(b)

(d)

(a): for the current sample x, a point y is sampled between 0 and p   (x), giving a point
figure 27.13:
(x, y) (black circle). then an interval of width w is placed around x, the blue bar. the ends of the bar
(b): the interval is increased by w
denote if the point is in the slice (green) or out of the slice (red).
(c): given an interval a sample x(cid:48) is taken uniformly in the interval.
until it hits a point out of the slice.
if the candidate x(cid:48) is not in the slice (red), p(x(cid:48)) < y, the candidate is rejected and the interval is shrunk.
(d): the sampling from the interval is repeated until a candidate is in the slice (green), and is subsequently
accepted.

after doing this for all the xi and xj pairs, we will end up with a graph in which we have clusters of like-
state bonded variables. the algorithm then simply chooses a random state for each cluster     that is, with
id203 0.5 all variables in the cluster are in state 1. this is useful since we are able to update variables,
even if they are strongly dependent. this is in contrast to say id150 which when the variables
are strongly dependent will only very rarely change the state of a variable from its neighbours since it is
strongly encouraged to agree with them; this results in a critical slowing down e   ect and id150
essentially freezes. see algorithm(27.5),    g(27.10) and    g(27.11). this technique has found application in
spatial statistics, particularly image restoration[147].

27.5.3 slice sampling

slice sampling[223] is an auxiliary variable technique that aims to overcome some of the di   culties in choos-
ing an appropriate length scale in methods such as metropolis sampling. the brief discussion here follows
z p   (x) where the normalisation
that presented in [198] and [44]. we want to draw samples from p(x) = 1
constant z is unknown. by introducing the auxiliary variable y and de   ning the distribution

(cid:26) 1/z for 0     y     p   (x)
(cid:90) p   (x)

otherwise

0

p(x, y) =

we have(cid:90)

p(x, y)dy =

0

1
z

   

1
z

dy =

p

(x) = p(x)

(27.5.19)

(27.5.20)

which shows that the marginal of p(x, y) over y is equal to the distribution we wish to draw samples from.
hence if we draw samples from p(x, y), we can ignore the y samples and we will have a valid sampling scheme
for p(x).

to draw from p(x, y) we use id150,    rst drawing from p(y|x) and then from p(x|y). drawing a
sample from p(y|x) means that we draw a value y from the uniform distribution u (y| [0, p   (x)]). given

draft november 9, 2017

573

draw a vertical coordinate y uniformly from the interval(cid:0)0, p   (xi)(cid:1).

create a horizontal interval (xleft, xright) that contains xi as follows:
draw r     u (r| (0, 1))
xleft = xi     rw, xright = xi + (1     r)w
while p   (xleft) > y do

algorithm 27.6 slice sampling (univariate).

xleft = xleft     w

end while
while p   (xright) > y do
xright = xright + w

1: choose a starting point x1 and step size w.
2: for i = 1 to l do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end for

end if
end while
xi+1 = x(cid:48)

end if

else

modify the interval (xleft, xright) as follows:
if x(cid:48) > xi then
xright = x(cid:48)
xleft = x(cid:48)

else

end while
accept = false
while accept = false do
draw a random value x(cid:48) uniformly from the unit interval (xleft, xright).
if p   (x(cid:48)) > y then
accept = true

importance sampling

(cid:46) create an initial interval

(cid:46) step out left

(cid:46) step out right

(cid:46) found a valid sample

(cid:46) shrinking

a sample y, one then draws a sample x from p(x|y). using p(x|y)     p(x, y) we see that p(x|y) is the
distribution over x such that p   (x) > y:

   
p(x|y)     i [p

(x) > y]

(27.5.21)
that is, x is uniformly distributed from the    slice    that satis   es p   (x) > y,    g(27.12). computing the nor-
malisation of this distribution is in general non-trivial since we would in principle need to search over all
x to    nd those for which p   (x) > y. the challenge in slice sampling is therefore to sample from the slice.
this can be addressed using mcmc techniques. ideally we would like to get as much of the slice as feasible,
since this will improve the    mixing    of the chain (the rate that we converge to the stationary distribution).
if we concentrate on the part of the slice only very local to the current x, then the samples move through
the space very slowly. if we attempt to guess at random a point a long way from x and check if it is in the
slice, this will be mostly unsuccessful.

we will not discuss how to do this in general and highlight only how this can be achieved for a univariate
slice. the happy compromise presented in algorithm(27.6)[223] and described in    g(27.13) determines an
appropriate incremental widening of an initial interval. once the largest potential interval is determined
we attempt to sample from this. if the sample point within the interval is in fact not in the slice, this is
rejected and the interval is shrunk.

27.6 importance sampling

importance sampling is a technique to approximate averages with respect to an intractable distribution
p(x). the term    sampling    is arguably a misnomer since the method does not attempt to draw samples from

574

draft november 9, 2017

importance sampling

z where p   (x) can be evaluated but z =(cid:82)

p(x). rather the method draws samples from a simpler importance distribution q(x) and then reweights
them such that averages with respect to p(x) can be approximated using the samples from q(x). consider
p(x) = p   (x)
x p   (x) is an intractable normalisation constant. the
average of f (x) with respect to p(x) is given by

let x1, . . . , xl be samples from q(x). we can then approximate the above average by

f (x)p(x) =

x

(cid:90)

(cid:90)

(cid:82)
(cid:82)
x f (x) p   (x)
p   (x)
q(x) q(x)

x

q(x) q(x)

=

(cid:82)
(cid:82)
x f (x)p   (x)
x p   (x)
(cid:80)l
(cid:80)l
l=1 f (xl) p   (xl)

q(xl)

p   (xl)
q(xl)

l=1

f (x)p(x)    

x

=

f (xl)wl

where we de   ne the normalised importance weights

(cid:80)l

p   (xl)/q(xl)
l=1 p   (xl)/q(xl)

wl =

,

with

wl = 1

l(cid:88)

l=1

l(cid:88)

l=1

in principle, reweighing the samples from q will give the correct result for the average with respect to p.

the importance weight vector w can be seen as a measure of how well q    ts p since for q = p, the weight
vector has uniform values, w = 1/l. hence the more variable the components of the weight vector are,
the less well-matched q is to p. since the weight is a measure of how well q matches p, unless q and p are
well-matched, there will typically be only a small number of dominant weights. this is particularly evident
in high dimensions. as an indication of this e   ect, consider a d-dimensional multivariate x. using u as
the vector of unnormalised importance weights with ui = p(xi)/q(xi), a measure of the variability of two
components of u is given by (dropping momentarily the notational dependence on x)

(cid:11) +(cid:10)u2

j

(cid:11)

    2(cid:104)ui(cid:105)(cid:104)uj(cid:105)

where the averages are with respect to q. the mean unnormalised weights are

q(x) = 1

and (cid:10)u2
for simplicity we assume that q(x) and p(x) both factorise, p(x) = (cid:81)d

q2(x)

q(x)

p(x)

q(x)

=

j

i

(cid:28) p(x)

(cid:29)

(cid:29)

further that the distributions are axis-aligned, being the same for each co-ordinate d. then

d=1 p(xd), q(x) = (cid:81)d

d=1 q(xd) and

i

x

(cid:68)

p(x)
q(x)

(cid:104)ui(cid:105) = (cid:104)uj(cid:105) =

(ui     uj)2(cid:69)

=(cid:10)u2
(cid:90)
(cid:28) p2(x)
(cid:11) =(cid:10)u2
(cid:11) =
(cid:28) p(x)
(cid:29)d
(cid:11) =
(cid:10)u2
(cid:69)
(cid:68) p(x)
(ui     uj)2(cid:69)
(cid:68)

q(x)

= 2

p(x)

p(x)

q(x)

i

(cid:32)(cid:28) p(x)

q(x)

(cid:29)d
p(x)     1

(cid:33)

since

> 1 for q (cid:54)= p (this is the 2-divergence, exercise(8.37)), the variability of the weights is

which grows exponentially with the dimension d. this means that a single unnormalised importance weight
is likely to dominate. after normalisation, typically, therefore in high dimensions the weight vector w will
have only a single signi   cantly non-zero component.

a method that can help address this weight dominance is resampling. given the weight distribution
w1, . . . , wl, one draws a set of l sample indices. this new set of indices will almost certainly contain
repeats since any of the original low-weight samples will most likely not be included. the weight of each of
these new samples is set uniformly to 1/l. this procedure helps select only the       ttest    of the samples and
is known as sampling importance resampling[254].

draft november 9, 2017

575

(27.6.1)

(27.6.2)

(27.6.3)

(27.6.4)

(27.6.5)

(27.6.6)

(27.6.7)

(27.6.8)

importance sampling

h1

v1

h2

v2

h3

v3

h4

v4

figure 27.14: a dynamic belief network. in many ap-
plications of interest, the emission distribution p(vt|ht)
is non-gaussian, leading to the formal intractability of
   ltering/smoothing.

27.6.1 sequential importance sampling

1:t from q(x1:t). if we assume that for the previous time, we have a set of paths xl

our interest is to apply importance sampling to temporal distributions p(x1:t) for which the importance
samples from q(x1:t) are paths x1:t. when a new observation at time t arrives, we require a set of sample
paths xl
1:t   1 along with
their importance weights wl
t by simply sampling from the
transition q(xt|x1:t   1) and updating the weights, without needing to explicitly sample whole new paths. to
see this, consider the unnormalised importance weights for a sample path xl

t   1, we can    nd the new importance weights wl

1:t

  wl

t =

p   (xl
1:t)
q(xl
1:t)

=

p   (xl
q(xl

1:t   1)
1:t   1)

p   (xl
1:t)
1:t   1)q(xl
t|xl

p   (xl

1:t   1)

,

  wl

1 =

p   (xl
1)
q(xl
1)

(27.6.9)

using p(x1:t) = p(xt|x1:t   1)p(x1:t   1), we can ignore constants and equivalently de   ne the unnormalised
weights recursively using

  wl

t =   wl

t   1  l
t,

where

  l
t    

p   (xl
q(xl

t|xl
1:t   1)
t|xl
1:t   1)

t > 1

(27.6.10)

(27.6.11)

this means that in sequential importance sampling (sis) we need only de   ne the conditional impor-
tance distribution q(xt|x1:t   1). the ideal setting of the sequential importance distribution is q(xt|x1:t   1) =
p(xt|x1:t   1), although this choice is impractical in most cases, as we discuss below. sequential importance
sampling is also known as particle    ltering.

dynamic belief networks

consider distributions with a hidden markov independence structure, see    g(27.14),

p(v1:t, h1:t) = p(v1|h1)p(h1)

t(cid:89)

t=2

(cid:124) (cid:123)(cid:122) (cid:125)

p(vt|ht)

emission

(cid:124)
(cid:123)(cid:122)
(cid:125)
p(ht|ht   1)

transition

(27.6.12)

(27.6.13)

(27.6.14)

where v1:t are observations and h1:t are the random variables. our interest is to draw sample paths h1:t
given the observations v1:t. in some models, such as the id48, this is straightforward. however, in other
cases, for example when the emission p(vt|ht) is intractable to normalise, we can use sis to draw samples
instead. for dynamic belief networks, equation (27.6.11) simpli   es to

  l
t    

p(vt|hl
q(hl

t|hl
1:t   1)

t)p(hl
t|hl

t   1)

the optimal importance distribution is achieved by using the importance transition

q(ht|h1:t   1)     p(vt|ht)p(ht|ht   1)

it is often the case, however, that drawing samples from this optimal q(ht|h1:t   1)
the transition is easy to sample from, a common sequential importance distribution is

is di   cult. in cases where

@@

q(ht|h1:t   1) = p(ht|ht   1)

in which case, from equation (27.6.13),   l
by

(27.6.15)
t = p(vt|ht) and the unnormalised weights are recursively de   ned

  wl

t =   wl

t   1p(vt|hl
t)

(27.6.16)

a drawback of this procedure is that after a small number of iterations only very few particle weights will be
signi   cantly non-zero due to the mismatch between the importance distribution q and the target distribution
p. this can be addressed using resampling, as described in section(27.6)[154, 88].

576

draft november 9, 2017

importance sampling

27.6.2 particle    ltering as an approximate forward pass

particle    ltering can be viewed as an approximation to the exact    ltering recursion. using    to represent
the    ltered distribution,

  (ht)     p(ht|v1:t)

the exact    ltering recursion is

(cid:90)

  (ht)     p(vt|ht)

p(ht|ht   1)  (ht   1)

ht   1

(27.6.17)

(27.6.18)

a pf can be viewed as an approximation of equation (27.6.18) in which the message   (ht   1) is approximated
by a sum of delta-spikes:

l(cid:88)

(cid:17)

  (ht   1)    

wl

t   1  

ht   1, hl

t   1

l=1

t   1 are the normalised importance weights (cid:80)l

(cid:16)

l(cid:88)

l=1

where wl
t   1 are the particles. in other
words, the    message is represented as a weighted mixture of delta-spikes where the weight and position of
the spikes are the parameters of the distribution. using equation (27.6.19) in equation (27.6.18), we have

t   1 = 1, and hl

l=1 wl

  (ht)    

1
z

p(vt|ht)

p(ht|hl

t   1)wl

t   1

(27.6.20)

the constant z is used to normalise the distribution   (ht). although   (ht   1) was a simple sum of delta-
spikes, in general   (ht) will not be     the delta-spikes get    broadened    by the transition and emission factors.
our task is then to approximate   (ht) as a new sum of delta-spikes. below we discuss a method to achieve
this for which explicit knowledge of the normalisation z is not required. this is useful since in many tracking
applications the normalisation of the emission p(vt|ht) is unknown.

a monte-carlo sampling approximation

a simple approach to forming an approximate mixture-of-delta functions representation of equation (27.6.20)
is to generate a set of points using sampling. in principle any sampling method can be used, including pow-
erful mcmc approaches.

in particle    lters, importance sampling is use to generate the new particles. that is we generate a set
of samples h1
from some importance distribution q(ht) which gives the unnormalised importance
weights

t , . . . , hl
t

t)(cid:80)l

p(vt|hl

  wl

t =

t   1)wl(cid:48)
t|hl(cid:48)
t   1

l(cid:48)=1 p(hl
q(hl
t)

de   ning the normalised weights:

wl

t =

t

  wl
l(cid:48)   wl(cid:48)

t(cid:80)
l(cid:88)

we obtain an approximation

  (ht)    

l=1

wl
t  

ht, hl
t

(cid:17)

(cid:16)

l(cid:88)

l=1

q(ht)     p(vt|ht)

p(ht|hl

t   1)wl

t   1

draft november 9, 2017

(27.6.19)

(27.6.21)

(27.6.22)

(27.6.23)

(27.6.24)

577

ideally one would use the importance distribution that makes the importance weights uniform, namely

importance sampling

figure 27.15: tracking an object with
a particle    lter containing 50 parti-
cles. the small circles are the par-
ticles, scaled by their weights. the
correct corner position of the face is
given by the         , the    ltered average
by the large circle    o   , and the most
likely particle by    +   .
ini-
tial position of the face without noise
and corresponding weights of the par-
(b): face with noisy back-
ticles.
ground and the tracked corner posi-
tion after 20 timesteps. the forward-
sampling-resampling pf method is
used to maintain a healthy pro-
portion of non-zero weights.
see
demoparticlefilter.m

(a):

(a)

(b)

l(cid:88)

l=1

however, this is often di   cult to sample from directly due to the unknown normalisation of the emission
p(vt|ht). a simpler alternative is to sample from the transition mixture:

q(ht) =

p(ht|hl

t   1)wl

t   1

(27.6.25)

to do so, one    rst samples a component l    from the histogram with weights w1
sample index, say l   , one then draws a sample from p(ht|hl   

t   1).

t   1. given this
in this case the unnormalised weights

t   1, . . . , wl

become simply

  wl
t = p(vt|hl
t)

(27.6.26)

this forward-sampling-resampling procedure is used in demoparticlefilter.m and in the following toy
example.

example 27.2 (a toy face-tracking example). at time t a binary face template is in a two-dimensional
location ht, which describes the upper-left corner of the template. at time t = 1 the position of the face is
known, see    g(27.15a). in subsequent times the face moves randomly according to

ht = ht   1 +     t

(27.6.27)

where   t     n (  t 0, i) is a two dimensional zero mean unit covariance noise vector. in addition, a fraction
of the binary pixels in the whole image are selected at random and their states    ipped. the aim is to try
to track the upper-left corner of the face through time.

we need to de   ne the emission distribution p(vt|ht) on the binary pixels with vi     {0, 1}. consider the
following compatibility function

  (vt, ht) = vt

t   v(ht)

578

(27.6.28)

draft november 9, 2017

510152025303540510152025303540102030405000.010.020.030.040.050.060.07particle weights510152025303540510152025303540102030405000.010.020.030.040.050.060.070.080.09particle weightsimportance sampling

@@

@@

@@

where   v(ht) is the vector representing the whole image with a clean face placed at position ht and zeros
outside of the face template. then   (vt, ht) measures the overlap between the face template at a speci   c
location and the noisy image restricted to the template pixels. the compatibility function is maximal when
the observed image vt has the face placed at position ht. we can

therefore de   ne

p(vt|ht)    

tht
vt
1tht

(27.6.29)

a subtlety is that ht is continuous, and in the compatibility function we    rst map ht to the nearest integer
pixel
representation. in    g(27.15a) 50 particles are used to track the face. the particles are plotted along
with their corresponding weights. for each t > 1, 5% of the pixels are selected at random in the image
and their states    ipped. using the forward-sampling-resampling method we can successfully track the face
despite the presence of the background clutter.

real tracking applications involve complex issues, including tracking multiple objects, transformations of
the object (scaling, rotation, morphology changes). nevertheless, the principles are largely the same and
many tracking applications work by seeking compatibility functions, often based on the colour histogram in
a template.

27.7 summary

    exact sampling can be achieved for models such as belief networks, although forward sampling in the case

of evidence can be ine   cient.

    provided independent samples are drawn from a distribution, only a small number of samples is required to
obtain a good estimate of an expectation. however, drawing independent samples from high-dimensional
non-standard distributions is computationally extremely di   cult.

    id115 methods are approximate sampling methods which converge to drawing samples
from the correct distribution in the limit of a large number of samples. whilst powerful, assessing conver-
gence of the method can be di   cult. also, samples are often highly dependent, so that a great number of
samples may be required to obtain a reliable estimate of an expectation.

27.8 code

potsample.m: exact sample from a set of potentials
ancestralsample.m: ancestral sample from a belief network
jtsample.m: sampling from a consistent junction tree
gibbssample.m: id150 from a set of potentials
demometropolis.m: demo of metropolis sampling for a bimodal distribution
metropolis.m: metropolis sample
logp.m: log of a bimodal distribution
demoparticlefilter.m: demo of particle    ltering (forward-sampling-resampling method)
placeobject.m: place an object in a grid
compat.m: compatibility function
demosampleid48.m: naive id150 for a id48

draft november 9, 2017

579

exercises

(27.9.1)

(27.9.2)

exercise 27.1 (box-muller method). let x1     u (x1| [0, 1]), x2     u (x2| [0, 1]) and

   2 log x1 cos 2  x2,

y2 =

   2 log x1 sin 2  x2

(cid:112)

27.9 exercises

(cid:112)

y1 =

show that

(cid:90)

p(y1, y2) =

p(y1|x1, x2)p(y2|x1, x2)p(x1)p(x2)dx1dx2 = n (y1 0, 1)n (y2 0, 1)

and suggest an algorithm to sample from a univariate normal distribution. hint: use the change of variable
result, result(8.1), for vectors y = (y1, y2) and x = (x1, x2).

exercise 27.2. consider the distribution p(x)     exp(sin(x)) for           x       . using rejection sampling
with q(x) = n

(cid:0)x 0,   2(cid:1) show that a suitable value for m such that p   (s)/q(x)     m is

m = e1+   2

2  2    2    2

(27.9.3)

for a suitably chosen   2 draw 10,000 samples from p(x) and plot the resulting histogram of the samples.

exercise 27.3. consider the distribution

p(x1, . . . , x6) = p(x1)p(x2)p(x3|x1, x2)p(x4|x3)p(x5|x3)p(x6|x4, x5)

(27.9.4)
for x5    xed in a given state x5, write down a distribution on the remaining variables p(cid:48)(x1, x2, x3, x4, x6)
and explain how forward (ancestral) sampling can be carried out for this new distribution.

exercise 27.4. consider an ising model on an m    m square lattice with nearest neighbour interactions:
(27.9.5)

i [xi = xj]

(cid:88)

i   j

p(x)     exp   

now consider the m    m grid as a checkerboard, and give each white square a label wi, and each black
square a label bj, so that each square is associated with a particular variable. show that

p(b1, b2, . . . ,|w1, w2, . . .) = p(b1|w1, w2, . . .)p(b2|w1, w2, . . .) . . .

(27.9.6)

that is, conditioned on the white variables, the black variables are independent. the converse is also true,
that conditioned on the black variables, the white variables are independent. explain how this can be exploited
by a id150 procedure. this procedure is known as checkerboard or black and white sampling.

exercise 27.5. consider the symmetric gaussian proposal distribution

and the target distribution

q i(cid:1)
(cid:0)x(cid:48) x,   2
(cid:0)x 0,   2
pi(cid:1)
(cid:29)

  q(x(cid:48)|x)

=    

  q(x(cid:48)

|x) = n

p(x) = n

(cid:28)

p(x(cid:48))
p(x)

log

n   2
q
2  2
p

where dim (x) = n . show that

(27.9.7)

(27.9.8)

(27.9.9)

discuss how this result relates to the id203 of accepting a metropolis-hastings update under a gaussian
proposal distribution in high-dimensions.

580

draft november 9, 2017

exercises

exercise 27.6. the    le demosampleid48.m performs naive id150 of the posterior p(h1:t|v1:t ) for
a id48 for t = 10. at each gibbs update a single variable ht is chosen, with the remaining h variables
clamped. the procedure starts from t = 1 and sweeps forwards through time. when the end time t = t
is reached, the joint state h1:t is taken as a sample from the posterior. the parameter    controls how
deterministic the hidden transition matrix p(ht|ht   1) will be. adjust demosampleid48.m to run 100 times
for the same   , computing a mean absolute error for the posterior marginal p(ht|v1:t ) over these 100 runs.
then repeat this for    = 0.1, 1, 10, 20. finally, repeat this whole procedure 20 times for di   erent random
draws of the transition and emission matrices, and average the errors in computing the smoothed posterior
marginal by id150. discuss why the performance of this id150 routine deteriorates with
increasing   .

exercise 27.7. consider the following matlab code snippet:

c=p(1); i=1; r=rand;
while r>c && i<n

i=i+1; c=c+p(i);

end
sample=i;

1. explain why this draws a sample state i from a discrete distribution with id203 p(i), i = 1, . . . , n.

2. explain how to e   ectively sample from the distribution p(i) = e       i/i!, 0 <    < 1, i = 0, . . . ,    which

contains an in   nite number of discrete states.

this exercise discusses a technique called approximate bayesian computation (abc) which
exercise 27.8.
(cid:48) can be gener-
aims to provide an approximation to the parameter posterior p(  |d) in cases where a sample d
|  ), even if the likelihood cannot be calculated up to some unknown normalisation
ated from the likelihood p(d
constant. for example, if we specify a model such as xt+1 = f (xt|  )+  t,   t     n (  t 0, 1), we can easily sam-
ple a dataset d
|  ).
consider drawing samples from a posterior p(  |d)     p(d|  )p(  ).

(cid:48) = {x1, . . . , xt} from this, without needing to specify the normalisation constant of p(d

(cid:48)

(cid:48)

1. show that for
(cid:48)

q(d,d

,   ) = q(d|d
the marginal distribution is

)p(d

(cid:48)

(cid:48)

|  )p(  ), where q(d|d

(cid:48)

d     d

) =   (cid:0)

(cid:48)(cid:1)

q(  |d) = p(  |d)

2. hence show that the following procedure generates a sample from p(  |d):

(a) sample    from p(  )
(b) sample a    candidate    dataset d
(c) if d

(cid:48) from p(d

(cid:48)

|  )

(cid:48) = d accept the candidate as a sample, otherwise make no sample and go to (a).

3. one may relax the delta-function constraint and use, for example, q(d|d

chosen   2. then

(cid:88)

l

(cid:16)

         l(cid:17)

p(  |d)    q(  |d) =

wl  

where wl    

(cid:80)l
q(d|dl)
l=1 q(d|dl)

(cid:0)

(cid:48),   2i(cid:1) for some

(cid:48)) = n

d d

(27.9.12)

explain how this approximate sampling procedure is related to the parzen estimator.
requires only the ability to sample a dataset d
of p(d

note that this
(cid:48) given        we do not explicitly require the normalisation

|  ) to do this.

(cid:48)

4. by considering a single d-dimensional datapoint x and sampled datapoint xi, both generated from the

(cid:0)x 0,   2id  d

(cid:1) show that for the unnormalised weight

same underlying distribution n

(cid:18)

(cid:0)x     xi(cid:1)2(cid:19)

ui = exp

1
2  2

   

draft november 9, 2017

(27.9.10)

(27.9.11)

(27.9.13)

581

++

++

++

the typical ratio of the weights can be assessed using

(cid:42)(cid:18)

(cid:19)2(cid:43)

log

ui
uj

n (x 0,  2i)n (xi 0,  2i)n (xj 0,  2i)

= 12d

  4
  4

exercises

(27.9.14)

explain why the normalised weights will typically be dominated by a single component, rendering the
sampling method described in part (3) above generally impractical.
this shows the    curse of dimen-
sionality    for methods such as abc, meaning that in general such techniques need to be used with
considerable care in high dimensional parameter spaces.

exercise 27.9.
soccer teams aces and bruisers are arch rivals. they have played each other already 20
times this season and will play one more    nal game of the season. the goalkeepers of both teams have
been the same all season and will also be the same for the    nal game. however, each team has a squad of
20 players, with 10 players being selected from each squad to make up a team of 11 players (10 plus the
goalkeeper) each game. the    le soccer.mat contains the history of the team sheets and which team won
the game (+1 for an aces win over bruisers and    1 for a loss). both team coaches use the universal rating
system for a player:

2 genius
1 superb
0 very average
   1 muppet
   2 worse than your grandma

the outcome (which assumes both goalkeepers are equally matched in their abilities), independently for each
game, is modelled by

p(aces beats bruisers|ta, tb, a, b) =   

i     btb
ata

i

(27.9.15)

where   (x) = 1/(1 + exp(   x)). here aj     {   2,   1, 0, 1, 2}, j = 1, . . . , 20 is the ability of player j in aces
and bj     {   2,   1, 0, 1, 2}, j = 1, . . . , 20 is the ability of player j in bruisers. the 10 players selected to play
in the aces team is given by ta

i     {1, . . . , 20}, and similarly for bruisers, tb

i     {1, . . . , 20}.

1. which are the 10 best players for aces and 10 best for bruisers?

2. given that we know the bruisers will    eld players numbered 1 to 10, what would be the best team of
players that aces can    eld to maximise their chance of winning? how does this relate to selecting the
10 best aces players above?

(cid:32) 10(cid:88)

(cid:16)

i=1

(cid:17)(cid:33)

exercise 27.10.
consider a setting in which there are d diseases and a patient can either have or not
have a particular disease di     {0, 1}, i = 1, . . . , d. here di = 1 means that the patient has disease i; di = 0
means the patient does not have disease i. a patient may have more than one disease. there are a set of
s symptoms the hospital can measure; if sj = 1, j = 1, . . . , s then the patient has symptom j; otherwise
sj = 0 means that the patient does not have symptom j. a simple disease-symptom network is given by

++

++

++

d(cid:89)

i=1

s(cid:89)

j=1

(cid:17)

p(s1, . . . , ss, d1, . . . , dd) =

p(sj|d)

where d = (d1, . . . , dd)t and

p(sj = 1|d) =   

wt

j d + bj

(cid:16)

p(di)

(27.9.16)

(27.9.17)

in the above   (x) is the standard logistic sigmoid function 1/(1 + exp(   x)); wj is a vector of parameters
relating symptom j to the diseases and bj is related to the prevalence of the symptom. the hospital provides
the collection of parameters w and b, the prior disease probabilities p (with p(di = 1) = pi) and a vector s

582

draft november 9, 2017

exercises

of symptoms for the patient, see symptomdiseasepars.mat. use id150 (using a reasonable amount
of burn-in and sub-sampling) to estimate the vector

[p(d1 = 1|s), . . . , p(dd = 1|s)]

(27.9.18)

++

exercise 27.11.
for a disease-symptom network similar to the previous question and a collection of n
patient records d = {(sn, dn) , n = 1, . . . , n}, derive the following for the prediction of the diseases for a
new patient with symptoms s:

(cid:90)

p(d|s,d) =

where

p(w, b, p|d)     p(w, b, p)

and explain how you could estimate

p(di = 1|s,d)

using sampling.

p(d|s, w, b, p)p(w, b, p|d)

w,b,p

n(cid:89)

n=1

p(sn|dn, w, b)p(dn|p)

(27.9.19)

(27.9.20)

(27.9.21)

draft november 9, 2017

583

exercises

584

draft november 9, 2017

chapter 28

deterministic approximate id136

sampling methods are popular and well known for approximate id136.
in this chapter we give an
introduction to the less well known class of deterministic approximation techniques. these have been
spectacularly successful in branches of the information sciences and many have their origins in the study of
large-scale physical systems.

28.1 introduction

deterministic approximate id136 methods are an alternative to the sampling techniques discussed in
chapter(27). drawing exact independent samples is typically computationally intractable and assessing the
quality of the sample estimates is di   cult. in this chapter we discuss some alternatives. the    rst, laplace   s
method, is a simple perturbation technique. the second class of methods are those that produce rigorous
bounds on quantities of interest. such methods are interesting since they provide certain knowledge     it
may be su   cient, for example, to show that a marginal id203 is greater than 0.1 in order to make an
informed decision. a further class of methods are the consistency methods, such as loopy belief propagation.
such methods have revolutionised certain    elds, including error correction[198]. it is important to bear in
mind that no single approximation technique, deterministic or stochastic, is going to beat all others on all
problems, given the same computational resources. in this sense, insight as to the properties of the various
approximations is useful in matching an approximation method to the problem at hand.

28.2 the laplace approximation

consider a distribution on a continuous variable of the form

p(x) =

   e(x)
e

1
z

(28.2.1)

the laplace method makes a gaussian approximation of p(x) based on a local perturbation expansion
around a mode x   . first we    nd the mode numerically, giving

x   

= argmin

x

e(x)

(28.2.2)

then a taylor expansion up to second order around this mode gives

e(x)     e(x   

) + (x     x   

)t    e|x    +

(28.2.3)
where h           e(x)|x    is the hessian evaluated at the mode. at the mode,    e|x    = 0, and an approxima-
tion of the distribution is given by the gaussian

)

)t h (x     x   

q(x) =

1
zq

2 (x   x   )th(x   x   ) = n
    1
e

(28.2.4)

1
2

(x     x   
, h   1(cid:1)
(cid:0)x x   

585

which has mean x    and covariance h   1, with zq =
estimate the integral

(cid:90)

(cid:90)

e

   e(x)    

x

e

x

   e(x   )    1

2 (x   x   )th(x   x   ) = e

det (2  h   1)

(cid:112)
   e(x   )(cid:112)

properties of kullback-leibler variational id136

det (2  h   1). we can use the above expansion to

(28.2.5)

the laplace gaussian    t to a distribution is not necessarily the    best    gaussian approximation. as we   ll see
below, other criteria, such as based on minimal kl divergence between p(x) and a gaussian approximation
may be more appropriate, depending on the context. a bene   t of laplace   s method is its relative simplicity
compared with other approximate id136 techniques.

28.3 properties of kullback-leibler variational id136

variational methods can be used to approximate a complex distribution p(x) by a simpler distribution q(x).
given a de   nition of discrepancy between an approximation q(x) to p(x), any free parameters of q(x) are
then set by minimising the discrepancy. this class of techniques are also called    mean    eld    methods in the
physics literature.

a particularly popular measure of the discrepancy between an approximation q(x) and the intractable
distribution p(x) is the id181

kl(q|p) = (cid:104)log q(cid:105)q     (cid:104)log p(cid:105)q

(28.3.1)

it is straightforward to show that kl(q|p)     0 and is zero if and only if the distributions p and q are identical,
see section(8.2.1). note that whilst the kl divergence cannot be negative, there is no upper bound on the
value it can potentially take so that the discrepancy can be    in   nitely    large.

28.3.1 bounding the normalisation constant

for a distribution of the form

p(x) =

1
z

e  (x)

we have

kl(q|p) = (cid:104)log q(x)(cid:105)q(x)     (cid:104)log p(x)(cid:105)q(x) = (cid:104)log q(x)(cid:105)q(x)     (cid:104)  (x)(cid:105)q(x) + log z

since kl(q|p)     0 this immediately gives the bound

log z        (cid:104)log q(x)(cid:105)q(x)

+(cid:104)  (x)(cid:105)q(x)

id178

energy

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:123)(cid:122)

(cid:124)

(28.3.2)

(28.3.3)

(28.3.4)

which is called the    free energy    bound in the physics community [255]. the kl(q|p) method provides
therefore a lower bound on the normalisation constant. the art is then to choose a class of approximating
distributions q such that both its id178 and the energy term are tractably computable.

in bayesian modelling the likelihood of the model m with parameters    generating data d is given by

28.3.2 bounding the marginal likelihood

(cid:90)

p(d|m) =

(cid:124)

(cid:123)(cid:122)

p(d|  ,m)

likelihood

  

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

p(  |m)

prior

(28.3.5)

this quantity is fundamental to model comparison. however, in cases where    is high-dimensional, the
integral over    is often di   cult to perform. using bayes    rule,

p(  |d,m) =

p(d|  ,m)p(  |m)

p(d|m)

586

(28.3.6)

draft november 9, 2017

properties of kullback-leibler variational id136

figure 28.1: fitting a mixture of gaussians p(x) (blue) with a single gaus-
sian. the green curve minimises kl(q|p) corresponding to    tting a local
model. the red curve minimises kl(p|q) corresponding to moment match-
ing.

and considering

kl(q(  )|p(  |d,m)) = (cid:104)log q(  )(cid:105)q(  )     (cid:104)log p(  |d,m)(cid:105)q(  )

= (cid:104)log q(  )(cid:105)q(  )     (cid:104)log p(d|  ,m)p(  |m)(cid:105)q(  ) + log p(d|m)

the non-negativity of the id181 gives the bound

log p(d|m)        (cid:104)log q(  )(cid:105)q(  ) + (cid:104)log p(d|  ,m)p(  |m)(cid:105)q(  )

= (cid:104)log p(d|  ,m)(cid:105)q(  )     kl(q(  )|p(  |m))

(28.3.7)

(28.3.8)

(28.3.9)

(28.3.10)

this bound holds for any distribution q(  ) and becomes equality when q(  ) = p(  |d,m). since using
the optimal setting is assumed computationally intractable, the idea in variational bounding is to choose a
distribution family for q(  ) for which the bound is computationally tractable, and then maximise the bound
with respect to any free parameters of q(  ). the resulting bound then can be used as a surrogate for the
exact marginal likelihood in model comparison.

28.3.3 bounding marginal quantities

the kullback-leibler approach provides a lower bound on normalisation constants. combined with an upper
bound, obtained using alternative methods, see for example [308] and exercise(28.6), we are able to bracket
marginals l     p(xi)     u, see exercise(28.9). the tightness of the resulting bracket gives an indication as to
how tight the bounding procedures are. even in cases where the resulting bracket is weak     for example it
might be that the result is that 0.1 < p(cancer = true) < 0.99, this may be su   cient for decision making
purposes since the id203 of cancer is su   ciently large to merit action.

28.3.4 gaussian approximations using kl divergence
minimising kl(q|p)
using a simple approximation q(x) of a more complex distribution p(x) by minimising kl(q|p) tends to give
a solution for q(x) that focuses on a local mode of p(x), thereby underestimating the variance of p(x). to
show this, consider approximating a mixture of two gaussians with equal variance   2,

(cid:0)x   ,   2(cid:1)(cid:1)

n

(cid:0)
(cid:0)x       ,   2(cid:1) + n
(cid:0)x m, s2(cid:1)

p(x) =

1
2

q(x) = n

see    g(28.1), with a single gaussian

we wish to    nd the optimal m, s2 that minimise

kl(q|p) = (cid:104)log q(x)(cid:105)q(x)     (cid:104)log p(x)(cid:105)q(x)

(28.3.11)

(28.3.12)

(28.3.13)

if we consider the case that the two gaussian components of p(x) are well separated,    (cid:29)   , then by setting
q(x) to be centred on the left mode at       the gaussian q(x) only has appreciable mass close to      , so that
the second mode at    has negligible contribution to the id181. in this sense one can
approximate p(x)     1

2 q(x), so that

kl(q|p)     (cid:104)log q(x)(cid:105)q(x)     (cid:104)log p(x)(cid:105)q(x) = log 2

draft november 9, 2017

(28.3.14)

587

   30   20   10010203000.050.10.150.20.250.30.350.4properties of kullback-leibler variational id136

representing a distribution of the form(cid:81)

figure 28.2: a planar pairwise markov random    eld on a set of variables x1, . . . , x25,
i   j   (xi, xj). in statistical physics such lat-
tice models include the ising model on binary    spin    variables xi     {+1,   1} with
  (xi, xj) = ewij xixj .

on the other hand, setting m = 0, which is the correct mean of the distribution p(x), very little of the mass
of the mixture is captured unless s2 is large, giving a poor    t and large kl divergence. another way to view
this is to consider kl(q|p) = (cid:104)log q(x)/p(x)(cid:105)q(x); provided q is close to p around where q has signi   cant mass,
of order 1 and the kl divergence small. setting m = 0 means that q(x)/p(x) is
the ratio q(x)/p(x) will be
large where q has signi   cant mass, and is therefore a poor    t. the optimal solution in this case is therefore
to place the gaussian close to a single mode. however, for two modes that are less well-separated, the
optimal solution will not necessarily be to place the gaussian around a local mode. in general, the optimal
gaussian    t needs to be determined numerically     that is, there is no closed form solution to    nding the
optimal mean and (co)variance parameters.

@@

minimising kl(p|q)
bearing in mind that in general kl(q|p) (cid:54)= kl(p|q), it is also useful to understand the properties of kl(p|q).
for    tting a gaussian q(x) = n

(cid:0)x m, s2(cid:1) to p based on kl(p|q), we have

kl(p|q) = (cid:104)log p(x)(cid:105)p(x)     (cid:104)log q(x)(cid:105)p(x) =    

1
2s2

minimising this with respect to m and

s2 we obtain:

s2 =(cid:10)(x     m)2(cid:11)

p(x)

m = (cid:104)x(cid:105)p(x) ,

log s2 + const.

(28.3.15)

1
2

@@

(28.3.16)

(cid:68)

(x     m)2(cid:69)

p(x)    

so that the optimal gaussian    t matches the    rst and second moments of p(x).

in the case of    g(28.1), the mean of p(x) is a zero, and the variance of p(x) is large. this solution is
therefore dramatically di   erent from that produced by    tting the gaussian using kl(q|p). the    t found
using kl(q|p) focusses on making q    t p well locally, see also exercise(28.17), whereas kl(p|q) focusses on
making q    t p well to the global statistics of the distribution (possibly at the expense of a good local match).

28.3.5 marginal and moment matching properties of minimising kl(p|q)

for simplicity, consider a factorised approximation q(x) =(cid:81)

i q(xi). then

kl(p|q) = (cid:104)log p(x)(cid:105)p(x)    

(cid:104)log q(xi)(cid:105)p(xi)

(28.3.17)

(cid:88)

i

the    rst entropic term is independent of q(x) so that up to a constant independent of q(x), the above is

kl(p(xi)|q(xi))

(28.3.18)

(cid:88)

i

so that optimally q(xi) = p(xi). that is, the optimal factorised approximation is to set the factors of q(xi)
to the marginals of p(xi), exercise(28.13).

another approximating distribution class that yields a known form for the approximation is the exponential
family. in this case minimising kl(p|q) corresponds to moment matching, see exercise(28.13). in practice,
one generally cannot compute the moments of p(x) (since the distribution p(x) is considered    intractable   ),
so that    tting q to p based only on kl(p|q) does not itself lead to a practical algorithm for approximate
id136. nevertheless, as we will see, it is a useful subroutine for local approximations, in particular
expectation propagation.

588

draft november 9, 2017

variational bounding using kl(q|p)

figure 28.3: a distribution on pixels. the    lled nodes indicate observed noisy pixels,
the unshaded nodes a markov random field on latent clean pixels. the task is to infer
the clean pixels given the noisy pixels. the mrf encourages the posterior distribution
on the clean pixels to contain neighbouring pixels in the same state.

28.4 variational bounding using kl(q|p)

in this section we discuss how to    t a distribution q(x) from some assumed family to an    intractable   
distribution p(x). as we saw above for the case of    tting gaussians, the optimal q needs to be found
numerically. this itself can be a complex task (indeed, formally this can be just as di   cult as performing
id136 directly with the intractable p) and the reader may wonder why we trade a di   cult id136
task for a potentially di   cult optimisation problem. the general idea is that the optimisation problem has
some local smoothness properties that enable one to rapidly    nd a reasonable optimum based on generic
optimisation methods. to make these ideas more concrete, we discuss a particular case of    tting q to a
formally intractable p in section(28.4.1) below.

28.4.1 pairwise markov random    eld

a canonical intractable distribution is the pairwise markov random field1 de   ned on binary variables
xi     {+1,   1}, i = 1, . . . , d,

(cid:80)

i,j wij xixj +(cid:80)

i bixi

p(x) =

1

z(w, b)

e

here the partition function z(w, b) ensures normalisation,

(cid:88)

(cid:80)
i,j wij xixj +(cid:80)

i bixi

z(w, b) =

e

(28.4.1)

(28.4.2)

x

since x2
motivational example for this model is described below.

i = 1, the terms wiix2

i are constant and without loss of generality we may set wii to zero. a

example 28.1 (bayesian image denoising). consider a binary image y generated by corrupting a clean
image x; our interest it to recover the clean image given the corrupted image. we assume a noisy pixel
generating process that takes each clean pixel xi       1 and    ips its binary state:

p(y|x) =

p(yi|xi),

p(yi|xi)     e  yixi

(28.4.3)

(cid:89)

i

the id203 that yi and xi are in the same state is e  /(e   +e     ). our interest is the posterior distribution
on clean pixels p(x|y). we assume that clean images are reasonably smooth and can be described using a
mrf prior de   ned on a lattice,    g(28.2),

ij wij xixj

(28.4.4)

(cid:80)
p(x)     e

where wij > 0, for neighbouring i and j, and wij = 0 otherwise. this encodes the assumption that clean
images tend to have neighbouring pixels in the same state. an isolated pixel in a di   erent state to its
neighbours is unlikely under this prior. we now have the joint distribution

p(x, y) = p(x)

p(yi|xi) =

1
z

e

(cid:80)

ij wij xixj +(cid:80)

(cid:89)

i

i   yixi

(28.4.5)

1whilst id136 with a general mrf is formally computationally intractable (no exact polynomial time methods are
known), two celebrated results that we mention in passing are that for the planar mrf model with pure interactions (b = 0),
the partition function is computable in polynomial time[171, 103, 192, 128, 261], as is the map state for attractive planar ising
models w > 0 [134], see section(28.9).

draft november 9, 2017

589

variational bounding using kl(q|p)

see    g(28.3), from which the posterior is given by

(cid:80)
p(y|x)p(x)
x p(y|x)p(x)     e

(cid:80)

ij wij xixj +(cid:80)

p(x|y) =

i   yixi

(28.4.6)

quantities such as the map state (most a posteriori probable image), marginals p(xi|y) and the normali-
sation constant are of interest. this example is equivalent to the markov network in example(4.2). on the
left below is the clean image, the middle is the noisy image, and on the right is the most likely posterior
clean image arg maxx p(x|y) found using iterated conditional modes, section(28.9.1).

we discuss how to compute the map state for example(28.1) in section(28.9) and concentrate    rst on a
technique that can bound the normalisation constant z, a quantity that is useful in model comparison.

kullback-leibler based methods

for the mrf we have

kl(q|p) = (cid:104)log q(cid:105)q    

wij (cid:104)xixj(cid:105)q    

bi (cid:104)xi(cid:105)q + log z     0

rewriting, this gives a bound on the log-partition function

(cid:124)

(cid:123)(cid:122)

(cid:125)

id178

log z        (cid:104)log q(cid:105)q

+

wij (cid:104)xixj(cid:105)q +

bi (cid:104)xi(cid:105)q

(cid:123)(cid:122)

energy

(cid:125)

ij

(cid:88)
(cid:88)
(cid:124)

ij

(cid:88)
(cid:88)

i

i

the bound saturates when q = p. this is of little help, however, since we cannot compute the averages
(cid:104)xixj(cid:105)p ,(cid:104)xi(cid:105)p with respect to this intractable distribution p. the idea of a variational method is to assume
a simpler tractable distribution q for which these averages can be computed, along with the id178 of q.
minimising the kl divergence with respect to any free parameters of q(x) is then equivalent to maximising
the lower bound on the log partition function.

factorised approximation

a    naive    assumption is the fully factorised distribution

q(x) =

qi(xi)

(cid:89)

i

(cid:88)

i

the graphical model of this approximation is given in    g(28.4a). in this case

log z        

(cid:104)log qi(cid:105)qi +

wij (cid:104)xixj(cid:105)q(xi,xj ) +

bi (cid:104)xi(cid:105)q(xi)

(cid:88)

ij

(cid:88)

i

(a)

(b)

(c)

tion.
tion.

q(x) =(cid:81)

figure 28.4:

(a): naive mean field approximation
(b): a spanning tree approxima-
(c): a decomposable (hypertree) approxima-

i qi(xi).

590

draft november 9, 2017

(28.4.7)

(28.4.8)

(28.4.9)

(28.4.10)

(28.4.11)

(28.4.12)

i q(xi), and (cid:104)xixj(cid:105) = (cid:104)xi(cid:105)(cid:104)xj(cid:105), i (cid:54)= j. for a binary variable, one may

variational bounding using kl(q|p)

for a factorised distribution q(x) =(cid:81)

use the convenient parametrization

qi(xi = 1) =

e  i

e  i + e     i

so that for xi     {   1, +1}

(cid:88)

i

(cid:88)

i(cid:54)=j

log z     b(  )    

h(  i) = log(cid:0)e  

     i(cid:1)

(cid:104)xi(cid:105)qi = +1    q(xi = 1)     1    q(xi =    1) = tanh(  i)

this gives the following lower bound on the log partition function:

(cid:88)

i

h(  i) +

wij tanh(  i) tanh(  j) +

bi tanh(  i)

(28.4.13)

where h(  i) is the binary id178 of a distribution parameterised according to equation (28.4.11):

i + e

      i tanh(  i)

(28.4.14)

finding the best factorised approximation in the minimal id181 sense then corre-
sponds to maximising the bound b(  ) with respect to the variational parameters   . the bound b, equation
(28.4.13), is generally non-convex in    and riddled with local optima. finding the globally optimal    is
therefore typically a computationally hard problem. it seems that we have simply replaced the computa-
tionally hard problem of computing log z by an equally hard computational problem of maximising b(  ).
indeed, written as a factor graph in   , the structure of this optimisation problem matches exactly that of
the original mrf. however, the hope is that by approximating a di   cult discrete summation by a contin-
uous optimisation problem, we will be able to bring to the table e   ective continuous variable optimisation
techniques. a particularly simple optimisation technique is to solve for the zero derivative of the bound
equation (28.4.13). di   erentiating and equating to zero, a little algebra leads to the requirement that the
optimal solution satis   es the equations

  i = bi +

wij tanh(  j),    i

(28.4.15)

one may show that sequentially updating any   i according to equation (28.4.15) increases b(  ). this is
called asynchronous updating and is guaranteed to lead to a (local) minimum of the kl divergence, see
section(28.4.3). once a converged solution    has been identi   ed, in addition to a bound on log z, we can
approximate

(cid:104)xi(cid:105)p     (cid:104)xi(cid:105)q = tanh(  i)

validity of the factorised approximation

(28.4.16)

when might one expect such a naive factorised approximation to work well? clearly, for equation (28.4.1),
if wij is very small, the distribution p will be e   ectively factorised and the approximation will be accurate.
a more interesting case is when each variable xi has many neighbours. in this case it is useful to write the
mrf as (ignoring the bias terms bi for simplicity)

p(x) =

ij wij xixj =

1
z

ed(cid:80)

i xi

1
d

(cid:80)

ed(cid:80)

i xizi

j wij xj =

1
z

an interesting question is how zi is distributed. we now invoke a circular (but self-consistent) argument:
let   s assume that p(x) is factorised. assuming that each wij is o (1), the mean of zi is

where the local       elds    are de   ned as

zi    

1
d

wijxj

(cid:104)zi(cid:105) =

1
d

wij (cid:104)xj(cid:105) = o (1)

draft november 9, 2017

(cid:88)

i,j

e

1
z

(cid:80)
(cid:88)

j

(cid:88)

j

(28.4.17)

(28.4.18)

(28.4.19)

591

variational bounding using kl(q|p)

(cid:16)

1     (cid:104)xk(cid:105)2(cid:17)

w2
ik

d(cid:88)

k=1

= o (1/d)

(28.4.20)

the variance is

(cid:11)

(cid:10)z2

i

    (cid:104)zi(cid:105)2 =

1
d2

xj in the summation(cid:80)
ed(cid:80)

p(x)    

1
z

i xi(cid:104)zi(cid:105)

(cid:89)

i

   

hence for large d the variance of the    eld zi is much smaller than its mean value. since each of the terms
j wijxj is independent, provided the wij are not extreme, the conditions of validity
of the central limit theorem hold[135], and zi will be gaussian distributed. in particular, as d increases the
   uctuations around the mean diminish, and we may write

p(xi)

(28.4.21)

the assumption that p is approximately factorised is therefore self-consistent in the limit of mrfs with
a large number of neighbours. hence the factorised approximation would appear to be reasonable in the
extreme limits of (i) a very weakly connected system wij     0, or (ii) a large densely connected system with
random weights. the fully factorised approximation is also called the naive mean    eld theory since for the
mrf case it assumes that we can replace the e   ect of the neighbours by a mean of the    eld at each site.

for a general intractable distribution p(x) on discrete or continuous x, the kl divergence between a factorised

isolating the dependency of the above on a single factor q(xi) we have

i q(xi) and p(x) is

(cid:104)log q(xi)(cid:105)q(xi)     (cid:104)log p(x)(cid:105)(cid:81)

i q(xi)

28.4.2 general mean    eld equations

approximation q(x) =(cid:81)
(cid:88)
(cid:68)

kl(q(x)|p(x)) =

i

(cid:104)log q(xi)(cid:105)q(xi)    

(cid:69)

j(cid:54)=i q(xj )

q(xi)

(cid:104)log p(x)(cid:105)(cid:81)

(cid:16)
(cid:104)log p(x)(cid:105)(cid:81)
(cid:16)
(cid:104)log p(x)(cid:105)(cid:81)

j(cid:54)=i q(xj )

j(cid:54)=i q(xj )

(cid:17)
(cid:17)

q(xi)     exp

up to a normalisation constant, this is therefore the kl divergence between q(xi) and a distribution pro-
portional to exp

so that the optimal setting for q(xi) satis   es

these are known as the mean-   eld equations and de   ne a new approximation factor in terms of the previous
approximation factors. note that if the normalisation constant of p(x) is unknown, this presents no problem
since this constant is simply absorbed into the normalisation of the factors q(xi). in other words one may
replace p(x) with the unnormalised p   (x) in equation (28.4.24). beginning with an initial randomly chosen
set of distributions q(xi), the mean-   eld equations are iterated until convergence. asynchronous updating
is guaranteed to decrease the kl divergence at each stage, as we show below.

28.4.3 asynchronous updating guarantees approximation improvement

for a factorised variational approximation equation (28.4.22), we claim that each update equation (28.4.24)
reduces the kullback-leibler approximation error. to show this we write a single updated distribution as

the joint distribution under this single update is

qnew
i =

1
zi

exp(cid:104)log p(x)(cid:105)(cid:81)
(cid:89)

j(cid:54)=i qold

j

qnew = qnew

i

qold
j

j(cid:54)=i

(cid:16)
        kl(qnew|p)     kl

(cid:17)

qold|p

our interest is the change in the approximation error under this single mean-   eld update:

592

draft november 9, 2017

(28.4.22)

(28.4.23)

(28.4.24)

(28.4.25)

(28.4.26)

(28.4.27)

figure 28.5:
(b): a structured singly-connected approximation.

(a): a toy    intractable    distribution.

variational bounding using kl(q|p)

x1

x4

x2

x1

x3

x4

x2

x3

(a)

(b)

using

kl(qnew|p) = (cid:104)log qnew

i

(cid:105)qnew

i

+

(cid:68)

(cid:88)

j(cid:54)=i

log qold

j

and de   ning the un-normalised distribution

j(cid:54)=i qold

j

= ziqnew

i

(cid:69)

(cid:68)

(cid:68)
(cid:104)log p(cid:105)(cid:81)
(cid:69)
i     (cid:104)log q
qold
   
i (cid:105)qold

i

q

then

   

i (xi) = exp(cid:104)log p(x)(cid:105)(cid:81)
(cid:68)

    = (cid:104)log qnew

log qold

i

(cid:105)qnew
i    
i     log zi    

qold

i    
log qold

i

(cid:69)

log qold

i

+ (cid:104)log q

qold
i

(cid:17)

(cid:68)

i
   
= (cid:104)log q
i (cid:105)qnew
=     log zi    
qold
=    kl
i

(cid:16)
(cid:17)
qnew|p

(cid:16)

i

|qnew
(cid:16)

    0

(cid:17)

    kl

qold|p

hence

kl

(cid:69)

j    

qold

(cid:68)

(cid:104)log p(x)(cid:105)(cid:81)

(cid:69)

j(cid:54)=i qold

j

qnew
i

(cid:69)

(cid:68)

+

(cid:69)

j(cid:54)=i qold

j

qold
i

(cid:104)log p(cid:105)(cid:81)

j(cid:54)=i qold

j

qnew
i

   
i (cid:105)qnew

i

+ (cid:104)log q

   
i (cid:105)qold

i

(28.4.28)

(28.4.29)

(28.4.30)

(28.4.31)

(28.4.32)

(28.4.33)

(28.4.34)

so that updating a single component of q at a time is guaranteed to improve the approximation. note that
this result is quite general, holding for any distribution p(x). in the case of a markov network the guaranteed
approximation improvement is equivalent to a guaranteed increase (strictly speaking a non-decrease) in the
lower bound on the partition function.

may not be tractably implementable. for this we need to be able to compute (cid:104)log p   (x)(cid:105)(cid:81)

remark 28.1 (intractable energy). even for a fully factorised approximation the mean-   eld equations
j(cid:54)=i q(xj ). for some

models of interest this is still not possible and additional approximations are required.

28.4.4 structured variational approximation

one can extend the factorised kl variational approximation by using non-factorised q(x)[257, 25]. those
for which averages of the variables can be computed in linear time include spanning trees,    g(28.4b) and
decomposable graphs    g(28.4c). for example, for the distribution,    g(28.5a),

p(x1, x2, x3, x4) =

1
z

  (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)  (x1, x3)

(28.4.35)

a tractable q distribution would be,    g(28.5b),

q(x1, x2, x3, x4) =

1
  z

    (x1, x2)     (x1, x3)     (x1, x4)

in this case we have

kl(q|p) = hq(x1, x2) + hq(x1, x3) + hq(x1, x4)     3hq(x1) +

(28.4.36)

(cid:88)

i   j

(cid:104)log   (xi, xj)(cid:105)q(xi,xj )

(28.4.37)

draft november 9, 2017

593

variational bounding using kl(q|p)

(cid:80)

where hq(x ) is the id178 of q(x ). since q is singly-connected, computing the marginals and id178
is straightforward (since the id178 requires only pairwise marginals on graph neighbours). in this case,
however, we cannot apply directly the standard mean    eld update equations since there are constraints
xj q(xi, xj) = q(xj). we can either deal with these using lagrange multipliers, or ensure that in perform-

ing an mean-   eld style update, we are including the contributions from coupled terms.

more generally one can exploit any structural approximation by use of, for example, the junction tree
algorithm to compute the required moments. however, the computational expense typically increases expo-
nentially with the hypertree width[316].

n(cid:88)

t(cid:89)

t=2

n(cid:88)

example 28.2 (robot arm: control via id136). control problems can also be cast as id136 problems,
see for example [166]. consider the position vt = (xt, yt)t of an n-link robot arm in a two-dimensional place,
where each link i     {1, . . . , n} in the arm is of unit length and angle hi,t:

xt =

cos hi,t,

yt =

sin hi,t

(28.4.38)

i=1

i=1

our interest is to use the robot arm to track a given sequence v1:t such that the joint angles ht do not
change much from one time to the next. this is a classical control problem that we can formulate as an
id136 problem using a model

p(v1:t , h1:t ) = p(v1|h1)p(h1)

where the terms are

      vt

(cid:32) n(cid:88)

p(vt|ht) = n

cos hi,t,

sin hi,t

i=1

i=1

p(vt|ht)p(ht|ht   1)
n(cid:88)

(cid:33)t

,   2i

       , p(ht|ht   1) = n

(cid:0)ht ht   1,   2i(cid:1)

(28.4.39)

(28.4.40)

solution to the

control problem is

one
sequence
arg maxh1:t p(h1:t|v1:t ). here we consider an alternative, the maximum posterior marginal solution at each
time arg maxht p(ht|v1:t ). due to the non-linear observation, this posterior marginal cannot be computed
exactly. a simple approximation is to use a fully factorised variational distribution p(h1:t|v1:t )     q(h1:t )
where

then given by the most

likely posterior

q(h1:t ) =

q(hi,t)

(28.4.41)

t(cid:89)

n(cid:89)

t=1

i=1

from the general form of the mean-   eld equations, the update for q(hi,t) is given by (for 1 < t < t )

   2 log q(hi,t) =

1
  2

where

(cid:0)hi,t       hi,t+1

(cid:1)2

1

+

1
  2

(cid:1)2

(cid:0)hi,t       hi,t   1
(cid:88)

+

1

j(cid:54)=i

(cid:88)

j(cid:54)=i

  2 (cos hi,t       i,t)2 +

  2 (sin hi,t       i,t)2 + const.

  hi,t+1     (cid:104)hi,t+1(cid:105) ,   i,t     xt    

(cid:104)cos hj,t(cid:105) ,   i,t     yt    

(cid:104)sin hj,t(cid:105)

(28.4.42)

(28.4.43)

and the above averages are with respect to the q marginal distribution. due to the non-linearities, the above
marginal distributions are non-gaussian. however, since they are only one-dimensional, the required aver-
ages can readily be computed using quadrature. the above mean-   eld equations are iterated to convergence.
see demorobotarm.m for further details and    g(28.6) for a demonstration. note that the planning problem
of starting from a point (x1, y1) to smoothly move the arm to a desired end point (xt , yt ) is a special case
of this framework. one can achieve this either by removing the observation terms for the intermediate times
or, equivalently, making the observation variances for those intermediate times extremely large.

594

draft november 9, 2017

local and kl variational approximations

(a)

(b)

(c)

figure 28.6: (a): the desired trajectory of the end point of a three link robot arm. green denotes time 1
(b): the learned trajectory based on a fully factorised kl variational approximation.
and red time 100.
(c): the robot arm sections every 2nd timestep, from time 1 (top left) to time 100 (bottom right). the
control problem of matching the trajectory using a smoothly changing angle set is solved using this simple
approximation.

28.5 local and kl variational approximations

in    tting a model parameterised by w to data d, one often encounters parameter posteriors of the form

p(w|d) =

1
z n (w   ,   ) f (w)

(cid:90)

where

z =

n (w   ,   ) f (w)dw

(28.5.1)

(28.5.2)

a classical example is bayesian id28, section(18.2) in which n (w   ,   ) represents a prior on
the weight w, f (w) represents the likelihood p(d|w) and z = p(d).
in all but limited special cases, the
function f (w) is not a simple squared exponential, resulting in a posterior distribution of a non-standard
inevitably, therefore, approximations are required. when the dimension of the parameter vector
form.

draft november 9, 2017

595

   2   1012   1   0.500.511.522.5   2   1012   1   0.500.511.522.5local and kl variational approximations

dim (w) = w is large,    nding an accurate posterior approximation is in general non-trivial. our particular
interest here is to form an approximation of p(w|d) that also gives a principled lower bound on the marginal
likelihood p(d).

28.5.1 local approximation

in a local method [226, 155, 125, 235], one replaces f by a suitable function for which the integral can be
computed. since in our case the integrand is composed of a gaussian in w, it is most convenient to bound
f (w) by an exponential quadratic function

f (w)     c(  )e

    1

2 wtf(  )w+wtf (  )

(28.5.3)

where the matrix f(  ), vector f (  ) and scalar c(  ) depend on the speci   c function f ;    is a variational
parameter that enables one to    nd the tightest bound. we will discuss explicit c, f and f functions later,
but for the moment leave them unspeci   ed. then from equation (28.5.2)

(cid:90)

(cid:18)

(cid:112)

z    

c(  )

det (2    )

exp

   

which can be expressed as
    1
2   t     1  
det (2    )

z     c(  )

(cid:112)

e

(cid:90)

exp

   

1
2

(cid:18)

1
2

(cid:19)
(w       )t      1 (w       )
(cid:19)
(cid:18)

wtaw + wtb

dw

where

a =      1 + f(  ),

b =      1   + f (  )

exp

   

wtf(  )w + wtf (  )

1
2

(cid:19)

dw

(28.5.4)

(28.5.5)

whilst both a and b are functions of   , we notationally drop this dependency to keep the description more
compact. completing the square and integrating, we have log z     b(  ), with

1
2

  t     1   +

1
2

btab    

1
2

log det (  a)

(28.5.6)

to obtain the tightest bound on log z, one maximises b(  ) with respect to   . in many practical problems
s=1 fs(w) for local site functions fs. by bounding each site individually, we obtain a

bound b(  1, . . . ,   m ). the bound may be then optimised numerically with respect to the vector   .

b(  )     log c(  )    

of interest, f (w) =(cid:81)m

28.5.2 kl variational approximation

an alternative to the above local bounding method is to consider a kl approach based on    tting a gaussian
to the distribution. de   ning

  p(w)     n (w   ,   ) f (w)

z

(28.5.7)

by    tting a gaussian q(w) = n (w m, s) based on minimising the kl(q(w)|  p(w)), we obtain the bound
log z     bkl(m, s) with

bkl(m, s)        (cid:104)log q(w)(cid:105)    

1
2

log det (2    )    

1
2

(w       )t      1 (w       )

+ (cid:104)log f (w)(cid:105)

(cid:68)

(cid:69)

where (cid:104)  (cid:105) denotes expectation with respect to q(w). one then numerically    nds the best parameters m, s
that maximise the bound. since the id178 of a gaussian is trivial, the only potentially problematic term
in evaluating this bound is (cid:104)log f (w)(cid:105). a class of functions for which (cid:104)log f (w)(cid:105)n (w m,s) is computationally
tractable is when f (w) = f (wth) for some    xed vector h. in this case, the projection wth is also gaussian
distributed and

n (w m,s)

= (cid:104)log f (a)(cid:105)n (a mth,htsh)

(28.5.8)

draft november 9, 2017

596

(cid:68)

(cid:69)

log f (wth)

mutual information maximisation : a kl variational approach

which can be readily computed using any one-dimensional integration routine. explicitly, as a function of
m, s, we have

2bkl(m, s)         log det (s) + w + log det (  )

(cid:16)

     1(cid:16)

s + (m       ) (m       )t(cid:17)(cid:17)

    trace

+ 2(cid:104)log f (a)(cid:105)n (a mth,htsh)

(28.5.9)

whilst, in general, the variational bounds are non-concave in their variational parameters, provided f is
log-concave then bkl(m, s) is jointly concave in m and s. by using structured covariances s the method
is scalable to very high dimensional problems [61].

relation between the kl and local bounds

the kl and local variational methods both provide a lower bound on the normalisation term z in equation
(28.5.2). it is interesting therefore to understand what relation there is between these bounds. using the
bound on f (w), we obtain a new bound

bkl(m, s)       bkl(m, s,   )

where

(cid:68)

  bkl        (cid:104)log q(w)(cid:105)   
which, using equation (28.5.5), can be written as

log det (2    )    

1
2

1
2

(w       )t      1 (w       )

+log c(  )   

  bkl =    (cid:104)log q(w)(cid:105)    

log det (2    ) + log c(  )    

1
2

1
2

  t     1      

1
2

wtaw

(cid:69)

(cid:68)

(cid:68)

(cid:69)

(cid:68)

+

wtf(  )w

(cid:68)

wtb

(cid:69)

+

1
2

(cid:69)

(28.5.10)

(cid:69)

wtf (  )

(28.5.11)

(cid:0)w a   1b, a   1(cid:1)

by de   ning

  q(w) = n

then

(28.5.12)

log det(cid:0)2  a   1(cid:1)

  bkl =    kl(q(w)|  q(w))    

1
2

log det (2    ) + log c(  )    

  t     1   +

1
2

bta   1b +

1
2

1
2

since m, s only appear via q(w) in the kl term, the tightest bound is given when m, s are set such that
q(w) =   q(w). at this setting the kl term in   bkl disappears and m and s are given by

s   =(cid:0)     1 + f(  )(cid:1)   1

,

m   = s  

(cid:0)     1   + f (  )(cid:1) .

(28.5.13)

for this setting of the variational parameters the bound matches the local bound (28.5.6). since bkl(m, s)    
  bkl(m, s,   ) we therefore have that,

bkl(m  , s  )       bkl(m  , s  ,   ) = b(  ).

importantly, the vg bound can be tightened beyond this setting:

m,s bkl(m, s)     bkl(m  , s  )
max

(28.5.14)

(28.5.15)

thus the optimal kl bound is provably tighter than both the local variational bound (28.5.6) and the kl
bound calculated using the optimal local moments m   and s  .

28.6 mutual information maximisation : a kl variational approach

here we take a short interlude to discuss an application of the kullback-leibler variational approach in
id205. a common goal is to maximise information transfer, measured by the mutual information
(see also de   nition(8.13))

i(x, y )     h(x)     h(x|y )

draft november 9, 2017

(28.6.1)

597

x

y1

y2

y3

y4

mutual information maximisation : a kl variational approach

figure 28.7: an information transfer problem. for a    xed distribution

p(x) and parameterised distributions p(yj|x) =   (cid:0)wt

i x(cid:1),    nd the op-

timal parameters wi that maximise the mutual information between
the variables x and y. such considerations are popular in theoretical
neuroscience and aim to understand how the receptive    elds wi of a
neuron relate to the statistics of the environment p(x).

algorithm 28.1 im algorithm for maximising mutual information i(x, y ) for    xed p(x) and adjustable
parameters are p(y|x,   ).
1: choose a class of approximating distributions q (for example factorised)
2: initialise the parameters   
3: repeat
4:

  new = argmax
qnew(x|y) = argmax

  

(cid:104)log q(x|y)(cid:105)p(x)p(y|x,  )

q(x|y)   q (cid:104)log q(x|y)(cid:105)p(x)p(y|x,  new)

5:

6: until converged

where the id178 and conditional id178 are de   ned

h(x)        (cid:104)log p(x)(cid:105)p(x) ,

h(x|y )        (cid:104)log p(x|y)(cid:105)p(x,y)

(28.6.2)

here we are interested in the situation in which p(x) is    xed, but p(y|x,   ) has adjustable parameters    that
we wish to set in order to maximise i(x, y ). in this case h(x) is constant and the optimisation problem is
equivalent to minimising the conditional id178 h(x|y ). unfortunately, in many cases of practical interest
h(x|y ) is computationally intractable. we discuss in section(28.6.1) a general procedure based on the
id181 to approximately maximise the mutual information.

example 28.3. consider a neural transmission system in which xi     {0, 1} denotes an emitting neuron in
a non-   ring state (0) or    ring state (1), and yj     {0, 1} a receiving neuron. if each receiving neuron    res
independently, depending only on the emitting neurons, we have

(cid:89)

i

p(y|x) =

p(yi|x)

where for example we could use

(cid:16)

(cid:17)

p(yi = 1|x) =   

wt

i x

(28.6.3)

(28.6.4)

given an empirical distribution on neural    rings p(x), our interest is to set the weights {wi} to maximise
information transfer, see    g(28.7). since p(x) is    xed, this requires maximising

(cid:104)log p(x|y)(cid:105)p(y|x)p(x)

(28.6.5)

the quantity p(x|y) = p(y|x)p(x)/p(y) is a non-factorised function of y (due to the term p(y). this means
that the conditional id178 is typically intractable to compute and we require an approximation.

28.6.1 the information maximisation algorithm

consider

kl(p(x|y)|q(x|y))     0

598

(28.6.6)

draft november 9, 2017

mutual information maximisation : a kl variational approach

c

a

e

d

b

f

figure 28.8: belief propagation can be derived by considering
how to compute the marginal of a variable on a mrf. in this
case the marginal p(d) depends on messages transmitted via the
neighbours of d. by de   ning local messages on the links of the
graph, a recursive algorithm for computing all marginals can be
derived, see text.

this immediately gives a bound

p(x|y) log p(x|y)    

p(x|y) log q(x|y)     0

multiplying both sides by p(y), we obtain

p(y)p(x|y) log p(x|y)    

p(x, y) log q(x|y)

(cid:88)

x

(cid:88)

x,y

(cid:88)

x

(cid:88)

x,y

from the de   nition, the left side of the above bound is    h(x|y ). hence

i(x, y )     h(x) + (cid:104)log q(x|y)(cid:105)p(x,y)       i(x, y )

(28.6.7)

(28.6.8)

(28.6.9)

from this lower bound on the mutual information we arrive at the information maximisation (im) algorithm[21].
given a distribution p(x) and a parameterised distribution p(y|x,   ), we seek to maximise   i(x, y ) with re-
spect to   . a co-ordinate wise optimisation procedure is presented in algorithm(28.1). the blahut-arimoto
algorithm in id205 (see for example [199]) is a special case in which the optimal decoder

q(x|y)     p(y|x,   )p(x)

(28.6.10)

is used. in applications where the blahut-arimoto algorithm is intractable to implement, the im algorithm
can provide an alternative by restricting q to a tractable family of distributions (tractable in the sense that
the lower bound can be computed). the blahut-arimoto algorithm is analogous to the em algorithm for
maximum likelihood and guarantees a non-decrease of the mutual information at each stage of the update,
see section(11.2.2). similarly, the im procedure is analogous to a variational em procedure and each step
of the procedure cannot decrease the lower bound on the mutual information.

28.6.2 linear gaussian decoder

a special case of the im framework is to use a linear gaussian decoder

plugging this into the bound, equation (28.6.9), and optimising with respect to   , and u, we obtain

q(x|y) = n (x uy,   )     log q(x|y) =    

(cid:68)

(x     uy) (x     uy)t(cid:69)

   =

1
2

(x     uy)t      1 (x     uy)    
(cid:68)
xyt(cid:69)(cid:68)
xyt(cid:69)(cid:68)

yyt(cid:69)   1
yyt(cid:69)   1(cid:68)

yxt(cid:69)(cid:19)

(cid:68)

n
2

   

,

u =

(cid:18)(cid:68)

xxt(cid:69)

   

i(x, y )     h(x)    

1
2

log det

where (cid:104)  (cid:105)     (cid:104)  (cid:105)p(x,y). using this setting in the bound we obtain

1
2

log det (2    )

(28.6.11)

(28.6.12)

(1 + log 2  )

(28.6.13)

where n = dim (x). this is equivalent to linsker   s as-if-gaussian approximation to the mutual information
[191]. one can therefore view linsker   s approach as a special case of the im algorithm restricted to linear-
gaussian decoders. in principle, one can therefore improve on linsker   s method by considering more powerful
non-linear-gaussian decoders. applications of this technique to neural systems are discussed in [21].

draft november 9, 2017

599

loopy belief propagation

x2

  

x

2   

x

i(

x

i)

xi

  xi   xj (xj)

xj

x1

  x1   xi (xi)
(x i)

    x i

   x 3

figure 28.9: loopy belief propagation. once a node has received
incoming messages from all neighbours (excluding the one it wants to
send a message to), it may send an outgoing message to a neighbour:

  xi   xj (xj) =

   (xi, xj)  x1   xi (xi)   x2   xi (xi)   x3   xi (xi)

(cid:88)

xi

x3

28.7 loopy belief propagation

belief propagation is a technique for exact id136 of marginals p(xi) for singly-connected distributions
p(x). there are di   erent formulations of bp, the most modern treatment being the sum-product algorithm
on the corresponding factor graph, as described in section(5.1.2). an important observation is that the
algorithm is purely local     the updates are unaware of the global structure of the graph. this means that
even if the graph is multiply-connected (it is loopy) one can still apply the algorithm and    see what happens   .
provided the loops in the graph are relatively long, one may hope that    loopy    bp will converge to a good
approximation of the true marginals. when the method converges the results can be surprisingly accurate.
in the following we will show how loopy bp can also be motivated by a variational objective. to do so, we
make a connection to the classical bp algorithm (rather than the factor graph sum-product algorithm). for
this reason we brie   y describe below the classical bp approach.

28.7.1 classical bp on an undirected graph

bp can be derived by considering how to calculate a marginal in terms of messages on an undirected
a,b,c,e,f p(a, b, c, d, e, f ) for the pairwise markov network
b    (d, b) denotes summation
over the states of the variable b. to compute the summation e   ciently we may distribute the summations
as follows:

graph. consider calculating the marginal p(d) =(cid:80)
in    g(28.8). we denote both a node and its state by the same symbol, so that(cid:80)
(cid:88)
(cid:124)

(cid:88)
(cid:124)
(cid:123)(cid:122)

(cid:88)
(cid:124)

(cid:88)
(cid:124)

(cid:88)
(cid:124)

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:123)(cid:122)

(28.7.1)

   (d, f )

   (a, d)

   (d, e)

   (b, d)

   (c, e)

p(d) =

  f   d(d)

  a   d(d)

  b   d(d)

  c   e(e)

(cid:125)
(cid:125)

(cid:125)

(cid:125)

(cid:125)

1
z

a

f

e

c

b

  e   d(d)

where we de   ne messages   n1   n2 (n2) sending information from node n1 to node n2 as a function of the
state of node n2. in general, a node xi passes a message to node xj via

  xi   xj (xj) =

   (xi, xj)

  xk   xi (xi)

(cid:89)

k   ne(i),k(cid:54)=j

see also    g(28.9). at convergence the marginal p(xi) is then given by

q(xi)    

  xj   xi (xi)

the prefactor being determined by normalisation. the pairwise marginals are approximated by

         (xi, xj)

       (cid:89)

      

q(xi, xj)    

k   ne(i)\j

  xk   xi (xi)

  xk   xj (xj)

k   ne(j)\i

for a singly-connected distribution p, this message passing scheme converges and the marginal corresponds
to the exact result. for multiply-connected (loopy) structures, belief propagation will generally result in an
approximation.

600

draft november 9, 2017

(cid:88)

xi

i   ne(j)

(cid:89)
       (cid:89)

(28.7.2)

(28.7.3)

(28.7.4)

loopy belief propagation

28.7.2 loopy bp as a variational procedure

a variational procedure that corresponds to loopy bp can be derived by considering the terms of a standard
variational approximation based on the id181 kl(q|p)[321]. we take as our example
a pairwise markov network de   ned on potentials   (xi, xj),

  (xi, xj)

(28.7.5)

(cid:89)

i   j

p(x) =

1
z

where i     j denotes the unique neighbouring edges on the graph (each edge is counted only once). using
an approximating distribution q(x), the kullback-leibler bound is

log z        (cid:104)log q(x)(cid:105)q(x)

+

(cid:104)log   (xi, xj)(cid:105)q(x)

(cid:124)

(cid:123)(cid:122)

id178

(cid:125)

(cid:123)(cid:122)

energy

(cid:125)

(cid:88)
(cid:124)

i   j

since

(cid:104)log   (xi, xj)(cid:105)q(x) = (cid:104)log   (xi, xj)(cid:105)q(xi,xj )

(28.7.6)

(28.7.7)

each contribution to the energy depends on q(x) only via the pairwise marginals q(xi, xj). this suggests that
these marginals should form the natural parameters of any approximation. can we then    nd an expression
for the id178    (cid:104)log q(x)(cid:105)q(x) in terms of these pairwise marginals? consider a case in which the required
marginals are

q(x1, x2), q(x2, x3), q(x3, x4)

(28.7.8)

either by appealing to the junction tree representation, or by straightforward algebra, one can show that
we can uniquely express q in terms of these marginals using

q(x) =

q(x1, x2)q(x2, x3)q(x3, x4)

q(x2)q(x3)

(28.7.9)

an intuitive way to arrive at this result is by examining the numerator of equation (28.7.9). the variable x2
appears twice, as does the variable x3 and, since any joint distribution cannot have such replicated variables,
we must compensate for    overcounting    x2 and x3 by dividing by these marginals. in this case, the id178
of q(x) can be written as

hq(x) =    (cid:104)log q(x)(cid:105)q(x) = hq(x1, x2) + hq(x2, x3) + hq(x3, x4)     hq(x2)     hq(x3)

more generally, from chapter(6), any decomposable graph can be represented as

(cid:81)
(cid:81)

c q(xc)
s q(xs)

q(x) =

(28.7.10)

(28.7.11)

where the q(xc) are the marginals de   ned on cliques of the graph, with xc being the variables of the clique,
and the q(xs) are de   ned on the separators (intersections of neighbouring cliques). the expression for the
id178 of the distribution is then given by a sum of marginal entropies minus the separator entropies.

bethe free energy

consider now a markov network corresponding to a non-decomposable graph, for example the 4-cycle

p(x) =

1
z

  (x1, x2)  (x2, x3)  (x3, x4)  (x4, x1)

the energy requires therefore that our approximating distribution de   nes the pairwise marginals

q(x1, x2), q(x2, x3), q(x3, x4), q(x4, x1)

(28.7.12)

(28.7.13)

assuming that these marginals are given, can we    nd an expression for the id178 of the joint distribution
q(x) in terms of its pairwise marginals q(xi, xj)? in general this is not possible since the graph contains loops

draft november 9, 2017

601

loopy belief propagation

(so that the junction tree representation would result in cliques greater than size 2). however, a simple    no
overcounting    approximation is to write

q(x)    

q(x1, x2)q(x2, x3)q(x3, x4)q(x4, x1)

q(x1)q(x2)q(x3)q(x4)

.

using this we can approximate the id178 as

hq(x)     hq(x1, x2) + hq(x2, x3) + hq(x3, x4) + hq(x1, x4)    

(28.7.14)

4(cid:88)

i=1

hq(xi)

(28.7.15)

respect to the parameters q(xi, xj) subject to marginal consistency constraints,(cid:80)

in general, to make the distribution    dimensionally consistent    we need to compensate by a factor q(xi)ci
where ci is the number of neighbours of variable xi minus 1. with this approximation the (negative) log
partition function is known as the bethe free energy. our interest is then to maximise this expression with
xj q(xi, xj) = q(xi). these
constraints may be enforced using lagrange multipliers   ij(xi). one can write the bethe free energy (the
approximated id181 up to a constant) as

(cid:88)

i   j

(cid:88)

i

(cid:88)

i   j

f(q,   )        

hq(xi, xj)+

cihq(xi)   

(cid:104)log   (xi, xj)(cid:105)q(xi,xj ) +

  ij(xi)

(cid:88)

(cid:88)

i   j

xi

      q(xi)    

(cid:88)

      

q(xi, xj)

xj

(28.7.16)

we neglect including lagrange terms to enforce normalisation of the q since these only add constant terms
which can be later absorbed by explicitly normalising q. expression (28.7.16) is no longer a bound on the
log partition function since the id178 approximation is not a lower bound on the true id178. the task
is now to minimize equation (28.7.16) with respect to the parameters, namely all the pairwise marginals
q(xi, xj) and the lagrange multipliers   . a simple scheme to optimise equation (28.7.16) is to use a    xed
point iteration by equating the derivatives of the bethe free energy with respect to the parameters q(xi, xj)
to zero, and likewise for the lagrange multipliers. di   erentiating with respect to q(xi, xj) and equating to
zero, we obtain

log q(xi, xj)     log   (xi, xj)       ij(xi)       ji(xj) + const. = 0

(28.7.17)

so that

q(xi, xj)       (xi, xj)    ij(xi)    ji(xj)

(28.7.18)
where     ij(xi)     exp   ij(xi). similarly, di   erentiating with respect to q(xi) and equating to zero, we obtain
(28.7.19)

  ij(xi) + const. = 0

(cid:88)

   ci log q(xi) +

j   ne(i)

so that

q(xi)    

1/ci
ij

    

(xi)

(cid:89)
(cid:89)

j   ne(i)

    ij(xi) =

  xk   xi (xi)

k   ne(i)\j

(28.7.3) since(cid:89)

(cid:89)

(cid:89)

1/ci
ij

    

(xi) =

j   ne(i)

j   ne(i)

k   ne(i)\j

from equation (28.7.18) we can match equation (28.7.4) by mapping

(cid:89)

j   ne(i)

we can verify that this assignment satis   es the single marginal requirements equation (28.7.20) and equation

  xk   xi (xi)1/ci =

  xj   xi (xi)

(28.7.22)

hence the    xed point equations for minimising the bethe free energy are equivalent to belief propagation
[321]. the convergence of loopy belief propagation can be heavily dependent on the topology of the graph
and also the message updating schedule[313, 217]. the potential bene   t of the bethe free energy viewpoint is
that it opens up the possibility of using more general optimisation techniques than bp. the so-called double-
loop techniques iteratively isolate convex contributions to the bethe free energy, interleaved with concave
contributions. at each stage, the resulting optimisiations can be carried out e   ciently[323, 146, 321].

602

draft november 9, 2017

(28.7.20)

(28.7.21)

expectation propagation

(a)

(b)

(c)

(a): the markov network (left) that we wish to approximate the marginals
figure 28.10:
p(w), p(x), p(y), p(z) for. all tables are drawn    rst from a uniform distribution and then raised to a power
(b):
   and renormalised. on the right is shown the naive mean    eld approximation factorised structure.
there are 64 = 1296 states of the distribution. shown is a randomly sampled distribution for    = 5 which
has many isolated peaks, suggesting the distribution is far from factorised. in this case the mf and gibbs
(c): as    is increased to 25, typically only one state of
sampling approximations may perform poorly.
the distribution dominates. whilst the distribution is then simple and essentially factorised,    nding this
maximal single state is numerically challenging. see demomfbpgibbs.m and    g(28.11).

validity of loopy belief propagation

for a markov network which has a loop, a change in a variable on the loop eventually reverberates back
to the same variable. however, if there are a large number of variables in the loop, and the individual
neighbouring links are not all extremely strong, the numerical e   ect of the loop is small in the sense that
the in   uence of the variable on itself will be small. in such cases one would expect the belief propagation
approximation to be accurate. an area of particular success for loopy belief propagation id136 is in error
correction based on low density parity check codes; these are usually explicitly designed to have this long-
loop property[198] so that loopy belief propagation produces good results. in many examples of practical
interest, however, loops can be very short (for example a markov network on a lattice).
in such cases
a naive implementation of loopy bp will most likely fail. a natural extension is to cluster variables to
alleviate strong local dependencies; this technique is called the kikuchi or cluster variation method [168].
more elaborate ways of id91 variables can be considered using region graphs[321, 314].

example 28.4. the    le demomfbpgibbs.m compares the performance of naive mean field (mf) theory,
belief propagation and unstructured id150 on marginal id136 in a pairwise markov network

p(w, x, y, z) =   wx(w, x)  wy(w, y)  wz(w, z)  xy(x, y)  xz(x, z)  yz(y, z)

(28.7.23)

in which all variables take 6 states. in the experiment the tables are selected from a uniform distribution
raised to a power   . for    close to zero, all the tables are essentially    at and therefore the variables become
independent, a situation for which mf, bp and id150 are ideally suited. as    is increased to 5, the
dependencies amongst the variables increase and the methods perform worse, especially mf and gibbs. as
   is increased to 25, the distribution becomes sharply peaked around a single state, such that the posterior
is e   ectively factorised, see    g(28.10). this suggests that a mf approximation (and also id150)
should work well. however,    nding this state is computationally di   cult and the methods often get stuck
in local minima, see    g(28.11). belief propagation seems less susceptible to being trapped in local minima
in this regime and tends to outperform both mf and id150.

28.8 expectation propagation

the messages in schemes such as belief propagation are not always representable in a compact form. the
switching linear dynamical system, as described in chapter(25) is such an instance, with the messages requir-

draft november 9, 2017

603

wxyzoriginal graphwxyzfactorised graph0500100000.050.10.150.2p distribution0500100000.20.40.60.81p distributioning an exponential amount of storage. this limits bp to cases such as discrete networks, or more generally
exponential family messages. expectation propagation extends the applicability of bp by projecting the
messages back to a chosen distribution family at each stage. this projection is obtained by using a kullback-
leibler measure[211, 264, 213].

expectation propagation

(cid:89)

i

p(x) =

1
z

consider a distribution de   ned on subsets of variables xi of the form

  i(xi)

(28.8.1)

in ep one identi   es those factors   i(xi) which, if replaced by simpler factors     i(xi), would render the
distribution   p(x) tractable. one then sets any free parameters of     i(xi) by minimising the kullback-leibler
divergence kl(p|  p). the general approach is outlined in algorithm(28.2). to gain some intuition into the
steps of the algorithm, we consider the speci   c example a pairwise markov network

p(x) =

1
z

  1,2(x1, x2)  2,3(x2, x3)  3,4(x3, x4)  4,1(x4, x1)

(28.8.2)

if we replace all terms   i,j(xi, xj) by approximate factors
with factor graph as depicted in    g(28.12a).
    i,j(xi)     i,j(xj) then the resulting joint distribution   p is factorised and hence tractable. since the variable
xi appears in more than one term from p, we need to index the approximation factors appropriately. a
convenient way to do this is

  p(x) =

1
  z

(cid:124)

    2   1 (x1)     1   2 (x2)

    3   2 (x2)     2   3 (x3)

    4   3 (x3)     3   4 (x4)

    1   4 (x4)     4   1 (x1)

(28.8.3)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

     1,2(x1,x2)

     2,3(x2,x3)

     3,4(x3,x4)

     4,1(x4,x1)

(cid:123)(cid:122)

(cid:125)

which is represented in    g(28.12b). the idea in ep is now to determine the optimal approximation factors
     by the self-consistent requirement that, on replacing approximation factors by their exact counterparts,
there is no di   erence to the marginals of   p. consider the approximation parameters     3   2 (x2) and     2   3 (x3).
to set these we    rst replace the contribution     3   2 (x2)     2   3 (x3) by the exact factor   2,3(x2, x3). this gives
a modi   ed approximation

  p    =

1
  z   

    2   1 (x1)     1   2 (x2)   (x2, x3)     4   3 (x3)     3   4 (x4)     1   4 (x4)     4   1 (x1) =

  2,3(x2, x3)   z   p

    3   2 (x2)     2   3 (x3)   z   
(28.8.4)

the intuition is that if all approximation parameters are set correctly, then replacing the approximation
factors by the exact counterpart should not change the computation of the marginals. to measure how
much using the modi   ed   p    changes from   p we use the id181 between this distribution
and our approximation,

kl(  p   |  p) = (cid:104)log   p   (cid:105)  p        (cid:104)log   p(cid:105)  p   

(28.8.5)

figure 28.11: the absolute error in computing the marginal p(xi)
for the graph in    g(28.10), averaged over all four marginals, for
loopy belief propagation (bp), id150 (gibbs) and naive
mean    eld using a factorised approximation (mf). plotted along
the horizontal axis is the parameter    that controls the    complex-
ity    of the true distribution, see    g(28.10). the vertical axis is the
error averaged over 100 random realisations for the true distribu-
tion. for small   , the distribution is essentially factorised and all
methods work reasonably well. as    increases the distribution
becomes more peaked around a limited number of states, and
   nding these states becomes increasingly di   cult. all methods
use a similar amount of computation with 50 updates for each of
the four variables.

604

draft november 9, 2017

051015202500.050.10.150.20.25    bpmfgibbsexpectation propagation

x1

x4

x2

x1

x2

x1

x3

x4

x3

x4

(a)

(b)

(c)

x2

x3

(a): multiply-connected fac-
figure 28.12:
(b): expecta-
tor graph representing p(x).
tion propagation approximates (a) in terms of a
tractable factor graph. the open squares indi-
cate that the factors are parameters of the ap-
proximation. the basic ep approximation is to
replace all factors in p(x) by product factors.
(c): tree structured ep.

we would like to set the parameters such that this id181 is minimal. since our
interest is in updating     3   2 (x2) and     2   3 (x3), we isolate the contribution from these parameters to the
id181 which is

(cid:68)

(cid:69)

kl(  p   |  p) = log   z    

log     3   2 (x2)     2   3 (x3)

+ const.

  p   (x2,x3)

(28.8.6)

also, since   p is factorised, up to a constant proportionality factor, the dependence of   z on     3   2 (x2) and
    2   3 (x3) is

    1   2 (x2)     3   2 (x2)

    2   3 (x3)     4   3 (x3)

(28.8.7)

x2

x3

di   erentiating the id181 equation (28.8.6) with respect to     3   2 (x2) and equating to
zero, we obtain

(cid:88)

  z    

(cid:88)

(cid:80)

(cid:80)

    1   2 (x2)     3   2 (x2)

    1   2 (x2)     3   2 (x2)

x2

=   p   (x2)

similarly, optimising w.r.t.     2   3 (x3) gives

    2   3 (x3)     4   3 (x3)

    2   3 (x3)     4   3 (x3)

x3

=   p   (x3)

(28.8.8)

(28.8.9)

approximating z

the above updates only determine the approximation factors up to a proportionality constant. this is    ne
if we only wish to    nd approximations for the marginals since any missing proportionality constants can be
determined by the requirement that each marginal is normalised. however if we wish to also approximate
z, we care about such factors. to address this we write the optimal updates as

    3   2 (x2) = z3   2

  p   (x2)

    1   2 (x2)

and

    2   3 (x3) = z2   3

  p   (x3)

    4   3 (x3)

(28.8.10)

(28.8.11)

where z3   2 and z2   3 are proportionality terms. we can determine these proportionalities by the requirement
that the term approximation     3   2 (x2)     2   3 (x3) has the same e   ect on the normalisation of   p as it has on

  p   . that is(cid:88)

(cid:88)

x1,x2,x3,x4

=

    2   1 (x1)     1   2 (x2)     3   2 (x2)     2   3 (x3)     4   3 (x3)     3   4 (x4)     1   4 (x4)     4   1 (x1)

    2   1 (x1)     1   2 (x2)   (x2, x3)     4   3 (x3)     3   4 (x4)     1   4 (x4)     4   1 (x1)

(28.8.12)

x1,x2,x3,x4

draft november 9, 2017

605

map for markov networks

which, on substituting in the updates equation (28.8.10) and equation (28.8.11), reduces to

z2   3z3   2 =

z   
2,3
  z2,3

where

and

  z2,3 =

   
2,3 =

z

(cid:88)
(cid:88)

x2,x3

x2,x3

    1   2 (x2)

  p   (x2)

  p   (x3)

    1   2 (x2)

    4   3 (x3)

    4   3 (x3)

    1   2 (x2)   (x2, x3)     4   3 (x3)

(28.8.13)

(28.8.14)

(28.8.15)

any choice of local normalisations z2   3, z3   2 that satis   es equation (28.8.13) su   ces to ensure that the
scale of the term approximation matches. for example, one may set

z2   3 = z3   2 =

z   
2,3
  z2,3

(28.8.16)

(cid:115)

once set, an approximation for the global normalisation constant of p is

z       z

(28.8.17)
the above gives a procedure for updating the terms     3   2 (x2) and     2   3 (x3). one then chooses another term
and updates the corresponding approximation factors. we repeat this until all approximation parameters
have converged (or a suitable termination criterion is reached in the case of non-convergence). the generic
procedure is outlined in algorithm(28.2).

comments on ep

    for the markov network example above, ep corresponds to belief propagation (the sum-product form
on the factor graph). this is intuitively clear since in both ep and bp the product of messages
incoming to a variable is proportional to the approximation of the marginal of that variable. a
di   erence, however, is the schedule:
in ep all messages corresponding to a term approximation are
updated simultaneously (in the above     3   2 (x2) and     2   3 (x3)), whereas in bp they are updated
sequentially.

    ep is a useful extension of bp to cases in which the bp messages cannot be easily represented;
introducing the exact factors    in the approximation   p increases the complexity of the approximation
to   p    which is resolved by projecting the approximation back to   p. in the case that the approximating
distribution   p is in the exponential family, the minimal kullback-leibler projection step equates to
matching moments of the approximating distribution to p   . see [264] for a more detailed discussion.
    in general there is no need to replace all terms in the joint distribution with factorised approximations.
one only needs that the resulting approximating distribution is tractable; this results in a structured
expectation propagation algorithm, see    g(28.12c).

    ep and its extensions are closely related to procedures such as tree-reweighting[308] and fractional

ep[317] designed to compensate for message overcounting e   ects.

28.9 map for markov networks

consider a markov network

p(x) =

1
z

ee(x)

then the most likely state is given by

   

x

= argmax

x

p(x) = argmax

x

e(x)

(28.9.1)

(28.9.2)

for a general markov network we cannot naively exploit id145 intuitions to    nd an exact
solution since the graph will generally be loopy. below we consider some general techniques that can be
used to approximate x   .

606

draft november 9, 2017

map for markov networks

algorithm 28.2 expectation propagation: approximation of p(x) = 1
z
1: choose terms     i(xi) to give a tractable distribution:

  p(x) =

1
  z

    i(xi)

2: initialise all parameters     i(xi).
3: repeat
4:
5:

select a term     i(xi) from   p to update.
replace the term     i(xi) by the exact term   i(xi) to form

  p          i(xi)

    j(xj)

6:

7:

find the parameters of     i(xi) by

    i(xi)     argmin
    i(xi)
(cid:89)
(cid:88)
set any proportionality terms of     i(xi) by requiring

kl(  p   |  p)
(cid:88)

(cid:89)

  i(xi)

x

    j(xj) =

    j(xj)

x

j

8: until converged
9: return

  p(x) =

1
  z

(cid:88)

(cid:89)

x

i

    i(xi)

    i(xi),

  z =

(cid:89)

i

(cid:89)

j(cid:54)=i

j(cid:54)=i

(cid:89)

i

(cid:81)

i   i(xi).

(28.8.18)

(28.8.19)

(28.8.20)

(28.8.21)

(28.8.22)

as an approximation to p(x), where   z approximates the normalisation constant z.

iterated conditional modes

a simple general approximate solution can be found as follows:    rst initialise all x at random. then select
a variable xi and    nd the state of xi that maximally improves e(x), keeping all other variables    xed.
one then repeats this selection and local maximal state computation until convergence. this axis aligned
optimisation procedure is called iterated conditional modes[38]. due to the markov properties it   s clear
that we can improve on this icm method by simultaneously optimising all variables conditioned on their
respective markov blankets (similar to the approach used in black-white sampling). another improvement
is to clamp a subset of the variables to reveal a singly-connected structure on the un-clamped variables, and
subsequently to    nd the exact map state on the un-clamped variables by a max-sum algorithm. one then
chooses a new subset of variables to clamp, thus    nding an approximate solution by solving a sequence of
tractable problems.

id209

a general approach is to decompose a di   cult optimisation problem into a set of easier problems. in this
approach we    rst identify tractable    slave    objectives es(x), s = 1, . . . , s such that the    master    objective
e(x) decomposes as

(cid:88)

e(x) =

es(x)

(28.9.3)

s

then the x that optimises the master problem is equivalent to optimising each slave problem es(xs) under
can be imposed by a lagrangian
the constraint that the slaves agree xs = x, s = 1, . . . , s [35]. this constraint

@@

draft november 9, 2017

607

map for markov networks

b

c

f

(a)

a

e

d

a

e

b

c

f

(b)

(a): a directed graph with edge
figure 28.13:
weights wij from node i to j with wij = 0 if no edge ex-
ists from i to j. (b): a graph cut partitions the nodes
into two groups s (blue) and t (red). the weight of
the cut is the sum of the weights of edges that leave s
(blue) and land in t (red). intuitively, it is clear that
after assigning nodes to state 1 (for blue) and 0 (red)
that the weight of the cut corresponds to the summed
weights of neighbours in di   erent states. here we high-
light those weight contributions. the non-highlighted
edges do not contribute to the cut weight. note that
only one of the edge directions contributes to the cut.

d

(cid:88)

(cid:88)
(cid:88)

s

s

l({xs} ,   ) =

es(xs) +   sxs

l(x,{xs} ,   ) =

finding the stationary point w.r.t. x, gives the constraint(cid:80)

  s (xs     x)

es(xs) +

s

(28.9.4)

s   s = 0, so that we may then consider

(28.9.5)

(28.9.6)

(28.9.7)

(28.9.8)

given   , we then optimise each slave problem

x

   
s = argmax

xs

(es(xs) +   sxs)

the lagrange dual, section(a.6.1), is given by (noting here we have a maximisation problem)

in this case the dual bound on the primal is

(es(xs) +   sxs)

xs

ls(  s) = max

(cid:88)
s ls(  s)     e(x

   

)

where x    is the solution of the primal problem x    = argmax
subgradient    method to minimise each ls(  s)

x

where    is a chosen positive constant. then we project,

(cid:48)
   
s =          x
  
s

(cid:88)

(cid:48)
s,

1
s

  

     =

which ensures that(cid:80)

s

(cid:48)
s         
  new
s =   

s   new

s = 0.

28.9.1 pairwise markov networks
consider a pairwise markov network p(x)     ee(x) with

e(x)    

fij (xi, xj) +

gi(xi, x0
i )

(cid:88)

i   j

(cid:88)

i

e(x). to update    one may use a    projected

(28.9.9)

(28.9.10)

(28.9.11)

where i     j denotes the set of neighbouring variables and f (xi, xj) = f (xj, xi). this means that an
undirected edge i     j in the graph corresponding to the markov network contributes a term f (xi, xj),
not 2f (xi, xj). here the terms f (xi, xj) represent pairwise interactions. the terms g(xi, x0
i ) represent
unary interactions, written for convenience in terms of a pairwise interaction with a    xed (non-variable) x0.
typically the term f (xi, xj) is used to ensure that neighbouring variables xi and xj are in similar states;
the term gi(xi, x0

i ) is used to bias xi to be close to a desired state x0
i .

608

draft november 9, 2017

(28.9.12)

(28.9.13)

map for markov networks

iterated conditional modes

such models have application in image restoration in which an observed noisy image x0 is to be cleaned, see
example(28.1) and    g(28.3). to do so we seek a clean image x for which each clean pixel value xi is close to
the observed noisy pixel value x0
i , whilst being in a similar state to its clean neighbours. in example(28.1)
we used icm to    nd the approximate most likely state by simply updating randomly chosen variables, one
at a time.

id209

for a binary markov network

e(x) =

1
2

xixjwij +

(cid:88)

ij

(cid:88)

i

cixi =

1
2

xtwx + xtc

we may de   ne tractable slave problems

1
2

es(x) =

xtwsx + xtcs

by identifying a set of tree structured matrices ws such that w =(cid:80)
c = (cid:80)

s ws and unary terms cs such that
s cs. the id209 technique then proceeds to solve each slave tree exactly, and subse-
quently updates    to push the tree solutions towards agreement [177]. similar general methods of solving
trees under the constraint of agreement are discussed in [308].

28.9.2 attractive binary markov networks

whilst, in general, no e   cient exact solution exists for the general map markov network problem, an
important tractable special case is discussed below. consider    nding the map of a markov network with
binary variables dom(xi) = {0, 1} and positive connections wij = wji     0. in this case our task is to    nd
the assignment x that maximises

cixi

(28.9.14)

where i     j denotes neighbouring variables.
for this particular case an e   cient exact map algorithm
exists for arbitrary topology of the interactions wij[134]. the algorithm    rst translates the map assignment
problem into an equivalent min s-t-cut problem[41], for which e   cient algorithms exist. in min s-t-cut, we
need a graph with positive weights on the edges. this is clearly satis   ed if wij > 0, although the bias term

(cid:88)

i   j

e(x)    

wiji [xi = xj] +

(cid:88)

i

(cid:80)

(cid:80)

i cixi needs to be addressed.

dealing with the bias terms

to translate the map assignment problem to a min-cut problem we need to deal with the additional term
i cixi. first consider the e   ect of including a new node x    and connecting this to each existing node i

with weight ci. using that for binary variables xi     {0, 1},

i [xi = xj] = xixj + (1     xi)(1     xj) = 2xixj     xi     xj + 1
(cid:88)

(cid:88)

this then adds a term
cii [xi = x   ] =

ci (xix    + (1     xi) (1     x   ))

i

i

if we set x    in state 1, this contributes

(cid:88)

cixi

i

draft november 9, 2017

(28.9.15)

(28.9.16)

(28.9.17)

609

map for markov networks

t

t

(a): a graph with bidirectional
figure 28.14:
weights wij = wji augmented with a source node s
and sink node t. each node has a corresponding bias
whose sign is indicated. the source node is linked
to the nodes corresponding to positive bias, and the
(b): a graph
nodes with negative bias to the sink.
cut partitions the nodes into two groups s (blue) and
t (red), where s is the union of the source node s
and nodes in state 1, t is the union of the sink node
t and nodes in state 0. the weight of the cut is the
sum of the edge weights from s (blue) to t (red). the
red lines indicate contributions to the cut, and can be
considered penalties since we wish to    nd the minimal
cut.

s

s

a+

e   

a

e

b   

c+

b

c

f +

(a)

f

(b)

d   

d

otherwise, if we set x    in state 0 we obtain

(cid:88)

i

(cid:88)

i

ci (1     xi) =    

cixi + const.

(28.9.18)

our requirement that the weights need to be positive can therefore be achieved by de   ning not a single
additional node x   , but rather two. we de   ne a source node xs, set to state 1 and connect it to those xi
which have positive ci, de   ning wsi = wis = ci. in addition we de   ne a sink node xt set to state 0 and
connect all nodes with negative ci, to xt, using weight wit = wti =    ci, (which is therefore positive). for
the source node clamped to xs = 1 and the sink node to xt = 0, then including the source and sink, we have

e(x) =

wiji [xi = xj] + const.

(28.9.19)

which is equal to the energy function, equation (28.9.14), with positive weights.

de   nition 28.1 (graph cut). for a graph g with nodes v1, . . . , vd, and weights wij > 0 a cut is a partition
of the nodes into two disjoint groups, called s and t . the weight of a cut is de   ned as the sum of the
weights that leave s and land in t , see    g(28.13).

for symmetric w, the weight of a cut corresponds to the sum of weights between mismatched neighbours,
see    g(28.13b). that is,

cut(x) =

wiji [xi (cid:54)= xj]

since i [xi (cid:54)= xj] = 1     i [xi = xj], we can de   ne the weight of the cut equivalently as
wiji [xi = xj] + const. =    e(x) + const.

wij (1     i [xi = xj]) =    

cut(x) =

(cid:88)

i   j

(28.9.20)

(28.9.21)

so that the minimal cut assignment will correspond to maximising e(x). in the markov network case, our
translation into a weighted graph with positive interactions then requires that we identify the source and

610

draft november 9, 2017

(cid:88)

i   j

(cid:88)
(cid:88)

i   j

i   j

map for markov networks

(a): noisy
figure 28.15:
greyscale image de   ned using
244 intensity labels
for each
(b): restored im-
pixel.
age. the   -expansion method
was used, with suitable interac-
tions w and bias c to ensure rea-
sonable results. from [50].

(a)

(b)

all other variables assigned to state 1 with s, and the sink and all variables in state 0 with t , see    g(28.14).
our task is then to    nd the minimal cut from s to t . a fundamental result in discrete mathematics is that
the min s-t-cut solution corresponds to the max-   ow solution from the source s to the sink t [41]. there are

e   cient algorithms for max-   ow, see for example [50], which take o(cid:0)d3(cid:1) operations or less, for a graph with
e   ciently in o(cid:0)d3(cid:1) operations.

d nodes. this means that one can    nd the exact map assignment of an attractive binary markov network
in maxflow.m we implement the ford-fulkerson (edmonds-karp-dinic

breadth    rst search variant)[94].

(cid:88)

i   j

(cid:88)

i   j

28.9.3 potts model
a markov network de   ned on variables with more than two states, xi     {0, 1, 2, . . . , s} is called a potts
model :

e(x) =

wiji [xi = xj] +

cii(cid:2)xi = x0

i

(cid:3)

(cid:88)

i

(28.9.22)

where we assume wij > 0 and the x0
i are known. this model has immediate application in non-binary image
restoration, and also in id91 based on a similarity score. this problem cannot be translated directly
into a graph-cut problem and no e   cient exact algorithm is known. a useful approach is to approximate
the problem as a sequence of binary problems, as we describe below.

potts to binary markov network translation

consider the   -expansion representation

xi = si   + (1     si)xold

i

(28.9.23)

where si     {0, 1} and        {0, 1, 2, . . . , s}. this restricts xi to be either the state xold
or   , depending on
the binary variable si. using a new binary vector variable s we can therefore restrict x to a subpart of the
full space and write a new objective function in terms of s alone (see below):

i

e(s) =

(cid:48)
ij

w

i [si = sj] +

(cid:48)
isi + const.
c

(28.9.24)

(cid:88)

i

for w(cid:48)
ij > 0. this new problem is of the form of an attractive binary markov network which can be solved
exactly using the graph cuts procedure. this translation imposes a constraint since the x cannot access all
the space, but enables us to solve the constrained problem e   ciently. we then choose another    value (at
random) and    nd the optimal s for the new   . in this way we are guaranteed to iteratively increase e.

draft november 9, 2017

611

for a given    and xold, the transformation of the potts model objective is given by using si     {0, 1} and
considering

i [xi = xj] = i(cid:104)

i = sj   + (1     sj)xold
si   + (1     si)xold
xold
i = xold
= sisjuij + aisi + bjsj + const.

= (1     si)(1     sj)i(cid:104)
    i(cid:104)

xold
i =   

(cid:105)

j

(cid:105)
+ i(cid:104)

(cid:105)

xold
j =   

xold
i = xold

j

j

(cid:105)
+ (1     si)sji(cid:104)
(cid:105)

with

uij     1     i(cid:104)

(cid:105)

+ si(1     sj)i(cid:104)

xold
i =   

xold
j =   

(cid:105)

further reading

+ sisj

(28.9.25)

(28.9.26)

with ai and bi de   ned in the obvious manner. by enumeration it is straightforward to show that uij is either
0, 1 or 2. using the mathematical identity, for si     {0, 1},

sisj =

1
2

(i [si = sj] + si + sj     1)

we can write,

(28.9.27)

uij
2

i [xi = xj] =

(i [si = sj] + si + sj) + aisi + bjsj + const.

(28.9.28)
hence terms wiji [xi = xj] translate to positive interaction terms i [si = sj] wijuij/2. all the unary terms
are easily exactly mapped into corresponding unary terms c(cid:48)
i de   ned as the sum of all unary terms
in si. this shows that the positive interaction wij in terms of the original variables x maps to a positive
interaction in the new variables s. hence we can    nd the maximal state of s using a graph cut algorithm.
a related procedure is described in [51].

isi for c(cid:48)

example 28.5 (potts model for image reconstruction). an example image restoration problem for nearest
neighbour interactions on a pixel lattice and suitably chosen w, c is given in    g(28.15). the images are
non-binary so that the optimal map assignment cannot be computed exactly in an e   cient way. the
alpha-expansion technique was used here combined with an e   cient min-cut approach to approximate the
map assignment, see [50] for details.

28.10 further reading

approximate id136 is a highly active research area and increasingly links to convex optimisation[49] are
being developed. see [308] for a general overview.

28.11 summary

    deterministic methods o   er an alternative to sampling techniques.
    for continuous distributions, perturbation approaches such as laplace   s method provide a simple approxi-

mation.

    variational bounding approaches, such as minimal kl divergence, can provide bounds on quantities of
interest, for example the normalisation constant of a distribution and the marginal likelihood. these are
part of a larger class of methods derived from convex analysis.

    the deterministic bounding approaches can be applied to other areas such as bounding the mutual infor-

mation.

    consistency methods such as loopy belief propagation can work extremely well when the structure of the
distribution is close to a tree. these methods have been very successful in id205 and error
correction.

612

draft november 9, 2017

exercises

    loopy belief propagation can also be motivated via the optimisation of the bethe free energy objective

function, although the method does not produce a bound on quantities of interest.

    for binary attractive markov networks we can    nd the map state exactly in polynomial time. this method
can be used as a subroutine in more complex multistate problems in which the map search problem is
broken into a sequence of binary map problems.

28.12 code

loopybp.m: loopy belief propagation (factor graph formalism)
demoloopybp.m: demo of loopy belief propagation
demomfbpgibbs.m: comparison of mean field, belief propagation and id150
demomrfclean.m: demo of analysing a dirty picture
maxflow.m: max-flow min-cut algorithm (ford-fulkerson)
binarymrfmap.m: optimising a binary markov network

28.13 exercises

exercise 28.1. the    le p.mat contains a distribution p(x, y, z) on ternary state variables. using brml-
toolbox,    nd the best approximation q(x, y)q(z) that minimises the id181 kl(q|p) and
state the value of the minimal id181 for the optimal q.

exercise 28.2. consider the pairwise markov network de   ned on a 2    2 lattice, as given in pmrf.mat.
using brmltoolbox,

1. find the optimal fully factorised approximation (cid:81)4
2. find the optimal fully factorised approximation(cid:81)4

factor graph formalism.

tions.

3. by pure enumeration, compute the exact marginals pi.

i=1 qbp

i

i=1 qm f

i

by loopy belief propagation, based on the

by solving the variational mean    eld equa-

4. averaged over all 4 variables, compute the mean expected deviation in the marginals

4(cid:88)

2(cid:88)

1
2

i=1

j=1

1
4

|qi(x = j)     pi(x = j)|

for both the bp and mf approximations, and comment on your results.

exercise 28.3. in loopybp.m the message schedule is chosen at random. modify the routine to choose a
schedule using a forward-reverse elimination sequence on a random spanning tree.

exercise 28.4 (double integration bounds). consider a bound

f (x)     g(x)

(cid:90) x

then for

  f (x)    

show that:

f (x)dx,

a

draft november 9, 2017

(cid:90) x

a

  g(x)    

g(x)dx

(28.13.1)

(28.13.2)

613

1.   f (x)       g(x),
2.   f (x)       g(x)
  f (x)    

(cid:90) x

a

for x     a
for all x, where

  f (x)dx

  g(x)    

(cid:90) x

a

  g(x)dx

exercises

(28.13.3)

the signi   cance is that this double integration (or summation in the case of discrete variables) is a general
procedure for generating a new bound from an existing bound [187].

exercise 28.5. this question concerns deriving both the standard mean    eld bound and more powerful lower
bounds for the id82.

1. starting from
ex     0

and using the double integration procedure, show that

ex     ea(1 + x     a)

(28.13.4)

2. by replacing x     stws for s     {0, 1}d, and a     hts+   derive a bound on the partition function of

a id82

z =

estws

(28.13.5)

s

3. show that this bound is equivalent to a naive mean    eld bound on the partition function. hint: optimise

with respect to       rst.

4. discuss how one can generate tighter bounds on the partition function of a boltzmann distribution by

further application of the double integration procedure.

exercise 28.6. consider a pairwise markov network

(cid:88)

p(x) =

extwx+btx

for symmetric w. consider the decomposition

i

w =

qiwi,

i = 1, . . . , i

where 0     qi     1 and (cid:80)
hint: consider (cid:104)ex(cid:105)     e(cid:104)x(cid:105). see also [308].

1
z

(cid:88)

(cid:90)

x

(cid:90)

j = log

p(x)f (x)

j    

p(x) log f (x)

x

i qi = 1, and the graph corresponding to each matrix wi is a tree. explain how to
form an upper bound on the normalisation z and discuss a naive method to    nd the tightest upper bound.

exercise 28.7. derive linkser   s bound on the mutual information, equation (28.6.13).

exercise 28.8. consider the average of a positive function f (x) with respect to a distribution p(x)

where f (x)     0. the simplest version of jensen   s inequality states that

1. by considering a distribution r(x)     p(x)f (x), and kl(q|r), for some variational distribution q(x),

show that

j        kl(q(x)|p(x)) + (cid:104)log f (x)(cid:105)q(x)

(28.13.10)

the bound saturates when q(x)     p(x)f (x). this shows that if we wish to approximate the average j,
the optimal choice for the approximating distribution q(x) depends on both the distribution p(x) and
integrand f (x).

614

draft november 9, 2017

(28.13.6)

(28.13.7)

(28.13.8)

(28.13.9)

exercises

2. furthermore, show that

j        kl(q(x)|p(x))     kl(q(x)|f (x))     h(q(x))

(28.13.11)

where h(q(x)) is the id178 of q(x). the    rst term encourages q to be close to p. the second
encourages q to be close to f , and the third encourages q to be sharply peaked.

exercise 28.9. for a markov network de   ned over d binary variables xi     {0, 1}, i = 1, . . . , d, we de   ne
(28.13.12)

extwx

p(x) =

1
z

show that

p(xi) =

z\i
z

where

z\i    

x1,...,xi   1,xi+1,...,xd

(cid:88)

extwx

(28.13.13)

(28.13.14)

and explain why a bound on the marginal p(xi) requires both upper and lower bounds on partition functions
z.

on replacing the observation factors p(vt|ht) by terms of the form ct exp(cid:0)

exercise 28.10. consider the model in example(28.2). implement a structured ep approximation based

    1
2 ht

t atht + ht

t bt

(cid:1).

exercise 28.11. consider a directed graph such that the capacity of an edge x     y is c(x, y)     0. the    ow
on an edge f (x, y)     0 must not exceed the capacity of the edge. the aim is to maximise the    ow from a
de   ned source node s to a de   ned sink node t. in addition    ow must be conserved such that for any node
other than the source or sink (y (cid:54)= s, y (cid:54)= t),

f (x, y) =

f (y, x)

(28.13.15)

(cid:88)

(cid:88)

x

x

a cut is de   ned as a partition of the nodes into two non-overlapping sets s and t such that s is in s and
t in t . show that:

1. the net    ow from s to t, val(f ) is the same as the net    ow from s to t :

(cid:88)

(cid:88)

val(f ) =

x   s,y   t

f (x, y)    

y   t ,x   s

f (y, x)

(28.13.16)

2. val(f )    

x   s,y   t f (x, y) namely that the    ow is upper bounded by the capacity of the cut.

the max-   ow-min-cut theorem further states that the maximal    ow is actually equal to the capacity of the
cut.

exercise 28.12 (potts to ising translation). consider the function e(x) de   ned on a set of multistate
variables dom(xi) = {0, 1, 2, . . . , s},

e(x) =

wiji [xi = xj] +

cii(cid:2)xi = x0

i

(cid:3)

(cid:88)

i

(28.13.17)

where wij > 0 and both the pixel states x0
maximisation of e(x). using the restricted parameterisation

i and ci are known. our interest is to    nd an approximate

xi = si   + (1     si)xold

(28.13.18)
where si     {0, 1} and for a given        {0, 1, 2, . . . , n}, show how the maximisation of e(x) is equivalent to
maximising   e(s) over the binary variables s, where

i

  e(s) =

(cid:48)
ij

w

i [si = sj] +

(cid:48)
isi + const.
c

(28.13.19)

(cid:88)

i

for w(cid:48)
exactly using the graph cuts procedure.

ij > 0. this new problem is of the form of an attractive binary markov network which can be solved

draft november 9, 2017

615

(cid:80)

(cid:88)

i   j

(cid:88)

i   j

exercise 28.13. consider an approximating distribution in the exponential family,

q(x) =

1

z(  )

e  tg(x)

we wish to use q(x) to approximate a distribution p(x) using the kl divergence kl(p|q).

1. show that optimally

(cid:104)g(x)(cid:105)p(x) = (cid:104)g(x)(cid:105)q(x)

2. show that a gaussian can be written in the exponential form

(cid:0)x   ,   2(cid:1) =

n

1

z(  )

e  tg(x)

exercises

(28.13.20)

(28.13.21)

(28.13.22)

(cid:0)x   ,   2(cid:1) to any distribution, in the minimal kl(p|q)

where g1(x) = x, g2(x) = x2 for suitably chosen   .

3. hence show that the optimal gaussian    t n

sense, matches the moments:

  2 =(cid:10)x2(cid:11)

p(x)     (cid:104)x(cid:105)2

p(x)

   = (cid:104)x(cid:105)p(x) ,
(cid:80)

(cid:88)

z(w, b) =

e

x

i,j wij xixj +(cid:80)

i bixi

exercise 28.14. for a pairwise binary markov network, p, with partition function

show that the mean (cid:104)xi(cid:105)p can be computed using a generating function approach

(cid:88)

(cid:80)

i   j wij xixj +(cid:80)

d
dbi

log z(w, b) =

1

z(w, b)

x

xie

i bixi = (cid:104)xi(cid:105)p

and that similarly the covariance is given by

(cid:104)xixj(cid:105)p     (cid:104)xi(cid:105)p (cid:104)xj(cid:105)p =

d2

dbidbj

log z(w, b)

(28.13.23)

(28.13.24)

(28.13.25)

(28.13.26)

by replacing log z(w, b) by the mean    eld bound b(  ) in equation (28.4.13), show that the approximate
mean and variance given by the naive mean    eld theory is equivalent to those obtained on replacing log z
by the lower bound in the above generating function.

exercise 28.15. the naive mean    eld theory applied to a pairwise markov network

(cid:80)
p(x)     e

i,j wij xixj +(cid:80)

dom(xi) = {0, 1}, gives a factorised approximation q(x) = (cid:81)

i bixi

this we can approximate

(28.13.27)

i q(xi), based on minimising kl(q|p). using

(cid:104)xixj(cid:105)p     (cid:104)xi(cid:105)q (cid:104)xj(cid:105)q ,

i (cid:54)= j

(28.13.28)

to produce a better, non-factorised approximation to (cid:104)xixj(cid:105)p we could    t a non-factorised q. the linear-
response method[167] may also be used, based on a perturbation expansion of the free energy. alternatively,
show that

(cid:104)xixj(cid:105)p = p(xi = 1, xj = 1) = p(xi = 1|xj = 1)p(xj = 1)

(28.13.29)

explain how this can be used to form an improved non-factorised approximation to (cid:104)xixj(cid:105)p.
exercise 28.16. derive the ep updates equation (28.8.8) and equation (28.8.9).

616

draft november 9, 2017

exercises

exercise 28.17. consider    tting a univariate gaussian q(x) = n
imising kl(q|p) with respect to   ,   2.

(cid:0)x   ,   2(cid:1) to a distribution p(x) by min-

1. show that, provided all quantities are well de   ned, the optimal variance and mean of the approximating

gaussian satisfy the implicit equations

(cid:68) d2

  2 =    

1

dx2 log p(x)

(cid:69)

,

q(x)

(cid:28) d

(cid:29)

log p(x)

= 0

q(x)

(28.13.30)

dx

2. hence relate the optimal variance to the maximal and minimal    local curvature    of p(x):

   

1
d2

dx2 log p(x)       2        

1
d2
dx2 log p(x)

maxx

3. consider a distribution p(x) that is a mixture of well-separated gaussians, p(x) =(cid:80)

minx

(28.13.31)

(cid:0)x   i,   2

(cid:1).

i

i pin

show that, to a good approximation, the optimal choice is to set the mean of the approximating distri-
bution to one of the components,    =   i. show furthermore that

  2
min

(cid:46)   2 (cid:46)   2

max

(28.13.32)

where   2
explain therefore why the optimal variance   2 will be smaller than the variance of p(x).

max are the minimal and maximal variances of the gaussian components of p(x), and

min,   2

draft november 9, 2017

617

exercises

618

draft november 9, 2017

part vi

appendix

619

appendix a

background mathematics

a.1 id202

a.1.1 vector algebra

let x denote the n-dimensional column vector with components

               

                x1

x2
...
xn

the vector with all elements equal to 1 is written 1, and similarly the vector with all zero values is written
0.

de   nition a.1 (scalar product). the scalar product w    x is de   ned as:

n(cid:88)

i=1

w    x =

wixi = wtx

the length of a vector is denoted |x|, the squared length is given by

|x|2 = xtx = x2 = x2

1 + x2

2 +        + x2

n

a unit vector x has |x| = 1. the scalar product has a natural geometric interpretation as:

w    x = |w||x| cos(  )

(a.1.1)

(a.1.2)

(a.1.3)

where    is the angle between the two vectors. thus if the lengths of two vectors are    xed their inner product
is largest when    = 0, whereupon one vector is a constant multiple of the other.
if the scalar product
xty = 0, then x and y are orthogonal (they are at right angles to each other). a set of vectors orthonormal
if they are mutually orthogonal and have unit length.

de   nition a.2 (linear dependence). a set of vectors x1, . . . , xn is linearly dependent if there exists a
vector xj that can be expressed as a linear combination of the other vectors. if the only solution to

  ixi = 0

(a.1.4)

n(cid:88)

i=1

is for all   i = 0, i = 1, . . . , n, the vectors x1, . . . , xn are linearly independent.

621

id202

figure a.1: resolving a vector a into components along the orthogonal
directions e and e   . the projection of a onto these two directions are
lengths    and    along the directions e and e   .

a.1.2 the scalar product as a projection

suppose that we wish to resolve the vector a into its components along the orthogonal directions speci   ed

by the unit vectors e and e   , see    g(a.1). that is |e| = |e|

scalar values    and    such that

    = 1 and e    e    = 0. we are required to    nd the

a =   e +   e   

from this we obtain

a    e =   e    e +   e   

   e,

a    e   

=   e    e   

+   e   

   e   

from the orthogonality and unit lengths of the vectors e and e   , this becomes simply

a    e =   ,

a    e   

=   

this means that we can write the vector a in terms of the orthonormal components e and e    as

a = (a    e) e + (a    e   

) e   

(a.1.5)

(a.1.6)

(a.1.7)

(a.1.8)

the scalar product between a and e projects the vector a onto the (unit) direction e. the projection of a
vector a onto a direction speci   ed by general f is a  f

|f|2 f .

a.1.3 lines in space

a line in 2 (or more) dimensions can be speci   ed as follows. the vector of any point along the line is given,
for some s, by the equation

p = a + su,

s     r.

(a.1.9)

where u is parallel to the line, and the line passes through the point a, see    g(a.2). an alternative
speci   cation can be given by realising that all vectors along the line are orthogonal to the normal of the
line, n (u and n are orthonormal). that is

(p     a)    n = 0     p    n = a    n

(a.1.10)

if the vector n is of unit length, the right hand side of the above represents the shortest distance from the
origin to the line, drawn by the dashed line in    g(a.2) (since this is the projection of a onto the normal
direction).

a.1.4 planes and hyperplanes

to de   ne a two-dimensional plane (in arbitrary dimensional space) one may specify two vectors u and v that
lie in the plane (they need not be mutually orthogonal), and a position vector a in the plane, see    g(a.3).
any vector p in the plane can then be written as

p = a + su + tv,

(s, t)     r.

(a.1.11)

figure a.2: a line can be speci   ed by some position vector on
the line, a, and a unit vector along the direction of the line, u.
in 2 dimensions, there is a unique direction, n, perpendicular to
the line. in three dimensions, the vectors perpendicular to the
direction of the line lie in a plane, whose normal vector is in the
direction of the line, u.

622

draft november 9, 2017

e*e    aee*aapnuid202

figure a.3: a plane can be speci   ed by a point in the
plane, a and two, non-parallel directions in the plane,
u and v. the normal to the plane is unique, and in
the same direction as the directed line from the origin
to the nearest point on the plane.

an alternative de   nition is given by considering that any vector within the plane must be orthogonal to the
normal of the plane n.

(p     a)    n = 0     p    n = a    n

(a.1.12)

the right hand side of the above represents the shortest distance from the origin to the plane, drawn by the
dashed line in    g(a.3). the advantage of this representation is that it has the same form as a line. indeed,
this representation of (hyper)planes is independent of the dimension of the space. in addition, only two
quantities need to be de   ned     the normal to the plane and the distance from the origin to the plane.

a.1.5 matrices
an m    n matrix a is a collection of scalar values arranged in a rectangle of m rows and n columns. a
vector can be considered as an n    1 matrix. the i, j element of matrix a can be written aij or more
conventionally aij. where more clarity is required, one may write [a]ij.

de   nition a.3 (matrix addition). for two matrices a and b of the same size,

[a + b]ij = [a]ij + [b]ij

(a.1.13)

de   nition a.4 (id127). for an l by n matrix a and an n by m matrix b, the product
ab is the l by m matrix with elements

[ab]ik =

[a]ij [b]jk ;

i = 1, . . . , l k = 1, . . . , m .

(a.1.14)

note that in general ba (cid:54)= ab. when ba = ab we say that they a and b commute. the matrix i is
the identity matrix , necessarily square, with 1   s on the diagonal and 0   s everywhere else. for clarity we may
also write im for a square m    m identity matrix. then for an m    n matrix a, ima = ain = a. the
identity matrix has elements [i]ij =   ij given by the kronecker delta:

n(cid:88)

j=1

(cid:26) 1 i = j

0 i (cid:54)= j

  ij    

(a.1.15)

bt(cid:105)

kj

(cid:104)
(cid:0)bt(cid:1)t

de   nition a.5 (transpose). the transpose bt of the n by m matrix b is the m by n matrix with
components

= bjk ;

k = 1, . . . , m j = 1, . . . , n .

(a.1.16)

= b and (ab)t = btat. if the shapes of the matrices a,b and c are such that it makes sense to

calculate the product abc, then

(abc)t = ctbtat

draft november 9, 2017

(a.1.17)

623

anuvpa square matrix a is symmetric if at = a. a square matrix is called hermitian if a = at    where    
denotes the complex conjugate operator. for hermitian matrices, the eigenvectors form an orthogonal set
with real eigenvalues.

id202

de   nition a.6 (trace).

(cid:88)

i

(cid:88)

i

  i

(a.1.18)

trace (a) =

aii =

where   i are the eigenvalues of a.

if we de   ne ui to be the vector with zeros everywhere expect for the ith entry, then a vector can be expressed

i xiui. then a linear transformation of x is given by

ax =

xiaui =

xiai

(a.1.19)

a.1.6 linear transformations

as x =(cid:80)

(cid:88)

(cid:88)

i

i

where ai is the ith column of a.

rotations

(cid:19)

(cid:18)
(cid:19)
(cid:18) cos   
(cid:18) cos        sin   

r =

sin   

,

sin   

cos   

    sin   
cos   

(cid:19)

which thus form the columns of the rotation matrix:

the unit vectors (1, 0)t and (0, 1)t under rotation by    radians transform to the vectors

(a.1.20)

(a.1.21)

multiplying a vector by r, rx, rotates the vector through    radians.

a.1.7 determinants

de   nition a.7 (determinant). for a square matrix a, the determinant is the volume of the transformation
of the matrix a (up to a sign change). that is, we take a hypercube of unit volume and map each vertex
under the transformation. the volume of the resulting object is de   ned as the determinant. writing
[a]ij = aij,

det

det

a21 a22

= a11a22     a21a12

(cid:19)
(cid:18)a11 a12
      a11 a12 a13
       = a11 (a22a33     a23a32)     a12 (a21a33     a31a23) + a13 (a21a32     a31a22)
(cid:18)a22 a23
(cid:19)

(cid:18)a21 a23

(cid:18)a21 a22

a21 a22 a23
a31 a32 a33

(cid:19)

(cid:19)

the determinant in the (3    3) case has the form

a11det

a32 a33

    a12det

a31 a33

+ a13det

a31 a32

(a.1.22)

(a.1.23)

(a.1.24)

the determinant of the (3    3) matrix a is given by the sum of terms (   1)i+1a1idet (ai) where ai is
the (2    2) matrix formed from a by removing the ith row and column. this form of the determinant

624

draft november 9, 2017

id202

(cid:16)

at(cid:17)

det

= det (a)

generalises to any dimension. that is, we can de   ne the determinant recursively as an expansion along the
top row of determinants of reduced matrices. the absolute value of the determinant is the volume of the
transformation.

for square matrices a and b of equal dimensions,

det (ab) = det (a) det (b) ,

det (i) = 1     det(cid:0)a   1(cid:1) = 1/det (a)

de   nition a.8 (orthogonal matrix). a square matrix a is orthogonal if aat = i = ata. from the
properties of the determinant, we see therefore that an orthogonal matrix has determinant   1 and hence
corresponds to a volume preserving transformation.

de   nition a.9 (matrix rank). for an m    n matrix x the rank of x is the maximum number of linearly
independent columns (or equivalently rows). the matrix is full rank if it has rank equal to min(m, n)    
otherwise the matrix is rank de   cient. a square matrix that is rank de   cient is singular.

a.1.8 matrix inversion

de   nition a.10 (matrix inversion). for a square matrix a, its inverse satis   es

a   1a = i = aa   1

(a.1.27)
it is not always possible to    nd a matrix a   1 such that a   1a = i, in which case a is singular . geo-
metrically, singular matrices correspond to projections: if we transform each of the vertices v of a binary
hypercube using av, the volume of the transformed hypercube is zero. hence if det (a) = 0, the matrix a is
a form of projection or    collapse    which means a is singular. given a vector y and a singular transformation,
a, one cannot uniquely identify a vector x for which y = ax. provided the inverses exist

for a non-square matrix a such that aat is invertible, then the right pseudo inverse, de   ned as

(ab)

   1 = b   1a   1

= at(cid:16)
(cid:16)

aat(cid:17)   1
(cid:17)   1

ata

at

a   

a   

=

and satis   es a   a = i.

satis   es aa    = i. the left pseudo inverse is given by

de   nition a.11 (matrix inversion lemma (woodbury formula)). provided the appropriate inverses exist:

(cid:16)

a + uvt(cid:17)   1
a + uvt(cid:17)
(cid:16)

det

(cid:16)

(cid:17)   1

= a   1     a   1u
(cid:16)

= det (a) det

i + vta   1u

(cid:17)

i + vta   1u

draft november 9, 2017

vta   1

(a.1.31)

(a.1.32)

625

(a.1.25)

(a.1.26)

(a.1.28)

(a.1.29)

(a.1.30)

id202

(a.1.33)

(a.1.34)

de   nition a.12 (block matrix inversion). for matrices a, b, c, d, provided the appropriate inverses

exist:(cid:20) a b

(cid:21)   1

c d

(cid:34)

=

(cid:0)a     bd   1c(cid:1)   1 bd   1
(cid:0)a     bd   1c(cid:1)   1
   d   1c(cid:0)a     bd   1c(cid:1)   1 d   1c(cid:0)a     bd   1c(cid:1)   1 bd   1
(cid:19)
(cid:18)a b

   

, the inverse matrix has elements

(cid:35)

a.1.9 computing the matrix inverse

for a 2    2 matrix, a =

(cid:18) d    b
(cid:19)

a

   c

c d
= a   1

1

ad     bc

the quantity ad     bc is the determinant of a. there are many ways to compute the inverse of a general
matrix, and we refer the reader to more specialised texts, such as [280, 130].
if one wants to solve only a linear system, ax = b, algebraically, the solution is given by x = a   1b. this
would suggest that one needs to compute the n   n matrix a   1. however, in practice, a   1 is not explicitly

required     only x is needed. this can be obtained more rapidly and with greater numerical precision using
gaussian elimination[280, 130].

a.1.10 eigenvalues and eigenvectors

the eigenvectors of a matrix correspond to a natural coordinate system in which the geometric transforma-
tion represented by a can be most easily understood.

de   nition a.13 (eigenvalues and eigenvectors). for an n    n square matrix a, e is an eigenvector of a
with eigenvalue    if

ae =   e

(a.1.35)

geometrically, the eigenvectors are special directions such that the e   ect of the transformation a along a
direction e is simply to scale the vector e. for a rotation matrix r in general there will be no direction
preserved under the rotation so that the eigenvalues and eigenvectors are complex valued (which is why the
fourier representation, which corresponds to representation in a rotated basis, is necessarily complex).

for an (n    n) dimensional matrix, there are (including repetitions) n eigenvalues, each with a corresponding
eigenvector. we can reform equation (a.1.35) as

(a       i) e = 0

(a.1.36)
we can write equation (a.1.36) as be = 0, where b     a       i. if b has an inverse, then a solution is
e = b   10 = 0, which trivially satis   es the eigen-equation. for any non-trivial solution to the problem
be = 0, we therefore need b to be non-invertible. this is equivalent to the condition that b has zero
determinant. hence    is an eigenvalue of a if

det (a       i) = 0

(a.1.37)

this is known as the characteristic equation. this determinant equation will be a polynomial in    of degree
n and the resulting equation is known as the characteristic polynomial. once we have found an eigenvalue,
the corresponding eigenvector can be found by substituting this value for    in equation (a.1.35) and solving
the linear equations for e. it may be that for an eigenvalue    the eigenvector is not unique and there is a
space of corresponding vectors. an important relation between the determinant and the eigenvalues of a
matrix is

det (a) =

i=1

626

  i

(a.1.38)

draft november 9, 2017

n(cid:89)

id202

(cid:88)

hence a matrix is singular if it has a zero eigenvalue. the trace of a matrix can be expressed as

trace (a) =

  i

(a.1.39)

i

for a real symmetric matrix a = at, and eigenvectors ei, ej, then (ei)tej = 0 if the eigenvalues   i and   j
are di   erent. this can be shown by considering:

aei =   iei     (ej)taei =   i(ej)tei

since a is symmetric, then

(a.1.40)

((ej)ta)ei = (aej)tei =   j(ej)tei       i(ej)tei =   j(ej)tei

(a.1.41)
if   i (cid:54)=   j, this condition can be satis   ed only if (ej)tei = 0, namely that the eigenvectors are orthogonal.
de   nition a.14 (trace-log formula). for a positive de   nite matrix a,

trace (log a)     log det (a)

(a.1.42)

note that the above logarithm of a matrix is not the element-wise logarithm. in matlab the required
function is logm. in general for an analytic function f (x), f (m) is de   ned via the power-series expansion
of the function. on the right, since det (a) is a scalar, the logarithm is the standard logarithm of a scalar.

a.1.11 matrix decompositions

de   nition a.15 (spectral decomposition). a real n   n symmetric matrix a has an eigen-decomposition

(a.1.43)

(a.1.44)

where   i is the eigenvalue of eigenvector ei and the eigenvectors form an orthogonal set,

in matrix notation

a = e  et

where e =(cid:2)e1, . . . , en(cid:3) is the matrix of eigenvectors and    the corresponding diagonal eigenvalue matrix.

(a.1.45)

more generally, for a square non-symmetric diagonalisable a we can write

a = e  e   1

(a.1.46)

de   nition a.16 (singular value decomposition). the svd decomposition of a n    p matrix x is

x = usvt

(a.1.47)

where dim u = n    n with utu = in. also dim v = p    p with vtv = ip.
the matrix s has dim s = n    p with zeros everywhere except on the diagonal entries. the singular values
are the diagonal entries [s]ii and are non-negative. the singular values are ordered so that the upper the left
diagonal element of s contains the largest singular value and sii     sjj for i < j. assuming n < p (otherwise
   thin    svd with n > p only the    rst p columns of u and s are computed giving a decomposition

transpose x), the computational complexity of computing the svd is o(cid:0)4n2p + 8np2 + 9n3(cid:1) [130]. for the

x = upspvt

where up has dimension n    p, and sp is the p    p diagonal matrix of singular values.

draft november 9, 2017

(a.1.48)

627

a =

n(cid:88)
(cid:0)ei(cid:1)t

i=1

  ieiet
i

(cid:0)ei(cid:1)t

ei

ej =   ij

de   nition a.17 (quadratic form).

xtax + xtb

multivariate calculus

(a.1.49)

de   nition a.18 (positive de   nite matrix). a symmetric matrix a with the property that xtax     0 for
any vector x is called positive semide   nite. a symmetric matrix a, with the property that xtax > 0 for
any vector x (cid:54)= 0 is called positive de   nite. a positive de   nite matrix has full rank and is thus invertible.
using the eigen-decomposition of a,

(cid:88)

(cid:88)

(cid:16)

xtei(cid:17)2

xtax =

  ixtei(ei)tx =

  i

(a.1.50)

i

i

which is greater than zero if and only if all the eigenvalues are positive. hence a is positive de   nite if and
only if all its eigenvalues are positive.

eigenfunctions

(cid:90)
(cid:90)

x

(cid:48)

(cid:48)
, x)  a(x) =   a  a(x

)

k(x

(a.1.51)

the eigenfunctions of a real symmetric kernel, k(x(cid:48), x) = k(x, x(cid:48)) are orthogonal:

   
b (x) =   ab

(cid:88)

x

  a(x)  

(a.1.52)
where      (x) is the complex conjugate of   (x). a kernel has a decomposition (provided the eigenvalues are
countable)

k(xi, xj) =

        (xi)  

   
  (xj)

then(cid:88)

i,j

  

yik(xi, xj)yj =

(cid:88)

i,j,  

    yi    (xi)  

   
  (xj)yj =

(cid:88)

  

    

(cid:32)(cid:88)
(cid:124)

i

(cid:33)
(cid:125)

(cid:32)(cid:88)
(cid:124)

i

yi    (xi)

(cid:123)(cid:122)

   
  (xi)

yi  

(cid:123)(cid:122)

(a.1.53)

(a.1.54)

(cid:33)
(cid:125)

z   
which is greater than zero if the eigenvalues are all positive (since for complex z, zz   
are uncountable the appropriate decomposition is

zi

i

(cid:90)

k(xi, xj) =

  (s)  (xi, s)  

   

(xj, s)ds

    0). if the eigenvalues

(a.1.55)

a.2 multivariate calculus

de   nition a.19 (partial derivative). consider a function of n variables, f (x1, x2, . . . , xn)     f (x). the
partial derivative of f w.r.t. xi is de   ned as the following limit (when it exists)

   f
   xi

= lim
h   0

f (x1, . . . , xi   1, xi + h, xi+1, . . . , xn)     f (x)

h

the gradient vector of f is denoted    f or g:

         

             f

...

   x1

   f
   xn

   f (x)     g(x)    

(a.2.1)

(a.2.2)

628

draft november 9, 2017

multivariate calculus

figure a.4: interpreting the gradient. the ellipses are contours
of constant function value, f = const. at any point x, the gradi-
ent vector    f (x) points along the direction of maximal increase
of the function.

a.2.1

interpreting the gradient vector

consider a function f (x) that depends on a vector x. we are interested in how the function changes when
the vector x changes by a small amount : x     x +   , where    is a vector whose length is very small.
according to a taylor expansion, the function f will change to

f (x +   ) = f (x) +

(cid:88)

i

  i

   f
   xi

+ o(cid:0)  2(cid:1)

we can interpret the summation above as the scalar product between the vector    f with components
[   f ]i =    f

   xi

and   .

f (x +   ) = f (x) + (   f )       + o(cid:0)  2(cid:1)

the gradient points along the direction in which the function increases most rapidly. to see this, consider
a direction   p (a unit length vector). then a displacement,    units along this direction changes the function
value to

f (x +      p)     f (x) +      f (x)      p

the direction   p for which the function has the largest change is that which maximises the overlap

   f (x)      p = |   f (x)||  p| cos    = |   f (x)| cos   

(a.2.5)

(a.2.6)

where    is the angle between   p and    f (x). the overlap is maximised when    = 0, giving   p =    f (x)/|   f (x)|.
hence, the direction along which the function changes the most rapidly is along    f (x).
a.2.2 higher derivatives

the second derivative of an n-variable function is de   ned by

i = 1, . . . , n; j = 1, . . . , n

(cid:18)    f

(cid:19)

   xj

   
   xi

which is usually written

   2f

   xi   xj

,

i (cid:54)= j

   2f
2 ,
   xi

i = j

if the partial derivatives    2f /   xi   xj and    2f /   xj   xi exist then

   2f /   xi   xj =    2f /   xj   xi .

                2f

   x1

...

   2f

2

            

. . .

. . .

   2f

   x1   xn

...

   2f
2
   xn

hf (x) =

   x1   xn

draft november 9, 2017

this is also denoted by       f . these n2 second partial derivatives are represented by a square, symmetric
matrix called the hessian matrix of f (x).

(a.2.3)

(a.2.4)

(a.2.7)

(a.2.8)

(a.2.9)

(a.2.10)

629

x1x2f(x)de   nition a.20 (chain rule). let each xj be parameterized by u1, . . . , um, i.e. xj = xj(u1, . . . , um).

n(cid:88)

j=1

   f
   u  

=

   f
   xj

   xj
   u  

or in vector notation

   

   u  

f (x(u)) =    f t(x(u))

   x(u)
   u  

inequalities

(a.2.11)

(a.2.12)

de   nition a.21 (directional derivative). assume f is di   erentiable. we de   ne the scalar directional
derivative (dvf )(x   ) of f in a direction v at a point x   . let x = x    + hv, then

(cid:12)(cid:12)(cid:12)(cid:12)h=0

=

(cid:88)

j

vj

   f
   xj

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x=x   

(dvf )(x   

) =

f (x   

d
dh

+ hv)

a.2.3 matrix calculus

=    f tv

(a.2.13)

de   nition a.22 (derivative of a matrix trace). for matrices a and b

   
   a

trace (ab)     bt

de   nition a.23 (derivative of log det (a)).

    log det (a) =    trace (log a) = trace(cid:0)a   1   a(cid:1)

so that

log det (a) = a   t

   
   a

de   nition a.24 (derivative of a matrix inverse). for an invertible matrix a,

   a   1        a   t   aa   1

a.3

inequalities

a.3.1 convexity

(a.2.14)

(a.2.15)

(a.2.16)

(a.2.17)

de   nition a.25 (convex function). a function f (x) is de   ned as convex if for any x, y and 0            1

f (  x + (1       )y)       f (x) + (1       )f (y)

if    f (x) is convex, f (x) is called concave.

(a.3.1)

an intuitive picture is given by considering the quantity   x + (1      )y. as we vary    from 0 to 1, this traces
points between x (   = 0) and y (   = 1). hence for    = 0 we start at the point x, f (x) and as    increase
trace a straight line towards the point y, f (y) at    = 1. convexity states that the function f always lies

630

draft november 9, 2017

multivariate optimisation

below this straight line. geometrically this means that the function f (x) is always always non-decreasing.
hence if d2f (x)/dx2     0 the function is convex. as an example, the function log x is concave since its second
derivative is negative:

d
dx

log x =

1
x

,

d2
dx2 log x =    

1
x2

a.3.2 jensen   s inequality

for a convex function, f (x), it follows directly from the de   nition of convexity that

f ((cid:104)x(cid:105)p(x))     (cid:104)f (x)(cid:105)p(x)

for any distribution p(x).

a.4 optimisation

(a.3.2)

(a.3.3)

de   nition a.26 (critical point). when all    rst-order partial derivatives at a point are zero (i.e.    f = 0)
then the point is said to be a stationary or critical point. a critical point can correspond to a minimum,
maximum or saddle point of the function.

there is a minimum of f at x    if f (x   )     f (x) for all x su   ciently close to x   . this requires x    to be a
stationary point,    f (x   ) = 0. the taylor expansion of f at the optimum is given by

@@

f (x   

+ hv) = f (x   

) +

h2vthf v + o(h3)

1
2

(a.4.1)

thus the minimum condition requires that vthf v     0, i.e. the hessian is non-negative de   nite.
de   nition a.27 (conditions for a minimum). su   cient conditions for a minimum at x    are (i)    f (x   ) = 0
and (ii) hf (x   ) is positive de   nite.

for a quadratic function f (x) = 1

2 xtax     btx + c, with symmetric a the condition    f (x   ) = 0 reads:

ax   

    b = 0

if a is invertible this equation has the unique solution x    = a   1b.
corresponds to the minimum point.

(a.4.2)
if a is positive de   nite then x   

a.5 multivariate optimisation

in most cases the optima of functions cannot be found by algebraic means alone and numerical techniques are
required. the search techniques that we consider here are iterative, i.e. we proceed towards the minimum x   
by a sequence of steps. perhaps the simplest approach to minimising a multivariate function f (x) is to break
the problem into a sequence of one-dimensional problems. by initialising the elements of x at random, one
then selects a component of x to update, keeping all others    xed. this coordinatewise optimisation decreases
the function via a sequence of one-dimensional minimisations. whilst such a procedure is simple, it can be
ine   cient in high-dimensions, particularly when there are strong dependencies between the components in
x. for this reason, it is useful to use gradient-based procedures that take the local geometry of the objective
into account. we will consider a general class of iterative gradient-based method for which, on the kth step,
we take a step of length   k in the direction pk,

xk+1 = xk +   kpk

draft november 9, 2017

(a.5.1)

631

multivariate optimisation

figure a.5: optimisation using line search along
steepest descent directions. following the steepest
way downhill from a point (and continuing for a    -
nite time in that direction) doesn   t always result in
the fastest way to get to the bottom.

a.5.1 id119 with    xed stepsize
locally, if we are at point xk, we can decrease f (x) by taking a step in the direction    g(x). to see why
id119 works, consider the general update

xk+1 = xk          xf

for small    we can expand f around xk using taylor   s theorem:

(a.5.2)

f (xk +   kpk)     f (xk)       ||   xf||

(a.5.3)
so that the change in f is    f =      ||   xf||2. if    is non-in   nitesimal, it is always possible that we will
step over the true minimum. making    very small guards against this, but means that the optimization
process will take a very long time to reach a minimum. a simple idea that can improve convergence of
id119 is to include at each iteration a proportion of the change from the previous iteration,
pk =    gk       gk   1, where    is the momentum coe   cient.
an unfortunate aspect of id119 is that the change in the function value depends on the coordinate
de   ne   f (y)     f (x).
system. consider a new coordinate system
then the change in the function   f under a gradient update is

x = my for an invertible square matrix m.

@@
@@

      f       f (y          y   f )       f (y)          ||   y   f||2

in the original coordinate system, the change in x is

since    y   f = mt   xf (x), transforming the change in y to a change in x we have

   x =         xf

m   y =      mmt   xf (x)

which, unless mmt = i, is not equal to    x. similarly the change in the function value is

      f =         xf (x)tmmt   xf (x)

which, except for an orthogonal m, is not equal to    f =      ||   xf||2.
a.5.2 id119 with line searches

(a.5.4)

(a.5.5)

an extension to the idea of id119 is to choose the direction of steepest descent, as indicated
by the gradient g, but to calculate the value of the step to take which most reduces the value of f when
moving in that direction. this involves solving the one-dimensional problem of minimizing f (xk +   kgk)
with respect to   k, and is known as a line search. to    nd the optimal step size at the k-th step, we choose
  k to minimize f (xk +   kpk). so setting f (  ) = f (xk +   pk), at this step we solve the one-dimensional
minimization problem for f (  ). thus our choice of   k =       will satisfy f (cid:48)(  k) = 0. now

(cid:48)

f

(  k) =

f (  k + h)|h=0 =

d
dh

f (xk +   kpk + hpk)|h=0

d
dh
d
dh

=

f (xk+1 + hpk)|h=0 = (dpk f )(xk+1) =    f t(xk+1)pk

(a.5.6)
so f (cid:48)(  k) = 0 means the directional derivative in the search direction must vanish at the new point and
this gives condition 0 = gt
k+1pk. if the step size is chosen to reduce f as much as it can in that direction,
then no further decrease in e can be made by moving in that direction for the moment. thus the next step
will have no component in that direction and will be at right angles to the previous just taken. this can
lead to zig-zag type behaviour in the optimisation.

632

draft november 9, 2017

multivariate optimisation

a.5.3 minimising quadratic functions using line search

consider minimising the quadratic function

f (x) =

1
2

xtax     btx + c

(a.5.7)

where a is positive de   nite and symmetric. although we know where the minimum of this simple function,
based on using id202, we wish to use this function as a toy model for more complex functions. as we
   zoom in    to the minimum of a smooth function, it will increasingly appear quadratic. hence at such small
scales, methods that work for the quadratic case should work in general smooth functions. if the general
function looks roughly quadratic on the larger scale, then these methods will also work well in that case.
one approach is to search along a particular direction p, and    nd a minimum along this direction. we can
then search for a deeper minimum by looking in di   erent directions. that is, we can search    rstly along a
line x +   p such that the function attains a minimum. this has solution,

   =

(b     ax)    p

ptap

=       f (x)    p

ptap

(a.5.8)

how should we now choose the next line search direction pnew? it would seem sensible to choose successive
line search directions p according to pnew =       f (x), so that each time we minimise the function along
the line of steepest descent. however, this is generally not the optimal choice, see    g(a.5). if the matrix
a were diagonal, then the minimisation is straightforward and can be carried out independently for each
dimension. if we could therefore    nd an invertible matrix p with the property that ptap is diagonal then
the solution is easy since for

  f (  x) =

1
2

  xtptap  x     btp  x + c

(a.5.9)

with x = p  x, we can compute the minimum for each dimension of   x separately and then retransform to
   nd x    = p  x   . the columns of such a matrix p are called conjugate vectors.

de   nition a.28 (conjugate vectors). the vectors pi, i = 1, . . . , k are called conjugate to the matrix a, if
and only if for i, j = 1, . . . , k and i (cid:54)= j:
i api > 0 .

pt
i apj = 0 and pt

(a.5.10)

the two conditions guarantee that conjugate vectors are linearly independent: assume that

k(cid:88)

i   1(cid:88)

k(cid:88)

0 =

  jpj =

  jpj +   ipi +

  jpj

(a.5.11)

j=1

j=1

j=i+1

now multiplying from the left with pt
as we can make this argument for any i = 1, . . . , k, all of the   i must be zero.

i a yields 0 =   ipt

i api. so   i is zero since we know that pt

i api > 0.

a.5.4 gram-schmidt construction of conjugate vectors

assume we already have k conjugate vectors p1, . . . , pk and let v be a vector which is linearly independent
of p1, . . . , pk. we then use a gram-schmidt procedure:

pk+1 = v    

pt
j av
pt
j apj

pj

(a.5.12)

k(cid:88)

j=1

for which it is clear that the vectors p1, . . . , pk+1 are conjugate if a is positive de   nite. we can construct n
conjugate vectors for a positive de   nite matrix in the following way: we start with n linearly independent
vectors u1, . . . , un. we then set p1 = u1 and use (a.5.12) to compute p2 from p1 and v = u2. next we set
v = u3 and compute p3 from p1, p2 and v. continuing in this manner we obtain n conjugate vectors. note
that at each stage of the procedure the vectors u1, . . . , uk span the same subspace as the vectors p1, . . . , pk.

draft november 9, 2017

633

multivariate optimisation

a.5.5 the conjugate vectors algorithm

let us assume that when minimising f (x) = 1
conjugate to a which we use as our search directions. using this our iterative solution takes the form

2 xtax     btx + c we    rst construct n vectors p1, . . . , pn

xk+1 = xk +   kpk .

where at each step we choose   k by an exact line search with

  k =    

pt
k gk
pt
k apk

.

(a.5.13)

(a.5.14)

this conjugate vectors algorithm has the geometrical interpretation that not only is the directional derivative
zero at the new point along the direction pk, it is zero along all the previous search directions p1, . . . , pk,
in particular    f t(xn+1)pi = 0, for i = 1, . . . , n;
known as the luenberger expanding subspace theorem.
that is

   f t(xn+1)(p1, p2, . . . , pn) = 0 .

(a.5.15)

the square matrix p = (p1, p2, . . . pn) is invertible since the pi are conjugate, so    f (xn+1) = 0 and the
point xn+1 is therefore the minimum x    of the quadratic function f . so in contrast to id119, for
a quadratic function the conjugate vectors algorithm converges in a    nite number of steps.

a.5.6 the conjugate gradients algorithm

the conjugate gradients algorithm is a special case of the conjugate vectors algorithm in which we construct
the conjugate vectors on-the-   y. after k-steps of the conjugate vectors algorithm we need to construct a
vector pk+1 which is conjugate to p1, . . . , pk. in the conjugate gradients algorithm one makes the special
choice v =       f (xk+1). the gradient at the new point xk+1 is orthogonal to pi, i = 1, . . . , k, so    f (xk+1)
is linearly independent of p1, . . . , pk and a valid choice for v, unless    f (xk+1) = 0. in the latter case xk+1
is our minimum and the algorithm terminates. using the notation gk =    f (xk), the equation for the new
search direction given by the gram-schmidt procedure equation (a.5.12) is:

pk+1 =    gk+1 +

pt
i agk+1
pt
i api

pi .

(a.5.16)

since gk+1 is orthogonal to pi, i = 1, . . . , k, we have pt

k+1gk+1 =    gt

k+1gk+1. so   k+1 can be written as

  k+1 =

gt
k+1gk+1
pt
k+1apk+1

,

(a.5.17)

and in particular   k+1 (cid:54)= 0. we now want to show that because we have been using the conjugate gradients
algorithm at the previous steps as well, in equation (a.5.16) all terms but the last in the sum over i vanish.
we shall assume that k > 0 since in the    rst step (k = 0) we just set p1 =    g1. first note that

gi+1     gi = axi+1     b     (axi     b) = a(xi+1     xi) =   iapi

and since   i (cid:54)= 0, then api = (gi+1     gi)/  i. so in equation (a.5.16):

pt
i agk+1 = gt

k+1api = gt

k+1(gi+1     gi)/  i = (gt

k+1gi+1     gt

k+1gi)/  i

(a.5.18)

(a.5.19)

since the pi were obtained by applying the gram-schmidt procedure to the gradients gi, we have gt
and gt

k+1gi = 0 for i = 1, . . . , k. this shows that

k+1pi = 0

k(cid:88)

i=1

i agk+1 = (gt
pt

k+1gi+1     gt

k+1gi)/  i =

hence equation (a.5.16) simpli   es to

pk+1 =    gk+1 +

gt
k+1gk+1/  k

pt

k apk

pk .

634

if 1     i < k
if i = k

(a.5.20)

(a.5.21)

draft november 9, 2017

(cid:26) 0

gt
k+1gk+1/  k

multivariate optimisation

algorithm a.1 conjugate gradients for minimising a function f (x)

1: k = 1
2: choose x1.
3: p1 =    g1
4: while gk (cid:54)= 0 do
  k = argmin

5:

  k

xk+1 := xk +   kpk
  k := gt
k+1gk+1/(gt
k gk)
pk+1 :=    gk+1 +   kpk
k = k + 1

6:
7:
8:
9:
10: end while

f (xk +   kpk)

(cid:46) line search

this can be brought into an even simpler form by applying equation (a.5.17) to   k:

gt
k+1gk+1
pt
k apk
we shall write this in the form

pk+1 =    gk+1 +

pt
k apk
gt
k gk

pk =    gk+1 +

gt
k+1gk+1
gt
k gk

pk

pk+1 =    gk+1 +   kpk where   k =

gt
k+1gk+1
gt
k gk

.

(a.5.22)

(a.5.23)

the formula (a.5.23) for   k is due to fletcher and reeves[245]. since the gradients are orthogonal,   k can
also be written as

  k =

gt
k+1(gk+1     gk)

gt
k gk

,

(a.5.24)

this is the polak-ribi`ere formula. the choice between the two expressions for   k can be of some importance
if f is not quadratic with the polak-ribi`ere formula the most commonly used. the important point here
is that we derived the above algorithm for the particular case of minimising a quadratic function f . by
writing the algorithm however only in terms of f and its gradients, we make no explicit reference to the
fact that f is quadratic. this means that we can therefore apply this algorithm to non-quadratic functions
as well. the more similar the function f is to a quadratic function, the more con   dent we can be that the
algorithm will    nd the minimum.

a.5.7 newton   s method

consider a function f (x) that we wish to    nd the minimum of. a taylor expansion up to second order gives

f (x +    ) = f (x) +    t   f +

1
2

   thf     + o(|   |3)

(a.5.25)

the matrix hf is the hessian. di   erentiating the right hand side with respect to     (or, equivalently,
completing the square), we    nd that the right hand side (ignoring the o(|   |3) term) has its lowest value
when

   f =    hf             =    h   1

f    f

(a.5.26)

(a.5.27)

@@

hence, an optimisation routine to minimise f is given by the newton update

xk+1 = xk      h   1

f    f

where the scalar 0 <   < 1 is often used in practice to improve the convergence. a bene   t of id77
over id119 is that the decrease in the objective function is invariant under a linear change of co-
x = my. de   ning   f (y)     f (x), since h   f = mthf m and    y   f = mt   xf the change in the
ordinates,
original x system is given by

@@

(cid:16)

(cid:17)   1

    m   y =     m

mthf m

mt   xf =     h   1

f    xf

which means that the change in the variable (and hence function value) is independent of the coordinate
system.

draft november 9, 2017

635

algorithm a.2 quasi-newton for minimising a function f (x)

multivariate optimisation

1: k = 1
2: choose x1
3:   h1 = i
4: while gk (cid:54)= 0 do
pk =       hkgk
  k = argmin

5:
6:

  k

xk+1 := xk +   kpk
sk = xk+1     xk, yk = gk+1     gk, and update   hk+1
k = k + 1

7:
8:
9:
10: end while

f (xk +   kpk)

(cid:46) line search

quasi-id77s

for large-scale problems both storing the hessian and solving the resulting linear system is computationally
demanding, especially if the matrix is close to singular. an alternative is to set up the iteration

xk+1 = xk       kskgk.

(a.5.28)
for sk = a   1 we have newton   s method, while if sk = i we have steepest descent. in general it would
seem to be a good idea to choose sk to be an approximation to the inverse hessian. also note that it is
important that sk be positive de   nite so that for small   k we obtain a descent method. the idea behind
most quasi-id77s is to try to construct an approximate inverse hessian   hk using information
gathered as the descent progresses, and to set sk =   hk. as we have seen, for a quadratic optimization
problem we have the relationship

gk+1     gk = a(xk+1     xk)

de   ning

sk = xk+1     xk

and

yk = gk+1     gk

we see that equation a.5.29 becomes

yk = ask

it is therefore reasonable to demand that

(a.5.29)

(a.5.30)

(a.5.31)

  hk+1yi = si

1     i     k

(a.5.32)
after n linearly independent steps we would then have   hn+1 = a   1. for k < n there are an in   nity of
solutions for   hk+1 satisfying equation a.5.32. a popular choice is the broyden-fletcher-goldfarb-shanno
(or bfgs) update, given by

(cid:32)

(cid:33)

  hk+1 =   hk +

1 +

  hkyk
yt
k
yt
k sk

skst
k
k yk    
st

skyt
k

  hk +   hkykst
k

st
k yk

(a.5.33)

this is a rank-2 correction to   hk constructed from the vectors sk and   hkyk. the direction vectors, pk =
      hkgk, produced by the algorithm obey

pt

i apj = 0

1     i < j     k,

  hk+1api = pi

1     i     k

(a.5.34)

the storage requirements for quasi id77s scale quadratically with the number of variables,
and hence these methods tend to be used only for smaller problems. limited memory bfgs reduces the
storage by only using the l latest updates in computing the approximate hessian inverse, equation (a.5.33).
in contrast, the memory requirements for pure conjugate gradient methods scale only linearly with the
dimension of x. as before, although the algorithm was derived with a quadratic function in mind, the    nal
form of the algorithm depends only on f and its gradients and may therefore be applied to non-quadratic
functions f .

636

draft november 9, 2017

constrained optimisation using lagrange multipliers

a.6 constrained optimisation using lagrange multipliers

consider    rst the problem of minimising f (x) subject to a single constraint c(x) = 0. a formal treatment
of this problem is beyond the scope of these notes and requires understanding the conditions under which
the optimum can be found[49]. as an informal argument, however, imagine that we have already identi   ed
an x that satis   es the constraint, that is c(x) = 0. how can we tell if this x minimises the function f ? we
are only allowed to search for lower function values around this x in directions which are consistent with
the constraint. for a small change   , the change in the constraint is,

c(x +   )     c(x) +          c(x)

let us also explore the change in f along a direction    where          c(x) = 0,

f (x +   )     f (x) +    f (x)      .

(a.6.1)

(a.6.2)

we are looking for a point x and direction    such that the change in f and c is minimal. this dual
requirement can be represented as to    nd x and    that minimise

|         f (x)|2 +    |         c(x)|2

where    > 0. this is a simple quadratic form and optimising for    gives that

   f (x) =      

         c(x)
         f (x)   c(x)

hence at the constrained optimum

   f (x) =      c(x)

(a.6.3)

(a.6.4)

(a.6.5)

for some scalar   . we can formulate this requirement as to look for x and    that constitute a stationary
point of the lagrangian

l(x,   ) = f (x)       c(x)

(a.6.6)

di   erentiating with respect to x, we obtain the requirement    f (x) =      c(x), and di   erentiating with
respect to   , we get that c(x) = 0.

in the multiple constraint case {ci(x) = 0} we    nd the stationary point of the lagrangian

(cid:88)

i

l(x,   ) = f (x)    

  ici(x).

a.6.1 lagrange dual

consider the    primal    problem

minimise f (x) subject to c(x) = 0

the lagrange dual is de   ned as

l(  ) = min

x

[f (x) +   c(x)]

by construction, for any x,

l(  )     f (x) +   c(x)

now consider the optimal x    that solves the primal problem equation (a.6.8). then

l(  )     f (x   

) +   c(x   

) = f (x   

)

draft november 9, 2017

(a.6.7)

(a.6.8)

(a.6.9)

(a.6.10)

(a.6.11)

637

where the last step follows since x    solves the primal problem, meaning that c(x   ) = 0. since l(  )     f (x   ),

the optimal    is given by solving the unconstrained    dual    problem

constrained optimisation using lagrange multipliers

   l(  )
max

in addition to providing a bound that enables one to bracket the optimal primal value,

l(  )     f (x   

)     f (x),

for c(x) = 0

(a.6.12)

(a.6.13)

the dual possesses the interesting property that it is concave, irrespective of whether f is convex. this
follows since by construction the function f (x) +   c(x) is concave in    for any point x. more explicitly,
consider

l(   +   ) = min

x

[f (x) + (   +   )c(x)]     f (x) +   c(x) +   c(x)

similarly,

l(         )     f (x) +   c(x)       c(x)

(a.6.14)

(a.6.15)

averaging equation (a.6.14) and equation (a.6.15), and taking the minimum over x of both sides, we have

1
2

(l(         ) + l(   +   ))     l(  )

which shows that l is concave.

(a.6.16)

638

draft november 9, 2017

bibliography

[1] l. f. abbott, j. a. varela, k. sen, and s. b. nelson. synaptic depression and cortical gain control. science,

275:220   223, 1997.

[2] d. h. ackley, g. e. hinton, and t. j. sejnowski. a learning algorithm for id82s. cognitive

science, 9:147   169, 1985.

[3] r. p. adams and d. j. c. mackay. bayesian online changepoint detection. cavendish laboratory, department

of physics, university of cambridge, cambridge, uk, 2006.

[4] e. airoldi, d. blei, e. xing, and s. fienberg. a latent mixed membership model for relational data. in linkkdd
   05: proceedings of the 3rd international workshop on link discovery, pages 82   89, new york, ny, usa, 2005.
acm.

[5] e. m. airoldi, d. m. blei, s. e. fienberg, and e. p. xing. mixed membership stochastic blockmodels. journal

of machine learning research, 9:1981   2014, 2008.

[6] d. l. alspach and h. w. sorenson. nonlinear bayesian estimation using gaussian sum approximations. ieee

transactions on automatic control, 17(4):439   448, 1972.

[7] s-i. amari. natural gradient learning for over and under-complete bases in ica. neural computation,

11:1875   1883, 1999.

[8] i. androutsopoulos, j. koutsias, k. v. chandrinos, and c. d. spyropoulos. an experimental comparison of
in proceedings of the
naive bayesian and keyword-based anti-spam    ltering with personal e-mail messages.
23rd annual international acm sigir conference on research and development in information retrieval, pages
160   167, new york, ny, usa, 2000. acm.

[9] n. arora, s. j russell, p. kidwell, and e. b. sudderth. global seismic monitoring as probabilistic id136.
in j. d. la   erty, c. k. i. williams, j. shawe-taylor, r. s. zemel, and a. culotta, editors, advances in neural
information processing systems 23, pages 73   81. curran associates, inc., 2010.

[10] s. arora and c. lund. hardness of approximations. in approximation algorithms for np-hard problems, pages

399   446. pws publishing co., boston, ma, usa, 1997.

[11] f. r. bach and m. i. jordan. thin junction trees. in t. g. dietterich, s. becker, and z. ghahramani, editors,
advances in neural information processing systems (nips), number 14, pages 569   576, cambridge, ma, 2001.
mit press.

[12] f. r. bach and m. i. jordan. a probabilistic interpretation of canonical correlation analysis. computer science

division and department of statistics 688, university of california berkeley, berkeley, usa, 2005.

[13] y. bar-shalom and xiao-rong li. estimation and tracking : principles, techniques and software. artech

house, norwood, ma, 1998.

[14] d. barber. dynamic id110s with deterministic tables. in s. becker, s. thrun, and k. obermayer,
editors, advances in neural information processing systems (nips), number 15, pages 713   720, cambridge,
ma, 2003. mit press.

[15] d. barber. learning in spiking neural assemblies. in s. becker, s. thrun, and k. obermayer, editors, advances
in neural information processing systems (nips), number 15, pages 149   156, cambridge, ma, 2003. mit press.

639

bibliography

bibliography

[16] d. barber. are two classi   ers performing equally? a treatment using bayesian hypothesis testing. idiap-

rr 57, idiap, rue de simplon 4, martigny, ch-1920, switerland, may 2004. idiap-rr 04-57.

[17] d. barber. the auxiliary variable trick for deriving kalman smoothers. idiap-rr 87, idiap, rue de simplon

4, martigny, ch-1920, switerland, december 2004. idiap-rr 04-87.

[18] d. barber. expectation correction for smoothing in switching linear gaussian state space models. journal of

machine learning research, 7:2515   2540, 2006.

[19] d. barber. clique matrices for statistical graph decomposition and parameterising restricted positive de   nite
matrices. in d. a. mcallester and p. myllymaki, editors, uncertainty in arti   cial intelligence, number 24, pages
26   33, corvallis, oregon, usa, 2008. auai press.

[20] d. barber and f. v. agakov. correlated sequence learning in a network of spiking neurons using maximum

likelihood. informatics research reports edi-inf-rr-0149, edinburgh university, 2002.

[21] d. barber and f.v. agakov. the im algorithm: a variational approach to information maximization.

in

advances in neural information processing systems (nips), number 16, 2004.

[22] d. barber and c. m. bishop. bayesian model comparison by monte carlo chaining. in m. c. mozer, m. i.
jordan, and t. petsche, editors, advances in neural information processing systems (nips), number 9, pages
333   339, cambridge, ma, 1997. mit press.

[23] d. barber and c. m. bishop. id108 in bayesian neural networks. in neural networks and machine

learning, pages 215   237. springer, 1998.

[24] d. barber and s. chiappa. uni   ed id136 for id58ian linear gaussian state-space models. in
b. sch  olkopf, j. platt, and t. ho   man, editors, advances in neural information processing systems (nips),
number 19, pages 81   88, cambridge, ma, 2007. mit press.

[25] d. barber and w. wiegerinck. tractable variational structures for approximating id114. in m. s.
kearns, s. a. solla, and d. a. cohn, editors, advances in neural information processing systems (nips),
number 11, pages 183   189, cambridge, ma, 1999. mit press.

[26] d. barber and c. k. i. williams. gaussian processes for bayesian classi   cation via hybrid monte carlo. in
m. c. mozer, m. i. jordan, and t. petsche, editors, advances in neural information processing systems nips
9, pages 340   346, cambridge, ma, 1997. mit press.

[27] r. j. baxter. exactly solved models in statistical mechanics. academic press, 1982.

[28] m. j. beal, f. falciani, z. ghahramani, c. rangel, and d. l. wild. a bayesian approach to reconstructing

genetic regulatory networks with hidden factors. bioinformatics, (21):349   356, 2005.

[29] a. becker and d. geiger. a su   ciently fast algorithm for    nding close to optimal clique trees. arti   cial

intelligence, 125(1-2):3   17, 2001.

[30] a. j. bell and t. j. sejnowski. an information-maximization approach to blind separation and blind decon-

volution. neural computation, 7(6):1129   1159, 1995.

[31] r. e. bellman. id145. princeton university press, princeton, nj, 1957. paperback edition by

dover publications (2003).

[32] y. bengio and p. frasconi.

input-output id48s for sequence processing.

ieee trans. neural networks,

(7):1231   1249, 1996.

[33] a. l. berger, s. d. della pietra, and v. j. d. della pietra. a maximum id178 approach to natural language

processing. computational linguistics, 22(1):39   71, 1996.

[34] j. o. berger. statistical decision theory and bayesian analysis. springer, second edition, 1985.

[35] d. p. bertsekas. nonid135. athena scienti   c, 2nd edition, 1999.

[36] d. p. bertsekas. id145 and optimal control. athena scienti   c, second edition, 2000.

[37] j. besag. spatial interactions and the statistical analysis of lattice systems. journal of the royal statistical

society, series b, 36(2):192   236, 1974.

[38] j. besag. on the statistical analysis of dirty pictures. journal of the royal statistical society, series b, 48:259   

302, 1986.

640

draft november 9, 2017

bibliography

bibliography

[39] j. besag and p. green. spatial statistics and bayesian computation. journal of the royal statistical society,

series b, 55:25   37, 1993.

[40] g. j. bierman. measurement updating using the u-d factorization. automatica, 12:375   382, 1976.

[41] n. l. biggs. discrete mathematics. oxford university press, 1990.

[42] k. binder and a. p. young. spin glasses: experimental facts, theoretical concepts, and open questions. rev.

mod. phys., 58(4):801   976, oct 1986.

[43] c. m. bishop. neural networks for pattern recognition. oxford university press, 1995.

[44] c. m. bishop. pattern recognition and machine learning. springer, 2006.

[45] c. m. bishop and m. svens  en. bayesian hierarchical mixtures of experts. in u. kjaerul    and c. meek, editors,
proceedings nineteenth conference on uncertainty in arti   cial intelligence, pages 57   64. morgan kaufmann,
2003.

[46] f. black and m. scholes. the pricing of options and corporate liabilities. journal of political economy,

81(3):637   654, 1973.

[47] d. blei, a. ng, and m. jordan. id44. journal of machine learning research, (3):993   1022,

2003.

[48] r. r. bouckaert. bayesian belief networks: from construction to id136. phd thesis, university of utrecht,

1995.

[49] s. boyd and l. vandenberghe. id76. cambridge university press, 2004.

[50] y. boykov and v. kolmogorov. an experimental comparison of min-cut/max-   ow algorithms for energy mini-

mization in vision. ieee trans. pattern anal. mach. intell., 26(9):1124   1137, 2004.

[51] y. boykov, o. veksler, and r. zabih. fast approximate energy minimization via graph cuts. ieee trans.

pattern anal. mach. intell., 23:1222   1239, 2001.

[52] c. bracegirdle and d. barber. switch-reset models : exact and approximate id136. in proceedings of the

fourteenth international conference on arti   cial intelligence and statistics (aistats), volume 10, 2011.

[53] m. brand.

incremental singular value decomposition of uncertain data with missing values.

in european

conference on id161 (eccv), pages 707   720, 2002.

[54] j. breese and d. heckerman. decision-theoretic troubleshooting: a framework for repair and experiment.
in e. horvitz and f. jensen, editors, uncertainty in arti   cial intelligence, number 12, pages 124   132, san
francisco, ca, 1996. morgan kaufmann.

[55] h. bunke and t. caelli. id48: applications in id161. machine perception and

arti   cial intelligence. world scienti   c publishing co., inc., river edge, nj, usa, 2001.

[56] w. buntine. theory re   nement on id110s. in uncertainty in arti   cial intelligence, number 7, pages

52   60, san francisco, ca, 1991. morgan kaufmann.

[57] a. cano and s. moral. advances in intelligent computing     ipmu 1994, chapter heuristic algorithms for the
triangulation of graphs, pages 98   107. number 945 in lectures notes in computer sciences. springer-verlag,
1995.

[58] o. capp  e, e. moulines, and t. ryden. id136 in id48. springer, new york, 2005.

[59] a. t. cemgil. bayesian id136 in non-negative matrix factorisation models. technical report cued/f-

infeng/tr.609, university of cambridge, july 2008.

[60] a. t. cemgil, b. kappen, and d. barber. a generative model for music transcription. ieee transactions on

audio, speech and language processing, 14(2):679   694, 2006.

[61] e. challis and d. barber. concave gaussian variational approximations for id136 in large-scale bayesian
linear models. in proceedings of the fourteenth international conference on arti   cial intelligence and statistics
(aistats). jmlr, 2011.

[62] h. s. chang, m. c. fu, j. hu, and s. i. marcus. simulation-based algorithms for id100.

springer, 2007.

draft november 9, 2017

641

bibliography

bibliography

[63] s. chiappa and d. barber. bayesian linear gaussian state space models for biosignal decomposition. signal

processing letters, 14(4):267   270, 2007.

[64] s. chib and m. dueker. non-markovian regime switching with endogenous states and time-varying state
strengths. econometric society 2004 north american summer meetings 600, econometric society, august 2004.

[65] c. k. chow and c. n. liu. approximating discrete id203 distributions with dependence trees. ieee

transactions on id205, 14(3):462   467, 1968.

[66] p. s. churchland and t. j. sejnowski. the computational brain. mit press, cambridge, ma, usa, 1994.

[67] d. cohn and h. chang. learning to probabilistically identify authoritative documents. in p. langley, editor,

international conference on machine learning, number 17, pages 167   174. morgan kaufmann, 2000.

[68] d. cohn and t. hofmann. the missing link - a probabilistic model of document content and hypertext

connectivity. number 13, pages 430   436, cambridge, ma, 2001. mit press.

[69] a. c. c. coolen, r. k  uhn, and p. sollich. theory of neural information processing systems. oxford university

press, 2005.

[70] g. f. cooper and e. herskovits. a bayesian method for the induction of probabilistic networks from data.

machine learning, 9(4):309   347, 1992.

[71] a. corduneanu and c. m. bishop. id58ian model selection for mixture distributions. in t. jaakkola

and t. richardson, editors, artifcial intelligence and statistics, pages 27   34. morgan kaufmann, 2001.

[72] m. t. cover and j. a. thomas. elements of id205. wiley, 1991.

[73] r. g. cowell, a. p. dawid, s. l. lauritzen, and d. j. spiegelhalter. probabilistic networks and id109.

springer, 1999.

[74] d. r. cox and n. wermuth. multivariate dependencies. chapman and hall, 1996.

[75] j. c. cox, s. a. ross, and m. rubinstein. option pricing: a simpli   ed approach. journal of financial

economics, 7:229   263, 1979.

[76] n. cristianini and j. shawe-taylor. an introduction to support vector machines. cambridge university press,

2000.

[77] p. dangauthier, r. herbrich, t. minka, and t. graepel. trueskill through time: revisiting the history of chess.
in b. sch  olkopf, j. platt, and t. ho   man, editors, advances in neural information processing systems (nips),
number 19, pages 569   576, cambridge, ma, 2007. mit press.

[78] h. a. david. the method of paired comparisons. oxford university press, new york, 1988.

[79] a. p. dawid. in   uence diagrams for causal modelling and id136. international statistical review, 70:161   189,

2002.

[80] a. p. dawid and s. l. lauritzen. hyper markov laws in the statistical analysis of decomposable graphical

models. annals of statistics, 21(3):1272   1317, 1993.

[81] p. dayan and l.f. abbott. theoretical neuroscience. mit press, 2001.

[82] p. dayan and g. e. hinton. using expectation-maximization for id23. neural computation,

9:271   278, 1997.

[83] t. de bie, n. cristianini, and r. rosipal. handbook of geometric computing : applications in pattern recogni-
tion, id161, neuralcomputing, and robotics, chapter eigenproblems in pattern recognition. springer-
verlag, 2005.

[84] r. dechter. bucket elimination: a unifying framework for probabilistic id136 algorithms. in e. horvitz
and f. jensen, editors, uncertainty in arti   cial intelligence, pages 211   219, san francisco, ca, 1996. morgan
kaufmann.

[85] a. p. dempster, n. m. laird, and d. b. rubin. maximum likelihood from incomplete data via the em

algorithm. journal of the royal statistical society. series b (methodological), 39(1):1   38, 1977.

[86] s. diederich and m. opper. learning of correlated patterns in spin-glass networks by local learning rules.

physical review letters, 58(9):949   952, 1986.

642

draft november 9, 2017

bibliography

bibliography

[87] r. diestel. id207. springer, 2005.

[88] a. doucet and a. m. johansen. a tutorial on id143ing and smoothing: fifteen years later. in d. crisan

and b. rozovsky, editors, oxford handbook of nonlinear filtering. oxford university press, 2009.

[89] r. o. duda, p. e. hart, and d. g. stork. pattern classi   cation. wiley-interscience publication, 2000.

[90] j. durbin. the    tting of time series models. rev. inst. int. stat., 28:233   243, 1960.

[91] r. durbin, s. r. eddy, a. krogh, and g. mitchison. biological sequence analysis : probabilistic models of

proteins and nucleic acids. cambridge university press, 1999.

[92] a. d  uring, a. c. c. coolen, and d. sherrington. phase diagram and storage capacity of sequence processing

neural networks. journal of physics a, 31:8607   8621, 1998.

[93] j. m. gutierrez e. castillo and a. s. hadi. id109 and probabilistic network models. springer verlag,

1997.

[94] j. edmonds and r. m. karp. theoretical improvements in algorithmic e   ciency for network    ow problems.

journal of the acm, 19(2):248   264, 1972.

[95] r. edwards and a. sokal. generalization of the fortium-kasteleyn-swendson-wang representation and monte

carlo algorithm. physical review d, 38:2009   2012, 1988.

[96] a. e. elo. the rating of chess players, past and present. arco, new york, second edition, 1986.

[97] r. f. engel. garch 101: the use of arch/garch models in applied econometrics. journal of economic

perspectives, 15(4):157   168, 2001.

[98] y. ephraim and w. j. j. roberts. revisiting autoregressive hidden markov modeling of speech signals. ieee

signal processing letters, 12(2):166   169, february 2005.

[99] e. erosheva, s. fienberg, and j. la   erty. mixed membership models of scienti   c publications. in proceedings

of the national academy of sciences, volume 101, pages 5220   5227, 2004.

[100] r-e. fan, p-h. chen, and c-j. lin. working set selection using second order information for training support

vector machines. journal of machine learning research, 6:1889   1918, 2005.

[101] p. fearnhead. exact and e   cient bayesian id136 for multiple changepoint problems. technical report,

deptartment of mathematics and statistics, lancaster university, 2003.

[102] g. h. fischer and i. w. molenaar. rasch models: foundations, recent developments, and applications. springer,

new york, 1995.

[103] m. e. fisher. statistical mechanics of dimers on a plane lattice. physical review, 124:1664   1672, 1961.

[104] b. frey. extending factor graphs as to unify directed and undirected id114. in c. meek and
u. kj  rul   , editors, uncertainty in arti   cial intelligence, number 19, pages 257   264. morgan kaufmann, 2003.

[105] n. friedman, d. geiger, and m. goldszmidt. id110 classi   ers. machine learning, 29:131   163,

1997.

[106] s. fr  uhwirth-schnatter. finite mixture and markov switching models. springer, 2006.

[107] m. frydenberg. the chain graph markov property. scandanavian journal of statistics, 17:333   353, 1990.

[108] t. furmston and d. barber. solving deterministic policy (po)mpds using expectation-maximisation and an-
tifreeze. in first international workshop on learning and data mining for robotics (lemir), september 2009.
in conjunction with ecml/pkdd-2009.

[109] t. furmston and d. barber. variational methods for id23. in teh. y. w. and m. titter-
ington, editors, proceedings of the thirteenth international conference on arti   cial intelligence and statistics
(aistats), volume 9, pages 241   248, chia laguna, sardinia, italy, may 13-15 2010. jmlr.

[110] t. furmston and d. barber. e   cient id136 in markov control problems.

in uncertainty in arti   cial

intelligence, number 27, corvallis, oregon, usa, 2011.

[111] t. furmston and d. barber. lagrange id209 for finite horizon id100. in

european conference on machine learning (ecml), 2011.

draft november 9, 2017

643

bibliography

bibliography

[112] a. galka, o. yamashita, t. ozaki, r. biscay, and p. valdes-sosa. a solution to the dynamical inverse problem

of eeg generation using spatiotemporal kalman    ltering. neuroimage, (23):435   453, 2004.

[113] p. gandhi, f. bromberg, and d. margaritis. learning markov network structure using few independence tests.

in proceedings of the siam international conference on data mining, pages 680   691, 2008.

[114] m. r. garey and d. s. johnson. computers and intractability, a guide to the theory of np-completeness.

w.h. freeman and company, new york, 1979.

[115] a. gelb. applied optimal estimation. mit press, 1974.

[116] a. gelman, g. o. roberts, and w. r. gilks. e   cient metropolis jumping rules. in j. o. bernardo, j. m. berger,
a. p. dawid, and a. f. m. smith, editors, bayesian statistics, volume 5, pages 599   607. oxford university press,
1996.

[117] s. geman and d. geman. stochastic relaxation, gibbs distributions, and the bayesian restoration of images. in
readings in uncertain reasoning, pages 452   472, san francisco, ca, usa, 1990. morgan kaufmann publishers
inc.

[118] m. g. genton. classes of kernels for machine learning: a statistics perspective. journal of machine learning

research, 2:299   312, 2001.

[119] w. gerstner and w. m. kistler. spiking neuron models. cambridge university press, 2002.

[120] z. ghahramani and m. j. beal. variational id136 for bayesian mixtures of factor analysers. in s. a. solla,
t. k. leen, and k-r. m  uller, editors, advances in neural information processing systems (nips), number 12,
pages 449   455, cambridge, ma, 2000. mit press.

[121] z. ghahramani and g. e. hinton. variational learning for switching state-space models. neural computation,

12(4):963   996, 1998.

[122] a. gibbons. algorithmic id207. cambridge university press, 1991.

[123] m. gibbs. bayesian gaussian processes for regression and classi   cation. phd thesis, university of cambridge,

1997.

[124] w. r. gilks, s. richardson, and d. j. spiegelhalter. id115 in practice. chapman & hall,

1996.

[125] m girolami. a variational method for learning sparse and overcomplete representations. neural computation,

13:2517   2532, 2001.

[126] m. girolami and a. kaban. on an equivalence between plsi and lda. in proceedings of the 26th annual
international acm sigir conference on research and development in information retrieval, pages 433   434,
new york, ny, usa, 2003. acm press.

[127] m. e. glickman. parameter estimation in large dynamic paired comparison experiments. applied statistics,

48:377   394, 1999.

[128] a. globerson and t. jaakkola. approximate id136 using planar graph decomposition.

in b. sch  olkopf,
j. platt, and t. ho   man, editors, advances in neural information processing systems (nips), number 19,
pages 473   480, cambridge, ma, 2007. mit press.

[129] d. goldberg, d. nichols, b. m. oki, and d. terry. using collaborative    ltering to weave an information tapestry.

communications acm, 35:61   70, 1992.

[130] g. h. golub and c. f. van loan. matrix computations. johns hopkins university press, 3rd edition, 1996.

[131] m. c. golumbic and i. ben-arroyo hartman. id207, combinatorics, and algorithms. springer-verlag,

2005.

[132] c. goutis. a graphical method for solving a decision analysis problem. ieee transactions on systems, man

and cybernetics, 25:1181   1193, 1995.

[133] p. j. green and b. w. silverman. nonparametric regression and generalized linear models, volume 58 of

monographs on statistics and applied id203. chapman and hall, 1994.

[134] d. m. greig, b. t. porteous, and a. h. seheult. exact maximum a posteriori estimation for binary images.

journal of the royal statistical society, series b, 2:271   279, 1989.

644

draft november 9, 2017

bibliography

bibliography

[135] g. grimmett and d. stirzaker. id203 and random processes. oxford university press, second edition,

1992.

[136] s. f. gull. bayesian data analysis: straight-line    tting. in j. skilling, editor, maximum id178 and bayesian

methods (cambridge 1988), pages 511   518. kluwer, 1989.

[137] a. k. gupta and d. k. nagar. matrix variate distributions. chapman and hall/crc, boca raton, florida

usa, 1999.

[138] d. j. hand and k. yu. idiot   s bayes   not so stupid after all? international statistical review, 69(3):385   398,

2001.

[139] d. r. hardoon, s. szedmak, and j. shawe-taylor. canonical correlation analysis: an overview with applica-

tion to learning methods. neural computation, 16(12):2639   2664, 2004.

[140] d. o. hebb. the organization of behavior. wiley, new york, 1949.

[141] d. heckerman. a tutorial on learning with id110s. technical report msr-tr-95-06, microsoft

research, redmond, wa, march 1996. revised november 1996.

[142] d. heckerman, d. geiger, and d. chickering. learning id110s: the combination of knowledge

and statistical data. machine learning, 20(3):197   243, 1995.

[143] r. herbrich, t. minka, and t. graepel. trueskilltm: a bayesian skill rating system. in b. sch  olkopf, j. platt,
and t. ho   man, editors, advances in neural information processing systems (nips), number 19, pages 569   576,
cambridge, ma, 2007. mit press.

[144] h. hermansky. should recognizers have ears? speech communication, 25:3   27, 1998.

[145] j. hertz, a. krogh, and r. palmer. introduction to the theory of neural computation. addison-wesley, 1991.

[146] t. heskes. convexity arguments for e   cient minimization of the bethe and kikuchi free energies. journal of

arti   cial intelligence research, 26:153   190, 2006.

[147] d. m. higdon. auxiliary variable methods for id115 with applications. journal of the

american statistical association, 93(442):585   595, 1998.

[148] g. e. hinton and r. r. salakhutdinov. reducing the dimensionality of data with neural networks. science,

(313):504   507, 2006.

[149] t. hofmann, j. puzicha, and m. i. jordan. learning from dyadic data. in m. s. kearns, s. a. solla, and d. a.
cohn, editors, advances in neural information processing systems (nips), pages 466   472, cambridge, ma,
1999. mit press.

[150] r. a. howard and j. e. matheson. in   uence diagrams. decision analysis, 2(3), 2005. republished version of

the original 1981 report.

[151] j. c. hull. options, futures, and other derivatives. prentice hall, 1997.

[152] a. hyv  arinen, j. karhunen, and e. oja. independent component analysis. wiley, 2001.

[153] aapo hyv  arinen. consistency of pseudolikelihood estimation of fully visible id82s. neural

computation, 18(10):2283   2292, 2006.

[154] m. isard and a. blake. condensation conditional density propagation for visual tracking. international

journal of id161, 29:5   28, 1998.

[155] t. s. jaakkola and m. i. jordan. variational probabilistic id136 and the qmr-dt network. journal of arti   cial

intelligence research, 10:291   322, 1999.

[156] t. s. jaakkola and m. i. jordan. bayesian parameter estimation via variational methods. statistics and

computing, 10(1):25   37, 2000.

[157] r. a. jacobs, f. peng, and m. a. tanner. a bayesian approach to model selection in hierarchical mixtures-of-

experts architectures. neural networks, 10(2):231   241, 1997.

[158] r. g. jarrett. a note on the intervals between coal-mining disasters. biometrika, (66):191   193, 1979.

[159] e. t. jaynes. id203 theory : the logic of science. cambridge university press, 2003.

draft november 9, 2017

645

bibliography

bibliography

[160] f. jensen, f. v. jensen, and d. dittmer. from in   uence diagrams to junction trees. in proceedings of the
10th annual conference on uncertainty in arti   cial intelligence (uai-94), pages 367   373, san francisco, ca,
1994. morgan kaufmann.

[161] f. v. jensen and f. jensen. optimal junction trees. in r. lopez de mantaras and d. poole, editors, uncertainty

in arti   cial intelligence, number 10, pages 360   366, san francisco, ca, 1994. morgan kaufmann.

[162] f. v. jensen and t. d. nielson. id110s and decision graphs. springer verlag, second edition, 2007.

[163] m. i. jordan and r. a. jacobs. hierarchical mixtures of experts and the em algorithm. neural computation,

6:181   214, 1994.

[164] b. h. juang, w. chou, and c. h. lee. minimum classi   cation error rate methods for id103. ieee

transactions on speech and audio processing, 5:257   265, 1997.

[165] l. p. kaelbling, m. l. littman, and a. r. cassandra. planning and acting in partially observable stochastic

domains. arti   cial intelligence, 101(1-2):99   134, 1998.

[166] h. j. kappen. an introduction to stochastic control theory, path integrals and id23.

in
proceedings 9th granada seminar on computational physics: computational and mathematical modeling of
cooperative behavior in neural systems, volume 887, pages 149   181. american institute of physics, 2007.

[167] h. j. kappen and f. b. rodr    guez. e   cient learning in id82s using linear response theory.

neural compution, 10(5):1137   1156, 1998.

[168] h. j. kappen and w. wiegerinck. novel iteration schemes for the cluster variation method. in t. g. dietterich,
s. becker, and z. ghahramani, editors, advances in neural information processing systems (nips), number 14,
pages 415   422, cambridge, ma, 2002. mit press.

[169] y. karklin and m. s. lewicki. emergence of complex cell properties by learning to generalize in natural scenes.

nature, (457):83   86, november 2008.

[170] g. karypis and v. kumar. a fast and high quality multilevel scheme for partitioning irregular graphs. siam

journal on scienti   c computing, 20(1):359   392, 1998.

[171] p. w. kasteleyn. dimer statistics and phase transitions. journal of mathematical physics, 4(2):287   293, 1963.

[172] s. a. kau   man. at home in the universe: the search for laws of self-organization and complexity. oxford

university press, oxford, uk, 1995.

[173] c-j. kim. dynamic linear models with markov-switching. journal of econometrics, 60:1   22, 1994.

[174] c-j. kim and c. r. nelson. state-space models with regime switching. mit press, 1999.

[175] g. kitagawa. the two-filter formula for smoothing and an implementation of the gaussian-sum smoother.

annals of the institute of statistical mathematics, 46(4):605   623, 1994.

[176] u. b. kjaerul    and a. l. madsen. id110s and in   uence diagrams : a guide to construction and

analysis. springer, 2008.

[177] n. komodakis, n. paragios, and g. tziritas. mrf optimization via id209: message-passing

revisited. in ieee 11th international conference on id161, iccv, pages 1   8, 2007.

[178] a. krogh, m. brown, i. mian, k. sjolander, and d. haussler. id48 in computational biology:

applications to protein modeling. journal of molecular biology, 235:1501   1531, 1994.

[179] s. kullback. id205 and statistics. dover, 1968.

[180] k. kurihara, m. welling, and y. w. teh. collapsed variational dirichlet process mixture models. in proceedings

of the international joint conference on arti   cial intelligence, volume 20, pages 2796   2801, 2007.

[181] j. la   erty, a. mccallum, and f. pereira. conditional random    elds: probabilistic models for segmenting and
in c. e. brodley and a. p. danyluk, editors, international conference on machine

labeling sequence data.
learning, number 18, pages 282   289, san francisco, ca, 2001. morgan kaufmann.

[182] h. lass. elements of pure and applied mathematics. mcgraw-hill (reprinted by dover), 1957.

[183] s. l. lauritzen. id114. oxford university press, 1996.

646

draft november 9, 2017

bibliography

bibliography

[184] s. l. lauritzen, a. p. dawid, b. n. larsen, and h-g. leimer. independence properties of directed markov    elds.

networks, 20:491   505, 1990.

[185] s. l. lauritzen and d. j. spiegelhalter. local computations with probabilities on graphical structures and their

application to id109. journal of royal statistical society b, 50(2):157     224, 1988.

[186] d. d. lee and h. s. seung. algorithms for non-negative id105. in t. k. leen, t. g. dietterich,
and v. tresp, editors, advances in neural information processing systems (nips), number 13, pages 556   562,
cambridge, ma, 2001. mit press.

[187] m. a. r. leisink and h. j. kappen. a tighter bound for id114. in neural computation, volume 13,

pages 2149   2171. mit press, 2001.

[188] v. lepar and p. p. shenoy. a comparison of lauritzen-spiegelhalter, hugin, and shenoy-shafer architectures
in g. cooper and s. moral, editors, uncertainty in

for computing marginals of id203 distributions.
arti   cial intelligence, number 14, pages 328   333, san francisco, ca, 1998. morgan kaufmann.

[189] u. lerner, r. parr, d. koller, and g. biswas. bayesian fault detection and diagnosis in dynamic systems. in

proceedings of the seventeenth national conference on arti   cial intelligence (aiii-00), pages 531   537, 2000.

[190] u. n. lerner. hybrid id110s for reasoning about complex systems. computer science department,

stanford university, 2002.

[191] r. linsker. improved local learning rule for information maximization and related applications. neural networks,

18(3):261   265, 2005.

[192] y. l. loh, e. w. carlson, and m. y. j. tan. bond-propagation algorithm for thermodynamic functions in

general two-dimensional ising models. physical review b, 76(1):014404, 2007.

[193] h. lopes and m. west. bayesian model assessment in factor analysis. statistica sinica, (14):41   67, 2003.

[194] t. j. loredo. from laplace to supernova sn 1987a: bayesian id136 in astrophysics.

in p.f. fougere,

editor, maximum id178 and bayesian methods, pages 81   142. kluwer, 1990.

[195] d. j. c. mackay. bayesian interpolation. neural computation, 4(3):415   447, 1992.

[196] d. j. c. mackay. probable networks and plausible predictions     a review of practical bayesian methods for

supervised neural networks. network: computation in neural systems, 6(3):469   505, 1995.

[197] d. j. c. mackay. introduction to gaussian processes. in neural networks and machine learning, volume 168
of nato advanced study institute on generalization in neural networks and machine learning, pages 133   165.
springer, august 1998.

[198] d. j. c. mackay. id205, id136 and learning algorithms. cambridge university press, 2003.

[199] u. madhow. fundamentals of digital communication. cambridge university press, 2008.

[200] k. v. mardia, j. t. kent, and j. m. bibby. multivariate analysis. academic press, 1997.

[201] h. markram, j. lubke, m. frotscher, and b. sakmann. regulation of synaptic e   cacy by coincidence of

postsynaptic aps and epsps. science, 275:213   215, 1997.

[202] a. mccallum, k. nigam, j. rennie, and k. seymore. automating the construction of internet portals with

machine learning. information retrieval journal, 3:127   163, 2000.

[203] g. mclachlan and t. krishnan. the em algorithm and extensions. john wiley and sons, 1997.

[204] g. mclachlan and d. peel. finite mixture models. wiley series in id203 and statistics. wiley-interscience,

2000.

[205] e. meeds, z. ghahramani, r. m. neal, and s. t. roweis. modeling dyadic data with binary latent factors.
in b. sch  olkopf, j. platt, and t. ho   man, editors, advances in neural information processing systems (nips),
volume 19, pages 977   984, cambridge, ma, 2007. mit press.

[206] m. meila. an accelerated chow and liu algorithm: fitting tree distributions to high-dimensional sparse
data. in i. bratko, editor, international conference on machine learning, pages 249   257, san francisco, ca,
1999. morgan kaufmann.

draft november 9, 2017

647

bibliography

bibliography

[207] m. meila and m. i. jordan. triangulation by continuous embedding.

in m. c. mozer, m. i. jordan, and
t. petsche, editors, advances in neural information processing systems (nips), number 9, pages 557   563,
cambridge, ma, 1997. mit press.

[208] b. mesot and d. barber. switching linear dynamical systems for noise robust id103. ieee

transactions of audio, speech and language processing, 15(6):1850   1858, 2007.

[209] n. meuleau, m. hauskrecht, k-e. kim, l. peshkin, kaelbling. l. p., t. dean, and c. boutilier. solving very
in proceedings of the fifteenth national conference on

large weakly coupled id100.
arti   cial intelligence, pages 165   172, 1998.

[210] t. mills. the econometric modelling of financial time series. cambridge university press, 2000.

[211] t. minka. expectation propagation for approximate bayesian id136. in j. breese and d. koller, editors,
uncertainty in arti   cial intelligence, number 17, pages 362   369, san francisco, ca, 2001. morgan kaufmann.

[212] t. minka. a comparison of numerical optimizers for id28. technical report, microsoft research,

2003. research.microsoft.com/   minka/papers/logreg.

[213] t. minka. divergence measures and message passing. technical report msr-tr-2005-173, microsoft research

ltd., cambridge, uk, december 2005.

[214] a. mira, j. m  ller, and g. o. roberts. perfect slice samplers. journal of the royal statistical society, 63(3):593   

606, 2001. series b (statistical methodology).

[215] c. mitchell, m. harper, and l. jamieson. on the complexity of explicit duration id48   s. speech and audio

processing, ieee transactions on, 3(3):213   217, may 1995.

[216] t. mitchell. machine learning. mcgraw-hill, 1997.

[217] j. mooij and h. j. kappen. su   cient conditions for convergence of loopy belief propagation. ieee information

theory, 53:4422   4437, 2007.

[218] j. w. moon and l. moser. on cliques in graphs. israel journal of mathematics, (3):23   28, 1965.

[219] a. moore.

a tutorial

on

kd-trees.

technical

report,

1991.

available

from

http://www.cs.cmu.edu/   awm/papers.html.

[220] j. moussouris. gibbs and markov random systems with constraints. journal of statistical physics, 10:11   33,

1974.

[221] r. m. neal. probabilistic id136 using id115 methods. crg-tr-93-1, dept. of computer

science, university of toronto, 1993.

[222] r. m. neal. markov chain sampling methods for dirichlet process mixture models. journal of computational

and graphical statistics, 9(2):249   265, 2000.

[223] r. m. neal. slice sampling. annals of statistics, 31:705   767, 2003.

[224] r. e. neapolitan. learning id110s. prentice hall, 2003.

[225] a. v. ne   an, l. luhong, p. xiaobo, x. liu, c. mao, and k. murphy. a coupled id48 for audio-visual speech
recognition. in ieee international conference on acoustics, speech, and signal processing, volume 2, pages
2013   2016, 2002.

[226] h. nickisch and m. seeger. convex id58ian id136 for large scale generalized linear models.

international conference on machine learning, 26:761   768, 2009.

[227] d. nilsson. an e   cient algorithm for    nding the m most probable con   gurations in a probabilistic expert

system. statistics and computing, 8:159   173, 1998.

[228] d. nilsson and j. goldberger. sequentially    nnding the n -best list in id48. internation joint

conference on arti   cial intelligence (ijcai), 17, 2001.

[229] a. b. noviko   . on convergence proofs on id88s. in symposium on the mathematical theory of automata
(new york, 1962), volume 12, pages 615   622, brooklyn, n.y., 1963. polytechnic press of polytechnic institute
of brooklyn.

648

draft november 9, 2017

bibliography

bibliography

[230] f. j. och and h. ney. discriminative training and maximum id178 models for statistical machine transla-
tion. in proceedings of the annual meeting of the association for computational linguistics, pages 295   302,
philadelphia, july 2002.

[231] b. a. olshausen and d. j. field. sparse coding with an overcomplete basis set: a strategy employed by v1?

vision research, 37:3311   3325, 1998.

[232] a. v. oppenheim, r. w. shafer, m. t. yoder, and w. t. padgett. discrete-time signal processing. prentice

hall, third edition, 2009.

[233] m. ostendorf, v. digalakis, and o. a. kimball. from id48s to segment models: a uni   ed view of stochastic

modeling for id103. ieee transactions on speech and audio processing, 4:360   378, 1995.

[234] p. paatero and u. tapper. positive id105: a non-negative factor model with optimal utilization

of error estimates of data values. environmetrics, 5:111   126, 1994.

[235] a. palmer, d. wipf, k. kreutz-delgado, and b. rao. variational em algorithms for non-gaussian latent variable
models. in b. sch  olkopf, j. platt, and t. ho   man, editors, advances in neural information processing systems
(nips), number 19, pages 1059   1066, cambridge, ma, 2006. mit press.

[236] v. pavlovic, j. m. rehg, and j. maccormick. learning switching linear models of human motion. in t. k. leen,
t. g. dietterich, and v. tresp, editors, advances in neural information processing systems (nips), number 13,
pages 981   987, cambridge, ma, 2001. mit press.

[237] j. pearl. probabilistic reasoning in intelligent systems : networks of plausible id136. morgan kaufmann,

1988.

[238] j. pearl. causality: models, reasoning and id136. cambridge university press, 2000.

[239] b. a. pearlmutter and l. c. parra. maximum likelihood blind source separation: a context-sensitive gen-
eralization of ica. in m. c. mozer, m. i. jordan, and t. petsche, editors, advances in neural information
processing systems (nips), number 9, pages 613   619, cambridge, ma, 1997. mit press.

[240] k. b. petersen and o. winther. the em algorithm in independent component analysis. in ieee international

conference on acoustics, speech, and signal processing, volume 5, pages 169   172, 2005.

[241] j-p. p   ster, t. toyiozumi, d. barber, and w. gerstner. optimal spike-timing dependent plasticity for precise

action potential firing in supervised learning. neural computation, 18:1309   1339, 2006.

[242] j. platt. fast training of support vector machines using sequential minimal optimization. in b. sch  olkopf,
c. j. c. burges, and a. j. smola, editors, advances in kernel methods - support vector learning, pages 185   208.
mit press, 1999.

[243] i. porteous, d. newman, a. ihler, a. asuncion, p. smyth, and m. welling. fast collapsed id150 for
id44. in kdd    08: proceeding of the 14th acm sigkdd international conference on
knowledge discovery and data mining, pages 569   577, new york, ny, usa, 2008. acm.

[244] j. e. potter and r. g. stern. statistical    ltering of space navigation measurements. in american institute of
aeronautics and astronautics guidance and control conference, volume 13, pages 775   801, cambridge, mass.,
august 1963.

[245] w. press, w. vettering, s. teukolsky, and b. flannery. numerical recipes in fortran. cambridge university

press, 1992.

[246] s. j. d. prince and j. h. elder. probabilistic id156 for id136s about identity. in

ieee 11th international conference on id161 iccv, pages 1   8, 2007.

[247] l. r. rabiner. a tutorial on id48 and selected applications in id103. proc. of

the ieee, 77(2):257   286, 1989.

[248] c. e. rasmussen and c. k. i. williams. gaussian processes for machine learning. mit press, 2006.

[249] h. e. rauch, g. tung, and c. t. striebel. maximum likelihood estimates of linear dynamic systems. american

institute of aeronautics and astronautics journal (aiaaj), 3(8):1445   1450, 1965.

[250] t. richardson and p. spirtes. ancestral graph markov models. annals of statistics, 30(4):962   1030, 2002.

[251] d. rose, r. e. tarjan, and e. s. lueker. algorithmic aspects of vertex elimination of graphs. siam journal on

computing, (5):266   283, 1976.

draft november 9, 2017

649

bibliography

bibliography

[252] f. rosenblatt. the id88: a probabilistic model for information storage and organization in the brain.

psychological review, 65(6):386   408, 1958.

[253] s. t. roweis and l. j. saul. nonlinear id84 by locally linear embedding. science,

290(5500):2323   2326, 2000.

[254] d. b. rubin. using the sir algorithm to simulate posterior distributions. in m. h. bernardo, k. m. degroot,

d. v. lindley, and a. f. m. smith, editors, bayesian statistics 3. oxford university press, 1988.

[255] d. saad and m. opper. advanced mean field methods theory and practice. mit press, 2001.

[256] r. salakhutdinov, s. roweis, and z. ghahramani. optimization with em and expectation-conjugate-gradient.
in t. fawcett and n. mishra, editors, international conference on machine learning, number 20, pages 672   679,
menlo park, ca, 2003. aaai press.

[257] l. k. saul and m. i. jordan. exploiting tractable substructures in intractable networks. in d. s. touretzky,
m. mozer, and m. e. hasselmo, editors, advances in neural information processing systems (nips), number 8,
pages 486   492, cambridge, ma, 1996. mit press.

[258] l. savage. the foundations of statistics. wiley, 1954.

[259] r. d. schachter. bayes-ball: the rational pastime (for determining irrelevance and requisite information in belief
networks and in   uence diagrams). in g. cooper and s. moral, editors, uncertainty in arti   cial intelligence,
number 14, pages 480   487, san francisco, ca, 1998. morgan kaufmann.

[260] b. sch  olkopf, a. smola, and k. r. m  uller. nonlinear component analysis as a kernel eigenvalue problem.

neural computation, 10:1299   1319, 1998.

[261] n. n. schraudolph and d. kamenetsky. e   cient exact id136 in planar ising models. in d. koller, d. schuur-
mans, y. bengio, and l. bottou, editors, advances in neural information processing systems (nips), number 21,
pages 1417   1424, cambridge, ma, 2009. mit press.

[262] e. schwarz. estimating the dimension of a model. annals of statistics, 6(2):461   464, 1978.

[263] m. seeger. gaussian processes for machine learning. international journal of neural systems, 14(2):69   106,

2004.

[264] m. seeger. expectation propagation for exponential families. technical report, department of eecs, berkeley,

2005. www.kyb.tuebingen.mpg.de/bs/people/seeger.

[265] j. shawe-taylor and n. cristianini. kernel methods for pattern analysis. cambridge university press, 2004.

[266] s. siddiqi, b. boots, and g. gordon. a constraint generation approach to learning stable linear dynamical
systems. in j. c. platt, d. koller, y. singer, and s. roweis, editors, advances in neural information processing
systems (nips), number 20, pages 1329   1336, cambridge, ma, 2008. mit press.

[267] t. silander, p. kontkanen, and p. myllym  aki. on sensitivity of the map id110 structure to the
equivalent sample size parameter. in r. parr and l. van der gaag, editors, uncertainty in arti   cial intelligence,
number 23, pages 360   367, corvallis, oregon, usa, 2007. auai press.

[268] s. s. skiena. the algorithm design manual. springer-verlag, new york, usa, 1998.

[269] e. smith and m. s. lewicki. e   cient auditory coding. nature, 439(7079):978   982, 2006.

[270] p. smolensky. parallel distributed processing: volume 1: foundations, chapter information processing in

dynamical systems: foundations of harmony theory, pages 194   281. mit press, cambridge, ma, 1986.

[271] g. sneddon. studies in the atmospheric sciences, chapter a statistical perspective on data assimilation in

numerical models. number 144 in lecture notes in statistics. springer-verlag, 2000.

[272] p. sollich. bayesian methods for support vector machines: evidence and predictive class probabilities. machine

learning, 46(1-3):21   52, 2002.

[273] d. x. song, d. wagner, and x. tian. timing analysis of keystrokes and timing attacks on ssh. in proceedings

of the 10th conference on usenix security symposium. usenix association, 2001.

[274] a. s. spanias. speech coding: a tutorial review. proceedings of the ieee, 82(10):1541   1582, oct 1994.

[275] p. spirtes, c. glymour, and r. scheines. causation, prediction, and search. mit press, 2 edition, 2000.

650

draft november 9, 2017

bibliography

bibliography

[276] n. srebro. maximum likelihood bounded tree-width markov networks. in j. breese and d. koller, editors,
uncertainty in arti   cial intelligence, number 17, pages 504   511, san francisco, ca, 2001. morgan kaufmann.

[277] h. steck. constraint-based structural learning in id110s using finite data sets. phd thesis,

technical university munich, 2001.

[278] h. steck. learning the id110 structure: dirichlet prior vs data. in d. a. mcallester and p. myl-
lymaki, editors, uncertainty in arti   cial intelligence, number 24, pages 511   518, corvallis, oregon, usa, 2008.
auai press.

[279] h. steck and t. jaakkola. on the dirichlet prior and bayesian id173. in s. becker, s. thrun, and

k. obermayer, editors, nips, pages 697   704. mit press, 2002.

[280] g. strang. id202 and its applications. brooks cole, 1988.

[281] m. studen  y. on mathematical description of probabilistic conditional independence structures. phd thesis,

academy of sciences of the czech republic, 2001.

[282] m. studen  y. on non-graphical description of models of conditional independence structure. in hsss workshop

on stochastic systems for individual behaviours. louvain la neueve, belgium, 22-23 january 2001.

[283] c. sutton and a. mccallum. an introduction to conditional random    elds for relational learning. in l. getoor

and b. taskar, editors, introduction to statistical relational learning. mit press, 2006.

[284] r. s. sutton and a. g. barto. id23: an introduction. mit press, 1998.

[285] r. j. swendsen and j-s. wang. nonuniversal critical dynamics in monte carlo simulations. physical review

letters, 58:86   88, 1987.

[286] b. k. sy. a recurrence local computation approach towards ordering composite beliefs in bayesian belief

networks. international journal of approximate reasoning, 8:17   50, 1993.

[287] t. sejnowski. the book of hebb. neuron, 24:773   776, 1999.

[288] r. e. tarjan and m. yannakakis. simple linear-time algorithms to test chordality of graphs, test acyclicity of

hypergraphs, and selectively reduce acyclic hypergraphs. siam journal on computing, 13(3):566   579, 1984.

[289] s. j. taylor. modelling financial time series. world scienti   c, second edition, 2008.

[290] y. w. teh, d. newman, and m. welling. a collapsed id58ian id136 algorithm for latent
dirichlet allocation. in j. c. platt, d. koller, y. singer, and s. roweis, editors, advances in neural information
processing systems (nips), number 20, pages 1481   1488, cambridge, ma, 2008. mit press.

[291] y. w. teh and m. welling. the uni   ed propagation and scaling algorithm. in t. g. dietterich, s. becker,
and z. ghahramani, editors, advances in neural information processing systems (nips), number 14, pages
953   960, cambridge, ma, 2002. mit press.

[292] r. tibshirani. regression shrinkage and selection via the lasso. journal of the royal statistical society (b),

58:267   288, 1996.

[293] h. tijms. understanding id203. cambridge university press, 2003.

[294] m. tipping and c. m. bishop. mixtures of probabilistic principal component analysers. neural computation,

11(2):443   482, 1999.

[295] m. e. tipping. sparse bayesian learning and the relevance vector machine. journal of machine learning

research, (1):211   244, 2001.

[296] m. e. tipping. sparse bayesian learning and the relevance vector machine. journal of machine learning

research, 1:211   244, 2001.

[297] m. e. tipping and c. m. bishop. probabilistic principal component analysis. journal of the royal statistical

society, series b, 61(3):611   622, 1999.

[298] d. m. titterington, a. f. m. smith, and u. e. makov. statistical analysis of    nite mixture distributions. wiley,

1985.

[299] m. toussaint, s. harmeling, and a. storkey. probabilistic id136 for solving (po)mdps. research report

edi-inf-rr-0934, university of edinburgh, school of informatics, 2006.

draft november 9, 2017

651

bibliography

bibliography

[300] m. tsodyks, k. pawelzik, and h. markram. neural networks with dynamic synapses. neural computation,

10:821   835, 1998.

[301] l. van der matten and g. hinton. visualizing data using id167. journal of machine learning research,

9:2579   2605, 2008.

[302] p. van overschee and b. de moor. subspace identi   cation for linear systems; theory, implementations,

applications. kluwer, 1996.

[303] v. vapnik. the nature of statistical learning theory. springer, new york, 1995.

[304] m. verhaegen and p. van dooren. numerical aspects of di   erent kalman filter implementations.

ieee

transactions of automatic control, 31(10):907   917, 1986.

[305] t. verma and j. pearl. causal networks : semantics and expressiveness. in r. d. schacter, t. s. levitt, l. n.
kanal, and j.f. lemmer, editors, uncertainty in arti   cial intelligence, volume 4, pages 69   76, amsterdam,
1990. north-holland.

[306] t. o. virtanen, a. t. cemgil, and s. j. godsill. bayesian extensions to nonnegative matrix factorisation for
audio signal modelling. in ieee international conference on acoustics, speech, and signal processing, pages
1825   1828, 2008.

[307] g. wahba. support vector machines, repreducing kernel hilbert spaces, and randomized gacv, pages 69   88.

mit press, 1999.

[308] m. j. wainwright and m. i. jordan. id114, exponential families, and variational id136. founda-

tions and trends in machine learning, 1(1-2):1   305, 2008.

[309] h. wallach. e   cient training of conditional random    elds. master   s thesis, division of informatics, university

of edinburgh, 2002.

[310] y. wang, j. hodges, and b. tang. classi   cation of web documents using a naive bayes method. 15th ieee

international conference on tools with arti   cial intelligence, pages 560   564, 2003.

[311] s. waterhouse, d. mackay, and t. robinson. bayesian methods for mixtures of experts. in d. s. touretzky,
m. mozer, and m. e. hasselmo, editors, advances in neural information processing systems (nips), number 8,
pages 351   357, cambridge, ma, 1996. mit press.

[312] c. watkins and p. dayan. id24. machine learning, 8:279   292, 1992.

[313] y. weiss and w. t. freeman. correctness of belief propagation in gaussian id114 of arbitrary

topology. neural computation, 13(10):2173   2200, 2001.

[314] m. welling, t. p. minka, and y. w. teh. structured region graphs: morphing ep into gbp. in f. bacchus
and t. jaakkola, editors, uncertainty in arti   cial intelligence, number 21, pages 609   614, corvallis, oregon,
usa, 2005. auai press.

[315] j. whittaker. id114 in applied multivariate statistics. john wiley & sons, 1990.

[316] w. wiegerinck. variational approximations between mean    eld theory and the junction tree algorithm. in
c. boutilier and m. goldszmidt, editors, uncertainty in arti   cial intelligence, number 16, pages 626   633, san
francisco, ca, 2000. morgan kaufmann.

[317] w. wiegerinck and t. heskes. fractional belief propagation. in s. becker, s. thrun, and k. obermayer, editors,
advances in neural information processing systems (nips), number 15, pages 438   445, cambridge, ma, 2003.
mit press.

[318] c. k. i. williams. computing with in   nite networks. in m. c. mozer, m. i. jordan, and t. petsche, editors,
advances in neural information processing systems nips 9, pages 295   301, cambridge, ma, 1997. mit press.

[319] c. k. i. williams and d. barber. bayesian classi   cation with gaussian processes. ieee trans pattern analysis

and machine intelligence, 20:1342   1351, 1998.

[320] c. yanover and y. weiss. finding the m most probable con   gurations using loopy belief propagation. in
s. thrun, l. saul, and b. sch  olkopf, editors, advances in neural information processing systems (nips),
number 16, pages 1457   1464, cambridge, ma, 2004. mit press.

[321] j. s. yedidia, w. t. freeman, and y. weiss. constructing free-energy approximations and generalized belief

propagation algorithms. id205, ieee transactions on, 51(7):2282   2312, july 2005.

652

draft november 9, 2017

bibliography

bibliography

[322] s. young, d. kershaw, j. odell, d. ollason, v. valtchev, and p. woodland. the htk book version 3.0.

cambridge university press, 2000.

[323] a. l. yuille and a. rangarajan. the concave-convex procedure. neural computation, 15(4):915   936, 2003.

[324] j.-h. zhao, p. l. h. yu, and q. jiang. ml estimation for factor analysis: em or non-em? statistics and

computing, 18(2):109   123, 2008.

[325] o. zoeter. monitoring non-linear and switching dynamical systems. phd thesis, radboud university nijmegen,

2005.

draft november 9, 2017

653

bibliography

bibliography

654

draft november 9, 2017

index

1     of     m coding, 233
n -max-product, 86
  -expansion, 595
  -recursion, 461
  -recursion, 462
  -recursion, 463
maximum likelihood

hop   eld network, 528

naive mean    eld, 576

absorbing state, 87
absorption, 99

in   uence diagram, 131

acceptance function, 553
active learning, 293
acyclic, 26
adjacency matrix, 28, 424
algebraic riccati equation, 496
ancestor, 26
ancestral ordering, 548
ancestral sampling, 548
antifreeze, 256
approximate id136, 115, 419, 569

belief propagation, 586, 587
bethe free energy, 585
double integration bound, 597
expectation propagation, 587
graph cut, 594
laplace approximation, 569
switching linear dynamical system, 508
variational approach, 570
variational id136, 573

ar model, see auto-regressive model
arch model, 488
arti   cial life, 532
asymmetry, 127
asynchronous updating, 575
auto-regressive model, 485

arch, 488
garch, 488
switching, 500
time-varying, 487

average, 158

backtracking, 83
bag of words, 234, 319
batch update, 356
baum-welch, 468
bayes information criterion, 275
bayes   

factor, 208, 267
model selection, 267
theorem, 9

bayes    rule, see bayes    theorem
bayesian

decision theory, 299
hypothesis testing, 267, 276
image denoising, 573
linear model, 367
mixture model, 412
model selection, 267
occam   s razor, 270
outcome analysis, 276

bayesian dirichlet score, 209
bayesian linear model, 386
bd score, 209

bdeu score, 209

bdeu score, 209
belief network

chest clinic, 51

belief network

asbestos-smoking-cancer, 202
cascade, 38
divorcing parents, 35
dynamic, 474
noisy and gate, 35
noisy logic gate, 35
noisy or gate, 35
sigmoid, 264
structure learning, 205
training

bayesian, 199

belief propagation, 78, 586

loopy, 584

auxiliary variable sampling, 555

belief revision, see max-product, 85

655

index

index

bellman   s equation, 135
bessel function, 392
beta

distribution, 165
function, 165, 193
bethe free energy, 586
bias, 174

unbiased estimator, 174

bigram, 468
binary id178, 575
binomial coe   cient, 163
binomial options pricing model, 141
bioinformatics, 475
black and white sampling, 566
black-box, 300
black-scholes option pricing, 142
blahut-arimoto algorithm, 583
id82, 59, 71, 214, 598

restricted, 71

bond propagation, 93
bonferroni inequality, 22
boolean network, 532
bradley-terry-luce model, 447
bucket elimination, 90
burn in, 551

calculus, 610
canonical correlation analysis, 332

constrained factor analysis, 439

canonical variates, 339
causal consistency, 127
causality, 47, 127

do calculus, 49
in   uence diagrams, 49
post intervention distribution, 49

cca

see canonical correlation analysis, 332

centering, 171
chain graph, 65

chain component, 65

chain rule, 612
chain structure, 84
changepoint model, 518
checkerboard, 566
chest clinic, 51

missing data, 262
with decisions, 147

children, see directed acyclic graph
cholesky, 340
chord, 26
chordal, 107
chow-liu tree, 211
classi   cation, 292, 357, 396

bayesian, 375
boundary, 231
error analysis, 276

linear parameter model, 352
multiple classes, 358
performance, 276

random guessing, 284

softmax, 358

clique, 26

decomposition, 423
graph, 98
matrix, 28, 423

cliquo, 28
cluster variation method, 587
id91, 293
collaborative    ltering, 324
collider, see directed acyclic graph
commute, 605
compatibility function, 565
competition model

bradley-terry-luce, 447
elo, 448
trueskill, 448

competition models, 447
concave function, 612
conditional id178, 582
conditional likelihood, 227
conditional mutual information, 237
id155, 8
conditional random    eld, 221, 473
conditioning, 170

loop cut set, 92

conjugate distribution, 173
exponential family, 172
gaussian, 178, 179
prior, 172

conjugate gradient, 357
conjugate gradients algorithm, 616
conjugate vector, 615
conjugate vectors algorithm, 616
connected components, 27
connected graph, 27
consistent, 103
consistent estimator, 174
control, 578
convex function, 612
correction smoother, 463, 494
correlation

matrix, 160

cosine similarity, 321
coupled id48, 474
covariance, 160
matrix, 160

covariance function, 387, 390

  -exponential, 392
construction, 391
gibbs, 393
isotropic, 392

656

draft november 9, 2017

index

index

mat  ern, 392
mercer kernel, 394
neural network, 393
non-stationary, 393
ornstein-uhlenbeck, 392, 394
periodic, 392
rational quadratic, 392
smoothness, 394
squared exponential, 392
stationary, 391

cpt, see id155 table
crf, see conditional random    eld
critical point, 613
cross-validation, 297
cumulant, 545
curse of dimensionality, 137, 350
cut set conditioning, 91
cycle, 26

d-map, see dependence map
dag, see directed acyclic graph
data

anomaly detection, 293
catagorical, 157
dyadic, 421
handwritten digits, 357
labelled, 291
monadic, 422
numerical, 157
ordinal, 157
unlabelled, 292

data compression, 415

vector quantisation, 415

decision boundary, 353
decision function, 294
decision theory, 121, 194, 299
decision tree, 122
decomposable, 107
degree, 29
degree of belief, 9
delta function, see dirac delta function

kronecker, 160

density estimation, 403

parzen estimator, 414

dependence
map, 69

descendant, 26
design matrix, 373, 386
determinant, 606
deterministic latent variable model, 532
di   erentiation, 611
digamma function, 183
digit data, 315
dijkstra   s algorithm, 88
dimension reduction
linear, 311, 314

draft november 9, 2017

supervised, 337

id84

linear, 316
non-linear, 317

dirac delta function, 160, 161
directed acyclic graph, 38

ancestor, 26
ancestral order, 28
cascade, 38
children, 26
collider, 40
descendant, 26
family, 26
immorality, 46
moralisation, 63
parents, 26

direction bias, 473
directional derivative, 612
dirichlet

distribution, 167

dirichlet process mixture models, 418
discount factor, 136
discriminative

approach, 300
training, 300

discriminative approach, 299
discriminative training, 471
dissimilarity function, 305
distributed computation, 525
distribution, 7

bernoulli, 163, 407
beta, 165, 167, 182, 200, 269
binomial, 163
categorical, 163
change of variables, 158, 181
conjugate, 178
continuous, 9
density, 9
dirichlet, 167, 183, 202, 208, 418, 419
discrete, 7
divergence, 161
double exponential, 166
empirical, 161, 295

average, 158
expectation, 158

exponential, 164
exponential family, 171

canonical, 171

gamma, 412
mode, 183

gauss-gamma, 179, 184
gauss-inverse-gamma, 178
gaussian

canonical exponential form, 172
conditioning, 170

657

index

index

conjugate, 178, 179
id178, 171
mixture, 186
multivariate, 168
normalisation, 180, 181
partitioned, 170
propagation, 170
system reversal, 170
univariate, 166

inverse gamma, 165
inverse wishart, 179
isotropic, 169
joint, 8
kurtosis, 160
laplace, 166
marginal, 8
mode, 159
moment, 159
multinomial, 163
normal, 166
poisson, 164, 185
polya, 418
scaled mixture, 167
skewness, 160
student   s t, 166
uniform, 162, 164
wishart, 412

domain, 7
double integration bound, 597
dual parameters, 351
dual representation, 351
dyadic data, 421
dynamic id110, 474
dynamic synapses, 536
dynamical system

linear, 483
non-linear, 532

dynamics reversal, 494

early stopping, 357
edge list, 27
e   cient ipf, 218
eigen

decomposition, 169, 314, 609
equation, 313
function, 610
problem, 333
spectrum, 394
value, 314, 608

elo model, 447
emission distribution, 460
emission matrix, 489
empirical

independence, 207

empirical distribution, 161, 176, 295
empirical risk, 295

penalised, 296

empirical risk minimisation, 296
energy, 245
id178, 110, 162, 186, 245, 582

di   erential, 162, 186
gaussian, 171

ep, see expectation propagation
error function, 353
estimator

consistent, 174

evidence, see marginal likelihood

hard, 35
likelihood, 37
soft, 35
uncertain, 35
virtual, 37

evidence procedure, 369
exact sampling, 548
expectation, see average
expectation correction, 512, 513
expectation maximisation, 151, 244, 325, 370, 468,

498

algorithm, 244, 245
antifreeze, 256
belief networks, 248
e-step, 245
energy, 245
id178, 245
failure case, 255
intractable energy, 254
m-step, 245
mixture model, 405
partial e-step, 253
partial m-step, 253
id58, 258
viterbi training, 254

expectation propagation, 587
exponential family, 171
canonical form, 171
conjugate, 172

extended observability matrix, 499

face model, 436
factor analysis, 431

factor rotation, 430
probabilistic pca, 438
training

em, 434
svd, 432
factor graph, 67
factor loading, 429
family, see directed acyclic graph
feature map, 330
   ltering, 461
   nance, 139

optimal investment, 142

658

draft november 9, 2017

index

index

option pricing, 140

   nite dimensional gaussian process, 387
fisher information, 184
fisher   s linear discriminant, 337
floyd-warshall-roy algorithm, 89
forward sampling, 548
forward-backward, 462
forward-   ltering-backward-sampling, 464
forward-sampling-resampling, 564

gamma

digamma, 183
distribution, 164
function, 166, 183

garch model, 488
gaussian

canonical representation, 168
distribution, 166
moment representation, 168, 492
sub, 160
super, 160

gaussian mixture model, 409

bayesian, 412
collapsing the mixture, 511
em algorithm, 409
in   nite problems, 412
id116, 415
parzen estimator, 414
symmetry breaking, 412

gaussian process, 385
classi   cation, 396
laplace approximation, 397
multiple classes, 400
regression, 388
smoothness, 394
weight space view, 386
gaussian sum    ltering, 508
gaussian sum smoothing, 512
generalisation, 291, 295
generalised pseudo bayes, 516
generative

approach, 299
model, 299
training, 299

generative approach, 299
id150, 549
glicko, 448
gmm, see gaussian mixture model
google, 457
gradient, 611

descent, 613

gram matrix, 351, 387
gram-schmidt procedure, 615
graph, 25

acyclic, 26
adjacency matrix, 28

draft november 9, 2017

chain, 65
chain structured, 84
chord, 26
chordal, 107
clique, 26, 28, 98
clique matrix, 28
cliquo, 28
connected, 27
cut, 594
cycle, 26
decomposable, 107
descendant, 28
directed, 25
disconnected, 111
edge list, 27
factor, 67
loop, 26
loopy, 27
multiply connected, 27, 105
neighbour, 26
path, 25
separation, 63
set chain, 115
singly connected, 27
skeleton, 46
spanning tree, 27
tree, 27
triangulated, 107
undirected, 25
vertex

degree, 29

graph cut algorithm, 594
graph partitioning, 421
gull-mackay iteration, 372

hamilton-jacobi equation, 135
hamiltonian dynamics, 556
hammersley-cli   ord theorem, 61
handwritten digits, 357
hankel matrix, 499
harmonium, see restricted id82
heaviside step function, 423
hebb, 526
hebb rule, 526
hedge fund, 282
hermitian, 606
hessian, 357, 611
hidden markov model, 110, 254, 460

   recursion, 461
   recursion, 462
coupled, 474
direction bias, 473
discriminative training, 471
duration model, 471
id178, 110
   ltering, 461

659

index

index

input-output, 472
likelihood, 463
most likely state (map), 464
pairwise marginal, 463
rauch tung striebel smoother, 463
smoothing

parallel, 462
sequential, 462

viterbi, 86
viterbi algorithm, 464

hidden variables, 241
id48, see hidden markov model
hop   eld network, 525

augmented, 534
capacity, 529
hebb rule, 527
heteroassociative, 534
maximum likelihood, 528
id88, 529
pseudo inverse rule, 527
sequence learning, 526

hybrid monte carlo, 555
hyper markov, 224
hyper tree, 109
hyperparameter, 172, 201, 369
hyperplane, 604
hypothesis testing, 267

bayesian error analysis, 276

i-map, see independence map
ica, 440
identi   ability, 497
identity matrix, 605
iid, see independent and identically distributed
im algorithm, see information-maximisation algorithm
immorality, 46
importance

distribution, 561
sampling, 560

particle    lter, 563
resampling, 562
sequential, 562

weight, 561

incidence matrix, 28
independence

bayesian, 208
conditional, 11, 32
empirical, 207
map, 68
markov equivalent, 46
mutual information, 163
naive bayes, 229
parameter, 199
perfect map, 69

independent and identically distributed, 173, 191
independent components analysis, 443

indicator function, 19
indicator model, 417
induced representation, 106
id136, 75

bond propagation, 93
bucket elimination, 90
causal, 47
cut set conditioning, 91
id48, 461
linear dynamical system, 490
map, 93
marginal, 75
markov decision process, 138
max-product, 83
message passing, 75
mixed, 89
mpm, 93
sum-product algorithm, 80
transfer matrix, 77
variable elimination, 75

in   uence diagram, 125

absorption, 131
asymmetry, 127
causal consistency, 127
chest clinic, 146
decision potential, 130
fundamental link, 127
information link, 125
junction tree, 130
no forgetting assumption, 126
partial order, 126
id203 potential, 130
solving, 129
utility, 125
utility potential, 130

information link, 125
information maximisation, 583
information retrieval, 320, 457
information-maximisation algorithm, 582
innovation noise, 485
input-output id48, 472
inverse modus ponens, 15
ipf, see iterative proportional    tting, 215

e   cient, 218

ising model, see markov network, 64, 93

approximate id136, 573

isotropic, 169, 411
isotropic covariance functions, 392
item response theory, 446
iterated conditional modes, 591
iterative proportional    tting, 215
iterative scaling, 220

je   rey   s rule, 36
jensen   s inequality, 598, 613
joseph   s symmetrized update, 493

660

draft november 9, 2017

index

index

jump markov model, see switching linear dynamical

system

junction tree, 101, 104

absorption, 99
algorithm, 97, 108
clique graph, 98
computational complexity, 110
conditional marginal, 112
consistent, 103
hyper tree, 109
in   uence diagram, 130
marginal likelihood, 111
most likely state, 113
normalisation constant, 110
potential, 98
running intersection property, 102
separator, 98
strong, 131
strong triangulation, 131
tree width, 109
triangulation, 106

id116, 415
kalman    lter, 489
kalman gain, 493
kd-tree, 306
kernel, 351, see covariance function

classi   er, 358

kidnapped robot, 466
kikuchi, 587
kl divergence, see id181
knn, see nearest neighbour
kronecker delta, 160, 605
id181, 161, 245, 571
kurtosis, 160

labelled data, 291
lagrange

dual, 619
multiplier, 619

lagrangian, 619
laplace approximation, 376, 397, 569
latent ability model, 445
id44, 419
latent linear model, 429
latent semantic analysis, 319
latent topic, 320
latent variable, 241

deterministic, 532
model, 241

lattice model, 63
lda regularised, 343
lds, see linear dynamical system
leaky integrate and    re model, 537
leapfrog discretisation, 557
learning

draft november 9, 2017

active, 293
anomaly detection, 293
bayesian, 192
belief network, 196
belief networks

em, 248

dirichlet prior, 202
id136, 191
nearest neighbour, 305
online, 293
query, 293
reinforcement, 293
semi-supervised, 294, 416
sequences, 468, 497, 526
sequential, 293
structure, 205
supervised, 291
unsupervised, 292, 429

learning rate, 355
likelihood, 111, 173, 463, 495, 511

bound, 244
marginal, 81, 173
model, 173, 205

approximate, 274

pseudo, 224

likelihood decomposable, 205
line search, 614, 615
id202, 603
linear dimension reduction, 311, 314

canonical correlation analysis, 332
latent semantic analysis, 319
non-negative matrix factorisation, 328
probabilistic latent semantic analysis, 325
supervised, 337
unsupervised, 311

id156, 337
id156, 337

as regression, 342
penalised, 342
regularised, 342

linear dynamical system, 170, 489

cross moment, 495
dynamics reversal, 494
   ltering, 492
identi   ability, 497
id136, 490
learning, 497
likelihood, 495
most likely state, 496
numerical stability, 491
riccati equations, 496
smoothing, 494
subspace method, 499
switching, 507
symmetrising updates, 493

661

index

index

linear gaussian state space model, 489
linear model, 345
bayesian, 367
classi   cation, 352
factor analysis, 431
latent, 429
regression, 346

linear parameter model, 346

bayesian, 368

linear id88, 354
linear separability, 354
linear transformation, 606
linearly independent, 603
linearly separable, 354
linsker   s as-if-gaussian approximation, 583
localisation, 465
logic

aristotle, 15

id28, 353
logistic sigmoid, 353
logit, 353
loop, 26
loop cut set, 92
loopy, 27
id168, 294, 295

squared, 295
zero-one, 294

loss matrix, 295
luenberger expanding subspace theorem, 616

mahalanobis distance, 305, 335
manifold

linear, 311
low dimensional, 311

map, see most probable a posteriori, see most prob-

able a posteriori

mar, see missing at random
margin, 359
soft, 360

marginal, 8

generalised, 131

marginal likelihood, 81, 111, 173, 243, 369, 389

approximate, 371, 376, 377, 399

marginalisation, 8
markov

blanket, 26
chain, 263, 455

   rst order, 484
stationary distribution, 456

equivalent, 46
global, 60
hyper, 224
local, 60
model, 455
pairwise, 60
random    eld, 220

approximation, 573

markov chain, 75, 139, 553

absorbing state, 87
id95, 457

id115, 552

auxiliary variable, 555

hybrid monte carlo, 555
slice sampling, 559
swendson-wang, 557

id150, 550
metropolis-hastings, 553
proposal distribution, 553
structured id150, 551

markov decision process, 133, 148

bellman   s equation, 135
discount factor, 136
partially observable, 144
planning, 138
policy iteration, 137
id23, 144
stationary deterministic policy, 138
temporally unbounded, 136
value iteration, 136
markov equivalence, 46
markov network, 58

id82, 59
continuous-state temporal, 483
discrete-state temporal, 455
gibbs distribution, 58
hammersley cli   ord theorem, 61
pairwise, 58
potential, 58

markov random    eld, 60, 599, 600

alpha-expansion, 595
attractive binary, 593
graph cut, 594
map, 590
potts model, 595

matrix, 605

adjacency, 28, 424
block inverse, 608
cholesky, 340
clique, 28
gram, 331
hankel, 499
incidence, 28
inversion, 607
inversion lemma, 607
orthogonal, 607
positive de   nite, 610
pseudo inverse, 607
rank, 607
stochastic, 77

matrix factorisation, 328
max-product, 83

662

draft november 9, 2017

index

index

n most probable states, 86

max-sum, 87
maximum cardinality checking, 108
maximum likelihood, 18, 173, 174, 176, 196, 355

belief network, 196
chow-liu tree, 212
counting, 196
empirical distribution, 176
factor analysis, 431
gaussian, 176
gradient optimisation, 261
markov network, 213
ml-ii, 195
naive bayes, 230
properties, 174

mcmc, see id115
mean    eld theory, 576

asynchronous updating, 576

mercer kernel, 394
message

passing, 100
schedule, 80, 101

message passing, 75
metropolis-hastings acceptance function, 553
metropolis-hastings sampling, 553
minimum clique cover, 424
missing at random, 242

completely, 243

missing data, 241
mixed id136, 89
mixed membership model, 419
mixing matrix, 440
mixture

gaussian, 186

mixture model, 403

bernoulli product, 407
dirichlet process mixture, 418
expectation maximisation, 405
factor analysis, 435
gaussian, 409
indicator approach, 417
markov chain, 458
pca, 435

mixture of experts, 416
mn, see markov network
mode, 159
model

auto-regressive, 485
changepoint, 518
deterministic latent variable, 532
faces, 436
leaky integrate and    re, 537
linear, 345
mixed membership, 419
mixture, 403

rasch, 445

model selection, 267
approximate, 274

moment, 159
moment representation, 492
momentum, 614
monadic data, 422
money

   nancial prediction, 282
loadsa, 282

moralisation, 63, 104
most probable a posteriori, 18, 173
most probable path

multiple-source multiple-sink, 89

most probable state

n most probable, 85

mrf, see markov random    eld
multiply connected, 27
multiply connected-distributions, 105
mutual information, 163, 212, 581

approximation, 581
conditional, 162
maximisation, 582

naive bayes, 229, 407

bayesian, 234
tree augmented, 236

naive mean    eld theory, 574
nearest neighbour, 305

probabilistic, 308

network    ow, 599
network modelling, 330
neural computation, 525
neural network, 365, 535

depression, 536
dynamic synapses, 536
leaky integrate and    re, 537

newton update, 357
newton   s method, 617
no forgetting assumption, 126
node

extremal, 85
simplicial, 85

non-negative matrix factorisation, 328
normal distribution, 166
normal equations, 347
normalised importance weights, 561

observed linear dynamical system, 483
occam   s razor, 270
one of m encoding, 157
online learning, 293
optimisation, 198, 613

broyden-fletcher-goldfarb-shanno, 618
conjugate gradients algorithm, 616
conjugate vectors algorithm, 616

draft november 9, 2017

663

index

index

constrained optimisation, 619
critical point, 613
id119, 613
luenberger expanding subspace theorem, 616
newton   s method, 617
quasi id77, 618

option pricing, 140
ordinary least squares, 345
orthogonal, 603
orthogonal least squares, 346
orthonormal, 603
outlier, 362
over-complete representation, 324
over-complete representations, 324
overcounting, 98
over   tting, 222, 272, 388

id95, 457
pairwise comparison models, 447
pairwise markov network, 58
parents, see directed acyclic graph
part-of-speech tagging, 475
partial least squares, 440
partial order, 126
partially observable mdp, 144
particle    lter, 562
partition function, 58, 598
partitioned matrix
inversion, 181

parzen estimator, 308, 414
path

blocked, 43

pc algorithm, 206
pca, see principal components analysis
id88, 354

id28, 355

perfect elimination order, 107
perfect map, see independence
perfect sampling, 548
planning, 138
plant monitoring, 293
plate, 192
poisson distribution, 164, 185
policy, 136

iteration, 137
stationary deterministic, 138

polya distribution, 418
pomdp, see partially observable mdp
positive de   nite
matrix, 387

parameterisation, 425

posterior, 173

dirichlet, 203

potential, 58
potts model, 595
precision, 168, 179, 368

664

prediction

auto-regression, 485
   nancial, 282
non-parametric, 385
parameteric, 385

predictive variance, 368
predictor-corrector, 461
principal components analysis, 311, 316

algorithm, 314
high dimensional data, 317
kernel, 330
latent semantic analysis, 319
missing data, 321
probabilistic, 438

principal directions, 314
printer nightmare, 226
missing data, 262

prior, 173
probabilistic latent semantic analysis, 325

conditional, 327
em algorithm, 325
probabilistic pca, 438
id203

conditional, 8

function, 198
density, 9, 158
frequentist, 9
posterior, 173
potential, 130
prior, 173
subjective, 9

probit, 353
probit regression, 353
projection, 604
proposal distribution, 553
pseudo inverse, 526
pseudo inverse

hop   eld network, 527

pseudo likelihood, 224

quadratic form, 610
quadratic programming, 359
query learning, 293
questionnaire analysis, 445

radial basis functions, 350
raleigh quotient, 340
random boolean networks, 532
rasch model, 445
bayesian, 446

rauch-tung-striebel, 463
reabsorption, 114
region graphs, 587
regresion

linear parameter model, 346

regression, 292, 386

draft november 9, 2017

index

index

logisitic, 353

regularisation, 272, 296, 348, 357
id23, 144, 293
rejection sampling, 545
relevance vector machine, 374, 381
reparameterisation, 97
representation
dual, 351
over-complete, 324
sparse, 324
under-complete, 324

resampling, 561
reset model, 518
residuals, 346
responsibility, 411
restricted id82, 71
riccati equation, 496
ridge regression, 349
risk, 294
robot arm, 578
robust classi   cation, 362
rose-tarjan-lueker elimination, 107
running intersection property, 102, 103

sample

mean, 161
variance, 161

sampling, 419

ancestral, 548
forward-   ltering-backward-sampling, 464, 495
gibbs, 549
importance, 560
multi-variate, 546
particle    lter, 563
rejection, 545
univariate, 544

sampling importance resampling, 561
scalar product, 603
scaled mixture, 167
search engine, 457
self localisation and mapping, 467
semi-supervised learning, 294

lower dimensional representations, 301

separator, 98
sequential importance sampling, 562
sequential minimal optimisation, 362
set chain, 115
shafer-shenoy propagation, 113
shortest path, 87
shortest weighted path, 88
sigmoid

logistic, 353

sigmoid belief network, 264
sigmoid function

approximate average, 378

simple path, 88

draft november 9, 2017

simplicial nodes, 107
simpson   s paradox, 47
singly connected, 27
singular, 607
singular value decomposition, 318, 609

thin, 341, 609

skeleton, 46, 206
skewness, 160
slice sampling, 559
smoothing, 462
softmax function, 358, 416
spam    ltering, 234
spanning tree, 27
sparse representation, 324
spectrogram, 487
id103, 474
spike response model, 535
squared euclidean distance, 305
squared loss, 295
standard deviation, 159
standard normal distribution, 166
stationary, 553

distribution, 78

stationary markov chain, 456
stochastic matrix, 77
stop words, 420
strong junction tree, 130
strong triangulation, 131
structure learning, 205

bayesian, 209
network scoring, 209
pc algorithm, 206
undirected, 224

structured expectation propagation, 590
subsampling, 551
subspace method, 499
sum-product, 584
sum-product algorithm, 80
supervised learning, 291

-semi, 294
classi   cation, 292
regression, 292

support vector machine, 359

chunking, 362
training, 362

support vectors, 360
svd, see singular value decomposition
id166, see support vector machine
swendson-wang sampling, 557
switching ar model, 500
switching kalman    lter, see switching linear dynam-

ical system

switching linear dynamical system, 507

changepoint model, 518
collapsing gaussians, 511

665

index

index

unlabelled data, 293
unsupervised learning, 292, 429
utility, 121, 294
matrix, 295
money, 121
potential, 130
zero-one loss, 294

validation, 297
cross, 297

value, 135
value iteration, 136
variable

hidden, 244
missing, 244
visible, 244

variable elimination, 75
variance, 159
variational approximation

factorised, 574
structured, 576, 577

id58, 256

expectation maximisation, 258

varimax, 430
vector algebra, 603
vector quantisation, 415
visualisation, 334
viterbi, 86, 254
viterbi algorithm, 464
viterbi alignment, 460
viterbi training, 254
volatility, 488
voronoi tessellation, 305

web

modelling, 330

website, 330

analysis, 457

whitening, 171, 186
woodbury formula, 607

xor function, 354

zero-one loss, 294, 362

expectation correction, 512
   ltering, 508
gaussian sum smoothing, 512
generalised pseudo bayes, 516
id136

computational complexity, 508

likelihood, 511
smoothing, 514

symmetry breaking, 412
system reversal, 170

tagging, 475
tall matrix, 324
term-document matrix, 319
test set, 291
text analysis, 329, 419

latent semantic analysis, 319
latent topic, 320
probabilistic latent semantic analysis, 325

tf-idf, 319
time-invariant lds, 496
tower of hanoi, 137
trace-log formula, 609
train set, 291
training

batch, 356
discriminative, 300
generative, 299
generative-discriminative, 300
id48, 468
linear dynamical system, 497
online, 356

transfer matrix, 77
transition distribution, 460
transition matrix, 483, 489
tree, 27, 76

chow-liu, 211

tree augmented network, 236
tree width, 109
triangulation, 106, 107

check, 109
greedy elimination, 107
maximum cardinality, 108
strong, 131
variable elimination, 107

trueskill, 448

uncertainty, 162
under-complete representation, 324
undirected graph, 25
undirected model

learning

hidden variable, 261
latent variable, 261
uniform distribution, 164
unit vector, 603

666

draft november 9, 2017

