i

id23:

an introduction

second edition, in progress

****complete draft****

november 5, 2017

richard s. sutton and andrew g. barto

c(cid:13) 2014, 2015, 2016, 2017

the text is now complete, except possibly for one more case study to be added to chapter 16. the
references still need to be thoroughly checked, and an index still needs to be added. please send any
errors to rich@richsutton.com and barto@cs.umass.edu. we are also very interested in correcting any
important omissions in the    bibliographical and historical remarks    at the end of each chapter. if
you think of something that really should have been cited, please let us know and we can try to get it
corrected before the    nal version is printed.

a bradford book

the mit press

cambridge, massachusetts

london, england

ii

in memory of a. harry klopf

contents

preface to the first edition

preface to the second edition

summary of notation

1 introduction

1.1 id23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 examples

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 elements of id23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4 limitations and scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.5 an extended example: tic-tac-toe . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7 early history of id23 . . . . . . . . . . . . . . . . . . . . . . . . . . .

i tabular solution methods

2 multi-armed bandits

2.1 a k-armed bandit problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 action-value methods

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 the 10-armed testbed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4

incremental implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5 tracking a nonstationary problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.6 optimistic initial values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.7 upper-con   dence-bound action selection . . . . . . . . . . . . . . . . . . . . . . . . . .

2.8 gradient bandit algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.9 associative search (contextual bandits) . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.10 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 finite id100

3.1 the agent   environment interface

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 goals and rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iii

ix

xi

xv

1

1

4

5

6

7

10

11

18

19

19

20

21

23

25

26

27

28

31

32

37

37

42

iv

contents

3.3 returns and episodes

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 uni   ed notation for episodic and continuing tasks

. . . . . . . . . . . . . . . . . . . .

3.5 policies and value functions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6 optimal policies and optimal value functions

. . . . . . . . . . . . . . . . . . . . . . .

3.7 optimality and approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.8 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 id145

4.1 policy evaluation (prediction)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 policy improvement

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 policy iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4 value iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.5 asynchronous id145 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6 generalized policy iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.7 e   ciency of id145 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.8 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 monte carlo methods

5.1 monte carlo prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 monte carlo estimation of action values

. . . . . . . . . . . . . . . . . . . . . . . . . .

5.3 monte carlo control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4 monte carlo control without exploring starts

. . . . . . . . . . . . . . . . . . . . . . .

5.5 o   -policy prediction via importance sampling . . . . . . . . . . . . . . . . . . . . . . .

5.6

incremental implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.7 o   -policy monte carlo control

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.8

5.9

*discounting-aware importance sampling . . . . . . . . . . . . . . . . . . . . . . . . . .

*per-reward importance sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.10 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 temporal-di   erence learning

6.1 td prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

45

46

50

54

55

59

60

62

64

67

69

70

71

71

75

76

79

80

82

84

89

90

92

93

94

97

97

6.2 advantages of td prediction methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

6.3 optimality of td(0) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

6.4 sarsa: on-policy td control

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

6.5 id24: o   -policy td control

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

6.6 expected sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

6.7 maximization bias and double learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 110

6.8 games, afterstates, and other special cases

. . . . . . . . . . . . . . . . . . . . . . . . 112

6.9 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113

7 n-step id64

115

7.1 n-step td prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

contents

v

7.2 n-step sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

7.3 n-step o   -policy learning by importance sampling

. . . . . . . . . . . . . . . . . . . . 121

7.4

*per-reward o   -policy methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

7.5 o   -policy learning without importance sampling:

the n-step tree backup algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

7.6

*a unifying algorithm: n-step q(  ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

7.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

8 planning and learning with tabular methods

131

8.1 models and planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

8.2 dyna: integrating planning, acting, and learning . . . . . . . . . . . . . . . . . . . . . 133

8.3 when the model is wrong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

8.4 prioritized sweeping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

8.5 expected vs. sample updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142

8.6 trajectory sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

8.7 real-time id145 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

8.8 planning at decision time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149

8.9 heuristic search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

8.10 rollout algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

8.11 id169 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153

8.12 summary of the chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155

8.13 summary of part i: dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156

ii approximate solution methods

9 on-policy prediction with approximation

160

161

9.1 value-function approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161

9.2 the prediction objective (ve) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162

9.3 stochastic-gradient and semi-gradient methods . . . . . . . . . . . . . . . . . . . . . . . 164

9.4 linear methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

9.5 feature construction for linear methods

. . . . . . . . . . . . . . . . . . . . . . . . . . 171

9.5.1 polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172

9.5.2 fourier basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

9.5.3 coarse coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

9.5.4 tile coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177

9.5.5 radial basis functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

9.6 nonlinear function approximation: arti   cial neural networks . . . . . . . . . . . . . . 182

9.7 least-squares td . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186

9.8 memory-based function approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

9.9 kernel-based function approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

9.10 looking deeper at on-policy learning: interest and emphasis

. . . . . . . . . . . . . . 190

vi

contents

9.11 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192

10 on-policy control with approximation

197

10.1 episodic semi-gradient control

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197

10.2 n-step semi-gradient sarsa

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200

10.3 average reward: a new problem setting for continuing tasks . . . . . . . . . . . . . . 202

10.4 deprecating the discounted setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205

10.5 n-step di   erential semi-gradient sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206

10.6 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206

11 *o   -policy methods with approximation

209

11.1 semi-gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210

11.2 examples of o   -policy divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

11.3 the deadly triad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215

11.4 linear value-function geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

11.5 stochastic id119 in the bellman error

. . . . . . . . . . . . . . . . . . . . . 219

11.6 the bellman error is not learnable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223

11.7 gradient-td methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

11.8 emphatic-td methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

11.9 reducing variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231

11.10 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232

12 eligibility traces

235

12.1 the   -return . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236

12.2 td(  ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239

12.3 n-step truncated   -return methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242

12.4 redoing updates: the online   -return algorithm . . . . . . . . . . . . . . . . . . . . . 243

12.5 true online td(  ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245

12.6 dutch traces in monte carlo learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247

12.7 sarsa(  ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248

12.8 variable    and    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253

12.9 o   -policy eligibility traces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254

12.10 watkins   s q(  ) to tree-backup(  ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

12.11 stable o   -policy methods with traces

. . . . . . . . . . . . . . . . . . . . . . . . . . . 259

12.12 implementation issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260

12.13 conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261

13 id189

265

13.1 policy approximation and its advantages . . . . . . . . . . . . . . . . . . . . . . . . . . 265

13.2 the policy gradient theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268

13.3 reinforce: monte carlo policy gradient . . . . . . . . . . . . . . . . . . . . . . . . . 270

13.4 reinforce with baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272

contents

vii

13.5 actor   critic methods

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273

13.6 policy gradient for continuing problems

. . . . . . . . . . . . . . . . . . . . . . . . . . 275

13.7 policy parameterization for continuous actions . . . . . . . . . . . . . . . . . . . . . . . 277

13.8 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

iii looking deeper

14 psychology

281

283

14.1 prediction and control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284

14.2 classical conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285

14.2.1 blocking and higher-order conditioning . . . . . . . . . . . . . . . . . . . . . . . 286

14.2.2 the rescorla   wagner model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287

14.2.3 the td model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289

14.2.4 td model simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290

14.3 instrumental conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297

14.4 delayed reinforcement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301

14.5 cognitive maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302

14.6 habitual and goal-directed behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303

14.7 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306

15 neuroscience

313

15.1 neuroscience basics

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314

15.2 reward signals, reinforcement signals, values, and prediction errors

. . . . . . . . . . 315

15.3 the reward prediction error hypothesis

. . . . . . . . . . . . . . . . . . . . . . . . . . 316

15.4 dopamine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318

15.5 experimental support for the reward prediction error hypothesis . . . . . . . . . . . . 321

15.6 td error/dopamine correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323

15.7 neural actor   critic

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328

15.8 actor and critic learning rules

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330

15.9 hedonistic neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334

15.10 collective id23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335

15.11 model-based methods in the brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337

15.12 addiction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339

15.13 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340

16 applications and case studies

349

16.1 td-gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349

16.2 samuel   s checkers player

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353

16.3 watson   s daily-double wagering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355

16.4 optimizing memory control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358

16.5 human-level video game play . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361

viii

contents

16.6 mastering the game of go . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365

16.6.1 alphago . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367

16.6.2 alphago zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370

16.7 personalized web services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372

16.8 thermal soaring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375

17 frontiers

379

17.1 general value functions and auxiliary tasks . . . . . . . . . . . . . . . . . . . . . . . . 379

17.2 temporal abstraction via options

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380

17.3 observations and state

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383

17.4 designing reward signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386

17.5 remaining issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388

17.6 id23 and the future of arti   cial intelligence . . . . . . . . . . . . . . 390

references

395

preface to the first edition

we    rst came to focus on what is now known as id23 in late 1979. we were both
at the university of massachusetts, working on one of the earliest projects to revive the idea that
networks of neuronlike adaptive elements might prove to be a promising approach to arti   cial adaptive
intelligence. the project explored the    heterostatic theory of adaptive systems    developed by a. harry
klopf. harry   s work was a rich source of ideas, and we were permitted to explore them critically and
compare them with the long history of prior work in adaptive systems. our task became one of teasing
the ideas apart and understanding their relationships and relative importance. this continues today,
but in 1979 we came to realize that perhaps the simplest of the ideas, which had long been taken for
granted, had received surprisingly little attention from a computational perspective. this was simply
the idea of a learning system that wants something, that adapts its behavior in order to maximize a
special signal from its environment. this was the idea of a    hedonistic    learning system, or, as we
would say now, the idea of id23.

like others, we had a sense that id23 had been thoroughly explored in the early
days of cybernetics and arti   cial intelligence. on closer inspection, though, we found that it had
been explored only slightly. while id23 had clearly motivated some of the earliest
computational studies of learning, most of these researchers had gone on to other things, such as
pattern classi   cation, supervised learning, and adaptive control, or they had abandoned the study of
learning altogether. as a result, the special issues involved in learning how to get something from the
environment received relatively little attention.
in retrospect, focusing on this idea was the critical
step that set this branch of research in motion. little progress could be made in the computational
study of id23 until it was recognized that such a fundamental idea had not yet been
thoroughly explored.

the    eld has come a long way since then, evolving and maturing in several directions. reinforcement
learning has gradually become one of the most active research areas in machine learning, arti   cial
intelligence, and neural network research. the    eld has developed strong mathematical foundations and
impressive applications. the computational study of id23 is now a large    eld, with
hundreds of active researchers around the world in diverse disciplines such as psychology, control theory,
arti   cial intelligence, and neuroscience. particularly important have been the contributions establishing
and developing the relationships to the theory of optimal control and id145. the
overall problem of learning from interaction to achieve goals is still far from being solved, but our
understanding of it has improved signi   cantly. we can now place component ideas, such as temporal-
di   erence learning, id145, and function approximation, within a coherent perspective
with respect to the overall problem.

our goal in writing this book was to provide a clear and simple account of the key ideas and algorithms
of id23. we wanted our treatment to be accessible to readers in all of the related
disciplines, but we could not cover all of these perspectives in detail. for the most part, our treatment
takes the point of view of arti   cial intelligence and engineering. coverage of connections to other    elds
we leave to others or to another time. we also chose not to produce a rigorous formal treatment of

ix

x

preface to the first edition

id23. we did not reach for the highest possible level of mathematical abstraction
and did not rely on a theorem   proof format. we tried to choose a level of mathematical detail that
points the mathematically inclined in the right directions without distracting from the simplicity and
potential generality of the underlying ideas.

[three paragraphs elided in favor of updated content in the second edition.]

in some sense we have been working toward this book for thirty years, and we have lots of people
to thank. first, we thank those who have personally helped us develop the overall view presented
in this book: harry klopf, for helping us recognize that id23 needed to be revived;
chris watkins, dimitri bertsekas, john tsitsiklis, and paul werbos, for helping us see the value of the
relationships to id145; john moore and jim kehoe, for insights and inspirations from
animal learning theory; oliver selfridge, for emphasizing the breadth and importance of adaptation;
and, more generally, our colleagues and students who have contributed in countless ways: ron williams,
charles anderson, satinder singh, sridhar mahadevan, steve bradtke, bob crites, peter dayan, and
leemon baird. our view of id23 has been signi   cantly enriched by discussions with
paul cohen, paul utgo   , martha steenstrup, gerry tesauro, mike jordan, leslie kaelbling, andrew
moore, chris atkeson, tom mitchell, nils nilsson, stuart russell, tom dietterich, tom dean, and bob
narendra. we thank michael littman, gerry tesauro, bob crites, satinder singh, and wei zhang
for providing speci   cs of sections 4.7, 15.1, 15.4, 15.5, and 15.6 respectively. we thank the air force
o   ce of scienti   c research, the national science foundation, and gte laboratories for their long and
farsighted support.

we also wish to thank the many people who have read drafts of this book and provided valuable
comments, including tom kalt, john tsitsiklis, pawel cichosz, olle g  allmo, chuck anderson, stu-
art russell, ben van roy, paul steenstrup, paul cohen, sridhar mahadevan, jette randlov, brian
sheppard, thomas o   connell, richard coggins, cristina versino, john h. hiett, andreas badelt, jay
ponte, joe beck, justus piater, martha steenstrup, satinder singh, tommi jaakkola, dimitri bert-
sekas, torbj  orn ekman, christina bj  orkman, jakob carlstr  om, and olle palmgren. finally, we thank
gwyn mitchell for helping in many ways, and harry stanton and bob prior for being our champions
at mit press.

preface to the second edition

the twenty years since the publication of the    rst edition of this book have seen tremendous progress
in arti   cial intelligence, propelled in large part by advances in machine learning, including advances
in id23. although the impressive computational power that became available is
responsible for some of these advances, new developments in theory and algorithms have been driving
forces as well. in the face of this progress, a second edition of our 1998 book was long overdue, and
we    nally began the project in 2013. our goal for the second edition was the same as our goal for the
   rst: to provide a clear and simple account of the key ideas and algorithms of id23
that is accessible to readers in all the related disciplines. the edition remains an introduction, and we
retain a focus on core, on-line learning algorithms. this edition includes some new topics that rose to
importance over the intervening years, and we expanded coverage of topics that we now understand
better. but we made no attempt to provide comprehensive coverage of the    eld, which has exploded in
many di   erent directions with outstanding contributions by many active researchers. we apologize for
having to leave out all but a handful of these contributions.

as in the    rst edition, we chose not to produce a rigorous formal treatment of id23,
or to formulate it in the most general terms. however, since the    rst edition, our deeper understanding
of some topics required a bit more mathematics to explain; we have set o    the more mathematical
parts in shaded boxes that the non-mathematically-inclined may choose to skip. we also use a slightly
di   erent notation than was used in the    rst edition. in teaching, we have found that the new notation
helps to address some common points of confusion.
it emphasizes the di   erence between random
variables, denoted with capital letters, and their instantiations, denoted in lower case. for example,
the state, action, and reward at time step t are denoted st, at, and rt, while their possible values
might be denoted s, a, and r. along with this, it is natural to use lower case for value functions (e.g.,
v  ) and restrict capitals to their tabular estimates (e.g., qt(s, a)). approximate value functions are
deterministic functions of random parameters and are thus also in lower case (e.g.,   v(s,wt)     v  (s)).
vectors, such as the weight vector wt (formerly   t) and the feature vector xt (formerly   t), are bold
and written in lowercase even if they are random variables. uppercase bold is reserved for matrices. in
the    rst edition we used special notations, pa
ss(cid:48), for the transition probabilities and expected
rewards. one weakness of that notation is that it still did not fully characterize the dynamics of the
rewards, giving only their expectations. another weakness is the excess of subscripts and superscripts.
in this edition we use the explicit notation of p(s(cid:48), r|s, a) for the joint id203 for the next state and
reward given the current state and action. all the changes in notation are summarized in a table on
page xv.

ss(cid:48) and ra

the second edition is signi   cantly expanded, and its top-level organization has been revamped.
after the introductory    rst chapter, the second edition is divided into three new parts. the    rst part
(chapters 2   8) treats as much of id23 as possible without going beyond the tabular
case for which exact solutions can be found. we cover both learning and planning methods for the
tabular case, as well as their uni   cation in n-step methods and in dyna. many algorithms presented in
this part are new to the second edition, including ucb, expected sarsa, double learning, tree-backup,
q(  ), rtdp, and mcts. doing the tabular case    rst, and thoroughly, enables core ideas to be developed

xi

xii

preface to the second edition

in the simplest possible setting. the whole second part of the book (chapters 9   13) is then devoted to
extending the ideas to function approximation. it has new sections on arti   cial neural networks, the
fourier basis, lstd, kernel-based methods, gradient-td and emphatic-td methods, average-reward
methods, true online td(  ), and policy-gradient methods. the second edition signi   cantly expands
the treatment of o   -policy learning,    rst for the tabular case in chapters 5   7, then with function
approximation in chapters 11 and 12. another change is that the second edition separates the forward-
view idea of n-step id64 (now treated more fully in chapter 7) from the backward-view idea
of eligibility traces (now treated independently in chapter 12). the third part of the book has large
new chapters on id23   s relationships to psychology (chapter 14) and neuroscience
(chapter 15), as well as an updated case-studies chapter including atari game playing, watson, and
alphago (chapter 16). still, out of necessity we have included only a small subset of all that has been
done in the    eld. our choices re   ect our long-standing interests in inexpensive model-free methods
that should scale well to large applications. the    nal chapter now includes a discussion of the future
societal impacts of id23. for better or worse, the second edition is about 60% longer
than the    rst.

this book is designed to be used as the primary text for a one- or two-semester course on rein-
forcement learning. for a one-semester course, the    rst ten chapters should be covered in order and
form a good core, to which can be added material from the other chapters, from other books such
as bertsekas and tsitsiklis (1996), weiring and van otterlo (2012), and szepesv  ari (2010), or from
the literature, according to taste. depending of the students    background, some additional material
on online supervised learning may be helpful. the ideas of options and option models are a natural
addition (sutton, precup and singh, 1999). a two-semester course can cover all the chapters as well
as supplementary material. the book can also be used as part of broader courses on machine learning,
arti   cial intelligence, or neural networks. in this case, it may be desirable to cover only a subset of
the material. we recommend covering chapter 1 for a brief overview, chapter 2 through section 2.4,
chapter 3, and then selecting sections from the remaining chapters according to time and interests.
chapter 6 is the most important for the subject and for the rest of the book. a course focusing on
machine learning or neural networks should cover chapters 9 and 10, and a course focusing on arti   cial
intelligence or planning should cover chapter 8. throughout the book, sections and chapters that are
more di   cult and not essential to the rest of the book are marked with a    . these can be omitted on
   rst reading without creating problems later on. some exercises are also marked with a     to indicate
that they are more advanced and not essential to understanding the basic material of the chapter.

most chapters end with a section entitled    bibliographical and historical remarks,    wherein we credit
the sources of the ideas presented in that chapter, provide pointers to further reading and ongoing
research, and describe relevant historical background. despite our attempts to make these sections
authoritative and complete, we have undoubtedly left out some important prior work. for that we again
apologize, and we welcome corrections and extensions for incorporation into the electronic version of
the book.

like the    rst edition, this edition of the book is dedicated to the memory of a. harry klopf. it was
harry who introduced us to each other, and it was his ideas about the brain and arti   cial intelligence
that launched our long excursion into id23. trained in neurophysiology and long
interested in machine intelligence, harry was a senior scientist a   liated with the avionics directorate
of the air force o   ce of scienti   c research (afosr) at wright-patterson air force base, ohio. he was
dissatis   ed with the great importance attributed to equilibrium-seeking processes, including homeostasis
and error-correcting pattern classi   cation methods, in explaining natural intelligence and in providing
a basis for machine intelligence. he noted that systems that try to maximize something (whatever that
might be) are qualitatively di   erent from equilibrium-seeking systems, and he argued that maximizing
systems hold the key to understanding important aspects of natural intelligence and for building arti   cial
intelligences. harry was instrumental in obtaining funding from afosr for a project to assess the
scienti   c merit of these and related ideas. this project was conducted in the late 1970s at the university

preface to the second edition

xiii

of massachusetts amherst (umass amherst), initially under the direction of michael arbib, william
kilmer, and nico spinelli, professors in the department of computer and information science at umass
amherst, and founding members of the cybernetics center for systems neuroscience at the university,
a farsighted group focusing on the intersection of neuroscience and arti   cial intelligence. barto, a
recent ph.d. from the university of michigan, was hired as post doctoral researcher on the project.
meanwhile, sutton, an undergraduate studying computer science and psychology at stanford, had
been corresponding with harry regarding their mutual interest in the role of stimulus timing in classical
conditioning. harry suggested to the umass group that sutton would be a great addition to the project.
thus, sutton became a umass graduate student, whose ph.d. was directed by barto, who had become
an associate professor. the study of id23 as presented in this book is rightfully an
outcome of that project instigated by harry and inspired by his ideas. further, harry was responsible
for bringing us, the authors, together in what has been a long and enjoyable interaction. by dedicating
this book to harry we honor his essential contributions, not only to the    eld of id23,
but also to our collaboration. we also thank professors arbib, kilmer, and spinelli for the opportunity
they provided to us to begin exploring these ideas. finally, we thank afosr for generous support over
the early years of our research, and the nsf for its generous support over many of the following years.
we have very many people to thank for their inspiration and help with this second edition. everyone
we acknowledged for their inspiration and help with the    rst edition deserve our deepest gratitude for
this edition as well, which would not exist were it not for their contributions to edition number one.
to that long list we must add many others who contributed speci   cally to the second edition. our
students over the many years that we have taught this material contributed in countless ways: exposing
errors, o   ering    xes, and   not the least   being confused in places where we could have explained things
better. we thank many readers on the internet for    nding errors and potential points of confusion in
the second edition, and speci   cally martha steenstrup for reading and providing detailed comments
throughout. the chapters on psychology and neuroscience could not have been written without the
help of many experts in those    elds. we thank john moore for his patient tutoring over many many
years on animal learning experiments, theory, and neuroscience, and for his careful reading of multiple
drafts of chapters 14 and 15. we also thank matt botvinick, nathaniel daw, peter dayan, and yael
niv for their penetrating comments on drafts of these chapter, their essential guidance through the
massive literature, and their interception of many of our errors in early drafts. of course, the remaining
errors in these chapters   and there must still be some   are totally our own. we owe phil thomas
thanks for helping us make these chapters accessible to non-psychologists and non-neuroscientists. we
thank jim houk for introducing us to the subject of information processing in the basal ganglia. jos  e
mart    nez, terry sejnowski, david silver, gerry tesauro, georgios theocharous, and phil thomas
generously helped us understand details of their id23 applications for inclusion in the
case-studies chapter and commented on drafts of these sections. special thanks are owed to david silver
for helping us better understand id169 and the deepmind go-playing programs.
we thank george konidaris for his help with the section on the fourier basis. emilio cartoni, stefan
dernbach, clemens rosenbaum, and patrick taylor helped us in a number important ways for which
we are most grateful.

sutton would also like to thank the members of the id23 and arti   cial intelligence
laboratory at the university of alberta for contributions to the second edition. he owes a particular
debt to rupam mahmood for essential contributions to the treatment of o   -policy monte carlo methods
in chapter 5, to hamid maei for helping develop the perspective on o   -policy learning presented in
chapter 11, to eric graves for conducting the experiments in chapter 13, to shantong zhang for
replicating and thus verifying almost all the experimental results, to kris de asis for improving the
new technical content of chapter 12, to harm van seijen for insights that led to the separation of
n-step methods from eligibility traces and, along with hado van hasselt, for the ideas involving exact
equivalence of forward and backward views of eligibility traces presented in chapter 12. sutton would
also like to gratefully acknowledge the support and freedom he was granted by the government of

xiv

preface to the second edition

alberta and the national science and engineering research council of canada throughout the period
during which the second edition was conceived and written. in particular, he would like to thank randy
goebel for creating a supportive and far-sighted environment for research in alberta.

summary of notation

capital letters are used for random variables, whereas lower case letters are used for the values of
random variables and for scalar functions. quantities that are required to be real-valued vectors are
written in bold and in lower case (even if random variables). matrices are bold capitals.
.
=
   
pr{x = x}
x     p
e[x]
argmaxa f (a) a value of a at which f (a) takes its maximal value
ln x
exp(x)
r

equality relationship that is true by de   nition
approximately equal
id203 that a random variable x takes on the value x
.
= pr{x = x}
the random variable x is selected with distribution p(x)

expectation of a random variable x, i.e., e[x] =(cid:80)x p(x)x

natural logarithm of x
ex, where e     2.71828 is the base of the natural logarithm
set of real numbers

  
  ,   
  
  
1predicate

id203 of taking a random action in an   -greedy policy
step-size parameters
discount-rate parameter
decay-rate parameter for eligibility traces
an indicator function (1 is the predicate is true, else 0)

in a multi-arm bandit problem:
k
t
q   (a)
qt(a)
nt(a)
ht(a)
  t(a)
  rt

number of actions (arms)
discrete time step or play number
true value (expected reward) of action a
estimate at time t of q   (a)
number of times action a has been selected up prior to time t
learned preference for selecting action a
id203 of selecting action a on time t
estimate at time t of the expected reward given   

in a markov decision process:
s, s(cid:48)
a
r
s
s+
a
r
   
   

states
an action
a reward
set of all nonterminal states
set of all states, including the terminal state
set of all actions
set of all possible rewards, a    nite subset of r
subset of, e.g., r     r
is an element of, e.g., s     s, r     r

xv

xvi

|s|
t
t, t (t)
at
st
rt
  
  (s)
  (a|s)
  (a|s,   )
gt
  gt:h
g  s
t
g  a
t
g  s
t:h
g  a
t:h

summary of notation

number of elements in set s

discrete time step
   nal time step of an episode, or of the episode including time step t
action at time t
state at time t, typically due, stochastically, to st   1 and at   1
reward at time t, typically due, stochastically, to st   1 and at   1
policy, decision-making rule
action taken in state s under deterministic policy   
id203 of taking action a in state s under stochastic policy   
id203 of taking action a in state s given parameter   

return (cumulative discounted reward) following time t (section 3.3)
   at return (uncorrected, undiscounted) from t + 1 to h (section 5.8)
  -return, corrected by estimated state values (section 12.1)
  -return, corrected by estimated action values (section 12.1)
truncated, corrected   -return, with state values (section 12.3)
truncated, corrected   -return, with action values (section 12.3)

p(s(cid:48), r|s, a)
p(s(cid:48)|s, a)
r(s, a, s(cid:48))

id203 of transition to state s(cid:48) with reward r, from state s and action a
id203 of transition to state s(cid:48), from state s taking action a
expected immediate reward on transition from s to s(cid:48) under action a

v  (s)
v   (s)
q  (s, a)
q   (s, a)
v, vt
q, qt

value of state s under policy    (expected return)
value of state s under the optimal policy
value of taking action a in state s under policy   
value of taking action a in state s under the optimal policy
array estimates of state-value function v   or v   
array estimates of action-value function q   or q   

temporal-di   erence error at t (a random variable) (section 6.1)
d-vector of weights underlying an approximate value function
ith component of learnable weight vector
dimensionality   the number of components of w
alternate dimensionality   the number of components of   
number of 1s in a sparse binary feature vector
approximate value of state s given weight vector w
alternate notation for   v(s,w)
approximate value of state   action pair s, a given weight vector w
vector of features visible when in state s
vector of features visible when in state s taking action a

  t
w, wt
wi, wt,i
d
d(cid:48)
m
  v(s,w)
vw(s)
  q(s, a, w)
x(s)
x(s, a)
xi(s), xi(s, a) ith component of vector x(s) or x(s, a)
xt
w(cid:62)x
  (s)
  
(cid:107)x(cid:107)2
v, vt
zt

shorthand for x(st) or x(st, at)
.
inner product of vectors, w(cid:62)x
onpolicy distribution over states (section 9.2)
|s|-vector of the   (s)

=(cid:80)i wixi; e.g.,   v(s,w)
  -weighted norm of any vector x(s), i.e.,(cid:80)i   (s)x(s)2

secondary d-vector of weights, used to learn w (chapter 11)
d-vector of eligibility traces at time t (chapter 12)

.
= w(cid:62)x(s)

  

i (section 11.4)

summary of notation

xvii

  ,   t
    
j(  ), j(  )
h(s, a,   )

parameter vector of target policy (chapter 13)
policy corresponding to parameter   
performance measure for policy    or     
a preference for selecting action a in state s based on   

b

behavior policy selecting actions while learning about target policy   ,

  t:h
  t
r(  )
  rt

a
b
wtd
i
p
d
x

or a baseline function b : s (cid:55)    r for policy-gradient methods
or a branching factor

importance sampling ratio for time t to time h (section 5.5)
importance sampling ratio for time t alone,   t =   t:t
average reward (reward rate) for policy    (section 10.3)
estimate of r(  ) at time t

.

.
= e[rt+1xt]

= e(cid:104)xt(cid:0)xt       xt+1(cid:1)(cid:62)(cid:105) (section 11.4)

d    d matrix a
d-dimensional vector b
.
= a   1b (d-vector)
td    xed point, wtd
identity matrix
|s|    |s| matrix of state-transition probabilities under   
|s|    |s| diagonal matrix with   (s) on its diagonal
|s|    d matrix with x(s) as its rows

xviii

summary of notation

chapter 1

introduction

the idea that we learn by interacting with our environment is probably the    rst to occur to us when
we think about the nature of learning. when an infant plays, waves its arms, or looks about, it has no
explicit teacher, but it does have a direct sensorimotor connection to its environment. exercising this
connection produces a wealth of information about cause and e   ect, about the consequences of actions,
and about what to do in order to achieve goals. throughout our lives, such interactions are undoubtedly
a major source of knowledge about our environment and ourselves. whether we are learning to drive a
car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and
we seek to in   uence what happens through our behavior. learning from interaction is a foundational
idea underlying nearly all theories of learning and intelligence.

in this book we explore a computational approach to learning from interaction. rather than directly
theorizing about how people or animals learn, we explore idealized learning situations and evaluate the
e   ectiveness of various learning methods. that is, we adopt the perspective of an arti   cial intelligence
researcher or engineer. we explore designs for machines that are e   ective in solving learning problems of
scienti   c or economic interest, evaluating the designs through mathematical analysis or computational
experiments. the approach we explore, called id23, is much more focused on goal-
directed learning from interaction than are other approaches to machine learning.

1.1 id23

id23 is learning what to do   how to map situations to actions   so as to maximize
a numerical reward signal. the learner is not told which actions to take, but instead must discover
which actions yield the most reward by trying them. in the most interesting and challenging cases,
actions may a   ect not only the immediate reward but also the next situation and, through that, all
subsequent rewards. these two characteristics   trial-and-error search and delayed reward   are the two
most important distinguishing features of id23.

id23, like many topics whose names end with    ing,    such as machine learning
and mountaineering, is simultaneously a problem, a class of solution methods that work well on the
problem, and the    eld that studies this problems and its solution methods. it is convenient to use a
single name for all three things, but at the same time essential to keep the three conceptually separate.
in particular, the distinction between problems and solution methods is very important in reinforcement
learning; failing to make this distinction is the source of a many confusions.

we formalize the problem of id23 using ideas from dynamical systems theory,
speci   cally, as the optimal control of incompletely-known id100. the details of this

1

2

chapter 1.

introduction

formalization must wait until chapter 3, but the basic idea is simply to capture the most important
aspects of the real problem facing a learning agent interacting over time with its environment to achieve
a goal. a learning agent must be able to sense the state of its environment to some extent and must be
able to take actions that a   ect the state. the agent also must have a goal or goals relating to the state of
the environment. id100 are intended to include just these three aspects   sensation,
action, and goal   in their simplest possible forms without trivializing any of them. any method that
is well suited to solving such problems we consider to be a id23 method.

id23 is di   erent from supervised learning, the kind of learning studied in most
current research in the    eld of machine learning. supervised learning is learning from a training set
of labeled examples provided by a knowledgable external supervisor. each example is a description of
a situation together with a speci   cation   the label   of the correct action the system should take to
that situation, which is often to identify a category to which the situation belongs. the object of this
kind of learning is for the system to extrapolate, or generalize, its responses so that it acts correctly
in situations not present in the training set. this is an important kind of learning, but alone it is
not adequate for learning from interaction. in interactive problems it is often impractical to obtain
examples of desired behavior that are both correct and representative of all the situations in which the
agent has to act. in uncharted territory   where one would expect learning to be most bene   cial   an
agent must be able to learn from its own experience.

id23 is also di   erent from what machine learning researchers call unsupervised
learning, which is typically about    nding structure hidden in collections of unlabeled data. the terms
supervised learning and unsupervised learning would seem to exhaustively classify machine learning
paradigms, but they do not. although one might be tempted to think of id23 as a
kind of unsupervised learning because it does not rely on examples of correct behavior, reinforcement
learning is trying to maximize a reward signal instead of trying to    nd hidden structure. uncovering
structure in an agent   s experience can certainly be useful in id23, but by itself does
not address the id23 problem of maximizing a reward signal. we therefore consider
id23 to be a third machine learning paradigm, alongside supervised learning and
unsupervised learning and perhaps other paradigms as well.

one of the challenges that arise in id23, and not in other kinds of learning, is the
trade-o    between exploration and exploitation. to obtain a lot of reward, a id23
agent must prefer actions that it has tried in the past and found to be e   ective in producing reward.
but to discover such actions, it has to try actions that it has not selected before. the agent has to
exploit what it has already experienced in order to obtain reward, but it also has to explore in order to
make better action selections in the future. the dilemma is that neither exploration nor exploitation
can be pursued exclusively without failing at the task. the agent must try a variety of actions and
progressively favor those that appear to be best. on a stochastic task, each action must be tried many
times to gain a reliable estimate of its expected reward. the exploration   exploitation dilemma has been
intensively studied by mathematicians for many decades, yet remains unresolved. for now, we simply
note that the entire issue of balancing exploration and exploitation does not even arise in supervised
and unsupervised learning, at least in their purest forms.

another key feature of id23 is that it explicitly considers the whole problem of a
goal-directed agent interacting with an uncertain environment. this is in contrast to many approaches
that consider subproblems without addressing how they might    t into a larger picture. for example, we
have mentioned that much of machine learning research is concerned with supervised learning without
explicitly specifying how such an ability would    nally be useful. other researchers have developed
theories of planning with general goals, but without considering planning   s role in real-time decision
making, or the question of where the predictive models necessary for planning would come from. al-
though these approaches have yielded many useful results, their focus on isolated subproblems is a
signi   cant limitation.

1.1. id23

3

id23 takes the opposite tack, starting with a complete, interactive, goal-seeking
agent. all id23 agents have explicit goals, can sense aspects of their environments,
and can choose actions to in   uence their environments. moreover, it is usually assumed from the
beginning that the agent has to operate despite signi   cant uncertainty about the environment it faces.
when id23 involves planning, it has to address the interplay between planning and
real-time action selection, as well as the question of how environment models are acquired and improved.
when id23 involves supervised learning, it does so for speci   c reasons that determine
which capabilities are critical and which are not. for learning research to make progress, important
subproblems have to be isolated and studied, but they should be subproblems that play clear roles in
complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be
   lled in.

by a complete, interactive, goal-seeking agent we do not always mean something like a complete
organism or robot. these are clearly examples, but a complete, interactive, goal-seeking agent can also
be a component of a larger behaving system. in this case, the agent directly interacts with the rest of
the larger system and indirectly interacts with the larger system   s environment. a simple example is
an agent that monitors the charge level of robot   s battery and sends commands to the robot   s control
architecture. this agent   s environment is the rest of the robot together with the robot   s environment.
one must look beyond the most obvious examples of agents and their environments to appreciate the
generality of the id23 framework.

one of the most exciting aspects of modern id23 is its substantive and fruitful
interactions with other engineering and scienti   c disciplines. id23 is part of a decades-
long trend within arti   cial intelligence and machine learning toward greater integration with statistics,
optimization, and other mathematical subjects. for example, the ability of some id23
methods to learn with parameterized approximators addresses the classical    curse of dimensionality    in
operations research and control theory. more distinctively, id23 has also interacted
strongly with psychology and neuroscience, with substantial bene   ts going both ways. of all the forms
of machine learning, id23 is the closest to the kind of learning that humans and
other animals do, and many of the core algorithms of id23 were originally inspired by
biological learning systems. id23 has also given back, both through a psychological
model of animal learning that better matches some of the empirical data, and through an in   uential
model of parts of the brain   s reward system. the body of this book develops the ideas of reinforcement
learning that pertain to engineering and arti   cial intelligence, with connections to psychology and
neuroscience summarized in chapters 14 and 15.

finally, id23 is also part of a larger trend in arti   cial intelligence back toward
simple general principles. since the late 1960   s, many arti   cial intelligence researchers presumed that
there are no general principles to be discovered, that intelligence is instead due to the possession of
a vast number of special purpose tricks, procedures, and heuristics. it was sometimes said that if we
could just get enough relevant facts into a machine, say one million, or one billion, then it would become
intelligent. methods based on general principles, such as search or learning, were characterized as    weak
methods,    whereas those based on speci   c knowledge were called    strong methods.    this view is still
common today, but not dominant. from our point of view, it was simply premature: too little e   ort
had been put into the search for general principles to conclude that there were none. modern arti   cial
intelligence now includes much research looking for general principles of learning, search, and decision
making, as well as trying to incorporate vast amounts of domain knowledge. it is not clear how far
back the pendulum will swing, but id23 research is certainly part of the swing back
toward simpler and fewer general principles of arti   cial intelligence.

4

chapter 1.

introduction

1.2 examples

a good way to understand id23 is to consider some of the examples and possible
applications that have guided its development.

    a master chess player makes a move. the choice is informed both by planning   anticipating
possible replies and counterreplies   and by immediate, intuitive judgments of the desirability of
particular positions and moves.

    an adaptive controller adjusts parameters of a petroleum re   nery   s operation in real time. the
controller optimizes the yield/cost/quality trade-o    on the basis of speci   ed marginal costs without
sticking strictly to the set points originally suggested by engineers.

    a gazelle calf struggles to its feet minutes after being born. half an hour later it is running at 20

miles per hour.

    a mobile robot decides whether it should enter a new room in search of more trash to collect or
start trying to    nd its way back to its battery recharging station. it makes its decision based
on the current charge level of its battery and how quickly and easily it has been able to    nd the
recharger in the past.

    phil prepares his breakfast. closely examined, even this apparently mundane activity reveals a
complex web of conditional behavior and interlocking goal   subgoal relationships: walking to the
cupboard, opening it, selecting a cereal box, then reaching for, grasping, and retrieving the box.
other complex, tuned, interactive sequences of behavior are required to obtain a bowl, spoon,
and milk jug. each step involves a series of eye movements to obtain information and to guide
reaching and locomotion. rapid judgments are continually made about how to carry the objects
or whether it is better to ferry some of them to the dining table before obtaining others. each
step is guided by goals, such as grasping a spoon or getting to the refrigerator, and is in service
of other goals, such as having the spoon to eat with once the cereal is prepared and ultimately
obtaining nourishment. whether he is aware of it or not, phil is accessing information about the
state of his body that determines his nutritional needs, level of hunger, and food preferences.

these examples share features that are so basic that they are easy to overlook. all involve interaction
between an active decision-making agent and its environment, within which the agent seeks to achieve
a goal despite uncertainty about its environment. the agent   s actions are permitted to a   ect the future
state of the environment (e.g., the next chess position, the level of reservoirs of the re   nery, the robot   s
next location and the future charge level of its battery), thereby a   ecting the options and opportunities
available to the agent at later times. correct choice requires taking into account indirect, delayed
consequences of actions, and thus may require foresight or planning.

at the same time, in all these examples the e   ects of actions cannot be fully predicted; thus the
agent must monitor its environment frequently and react appropriately. for example, phil must watch
the milk he pours into his cereal bowl to keep it from over   owing. all these examples involve goals
that are explicit in the sense that the agent can judge progress toward its goal based on what it can
sense directly. the chess player knows whether or not he wins, the re   nery controller knows how much
petroleum is being produced, the mobile robot knows when its batteries run down, and phil knows
whether or not he is enjoying his breakfast.

in all of these examples the agent can use its experience to improve its performance over time. the
chess player re   nes the intuition he uses to evaluate positions, thereby improving his play; the gazelle
calf improves the e   ciency with which it can run; phil learns to streamline making his breakfast. the
knowledge the agent brings to the task at the start   either from previous experience with related tasks
or built into it by design or evolution   in   uences what is useful or easy to learn, but interaction with
the environment is essential for adjusting behavior to exploit speci   c features of the task.

1.3. elements of id23

5

1.3 elements of id23

beyond the agent and the environment, one can identify four main subelements of a reinforcement
learning system: a policy, a reward signal , a value function, and, optionally, a model of the environment.

a policy de   nes the learning agent   s way of behaving at a given time. roughly speaking, a policy
is a mapping from perceived states of the environment to actions to be taken when in those states.
it corresponds to what in psychology would be called a set of stimulus   response rules or associations.
in some cases the policy may be a simple function or lookup table, whereas in others it may involve
extensive computation such as a search process. the policy is the core of a id23 agent
in the sense that it alone is su   cient to determine behavior. in general, policies may be stochastic.

a reward signal de   nes the goal in a id23 problem. on each time step, the envi-
ronment sends to the id23 agent a single number called the reward. the agent   s sole
objective is to maximize the total reward it receives over the long run. the reward signal thus de   nes
what are the good and bad events for the agent. in a biological system, we might think of rewards as
analogous to the experiences of pleasure or pain. they are the immediate and de   ning features of the
problem faced by the agent. the reward signal is the primary basis for altering the policy; if an action
selected by the policy is followed by low reward, then the policy may be changed to select some other
action in that situation in the future. in general, reward signals may be stochastic functions of the state
of the environment and the actions taken.

whereas the reward signal indicates what is good in an immediate sense, a value function speci   es
what is good in the long run. roughly speaking, the value of a state is the total amount of reward an
agent can expect to accumulate over the future, starting from that state. whereas rewards determine
the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability
of states after taking into account the states that are likely to follow, and the rewards available in those
states. for example, a state might always yield a low immediate reward but still have a high value
because it is regularly followed by other states that yield high rewards. or the reverse could be true.
to make a human analogy, rewards are somewhat like pleasure (if high) and pain (if low), whereas
values correspond to a more re   ned and farsighted judgment of how pleased or displeased we are that
our environment is in a particular state. expressed this way, we hope it is clear that value functions
formalize a basic and familiar idea.

rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. without
rewards there could be no values, and the only purpose of estimating values is to achieve more reward.
nevertheless, it is values with which we are most concerned when making and evaluating decisions.
action choices are made based on value judgments. we seek actions that bring about states of highest
value, not highest reward, because these actions obtain the greatest amount of reward for us over the
long run. unfortunately, it is much harder to determine values than it is to determine rewards. rewards
are basically given directly by the environment, but values must be estimated and re-estimated from the
sequences of observations an agent makes over its entire lifetime. in fact, the most important component
of almost all id23 algorithms we consider is a method for e   ciently estimating values.
the central role of value estimation is arguably the most important thing we have learned about
id23 over the last few decades.

the fourth and    nal element of some id23 systems is a model of the environment.
this is something that mimics the behavior of the environment, or more generally, that allows id136s
to be made about how the environment will behave. for example, given a state and action, the model
might predict the resultant next state and next reward. models are used for planning, by which we
mean any way of deciding on a course of action by considering possible future situations before they are
actually experienced. methods for solving id23 problems that use models and planning
are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-
error learners   viewed as almost the opposite of planning.
in chapter 8 we explore reinforcement

6

chapter 1.

introduction

learning systems that simultaneously learn by trial and error, learn a model of the environment, and
use the model for planning. modern id23 spans the spectrum from low-level, trial-
and-error learning to high-level, deliberative planning.

1.4 limitations and scope

from the preceding discussion, it should be clear that id23 relies heavily on the
concept of state   as input to the policy and value function, and as both input to and output from the
model. informally, we can think of the state as a signal conveying to the agent some sense of    how the
environment is    at a particular time. the formal de   nition of state as we use it here is given by the
framework of id100 presented in chapter 3. more generally, however, we encourage
the reader to follow the informal meaning and think of the state as whatever information is available
to the agent about its environment. in e   ect, we assume that the state signal is produced by some
preprocessing system that is nominally part of the agent   s environment. we do not address the issues
of constructing, changing, or learning the state signal in this book. we take this approach not because
we consider state representation to be unimportant, but in order to focus fully on the decision-making
issues. in other words, our main concern is not with designing the state signal, but with deciding what
action to take as a function of whatever state signal is available. (we do touch brie   y on state design
and construction in the last chapter in section 17.3.)

most of the id23 methods we consider in this book are structured around estimating
value functions, but it is not strictly necessary to do this to solve id23 problems.
for example, methods such as id107, genetic programming, simulated annealing, and
other optimization methods have been used to approach id23 problems without ever
appealing to value functions. these methods evaluate the    lifetime    behavior of many non-learning
agents, each using a di   erent policy for interacting with its environment, and select those that are able
to obtain the most reward. we call these evolutionary methods because their operation is analogous
to the way biological evolution produces organisms with skilled behavior even when they do not learn
during their individual lifetimes. if the space of policies is su   ciently small, or can be structured so
that good policies are common or easy to    nd   or if a lot of time is available for the search   then
evolutionary methods can be e   ective. in addition, evolutionary methods have advantages on problems
in which the learning agent cannot sense the complete state of its environment.

our focus is on id23 methods that learn while interacting with the environment,
which evolutionary methods do not do. methods able to take advantage of the details of individual
behavioral interactions can be much more e   cient than evolutionary methods in many cases. evolu-
tionary methods ignore much of the useful structure of the id23 problem: they do
not use the fact that the policy they are searching for is a function from states to actions; they do
not notice which states an individual passes through during its lifetime, or which actions it selects. in
some cases this information can be misleading (e.g., when states are misperceived), but more often it
should enable more e   cient search. although evolution and learning share many features and naturally
work together, we do not consider evolutionary methods by themselves to be especially well suited to
id23 problems and, accordingly, we do not cover them in this book.

however, we do include some methods that, like evolutionary methods, do not appeal to value
functions. these methods search in spaces of policies de   ned by a collection of numerical parameters.
they estimate the directions the parameters should be adjusted in order to most rapidly improve
a policy   s performance. unlike evolutionary methods, however, they produce these estimates while
the agent is interacting with its environment and so can take advantage of the details of individual
behavioral interactions. methods like this have proven useful in many problems, and some of the
simplest id23 methods fall into this category (see chapter 13). in the end, however,
the best methods of this type tend to include value functions in some form.

1.5. an extended example: tic-tac-toe

7

1.5 an extended example: tic-tac-toe

to illustrate the general idea of id23 and contrast it with other approaches, we next
consider a single example in more detail.

consider the familiar child   s game of tic-tac-toe. two players take turns
playing on a three-by-three board. one player plays xs and the other
os until one player wins by placing three marks in a row, horizontally,
vertically, or diagonally, as the x player has in the game shown to the
right. if the board    lls up with neither player getting three in a row, the
game is a draw. because a skilled player can play so as never to lose, let us
assume that we are playing against an imperfect player, one whose play is
sometimes incorrect and allows us to win. for the moment, in fact, let us
consider draws and losses to be equally bad for us. how might we construct
a player that will    nd the imperfections in its opponent   s play and learn to maximize its chances of
winning?

although this is a simple problem, it cannot readily be solved in a satisfactory way through classical
techniques. for example, the classical    minimax    solution from game theory is not correct here because
it assumes a particular way of playing by the opponent. for example, a minimax player would never
reach a game state from which it could lose, even if in fact it always won from that state because of
incorrect play by the opponent. classical optimization methods for sequential decision problems, such
as id145, can compute an optimal solution for any opponent, but require as input a
complete speci   cation of that opponent, including the probabilities with which the opponent makes each
move in each board state. let us assume that this information is not available a priori for this problem,
as it is not for the vast majority of problems of practical interest. on the other hand, such information
can be estimated from experience, in this case by playing many games against the opponent. about
the best one can do on this problem is    rst to learn a model of the opponent   s behavior, up to some
level of con   dence, and then apply id145 to compute an optimal solution given the
approximate opponent model.
in the end, this is not that di   erent from some of the reinforcement
learning methods we examine later in this book.

an evolutionary method applied to this problem would directly search the space of possible policies
for one with a high id203 of winning against the opponent. here, a policy is a rule that tells
the player what move to make for every state of the game   every possible con   guration of xs and
os on the three-by-three board. for each policy considered, an estimate of its winning id203
would be obtained by playing some number of games against the opponent. this evaluation would then
direct which policy or policies were considered next. a typical evolutionary method would hill-climb
in policy space, successively generating and evaluating policies in an attempt to obtain incremental
improvements. or, perhaps, a genetic-style algorithm could be used that would maintain and evaluate
a population of policies. literally hundreds of di   erent optimization methods could be applied.

here is how the tic-tac-toe problem would be approached with a method making use of a value
function. first we set up a table of numbers, one for each possible state of the game. each number will
be the latest estimate of the id203 of our winning from that state. we treat this estimate as the
state   s value, and the whole table is the learned value function. state a has higher value than state b,
or is considered    better    than state b, if the current estimate of the id203 of our winning from a
is higher than it is from b. assuming we always play xs, then for all states with three xs in a row the
id203 of winning is 1, because we have already won. similarly, for all states with three os in a
row, or that are       lled up,    the correct id203 is 0, as we cannot win from them. we set the initial
values of all the other states to 0.5, representing a guess that we have a 50% chance of winning.

we play many games against the opponent. to select our moves we examine the states that would
result from each of our possible moves (one for each blank space on the board) and look up their current

xxxooxo8

chapter 1.

introduction

values in the table. most of the time we move greedily, selecting the move that leads to the state with
greatest value, that is, with the highest estimated id203 of winning. occasionally, however, we
select randomly from among the other moves instead. these are called exploratory moves because
they cause us to experience states that we might otherwise never see. a sequence of moves made and
considered during a game can be diagrammed as in figure 1.1.

figure 1.1: a sequence of tic-tac-toe moves. the solid lines represent the moves taken during a game; the
dashed lines represent moves that we (our id23 player) considered but did not make. our
second move was an exploratory move, meaning that it was taken even though another sibling move, the one
leading to e   , was ranked higher. exploratory moves do not result in any learning, but each of our other moves
does, causing updates as suggested by the curved arrow in which estimated values are moved up the tree from
later nodes to earlier as detailed in the text.

while we are playing, we change the values of the states in which we    nd ourselves during the game.
we attempt to make them more accurate estimates of the probabilities of winning. to do this, we    back
up    the value of the state after each greedy move to the state before the move, as suggested by the
arrows in figure 1.1. more precisely, the current value of the earlier state is updated to be closer to
the value of the later state. this can be done by moving the earlier state   s value a fraction of the way
toward the value of the later state. if we let s denote the state before the greedy move, and s(cid:48) the state
after the move, then the update to the estimated value of s, denoted v (s), can be written as

v (s)     v (s) +   (cid:104)v (s(cid:48))     v (s)(cid:105),

where    is a small positive fraction called the step-size parameter, which in   uences the rate of learning.
this update rule is an example of a temporal-di   erence learning method, so called because its changes
are based on a di   erence, v (s(cid:48))     v (s), between estimates at two di   erent times.

the method described above performs quite well on this task. for example, if the step-size param-
eter is reduced properly over time, then this method converges, for any    xed opponent, to the true
probabilities of winning from each state given optimal play by our player. furthermore, the moves then
taken (except on exploratory moves) are in fact the optimal moves against this (imperfect) opponent.
in other words, the method converges to an optimal policy for playing the game against this opponent.

..   our move{opponent's move{our move{starting position         abc*dee*opponent's move{c   f   g*gopponent's move{our move{.   1.5. an extended example: tic-tac-toe

9

if the step-size parameter is not reduced all the way to zero over time, then this player also plays well
against opponents that slowly change their way of playing.

this example illustrates the di   erences between evolutionary methods and methods that learn value
functions. to evaluate a policy an evolutionary method holds the policy    xed and plays many games
against the opponent, or simulates many games using a model of the opponent. the frequency of wins
gives an unbiased estimate of the id203 of winning with that policy, and can be used to direct
the next policy selection. but each policy change is made only after many games, and only the    nal
outcome of each game is used: what happens during the games is ignored. for example, if the player
wins, then all of its behavior in the game is given credit, independently of how speci   c moves might have
been critical to the win. credit is even given to moves that never occurred! value function methods, in
contrast, allow individual states to be evaluated. in the end, evolutionary and value function methods
both search the space of policies, but learning a value function takes advantage of information available
during the course of play.

this simple example illustrates some of the key features of id23 methods. first,
there is the emphasis on learning while interacting with an environment, in this case with an opponent
player. second, there is a clear goal, and correct behavior requires planning or foresight that takes into
account delayed e   ects of one   s choices. for example, the simple id23 player would
learn to set up multi-move traps for a shortsighted opponent. it is a striking feature of the reinforcement
learning solution that it can achieve the e   ects of planning and lookahead without using a model of
the opponent and without conducting an explicit search over possible sequences of future states and
actions.

while this example illustrates some of the key features of id23, it is so simple that
it might give the impression that id23 is more limited than it really is. although tic-
tac-toe is a two-person game, id23 also applies in the case in which there is no external
adversary, that is, in the case of a    game against nature.    id23 also is not restricted
to problems in which behavior breaks down into separate episodes, like the separate games of tic-tac-toe,
with reward only at the end of each episode. it is just as applicable when behavior continues inde   nitely
and when rewards of various magnitudes can be received at any time. id23 is also
applicable to problems that do not even break down into discrete time steps, like the plays of tic-tac-
toe. the general principles apply to continuous-time problems as well, although the theory gets more
complicated and we omit it from this introductory treatment.

tic-tac-toe has a relatively small,    nite state set, whereas id23 can be used when
the state set is very large, or even in   nite. for example, gerry tesauro (1992, 1995) combined the
algorithm described above with an arti   cial neural network to learn to play backgammon, which has
approximately 1020 states. with this many states it is impossible ever to experience more than a small
fraction of them. tesauro   s program learned to play far better than any previous program, and now
plays at the level of the world   s best human players (see chapter 16). the neural network provides
the program with the ability to generalize from its experience, so that in new states it selects moves
based on information saved from similar states faced in the past, as determined by its network. how
well a id23 system can work in problems with such large state sets is intimately tied
to how appropriately it can generalize from past experience. it is in this role that we have the greatest
need for supervised learning methods with id23. neural networks and deep learning
(section 9.6) are not the only, or necessarily the best, way to do this.

in this tic-tac-toe example, learning started with no prior knowledge beyond the rules of the game,
but id23 by no means entails a tabula rasa view of learning and intelligence. on the
contrary, prior information can be incorporated into id23 in a variety of ways that
can be critical for e   cient learning. we also had access to the true state in the tic-tac-toe example,
whereas id23 can also be applied when part of the state is hidden, or when di   erent
states appear to the learner to be the same.

10

chapter 1.

introduction

finally, the tic-tac-toe player was able to look ahead and know the states that would result from each
of its possible moves. to do this, it had to have a model of the game that allowed it to    think about   
how its environment would change in response to moves that it might never make. many problems
are like this, but in others even a short-term model of the e   ects of actions is lacking. reinforcement
learning can be applied in either case. no model is required, but models can easily be used if they are
available or can be learned (chapter 8).

on the other hand, there are id23 methods that do not need any kind of environ-
ment model at all. model-free systems cannot even think about how their environments will change in
response to a single action. the tic-tac-toe player is model-free in this sense with respect to its oppo-
nent: it has no model of its opponent of any kind. because models have to be reasonably accurate to be
useful, model-free methods can have advantages over more complex methods when the real bottleneck in
solving a problem is the di   culty of constructing a su   ciently accurate environment model. model-free
methods are also important building blocks for model-based methods. in this book we devote several
chapters to model-free methods before we discuss how they can be used as components of more complex
model-based methods.

id23 can be used at both high and low levels in a system. although the tic-tac-
toe player learned only about the basic moves of the game, nothing prevents id23
from working at higher levels where each of the    actions    may itself be the application of a possibly
elaborate problem-solving method. in hierarchical learning systems, id23 can work
simultaneously on several levels.

exercise 1.1: self-play suppose, instead of playing against a random opponent, the reinforcement
learning algorithm described above played against itself, with both sides learning. what do you think
(cid:3)
would happen in this case? would it learn a di   erent policy for selecting moves?
exercise 1.2: symmetries many tic-tac-toe positions appear di   erent but are really the same because
of symmetries. how might we amend the learning process described above to take advantage of this?
in what ways would this change improve the learning process? now think again. suppose the opponent
did not take advantage of symmetries. in that case, should we? is it true, then, that symmetrically
(cid:3)
equivalent positions should necessarily have the same value?
exercise 1.3: greedy play suppose the id23 player was greedy, that is, it always
played the move that brought it to the position that it rated the best. might it learn to play better, or
(cid:3)
worse, than a nongreedy player? what problems might occur?
exercise 1.4: learning from exploration suppose learning updates occurred after all moves, including
exploratory moves. if the step-size parameter is appropriately reduced over time (but not the tendency
to explore), then the state values would converge to a set of probabilities. what are the two sets of
probabilities computed when we do, and when we do not, learn from exploratory moves? assuming
that we do continue to make exploratory moves, which set of probabilities might be better to learn?
(cid:3)
which would result in more wins?
exercise 1.5: other improvements can you think of other ways to improve the id23
(cid:3)
player? can you think of any better way to solve the tic-tac-toe problem as posed?

1.6 summary

id23 is a computational approach to understanding and automating goal-directed
learning and decision making. it is distinguished from other computational approaches by its emphasis
on learning by an agent from direct interaction with its environment, without relying on exemplary
supervision or complete models of the environment. in our opinion, id23 is the    rst
   eld to seriously address the computational issues that arise when learning from interaction with an

1.7. early history of id23

11

environment in order to achieve long-term goals.

id23 uses the formal framework of id100 to de   ne the inter-
action between a learning agent and its environment in terms of states, actions, and rewards. this
framework is intended to be a simple way of representing essential features of the arti   cial intelligence
problem. these features include a sense of cause and e   ect, a sense of uncertainty and nondeterminism,
and the existence of explicit goals.

the concepts of value and value functions are the key features of most of the id23
methods that we consider in this book. we take the position that value functions are important for
e   cient search in the space of policies. the use of value functions distinguishes id23
methods from evolutionary methods that search directly in policy space guided by scalar evaluations of
entire policies.

1.7 early history of id23

the early history of id23 has two main threads, both long and rich, that were pursued
independently before intertwining in modern id23. one thread concerns learning by
trial and error that started in the psychology of animal learning. this thread runs through some of
the earliest work in arti   cial intelligence and led to the revival of id23 in the early
1980s. the other thread concerns the problem of optimal control and its solution using value functions
and id145. for the most part, this thread did not involve learning. although the
two threads have been largely independent, the exceptions revolve around a third, less distinct thread
concerning temporal-di   erence methods such as the one used in the tic-tac-toe example in this chapter.
all three threads came together in the late 1980s to produce the modern    eld of id23
as we present it in this book.

the thread focusing on trial-and-error learning is the one with which we are most familiar and about
which we have the most to say in this brief history. before doing that, however, we brie   y discuss the
optimal control thread.

the term    optimal control    came into use in the late 1950s to describe the problem of designing a
controller to minimize a measure of a dynamical system   s behavior over time. one of the approaches
to this problem was developed in the mid-1950s by richard bellman and others through extending a
nineteenth century theory of hamilton and jacobi. this approach uses the concepts of a dynamical
system   s state and of a value function, or    optimal return function,    to de   ne a functional equation, now
often called the bellman equation. the class of methods for solving optimal control problems by solving
this equation came to be known as id145 (bellman, 1957a). bellman (1957b) also
introduced the discrete stochastic version of the optimal control problem known as markovian decision
processes (mdps), and ronald howard (1960) devised the policy iteration method for mdps. all of
these are essential elements underlying the theory and algorithms of modern id23.

id145 is widely considered the only feasible way of solving general stochastic optimal
control problems. it su   ers from what bellman called    the curse of dimensionality,    meaning that its
computational requirements grow exponentially with the number of state variables, but it is still far more
e   cient and more widely applicable than any other general method. id145 has been
extensively developed since the late 1950s, including extensions to partially observable mdps (surveyed
by lovejoy, 1991), many applications (surveyed by white, 1985, 1988, 1993), approximation methods
(surveyed by rust, 1996), and asynchronous methods (bertsekas, 1982, 1983). many excellent modern
treatments of id145 are available (e.g., bertsekas, 2005, 2012; puterman, 1994; ross,
1983; and whittle, 1982, 1983). bryson (1996) provides an authoritative history of optimal control.

connections between optimal control and id145, on the one hand, and learning, on
the other, were slow to be recognized. we cannot be sure about what accounted for this separation,

12

chapter 1.

introduction

but its main cause was likely the separation between the disciplines involved and their di   erent goals.
also contributing may have been the prevalent view of id145 as an o   -line computa-
tion depending essentially on accurate system models and analytic solutions to the bellman equation.
further, the simplest form of id145 is a computation that proceeds backwards in time,
making it di   cult to see how it could be involved in a learning process that must proceed in a forward
direction. some of the earliest work in id145, such as that by bellman and dreyfus
(1959) might now be classi   ed as following a learning approach. witten   s (1977) work (discussed be-
low) certainly quali   es as a combination of learning and dynamic-programming ideas. werbos (1987)
argued explicitly greater interrelation of id145 and learning methods and its relevance
to understanding neural and cognitive mechanisms. for us the full integration of id145
methods with on-line learning did not occur until the work of chris watkins in 1989, whose treatment
of id23 using the mdp formalism has been widely adopted (watkins, 1989). since
then these relationships have been extensively developed by many researchers, most particularly by
dimitri bertsekas and john tsitsiklis (1996), who coined the term    neuroid145    to
refer to the combination of id145 and neural networks. another term currently in
use is    approximate id145.    these various approaches emphasize di   erent aspects of
the subject, but they all share with id23 an interest in circumventing the classical
shortcomings of id145.

we would consider all of the work in optimal control also to be, in a sense, work in reinforcement learn-
ing. we de   ne a id23 method as any e   ective way of solving id23
problems, and it is now clear that these problems are closely related to optimal control problems, par-
ticularly stochastic optimal control problems such as those formulated as mdps. accordingly, we must
consider the solution methods of optimal control, such as id145, also to be reinforce-
ment learning methods. because almost all of the conventional methods require complete knowledge
of the system to be controlled, it feels a little unnatural to say that they are part of reinforcement
learning. on the other hand, many id145 algorithms are incremental and iterative.
like learning methods, they gradually reach the correct answer through successive approximations. as
we show in the rest of this book, these similarities are far more than super   cial. the theories and
solution methods for the cases of complete and incomplete knowledge are so closely related that we feel
they must be considered together as part of the same subject matter.

let us return now to the other major thread leading to the modern    eld of id23,
that centered on the idea of trial-and-error learning. we only touch on the major points of contact
here, taking up this topic in more detail in chapter 14. according to american psychologist r. s.
woodworth the idea of trial-and-error learning goes as far back as the 1850s to alexander bain   s
discussion of learning by    groping and experiment    and more explicitly to the british ethologist and
psychologist conway lloyd morgan   s 1894 use of the term to describe his observations of animal behavior
(woodworth, 1938). perhaps the    rst to succinctly express the essence of trial-and-error learning as a
principle of learning was edward thorndike:

of several responses made to the same situation, those which are accompanied or closely
followed by satisfaction to the animal will, other things being equal, be more    rmly connected
with the situation, so that, when it recurs, they will be more likely to recur; those which are
accompanied or closely followed by discomfort to the animal will, other things being equal,
have their connections with that situation weakened, so that, when it recurs, they will be
less likely to occur. the greater the satisfaction or discomfort, the greater the strengthening
or weakening of the bond. (thorndike, 1911, p. 244)

thorndike called this the    law of e   ect    because it describes the e   ect of reinforcing events on the
tendency to select actions. thorndike later modi   ed the law to better account for accumulating data
on animal learning (such as di   erences between the e   ects of reward and punishment), and the law in
its various forms has generated considerable controversy among learning theorists (e.g., see gallistel,

1.7. early history of id23

13

2005; herrnstein, 1970; kimble, 1961, 1967; mazur, 1994). despite this, the law of e   ect   in one form
or another   is widely regarded as a basic principle underlying much behavior (e.g., hilgard and bower,
1975; dennett, 1978; campbell, 1960; cziko, 1995). it is the basis of the in   uential learning theories of
clark hull and experimental methods of b. f. skinner (e.g., hull, 1943; skinner, 1938).

the term    reinforcement    in the context of animal learning came into use well after thorndike   s
expression of the law of e   ect, to the best of our knowledge    rst appearing in this context in the 1927
english translation of pavlov   s monograph on conditioned re   exes. reinforcement is the strengthening
of a pattern of behavior as a result of an animal receiving a stimulus   a reinforcer   in an appropri-
ate temporal relationship with another stimulus or with a response. some psychologists extended its
meaning to include the process of weakening in addition to strengthening, as well applying when the
omission or termination of an event changes behavior. reinforcement produces changes in behavior
that persist after the reinforcer is withdrawn, so that a stimulus that attracts an animal   s attention or
that energizes its behavior without producing lasting changes is not considered to be a reinforcer.

the idea of implementing trial-and-error learning in a computer appeared among the earliest thoughts
about the possibility of arti   cial intelligence. in a 1948 report, alan turing described a design for a
   pleasure-pain system    that worked along the lines of the law of e   ect:

when a con   guration is reached for which the action is undetermined, a random choice for
the missing data is made and the appropriate entry is made in the description, tentatively,
and is applied. when a pain stimulus occurs all tentative entries are cancelled, and when a
pleasure stimulus occurs they are all made permanent. (turing, 1948)

many ingenious electro-mechanical machines were constructed that demonstrated trial-and-error learn-
ing. the earliest may have been a machine built by thomas ross (1933) that was able to    nd its way
through a simple maze and remember the path through the settings of switches.
in 1951 w. grey
walter, already known for his    mechanical tortoise    (walter, 1950), built a version capable of a simple
form of learning (walter, 1951). in 1952 claude shannon demonstrated a maze-running mouse named
theseus that used trial and error to    nd its way through a maze, with the maze itself remembering the
successful directions via magnets and relays under its    oor (shannon, 1951, 1952). j. a. deutsch (1954)
described a maze-solving machine based on his behavior theory (deutsch, 1953) that has some proper-
ties in common with model-based id23 (chapter 8). in his ph.d. dissertation, marvin
minsky (1954) discussed computational models of id23 and described his construction
of an analog machine composed of components he called snarcs (stochastic neural-analog reinforce-
ment calculators) meant to resemble modi   able synaptic connections in the brain (chapter 15) the
fascinating web site cyberneticzoo.com contains a wealth of information on these and many other
electro-mechanical learning machines.

building electro-mechanical learning machines gave way to programming digital computers to perform
various types of learning, some of which implemented trial-and-error learning. farley and clark (1954)
described a digital simulation of a neural-network learning machine that learned by trial and error. but
their interests soon shifted from trial-and-error learning to generalization and pattern recognition, that
is, from id23 to supervised learning (clark and farley, 1955). this began a pattern of
confusion about the relationship between these types of learning. many researchers seemed to believe
that they were studying id23 when they were actually studying supervised learning.
for example, neural network pioneers such as rosenblatt (1962) and widrow and ho    (1960) were
clearly motivated by id23   they used the language of rewards and punishments   
but the systems they studied were supervised learning systems suitable for pattern recognition and
perceptual learning. even today, some researchers and textbooks minimize or blur the distinction
between these types of learning. for example, some neural-network textbooks have used the term
   trial-and-error    to describe networks that learn from training examples. this is an understandable
confusion because these networks use error information to update connection weights, but this misses
the essential character of trial-and-error learning as selecting actions on the basis of evaluative feedback

14

chapter 1.

introduction

that does not rely on knowledge of what the correct action should be.

partly as a result of these confusions, research into genuine trial-and-error learning became rare in
the 1960s and 1970s, although there were notable exceptions. in the 1960s the terms    reinforcement   
and    id23    were used in the engineering literature for the    rst time to describe
engineering uses of trial-and-error learning (e.g., waltz and fu, 1965; mendel, 1966; fu, 1970; mendel
and mcclaren, 1970). particularly in   uential was minsky   s paper    steps toward arti   cial intelligence   
(minsky, 1961), which discussed several issues relevant to trial-and-error learning, including prediction,
expectation, and what he called the basic credit-assignment problem for complex id23
systems: how do you distribute credit for success among the many decisions that may have been
involved in producing it? all of the methods we discuss in this book are, in a sense, directed toward
solving this problem. minsky   s paper is well worth reading today.

in the next few paragraphs we discuss some of the other exceptions and partial exceptions to the
relative neglect of computational and theoretical study of genuine trial-and-error learning in the 1960s
and 1970s.

one of these was the work by a new zealand researcher named john andreae. andreae (1963)
developed a system called stella that learned by trial and error in interaction with its environment.
this system included an internal model of the world and, later, an    internal monologue    to deal with
problems of hidden state (andreae, 1969a). andreae   s later work (1977) placed more emphasis on
learning from a teacher, but still included learning by trial and error, with the generation of novel
events being one of the system   s goals. a feature of this work was a    leakback process,    elaborated
more fully in andreae (1998), that implemented a credit-assignment mechanism similar to the backing-
up update operations that we describe. unfortunately, his pioneering research was not well known, and
did not greatly impact subsequent id23 research.

more in   uential was the work of donald michie. in 1961 and 1963 he described a simple trial-and-
error learning system for learning how to play tic-tac-toe (or naughts and crosses) called menace (for
matchbox educable naughts and crosses engine). it consisted of a matchbox for each possible game
position, each matchbox containing a number of colored beads, a di   erent color for each possible move
from that position. by drawing a bead at random from the matchbox corresponding to the current
game position, one could determine menace   s move. when a game was over, beads were added
to or removed from the boxes used during play to reinforce or punish menace   s decisions. michie
and chambers (1968) described another tic-tac-toe reinforcement learner called glee (game learning
expectimaxing engine) and a id23 controller called boxes. they applied boxes to
the task of learning to balance a pole hinged to a movable cart on the basis of a failure signal occurring
only when the pole fell or the cart reached the end of a track. this task was adapted from the earlier
work of widrow and smith (1964), who used supervised learning methods, assuming instruction from
a teacher already able to balance the pole. michie and chambers   s version of pole-balancing is one of
the best early examples of a id23 task under conditions of incomplete knowledge. it
in   uenced much later work in id23, beginning with some of our own studies (barto,
sutton, and anderson, 1983; sutton, 1984). michie consistently emphasized the role of trial and error
and learning as essential aspects of arti   cial intelligence (michie, 1974).

widrow, gupta, and maitra (1973) modi   ed the least-mean-square (lms) algorithm of widrow and
ho    (1960) to produce a id23 rule that could learn from success and failure signals
instead of from training examples. they called this form of learning    selective bootstrap adaptation   
and described it as    learning with a critic    instead of    learning with a teacher.    they analyzed this rule
and showed how it could learn to play blackjack. this was an isolated foray into id23
by widrow, whose contributions to supervised learning were much more in   uential. our use of the term
   critic    is derived from widrow, gupta, and maitra   s paper. buchanan, mitchell, smith, and johnson
(1978) independently used the term critic in the context of machine learning (see also dietterich and
buchanan, 1984), but for them a critic is an expert system able to do more than evaluate performance.

1.7. early history of id23

15

research on learning automata had a more direct in   uence on the trial-and-error thread leading
to modern id23 research. these are methods for solving a nonassociative, purely
selectional learning problem known as the k-armed bandit by analogy to a slot machine, or    one-armed
bandit,    except with k levers (see chapter 2). learning automata are simple, low-memory machines for
improving the id203 of reward in these problems. learning automata originated with work in the
1960s of the russian mathematician and physicist m. l. tsetlin and colleagues (published posthumously
in tsetlin, 1973) and has been extensively developed since then within engineering (see narendra and
thathachar, 1974, 1989). these developments included the study of stochastic learning automata,
which are methods for updating action probabilities on the basis of reward signals. stochastic learning
automata were foreshadowed by earlier work in psychology, beginning with william estes    1950 e   ort
toward a statistical theory of learning (estes, 1950) and further developed by others, most famously by
psychologist robert bush and statistician frederick mosteller (bush and mosteller, 1955).

the statistical learning theories developed in psychology were adopted by researchers in economics,
leading to a thread of research in that    eld devoted to id23. this work began in 1973
with the application of bush and mosteller   s learning theory to a collection of classical economic models
(cross, 1973). one goal of this research was to study arti   cial agents that act more like real people
than do traditional idealized economic agents (arthur, 1991). this approach expanded to the study of
id23 in the context of game theory. although id23 in economics
developed largely independently of the early work in arti   cial intelligence, id23 and
game theory is a topic of current interest in both    elds, but one that is beyond the scope of this book.
camerer (2003) discusses the id23 tradition in economics, and now  e et al. (2012)
provide an overview of the subject from the point of view of multi-agent extensions to the approach
that we introduce in this book. reinforcement in the context of game theory is a much di   erent subject
than id23 used in programs to play tic-tac-toe, checkers, and other recreational games.
see, for example, szita (2012) for an overview of this aspect of id23 and games.

john holland (1975) outlined a general theory of adaptive systems based on selectional principles.
his early work concerned trial and error primarily in its nonassociative form, as in evolutionary methods
and the k-armed bandit. in 1976 and more fully in 1986, he introduced classi   er systems, true rein-
forcement learning systems including association and value functions. a key component of holland   s
classi   er systems was the    bucket-brigade algorithm    for credit assignment that is closely related to the
temporal di   erence algorithm used in our tic-tac-toe example and discussed in chapter 6. another key
component was a genetic algorithm, an evolutionary method whose role was to evolve useful representa-
tions. classi   er systems have been extensively developed by many researchers to form a major branch
of id23 research (reviewed by urbanowicz and moore, 2009), but id107   
which we do not consider to be id23 systems by themselves   have received much more
attention, as have other approaches to evolutionary computation (e.g., fogel, owens and walsh, 1966,
and koza, 1992).

the individual most responsible for reviving the trial-and-error thread to id23
within arti   cial intelligence was harry klopf (1972, 1975, 1982). klopf recognized that essential as-
pects of adaptive behavior were being lost as learning researchers came to focus almost exclusively on
supervised learning. what was missing, according to klopf, were the hedonic aspects of behavior, the
drive to achieve some result from the environment, to control the environment toward desired ends and
away from undesired ends. this is the essential idea of trial-and-error learning. klopf   s ideas were
especially in   uential on the authors because our assessment of them (barto and sutton, 1981a) led to
our appreciation of the distinction between supervised and id23, and to our eventual
focus on id23. much of the early work that we and colleagues accomplished was di-
rected toward showing that id23 and supervised learning were indeed di   erent (barto,
sutton, and brouwer, 1981; barto and sutton, 1981b; barto and anandan, 1985). other studies showed
how id23 could address important problems in neural network learning, in particular,
how it could produce learning algorithms for multilayer networks (barto, anderson, and sutton, 1982;

16

chapter 1.

introduction

barto and anderson, 1985; barto and anandan, 1985; barto, 1985, 1986; barto and jordan, 1987). we
say more about id23 and neural networks in chapter 15.

we turn now to the third thread to the history of id23, that concerning temporal-
di   erence learning. temporal-di   erence learning methods are distinctive in being driven by the di   er-
ence between temporally successive estimates of the same quantity   for example, of the id203 of
winning in the tic-tac-toe example. this thread is smaller and less distinct than the other two, but it
has played a particularly important role in the    eld, in part because temporal-di   erence methods seem
to be new and unique to id23.

the origins of temporal-di   erence learning are in part in animal learning psychology, in particular,
in the notion of secondary reinforcers. a secondary reinforcer is a stimulus that has been paired with
a primary reinforcer such as food or pain and, as a result, has come to take on similar reinforcing
properties. minsky (1954) may have been the    rst to realize that this psychological principle could be
important for arti   cial learning systems. arthur samuel (1959) was the    rst to propose and implement
a learning method that included temporal-di   erence ideas, as part of his celebrated checkers-playing
program.

samuel made no reference to minsky   s work or to possible connections to animal learning. his inspira-
tion apparently came from claude shannon   s (1950) suggestion that a computer could be programmed
to use an evaluation function to play chess, and that it might be able to improve its play by modifying
this function on-line.
(it is possible that these ideas of shannon   s also in   uenced bellman, but we
know of no evidence for this.) minsky (1961) extensively discussed samuel   s work in his    steps    paper,
suggesting the connection to secondary reinforcement theories, both natural and arti   cial.

as we have discussed, in the decade following the work of minsky and samuel, little computational
work was done on trial-and-error learning, and apparently no computational work at all was done on
temporal-di   erence learning. in 1972, klopf brought trial-and-error learning together with an impor-
tant component of temporal-di   erence learning. klopf was interested in principles that would scale to
learning in large systems, and thus was intrigued by notions of local reinforcement, whereby subcompo-
nents of an overall learning system could reinforce one another. he developed the idea of    generalized
reinforcement,    whereby every component (nominally, every neuron) views all of its inputs in reinforce-
ment terms: excitatory inputs as rewards and inhibitory inputs as punishments. this is not the same
idea as what we now know as temporal-di   erence learning, and in retrospect it is farther from it than
was samuel   s work. on the other hand, klopf linked the idea with trial-and-error learning and related
it to the massive empirical database of animal learning psychology.

sutton (1978a, 1978b, 1978c) developed klopf   s ideas further, particularly the links to animal learning
theories, describing learning rules driven by changes in temporally successive predictions. he and barto
re   ned these ideas and developed a psychological model of classical conditioning based on temporal-
di   erence learning (sutton and barto, 1981a; barto and sutton, 1982). there followed several other
in   uential psychological models of classical conditioning based on temporal-di   erence learning (e.g.,
klopf, 1988; moore et al., 1986; sutton and barto, 1987, 1990). some neuroscience models developed
at this time are well interpreted in terms of temporal-di   erence learning (hawkins and kandel, 1984;
byrne, gingrich, and baxter, 1990; gelperin, hop   eld, and tank, 1985; tesauro, 1986; friston et al.,
1994), although in most cases there was no historical connection.

our early work on temporal-di   erence learning was strongly in   uenced by animal learning theories
and by klopf   s work. relationships to minsky   s    steps    paper and to samuel   s checkers players appear
to have been recognized only afterward. by 1981, however, we were fully aware of all the prior work
mentioned above as part of the temporal-di   erence and trial-and-error threads. at this time we de-
veloped a method for using temporal-di   erence learning combined with trial-and-error learning, known
as the actor   critic architecture, and applied this method to michie and chambers   s pole-balancing
problem (barto, sutton, and anderson, 1983). this method was extensively studied in sutton   s (1984)
ph.d. dissertation and extended to use id26 neural networks in anderson   s (1986) ph.d.

17

dissertation. around this time, holland (1986) incorporated temporal-di   erence ideas explicitly into
his classi   er systems in the form of his bucket-brigade algorithm. a key step was taken by sutton in
1988 by separating temporal-di   erence learning from control, treating it as a general prediction method.
that paper also introduced the td(  ) algorithm and proved some of its convergence properties.

as we were    nalizing our work on the actor   critic architecture in 1981, we discovered a paper by
ian witten (1977) which appears to be the earliest publication of a temporal-di   erence learning rule.
he proposed the method that we now call tabular td(0) for use as part of an adaptive controller for
solving mdps. witten   s work was a descendant of andreae   s early experiments with stella and
other trial-and-error learning systems. thus, witten   s 1977 paper spanned both major threads of
id23 research   trial-and-error learning and optimal control   while making a distinct
early contribution to temporal-di   erence learning.

the temporal-di   erence and optimal control threads were fully brought together in 1989 with chris
watkins   s development of id24. this work extended and integrated prior work in all three threads
of id23 research. paul werbos (1987) contributed to this integration by arguing for
the convergence of trial-and-error learning and id145 since 1977. by the time of
watkins   s work there had been tremendous growth in id23 research, primarily in the
machine learning sub   eld of arti   cial intelligence, but also in neural networks and arti   cial intelligence
more broadly. in 1992, the remarkable success of gerry tesauro   s backgammon playing program, td-
gammon, brought additional attention to the    eld.

in the time since publication of the    rst edition of this book, a    ourishing sub   eld of neuroscience
developed that focuses on the relationship between id23 algorithms and reinforcement
learning in the nervous system. most responsible for this is an uncanny similarity between the behavior
of temporal-di   erence algorithms and the activity of dopamine producing neurons in the brain, as
pointed out by a number of researchers (friston et al., 1994; barto, 1995a; houk, adams, and barto,
1995; montague, dayan, and sejnowski, 1996; and schultz, dayan, and montague, 1997). chapter 15
provides an introduction to this exciting aspect of id23.

other important contributions made in the recent history of id23 are too numerous
to mention in this brief account; we cite many of these at the end of the individual chapters in which
they arise.

bibliographical remarks

for additional general coverage of id23, we refer the reader to the books by szepesv  ari
(2010), bertsekas and tsitsiklis (1996), kaelbling (1993a), and sugiyama et al. (2013). books that take
a control or operations research perspective include those of si et al. (2004), powell (2011), lewis and
liu (2012), and bertsekas (2012). cao   s (2009) review places id23 in the context of
other approaches to learning and optimization of stochastic dynamic systems. three special issues of
the journal machine learning focus on id23: sutton (1992), kaelbling (1996), and
singh (2002). useful surveys are provided by barto (1995b); kaelbling, littman, and moore (1996);
and keerthi and ravindran (1997). the volume edited by weiring and van otterlo (2012) provides an
excellent overview of recent developments.

1.2

the example of phil   s breakfast in this chapter was inspired by agre (1988).

1.5

the temporal-di   erence method used in the tic-tac-toe example is developed in chapter 6.

part i: tabular solution methods

in this part of the book we describe almost all the core ideas of id23 algorithms
in their simplest forms: that in which the state and action spaces are small enough for the approxi-
mate value functions to be represented as arrays, or tables. in this case, the methods can often    nd
exact solutions, that is, they can often    nd exactly the optimal value function and the optimal policy.
this contrasts with the approximate methods described in the next part of the book, which only    nd
approximate solutions, but which in return can be applied e   ectively to much larger problems.

the    rst chapter of this part of the book describes solution methods for the special case of the
id23 problem in which there is only a single state, called bandit problems. the
second chapter describes the general problem formulation that we treat throughout the rest of the
book      nite id100   and its main ideas including bellman equations and value
functions.

the next three chapters describe three fundamental classes of methods for solving    nite markov
decision problems: id145, monte carlo methods, and temporal-di   erence learning.
each class of methods has its strengths and weaknesses. id145 methods are well
developed mathematically, but require a complete and accurate model of the environment. monte
carlo methods don   t require a model and are conceptually simple, but are not well suited for step-
by-step incremental computation. finally, temporal-di   erence methods require no model and are fully
incremental, but are more complex to analyze. the methods also di   er in several ways with respect to
their e   ciency and speed of convergence.

the remaining two chapters describe how these three classes of methods can be combined to obtain
the best features of each of them. in one chapter we describe how the strengths of monte carlo methods
can be combined with the strengths of temporal-di   erence methods via the use of eligibility traces. in
the    nal chapter of this part of the book we show how temporal-di   erence learning methods can be
combined with model learning and planning methods (such as id145) for a complete
and uni   ed solution to the tabular id23 problem.

18

chapter 2

multi-armed bandits

the most important feature distinguishing id23 from other types of learning is that
it uses training information that evaluates the actions taken rather than instructs by giving correct
actions. this is what creates the need for active exploration, for an explicit search for good behavior.
purely evaluative feedback indicates how good the action taken was, but not whether it was the best or
the worst action possible. purely instructive feedback, on the other hand, indicates the correct action
to take, independently of the action actually taken. this kind of feedback is the basis of supervised
learning, which includes large parts of pattern classi   cation, arti   cial neural networks, and system
identi   cation. in their pure forms, these two kinds of feedback are quite distinct: evaluative feedback
depends entirely on the action taken, whereas instructive feedback is independent of the action taken.

in this chapter we study the evaluative aspect of id23 in a simpli   ed setting, one
that does not involve learning to act in more than one situation. this nonassociative setting is the
one in which most prior work involving evaluative feedback has been done, and it avoids much of the
complexity of the full id23 problem. studying this case enables us to see most clearly
how evaluative feedback di   ers from, and yet can be combined with, instructive feedback.

the particular nonassociative, evaluative feedback problem that we explore is a simple version of
the k-armed bandit problem. we use this problem to introduce a number of basic learning methods
which we extend in later chapters to apply to the full id23 problem. at the end
of this chapter, we take a step closer to the full id23 problem by discussing what
happens when the bandit problem becomes associative, that is, when actions are taken in more than
one situation.

2.1 a k-armed bandit problem

consider the following learning problem. you are faced repeatedly with a choice among k di   erent op-
tions, or actions. after each choice you receive a numerical reward chosen from a stationary id203
distribution that depends on the action you selected. your objective is to maximize the expected total
reward over some time period, for example, over 1000 action selections, or time steps.

this is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or
   one-armed bandit,    except that it has k levers instead of one. each action selection is like a play of one
of the slot machine   s levers, and the rewards are the payo   s for hitting the jackpot. through repeated
action selections you are to maximize your winnings by concentrating your actions on the best levers.
another analogy is that of a doctor choosing between experimental treatments for a series of seriously
ill patients. each action is the selection of a treatment, and each reward is the survival or well-being

19

20

chapter 2. multi-armed bandits

of the patient. today the term    bandit problem    is sometimes used for a generalization of the problem
described above, but in this book we use it to refer just to this simple case.

in our k-armed bandit problem, each of the k actions has an expected or mean reward given that
that action is selected; let us call this the value of that action. we denote the action selected on time
step t as at, and the corresponding reward as rt. the value then of an arbitrary action a, denoted
q   (a), is the expected reward given that a is selected:

q   (a)

.
= e[rt | at = a] .

if you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you
would always select the action with highest value. we assume that you do not know the action values
with certainty, although you may have estimates. we denote the estimated value of action a at time
step t as qt(a). we would like qt(a) to be close to q   (a).

if you maintain estimates of the action values, then at any time step there is at least one action whose
estimated value is greatest. we call these the greedy actions. when you select one of these actions,
we say that you are exploiting your current knowledge of the values of the actions.
if instead you
select one of the nongreedy actions, then we say you are exploring, because this enables you to improve
your estimate of the nongreedy action   s value. exploitation is the right thing to do to maximize the
expected reward on the one step, but exploration may produce the greater total reward in the long run.
for example, suppose a greedy action   s value is known with certainty, while several other actions are
estimated to be nearly as good but with substantial uncertainty. the uncertainty is such that at least
one of these other actions probably is actually better than the greedy action, but you don   t know which
one. if you have many time steps ahead on which to make action selections, then it may be better to
explore the nongreedy actions and discover which of them are better than the greedy action. reward is
lower in the short run, during exploration, but higher in the long run because after you have discovered
the better actions, you can exploit them many times. because it is not possible both to explore and
to exploit with any single action selection, one often refers to the    con   ict    between exploration and
exploitation.

in any speci   c case, whether it is better to explore or exploit depends in a complex way on the precise
values of the estimates, uncertainties, and the number of remaining steps. there are many sophisticated
methods for balancing exploration and exploitation for particular mathematical formulations of the k-
armed bandit and related problems. however, most of these methods make strong assumptions about
stationarity and prior knowledge that are either violated or impossible to verify in applications and in
the full id23 problem that we consider in subsequent chapters. the guarantees of
optimality or bounded loss for these methods are of little comfort when the assumptions of their theory
do not apply.

in this book we do not worry about balancing exploration and exploitation in a sophisticated way; we
worry only about balancing them at all. in this chapter we present several simple balancing methods for
the k-armed bandit problem and show that they work much better than methods that always exploit.
the need to balance exploration and exploitation is a distinctive challenge that arises in reinforcement
learning; the simplicity of our version of the k-armed bandit problem enables us to show this in a
particularly clear form.

2.2 action-value methods

we begin by looking more closely at some simple methods for estimating the values of actions and for
using the estimates to make action selection decisions. recall that the true value of an action is the
mean reward when that action is selected. one natural way to estimate this is by averaging the rewards

2.3. the 10-armed testbed

21

actually received:

qt(a)

.
=

sum of rewards when a taken prior to t

number of times a taken prior to t

i=1 ri    1ai=a

1ai=a

= (cid:80)t   1
(cid:80)t   1

i=1

,

(2.1)

where 1predicate denotes the random variable that is 1 if predicate is true and 0 if it is not.
if the
denominator is zero, then we instead de   ne qt(a) as some default value, such as 0. as the denominator
goes to in   nity, by the law of large numbers, qt(a) converges to q   (a). we call this the sample-average
method for estimating action values because each estimate is an average of the sample of relevant
rewards. of course this is just one way to estimate action values, and not necessarily the best one.
nevertheless, for now let us stay with this simple estimation method and turn to the question of how
the estimates might be used to select actions.

the simplest action selection rule is to select one of the actions with the highest estimated value,
that is, one of the greedy actions as de   ned in the previous section. if there is more than one greedy
action, then a selection is made among them in some arbitrary way, perhaps randomly. we write this
greedy action selection method as

at

.
= argmax

a

qt(a),

(2.2)

where argmaxa denotes the action a for which the expression that follows is maximized (again, with ties
broken arbitrarily). greedy action selection always exploits current knowledge to maximize immediate
reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.
a simple alternative is to behave greedily most of the time, but every once in a while, say with small
id203   , instead select randomly from among all the actions with equal id203, independently
of the action-value estimates. we call methods using this near-greedy action selection rule   -greedy
methods. an advantage of these methods is that, in the limit as the number of steps increases, every
action will be sampled an in   nite number of times, thus ensuring that all the qt(a) converge to q   (a).
this of course implies that the id203 of selecting the optimal action converges to greater than
1       , that is, to near certainty. these are just asymptotic guarantees, however, and say little about
the practical e   ectiveness of the methods.

exercise 2.1 in   -greedy action selection, for the case of two actions and    = 0.5, what is the
id203 that the greedy action is selected?

exercise 2.2: bandit example consider a k-armed bandit problem with k = 4 actions, denoted
1, 2, 3, and 4. consider applying to this problem a bandit algorithm using   -greedy action selection,
sample-average action-value estimates, and initial estimates of q1(a) = 0, for all a. suppose the initial
sequence of actions and rewards is a1 = 1, r1 = 1, a2 = 2, r2 = 1, a3 = 2, r3 = 2, a4 = 2, r4 = 2,
a5 = 3, r5 = 0. on some of these time steps the    case may have occurred, causing an action to be
selected at random. on which time steps did this de   nitely occur? on which time steps could this
(cid:3)
possibly have occurred?

2.3 the 10-armed testbed

to roughly assess the relative e   ectiveness of the greedy and   -greedy methods, we compared them
numerically on a suite of test problems. this was a set of 2000 randomly generated k-armed bandit
problems with k = 10. for each bandit problem, such as the one shown in figure 2.1, the action values,
q   (a), a = 1, . . . , 10, were selected according to a normal (gaussian) distribution with mean 0 and
variance 1. then, when a learning method applied to that problem selected action at at time step t,
the actual reward, rt, was selected from a normal distribution with mean q   (at) and variance 1. these
distributions are shown in gray in figure 2.1. we call this suite of test tasks the 10-armed testbed. for

22

chapter 2. multi-armed bandits

figure 2.1: an example bandit problem from the 10-armed testbed. the true value q   (a) of each of the ten
actions was selected according to a normal distribution with mean zero and unit variance, and then the actual
rewards were selected according to a mean q   (a) unit variance normal distribution, as suggested by these gray
distributions.

any learning method, we can measure its performance and behavior as it improves with experience over
1000 time steps when applied to one of the bandit problems. this makes up one run. repeating this
for 2000 independent runs, each with a di   erent bandit problem, we obtained measures of the learning
algorithm   s average behavior.

figure 2.2 compares a greedy method with two   -greedy methods (   = 0.01 and    = 0.1), as described
above, on the 10-armed testbed. all the methods formed their action-value estimates using the sample-
average technique. the upper graph shows the increase in expected reward with experience. the greedy
method improved slightly faster than the other methods at the very beginning, but then leveled o    at
a lower level. it achieved a reward-per-step of only about 1, compared with the best possible of about
1.55 on this testbed. the greedy method performed signi   cantly worse in the long run because it often
got stuck performing suboptimal actions. the lower graph shows that the greedy method found the
optimal action in only approximately one-third of the tasks. in the other two-thirds, its initial samples
of the optimal action were disappointing, and it never returned to it. the   -greedy methods eventually
performed better because they continued to explore and to improve their chances of recognizing the
optimal action. the    = 0.1 method explored more, and usually found the optimal action earlier, but
it never selected that action more than 91% of the time. the    = 0.01 method improved more slowly,
but eventually would perform better than the    = 0.1 method on both performance measures shown in
the    gure. it is also possible to reduce    over time to try to get the best of both high and low values.

the advantage of   -greedy over greedy methods depends on the task. for example, suppose the
reward variance had been larger, say 10 instead of 1. with noisier rewards it takes more exploration to
   nd the optimal action, and   -greedy methods should fare even better relative to the greedy method.
on the other hand, if the reward variances were zero, then the greedy method would know the true

0123-3-2-1q   (1)q   (2)q   (3)q   (4)q   (5)q   (6)q   (7)q   (8)q   (9)q   (10)rewarddistribution12635478910action2.4.

incremental implementation

23

figure 2.2: average performance of   -greedy action-value methods on the 10-armed testbed. these data are
averages over 2000 runs with di   erent bandit problems. all methods used sample averages as their action-value
estimates.

value of each action after trying it once. in this case the greedy method might actually perform best
because it would soon    nd the optimal action and then never explore. but even in the deterministic
case there is a large advantage to exploring if we weaken some of the other assumptions. for example,
suppose the bandit task were nonstationary, that is, the true values of the actions changed over time.
in this case exploration is needed even in the deterministic case to make sure one of the nongreedy
actions has not changed to become better than the greedy one. as we shall see in the next few
chapters, nonstationarity is the case most commonly encountered in id23. even if
the underlying task is stationary and deterministic, the learner faces a set of banditlike decision tasks
each of which changes over time as learning proceeds and the agent   s policy changes. reinforcement
learning requires a balance between exploration and exploitation.

exercise 2.3 in the comparison shown in figure 2.2, which method will perform best in the long run
in terms of cumulative reward and id203 of selecting the best action? how much better will it
(cid:3)
be? express your answer quantitatively.

2.4 incremental implementation

the action-value methods we have discussed so far all estimate action values as sample averages of
observed rewards. we now turn to the question of how these averages can be computed in a computa-
tionally e   cient manner, in particular, with constant memory and constant per-time-step computation.
to simplify notation we concentrate on a single action. let ri now denote the reward received after

 = 0 (greedy)   = 0 (greedy)00.511.5averagereward02505007501000steps0%20%40%60%80%100%%optimalaction02505007501000steps = 0.01 = 0.1    = 0.1 = 0.01 1124

chapter 2. multi-armed bandits

the ith selection of this action, and let qn denote the estimate of its action value after it has been
selected n     1 times, which we can now write simply as

.
=

qn

r1 + r2 +        + rn   1

n     1

.

the obvious implementation would be to maintain a record of all the rewards and then perform this
computation whenever the estimated value was needed. however, if this is done, then the memory and
computational requirements would grow over time as more rewards are seen. each additional reward
would require additional memory to store it and additional computation to compute the sum in the
numerator.

as you might suspect, this is not really necessary.

it is easy to devise incremental formulas for
updating averages with small, constant computation required to process each new reward. given qn
and the nth reward, rn, the new average of all n rewards can be computed by

qn+1 =

=

=

=

=

1

1

1

ri

1
n

n(cid:88)i=1
n(cid:32)rn +
ri(cid:33)
n   1(cid:88)i=1
n(cid:32)rn + (n     1)
n(cid:16)rn + (n     1)qn(cid:17)
n(cid:16)rn + nqn     qn(cid:17)
n(cid:104)rn     qn(cid:105),

1

1

1

n     1

= qn +

ri(cid:33)

n   1(cid:88)i=1

(2.3)

which holds even for n = 1, obtaining q2 = r1 for arbitrary q1. this implementation requires memory
only for qn and n, and only the small computation (2.3) for each new reward. pseudocode for a
complete bandit algorithm using incrementally computed sample averages and   -greedy action selection
is shown in the box on the next page. the function bandit(a) is assumed to take an action and return
a corresponding reward.

a simple bandit algorithm

initialize, for a = 1 to k:

q(a)     0
n (a)     0

repeat forever:

(cid:26) arg maxa q(a)

a    
r     bandit(a)
n (a)     n (a) + 1
q(a)     q(a) + 1

n (a)

(cid:2)r     q(a)(cid:3)

with id203 1       

(breaking ties randomly)

a random action with id203   

the update rule (2.3) is of a form that occurs frequently throughout this book. the general form is

newestimate     oldestimate + stepsize(cid:104)target     oldestimate(cid:105).

(2.4)

2.5. tracking a nonstationary problem

25

the expression(cid:2)target   oldestimate(cid:3) is an error in the estimate. it is reduced by taking a step toward

the    target.    the target is presumed to indicate a desirable direction in which to move, though it may
be noisy. in the case above, for example, the target is the nth reward.

note that the step-size parameter (stepsize) used in the incremental method described above changes
from time step to time step. in processing the nth reward for action a, the method uses the step-size
parameter 1
n . in this book we denote the step-size parameter by    or, more generally, by   t(a). we
sometimes use the informal shorthand    = 1
n , leaving the dependence of n on the
action implicit, just as we have in this section.

n when   t(a) = 1

2.5 tracking a nonstationary problem

the averaging methods discussed so far are appropriate for stationary bandit problems, that is, for
bandit problems in which the reward probabilities do not change over time. as noted earlier, we often
encounter id23 problems that are e   ectively nonstationary. in such cases it makes
sense to give more weight to recent rewards than to long-past rewards. one of the most popular ways
of doing this is to use a constant step-size parameter. for example, the incremental update rule (2.3)
for updating an average qn of the n     1 past rewards is modi   ed to be

where the step-size parameter        (0, 1] is constant.1 this results in qn+1 being a weighted average of
past rewards and the initial estimate q1:

qn+1

.

= qn +   (cid:104)rn     qn(cid:105),

qn+1 = qn +   (cid:104)rn     qn(cid:105)

=   rn + (1       )qn
=   rn + (1       ) [  rn   1 + (1       )qn   1]
=   rn + (1       )  rn   1 + (1       )2qn   1
=   rn + (1       )  rn   1 + (1       )2  rn   2 +

= (1       )nq1 +

       + (1       )n   1  r1 + (1       )nq1
n(cid:88)i=1

  (1       )n   iri.

(2.5)

(2.6)

we call this a weighted average because the sum of the weights is (1       )n +(cid:80)n
i=1   (1       )n   i = 1,
as you can check for yourself. note that the weight,   (1       )n   i, given to the reward ri depends
on how many rewards ago, n     i, it was observed. the quantity 1        is less than 1, and thus the
weight given to ri decreases as the number of intervening rewards increases. in fact, the weight decays
exponentially according to the exponent on 1       .
(if 1        = 0, then all the weight goes on the
very last reward, rn, because of the convention that 00 = 1.) accordingly, this is sometimes called an
exponential recency-weighted average.

sometimes it is convenient to vary the step-size parameter from step to step. let   n(a) denote the
step-size parameter used to process the reward received after the nth selection of action a. as we have
noted, the choice   n(a) = 1
n results in the sample-average method, which is guaranteed to converge to
the true action values by the law of large numbers. but of course convergence is not guaranteed for all
choices of the sequence {  n(a)}. a well-known result in stochastic approximation theory gives us the
1the notation (a, b] as a set denotes the real interval between a and b including b but not including a. thus, here we

are saying that 0 <        1.

26

chapter 2. multi-armed bandits

conditions required to assure convergence with id203 1:

   (cid:88)n=1

  n(a) =    

and

   (cid:88)n=1

  2
n(a) <    .

(2.7)

the    rst condition is required to guarantee that the steps are large enough to eventually overcome any
initial conditions or random    uctuations. the second condition guarantees that eventually the steps
become small enough to assure convergence.

note that both convergence conditions are met for the sample-average case,   n(a) = 1

n , but not for
the case of constant step-size parameter,   n(a) =   . in the latter case, the second condition is not
met, indicating that the estimates never completely converge but continue to vary in response to the
most recently received rewards. as we mentioned above, this is actually desirable in a nonstationary
environment, and problems that are e   ectively nonstationary are the most common in reinforcement
learning. in addition, sequences of step-size parameters that meet the conditions (2.7) often converge
very slowly or need considerable tuning in order to obtain a satisfactory convergence rate. although
sequences of step-size parameters that meet these convergence conditions are often used in theoretical
work, they are seldom used in applications and empirical research.

exercise 2.4 if the step-size parameters,   n, are not constant, then the estimate qn is a weighted
average of previously received rewards with a weighting di   erent from that given by (2.6). what is
the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of
(cid:3)
step-size parameters?
exercise 2.5 (programming) design and conduct an experiment to demonstrate the di   culties
that sample-average methods have for nonstationary problems. use a modi   ed version of the 10-armed
testbed in which all the q   (a) start out equal and then take independent id93 (say by adding
a normally distributed increment with mean zero and standard deviation 0.01 to all the q   (a) on each
step). prepare plots like figure 2.2 for an action-value method using sample averages, incrementally
computed, and another action-value method using a constant step-size parameter,    = 0.1. use    = 0.1
(cid:3)
and longer runs, say of 10,000 steps.

2.6 optimistic initial values

all the methods we have discussed so far are dependent to some extent on the initial action-value
estimates, q1(a). in the language of statistics, these methods are biased by their initial estimates. for
the sample-average methods, the bias disappears once all actions have been selected at least once, but
for methods with constant   , the bias is permanent, though decreasing over time as given by (2.6). in
practice, this kind of bias is usually not a problem and can sometimes be very helpful. the downside is
that the initial estimates become, in e   ect, a set of parameters that must be picked by the user, if only
to set them all to zero. the upside is that they provide an easy way to supply some prior knowledge
about what level of rewards can be expected.

initial action values can also be used as a simple way to encourage exploration. suppose that instead
of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to +5.
recall that the q   (a) in this problem are selected from a normal distribution with mean 0 and variance 1.
an initial estimate of +5 is thus wildly optimistic. but this optimism encourages action-value methods
to explore. whichever actions are initially selected, the reward is less than the starting estimates; the
learner switches to other actions, being    disappointed    with the rewards it is receiving. the result is
that all actions are tried several times before the value estimates converge. the system does a fair
amount of exploration even if greedy actions are selected all the time.

figure 2.3 shows the performance on the 10-armed bandit testbed of a greedy method using q1(a) =
initially, the

+5, for all a. for comparison, also shown is an   -greedy method with q1(a) = 0.

2.7. upper-confidence-bound action selection

27

figure 2.3: the e   ect of optimistic initial action-value estimates on the 10-armed testbed. both methods used
a constant step-size parameter,    = 0.1.

optimistic method performs worse because it explores more, but eventually it performs better because
its exploration decreases with time. we call this technique for encouraging exploration optimistic initial
values. we regard it as a simple trick that can be quite e   ective on stationary problems, but it is far
from being a generally useful approach to encouraging exploration. for example, it is not well suited to
nonstationary problems because its drive for exploration is inherently temporary. if the task changes,
creating a renewed need for exploration, this method cannot help. indeed, any method that focuses
on the initial conditions in any special way is unlikely to help with the general nonstationary case.
the beginning of time occurs only once, and thus we should not focus on it too much. this criticism
applies as well to the sample-average methods, which also treat the beginning of time as a special event,
averaging all subsequent rewards with equal weights. nevertheless, all of these methods are very simple,
and one of them   or some simple combination of them   is often adequate in practice. in the rest of
this book we make frequent use of several of these simple exploration techniques.

exercise 2.6: mysterious spikes the results shown in figure 2.3 should be quite reliable because
they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. why, then, are there
oscillations and spikes in the early part of the curve for the optimistic method? in other words, what
might make this method perform particularly better or worse, on average, on particular early steps? (cid:3)

2.7 upper-con   dence-bound action selection

exploration is needed because there is always uncertainty about the accuracy of the action-value es-
timates. the greedy actions are those that look best at present, but some of the other actions may
actually be better.   -greedy action selection forces the non-greedy actions to be tried, but indiscrim-
inately, with no preference for those that are nearly greedy or particularly uncertain.
it would be
better to select among the non-greedy actions according to their potential for actually being optimal,
taking into account both how close their estimates are to being maximal and the uncertainties in those
estimates. one e   ective way of doing this is to select actions according to

at

.
= argmax

a

(cid:34)qt(a) + c(cid:115) ln t

nt(a)(cid:35) ,

(2.8)

where ln t denotes the natural logarithm of t (the number that e     2.71828 would have to be raised to
in order to equal t), nt(a) denotes the number of times that action a has been selected prior to time

0%20%40%60%80%100%%optimalaction02004006008001000playsoptimistic, greedyq0 = 5,    = 0realistic, !-greedyq0 = 0,    = 0.111steps  128

chapter 2. multi-armed bandits

t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. if nt(a) = 0,
then a is considered to be a maximizing action.

the idea of this upper con   dence bound (ucb) action selection is that the square-root term is a
measure of the uncertainty or variance in the estimate of a   s value. the quantity being max   ed over is
thus a sort of upper bound on the possible true value of action a, with c determining the con   dence level.
each time a is selected the uncertainty is presumably reduced: nt(a) increments, and, as it appears in
the denominator, the uncertainty term decreases. on the other hand, each time an action other than a
is selected, t increases but nt(a) does not; because t appears in the numerator, the uncertainty estimate
increases. the use of the natural logarithm means that the increases get smaller over time, but are
unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have
already been selected frequently, will be selected with decreasing frequency over time.

results with ucb on the 10-armed testbed are shown in figure 2.4. ucb often performs well,
as shown here, but is more di   cult than   -greedy to extend beyond bandits to the more general
id23 settings considered in the rest of this book. one di   culty is in dealing with
nonstationary problems; methods more complex than those presented in section 2.5 would be needed.
another di   culty is dealing with large state spaces, particularly when using function approximation as
developed in part ii of this book. in these more advanced settings the idea of ucb action selection is
usually not practical.

2.8 gradient bandit algorithms

so far in this chapter we have considered methods that estimate action values and use those estimates
to select actions. this is often a good approach, but it is not the only one possible. in this section
we consider learning a numerical preference for each action a, which we denote ht(a). the larger the
preference, the more often that action is taken, but the preference has no interpretation in terms of
reward. only the relative preference of one action over another is important; if we add 1000 to all the
preferences there is no e   ect on the action probabilities, which are determined according to a soft-max
distribution (i.e., gibbs or boltzmann distribution) as follows:

pr{at = a}

.
=

.
=   t(a),

eht(a)
b=1 eht(b)

(cid:80)k

(2.9)

figure 2.4: average performance of ucb action selection on the 10-armed testbed. as shown, ucb generally
performs better than   -greedy action selection, except in the    rst k steps, when it selects randomly among the
as-yet-untried actions.

1 -greedy    = 0.1ucb  c = 2averagerewardsteps2.8. gradient bandit algorithms

29

where here we have also introduced a useful new notation,   t(a), for the id203 of taking action a
at time t. initially all preferences are the same (e.g., h1(a) = 0, for all a) so that all actions have an
equal id203 of being selected.

exercise 2.7 show that in the case of two actions, the soft-max distribution is the same as that given
(cid:3)
by the logistic, or sigmoid, function often used in statistics and arti   cial neural networks.
there is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent.

on each step, after selecting action at and receiving the reward rt, preferences are updated by:

ht+1(at)
ht+1(a)

.

.

= ht(at) +   (cid:0)rt       rt(cid:1)(cid:0)1       t(at)(cid:1),
= ht(a)       (cid:0)rt       rt(cid:1)  t(a),

and
for all a (cid:54)= at,

(2.10)

where    > 0 is a step-size parameter, and   rt     r is the average of all the rewards up through and
including time t, which can be computed incrementally as described in section 2.4 (or section 2.5 if
the problem is nonstationary). the   rt term serves as a baseline with which the reward is compared.
if the reward is higher than the baseline, then the id203 of taking at in the future is increased,
and if the reward is below baseline, then id203 is decreased. the non-selected actions move in the
opposite direction.

figure 2.5 shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in
which the true expected rewards were selected according to a normal distribution with a mean of +4
instead of zero (and with unit variance as before). this shifting up of all the rewards has absolutely
no e   ect on the gradient bandit algorithm because of the reward baseline term, which instantaneously
adapts to the new level. but if the baseline were omitted (that is, if   rt was taken to be constant zero
in (2.10)), then performance would be signi   cantly degraded, as shown in the    gure.

figure 2.5: average performance of the gradient bandit algorithm with and without a reward baseline on the
10-armed testbed when the q   (a) are chosen to be near +4 rather than near zero.

the bandit gradient algorithm as stochastic gradient ascent

one can gain a deeper insight into the gradient bandit algorithm by understanding it as a
stochastic approximation to gradient ascent.
in exact gradient ascent, each preference ht(a)
would be incremented proportional to the increment   s e   ect on performance:

ht+1(a)

.
= ht(a) +   

    e [rt]
   ht(a)

,

(2.11)

%optimalactionsteps   = 0.1100%80%60%40%20%0%   = 0.4   = 0.1   = 0.4without baselinewith baseline1250500750100030

chapter 2. multi-armed bandits

where the measure of performance here is the expected reward:

e[rt] =(cid:88)b

  t(b)q   (b),

and the measure of the increment   s e   ect is the partial derivative of this performance measure
with respect to the preference. of course, it is not possible to implement gradient ascent exactly
in our case because by assumption we do not know the q   (b), but in fact the updates of our
algorithm (2.10) are equal to (2.11) in expected value, making the algorithm an instance of
stochastic gradient ascent. the calculations showing this require only beginning calculus, but
take several steps. first we take a closer look at the exact performance gradient:

    e[rt]
   ht(a)

   

=

  t(b)q   (b)(cid:35)

   ht(a)(cid:34)(cid:88)b
=(cid:88)b
=(cid:88)b (cid:0)q   (b)     xt(cid:1)       t(b)

      t(b)
   ht(a)

q   (b)

   ht(a)

,

where xt can be any scalar that does not depend on b. we can include it here because the
      t(b)
   ht(a) = 0. as ht(a) is changed, some actions   
probabilities go up and some down, but the sum of the changes must be zero because the sum
of the probabilities must remain one.

gradient sums to zero over all the actions, (cid:80)b
  t(b)(cid:0)q   (b)     xt(cid:1)       t(b)

=(cid:88)b

    e[rt]
   ht(a)

   ht(a)

/  t(b)

the equation is now in the form of an expectation, summing over all possible values b of the
random variable at, then multiplying by the id203 of taking those values. thus:

= e(cid:20)(cid:0)q   (at)     xt(cid:1)       t(at)
= e(cid:20)(cid:0)rt       rt(cid:1)       t(at)

/  t(at)(cid:21)
/  t(at)(cid:21) ,

   ht(a)

   ht(a)

where here we have chosen xt =   rt and substituted rt for q   (at), which is permitted because
e[rt|at] = q   (at) and because the rt (given at) is uncorrelated with anything else. shortly we
   ht(a) =   t(b)(cid:0)1a=b       t(a)(cid:1), where 1a=b is de   ned to be 1 if a = b, else 0.
will establish that       t(b)
assuming that for now, we have

= e(cid:2)(cid:0)rt       rt(cid:1)  t(at)(cid:0)1a=at       t(a)(cid:1)/  t(at)(cid:3)
= e(cid:2)(cid:0)rt       rt(cid:1)(cid:0)1a=at       t(a)(cid:1)(cid:3) .

recall that our plan has been to write the performance gradient as an expectation of something
that we can sample on each step, as we have just done, and then update on each step proportional
to the sample. substituting a sample of the expectation above for the performance gradient in
(2.11) yields:

ht+1(a) = ht(a) +   (cid:0)rt       rt(cid:1)(cid:0)1a=at       t(a)(cid:1),

for all a,

2.9. associative search (contextual bandits)

31

which you may recognize as being equivalent to our original algorithm (2.10).

thus it remains only to show that       t(b)

standard quotient rule for derivatives:

   ht(a) =   t(b)(cid:0)1a=b       t(a)(cid:1), as we assumed. recall the

   

   x(cid:20) f (x)
g(x)(cid:21) =

   f (x)

   x g(x)     f (x)    g(x)

   x

g(x)2

.

using this, we can write

      t(b)
   ht(a)

=

   

   ht(a)

  t(b)

c=1 eht(c)
   ht(a)

   

=

=

=

   eht(b)

eht(b)

   ht(a)(cid:34)
c=1 eht(c)(cid:35)
(cid:80)k
c=1 eht(c)     eht(b)    (cid:80)k
   ht(a)(cid:80)k
c=1 eht(c)(cid:17)2
(cid:16)(cid:80)k
1a=beht(b)(cid:80)k
(cid:16)(cid:80)k
c=1 eht(c)(cid:17)2
1a=beht(b)
c=1 eht(c)    
(cid:80)k
(cid:16)(cid:80)k
c=1 eht(c)(cid:17)2
=   t(b)(cid:0)1a=b       t(a)(cid:1).

= 1a=b  t(b)       t(b)  t(a)

eht(b)eht(a)

=

c=1 eht(c)     eht(b)eht(a)

(by the quotient rule)

(because    ex

   x = ex)

q.e.d.

we have just shown that the expected update of the gradient bandit algorithm is equal to the
gradient of expected reward, and thus that the algorithm is an instance of stochastic gradient
ascent. this assures us that the algorithm has robust convergence properties.

note that we did not require any properties of the reward baseline other than that it does not
depend on the selected action. for example, we could have set it to zero, or to 1000, and the
algorithm would still be an instance of stochastic gradient ascent. the choice of the baseline does
not a   ect the expected update of the algorithm, but it does a   ect the variance of the update
and thus the rate of convergence (as shown, e.g., in figure 2.5). choosing it as the average of
the rewards may not be the very best, but it is simple and works well in practice.

2.9 associative search (contextual bandits)

so far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no
need to associate di   erent actions with di   erent situations. in these tasks the learner either tries to
   nd a single best action when the task is stationary, or tries to track the best action as it changes over
time when the task is nonstationary. however, in a general id23 task there is more
than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are
best in those situations. to set the stage for the full problem, we brie   y discuss the simplest way in
which nonassociative tasks extend to the associative setting.

as an example, suppose there are several di   erent k-armed bandit tasks, and that on each step you
confront one of these chosen at random. thus, the bandit task changes randomly from step to step.

32

chapter 2. multi-armed bandits

this would appear to you as a single, nonstationary k-armed bandit task whose true action values
change randomly from step to step. you could try using one of the methods described in this chapter
that can handle nonstationarity, but unless the true action values change slowly, these methods will
not work very well. now suppose, however, that when a bandit task is selected for you, you are given
some distinctive clue about its identity (but not its action values). maybe you are facing an actual
slot machine that changes the color of its display as it changes its action values. now you can learn a
policy associating each task, signaled by the color you see, with the best action to take when facing that
task   for instance, if red, select arm 1; if green, select arm 2. with the right policy you can usually
do much better than you could in the absence of any information distinguishing one bandit task from
another.

this is an example of an associative search task, so called because it involves both trial-and-error
learning to search for the best actions, and association of these actions with the situations in which they
are best. associative search tasks are often now called contextual bandits in the literature. associative
search tasks are intermediate between the k-armed bandit problem and the full id23
problem. they are like the full id23 problem in that they involve learning a policy,
but like our version of the k-armed bandit problem in that each action a   ects only the immediate
reward.
if actions are allowed to a   ect the next situation as well as the reward, then we have the
full id23 problem. we present this problem in the next chapter and consider its
rami   cations throughout the rest of the book.

exercise 2.8 suppose you face a 2-armed bandit task whose true action values change randomly from
time step to time step. speci   cally, suppose that, for any time step, the true values of actions 1 and
2 are respectively 0.1 and 0.2 with id203 0.5 (case a), and 0.9 and 0.8 with id203 0.5 (case
b). if you are not able to tell which case you face at any step, what is the best expectation of success
you can achieve and how should you behave to achieve it? now suppose that on each step you are told
whether you are facing case a or case b (although you still don   t know the true action values). this
is an associative search task. what is the best expectation of success you can achieve in this task, and
(cid:3)
how should you behave to achieve it?

2.10 summary

we have presented in this chapter several simple ways of balancing exploration and exploitation. the
  -greedy methods choose randomly a small fraction of the time, whereas ucb methods choose deter-
ministically but achieve exploration by subtly favoring at each step the actions that have so far received
fewer samples. gradient bandit algorithms estimate not action values, but action preferences, and favor
the more preferred actions in a graded, probabilistic manner using a soft-max distribution. the simple
expedient of initializing estimates optimistically causes even greedy methods to explore signi   cantly.

it is natural to ask which of these methods is best. although this is a di   cult question to answer
in general, we can certainly run them all on the 10-armed testbed that we have used throughout this
chapter and compare their performances. a complication is that they all have a parameter; to get a
meaningful comparison we have to consider their performance as a function of their parameter. our
graphs so far have shown the course of learning over time for each algorithm and parameter setting, to
produce a learning curve for that algorithm and parameter setting. if we plotted learning curves for
all algorithms and all parameter settings, then the graph would be too complex and crowded to make
clear comparisons. instead we summarize a complete learning curve by its average value over the 1000
steps; this value is proportional to the area under the learning curve. figure 2.6 shows this measure
for the various bandit algorithms from this chapter, each as a function of its own parameter shown on
a single scale on the x-axis. this kind of graph is called a parameter study. note that the parameter
values are varied by factors of two and presented on a log scale. note also the characteristic inverted-u
shapes of each algorithm   s performance; all the algorithms perform best at an intermediate value of

2.10. summary

33

figure 2.6: a parameter study of the various bandit algorithms presented in this chapter. each point is the
average reward obtained over 1000 steps with a particular algorithm at a particular setting of its parameter.

their parameter, neither too large nor too small. in assessing a method, we should attend not just to
how well it does at its best parameter setting, but also to how sensitive it is to its parameter value. all
of these algorithms are fairly insensitive, performing well over a range of parameter values varying by
about an order of magnitude. overall, on this problem, ucb seems to perform best.

despite their simplicity, in our opinion the methods presented in this chapter can fairly be considered
the state of the art. there are more sophisticated methods, but their complexity and assumptions make
them impractical for the full id23 problem that is our real focus. starting in chapter 5
we present learning methods for solving the full id23 problem that use in part the
simple methods explored in this chapter.

although the simple methods explored in this chapter may be the best we can do at present, they

are far from a fully satisfactory solution to the problem of balancing exploration and exploitation.

one well-studied approach to balancing exploration and exploitation in k-armed bandit problems is
to compute special functions called gittins indices. these provide an optimal solution to a certain kind
of bandit problem more general than that considered here, but this approach assumes that the prior
distribution of possible problems is known. unfortunately, neither the theory nor the computational
tractability of this method appear to generalize to the full id23 problem that we
consider in the rest of the book.

bayesian methods assume a known initial distribution over the action values and then update the
distribution exactly after each step (assuming that the true action values are stationary). in general,
the update computations can be very complex, but for certain special distributions (called conjugate
priors) they are easy. one possibility is to then select actions at each step according to their posterior
id203 of being the best action. this method, sometimes called posterior sampling or thompson
sampling, often performs similarly to the best of the distribution-free methods we have presented in
this chapter.

in the bayesian setting it is even conceivable to compute the optimal balance between exploration
and exploitation. one can compute for any possible action the id203 of each possible immediate
reward and the resultant posterior distributions over action values. this evolving distribution becomes
the information state of the problem. given a horizon, say of 1000 steps, one can consider all possible
actions, all possible resulting rewards, all possible next actions, all next rewards, and so on for all 1000
steps. given the assumptions, the rewards and probabilities of each possible chain of events can be
determined, and one need only pick the best. but the tree of possibilities grows extremely rapidly; even

averagerewardover    rst 1000 steps1.51.41.31.21.11 -greedyucbgradientbanditgreedy withoptimisticinitialization   = 0.11241/21/41/81/161/321/641/128",   ,c,q034

chapter 2. multi-armed bandits

if there were only two actions and two rewards, the tree would have 22000 leaves. it is generally not
feasible to perform this immense computation exactly, but perhaps it could be approximated e   ciently.
this approach would e   ectively turn the bandit problem into an instance of the full reinforcement
learning problem. in the end, we may be able to use approximate id23 methods such
as those presented in part ii of this book to approach this optimal solution. but that is a topic for
research and beyond the scope of this introductory book.

exercise 2.9 (programming) make a    gure analogous to figure 2.6 for the non-stationary case
outlined in exercise 2.5. include the constant-step-size   -greedy algorithm with    = 0.1. use runs of
200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average
(cid:3)
reward over the last 100,000 steps.

bibliographical and historical remarks

2.1

bandit problems have been studied in statistics, engineering, and psychology. in statistics, ban-
dit problems fall under the heading    sequential design of experiments,    introduced by thomp-
son (1933, 1934) and robbins (1952), and studied by bellman (1956). berry and fristedt (1985)
provide an extensive treatment of bandit problems from the perspective of statistics. narendra
and thathachar (1989) treat bandit problems from the engineering perspective, providing a
good discussion of the various theoretical traditions that have focused on them. in psychology,
bandit problems have played roles in statistical learning theory (e.g., bush and mosteller, 1955;
estes, 1950).

the term greedy is often used in the heuristic search literature (e.g., pearl, 1984). the con   ict
between exploration and exploitation is known in control engineering as the con   ict between
identi   cation (or estimation) and control (e.g., witten, 1976). feldbaum (1965) called it the
dual control problem, referring to the need to solve the two problems of identi   cation and
control simultaneously when trying to control a system under uncertainty. in discussing aspects
of id107, holland (1975) emphasized the importance of this con   ict, referring to
it as the con   ict between the need to exploit and the need for new information.

2.2

action-value methods for our k-armed bandit problem were    rst proposed by thathachar and
sastry (1985). these are often called estimator algorithms in the learning automata literature.
the term action value is due to watkins (1989). the    rst to use   -greedy methods may also
have been watkins (1989, p. 187), but the idea is so simple that some earlier use seems likely.

2.4   5 this material falls under the general heading of stochastic iterative algorithms, which is well

covered by bertsekas and tsitsiklis (1996).

2.6

optimistic initialization was used in id23 by sutton (1996).

2.7

2.8

early work on using estimates of the upper con   dence bound to select actions was done by lai
and robbins (1985), kaelbling (1993b), and agrawal (1995). the ucb algorithm we present
here is called ucb1 in the literature and was    rst developed by auer, cesa-bianchi and fischer
(2002).

gradient bandit algorithms are a special case of the gradient-based id23
algorithms introduced by williams (1992), and that later developed into the actor   critic and
policy-gradient algorithms that we treat later in this book. our development here was in   uenced
by that by balaraman ravindran (personal communication). further discussion of the choice

2.10. summary

35

2.9

2.10

of baseline is provided there and by greensmith, bartlett, and baxter (2001, 2004) and dick
(2015).

the term soft-max for the action selection rule (2.9) is due to bridle (1990). this rule appears
to have been    rst proposed by luce (1959).

the term associative search and the corresponding problem were introduced by barto, sutton,
and brouwer (1981). the term associative id23 has also been used for asso-
ciative search (barto and anandan, 1985), but we prefer to reserve that term as a synonym for
the full id23 problem (as in sutton, 1984). (and, as we noted, the modern
literature also uses the term    contextual bandits    for this problem.) we note that thorndike   s
law of e   ect (quoted in chapter 1) describes associative search by referring to the formation
of associative links between situations (states) and actions. according to the terminology of
operant, or instrumental, conditioning (e.g., skinner, 1938), a discriminative stimulus is a stim-
ulus that signals the presence of a particular reinforcement contingency. in our terms, di   erent
discriminative stimuli correspond to di   erent states.

bellman (1956) was the    rst to show how id145 could be used to compute
the optimal balance between exploration and exploitation within a bayesian formulation of
the problem. the gittins index approach is due to gittins and jones (1974). du    (1995)
showed how it is possible to learn gittins indices for bandit problems through reinforcement
learning. the survey by kumar (1985) provides a good discussion of bayesian and non-bayesian
approaches to these problems. the term information state comes from the literature on partially
observable mdps; see, e.g., lovejoy (1991).

other theoretical research focuses on the e   ciency of exploration, usually expressed as how
quickly an algorithm can approach an optimal policy. one way to formalize exploration e   -
ciency is by adapting to id23 the notion of sample complexity for a supervised
learning algorithm, which is the number of training examples the algorithm needs to attain a
desired degree of accuracy in learning the target function. a de   nition of the sample complex-
ity of exploration for a id23 algorithm is the number of time steps in which
the algorithm does not select near-optimal actions (kakade, 2003). li (2012) discusses this
and several other approaches in a survey of theoretical approaches to exploration e   ciency in
id23.

36

chapter 2. multi-armed bandits

chapter 3

finite id100

in this chapter we introduce the formal problem of    nite id100, or    nite mdps,
which we try to solve in the rest of the book. this problem involves evaluative feedback, as in bandits,
but also an associative aspect   choosing di   erent actions in di   erent situations. mdps are a classical
formalization of sequential decision making, where actions in   uence not just immediate rewards, but also
subsequent situations, or states, and through those future rewards. thus mdps involve delayed reward
and the need to tradeo    immediate and delayed reward. whereas in bandit problems we estimated
the value q   (a) of each action a, in mdps we estimate the value q   (s, a) of each action a in each state
s, or we estimate the value v   (s) of each state given optimal action selections. these state-dependent
quantities are essential to accurately assigning credit for long-term consequences to individual action
selections .

mdps are a mathematically idealized form of the id23 problem for which precise
theoretical statements can be made. we introduce key elements of the problem   s mathematical struc-
ture, such as returns, value functions, and bellman equations. we try to convey the wide range of
applications that can be formulated as    nite mdps. as in all of arti   cial intelligence, there is a tension
between breadth of applicability and mathematical tractability. in this chapter we introduce this ten-
sion and discuss some of the trade-o   s and challenges that it implies. some ways in which reinforcement
learning can be taken beyond mdps are treated in chapter 17.

3.1 the agent   environment interface

mdps are meant to be a straightforward framing of the problem of learning from interaction to achieve
a goal. the learner and decision maker is called the agent. the thing it interacts with, comprising
everything outside the agent, is called the environment. these interact continually, the agent selecting
actions and the environment responding to these actions and presenting new situations to the agent.1
the environment also gives rise to rewards, special numerical values that the agent seeks to maximize
over time through its choice of actions. see figure 3.1.

more speci   cally, the agent and environment interact at each of a sequence of discrete time steps,
t = 0, 1, 2, 3, . . ..2 at each time step t, the agent receives some representation of the environment   s state,
st     s, and on that basis selects an action, at     a(s).3 one time step later, in part as a consequence of
1we use the terms agent, environment, and action instead of the engineers    terms controller, controlled system (or

plant), and control signal because they are meaningful to a wider audience.

2we restrict attention to discrete time to keep things as simple as possible, even though many of the ideas can be

extended to the continuous-time case (e.g., see bertsekas and tsitsiklis, 1996; doya, 1996).

3to simplify notation, we sometimes assume the special case in which the action set is the same in all states and write

37

38

chapter 3. finite id100

figure 3.1: the agent   environment interaction in a markov decision process.

its action, the agent receives a numerical reward , rt+1     r     r, and    nds itself in a new state, st+1.4
the mdp and agent together thereby give rise to a sequence or trajectory that begins like this:

s0, a0, r1, s1, a1, r2, s2, a2, r3, . . .

(3.1)

in a    nite mdp, the sets of states, actions, and rewards (s, a, and r) all have a    nite number of
elements. in this case, the random variables rt and st have well de   ned discrete id203 distribu-
tions dependent only on the preceding state and action. that is, for particular values of these random
variables, s(cid:48)     s and r     r, there is a id203 of those values occurring at time t, given particular
values of the preceding state and action:

p(s(cid:48), r|s, a)

.
= pr{st = s(cid:48), rt = r | st   1 = s, at   1 = a},

(3.2)
for all s(cid:48), s     s, r     r, and a     a(s). the dot over the equals sign in this equation reminds us that it
is a de   nition (in this case of the function p) rather than a fact that follows from previous de   nitions.
the function p : s    r    s    a     [0, 1] is an ordinary deterministic function of four arguments. the    |   
in the middle of it comes from the notation for id155, but here it just reminds us that
p speci   es a id203 distribution for each choice of s and a, that is, that

(cid:88)s(cid:48)   s(cid:88)r   r

p(s(cid:48), r|s, a) = 1, for all s     s, a     a(s).

(3.3)

the probabilities given by the four-argument function p completely characterize the dynamics of a
   nite mdp. from it, one can compute anything else one might want to know about the environment,
such as the state-transition probabilities (which we denote, with a slight abuse of notation, as a three-
argument function p : s    s    a     [0, 1]),

p(s(cid:48)|s, a)

.

= pr{st = s(cid:48) | st   1 = s, at   1 = a} = (cid:88)r   r

p(s(cid:48), r|s, a).

(3.4)

we can also compute the expected rewards for state   action pairs as a two-argument function r : s  a    
r:

r(s, a)

.

= e[rt | st   1 = s, at   1 = a] = (cid:88)r   r

r(cid:88)s(cid:48)   s

p(s(cid:48), r|s, a),

(3.5)

or the expected rewards for state   action   next-state triples as a three-argument function r : s   a   s    
r,

r(s, a, s(cid:48))

.

= e[rt | st   1 = s, at   1 = a, st = s(cid:48)] = (cid:88)r   r

r

p(s(cid:48), r|s, a)
p(s(cid:48)|s, a)

.

(3.6)

it simply as a.

4we use rt+1 instead of rt to denote the reward due to at because it emphasizes that the next reward and next

state, rt+1 and st+1, are jointly determined. unfortunately, both conventions are widely used in the literature.

agentenvironmentactionatrewardrtstatestrt+1st+13.1. the agent   environment interface

39

in this book, we usually use the four-argument p function (3.2), but each of these other notations are
occasionally convenient.

the mdp framework is abstract and    exible and can be applied to many di   erent problems in many
di   erent ways. for example, the time steps need not refer to    xed intervals of real time; they can refer
to arbitrary successive stages of decision making and acting. the actions can be low-level controls, such
as the voltages applied to the motors of a robot arm, or high-level decisions, such as whether or not to
have lunch or to go to graduate school. similarly, the states can take a wide variety of forms. they can
be completely determined by low-level sensations, such as direct sensor readings, or they can be more
high-level and abstract, such as symbolic descriptions of objects in a room. some of what makes up a
state could be based on memory of past sensations or even be entirely mental or subjective. for example,
an agent could be in the state of not being sure where an object is, or of having just been surprised
in some clearly de   ned sense. similarly, some actions might be totally mental or computational. for
example, some actions might control what an agent chooses to think about, or where it focuses its
attention. in general, actions can be any decisions we want to learn how to make, and the states can
be anything we can know that might be useful in making them.

in particular, the boundary between agent and environment is typically not the same as the physical
boundary of robot   s or animal   s body. usually, the boundary is drawn closer to the agent than that.
for example, the motors and mechanical linkages of a robot and its sensing hardware should usually
be considered parts of the environment rather than parts of the agent. similarly, if we apply the mdp
framework to a person or animal, the muscles, skeleton, and sensory organs should be considered part
of the environment. rewards, too, presumably are computed inside the physical bodies of natural and
arti   cial learning systems, but are considered external to the agent.

the general rule we follow is that anything that cannot be changed arbitrarily by the agent is
considered to be outside of it and thus part of its environment. we do not assume that everything in
the environment is unknown to the agent. for example, the agent often knows quite a bit about how
its rewards are computed as a function of its actions and the states in which they are taken. but we
always consider the reward computation to be external to the agent because it de   nes the task facing
the agent and thus must be beyond its ability to change arbitrarily. in fact, in some cases the agent may
know everything about how its environment works and still face a di   cult id23 task,
just as we may know exactly how a puzzle like rubik   s cube works, but still be unable to solve it. the
agent   environment boundary represents the limit of the agent   s absolute control, not of its knowledge.

the agent   environment boundary can be located at di   erent places for di   erent purposes.

in a
complicated robot, many di   erent agents may be operating at once, each with its own boundary. for
example, one agent may make high-level decisions which form part of the states faced by a lower-
level agent that implements the high-level decisions. in practice, the agent   environment boundary is
determined once one has selected particular states, actions, and rewards, and thus has identi   ed a
speci   c decision making task of interest.

the mdp framework is a considerable abstraction of the problem of goal-directed learning from
interaction. it proposes that whatever the details of the sensory, memory, and control apparatus, and
whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be
reduced to three signals passing back and forth between an agent and its environment: one signal to
represent the choices made by the agent (the actions), one signal to represent the basis on which the
choices are made (the states), and one signal to de   ne the agent   s goal (the rewards). this framework
may not be su   cient to represent all decision-learning problems usefully, but it has proved to be widely
useful and applicable.

of course, the particular states and actions vary greatly from task to task, and how they are repre-
sented can strongly a   ect performance. in id23, as in other kinds of learning, such
representational choices are at present more art than science. in this book we o   er some advice and
examples regarding good ways of representing states and actions, but our primary focus is on general

40

chapter 3. finite id100

principles for learning how to behave once the representations have been selected.

example 3.1: bioreactor suppose id23 is being applied to determine moment-by-
moment temperatures and stirring rates for a bioreactor (a large vat of nutrients and bacteria used to
produce useful chemicals). the actions in such an application might be target temperatures and target
stirring rates that are passed to lower-level control systems that, in turn, directly activate heating
elements and motors to attain the targets. the states are likely to be thermocouple and other sensory
readings, perhaps    ltered and delayed, plus symbolic inputs representing the ingredients in the vat
and the target chemical. the rewards might be moment-by-moment measures of the rate at which
the useful chemical is produced by the bioreactor. notice that here each state is a list, or vector, of
sensor readings and symbolic inputs, and each action is a vector consisting of a target temperature
and a stirring rate. it is typical of id23 tasks to have states and actions with such
structured representations. rewards, on the other hand, are always single numbers.

example 3.2: pick-and-place robot consider using id23 to control the motion
of a robot arm in a repetitive pick-and-place task. if we want to learn movements that are fast and
smooth, the learning agent will have to control the motors directly and have low-latency information
about the current positions and velocities of the mechanical linkages. the actions in this case might
be the voltages applied to each motor at each joint, and the states might be the latest readings of joint
angles and velocities. the reward might be +1 for each object successfully picked up and placed. to
encourage smooth movements, on each time step a small, negative reward can be given as a function of
the moment-to-moment    jerkiness    of the motion.

example 3.3: recycling robot a mobile robot has the job of collecting empty soda cans in
an o   ce environment. it has sensors for detecting cans, and an arm and gripper that can pick them
up and place them in an onboard bin; it runs on a rechargeable battery. the robot   s control system
has components for interpreting sensory information, for navigating, and for controlling the arm and
gripper. high-level decisions about how to search for cans are made by a id23 agent
based on the current charge level of the battery. this agent has to decide whether the robot should (1)
actively search for a can for a certain period of time, (2) remain stationary and wait for someone to
bring it a can, or (3) head back to its home base to recharge its battery. this decision has to be made
either periodically or whenever certain events occur, such as    nding an empty can. the agent therefore
has three actions, and the state is primarily determined by the state of the battery. the rewards might
be zero most of the time, but then become positive when the robot secures an empty can, or large and
negative if the battery runs all the way down. in this example, the id23 agent is not
the entire robot. the states it monitors describe conditions within the robot itself, not conditions of the
robot   s external environment. the agent   s environment therefore includes the rest of the robot, which
might contain other complex decision-making systems, as well as the robot   s external environment.

exercise 3.1 devise three example tasks of your own that    t into the mdp framework, identifying for
each its states, actions, and rewards. make the three examples as di   erent from each other as possible.
the framework is abstract and    exible and can be applied in many di   erent ways. stretch its limits in
(cid:3)
some way in at least one of your examples.
exercise 3.2 is the mdp framework adequate to usefully represent all goal-directed learning tasks?
(cid:3)
can you think of any clear exceptions?
exercise 3.3 consider the problem of driving. you could de   ne the actions in terms of the accelerator,
steering wheel, and brake, that is, where your body meets the machine. or you could de   ne them farther
out   say, where the rubber meets the road, considering your actions to be tire torques. or you could
de   ne them farther in   say, where your brain meets your body, the actions being muscle twitches to
control your limbs. or you could go to a really high level and say that your actions are your choices of
where to drive. what is the right level, the right place to draw the line between agent and environment?

3.1. the agent   environment interface

41

on what basis is one location of the line to be preferred over another? is there any fundamental reason
(cid:3)
for preferring one location over another, or is it a free choice?

exercise 3.4 if the current state is st, and actions are selected according to stochastic policy   , then
(cid:3)
what is the expectation of rt+1 in terms of the four-argument function p (3.2)?

example 3.4: recycling robot mdp the recycling robot (example 3.3) can be turned into a
simple example of an mdp by simplifying it and providing some more details. (our aim is to produce
a simple example, not a particularly realistic one.) recall that the agent makes a decision at times
determined by external events (or by other parts of the robot   s control system). at each such time the
robot decides whether it should (1) actively search for a can, (2) remain stationary and wait for someone
to bring it a can, or (3) go back to home base to recharge its battery. suppose the environment works as
follows. the best way to    nd cans is to actively search for them, but this runs down the robot   s battery,
whereas waiting does not. whenever the robot is searching, the possibility exists that its battery will
become depleted.
in this case the robot must shut down and wait to be rescued (producing a low
reward).

the agent makes its decisions solely as a function of the energy level of the battery. it can distinguish
two levels, high and low, so that the state set is s = {high, low}. let us call the possible decisions   the
agent   s actions   wait, search, and recharge. when the energy level is high, recharging would always
be foolish, so we do not include it in the action set for this state. the agent   s action sets are

a(high)

a(low)

.
= {search, wait}
.
= {search, wait, recharge}.

if the energy level is high, then a period of active search can always be completed without risk of
depleting the battery. a period of searching that begins with a high energy level leaves the energy level
high with id203    and reduces it to low with id203 1       . on the other hand, a period
of searching undertaken when the energy level is low leaves it low with id203    and depletes
the battery with id203 1       .
in the latter case, the robot must be rescued, and the battery
is then recharged back to high. each can collected by the robot counts as a unit reward, whereas a
reward of    3 results whenever the robot has to be rescued. let rsearch and rwait, with rsearch > rwait,
respectively denote the expected number of cans the robot will collect (and hence the expected reward)
while searching and while waiting. finally, to keep things simple, suppose that no cans can be collected
during a run home for recharging, and that no cans can be collected on a step in which the battery is
depleted. this system is then a    nite mdp, and we can write down the transition probabilities and the
expected rewards, as in table 3.1.

a transition graph is a useful way to summarize the dynamics of a    nite mdp. figure 3.2 shows the
transition graph for the recycling robot example. there are two kinds of nodes: state nodes and action
nodes. there is a state node for each possible state (a large open circle labeled by the name of the
state), and an action node for each state   action pair (a small solid circle labeled by the name of the
action and connected by a line to the state node). starting in state s and taking action a moves you
along the line from state node s to action node (s, a). then the environment responds with a transition
to the next state   s node via one of the arrows leaving action node (s, a). each arrow corresponds to
a triple (s, s(cid:48), a), where s(cid:48) is the next state, and we label the arrow with the transition id203,
p(s(cid:48)|s, a), and the expected reward for that transition, r(s, a, s(cid:48)). note that the transition probabilities
labeling the arrows leaving an action node always sum to 1.

exercise 3.5 give a table analogous to to table 3.1, but for p(s(cid:48), r|s, a). it should have columns for
(cid:3)
s, a, s(cid:48), r, and p(s(cid:48), r|s, a), and a row for every 4-tuple for which p(s(cid:48), r|s, a) > 0.

42

chapter 3. finite id100

s
high
high
low
low
high
high
low
low
low
low

a
search
search
search
search
wait
wait
wait
wait
recharge
recharge

s(cid:48)
high
low
high
low
high
low
high
low
high
low

p(s(cid:48)|s, a)
  
1       
1       
  
1
0
0
1
1
0

r(s, a, s(cid:48))
rsearch
rsearch
   3
rsearch
rwait
rwait
rwait
rwait
0
0.

table 3.1: transition probabilities and expected rewards for the    nite mdp of the recycling robot example.
there is a row for each possible combination of current state, s, next state, s(cid:48), and action possible in the current
state, a     a(s).

figure 3.2: transition graph for the recycling robot example.

3.2 goals and rewards

in id23, the purpose or goal of the agent is formalized in terms of a special signal,
called the reward, passing from the environment to the agent. at each time step, the reward is a simple
number, rt     r. informally, the agent   s goal is to maximize the total amount of reward it receives.
this means maximizing not immediate reward, but cumulative reward in the long run. we can clearly
state this informal idea as the reward hypothesis:

that all of what we mean by goals and purposes can be well thought of as the maximization
of the expected value of the cumulative sum of a received scalar signal (called reward).

the use of a reward signal to formalize the idea of a goal is one of the most distinctive features of
id23.

although formulating goals in terms of reward signals might at    rst appear limiting, in practice it
has proved to be    exible and widely applicable. the best way to see this is to consider examples of how
it has been, or could be, used. for example, to make a robot learn to walk, researchers have provided
reward on each time step proportional to the robot   s forward motion. in making a robot learn how
to escape from a maze, the reward is often    1 for every time step that passes prior to escape; this
encourages the agent to escape as quickly as possible. to make a robot learn to    nd and collect empty
soda cans for recycling, one might give it a reward of zero most of the time, and then a reward of +1 for

searchhighlow1,  0 1   ! ,      3searchrechargewaitwaitsearch1   " ,  r! ,  r search", rsearch1,  r wait1,  r wait3.6.markovdecisionprocesses59ss0ap(s0|s,a)r(s,a,s0)highhighsearch   rsearchhighlowsearch1    rsearchlowhighsearch1   3lowlowsearch rsearchhighhighwait1rwaithighlowwait0rwaitlowhighwait0rwaitlowlowwait1rwaitlowhighrecharge10lowlowrecharge00.table3.1:transitionprobabilitiesandexpectedrewardsforthe   nitemdpoftherecyclingrobotexample.thereisarowforeachpossiblecombinationofcurrentstate,s,nextstate,s0,andactionpossibleinthecurrentstate,a2a(s).iss={high,low}.letuscallthepossibledecisions   theagent   sactions   wait,search,andrecharge.whentheenergylevelishigh,rechargingwouldalwaysbefoolish,sowedonotincludeitintheactionsetforthisstate.theagent   sactionsetsarea(high)={search,wait}a(low)={search,wait,recharge}.iftheenergylevelishigh,thenaperiodofactivesearchcanalwaysbecompletedwithoutriskofdepletingthebattery.aperiodofsearchingthatbeginswithahighenergylevelleavestheenergylevelhighwithid203   andreducesittolowwithid2031    .ontheotherhand,aperiodofsearchingundertakenwhentheenergylevelislowleavesitlowwithid203 anddepletesthebatterywithid2031  .inthelattercase,therobotmustberescued,andthebatteryisthenrechargedbacktohigh.eachcancollectedbytherobotcountsasaunitreward,whereasarewardof 3resultswhenevertherobothastoberescued.letrsearchandrwait,withrsearch>rwait,respectivelydenotetheexpectednumberofcanstherobotwillcollect(andhencetheexpectedreward)whilesearchingandwhilewaiting.finally,tokeepthingssimple,supposethatnocanscanbecollectedduringarunhomeforrecharging,andthatnocanscanbecollectedonastepinwhichthebatteryisdepleted.thissystemisthena   nitemdp,andwecanwritedownthetransitionprobabilitiesandtheexpectedrewards,asintable3.1.atransitiongraphisausefulwaytosummarizethedynamicsofa   nite3.6.markovdecisionprocesses59ss0ap(s0|s,a)r(s,a,s0)highhighsearch   rsearchhighlowsearch1    rsearchlowhighsearch1   3lowlowsearch rsearchhighhighwait1rwaithighlowwait0rwaitlowhighwait0rwaitlowlowwait1rwaitlowhighrecharge10lowlowrecharge00.table3.1:transitionprobabilitiesandexpectedrewardsforthe   nitemdpoftherecyclingrobotexample.thereisarowforeachpossiblecombinationofcurrentstate,s,nextstate,s0,andactionpossibleinthecurrentstate,a2a(s).iss={high,low}.letuscallthepossibledecisions   theagent   sactions   wait,search,andrecharge.whentheenergylevelishigh,rechargingwouldalwaysbefoolish,sowedonotincludeitintheactionsetforthisstate.theagent   sactionsetsarea(high)={search,wait}a(low)={search,wait,recharge}.iftheenergylevelishigh,thenaperiodofactivesearchcanalwaysbecompletedwithoutriskofdepletingthebattery.aperiodofsearchingthatbeginswithahighenergylevelleavestheenergylevelhighwithid203   andreducesittolowwithid2031    .ontheotherhand,aperiodofsearchingundertakenwhentheenergylevelislowleavesitlowwithid203 anddepletesthebatterywithid2031  .inthelattercase,therobotmustberescued,andthebatteryisthenrechargedbacktohigh.eachcancollectedbytherobotcountsasaunitreward,whereasarewardof 3resultswhenevertherobothastoberescued.letrsearchandrwait,withrsearch>rwait,respectivelydenotetheexpectednumberofcanstherobotwillcollect(andhencetheexpectedreward)whilesearchingandwhilewaiting.finally,tokeepthingssimple,supposethatnocanscanbecollectedduringarunhomeforrecharging,andthatnocanscanbecollectedonastepinwhichthebatteryisdepleted.thissystemisthena   nitemdp,andwecanwritedownthetransitionprobabilitiesandtheexpectedrewards,asintable3.1.atransitiongraphisausefulwaytosummarizethedynamicsofa   nite3.6.markovdecisionprocesses59ss0ap(s0|s,a)r(s,a,s0)highhighsearch   rsearchhighlowsearch1    rsearchlowhighsearch1   3lowlowsearch rsearchhighhighwait1rwaithighlowwait0rwaitlowhighwait0rwaitlowlowwait1rwaitlowhighrecharge10lowlowrecharge00.table3.1:transitionprobabilitiesandexpectedrewardsforthe   nitemdpoftherecyclingrobotexample.thereisarowforeachpossiblecombinationofcurrentstate,s,nextstate,s0,andactionpossibleinthecurrentstate,a2a(s).iss={high,low}.letuscallthepossibledecisions   theagent   sactions   wait,search,andrecharge.whentheenergylevelishigh,rechargingwouldalwaysbefoolish,sowedonotincludeitintheactionsetforthisstate.theagent   sactionsetsarea(high)={search,wait}a(low)={search,wait,recharge}.iftheenergylevelishigh,thenaperiodofactivesearchcanalwaysbecompletedwithoutriskofdepletingthebattery.aperiodofsearchingthatbeginswithahighenergylevelleavestheenergylevelhighwithid203   andreducesittolowwithid2031    .ontheotherhand,aperiodofsearchingundertakenwhentheenergylevelislowleavesitlowwithid203 anddepletesthebatterywithid2031  .inthelattercase,therobotmustberescued,andthebatteryisthenrechargedbacktohigh.eachcancollectedbytherobotcountsasaunitreward,whereasarewardof 3resultswhenevertherobothastoberescued.letrsearchandrwait,withrsearch>rwait,respectivelydenotetheexpectednumberofcanstherobotwillcollect(andhencetheexpectedreward)whilesearchingandwhilewaiting.finally,tokeepthingssimple,supposethatnocanscanbecollectedduringarunhomeforrecharging,andthatnocanscanbecollectedonastepinwhichthebatteryisdepleted.thissystemisthena   nitemdp,andwecanwritedownthetransitionprobabilitiesandtheexpectedrewards,asintable3.1.atransitiongraphisausefulwaytosummarizethedynamicsofa   nite3.6.markovdecisionprocesses59ss0ap(s0|s,a)r(s,a,s0)highhighsearch   rsearchhighlowsearch1    rsearchlowhighsearch1   3lowlowsearch rsearchhighhighwait1rwaithighlowwait0rwaitlowhighwait0rwaitlowlowwait1rwaitlowhighrecharge10lowlowrecharge00.table3.1:transitionprobabilitiesandexpectedrewardsforthe   nitemdpoftherecyclingrobotexample.thereisarowforeachpossiblecombinationofcurrentstate,s,nextstate,s0,andactionpossibleinthecurrentstate,a2a(s).iss={high,low}.letuscallthepossibledecisions   theagent   sactions   wait,search,andrecharge.whentheenergylevelishigh,rechargingwouldalwaysbefoolish,sowedonotincludeitintheactionsetforthisstate.theagent   sactionsetsarea(high)={search,wait}a(low)={search,wait,recharge}.iftheenergylevelishigh,thenaperiodofactivesearchcanalwaysbecompletedwithoutriskofdepletingthebattery.aperiodofsearchingthatbeginswithahighenergylevelleavestheenergylevelhighwithid203   andreducesittolowwithid2031    .ontheotherhand,aperiodofsearchingundertakenwhentheenergylevelislowleavesitlowwithid203 anddepletesthebatterywithid2031  .inthelattercase,therobotmustberescued,andthebatteryisthenrechargedbacktohigh.eachcancollectedbytherobotcountsasaunitreward,whereasarewardof 3resultswhenevertherobothastoberescued.letrsearchandrwait,withrsearch>rwait,respectivelydenotetheexpectednumberofcanstherobotwillcollect(andhencetheexpectedreward)whilesearchingandwhilewaiting.finally,tokeepthingssimple,supposethatnocanscanbecollectedduringarunhomeforrecharging,andthatnocanscanbecollectedonastepinwhichthebatteryisdepleted.thissystemisthena   nitemdp,andwecanwritedownthetransitionprobabilitiesandtheexpectedrewards,asintable3.1.atransitiongraphisausefulwaytosummarizethedynamicsofa   nite3.6.markovdecisionprocesses59ss0ap(s0|s,a)r(s,a,s0)highhighsearch   rsearchhighlowsearch1    rsearchlowhighsearch1   3lowlowsearch rsearchhighhighwait1rwaithighlowwait0rwaitlowhighwait0rwaitlowlowwait1rwaitlowhighrecharge10lowlowrecharge00.table3.1:transitionprobabilitiesandexpectedrewardsforthe   nitemdpoftherecyclingrobotexample.thereisarowforeachpossiblecombinationofcurrentstate,s,nextstate,s0,andactionpossibleinthecurrentstate,a2a(s).iss={high,low}.letuscallthepossibledecisions   theagent   sactions   wait,search,andrecharge.whentheenergylevelishigh,rechargingwouldalwaysbefoolish,sowedonotincludeitintheactionsetforthisstate.theagent   sactionsetsarea(high)={search,wait}a(low)={search,wait,recharge}.iftheenergylevelishigh,thenaperiodofactivesearchcanalwaysbecompletedwithoutriskofdepletingthebattery.aperiodofsearchingthatbeginswithahighenergylevelleavestheenergylevelhighwithid203   andreducesittolowwithid2031    .ontheotherhand,aperiodofsearchingundertakenwhentheenergylevelislowleavesitlowwithid203 anddepletesthebatterywithid2031  .inthelattercase,therobotmustberescued,andthebatteryisthenrechargedbacktohigh.eachcancollectedbytherobotcountsasaunitreward,whereasarewardof 3resultswhenevertherobothastoberescued.letrsearchandrwait,withrsearch>rwait,respectivelydenotetheexpectednumberofcanstherobotwillcollect(andhencetheexpectedreward)whilesearchingandwhilewaiting.finally,tokeepthingssimple,supposethatnocanscanbecollectedduringarunhomeforrecharging,andthatnocanscanbecollectedonastepinwhichthebatteryisdepleted.thissystemisthena   nitemdp,andwecanwritedownthetransitionprobabilitiesandtheexpectedrewards,asintable3.1.atransitiongraphisausefulwaytosummarizethedynamicsofa   nite3.3. returns and episodes

43

each can collected. one might also want to give the robot negative rewards when it bumps into things
or when somebody yells at it. for an agent to learn to play checkers or chess, the natural rewards are
+1 for winning,    1 for losing, and 0 for drawing and for all nonterminal positions.

you can see what is happening in all of these examples. the agent always learns to maximize its
reward. if we want it to do something for us, we must provide rewards to it in such a way that in
maximizing them the agent will also achieve our goals. it is thus critical that the rewards we set up
truly indicate what we want accomplished. in particular, the reward signal is not the place to impart
to the agent prior knowledge about how to achieve what we want it to do.5 for example, a chess-
playing agent should be rewarded only for actually winning, not for achieving subgoals such as taking
its opponent   s pieces or gaining control of the center of the board. if achieving these sorts of subgoals
were rewarded, then the agent might    nd a way to achieve them without achieving the real goal. for
example, it might    nd a way to take the opponent   s pieces even at the cost of losing the game. the
reward signal is your way of communicating to the robot what you want it to achieve, not how you
want it achieved.6

3.3 returns and episodes

so far we have discussed the objective of learning informally. we have said that the agent   s goal is to
maximize the cumulative reward it receives in the long run. how might this be de   ned formally? if the
sequence of rewards received after time step t is denoted rt+1, rt+2, rt+3, . . ., then what precise aspect
of this sequence do we wish to maximize? in general, we seek to maximize the expected return, where
the return, denoted gt, is de   ned as some speci   c function of the reward sequence. in the simplest case
the return is the sum of the rewards:

gt

.
= rt+1 + rt+2 + rt+3 +        + rt ,

(3.7)

where t is a    nal time step. this approach makes sense in applications in which there is a natural notion
of    nal time step, that is, when the agent   environment interaction breaks naturally into subsequences,
which we call episodes,7 such as plays of a game, trips through a maze, or any sort of repeated interaction.
each episode ends in a special state called the terminal state, followed by a reset to a standard starting
state or to a sample from a standard distribution of starting states. even if you think of episodes as
ending in di   erent ways, such as winning and losing a game, the next episode begins independently of
how the previous one ended. thus the episodes can all be considered to end in the same terminal state,
with di   erent rewards for the di   erent outcomes. tasks with episodes of this kind are called episodic
tasks. in episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted s,
from the set of all states plus the terminal state, denoted s+. the time of termination, t , is a random
variable that normally varies from episode to episode.

on the other hand, in many cases the agent   environment interaction does not break naturally into
identi   able episodes, but goes on continually without limit. for example, this would be the natural way
to formulate an on-going process-control task, or an application to a robot with a long life span. we
call these continuing tasks. the return formulation (3.7) is problematic for continuing tasks because
the    nal time step would be t =    , and the return, which is what we are trying to maximize, could
itself easily be in   nite. (for example, suppose the agent receives a reward of +1 at each time step.)
thus, in this book we usually use a de   nition of return that is slightly more complex conceptually but
much simpler mathematically.

5better places for imparting this kind of prior knowledge are the initial policy or initial value function, or in in   uences

on these.

6section 17.4 delves further into the issue of designing e   ective reward signals.
7episodes are sometimes called    trials    in the literature.

44

chapter 3. finite id100

the additional concept that we need is that of discounting. according to this approach, the agent
tries to select actions so that the sum of the discounted rewards it receives over the future is maximized.
in particular, it chooses at to maximize the expected discounted return:

gt

.
= rt+1 +   rt+2 +   2rt+3 +        =

  krt+k+1,

   (cid:88)k=0

(3.8)

where    is a parameter, 0            1, called the discount rate.
the discount rate determines the present value of future rewards: a reward received k time steps in
the future is worth only   k   1 times what it would be worth if it were received immediately. if    < 1,
the in   nite sum in (3.8) has a    nite value as long as the reward sequence {rk} is bounded. if    = 0,
the agent is    myopic    in being concerned only with maximizing immediate rewards: its objective in this
case is to learn how to choose at so as to maximize only rt+1. if each of the agent   s actions happened
to in   uence only the immediate reward, not future rewards as well, then a myopic agent could maximize
(3.8) by separately maximizing each immediate reward. but in general, acting to maximize immediate
reward can reduce access to future rewards so that the return is reduced. as    approaches 1, the return
objective takes future rewards into account more strongly; the agent becomes more farsighted.

example 3.5: pole-balancing the objective in this task is to apply forces to a cart moving along
a track so as to keep a pole hinged to the cart from falling over: a failure is said to occur if the pole

falls past a given angle from vertical or if the cart runs o    the track. the pole is reset to vertical
after each failure. this task could be treated as episodic, where the natural episodes are the repeated
attempts to balance the pole. the reward in this case could be +1 for every time step on which failure
did not occur, so that the return at each time would be the number of steps until failure. in this case,
successful balancing forever would mean a return of in   nity. alternatively, we could treat pole-balancing
as a continuing task, using discounting. in this case the reward would be    1 on each failure and zero
at all other times. the return at each time would then be related to      k, where k is the number of
time steps before failure. in either case, the return is maximized by keeping the pole balanced for as
long as possible.

exercise 3.6 the equations in section 3.1 are for the continuing case and need to be modi   ed (very
slightly) to apply to episodic tasks. show that you know the modi   cations needed by giving the modi   ed
(cid:3)
version of (3.3).
exercise 3.7 suppose you treated pole-balancing as an episodic task but also used discounting, with
all rewards zero except for    1 upon failure. what then would the return be at each time? how does
(cid:3)
this return di   er from that in the discounted, continuing formulation of this task?
exercise 3.8 imagine that you are designing a robot to run a maze. you decide to give it a reward of
+1 for escaping from the maze and a reward of zero at all other times. the task seems to break down
naturally into episodes   the successive runs through the maze   so you decide to treat it as an episodic
task, where the goal is to maximize expected total reward (3.7). after running the learning agent for

3.4. unified notation for episodic and continuing tasks

45

a while, you    nd that it is showing no improvement in escaping from the maze. what is going wrong?
(cid:3)
have you e   ectively communicated to the agent what you want it to achieve?
returns at successive time steps are related to each other in a way that is important for the theory

and algorithms of id23:

gt

.
= rt+1 +   rt+2 +   2rt+3 +   3rt+4 +       
= rt+1 +   (cid:0)rt+2 +   rt+3 +   2rt+4 +       (cid:1)

= rt+1 +   gt+1

(3.9)

note that this works for all time steps t < t , even if termination occurs at t + 1, if we de   ne gt = 0.
this often makes it easy to compute returns from reward sequences.

exercise 3.9 suppose    = 0.5 and the following sequence of rewards is received r1 =    1, r2 = 2,
r3 = 6, r4 = 3, and r5 = 2, with t = 5. what are g0, g1, . . ., g5? hint: work backwards.

note that although the return (3.8) is a sum of an in   nite number of terms, it is still    nite if the
reward is nonzero and constant   if    < 1. for example, if the reward is a constant +1, then the return
is

gt =

   (cid:88)k=0

  k =

1
1       

.

(3.10)

exercise 3.10 suppose    = 0.9 and the reward sequence is r1 = 2 followed by an in   nite sequence of
(cid:3)
7s. what are g1 and g0?
(cid:3)

exercise 3.11 prove (3.10).

3.4 uni   ed notation for episodic and continuing tasks

in the preceding section we described two kinds of id23 tasks, one in which the agent   
environment interaction naturally breaks down into a sequence of separate episodes (episodic tasks),
and one in which it does not (continuing tasks). the former case is mathematically easier because each
action a   ects only the    nite number of rewards subsequently received during the episode. in this book
we consider sometimes one kind of problem and sometimes the other, but often both. it is therefore
useful to establish one notation that enables us to talk precisely about both cases simultaneously.

to be precise about episodic tasks requires some additional notation. rather than one long sequence
of time steps, we need to consider a series of episodes, each of which consists of a    nite sequence of
time steps. we number the time steps of each episode starting anew from zero. therefore, we have to
refer not just to st, the state representation at time t, but to st,i, the state representation at time t of
episode i (and similarly for at,i, rt,i,   t,i, ti, etc.). however, it turns out that when we discuss episodic
tasks we almost never have to distinguish between di   erent episodes. we are almost always considering
a particular single episode, or stating something that is true for all episodes. accordingly, in practice
we almost always abuse notation slightly by dropping the explicit reference to episode number. that
is, we write st to refer to st,i, and so on.

we need one other convention to obtain a single notation that covers both episodic and continuing
tasks. we have de   ned the return as a sum over a    nite number of terms in one case (3.7) and as a
sum over an in   nite number of terms in the other (3.8). these can be uni   ed by considering episode
termination to be the entering of a special absorbing state that transitions only to itself and that

46

chapter 3. finite id100

generates only rewards of zero. for example, consider the state transition diagram:

here the solid square represents the special absorbing state corresponding to the end of an episode.
starting from s0, we get the reward sequence +1, +1, +1, 0, 0, 0, . . .. summing these, we get the same
return whether we sum over the    rst t rewards (here t = 3) or over the full in   nite sequence. this
remains true even if we introduce discounting. thus, we can de   ne the return, in general, according to
(3.8), using the convention of omitting episode numbers when they are not needed, and including the
possibility that    = 1 if the sum remains de   ned (e.g., because all episodes terminate). alternatively,
we can also write the return as

.
=

gt

t(cid:88)k=t+1

  k   t   1rk,

(3.11)

including the possibility that t =     or    = 1 (but not both). we use these conventions throughout the
rest of the book to simplify notation and to express the close parallels between episodic and continuing
tasks. (later, in chapter 10, we will introduce a formulation that is both continuing and undiscounted.)

3.5 policies and value functions

almost all id23 algorithms involve estimating value functions   functions of states (or
of state   action pairs) that estimate how good it is for the agent to be in a given state (or how good
it is to perform a given action in a given state). the notion of    how good    here is de   ned in terms
of future rewards that can be expected, or, to be precise, in terms of expected return. of course the
rewards the agent can expect to receive in the future depend on what actions it will take. accordingly,
value functions are de   ned with respect to particular ways of acting, called policies.

formally, a policy is a mapping from states to probabilities of selecting each possible action. if the
agent is following policy    at time t, then   (a|s) is the id203 that at = a if st = s. like p,   
is an ordinary function; the    |    in the middle of   (a|s) merely reminds that it de   nes a id203
distribution over a     a(s) for each s     s. id23 methods specify how the agent   s
policy is changed as a result of its experience.

the value of a state s under a policy   , denoted v  (s), is the expected return when starting in s and

following    thereafter. for mdps, we can de   ne v   formally by

v  (s)

.

= e  [gt | st = s] = e  (cid:34)    (cid:88)k=0

  krt+k+1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

st = s(cid:35) , for all s     s,

(3.12)

where e  [  ] denotes the expected value of a random variable given that the agent follows policy   , and
t is any time step. note that the value of the terminal state, if any, is always zero. we call the function
v   the state-value function for policy   .

similarly, we de   ne the value of taking action a in state s under a policy   , denoted q  (s, a), as the

expected return starting from s, taking the action a, and thereafter following policy   :

q  (s, a)

.

= e  [gt | st = s, at = a] = e  (cid:34)    (cid:88)k=0

st = s, at = a(cid:35) .

  krt+k+1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(3.13)

r1 = +1s0s1r2 = +1s2r3 = +1r4 = 0r5 = 0. . .3.5. policies and value functions

47

we call q   the action-value function for policy   .

the value functions v   and q   can be estimated from experience. for example, if an agent follows
policy    and maintains an average, for each state encountered, of the actual returns that have followed
that state, then the average will converge to the state   s value, v  (s), as the number of times that state
is encountered approaches in   nity. if separate averages are kept for each action taken in each state,
then these averages will similarly converge to the action values, q  (s, a). we call estimation methods
of this kind monte carlo methods because they involve averaging over many random samples of actual
returns. these kinds of methods are presented in chapter 5. of course, if there are very many states,
then it may not be practical to keep separate averages for each state individually. instead, the agent
would have to maintain v   and q   as parameterized functions (with fewer parameters than states) and
adjust the parameters to better match the observed returns. this can also produce accurate estimates,
although much depends on the nature of the parameterized function approximator. these possibilities
are discussed in part ii of the book.

a fundamental property of value functions used throughout id23 and dynamic pro-
gramming is that they satisfy recursive relationships similar to that which we have already established
for the return (3.9). for any policy    and any state s, the following consistency condition holds between
the value of s and the value of its possible successor states:

v  (s)

.
= e  [gt | st = s]
= e  [rt+1 +   gt+1 | st = s]
=(cid:88)a
  (a|s)(cid:88)s(cid:48) (cid:88)r
=(cid:88)a
  (a|s)(cid:88)s(cid:48),r

p(s(cid:48), r|s, a)(cid:104)r +   e  [gt+1|st+1 = s(cid:48)](cid:105)
p(s(cid:48), r|s, a)(cid:104)r +   v  (s(cid:48))(cid:105),

for all s     s,

(by (3.9))

(3.14)

where it is implicit that the actions, a, are taken from the set a(s), that the next states, s(cid:48), are taken
from the set s (or from s+ in the case of an episodic problem), and that the rewards, r, are taken from
the set r. note also how in the last equation we have merged the two sums, one over all the values of
s(cid:48) and the other over all the values of r, into one sum over all the possible values of both. we use this
kind of merged sum often to simplify formulas. note how the    nal expression can be read easily as an
expected value. it is really a sum over all values of the three variables, a, s(cid:48), and r. for each triple,
we compute its id203,   (a|s)p(s(cid:48), r|s, a), weight the quantity in brackets by that id203, then
sum over all possibilities to get an expected value.

equation (3.14) is the bellman equation for v  . it expresses a relationship between the value of a state

and the values of its successor states. think of looking ahead from a state to
its possible successor states, as suggested by the diagram to the right. each
open circle represents a state and each solid circle represents a state   action
pair. starting from state s, the root node at the top, the agent could take
any of some set of actions   three are shown in the diagram   based on its
policy   . from each of these, the environment could respond with one of
several next states, s(cid:48) (two are shown in the    gure), along with a reward, r,
depending on its dynamics given by the function p. the bellman equation
(3.14) averages over all the possibilities, weighting each by its id203
of occurring.
it states that the value of the start state must equal the
(discounted) value of the expected next state, plus the reward expected along the way.

backup diagram for v  

the value function v   is the unique solution to its bellman equation. we show in subsequent chapters
how this bellman equation forms the basis of a number of ways to compute, approximate, and learn v  .
we call diagrams like that above backup diagrams because they diagram relationships that form the
basis of the update or backup operations that are at the heart of id23 methods. these
operations transfer value information back to a state (or a state   action pair) from its successor states

   ss0   rpa48

chapter 3. finite id100

(or state   action pairs). we use backup diagrams throughout the book to provide graphical summaries
of the algorithms we discuss. (note that, unlike transition graphs, the state nodes of backup diagrams
do not necessarily represent distinct states; for example, a state might be its own successor.)

example 3.6: gridworld figure 3.3 (left) shows a rectangular gridworld representation of a simple
   nite mdp. the cells of the grid correspond to the states of the environment. at each cell, four actions
are possible: north, south, east, and west, which deterministically cause the agent to move one cell
in the respective direction on the grid. actions that would take the agent o    the grid leave its location
unchanged, but also result in a reward of    1. other actions result in a reward of 0, except those that
move the agent out of the special states a and b. from state a, all four actions yield a reward of +10
and take the agent to a(cid:48). from state b, all actions yield a reward of +5 and take the agent to b(cid:48).

figure 3.3: gridworld example: exceptional reward dynamics (left) and state-value function for the equiprob-
able random policy (right).

suppose the agent selects all four actions with equal id203 in all states. figure 3.3 (right)
shows the value function, v  , for this policy, for the discounted reward case with    = 0.9. this value
function was computed by solving the system of linear equations (3.14). notice the negative values near
the lower edge; these are the result of the high id203 of hitting the edge of the grid there under
the random policy. state a is the best state to be in under this policy, but its expected return is less
than 10, its immediate reward, because from a the agent is taken to a(cid:48), from which it is likely to run
into the edge of the grid. state b, on the other hand, is valued more than 5, its immediate reward,
because from b the agent is taken to b(cid:48), which has a positive value. from b(cid:48) the expected penalty
(negative reward) for possibly running into an edge is more than compensated for by the expected gain
for possibly stumbling onto a or b.

exercise 3.12 the bellman equation (3.14) must hold for each state for the value function v   shown
in figure 3.3 (right) of example 3.6. show numerically that this equation holds for the center state,
valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4,    0.4, and +0.7. (these
(cid:3)
numbers are accurate only to one decimal place.)

exercise 3.13 what is the bellman equation for action values, that is, for q  ? it must
give the action value q  (s, a) in terms of the action values, q  (s(cid:48), a(cid:48)), of possible succes-
sors to the state   action pair (s, a). hint: the backup diagram to the right corresponds
to this equation. show the sequence of equations analogous to (3.14), but for action
(cid:3)
values.
example 3.7: golf to formulate playing a hole of golf as a id23
task, we count a penalty (negative reward) of    1 for each stroke until we hit the ball
into the hole. the state is the location of the ball. the value of a state is the negative
of the number of strokes to the hole from that location. our actions are how we aim and swing at the
ball, of course, and which club we select. let us take the former as given and consider just the choice
of club, which we assume is either a putter or a driver. the upper part of figure 3.4 shows a possible
state-value function, vputt(s), for the policy that always uses the putter. the terminal state in-the-hole
has a value of 0. from anywhere on the green we assume we can make a putt; these states have value

3.7.valuefunctions63s,asas'ra's'r(b)(a)figure3.4:backupdiagramsfor(a)v   and(b)q   .thestatesoftheenvironment.ateachcell,fouractionsarepossible:north,south,east,andwest,whichdeterministicallycausetheagenttomoveonecellintherespectivedirectiononthegrid.actionsthatwouldtaketheagento   thegridleaveitslocationunchanged,butalsoresultinarewardof 1.otheractionsresultinarewardof0,exceptthosethatmovetheagentoutofthespecialstatesaandb.fromstatea,allfouractionsyieldarewardof+10andtaketheagenttoa0.fromstateb,allactionsyieldarewardof+5andtaketheagenttob0.supposetheagentselectsallfouractionswithequalid203inallstates.figure3.5bshowsthevaluefunction,v   ,forthispolicy,forthedis-countedrewardcasewith =0.9.thisvaluefunctionwascomputedbysolv-ingthesystemofequations(3.10).noticethenegativevaluesneartheloweredge;thesearetheresultofthehighid203ofhittingtheedgeofthegridthereundertherandompolicy.stateaisthebeststatetobeinunderthispol-icy,butitsexpectedreturnislessthan10,itsimmediatereward,becausefromatheagentistakentoa0,fromwhichitislikelytorunintotheedgeofthegrid.stateb,ontheotherhand,isvaluedmorethan5,itsimmediatereward,becausefrombtheagentistakentob0,whichhasapositivevalue.fromb0theexpectedpenalty(negativereward)forpossiblyrunningintoanedgeismore3.38.84.45.31.51.53.02.31.90.50.10.70.70.4-0.4-1.0-0.4-0.4-0.6-1.2-1.9-1.3-1.2-1.4-2.0aba'b'+10+5actions(a)(b)figure3.5:gridexample:(a)exceptionalrewarddynamics;(b)state-valuefunctionfortheequiprobablerandompolicy.rs0s,aa0   pq   backupdiagram3.5. policies and value functions

49

   1. o    the green we cannot reach the hole by putting, and the value is greater. if we can reach the
green from a state by putting, then that state must have value one less than the green   s value, that is,
   2. for simplicity, let us assume we can putt very precisely and deterministically, but with a limited
range. this gives us the sharp contour line labeled    2 in the    gure; all locations between that line and
the green require exactly two strokes to complete the hole. similarly, any location within putting range
of the    2 contour line must have a value of    3, and so on to get all the contour lines shown in the
   gure. putting doesn   t get us out of sand traps, so they have a value of       . overall, it takes us six
strokes to get from the tee to the hole by putting.

figure 3.4: a golf example: the state-value function for putting (upper) and the optimal action-value function
for using the driver (lower).

exercise 3.14 in the gridworld example, rewards are positive for goals, negative for running into the
edge of the world, and zero the rest of the time. are the signs of these rewards important, or only
the intervals between them? prove, using (3.8), that adding a constant c to all the rewards adds a
constant, vc, to the values of all states, and thus does not a   ect the relative values of any states under
(cid:3)
any policies. what is vc in terms of c and   ?
exercise 3.15 now consider adding a constant c to all the rewards in an episodic task, such as maze
running. would this have any e   ect, or would it leave the task unchanged as in the continuing task
(cid:3)
above? why or why not? give an example.
exercise 3.16 the value of a state depends on the values of the actions possible in that state and on
how likely each action is to be taken under the current policy. we can think of this in terms of a small
backup diagram rooted at the state and considering each possible action:

give the equation corresponding to this intuition and diagram for the value at the root node, v  (s), in
terms of the value at the expected leaf node, q  (s, a), given st = s. this equation should include an

q*(s,driver)vputtsandgreen!1sand!2!2!3!4!1!5!6!4!3!3!2!4sandgreen!1sand!2!3!200!"!"vputtq   (s,driver)stakenwithid203   (a|s)v   (s)q   (s,a)a1a2a350

chapter 3. finite id100

expectation conditioned on following the policy,   . then give a second equation in which the expected
value is written out explicitly in terms of   (a|s) such that no expected value notation appears in the
(cid:3)
equation.
exercise 3.17 the value of an action, q  (s, a), depends on the expected next reward and the expected
sum of the remaining rewards. again we can think of this in terms of a small backup diagram, this one
rooted at an action (state   action pair) and branching to the possible next states:

give the equation corresponding to this intuition and diagram for the action value, q  (s, a), in terms
of the expected next reward, rt+1, and the expected next state value, v  (st+1), given that st = s and
at = a. this equation should include an expectation but not one conditioned conditioned on following
the policy. then give a second equation, writing out the expected value explicitly in terms of p(s(cid:48), r|s, a)
(cid:3)
de   ned by (3.2), such that no expected value notation appears in the equation.

3.6 optimal policies and optimal value functions

solving a id23 tasid116, roughly,    nding a policy that achieves a lot of reward over
the long run. for    nite mdps, we can precisely de   ne an optimal policy in the following way. value
functions de   ne a partial ordering over policies. a policy    is de   ned to be better than or equal to
a policy   (cid:48) if its expected return is greater than or equal to that of   (cid:48) for all states. in other words,
         (cid:48) if and only if v  (s)     v  (cid:48)(s) for all s     s. there is always at least one policy that is better
than or equal to all other policies. this is an optimal policy. although there may be more than one,
we denote all the optimal policies by      . they share the same state-value function, called the optimal
state-value function, denoted v   , and de   ned as

v   (s)

.
= max

  

v  (s),

(3.15)

for all s     s.

optimal policies also share the same optimal action-value function, denoted q   , and de   ned as

q   (s, a)

.
= max

  

q  (s, a),

(3.16)

for all s     s and a     a(s). for the state   action pair (s, a), this function gives the expected return for
taking action a in state s and thereafter following an optimal policy. thus, we can write q    in terms of
v    as follows:

q   (s, a) = e[rt+1 +   v   (st+1) | st = s, at = a] .

(3.17)

example 3.8: optimal value functions for golf the lower part of figure 3.4 shows the contours
of a possible optimal action-value function q   (s, driver). these are the values of each state if we    rst
play a stroke with the driver and afterward select either the driver or the putter, whichever is better.
the driver enables us to hit the ball farther, but with less accuracy. we can reach the hole in one shot
using the driver only if we are already very close; thus the    1 contour for q   (s, driver) covers only
a small portion of the green. if we have two strokes, however, then we can reach the hole from much
farther away, as shown by the    2 contour. in this case we don   t have to drive all the way to within the
small    1 contour, but only to anywhere on the green; from there we can use the putter. the optimal

s,aq   (s,a)s03s02s01r1r2r3expectedrewardsv   (s)3.6. optimal policies and optimal value functions

51

action-value function gives the values after committing to a particular    rst action, in this case, to the
driver, but afterward using whichever actions are best. the    3 contour is still farther out and includes
the starting tee. from the tee, the best sequence of actions is two drives and one putt, sinking the ball
in three strokes.

because v    is the value function for a policy, it must satisfy the self-consistency condition given by
the bellman equation for state values (3.14). because it is the optimal value function, however, v      s
consistency condition can be written in a special form without reference to any speci   c policy. this is
the bellman equation for v   , or the bellman optimality equation. intuitively, the bellman optimality
equation expresses the fact that the value of a state under an optimal policy must equal the expected
return for the best action from that state:

v   (s) = max
a   a(s)
= max

a

a

(s, a)

q     
e     
[gt | st = s, at = a]
e     
[rt+1 +   gt+1 | st = s, at = a]
e[rt+1 +   v   (st+1) | st = s, at = a]
a (cid:88)s(cid:48),r
p(s(cid:48), r|s, a)(cid:2)r +   v   (s(cid:48))(cid:3).

a

= max

= max

= max

(by (3.9))

(3.18)

(3.19)

the last two equations are two forms of the bellman optimality equation for v   . the bellman optimality
equation for q    is

q   (s, a) = e(cid:104)rt+1 +    max

q   (st+1, a(cid:48))(cid:12)(cid:12)(cid:12) st = s, at = a(cid:105)

q   (s(cid:48), a(cid:48))(cid:105).

p(s(cid:48), r|s, a)(cid:104)r +    max

= (cid:88)s(cid:48),r

a(cid:48)

a(cid:48)

(3.20)

the backup diagrams in figure 3.5 show graphically the spans of future states and actions considered
in the bellman optimality equations for v    and q   . these are the same as the backup diagrams for v  
and q   presented earlier except that arcs have been added at the agent   s choice points to represent that
the maximum over that choice is taken rather than the expected value given some policy. the backup
diagram on the left graphically represents the bellman optimality equation (3.19) and the backup
diagram on the right graphically represents (3.20).

figure 3.5: backup diagrams for v    and q   

for    nite mdps, the bellman optimality equation for v   (3.19) has a unique solution independent
of the policy. the bellman optimality equation is actually a system of equations, one for each state, so
if there are n states, then there are n equations in n unknowns. if the dynamics p of the environment
are known, then in principle one can solve this system of equations for v    using any one of a variety of
methods for solving systems of nonlinear equations. one can solve a related set of equations for q   .

once one has v   , it is relatively easy to determine an optimal policy. for each state s, there will be
one or more actions at which the maximum is obtained in the bellman optimality equation. any policy

ss0arrs0s,aa0maxmax(v   )(q   )52

chapter 3. finite id100

that assigns nonzero id203 only to these actions is an optimal policy. you can think of this as a
one-step search. if you have the optimal value function, v   , then the actions that appear best after a
one-step search will be optimal actions. another way of saying this is that any policy that is greedy
with respect to the optimal evaluation function v    is an optimal policy. the term greedy is used in
computer science to describe any search or decision procedure that selects alternatives based only on
local or immediate considerations, without considering the possibility that such a selection may prevent
future access to even better alternatives. consequently, it describes policies that select actions based
only on their short-term consequences. the beauty of v    is that if one uses it to evaluate the short-
term consequences of actions   speci   cally, the one-step consequences   then a greedy policy is actually
optimal in the long-term sense in which we are interested because v    already takes into account the
reward consequences of all possible future behavior. by means of v   , the optimal expected long-term
return is turned into a quantity that is locally and immediately available for each state. hence, a
one-step-ahead search yields the long-term optimal actions.

having q    makes choosing optimal actions even easier. with q   , the agent does not even have to
for any state s, it can simply    nd any action that maximizes q   (s, a).
do a one-step-ahead search:
the action-value function e   ectively caches the results of all one-step-ahead searches. it provides the
optimal expected long-term return as a value that is locally and immediately available for each state   
action pair. hence, at the cost of representing a function of state   action pairs, instead of just of states,
the optimal action-value function allows optimal actions to be selected without having to know anything
about possible successor states and their values, that is, without having to know anything about the
environment   s dynamics.

example 3.9: bellman optimality equations for the recycling robot using (3.19), we
can explicitly give the bellman optimality equation for the recycling robot example. to make things
more compact, we abbreviate the states high and low, and the actions search, wait, and recharge
respectively by h, l, s, w, and re. since there are only two states, the bellman optimality equation
consists of two equations. the equation for v   (h) can be written as follows:

v   (h) = max(cid:26) p(h|h, s)[r(h, s, h) +   v   (h)] + p(l|h, s)[r(h, s, l) +   v   (l)],
p(h|h, w)[r(h, w, h) +   v   (h)] + p(l|h, w)[r(h, w, l) +   v   (l)] (cid:27)

= max(cid:26)   [rs +   v   (h)] + (1       )[rs +   v   (l)],
= max(cid:26) rs +   [  v   (h) + (1       )v   (l)],
(cid:27) .

1[rw +   v   (h)] + 0[rw +   v   (l)]

rw +   v   (h)

(cid:27)

following the same procedure for v   (l) yields the equation

v   (l) = max         

  rs     3(1       ) +   [(1       )v   (h) +   v   (l)]
rw +   v   (l),
  v   (h)

.

         

for any choice of rs, rw,   ,   , and   , with 0        < 1, 0       ,        1, there is exactly one pair of numbers,
v   (h) and v   (l), that simultaneously satisfy these two nonlinear equations.
suppose we solve the bellman equation for v    for the
example 3.10: solving the gridworld
simple grid task introduced in example 3.6 and shown again in figure 3.6 (left). recall that state
a is followed by a reward of +10 and transition to state a(cid:48), while state b is followed by a reward of
+5 and transition to state b(cid:48). figure 3.6 (middle) shows the optimal value function, and figure 3.6
(right) shows the corresponding optimal policies. where there are multiple arrows in a cell, all of the
corresponding actions are optimal.

3.6. optimal policies and optimal value functions

53

figure 3.6: optimal solutions to the gridworld example.

explicitly solving the bellman optimality equation provides one route to    nding an optimal policy,
and thus to solving the id23 problem. however, this solution is rarely directly useful.
it is akin to an exhaustive search, looking ahead at all possibilities, computing their probabilities of
occurrence and their desirabilities in terms of expected rewards. this solution relies on at least three
assumptions that are rarely true in practice: (1) we accurately know the dynamics of the environment;
(2) we have enough computational resources to complete the computation of the solution; and (3)
the markov property. for the kinds of tasks in which we are interested, one is generally not able to
implement this solution exactly because various combinations of these assumptions are violated. for
example, although the    rst and third assumptions present no problems for the game of backgammon,
the second is a major impediment. since the game has about 1020 states, it would take thousands of
years on today   s fastest computers to solve the bellman equation for v   , and the same is true for    nding
q   . in id23 one typically has to settle for approximate solutions.

many di   erent decision-making methods can be viewed as ways of approximately solving the bellman
optimality equation. for example, heuristic search methods can be viewed as expanding the right-hand
side of (3.19) several times, up to some depth, forming a    tree    of possibilities, and then using a heuristic
evaluation function to approximate v    at the    leaf    nodes. (heuristic search methods such as a    are
almost always based on the episodic case.) the methods of id145 can be related
even more closely to the bellman optimality equation. many id23 methods can be
clearly understood as approximately solving the bellman optimality equation, using actual experienced
transitions in place of knowledge of the expected transitions. we consider a variety of such methods in
the following chapters.

exercise 3.18 draw or describe the optimal state-value function for the golf example.

(cid:3)
exercise 3.19 draw or describe the contours of the optimal action-value function for putting,
(cid:3)
q   (s, putter), for the golf example.
(cid:3)
exercise 3.20 give the bellman equation for q    for the recycling robot.
exercise 3.21 figure 3.6 gives the optimal value of the best state of the gridworld as 24.4, to one
decimal place. use your knowledge of the optimal policy and (3.8) to express this value symbolically,
(cid:3)
and then to compute it to three decimal places.

a) gridworldb) v*c) !*22.024.422.019.417.519.822.019.817.816.017.819.817.816.014.416.017.816.014.413.014.416.014.413.011.7aba'b'+10+5v*  *gridworldv         54

chapter 3. finite id100

exercise 3.22 consider the continuing mdp shown on to the
right. the only decision to be made is that in the top state,
where two actions are available, left and right. the numbers
show the rewards that are received deterministically after each
action. there are exactly two deterministic policies,   left and
  right. what policy is optimal if    = 0? if    = 0.9? if    = 0.5? (cid:3)
(cid:3)
exercise 3.23 give an equation for v    in terms of q   .
exercise 3.24 give an equation for q    in terms of v    and the world   s dynamics, p(s(cid:48), r|s, a).
exercise 3.25 give an equation for       in terms of q   .
exercise 3.26 give an equation for       in terms of v    and the world   s dynamics, p(s(cid:48), r|s, a).

(cid:3)
(cid:3)
(cid:3)

3.7 optimality and approximation

we have de   ned optimal value functions and optimal policies. clearly, an agent that learns an optimal
policy has done very well, but in practice this rarely happens. for the kinds of tasks in which we are
interested, optimal policies can be generated only with extreme computational cost. a well-de   ned
notion of optimality organizes the approach to learning we describe in this book and provides a way to
understand the theoretical properties of various learning algorithms, but it is an ideal that agents can
only approximate to varying degrees. as we discussed above, even if we have a complete and accurate
model of the environment   s dynamics, it is usually not possible to simply compute an optimal policy by
solving the bellman optimality equation. for example, board games such as chess are a tiny fraction
of human experience, yet large, custom-designed computers still cannot compute the optimal moves.
a critical aspect of the problem facing the agent is always the computational power available to it, in
particular, the amount of computation it can perform in a single time step.

the memory available is also an important constraint. a large amount of memory is often required
to build up approximations of value functions, policies, and models. in tasks with small,    nite state
sets, it is possible to form these approximations using arrays or tables with one entry for each state
(or state   action pair). this we call the tabular case, and the corresponding methods we call tabular
methods. in many cases of practical interest, however, there are far more states than could possibly be
entries in a table. in these cases the functions must be approximated, using some sort of more compact
parameterized function representation.

our framing of the id23 problem forces us to settle for approximations. however,
it also presents us with some unique opportunities for achieving useful approximations. for example,
in approximating optimal behavior, there may be many states that the agent faces with such a low
id203 that selecting suboptimal actions for them has little impact on the amount of reward the
agent receives. tesauro   s backgammon player, for example, plays with exceptional skill even though
it might make very bad decisions on board con   gurations that never occur in games against experts.
in fact, it is possible that td-gammon makes bad decisions for a large fraction of the game   s state
set. the on-line nature of id23 makes it possible to approximate optimal policies in
ways that put more e   ort into learning to make good decisions for frequently encountered states, at the
expense of less e   ort for infrequently encountered states. this is one key property that distinguishes
id23 from other approaches to approximately solving mdps.

+200+1leftright3.8. summary

3.8 summary

55

let us summarize the elements of the id23 problem that we have presented in this
chapter. id23 is about learning from interaction how to behave in order to achieve
a goal. the id23 agent and its environment interact over a sequence of discrete time
steps. the speci   cation of their interface de   nes a particular task: the actions are the choices made by
the agent; the states are the basis for making the choices; and the rewards are the basis for evaluating
the choices. everything inside the agent is completely known and controllable by the agent; everything
outside is incompletely controllable but may or may not be completely known. a policy is a stochastic
rule by which the agent selects actions as a function of states. the agent   s objective is to maximize the
amount of reward it receives over time.

when the id23 setup described above is formulated with well de   ned transition
probabilities it constitutes a markov decision process (mdp). a    nite mdp is an mdp with    nite state,
action, and (as we formulate it here) reward sets. much of the current theory of id23
is restricted to    nite mdps, but the methods and ideas apply more generally.

the return is the function of future rewards that the agent seeks to maximize. it has several di   erent
de   nitions depending upon the nature of the task and whether one wishes to discount delayed reward.
the undiscounted formulation is appropriate for episodic tasks, in which the agent   environment inter-
action breaks naturally into episodes; the discounted formulation is appropriate for continuing tasks,
in which the interaction does not naturally break into episodes but continues without limit. we try to
de   ne the returns for the two kinds of tasks such that one set of equations can apply to both both the
episodic and continuing cases.

a policy   s value functions assign to each state, or state   action pair, the expected return from that
state, or state   action pair, given that the agent uses the policy. the optimal value functions assign to
each state, or state   action pair, the largest expected return achievable by any policy. a policy whose
value functions are optimal is an optimal policy. whereas the optimal value functions for states and
state   action pairs are unique for a given mdp, there can be many optimal policies. any policy that is
greedy with respect to the optimal value functions must be an optimal policy. the bellman optimality
equations are special consistency conditions that the optimal value functions must satisfy and that can,
in principle, be solved for the optimal value functions, from which an optimal policy can be determined
with relative ease.

a id23 problem can be posed in a variety of di   erent ways depending on assump-
tions about the level of knowledge initially available to the agent. in problems of complete knowledge,
the agent has a complete and accurate model of the environment   s dynamics. if the environment is an
mdp, then such a model consists of the complete four-argument dynamics function p (3.2). in problems
of incomplete knowledge, a complete and perfect model of the environment is not available.

even if the agent has a complete and accurate environment model, the agent is typically unable to
perform enough computation per time step to fully use it. the memory available is also an important
constraint. memory may be required to build up accurate approximations of value functions, policies,
and models. in most cases of practical interest there are far more states than could possibly be entries
in a table, and approximations must be made.

a well-de   ned notion of optimality organizes the approach to learning we describe in this book and
provides a way to understand the theoretical properties of various learning algorithms, but it is an
ideal that id23 agents can only approximate to varying degrees.
in reinforcement
learning we are very much concerned with cases in which optimal solutions cannot be found but must
be approximated in some way.

56

chapter 3. finite id100

bibliographical and historical remarks

the id23 problem is deeply indebted to the idea of id100 (mdps)
from the    eld of optimal control. these historical in   uences and other major in   uences from psychology
are described in the brief history given in chapter 1. id23 adds to mdps a focus on
approximation and incomplete information for realistically large problems. mdps and the reinforcement
learning problem are only weakly linked to traditional learning and decision-making problems in arti   cial
intelligence. however, arti   cial intelligence is now vigorously exploring mdp formulations for planning
and decision making from a variety of perspectives. mdps are more general than previous formulations
used in arti   cial intelligence in that they permit more general kinds of goals and uncertainty.

the theory of mdps is treated by, e.g., bertsekas (2005), white (1969), whittle (1982, 1983), and
puterman (1994). a particularly compact treatment of the    nite case is given by ross (1983). mdps are
also studied under the heading of stochastic optimal control, where adaptive optimal control methods
are most closely related to id23 (e.g., kumar, 1985; kumar and varaiya, 1986).

the theory of mdps evolved from e   orts to understand the problem of making sequences of decisions
under uncertainty, where each decision can depend on the previous decisions and their outcomes. it is
sometimes called the theory of multistage decision processes, or sequential decision processes, and has
roots in the statistical literature on sequential sampling beginning with the papers by thompson (1933,
1934) and robbins (1952) that we cited in chapter 2 in connection with bandit problems (which are
prototypical mdps if formulated as multiple-situation problems).

the earliest instance of which we are aware in which id23 was discussed using
the mdp formalism is andreae   s (1969b) description of a uni   ed view of learning machines. witten
and corbin (1973) experimented with a id23 system later analyzed by witten (1977)
using the mdp formalism. although he did not explicitly mention mdps, werbos (1977) suggested
approximate solution methods for stochastic optimal control problems that are related to modern re-
inforcement learning methods (see also werbos, 1982, 1987, 1988, 1989, 1992). although werbos   s
ideas were not widely recognized at the time, they were prescient in emphasizing the importance of
approximately solving optimal control problems in a variety of domains, including arti   cial intelligence.
the most in   uential integration of id23 and mdps is due to watkins (1989).

3.1

our characterization of the dynamics of an mdp in terms of p(s(cid:48), r|s, a) is slightly unusual. it
is more common in the mdp literature to describe the dynamics in terms of the state transition
probabilities p(s(cid:48)|s, a) and expected next rewards r(s, a). in id23, however,
we more often have to refer to individual actual or sample rewards (rather than just their
expected values). our notation also makes it plainer that st and rt are in general jointly
determined, and thus must have the same time index. in teaching id23, we
have found our notation to be more straightforward conceptually and easier to understand.

for a good intuitive discussion of the system-theoretic concept of state, see minsky (1967).

the bioreactor example is based on the work of ungar (1990) and miller and williams (1992).
the recycling robot example was inspired by the can-collecting robot built by jonathan connell
(1989).

3.2

the reward hypothesis was suggested by michael littman (personal communication).

3.3   4 the terminology of episodic and continuing tasks is di   erent from that usually used in the
mdp literature. in that literature it is common to distinguish three types of tasks: (1)    nite-
horizon tasks, in which interaction terminates after a particular    xed number of time steps;
(2) inde   nite-horizon tasks, in which interaction can last arbitrarily long but must eventually
terminate; and (3) in   nite-horizon tasks, in which interaction does not terminate. our episodic
and continuing tasks are similar to inde   nite-horizon and in   nite-horizon tasks, respectively,

3.8. summary

57

but we prefer to emphasize the di   erence in the nature of the interaction. this di   erence
seems more fundamental than the di   erence in the objective functions emphasized by the usual
terms. often episodic tasks use an inde   nite-horizon objective function and continuing tasks
an in   nite-horizon objective function, but we see this as a common coincidence rather than a
fundamental di   erence.

the pole-balancing example is from michie and chambers (1968) and barto, sutton, and
anderson (1983).

3.5   6 assigning value on the basis of what is good or bad in the long run has ancient roots.

in
control theory, mapping states to numerical values representing the long-term consequences of
control decisions is a key part of optimal control theory, which was developed in the 1950s by
extending nineteenth century state-function theories of classical mechanics (see, e.g., schultz
and melsa, 1967). in describing how a computer could be programmed to play chess, shannon
(1950) suggested using an evaluation function that took into account the long-term advantages
and disadvantages of chess positions.
watkins   s (1989) id24 algorithm for estimating q    (chapter 6) made action-value func-
tions an important part of id23, and consequently these functions are often
called    q-functions.    but the idea of an action-value function is much older than this. shan-
non (1950) suggested that a function h(p, m ) could be used by a chess-playing program to
decide whether a move m in position p is worth exploring. michie   s (1961, 1963) menace
system and michie and chambers   s (1968) boxes system can be understood as estimating
action-value functions.
in classical physics, hamilton   s principal function is an action-value
function; newtonian dynamics are greedy with respect to this function (e.g., goldstein, 1957).
action-value functions also played a central role in denardo   s (1967) theoretical treatment of
dp in terms of contraction mappings.
what we call the bellman equation for v    was popularized by richard bellman (1957a), who
called it the    basic functional equation.    the counterpart of the bellman optimality equation
for continuous time and state problems is known as the hamilton   jacobi   bellman equation (or
often just the hamilton   jacobi equation), indicating its roots in classical physics (e.g., schultz
and melsa, 1967).

the golf example was suggested by chris watkins.

58

chapter 3. finite id100

chapter 4

id145

the term id145 (dp) refers to a collection of algorithms that can be used to compute
optimal policies given a perfect model of the environment as a markov decision process (mdp). classi-
cal dp algorithms are of limited utility in id23 both because of their assumption of a
perfect model and because of their great computational expense, but they are still important theoreti-
cally. dp provides an essential foundation for the understanding of the methods presented in the rest
of this book. in fact, all of these methods can be viewed as attempts to achieve much the same e   ect
as dp, only with less computation and without assuming a perfect model of the environment.

starting with this chapter, we usually assume that the environment is a    nite mdp. that is, we
assume that its state, action, and reward sets, s, a, and r, are    nite, and that its dynamics are given
by a set of probabilities p(s(cid:48), r|s, a), for all s     s, a     a(s), r     r, and s(cid:48)     s+ (s+ is s plus a terminal
state if the problem is episodic). although dp ideas can be applied to problems with continuous state
and action spaces, exact solutions are possible only in special cases. a common way of obtaining
approximate solutions for tasks with continuous states and actions is to quantize the state and action
spaces and then apply    nite-state dp methods. the methods we explore in chapter 9 are applicable
to continuous problems and are a signi   cant extension of that approach.

the key idea of dp, and of id23 generally, is the use of value functions to organize
and structure the search for good policies. in this chapter we show how dp can be used to compute
the value functions de   ned in chapter 3. as discussed there, we can easily obtain optimal policies once
we have found the optimal value functions, v    or q   , which satisfy the bellman optimality equations:

v   (s) = max
= max

a

e[rt+1 +   v   (st+1) | st = s, at = a]
p(s(cid:48), r|s, a)(cid:104)r +   v   (s(cid:48))(cid:105)
a (cid:88)s(cid:48),r

or

q   (s, a) = e(cid:104)rt+1 +    max

q   (st+1, a(cid:48))(cid:12)(cid:12)(cid:12) st = s, at = a(cid:105)

q   (s(cid:48), a(cid:48))(cid:105),

p(s(cid:48), r|s, a)(cid:104)r +    max

= (cid:88)s(cid:48),r

a(cid:48)

a(cid:48)

(4.1)

(4.2)

for all s     s, a     a(s), and s(cid:48)     s+. as we shall see, dp algorithms are obtained by turning bellman
equations such as these into assignments, that is, into update rules for improving approximations of the
desired value functions.

59

60

chapter 4. id145

4.1 policy evaluation (prediction)

v  (s)

first we consider how to compute the state-value function v   for an arbitrary policy   . this is called
policy evaluation in the dp literature. we also refer to it as the prediction problem. recall from
chapter 3 that, for all s     s,
.
= e  [gt | st = s]
= e  [rt+1 +   gt+1 | st = s]
= e  [rt+1 +   v  (st+1) | st = s]
=(cid:88)a

p(s(cid:48), r|s, a)(cid:104)r +   v  (s(cid:48))(cid:105),

  (a|s)(cid:88)s(cid:48),r

(from (3.9))

(4.3)

(4.4)

where   (a|s) is the id203 of taking action a in state s under policy   , and the expectations are
subscripted by    to indicate that they are conditional on    being followed. the existence and uniqueness
of v   are guaranteed as long as either    < 1 or eventual termination is guaranteed from all states under
the policy   .

if the environment   s dynamics are completely known, then (4.4) is a system of |s| simultaneous linear
equations in |s| unknowns (the v  (s), s     s). in principle, its solution is a straightforward, if tedious,
computation. for our purposes, iterative solution methods are most suitable. consider a sequence
of approximate value functions v0, v1, v2, . . ., each mapping s+ to r (the real numbers). the initial
approximation, v0, is chosen arbitrarily (except that the terminal state, if any, must be given value 0),
and each successive approximation is obtained by using the bellman equation for v   (4.4) as an update
rule:

vk+1(s)

.
= e  [rt+1 +   vk(st+1) | st = s]
= (cid:88)a

p(s(cid:48), r|s, a)(cid:104)r +   vk(s(cid:48))(cid:105),

  (a|s)(cid:88)s(cid:48),r

(4.5)

for all s     s. clearly, vk = v   is a    xed point for this update rule because the bellman equation for v  
assures us of equality in this case. indeed, the sequence {vk} can be shown in general to converge to
v   as k         under the same conditions that guarantee the existence of v  . this algorithm is called
iterative policy evaluation.

to produce each successive approximation, vk+1 from vk, iterative policy evaluation applies the same
operation to each state s: it replaces the old value of s with a new value obtained from the old values of
the successor states of s, and the expected immediate rewards, along all the one-step transitions possible
under the policy being evaluated. we call this kind of operation an expected update. each iteration of
iterative policy evaluation updates the value of every state once to produce the new approximate value
function vk+1. there are several di   erent kinds of expected updates, depending on whether a state (as
here) or a state   action pair is being updated, and depending on the precise way the estimated values of
the successor states are combined. all the updates done in dp algorithms are called expected updates
because they are based on an expectation over all possible next states rather than on a sample next
state. the nature of an update can be expressed in an equation, as above, or in an backup diagram
like those introduced in chapter 3. for example, the backup diagram corresponding to the expected
update used in iterative policy evaluation is shown on page 47.

to write a sequential computer program to implement iterative policy evaluation as given by (4.5)
you would have to use two arrays, one for the old values, vk(s), and one for the new values, vk+1(s).
with two arrays, the new values can be computed one by one from the old values without the old values
being changed. of course it is easier to use one array and update the values    in place,    that is, with
each new value immediately overwriting the old one. then, depending on the order in which the states
are updated, sometimes new values are used instead of old ones on the right-hand side of (4.5). this

4.1. policy evaluation (prediction)

61

in-place algorithm also converges to v  ; in fact, it usually converges faster than the two-array version,
as you might expect, since it uses new data as soon as they are available. we think of the updates as
being done in a sweep through the state space. for the in-place algorithm, the order in which states
have their values updated during the sweep has a signi   cant in   uence on the rate of convergence. we
usually have the in-place version in mind when we think of dp algorithms.

a complete in-place version of iterative policy evaluation is shown in the box below. note how it
handles termination. formally, iterative policy evaluation converges only in the limit, but in practice
it must be halted short of this. the boxed algorithm tests the quantity maxs   s |vk+1(s)     vk(s)| after
each sweep and stops when it is su   ciently small.

iterative policy evaluation

input   , the policy to be evaluated
initialize an array v (s) = 0, for all s     s+
repeat

        0
for each s     s:
v     v (s)
        max(   ,|v     v (s)|)

v (s)    (cid:80)a   (a|s)(cid:80)s(cid:48),r p(s(cid:48), r|s, a)(cid:2)r +   v (s(cid:48))(cid:3)

until     <    (a small positive number)
output v     v  

example 4.1 consider the 4  4 gridworld shown below.

the nonterminal states are s = {1, 2, . . . , 14}. there are four actions possible in each state, a =
{up, down, right, left}, which deterministically cause the corresponding state transitions, except that
actions that would take the agent o    the grid in fact leave the state unchanged. thus, for instance,
p(6,   1|5, right) = 1, p(7,   1|7, right) = 1, and p(10, r|5, right) = 0 for all r     r. this is an
undiscounted, episodic task. the reward is    1 on all transitions until the terminal state is reached.
the terminal state is shaded in the    gure (although it is shown in two places, it is formally one state).
the expected reward function is thus r(s, a, s(cid:48)) =    1 for all states s, s(cid:48) and actions a. suppose the
agent follows the equiprobable random policy (all actions equally likely). the left side of figure 4.1
shows the sequence of value functions {vk} computed by iterative policy evaluation. the    nal estimate
is in fact v  , which in this case gives for each state the negation of the expected number of steps from
that state until termination.

exercise 4.1 in example 4.1, if    is the equiprobable random policy, what is q  (11, down)? what is
(cid:3)
q  (7, down)?
exercise 4.2 in example 4.1, suppose a new state 15 is added to the gridworld just below state 13,
and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively.
assume that the transitions from the original states are unchanged. what, then, is v  (15) for the

actionsr  =  !1on all transitions1234567891011121314rt= 162

chapter 4. id145

figure 4.1: convergence of iterative policy evaluation on a small gridworld. the left column is the sequence
of approximations of the state-value function for the random policy (all actions equal). the right column is
the sequence of greedy policies corresponding to the value function estimates (arrows are shown for all actions
achieving the maximum). the last policy is guaranteed only to be an improvement over the random policy, but
in this case it, and all policies after the third iteration, are optimal.

equiprobable random policy? now suppose the dynamics of state 13 are also changed, such that action
down from state 13 takes the agent to the new state 15. what is v  (15) for the equiprobable random
(cid:3)
policy in this case?
exercise 4.3 what are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function
(cid:3)
q   and its successive approximation by a sequence of functions q0, q1, q2, . . . ?

4.2 policy improvement

our reason for computing the value function for a policy is to help    nd better policies. suppose we
have determined the value function v   for an arbitrary deterministic policy   . for some state s we
would like to know whether or not we should change the policy to deterministically choose an action
a (cid:54)=   (s). we know how good it is to follow the current policy from s   that is v  (s)   but would it be

 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.0-1.7-2.0-2.0-1.7-2.0-2.0-2.0-2.0-2.0-2.0-1.7-2.0-2.0-1.7-2.4-2.9-3.0-2.4-2.9-3.0-2.9-2.9-3.0-2.9-2.4-3.0-2.9-2.4-6.1-8.4-9.0-6.1-7.7-8.4-8.4-8.4-8.4-7.7-6.1-9.0-8.4-6.1-14.-20.-22.-14.-18.-20.-20.-20.-20.-18.-14.-22.-20.-14.vk  for therandom policygreedy policyw.r.t. vkk = 0k = 1k = 2k = 10k = !k = 3optimal policyrandom policy 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0vkvk4.2. policy improvement

63

better or worse to change to the new policy? one way to answer this question is to consider selecting
a in s and thereafter following the existing policy,   . the value of this way of behaving is

q  (s, a)

.
= e[rt+1 +   v  (st+1) | st = s, at = a]
= (cid:88)s(cid:48),r

p(s(cid:48), r|s, a)(cid:104)r +   v  (s(cid:48))(cid:105).

(4.6)

the key criterion is whether this is greater than or less than v  (s). if it is greater   that is, if it is better
to select a once in s and thereafter follow    than it would be to follow    all the time   then one would
expect it to be better still to select a every time s is encountered, and that the new policy would in
fact be a better one overall.

that this is true is a special case of a general result called the policy improvement theorem. let   

and   (cid:48) be any pair of deterministic policies such that, for all s     s,

(4.7)

q  (s,   (cid:48)(s))     v  (s).

then the policy   (cid:48) must be as good as, or better than,   . that is, it must obtain greater or equal
expected return from all states s     s:

v  (cid:48)(s)     v  (s).

(4.8)

moreover, if there is strict inequality of (4.7) at any state, then there must be strict inequality of (4.8)
at least one state. this result applies in particular to the two policies that we considered in the previous
paragraph, an original deterministic policy,   , and a changed policy,   (cid:48), that is identical to    except
that   (cid:48)(s) = a (cid:54)=   (s). obviously, (4.7) holds at all states other than s. thus, if q  (s, a) > v  (s), then
the changed policy is indeed better than   .

the idea behind the proof of the policy improvement theorem is easy to understand. starting from

(4.7), we keep expanding the q   side with (4.6) and reapplying (4.7) until we get v  (cid:48)(s):

v  (s)     q  (s,   (cid:48)(s))

= e[rt+1 +   v  (st+1) | st = s, at =   (cid:48)(a)]
= e  (cid:48)[rt+1 +   v  (st+1) | st = s]
    e  (cid:48)[rt+1 +   q  (st+1,   (cid:48)(st+1)) | st = s]
= e  (cid:48)[rt+1 +   e  (cid:48)[rt+2 +   v  (st+2)] | st = s]

...

= e  (cid:48)(cid:2)rt+1 +   rt+2 +   2v  (st+2)(cid:12)(cid:12) st = s(cid:3)
    e  (cid:48)(cid:2)rt+1 +   rt+2 +   2rt+3 +   3v  (st+3)(cid:12)(cid:12) st = s(cid:3)
    e  (cid:48)(cid:2)rt+1 +   rt+2 +   2rt+3 +   3rt+4 +       (cid:12)(cid:12) st = s(cid:3)

= v  (cid:48)(s).

(by (4.6))

so far we have seen how, given a policy and its value function, we can easily evaluate a change in the
policy at a single state to a particular action. it is a natural extension to consider changes at all states
and to all possible actions, selecting at each state the action that appears best according to q  (s, a).
in other words, to consider the new greedy policy,   (cid:48), given by

  (cid:48)(s)

.
= argmax

a

= argmax

= argmax

q  (s, a)
e[rt+1 +   v  (st+1) | st = s, at = a]
p(s(cid:48), r|s, a)(cid:104)r +   v  (s(cid:48))(cid:105),

a

a (cid:88)s(cid:48),r

(4.9)

64

chapter 4. id145

where argmaxa denotes the value of a at which the expression that follows is maximized (with ties
broken arbitrarily). the greedy policy takes the action that looks best in the short term   after one
step of lookahead   according to v  . by construction, the greedy policy meets the conditions of the
policy improvement theorem (4.7), so we know that it is as good as, or better than, the original policy.
the process of making a new policy that improves on an original policy, by making it greedy with
respect to the value function of the original policy, is called policy improvement.

suppose the new greedy policy,   (cid:48), is as good as, but not better than, the old policy   . then v   = v  (cid:48),

and from (4.9) it follows that for all s     s:

v  (cid:48)(s) = max

= max

a

e[rt+1 +   v  (cid:48)(st+1) | st = s, at = a]
p(s(cid:48), r|s, a)(cid:104)r +   v  (cid:48)(s(cid:48))(cid:105).
a (cid:88)s(cid:48),r

but this is the same as the bellman optimality equation (4.1), and therefore, v  (cid:48) must be v   , and both
   and   (cid:48) must be optimal policies. policy improvement thus must give us a strictly better policy except
when the original policy is already optimal.

so far in this section we have considered the special case of deterministic policies. in the general case,
a stochastic policy    speci   es probabilities,   (a|s), for taking each action, a, in each state, s. we will
not go through the details, but in fact all the ideas of this section extend easily to stochastic policies.
in particular, the policy improvement theorem carries through as stated for the stochastic case.
in
addition, if there are ties in policy improvement steps such as (4.9)   that is, if there are several actions
at which the maximum is achieved   then in the stochastic case we need not select a single action from
among them. instead, each maximizing action can be given a portion of the id203 of being selected
in the new greedy policy. any apportioning scheme is allowed as long as all submaximal actions are
given zero id203.

the last row of figure 4.1 shows an example of policy improvement for stochastic policies. here the
original policy,   , is the equiprobable random policy, and the new policy,   (cid:48), is greedy with respect
to v  . the value function v   is shown in the bottom-left diagram and the set of possible   (cid:48) is shown
in the bottom-right diagram. the states with multiple arrows in the   (cid:48) diagram are those in which
several actions achieve the maximum in (4.9); any apportionment of id203 among these actions is
permitted. the value function of any such policy, v  (cid:48)(s), can be seen by inspection to be either    1,    2,
or    3 at all states, s     s, whereas v  (s) is at most    14. thus, v  (cid:48)(s)     v  (s), for all s     s, illustrating
policy improvement. although in this case the new policy   (cid:48) happens to be optimal, in general only an
improvement is guaranteed.

4.3 policy iteration

once a policy,   , has been improved using v   to yield a better policy,   (cid:48), we can then compute v  (cid:48) and
improve it again to yield an even better   (cid:48)(cid:48). we can thus obtain a sequence of monotonically improving
policies and value functions:

  0

e       v  0

i         1

e       v  1

i         2

e             

i            

e       v   ,

where e       denotes a policy evaluation and i       denotes a policy improvement. each policy is guaranteed
to be a strict improvement over the previous one (unless it is already optimal). because a    nite mdp
has only a    nite number of policies, this process must converge to an optimal policy and optimal value
function in a    nite number of iterations.

this way of    nding an optimal policy is called policy iteration. a complete algorithm is given in
the box on the next page. note that each policy evaluation, itself an iterative computation, is started

4.3. policy iteration

65

policy iteration (using iterative policy evaluation)

1. initialization

v (s)     r and   (s)     a(s) arbitrarily for all s     s

2. policy evaluation

repeat

        0
for each s     s:
v     v (s)
        max(   ,|v     v (s)|)

v (s)    (cid:80)s(cid:48),r p(s(cid:48), r|s,   (s))(cid:2)r +   v (s(cid:48))(cid:3)

until     <    (a small positive number)

3. policy improvement
policy-stable     true
for each s     s:
old-action       (s)
if old-action (cid:54)=   (s), then policy-stable     f alse

  (s)     argmaxa(cid:80)s(cid:48),r p(s(cid:48), r|s, a)(cid:2)r +   v (s(cid:48))(cid:3)

if policy-stable, then stop and return v     v    and             ; else go to 2

with the value function for the previous policy. this typically results in a great increase in the speed of
convergence of policy evaluation (presumably because the value function changes little from one policy
to the next).

policy iteration often converges in surprisingly few iterations. this is illustrated by the example
in figure 4.1. the bottom-left diagram shows the value function for the equiprobable random policy,
and the bottom-right diagram shows a greedy policy for this value function. the policy improvement
theorem assures us that these policies are better than the original random policy. in this case, however,
these policies are not just better, but optimal, proceeding to the terminal states in the minimum number
of steps. in this example, policy iteration would    nd the optimal policy after just one iteration.

exercise 4.4 the policy iteration algorithm given on this page has a subtle bug in that it may never
terminate if the policy continually switches between two or more policies that are equally good. this is
ok for pedagogy, but not for actual use. modi   y the pseudocode so that convergence is guaranteed. (cid:3)
example 4.2: jack   s car rental jack manages two locations for a nationwide car rental company.
each day, some number of customers arrive at each location to rent cars. if jack has a car available, he
rents it out and is credited $10 by the national company. if he is out of cars at that location, then the
business is lost. cars become available for renting the day after they are returned. to help ensure that
cars are available where they are needed, jack can move them between the two locations overnight, at
a cost of $2 per car moved. we assume that the number of cars requested and returned at each location
are poisson random variables, meaning that the id203 that the number is n is   n
n! e     , where    is
the expected number. suppose    is 3 and 4 for rental requests at the    rst and second locations and
3 and 2 for returns. to simplify the problem slightly, we assume that there can be no more than 20
cars at each location (any additional cars are returned to the nationwide company, and thus disappear
from the problem) and a maximum of    ve cars can be moved from one location to the other in one
night. we take the discount rate to be    = 0.9 and formulate this as a continuing    nite mdp, where
the time steps are days, the state is the number of cars at each location at the end of the day, and
the actions are the net numbers of cars moved between the two locations overnight. figure 4.2 shows

66

chapter 4. id145

figure 4.2: the sequence of policies found by policy iteration on jack   s car rental problem, and the    nal
state-value function. the    rst    ve diagrams show, for each number of cars at each location at the end of the
day, the number of cars to be moved from the    rst location to the second (negative numbers indicate transfers
from the second location to the    rst). each successive policy is a strict improvement over the previous policy,
and the last policy is optimal.

the sequence of policies found by policy iteration starting from the policy that never moves any cars.

exercise 4.5 (programming) write a program for policy iteration and re-solve jack   s car rental
problem with the following changes. one of jack   s employees at the    rst location rides a bus home
each night and lives near the second location. she is happy to shuttle one car to the second location
for free. each additional car still costs $2, as do all cars moved in the other direction. in addition,
jack has limited parking space at each location. if more than 10 cars are kept overnight at a location
(after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot
(independent of how many cars are kept there). these sorts of nonlinearities and arbitrary dynamics
often occur in real problems and cannot easily be handled by optimization methods other than dynamic
programming. to check your program,    rst replicate the results given for the original problem. if your
(cid:3)
computer is too slow for the full problem, cut all the numbers of cars in half.

exercise 4.6 how would policy iteration be de   ned for action values? give a complete algorithm
for computing q   , analogous to that on page 65 for computing v   . please pay special attention to this
(cid:3)
exercise, because the ideas involved will be used throughout the rest of the book.

exercise 4.7 suppose you are restricted to considering only policies that are  -soft, meaning that
the id203 of selecting each action in each state, s, is at least  /|a(s)|. describe qualitatively the
changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration
(cid:3)
algorithm for v    (page 65).

4v612#cars at second location042020020#cars at first location115!1!2-4432432!3005!1!2!3!412340"1"0"2!3!4!201234!1"32!4!3!201345!1"4#cars at second location#cars at first location5200020v44.4. value iteration

67

4.4 value iteration

one drawback to policy iteration is that each of its iterations involves policy evaluation, which may
itself be a protracted iterative computation requiring multiple sweeps through the state set. if policy
evaluation is done iteratively, then convergence exactly to v   occurs only in the limit. must we wait
for exact convergence, or can we stop short of that? the example in figure 4.1 certainly suggests that
it may be possible to truncate policy evaluation. in that example, policy evaluation iterations beyond
the    rst three have no e   ect on the corresponding greedy policy.

in fact, the policy evaluation step of policy iteration can be truncated in several ways without losing
the convergence guarantees of policy iteration. one important special case is when policy evaluation
is stopped after just one sweep (one update of each state). this algorithm is called value iteration. it
can be written as a particularly simple update operation that combines the policy improvement and
truncated policy evaluation steps:

vk+1(s)

.
= max

= max

a

e[rt+1 +   vk(st+1) | st = s, at = a]
p(s(cid:48), r|s, a)(cid:104)r +   vk(s(cid:48))(cid:105),
a (cid:88)s(cid:48),r

(4.10)

for all s     s. for arbitrary v0, the sequence {vk} can be shown to converge to v    under the same
conditions that guarantee the existence of v   .

another way of understanding value iteration is by reference to the bellman optimality equation
(4.1). note that value iteration is obtained simply by turning the bellman optimality equation into
an update rule. also note how the value iteration update is identical to the policy evaluation update
(4.5) except that it requires the maximum to be taken over all actions. another way of seeing this close
relationship is to compare the backup diagrams for these algorithms on page 47 (policy evaluation) and
on the left of figure 3.5 (value iteration). these two are the natural backup operations for computing
v   and v   .

finally, let us consider how value iteration terminates. like policy evaluation, value iteration formally
requires an in   nite number of iterations to converge exactly to v   . in practice, we stop once the value
function changes by only a small amount in a sweep. the box shows a complete algorithm with this
kind of termination condition.

value iteration
initialize array v arbitrarily (e.g., v (s) = 0 for all s     s+)
repeat

        0
for each s     s:
v     v (s)
        max(   ,|v     v (s)|)

v (s)     maxa(cid:80)s(cid:48),r p(s(cid:48), r|s, a)(cid:2)r +   v (s(cid:48))(cid:3)

until     <    (a small positive number)

output a deterministic policy,             , such that

  (s) = argmaxa(cid:80)s(cid:48),r p(s(cid:48), r|s, a)(cid:2)r +   v (s(cid:48))(cid:3)

value iteration e   ectively combines, in each of its sweeps, one sweep of policy evaluation and one
sweep of policy improvement. faster convergence is often achieved by interposing multiple policy
evaluation sweeps between each policy improvement sweep. in general, the entire class of truncated

68

chapter 4. id145

policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation
updates and some of which use value iteration updates. since the max operation in (4.10) is the only
di   erence between these updates, this just means that the max operation is added to some sweeps of
policy evaluation. all of these algorithms converge to an optimal policy for discounted    nite mdps.

example 4.3: gambler   s problem a gambler has the opportunity to make bets on the outcomes
of a sequence of coin    ips. if the coin comes up heads, he wins as many dollars as he has staked on
that    ip; if it is tails, he loses his stake. the game ends when the gambler wins by reaching his goal
of $100, or loses by running out of money. on each    ip, the gambler must decide what portion of his
capital to stake, in integer numbers of dollars. this problem can be formulated as an undiscounted,
episodic,    nite mdp. the state is the gambler   s capital, s     {1, 2, . . . , 99} and the actions are stakes,
a     {0, 1, . . . , min(s, 100     s)}. the reward is zero on all transitions except those on which the gambler
reaches his goal, when it is +1. the state-value function then gives the id203 of winning from
each state. a policy is a mapping from levels of capital to stakes. the optimal policy maximizes
the id203 of reaching the goal. let ph denote the id203 of the coin coming up heads. if
ph is known, then the entire problem is known and it can be solved, for instance, by value iteration.
figure 4.3 shows the change in the value function over successive sweeps of value iteration, and the    nal
policy found, for the case of ph = 0.4. this policy is optimal, but not unique. in fact, there is a whole
family of optimal policies, all corresponding to ties for the argmax action selection with respect to the
optimal value function. can you guess what the entire family looks like?

figure 4.3: the solution to the gambler   s problem for ph = 0.4. the upper graph shows the value function
found by successive sweeps of value iteration. the lower graph shows the    nal policy.

exercise 4.8 why does the optimal policy for the gambler   s problem have such a curious form? in
particular, for capital of 50 it bets it all on one    ip, but for capital of 51 it does not. why is this a
(cid:3)
good policy?

99755025111020304050100.20.40.60.8125507599capitalcapitalvalueestimatesfinalpolicy(stake)sweep 1sweep 2sweep 3sweep 324.5. asynchronous id145

69

exercise 4.9 (programming) implement value iteration for the gambler   s problem and solve it for
ph = 0.25 and ph = 0.55. in programming, you may    nd it convenient to introduce two dummy states
corresponding to termination with capital of 0 and 100, giving them values of 0 and 1 respectively.
(cid:3)
show your results graphically, as in figure 4.3. are your results stable as        0?
exercise 4.10 what is the analog of the value iteration update (4.10) for action values, qk+1(s, a)? (cid:3)

4.5 asynchronous id145

a major drawback to the dp methods that we have discussed so far is that they involve operations
over the entire state set of the mdp, that is, they require sweeps of the state set. if the state set is very
large, then even a single sweep can be prohibitively expensive. for example, the game of backgammon
has over 1020 states. even if we could perform the value iteration update on a million states per second,
it would take over a thousand years to complete a single sweep.

asynchronous dp algorithms are in-place iterative dp algorithms that are not organized in terms
of systematic sweeps of the state set. these algorithms update the values of states in any order
whatsoever, using whatever values of other states happen to be available. the values of some states
may be updated several times before the values of others are updated once. to converge correctly,
however, an asynchronous algorithm must continue to update the values of all the states: it can   t ignore
any state after some point in the computation. asynchronous dp algorithms allow great    exibility in
selecting states to update.

for example, one version of asynchronous value iteration updates the value, in place, of only one
state, sk, on each step, k, using the value iteration update (4.10). if 0        < 1, asymptotic convergence
to v    is guaranteed given only that all states occur in the sequence {sk} an in   nite number of times
(the sequence could even be stochastic). (in the undiscounted episodic case, it is possible that there are
some orderings of updates that do not result in convergence, but it is relatively easy to avoid these.)
similarly, it is possible to intermix policy evaluation and value iteration updates to produce a kind
of asynchronous truncated policy iteration. although the details of this and other more unusual dp
algorithms are beyond the scope of this book, it is clear that a few di   erent updates form building
blocks that can be used    exibly in a wide variety of sweepless dp algorithms.

of course, avoiding sweeps does not necessarily mean that we can get away with less computation. it
just means that an algorithm does not need to get locked into any hopelessly long sweep before it can
make progress improving a policy. we can try to take advantage of this    exibility by selecting the states
to which we apply updates so as to improve the algorithm   s rate of progress. we can try to order the
updates to let value information propagate from state to state in an e   cient way. some states may not
need their values updated as often as others. we might even try to skip updating some states entirely
if they are not relevant to optimal behavior. some ideas for doing this are discussed in chapter 8.

asynchronous algorithms also make it easier to intermix computation with real-time interaction. to
solve a given mdp, we can run an iterative dp algorithm at the same time that an agent is actually
experiencing the mdp. the agent   s experience can be used to determine the states to which the dp
algorithm applies its updates. at the same time, the latest value and policy information from the dp
algorithm can guide the agent   s decision making. for example, we can apply updates to states as the
agent visits them. this makes it possible to focus the dp algorithm   s updates onto parts of the state
set that are most relevant to the agent. this kind of focusing is a repeated theme in reinforcement
learning.

70

chapter 4. id145

4.6 generalized policy iteration

policy iteration consists of two simultaneous, interacting processes, one making the value function
consistent with the current policy (policy evaluation), and the other making the policy greedy with
respect to the current value function (policy improvement). in policy iteration, these two processes
alternate, each completing before the other begins, but this is not really necessary. in value iteration, for
example, only a single iteration of policy evaluation is performed in between each policy improvement.
in asynchronous dp methods, the evaluation and improvement processes are interleaved at an even
   ner grain. in some cases a single state is updated in one process before returning to the other. as long
as both processes continue to update all states, the ultimate result is typically the same   convergence
to the optimal value function and an optimal policy.

we use the term generalized policy iteration (gpi) to refer to the general
idea of letting policy evaluation and policy improvement processes interact,
independent of the granularity and other details of the two processes. al-
most all id23 methods are well described as gpi. that
is, all have identi   able policies and value functions, with the policy always
being improved with respect to the value function and the value function
always being driven toward the value function for the policy, as suggested
by the diagram to the right. it is easy to see that if both the evaluation
process and the improvement process stabilize, that is, no longer produce
changes, then the value function and policy must be optimal. the value
function stabilizes only when it is consistent with the current policy, and
the policy stabilizes only when it is greedy with respect to the current value
function. thus, both processes stabilize only when a policy has been found
that is greedy with respect to its own evaluation function. this implies
that the bellman optimality equation (4.1) holds, and thus that the policy
and the value function are optimal.

the evaluation and improvement processes in gpi can be viewed as both

competing and cooperating. they compete in the sense that they pull in opposing directions. making
the policy greedy with respect to the value function typically makes the value function incorrect for the
changed policy, and making the value function consistent with the policy typically causes that policy
no longer to be greedy. in the long run, however, these two processes interact to    nd a single joint
solution: the optimal value function and an optimal policy.

one might also think of the interaction between
the evaluation and improvement processes in gpi
in terms of two constraints or goals   for example,
as two lines in two-dimensional space as suggested
by the diagram to the right. although the real ge-
ometry is much more complicated than this, the
diagram suggests what happens in the real case.
each process drives the value function or policy
toward one of the lines representing a solution to
one of the two goals. the goals interact because
the two lines are not orthogonal. driving directly
toward one goal causes some movement away from
the other goal. inevitably, however, the joint pro-
cess is brought closer to the overall goal of optimality. the arrows in this diagram correspond to the
behavior of policy iteration in that each takes the system all the way to achieving one of the two goals
completely.
in either case,
the two processes together achieve the overall goal of optimality even though neither is attempting to

in gpi one could also take smaller, incomplete steps toward each goal.

evaluationimprovement    greedy(v)v   v v   v         v   ,         =greedy(v)v,   v=v   4.7. efficiency of id145

71

achieve it directly.

4.7 e   ciency of id145

dp may not be practical for very large problems, but compared with other methods for solving mdps,
dp methods are actually quite e   cient. if we ignore a few technical details, then the (worst case) time
dp methods take to    nd an optimal policy is polynomial in the number of states and actions. if n and k
denote the number of states and actions, this means that a dp method takes a number of computational
operations that is less than some polynomial function of n and k. a dp method is guaranteed to    nd an
optimal policy in polynomial time even though the total number of (deterministic) policies is kn. in this
sense, dp is exponentially faster than any direct search in policy space could be, because direct search
would have to exhaustively examine each policy to provide the same guarantee. id135
methods can also be used to solve mdps, and in some cases their worst-case convergence guarantees
are better than those of dp methods. but id135 methods become impractical at a much
smaller number of states than do dp methods (by a factor of about 100). for the largest problems,
only dp methods are feasible.

dp is sometimes thought to be of limited applicability because of the curse of dimensionality, the fact
that the number of states often grows exponentially with the number of state variables. large state sets
do create di   culties, but these are inherent di   culties of the problem, not of dp as a solution method.
in fact, dp is comparatively better suited to handling large state spaces than competing methods such
as direct search and id135.

in practice, dp methods can be used with today   s computers to solve mdps with millions of states.
both policy iteration and value iteration are widely used, and it is not clear which, if either, is better
in general. in practice, these methods usually converge much faster than their theoretical worst-case
run times, particularly if they are started with good initial value functions or policies.

on problems with large state spaces, asynchronous dp methods are often preferred. to complete
even one sweep of a synchronous method requires computation and memory for every state. for some
problems, even this much memory and computation is impractical, yet the problem is still potentially
solvable because relatively few states occur along optimal solution trajectories. asynchronous methods
and other variations of gpi can be applied in such cases and may    nd good or optimal policies much
faster than synchronous methods can.

4.8 summary

in this chapter we have become familiar with the basic ideas and algorithms of id145
as they relate to solving    nite mdps. policy evaluation refers to the (typically) iterative computation
of the value functions for a given policy. policy improvement refers to the computation of an improved
policy given the value function for that policy. putting these two computations together, we obtain
policy iteration and value iteration, the two most popular dp methods. either of these can be used to
reliably compute optimal policies and value functions for    nite mdps given complete knowledge of the
mdp.

classical dp methods operate in sweeps through the state set, performing an expected update op-
eration on each state. each such operation updates the value of one state based on the values of all
possible successor states and their probabilities of occurring. expected updates are closely related to
bellman equations: they are little more than these equations turned into assignment statements. when
the updates no longer result in any changes in value, convergence has occurred to values that satisfy the
corresponding bellman equation. just as there are four primary value functions (v  , v   , q  , and q   ),
there are four corresponding bellman equations and four corresponding expected updates. an intuitive

72

chapter 4. id145

view of the operation of dp updates is given by their backup diagrams.

insight into dp methods and, in fact, into almost all id23 methods, can be gained by
viewing them as generalized policy iteration (gpi). gpi is the general idea of two interacting processes
revolving around an approximate policy and an approximate value function. one process takes the
policy as given and performs some form of policy evaluation, changing the value function to be more
like the true value function for the policy. the other process takes the value function as given and
performs some form of policy improvement, changing the policy to make it better, assuming that the
value function is its value function. although each process changes the basis for the other, overall they
work together to    nd a joint solution: a policy and value function that are unchanged by either process
and, consequently, are optimal. in some cases, gpi can be proved to converge, most notably for the
classical dp methods that we have presented in this chapter. in other cases convergence has not been
proved, but still the idea of gpi improves our understanding of the methods.

it is not necessary to perform dp methods in complete sweeps through the state set. asynchronous
dp methods are in-place iterative methods that update states in an arbitrary order, perhaps stochas-
tically determined and using out-of-date information. many of these methods can be viewed as    ne-
grained forms of gpi.

finally, we note one last special property of dp methods. all of them update estimates of the values
of states based on estimates of the values of successor states. that is, they update estimates on the
basis of other estimates. we call this general idea id64. many id23 methods
perform id64, even those that do not require, as dp requires, a complete and accurate model
of the environment. in the next chapter we explore id23 methods that do not require a
model and do not bootstrap. in the chapter after that we explore methods that do not require a model
but do bootstrap. these key features and properties are separable, yet can be mixed in interesting
combinations.

bibliographical and historical remarks

the term    id145    is due to bellman (1957a), who showed how these methods could be
applied to a wide range of problems. extensive treatments of dp can be found in many texts, including
bertsekas (2005, 2012), bertsekas and tsitsiklis (1996), dreyfus and law (1977), ross (1983), white
(1969), and whittle (1982, 1983). our interest in dp is restricted to its use in solving mdps, but dp
also applies to other types of problems. kumar and kanal (1988) provide a more general look at dp.

to the best of our knowledge, the    rst connection between dp and id23 was made
by minsky (1961) in commenting on samuel   s checkers player. in a footnote, minsky mentioned that
it is possible to apply dp to problems in which samuel   s backing-up process can be handled in closed
analytic form. this remark may have misled arti   cial intelligence researchers into believing that dp
was restricted to analytically tractable problems and therefore largely irrelevant to arti   cial intelli-
gence. andreae (1969b) mentioned dp in the context of id23, speci   cally policy
iteration, although he did not make speci   c connections between dp and learning algorithms. wer-
bos (1977) suggested an approach to approximating dp called    heuristic id145    that
emphasizes gradient-descent methods for continuous-state problems (werbos, 1982, 1987, 1988, 1989,
1992). these methods are closely related to the id23 algorithms that we discuss in
this book. watkins (1989) was explicit in connecting id23 to dp, characterizing a
class of id23 methods as    incremental id145.   

4.1   4 these sections describe well-established dp algorithms that are covered in any of the general
dp references cited above. the policy improvement theorem and the policy iteration algorithm
are due to bellman (1957a) and howard (1960). our presentation was in   uenced by the local
view of policy improvement taken by watkins (1989). our discussion of value iteration as a

4.8. summary

73

form of truncated policy iteration is based on the approach of puterman and shin (1978), who
presented a class of algorithms called modi   ed policy iteration, which includes policy iteration
and value iteration as special cases. an analysis showing how value iteration can be made to
   nd an optimal policy in    nite time is given by bertsekas (1987).

iterative policy evaluation is an example of a classical successive approximation algorithm for
solving a system of linear equations. the version of the algorithm that uses two arrays, one
holding the old values while the other is updated, is often called a jacobi-style algorithm,
after jacobi   s classical use of this method. it is also sometimes called a synchronous algorithm
because the e   ect is as if all the values are updated at the same time. the second array is needed
to simulate this parallel computation sequentially. the in-place version of the algorithm is often
called a gauss   seidel-style algorithm after the classical gauss   seidel algorithm for solving
systems of linear equations. in addition to iterative policy evaluation, other dp algorithms can
be implemented in these di   erent versions. bertsekas and tsitsiklis (1989) provide excellent
coverage of these variations and their performance di   erences.

4.5

asynchronous dp algorithms are due to bertsekas (1982, 1983), who also called them dis-
tributed dp algorithms. the original motivation for asynchronous dp was its implementation
on a multiprocessor system with communication delays between processors and no global syn-
chronizing clock. these algorithms are extensively discussed by bertsekas and tsitsiklis (1989).
jacobi-style and gauss   seidel-style dp algorithms are special cases of the asynchronous ver-
sion. williams and baird (1990) presented dp algorithms that are asynchronous at a    ner
grain than the ones we have discussed: the update operations themselves are broken into steps
that can be performed asynchronously.

4.7

this section, written with the help of michael littman, is based on littman, dean, and kael-
bling (1995). the phrase    curse of dimensionality    is due to bellman (1957).

74

chapter 4. id145

chapter 5

monte carlo methods

in this chapter we consider our    rst learning methods for estimating value functions and discovering
optimal policies. unlike the previous chapter, here we do not assume complete knowledge of the
environment. monte carlo methods require only experience   sample sequences of states, actions, and
rewards from actual or simulated interaction with an environment. learning from actual experience
is striking because it requires no prior knowledge of the environment   s dynamics, yet can still attain
optimal behavior. learning from simulated experience is also powerful. although a model is required,
the model need only generate sample transitions, not the complete id203 distributions of all
possible transitions that is required for id145 (dp). in surprisingly many cases it is
easy to generate experience sampled according to the desired id203 distributions, but infeasible
to obtain the distributions in explicit form.

monte carlo methods are ways of solving the id23 problem based on averaging
sample returns. to ensure that well-de   ned returns are available, here we de   ne monte carlo methods
only for episodic tasks. that is, we assume experience is divided into episodes, and that all episodes
eventually terminate no matter what actions are selected. only on the completion of an episode are
value estimates and policies changed. monte carlo methods can thus be incremental in an episode-by-
episode sense, but not in a step-by-step (online) sense. the term    monte carlo    is often used more
broadly for any estimation method whose operation involves a signi   cant random component. here we
use it speci   cally for methods based on averaging complete returns (as opposed to methods that learn
from partial returns, considered in the next chapter).

monte carlo methods sample and average returns for each state   action pair much like the bandit
methods we explored in chapter 2 sample and average rewards for each action. the main di   erence is
that now there are multiple states, each acting like a di   erent bandit problem (like an associative-search
or contextual bandit) and the di   erent bandit problems are interrelated. that is, the return after taking
an action in one state depends on the actions taken in later states in the same episode. because all the
action selections are undergoing learning, the problem becomes nonstationary from the point of view
of the earlier state.

to handle the nonstationarity, we adapt the idea of general policy iteration (gpi) developed in
chapter 4 for dp. whereas there we computed value functions from knowledge of the mdp, here
we learn value functions from sample returns with the mdp. the value functions and corresponding
policies still interact to attain optimality in essentially the same way (gpi). as in the dp chapter,    rst
we consider the prediction problem (the computation of v   and q   for a    xed arbitrary policy   ) then
policy improvement, and,    nally, the control problem and its solution by gpi. each of these ideas taken
from dp is extended to the monte carlo case in which only sample experience is available.

75

76

chapter 5. monte carlo methods

5.1 monte carlo prediction

we begin by considering monte carlo methods for learning the state-value function for a given policy.
recall that the value of a state is the expected return   expected cumulative future discounted reward   
starting from that state. an obvious way to estimate it from experience, then, is simply to average the
returns observed after visits to that state. as more returns are observed, the average should converge
to the expected value. this idea underlies all monte carlo methods.

in particular, suppose we wish to estimate v  (s), the value of a state s under policy   , given a set
of episodes obtained by following    and passing through s. each occurrence of state s in an episode
is called a visit to s. of course, s may be visited multiple times in the same episode; let us call the
   rst time it is visited in an episode the    rst visit to s. the    rst-visit mc method estimates v  (s)
as the average of the returns following    rst visits to s, whereas the every-visit mc method averages
the returns following all visits to s. these two monte carlo (mc) methods are very similar but have
slightly di   erent theoretical properties. first-visit mc has been most widely studied, dating back to the
1940s, and is the one we focus on in this chapter. every-visit mc extends more naturally to function
approximation and eligibility traces, as discussed in chapters 9 and 12. first-visit mc is shown in
procedural form in the box.

first-visit mc prediction, for estimating v     v  
initialize:

       policy to be evaluated
v     an arbitrary state-value function
returns(s)     an empty list, for all s     s

repeat forever:

generate an episode using   
for each state s appearing in the episode:

g     the return that follows the    rst occurrence of s
append g to returns(s)
v (s)     average(returns(s))

both    rst-visit mc and every-visit mc converge to v  (s) as the number of visits (or    rst visits)
to s goes to in   nity. this is easy to see for the case of    rst-visit mc. in this case each return is an
independent, identically distributed estimate of v  (s) with    nite variance. by the law of large numbers
the sequence of averages of these estimates converges to their expected value. each average is itself
an unbiased estimate, and the standard deviation of its error falls as 1/   n, where n is the number of
returns averaged. every-visit mc is less straightforward, but its estimates also converge quadratically
to v  (s) (singh and sutton, 1996).

the use of monte carlo methods is best illustrated through an example.

example 5.1: blackjack the object of the popular casino card game of blackjack is to obtain cards
the sum of whose numerical values is as great as possible without exceeding 21. all face cards count as
10, and an ace can count either as 1 or as 11. we consider the version in which each player competes
independently against the dealer. the game begins with two cards dealt to both dealer and player. one
of the dealer   s cards is face up and the other is face down. if the player has 21 immediately (an ace and
a 10-card), it is called a natural. he then wins unless the dealer also has a natural, in which case the
game is a draw. if the player does not have a natural, then he can request additional cards, one by one
(hits), until he either stops (sticks) or exceeds 21 (goes bust). if he goes bust, he loses; if he sticks, then
it becomes the dealer   s turn. the dealer hits or sticks according to a    xed strategy without choice: he
sticks on any sum of 17 or greater, and hits otherwise. if the dealer goes bust, then the player wins;
otherwise, the outcome   win, lose, or draw   is determined by whose    nal sum is closer to 21.

5.1. monte carlo prediction

77

playing blackjack is naturally formulated as an episodic    nite mdp. each game of blackjack is an
episode. rewards of +1,    1, and 0 are given for winning, losing, and drawing, respectively. all rewards
within a game are zero, and we do not discount (   = 1); therefore these terminal rewards are also the
returns. the player   s actions are to hit or to stick. the states depend on the player   s cards and the
dealer   s showing card. we assume that cards are dealt from an in   nite deck (i.e., with replacement) so
that there is no advantage to keeping track of the cards already dealt. if the player holds an ace that he
could count as 11 without going bust, then the ace is said to be usable. in this case it is always counted
as 11 because counting it as 1 would make the sum 11 or less, in which case there is no decision to be
made because, obviously, the player should always hit. thus, the player makes decisions on the basis
of three variables: his current sum (12   21), the dealer   s one showing card (ace   10), and whether or not
he holds a usable ace. this makes for a total of 200 states.

consider the policy that sticks if the player   s sum is 20 or 21, and otherwise hits. to    nd the state-
value function for this policy by a monte carlo approach, one simulates many blackjack games using
the policy and averages the returns following each state. note that in this task the same state never
recurs within one episode, so there is no di   erence between    rst-visit and every-visit mc methods. in
this way, we obtained the estimates of the state-value function shown in figure 5.1. the estimates for
states with a usable ace are less certain and less regular because these states are less common. in any
event, after 500,000 games the value function is very well approximated.

although we have complete knowledge of the environment in this task, it would not be easy to apply
dp methods to compute the value function. dp methods require the distribution of next events   in
particular, they require the environments dynamics as given by the four-argument function p   and
it is not easy to determine this for blackjack. for example, suppose the player   s sum is 14 and he
chooses to stick. what is his id203 of terminating with with a reward of +1 as a function of the
dealer   s showing card? all of the probabilities must be computed before dp can be applied, and such
computations are often complex and error-prone. in contrast, generating the sample games required by
monte carlo methods is easy. this is the case surprisingly often; the ability of monte carlo methods to
work with sample episodes alone can be a signi   cant advantage even when one has complete knowledge
of the environment   s dynamics.

figure 5.1: approximate state-value functions for the blackjack policy that sticks only on 20 or 21, computed
by monte carlo policy evaluation.

+1!1adealer showing1012player sum21after 500,000 episodesafter 10,000 episodesusableacenousableace78

chapter 5. monte carlo methods

can we generalize the idea of backup diagrams to monte carlo algorithms? the general idea of
an backup diagram is to show at the top the root node to be updated and to show below all the
transitions and leaf nodes whose rewards and estimated values contribute to the update. for monte
carlo estimation of v  , the root is a state node, and below it is the entire trajectory of transitions
along a particular single episode, ending at the terminal state, as shown to the right. whereas the dp
diagram (page 47) shows all possible transitions, the monte carlo diagram shows only those sampled on
the one episode. whereas the dp diagram includes only one-step transitions, the monte carlo diagram
goes all the way to the end of the episode. these di   erences in the diagrams accurately re   ect the
fundamental di   erences between the algorithms.

an important fact about monte carlo methods is that the estimates for each state are inde-
pendent. the estimate for one state does not build upon the estimate of any other state, as is
the case in dp. in other words, monte carlo methods do not bootstrap as we de   ned it in the
previous chapter.

in particular, note that the computational expense of estimating the value of a single state is
independent of the number of states. this can make monte carlo methods particularly attractive
when one requires the value of only one or a subset of states. one can generate many sample
episodes starting from the states of interest, averaging returns from only these states, ignoring
all others. this is a third advantage monte carlo methods can have over dp methods (after
the ability to learn from actual experience and from simulated experience).

example 5.2: soap bubble
suppose a wire frame forming a closed loop is dunked in soapy water to form a soap surface or
bubble conforming at its edges to the wire frame. if the geometry of the wire frame is irregular but
known, how can you compute the shape of the surface? the shape has the property that the total force
on each point exerted by neighboring points is zero (or else the shape would change). this means that
the surface   s height at any point is the average of its heights at points in a small circle around that
point. in addition, the surface must meet at its boundaries with the wire frame. the usual approach
to problems of this kind is to put a grid over the area covered by the surface and solve for its height
at the grid points by an iterative computation. grid points at the boundary are forced to the wire
frame, and all others are adjusted toward the average of the heights of their four nearest neighbors.
this process then iterates, much like dp   s iterative policy evaluation, and ultimately converges to a
close approximation to the desired surface.

this is similar to the kind of problem for which monte
carlo methods were originally designed. instead of the
iterative computation described above, imagine stand-
ing on the surface and taking a random walk, stepping
randomly from grid point to neighboring grid point,
with equal id203, until you reach the boundary.
it turns out that the expected value of the height at
the boundary is a close approximation to the height of
the desired surface at the starting point (in fact, it is
exactly the value computed by the iterative method de-
scribed above). thus, one can closely approximate the
height of the surface at a point by simply averaging the
boundary heights of many walks started at the point. if
one is interested in only the value at one point, or any
   xed small set of points, then this monte carlo method
can be far more e   cient than the iterative method based on local consistency.

a bubble on a wire loop

5.2. monte carlo estimation of action values

79

exercise 5.1 consider the diagrams on the right in figure 5.1. why does the estimated value function
jump up for the last two rows in the rear? why does it drop o    for the whole last row on the left? why
(cid:3)
are the frontmost values higher in the upper diagrams than in the lower?

5.2 monte carlo estimation of action values

if a model is not available, then it is particularly useful to estimate action values (the values of state   
action pairs) rather than state values. with a model, state values alone are su   cient to determine a
policy; one simply looks ahead one step and chooses whichever action leads to the best combination of
reward and next state, as we did in the chapter on dp. without a model, however, state values alone
are not su   cient. one must explicitly estimate the value of each action in order for the values to be
useful in suggesting a policy. thus, one of our primary goals for monte carlo methods is to estimate
q   . to achieve this, we    rst consider the policy evaluation problem for action values.

the policy evaluation problem for action values is to estimate q  (s, a), the expected return when
starting in state s, taking action a, and thereafter following policy   . the monte carlo methods for
this are essentially the same as just presented for state values, except now we talk about visits to a
state   action pair rather than to a state. a state   action pair s, a is said to be visited in an episode if
ever the state s is visited and action a is taken in it. the every-visit mc method estimates the value
of a state   action pair as the average of the returns that have followed all the visits to it. the    rst-visit
mc method averages the returns following the    rst time in each episode that the state was visited and
the action was selected. these methods converge quadratically, as before, to the true expected values
as the number of visits to each state   action pair approaches in   nity.

the only complication is that many state   action pairs may never be visited. if    is a deterministic
policy, then in following    one will observe returns only for one of the actions from each state. with
no returns to average, the monte carlo estimates of the other actions will not improve with experience.
this is a serious problem because the purpose of learning action values is to help in choosing among
the actions available in each state. to compare alternatives we need to estimate the value of all the
actions from each state, not just the one we currently favor.

this is the general problem of maintaining exploration, as discussed in the context of the k-armed
bandit problem in chapter 2. for policy evaluation to work for action values, we must assure continual
exploration. one way to do this is by specifying that the episodes start in a state   action pair, and that
every pair has a nonzero id203 of being selected as the start. this guarantees that all state   action
pairs will be visited an in   nite number of times in the limit of an in   nite number of episodes. we call
this the assumption of exploring starts.

the assumption of exploring starts is sometimes useful, but of course it cannot be relied upon in
general, particularly when learning directly from actual interaction with an environment. in that case
the starting conditions are unlikely to be so helpful. the most common alternative approach to assuring
that all state   action pairs are encountered is to consider only policies that are stochastic with a nonzero
id203 of selecting all actions in each state. we discuss two important variants of this approach in
later sections. for now, we retain the assumption of exploring starts and complete the presentation of
a full monte carlo control method.

exercise 5.2 what is the backup diagram for monte carlo estimation of q  ?

(cid:3)

80

chapter 5. monte carlo methods

5.3 monte carlo control

we are now ready to consider how monte carlo estimation can be used in
control, that is, to approximate optimal policies. the overall idea is to pro-
ceed according to the same pattern as in the dp chapter, that is, according
to the idea of generalized policy iteration (gpi). in gpi one maintains both
an approximate policy and an approximate value function. the value func-
tion is repeatedly altered to more closely approximate the value function for
the current policy, and the policy is repeatedly improved with respect to the
current value function, as suggested by the diagram to the right. these two
kinds of changes work against each other to some extent, as each creates
a moving target for the other, but together they cause both policy and value function to approach
optimality.

to begin, let us consider a monte carlo version of classical policy iteration.

in this method, we
perform alternating complete steps of policy evaluation and policy improvement, beginning with an
arbitrary policy   0 and ending with the optimal policy and optimal action-value function:

i         1

e       q  0
i            
  0
e       denotes a complete policy evaluation and

e       q  1

e             

i         2

e       q   ,

i       denotes a complete policy improvement.
where
policy evaluation is done exactly as described in the preceding section. many episodes are experienced,
with the approximate action-value function approaching the true function asymptotically. for the
moment, let us assume that we do indeed observe an in   nite number of episodes and that, in addition,
the episodes are generated with exploring starts. under these assumptions, the monte carlo methods
will compute each q  k exactly, for arbitrary   k.

policy improvement is done by making the policy greedy with respect to the current value function.
in this case we have an action-value function, and therefore no model is needed to construct the greedy
policy. for any action-value function q, the corresponding greedy policy is the one that, for each s     s,
deterministically chooses an action with maximal action-value:

  (s)

.
= arg max

a

q(s, a).

(5.1)

policy improvement then can be done by constructing each   k+1 as the greedy policy with respect to
q  k . the policy improvement theorem (section 4.2) then applies to   k and   k+1 because, for all s     s,

q  k (s,   k+1(s)) = q  k (s, argmax

a

q  k (s, a))

a

= max
q  k (s, a)
    q  k (s,   k(s))
    v  k (s).

as we discussed in the previous chapter, the theorem assures us that each   k+1 is uniformly better than
  k, or just as good as   k, in which case they are both optimal policies. this in turn assures us that the
overall process converges to the optimal policy and optimal value function. in this way monte carlo
methods can be used to    nd optimal policies given only sample episodes and no other knowledge of the
environment   s dynamics.

we made two unlikely assumptions above in order to easily obtain this guarantee of convergence for
the monte carlo method. one was that the episodes have exploring starts, and the other was that
policy evaluation could be done with an in   nite number of episodes. to obtain a practical algorithm
we will have to remove both assumptions. we postpone consideration of the    rst assumption until later
in this chapter.

evaluationimprovement   q    greedy(q)q q   5.3. monte carlo control

81

for now we focus on the assumption that policy evaluation operates on an in   nite number of episodes.
this assumption is relatively easy to remove. in fact, the same issue arises even in classical dp methods
such as iterative policy evaluation, which also converge only asymptotically to the true value function.
in both dp and monte carlo cases there are two ways to solve the problem. one is to hold    rm to
the idea of approximating q  k in each policy evaluation. measurements and assumptions are made to
obtain bounds on the magnitude and id203 of error in the estimates, and then su   cient steps are
taken during each policy evaluation to assure that these bounds are su   ciently small. this approach
can probably be made completely satisfactory in the sense of guaranteeing correct convergence up to
some level of approximation. however, it is also likely to require far too many episodes to be useful in
practice on any but the smallest problems.

there is a second approach to avoiding the in   nite number of episodes nominally required for policy
evaluation, in which we give up trying to complete policy evaluation before returning to policy im-
provement. on each evaluation step we move the value function toward q  k , but we do not expect to
actually get close except over many steps. we used this idea when we    rst introduced the idea of gpi
in section 4.6. one extreme form of the idea is value iteration, in which only one iteration of iterative
policy evaluation is performed between each step of policy improvement. the in-place version of value
iteration is even more extreme; there we alternate between improvement and evaluation steps for single
states.

for monte carlo policy evaluation it is natural to alternate between evaluation and improvement on
an episode-by-episode basis. after each episode, the observed returns are used for policy evaluation,
and then the policy is improved at all the states visited in the episode. a complete simple algorithm
along these lines, which we call monte carlo es, for monte carlo with exploring starts, is given in the
box.

monte carlo es (exploring starts), for estimating             
initialize, for all s     s, a     a(s):

q(s, a)     arbitrary
  (s)     arbitrary
returns(s, a)     empty list

repeat forever:

choose s0     s and a0     a(s0) s.t. all pairs have id203 > 0
generate an episode starting from s0, a0, following   
for each pair s, a appearing in the episode:

g     the return that follows the    rst occurrence of s, a
append g to returns(s, a)
q(s, a)     average(returns(s, a))
for each s in the episode:
  (s)     arg maxa q(s, a)

in monte carlo es, all the returns for each state   action pair are accumulated and averaged, irre-
spective of what policy was in force when they were observed. it is easy to see that monte carlo es
cannot converge to any suboptimal policy. if it did, then the value function would eventually converge
to the value function for that policy, and that in turn would cause the policy to change. stability is
achieved only when both the policy and the value function are optimal. convergence to this optimal
   xed point seems inevitable as the changes to the action-value function decrease over time, but has not
yet been formally proved. in our opinion, this is one of the most fundamental open theoretical questions
in id23 (for a partial solution, see tsitsiklis, 2002).

82

chapter 5. monte carlo methods

example 5.3: solving blackjack
it is straightforward to apply monte carlo es to blackjack.
since the episodes are all simulated games, it is easy to arrange for exploring starts that include all
possibilities. in this case one simply picks the dealer   s cards, the player   s sum, and whether or not the
player has a usable ace, all at random with equal id203. as the initial policy we use the policy
evaluated in the previous blackjack example, that which sticks only on 20 or 21. the initial action-value
function can be zero for all state   action pairs. figure 5.2 shows the optimal policy for blackjack found
by monte carlo es. this policy is the same as the    basic    strategy of thorp (1966) with the sole
exception of the leftmost notch in the policy for a usable ace, which is not present in thorp   s strategy.
we are uncertain of the reason for this discrepancy, but con   dent that what is shown here is indeed the
optimal policy for the version of blackjack we have described.

figure 5.2: the optimal policy and state-value function for blackjack, found by monte carlo es (figure
5.4). the state-value function shown was computed from the action-value function found by monte carlo es.

5.4 monte carlo control without exploring starts

how can we avoid the unlikely assumption of exploring starts? the only general way to ensure that all
actions are selected in   nitely often is for the agent to continue to select them. there are two approaches
to ensuring this, resulting in what we call on-policy methods and o   -policy methods. on-policy methods
attempt to evaluate or improve the policy that is used to make decisions, whereas o   -policy methods
evaluate or improve a policy di   erent from that used to generate the data. the monte carlo es method
developed above is an example of an on-policy method. in this section we show how an on-policy monte
carlo control method can be designed that does not use the unrealistic assumption of exploring starts.
o   -policy methods are considered in the next section.

in on-policy control methods the policy is generally soft, meaning that   (a|s) > 0 for all s     s and
all a     a(s), but gradually shifted closer and closer to a deterministic optimal policy. many of the
methods discussed in chapter 2 provide mechanisms for this. the on-policy method we present in this
section uses   -greedy policies, meaning that most of the time they choose an action that has maximal
estimated action value, but with id203    they instead select an action at random. that is, all
, and the remaining bulk of the
nongreedy actions are given the minimal id203 of selection,

 

|a(s)|

usableacenousableace2010a23456789dealer showingplayer sumhitstick19211112131415161718!*10a23456789hitstick2019211112131415161718v*211012adealer showingplayer sum10a1221+1"1v*usableacenousableace2010a23456789dealer showingplayer sumhitstick19211112131415161718!*10a23456789hitstick2019211112131415161718v*211012adealer showingplayer sum10a1221+1"1v*usableacenousableace2010a23456789dealer showingplayer sumhitstick19211112131415161718!*10a23456789hitstick2019211112131415161718v*211012adealer showingplayer sum10a1221+1"1v*dealer showingplayer sum**5.4. monte carlo control without exploring starts

83

id203, 1        +  
|a(s)|
policies, de   ned as policies for which   (a|s)      
|a(s)|
  -soft policies,   -greedy policies are in some sense those that are closest to greedy.

, is given to the greedy action. the   -greedy policies are examples of   -soft
for all states and actions, for some    > 0. among

the overall idea of on-policy monte carlo control is still that of gpi. as in monte carlo es, we
use    rst-visit mc methods to estimate the action-value function for the current policy. without the
assumption of exploring starts, however, we cannot simply improve the policy by making it greedy
with respect to the current value function, because that would prevent further exploration of nongreedy
actions. fortunately, gpi does not require that the policy be taken all the way to a greedy policy, only
that it be moved toward a greedy policy. in our on-policy method we will move it only to an   -greedy
policy. for any   -soft policy,   , any   -greedy policy with respect to q   is guaranteed to be better than
or equal to   . the complete algorithm is given in the box below.

on-policy    rst-visit mc control (for   -soft policies), estimates             
initialize, for all s     s, a     a(s):

q(s, a)     arbitrary
returns(s, a)     empty list
  (a|s)     an arbitrary   -soft policy

repeat forever:

(a) generate an episode using   
(b) for each pair s, a appearing in the episode:

g     the return that follows the    rst occurrence of s, a
append g to returns(s, a)
q(s, a)     average(returns(s, a))
(c) for each s in the episode:
a        arg maxa q(s, a)
for all a     a(s):

(cid:26) 1        +   /|a(s)|

  (a|s)    

  /|a(s)|

if a = a   
if a (cid:54)= a   

(with ties broken arbitrarily)

that any   -greedy policy with respect to q   is an improvement over any   -soft policy    is assured
by the policy improvement theorem. let   (cid:48) be the   -greedy policy. the conditions of the policy
improvement theorem apply because for any s     s:

q  (s,   (cid:48)(s)) = (cid:88)a

 

  (cid:48)(a|s)q  (s, a)
|a(s)|(cid:88)a
|a(s)|(cid:88)a

 

=

   

=

q  (s, a) + (1       ) max

a

q  (s, a)

(5.2)

q  (s, a) + (1       )(cid:88)a

  (a|s)      
|a(s)|

1       

q  (s, a)

(the sum is a weighted average with nonnegative weights summing to 1, and as such it must be less
than or equal to the largest number averaged)

 

|a(s)|(cid:88)a

q  (s, a)    

 

|a(s)|(cid:88)a

q  (s, a) + (cid:88)a

  (a|s)q  (s, a)

= v  (s).

thus, by the policy improvement theorem,   (cid:48)        (i.e., v  (cid:48)(s)     v  (s), for all s     s). we now prove
that equality can hold only when both   (cid:48) and    are optimal among the   -soft policies, that is, when
they are better than or equal to all other   -soft policies.

84

chapter 5. monte carlo methods

consider a new environment that is just like the original environment, except with the requirement
that policies be   -soft    moved inside    the environment. the new environment has the same action and
state set as the original and behaves as follows. if in state s and taking action a, then with id203
1        the new environment behaves exactly like the old environment. with id203    it repicks the
action at random, with equal probabilities, and then behaves like the old environment with the new,
random action. the best one can do in this new environment with general policies is the same as the

value functions for the new environment. then a policy    is optimal among   -soft policies if and only

best one could do in the original environment with   -soft policies. let (cid:101)v    and (cid:101)q    denote the optimal
if v   =(cid:101)v   . from the de   nition of(cid:101)v    we know that it is the unique solution to

 

(cid:101)v   (s) = (1       ) max
a (cid:101)q   (s, a) +
a (cid:88)s(cid:48),r
|a(s)|(cid:88)a (cid:88)s(cid:48),r

= (1       ) max
 

+

v  (s) = (1       ) max

a

q  (s, a) +

= (1       ) max
 

+

a (cid:88)s(cid:48),r
|a(s)|(cid:88)a (cid:88)s(cid:48),r

|a(s)|(cid:88)a (cid:101)q   (s, a)
p(s(cid:48), r|s, a)(cid:104)r +   (cid:101)v   (s(cid:48))(cid:105)
p(s(cid:48), r|s, a)(cid:104)r +   (cid:101)v   (s(cid:48))(cid:105).
|a(s)|(cid:88)a

p(s(cid:48), r|s, a)(cid:104)r +   v  (s(cid:48))(cid:105)

q  (s, a)

 

p(s(cid:48), r|s, a)(cid:104)r +   v  (s(cid:48))(cid:105).

when equality holds and the   -soft policy    is no longer improved, then we also know, from (5.2), that

however, this equation is the same as the previous one, except for the substitution of v   for(cid:101)v   . since
(cid:101)v    is the unique solution, it must be that v   =(cid:101)v   .

in essence, we have shown in the last few pages that policy iteration works for   -soft policies. using
the natural notion of greedy policy for   -soft policies, one is assured of improvement on every step,
except when the best policy has been found among the   -soft policies. this analysis is independent of
how the action-value functions are determined at each stage, but it does assume that they are computed
exactly. this brings us to roughly the same point as in the previous section. now we only achieve
the best policy among the   -soft policies, but on the other hand, we have eliminated the assumption of
exploring starts.

5.5 o   -policy prediction via importance sampling

all learning control methods face a dilemma: they seek to learn action values conditional on sub-
sequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to
   nd the optimal actions). how can they learn about the optimal policy while behaving according to
an exploratory policy? the on-policy approach in the preceding section is actually a compromise   it
learns action values not for the optimal policy, but for a near-optimal policy that still explores. a more
straightforward approach is to use two policies, one that is learned about and that becomes the optimal
policy, and one that is more exploratory and is used to generate behavior. the policy being learned
about is called the target policy, and the policy used to generate behavior is called the behavior policy.
in this case we say that learning is from data    o       the target policy, and the overall process is termed
o   -policy learning.

5.5. off-policy prediction via importance sampling

85

throughout the rest of this book we consider both on-policy and o   -policy methods. on-policy
methods are generally simpler and are considered    rst. o   -policy methods require additional concepts
and notation, and because the data is due to a di   erent policy, o   -policy methods are often of greater
variance and are slower to converge. on the other hand, o   -policy methods are more powerful and
general. they include on-policy methods as the special case in which the target and behavior policies
are the same. o   -policy methods also have a variety of additional uses in applications. for example,
they can often be applied to learn from data generated by a conventional non-learning controller, or
from a human expert. o   -policy learning is also seen by some as key to learning multi-step predictive
models of the world   s dynamics (sutton, 2009, sutton et al., 2011).

in this section we begin the study of o   -policy methods by considering the prediction problem, in
which both target and behavior policies are    xed. that is, suppose we wish to estimate v   or q  , but
all we have are episodes following another policy b, where b (cid:54)=   . in this case,    is the target policy, b
is the behavior policy, and both policies are considered    xed and given.

in order to use episodes from b to estimate values for   , we require that every action taken under
   is also taken, at least occasionally, under b. that is, we require that   (a|s) > 0 implies b(a|s) >
0. this is called the assumption of coverage.
it follows from coverage that b must be stochastic in
states where it is not identical to   . the target policy   , on the other hand, may be deterministic,
and, in fact, this is a case of particular interest in control problems. in control, the target policy is
typically the deterministic greedy policy with respect to the current action-value function estimate.
this policy becomes a deterministic optimal policy while the behavior policy remains stochastic and
more exploratory, for example, an   -greedy policy. in this section, however, we consider the prediction
problem, in which    is unchanging and given.

almost all o   -policy methods utilize importance sampling, a general technique for estimating expected
values under one distribution given samples from another. we apply importance sampling to o   -policy
learning by weighting returns according to the relative id203 of their trajectories occurring under
the target and behavior policies, called the importance-sampling ratio. given a starting state st, the
id203 of the subsequent state   action trajectory, at, st+1, at+1, . . . , st , occurring under any policy
   is

pr{at, st+1, at+1, . . . , st | st, at:t   1       }

=   (at|st)p(st+1|st, at)  (at+1|st+1)       p(st |st   1, at   1)

=

  (ak|sk)p(sk+1|sk, ak),

t   1(cid:89)k=t

where p here is the state-transition id203 function de   ned by (3.4). thus, the relative id203
of the trajectory under the target and behavior policies (the importance-sampling ratio) is

  t:t   1

.

= (cid:81)t   1
(cid:81)t   1

k=t   (ak|sk)p(sk+1|sk, ak)
k=t b(ak|sk)p(sk+1|sk, ak)

=

t   1(cid:89)k=t

  (ak|sk)
b(ak|sk)

.

(5.3)

although the trajectory probabilities depend on the mdp   s transition probabilities, which are generally
unknown, they appear identically in both the numerator and denominator, and thus cancel. the
importance sampling ratio ends up depending only on the two policies and the sequence, not on the
mdp.

now we are ready to give a monte carlo algorithm that uses a batch of observed episodes following
policy b to estimate v  (s). it is convenient here to number time steps in a way that increases across
episode boundaries. that is, if the    rst episode of the batch ends in a terminal state at time 100, then
the next episode begins at time t = 101. this enables us to use time-step numbers to refer to particular
steps in particular episodes.
in particular, we can de   ne the set of all time steps in which state s

86

chapter 5. monte carlo methods

is visited, denoted t(s). this is for an every-visit method; for a    rst-visit method, t(s) would only
include time steps that were    rst visits to s within their episodes. also, let t (t) denote the    rst time
of termination following time t, and gt denote the return after t up through t (t). then {gt}t   t(s) are

the returns that pertain to state s, and (cid:8)  t:t (t)   1(cid:9)t   t(s) are the corresponding importance-sampling

ratios. to estimate v  (s), we simply scale the returns by the ratios and average the results:

.

(5.4)

when importance sampling is done as a simple average in this way it is called ordinary importance
sampling.

an important alternative is weighted importance sampling, which uses a weighted average, de   ned as

v (s)

.

= (cid:80)t   t(s)   t:t (t)   1gt

|t(s)|

.

= (cid:80)t   t(s)   t:t (t)   1gt
(cid:80)t   t(s)   t:t (t)   1

v (s)

,

(5.5)

or zero if the denominator is zero. to understand these two varieties of importance sampling, consider
their estimates after observing a single return. in the weighted-average estimate, the ratio   t:t (t)   1
for the single return cancels in the numerator and denominator, so that the estimate is equal to the
observed return independent of the ratio (assuming the ratio is nonzero). given that this return was the
only one observed, this is a reasonable estimate, but its expectation is vb(s) rather than v  (s), and in
this statistical sense it is biased. in contrast, the simple average (5.4) is always v  (s) in expectation (it
is unbiased), but it can be extreme. suppose the ratio were ten, indicating that the trajectory observed
is ten times as likely under the target policy as under the behavior policy. in this case the ordinary
importance-sampling estimate would be ten times the observed return. that is, it would be quite far
from the observed return even though the episode   s trajectory is considered very representative of the
target policy.

formally, the di   erence between the two kinds of importance sampling is expressed in their biases and
variances. the ordinary importance-sampling estimator is unbiased whereas the weighted importance-
sampling estimator is biased (the bias converges asymptotically to zero). on the other hand, the
variance of the ordinary importance-sampling estimator is in general unbounded because the variance
of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single
return is one. in fact, assuming bounded returns, the variance of the weighted importance-sampling
estimator converges to zero even if the variance of the ratios themselves is in   nite (precup, sutton,
and dasgupta 2001). in practice, the weighted estimator usually has dramatically lower variance and
is strongly preferred. nevertheless, we will not totally abandon ordinary importance sampling as it
is easier to extend to the approximate methods using function approximation that we explore in the
second part of this book.

a complete every-visit mc algorithm for o   -policy policy evaluation using weighted importance

sampling is given in the next section on page 90.

example 5.4: o   -policy estimation of a blackjack state value
we applied both ordinary and weighted importance-sampling methods to estimate the value of a single
blackjack state from o   -policy data. recall that one of the advantages of monte carlo methods is that
they can be used to evaluate a single state without forming estimates for any other states.
in this
example, we evaluated the state in which the dealer is showing a deuce, the sum of the player   s cards is
13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three
aces). the data was generated by starting in this state then choosing to hit or stick at random with
equal id203 (the behavior policy). the target policy was to stick only on a sum of 20 or 21, as
in example 5.1. the value of this state under the target policy is approximately    0.27726 (this was

5.5. off-policy prediction via importance sampling

87

determined by separately generating one-hundred million episodes using the target policy and averaging
their returns). both o   -policy methods closely approximated this value after 1000 o   -policy episodes
using the random policy. to make sure they did this reliably, we performed 100 independent runs, each
starting from estimates of zero and learning for 10,000 episodes. figure 5.3 shows the resultant learning
curves   the squared error of the estimates of each method as a function of number of episodes, averaged
over the 100 runs. the error approaches zero for both algorithms, but the weighted importance-sampling
method has much lower error at the beginning, as is typical in practice.

figure 5.3: weighted importance sampling produces lower error estimates of the value of a single blackjack
state from o   -policy episodes (see example 5.4).

example 5.5: in   nite variance the estimates of ordinary importance sampling will typically have
in   nite variance, and thus unsatisfactory convergence properties, whenever the scaled returns have
in   nite variance   and this can easily happen in o   -policy learning when trajectories contain loops. a
simple example is shown inset in figure 5.4. there is only one nonterminal state s and two actions,
right and left. the right action causes a deterministic transition to termination, whereas the left action
transitions, with id203 0.9, back to s or, with id203 0.1, on to termination. the rewards are
+1 on the latter transition and otherwise zero. consider the target policy that always selects left. all
episodes under this policy consist of some number (possibly zero) of transitions back to s followed by
termination with a reward and return of +1. thus the value of s under the target policy is 1 (   = 1).
suppose we are estimating this value from o   -policy data using the behavior policy that selects right
and left with equal id203.

the lower part of figure 5.4 shows ten independent runs of the    rst-visit mc algorithm using ordinary
importance sampling. even after millions of episodes, the estimates fail to converge to the correct value
of 1.
in contrast, the weighted importance-sampling algorithm would give an estimate of exactly 1
forever after the    rst episode that ended with the left action. all returns not equal to 1 (that is, ending
with the right action) would be inconsistent with the target policy and thus would have a   t:t (t)   1
of zero and contribute neither to the numerator nor denominator of (5.5). the weighted importance-
sampling algorithm produces a weighted average of only the returns consistent with the target policy,
and all of these would be exactly 1.

we can verify that the variance of the importance-sampling-scaled returns is in   nite in this example
by a simple calculation. the variance of any random variable x is the expected value of the deviation
from its mean   x, which can be written

var[x]

.

= e(cid:104)(cid:0)x       x(cid:1)2(cid:105) = e(cid:2)x 2     2x   x +   x 2(cid:3) = e(cid:2)x 2(cid:3)       x 2.

ordinary importance samplingweighted importance samplingepisodes (log scale)010100100010,000meansquareerror(average over100 runs)02488

chapter 5. monte carlo methods

figure 5.4: ordinary importance sampling produces surprisingly unstable estimates on the one-state mdp
shown inset (example 5.5). the correct estimate here is 1 (   = 1), and, even though this is the expected value
of a sample return (after importance sampling), the variance of the samples is in   nite, and the estimates do not
convergence to this value. these results are for o   -policy    rst-visit mc.

thus, if the mean is    nite, as it is in our case, the variance is in   nite if and only if the expectation of
the square of the random variable is in   nite. thus, we need only show that the expected square of the
importance-sampling-scaled return is in   nite:

eb      (cid:32)t   1(cid:89)t=0

  (at|st)
b(at|st)

g0(cid:33)2       .

to compute this expectation, we break it down into cases based on episode length and termination.
first note that, for any episode ending with the right action, the importance sampling ratio is zero,
because the target policy would never take this action; these episodes thus contribute nothing to the
expectation (the quantity in parenthesis will be zero) and can be ignored. we need only consider episodes
that involve some number (possibly zero) of left actions that transition back to the nonterminal state,
followed by a left action transitioning to termination. all of these episodes have a return of 1, so the
g0 factor can be ignored. to get the expected square we need only consider each length of episode,
multiplying the id203 of the episode   s occurrence by the square of its importance-sampling ratio,
and add these up:

=

1

2    0.1(cid:18) 1
0.5(cid:19)2
2    0.1(cid:18) 1

1

+

1
2    0.9   
1
2    0.9   

1
2    0.9   

+

+       
= 0.1

   (cid:88)k=0

1

1

0.5

0.5(cid:19)2
2    0.1(cid:18) 1
   (cid:88)k=0

0.5

0.9k    2k    2 = 0.2

1.8k =    .

1
0.5

1

0.5(cid:19)2

(the length 1 episode)

(the length 2 episode)

(the length 3 episode)

1100,0001,000,00010,000,000100,000,00020.10.9r=+1s   (left|s)=1leftrightr=0r=0b(left|s)=12v   (s)monte-carlo estimate of           with ordinaryimportance sampling(ten runs)episodes (log scale)110100100010,00005.6.

incremental implementation

89

exercise 5.3 what is the equation analogous to (5.5) for action values q(s, a) instead of state values
(cid:3)
v (s), again given returns generated using b?
exercise 5.4 in learning curves such as those shown in figure 5.3 error generally decreases with
training, as indeed happened for the ordinary importance-sampling method. but for the weighted
importance-sampling method error    rst increased and then decreased. why do you think this happened?
(cid:3)
exercise 5.5 the results with example 5.5 and shown in figure 5.4 used a    rst-visit mc method.
suppose that instead an every-visit mc method was used on the same problem. would the variance of
(cid:3)
the estimator still be in   nite? why or why not?

5.6 incremental implementation

monte carlo prediction methods can be implemented incrementally, on an episode-by-episode basis,
using extensions of the techniques described in chapter 2 (section 2.4). whereas in chapter 2 we
averaged rewards, in monte carlo methods we average returns. in all other respects exactly the same
methods as used in chapter 2 can be used for on-policy monte carlo methods. for o   -policy monte
carlo methods, we need to separately consider those that use ordinary importance sampling and those
that use weighted importance sampling.

in ordinary importance sampling, the returns are scaled by the importance sampling ratio   t:t (t)   1
(5.3), then simply averaged. for these methods we can again use the incremental methods of chapter 2,
but using the scaled returns in place of the rewards of that chapter. this leaves the case of o   -policy
methods using weighted importance sampling. here we have to form a weighted average of the returns,
and a slightly di   erent incremental algorithm is required.

suppose we have a sequence of returns g1, g2, . . . , gn   1, all starting in the same state and each with

a corresponding random weight wi (e.g., wi =   t:t (t)   1). we wish to form the estimate

vn

k=1 wkgk

k=1 wk

.

= (cid:80)n   1
(cid:80)n   1

,

n     2,

(5.6)

and keep it up-to-date as we obtain a single additional return gn. in addition to keeping track of vn,
we must maintain for each state the cumulative sum cn of the weights given to the    rst n returns. the
update rule for vn is

vn+1

.
= vn +

wn

cn(cid:104)gn     vn(cid:105),

n     1,

and

(5.7)

.
= cn + wn+1,

cn+1
.
where c0
= 0 (and v1 is arbitrary and thus need not be speci   ed). the box on the next page contains
a complete episode-by-episode incremental algorithm for monte carlo policy evaluation. the algorithm
is nominally for the o   -policy case, using weighted importance sampling, but applies as well to the
on-policy case just by choosing the target and behavior policies as the same (in which case (   = b),
w is always 1). the approximation q converges to q   (for all encountered state   action pairs) while
actions are selected according to a potentially di   erent policy, b.

exercise 5.6 modify the algorithm for    rst-visit mc policy evaluation (section 5.1) to use the incre-
(cid:3)
mental implementation for sample averages described in section 2.4.
exercise 5.7 derive the weighted-average update rule (5.7) from (5.6). follow the pattern of the
(cid:3)
derivation of the unweighted rule (2.3).

90

chapter 5. monte carlo methods

o   -policy mc prediction, for estimating q     q  
input: an arbitrary target policy   
initialize, for all s     s, a     a(s):

q(s, a)     arbitrary
c(s, a)     0

repeat forever:

b     any policy with coverage of   
generate an episode using b:
g     0
w     1
for t = t     1, t     2, . . . down to 0:

s0, a0, r1, . . . , st   1, at   1, rt , st

g       g + rt+1
c(st, at)     c(st, at) + w
q(st, at)     q(st, at) +
w     w   (at|st)
b(at|st)
if w = 0 then exit for loop

c(st,at) [g     q(st, at)]

w

5.7 o   -policy monte carlo control

we are now ready to present an example of the second class of learning control methods we consider in
this book: o   -policy methods. recall that the distinguishing feature of on-policy methods is that they
estimate the value of a policy while using it for control. in o   -policy methods these two functions are
separated. the policy used to generate behavior, called the behavior policy, may in fact be unrelated
to the policy that is evaluated and improved, called the target policy. an advantage of this separation
is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to
sample all possible actions.

o   -policy monte carlo control methods use one of the techniques presented in the preceding two
sections. they follow the behavior policy while learning about and improving the target policy. these
techniques require that the behavior policy has a nonzero id203 of selecting all actions that might
be selected by the target policy (coverage). to explore all possibilities, we require that the behavior
policy be soft (i.e., that it select all actions in all states with nonzero id203).

the box on the next page shows an o   -policy monte carlo control method, based on gpi and
weighted importance sampling, for estimating       and q   . the target policy              is the greedy policy
with respect to q, which is an estimate of q  . the behavior policy b can be anything, but in order to
assure convergence of    to the optimal policy, an in   nite number of returns must be obtained for each
pair of state and action. this can be assured by choosing b to be   -soft. the policy    converges to
optimal at all encountered states even though actions are selected according to a di   erent soft policy b,
which may change between or even within episodes.

a potential problem is that this method learns only from the tails of episodes, when all of the
remaining actions in the episode are greedy. if nongreedy actions are common, then learning will be slow,
particularly for states appearing in the early portions of long episodes. potentially, this could greatly
slow learning. there has been insu   cient experience with o   -policy monte carlo methods to assess how
serious this problem is. if it is serious, the most important way to address it is probably by incorporating
temporal-di   erence learning, the algorithmic idea developed in the next chapter. alternatively, if    is
less than 1, then the idea developed in the next section may also help signi   cantly.

5.7. off-policy monte carlo control

91

o   -policy mc control, for estimating             
initialize, for all s     s, a     a(s):

q(s, a)     arbitrary
c(s, a)     0
  (s)     arg maxa q(st, a)

(with ties broken consistently)

repeat forever:

b     any soft policy
generate an episode using b:
g     0
w     1
for t = t     1, t     2, . . . down to 0:

s0, a0, r1, . . . , st   1, at   1, rt , st

g       g + rt+1
c(st, at)     c(st, at) + w
q(st, at)     q(st, at) +
  (st)     arg maxa q(st, a)
if at (cid:54)=   (st) then exit for loop
w     w 1

w

b(at|st)

c(st,at) [g     q(st, at)]

(with ties broken consistently)

exercise 5.8: racetrack (programming) consider driving a race car around a turn like those
shown in figure 5.5. you want to go as fast as possible, but not so fast as to run o    the track. in our
simpli   ed racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. the
velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. the
actions are increments to the velocity components. each may be changed by +1,    1, or 0 in one step,
for a total of nine actions. both velocity components are restricted to be nonnegative and less than 5,
and they cannot both be zero except at the starting line. each episode begins in one of the randomly
selected start states with both velocity components zero and ends when the car crosses the    nish line.
the rewards are    1 for each step until the car crosses the    nish line. if the car hits the track boundary,
it is moved back to a random position on the starting line, both velocity components are reduced to
zero, and the episode continues. before updating the car   s location at each time step, check to see if
the projected path of the car intersects the track boundary. if it intersects the    nish line, the episode

figure 5.5: a couple of right turns for the racetrack task.

starting linefinishlinestarting linefinishline92

chapter 5. monte carlo methods

ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent
back to the starting line. to make the task more challenging, with id203 0.1 at each time step
the velocity increments are both zero, independently of the intended increments. apply a monte carlo
control method to this task to compute the optimal policy from each starting state. exhibit several
(cid:3)
trajectories following the optimal policy (but turn the noise o    for these trajectories).

5.8 *discounting-aware importance sampling

the o   -policy methods that we have considered so far are based on forming importance-sampling weights
for returns considered as unitary wholes, without taking into account the returns    internal structures as
sums of discounted rewards. we now brie   y consider cutting-edge research ideas for using this structure
to signi   cantly reduce the variance of o   -policy estimators.

for example, consider the case where episodes are long and    is signi   cantly less than 1. for concrete-
ness, say that episodes last 100 steps and that    = 0. the return from time 0 will then be just g0 = r1,
but its importance sampling ratio will be a product of 100 factors,   (a0|s0)
b(a99|s99) . in
b(a0|s0)
ordinary importance sampling, the return will be scaled by the entire product, but it is really only
necessary to scale by the    rst factor, by   (a0|s0)
b(a99|s99) are irrele-
vant because after the    rst reward the return has already been determined. these later factors are all
independent of the return and of expected value 1; they do not change the expected update, but they
add enormously to its variance. in some cases they could even make the variance in   nite. let us now
consider an idea for avoiding this large extraneous variance.

b(a0|s0) . the other 99 factors   (a1|s1)

  (a1|s1)
b(a1|s1)          (a99|s99)

b(a1|s1)          (a99|s99)

the essence of the idea is to think of discounting as determining a id203 of termination or,
equivalently, a degree of partial termination. for any        [0, 1), we can think of the return g0 as partly
terminating in one step, to the degree 1       , producing a return of just the    rst reward, r1, and as
partly terminating after two steps, to the degree (1     )  , producing a return of r1 +r2, and so on. the
latter degree corresponds to terminating on the second step, 1       , and not having already terminated
on the    rst step,   . the degree of termination on the third step is thus (1      )  2, with the   2 re   ecting
that termination did not occur on either of the    rst two steps. the partial returns here are called    at
partial returns:

  gt:h

.
= rt+1 + rt+2 +        + rh,

0     t < h     t,

where       at    denotes the absence of discounting, and    partial    denotes that these returns do not extend
all the way to termination but instead stop at h, called the horizon (and t is the time of termination of
the episode). the conventional full return gt can be viewed as a sum of    at partial returns as suggested
above as follows:

gt

.
= rt+1 +   rt+2 +   2rt+3 +        +   t   t   1rt
= (1       )rt+1

+ (1       )   (rt+1 + rt+2)
+ (1       )  2 (rt+1 + rt+2 + rt+3)
...
+ (1       )  t   t   2 (rt+1 + rt+2 +        + rt   1)
+   t   t   1 (rt+1 + rt+2 +        + rt )
= (1       )

  h   t   1   gt:h +   t   t   1   gt:t .

t   1(cid:88)h=t+1

5.9. *per-reward importance sampling

93

now we need to scale the    at partial returns by an importance sampling ratio that is similarly
truncated. as   gt:h only involves rewards up to a horizon h, we only need the ratio of the probabilities
up to h. we de   ne an ordinary importance-sampling estimator, analogous to (5.4), as

= (cid:80)t   t(s)(cid:16)(1       )(cid:80)t (t)   1

.

v (s)

|t(s)|

h=t+1   h   t   1  t:h   1   gt:h +   t (t)   t   1  t:t (t)   1

  gt:t (t)(cid:17)

,

(5.8)

and a weighted importance-sampling estimator, analogous to (5.5), as

v (s)

.

= (cid:80)t   t(s)(cid:16)(1       )(cid:80)t (t)   1

(cid:80)t   t(s)(cid:16)(1       )(cid:80)t (t)   1

h=t+1   h   t   1  t:h   1   gt:h +   t (t)   t   1  t:t (t)   1

h=t+1   h   t   1  t:h   1 +   t (t)   t   1  t:t (t)   1(cid:17)

  gt:t (t)(cid:17)

we call these two estimators discounting-aware importance sampling estimators. they take into account
the discount rate but have no e   ect (are the same as the o   -policy estimators from section 5.5) if    = 1.

.

(5.9)

5.9 *per-reward importance sampling

there is one more way in which the structure of the return as a sum of rewards can be taken into account
in o   -policy importance sampling, a way that may be able to reduce variance even in the absence of
discounting (that is, even if    = 1). in the o   -policy estimators (5.4) and (5.5), each term of the sum
in the numerator is itself a sum:

  t:t   1gt =   t:t   1(cid:0)rt+1 +   rt+2 +        +   t   t   1rt(cid:1)

=   t:t   1rt+1 +     t:t   1rt+2 +        +   t   t   1  t:t   1rt .

(5.10)

the o   -policy estimators rely on the expected values of these terms; let us see if we can write them
in a simpler way. note that each sub-term of (5.10) is a product of a random reward and a random
importance-sampling ratio. for example, the    rst sub-term can be written, using (5.3), as

  t:t   1rt+1 =

  (at|st)
b(at|st)

  (at+1|st+1)
b(at+1|st+1)

  (at+2|st+2)
b(at+2|st+2)       

  (at   1|st   1)
b(at   1|st   1)

rt+1.

now notice that, of all these factors, only the    rst and the last (the reward) are correlated; all the other
ratios are independent random variables whose expected value is one:

e(cid:20)   (ak|sk)
b(ak|sk)(cid:21) .

=(cid:88)a

b(a|sk)

  (a|sk)
b(a|sk)

=(cid:88)a

  (a|sk) = 1.

(5.11)

thus, because the expectation of the product of independent random variables is the product of their
expectations, all the ratios except the    rst drop out in expectation, leaving just

e[  t:t   1rt+1] = e[  t:trt+1] .

if we repeat this analysis for the kth term of (5.10), we get

e[  t:t   1rt+k] = e[  t:t+k   1rt+k] .

it follows then that the expectation of our original term (5.10) can be written

e[  t:t   1gt] = e(cid:104)   gt(cid:105) ,

94

where

chapter 5. monte carlo methods

  gt =   t:trt+1 +     t:t+1rt+2 +   2  t:t+2rt+3 +        +   t   t   1  t:t   1rt .

we call this idea per-reward importance sampling.
it follows immediately that there is an alter-
nate importance-sampling estimator, with the same unbiased expectation as the ordinary-importance-
sampling estimator (5.4), using   gt:

v (s)

.

= (cid:80)t   t(s)

|t(s)|

  gt

,

(5.12)

which we might expect to sometimes be of lower variance.

is there a per-reward version of weighted importance sampling? this is less clear. so far, all the
estimators that have been proposed for this that we know of are not consistent (that is, they do not
converge to the true value with in   nite data).

   exercise 5.9 modify the algorithm for o   -policy monte carlo control (page 91) to use the idea of the
truncated weighted-average estimator (5.9). note that you will    rst need to convert this equation to
(cid:3)
action values.

5.10 summary

the monte carlo methods presented in this chapter learn value functions and optimal policies from
experience in the form of sample episodes. this gives them at least three kinds of advantages over
dp methods. first, they can be used to learn optimal behavior directly from interaction with the
environment, with no model of the environment   s dynamics. second, they can be used with simulation
or sample models. for surprisingly many applications it is easy to simulate sample episodes even though
it is di   cult to construct the kind of explicit model of transition probabilities required by dp methods.
third, it is easy and e   cient to focus monte carlo methods on a small subset of the states. a region of
special interest can be accurately evaluated without going to the expense of accurately evaluating the
rest of the state set (we explore this further in chapter 8).

a fourth advantage of monte carlo methods, which we discuss later in the book, is that they may
be less harmed by violations of the markov property. this is because they do not update their value
estimates on the basis of the value estimates of successor states. in other words, it is because they do
not bootstrap.

in designing monte carlo control methods we have followed the overall schema of generalized policy
iteration (gpi) introduced in chapter 4. gpi involves interacting processes of policy evaluation and
policy improvement. monte carlo methods provide an alternative policy evaluation process. rather
than use a model to compute the value of each state, they simply average many returns that start in the
state. because a state   s value is the expected return, this average can become a good approximation to
the value. in control methods we are particularly interested in approximating action-value functions,
because these can be used to improve the policy without requiring a model of the environment   s tran-
sition dynamics. monte carlo methods intermix policy evaluation and policy improvement steps on an
episode-by-episode basis, and can be incrementally implemented on an episode-by-episode basis.

maintaining su   cient exploration is an issue in monte carlo control methods.

it is not enough
just to select the actions currently estimated to be best, because then no returns will be obtained for
alternative actions, and it may never be learned that they are actually better. one approach is to ignore
this problem by assuming that episodes begin with state   action pairs randomly selected to cover all
possibilities. such exploring starts can sometimes be arranged in applications with simulated episodes,
but are unlikely in learning from real experience. in on-policy methods, the agent commits to always

5.10. summary

95

exploring and tries to    nd the best policy that still explores.
explores, but learns a deterministic optimal policy that may be unrelated to the policy followed.

in o   -policy methods, the agent also

o   -policy prediction refers to learning the value function of a target policy from data generated by a
di   erent behavior policy. such learning methods are based on some form of importance sampling, that
is, on weighting returns by the ratio of the probabilities of taking the observed actions under the two
policies. ordinary importance sampling uses a simple average of the weighted returns, whereas weighted
importance sampling uses a weighted average. ordinary importance sampling produces unbiased es-
timates, but has larger, possibly in   nite, variance, whereas weighted importance sampling always has
   nite variance and is preferred in practice. despite their conceptual simplicity, o   -policy monte carlo
methods for both prediction and control remain unsettled and are a subject of ongoing research.

the monte carlo methods treated in this chapter di   er from the dp methods treated in the previous
chapter in two major ways. first, they operate on sample experience, and thus can be used for direct
learning without a model. second, they do not bootstrap. that is, they do not update their value
estimates on the basis of other value estimates. these two di   erences are not tightly linked, and can
be separated. in the next chapter we consider methods that learn from experience, like monte carlo
methods, but also bootstrap, like dp methods.

bibliographical and historical remarks

the term    monte carlo    dates from the 1940s, when physicists at los alamos devised games of chance
that they could study to help understand complex physical phenomena relating to the atom bomb.
coverage of monte carlo methods in this sense can be found in several textbooks (e.g., kalos and
whitlock, 1986; rubinstein, 1981).

5.1   2 singh and sutton (1996) distinguished between every-visit and    rst-visit mc methods and
proved results relating these methods to id23 algorithms. the blackjack
example is based on an example used by widrow, gupta, and maitra (1973). the soap bubble
example is a classical dirichlet problem whose monte carlo solution was    rst proposed by
kakutani (1945; see hersh and griego, 1969; doyle and snell, 1984).

barto and du    (1994) discussed policy evaluation in the context of classical monte carlo
algorithms for solving systems of linear equations. they used the analysis of curtiss (1954) to
point out the computational advantages of monte carlo policy evaluation for large problems.

5.3   4 monte carlo es was introduced in the 1998 edition of this book. that may have been the
   rst explicit connection between monte carlo estimation and control methods based on policy
iteration. an early use of monte carlo methods to estimate action values in a reinforcement
learning context was by michie and chambers (1968). in pole balancing (page 44), they used
averages of episode durations to assess the worth (expected balancing    life   ) of each possible
action in each state, and then used these assessments to control action selections. their method
is similar in spirit to monte carlo es with every-visit mc estimates. narendra and wheeler
(1986) studied a monte carlo method for ergodic    nite markov chains that used the return
accumulated between successive visits to the same state as a reward for adjusting a learning
automaton   s action probabilities.

5.5

e   cient o   -policy learning has become recognized as an important challenge that arises in
several    elds. for example, it is closely related to the idea of    interventions    and    counterfac-
tuals    in probabalistic graphical (bayesian) models (e.g., pearl, 1995; balke and pearl, 1994).
o   -policy methods using importance sampling have a long history and yet still are not well

96

5.7

5.8

5.9

chapter 5. monte carlo methods

understood. weighted importance sampling, which is also sometimes called normalized impor-
tance sampling (e.g., koller and friedman, 2009), is discussed by rubinstein (1981), hesterberg
(1988), shelton (2001), and liu (2001) among others.

the target policy in o   -policy learning is sometimes referred to in the literature as the    esti-
mation    policy, as it was in the    rst edition of this book.

the racetrack exercise is adapted from barto, bradtke, and singh (1995), and from gardner
(1973).

our treatment of the idea of discounting-aware importance sampling is based on the analysis
of sutton, mahmood, precup, and van hasselt (2014). it has been worked out most fully to
date by mahmood (in preparation; mahmood, van hasselt, and sutton, 2014).

per-reward importance sampling was introduced by precup, sutton, and singh (2000), who
called it    per-decision    importance sampling. these works also combine o   -policy learning
with temporal-di   erence learning, eligibility traces, and approximation methods, introducing
subtle issues that we consider in later chapters.

chapter 6

temporal-di   erence learning

if one had to identify one idea as central and novel to id23, it would undoubtedly be
temporal-di   erence (td) learning. td learning is a combination of monte carlo ideas and dynamic
programming (dp) ideas. like monte carlo methods, td methods can learn directly from raw expe-
rience without a model of the environment   s dynamics. like dp, td methods update estimates based
in part on other learned estimates, without waiting for a    nal outcome (they bootstrap). the relation-
ship between td, dp, and monte carlo methods is a recurring theme in the theory of reinforcement
learning; this chapter is the beginning of our exploration of it. before we are done, we will see that
these ideas and methods blend into each other and can be combined in many ways. in particular, in
chapter 7 we introduce n-step algorithms, which provide a bridge from td to monte carlo methods,
and in chapter 12 we introduce the td(  ) algorithm, which seaid113ssly uni   es them.

as usual, we start by focusing on the policy evaluation or prediction problem, the problem of esti-
mating the value function v   for a given policy   . for the control problem (   nding an optimal policy),
dp, td, and monte carlo methods all use some variation of generalized policy iteration (gpi). the
di   erences in the methods are primarily di   erences in their approaches to the prediction problem.

6.1 td prediction

both td and monte carlo methods use experience to solve the prediction problem. given some
experience following a policy   , both methods update their estimate v of v   for the nonterminal states
st occurring in that experience. roughly speaking, monte carlo methods wait until the return following
the visit is known, then use that return as a target for v (st). a simple every-visit monte carlo method
suitable for nonstationary environments is

v (st)     v (st) +   (cid:104)gt     v (st)(cid:105),

where gt is the actual return following time t, and    is a constant step-size parameter (c.f., equation
2.4). let us call this method constant-   mc. whereas monte carlo methods must wait until the end
of the episode to determine the increment to v (st) (only then is gt known), td methods need to wait
only until the next time step. at time t + 1 they immediately form a target and make a useful update
using the observed reward rt+1 and the estimate v (st+1). the simplest td method makes the update

v (st)     v (st) +   (cid:104)rt+1 +   v (st+1)     v (st)(cid:105)

immediately on transition to st+1 and receiving rt+1. in e   ect, the target for the monte carlo update
is gt, whereas the target for the td update is rt+1 +   v (st+1). this td method is called td(0), or

(6.1)

(6.2)

97

98

chapter 6. temporal-difference learning

one-step td, because it is a special case of the td(  ) and n-step td methods developed in chapter 12
and chapter 7. the box below speci   es td(0) completely in procedural form.

tabular td(0) for estimating v  

input: the policy    to be evaluated
initialize v (s) arbitrarily (e.g., v (s) = 0, for all s     s+)
repeat (for each episode):

initialize s
repeat (for each step of episode):

v (s)     v (s) +   (cid:2)r +   v (s(cid:48))     v (s)(cid:3)

a     action given by    for s
take action a, observe r, s(cid:48)
s     s(cid:48)

until s is terminal

because td(0) bases its update in part on an existing estimate, we say that it is a id64

method, like dp. we know from chapter 3 that

v  (s)

.
= e  [gt | st = s]
= e  [rt+1 +   gt+1 | st = s]
= e  [rt+1 +   v  (st+1) | st = s] .

(6.3)

(from (3.9))

(6.4)

roughly speaking, monte carlo methods use an estimate of (6.3) as a target, whereas dp methods use
an estimate of (6.4) as a target. the monte carlo target is an estimate because the expected value
in (6.3) is not known; a sample return is used in place of the real expected return. the dp target
is an estimate not because of the expected values, which are assumed to be completely provided by a
model of the environment, but because v  (st+1) is not known and the current estimate, v (st+1), is
used instead. the td target is an estimate for both reasons: it samples the expected values in (6.4)
and it uses the current estimate v instead of the true v  . thus, td methods combine the sampling of
monte carlo with the id64 of dp. as we shall see, with care and imagination this can take us
a long way toward obtaining the advantages of both monte carlo and dp methods.

shown to the right is the backup diagram for tabular td(0). the value estimate for
the state node at the top of the backup diagram is updated on the basis of the one sample
transition from it to the immediately following state. we refer to td and monte carlo
updates as sample updates because they involve looking ahead to a sample successor state
(or state   action pair), using the value of the successor and the reward along the way to
compute a backed-up value, and then updating the value of the original state (or state   
action pair) accordingly. sample updates di   er from the expected updates of dp methods
in that they are based on a single sample successor rather than on a complete distribution
of all possible successors.

td(0)

finally, note that the quantity in brackets in the td(0) update is a sort of error, measuring the
di   erence between the estimated value of st and the better estimate rt+1 +   v (st+1). this quantity,
called the td error, arises in various forms throughout id23:

  t

.
= rt+1 +   v (st+1)     v (st).

(6.5)

notice that the td error at each time is the error in the estimate made at that time. because the td
error depends on the next state and next reward, it is not actually available until one time step later.
that is,   t is the error in v (st), available at time t + 1. also note that if the array v does not change
during the episode (as it does not in monte carlo methods), then the monte carlo error can be written

6.1. td prediction

as a sum of td errors:

gt     v (st) = rt+1 +   gt+1     v (st) +   v (st+1)       v (st+1)

=   t +   (cid:0)gt+1     v (st+1)(cid:1)
=   t +     t+1 +   2(cid:0)gt+2     v (st+2)(cid:1)
=   t +     t+1 +   2  t+2 +        +   t   t   1  t   1 +   t   t(cid:0)gt     v (st )(cid:1)
=   t +     t+1 +   2  t+2 +        +   t   t   1  t   1 +   t   t(cid:0)0     0(cid:1)
t   1(cid:88)k=t

  k   t  k.

=

99

(from (3.9))

(6.6)

this identity is not exact if v is updated during the episode (as it is in td(0)), but if the step size is
small then it may still hold approximately. generalizations of this identity play an important role in
the theory and algorithms of temporal-di   erence learning.

exercise 6.1 if v changes during the episode, then (6.6) only holds approximately; what would the
di   erence be between the two sides? let vt denote the array of state values used at time t in the td
error (6.5) and in the td update (6.2). redo the derivation above to determine the additional amount
(cid:3)
that must be added to the sum of td errors in order to equal the monte carlo error.

example 6.1: driving home each day as you drive home from work, you try to predict how long
it will take to get home. when you leave your o   ce, you note the time, the day of week, the weather,
and anything else that might be relevant. say on this friday you are leaving at exactly 6 o   clock, and
you estimate that it will take 30 minutes to get home. as you reach your car it is 6:05, and you notice
it is starting to rain. tra   c is often slower in the rain, so you reestimate that it will take 35 minutes
from then, or a total of 40 minutes. fifteen minutes later you have completed the highway portion of
your journey in good time. as you exit onto a secondary road you cut your estimate of total travel
time to 35 minutes. unfortunately, at this point you get stuck behind a slow truck, and the road is too
narrow to pass. you end up having to follow the truck until you turn onto the side street where you
live at 6:40. three minutes later you are home. the sequence of states, times, and predictions is thus
as follows:

state
leaving o   ce, friday at 6
reach car, raining
exiting highway
2ndary road, behind truck
entering home street
arrive home

elapsed time

(minutes)

predicted

predicted
time to go total time

0
5
20
30
40
43

30
35
15
10
3
0

30
40
35
40
43
43

the rewards in this example are the elapsed times on each leg of the journey.1 we are not discounting
(   = 1), and thus the return for each state is the actual time to go from that state. the value of each
state is the expected time to go. the second column of numbers gives the current estimated value for
each state encountered.

a simple way to view the operation of monte carlo methods is to plot the predicted total time (the
last column) over the sequence, as in figure 6.1 (left). the arrows show the changes in predictions
recommended by the constant-   mc method (6.1), for    = 1. these are exactly the errors between

1if this were a control problem with the objective of minimizing travel time, then we would of course make the rewards
the negative of the elapsed time. but since we are concerned here only with prediction (policy evaluation), we can keep
things simple by using positive numbers.

100

chapter 6. temporal-difference learning

figure 6.1: changes recommended in the driving home example by monte carlo methods (left) and td
methods (right).

the estimated value (predicted time to go) in each state and the actual return (actual time to go). for
example, when you exited the highway you thought it would take only 15 minutes more to get home,
but in fact it took 23 minutes. equation 6.1 applies at this point and determines an increment in the
estimate of time to go after exiting the highway. the error, gt     v (st), at this time is eight minutes.
suppose the step-size parameter,   , is 1/2. then the predicted time to go after exiting the highway
would be revised upward by four minutes as a result of this experience. this is probably too large a
change in this case; the truck was probably just an unlucky break. in any event, the change can only be
made o   -line, that is, after you have reached home. only at this point do you know any of the actual
returns.

is it necessary to wait until the    nal outcome is known before learning can begin? suppose on another
day you again estimate when leaving your o   ce that it will take 30 minutes to drive home, but then
you become stuck in a massive tra   c jam. twenty-   ve minutes after leaving the o   ce you are still
bumper-to-bumper on the highway. you now estimate that it will take another 25 minutes to get home,
for a total of 50 minutes. as you wait in tra   c, you already know that your initial estimate of 30
minutes was too optimistic. must you wait until you get home before increasing your estimate for the
initial state? according to the monte carlo approach you must, because you don   t yet know the true
return.

according to a td approach, on the other hand, you would learn immediately, shifting your initial
estimate from 30 minutes toward 50. in fact, each estimate would be shifted toward the estimate that
immediately follows it. returning to our    rst day of driving, figure 6.1 (right) shows the changes in
the predictions recommended by the td rule (6.2) (these are the changes made by the rule if    = 1).
each error is proportional to the change over time of the prediction, that is, to the temporal di   erences
in predictions.

besides giving you something to do while waiting in tra   c, there are several computational reasons
why it is advantageous to learn based on your current predictions rather than waiting until termination
when you know the actual return. we brie   y discuss some of these in the next section.

exercise 6.2 this is an exercise to help develop your intuition about why td methods are often more
e   cient than monte carlo methods. consider the driving home example and how it is addressed by
td and monte carlo methods. can you imagine a scenario in which a td update would be better on
average than a monte carlo update? give an example scenario   a description of past experience and
a current state   in which you would expect the td update to be better. here   s a hint: suppose you
have lots of experience driving home from work. then you move to a new building and a new parking
lot (but you still enter the highway at the same place). now you are starting to learn predictions for

road30354045predictedtotaltraveltimeleavingofficeexitinghighway2ndaryhomearrivesituationactual outcomereachcarstreethomeactualoutcomesituation30354045predictedtotaltraveltimeroadleavingofficeexitinghighway2ndaryhomearrivereachcarstreethome6.2. advantages of td prediction methods

101

the new building. can you see why td updates are likely to be much better, at least initially, in this
(cid:3)
case? might the same sort of thing happen in the original task?

6.2 advantages of td prediction methods

td methods update their estimates based in part on other estimates. they learn a guess from a guess   
they bootstrap. is this a good thing to do? what advantages do td methods have over monte carlo
and dp methods? developing and answering such questions will take the rest of this book and more.
in this section we brie   y anticipate some of the answers.

obviously, td methods have an advantage over dp methods in that they do not require a model of

the environment, of its reward and next-state id203 distributions.

the next most obvious advantage of td methods over monte carlo methods is that they are naturally
implemented in an on-line, fully incremental fashion. with monte carlo methods one must wait until
the end of an episode, because only then is the return known, whereas with td methods one need wait
only one time step. surprisingly often this turns out to be a critical consideration. some applications
have very long episodes, so that delaying all learning until the end of the episode is too slow. other
applications are continuing tasks and have no episodes at all. finally, as we noted in the previous
chapter, some monte carlo methods must ignore or discount episodes on which experimental actions
are taken, which can greatly slow learning. td methods are much less susceptible to these problems
because they learn from each transition regardless of what subsequent actions are taken.

but are td methods sound? certainly it is convenient to learn one guess from the next, without
waiting for an actual outcome, but can we still guarantee convergence to the correct answer? happily,
the answer is yes. for any    xed policy   , td(0) has been proved to converge to v  , in the mean for a
constant step-size parameter if it is su   ciently small, and with id203 1 if the step-size parameter
decreases according to the usual stochastic approximation conditions (2.7). most convergence proofs
apply only to the table-based case of the algorithm presented above (6.2), but some also apply to the
case of general linear function approximation. these results are discussed in a more general setting in
chapter 9.

if both td and monte carlo methods converge asymptotically to the correct predictions, then a
natural next question is    which gets there    rst?    in other words, which method learns faster? which
makes the more e   cient use of limited data? at the current time this is an open question in the sense
that no one has been able to prove mathematically that one method converges faster than the other. in
fact, it is not even clear what is the most appropriate formal way to phrase this question! in practice,
however, td methods have usually been found to converge faster than constant-   mc methods on
stochastic tasks, as illustrated in example 6.2.

exercise 6.3 from the results shown in the left graph of the random walk example (on the next page)
it appears that the    rst episode results in a change in only v (a). what does this tell you about what
happened on the    rst episode? why was only the estimate for this one state changed? by exactly how
(cid:3)
much was it changed?

exercise 6.4 the speci   c results shown in the right graph of the random walk example are dependent
on the value of the step-size parameter,   . do you think the conclusions about which algorithm is
better would be a   ected if a wider range of    values were used? is there a di   erent,    xed value of    at
(cid:3)
which either algorithm would have performed signi   cantly better than shown? why or why not?
   exercise 6.5 in the right graph of the random walk example, the rms error of the td method seems
to go down and then up again, particularly at high      s. what could have caused this? do you think
this always occurs, or might it be a function of how the approximate value function was initialized? (cid:3)

102

chapter 6. temporal-difference learning

example 6.2 random walk

in this example we empirically compare the prediction abilities of td(0) and constant-   mc
when applied to the following markov reward process:

a markov reward process, or mrp, is a markov decision process without actions. we will often
use mrps when focusing on the prediction problem, in which there is no need to distinguish
the dynamics due to the environment from those due to the agent. in this mrp, all episodes
start in the center state, c, then proceed either left or right by one state on each step, with
equal id203. episodes terminate either on the extreme left or the extreme right. when an
episode terminates on the right, a reward of +1 occurs; all other rewards are zero. for example, a
typical episode might consist of the following state-and-reward sequence: c, 0, b, 0, c, 0, d, 0, e, 1.
because this task is undiscounted, the true value of each state is the id203 of terminating
on the right if starting from that state. thus, the true value of the center state is v  (c) = 0.5.
the true values of all the states, a through e, are 1

6 , 2

6 , 3

6 , 4

6 , and 5
6 .

the left graph above shows the values learned after various numbers of episodes on a single
run of td(0). the estimates after 100 episodes are about as close as they ever come to the
true values   with a constant step-size parameter (   = 0.1 in this example), the values    uctuate
inde   nitely in response to the outcomes of the most recent episodes. the right graph shows
learning curves for the two methods for various values of   . the performance measure shown
is the root mean-squared (rms) error between the value function learned and the true value
function, averaged over the    ve states, then averaged over 100 runs. in all cases the approximate
value function was initialized to the intermediate value v (s) = 0.5, for all s. the td method
was consistently better than the mc method on this task.

exercise 6.6 in example 6.2 we stated that the true values for the random walk example are 1
and 5
which would you guess we actually used? why?

6 , 4
6 ,
6 , for states a through e. describe at least two di   erent ways that these could have been computed.
(cid:3)

6 , 3

6 , 2

abcde100000start0.800.20.40.6abcde0101100stateestimatedvaluetrue valuesestimatedvalue00.050.10.150.20.250255075100walks / episodestdmc!=.05!=.01!=.1!=.15!=.02!=.04!=.03rms error,averagedover statesempirical rms error, averaged over states6.3. optimality of td(0)

103

6.3 optimality of td(0)

suppose there is available only a    nite amount of experience, say 10 episodes or 100 time steps. in this
case, a common approach with incremental learning methods is to present the experience repeatedly
until the method converges upon an answer. given an approximate value function, v , the increments
speci   ed by (6.1) or (6.2) are computed for every time step t at which a nonterminal state is visited,
but the value function is changed only once, by the sum of all the increments. then all the available
experience is processed again with the new value function to produce a new overall increment, and so
on, until the value function converges. we call this batch updating because updates are made only after
processing each complete batch of training data.

under batch updating, td(0) converges deterministically to a single answer independent of the step-
size parameter,   , as long as    is chosen to be su   ciently small. the constant-   mc method also
converges deterministically under the same conditions, but to a di   erent answer. understanding these
two answers will help us understand the di   erence between the two methods. under normal updating
the methods do not move all the way to their respective batch answers, but in some sense they take
steps in these directions. before trying to understand the two answers in general, for all possible tasks,
we    rst look at a few examples.

example 6.3: random walk under batch updating batch-updating versions of td(0) and
constant-   mc were applied as follows to the random walk prediction example (example 6.2). after
each new episode, all episodes seen so far were treated as a batch. they were repeatedly presented to the
algorithm, either td(0) or constant-   mc, with    su   ciently small that the value function converged.
the resulting value function was then compared with v  , and the average root mean-squared error
across the    ve states (and across 100 independent repetitions of the whole experiment) was plotted to
obtain the learning curves shown in figure 6.2. note that the batch td method was consistently better
than the batch monte carlo method.

figure 6.2: performance of td(0) and constant-   mc under batch training on the random walk task.

under batch training, constant-   mc converges to values, v (s), that are sample averages of the
actual returns experienced after visiting each state s. these are optimal estimates in the sense that
they minimize the mean-squared error from the actual returns in the training set. in this sense it is
surprising that the batch td method was able to perform better according to the root mean-squared
error measure shown in figure 6.2. how is it that batch td was able to perform better than this
optimal method? the answer is that the monte carlo method is optimal only in a limited way, and
that td is optimal in a way that is more relevant to predicting returns. but    rst let   s develop our

.0.05.1.15.2.250255075100tdmcbatch trainingwalks / episodesrms error,averagedover states104

chapter 6. temporal-difference learning

intuitions about di   erent kinds of optimality through another example. consider example 6.4.

example 6.4 you are the predictor

place yourself now in the role of the predictor of returns for an unknown markov reward

process. suppose you observe the following eight episodes:

a, 0, b, 0
b, 1
b, 1
b, 1

b, 1
b, 1
b, 1
b, 0

this means that the    rst episode started in state a, transitioned to b with a reward of 0,
and then terminated from b with a reward of 0. the other seven episodes were even shorter,
starting from b and terminating immediately. given this batch of data, what would you say
are the optimal predictions, the best values for the estimates v (a) and v (b)? everyone would
probably agree that the optimal value for v (b) is 3
4 , because six out of the eight times in state b
the process terminated immediately with a return of 1, and the other two times in b the process
terminated immediately with a return of 0.

but what is the optimal value for the estimate v (a) given this
data? here there are two reasonable answers. one is to observe
that 100% of the times the process was in state a it traversed
immediately to b (with a reward of 0); and since we have already
decided that b has value 3
4 as
well. one way of viewing this answer is that it is based on    rst
modeling the markov process, in this case as shown to the right,
and then computing the correct estimates given the model, which
indeed in this case gives v (a) = 3
4 . this is also the answer that
batch td(0) gives.

4 , therefore a must have value 3

the other reasonable answer is simply to observe that we have seen a once and the return
that followed it was 0; we therefore estimate v (a) as 0. this is the answer that batch monte
carlo methods give. notice that it is also the answer that gives minimum squared error on
the training data. in fact, it gives zero error on the data. but still we expect the    rst answer
to be better.
if the process is markov, we expect that the    rst answer will produce lower
error on future data, even though the monte carlo answer is better on the existing data.

example 6.4 illustrates a general di   erence between the estimates found by batch td(0) and batch
monte carlo methods. batch monte carlo methods always    nd the estimates that minimize mean-
squared error on the training set, whereas batch td(0) always    nds the estimates that would be
exactly correct for the maximum-likelihood model of the markov process. in general, the maximum-
likelihood estimate of a parameter is the parameter value whose id203 of generating the data is
greatest. in this case, the maximum-likelihood estimate is the model of the markov process formed
in the obvious way from the observed episodes: the estimated transition id203 from i to j is
the fraction of observed transitions from i that went to j, and the associated expected reward is the
average of the rewards observed on those transitions. given this model, we can compute the estimate
of the value function that would be exactly correct if the model were exactly correct. this is called the
certainty-equivalence estimate because it is equivalent to assuming that the estimate of the underlying
process was known with certainty rather than being approximated. in general, batch td(0) converges
to the certainty-equivalence estimate.

abr = 1100%75%25%r = 0r = 06.4. sarsa: on-policy td control

105

this helps explain why td methods converge more quickly than monte carlo methods. in batch
form, td(0) is faster than monte carlo methods because it computes the true certainty-equivalence
estimate. this explains the advantage of td(0) shown in the batch results on the random walk task
(figure 6.2). the relationship to the certainty-equivalence estimate may also explain in part the speed
advantage of nonbatch td(0) (e.g., example 6.2, 102, right graph). although the nonbatch methods
do not achieve either the certainty-equivalence or the minimum squared-error estimates, they can be
understood as moving roughly in these directions. nonbatch td(0) may be faster than constant-   mc
because it is moving toward a better estimate, even though it is not getting all the way there. at the
current time nothing more de   nite can be said about the relative e   ciency of on-line td and monte
carlo methods.

finally, it is worth noting that although the certainty-equivalence estimate is in some sense an
optimal solution, it is almost never feasible to compute it directly. if n is the number of states, then
just forming the maximum-likelihood estimate of the process may require n 2 memory, and computing
the corresponding value function requires on the order of n 3 computational steps if done conventionally.
in these terms it is indeed striking that td methods can approximate the same solution using memory
no more than n and repeated computations over the training set. on tasks with large state spaces,
td methods may be the only feasible way of approximating the certainty-equivalence solution.

   exercise 6.7 design an o   -policy version of the td(0) update that can be used with arbitrary target
policy    and covering behavior policy b, using at each step t the importance sampling ratio   t:t (5.3).
(cid:3)

6.4 sarsa: on-policy td control

we turn now to the use of td prediction methods for the control problem. as usual, we follow the
pattern of generalized policy iteration (gpi), only this time using td methods for the evaluation
or prediction part. as with monte carlo methods, we face the need to trade o    exploration and
exploitation, and again approaches fall into two main classes: on-policy and o   -policy. in this section
we present an on-policy td control method.

the    rst step is to learn an action-value function rather than a state-value function. in particular,
for an on-policy method we must estimate q  (s, a) for the current behavior policy    and for all states
s and actions a. this can be done using essentially the same td method described above for learning
v  . recall that an episode consists of an alternating sequence of states and state   action pairs:

in the previous section we considered transitions from state to state and learned the values of states.
now we consider transitions from state   action pair to state   action pair, and learn the values of state   
action pairs. formally these cases are identical: they are both markov chains with a reward process.
the theorems assuring the convergence of state values under td(0) also apply to the corresponding
algorithm for action values:

q(st, at)     q(st, at) +   (cid:104)rt+1 +   q(st+1, at+1)     q(st, at)(cid:105).

this update is done after every transition from a nonterminal state st. if st+1 is terminal,
then q(st+1, at+1) is de   ned as zero. this rule uses every element of the quintuple of events,
(st, at, rt+1, st+1, at+1), that make up a transition from one state   action pair to the next.
this quintuple gives rise to the name sarsa for the algorithm. the backup diagram for sarsa
is as shown to the right.

(6.7)

sarsa

atrt+1stat+1rt+2st+1at+2rt+3st+2at+3st+3. . .. . .106

chapter 6. temporal-difference learning

exercise 6.8 show that an action-value version of (6.6) holds for the action-value form of the td
error   t = rt+1 +   q(st+1, at+1     q(st, at), again assuming that the values don   t change from step to
(cid:3)
step.
it is straightforward to design an on-policy control algorithm based on the sarsa prediction method.
as in all on-policy methods, we continually estimate q   for the behavior policy   , and at the same time
change    toward greediness with respect to q  . the general form of the sarsa control algorithm is given
in the box below.

sarsa (on-policy td control) for estimating q     q   
initialize q(s, a), for all s     s, a     a(s), arbitrarily, and q(terminal-state,  ) = 0
repeat (for each episode):

initialize s
choose a from s using policy derived from q (e.g.,  -greedy)
repeat (for each step of episode):

q(s, a)     q(s, a) +   (cid:2)r +   q(s(cid:48), a(cid:48))     q(s, a)(cid:3)

take action a, observe r, s(cid:48)
choose a(cid:48) from s(cid:48) using policy derived from q (e.g.,  -greedy)
s     s(cid:48); a     a(cid:48);
until s is terminal

the convergence properties of the sarsa algorithm depend on the nature of the policy   s dependence
on q. for example, one could use   -greedy or   -soft policies. sarsa converges with id203 1 to an
optimal policy and action-value function as long as all state   action pairs are visited an in   nite number
of times and the policy converges in the limit to the greedy policy (which can be arranged, for example,
with   -greedy policies by setting    = 1/t).

example 6.5: windy gridworld shown inset in figure 6.3 is a standard gridworld, with start and
goal states, but with one di   erence: there is a crosswind upward through the middle of the grid. the
actions are the standard four   up, down, right, and left   but in the middle region the resultant
next states are shifted upward by a    wind,    the strength of which varies from column to column. the
strength of the wind is given below each column, in number of cells shifted upward. for example, if
you are one cell to the right of the goal, then the action left takes you to the cell just above the goal.
let us treat this as an undiscounted episodic task, with constant rewards of    1 until the goal state is
reached.

figure 6.3: results of sarsa applied to a gridworld (shown inset) in which movement is altered by a location-
dependent, upward    wind.    a trajectory under the optimal policy is also shown.

010002000300040005000600070008000050100150170episodestime stepssg0000111122actions6.5. id24: off-policy td control

107

the graph in figure 6.3 shows the results of applying   -greedy sarsa to this task, with    = 0.1,
   = 0.5, and the initial values q(s, a) = 0 for all s, a. the increasing slope of the graph shows
that the goal is reached more and more quickly over time. by 8000 time steps, the greedy policy
was long since optimal (a trajectory from it is shown inset); continued   -greedy exploration kept the
average episode length at about 17 steps, two more than the minimum of 15. note that monte carlo
methods cannot easily be used on this task because termination is not guaranteed for all policies.
if a policy was ever found that caused the agent to stay in the same state, then the next episode
would never end. step-by-step learning methods such as sarsa do not have this problem because
they quickly learn during the episode that such policies are poor, and switch to something else.

exercise 6.9: windy gridworld with king   s moves re-solve the windy gridworld task assuming eight
possible actions, including the diagonal moves, rather than the usual four. how much better can you do
with the extra actions? can you do even better by including a ninth action that causes no movement
(cid:3)
at all other than that caused by the wind?
exercise 6.10: stochastic wind re-solve the windy gridworld task with king   s moves, assuming
that the e   ect of the wind, if there is any, is stochastic, sometimes varying by 1 from the mean values
given for each column. that is, a third of the time you move exactly according to these values, as in
the previous exercise, but also a third of the time you move one cell above that, and another third of
the time you move one cell below that. for example, if you are one cell to the right of the goal and
you move left, then one-third of the time you move one cell above the goal, one-third of the time you
(cid:3)
move two cells above the goal, and one-third of the time you move to the goal.

6.5 id24: o   -policy td control

one of the early breakthroughs in id23 was the development of an o   -policy td
control algorithm known as id24 (watkins, 1989), de   ned by

q(st, at)     q(st, at) +   (cid:104)rt+1 +    max

a

q(st+1, a)     q(st, at)(cid:105).

(6.8)

in this case, the learned action-value function, q, directly approximates q   , the optimal action-value
function, independent of the policy being followed. this dramatically simpli   es the analysis of the
algorithm and enabled early convergence proofs. the policy still has an e   ect in that it determines
which state   action pairs are visited and updated. however, all that is required for correct convergence
is that all pairs continue to be updated. as we observed in chapter 5, this is a minimal requirement
in the sense that any method guaranteed to    nd optimal behavior in the general case must require it.
under this assumption and a variant of the usual stochastic approximation conditions on the sequence
of step-size parameters, q has been shown to converge with id203 1 to q   .

id24 (o   -policy td control) for estimating             
initialize q(s, a), for all s     s, a     a(s), arbitrarily, and q(terminal-state,  ) = 0
repeat (for each episode):

initialize s
repeat (for each step of episode):

q(s, a)     q(s, a) +   (cid:2)r +    maxa q(s(cid:48), a)     q(s, a)(cid:3)

choose a from s using policy derived from q (e.g.,  -greedy)
take action a, observe r, s(cid:48)
s     s(cid:48)

until s is terminal

108

chapter 6. temporal-difference learning

what is the backup diagram for id24? the rule (6.8) updates a state   action pair, so the top
node, the root of the update, must be a small,    lled action node. the update is also from action nodes,
maximizing over all those actions possible in the next state. thus the bottom nodes of the backup
diagram should be all these action nodes. finally, remember that we indicate taking the maximum of
these    next action    nodes with an arc across them (figure 3.5-right). can you guess now what the
diagram is? if so, please do make a guess before turning to the answer in figure 6.5 on page 109.

example 6.6: cli    walking this gridworld example compares sarsa and id24, highlighting
the di   erence between on-policy (sarsa) and o   -policy (id24) methods. consider the gridworld
shown in the upper part of figure 6.4. this is a standard undiscounted, episodic task, with start and
goal states, and the usual actions causing movement up, down, right, and left. reward is    1 on all
transitions except those into the region marked    the cli   .    stepping into this region incurs a reward
of    100 and sends the agent instantly back to the start.

the lower part of figure 6.4 shows the performance of the sarsa and id24 methods with   -
greedy action selection,    = 0.1. after an initial transient, id24 learns values for the optimal
policy, that which travels right along the edge of the cli   . unfortunately, this results in its occasionally
falling o    the cli    because of the   -greedy action selection. sarsa, on the other hand, takes the action
selection into account and learns the longer but safer path through the upper part of the grid. although
id24 actually learns the values of the optimal policy, its on-line performance is worse than that of
sarsa, which learns the roundabout policy. of course, if    were gradually reduced, then both methods
would asymptotically converge to the optimal policy.

figure 6.4: the cli   -walking task. the results are from a single run, but smoothed by averaging the reward
sums from 10 successive episodes.

exercise 6.11 why is id24 considered an o   -policy control method?

(cid:3)

rewardperepsiode!100!75!50!250100200300400500episodessarsaid24sgr = !100the cliffr = !1safe pathoptimal pathrrsum of rewardsduringepisode6.6. expected sarsa

109

id24

expected sarsa

figure 6.5: the backup diagrams for id24 and expected sarsa.

6.6 expected sarsa

consider the learning algorithm that is just like id24 except that instead of the maximum over
next state   action pairs it uses the expected value, taking into account how likely each action is under
the current policy. that is, consider the algorithm with the update rule

q(st, at)     q(st, at) +   (cid:104)rt+1 +    e[q(st+1, at+1) | st+1]     q(st, at)(cid:105)
  (a|st+1)q(st+1, a)     q(st, at)(cid:105),

    q(st, at) +   (cid:104)rt+1 +   (cid:88)a

(6.9)

but that otherwise follows the schema of id24. given the next state, st+1, this algorithm moves
deterministically in the same direction as sarsa moves in expectation, and accordingly it is called
expected sarsa. its backup diagram is shown on the right in figure 6.5.

expected sarsa is more complex computationally than sarsa but, in return, it eliminates the variance
due to the random selection of at+1. given the same amount of experience we might expect it to
perform slightly better than sarsa, and indeed it generally does. figure 6.6 shows summary results
on the cli   -walking task with expected sarsa compared to sarsa and id24. expected sarsa
retains the signi   cant advantage of sarsa over id24 on this problem. in addition, expected sarsa

figure 6.6: interim and asymptotic performance of td control methods on the cli   -walking task as a function
of   . all algorithms used an   -greedy policy with    = 0.1. asymptotic performance is an average over 100,000
episodes whereas interim performance is an average over the    rst 100 episodes. these data are averages of over
50,000 and 10 runs for the interim and asymptotic cases respectively. the solid circles mark the best interim
performance of each method. adapted from van seijen et al. (2009).

wethenpresentresultsontwoversionsofthewindygridworldproblem,onewithadeterministicenvironmentandonewithastochasticenvironment.wedosoinordertoevaluatethein   uenceofenvironmentstochasticityontheperformancedifferencebetweenexpectedsarsaandsarsaandcon   rmthe   rstpartofhypothesis2.wethenpresentresultsfordifferentamountsofpolicystochasticitytocon   rmthesecondpartofhypothesis2.forcompleteness,wealsoshowtheperformanceofid24onthisproblem.finally,wepresentresultsinotherdomainsverifyingtheadvantagesofexpectedsarsainabroadersetting.allresultspresentedbelowareaveragedovernumerousindependenttrialssuchthatthestandarderrorbecomesnegligible.a.cliffwalkingwebeginbytestinghypothesis1usingthecliffwalkingtask,anundiscounted,episodicnavigationtaskinwhichtheagenthasto   nditswayfromstarttogoalinadeterministicgridworld.alongtheedgeofthegridworldisacliff(seefigure1).theagentcantakeanyoffourmovementactions:up,down,leftandright,eachofwhichmovestheagentonesquareinthecorrespondingdirection.eachstepresultsinarewardof-1,exceptwhentheagentstepsintothecliffarea,whichresultsinarewardof-100andanimmediatereturntothestartstate.theepisodeendsuponreachingthegoalstate.sgfig.1.thecliffwalkingtask.theagenthastomovefromthestart[s]tothegoal[g],whileavoidingsteppingintothecliff(greyarea).weevaluatedtheperformanceoverthe   rstnepisodesasafunctionofthelearningrate  usingan   -greedypolicywith   =0.1.figure2showstheresultforn=100andn=100,000.weaveragedtheresultsover50,000runsand10runs,respectively.discussion.expectedsarsaoutperformsid24andsarsaforalllearningratevalues,con   rminghypothesis1andprovidingsomeevidenceforhypothesis2.theoptimal  valueofexpectedsarsaforn=100is1,whileforsarsaitislower,asexpectedforadeterministicproblem.thattheoptimalvalueofid24isalsolowerthan1issurprising,sinceid24alsohasnostochasticityinitsupdatesinadeterministicenvironment.ourexplanationisthatid24   rstlearnspoliciesthataresub-optimalinthegreedysense,i.e.walkingtowardsthegoalwithadetourfurtherfromthecliff.id24iterativelyoptimizestheseearlypolicies,resultinginapathmorecloselyalongthecliff.however,althoughthispathisbetterintheoff-linesense,intermsofon-lineperformanceitisworse.alargevalueof  ensuresthegoalisreachedquickly,butavaluesomewhatlowerthan1ensuresthattheagentdoesnottrytowalkrightontheedgeofthecliffimmediately,resultinginaslightlybetteron-lineperformance.forn=100,000,theaveragereturnisequalforall  valuesincaseofexpectedsarsaandid24.thisindicatesthatthealgorithmshaveconvergedlongbeforetheendoftherunforall  values,sincewedonotseeanyeffectoftheinitiallearningphase.forsarsatheperformancecomesclosetotheperformanceofexpectedsarsaonlyfor  =0.1,whileforlarge  ,theperformanceforn=100,000evendropsbelowtheperformanceforn=100.thereasonisthatforlargevaluesof  theqvaluesofsarsadiverge.althoughthepolicyisstillimprovedovertheinitialrandompolicyduringtheearlystagesoflearning,divergencecausesthepolicytogetworseinthelongrun.0.10.20.30.40.50.60.70.80.91   160   140   120   100   80   60   40   200alphaaverage return  n = 100, sarsan = 100, q   learningn = 100, expected sarsan = 1e5, sarsan = 1e5, q   learningn = 1e5, expected sarsafig.2.averagereturnonthecliffwalkingtaskoverthe   rstnepisodesforn=100andn=100,000usingan   -greedypolicywith   =0.1.thebigdotsindicatethemaximalvalues.b.windygridworldweturntothewindygridworldtasktofurthertesthy-pothesis2.thewindygridworldtaskisanothernavigationtask,wheretheagenthasto   nditswayfromstarttogoal.thegridhasaheightof7andawidthof10squares.thereisawindblowinginthe   up   directioninthemiddlepartofthegrid,withastrengthof1or2dependingonthecolumn.figure3showsthegridworldwithanumberbeloweachcolumnindicatingthewindstrength.again,theagentcanchoosebetweenfourmovementactions:up,down,leftandright,eachresultinginarewardof-1.theresultofanactionisamovementof1squareinthecorrespondingdirectionplusanadditionalmovementinthe   up   direction,correspondingwiththewindstrength.forexample,whentheagentisinthesquarerightofthegoalandtakesa   left   action,itendsupinthesquarejustabovethegoal.1)deterministicenvironment:we   rstconsiderade-terministicenvironment.asinthecliffwalkingtask,weusean   -greedypolicywith   =0.1.figure4showstheperformanceasafunctionofthelearningrate  overthe   rstnepisodesforn=100andn=100,000.forn=100expected sarsasarsaid24asymptotic performanceinterim performanceid24sum of rewardsper episode   10.10.20.40.60.80.30.50.70.90-40-80-120110

chapter 6. temporal-difference learning

shows a signi   cant improvement over sarsa over a wide range of values for the step-size parameter   .
in cli    walking the state transitions are all deterministic and all randomness comes from the policy.
in such cases, expected sarsa can safely set    = 1 without su   ering any degradation of asymptotic
performance, whereas sarsa can only perform well in the long run at a small value of   , at which
short-term performance is poor. in this and other examples there is a consistent empirical advantage
of expected sarsa over sarsa.

in these cli    walking results expected sarsa was used on-policy, but in general it might use a policy
di   erent from the target policy    to generate behavior, in which case it becomes an o   -policy algo-
rithm. for example, suppose    is the greedy policy while behavior is more exploratory; then expected
sarsa is exactly id24. in this sense expected sarsa subsumes and generalizes id24 while
reliably improving over sarsa. except for the small additional computational cost, expected sarsa may
completely dominate both of the other more-well-known td control algorithms.

6.7 maximization bias and double learning

all the control algorithms that we have discussed so far involve maximization in the construction of
their target policies. for example, in id24 the target policy is the greedy policy given the current
action values, which is de   ned with a max, and in sarsa the policy is often   -greedy, which also involves
a maximization operation. in these algorithms, a maximum over estimated values is used implicitly as
an estimate of the maximum value, which can lead to a signi   cant positive bias. to see why, consider a
single state s where there are many actions a whose true values, q(s, a), are all zero but whose estimated
values, q(s, a), are uncertain and thus distributed some above and some below zero. the maximum
of the true values is zero, but the maximum of the estimates is positive, a positive bias. we call this
maximization bias.

example 6.7: maximization bias example the small mdp shown inset in figure 6.7 provides
a simple example of how maximization bias can harm the performance of td control algorithms. the
mdp has two non-terminal states a and b. episodes always start in a with a choice between two
actions, left and right. the right action transitions immediately to the terminal state with a reward
and return of zero. the left action transitions to b, also with a reward of zero, from which there are

figure 6.7: comparison of id24 and double id24 on a simple episodic mdp (shown inset). q-
learning initially learns to take the left action much more often than the right action, and always takes it
signi   cantly more often than the 5% minimum id203 enforced by   -greedy action selection with    = 0.1. in
contrast, double id24 is essentially una   ected by maximization bias. these data are averaged over 10,000
runs. the initial action-value estimates were zero. any ties in   -greedy action selection were broken randomly.

barightleft0. . .n( 0.1,1)0id24doubleid24episodes1001200300% leftactionsfrom a100%75%50%25%5%0optimal6.7. maximization bias and double learning

111

many possible actions all of which cause immediate termination with a reward drawn from a normal
distribution with mean    0.1 and variance 1.0. thus, the expected return for any trajectory starting
with left is    0.1, and thus taking left in state a is always a mistake. nevertheless, our control methods
may favor left because of maximization bias making b appear to have a positive value. figure 6.7 shows
that id24 with   -greedy action selection initially learns to strongly favor the left action on this
example. even at asymptote, id24 takes the left action about 5% more often than is optimal at
our parameter settings (   = 0.1,    = 0.1, and    = 1).

are there algorithms that avoid maximization bias? to start, consider a bandit case in which we have
noisy estimates of the value of each of many actions, obtained as sample averages of the rewards received
on all the plays with each action. as we discussed above, there will be a positive maximization bias if
we use the maximum of the estimates as an estimate of the maximum of the true values. one way to
view the problem is that it is due to using the same samples (plays) both to determine the maximizing
action and to estimate its value. suppose we divided the plays in two sets and used them to learn two
independent estimates, call them q1(a) and q2(a), each an estimate of the true value q(a), for all a     a.
we could then use one estimate, say q1, to determine the maximizing action a    = argmaxa q1(a), and
the other, q2, to provide the estimate of its value, q2(a   ) = q2(argmaxa q1(a)). this estimate will
then be unbiased in the sense that e[q2(a   )] = q(a   ). we can also repeat the process with the role of
the two estimates reversed to yield a second unbiased estimate q1(argmaxa q2(a)). this is the idea of
double learning. note that although we learn two estimates, only one estimate is updated on each play;
double learning doubles the memory requirements, but does not increase the amount of computation
per step.

the idea of double learning extends naturally to algorithms for full mdps. for example, the double
learning algorithm analogous to id24, called double id24, divides the time steps in two,
perhaps by    ipping a coin on each step. if the coin comes up heads, the update is

q1(st, at)     q1(st, at) +   (cid:104)rt+1 +   q2(cid:0)st+1, argmax

a

q1(st+1, a)(cid:1)     q1(st, at)(cid:105).

(6.10)

if the coin comes up tails, then the same update is done with q1 and q2 switched, so that q2 is updated.
the two approximate value functions are treated completely symmetrically. the behavior policy can
use both action-value estimates. for example, an   -greedy policy for double id24 could be based
on the average (or sum) of the two action-value estimates. a complete algorithm for double id24
is given below. this is the algorithm used to produce the results in figure 6.7. in that example, double
learning seems to eliminate the harm caused by maximization bias. of course there are also double
versions of sarsa and expected sarsa.

double id24
initialize q1(s, a) and q2(s, a), for all s     s, a     a(s), arbitrarily
initialize q1(terminal-state,  ) = q2(terminal-state,  ) = 0
repeat (for each episode):

initialize s
repeat (for each step of episode):

choose a from s using policy derived from q1 and q2 (e.g.,   -greedy in q1 + q2)
take action a, observe r, s(cid:48)
with 0.5 probabilility:

(cid:16)
(cid:16)

(cid:0)s(cid:48), argmaxa q1(s(cid:48), a)(cid:1)     q1(s, a)
(cid:0)s(cid:48), argmaxa q2(s(cid:48), a)(cid:1)     q2(s, a)

(cid:17)
(cid:17)

q1(s, a)     q1(s, a) +   

r +   q2

q2(s, a)     q2(s, a) +   

r +   q1

else:

s     s(cid:48)

until s is terminal

112

chapter 6. temporal-difference learning

   exercise 6.12 what are the update equations for double expected sarsa with an   -greedy target
(cid:3)
policy?

6.8 games, afterstates, and other special cases

in this book we try to present a uniform approach to a wide class of tasks, but of course there are
always exceptional tasks that are better treated in a specialized way. for example, our general approach
involves learning an action-value function, but in chapter 1 we presented a td method for learning
to play tic-tac-toe that learned something much more like a state-value function. if we look closely at
that example, it becomes apparent that the function learned there is neither an action-value function
nor a state-value function in the usual sense. a conventional state-value function evaluates states in
which the agent has the option of selecting an action, but the state-value function used in tic-tac-toe
evaluates board positions after the agent has made its move. let us call these afterstates, and value
functions over these, afterstate value functions. afterstates are useful when we have knowledge of an
initial part of the environment   s dynamics but not necessarily of the full dynamics. for example, in
games we typically know the immediate e   ects of our moves. we know for each possible chess move
what the resulting position will be, but not how our opponent will reply. afterstate value functions are
a natural way to take advantage of this kind of knowledge and thereby produce a more e   cient learning
method.

the reason it is more e   cient to design algorithms in terms of afterstates is apparent from the
tic-tac-toe example. a conventional action-value function would map from positions and moves to an
estimate of the value. but many position   move pairs produce the same resulting position, as in this
example:

in such cases the position   move pairs are di   erent but produce the same    afterposition,    and thus must
have the same value. a conventional action-value function would have to separately assess both pairs,
whereas an afterstate value function would immediately assess both equally. any learning about the
position   move pair on the left would immediately transfer to the pair on the right.

afterstates arise in many tasks, not just games. for example, in queuing tasks there are actions
such as assigning customers to servers, rejecting customers, or discarding information. in such cases
the actions are in fact de   ned in terms of their immediate e   ects, which are completely known.

it is impossible to describe all the possible kinds of specialized problems and corresponding specialized
learning algorithms. however, the principles developed in this book should apply widely. for example,
afterstate methods are still aptly described in terms of generalized policy iteration, with a policy and
(afterstate) value function interacting in essentially the same way. in many cases one will still face the
choice between on-policy and o   -policy methods for managing the need for persistent exploration.

exercise 6.13 describe how the task of jack   s car rental (example 4.2) could be reformulated in
terms of afterstates. why, in terms of this speci   c task, would such a reformulation be likely to speed

xoxxo+xo+xx6.9. summary

convergence?

6.9 summary

113

(cid:3)

in this chapter we introduced a new kind of learning method, temporal-di   erence (td) learning, and
showed how it can be applied to the id23 problem. as usual, we divided the overall
problem into a prediction problem and a control problem. td methods are alternatives to monte carlo
methods for solving the prediction problem. in both cases, the extension to the control problem is via
the idea of generalized policy iteration (gpi) that we abstracted from id145. this is
the idea that approximate policy and value functions should interact in such a way that they both move
toward their optimal values.

one of the two processes making up gpi drives the value function to accurately predict returns for
the current policy; this is the prediction problem. the other process drives the policy to improve locally
(e.g., to be   -greedy) with respect to the current value function. when the    rst process is based on
experience, a complication arises concerning maintaining su   cient exploration. we can classify td
control methods according to whether they deal with this complication by using an on-policy or o   -
policy approach. sarsa is an on-policy method, and id24 is an o   -policy method. expected sarsa
is also an o   -policy method as we present it here. there is a third way in which td methods can
be extended to control which we did not include in this chapter, called actor   critic methods. these
methods are covered in full in chapter 13.

the methods presented in this chapter are today the most widely used id23 meth-
ods. this is probably due to their great simplicity: they can be applied on-line, with a minimal amount
of computation, to experience generated from interaction with an environment; they can be expressed
nearly completely by single equations that can be implemented with small computer programs. in the
next few chapters we extend these algorithms, making them slightly more complicated and signi   cantly
more powerful. all the new algorithms will retain the essence of those introduced here: they will be
able to process experience on-line, with relatively little computation, and they will be driven by td
errors. the special cases of td methods introduced in the present chapter should rightly be called
one-step, tabular, model-free td methods. in the next two chapters we extend them to multistep forms
(a link to monte carlo methods) and forms that include a model of the environment (a link to planning
and id145). then, in the second part of the book we extend them to various forms of
function approximation rather than tables (a link to deep learning and arti   cial neural networks).

finally, in this chapter we have discussed td methods entirely within the context of reinforcement
learning problems, but td methods are actually more general than this. they are general methods for
learning to make long-term predictions about dynamical systems. for example, td methods may be
relevant to predicting    nancial data, life spans, election outcomes, weather patterns, animal behavior,
demands on power stations, or customer purchases.
it was only when td methods were analyzed
as pure prediction methods, independent of their use in id23, that their theoretical
properties    rst came to be well understood. even so, these other potential applications of td learning
methods have not yet been extensively explored.

bibliographical and historical remarks

as we outlined in chapter 1, the idea of td learning has its early roots in animal learning psychology
and arti   cial intelligence, most notably the work of samuel (1959) and klopf (1972). samuel   s work is
described as a case study in section 16.2. also related to td learning are holland   s (1975, 1976) early
ideas about consistency among value predictions. these in   uenced one of the authors (barto), who
was a graduate student from 1970 to 1975 at the university of michigan, where holland was teaching.

114

chapter 6. temporal-difference learning

holland   s ideas led to a number of td-related systems, including the work of booker (1982) and the
bucket brigade of holland (1986), which is related to sarsa as discussed below.

6.1   2 most of the speci   c material from these sections is from sutton (1988), including the td(0)
algorithm, the random walk example, and the term    temporal-di   erence learning.    the charac-
terization of the relationship to id145 and monte carlo methods was in   uenced
by watkins (1989), werbos (1987), and others. the use of backup diagrams was new to the
   rst edition of this book.

tabular td(0) was proved to converge in the mean by sutton (1988) and with id203
1 by dayan (1992), based on the work of watkins and dayan (1992). these results were
extended and strengthened by jaakkola, jordan, and singh (1994) and tsitsiklis (1994) by
using extensions of the powerful existing theory of stochastic approximation. other extensions
and generalizations are covered in later chapters.

the optimality of the td algorithm under batch training was established by sutton (1988).
illuminating this result is barnard   s (1993) derivation of the td algorithm as a combination of
one step of an incremental method for learning a model of the markov chain and one step of a
method for computing predictions from the model. the term certainty equivalence is from the
adaptive control literature (e.g., goodwin and sin, 1984).

the sarsa algorithm was introduced by rummery and niranjan (1994). they explored it in
conjunction with neural networks and called it    modi   ed connectionist id24   . the name
   sarsa    was introduced by sutton (1996). the convergence of one-step tabular sarsa (the form
treated in this chapter) has been proved by singh, jaakkola, littman, and szepesv  ari (2000).
the    windy gridworld    example was suggested by tom kalt.

holland   s (1986) bucket brigade idea evolved into an algorithm closely related to sarsa. the
original idea of the bucket brigade involved chains of rules triggering each other; it focused
on passing credit back from the current rule to the rules that triggered it. over time, the
bucket brigade came to be more like td learning in passing credit back to any temporally
preceding rule, not just to the ones that triggered the current rule. the modern form of the
bucket brigade, when simpli   ed in various natural ways, is nearly identical to one-step sarsa,
as detailed by wilson (1994).

id24 was introduced by watkins (1989), whose outline of a convergence proof was made
rigorous by watkins and dayan (1992). more general convergence results were proved by
jaakkola, jordan, and singh (1994) and tsitsiklis (1994).

expected sarsa was    rst described in an exercise in the    rst edition of this book, then fully
investigated by van seijen, van hasselt, whiteson, and weiring (2009). they established its
convergence properties and conditions under which it will outperform regular sarsa and q-
learning. our figure 6.6 is adapted from their results. our presentation di   ers slightly from
theirs in that they de   ne    expected sarsa    to be an on-policy method exclusively, whereas we
use this name for the general algorithm in which the target and behavior policies are allowed
to di   er. the general o   -policy view of expected sarsa was    rst noted by van hasselt (2011),
who called it    general id24   .

maximization bias and double learning were introduced and extensively investigated by hado
van hasselt (2010, 2011). the example mdp in figure 6.7 was adapted from that in his figure
4.1 (van hasselt, 2011).

the notion of an afterstate is the same as that of a    post-decision state    (van roy, bertsekas,
lee, and tsitsiklis, 1997; powell, 2010).

6.3

6.4

6.5

6.6

6.7

6.8

chapter 7

n-step id64

in this chapter we unify the monte carlo (mc) methods and the one-step temporal-di   erence (td)
methods presented in the previous two chapters. neither mc methods nor one-step td methods are
always the best. in this chapter we present n-step td methods that generalize both methods so that
one can shift from one to the other smoothly as needed to meet the demands of a particular task. n-step
methods span a spectrum with mc methods at one end and one-step td methods at the other. the
best methods are often intermediate between the two extremes.

another way of looking at the bene   ts of n-step methods is that they free you from the tyranny of
the time step. with one-step td methods the same time step determines how often the action can be
changed and the time interval over which id64 is done. in many applications one wants to be
able to update the action very fast to take into account anything that has changed, but id64
works best if it is over a length of time in which a signi   cant and recognizable state change has occurred.
with one-step td methods, these time intervals are the same, and so a compromise must be made.
n-step methods enable id64 to occur over multiple steps, freeing us from the tyranny of the
single time step.

the idea of n-step methods is usually used as an introduction to the algorithmic idea of eligibility
traces (chapter 12), which enable id64 over multiple time intervals simultaneously. here we
instead consider the n-step id64 idea on its own, postponing the treatment of eligibility-trace
mechanisms until later. this allows us to separate the issues better, dealing with as many of them as
possible in the simpler n-step setting.

as usual, we    rst consider the prediction problem and then the control problem. that is, we    rst
consider how n-step methods can help in predicting returns as a function of state for a    xed policy (i.e.,
in estimating v  ). then we extend the ideas to action values and control methods.

7.1 n-step td prediction

what is the space of methods lying between monte carlo and td methods? consider estimating v  
from sample episodes generated using   . monte carlo methods perform an update for each state based
on the entire sequence of observed rewards from that state until the end of the episode. the update
of one-step td methods, on the other hand, is based on just the one next reward, id64 from
the value of the state one step later as a proxy for the remaining rewards. one kind of intermediate
method, then, would perform an update based on an intermediate number of rewards: more than one,
but less than all of them until termination. for example, a two-step update would be based on the    rst
two rewards and the estimated value of the state two steps later. similarly, we could have three-step

115

116

chapter 7. n -step id64

updates, four-step updates, and so on. figure 7.1 shows the backup diagrams of the spectrum of n-step
updates for v  , with the one-step td update on the left and the up-until-termination monte carlo
update on the right.

figure 7.1: the backup diagrams of n-step methods. these methods form a spectrum ranging from one-step
td methods to monte carlo methods.

the methods that use n-step updates are still td methods because they still change an earlier
estimate based on how it di   ers from a later estimate. now the later estimate is not one step later,
but n steps later. methods in which the temporal di   erence extends over n steps are called n-step td
methods. the td methods introduced in the previous chapter all used one-step updates, which is why
we called them one-step td methods.

more formally, consider the update of the estimated value of state st as a result of the state   reward
sequence, st, rt+1, st+1, rt+2, . . . , rt , st (omitting the actions). we know that in monte carlo updates
the estimate of v  (st) is updated in the direction of the complete return:

gt

.
= rt+1 +   rt+2 +   2rt+3 +        +   t   t   1rt ,

where t is the last time step of the episode. let us call this quantity the target of the update. whereas
in monte carlo updates the target is the return, in one-step updates the target is the    rst reward plus
the discounted estimated value of the next state, which we call the one-step return:

gt:t+1

.
= rt+1 +   vt(st+1),

where vt : s     r here is the estimate at time t of v  . the subscripts on gt:t+1 indicate that it is a
truncated return for time t using rewards up until time t + 1, with the discounted estimate   vt(st+1)
taking the place of the other terms   rt+2 +   2rt+3 +        +   t   t   1rt of the full return, as discussed
in the previous chapter. our point now is that this idea makes just as much sense after two steps as it
does after one. the target for a two-step update is the two-step return:

gt:t+2

.
= rt+1 +   rt+2 +   2vt+1(st+2),

where now   2vt+1(st+2) corrects for the absence of the terms   2rt+3 +   3rt+4 +        +   t   t   1rt .
similarly, the target for an arbitrary n-step update is the n-step return:

gt:t+n

.
= rt+1 +   rt+2 +        +   n   1rt+n +   nvt+n   1(st+n),

(7.1)

1-step tdand td(0)2-step td3-step tdn-step td   -step tdand monte carlo                        7.1. n -step td prediction

117

for all n, t such that n     1 and 0     t < t    n. all n-step returns can be considered approximations to the
full return, truncated after n steps and then corrected for the remaining missing terms by vt+n   1(st+n).
if t+n     t (if the n-step return extends to or beyond termination), then all the missing terms are taken
.
= gt if t + n     t ).
as zero, and the n-step return de   ned to be equal to the ordinary full return (gt:t+n
note that n-step returns for n > 1 involve future rewards and states that are not available at the
time of transition from t to t + 1. no real algorithm can use the n-step return until after it has seen
rt+n and computed vt+n   1. the    rst time these are available is t + n. the natural state-value learning
algorithm for using n-step returns is thus

vt+n(st)

.

= vt+n   1(st) +   (cid:2)gt:t+n     vt+n   1(st)(cid:3),

0     t < t,

(7.2)

while the values of all other states remain unchanged: vt+n(s) = vt+n   1(s), for all s(cid:54)= st. we call this
algorithm n-step td. note that no changes at all are made during the    rst n     1 steps of each episode.
to make up for that, an equal number of additional updates are made at the end of the episode, after
termination and before starting the next episode.

n-step td for estimating v     v  
initialize v (s) arbitrarily, s     s
parameters: step size        (0, 1], a positive integer n
all store and access operations (for st and rt) can take their index mod n

repeat (for each episode):

if t < t , then:

initialize and store s0 (cid:54)= terminal
t        for t = 0, 1, 2, . . . :
|
|
|
|
|
|
|
|
|
until    = t     1

g    (cid:80)min(   +n,t )

       t     n + 1
if        0:

  i        1ri

i=   +1

take an action according to   (  |st)
observe and store the next reward as rt+1 and the next state as st+1
if st+1 is terminal, then t     t + 1

(   is the time whose state   s estimate is being updated)

if    + n < t , then: g     g +   nv (s   +n)
v (s   )     v (s   ) +    [g     v (s   )]

(g   :   +n)

exercise 7.1 in chapter 6 we noted that the monte carlo error can be written as the sum of td
errors (6.6) if the value estimates don   t change from step to step. show that the n-step error used in
(7.2) can also be written as a sum td errors (again if the value estimates don   t change) generalizing
(cid:3)
the earlier result.

exercise 7.2 (programming) with an n-step method, the value estimates do change from step to
step, so an algorithm that used the sum of td errors (see previous exercise) in place of the error in
(7.2) would actually be a slightly di   erent algorithm. would it be a better algorithm or a worse one?
(cid:3)
devise and program a small experiment to answer this question empirically.
the n-step return uses the value function vt+n   1 to correct for the missing rewards beyond rt+n.
an important property of n-step returns is that their expectation is guaranteed to be a better estimate
of v   than vt+n   1 is, in a worst-state sense. that is, the worst error of the expected n-step return is

118

chapter 7. n -step id64

guaranteed to be less than or equal to   n times the worst error under vt+n   1:

max

s (cid:12)(cid:12)(cid:12)e  [gt:t+n|st = s]     v  (s)(cid:12)(cid:12)(cid:12)       n max

s (cid:12)(cid:12)(cid:12)vt+n   1(s)     v  (s)(cid:12)(cid:12)(cid:12),

for all n     1. this is called the error reduction property of n-step returns. because of the error reduction
property, one can show formally that all n-step td methods converge to the correct predictions under
appropriate technical conditions. the n-step td methods thus form a family of sound methods, with
one-step td methods and monte carlo methods as extreme members.

(7.3)

example 7.1: n-step td methods on the random walk consider using n-step td methods
on the 5-state random walk task described in example 6.2. suppose the    rst episode progressed directly
from the center state, c, to the right, through d and e, and then terminated on the right with a return
of 1. recall that the estimated values of all the states started at an intermediate value, v (s) = 0.5.
as a result of this experience, a one-step method would change only the estimate for the last state,
v (e), which would be incremented toward 1, the observed return. a two-step method, on the other
hand, would increment the values of the two states preceding termination: v (d) and v (e) both would
be incremented toward 1. a three-step method, or any n-step method for n > 2, would increment the
values of all three of the visited states toward 1, all by the same amount.

figure 7.2: performance of n-step td methods as a function of   , for various values of n, on a 19-state random
walk task (example 7.1).

which value of n is better? figure 7.2 shows the results of a simple empirical test for a larger random
walk process, with 19 states instead of 5 (and with a    1 outcome on the left, all values initialized to 0),
which we use as a running example in this chapter. results are shown for n-step td methods with a
range of values for n and   . the performance measure for each parameter setting, shown on the vertical
axis, is the square-root of the average squared error between the predictions at the end of the episode
for the 19 states and their true values, then averaged over the    rst 10 episodes and 100 repetitions of the
whole experiment (the same sets of walks were used for all parameter settings). note that methods with
an intermediate value of n worked best. this illustrates how the generalization of td and monte carlo
methods to n-step methods can potentially perform better than either of the two extreme methods.

   averagerms errorover 19 statesand    rst 10 episodesn=1n=2n=4n=8n=16n=32n=32n=641285122567.2. n -step sarsa

119

exercise 7.3 why do you think a larger random walk task (19 states instead of 5) was used in the
examples of this chapter? would a smaller walk have shifted the advantage to a di   erent value of n?
how about the change in left-side outcome from 0 to    1 made in the larger walk? do you think that
(cid:3)
made any di   erence in the best value of n?

7.2 n-step sarsa

how can n-step methods be used not just for prediction, but for control? in this section we show
how n-step methods can be combined with sarsa in a straightforward way to produce an on-policy td
control method. the n-step version of sarsa we call n-step sarsa, and the original version presented in
the previous chapter we henceforth call one-step sarsa, or sarsa(0).

the main idea is to simply switch states for actions (state   action pairs) and then use an   -greedy
policy. the backup diagrams for n-step sarsa (shown in figure 7.3), like those of n-step td (figure 7.1),
are strings of alternating states and actions, except that the sarsa ones all start and end with an action
rather a state. we rede   ne n-step returns (update targets) in terms of estimated action values:

gt:t+n

with gt:t+n

.
= rt+1 +   rt+2 +        +   n   1rt+n +   nqt+n   1(st+n, at+n), n     1, 0     t < t     n, (7.4)
.
= gt if t + n     t . the natural algorithm is then

qt+n(st, at)

.
= qt+n   1(st, at) +    [gt:t+n     qt+n   1(st, at)] ,

0     t < t,

(7.5)

while the values of all other states remain unchanged: qt+n(s, a) = qt+n   1(s, a), for all s, a such that
s (cid:54)= st or a (cid:54)= at. this is the algorithm we call n-step sarsa. pseudocode is shown in the box on the
next page, and an example of why it can speed up learning compared to one-step methods is given in
figure 7.4.

figure 7.3: the backup diagrams for the spectrum of n-step methods for state   action values. they range from
the one-step update of sarsa(0) to the up-until-termination update of the monte carlo method. in between are
the n-step updates, based on n steps of real rewards and the estimated value of the nth next state   action pair,
all appropriately discounted. on the far right is the backup diagram for n-step expected sarsa.

1-step sarsaaka sarsa(0)2-step sarsa3-step sarsan-step sarsa   -step sarsaaka monte carlon-step expected sarsa120

chapter 7. n -step id64

n-step sarsa for estimating q     q   , or q     q   for a given   
initialize q(s, a) arbitrarily, for all s     s, a     a
initialize    to be   -greedy with respect to q, or to a    xed given policy
parameters: step size        (0, 1], small    > 0, a positive integer n
all store and access operations (for st, at, and rt) can take their index mod n

repeat (for each episode):

take action at
observe and store the next reward as rt+1 and the next state as st+1
if st+1 is terminal, then:

t     t + 1
select and store an action at+1       (  |st+1)

(   is the time whose estimate is being updated)

if t < t , then:

initialize and store s0 (cid:54)= terminal
select and store an action a0       (  |s0)
t        for t = 0, 1, 2, . . . :
|
|
|
|
|
|
|
|
|
|
|
|
|
until    = t     1

g    (cid:80)min(   +n,t )

       t     n + 1
if        0:

  i        1ri

else:

i=   +1

if    + n < t , then g     g +   nq(s   +n, a   +n)
q(s   , a   )     q(s   , a   ) +    [g     q(s   , a   )]
if    is being learned, then ensure that   (  |s   ) is   -greedy wrt q

(g   :   +n)

figure 7.4: gridworld example of the speedup of policy learning due to the use of n-step methods. the    rst
panel shows the path taken by an agent in a single episode, ending at a location of high reward, marked by
the g. in this example the values were all initially 0, and all rewards were zero except for a positive reward at
g. the arrows in the other two panels show which action values were strengthened as a result of this path by
one-step and n-step sarsa methods. the one-step method strengthens only the last action of the sequence of
actions that led to the high reward, whereas the n-step method strengthens the last n actions of the sequence,
so that much more is learned from the one episode.

what about expected sarsa? the backup diagram for the n-step version of expected sarsa is shown
on the far right in figure 7.3. it consists of a linear string of sample actions and states, just as in n-step
sarsa, except that its last element is a branch over all action possibilities weighted, as always, by their
id203 under   . this algorithm can be described by the same equation as n-step sarsa (above)

path takenaction values increasedby one-step sarsaaction values increasedby sarsa(!) with !=0.9by 10-step sarsaggg7.3. n -step off-policy learning by importance sampling

121

except with the n-step return rede   ned as

gt:t+n

.

= rt+1 +        +   n   1rt+n +   n(cid:88)a

  (a|st+n)qt+n   1(st+n, a),

(7.6)

for all n and t such that n     1 and 0     t     t     n.

7.3 n-step o   -policy learning by importance sampling

recall that o   -policy learning is learning the value function for one policy,   , while following another
policy, b. often,    is the greedy policy for the current action-value-function estimate, and b is a more
exploratory policy, perhaps   -greedy. in order to use the data from b we must take into account the
di   erence between the two policies, using their relative id203 of taking the actions that were taken
(see section 5.5). in n-step methods, returns are constructed over n steps, so we are interested in the
relative id203 of just those n actions. for example, to make a simple o   -policy version of n-step
td, the update for time t (actually made at time t + n) can simply be weighted by   t:t+n   1:

vt+n(st)

.
= vt+n   1(st) +     t:t+n   1 [gt:t+n     vt+n   1(st)] ,

0     t < t,

(7.7)

where   t:t+n   1, called the importance sampling ratio, is the relative id203 under the two policies
of taking the n actions from at to at+n   1 (cf. eq. 5.3):

.
=

  t:h

min(h,t   1)(cid:89)k=t

  (ak|sk)
b(ak|sk)

.

(7.8)

for example, if any one of the actions would never be taken by    (i.e.,   (ak|sk) = 0) then the n-step
return should be given zero weight and be totally ignored. on the other hand, if by chance an action is
taken that    would take with much greater id203 than b does, then this will increase the weight
that would otherwise be given to the return. this makes sense because that action is characteristic of   
(and therefore we want to learn about it) but is selected only rarely by b and thus rarely appears in the
data. to make up for this we have to over-weight it when it does occur. note that if the two policies
are actually the same (the on-policy case) then the importance sampling ratio is always 1. thus our
new update (7.7) generalizes and can completely replace our earlier n-step td update. similarly, our
previous n-step sarsa update can be completely replaced by a simple o   -policy form:

qt+n(st, at)

.
= qt+n   1(st, at) +     t+1:t+n   1 [gt:t+n     qt+n   1(st, at)] ,

(7.9)

for 0     t < t . note that the importance sampling ratio here starts one step later than for n-step td
(above). this is because here we are updating a state   action pair. we do not have to care how likely
we were to select the action; now that we have selected it we want to learn fully from what happens,
with importance sampling only for subsequent actions. pseudocode for the full algorithm is shown in
the box.

the o   -policy version of n-step expected sarsa would use the same update as above for n-step sarsa
except that the importance sampling ratio would have one less factor in it. that is, the above equation
would use   t+1:t+n   2 instead of   t+1:t+n   1, and of course it would use the expected sarsa version of
the n-step return (7.6). this is because in expected sarsa all possible actions are taken into account
in the last state; the one actually taken has no e   ect and does not have to be corrected for.

122

chapter 7. n -step id64

o   -policy n-step sarsa for estimating q     q   , or q     q   for a given   
input: an arbitrary behavior policy b such that b(a|s) > 0, for all s     s, a     a
initialize q(s, a) arbitrarily, for all s     s, a     a
initialize    to be   -greedy with respect to q, or as a    xed given policy
parameters: step size        (0, 1], small    > 0, a positive integer n
all store and access operations (for st, at, and rt) can take their index mod n

repeat (for each episode):

else:

if t < t , then:

initialize and store s0 (cid:54)= terminal
select and store an action a0     b(  |s0)
t        for t = 0, 1, 2, . . . :
|
|
|
|
|
|
|
|
|
|
|
|
|
|
until    = t     1

      (cid:81)min(   +n   1,t   1)
g    (cid:80)min(   +n,t )

       t     n + 1
if        0:

  (ai|si)
b(ai|si)

  i        1ri

i=   +1

i=   +1

take action at
observe and store the next reward as rt+1 and the next state as st+1
if st+1 is terminal, then:

t     t + 1
select and store an action at+1     b(  |st+1)

(   is the time whose estimate is being updated)

if    + n < t , then: g     g +   nq(s   +n, a   +n)
q(s   , a   )     q(s   , a   ) +      [g     q(s   , a   )]
if    is being learned, then ensure that   (  |s   ) is   -greedy wrt q

(     +1:t+n   1)

(g   :   +n)

7.4

*per-reward o   -policy methods

the multi-step o   -policy methods presented in the previous section are very simple and conceptually
clear, but are probably not the most e   cient. a more sophisticated approach would use per-reward
importance sampling ideas such as were introduced in section 5.9. to understand this approach,    rst
note that the ordinary n-step return (7.1), like all returns, can be written recursively:

gt:h = rt+1 +   gt+1:h.

now consider the e   ect of following a behavior policy b (cid:54)=    that is not the same as the target policy
  . all of the resulting experience, including the    rst reward rt+1 and the next state st+1 must be
weighted by the importance sampling ratio for time t,   t =   (at|st)
b(at|st) . one might be tempted to simply
weight the righthand side of the above equation, but one can do better. suppose the action at time t
would never be selected by   , so that   t is zero. then a simple weighting would result in the n-step
return being zero, which could result in high variance when it was used as a target. instead, in this
more sophisticated approach, one uses an alternate, o   -policy de   nition of the n-step return, as

gt:h

.
=   t (rt+1 +   gt+1:h) + (1       t)vh   1(st),
.
where gt:t
= vt   1(st). now, if   t is zero, instead of the target being zero and causing the estimate to
shrink, the target is the same as the estimate and causes no change. the importance sampling ratio

t < h     t,

(7.10)

7.4. *per-reward off-policy methods

123

being zero means we should ignore the sample, so leaving the estimate unchanged seems an appropriate
outcome. notice that the second, additional term does not change the expected update; the importance
sampling ratio has expected value one (section 5.9) and is uncorrelated with the estimate, so the
expected value of the second term is zero. also note that the o   -policy de   nition (7.10) is a strict
generalization of the earlier on-policy de   nition of the n-step return (7.1), as the two are identical in
the on-policy case, in which   t is always 1.

for a conventional n-step method, the learning rule to use in conjunction with (7.10) is the n-step

td update (7.2), which has no explicit importance sampling ratios other than those embedded in g.

exercise 7.4 write the pseudocode for the o   -policy state-value prediction algorithm described above.
(cid:3)

for action values, the o   -policy de   nition of the n-step return is a little di   erent because the    rst
action does not play a role in the importance sampling. we are learning the value of that action and it
does not matter if it was unlikely or even impossible under the target policy. it has been taken and now
full unit weight must be given to the reward and state that follows it. importance sampling will apply
only to the actions that follow it. the o   -policy recursive de   nition of the n-step return for action
values is

.

gt:h

= rt+1 +   (cid:0)  t+1gt+1:h + (1       t+1)   qt+1(cid:1) ,
=(cid:80)a   (a|st)qt   1(st, a). a complete n-step o   -policy action-value prediction algorithm would

with   qt
combine (7.11) and (7.5). if the recursion ends with gt:t
analogous to sarsa, and if gt:t

.
=   qt, then the resultant algorithm is analogous to expected sarsa.

.
= q(st, at), then the resultant algorithm is

t < h     t,

(7.11)

.

exercise 7.5 write the pseudocode for the o   -policy action-value prediction algorithm described
(cid:3)
immediately above. specify both sarsa and expected sarsa variations.
exercise 7.6 show that the general (o   -policy) version of the n-step return (7.10) can still be written
exactly and compactly as the sum of state-based td errors (6.5) if the approximate state value function
(cid:3)
does not change.
exercise 7.7 repeat the above exercise for the action version of the o   -policy n-step return (7.11)
(cid:3)
and the expected sarsa td error (the quantity in brackets in equation 6.9).
exercise 7.8 (programming) devise a small o   -policy prediction problem and use it to show that
the o   -policy learning algorithm using (7.10) and (7.2) is more data e   cient than the simpler algorithm
(cid:3)
using (7.1) and (7.7).
the importance sampling that we have used in this section, the previous section, and in chapter 5
enables o   -policy learning, but at the cost of increasing the variance of the updates. the high variance
forces us to use a small step-size parameter, resulting in slow learning. it is probably inevitable that
o   -policy training is slower than on-policy training   after all, the data is less relevant to what you
are trying to learn. however, it is probably also true that the methods we have presented here can
be improved on. one possibility is to rapidly adapt the step sizes to the observed variance, as in the
autostep method (mahmood et al, 2012). another promising approach is the invariant updates of
karampatziakis and langford (2010) as extended to td by tian (in preparation). the usage technique
of mahmood (2017; mahmood and sutton, 2015) is probably also part of the solution. in the next
section we consider an o   -policy learning method that does not use importance sampling.

124

chapter 7. n -step id64

7.5 o   -policy learning without importance sampling:

the n-step tree backup algorithm

is o   -policy learning possible without importance sampling? id24 and expected sarsa from
chapter 6 do this for the one-step case, but is there a corresponding multi-step algorithm? in this
section we present just such an n-step method, called the tree-backup algorithm.

the idea of the algorithm is suggested by the 3-step tree-backup backup diagram
shown to the right. down the central spine and labeled in the diagram are three sample
states and rewards, and two sample actions. these are the random variables representing
the events occurring after the initial state   action pair st, at. hanging o    to the sides
of each state are the actions that were not selected. (for the last state, all the actions
are considered to have not (yet) been selected.) because we have no sample data for
the unselected actions, we bootstrap and use the estimates of their values in forming
the target for the update. this slightly extends the idea of an backup diagram. so
far we have always updated the estimated value of the node at the top of the diagram
toward a target combining the rewards along the way (appropriately discounted) and
the estimated values of the nodes at the bottom. in the tree-backup update, the target
includes all these things plus the estimated values of the dangling action nodes hanging
o    the sides, at all levels. this is why it is called a tree-backup update; it is an update
from the entire tree of of estimated action values.

more precisely, the update is from the estimated action values of the leaf nodes
of the tree. the action nodes in the interior, corresponding to the actual actions
taken, do not participate. each leaf node contributes to the target with a weight
proportional to its id203 of occurring under the target policy   . thus each
   rst-level action a contributes with a weight of   (a|st+1), except that the action
is
actually taken, at+1, does not contribute at all.
used to weight all the second-level action values. thus, each non-selected second-level action a(cid:48)
contributes with weight   (at+1|st+1)  (a(cid:48)|st+2). each third-level action contributes with weight
  (at+1|st+1)  (at+2|st+2)  (a(cid:48)(cid:48)|st+3), and so on. it is as if each arrow to an action node in the di-
agram is weighted by the action   s id203 of being selected under the target policy and, if there is
a tree below the action, then that weight applies to all the leaf nodes in the tree.

its id203,   (at+1|st+1),

the 3-step
tree-backup

update

we can think of the 3-step tree-backup update as consisting of 6 half-steps, alternating between
sample half-steps from an action to a subsequent state, and expected half-steps considering from that
state all possible actions with their probabilities of occuring under the policy.

the one-step return (target) of the tree-backup algorithm is the same as that of expected sarsa. it

can be written
.

gt:t+1

= rt+1 +   (cid:88)a

=   (cid:48)t + qt   1(st, at),

  (a|st+1)qt(st+1, a)

where   (cid:48)t is a modi   ed form of the td error from expected sarsa:

  (a|st+1)qt(st+1, a)     qt   1(st, at).

(7.12)

  (cid:48)t

.

= rt+1 +   (cid:88)a

with these, the general n-step returns of the tree-backup algorithm can be de   ned recursively, and then
as a sum of td errors:

gt:t+n

.

= rt+1 +    (cid:88)a(cid:54)=at+1
=   (cid:48)t + qt   1(st, at)         (at+1|st+1)qt(st+1, at+1) +     (at+1|st+1)gt+1:t+n

  (a|st+1)qt(st+1, a) +      (at+1|st+1)gt+1:t+n

(7.13)

st,atat+1rt+1st+1st+2rt+2at+2rt+3st+37.5. the n -step tree backup algorithm

125

= qt   1(st, at) +   (cid:48)t +     (at+1|st+1)(cid:0)gt+1:t+n     qt(st+1, at+1)(cid:1)
= qt   1(st, at) +   (cid:48)t +     (at+1|st+1)  (cid:48)t+1 +   2  (at+1|st+1)  (at+2|st+2)  (cid:48)t+2 +       

= qt   1(st, at) +

min(t+n   1,t   1)(cid:88)k=t

  (cid:48)k

k(cid:89)i=t+1

    (ai|si),

under the usual convention that a degenerate product with no factors is 1. this target is then used
with the usual action-value update rule from n-step sarsa:

qt+n(st, at)

.
= qt+n   1(st, at) +    [gt:t+n     qt+n   1(st, at)] ,

(7.5)

while the values of all other state   action pairs remain unchanged: qt+n(s, a) = qt+n   1(s, a), for all
s, a such that s(cid:54)= st or a(cid:54)= at. pseudocode for this algorithm is shown in the box.

n-step tree backup for estimating q     q   , or q     q   for a given   
initialize q(s, a) arbitrarily, for all s     s, a     a
initialize    to be   -greedy with respect to q, or as a    xed given policy
parameters: step size        (0, 1], small    > 0, a positive integer n
all store and access operations can take their index mod n

repeat (for each episode):

take action at
observe the next reward r; observe and store the next state as st+1
if st+1 is terminal:

store r +   (cid:80)a   (a|st+1)q(st+1, a)     qt as   t

select arbitrarily and store an action as at+1
store q(st+1, at+1) as qt+1
store   (at+1|st+1) as   t+1

(   is the time whose estimate is being updated)

else:

if t < t :

t     t + 1
store r     qt as   t

initialize and store s0 (cid:54)= terminal
select and store an action a0       (  |s0)
store q(s0, a0) as q0
t        for t = 0, 1, 2, . . . :
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
until    = t     1

       t     n + 1
if        0:
z     1
g     q  
for k =   , . . . , min(   + n     1, t     1):

g     g + z  k
z       z  k+1

q(s   , a   )     q(s   , a   ) +    [g     q(s   , a   )]
if    is being learned, then ensure that   (a|s   ) is   -greedy wrt q(s   ,  )

126

chapter 7. n -step id64

7.6 *a unifying algorithm: n-step q(  )

so far in this chapter we have considered three di   erent kinds of action-value algorithms, corresponding
to the    rst three backup diagrams shown in figure 7.5. n-step sarsa has all sample transitions, the
tree-backup algorithm has all state-to-action transitions fully branched without sampling, and n-step
expected sarsa has all sample transitions except for the last state-to-action one, which is fully branched
with an expected value. to what extent can these algorithms be uni   ed?

one idea for uni   cation is suggested by the fourth backup diagram in figure 7.5. this is the idea
that one might decide on a step-by-step basis whether one wanted to take the action as a sample, as in
sarsa, or consider the expectation over all actions instead, as in the tree-backup update. then, if one
chose always to sample, one would obtain sarsa, whereas if one chose never to sample, one would get
the tree-backup algorithm. expected sarsa would be the case where one chose to sample for all steps
except for the last one. and of course there would be many other possibilities, as suggested by the last
diagram in the    gure. to increase the possibilities even further we can consider a continuous variation
between sampling and expectation. let   t     [0, 1] denote the degree of sampling on step t, with    = 1
denoting full sampling and    = 0 denoting a pure expectation with no sampling. the random variable
  t might be set as a function of the state, action, or state   action pair at time t. we call this proposed
new algorithm n-step q(  ).

figure 7.5: the backup diagrams of the three kinds of n-step action-value updates considered so far
in this chapter (4-step case) plus the backup diagram of a fourth kind of update that uni   es them all.
the         s indicate half transitions on which importance sampling is required in the o   -policy case. the
fourth kind of update uni   es all the others by choosing on a state-by-state basis whether to sample
(  t = 1) or not (  t = 0).

                            =1 =0 =1 =04-stepsarsa4-steptree backup4-stepexpected sarsa4-step q( )7.6. *a unifying algorithm: n -step q(  )

127

now let us develop the equations of n-step q(  ). first note that the n-step return of sarsa (7.4) can

be written in terms of its own pure-sample-based td error:

gt:t+n = qt   1(st, at) +

min(t+n   1,t   1)(cid:88)k=t

  k   t [rk+1 +   qk(sk+1, ak+1)     qk   1(sk, ak)]

this suggests that we may be able to cover both cases if we generalize the td error to slide with   t
from its expectation to its sampling form:

  t

with

  qt

as usual. using these we can de   ne the n-step returns of q(  ) as:

.

.

.

  (a|st)qt   1(st, a),

= rt+1 +   (cid:2)  t+1qt(st+1, at+1) + (1       t+1)   qt+1(cid:3)     qt   1(st, at),
=(cid:88)a
= rt+1 +   (cid:2)  t+1qt(st+1, at+1) + (1       t+1)   qt+1(cid:3)
= rt+1 +   (cid:2)  t+1qt(st+1, at+1) + (1       t+1)   qt+1(cid:3)
+   (1       t+1)  (at+1|st+1)(cid:2)rt+2 +   (cid:2)  t+2qt(st+2, at+2) + (1       t+2)   qt+2(cid:3)(cid:3)
+     t+1(cid:2)rt+2 +   (cid:2)  t+2qt(st+2, at+2) + (1       t+2)   qt+2(cid:3)(cid:3)

      (1       t+1)  (at+1|st+1)qt(st+1, at+1)

=   t + qt   1(st, at),
.

        t+1qt(st+1, at+1)

= qt   1(st, at) +   t

gt:t+1

gt:t+2

+   (1       t+1)  (at+1|st+1)  t+1
+     t+1  t+1

= qt   1(st, at) +   t +   (cid:2)(1       t+1)  (at+1|st+1) +   t+1(cid:3)  t+1

min(t+n   1,t   1)(cid:88)k=t

  k

k(cid:89)i=t+1

  (cid:2)(1       i)  (ai|si) +   i(cid:3).

.
= qt   1(st, at) +

gt:t+n

(7.14)

(7.15)

(7.16)

under on-policy training, this return is ready to be used in an update such as that for n-step sarsa
(7.5). for the o   -policy case, we need to take    into account in the importance sampling ratio, which
we rede   ne more generally as

.
=

  t:h

min(h,t   1)(cid:89)k=t

(cid:18)  k

  (ak|sk)
b(ak|sk)

+ 1       k(cid:19) .

(7.17)

after this we can then use the usual general (o   -policy) update for n-step sarsa (7.9). a complete
algorithm is given in the box on the next page.

128

chapter 7. n -step id64

o   -policy n-step q(  ) for estimating q     q   , or q     q   for a given   
input: an arbitrary behavior policy b such that b(a|s) > 0, for all s     s, a     a
initialize q(s, a) arbitrarily, for all s     s, a     a
initialize    to be   -greedy with respect to q, or as a    xed given policy
parameters: step size        (0, 1], small    > 0, a positive integer n
all store and access operations can take their index mod n

repeat (for each episode):

take action at
observe the next reward r; observe and store the next state as st+1
if st+1 is terminal:

store r +     t+1qt+1 +   (1       t+1)(cid:80)a   (a|st+1)q(st+1, a)     qt as   t

(   is the time whose estimate is being updated)

t     t + 1
store   t     r     qt
select and store an action at+1     b(  |st+1)
select and store   t+1
store q(st+1, at+1) as qt+1

else:

if t < t :

initialize and store s0 (cid:54)= terminal
select and store an action a0     b(  |s0)
store q(s0, a0) as q0
t        for t = 0, 1, 2, . . . :
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
until    = t     1

       t     n + 1
if        0:
       1
z     1
g     q  
for k =   , . . . , min(   + n     1, t     1):

z       z(cid:2)(1       k+1)  k+1 +   k+1(cid:3)

g     g + z  k
         (1       k +   k  k)

store   (at+1|st+1) as   t+1
store   (at+1|st+1)
b(at+1|st+1) as   t+1

q(s   , a   )     q(s   , a   ) +      [g     q(s   , a   )]
if    is being learned, then ensure that   (a|s   ) is   -greedy wrt q(s   ,  )

7.7. summary

7.7 summary

129

in this chapter we have developed a range of temporal-di   erence learning methods that lie in between
the one-step td methods of the previous chapter and the monte carlo methods of the chapter before.
methods that involve an intermediate amount of id64 are important because they will typically
perform better than either extreme.

our focus in this chapter has been on n-step methods, which look ahead to
the next n rewards, states, and actions. the two 4-step backup diagrams to
the right together summarize most of the methods introduced. the state-value
update shown is for n-step td with importance sampling, and the action-value
update is for n-step q(  ), which generalizes expected sarsa and id24.
all n-step methods involve a delay of n time steps before updating, as only
then are all the required future events known. a further drawback is that they
involve more computation per time step than previous methods. compared
to one-step methods, n-step methods also require more memory to record the
states, actions, rewards, and sometimes other variables over the last n time
steps. eventually, in chapter 12, we will see how multi-step td methods can
be implemented with minimal memory and computational complexity using
eligibility traces, but there will always be some additional computation beyond
one-step methods. such costs can be well worth paying to escape the tyranny
of the single time step.

although n-step methods are more complex than those using eligibility
traces, they have the great bene   t of being conceptually clear. we have sought
to take advantage of this by developing two approaches to o   -policy learning in
the n-step case. one, based on importance sampling is conceptually simple but
can be of high variance. if the target and behavior policies are very di   erent
it probably needs some new algorithmic ideas before it can be e   cient and
practical. the other, based on tree-backup updates, is the natural extension
of id24 to the multi-step case with stochastic target policies. it involves
no importance sampling but, again if the target and behavior policies are substantially di   erent, the
id64 may span only a few steps even if n is large.

bibliographical and historical remarks

7.1   2 the notion of n-step returns is due to watkins (1989), who also    rst discussed their error
reduction property. n-step algorithms were explored in the    rst edition of this book, in which
they were treated as of conceptual interest, but not feasible in practice. the work of cichosz
(1995) and particularly van seijen (2016) showed that they are actually completely practical
algorithms. given this, and their conceptual clarity and simplicity, we have chosen to highlight
them here in the second edition. in particular, we now postpone all discussion of the backward
view and of eligibility traces until chapter 12.

the results in the random walk examples were made for this text based on work of sutton
(1988) and singh and sutton (1996). the use of backup diagrams to describe these and other
algorithms in this chapter is new.

7.3   5 the developments in these sections are based on the work of precup, sutton, and singh (2000),
precup, sutton, and dasgupta (2001), and sutton, mahmood, precup, and van hasselt (2014).

the tree-backup algorithm is due to precup, sutton, and singh (2000), but the presentation of
it here is new.

       =1 =0 =1 =04-step q( )            4-steptd130

7.6

chapter 7. n -step id64

the q(  ) algorithm is new to this text, but has been explored further by de asis, hernandez-
garcia, holland, and sutton (2017).

chapter 8

planning and learning with tabular
methods

in this chapter we develop a uni   ed view of id23 methods that require a model of
the environment, such as id145 and heuristic search, and methods that can be used
without a model, such as monte carlo and temporal-di   erence methods. these are respectively called
model-based and model-free id23 methods. model-based methods rely on planning as
their primary component, while model-free methods primarily rely on learning. although there are real
di   erences between these two kinds of methods, there are also great similarities. in particular, the heart
of both kinds of methods is the computation of value functions. moreover, all the methods are based on
looking ahead to future events, computing a backed-up value, and then using it as an update target for
an approximate value function. earlier in this book we presented monte carlo and temporal-di   erence
methods as distinct alternatives, then showed how they can be uni   ed by n-step methods. our goal in
this chapter is a similar integration of model-based and model-free methods. having established these
as distinct in earlier chapters, we now explore the extent to which they can be intermixed.

8.1 models and planning

by a model of the environment we mean anything that an agent can use to predict how the environment
will respond to its actions. given a state and an action, a model produces a prediction of the resultant
next state and next reward.
if the model is stochastic, then there are several possible next states
and next rewards, each with some id203 of occurring. some models produce a description of all
possibilities and their probabilities; these we call distribution models. other models produce just one
of the possibilities, sampled according to the probabilities; these we call sample models. for example,
consider modeling the sum of a dozen dice. a distribution model would produce all possible sums
and their probabilities of occurring, whereas a sample model would produce an individual sum drawn
according to this id203 distribution. the kind of model assumed in id145   
estimates of the mdp   s dynamics, p(s(cid:48), r|s, a)   is a distribution model. the kind of model used in
the blackjack example in chapter 5 is a sample model. distribution models are stronger than sample
models in that they can always be used to produce samples. however, in many applications it is much
easier to obtain sample models than distribution models. the dozen dice are a simple example of this.
it would be easy to write a computer program to simulate the dice rolls and return the sum, but harder
and more error-prone to    gure out all the possible sums and their probabilities.

models can be used to mimic or simulate experience. given a starting state and action, a sample

131

132

chapter 8. planning and learning with tabular methods

model produces a possible transition, and a distribution model generates all possible transitions weighted
by their probabilities of occurring. given a starting state and a policy, a sample model could produce
an entire episode, and a distribution model could generate all possible episodes and their probabilities.
in either case, we say the model is used to simulate the environment and produce simulated experience.

the word planning is used in several di   erent ways in di   erent    elds. we use the term to refer to any
computational process that takes a model as input and produces or improves a policy for interacting
with the modeled environment:

in arti   cial intelligence, there are two distinct approaches to planning according to our de   nition.
state-space planning, which includes the approach we take in this book, is viewed primarily as a search
through the state space for an optimal policy or an optimal path to a goal. actions cause transitions
from state to state, and value functions are computed over states. in what we call plan-space planning,
planning is instead a search through the space of plans. operators transform one plan into another, and
value functions, if any, are de   ned over the space of plans. plan-space planning includes evolutionary
methods and    partial-order planning,    a common kind of planning in arti   cial intelligence in which the
ordering of steps is not completely determined at all stages of planning. plan-space methods are di   cult
to apply e   ciently to the stochastic sequential decision problems that are the focus in reinforcement
learning, and we do not consider them further (but see, e.g., russell and norvig, 2010).

the uni   ed view we present in this chapter is that all state-space planning methods share a common
structure, a structure that is also present in the learning methods presented in this book. it takes the
rest of the chapter to develop this view, but there are two basic ideas: (1) all state-space planning
methods involve computing value functions as a key intermediate step toward improving the policy,
and (2) they compute value functions by updates or backup operations applied to simulated experience.
this common structure can be diagrammed as follows:

id145 methods clearly    t this structure: they make sweeps through the space of
states, generating for each state the distribution of possible transitions. each distribution is then used
to compute a backed-up value (update target) and update the state   s estimated value. in this chapter we
argue that various other state-space planning methods also    t this structure, with individual methods
di   ering only in the kinds of updates they do, the order in which they do them, and in how long the
backed-up information is retained.

viewing planning methods in this way emphasizes their relationship to the learning methods that
we have described in this book. the heart of both learning and planning methods is the estimation of
value functions by backing-up update operations. the di   erence is that whereas planning uses simulated
experience generated by a model, learning methods use real experience generated by the environment.
of course this di   erence leads to a number of other di   erences, for example, in how performance is
assessed and in how    exibly experience can be generated. but the common structure means that many
ideas and algorithms can be transferred between planning and learning. in particular, in many cases a
learning algorithm can be substituted for the key update step of a planning method. learning methods
require only experience as input, and in many cases they can be applied to simulated experience just
as well as to real experience. the box below shows a simple example of a planning method based
on one-step tabular id24 and on random samples from a sample model. this method, which
we call random-sample one-step tabular q-planning, converges to the optimal policy for the model
under the same conditions that one-step tabular id24 converges to the optimal policy for the real
environment (each state   action pair must be selected an in   nite number of times in step 1, and    must
decrease appropriately over time).

planningmodelpolicyvaluesbackupsmodelsimulatedexperiencepolicyupdatesbackups8.2. dyna: integrating planning, acting, and learning

133

random-sample one-step tabular q-planning

do forever:

1. select a state, s     s, and an action, a     a(s), at random
2. send s, a to a sample model, and obtain

3. apply one-step tabular id24 to s, a, r, s(cid:48):

a sample next reward, r, and a sample next state, s(cid:48)

q(s, a)     q(s, a) +   (cid:2)r +    maxa q(s(cid:48), a)     q(s, a)(cid:3)

in addition to the uni   ed view of planning and learning methods, a second theme in this chapter
is the bene   ts of planning in small, incremental steps. this enables planning to be interrupted or
redirected at any time with little wasted computation, which appears to be a key requirement for
e   ciently intermixing planning with acting and with learning of the model. planning in very small
steps may be the most e   cient approach even on pure planning problems if the problem is too large to
be solved exactly.

8.2 dyna: integrating planning, acting, and learning

when planning is done on-line, while interacting with the environment, a number of interesting issues
arise. new information gained from the interaction may change the model and thereby interact with
planning. it may be desirable to customize the planning process in some way to the states or decisions
currently under consideration, or expected in the near future. if decision making and model learning
are both computation-intensive processes, then the available computational resources may need to be
divided between them. to begin exploring these issues, in this section we present dyna-q, a simple
architecture integrating the major functions needed in an on-line planning agent. each function appears
in dyna-q in a simple, almost trivial, form. in subsequent sections we elaborate some of the alternate
ways of achieving each function and the trade-o   s between them. for now, we seek merely to illustrate
the ideas and stimulate your intuition.

within a planning agent, there are at least two roles for real experience: it can be used to improve the
model (to make it more accurately match the real environment) and it can be used to directly improve
the value function and policy using the kinds of id23 methods we have discussed in
previous chapters. the former we call model-learning, and the latter we call direct id23
(direct rl). the possible relationships between experience, model, values, and policy are summarized
in figure 8.1. each arrow shows a relationship of in   uence and presumed improvement. note how
experience can improve value functions and policies either directly or indirectly via the model. it is the
latter, which is sometimes called indirect id23, that is involved in planning.

figure 8.1: relationships among learning, planning, and acting.

planningvalue/policyexperiencemodelmodellearningactingdirectrl134

chapter 8. planning and learning with tabular methods

both direct and indirect methods have advantages and disadvantages. indirect methods often make
fuller use of a limited amount of experience and thus achieve a better policy with fewer environmental
interactions. on the other hand, direct methods are much simpler and are not a   ected by biases in
the design of the model. some have argued that indirect methods are always superior to direct ones,
while others have argued that direct methods are responsible for most human and animal learning.
related debates in psychology and arti   cial intelligence concern the relative importance of cognition as
opposed to trial-and-error learning, and of deliberative planning as opposed to reactive decision making
(see chapter 14 for discussion of some of these issues from the perspective of psychology). our view is
that the contrast between the alternatives in all these debates has been exaggerated, that more insight
can be gained by recognizing the similarities between these two sides than by opposing them. for
example, in this book we have emphasized the deep similarities between id145 and
temporal-di   erence methods, even though one was designed for planning and the other for model-free
learning.

dyna-q includes all of the processes shown in figure 8.1   planning, acting, model-learning, and
direct rl   all occurring continuously. the planning method is the random-sample one-step tabular
q-planning method given in figure 8.1. the direct rl method is one-step tabular id24. the
model-learning method is also table-based and assumes the environment is deterministic. after each
transition st, at     rt+1, st+1, the model records in its table entry for st, at the prediction that
rt+1, st+1 will deterministically follow. thus, if the model is queried with a state   action pair that has
been experienced before, it simply returns the last-observed next state and next reward as its prediction.
during planning, the q-planning algorithm randomly samples only from state   action pairs that have
previously been experienced (in step 1), so the model is never queried with a pair about which it has
no information.

the overall architecture of dyna agents, of which the dyna-q algorithm is one example, is shown in
figure 8.2. the central column represents the basic interaction between agent and environment, giving
rise to a trajectory of real experience. the arrow on the left of the    gure represents direct reinforcement
learning operating on real experience to improve the value function and the policy. on the right are
model-based processes. the model is learned from real experience and gives rise to simulated experience.
we use the term search control to refer to the process that selects the starting states and actions for the

figure 8.2: the general dyna architecture. real experience, passing back and forth between the environment
and the policy, a   ects policy and value functions in much the same way as does simulated experience generated
by the model of the environment.

realdirect rlupdatemodelplanning updatesearchcontrolpolicy/value functionsexperiencemodellearningenvironmentsimulatedexperience8.2. dyna: integrating planning, acting, and learning

135

simulated experiences generated by the model. finally, planning is achieved by applying reinforcement
learning methods to the simulated experiences just as if they had really happened. typically, as in
dyna-q, the same id23 method is used both for learning from real experience and
for planning from simulated experience. the id23 method is thus the       nal common
path    for both learning and planning. learning and planning are deeply integrated in the sense that
they share almost all the same machinery, di   ering only in the source of their experience.

conceptually, planning, acting, model-learning, and direct rl occur simultaneously and in parallel
in dyna agents. for concreteness and implementation on a serial computer, however, we fully specify
the order in which they occur within a time step. in dyna-q, the acting, model-learning, and direct
rl processes require little computation, and we assume they consume just a fraction of the time. the
remaining time in each step can be devoted to the planning process, which is inherently computation-
intensive. let us assume that there is time in each step, after acting, model-learning, and direct rl,
to complete n iterations (steps 1   3) of the q-planning algorithm.
in the pseudocode algorithm for
dyna-q in the box below, m odel(s, a) denotes the contents of the (predicted next state and reward) for
state   action pair (s, a). direct id23, model-learning, and planning are implemented
by steps (d), (e), and (f), respectively. if (e) and (f) were omitted, the remaining algorithm would be
one-step tabular id24.

tabular dyna-q
initialize q(s, a) and m odel(s, a) for all s     s and a     a(s)
do forever:

(d) q(s, a)     q(s, a) +   (cid:2)r +    maxa q(s(cid:48), a)     q(s, a)(cid:3)

(a) s     current (nonterminal) state
(b) a      -greedy(s, q)
(c) execute action a; observe resultant reward, r, and state, s(cid:48)
(e) m odel(s, a)     r, s(cid:48) (assuming deterministic environment)
(f) repeat n times:

s     random previously observed state
a     random action previously taken in s
r, s(cid:48)     m odel(s, a)

q(s, a)     q(s, a) +   (cid:2)r +    maxa q(s(cid:48), a)     q(s, a)(cid:3)

example 8.1: dyna maze consider the simple maze shown inset in figure 8.3. in each of the 47
states there are four actions, up, down, right, and left, which take the agent deterministically to the
corresponding neighboring states, except when movement is blocked by an obstacle or the edge of the
maze, in which case the agent remains where it is. reward is zero on all transitions, except those into
the goal state, on which it is +1. after reaching the goal state (g), the agent returns to the start state
(s) to begin a new episode. this is a discounted, episodic task with    = 0.95.

the main part of figure 8.3 shows average learning curves from an experiment in which dyna-q
agents were applied to the maze task. the initial action values were zero, the step-size parameter was
   = 0.1, and the exploration parameter was   = 0.1. when selecting greedily among actions, ties were
broken randomly. the agents varied in the number of planning steps, n, they performed per real step.
for each n, the curves show the number of steps taken by the agent to reach the goal in each episode,
averaged over 30 repetitions of the experiment.
in each repetition, the initial seed for the random
number generator was held constant across algorithms. because of this, the    rst episode was exactly
the same (about 1700 steps) for all values of n, and its data are not shown in the    gure. after the    rst
episode, performance improved for all values of n, but much more rapidly for larger values. recall that
the n = 0 agent is a nonplanning agent, using only direct id23 (one-step tabular q-
learning). this was by far the slowest agent on this problem, despite the fact that the parameter values
(   and   ) were optimized for it. the nonplanning agent took about 25 episodes to reach (  -)optimal

136

chapter 8. planning and learning with tabular methods

figure 8.3: a simple maze (inset) and the average learning curves for dyna-q agents varying in their number
of planning steps (n) per real step. the task is to travel from s to g as quickly as possible.

performance, whereas the n = 5 agent took about    ve episodes, and the n = 50 agent took only three
episodes.

figure 8.4 shows why the planning agents found the solution so much faster than the nonplanning
agent. shown are the policies found by the n = 0 and n = 50 agents halfway through the second episode.
without planning (n = 0), each episode adds only one additional step to the policy, and so only one
step (the last) has been learned so far. with planning, again only one step is learned during the    rst
episode, but here during the second episode an extensive policy has been developed that by the end of
the episode will reach almost back to the start state. this policy is built by the planning process while
the agent is still wandering near the start state. by the end of the third episode a complete optimal
policy will have been found and perfect performance attained.

figure 8.4: policies found by planning and nonplanning dyna-q agents halfway through the second episode.
the arrows indicate the greedy action in each state; if no arrow is shown for a state, then all of its action values
were equal. the black square indicates the location of the agent.

in dyna-q, learning and planning are accomplished by exactly the same algorithm, operating on
real experience for learning and on simulated experience for planning. because planning proceeds
incrementally, it is trivial to intermix planning and acting. both proceed as fast as they can. the agent
is always reactive and always deliberative, responding instantly to the latest sensory information and

28006004002001420103040500 planning steps(direct rl only)episodesstepsperepisode5 planning steps50 planning stepssgactionssgsgwithout planning (n=0)with planning (n=50)nn8.3. when the model is wrong

137

yet always planning in the background. also ongoing in the background is the model-learning process.
as new information is gained, the model is updated to better match reality. as the model changes, the
ongoing planning process will gradually compute a di   erent way of behaving to match the new model.

exercise 8.1 the nonplanning method looks particularly poor in figure 8.4 because it is a one-step
method; a method using multi-step id64 would do better. do you think one of the multi-step
id64 methods from chapter 7 could do as well as the dyna method? explain why or why not.

8.3 when the model is wrong

in the maze example presented in the previous section, the changes in the model were relatively modest.
the model started out empty, and was then    lled only with exactly correct information. in general, we
cannot expect to be so fortunate. models may be incorrect because the environment is stochastic and
only a limited number of samples have been observed, or because the model was learned using function
approximation that has generalized imperfectly, or simply because the environment has changed and
its new behavior has not yet been observed. when the model is incorrect, the planning process is likely
to compute a suboptimal policy.

in some cases, the suboptimal policy computed by planning quickly leads to the discovery and
correction of the modeling error. this tends to happen when the model is optimistic in the sense of
predicting greater reward or better state transitions than are actually possible. the planned policy
attempts to exploit these opportunities and in doing so discovers that they do not exist.

example 8.2: blocking maze a maze example illustrating this relatively minor kind of modeling
error and recovery from it is shown in figure 8.5. initially, there is a short path from start to goal, to
the right of the barrier, as shown in the upper left of the    gure. after 1000 time steps, the short path
is    blocked,    and a longer path is opened up along the left-hand side of the barrier, as shown in upper
right of the    gure. the graph shows average cumulative reward for a dyna-q agent and an enhanced
dyna-q+ agent to be described shortly. the    rst part of the graph shows that both dyna agents found
the short path within 1000 steps. when the environment changed, the graphs become    at, indicating
a period during which the agents obtained no reward because they were wandering around behind the
barrier. after a while, however, they were able to    nd the new opening and the new optimal behavior.

figure 8.5: average performance of dyna agents on a blocking task. the left environment was used for the    rst
1000 steps, the right environment for the rest. dyna-q+ is dyna-q with an exploration bonus that encourages
exploration.

cumulativereward0100020003000time steps1500dyna-q+dyna-qdyna-aid19gsdyna-q138

chapter 8. planning and learning with tabular methods

greater di   culties arise when the environment changes to become better than it was before, and yet
the formerly correct policy does not reveal the improvement. in these cases the modeling error may not
be detected for a long time, if ever, as we see in the next example.

example 8.3: shortcut maze the problem caused by this kind of environmental change is illus-
trated by the maze example shown in figure 8.6. initially, the optimal path is to go around the left
side of the barrier (upper left). after 3000 steps, however, a shorter path is opened up along the right
side, without disturbing the longer path (upper right). the graph shows that the regular dyna-q agent
never switched to the shortcut. in fact, it never realized that it existed. its model said that there was
no shortcut, so the more it planned, the less likely it was to step to the right and discover it. even with
an   -greedy policy, it is very unlikely that an agent will take so many exploratory actions as to discover
the shortcut.

figure 8.6: average performance of dyna agents on a shortcut task. the left environment was used for the
   rst 3000 steps, the right environment for the rest.

the general problem here is another version of the con   ict between exploration and exploitation.
in a planning context, exploration means trying actions that improve the model, whereas exploitation
means behaving in the optimal way given the current model. we want the agent to explore to    nd
changes in the environment, but not so much that performance is greatly degraded. as in the earlier
exploration/exploitation con   ict, there probably is no solution that is both perfect and practical, but
simple heuristics are often e   ective.

the dyna-q+ agent that did solve the shortcut maze uses one such heuristic. this agent keeps track
for each state   action pair of how many time steps have elapsed since the pair was last tried in a real
interaction with the environment. the more time that has elapsed, the greater (we might presume) the
chance that the dynamics of this pair has changed and that the model of it is incorrect. to encourage
behavior that tests long-untried actions, a special    bonus reward    is given on simulated experiences
involving these actions. in particular, if the modeled reward for a transition is r, and the transition has
not been tried in    time steps, then planning updates are done as if that transition produced a reward
of r +         , for some small   . this encourages the agent to keep testing all accessible state transitions
and even to    nd long sequences of actions in order to carry out such tests.1 of course all this testing

1the dyna-q+ agent was changed in two other ways as well. first, actions that had never been tried before from a
state were allowed to be considered in the planning step (f) of the tabular dyna-q algorithm in the box above. second,
the initial model for such actions was that they would lead back to the same state with a reward of zero.

cumulativerewardsggs030006000time steps4000dyna-q+dyna-qdyna-acdyna-q8.4. prioritized sweeping

139

has its cost, but in many cases, as in the shortcut maze, this kind of computational curiosity is well
worth the extra exploration.

exercise 8.2 why did the dyna agent with exploration bonus, dyna-q+, perform better in the    rst
phase as well as in the second phase of the blocking and shortcut experiments?

exercise 8.3 careful inspection of figure 8.6 reveals that the di   erence between dyna-q+ and dyna-
q narrowed slightly over the    rst part of the experiment. what is the reason for this?

exercise 8.4 (programming) the exploration bonus described above actually changes the estimated
values of states and actions. is this necessary? suppose the bonus         was used not in updates, but
solely in action selection. that is, suppose the action selected was always that for which q(st, a) +

  (cid:112)   (st, a) was maximal. carry out a gridworld experiment that tests and illustrates the strengths and

weaknesses of this alternate approach.

8.4 prioritized sweeping

in the dyna agents presented in the preceding sections, simulated transitions are started in state   action
pairs selected uniformly at random from all previously experienced pairs. but a uniform selection is
usually not the best; planning can be much more e   cient if simulated transitions and updates are
focused on particular state   action pairs. for example, consider what happens during the second episode
of the    rst maze task (figure 8.4). at the beginning of the second episode, only the state   action pair
leading directly into the goal has a positive value; the values of all other pairs are still zero. this means
that it is pointless to perform updates along almost all transitions, because they take the agent from
one zero-valued state to another, and thus the updates would have no e   ect. only an update along a
transition into the state just prior to the goal, or from it, will change any values. if simulated transitions
are generated uniformly, then many wasteful updates will be made before stumbling onto one of these
useful ones. as planning progresses, the region of useful updates grows, but planning is still far less
e   cient than it would be if focused where it would do the most good. in the much larger problems that
are our real objective, the number of states is so large that an unfocused search would be extremely
ine   cient.

this example suggests that search might be usefully focused by working backward from goal states.
of course, we do not really want to use any methods speci   c to the idea of    goal state.    we want
methods that work for general reward functions. goal states are just a special case, convenient for
stimulating intuition. in general, we want to work back not just from goal states but from any state
whose value has changed. suppose that the values are initially correct given the model, as they were in
the maze example prior to discovering the goal. suppose now that the agent discovers a change in the
environment and changes its estimated value of one state, either up or down. typically, this will imply
that the values of many other states should also be changed, but the only useful one-step updates are
those of actions that lead directly into the one state whose value has been changed. if the values of
these actions are updated, then the values of the predecessor states may change in turn. if so, then
actions leading into them need to be updated, and then their predecessor states may have changed. in
this way one can work backward from arbitrary states that have changed in value, either performing
useful updates or terminating the propagation. this general idea might be termed backward focusing
of planning computations.

as the frontier of useful updates propagates backward, it often grows rapidly, producing many state   
action pairs that could usefully be updated. but not all of these will be equally useful. the values of
some states may have changed a lot, whereas others may have changed little. the predecessor pairs
of those that have changed a lot are more likely to also change a lot.
in a stochastic environment,
variations in estimated transition probabilities also contribute to variations in the sizes of changes
and in the urgency with which pairs need to be updated.
it is natural to prioritize the updates

140

chapter 8. planning and learning with tabular methods

according to a measure of their urgency, and perform them in order of priority. this is the idea behind
prioritized sweeping. a queue is maintained of every state   action pair whose estimated value would
change nontrivially if updated , prioritized by the size of the change. when the top pair in the queue is
updated, the e   ect on each of its predecessor pairs is computed. if the e   ect is greater than some small
threshold, then the pair is inserted in the queue with the new priority (if there is a previous entry of the
pair in the queue, then insertion results in only the higher priority entry remaining in the queue). in
this way the e   ects of changes are e   ciently propagated backward until quiescence. the full algorithm
for the case of deterministic environments is given in the box.

prioritized sweeping for a deterministic environment

initialize q(s, a), m odel(s, a), for all s, a, and p queue to empty
do forever:

(a) s     current (nonterminal) state
(b) a     policy(s, q)
(c) execute action a; observe resultant reward, r, and state, s(cid:48)
(d) m odel(s, a)     r, s(cid:48)
(e) p     |r +    maxa q(s(cid:48), a)     q(s, a)|.
(f) if p >   , then insert s, a into p queue with priority p
(g) repeat n times, while p queue is not empty:

s, a     f irst(p queue)
r, s(cid:48)     m odel(s, a)
q(s, a)     q(s, a) +   (cid:2)r +    maxa q(s(cid:48), a)     q(s, a)(cid:3)
repeat, for all   s,   a predicted to lead to s:
  r     predicted reward for   s,   a, s
p     |   r +    maxa q(s, a)     q(   s,   a)|.
if p >    then insert   s,   a into p queue with priority p

example 8.4 prioritized sweeping on mazes

prioritized sweeping has been found to dramatically increase the speed at which optimal solutions
are found in maze tasks, often by a factor of 5 to 10. a typical example is shown below.

these data are for a sequence of maze tasks of exactly the same structure as the one shown
in figure 8.3, except that they vary in the grid resolution. prioritized sweeping maintained a
decisive advantage over unprioritized dyna-q. both systems made at most n = 5 updates per
environmental interaction.

backupsuntiloptimalsolution1010310410510610710204794186376752150430086016gridworld size (#states)dyna-qprioritizedsweepingupdates8.4. prioritized sweeping

141

example 8.5 rod maneuvering

the objective in this task is to maneuver a rod around some awkwardly placed obstacles within
a limited rectangular work space to a goal position in the fewest number of steps. the rod can
be translated along its long axis or perpendicular to that axis, or it can be rotated in either
direction around its center. the distance of each movement is approximately 1/20 of the work
space, and the rotation increment is 10 degrees. translations are deterministic and quantized to
one of 20    20 positions. the    gure below shows the obstacles and the shortest solution from
start to goal, found by prioritized sweeping.

this problem is deterministic, but has four actions and 14,400 potential states (some of these
are unreachable because of the obstacles). this problem is probably too large to be solved with
unprioritized methods. figure reprinted from moore and atkeson (1993).

extensions of prioritized sweeping to stochastic environments are straightforward. the model is
maintained by keeping counts of the number of times each state   action pair has been experienced and
of what the next states were. it is natural then to update each pair not with a sample update, as we
have been using so far, but with an expected update, taking into account all possible next states and
their probabilities of occurring.

prioritized sweeping is just one way of distributing computations to improve planning e   ciency, and
probably not the best way. one of prioritized sweeping   s limitations is that it uses expected updates,
which in stochastic environments may waste lots of computation on low-id203 transitions. as we
show in the following section, sample updates can in many cases get closer to the true value function
with less computation despite the variance introduced by sampling. sample updates can win because
they break the overall backing-up computation into smaller pieces   those corresponding to individual
transitions   which then enables it to be focused more narrowly on the pieces that will have the largest
impact. this idea was taken to what may be its logical limit in the    small backups    introduced by
van seijen and sutton (2013). these are updates along a single transition, like a sample update, but
based on the id203 of the transition without sampling, as in an expected update. by selecting the
order in which small updates are done it is possible to greatly improve planning e   ciency beyond that
possible with prioritized sweeping.

we have suggested in this chapter that all kinds of state-space planning can be viewed as sequences
of value updates, varying only in the type of update, expected or sample, large or small, and in the
order in which the updates are done. in this section we have emphasized backward focusing, but this

startgoal142

chapter 8. planning and learning with tabular methods

is just one strategy. for example, another would be to focus on states according to how easily they
can be reached from the states that are visited frequently under the current policy, which might be
called forward focusing. peng and williams (1993) and barto, bradtke and singh (1995) have explored
versions of forward focusing, and the methods introduced in the next few sections take it to an extreme
form.

8.5 expected vs. sample updates

the examples in the previous sections give some idea of the range of possibilities for combining methods
of learning and planning. in the rest of this chapter, we analyze some of the component ideas involved,
starting with the relative advantages of expected and sample updates.

much of this book has been about di   erent kinds of value-function updates, and we have considered
a great many varieties. focusing for the moment on one-step updates, they vary primarily along three
binary dimensions. the    rst two dimensions are whether they update state values or action values
and whether they estimate the value for the optimal policy or for an arbitrary given policy. these two
dimensions give rise to four classes of updates for approximating the four value functions, q   , v   , q  , and

figure 8.7: backup diagrams for all the one-step updates considered in this book.

valueestimatedexpected updates(dp)sample updates (one-step td)   ss0   rpaq   (s,a)q   (s,a)v   (s)v   (s)ss0rmaxappolicy evaluationvalue iterationrs0s,aa0   pq-policy evaluationrs0s,aa0maxpq-value iterationsas0rrs0s,aa0rs0s,amaxtd(0)sarsaid24a08.5. expected vs. sample updates

143

v  . the other binary dimension is whether the updates are expected updates, considering all possible
events that might happen, or sample updates, considering a single sample of what might happen. these
three binary dimensions give rise to eight cases, seven of which correspond to speci   c algorithms, as
shown in figure 8.7. (the eighth case does not seem to correspond to any useful update.) any of these
one-step updates can be used in planning methods. the dyna-q agents discussed earlier use q    sample
updates, but they could just as well use q    expected updates, or either expected or sample q   updates.
the dyna-ac system uses v   sample updates together with a learning policy structure. for stochastic
problems, prioritized sweeping is always done using one of the expected updates.

when we introduced one-step sample updates in chapter 6, we presented them as substitutes for
expected updates. in the absence of a distribution model, expected updates are not possible, but sample
updates can be done using sample transitions from the environment or a sample model. implicit in that
point of view is that expected updates, if possible, are preferable to sample updates. but are they?
expected updates certainly yield a better estimate because they are uncorrupted by sampling error,
but they also require more computation, and computation is often the limiting resource in planning.
to properly assess the relative merits of expected and sample updates for planning we must control for
their di   erent computational requirements.

for concreteness, consider the expected and sample updates for approximating q   , and the special
case of discrete states and actions, a table-lookup representation of the approximate value function, q,
and a model in the form of estimated dynamics,   p(s(cid:48), r|s, a). the expected update for a state   action
pair, s, a, is:

q(s, a)    (cid:88)s(cid:48),r

  p(s(cid:48), r|s, a)(cid:104)r +    max

a(cid:48)

q(s(cid:48), a(cid:48))(cid:105).

(8.1)

the corresponding sample update for s, a, given a sample next state and reward, s(cid:48) and r (from the
model), is the id24-like update:

q(s, a)     q(s, a) +   (cid:104)r +    max

a(cid:48)

q(s(cid:48), a(cid:48))     q(s, a)(cid:105),

where    is the usual positive step-size parameter.

(8.2)

the di   erence between these expected and sample updates is signi   cant to the extent that the
environment is stochastic, speci   cally, to the extent that, given a state and action, many possible next
states may occur with various probabilities. if only one next state is possible, then the expected and
sample updates given above are identical (taking    = 1). if there are many possible next states, then
there may be signi   cant di   erences. in favor of the expected update is that it is an exact computation,
resulting in a new q(s, a) whose correctness is limited only by the correctness of the q(s(cid:48), a(cid:48)) at successor
states. the sample update is in addition a   ected by sampling error. on the other hand, the sample
update is cheaper computationally because it considers only one next state, not all possible next states.
in practice, the computation required by update operations is usually dominated by the number of
state   action pairs at which q is evaluated. for a particular starting pair, s, a, let b be the branching
factor (i.e., the number of possible next states, s(cid:48), for which   p(s(cid:48)|s, a) > 0). then an expected update
of this pair requires roughly b times as much computation as a sample update.

if there is enough time to complete an expected update, then the resulting estimate is generally better
than that of b sample updates because of the absence of sampling error. but if there is insu   cient time
to complete a expected update, then sample updates are always preferable because they at least make
some improvement in the value estimate with fewer than b updates. in a large problem with many
state   action pairs, we are often in the latter situation. with so many state   action pairs, expected
updates of all of them would take a very long time. before that we may be much better o    with a few
sample updates at many state   action pairs than with expected updates at a few pairs. given a unit
of computational e   ort, is it better devoted to a few expected updates or to b times as many sample
updates?

144

chapter 8. planning and learning with tabular methods

figure 8.8: comparison of e   ciency of expected and sample updates.

figure 8.8 shows the results of an analysis that suggests an answer to this question. it shows the
estimation error as a function of computation time for expected and sample updates for a variety of
branching factors, b. the case considered is that in which all b successor states are equally likely and
in which the error in the initial estimate is 1. the values at the next states are assumed correct, so the
expected update reduces the error to zero upon its completion. in this case, sample updates reduce the
bt where t is the number of sample updates that have been performed (assuming
sample averages, i.e.,    = 1/t). the key observation is that for moderately large b the error falls
dramatically with a tiny fraction of b updates. for these cases, many state   action pairs could have
their values improved dramatically, to within a few percent of the e   ect of an expected update, in the
same time that a single state   action pair could undergo an expected update.

error according to(cid:113) b   1

the advantage of sample updates shown in figure 8.8 is probably an underestimate of the real e   ect.
in a real problem, the values of the successor states would be estimates that are themselves updated.
by causing estimates to be more accurate sooner, sample updates will have a second advantage in that
the values backed up from the successor states will be more accurate. these results suggest that sample
updates are likely to be superior to expected updates on problems with large stochastic branching
factors and too many states to be solved exactly.

8.6 trajectory sampling

in this section we compare two ways of distributing updates. the classical approach, from dynamic
programming, is to perform sweeps through the entire state (or state   action) space, updating each state
(or state   action pair) once per sweep. this is problematic on large tasks because there may not be
time to complete even one sweep. in many tasks the vast majority of the states are irrelevant because
they are visited only under very poor policies or with very low id203. exhaustive sweeps implicitly
devote equal time to all parts of the state space rather than focusing where it is needed. as we discussed
in chapter 4, exhaustive sweeps and the equal treatment of all states that they imply are not necessary
properties of id145. in principle, updates can be distributed any way one likes (to
assure convergence, all states or state   action pairs must be visited in the limit an in   nite number of
times; although an exception to this is discussed in section 8.7 below), but in practice exhaustive sweeps
are often used.

the second approach is to sample from the state or state   action space according to some distribution.
one could sample uniformly, as in the dyna-q agent, but this would su   er from some of the same
problems as exhaustive sweeps. more appealing is to distribute updates according to the on-policy

b = 2 (branching factor)b =10b =100b =1000b =10,000sampleupdatesexpectedupdates1001b2brms errorin valueestimatenumber of                         computationsmaxa0q(s0,a0)8.6. trajectory sampling

145

distribution, that is, according to the distribution observed when following the current policy. one
advantage of this distribution is that it is easily generated; one simply interacts with the model, following
the current policy. in an episodic task, one starts in a start state (or according to the starting-state
distribution) and simulates until the terminal state.
in a continuing task, one starts anywhere and
just keeps simulating.
in either case, sample state transitions and rewards are given by the model,
and sample actions are given by the current policy. in other words, one simulates explicit individual
trajectories and performs updates at the state or state   action pairs encountered along the way. we call
this way of generating experience and updates trajectory sampling.

it is hard to imagine any e   cient way of distributing updates according to the on-policy distribution
other than by trajectory sampling. if one had an explicit representation of the on-policy distribution,
then one could sweep through all states, weighting the update of each according to the on-policy dis-
tribution, but this leaves us again with all the computational costs of exhaustive sweeps. possibly one
could sample and update individual state   action pairs from the distribution, but even if this could
be done e   ciently, what bene   t would this provide over simulating trajectories? even knowing the
on-policy distribution in an explicit form is unlikely. the distribution changes whenever the policy
changes, and computing the distribution requires computation comparable to a complete policy eval-
uation. consideration of such other possibilities makes trajectory sampling seem both e   cient and
elegant.

is the on-policy distribution of updates a good one? intuitively it seems like a good choice, at least
better than the uniform distribution. for example, if you are learning to play chess, you study positions
that might arise in real games, not random positions of chess pieces. the latter may be valid states,
but to be able to accurately value them is a di   erent skill from evaluating positions in real games.
we will also see in part ii that the on-policy distribution has signi   cant advantages when function
approximation is used. whether or not function approximation is used, one might expect on-policy
focusing to signi   cantly improve the speed of planning.

focusing on the on-policy distribution could be bene   cial because it causes vast, uninteresting parts
of the space to be ignored, or it could be detrimental because it causes the same old parts of the space to
be updated over and over. we conducted a small experiment to assess the e   ect empirically. to isolate
the e   ect of the update distribution, we used entirely one-step expected tabular updates, as de   ned
by (8.1). in the uniform case, we cycled through all state   action pairs, updating each in place, and
in the on-policy case we simulated episodes, all starting in the same state, updating each state   action
pair that occurred under the current  -greedy policy (  = 0.1). the tasks were undiscounted episodic
tasks, generated randomly as follows. from each of the |s| states, two actions were possible, each of
which resulted in one of b next states, all equally likely, with a di   erent random selection of b states for
each state   action pair. the branching factor, b, was the same for all state   action pairs. in addition,
on all transitions there was a 0.1 id203 of transition to the terminal state, ending the episode.
we used episodic tasks to get a clear measure of the quality of the current policy. at any point in the
planning process one can stop and exhaustively compute v    (s0), the true value of the start state under
the greedy policy,     , given the current action-value function q, as an indication of how well the agent
would do on a new episode on which it acted greedily (all the while assuming the model is correct).

the upper part of figure 8.9 shows results averaged over 200 sample tasks with 1000 states and
branching factors of 1, 3, and 10. the quality of the policies found is plotted as a function of the
number of expected updates completed. in all cases, sampling according to the on-policy distribution
resulted in faster planning initially and retarded planning in the long run. the e   ect was stronger, and
the initial period of faster planning was longer, at smaller branching factors. in other experiments, we
found that these e   ects also became stronger as the number of states increased. for example, the lower
part of figure 8.9 shows results for a branching factor of 1 for tasks with 10,000 states. in this case the
advantage of on-policy focusing is large and long-lasting.

all of these results make sense. in the short term, sampling according to the on-policy distribution

146

chapter 8. planning and learning with tabular methods

figure 8.9: relative e   ciency of updates distributed uniformly across the state space versus focused on sim-
ulated on-policy trajectories, each starting in the same state. results are for randomly generated tasks of two
sizes and various branching factors, b.

helps by focusing on states that are near descendants of the start state. if there are many states and
a small branching factor, this e   ect will be large and long-lasting. in the long run, focusing on the
on-policy distribution may hurt because the commonly occurring states all already have their correct
values. sampling them is useless, whereas sampling other states may actually perform some useful
work. this presumably is why the exhaustive, unfocused approach does better in the long run, at least
for small problems. these results are not conclusive because they are only for problems generated in
a particular, random way, but they do suggest that sampling according to the on-policy distribution
can be a great advantage for large problems, in particular for problems in which a small subset of the
state   action space is visited under the on-policy distribution.

8.7 real-time id145

real-time id145, or rtdp, is an on-policy trajectory-sampling version of dp   s value-
iteration algorithm. because it is closely related to conventional sweep-based policy iteration, rtdp
illustrates in a particularly clear way some of the advantages that on-policy trajectory sampling can
provide. rtdp updates the values of states visited in actual or simulated trajectories by means of
expected tabular value-iteration updates as de   ned by (4.10). it is basically the algorithm that produced
the on-policy results shown in figure 8.9.

b=10b=3b=1on-policyuniform1000 states0123value ofstart stateundergreedypolicy05,00010,00015,00020,000computation time, in full backups0123value ofstart stateundergreedypolicy050,000100,000150,000200,000computation time, in full backupsb=110,000 statesuniformon-policyuniformon-policyon-policyuniformexpected updatesexpected updates8.7. real-time id145

147

the close connection between rtdp and conventional dp makes it possible to derive some theoretical
results by adapting existing theory. rtdp is an example of an asynchronous dp algorithm as described
in section 4.5. asynchronous dp algorithms are not organized in terms of systematic sweeps of the state
set; they update state values in any order whatsoever, using whatever values of other states happen to
be available. in rtdp, the update order is dictated by the order states are visited in real or simulated
trajectories.

if trajectories can start only from a designated set of start states, and if you are interested in
the prediction problem for a given policy, then on-policy trajectory sampling allows the algorithm
to completely skip states that cannot be reached by the given policy from any of the start states:
unreachable states are irrelevant to the prediction problem. for a control problem, where the goal is
to    nd an optimal policy instead of evaluating a given policy, there might well be states that cannot
be reached by any optimal policy from any of the start states, and there is no need to specify optimal
actions for these irrelevant states. what is needed is an optimal partial policy, meaning a policy that is
optimal for the relevant states but can specify arbitrary actions, or even be unde   ned, for the irrelevant
states (see the illustration below).

this can be done,

but    nding such an optimal partial pol-
icy with an on-policy trajectory-sampling
control method,
such as sarsa (sec-
tion 6.4),
in general requires visiting all
state   action pairs   even those that will
turn out to be irrelevant   an in   nite num-
ber of times.
for
example, by using exploring starts (sec-
tion 5.3). this is true for rtdp as well:
for episodic tasks with exploring starts,
rtdp is an asynchronous value-iteration
algorithm that converges to optimal po-
lices for discounted    nite mdps (and for
the undiscounted case under certain conditions). unlike the situation for a prediction problem, it is
generally not possible to stop updating any state or state   action pair if convergence to an optimal policy
is important.

the most interesting result for rtdp is that for certain types of problems satisfying reasonable
conditions, rtdp is guaranteed to    nd a policy that is optimal on the relevant states without visiting
every state in   nitely often, or even without visiting some states at all. indeed, in some problems, only
a small fraction of the states need to be visited. this can be a great advantage for problems with very
large state sets, where even a single sweep may not be feasible.

the tasks for which this result holds are undiscounted episodic tasks for mdps with absorbing goal
states that generate zero rewards, as described in section 3.4. at every step of a real or simulated
trajectory, rtdp selects a greedy action (breaking ties randomly) and applies the expected value-
iteration update operation to the current state. it can also update the values of an arbitrary collection
of other states at each step; for example, it can update the values of states visited in a limited-horizon
look-ahead search from the current state.

for these problems, with each episode beginning in a state randomly chosen from the set of start
states, and ending at a goal state, rtdp converges (with id203 one) to a policy that is optimal
for all the relevant states2 provided the following conditions are satis   ed: 1) the initial value of every
goal state is zero, 2) there exists at least one policy that guarantees that a goal state will be reached
with id203 one from any start state, 3) all rewards for transitions from non-goal states are strictly
negative, and 4) all the initial values are equal to, or greater than, their optimal values (which can be

2this policy might be stochastic because rtdp continues to randomly select among all the greedy actions.

start statesirrelevant states: unreachable  from any start stateunder any optimal policyrelevant statesreachable from some start state under some optimal policy148

chapter 8. planning and learning with tabular methods

satis   ed by simply setting the initial values of all states to zero). this result was proved by barto,
bradtke, and singh (1995) by combining results for asynchronous dp with results about a heuristic
search algorithm known as learning real-time a* due to korf (1990).

tasks having these properties are examples of stochastic optimal path problems, which are usually
stated in terms of cost minimization instead of as reward maximization, as we do here. maximizing the
negative returns in our version is equivalent to minimizing the costs of paths from a start state to a goal
state. examples of this kind of task are minimum-time control tasks, where each time step required to
reach a goal produces a reward of    1, or problems like the golf example in section 3.5, whose objective
is to hit the hole with the fewest strokes.

example 8.6: rtdp on the racetrack the racetrack problem of exercise 5.7 (page 91) is a
stochastic optimal path problem. comparing rtdp and the conventional dp value iteration algorithm
on an example racetrack problem illustrates some of the advantages of on-policy trajectory sampling.

recall from the exercise that an agent has to learn how to drive a car around a turn like those shown
in figure 5.5 and cross the    nish line as quickly as possible while staying on the track. start states
are all the zero-speed states on the starting line; the goal states are all the states that can be reached
in one time step by crossing the    nish line from inside the track. unlike exercise 5.7, here there is no
limit on the car   s speed, so the state set is potentially in   nite. however, the set of states that can be
reached from the set of start states via any policy is    nite and can be considered to be the state set
of the problem. each episode begins in a randomly selected start state and ends when the car crosses
the    nish line. the rewards are    1 for each step until the car crosses the    nish line. if the car hits the
track boundary, it is moved back to a random start state, and the episode continues.

a racetrack similar to the small racetrack on the left of figure 5.5 has 9,115 states reachable from
start states by any policy, only 599 of which are relevant, meaning that they are reachable from some
start state via some optimal policy. (the number of relevant states was estimated by counting the
states visited while executing optimal actions for 107 episodes.)

the table below compares solving this task by conventional dp and by rtdp. these results are
averages over 25 runs, each begun with a di   erent random number seed. conventional dp in this case
is value iteration using exhaustive sweeps of the state set, with values updated one state at a time in
place, meaning that the update for each state uses the most recent values of the other states (this is
the gauss-seidel version of value iteration, which was found to be approximately twice as fast as the
jacobi version on this problem. see section 4.8.) no special attention was paid to the ordering of the
updates; other orderings could have produced faster convergence. initial values were all zero for each
run of both methods. dp was judged to have converged when the maximum change in a state value
over a sweep was less than 10   4, and rtdp was judged to have converged when the average time to
cross the    nish line over 20 episodes appeared to stabilize at an asymptotic number of steps. this
version of rtdp updated only the value of the current state on each step.

average computation to convergence
average number of updates to convergence
average number of updates per episode
% of states updated     100 times
% of states updated     10 times
% of states updated 0 times

dp

28 sweeps
252,784

   
   
   
   

rtdp
4000 episodes
127,600
31.9
98.45
80.51
3.18

both methods produced policies averaging between 14 and 15 steps to cross the    nish line, but rtdp
required only roughly half of the updates that dp did. this is the result of rtdp   s on-policy trajectory
sampling. whereas the value of every state was updated in each sweep of dp, rtdp focused updates
on fewer states. in an average run, rtdp updated the values of 98.45% of the states no more than 100
times and 80.51% of the states no more than 10 times; the values of about 290 states were not updated
at all in an average run.

8.8. planning at decision time

149

another advantage of rtdp is that as the value function approaches the optimal value function v   ,
the policy used by the agent to generate trajectories approaches an optimal policy because it is always
greedy with respect to the current value function. this is in contrast to the situation in conventional
value iteration. in practice, value iteration terminates when the value function changes by only a small
amount in a sweep, which is how we terminated it to obain the results in the table above. at this point,
the value function closely approximates v   , and a greedy policy is close to an optimal policy. however,
it is possible that policies that are greedy with respect to the latest value function were optimal, or
nearly so, long before value iteration terminates. (recall from chapter 4 that optimal policies can be
greedy with respect to many di   erent value functions, not just v   .) checking for the emergence of
an optimal policy before value iteration converges is not a part of the conventional dp algorithm and
requires considerable additional computation.

in the racetrack example, by running many test episodes after each dp sweep, with actions selected
greedily according to the result of that sweep, it was possible to estimate the earliest point in the
dp computation at which the approximated optimal evaluation function was good enough so that the
corresponding greedy policy was nearly optimal. for this racetrack, a close-to-optimal policy emerged
after 15 sweeps of value iteration, or after 136,725 value-iteration updates. this is considerably less
than the 252,784 updates dp needed to converge to v   , but sill more than the 127,600 updates rtdp
required.

although these simulations are certainly not de   nitive comparisons of the rtdp with conven-
tional sweep-based value iteration, they illustrate some of advantages of on-policy trajectory sampling.
whereas conventional value iteration continued to update the value of all the states, rtdp strongly
focused on subsets of the states that were relevant to the problem   s objective. this focus became in-
creasingly narrow as learning continued. because the convergence theorem for rtdp applies to the
simulations, we know that rtdp eventually would have focused only on relevant states, i.e., on states
making up optimal paths. rtdp achieved nearly optimal control with about 50% of the computation
required by sweep-based value iteration.

8.8 planning at decision time

planning can be used in at least two ways. the one we have considered so far in this chapter, typi   ed
by id145 and dyna, is to use planning to gradually improve a policy or value function
on the basis of simulated experience obtained from a model (either a sample or a distribution model).
selecting actions is then a matter of comparing the current state   s action values obtained from a table
in the tabular case we have thus far considered, or by evaluating a mathematical expression in the
approximate methods we consider in part ii below. well before an action is selected for any current
state st, planning has played a part in improving the table entries, or the mathematical expression,
needed to select the action for many states, including st. used this way, planning is not focussed on
the current state. we call planning used in this way background planning.

the other way to use planning is to begin and complete it after encountering each new state st, as
a computation whose output is the selection of a single action at; on the next step planning begins
anew with st+1 to produce at+1, and so on. the simplest, and almost degenerate, example of this
use of planning is when only state values are available, and an action is selected by comparing the
values of model-predicted next states for each action (or by comparing the values of afterstates as in
the tic-tac-toe example in chapter 1). more generally, planning used in this way can look much deeper
than one-step-ahead and evaluate action choices leading to many di   erent predicted state and reward
trajectories. unlike the    rst use of planning, here planning focuses on a particular state. we call this
decision-time planning.

these two ways of thinking about planning   using simulated experience to gradually improve a policy
or value function, or using simulated experience to select an action for the current state   can blend

150

chapter 8. planning and learning with tabular methods

together in natural and interesting ways, but they have tended to be studied separately, and that is a
good way to    rst understand them. let us now take a closer look at decision-time planning.

even when planning is only done at decision time, we can still view it, as we did in section 8.1, as
proceeding from simulated experience to updates and values, and ultimately to a policy. it is just that
now the values and policy are speci   c to the current state and the action choices available there, so
much so that the values and policy created by the planning process are typically discarded after being
used to select the current action. in many applications this is not a great loss because there are very
many states and we are unlikely to return to the same state for a long time. in general, one may want
to do a mix of both:
focus planning on the current state and store the results of planning so as to
be that much farther along should one return to the same state later. decision-time planning is most
useful in applications in which fast responses are not required. in chess playing programs, for example,
one may be permitted seconds or minutes of computation for each move, and strong programs may
plan dozens of moves ahead within this time. on the other hand, if low latency action selection is the
priority, then one is generally better o    doing planning in the background to compute a policy that can
then be rapidly applied to each newly encountered state.

8.9 heuristic search

the classical state-space planning methods in arti   cial intelligence are decision-time planning methods
collectively known as heuristic search. in heuristic search, for each state encountered, a large tree of
possible continuations is considered. the approximate value function is applied to the leaf nodes and
then backed up toward the current state at the root. the backing up within the search tree is just the
same as in the expected updates with maxes (those for v    and q   ) discussed throughout this book. the
backing up stops at the state   action nodes for the current state. once the backed-up values of these
nodes are computed, the best of them is chosen as the current action, and then all backed-up values
are discarded.

in conventional heuristic search no e   ort is made to save the backed-up values by changing the
approximate value function.
in fact, the value function is generally designed by people and never
changed as a result of search. however, it is natural to consider allowing the value function to be
improved over time, using either the backed-up values computed during heuristic search or any of the
other methods presented throughout this book. in a sense we have taken this approach all along. our
greedy,   -greedy, and ucb (section 2.7) action-selection methods are not unlike heuristic search, albeit
on a smaller scale. for example, to compute the greedy action given a model and a state-value function,
we must look ahead from each possible action to each possible next state, take into account the rewards
and estimated values, and then pick the best action. just as in conventional heuristic search, this
process computes backed-up values of the possible actions, but does not attempt to save them. thus,
heuristic search can be viewed as an extension of the idea of a greedy policy beyond a single step.

the point of searching deeper than one step is to obtain better action selections.

if one has a
perfect model and an imperfect action-value function, then in fact deeper search will usually yield
better policies.3 certainly, if the search is all the way to the end of the episode, then the e   ect of the
imperfect value function is eliminated, and the action determined in this way must be optimal. if the
search is of su   cient depth k such that   k is very small, then the actions will be correspondingly near
optimal. on the other hand, the deeper the search, the more computation is required, usually resulting
in a slower response time. a good example is provided by tesauro   s grandmaster-level backgammon
player, td-gammon (section 16.1). this system used td learning to learn an afterstate value function
through many games of self-play, using a form of heuristic search to make its moves. as a model, td-
gammon used a priori knowledge of the probabilities of dice rolls and the assumption that the opponent

3there are interesting exceptions to this. see, e.g., pearl (1984).

8.9. heuristic search

151

always selected the actions that td-gammon rated as best for it. tesauro found that the deeper the
heuristic search, the better the moves made by td-gammon, but the longer it took to make each move.
backgammon has a large branching factor, yet moves must be made within a few seconds. it was only
feasible to search ahead selectively a few steps, but even so the search resulted in signi   cantly better
action selections.

we should not overlook the most obvious way in which heuristic search focuses updates: on the
current state. much of the e   ectiveness of heuristic search is due to its search tree being tightly focused
on the states and actions that might immediately follow the current state. you may spend more of your
life playing chess than checkers, but when you play checkers, it pays to think about checkers and about
your particular checkers position, your likely next moves, and successor positions. no matter how you
select actions, it is these states and actions that are of highest priority for updates and where you most
urgently want your approximate value function to be accurate. not only should your computation be
preferentially devoted to imminent events, but so should your limited memory resources. in chess, for
example, there are far too many possible positions to store distinct value estimates for each of them, but
chess programs based on heuristic search can easily store distinct estimates for the millions of positions
they encounter looking ahead from a single position. this great focusing of memory and computational
resources on the current decision is presumably the reason why heuristic search can be so e   ective.

the distribution of updates can be altered in similar ways to focus on the current state and its
likely successors. as a limiting case we might use exactly the methods of heuristic search to construct
a search tree, and then perform the individual, one-step updates from bottom up, as suggested by
figure 8.10. if the updates are ordered in this way and a tabular representation is used, then exactly
the same overall update would be achieved as in depth-   rst heuristic search. any state-space search can
be viewed in this way as the piecing together of a large number of individual one-step updates. thus,
the performance improvement observed with deeper searches is not due to the use of multistep updates
as such. instead, it is due to the focus and concentration of updates on states and actions immediately
downstream from the current state. by devoting a large amount of computation speci   cally relevant
to the candidate actions, decision-time planning can produce better decisions than can be produced by
relying on unfocused updates.

figure 8.10: heuristic search can be implemented as a sequence of one-step updates (shown here outlined)
backing up values from the leaf nodes toward the root. the ordering shown here is for a selective depth-   rst
search.

12345678910152

chapter 8. planning and learning with tabular methods

8.10 rollout algorithms

rollout algorithms are decision-time planning algorithms based on monte carlo control applied to
simulated trajectories that all begin at the current environment state. they estimate action values for
a given policy by averaging the returns of many simulated trajectories that start with each possible
action and then follow the given policy. when the action-value estimates are considered to be accurate
enough, the action (or one of the actions) having the highest estimated value is executed, after which
the process is carried out anew from the resulting next state. as explained by tesauro and galperin
(1997), who experimented with rollout algorithms for playing backgammon, the term    rollout    comes
from estimating the value of a backgammon position by playing out, i.e.,    rolling out,    the position
many times to the game   s end with randomly generated sequences of dice rolls, where the moves of both
players are made by some    xed policy.

unlike the monte carlo control algorithms described in chapter 5, the goal of a rollout algorithm is
not to estimate a complete optimal action-value function, q   , or a complete action-value function, q  ,
for a given policy   . instead, they produce monte carlo estimates of action values only for each current
state and for a given policy usually called the rollout policy. as decision-time planning algorithms,
rollout algorithms make immediate use of these action-value estimates, then discard them. this makes
rollout algorithms relatively simple to implement because there is no need to sample outcomes for every
state-action pair, and there is no need to approximate a function over either the state space or the
state-action space.

what then do rollout algorithms accomplish? the policy improvement theorem described in sec-
tion 4.2 tells us that given any two policies    and   (cid:48) that are identical except that   (cid:48)(s) = a (cid:54)=   (s)
for some state s, if q  (s, a)     v  (s), then policy   (cid:48) is as good as, or better, than   . moreover, if
the inequality is strict, then   (cid:48) is in fact better than   . this applies to rollout algorithms where s
is the current state and    is the rollout policy. averaging the returns of the simulated trajectories
produces estimates of q  (s, a(cid:48)) for each action a(cid:48)     a(s). then the policy that selects an action in s
that maximizes these estimates and thereafter follows    is a good candidate for a policy that improves
over   . the result is like one step of the policy-iteration algorithm of id145 discussed
in section 4.3 (though it is more like one step of asynchronous value iteration described in section 4.5
because it changes the action for just the current state).

in other words, the aim of a rollout algorithm is to improve upon the default policy; not to    nd
an optimal policy. experience has shown that rollout algorithms can be surprisingly e   ective. for
example, tesauro and galperin (1997) were surprised by the dramatic improvements in backgammon
playing ability produced by the rollout method. in some applications, a rollout algorithm can produce
good performance even if the rollout policy is completely random. but clearly, the performance of the
improved policy depends on the performance of the rollout policy and the accuracy of the monte carlo
value estimates: the better the rollout policy and the more accurate the value estimates, the better the
policy produced by a rollout algorithm is likely be.

this involves important tradeo   s because better rollout policies typically mean that more time is
needed to simulate enough trajectories to obtain good value estimates. as decision-time planning
methods, rollout algorithms usually have to meet strict time constraints. the computation time needed
by a rollout algorithm depends on the number of actions that have to be evaluated for each decision,
the number of time steps in the simulated trajectories needed to obtain useful sample returns, the time
it takes the rollout policy to make decisions, and the number of simulated trajectories needed to obtain
good monte carlo action-value estimates.

balancing these factors is important in any application of rollout methods, though there are several
ways to ease the challenge. because the monte carlo trials are independent of one another, it is pos-
sible to run many trials in parallel on separate processors. another tact is to truncate the simulated
trajectories short of complete episodes, correcting the truncated returns by means of a stored evalu-

8.11. id169

153

ation function (which brings into play all that we have said about truncated returns and updates in
the preceding chapters). it is also possible, as tesauro and galperin (1997) suggest, to monitor the
monte carlo simulations and prune away candidate actions that are unlikely to turn out to be the
best, or whose values are close enough to that of the current best that choosing them instead would
make no real di   erence (though tesauro and galperin point out that this would complicate a parallel
implementation).

we do not ordinarily think of rollout algorithms as learning algorithms because they do not maintain
long-term memories of values or policies. however, these algorithms take advantage of some of the
features of id23 that we have emphasized in this book. as instances of monte carlo
control, they estimate action values by averaging the returns of a collection of sample trajectories, in
this case trajectories of simulated interactions with a sample model of the environment. in this way they
are like id23 algorithms in avoiding the exhaustive sweeps of id145
by trajectory sampling, and in avoiding the need for distribution models by relying on sample, instead
of expected, updates. finally, rollout algorithms take advantage of the policy improvement property
by acting greedily with respect to the estimated action values.

8.11 id169

id169 (mcts) is a recent and strikingly successful example of decision-time planning.
at is base, mcts is a rollout algorithm as described above, but enhanced by the addition of a means
for accumulating value estimates obtained from the monte carlo simulations in order to successively
direct simulations toward more highly-rewarding trajectories. mcts is largely responsible for the
improvement in computer go from a weak amateur level in 2005 to a grandmaster level (6 dan or more)
in 2015. many variations of the basic algorithm have been developed, including a variant that we discuss
in section 16.6 that was critical for the stunning 2016 victories of the program alphago over an 18-time
world champion go player. mcts has proved to be e   ective in a wide variety of competitive settings,
including general game playing (e.g., see finnsson and bj  ornsson, 2008; genesereth and thielscher,
2014), but it is not limited to games; it can be e   ective for single-agent sequential decision problems if
there is an environment model simple enough for fast multistep simulation.

mcts is executed after encountering each new state to select the agent   s action for that state; it
is executed again to select the action for the next state, and so on. as in a rollout algorithm, each
execution is an iterative process that simulates many trajectories starting from the current state and
running to a terminal state (or until discounting makes any further reward negligible as a contribution
to the return). the core idea of mcts is to successively focus multiple simulations starting at the
current state by extending the initial portions of trajectories that have received high evaluations from
earlier simulations. mcts does not have to retain approximate value functions or policies from one
action selection to the next, though in many implementations it retains selected action values likely to
be useful for its next execution.

for the most part, the actions in the simulated trajectories are generated using a simple policy,
usually called a rollout policy as it is for simpler rollout algorithms. when both the rollout policy and
the model do not require a lot of computation, many simulated trajectories can be generated in a short
period of time. as in any tabular monte carlo method, the value of a state   action pair is estimated
as the average of the (simulated) returns from that pair. monte carlo value estimates are maintained
only for the subset of state   action pairs that are most likely to be reached in a few steps, which form a
tree rooted at the current state, as illustrated in figure 8.11. mcts incrementally extends the tree by
adding nodes representing states that look promising based on the results of the simulated trajectories.
any simulated trajectory will pass through the tree and then exit it at some leaf node. outside the tree
and at the leaf nodes the rollout policy is used for action selections, but at the states inside the tree
something better is possible. for these states we have value estimates for of at least some of the actions,

154

chapter 8. planning and learning with tabular methods

so we can pick among them using an informed policy, called the tree policy, that balances exploration
and exploitation. for example, the tree policy could select actions using an   -greedy or ucb selection
rule (chapter 2).

in more detail, each iteration of a basic version of mcts consists of the following four steps as

illustrated in figure 8.11:

1. selection. starting at the root node, a tree policy based on the action values attached to the

edges of the tree traverses the tree to select a leaf node.

2. expansion. on some iterations (depending on details of the application), the tree is expanded
from the selected leaf node by adding one or more child nodes reached from the selected node via
unexplored actions.

3. simulation. from the selected node, or from one of its newly-added child nodes (if any), sim-
ulation of a complete episode is run with actions selected by the rollout policy. the result is a
monte carlo trial with actions selected    rst by the tree policy and beyond the tree by the rollout
policy.

4. backup. the return generated by the simulated episode is backed up to update, or to initialize,
the action values attached to the edges of the tree traversed by the tree policy in this iteration
of mcts. no values are saved for the states and actions visited by the rollout policy beyond the
tree. figure 8.11 illustrates this by showing a backup from the terminal state of the simulated
trajectory directly to the state   action node in the tree where the rollout policy began (though in
general, the entire return over the simulated trajectory is backed up to this state   action node).

figure 8.11: id169. when the environment changes to a new state, mcts executes as
many iterations as possible before an action needs to be selected, incrementally building a tree whose root node
represents the current state. each iteration consists of the four operations selection, expansion (though
possibly skipped on some iterations), simulation, and backup, as explained in the text and illustrated by the
bold arrows in the trees. adapted from chaslot, bakkes, szita, and spronck (2008).

selectionsimulationexpansionbackuprepeat while time remains tree policyrolloutpolicy8.12. summary of the chapter

155

mcts continues executing these four steps, starting each time at the tree   s root node, until no more
time is left, or some other computational resource is exhausted. then,    nally, an action from the
root node (which still represents the current state of the environment) is selected according to some
mechanism that depends on the accumulated statistics in the tree; for example, it may be an action
having the largest action value of all the actions available from the root state, or perhaps the action
with the largest visit count to avoid selecting outliers. this is the action mcts actually selects. after
the environment transitions to a new state, mcts is run again, sometimes starting with a tree of a
single root node representing the new state, but often starting with a tree containing any descendants
of this node left over from the tree constructed by the previous execution of mcts; all the remaining
nodes are discarded, along with the action values associated with them.

mcts was    rst proposed to select moves in programs playing two-person competitive games, such as
go. for game playing, each simulated episode is one complete play of the game in which both players
select actions by the tree and rollout policies. section 16.6 describes an extension of mcts used in the
alphago program that combines the monte carlo evaluations of mcts with action values learned by
a deep ann via self-play id23.

relating mcts to the id23 principles we describe in this book provides some
insight into how it achieves such impressive results. at its base, mcts is a decision-time planning
algorithm based on monte carlo control applied to simulations that start from the root state; that is,
it is a kind of rollout algorithm as described in the previous section. it therefore bene   ts from online,
incremental, sample-based value estimation and policy improvement. beyond this, it saves action-value
estimates attached to the tree edges and updates them using id23   s sample updates.
this has the e   ect of focusing the monte carlo trials on trajectories whose initial segments are common
to high-return trajectories previously simulated. further, by incrementally expanding the tree, mcts
e   ectively grows a lookup table to store a partial action-value function, with memory allocated to the
estimated values of state   action pairs visited in the initial segments of high-yielding sample trajectories.
mcts thus avoids the problem of globally approximating an action-value function while it retrains the
bene   t of using past experience to guide exploration.

the striking success of decision-time planning by mcts has deeply in   uenced arti   cial intelligence,
and many researchers are studying modi   cations and extensions of the basic procedure for use in both
games and single-agent applications.

8.12 summary of the chapter

planning requires a model of the environment. a distribution model consists of the probabilities of
next states and rewards for possible actions; a sample model produces single transitions and rewards
generated according to these probabilities. id145 requires a distribution model because
it uses expected updates, which involve computing expectations over all the possible next states and
rewards. a sample model, on the other hand, is what is needed to simulate interacting with the
environment during which sample updates, like those used by many id23 algorithms,
can be used. sample models are generally much easier to obtain than distribution models.

we have presented a perspective emphasizing the surprisingly close relationships between planning
optimal behavior and learning optimal behavior. both involve estimating the same value functions,
and in both cases it is natural to update the estimates incrementally, in a long series of small backing-
up operations. this makes it straightforward to integrate learning and planning processes simply by
allowing both to update the same estimated value function. in addition, any of the learning methods can
be converted into planning methods simply by applying them to simulated (model-generated) experience
rather than to real experience. in this case learning and planning become even more similar; they are
possibly identical algorithms operating on two di   erent sources of experience.

156

chapter 8. planning and learning with tabular methods

it is straightforward to integrate incremental planning methods with acting and model-learning.
planning, acting, and model-learning interact in a circular fashion (figure 8.1), each producing what
the other needs to improve; no other interaction among them is either required or prohibited. the
most natural approach is for all processes to proceed asynchronously and in parallel. if the processes
must share computational resources, then the division can be handled almost arbitrarily   by whatever
organization is most convenient and e   cient for the task at hand.

in this chapter we have touched upon a number of dimensions of variation among state-space planning
methods. one dimension is the variation in the size of updates. the smaller the updates, the more
incremental the planning methods can be. among the smallest updates are one-step sample updates,
as in dyna. another important dimension is the distribution of updates, that is, of the focus of search.
prioritized sweeping focuses backward on the predecessors of states whose values have recently changed.
on-policy trajectory sampling focuses on states or state   action pairs that the agent is likely to encounter
when controlling its environment. this can allow computation to skip over parts of the state space that
are irrelevant to the prediction or control problem. real-time id145, an on-policy
trajectory sampling version of value iteration, illustrates some of the advantages this strategy has over
conventional sweep-based policy iteration.

planning can also focus forward from pertinent states, such as states actually encountered during an
agent-environment interaction. the most important form of this is when planning is done at decision
time, that is, as part of the action-selection process. classical heuristic search as studied in arti   cial
intelligence is an example of this. other examples are rollout algorithms and id169
that bene   t from online, incremental, sample-based value estimation and policy improvement.

8.13 summary of part i: dimensions

this chapter concludes part i of this book. in it we have tried to present id23 not as
a collection of individual methods, but as a coherent set of ideas cutting across methods. each idea can
be viewed as a dimension along which methods vary. the set of such dimensions spans a large space
of possible methods. by exploring this space at the level of dimensions we hope to obtain the broadest
and most lasting understanding. in this section we use the concept of dimensions in method space to
recapitulate the view of id23 developed so far in this book.

all of the methods we have explored so far in this book have three key ideas in common:    rst,
they all seek to estimate value functions; second, they all operate by backing up values along actual or
possible state trajectories; and third, they all follow the general strategy of generalized policy iteration
(gpi), meaning that they maintain an approximate value function and an approximate policy, and they
continually try to improve each on the basis of the other. these three ideas are central to the subjects
covered in this book. we suggest that value functions, backing-up value updates, and gpi are powerful
organizing principles potentially relevant to any model of intelligence, whether arti   cial or natural.

two of the most important dimensions along which the methods vary are shown in figure 8.12. these
dimensions have to do with the kind of update used to improve the value function. the horizontal
dimension is whether they are sample updates (based on a sample trajectory) or expected updates
(based on a distribution of possible trajectories). expected updates require a distribution model,
whereas sample updates need only a sample model, or can be done from actual experience with no
model at all (another dimension of variation). the vertical dimension of figure 8.12 corresponds to the
depth of updates, that is, to the degree of id64. at three of the four corners of the space are
the three primary methods for estimating values: dp, td, and monte carlo. along the left edge of the
space are the sample-update methods, ranging from one-step td updates to full-return monte carlo
updates. between these is a spectrum including methods based on n-step updates (and in chapter 12
we will extend this to mixtures of n-step updates such as the   -updates implemented by eligibility
traces).

8.13. summary of part i: dimensions

157

figure 8.12: a slice through the space of id23 methods, highlighting the two of the most
important dimensions explored in part i of this book: the depth and width of the updates.

dp methods are shown in the extreme upper-right corner of the space because they involve one-step
expected updates. the lower-right corner is the extreme case of expected updates so deep that they run
all the way to terminal states (or, in a continuing task, until discounting has reduced the contribution of
any further rewards to a negligible level). this is the case of exhaustive search. intermediate methods
along this dimension include heuristic search and related methods that search and update up to a
limited depth, perhaps selectively. there are also methods that are intermediate along the horizontal
dimension. these include methods that mix expected and sample updates, as well as the possibility of
methods that mix samples and distributions within a single update. the interior of the square is    lled
in to represent the space of all such intermediate methods.

a third dimension that we have emphasized in this book is the binary distinction between on-policy
and o   -policy methods.
in the former case, the agent learns the value function for the policy it is
currently following, whereas in the latter case it learns the value function for the policy for a di   erent
policy, often the one that the agent currently thinks is best. the policy generating behavior is typically
di   erent from what is currently thought best because of the need to explore. this third dimension
might be visualized as perpendicular to the plane of the page in figure 8.12.

in addition to the three dimensions just discussed, we have identi   ed a number of others throughout

the book:

de   nition of return is the task episodic or continuing, discounted or undiscounted?

action values vs. state values vs. afterstate values what kind of values should be estimated?
if only state values are estimated, then either a model or a separate policy (as in actor   critic
methods) is required for action selection.

action selection/exploration how are actions selected to ensure a suitable trade-o    between ex-

widthof updatedepth(length)of updatetemporal-differencelearningdynamicprogrammingmontecarlo...exhaustivesearch158

chapter 8. planning and learning with tabular methods

ploration and exploitation? we have considered only the simplest ways to do this:   -greedy,
optimistic initialization of values, softmax, and upper con   dence bound.

synchronous vs. asynchronous are the updates for all states performed simultaneously or one by

one in some order?

real vs. simulated should one update based on real experience or simulated experience? if both,

how much of each?

location of updates what states or state   action pairs should be updated? model-free methods
can choose only among the states and state   action pairs actually encountered, but model-based
methods can choose arbitrarily. there are many possibilities here.

timing of updates should updates be done as part of selecting actions, or only afterward?

memory for updates how long should updated values be retained? should they be retained perma-

nently, or only while computing an action selection, as in heuristic search?

of course, these dimensions are neither exhaustive nor mutually exclusive. individual algorithms di   er
in many other ways as well, and many algorithms lie in several places along several dimensions. for
example, dyna methods use both real and simulated experience to a   ect the same value function. it is
also perfectly sensible to maintain multiple value functions computed in di   erent ways or over di   erent
state and action representations. these dimensions do, however, constitute a coherent set of ideas for
describing and exploring a wide space of possible methods.

the most important dimension not mentioned here, and not covered in part i of this book, is
that of function approximation. function approximation can be viewed as an orthogonal spectrum of
possibilities ranging from tabular methods at one extreme through state aggregation, a variety of linear
methods, and then a diverse set of nonlinear methods. this dimension is explored in part ii.

bibliographical and historical remarks

8.1

8.2

the overall view of planning and learning presented here has developed gradually over a number
of years, in part by the authors (sutton, 1990, 1991a, 1991b; barto, bradtke, and singh, 1991,
1995; sutton and pinette, 1985; sutton and barto, 1981b); it has been strongly in   uenced by
agre and chapman (1990; agre 1988), bertsekas and tsitsiklis (1989), singh (1993), and others.
the authors were also strongly in   uenced by psychological studies of latent learning (tolman,
1932) and by psychological views of the nature of thought (e.g., galanter and gerstenhaber,
1956; craik, 1943; campbell, 1960; dennett, 1978). in the part iii of the book, section 14.6
relates model-based and model-free methods to psychological theories of learning and behavior,
and section 15.11 discusses ideas about how the brain might implement these types of methods.

the terms direct and indirect, which we use to describe di   erent kinds of id23,
are from the adaptive control literature (e.g., goodwin and sin, 1984), where they are used to
make the same kind of distinction. the term system identi   cation is used in adaptive control
for what we call model-learning (e.g., goodwin and sin, 1984; ljung and s  oderstrom, 1983;
young, 1984). the dyna architecture is due to sutton (1990), and the results in this and the
next section are based on results reported there. barto and singh (1991) consider some of the
issues in comparing direct and indirect id23 methods.

8.3

there have been several works with model-based id23 that take the idea of
exploration bonuses and optimistic initialization to its logical extreme, in which all incompletely

159

explored choices are assumed maximally rewarding and optimal paths are computed to test
them. the e3 algorithm of kearns and singh (2002) and the r-max algorithm of brafman and
tennenholtz (2003) are guaranteed to    nd a near-optimal solution in time polynomial in the
number of states and actions. this is usually too slow for practical algorithms but is probably
the best that can be done in the worst case.

8.4

prioritized sweeping was developed simultaneously and independently by moore and atkeson
(1993) and peng and williams (1993). the results in the box on page 140 are due to peng
and williams (1993). the results in the box on page 141 are due to moore and atkeson. key
subsequent work in this area includes that by mcmahan and gordon (2005) and by van seijen
and sutton (2013).

8.5

this section was strongly in   uenced by the experiments of singh (1993).

8.6   7 trajectory sampling has implicitly been a part of id23 from the outset, but
it was most explicitly emphasized by barto, bradtke, and singh (1995) in their introduction
of rtdp. they recognized that korf   s (1990) learning real-time a* (lrta*) algorithm is an
asynchronous dp algorithm that applies to stochastic problems as well as the deterministic
problems on which korf focused. beyond lrta*, rtdp includes the option of updating the
values of many states in the time intervals between the execution of actions. barto et al. (1995)
proved the convergence result described here by combining korf   s (1990) convergence proof
for lrta* with the result of bertsekas (1982) (also bertsekas and tsitsiklis, 1989) ensuring
convergence of asynchronous dp for stochastic shortest path problems in the undiscounted
case. combining model-learning with rtdp is called adaptive rtdp, also presented by barto
et al. (1995) and discussed by barto (2011).

8.9

8.10

for further reading on heuristic search, the reader is encouraged to consult texts and surveys
such as those by russell and norvig (2009) and korf (1988). peng and williams (1993) explored
a forward focusing of updates much as is suggested in this section.

abramson   s (1990) expected-outcome model is a rollout algorithm applied to two-person games
in which the play of both simulated players is random. he argued that even with random play,
it is a    powerful heuristic    that is    precise, accurate, easily estimable, e   ciently calculable, and
domain-independent.    tesauro and galperin (1997) demonstrated the e   ectiveness of rollout
algorithms for improving the play of backgammon programs, adopting the term    rollout    from
its use in evaluating backgammon positions by playing out positions with di   erent randomly
generating sequences of dice rolls. bertsekas, tsitsiklis, and wu (1997) examine rollout algo-
rithms applied to combinatorial optimization problems, and bertsekas (2013) surveys their use
in discrete deterministic optimization problems, remarking that they are    often surprisingly
e   ective.   

8.11

the central ideas of mcts were introduced by coulom (2006) and by kocsis and szepesv  ari
(2006). they built upon previous research with monte carlo planning algorithms as reviewed
by these authors. browne, powley, whitehouse, lucas, cowling, rohlfshagen, tavener, perez,
samothrakis, and colton (2012) is an excellent survey of mcts methods and their applications.
david silver contributed to the ideas and presentation in this section.

part ii: approximate solution methods

in the second part of the book we extend the tabular methods presented in the    rst part to apply
to problems with arbitrarily large state spaces. in many of the tasks to which we would like to apply
id23 the state space is combinatorial and enormous; the number of possible camera
images, for example, is much larger than the number of atoms in the universe. in such cases we cannot
expect to    nd an optimal policy or the optimal value function even in the limit of in   nite time and
data; our goal instead is to    nd a good approximate solution using limited computational resources. in
this part of the book we explore such approximate solution methods.

the problem with large state spaces is not just the memory needed for large tables, but the time and
data needed to    ll them accurately. in many of our target tasks, almost every state encountered will
never have been seen before. to make sensible decisions in such states it is necessary to generalize from
previous encounters with di   erent states that are in some sense similar to the current one. in other
words, the key issue is that of generalization. how can experience with a limited subset of the state
space be usefully generalized to produce a good approximation over a much larger subset?

fortunately, generalization from examples has already been extensively studied, and we do not need
to invent totally new methods for use in id23. to some extent we need only combine
id23 methods with existing generalization methods. the kind of generalization we
require is often called function approximation because it takes examples from a desired function (e.g.,
a value function) and attempts to generalize from them to construct an approximation of the entire
function. function approximation is an instance of supervised learning, the primary topic studied in
machine learning, arti   cial neural networks, pattern recognition, and statistical curve    tting. in theory,
any of the methods studied in these    elds can be used in the role of function approximator within
id23 algorithms, although in practice some    t more easily into this role than others.

id23 with function approximation involves a number of new issues that do not
normally arise in conventional supervised learning, such as nonstationarity, id64, and delayed
targets. we introduce these and other issues successively over the    ve chapters of this part. initially we
restrict attention to on-policy training, treating in chapter 9 the prediction case, in which the policy
is given and only its value function is approximated, and then in chapter 10 the control case, in which
an approximation to the optimal policy is found. the challenging problem of o   -policy learning with
function approximation is treated in chapter 11. in each of these three chapters we will have to return to
   rst principles and re-examine the objectives of the learning to take into account function approximation.
chapter 12 introduces and analyzes the algorithmic mechanism of eligibility traces, which dramatically
improves the computational properties of multi-step id23 methods in many cases.
the    nal chapter of this part explores a di   erent approach to control, policy-gradient methods, which
approximate the optimal policy directly and need never form an approximate value function (although
they may be much more e   cient if they do approximate a value function as well the policy).

160

chapter 9

on-policy prediction with
approximation

in this chapter, we begin our study of function approximation in id23 by considering
its use in estimating the state-value function from on-policy data, that is, in approximating v   from
experience generated using a known policy   . the novelty in this chapter is that the approximate
value function is represented not as a table but as a parameterized functional form with weight vector
w     rd. we will write   v(s,w)     v  (s) for the approximate value of state s given weight vector w. for
example,   v might be a linear function in features of the state, with w the vector of feature weights.
more generally,   v might be the function computed by a multi-layer arti   cial neural network, with w the
vector of connection weights in all the layers. by adjusting the weights, any of a wide range of di   erent
functions can be implemented by the network. or   v might be the function computed by a decision tree,
where w is all the numbers de   ning the split points and leaf values of the tree. typically, the number
of weights (the dimensionality of w) is much less than the number of states (d (cid:28) |s|), and changing one
weight changes the estimated value of many states. consequently, when a single state is updated, the
change generalizes from that state to a   ect the values of many other states. such generalization makes
the learning potentially more powerful but also potentially more di   cult to manage and understand.

perhaps surprisingly, extending id23 to function approximation also makes it ap-
plicable to partially observable problems, in which the full state is not available to the agent. if the
parameterized function form for   v does not allow the estimated value to depend on certain aspects of
the state, then it is just as if those aspects are unobservable. in fact, all the theoretical results for
methods using function approximation presented in this part of the book apply equally well to cases of
partial observability. what function approximation can   t do, however, is augment the state represen-
tation with memories of past observations. some such possible further extensions are discussed brie   y
in section 17.3.

9.1 value-function approximation

all of the prediction methods covered in this book have been described as updates to an estimated
value function that shift its value at particular states toward a    backed-up value,    or update target,
for that state. let us refer to an individual update by the notation s (cid:55)    u, where s is the state
updated and u is the update target that s   s estimated value is shifted toward. for example, the monte
carlo update for value prediction is st (cid:55)    gt, the td(0) update is st (cid:55)    rt+1 +     v(st+1,wt), and
the n-step td update is st (cid:55)    gt:t+n. in the dp (id145) policy-evaluation update,

161

162

chapter 9. on-policy prediction with approximation

s (cid:55)    e  [rt+1 +     v(st+1,wt) | st = s], an arbitrary state s is updated, whereas in the other cases the
state encountered in actual experience, st, is updated.

it is natural to interpret each update as specifying an example of the desired input   output behavior
of the value function. in a sense, the update s (cid:55)    u means that the estimated value for state s should
be more like the update target u. up to now, the actual update has been trivial: the table entry for
s   s estimated value has simply been shifted a fraction of the way toward u, and the estimated values of
all other states were left unchanged. now we permit arbitrarily complex and sophisticated methods to
implement the update, and updating at s generalizes so that the estimated values of many other states
are changed as well. machine learning methods that learn to mimic input   output examples in this
way are called supervised learning methods, and when the outputs are numbers, like u, the process is
often called function approximation. function approximation methods expect to receive examples of the
desired input   output behavior of the function they are trying to approximate. we use these methods
for value prediction simply by passing to them the s (cid:55)    g of each update as a training example. we
then interpret the approximate function they produce as an estimated value function.

viewing each update as a conventional training example in this way enables us to use any of a
wide range of existing function approximation methods for value prediction.
in principle, we can
use any method for supervised learning from examples, including arti   cial neural networks, decision
trees, and various kinds of multivariate regression. however, not all function approximation methods
are equally well suited for use in id23. the most sophisticated neural network and
statistical methods all assume a static training set over which multiple passes are made. in reinforcement
learning, however, it is important that learning be able to occur on-line, while the agent interacts with
its environment or with a model of its environment. to do this requires methods that are able to learn
e   ciently from incrementally acquired data.
in addition, id23 generally requires
function approximation methods able to handle nonstationary target functions (target functions that
change over time). for example, in control methods based on gpi (generalized policy iteration) we often
seek to learn q   while    changes. even if the policy remains the same, the target values of training
examples are nonstationary if they are generated by id64 methods (dp and td learning).
methods that cannot easily handle such nonstationarity are less suitable for id23.

9.2 the prediction objective (ve)

up to now we have not speci   ed an explicit objective for prediction. in the tabular case a continuous
measure of prediction quality was not necessary because the learned value function could come to equal
the true value function exactly. moreover, the learned values at each state were decoupled   an update
at one state a   ected no other. but with genuine approximation, an update at one state a   ects many
others, and it is not possible to get the values of all states exactly correct. by assumption we have
far more states than weights, so making one state   s estimate more accurate invariably means making
others    less accurate. we are obligated then to say which states we care most about. we must specify

a state weighting or distribution   (s)     0,(cid:80)s   (s) = 1, representing how much we care about the error

in each state s. by the error in a state s we mean the square of the di   erence between the approximate
value   v(s,w) and the true value v  (s). weighting this over the state space by   , we obtain a natural
objective function, the mean squared value error, denoted ve:

ve(w)

.

=(cid:88)s   s

  (s)(cid:104)v  (s)       v(s,w)(cid:105)2

.

(9.1)

the square root of this measure, the root ve, gives a rough measure of how much the approximate
values di   er from the true values and is often used in plots. often   (s) is chosen to be the fraction of
time spent in s. under on-policy training this is called the on-policy distribution; we focus entirely on

9.2. the prediction objective (ve)

163

this case in this chapter. in continuing tasks, the on-policy distribution is the stationary distribution
under   .

the on-policy distribution in episodic tasks

in an episodic task, the on-policy distribution is a little di   erent in that it depends on how the
initial states of episodes are chosen. let h(s) denote the id203 that an episode begins in
each state s, and let   (s) denote the number of time steps spent, on average, in state s in a
single episode. time is spent in a state s if episodes start in s, or if transitions are made into s
from a preceding state   s in which time is spent:

  (s) = h(s) +(cid:88)  s

  (  s)(cid:88)a

  (a|  s)p(s|   s, a),

for all s     s.

(9.2)

this system of equations can be solved for the expected number of visits   (s). the on-policy
distribution is then the fraction of time spent in each state normalized to sum to one:

  (s)

(cid:80)s   (s)

  (s) =

,

for all s     s.

(9.3)

this is the natural choice without discounting. if there is discounting (   < 1) it should be treated
as a form of termination, which can be done simply by including a factor of    in the second
term of (9.2). although this is more general, it would complicate the following presentation of
algorithms and concerns a rare case that we don   t treat in this chapter, so we omit it here.

the two cases, continuing and episodic, behave similarly, but with approximation they must be
treated separately in formal analyses, as we will see repeatedly in this part of the book. this completes
the speci   cation of the learning objective.

but it is not completely clear that the ve is the right performance objective for reinforcement
learning. remember that our ultimate purpose   the reason we are learning a value function   is to    nd
a better policy. the best value function for this purpose is not necessarily the best for minimizing ve.
nevertheless, it is not yet clear what a more useful alternative goal for value prediction might be. for
now, we will focus on ve.

an ideal goal in terms of ve would be to    nd a global optimum, a weight vector w    for which
ve(w   )     ve(w) for all possible w. reaching this goal is sometimes possible for simple function
approximators such as linear ones, but is rarely possible for complex function approximators such as
arti   cial neural networks and id90. short of this, complex function approximators may seek
to converge instead to a local optimum, a weight vector w    for which ve(w   )     ve(w) for all w in
some neighborhood of w   . although this guarantee is only slightly reassuring, it is typically the best
that can be said for nonlinear function approximators, and often it is enough. still, for many cases of
interest in id23 there is no guarantee of convergence to an optimum, or even to within
a bounded distance of an optimum. some methods may in fact diverge, with their ve approaching
in   nity in the limit.

in the last two sections we outlined a framework for combining a wide range of id23
methods for value prediction with a wide range of function approximation methods, using the updates
of the former to generate training examples for the latter. we also described a ve performance measure
which these methods may aspire to minimize. the range of possible function approximation methods is
far too large to cover all, and anyway too little is known about most of them to make a reliable evaluation
or recommendation. of necessity, we consider only a few possibilities. in the rest of this chapter we
focus on function approximation methods based on gradient principles, and on linear gradient-descent
methods in particular. we focus on these methods in part because we consider them to be particularly

164

chapter 9. on-policy prediction with approximation

promising and because they reveal key theoretical issues, but also because they are simple and our space
is limited.

9.3 stochastic-gradient and semi-gradient methods

we now develop in detail one class of learning methods for function approximation in value prediction,
those based on stochastic id119 (sgd). sgd methods are among the most widely used of
all function approximation methods and are particularly well suited to online id23.

in gradient-descent methods, the weight vector is a column vector with a    xed number of real valued
.
= (w1, w2, . . . , wd)(cid:62),1 and the approximate value function   v(s,w) is a di   erentiable
components, w
function of w for all s     s. we will be updating w at each of a series of discrete time steps, t =
0, 1, 2, 3, . . ., so we will need a notation wt for the weight vector at each step. for now, let us assume
that, on each step, we observe a new example st (cid:55)    v  (st) consisting of a (possibly randomly selected)
state st and its true value under the policy. these states might be successive states from an interaction
with the environment, but for now we do not assume so. even though we are given the exact, correct
values, v  (st) for each st, there is still a di   cult problem because our function approximator has limited
resources and thus limited resolution. in particular, there is generally no w that gets all the states, or
even all the examples, exactly correct. in addition, we must generalize to all the other states that have
not appeared in examples.

we assume that states appear in examples with the same distribution,   , over which we are trying
to minimize the ve as given by (9.1). a good strategy in this case is to try to minimize error on
the observed examples. stochastic gradient-descent (sgd) methods do this by adjusting the weight
vector after each example by a small amount in the direction that would most reduce the error on that
example:

wt+1

1
2

.
= wt    

     (cid:104)v  (st)       v(st,wt)(cid:105)2

= wt +   (cid:104)v  (st)       v(st,wt)(cid:105)     v(st,wt),

(9.4)

(9.5)

where    is a positive step-size parameter, and    f (w), for any scalar expression f (w), denotes the
vector of partial derivatives with respect to the components of the weight vector:

   f (w)

.

=(cid:18)    f (w)

   w1

,

   f (w)
   w2

, . . . ,

   f (w)

   wd (cid:19)(cid:62)

.

(9.6)

this derivative vector is the gradient of f with respect to w. sgd methods are    id119   
methods because the overall step in wt is proportional to the negative gradient of the example   s squared
error (9.4). this is the direction in which the error falls most rapidly. id119 methods are
called    stochastic    when the update is done, as here, on only a single example, which might have been
selected stochastically. over many examples, making small steps, the overall e   ect is to minimize an
average performance measure such as the ve.

it may not be immediately apparent why sgd takes only a small step in the direction of the gradient.
could we not move all the way in this direction and completely eliminate the error on the example? in
many cases this could be done, but usually it is not desirable. remember that we do not seek or expect
to    nd a value function that has zero error for all states, but only an approximation that balances the
errors in di   erent states. if we completely corrected each example in one step, then we would not    nd
such a balance. in fact, the convergence results for sgd methods assume that    decreases over time.

1the (cid:62) denotes transpose, needed here to turn the horizontal row vector in the text into a vertical column vector; in

this book vectors are generally taken to be column vectors unless explicitly written out horizontally or transposed.

9.3. stochastic-gradient and semi-gradient methods

165

if it decreases in such a way as to satisfy the standard stochastic approximation conditions (2.7), then
the sgd method (9.5) is guaranteed to converge to a local optimum.

we turn now to the case in which the target output, here denoted ut     r, of the tth training example,
st (cid:55)    ut, is not the true value, v  (st), but some, possibly random, approximation to it. for example,
ut might be a noise-corrupted version of v  (st), or it might be one of the id64 targets using
  v mentioned in the previous section. in these cases we cannot perform the exact update (9.5) because
v  (st) is unknown, but we can approximate it by substituting ut in place of v  (st). this yields the
following general sgd method for state-value prediction:

wt+1

.

= wt +   (cid:104)ut       v(st,wt)(cid:105)     v(st,wt).

(9.7)

if ut is an unbiased estimate, that is, if e[ut|st = s] = v  (st), for each t, then wt is guaranteed to
converge to a local optimum under the usual stochastic approximation conditions (2.7) for decreasing
  .

for example, suppose the states in the examples are the states generated by interaction (or simulated
interaction) with the environment using policy   . because the true value of a state is the expected value
.
of the return following it, the monte carlo target ut
= gt is by de   nition an unbiased estimate of v  (st).
with this choice, the general sgd method (9.7) converges to a locally optimal approximation to v  (st).
thus, the gradient-descent version of monte carlo state-value prediction is guaranteed to    nd a locally
optimal solution. pseudocode for a complete algorithm is shown in the box below.

gradient monte carlo algorithm for estimating   v     v  
input: the policy    to be evaluated
input: a di   erentiable function   v : s    rd     r
initialize value-function weights w as appropriate (e.g., w = 0)
repeat forever:

generate an episode s0, a0, r1, s1, a1, . . . , rt , st using   
for t = 0, 1, . . . , t     1:

w     w +   (cid:2)gt       v(st,w)(cid:3)     v(st,w)

one does not obtain the same guarantees if a id64 estimate of v  (st) is used as the target ut

in (9.7). id64 targets such as n-step returns gt:t+n or the dp target(cid:80)a,s(cid:48),r   (a|st)p(s(cid:48), r|st, a)[r+

    v(s(cid:48),wt)] all depend on the current value of the weight vector wt, which implies that they will be bi-
ased and that they will not produce a true gradient-descent method. one way to look at this is that
the key step from (9.4) to (9.5) relies on the target being independent of wt. this step would not be
valid if a id64 estimate were used in place of v  (st). id64 methods are not in fact
instances of true id119 (barnard, 1993). they take into account the e   ect of changing the
weight vector wt on the estimate, but ignore its e   ect on the target. they include only a part of the
gradient and, accordingly, we call them semi-gradient methods.

although semi-gradient (id64) methods do not converge as robustly as gradient methods,
they do converge reliably in important cases such as the linear case discussed in the next section.
moreover, they o   er important advantages that make them often clearly preferred. one reason for
this is that they typically enable signi   cantly faster learning, as we have seen in chapters 6 and 7.
another is that they enable learning to be continual and online, without waiting for the end of an
episode. this enables them to be used on continuing problems and provides computational advantages.
.
a prototypical semi-gradient method is semi-gradient td(0), which uses ut
= rt+1 +     v(st+1,w) as
its target. complete pseudocode for this method is given in the box below.

166

chapter 9. on-policy prediction with approximation

semi-gradient td(0) for estimating   v     v  
input: the policy    to be evaluated
input: a di   erentiable function   v : s+    rd     r such that   v(terminal,  ) = 0
initialize value-function weights w arbitrarily (e.g., w = 0)
repeat (for each episode):

initialize s
repeat (for each step of episode):

choose a       (  |s)
take action a, observe r, s(cid:48)

w     w +   (cid:2)r +     v(s(cid:48),w)       v(s,w)(cid:3)     v(s,w)
s     s(cid:48)

until s(cid:48) is terminal

state aggregation is a simple form of generalizing function approximation in which states are grouped
together, with one estimated value (one component of the weight vector w) for each group. the value
of a state is estimated as its group   s component, and when the state is updated, that component alone
is updated. state aggregation is a special case of sgd (9.7) in which the gradient,      v(st,wt), is 1 for
st   s group   s component and 0 for the other components.

example 9.1: state aggregation on the 1000-state random walk consider a 1000-state
version of the random walk task (examples 6.2 and 7.1 on pages 102 and 118). the states are numbered
from 1 to 1000, left to right, and all episodes begin near the center, in state 500. state transitions are
from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring
states to its right, all with equal id203. of course, if the current state is near an edge, then there
may be fewer than 100 neighbors on that side of it. in this case, all the id203 that would have
gone into those missing neighbors goes into the id203 of terminating on that side (thus, state 1 has
a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). as
usual, termination on the left produces a reward of    1, and termination on the right produces a reward
of +1. all other transitions have a reward of zero. we use this task as a running example throughout
this section.

figure 9.1 shows the true value function v   for this task.

it is nearly a straight line, but tilted
slightly toward the horizontal and curving further in this direction for the last 100 states at each end.
also shown is the    nal approximate value function learned by the gradient monte-carlo algorithm with
state aggregation after 100,000 episodes with a step size of    = 2    10   5. for the state aggregation,
the 1000 states were partitioned into 10 groups of 100 states each (i.e., states 1   100 were one group,
states 101   200 were another, and so on). the staircase e   ect shown in the    gure is typical of state
aggregation; within each group, the approximate value is constant, and it changes abruptly from one
group to the next. these approximate values are close to the global minimum of the ve (9.1).

some of the details of the approximate values are best appreciated by reference to the state distri-
bution    for this task, shown in the lower portion of the    gure with a right-side scale. state 500, in
the center, is the    rst state of every episode, but is rarely visited again. on average, about 1.37% of
the time steps are spent in the start state. the states reachable in one step from the start state are
the second most visited, with about 0.17% of the time steps being spent in each of them. from there
   falls o    almost linearly, reaching about 0.0147% at the extreme states 1 and 1000. the most visible
e   ect of the distribution is on the leftmost groups, whose values are clearly shifted higher than the
unweighted average of the true values of states within the group, and on the rightmost groups, whose
values are clearly shifted lower. this is due to the states in these areas having the greatest asymmetry

9.4. linear methods

167

figure 9.1: function approximation by state aggregation on the 1000-state random walk task, using the gradient
monte carlo algorithm (page 165).

in their weightings by   . for example, in the leftmost group, state 99 is weighted more than 3 times
more strongly than state 0. thus the estimate for the group is biased toward the true value of state 99,
which is higher than the true value of state 0.

9.4 linear methods

one of the most important special cases of function approximation is that in which the approximate
function,   v(  ,w), is a linear function of the weight vector, w. corresponding to every state s, there is a
.
= (x1(s), x2(s), . . . , xd(s))(cid:62), with the same number of components as w. linear
real-valued vector x(s)
methods approximate state-value function by the inner product between w and x(s):

  v(s,w)

.
= w(cid:62)x(s)

.
=

d(cid:88)i=1

wixi(s).

(9.8)

in this case the approximate value function is said to be linear in the weights, or simply linear.

the vector x(s) is called a feature vector representing state s. each component xi(s) of x(s) is the
value of a function xi : s     r. we think of a feature as the entirety of one of these functions, and we
call its value for a state s a feature of s. for linear methods, features are basis functions because they
form a linear basis for the set of approximate functions. constructing d-dimensional feature vectors to
represent states is the same as selecting a set of d basis functions. features may be de   ned in many
di   erent ways; we cover a few possibilities in the next sections.

it is natural to use sgd updates with linear function approximation. the gradient of the approximate

value function with respect to w in this case is

     v(s,w) = x(s).

thus, in the linear case the general sgd update (9.7) reduces to a particularly simple form:

wt+1

.

= wt +   (cid:104)ut       v(st,wt)(cid:105)x(st).

because it is so simple, the linear sgd case is one of the most favorable for mathematical analysis.
almost all useful convergence results for learning systems of all kinds are for linear (or simpler) function
approximation methods.

0statevaluescale    true valuev       approximate mc value  v    state distribution         0.00170.0137distributionscale100010-11  168

chapter 9. on-policy prediction with approximation

in particular, in the linear case there is only one optimum (or, in degenerate cases, one set of equally
good optima), and thus any method that is guaranteed to converge to or near a local optimum is
automatically guaranteed to converge to or near the global optimum. for example, the gradient monte
carlo algorithm presented in the previous section converges to the global optimum of the ve under
linear function approximation if    is reduced over time according to the usual conditions.

the semi-gradient td(0) algorithm presented in the previous section also converges under linear
function approximation, but this does not follow from general results on sgd; a separate theorem is
necessary. the weight vector converged to is also not the global optimum, but rather a point near the
local optimum. it is useful to consider this important case in more detail, speci   cally for the continuing
case. the update at each time t is

(9.9)

(9.10)

(9.11)

wt+1

.

= wt +   (cid:16)rt+1 +   w(cid:62)t xt+1     w(cid:62)t xt(cid:17)xt
= wt +   (cid:16)rt+1xt     xt(cid:0)xt       xt+1(cid:1)(cid:62)wt(cid:17),

where here we have used the notational shorthand xt = x(st). once the system has reached steady
state, for any given wt, the expected next weight vector can be written

e[wt+1|wt] = wt +   (b     awt),

where

b

.
= e[rt+1xt]     rd

and a

.

= e(cid:104)xt(cid:0)xt       xt+1(cid:1)(cid:62)(cid:105)     rd    rd

from (9.10) it is clear that, if the system converges, it must converge to the weight vector wtd at which

b     awtd = 0

b = awtd
.
= a   1b.

wtd

   
   

(9.12)

this quantity is called the td    xed point. in fact linear semi-gradient td(0) converges to this point.
some of the theory proving its convergence, and the existence of the inverse above, is given in the box.

proof of convergence of linear td(0)

what properties assure convergence of the linear td(0) algorithm (9.9)? some insight can be
gained by rewriting (9.10) as

e[wt+1|wt] = (i       a)wt +   b.

(9.13)

note that the matrix a multiplies the weight vector wt and not b; only a is important to
convergence. to develop intuition, consider the special case in which a is a diagonal matrix. if
any of the diagonal elements are negative, then the corresponding diagonal element of i       a
will be greater than one, and the corresponding component of wt will be ampli   ed, which will
lead to divergence if continued. on the other hand, if the diagonal elements of a are all positive,
then    can be chosen smaller than one over the largest of them, such that i       a is diagonal
with all diagonal elements between 0 and 1. in this case the    rst term of the update tends to
shrink wt, and stability is assured. in general case, wt will be reduced toward zero whenever a
is positive de   nite, meaning y(cid:62)ay > 0 for real vector y. positive de   niteness also ensures that
the inverse a   1 exists.

9.4. linear methods

169

for linear td(0), in the continuing case with    < 1, the a matrix (9.11) can be written

p(r, s(cid:48)|s, a)x(s)(cid:0)x(s)       x(s(cid:48))(cid:1)(cid:62)

  (s)(cid:88)a
  (s)(cid:88)s(cid:48)
  (s)x(s)(cid:18)x(s)       (cid:88)s(cid:48)

  (a|s)(cid:88)r,s(cid:48)
p(s(cid:48)|s)x(s)(cid:0)x(s)       x(s(cid:48))(cid:1)(cid:62)
p(s(cid:48)|s)x(s(cid:48))(cid:19)(cid:62)

a =(cid:88)s
=(cid:88)s
=(cid:88)s
= x(cid:62)d(i       p)x,

where   (s) is the stationary distribution under   , p(s(cid:48)|s) is the id203 of transition from s
to s(cid:48) under policy   , p is the |s|    |s| matrix of these probabilities, d is the |s|    |s| diagonal
matrix with the   (s) on its diagonal, and x is the |s|    d matrix with x(s) as its rows. from
here it is clear that the inner matrix d(i       p) is key to determining the positive de   niteness
of a.
for a key matrix of this type, positive de   niteness is assured if all of its columns sum to a
nonnegative number. this was shown by sutton (1988, p. 27) based on two previously established
theorems. one theorem says that any matrix m is positive de   nite if and only if the symmetric
matrix s = m + m(cid:62) is positive de   nite (sutton 1988, appendix). the second theorem says
that any symmetric real matrix s is positive de   nite if all of its diagonal entries are positive and
greater than the sum of the corresponding o   -diagonal entries (varga 1962, p. 23). for our key
matrix, d(i      p), the diagonal entries are positive and the o   -diagonal entries are negative, so
all we have to show is that each row sum plus the corresponding column sum is positive. the
row sums are all positive because p is a stochastic matrix and    < 1. thus it only remains to
show that the column sums are nonnegative. note that the row vector of the column sums of
any matrix m can be written as 1(cid:62)m, where 1 is the column vector with all components equal
to 1. let    denote the |s|-vector of the   (s), where    = p(cid:62)   by virtue of    being the stationary
distribution. the column sums of our key matrix, then, are:

1(cid:62)d(i       p) =   (cid:62)(i       p)
=   (cid:62)         (cid:62)p
=   (cid:62)         (cid:62)
= (1       )  ,

(because    is the stationary distribution)

all components of which are positive. thus, the key matrix and its a matrix are positive de   nite,
and on-policy td(0) is stable. (additional conditions and a schedule for reducing    over time
are needed to prove convergence with id203 one.)

at the td    xed point, it has also been proven (in the continuing case) that the ve is within a

bounded expansion of the lowest possible error:

ve(wtd)    

1
1       

min

w

ve(w).

(9.14)

that is, the asymptotic error of the td method is no more than 1
1      times the smallest possible error,
that attained in the limit by the monte carlo method. because    is often near one, this expansion
factor can be quite large, so there is substantial potential loss in asymptotic performance with the td
method. on the other hand, recall that the td methods are often of vastly reduced variance compared
to monte carlo methods, and thus faster, as we saw in chapters 6 and 7. which method will be best

170

chapter 9. on-policy prediction with approximation

depends on the nature of the approximation and problem, and on how long learning continues.

.

a bound analogous to (9.14) applies to other on-policy id64 methods as well. for example,

linear semi-gradient dp (eq. 9.7 with ut
according to the on-policy distribution will also converge to the td    xed point. one-step semi-gradient
action-value methods, such as semi-gradient sarsa(0) covered in the next chapter converge to an anal-
ogous    xed point and an analogous bound. for episodic tasks, there is a slightly di   erent but related
bound (see bertsekas and tsitsiklis, 1996). there are also a few technical conditions on the rewards,
features, and decrease in the step-size parameter, which we have omitted here. the full details can be
found in the original paper (tsitsiklis and van roy, 1997).

=(cid:80)a   (a|st)(cid:80)s(cid:48),r p(s(cid:48), r|st, a)[r +     v(s(cid:48),wt)]) with updates

critical to the these convergence results is that states are updated according to the on-policy dis-
tribution. for other update distributions, id64 methods using function approximation may
actually diverge to in   nity. examples of this and a discussion of possible solution methods are given in
chapter 11.

example 9.2: id64 on the 1000-state random walk state aggregation is a special
case of linear function approximation, so let   s return to the 1000-state random walk to illustrate some of
the observations made in this chapter. the left panel of figure 9.2 shows the    nal value function learned
by the semi-gradient td(0) algorithm (page 166) using the same state aggregation as in example 9.1.
we see that the near-asymptotic td approximation is indeed farther from the true values than the
monte carlo approximation shown in figure 9.1.

nevertheless, td methods retain large potential advantages in learning rate, and generalize monte
carlo methods, as we investigated fully with the multi-step td methods of chapter 7. the right panel
of figure 9.2 shows results with an n-step semi-gradient td method using state aggregation and the
1000-state random walk that are strikingly similar to those we obtained earlier with tabular methods
and the 19-state random walk (figure 7.2). to obtain such quantitatively similar results we switched
the state aggregation to 20 groups of 50 states each. the 20 groups are then quantitatively close to the
19 states of the tabular problem. in particular, the state transitions of at-most 100 states to the right
or left, or 50 states on average, were quantitively analogous to the single-state state transitions of the
tabular system. to complete the match, we use here the same performance measure   an unweighted
average of the rms error over all states and over the    rst 10 episodes   rather than a ve objective as
is otherwise more appropriate when using function approximation.

figure 9.2: id64 with state aggregation on the 1000-state random walk task. left: asymptotic values
of semi-gradient td are worse than the asymptotic monte carlo values in figure 9.1. right: performance of n-
step methods with state-aggregation are strikingly similar to those with tabular representations (cf. figure 7.2).

   averagerms errorover 1000 statesand    rst 10 episodesn=1n=2n=4n=8n=16n=32n=64128512256state    true valuev       approximate td value10-111000  v9.5. feature construction for linear methods

171

the semi-gradient n-step td algorithm we used in this example is the natural extension of the
tabular n-step td algorithm presented in chapter 7 to semi-gradient function approximation. the key
equation, analogous to (7.2), is

wt+n

.
= wt+n   1 +    [gt:t+n       v(st,wt+n   1)]     v(st,wt+n   1),

0     t < t,

where the n-step return is generalized from (7.1) to

gt:t+n

.
= rt+1 +   rt+2 +        +   n   1rt+n +   n  v(st+n,wt+n   1), 0     t     t     n.

pseudocode for the complete algorithm is given in the box below.

(9.15)

(9.16)

n-step semi-gradient td for estimating   v     v  
input: the policy    to be evaluated
input: a di   erentiable function   v : s+    rd     r such that   v(terminal,  ) = 0
parameters: step size        (0, 1], a positive integer n
all store and access operations (st and rt) can take their index mod n

initialize value-function weights w arbitrarily (e.g., w = 0)
repeat (for each episode):

if t < t , then:

initialize and store s0 (cid:54)= terminal
t        for t = 0, 1, 2, . . . :
|
|
|
|
|
|
|
|
|
until    = t     1

g    (cid:80)min(   +n,t )

       t     n + 1
if        0:

  i        1ri

i=   +1

take an action according to   (  |st)
observe and store the next reward as rt+1 and the next state as st+1
if st+1 is terminal, then t     t + 1

(   is the time whose state   s estimate is being updated)

if    + n < t , then: g     g +   n  v(s   +n,w)
w     w +    [g       v(s   ,w)]     v(s   ,w)

(g   :   +n)

9.5 feature construction for linear methods

linear methods are interesting because of their convergence guarantees, but also because in practice
they can be very e   cient in terms of both data and computation. whether or not this is so depends
critically on how the states are represented in terms of features, which we investigate in this large section.
choosing features appropriate to the task is an important way of adding prior domain knowledge to
id23 systems. intuitively, the features should correspond to the aspects of the state
space along which generalization may be appropriate. if we are valuing geometric objects, for example,
we might want to have features for each possible shape, color, size, or function. if we are valuing states
of a mobile robot, then we might want to have features for locations, degrees of remaining battery
power, recent sonar readings, and so on.

a limitation of the linear form is that it cannot take into account any interactions between features,
such as the presence of feature i being good only in the absence of feature j. for example, in the
pole-balancing task (example 3.4), a high angular velocity can be either good or bad depending on

172

chapter 9. on-policy prediction with approximation

the angle. if the angle is high, then high angular velocity means an imminent danger of falling   a bad
state   whereas if the angle is low, then high angular velocity means the pole is righting itself   a good
state. a linear value function could not represent this if its features coded separately for the angle and
the angular velocity. it needs instead, or in addition, features for combinations of these two underlying
state dimensions. in the following subsections we consider a variety of general ways of doing this.

9.5.1 polynomials

the states of many problems are initially expressed as numbers, such as positions and velocities in
the pole-balancing task (example 3.4), the number of cars in each lot in the jack   s car rental problem
(example 4.2), or the gambler   s capital in the gambler problem (example 4.3).
in these types of
problems, function approximation for id23 has much in common with the familiar
tasks of interpolation and regression. various families of features commonly used for interpolation
and regression can also be used in id23. polynomials make up one of the simplest
families of features used for interpolation and regression consists. while the basic polynomial features
we discuss here do not work as well as other types of features in id23, they serve as
a good introduction because they are simple and familiar.

as an example, suppose a id23 problem has states with two numerical dimensions.
for a single representative state s, let its two numbers be s1     r and s2     r. you might choose to
represent s simply by its two state dimensions, so that x(s) = (s1, s2)(cid:62), but then you would not be
able to take into account any interactions between these dimensions. in addition, if both s1 and s2
were zero, then the approximate value would have to also be zero. both limitations can be overcome
by instead representing s by the four-dimensional feature vector x(s) = (1, s1, s2, s1s2)(cid:62). the initial
1 feature allows the representation of a   ne functions in the original state numbers, and the    nal
product feature, s1s2, enables interactions to be taken into account. or you might choose to use higher-
2)(cid:62) to take more complex
dimensional feature vectors like x(s) = (1, s1, s2, s1s2, s2
interactions into account. such feature vectors enable approximations as arbitrary quadratic functions
of the state numbers   even though the approximation is still linear in the weights that have to be
learned. generalizing this example from two to k numbers, we can represent highly-complex interactions
among a problem   s state dimensions:

2, s1s2

1s2, s2

1, s2

2, s2

1s2

suppose each state s corresponds to k numbers, s1, s2, ..., sk, with each si     r. for this
k-dimensional state space, each order-n polynomial-basis feature xi can be written as

xi(s) =   k

j=1sci,j

j

,

(9.17)

where each ci,j is an integer in the set {0, 1, . . . , n} for an integer n     0. these features make
up the order-n polynomial basis for dimension k, which contains (n + 1)k di   erent features.

higher-order polynomial bases allow for more accurate approximations of more complicated func-
tions. but because the number of features in an order-n polynomial basis grows exponentially with the
dimension k of the natural state space (if n > 0), it is generally necessary to select a subset of them
for function approximation. this can be done using prior beliefs about the nature of the function to
be approximated, and some automated selection methods developed for polynomial regression can be
adapted to deal with the incremental and nonstationary nature of id23.

exercise 9.1 why does (9.17) de   ne (n + 1)k distinct features for dimension k?

exercise 9.2 what n and ci,j produce the feature vectors x(s) = (1, s1, s2, s1s2, s2
(cid:3)

1, s2

2, s1s2

2, s2

1s2, s2

1s2

(cid:3)
2)(cid:62)?

9.5. feature construction for linear methods

173

9.5.2 fourier basis

another linear function approximation method is based on the time-honored fourier series, which
expresses periodic functions as weighted sums of sine and cosine basis functions (features) of di   erent
frequencies. (a function f is periodic if f (x) = f (x +    ) for all x and some period    .) the fourier
series and the more general fourier transform are widely used in applied sciences in part because if a
function to be approximated is known, then the basis function weights are given by simple formulae
and, further, with enough basis functions essentially any function can be approximated as accurately as
desired. in id23, where the functions to be approximated are unknown, fourier basis
functions are of interest because they are easy to use and can perform well in a range of reinforcement
learning problems.

first consider the one-dimensional case. the usual fourier series representation of a function of one
dimension having period    represents the function as a linear combination of sine and cosine functions
that are each periodic with periods that evenly divide    (in other words, whose frequencies are integer
multiples of a fundamental frequency 1/   ). but if you are interested in approximating an aperiodic
function de   ned over a bounded interval, then you can use these fourier basis featues with    set to the
length the interval. the function of interest is then just one period of the periodic linear combination
of the sine and cosine features.

furthermore, if you set    to twice the length of the interval of interest and restrict attention to the
approximation over the half interval [0,    /2], then you can use just the cosine features. this is possible
because you can represent any even function, that is, any function that is symmetric about the origin,
with just the cosine basis. so any function over the half-period [0,    /2] can be approximated as closely as
desired with enough cosine features. (saying    any function    is not exactly correct because the function
has to be mathematically well-behaved, but we skip this technicality here.) alternatively, it is possible
to use just sine features, linear combinations of which are always odd functions, that is functions that
are anti-symmetric about the origin. but it is generally better to keep just the cosine features because
   half-even    functions tend to be easier to approximate than    half-odd    functions since the latter are
often discontinuous at the origin. of course, this does not rule out using both sine and cosine features
to approximate over the interval [0,    /2], which might advantageous in some circumstances.

following this logic and letting    = 2 so that the features are de   ned over the half-   interval [0, 1],

the one-dimensional order-n fourier cosine basis consists of the n + 1 features

xi(s) = cos(i  s), s     [0, 1],

for i = 0, . . . , n. figure 9.3 shows one-dimensional fourier cosine features xi, for i = 1, 2, 3, 4; x0 is a
constant function.

figure 9.3: one-dimensional fourier cosine-basis features xi, i = 1, 2, 3, 4, for approximating functions over
the interval [0, 1]. after konidaris et al. (2011).

00.10.20.30.40.50.60.70.80.91   1   0.8   0.6   0.4   0.200.20.40.60.81univariate fourier basis function k=11-11000.10.20.30.40.50.60.70.80.91   1   0.8   0.6   0.4   0.200.20.40.60.81univariate fourier basis function k=21-11000.10.20.30.40.50.60.70.80.91   1   0.8   0.6   0.4   0.200.20.40.60.81univariate fourier basis function k=31-11000.10.20.30.40.50.60.70.80.91   1   0.8   0.6   0.4   0.200.20.40.60.81univariate fourier basis function k=41-110174

chapter 9. on-policy prediction with approximation

this same reasoning applies to the fourier cosine series approximation in the multi-dimensional case

as described in the box below.

suppose each state s corresponds to a vector of k numbers, s = (s1, s2, ..., sk)(cid:62), with each
si     [0, 1]. the ith feature in the order-n fourier cosine basis can then be written

(9.18)

xi(s) = cos(cid:0)   s(cid:62)ci(cid:1) ,

1, . . . , ci

k)(cid:62), with ci

j     {0, . . . , n} for j = 1, . . . , k and i = 0, . . . , (n + 1)k. this
where ci = (ci
de   nes a feature for each of the (n + 1)k possible integer vectors ci. the inner product s(cid:62)ci has
the e   ect of assigning an integer in {0, . . . , n} to each dimension of s. as in the one-dimensional
case, this integer determines the feature   s frequency along that dimension. the features can of
course be shifted and scaled to suit the bounded state space of a particular application.

as an example, consider the k = 2 case in which s = (s1, s2)(cid:62), where each ci = (ci
2)(cid:62). figure 9.4
shows a selection of six fourier cosine features, each labeled by the vector ci that de   nes it (s1 is the
horizontal axis and ci is shown as a row vector with the index i omitted). any zero in c means the
feature is constant along that state dimension. so if c = (0, 0)(cid:62), the feature is constant over both
dimensions; if c = (c1, 0)(cid:62) the feature is constant over the second dimension and varies over the    rst
with frequency depending on c1; and similarly, for c = (0, c2)(cid:62). when c = (c1, c2)(cid:62) with neither
cj = 0, the feature varies along both dimensions and represents an interaction between the two state
variables. the values of c1 and c2 determine the frequency along each dimension, and their ratio gives
the direction of the interaction.

1, ci

when using fourier cosine features with a learning algorithm such as (9.7), semi-gradient td(0),
or semi-gradient sarsa, it may be helpful to use a di   erent step-size parameter for each feature.
if
   is the basic step-size parameter, then konidaris, osentoski, and thomas (2011) suggest setting the
j = 0, in which

k)2 (except when each ci

step-size parameter for feature xi to   i =   /(cid:112)(ci

1)2 +        + (ci

figure 9.4: a selection of six two-dimensional fourier cosine features, each labeled by the vector ci that de   nes
it (s1 is the horizontal axis, and ci is shown with the index i omitted). after konidaris et al. (2011).

c = (0, 1)11001100c=(0,1)>c = (1, 0)11001100c=(1,0)>c = (1, 1)11001100c=(1,1)>c = (1, 5)11001100c=(0,5)>c = (2, 5)11001100c=(2,5)>1100c=(5,2)>9.5. feature construction for linear methods

175

case   i =   ).

fourier cosine features with sarsa can produce good performance compared to several other collections
of basis functions, including polynomial and radial basis functions. not surprisingly, however, fourier
features have trouble with discontinuities because it is di   cult to avoid    ringing    around points of
discontinuity unless very high frequency basis functions are included.

the number of features in the order-n fourier basis grows exponentially with the dimension of the
state space, but if that dimension is small enough (e.g., k     5), one can select n so that all of the order-n
fourier features can be used. this makes the selection of features more-or-less automatic. for high
dimension state spaces, however, it is necessary to select a subset of these features. this can be done
using prior beliefs about the nature of the function to be approximated, and some automated selection
methods can be adapted to deal with the incremental and nonstationary nature of reinforcement learn-
ing. an advantage of fourier basis features in this regard are that it is easy to select features by setting
the ci vectors to account for suspected interactions among the state variables, and by limiting the values
in the cj vectors so that the approximation can    lter out high frequency components considered to be
noise. on the other hand, because fourier features are non-zero over the entire state space (with the
few zeros excepted), they represent global properties of states, which can make it di   cult to    nd good
ways to represent local properties.

figure 9.5 shows learning curves comparing the fourier and polynomial bases on the 1000-state

random walk example. in general, we do not recommend using polynomials for online learning.2

figure 9.5: fourier basis vs polynomials on the 1000-state random walk. shown are learning curves for the
gradient monte carlo method with fourier and polynomial bases of order 5, 10, and 20. the step-size parameters
were roughly optimized for each case:    = 0.0001 for the polynomial basis and    = 0.00005 for the fourier
basis.

exercise 9.3 why does (9.18) de   ne (n + 1)k distinct features?

(cid:3)

9.5.3 coarse coding

consider a task in which the natural representation of the state set is a continuous two-dimensional
space. one kind of representation for this case is made up of features corresponding to circles in state
space, as shown in figure 9.6. if the state is inside a circle, then the corresponding feature has the value

2there are families of polynomials more complicated than those we have discussed, for example, di   erent families of
orthogonal polynomials, and these might work better, but at present there is little experience with them in reinforcement
learning.

.4.3.2.1005000episodespolynomial basisfourier basispve176

chapter 9. on-policy prediction with approximation

figure 9.6: coarse coding. generalization from state s to state s(cid:48) depends on the number of their features
whose receptive    elds (in this case, circles) overlap. these states have one feature in common, so there will be
slight generalization between them.

1 and is said to be present; otherwise the feature is 0 and is said to be absent. this kind of 1   0-valued
feature is called a binary feature. given a state, which binary features are present indicate within which
circles the state lies, and thus coarsely code for its location. representing a state with features that
overlap in this way (although they need not be circles or binary) is known as coarse coding.

assuming linear gradient-descent function approximation, consider the e   ect of the size and density
of the circles. corresponding to each circle is a single weight (a component of w) that is a   ected by
learning. if we train at one state, a point in the space, then the weights of all circles intersecting that
state will be a   ected. thus, by (9.8), the approximate value function will be a   ected at all states within
the union of the circles, with a greater e   ect the more circles a point has    in common    with the state,
as shown in figure 9.6. if the circles are small, then the generalization will be over a short distance, as
in figure 9.7 (left), whereas if they are large, it will be over a large distance, as in figure 9.7 (middle).
moreover, the shape of the features will determine the nature of the generalization. for example, if
they are not strictly circular, but are elongated in one direction, then generalization will be similarly
a   ected, as in figure 9.7 (right).

figure 9.7: generalization in linear function approximation methods is determined by the sizes and shapes of
the features    receptive    elds. all three of these cases have roughly the same number and density of features.

features with large receptive    elds give broad generalization, but might also seem to limit the learned
function to a coarse approximation, unable to make discriminations much    ner than the width of the
receptive    elds. happily, this is not the case. initial generalization from one point to another is indeed
controlled by the size and shape of the receptive    elds, but acuity, the    nest discrimination ultimately

s0sa) narrow generalizationb) broad generalizationc) asymmetric generalization9.5. feature construction for linear methods

177

possible, is controlled more by the total number of features.

example 9.3: coarseness of coarse coding this example illustrates the e   ect on learning of the
size of the receptive    elds in coarse coding. linear function approximation based on coarse coding and
(9.7) was used to learn a one-dimensional square-wave function (shown at the top of figure 9.8). the
values of this function were used as the targets, ut. with just one dimension, the receptive    elds were
intervals rather than circles. learning was repeated with three di   erent sizes of the intervals: narrow,
medium, and broad, as shown at the bottom of the    gure. all three cases had the same density of
features, about 50 over the extent of the function being learned. training examples were generated
uniformly at random over this extent. the step-size parameter was    = 0.2
n , where n is the number of
features that were present at one time. figure 9.8 shows the functions learned in all three cases over
the course of learning. note that the width of the features had a strong e   ect early in learning. with
broad features, the generalization tended to be broad; with narrow features, only the close neighbors of
each trained point were changed, causing the function learned to be more bumpy. however, the    nal
function learned was a   ected only slightly by the width of the features. receptive    eld shape tends to
have a strong e   ect on generalization but little e   ect on asymptotic solution quality.

figure 9.8: example of feature width   s strong e   ect on initial generalization (   rst row) and weak e   ect on
asymptotic accuracy (last row).

9.5.4 tile coding

tile coding is a form of coarse coding for multi-dimensional continuous spaces that is    exible and
computationally e   cient. it may be the most practical feature representation for modern sequential
digital computers. open-source software is available for many kinds of tile coding.

in tile coding the receptive    elds of the features are grouped into partitions of the state space. each
such partition is called a tiling, and each element of the partition is called a tile. for example, the
simplest tiling of a two-dimensional state space is a uniform grid such as that shown on the left side of
figure 9.9. the tiles or receptive    eld here are squares rather than the circles in figure 9.6. if just this
single tiling were used, then the state indicated by the white spot would be represented by the single
feature whose tile it falls within; generalization would be complete to all states within the same tile and
nonexistent to states outside it. with just one tiling, we would not have coarse coding by just a case
of state aggregation.

1040160640256010240narrowfeaturesdesiredfunctionmediumfeaturesbroadfeatures#examplesapprox-imationfeaturewidth178

chapter 9. on-policy prediction with approximation

figure 9.9: multiple, overlapping grid-tilings on a limited two-dimensional space. these tilings are o   set from
one another by a uniform amount in each dimension.

to get the strengths of coarse coding requires overlapping receptive    elds, and by de   nition the tiles
of a partition do not overlap. to get true coarse coding with tile coding, multiple tilings are used,
each o   set by a fraction of a tile width. a simple case with four tilings is shown on the right side of
figure 9.9. every state, such as that indicated by the white spot, falls in exactly one tile in each of
the four tilings. these four tiles correspond to four features that become active when the state occurs.
speci   cally, the feature vector x(s) has one component for each tile in each tiling. in this example there
are 4    4    4 = 64 components, all of which will be 0 except for the four corresponding to the tiles that
s falls within. figure 9.10 shows the advantage of multiple o   set tilings (coarse coding) over a single
tiling on the 1000-state random walk example.

an immediate practical advantage of tile coding is that, because it works with partitions, the overall
number of features that are active at one time is the same for any state. exactly one feature is present
in each tiling, so the total number of features present is always the same as the number of tilings. this
allows the step-size parameter,   , to be set in an easy, intuitive way. for example, choosing    = 1
n ,
where n is the number of tilings, results in exact one-trial learning. if the example s (cid:55)    v is trained
on, then whatever the prior estimate,   v(s,wt), the new estimate will be   v(s,wt+1) = v. usually one

figure 9.10: why we use coarse coding. shown are learning curves on the 1000-state random walk example
for the gradient monte carlo algorithm with a single tiling and with multiple tilings. the space of 1000 states
was treated as a single continuous dimension, covered with tiles each 200 states wide. the multiple tilings were
o   set from each other by 4 states. the step-size parameter was set so that the initial learning rate in the two
cases was the same,    = 0.0001 for the single tiling and    = 0.0001/50 for the 50 tilings.

point in state spaceto berepresentedtiling 1tiling 2tiling 3tiling 4continuous 2d state spacefour activetiles/features overlap the pointand are used to represent it.4.3.2.10 averagedover 30 runs05000episodesstate aggregation(one tiling)tile coding (50 tilings)pve9.5. feature construction for linear methods

179

wishes to change more slowly than this, to allow for generalization and stochastic variation in target
outputs. for example, one might choose    = 1
10n , in which case the estimate for the trained state
would move one-tenth of the way to the target in one update, and neighboring states will be moved
less, proportional to the number of tiles they have in common.

tile coding also gains computational advantages from its use of binary feature vectors. because each
component is either 0 or 1, the weighted sum making up the approximate value function (9.8) is almost
trivial to compute. rather than performing d multiplications and additions, one simply computes the
indices of the n (cid:28) d active features and then adds up the n corresponding components of the weight
vector.

generalization occurs to states other than the one trained if those states fall within any of the same
tiles, proportional to the number of tiles in common. even the choice of how to o   set the tilings
from each other a   ects generalization. if they are o   set uniformly in each dimension, as they were in
figure 9.9, then di   erent states can generalize in qualitatively di   erent ways, as shown below in the
upper half of figure 9.11. each of the eight sub   gures show the pattern of generalization from a trained
state to nearby points. in this example their are eight tilings, thus 64 subregions within a tile that
generalize distinctly, but all according to one of these eight patterns. note how uniform o   sets result
in a strong e   ect along the diagonal in many patterns. these artifacts can be avoided if the tilings are
o   set asymmetrically, as shown in the lower half of the    gure. these lower generalization patterns are

figure 9.11: why tile asymmetrical o   sets are preferred in tile coding. shown is the strength of generalization
from a trained state, indicated by the small black plus, to nearby states, for the case of eight tilings. if the tilings
are uniformly o   set (above), then there are diagonal artifacts and substantial variations in the generalization,
whereas with asymmetrically o   set tilings the generalization is more spherical and homogeneous.

possible generalizations for uniformly o   set tilingspossible generalizationsfor asymmetrically o   set tilings180

chapter 9. on-policy prediction with approximation

better because they are all well centered on the trained state with no obvious asymmetries.

tilings in all cases are o   set from each other by a fraction of a tile width in each dimension. if w
denotes the tile width and n the number of tilings, then w
n is a fundamental unit. within small squares
w
n on a side, all states activate the same tiles, have the same feature representation, and the same
approximated value. if a state is moved by w
n in any cartesian direction, the feature representation
changes by one component/tile. uniformly o   set tilings are o   set from each other by exactly this unit
distance. for a two-dimensional space, we say that each tiling is o   set by the displacement vector
(1, 1), meaning that it is o   set from the previous tiling by w
n times this vector. in these terms, the
asymmetrically o   set tilings shown in the lower part of figure 9.11 are o   set by a displacement vector
of (1, 3).

extensive studies have been made of the e   ect of di   erent displacement vectors on the generalization
of tile coding (parks and militzer, 1991; an, 1991; an, miller and parks, 1991; miller, glanz and carter,
1991), assessing their homegeneity and tendency toward diagonal artifacts like those seen for the (1, 1)
displacement vectors. based on this work, miller and glanz (1996) recommend using displacement
vectors consisting of the    rst odd integers. in particular, for a continuous space of dimension k, a good
choice is to use the    rst odd integers (1, 3, 5, 7, . . . , 2k     1), with n (the number of tilings) set to an
integer power of 2 greater than or equal to 4k. this is what we have done to produce the tilings in
the lower half of figure 9.11, in which k = 2, n = 23     4k, and the displacement vector is (1, 3). in
a three-dimensional case, the    rst four tilings would be o   set in total from a base position by (0, 0, 0),
(1, 3, 5), (2, 6, 10), and (3, 9, 15). open-source software that can e   ciently make tilings like this for any
k is readily available.

in choosing a tiling strategy, one has to pick the number of the tilings and the shape of the tiles. the
number of tilings, along with the size of the tiles, determines the resolution or    neness of the asymptotic
approximation, as in general coarse coding and illustrated in figure 9.8. the shape of the tiles will
determine the nature of generalization as in figure 9.7. square tiles will generalize roughly equally
in each dimension as indicated in figure 9.11 (lower). tiles that are elongated along one dimension,
such as the stripe tilings in figure 9.12 (middle), will promote generalization along that dimension.
the tilings in figure 9.12 (middle) are also denser and thinner on the left, promoting discrimination
along the horizonal dimension at lower values along that dimension. the diagonal stripe tiling in
figure 9.12 (right) will promote generalization along one diagonal. in higher dimensions, axis-aligned
stripes correspond to ignoring some of the dimensions in some of the tilings, that is, to hyperplanar
slices. irregular tilings such as shown in figure 9.12 (left) are also possible, though rare in practice and
beyond the standard software.

in practice, it is often desirable to use di   erent shaped tiles in di   erent tilings. for example, one
might use some vertical stripe tilings and some horizontal stripe tilings. this would encourage gener-

figure 9.12: tilings need not be grids. they can be arbitrarily shaped and non-uniform, while still in many
cases being computationally e   cient to compute.

a) irregularb) log stripesc) diagonal stripes9.5. feature construction for linear methods

181

alization along either dimension. however, with stripe tilings alone it is not possible to learn that a
particular conjunction of horizontal and vertical coordinates has a distinctive value (whatever is learned
for it will bleed into states with the same horizontal and vertical coordinates). for this one needs the
conjunctive rectangular tiles such as originally shown in figure 9.9. with multiple tilings   some hori-
zontal, same vertical, and some conjunctive   one can get everything: a preference for generalizing along
each dimension, yet the ability to learn speci   c values for conjunctions (see sutton, 1996 for examples).
the choice of tilings determines generalization, and until this choice can be e   ectively automated, it
is important that tile coding enables the choice to be made    exibly and in a way that makes sense to
people.

another useful trick for reducing memory requirements is hashing   a con-
sistent pseudo-random collapsing of a large tiling into a much smaller set of
tiles. hashing produces tiles consisting of noncontiguous, disjoint regions ran-
domly spread throughout the state space, but that still form an exhaustive
partition. for example, one tile might consist of the four subtiles shown to
the right. through hashing, memory requirements are often reduced by large
factors with little loss of performance. this is possible because high resolution
is needed in only a small fraction of the state space. hashing frees us from the
curse of dimensionality in the sense that memory requirements need not be exponential in the number
of dimensions, but need merely match the real demands of the task. good open-source implementations
of tile coding, including hashing, are widely available.

exercise 9.4 suppose we believe that one of two state dimensions is more likely to have an e   ect
on the value function than is the other, that generalization should be primarily across this dimension
rather than along it. what kind of tilings could be used to take advantage of this prior knowledge? (cid:3)

9.5.5 radial basis functions

radial basis functions (rbfs) are the natural generalization of coarse coding to continuous-valued
features. rather than each feature being either 0 or 1, it can be anything in the interval [0, 1], re   ecting
various degrees to which the feature is present. a typical rbf feature, xi, has a gaussian (bell-shaped)
response xi(s) dependent only on the distance between the state, s, and the feature   s prototypical or
center state, ci, and relative to the feature   s width,   i:

xi(s)

.

= exp(cid:18)   ||s     ci||2

2  2
i

(cid:19) .

the norm or distance metric of course can be chosen in whatever way seems most appropriate to the
states and task at hand. figure 9.13 shows a one-dimensional example with a euclidean distance metric.

the primary advantage of rbfs over binary features is that they produce approximate functions
that vary smoothly and are di   erentiable. although this is appealing, in most cases it has no practical
signi   cance. nevertheless, extensive studies have been made of graded response functions such as rbfs
in the context of tile coding (an, 1991; miller et al., 1991; an, miller and parks, 1991; lane, handelman

figure 9.13: one-dimensional radial basis functions.

onetileci!ici+1ci-1182

chapter 9. on-policy prediction with approximation

and gelfand, 1992). all of these methods require substantial additional computational complexity
(over tile coding) and often reduce performance when there are more than two state dimensions. in
high dimensions the edges of tiles are much more important, and it has proven di   cult to obtain well
controlled graded tile activations near the edges.

an rbf network is a linear function approximator using rbfs for its features. learning is de   ned by
equations (9.7) and (9.8), exactly as in other linear function approximators. in addition, some learning
methods for rbf networks change the centers and widths of the features as well, bringing them into the
realm of nonlinear function approximators. nonlinear methods may be able to    t target functions much
more precisely. the downside to rbf networks, and to nonlinear rbf networks especially, is greater
computational complexity and, often, more manual tuning before learning is robust and e   cient.

9.6 nonlinear function approximation: arti   cial neural net-

works

arti   cial neural networks (anns) are widely used for nonlinear function approximation. an ann
is a network of interconnected units that have some of the properties of neurons, main component
of nervous systems. anns have a long history, with the latest advances in training deeply-layered
anns being responsible for some of the most impressive abilities of machine learning systems, including
id23 systems. in chapter 16 we describe several impressive examples of reinforcement
learning systems that use ann function approximation.

figure 9.14 shows a generic feedforward ann, meaning that there are no loops in the network,
that is, there are no paths within the network by which a unit   s output can in   uence its input. the
network in the    gure has an output layer consisting of two output units, an input layer with four input
units, and two hidden layers: layers that are neither input nor output layers. a real-valued weight is
associated with each link. a weight roughly corresponds to the e   cacy of a synaptic connection in
a real neural network (see section 15.1). if an ann has at least one loop in its connections, it is a
recurrent rather than a feedforward ann. although both feedforward and recurrent anns have been
used in id23, here we look only at the simpler feedforward case.

the units (the circles in figure 9.14) are typically semi-linear units, meaning that they compute
a weighted sum of their input signals and then apply to the result a nonlinear function, called the

figure 9.14: a generic feedforward neural network with four input units, two output units, and two hidden
layers.

9.6. nonlinear fa: id158s

183

activation function, to produce the unit   s output, or activation. di   erent id180 are used,
but they are typically s-shaped, or sigmoid, functions such as the logistic function f (x) = 1/(1 + e   x),
though sometimes the recti   er nonlinearity f (x) = max(0, x) is used. a step function like f (x) = 1 if
x       , and 0 otherwise, results in a binary unit with threshold   . it is often useful for units in di   erent
layers to use di   erent id180.

the activation of each output unit of a feedforward ann is a nonlinear function of the activation
patterns over the network   s input units. the functions are parameterized by the network   s connection
weights. an ann with no hidden layers can represent only a very small fraction of the possible input-
output functions. however an ann with a single hidden layer containing a large enough    nite number
of sigmoid units can approximate any continuous function on a compact region of the network   s input
space to any degree of accuracy (cybenko, 1989). this is also true for other nonlinear activation
functions that satisfy mild conditions, but nonlinearity is essential:
if all the units in a multi-layer
feedforward ann have linear id180, the entire network is equivalent to a network with
no hidden layers (because linear functions of linear functions are themselves linear).

despite this    universal approximation    property of one-hidden-layer anns, both experience and
theory show that approximating the complex functions needed for many arti   cial intelligence tasks is
made easier   indeed may require   abstractions that are hierarchical compositions of many layers of
lower-level abstractions, that is, abstractions produced by deep architectures such as anns with many
hidden layers. (see bengio, 2009, for a thorough review.) the successive layers of a deep ann compute
increasingly abstract representations of the network   s    raw    input, with each unit providing a feature
contributing to a hierarchical representation of the overall input-output function of the network.

training the hidden layers of an ann is therefore a way to automatically create features appropriate
for a given problem so that hierarchical representations can be produced without relying exclusively on
hand-crafted features. this has been an enduring challenge for arti   cial intelligence and explains why
learning algorithms for anns with hidden layers have received so much attention over the years. anns
typically learn by a stochastic gradient method (section 9.3). each weight is adjusted in a direction
aimed at improving the network   s overall performance as measured by an objective function to be either
minimized or maximized. in the most common supervised learning case, the objective function is the
expected error, or loss, over a set of labeled training examples. in id23, anns can
use td errors to learn value functions, or they can aim to maximize expected reward as in a gradient
bandit (section 2.8) or a policy-gradient algorithm (chapter 13). in all of these cases it is necessary to
estimate how a change in each connection weight would in   uence the network   s overall performance, in
other words, to estimate the partial derivative of an objective function with respect to each weight, given
the current values of all the network   s weights. the gradient is the vector of these partial derivatives.

the most successful way to do this for anns with hidden layers (provided the units have di   eren-
tiable id180) is the id26 algorithm, which consists of alternating forward and
backward passes through the network. each forward pass computes the activation of each unit given the
current activations of the network   s input units. after each forward pass, a backward pass e   ciently
computes a partial derivative for each weight. (as in other stochastic gradient learning algorithms,
the vector of these partial derivatives is an estimate of the true gradient.) in section 15.10 we discuss
methods for training anns with hidden layers that use id23 principles instead of
id26. these methods are less e   cient than the id26 algorithm, but they may
be closer to how real neural networks learn.

the id26 algorithm can produce good results for shallow networks having 1 or 2 hidden
layers, but it may not work well for deeper anns.
in fact, training a network with k + 1 hidden
layers can actually result in poorer performance than training a network with k hidden layers, even
though the deeper network can represent all the functions that the shallower network can (bengio,
2009). explaining results like these is not easy, but several factors are important. first, the large
number of weights in a typical deep ann makes it di   cult to avoid the problem of over   tting, that

184

chapter 9. on-policy prediction with approximation

is, the problem of failing to generalize correctly to cases on which the network has not been trained.
second, id26 does not work well for deep anns because the partial derivatives computed
by its backward passes either decay rapidly toward the input side of the network, making learning by
deep layers extremely slow, or the partial derivatives grow rapidly toward the input side of the network,
making learning unstable. methods for dealing with these problems are largely responsible for many
impressive recent results achieved by systems that use deep anns.

over   tting is a problem for any function approximation method that adjusts functions with many
degrees of freedom on the basis of limited training data. it is less of a problem for on-line reinforcement
learning that does not rely on limited training sets, but generalizing e   ectively is still an important
issue. over   tting is a problem for anns in general, but especially so for deep anns because they tend
to have very large numbers of weights. many methods have been developed for reducing over   tting.
these include stopping training when performance begins to decrease on validation data di   erent from
the training data (cross validation), modifying the objective function to discourage complexity of the
approximation (id173), and introducing dependencies among the weights to reduce the number
of degrees of freedom (e.g., weight sharing).

a particularly e   ective method for reducing over   tting by deep anns is the dropout method in-
troduced by srivastava, hinton, krizhevsky, sutskever, and salakhutdinov (2014). during training,
units are randomly removed from the network (dropped out) along with their connections. this can be
thought of as training a large number of    thinned    networks. combining the results of these thinned
networks at test time is a way to improve generalization performance. the dropout method e   ciently
approximates this combination by multiplying each outgoing weight of a unit by the id203 that
that unit was retained during training. srivastava et al. found that this method signi   cantly improves
generalization performance. it encourages individual hidden units to learn features that work well with
random collections of other features. this increases the versatility of the features formed by the hidden
units so that the network does not overly specialize to rarely-occurring cases.

hinton, osindero, and teh (2006) took a major step toward solving the problem of training the
deep layers of a deep ann in their work with id50, layered networks closely related
to the deep anns discussed here. in their method, the deepest layers are trained one at a time using
an unsupervised learning algorithm. without relying on the overall objective function, unsupervised
learning can extract features that capture statistical regularities of the input stream. the deepest layer
is trained    rst, then with input provided by this trained layer, the next deepest layer is trained, and so
on, until the weights in all, or many, of the network   s layers are set to values that now act as initial values
for supervised learning. the network is then    ne-tuned by id26 with respect to the overall
objective function. studies show that this approach generally works much better than id26
with weights initialized with random values. the better performance of networks trained with weights
initialized this way could be due to many factors, but one idea is that this method places the network
in a region of weight space from which a gradient-based algorithm can make good progress.

batch id172 (io   e and szegedy, 2015) is another technique that makes it easier to train
deep anns. it has long been known that ann learning is easier if the network input is normalized, for
example, by adjusting each input variable to have zero mean and unit variance. batch id172 for
training deep anns normalizes the output of deep layers before they feed into the following layer. io   e
and szegedy (2015) used statistics from subsets, or    mini-batches,    of training examples to normalize
these between-layer signals to improve the learning rate of deep anns.

another technique useful for training deep anns is deep residual learning (he, zhang, ren, and
sun, 2016). sometimes it is easier to learn how a function di   ers from the identity function than to
learn the function itself. then adding this di   erence, or residual function, to the input produces the
desired function. in deep anns, a block of layers can be made to learn a residual function simply by
adding shortcut, or skip, connections around the block. these connections add the input to the block
to its output, and no additional weights are needed. he et al. (2016) evaluated this method using deep

9.6. nonlinear fa: id158s

185

convolutional networks with skip connections around every pair of adjacent layers,    nding substantial
improvement over networks without the skip connections on benchmark image classi   cation tasks. both
batch id172 and deep residual learning were used in the id23 application to
the game of go that we describe in chapter 16.

a type of deep ann that has proven to be very successful in applications, including impressive rein-
forcement learning applications (chapter 16), is the deep convolutional network. this type of network
is specialized for processing high-dimensional data arranged in spatial arrays, such as images. it was
inspired by how early visual processing works in the brain (lecun, bottou, bengio and ha   ner, 1998).
because of its special architecture, a deep convolutional network can be trained by id26
without resorting to methods like those described above to train the deep layers.

figure 9.15 illustrates the architecture of a deep convolutional network. this instance, from lecun
et al. (1998), was designed to recognize hand-written characters. it consists of alternating convolutional
and subsampling layers, followed by several fully connected    nal layers. each convolutional layer pro-
duces a number of feature maps. a feature map is a pattern of activity over an array of units, where
each unit performs the same operation on data in its receptive    eld, which is the part of the data it
   sees    from the preceding layer (or from the external input in the case of the    rst convolutional layer).
the units of a feature map are identical to one another except that their receptive    elds, which are all
the same size and shape, are shifted to di   erent locations on the arrays of incoming data. units in the
same feature map share the same weights. this means that a feature map detects the same feature
no matter where it is located in the input array. in the network in figure 9.15, for example, the    rst
convolutional layer produces 6 feature maps, each consisting of 28    28 units. each unit in each feature
map has a 5    5 receptive    eld, and these receptive    elds overlap (in this case by four columns and
four rows). consequently, each of the 6 feature maps is speci   ed by just 25 adjustable weights.

the subsampling layers of a deep convolutional network reduce the spatial resolution of the feature
maps. each feature map in a subsampling layer consists of units that average over a receptive    eld
of units in the feature maps of the preceding convolutional layer. for example, each unit in each of
the 6 feature maps in the    rst subsampling layer of the network of figure 9.15 averages over a 2    2
non-overlapping receptive    eld over one of the feature maps produced by the    rst convolutional layer,
resulting in six 14    14 feature maps. subsampling layers reduce the network   s sensitivity to the spatial
locations of the features detected, that is, they help make the network   s responses spatially invariant.
this is useful because a feature detected at one place in an image is likely to be useful at other places
as well.

advances in the design and training of anns   of which we have only mentioned a few   all contribute
to id23. although current id23 theory is mostly limited to meth-

figure 9.15: deep convolutional network. republished with permission of proceedings of the ieee, from
gradient-based learning applied to document recognition, lecun, bottou, bengio, and ha   ner, volume 86,
1998; permission conveyed through copyright clearance center, inc.

186

chapter 9. on-policy prediction with approximation

ods using tabular or linear function approximation methods, the impressive performances of notable
id23 applications owe much of their success to nonlinear function approximation by
multi-layer anns. we discuss several of these applications in chapter 16.

9.7 least-squares td

all the methods we have discussed so far in this chapter have required computation per time step
proportional to the number of parameters. with more computation, however, one can do better. in
this section we present a method for linear function approximation that is arguably the best that can
be done for this case.

as we established in section 9.4 td(0) with linear function approximation converges asymptotically

(for appropriately decreasing step sizes) to the td    xed point:

wtd = a   1b,

where

a

.

= e(cid:2)xt(xt       xt+1)(cid:62)(cid:3)

and

.
= e[rt+1xt] .

b

why, one might ask, must we compute this solution iteratively? this is wasteful of data! could one
not do better by computing estimates of a and b, and then directly computing the td    xed point?
the least-squares td algorithm, commonly known as lstd, does exactly this. it forms the natural
estimates

xk(xk       xk+1)(cid:62) +   i

and

rt+1xk,

(9.19)

where i is the identity matrix, and   i, for some small    > 0, ensures that (cid:98)at is always invertible. it

might seem that these estimates should both be divided by t + 1, and indeed they should; as de   ned
here, these are really estimates of t + 1 times a and t + 1 times b. however, the t + 1 factor will not
matter, as when we use these estimates we will be e   ectively dividing one by the other. lstd estimates
the td    xed point as

.
=

t   1(cid:88)k=0

(cid:98)bt

(9.20)

this algorithm is the most data e   cient form of linear td(0), but it is also more expensive compu-
tationally. recall that semi-gradient td(0) requires memory and per-step computation that is only
o(d).

how complex is lstd? as it is written above the complexity seems to increase with t, but the two
approximations in (9.19) could be implemented incrementally using the techniques we have covered
earlier (e.g., in chapter 2) so that they can be done in constant time per step. even so, the update for

update; its computational complexity would be o(d2), and of course the memory required to hold the

computational complexity of a general inverse computation is o(d3). fortunately, an inverse of a
matrix of our special form   a sum of outer products   can also be updated incrementally with only
o(d2) computations, as

(cid:98)at would involve an outer product (a column vector times a row vector) and thus would be a matrix
(cid:98)at matrix would be o(d2).
a potentially greater problem is that our    nal computation (9.20) uses the inverse of (cid:98)at, and the
t =(cid:16)(cid:98)at   1 + xt(xt       xt+1)(cid:62)(cid:17)   1
(cid:98)a   1
t   1xt(xt       xt+1)(cid:62)(cid:98)a   1
= (cid:98)a   1
1 + (xt       xt+1)(cid:62)(cid:98)a   1

t   1     (cid:98)a   1

t   1
t   1xt

(from (9.19))

(9.21)

,

.
=

t   1(cid:88)k=0

(cid:98)at

wt

.

t (cid:98)bt.
= (cid:98)a   1

9.8. memory-based function approximation

187

.
=   i. although the identity (9.21), known as the sherman-morrison formula, is
super   cially complicated, it involves only vector-matrix and vector-vector multiplications and thus is
, maitain it with (9.21), and then use it in (9.20),
all with only o(d2) memory and per-step computation. the complete algorithm is given in the box
below.

for t > 0, with (cid:98)a0
only o(d2). thus we can store the inverse matrix (cid:98)a   1

t

lstd for estimating   v     v   (o(d2) version)
input: feature representation x(s)     rd, for all s     s, x(terminal)
(cid:100)a   1          1i
(cid:98)b     0

repeat (for each episode):

initialize s; obtain corresponding x
repeat (for each step of episode):

.
= 0

an d    d matrix
an d-dimensional vector

choose a       (  |s)
take action a, observe r, s(cid:48); obtain corresponding x(cid:48)

v     (cid:100)a   1(cid:62)(x       x(cid:48))
(cid:100)a   1     (cid:100)a   1    (cid:0)(cid:100)a   1x(cid:1)v(cid:62)/(cid:0)1 + v(cid:62)x(cid:1)
(cid:98)b    (cid:98)b + rx
       (cid:100)a   1(cid:98)b
s     s(cid:48); x     x(cid:48)
until s(cid:48) is terminal

of course, o(d2) is still signi   cantly more expensive than the o(d) of semi-gradient td. whether
the greater data e   ciency of lstd is worth this computational expense depends on how large d is,
how important it is to learn quickly, and the expense of other parts of the system. the fact that
lstd requires no step-size parameter is sometimes also touted, but the advantage of this is probably
overstated. lstd does not require a step size, but it does requires   ; if    is chosen too small the
sequence of inverses can vary wildly, and if    is chosen too large then learning is slowed. in addition,
lstd   s lack of a step size parameter means that it never forgets. this is sometimes desirable, but it
is problematic if the target policy    changes as it does in id23 and gpi. in control
applications, lstd typically has to be combined with some other mechanism to induce forgeting,
mooting any initial advantage of not requiring a step size parameter.

9.8 memory-based function approximation

so far we have discussed the parametric approach to approximating value functions. in this approach,
a learning algorithm adjusts the parameters of a functional form intended to approximate the value
function over a problem   s entire state space. each update, s (cid:55)    g, is a training example used by the
learning algorithm to change the parameters with the aim of reducing the approximation error. after
the update, the training example can be discarded (although it might be saved to be used again). when
an approximate value of a state (which we will call the query state) is needed, the function is simply
evaluated at that state using the latest parameters produced by the learning algorithm.

memory-based function approximation methods are very di   erent. they simply save training ex-
amples in memory as they arrive (or at least save a subset of the examples) without updating any
parameters. then, whenever a query state   s value estimate is needed, a set of examples is retrieved
from memory and used to compute a value estimate for the query state. this approach is sometimes

188

chapter 9. on-policy prediction with approximation

called lazy learning because processing training examples is postponed until the system is queried to
provide an output.

memory-based function approximation methods are prime examples of nonparametric methods. un-
like parametric methods, the approximating function   s form is not limited to a    xed parameterized
class of functions, such as linear functions or polynomials, but is instead determined by the training
examples themselves, together with some means for combining them to output estimated values for
query states. as more training examples accumulate in memory, one expects nonparametric methods
to produce increasingly accurate approximations of any target function.

there are many di   erent memory-based methods depending on how the stored training examples
are selected and how they are used to respond to a query. here, we focus on local-learning methods
that approximate a value function only locally in the neighborhood of the current query state. these
methods retrieve a set of training examples from memory whose states are judged to be the most
relevant to the query state, where relevance usually depends on the distance between states: the closer
a training example   s state is to the query state, the more relevant it is considered to be, where distance
can be de   ned in many di   erent ways. after the query state is given a value, the local approximation
is discarded.

the simplest example of the memory-based approach is the nearest neighbor method, which simply
   nds the example in memory whose state is closest to the query state and returns that example   s value
as the approximate value of the query state. in other words, if the query state is s, and s(cid:48) (cid:55)    g is the
example in memory in which s(cid:48) is the closest state to s, then g is returned as the approximate value
of s. slightly more complicated are weighted average methods that retrieve a set of nearest neighbor
examples and return a weighted average of their target values, where the weights generally decrease
with increasing distance between their states and the query state. locally weighted regression is similar,
but it    ts a surface to the values of a set of nearest states by means of a parametric approximation
method that minimizes a weighted error measure like (9.1), where the weights depend on distances from
the query state. the value returned is the evaluation of the locally-   tted surface at the query state,
after which the local approximation surface is discarded.

being nonparametric, memory-based methods have the advantage over parametric methods of not
limiting approximations to pre-speci   ed functional forms. this allows accuracy to improve as more data
accumulates. memory-based local approximation methods have other properties that make them well
suited for id23. because trajectory sampling is of such importance in reinforcement
learning, as discussed in section 8.6, memory-based local methods can focus function approximation
on local neighborhoods of states (or state   action pairs) visited in real or simulated trajectories. there
may be no need for global approximations because many areas of the state space will never (or almost
never) be reached. in addition, memory-based methods allow an agent   s experience to have a relatively
immediate a   ect on value estimates in the neighborhood if its environment   s current state, in contrast
with a parametric method   s need to incrementally adjust parameters of a global approximation.

avoiding global approximations is also a way to address the curse of dimensionality. for example,
for a state space with k dimensions, a tabular method storing a global approximation requires memory
exponential in k. on the other hand, in storing examples for a memory-based method, each example
requires memory proportional to k, and the memory required to store, say, n examples is linear in n.
nothing is exponential in k or n. of course, the critical remaining issue is whether a memory-based
method can answer queries quickly enough to be useful to an agent. a related concern is how speed
degrades as the size of the memory grows. finding nearest neighbors in a large database can take too
long to be practical in many applications.

proponents of memory-based methods have developed ways to accelerate the nearest neighbor search.
using parallel computers or special purpose hardware is one approach; another is the use of special multi-
dimensional data structures to store the training data. one data structure studied for this application
is the k-d tree (short for k-dimensional tree), which recursively splits a k-dimensional space into regions

9.9. kernel-based function approximation

189

arranged as nodes of a binary tree. depending on the amount of data and how it is distributed over
the state space, nearest-neighbor search using k-d trees can quickly eliminate large regions of the space
in the search for neighbors, making the searches feasible in some problems where naive searches would
take too long.

locally weighted regression additionally requires fast ways to do the local regression computations
which have to be repeated to answer each query. researchers have developed many ways to address
these problems, including methods for forgetting entries in order to keep the size of the database within
bounds. the bibliographic and historical comments section at the end of this chapter points to some of
the relevant literature, including a selection of papers describing applications of memory-based learning
to id23.

9.9 kernel-based function approximation

memory-based methods such as the weighted average and locally weighted regression methods described
above depend on assigning weights to examples s(cid:48)
(cid:55)    g in the database depending on the distance
between s(cid:48) and a query states s. the function that assigns these weights is called a id81,
or simply a kernel. in the weighted average and locally weighted regressions methods, for example, a
id81 k : r     r assigns weights to distances between states. more generally, weights do not
have to depend on distances; they can depend on some other measure of similarity between states. in
this case, k : s   s     r, so that k(s, s(cid:48)) is the weight given to data about s(cid:48) in its in   uence on answering
queries about s.
viewed slightly di   erently, k(s, s(cid:48)) is a measure of the strength of generalization from s(cid:48) to s. kernel
functions numerically express how relevant knowledge about any state is to any other state. as an
example, the strengths of generalization for tile coding shown in figure 9.11 correspond to di   erent
id81s resulting from uniform and asymmetrical tile o   sets. although tile coding does not
explicitly use a id81 in its operation, it generalizes according to one. in fact, as we discuss
more below, the strength of generalization resulting from linear parametric function approximation can
always be described by a id81.

kernel regression is the memory-based method that computes a kernel weighted average of the
targets of all examples stored in memory, assigning the result to the query state. if d is the set of
stored examples, and g(s(cid:48)) denotes the target for state s(cid:48) in a stored example, then kernel regression
approximates the target function, in this case a value function depending on d, as

  v(s,d) = (cid:88)s(cid:48)   d

k(s, s(cid:48))g(s(cid:48)).

(9.22)

the weighted average method described above is a special case in which k(s, s(cid:48)) is non-zero only when
s and s(cid:48) are close to one another so that the sum need not be computed over all of d.

a common kernel is the gaussian radial basis function (rbf) used in rbf function approximation as
described in section 9.5.5. in the method described there, rbfs are features whose centers and widths
are either    xed from the start, with centers presumably concentrated in areas where many examples are
expected to fall, or are adjusted in some way during learning. barring methods that adjust centers and
widths, this is a linear parametric method whose parameters are the weights of each rbf, which are
typically learned by stochastic gradient, or semi-gradient, descent. the form of the approximation is a
linear combination of the pre-determined rbfs. kernel regression with an rbf kernel di   ers from this
in two ways. first, it is memory-based: the rbfs are centered on the states of the stored examples.
second, it is nonparametric: there are no parameters to learn; the response to a query is given by (9.22).

of course, many issues have to be addressed for practical implementation of kernel regression, issues
that are beyond the scope or our brief discussion. however, it turns out that any linear parametric

190

chapter 9. on-policy prediction with approximation

regression method like those we described in section 9.4, with states represented by feature vectors
x(s) = (x1(s), x2(s), . . . , xd(s))(cid:62), can be recast as kernel regression where k(s, s(cid:48)) is the inner product
of the feature vector representations of s and s(cid:48); that is

k(s, s(cid:48)) = x(s)(cid:62)x(s(cid:48)).

(9.23)

kernel regression with this id81 produces the same approximation that a linear parametric
method would if it used these feature vectors and learned with the same training data.

we skip the mathematical justi   cation for this, which can be found in any modern machine learning
text, such as bishop (2006), and simply point out an important implication. instead of constructing
features for linear parametric function approximators, one can instead construct id81s di-
rectly without referring at all to feature vectors. not all id81s can be expressed as inner
products of feature vectors as in (9.23), but a id81 that can be expressed like this can o   er
signi   cant advantages over the equivalent parametric method. for many sets of feature vectors, (9.23)
has a compact functional form that can be evaluated without any computation taking place in the
d-dimensional feature space. in these cases, kernel regression is much less complex than directly using a
linear parametric method with states represented by these feature vectors. this is the so-called    kernel
trick    that allows e   ectively working in the high-dimension of an expansive feature space while actually
working only with the set of stored training examples. the kernel trick is the basis of many machine
learning methods, and researchers have shown how it can sometimes bene   t id23.

9.10 looking deeper at on-policy learning: interest and em-

phasis

the algorithms we have considered so far in this chapter have treated all the states encountered equally,
as if they were all equally important. in some cases, however, we are more interested in some states
than others. in discounted episodic problems, for example, we may be more interested in accurately
valuing early states in the episode than in later states where discounting may have made the rewards
much less important to the value of the start state. or, if an action-value function is being learned,
it may be less important to accurately value poor actions whose value is much less than the greedy
action. function approximation resources are always limited, and if they were used in a more targeted
way, then performance could be improved.

one reason we have treated all states encountered equally is that then we are updating according to
the on-policy distribution, for which stronger theoretical results are available for semi-gradient methods.
recall that the on-policy distribution was de   ned as the distribution of states encountered in an mdp
while following the target policy. now we will generalize this concept signi   cantly. rather than having
one on-policy distribution for the mdp, we will have many. all of them will have in common that they
are a distribution of states encountered in trajectories while following the target policy, but they will
vary in how the trajectories are, in a sense, initiated.

we now introduce some new concepts. first we introduce a non-negative scalar measure, a random
variable it called interest, indicating the degree to which we are interested in accurately valuing the
state (or state   action pair) at time t. if we don   t care at all about the state, then the interest should
be zero; if we fully care, it might be one, though it is formally allowed take any non-negative value.
the interest can be set in any causal way; for example, it may depend on the trajectory up to time t or
the learned parameters at time t. the distribution    in the ve (9.1) is then de   ned as the distribution
of states encountered while following the target policy, weighted by the interest. second, we introduce
another non-negative scalar random variable, the emphasis mt. this scalar multiplies the learning
update and thus emphasizes or de-emphasizes the learning done at time t. the general n-step learning

9.10. looking deeper: interest and emphasis

rule, replacing (9.15), is

wt+n

.
= wt+n   1 +   mt [gt:t+n       v(st,wt+n   1)]     v(st,wt+n   1),

0     t < t,

191

(9.24)

with the n-step return given by (9.16) and the emphasis determined recursively from the interest by:

mt = it +   id4   n,

0     t < t,

(9.25)

.
= 0, for all t < 0. these equations are taken to include the monte carlo case, for which

with mt
gt:t+n = gt, all the updates are made at end of the episode, n = t     t, and mt = it.

example 9.4 illustrates how interest and emphasis can result in more accurate value estimates.

example 9.4: interest and emphasis

to see the potential bene   ts of using interest and emphasis, consider the four-state markov
reward process shown below:

episodes start in the leftmost state, then transition one state to the right, with a reward of +1,
on each step until the terminal state is reached. the true value of the    rst state is thus 4, of the
second state 3, and so on as shown below each state. these are the true values; the estimated
values can only approximate these because they are constrained by the parameterization. there
are two components to the parameter vector w = (w1, w2)(cid:62), and the parameterization is as
written inside each state. the estimated values of the    rst two states are given by w1 alone
and thus must be the same even though their true values are di   erent. similarly, the estimated
values of the third and fourth states are given by w2 alone and must be the same even though
their true values are di   erent. suppose that we are interested in accurately valuing only the
leftmost state; we assign it an interest of 1 while all the other states are assigned an interest of
0, as indicated above the states.

first consider applying gradient monte carlo algorithms to this problem. the algorithms
presented earlier in this chapter that do not take into account interest and emphasis (in (9.7)
and the box on page 165) will converge (for decreasing step sizes) to the parameter vector
w    = (3.5, 1.5), which gives the    rst state   the only one we are interested in   a value of 3.5
(i.e., intermediate between the true values of the    rst and second states). the methods presented
in this section that do use interest and emphasis, on the other hand, will learn the value of the
   rst state exactly correctly; w1 will converge to 4 while w2 will never be updated because the
emphasis is zero in all states save the leftmost.

now consider applying two-step semi-gradient td methods. the methods from earlier in
this chapter without interest and emphasis (in (9.15) and (9.16) and the box on page 171) will
again converge to w    = (3.5, 1.5), while the methods with interest and emphasis converge to
w    = (4, 2). the latter produces the exactly correct values for the    rst state and for the third
state (which the    rst state bootstraps from) while never making any updates corresponding to
the second or fourth states.

+1+1+1+1   1   1   2   2v   =4v   =3v   =2v   =1i=1i=0i=0i=0192

chapter 9. on-policy prediction with approximation

9.11 summary

id23 systems must be capable of generalization if they are to be applicable to arti   cial
intelligence or to large engineering applications. to achieve this, any of a broad range of existing methods
for supervised-learning function approximation can be used simply by treating each update as a training
example.

perhaps the most suitable supervised learning methods are those using parameterized function ap-
proximation, in which the policy is parameterized by a weight vector w. although the weight vector has
many components, the state space is much larger still and we must settle for an approximate solution.
we de   ned ve(w) as a measure of the error in the values v  w (s) for a weight vector w under the
on-policy distribution,   . the ve gives us a clear way to rank di   erent value-function approximations
in the on-policy case.

to    nd a good weight vector, the most popular methods are variations of stochastic id119
(sgd). in this chapter we have focused on the on-policy case with a    xed policy, also known as policy
evaluation or prediction; a natural learning algorithm for this case is n-step semi-gradient td, which
includes gradient monte carlo and semi-gradient td(0) algorithms as the special cases when n =     and
n = 1 respectively. semi-gradient td methods are not true gradient methods. in such id64
methods (including dp), the weight vector appears in the update target, yet this is not taken into
account in computing the gradient   thus they are semi -gradient methods. as such, they cannot rely
on classical sgd results.

nevertheless, good results can be obtained for semi-gradient methods in the special case of linear
function approximation, in which the value estimates are sums of features times corresponding weights.
the linear case is the most well understood theoretically and works well in practice when provided
with appropriate features. choosing the features is one of the most important ways of adding prior
domain knowledge to id23 systems. they can be chosen as polynomials, but this case
generalizes poorly in the online learning setting typically considered in id23. better
is to choose features according the fourier basis, or according to some form of coarse coding with sparse
overlapping receptive    elds. tile coding is a form of coarse coding that is particularly computationally
e   cient and    exible. radial basis functions are useful for one- or two-dimensional tasks in which a
smoothly varying response is important. lstd is the most data-e   cient linear td prediction method,
but requires computation proportional to the square of the number of weights, whereas all the other
methods are of complexity linear in the number of weights. nonlinear methods include arti   cial neural
networks trained by id26 and variations of sgd; these methods have become very popular
in recent years under the name deep id23.

linear semi-gradient n-step td is guaranteed to converge under standard conditions, for all n, to
a ve that is within a bound of the optimal error. this bound is always tighter for higher n and
approaches zero as n        . however, in practice that choice results in very slow learning, and some
degree of id64 (1 < n <    ) is usually preferrable.

bibliographical and historical remarks

generalization and function approximation have always been an integral part of id23.
bertsekas and tsitsiklis (1996), bertsekas (2012), and sugiyama et al. (2013) present the state of
the art in function approximation in id23. some of the early work with function
approximation in id23 is discussed at the end of this section.

9.3

gradient-descent methods for minimizing mean-squared error in supervised learning are well
known. widrow and ho    (1960) introduced the least-mean-square (lms) algorithm, which is
the prototypical incremental gradient-descent algorithm. details of this and related algorithms

9.11. summary

193

are provided in many texts (e.g., widrow and stearns, 1985; bishop, 1995; duda and hart,
1973).

semi-gradient td(0) was    rst explored by sutton (1984, 1988), as part of the linear td(  )
algorithm that we will treat in chapter 12. the term    semi-gradient    to describe these boot-
strapping methods is new to the second edition of this book.

the earliest use of state aggregation in id23 may have been michie and
chambers   s boxes system (1968). the theory of state aggregation in id23
has been developed by singh, jaakkola, and jordan (1995) and tsitsiklis and van roy (1996).
state aggregation has been used in id145 from its earliest days (e.g., bellman,
1957a).

9.4

sutton (1988) proved convergence of linear td(0) in the mean to the minimal ve solution for
the case in which the feature vectors, {x(s) : s     s}, are linearly independent. convergence
with id203 1 was proved by several researchers at about the same time (peng, 1993; dayan
and sejnowski, 1994; tsitsiklis, 1994; gurvits, lin, and hanson, 1994). in addition, jaakkola,
jordan, and singh (1994) proved convergence under on-line updating. all of these results
assumed linearly independent feature vectors, which implies at least as many components to
wt as there are states. convergence for the more important case of general (dependent) feature
vectors was    rst shown by dayan (1992). a signi   cant generalization and strengthening of
dayan   s result was proved by tsitsiklis and van roy (1997). they proved the main result
presented in this section, the bound on the asymptotic error of linear id64 methods.

9.5

our presentation of the range of possibilities for linear function approximation is based on that
by barto (1990).

9.5.2 konidaris, osentoski, and thomas (2011) introduced the fourier basis in a simple form suit-
able for id23 problems with multi-dimensional continuous state spaces and
functions that do not have to be periodic.

9.5.3 the term coarse coding is due to hinton (1984), and our figure 9.6 is based on one of his
   gures. waltz and fu (1965) provide an early example of this type of function approximation
in a id23 system.

9.5.4 tile coding, including hashing, was introduced by albus (1971, 1981). he described it in terms
of his    cerebellar model articulator controller,    or cmac, as tile coding is sometimes known in
the literature. the term    tile coding    was new to the    rst edition of this book, though the idea
of describing cmac in these terms is taken from watkins (1989). tile coding has been used
in many id23 systems (e.g., shewchuk and dean, 1990; lin and kim, 1991;
miller, scalera, and kim, 1994; sofge and white, 1992; tham, 1994; sutton, 1996; watkins,
1989) as well as in other types of learning control systems (e.g., kraft and campagna, 1990;
kraft, miller, and dietz, 1992). this section draws heavily on the work of miller and glanz
(1996).

9.5.5 function approximation using radial basis functions has received wide attention ever since being
related to neural networks by broomhead and lowe (1988). powell (1987) reviewed earlier uses
of rbfs, and poggio and girosi (1989, 1990) extensively developed and applied this approach.

9.6

the introduction of the threshold logic unit as an abstract model neuron by mcculloch and
pitts (1943) was the beginning of arti   cial neural networks (anns). the history of anns as
learning methods for classi   cation or regression has passed through several stages: roughly, the

194

chapter 9. on-policy prediction with approximation

id88 (rosenblatt, 1962) and adaline (adaptive linear element) (widrow and ho   ,
1960) stage of learning by single-layer anns, the error-id26 stage (werbos, 1974;
lecun, 1985; parker, 1985; rumelhart, hinton, and williams, 1986) of learning by multi-layer
anns, and the current deep-learning stage with its emphasis on representation learning (e.g.,
bengio, courville, and vincent, 2012; goodfellow, bengio, and courville, 2016). examples of
the many books on anns are haykin (1994), bishop (1995), and ripley (2007).

anns as function approximation for id23 goes back to the early neural net-
works of farley and clark (1954), who used reinforcement-like learning to modify the weights of
linear threshold functions representing policies. widrow, gupta, and maitra (1973) presented
a neuron-like linear threshold unit implementing a learning process they called learning with
a critic or selective bootstrap adaptation, a reinforcement-learning variant of the adaline
algorithm. werbos (1974, 1987, 1994) developed an approach to prediction and control that
uses anns trained by error backpropation to learn policies and value functions using td-like
algorithms. barto, sutton, and brouwer (1981) and barto and sutton (1981b) extended the
idea of an associative memory network (e.g., kohonen, 1977; anderson, silverstein, ritz, and
jones, 1977) to id23. barto, anderson, and sutton (1982) used a two-layer
ann to learn a nonlinear control policy, and emphasized the    rst layer   s role of learning a
suitable representation. hampson (1983, 1989) was an early proponent of multilayer anns
for learning value functions. barto, sutton, and anderson (1983) presented an actor   critic
algorithm in the form of an ann learning to balance a simulated pole (see sections 15.7 and
15.8). barto and anandan (1985) introduced a stochastic version of widrow et al.   s (1973)
selective bootstrap algorithm called the associative reward-penalty (ar   p ) algorithm. barto
(1985, 1986) and barto and jordan (1987) described multi-layer anns consisting of ar   p
units trained with a globally-broadcast reinforcement signal to learn classi   cation rules that
are not linearly separable. barto (1985) discussed this approach to anns and how this type
of learning rule is related to others in the literature at that time. (see section 15.10 for addi-
tional discussion of this approach to training multi-layer anns.) anderson (1986, 1987, 1989)
evaluated numerous methods for training multilayer anns and showed that an actor   critic
algorithm in which both the actor and critic were implemented by two-layer anns trained
by error id26 outperformed single-layer anns in the pole-balancing and tower of
hanoi tasks. williams (1988) described several ways that id26 and reinforcement
learning can be combined for training anns. gullapalli (1990) and williams (1992) devised
id23 algorithms for neuron-like units having continuous, rather than binary,
outputs. barto, sutton, and watkins (1990) argued that anns can play signi   cant roles for
approximating functions required for solving sequential decision problems. williams (1992)
related reinforce learning rules (section 13.3) to the error id26 method for
training multi-layer anns. tesauro   s td-gammon (tesauro 1992, 1994; section 16.1) in   uen-
tially demonstrated the learning abilities of td(  ) algorithm with function approximation by
multi-layer anns in learning to play backgammon. the alphago and alphago zero programs
of silver et al. (2016, 2017; section 16.6) used id23 with deep convolutional
anns in achieving impressive results with the game of go. schmidhuber (2015) reviews appli-
cations of anns in id23, including applications of recurrent anns.

9.7

lstd is due to bradtke and barto (see bradtke, 1993, 1994; bradtke and barto, 1996; bradtke,
ydstie, and barto, 1994), and was further developed by boyan (1999, 2002) and nedi  c and
bertsekas (2003). the incremental update of the inverse matrix has been known at least since
1949 (sherman and morrison, 1949). an extension of least-squares methods to control was
introduced by lagoudakis and parr (2003).

9.8

our discussion of memory-based function approximation is largely based on the review of
locally weighted learning by atkeson, moore, and schaal (1997). atkeson (1992) discussed the

9.11. summary

195

use of locally weighted regression in memory-based robot learning and supplied an extensive
bibliography covering the history of the idea. stan   ll and waltz (1986) in   uentially argued for
the importance of memory based methods in arti   cial intelligence, especially in light of parallel
architectures then becoming available, such as the connection machine. baird and klopf (1993)
introduced a novel memory-based approach and used it as the function approximation method
for id24 applied to the pole-balancing task. schaal and atkeson (1994) applied locally
weighted regression to a robot juggling control problem, where it was used to learn a system
model. ping (1995) used the pole-balancing task to experiment with several nearest-neighbor
methods for approximating value functions, policies, and environment models. tadepalli and
ok (1996) obtained promising results with locally-weighted id75 to learn a value
function for a simulated automatic guided vehicle task. bottou and vapnik (1996) demonstrated
surprising e   ciency of several local learning algorithms compared to non-local algorithms in
some pattern recognition tasks, discussing the impact of local learning on generalization.

bentley (1975) introduced k-d trees and reported observing average running time of o(log n)
for nearest neighbor search over n records. friedman, bentley, and finkel (1977) clari   ed the
algorithm for nearest neighbor search with k-d trees. omohundro (1987) discussed e   ciency
gains possible with hierarchical data structures such as k-d-trees. moore, schneider, and deng
(1997) introduced the use of k-d trees for e   cient locally weighted regression.

9.9

the origin of kernel regression is the method of potential functions of aizerman, braverman,
and rozonoer (1964). they likened the data to point electric charges of various signs and
magnitudes distributed over space. the resulting electric potential over space produced by
summing the potentials of the point charges corresponded to the interpolated surface. in this
analogy, the id81 is the potential of a point charge, which falls o    as the reciprocal
of the distance from the charge. connell and utgo    (1987) applied an actor   critic method
to the pole-balancing task in which the critic approximated the value function using kernel
regression with an inverse-distance weighting. predating widespread interest in kernel regression
in machine learning, these authors did not use the term kernel, but referred to    shepard   s
method    (shepard, 1968). other kernel-based approaches to id23 include
those of ormoneit and sen (2002), dietterich and wang (2002), xu, xie, hu, nu, and lu
(2005), taylor and parr (2009), barreto, precup, and pineau (2011), and bhat, farias, and
moallemi (2012).

9.10

see the bibliographical notes for emphatic-td methods in section 11.8.

the earliest example we know of in which function approximation methods were used for learning
value functions was samuel   s checkers player (1959, 1967). samuel followed shannon   s (1950) suggestion
that a value function did not have to be exact to be a useful guide to selecting moves in a game and that it
might be approximated by linear combination of features. in addition to linear function approximation,
samuel experimented with lookup tables and hierarchical lookup tables called signature tables (gri   th,
1966, 1974; page, 1977; biermann, fair   eld, and beres, 1982).

at about the same time as samuel   s work, bellman and dreyfus (1959) proposed using function
approximation methods with dp. (it is tempting to think that bellman and samuel had some in   uence
on one another, but we know of no reference to the other in the work of either.) there is now a
fairly extensive literature on function approximation methods and dp, such as multigrid methods and
methods using splines and orthogonal polynomials (e.g., bellman and dreyfus, 1959; bellman, kalaba,
and kotkin, 1973; daniel, 1976; whitt, 1978; reetz, 1977; schweitzer and seidmann, 1985; chow and
tsitsiklis, 1991; kushner and dupuis, 1992; rust, 1996).

holland   s (1986) classi   er system used a selective feature-match technique to generalize evaluation
information across state   action pairs. each classi   er matched a subset of states having speci   ed values

196

chapter 9. on-policy prediction with approximation

for a subset of features, with the remaining features having arbitrary values (   wild cards   ). these
subsets were then used in a conventional state-aggregation approach to function approximation. hol-
land   s idea was to use a genetic algorithm to evolve a set of classi   ers that collectively would implement
a useful action-value function. holland   s ideas in   uenced the early research of the authors on rein-
forcement learning, but we focused on di   erent approaches to function approximation. as function
approximators, classi   ers are limited in several ways. first, they are state-aggregation methods, with
concomitant limitations in scaling and in representing smooth functions e   ciently.
in addition, the
matching rules of classi   ers can implement only aggregation boundaries that are parallel to the feature
axes. perhaps the most important limitation of conventional classi   er systems is that the classi   ers
are learned via the genetic algorithm, an evolutionary method. as we discussed in chapter 1, there
is available during learning much more detailed information about how to learn than can be used by
evolutionary methods. this perspective led us to instead adapt supervised learning methods for use
in id23, speci   cally gradient-descent and neural network methods. these di   erences
between holland   s approach and ours are not surprising because holland   s ideas were developed during
a period when neural networks were generally regarded as being too weak in computational power to
be useful, whereas our work was at the beginning of the period that saw widespread questioning of
that conventional wisdom. there remain many opportunities for combining aspects of these di   erent
approaches.

christensen and korf (1986) experimented with regression methods for modifying coe   cients of linear
value function approximations in the game of chess. chapman and kaelbling (1991) and tan (1991)
adapted decision-tree methods for learning value functions. explanation-based learning methods have
also been adapted for learning value functions, yielding compact representations (yee, saxena, utgo   ,
and barto, 1990; dietterich and flann, 1995).

chapter 10

on-policy control with
approximation

in this chapter we return to the control problem, now with parametric approximation of the action-
value function   q(s, a, w)     q   (s, a), where w     rd is a    nite-dimensional weight vector. we continue to
restrict attention to the on-policy case, leaving o   -policy methods to chapter 11. the present chapter
features the semi-gradient sarsa algorithm, the natural extension of semi-gradient td(0) (last chapter)
to action values and to on-policy control. in the episodic case, the extension is straightforward, but in
the continuing case we have to take a few steps backward and re-examine how we have used discounting
to de   ne an optimal policy. surprisingly, once we have genuine function approximation we have to give
up discounting and switch to a new    average-reward    formulation of the control problem, with new
   di   erential    value functions.

starting    rst in the episodic case, we extend the function approximation ideas presented in the last
chapter from state values to action values. then we extend them to control following the general pattern
of on-policy gpi, using   -greedy for action selection. we show results for n-step linear sarsa on the
mountain car problem. then we turn to the continuing case and repeat the development of these ideas
for the average-reward case with di   erential values.

10.1 episodic semi-gradient control

the extension of the semi-gradient prediction methods of chapter 9 to action values is straightforward.
in this case it is the approximate action-value function,   q     q  , that is represented as a parameterized
functional form with weight vector w. whereas before we considered random training examples of the
form st (cid:55)    ut, now we consider examples of the form st, at (cid:55)    ut. the update target ut can be any
approximation of q  (st, at), including the usual backed-up values such as the full monte carlo return,
gt, or any of the n-step sarsa returns (7.4). the general gradient-descent update for action-value
prediction is

for example, the update for the one-step sarsa method is

wt+1

wt+1

.

= wt +   (cid:104)ut       q(st, at, wt)(cid:105)     q(st, at, wt).
= wt +   (cid:104)rt+1 +      q(st+1, at+1, wt)       q(st, at, wt)(cid:105)     q(st, at, wt).

.

we call this method episodic semi-gradient one-step sarsa. for a constant policy, this method converges
in the same way that td(0) does, with the same kind of error bound (9.14).

(10.1)

(10.2)

197

198

chapter 10. on-policy control with approximation

to form control methods, we need to couple such action-value prediction methods with techniques
for policy improvement and action selection. suitable techniques applicable to continuous actions, or
to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution.
on the other hand, if the action set is discrete and not too large, then we can use the techniques
already developed in previous chapters. that is, for each possible action a available in the current state
st, we can compute   q(st, a, wt) and then    nd the greedy action a   t = argmaxa   q(st, a, wt). policy
improvement is then done (in the on-policy case treated in this chapter) by changing the estimation
policy to a soft approximation of the greedy policy such as the   -greedy policy. actions are selected
according to this same policy. pseudocode for the complete algorithm is given in the box.

episodic semi-gradient sarsa for estimating   q     q   
input: a di   erentiable function   q : s    a    rd     r
initialize value-function weights w     rd arbitrarily (e.g., w = 0)
repeat (for each episode):

s, a     initial state and action of episode (e.g.,   -greedy)
repeat (for each step of episode):

take action a, observe r, s(cid:48)
if s(cid:48) is terminal:

go to next episode

w     w +   (cid:2)r       q(s, a, w)(cid:3)     q(s, a, w)
choose a(cid:48) as a function of   q(s(cid:48),  , w) (e.g.,   -greedy)
w     w +   (cid:2)r +      q(s(cid:48), a(cid:48), w)       q(s, a, w)(cid:3)     q(s, a, w)
s     s(cid:48)
a     a(cid:48)

example 10.1: mountain car task consider the task of driving an underpowered car up a steep
mountain road, as suggested by the diagram in the upper left of figure 10.1. the di   culty is that
gravity is stronger than the car   s engine, and even at full throttle the car cannot accelerate up the steep
slope. the only solution is to    rst move away from the goal and up the opposite slope on the left.
then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even
though it is slowing down the whole way. this is a simple example of a continuous control task where
things have to get worse in a sense (farther from the goal) before they can get better. many control
methodologies have great di   culties with tasks of this kind unless explicitly aided by a human designer.
the reward in this problem is    1 on all time steps until the car moves past its goal position at the
top of the mountain, which ends the episode. there are three possible actions: full throttle forward
(+1), full throttle reverse (   1), and zero throttle (0). the car moves according to a simpli   ed physics.
its position, xt, and velocity,   xt, are updated by

xt+1

  xt+1

.

.

= bound(cid:2)xt +   xt+1(cid:3)
= bound(cid:2)   xt + 0.001at     0.0025 cos(3xt)(cid:3),

where the bound operation enforces    1.2     xt+1     0.5 and    0.07       xt+1     0.07. in addition, when
  xt+1 was reset to zero. when it reached the right bound, the goal was
xt+1 reached the left bound,
reached and the episode was terminated. each episode started from a random position xt     [   0.6,   0.4)
and zero velocity. to convert the two continuous state variables to binary features, we used grid-tilings
as in figure 9.9. we used 8 tilings, with each tile covering 1/8th of the bounded distance in each
dimension, and asymmetrical o   sets as described in section 9.5.4.1 the feature vectors x(s, a) created

1in particular, we used the tile-coding software, available on the web, version 3 (python), with iht=iht(4096) and

10.1. episodic semi-gradient control

199

figure 10.1: the mountain car task (upper left panel) and the cost-to-go function (    maxa   q(s, a, w)) learned
during one run.

by tile coding were then combined linearly with the parameter vector to approximate the action-value
function:

  q(s, a, w)

.

= w(cid:62)x(s, a) =(cid:88)i

wi    xi(s, a),

for each pair of state, s, and action, a.

(10.3)

figure 10.1 shows what typically happens while learning to solve this task with this form of function
approximation.2 shown is the negative of the value function (the cost-to-go function) learned on a
single run. the initial action values were all zero, which was optimistic (all true values are negative
in this task), causing extensive exploration to occur even though the exploration parameter,   , was 0.
this can be seen in the middle-top panel of the    gure, labeled    step 428   . at this time not even one
episode had been completed, but the car has oscillated back and forth in the valley, following circular
trajectories in state space. all the states visited frequently are valued worse than unexplored states,
because the actual rewards have been worse than what was (unrealistically) expected. this continually
drives the agent away from wherever it has been, to explore new states, until a solution is found.

   grefcar-learning-curves shows several learning curves for semi-gradient sarsa on this problem, with

various step sizes.

exercise 10.1 why have we not considered monte carlo methods in this chapter?

(cid:3)

tiles(iht, 8, [8*x/(0.5+1.2), 8*xdot/(0.07+0.07)], a) to get the indices of the ones in the feature vector for state
(x, xdot) and action a.

2this data is actually from the    semi-gradient sarsa(  )    algorithm that we will not meet until chapter 12, but

semi-gradient sarsa behaves similarly.

!1.2position0.6step 428goalposition40!.07.07velocityvelocityvelocityvelocityvelocityvelocitypositionpositionposition02701200104046episode 12episode 104episode 1000episode 9000mountain   cargoal200

chapter 10. on-policy control with approximation

figure 10.2: mountain car learning curves for the semi-gradient sarsa method with tile-coding function
approximation and   -greedy action selection.

10.2 n-step semi-gradient sarsa

we can obtain an n-step version of episodic semi-gradient sarsa by using an n-step return as the update
target in the semi-gradient sarsa update equation (10.1). the n-step return immediately generalizes
from its tabular form (7.4) to a function approximation form:

gt:t+n

with gt:t+n

wt+n

.
= rt+1 +   rt+2 +       +   n   1rt+n +   n   q(st+n, at+n, wt+n   1), n     1, 0     t < t     n, (10.4)
.
= gt if t + n     t , as usual. the n-step update equation is
.
= wt+n   1 +    [gt:t+n       q(st, at, wt+n   1)]     q(st, at, wt+n   1),

0     t < t.

(10.5)

episodic semi-gradient n-step sarsa for estimating   q     q   , or   q     q  
input: a di   erentiable function   q : s    a    rd     r, possibly   
initialize value-function weight vector w arbitrarily (e.g., w = 0)
parameters: step size    > 0, small    > 0, a positive integer n
all store and access operations (st, at, and rt) can take their index mod n

repeat (for each episode):

if t < t , then:

take action at
observe and store the next reward as rt+1 and the next state as st+1
if st+1 is terminal, then:

initialize and store s0 (cid:54)= terminal
select and store an action a0       (  |s0) or   -greedy wrt   q(s0,  , w)
t        for t = 0, 1, 2, . . . :
|
|
|
|
|
|
|
|
|
|
|
|
until    = t     1

if    + n < t , then g     g +   n   q(s   +n, a   +n, w)
w     w +    [g       q(s   , a   , w)]     q(s   , a   , w)

t     t + 1
select and store at+1       (  |st+1) or   -greedy wrt   q(st+1,  , w)

g    (cid:80)min(   +n,t )

(   is the time whose estimate is being updated)

       t     n + 1
if        0:

  i        1ri

else:

i=   +1

(g   :   +n)

10020040010000mountain carsteps per episodelog scaleaveraged over 100 runsepisode500   =0.5/8   =0.1/8   =0.2/810.2. n -step semi-gradient sarsa

201

as we have seen before, performance is best if an intermediate level of id64 is used, corre-
sponding to an n larger than 1. figure 10.3 shows how this algorithm tends to learn faster and obtain
a better asymptotic performance at n = 8 than at n = 1 on the mountain car task. figure 10.4 shows
the results of a more detailed study of the e   ect of the parameters    and n on the rate of learning on
this task.

figure 10.3: one-step vs multi-step performance of n-step semi-gradient sarsa on the mountain car task.
good step sizes were used:    = 0.5/8 for n = 1 and    = 0.3/8 for n = 8.

figure 10.4: e   ect of the    and n on early performance of n-step semi-gradient sarsa and tile-coding function
approximation on the mountain car task. as usual, an intermediate level of id64 (n = 4) performed
best. these results are for selected    values, on a log scale, and then connected by straight lines. the standard
errors ranged from 0.5 (less than the line width) for n = 1 to about 4 for n = 16, so the main e   ects are all
statistically signi   cant.

exercise 10.2 give pseudocode for semi-gradient one-step expected sarsa for control.

(cid:3)
exercise 10.3 why do the results shown in figure 10.4 have higher standard errors at large n than
(cid:3)
at low n?

10020040010000mountain carsteps per episodelog scaleaveraged over 100 runsepisode500n=1n=822024026030000.511.5mountain carsteps per episodeaveraged over   rst 50 episodesand 100 runs       number of tilings (8)280n=1n=2n=4n=8n=16n=8n=4n=2n=16n=1202

chapter 10. on-policy control with approximation

10.3 average reward: a new problem setting for continuing

tasks

we now introduce a third classical setting   alongside the episodic and discounted settings   for formu-
lating the goal in markov decision problems (mdps). like the discounted setting, the average reward
setting applies to continuing problems, problems for which the interaction between agent and environ-
ment goes on and on forever without termination or start states. unlike that setting, however, there is
no discounting   the agent cares just as much about delayed rewards as it does about immediate reward.
the average-reward setting is one of the major settings considered in the classical theory of dynamic
programming and, though less often, in id23. as we discuss in the next section, the
discounted setting is problematic with function approximation, and thus the average-reward setting is
needed to replace it.

in the average-reward setting, the quality of a policy    is de   ned as the average rate of reward while

following that policy, which we denote as r(  ):

r(  )

e[rt | a0:t   1       ]

.
= lim
h      
= lim
t      

=(cid:88)s

h(cid:88)t=1
1
h
e[rt | a0:t   1       ] ,
    (s)(cid:88)a
  (a|s)(cid:88)s(cid:48),r

p(s(cid:48), r|s, a)r,

(10.6)

where the expectations are conditioned on the prior actions, a0, a1, . . . , at   1, being taken according to
.
= limt       pr{st = s|a0:t   1       }, which is assumed
  , and      is the steady-state distribution,     (s)
to exist and to be independent of s0. this property is known as ergodicity. it means that where the
mdp starts or any early decision made by the agent can have only a temporary e   ect; in the long run
your expectation of being in a state depends only on the policy and the mdp transition probabilities.
ergodicity is su   cient to guarantee the existence of the limits in the equations above.

there are subtle distinctions that can be drawn between di   erent kinds of optimality in the undis-
counted continuing case. nevertheless, for most practical purposes it may be adequate simply to order
policies according to their average reward per time step, in other words, according to their r(  ). this
quantity is essentially the average reward under   , as suggested by (10.6). in particular, we consider
all policies that attain the maximal value of r(  ) to be optimal.

note that the steady state distribution is the special distribution under which, if you select actions

according to   , you remain in the same distribution. that is, for which

  (a|s)p(s(cid:48)|s, a) =     (s(cid:48)).

(10.7)

(cid:88)s

    (s)(cid:88)a

in the average-reward setting, returns are de   ned in terms of di   erences between rewards and the

average reward:

gt

.
= rt+1     r(  ) + rt+2     r(  ) + rt+3     r(  ) +        .

(10.8)

this is known as the di   erential return, and the corresponding value functions are known as di   erential
value functions. they are de   ned in the same way and we will use the same notation for them as we
.
= e  [gt|st = s, at = a] (similarly for v    and q   ).
have all along: v  (s)
di   erential value functions also have bellman equations, just slightly di   erent from those we have seen
earlier. we simply remove all   s and replace all rewards by the di   erence between the reward and the
true average reward:

.
= e  [gt|st = s] and q  (s, a)

v  (s) =(cid:88)a

  (a|s)(cid:88)r,s(cid:48)

p(s(cid:48), r|s, a)(cid:104)r     r(  ) + v  (s(cid:48))(cid:105),

10.3. average reward: a new setting for continuing tasks

203

v   (s) = max

p(s(cid:48), r|s, a)(cid:104)r     r(  ) +(cid:88)a(cid:48)
q  (s, a) =(cid:88)r,s(cid:48)
p(s(cid:48), r|s, a)(cid:104)r     max
a (cid:88)r,s(cid:48)
p(s(cid:48), r|s, a)(cid:104)r     max
q   (s, a) =(cid:88)r,s(cid:48)

  (a(cid:48)|s(cid:48))q  (s(cid:48), a(cid:48))(cid:105),
r(  ) + v   (s(cid:48))(cid:105), and
q   (s(cid:48), a(cid:48))(cid:105)

r(  ) + max

a(cid:48)

  

  

(cf. eqs. 3.14, 4.1, and 4.2).

there is also a di   erential form of the two td errors:
.
= rt+1      rt+1 +   v(st+1,wt)       v(st,wt), and

  t

  t

.
= rt+1      rt+1 +   q(st+1, at+1, wt)       q(st, at, wt),

(10.9)

(10.10)

where   rt is an estimate at time t of the average reward r(  ). with these alternate de   nitions, most of
our algorithms and many theoretical results carry through to the average-reward setting.

for example, the average reward version of semi-gradient sarsa is de   ned just as in (10.2) except

with the di   erential version of the td error. that is, by

wt+1

.
= wt +     t     q(st, at, wt),

(10.11)

with   t given by (10.10). the pseudocode for the complete algorithm is given in the box.

di   erential semi-gradient sarsa for estimating   q     q   
input: a di   erentiable function   q : s    a    rd     r
parameters: step sizes   ,    > 0
initialize value-function weights w     rd arbitrarily (e.g., w = 0)
initialize average reward estimate   r arbitrarily (e.g.,   r = 0)
initialize state s, and action a

repeat (for each step):

take action a, observe r, s(cid:48)
choose a(cid:48) as a function of   q(s(cid:48),  , w) (e.g.,   -greedy)
       r       r +   q(s(cid:48), a(cid:48), w)       q(s, a, w)
  r       r +     
w     w +          q(s, a, w)
s     s(cid:48)
a     a(cid:48)

204

chapter 10. on-policy control with approximation

example 10.2: an access-control queuing task this is a decision task involving access control
to a set of k servers. customers of four di   erent priorities arrive at a single queue. if given access to a
server, the customers pay a reward of 1, 2, 4, or 8 to the server, depending on their priority, with higher
priority customers paying more. in each time step, the customer at the head of the queue is either
accepted (assigned to one of the servers) or rejected (removed from the queue, with a reward of zero).
in either case, on the next time step the next customer in the queue is considered. the queue never
empties, and the priorities of the customers in the queue are equally randomly distributed. of course a
customer cannot be served if there is no free server; the customer is always rejected in this case. each
busy server becomes free with id203 p on each time step. although we have just described them
for de   niteness, let us assume the statistics of arrivals and departures are unknown. the task is to
decide on each step whether to accept or reject the next customer, on the basis of his priority and the
number of free servers, so as to maximize long-term reward without discounting.

in this example we consider a tabular solution to this problem. although there is no generalization
between states, we can still consider it in the general function approximation setting as this setting
generalizes the tabular setting. thus we have a di   erential action-value estimate for each pair of state
(number of free servers and priority of the customer at the head of the queue) and action (accept or
reject). figure 10.5 shows the solution found by di   erential semi-gradient sarsa for this task with
k = 10 and p = 0.06. the algorithm parameters were    = 0.01,    = 0.01, and   = 0.1. the initial
action values and   r were zero.

figure 10.5: the policy and value function found by di   erential semi-gradient one-step sarsa on the access-
control queuing task after 2 million steps. the drop on the right of the graph is probably due to insu   cient
data; many of these states were never experienced. the value learned for   r was about 2.31.

-10-50100di   erentialvalue of best actionnumber of free servers123456789100!15!10!5057priority 8priority 4priority 2priority 1number of free servers428acceptreject12345678910number of free serverspriority1policyvalue ofbest actionvaluefunction512345678910priority 8priority 4priority 2priority 1policyvaluefunction10.4. deprecating the discounted setting

205

10.4 deprecating the discounted setting

the continuing, discounted problem formulation has been very useful in the tabular case, in which the
returns from each state can be separately identi   ed and averaged. but in the approximate case it is
questionable whether one should ever use this problem formulation.

to see why, consider an in   nite sequence of returns with no beginning or end, and no clearly identi   ed
states. the states might be represented only by feature vectors, which may do little to distinguish the
states from each other. as a special case, all of the feature vectors may be the same. thus one really
has only the reward sequence (and the actions), and performance has to be assessed purely from these.
how could it be done? one way is by averaging the rewards over a long interval   this is the idea of
the average-reward setting. how could discounting be used? well, for each time step we could measure
the discounted return. some returns would be small and some big, so again we would have to average
them over a su   ciently large time interval. in the continuing setting there are no starts and ends, and
no special time steps, so there is nothing else that could be done. however, if you do this, it turns out
that the average of the discounted returns is proportional to the average reward. in fact, for policy
  , the average of the discounted returns is always r(  )/(1       ), that is, it is essentially the average
reward, r(  ). in particular, the ordering of all policies in the average discounted return setting would
be exactly the same as in the average-reward setting. the discount rate    thus has no e   ect on the
problem formulation. it could in fact be zero and the ranking would be unchanged.

this surprising fact is proven in the box, but the basic idea can be seen via a symmetry argument.
each time step is exactly the same as every other. with discounting, every reward will appear exactly
once in each position in some return. the tth reward will appear undiscounted in the t     1st return,
discounted once in the t    2nd return, and discounted 999 times in the t    1000th return. the weight on

the futility of discounting in continuing problems

perhaps discounting can be saved by choosing an objective that sums discounted values over the
distribution with which states occur under the policy:

(where v  

   is the discounted value function)

  (s)

    (s)v  

j(  ) =(cid:88)s
=(cid:88)s
    (s)(cid:88)a
= r(  ) +(cid:88)s
= r(  ) +   (cid:88)s(cid:48)
= r(  ) +   (cid:88)s(cid:48)

  (s(cid:48))]

  (a|s)(cid:88)s(cid:48) (cid:88)r
    (s)(cid:88)a
  (s(cid:48))(cid:88)s

  (a|s)(cid:88)s(cid:48) (cid:88)r
    (s)(cid:88)a

p(s(cid:48), r|s, a) [r +   v  
p(s(cid:48), r|s, a)  v  
  (a|s)p(s(cid:48)|s, a)

v  

  (s(cid:48))

  (s(cid:48))    (s(cid:48))
v  

= r(  ) +   j(  )
= r(  ) +   r(  ) +   2j(  )
= r(  ) +   r(  ) +   2r(  ) +   3r(  ) +       
=

r(  ).

1
1       

(bellman eq.)

(from (10.6))

(from (3.4))

(from (10.7))

the proposed discounted objective orders policies identically to the undiscounted (average re-
ward) objective. we have failed to save discounting!

206

chapter 10. on-policy control with approximation

the tth reward is thus 1+   +  2 +  3 +       = 1/(1     ). since all states are the same, they are all weighted
by this, and thus the average of the returns will be this times the average reward, or r(  )/(1       ).

so in this key case, which the discounted case was invented for, discounting is not applicable. the

discounted case is still pertinent, or at least possible, for the episodic case.

10.5 n-step di   erential semi-gradient sarsa

in order to generalize to n-step id64, we need an n-step version of the td error. we begin by
generalizing the n-step return (7.4) to its di   erential form, with function approximation:

gt:t+n

.
= rt+1       rt+1 + rt+2       rt+2 +        + rt+n       rt+n +   q(st+n, at+n, wt+n   1), (10.12)
.
= gt as

where   r is an estimate of r(  ), n     1, and t + n < t . if t + n     t , then we de   ne gt:t+n
usual. the n-step td error is then

  t

.
= gt:t+n       q(st, at, w),

(10.13)

after which we can apply our usual semi-gradient sarsa update (10.11). pseudocode for the complete
algorithm is given in the box.

di   erential semi-gradient n-step sarsa for estimating   q     q   , or   q     q  
input: a di   erentiable function   q : s    a    rm     r, a policy   
initialize value-function weights w     rm arbitrarily (e.g., w = 0)
initialize average-reward estimate   r     r arbitrarily (e.g.,   r = 0)
parameters: step size   ,    > 0, a positive integer n
all store and access operations (st, at, and rt) can take their index mod n

initialize and store s0 and a0
for t = 0, 1, 2, . . . :
take action at
observe and store the next reward as rt+1 and the next state as st+1
select and store an action at+1       (  |st+1), or   -greedy wrt   q(s0,  , w)
       t     n + 1
if        0:

(   is the time whose estimate is being updated)

i=   +1(ri       r) +   q(s   +n, a   +n, w)       q(s   , a   , w)

      (cid:80)   +n
  r       r +     
w     w +          q(s   , a   , w)

10.6 summary

in this chapter we have extended the ideas of parameterized function approximation and semi-gradient
descent, introduced in the previous chapter, to control. the extension is immediate for the episodic case,
but for the continuing case we have to introduce a whole new problem formulation based on maximizing
the average reward per time step. surprisingly, the discounted formulation cannot be carried over to
control in the presence of approximations. in the approximate case most policies cannot be represented
by a value function. the arbitrary policies that remain need to be ranked, and the scalar average reward
r(  ) provides an e   ective way to do this.

10.6. summary

207

the average reward formulation involves new di   erential versions of value functions, bellman equa-
tions, and td errors, but all of these parallel the old ones, and the conceptual changes are small. there
is also a new parallel set of di   erential algorithms for the average-reward case. we illustrate this by
developing di   erential versions of semi-gradient n-step sarsa.

bibliographical and historical remarks

10.1

10.2

10.3

semi-gradient sarsa with function approximation was    rst explored by rummery and niranjan
(1994). linear semi-gradient sarsa with   -greedy action selection does not converge in the
usual sense, but does enter a bounded region near the best solution (gordon, 1995). precup
and perkins (2003) showed convergence in a di   erentiable action selection setting. see also
perkins and pendrith (2002) and melo, meyn, and ribiero (2008). the mountain   car example
is based on a similar task studied by moore (1990), but the exact form used here is from sutton
(1996).

episodic n-step semi-gradient sarsa is based on the forward sarsa(  ) algorithm of van seijen
(2016). the empirical results shown here are new to the second edition of this text.

the average-reward formulation has been described for id145 (e.g., puterman,
1994) and from the point of view of id23 (mahadevan, 1996; tadepalli and
ok, 1994; bertsekas and tsitiklis, 1996; tsitsiklis and van roy, 1999). the algorithm described
here is the on-policy analog of the    r-learning    algorithm introduced by schwartz (1993). the
name r-learning was probably meant to be the alphabetic successor to id24, but we prefer
to think of it as a reference to the learning of di   erential or relative values. the access-control
queuing example was suggested by the work of carlstr  om and nordstr  om (1997).

10.4

the recognition of the limitations of discounting as a formulation of the reinforcement learn-
ing problem with function approximation became apparent to the authors shortly after the
publication of the    rst edition of this text. the second edition of this book may be the    rst
publication of the demonstration of the futility of discounting in the box on page 205.

10.5

the di   erential version of n-step semi-gradient sarsa is new to this text and has not been
signi   cantly studied.

208

chapter 10. on-policy control with approximation

chapter 11

*o   -policy methods with
approximation

this book has treated on-policy and o   -policy learning methods since chapter 5 primarily as two
alternative ways of handling the con   ict between exploitation and exploration inherent in learning forms
of generalized policy iteration. the two chapters preceding this have treated the on-policy case with
function approximation, and in this chapter we treat the o    -policy case with function approximation.
the extension to function approximation turns out to be signi   cantly di   erent and harder for o   -policy
learning than it is for on-policy learning. the tabular o   -policy methods developed in chapters 6
and 7 readily extend to semi-gradient algorithms, but these algorithms do not converge as robustly
as they do under on-policy training.
in this chapter we explore the convergence problems, take a
closer look at the theory of linear function approximation, introduce a notion of learnability, and then
discuss new algorithms with stronger convergence guarantees for the o   -policy case. in the end we will
have improved methods, but the theoretical results will not be as strong, nor the empirical results as
satisfying, as they are for on-policy learning. along the way, we will gain a deeper understanding of
approximation in id23 for on-policy learning as well as o   -policy learning.

recall that in o   -policy learning we seek to learn a value function for a target policy   , given data
due to a di   erent behavior policy b. in the prediction case, both policies are static and given, and we
seek to learn either state values   v     v   or action values   q     q  . in the control case, action values are
learned, and both policies typically change during learning      being the greedy policy with respect to
  q, and b being something more exploratory such as the   -greedy policy with respect to   q.

the challenge of o   -policy learning can be divided into two parts, one that arises in the tabular
case and one that arises only with function approximation. the    rst part of the challenge has to do
with the target of the update (not to be confused with the target policy), and the second part has to
do with the distribution of the updates. the techniques related to importance sampling developed in
chapters 5 and 7 deal with the    rst part; these may increase variance but are needed in all successful
algorithms, tabular and approximate. the extension of these techniques to function approximation are
quickly dealt with in the    rst section of this chapter.

something more is needed for the second part of the challenge of o   -policy learning with function
approximation because the distribution of updates in the o   -policy case is not according to the on-
policy distribution. the on-policy distribution is important to the stability of semi-gradient methods.
two general approaches have been explored to deal with this. one is to use importance sampling
methods again, this time to warp the update distribution back to the on-policy distribution, so that
semi-gradient methods are guaranteed to converge (in the linear case). the other is to develop true
gradient methods that do not rely on any special distribution for stability. we present methods based

209

210

chapter 11. *off-policy methods with approximation

on both approaches. this is a cutting-edge research area, and it is not clear which of these approaches
is most e   ective in practice.

11.1 semi-gradient methods

we begin by describing how the methods developed in earlier chapters for the o   -policy case extend
readily to function approximation as semi-gradient methods. these methods address the    rst part of
the challenge of o   -policy learning (changing the update targets) but not the second part (changing
the update distribution). accordingly, these methods may diverge in some cases, and in that sense are
not sound, but still they are often successfully used. remember that these methods are guaranteed
stable and asymptotically unbiased for the tabular case, which corresponds to a special case of function
approximation. so it may still be possible to combine them with feature selection methods in such a
way that the combined system could be assured stable. in any event, these methods are simple and
thus a good place to start.

in chapter 7 we described a variety of tabular o   -policy algorithms. to convert them to semi-gradient
form, we simply replace the update to an array (v or q) to an update to a weight vector (w), using
the approximate value function (  v or   q) and its gradient. many of these algorithms use the per-step
importance sampling ratio:

.
=   t:t =

  t

  (at|st)
b(at|st)

.

(11.1)

for example, the one-step, state-value algorithm is semi-gradient o   -policy td(0), which is just like
the corresponding on-policy algorithm (page 166) except for the addition of   t:

wt+1

.
= wt +     t  t     v(st,wt),

(11.2)

where   t is de   ned appropriately depending on whether the problem is episodic and discounted, or
continuing and undiscounted using average reward:

  t

.
= rt+1 +     v(st+1,wt)       v(st,wt), or

  t

.
= rt+1       rt +   v(st+1,wt)       v(st,wt).

for action values, the one-step algorithm is semi-gradient expected sarsa:

wt+1

.
= wt +     t     q(st, at, wt), with

  t

  t

.

= rt+1 +   (cid:88)a
= rt+1       rt +(cid:88)a

.

  (a|st+1)  q(st+1, a, wt)       q(st, at, wt), or

  (a|st+1)  q(st+1, a, wt)       q(st, at, wt).

(11.3)

(11.4)

(11.5)

(episodic)

(continuing)

note that this algorithm does not use importance sampling. in the tabular case it is clear that this is
appropriate because the only sample action is at, and in learning its value we do not have to consider
any other actions. with function approximation it is less clear because we might want to weight dif-
ferent state   action pairs di   erently once they all contribute to the same overall approximation. proper
resolution of this issue awaits a more thorough understanding of the theory of function approximation
in id23.

11.2. examples of off-policy divergence

211

in the multi-step generalizations of these algorithms, both the state-value and action-value algorithms

involve importance sampling. for example, the n-step version of semi-gradient expected sarsa is

wt+n

.
= wt+n   1 +     t+1          t+n   1 [gt:t+n       q(st, at, wt+n   1)]     q(st, at, wt+n   1)

(11.6)

with

gt:t+n

gt:t+n

.
= rt+1 +        +   n   1rt+n +   n   q(st+n, at+n, wt+n   1), or
.
= rt+1       rt +        + rt+n       rt+n   1 +   q(st+n, at+n, wt+n   1),

(episodic)

(continuing)

where here we are being slightly informal in our treatment of the ends of episodes. in the    rst equation,
the   ks for k     t (where t is the last time step of the episode) should be taken to be 1, and gt:n
should be taken to be gt if t + n     t .

recall that we also presented in chapter 7 an o   -policy algorithm that does not involve importance

sampling at all: the n-step tree-backup algorithm. here is its semi-gradient version:

wt+n

.
= wt+n   1 +    [gt:t+n       q(st, at, wt+n   1)]     q(st, at, wt+n   1), with

gt:t+n

.
=   q(st, at, wt   1) +

t+n   1(cid:88)k=t

  k

k(cid:89)i=t+1

    (ai|si),

(11.7)

(11.8)

with   t as de   ned at the top of this page for expected sarsa. we also de   ned in chapter 7 an algorithm
that uni   es all action-value algorithms: n-step q(  ). we leave the semi-gradient form of that algorithm,
and also of the n-step state-value algorithm, as exercises for the reader.

exercise 11.1 convert the equation of n-step o   -policy td (7.7) to semi-gradient form. give accom-
(cid:3)
panying de   nitions of the return for both the episodic and continuing cases.
   exercise 11.2 convert the equations of n-step q(  ) (7.9, 7.14, 7.16, and 7.17) to semi-gradient form.
(cid:3)
give de   nitions that cover both the episodic and continuing cases.

11.2 examples of o   -policy divergence

in this section we begin to discuss the second part of the challenge of o   -policy learning with function
approximation   that the distribution of updates does not match the on-policy distribution. we describe
some instructive counterexamples to o   -policy learning   cases where semi-gradient and other simple
algorithms are unstable and diverge.

to establish intuitions, it is best to consider    rst a very simple example. suppose, perhaps as part
of a larger mdp, there are two states whose estimated values are of the functional form w and 2w,
where the parameter vector w consists of only a single component w. this occurs under linear function
approximation if the feature vectors for the two states are each simple numbers (single-component vec-
tors), in this case 1 and 2. in the    rst state, only one action is available, and it results deterministically
in a transition to the second state with a reward of 0:

where the expressions inside the two circles indicate the two state   s values.

suppose initially w = 10. the transition will then be from a state of estimated value 10 to a state
it will look like a good transition, and w will be increased to raise the    rst

of estimated value 20.

2w02w212

chapter 11. *off-policy methods with approximation

state   s estimated value. if    is nearly 1, then the td error will be nearly 10, and, if    = 0.1, then w
will be increased to nearly 11 in trying to reduce the td error. however, the second state   s estimated
value will also be increased, to nearly 22. if the transition occurs again, then it will be from a state
of estimated value    11 to a state of estimated value    22, for a td error of    11   larger, not smaller,
than before. it will look even more like the    rst state is undervalued, and its value will be increased
again, this time to    12.1. this looks bad, and in fact with further updates w will diverge to in   nity.
to see this de   nitively we have to look more carefully at the sequence of updates. the td error on

a transition between the two states is

  t = rt+1 +     v(st+1,wt)       v(st,wt) = 0 +   2wt     wt = (2       1)wt,

and the o   -policy semi-gradient td(0) update (from (11.2)) is

wt+1 = wt +     t  t     v(st,wt) = wt +       1    (2       1)wt    1 =(cid:0)1 +   (2       1)(cid:1)wt.

note that the importance sampling ratio,   t, is 1 on this transition because there is only one action
available from the    rst state, so its probabilities of being taken under the target and behavior policies
must both be 1.
in the    nal update above, the new parameter is the old parameter times a scalar
constant, 1 +   (2       1). if this constant is greater than 1, then the system is unstable and w will go to
positive or negative in   nity depending on its initial value. here this constant is greater than 1 whenever
   > 0.5. note that stability does not depend on the speci   c step size, as long as    > 0. smaller or
larger step sizes would a   ect the rate at which w goes to in   nity, but not whether it goes there or not.

key to this example is that the one transition occurs repeatedly without w being updated on other
transitions. this is possible under o   -policy training because the behavior policy might select actions
on those other transitions which the target policy never would. for these transitions,   t would be zero
and no update would be made. under on-policy training, however,   t is always one. each time there
is a transition from the w state to the 2w state, increasing w, there would also have to be a transition
out of the 2w state. that transition would reduce w, unless it were to a state whose value was higher
(because    < 1) than 2w, and then that state would have to be followed by a state of even higher value,
or else again w would be reduced. each state can support the one before only by creating a higher
expectation. eventually the piper must be paid. in the on-policy case the promise of future reward
must be kept and the system is kept in check. but in the o   -policy case, a promise can be made and
then, after taking an action that the target policy never would, forgotten and forgiven.

this simple example communicates much of the reason why o   -policy training can lead to divergence,
but it is not completely convincing because it is not complete   it is just a fragment of a complete mdp.
can there really be a complete system with instability? a simple complete example of divergence is
baird   s counterexample. consider the episodic seven-state, two-action mdp shown in figure 11.1. the
dashed action takes the system to one of the six upper states with equal id203, whereas the solid
action takes the system to the seventh state. the behavior policy b selects the dashed and solid actions
with probabilities 6
7 , so that the next-state distribution under it is uniform (the same for all
nonterminal states), which is also the starting distribution for each episode. the target policy    always
takes the solid action, and so the on-policy distribution (for   ) is concentrated in the seventh state.
the reward is zero on all transitions. the discount rate is    = 0.99.

7 and 1

consider estimating the state-value under the linear parameterization indicated by the expression
shown in each state circle. for example, the estimated value of the leftmost state is 2w1 + w8, where
the subscript corresponds to the component of the overall weight vector w     r8; this corresponds to a
feature vector for the    rst state being x(1) = (2, 0, 0, 0, 0, 0, 0, 1)(cid:62). the reward is zero on all transitions,
so the true value function is v  (s) = 0, for all s, which can be exactly approximated if w = 0. in
fact, there are many solutions, as there are more components to the weight vector (8) than there are
nonterminal states (7). moreover, the set of feature vectors, {x(s) : s     s}, is a linearly independent
set. in all these ways this task seems a favorable case for linear function approximation.

11.2. examples of off-policy divergence

213

figure 11.1: baird   s counterexample. the approximate state-value function for this markov process is of the
form shown by the linear expressions inside each state. the solid action usually results in the seventh state,
and the dashed action usually results in one of the other six states, each with equal id203. the reward is
always zero.

if we apply semi-gradient td(0) to this problem (11.2), then the weights diverge to in   nity, as shown
in figure 11.2 (left). the instability occurs for any positive step size, no matter how small. in fact,
it even occurs if a expected update is done as in id145 (dp). if we do a dp-style
expected update instead of a sample (learning) update, as shown in figure 11.2 (right). that is, if the
weight vector, wk, is updated for all states all at the same time in a semi-gradient way, using the dp
(expectation-based) target:

wk+1

.
= wk +

  

|s|(cid:88)s (cid:16)e[rt+1 +     v(st+1,wk) | st = s]       v(s,wk)(cid:17)     v(s,wk).

(11.9)

in this case, there is no randomness and no asynchrony, just as in a classical dp update. the method is
conventional except in its use of semi-gradient function approximation. yet still the system is unstable.

figure 11.2: demonstration of instability on baird   s counterexample. shown are the evolution of the compo-
nents of the parameter vector w of the two semi-gradient algorithms. the step size was    = 0.01, and the initial
weights were w = (1, 1, 1, 1, 1, 1, 10, 1)(cid:62).

2w2+w82w1+w82w3+w82w4+w82w5+w82w6+w8w7+2w8  (dashed|  )=6/7  (solid|  )=1/7   (solid|  )=1 =0.99w8w8stepsw7w1   w6sweepssemi-gradient off-policy tdsemi-gradient dpw1   w6w7214

chapter 11. *off-policy methods with approximation

if we alter just the distribution of dp updates in baird   s counterexample, from the uniform distribu-
tion to the on-policy distribution (which generally requires asynchronous updating), then convergence
is guaranteed to a solution with error bounded by (9.14). this example is striking because the td
and dp methods used are arguably the simplest and best-understood id64 methods, and the
linear, semi-descent method used is arguably the simplest and best-understood kind of function ap-
proximation. the example shows that even the simplest combination of id64 and function
approximation can be unstable if the updates are not done according to the on-policy distribution.

there are also counterexamples similar to baird   s showing divergence for id24. this is cause
for concern because otherwise id24 has the best convergence guarantees of all control methods.
considerable e   ort has gone into trying to    nd a remedy to this problem or to obtain some weaker, but
still workable, guarantee. for example, it may be possible to guarantee convergence of id24 as
long as the behavior policy is su   ciently close to the target policy, for example, when it is the   -greedy
policy. to the best of our knowledge, id24 has never been found to diverge in this case, but there
has been no theoretical analysis. in the rest of this section we present several other ideas that have
been explored.

suppose that instead of taking just a step toward the expected one-step return on each iteration, as
in baird   s counterexample, we actually change the value function all the way to the best, least-squares
approximation. would this solve the instability problem? of course it would if the feature vectors,
{x(s) : s     s}, formed a linearly independent set, as they do in baird   s counterexample, because then
exact approximation is possible on each iteration and the method reduces to standard tabular dp. but
of course the point here is to consider the case when an exact solution is not possible. in this case
stability is not guaranteed even when forming the best approximation at each iteration, as shown by
the counterexample in the box.

tsitsiklis and van roy   s counterexample to dp policy evaluation with least-squares linear func-
tion approximation

the simplest full counterexample to the least-squares idea is
the w-to-2w example (from earlier in this section) extended
with a terminal state, as shown to the right. as before, the
estimated value of the    rst state is w, and the estimated
value of the second state is 2w. the reward is zero on all
transitions, so the true values are zero at both states, which
is exactly representable with w = 0. if we set wk+1 at each
step so as to minimize the ve between the estimated value
and the expected one-step return, then we have

wk+1 = arg min

w   r (cid:88)s   s(cid:16)  v(s,w)     e  (cid:2)rt+1 +     v(st+1,wk)(cid:12)(cid:12) st = s(cid:3)(cid:17)2
w   r (cid:0)w       2wk(cid:1)2

+(cid:0)2w     (1       )  2wk(cid:1)2

(11.10)

= arg min
6     4 

=

5

  wk.

the sequence {wk} diverges when    > 5

6   4   and w0 (cid:54)= 0.

another way to try to prevent instability is to use special methods for function approximation. in
particular, stability is guaranteed for function approximation methods that do not extrapolate from
the observed targets. these methods, called averagers, include nearest neighbor methods and locally
weighted regression, but not popular methods such as tile coding and arti   cial neural networks.

1       w2w11.3. the deadly triad

215

11.3 the deadly triad

our discussion so far can be summarized by saying that the danger of instability and divergence arises
whenever we combine all of the following three elements, making up what we call the deadly triad :

function approximation a powerful, scalable way of generalizing from a state space much larger
than the memory and computational resources (e.g., linear function approximation or arti   cial
neural networks).

id64 update targets that include existing estimates (as in id145 or td
methods) rather than relying exclusively on actual rewards and complete returns (as in mc
methods).

o   -policy training training on a distribution of transitions other than that produced by the tar-
get policy. sweeping through the state space and updating all states uniformly, as in dynamic
programming, does not respect the target policy and is an example of o   -policy training.

in particular, note that the danger is not due to control, or to generalized policy iteration. those cases
are more complex to analyze, but the instability arises in the simpler prediction case whenever it includes
all three elements of the deadly triad. the danger is also not due to learning or to uncertainties about
the environment, because it occurs just as strongly in planning methods, such as id145,
in which the environment is completely known.

if any two elements of the deadly triad are present, but not all three, then instability can be avoided.

it is natural, then, to go through the three and see if there is any one that can be given up.

of the three, function approximation most clearly cannot be given up. we need methods that scale to
large problems and to great expressive power. we need at least linear function approximation with many
features and parameters. state aggregation or nonparametric methods whose complexity grows with
data are too weak or too expensive. least-squares methods such as lstd are of quadratic complexity
and are therefore too expensive for large problems.

doing without id64 is possible, at the cost of computational and data e   ciency. perhaps
most important are the losses in computational e   ciency. monte carlo (non-id64) methods
require memory to save everything that happens between making each prediction and obtaining the
   nal return, and all their computation is done once the    nal return is obtained. the cost of these
computational issues is not apparent on serial von neumann computers, but would be on specialized
hardware. with id64 and eligibility traces (chapter 12), data can be dealt with when and
where it is generated, then need never be used again. the savings in communication and memory made
possible by id64 are great.

the losses in data e   ciency by giving up id64 are also signi   cant. we have seen this re-
peatedly, such as in chapters 7 (figure 7.2) and 9 (figure 9.2), where some degree of id64
performed much better than monte carlo methods on the random-walk prediction task, and in chap-
ter 10 where the same was seen on the mountain-car control task (figure 10.4). many other problems
show much faster learning with id64 (e.g., see figure 12.14). id64 often results in
faster learning because it allows learning to take advantage of the state property, the ability to recog-
nize a state upon returning to it. on the other hand, id64 can impair learning on problems
where the state representation is poor and causes poor generalization (e.g., this seems to be the case
on tetris, see s  im  sek, alg  orta, and kothiyal, 2016). a poor state representation can also result in
bias; this is the reason for the poorer bound on the asymptotic approximation quality of id64
methods (equation 9.14). on balance, the ability to bootstrap has to be considered extremely valuable.
one may sometimes choose not to use it by selecting long multistep updates (or a large id64
parameter,        1; see chapter 12) but often id64 greatly increases e   ciency. it is an ability
that we would very much like to keep in our toolkit.

216

chapter 11. *off-policy methods with approximation

finally, there is o   -policy learning; can we give that up? on-policy methods are often adequate.
for model-free id23, one can simply use sarsa rather than id24. o   -policy
methods free behavior from the target policy. this could be considered an appealing convenience but
not a necessity. however, o   -policy learning is essential to other anticipated use cases, cases that we
have not yet mentioned in this book but may be important to the larger goal of creating a powerful
intelligent agent.

in these use cases, the agent learns not just a single value function and single policy, but large
numbers of them in parallel. there is extensive psychological evidence that people and animals learn
to predict many di   erent sensory events, not just rewards. we can be surprised by unusual events, and
correct our predictions about them, even if they are of neutral valence (neither good nor bad). this
kind of prediction presumably underlies predictive models of the world such as are used in planning.
we predict what we will see after eye movements, how long it will take to walk home, the id203 of
making a jump shot in basketball, and the satisfaction we will get from taking on a new project. in all
these cases, the events we would like to predict depend on our acting in a certain way. to learn them
all, in parallel, requires learning from the one stream of experience. there are many target policies, and
thus the one behavior policy cannot equal all of them. yet parallel learning is conceptually possible
because the behavior policy may overlap in part with many of the target policies. to take full advantage
of this requires o   -policy learning.

11.4 linear value-function geometry

to better understand the stability challenge of o   -policy learning, it is helpful to think about value
function approximation more abstractly and independently of how learning is done. we can imagine
the space of all possible state-value functions   all functions from states to real numbers v : s     r.
most of these value functions do not correspond to any policy. more important for our purposes is that
most are not representable by the function approximator, which by design has far fewer parameters
than there are states.

given an enumeration of the state space s = {s1, s2, . . . , s|s|}, any value function v corresponds to a
vector listing the value of each state in order [v(s1), v(s2), . . . , v(s|s|)](cid:62). this vector representation of a
value function has as many components as there are states. in most cases where we want to use function
approximation, this would be far too many components to represent the vector explicitly. nevertheless,
the idea of this vector is conceptually useful. in the following, we treat a value function and its vector
representation interchangably.

to develop intuitions, consider the case with three states s = {s1, s2, s3} and two parameters w =
(w1, w2)(cid:62). we can then view all value functions/vectors as points in a three-dimensional space. the
parameters provide an alternative coordinate system over a two-dimensional subspace. any weight
vector w = (w1, w2)(cid:62) is a point in the two-dimensional subspace and thus also a complete value function
vw that assigns values to all three states. with general function approximation the relationship between
the full space and the subspace of representable functions could be complex, but in the case of linear
value-function approximation the subspace is a simple plane, as suggested by figure 11.3.

now consider a single    xed policy   . we assume that its true value function, v  , is too complex to
be represented exactly as an approximation. thus v   is not in the subspace; in the    gure it is depicted
as being above the planar subspace of representable functions.

if v   cannot be represented exactly, what representable value function is closest to it? this turns out
to be a subtle question with multiple answers. to begin, we need a measure of the distance between two
value functions. given two value functions v1 and v2, we can talk about the vector di   erence between
them, v = v1     v2. if v is small, then the two value functions are close to each other. but how are
we to measure the size of this di   erence vector? the conventional euclidean norm is not appropriate

11.4. linear value-function geometry

217

figure 11.3: the geometry of linear value-function approximation. shown is the three-dimensional space of all
value functions over three states, while shown as a plane is the subspace of all value functions representable by
a linear function approximator with parameter w = (w1, w2)(cid:62). the true value function v   is in the larger space
and can be projected down (into the subspace, using a projection operator   ) to its best approximation in the
value error (ve) sense. the best approximators in the bellman error (be), projected bellman error (pbe),
and temporal di   erence error (tde) senses are all potentially di   erent and are shown in the lower right. (ve,
be, and pbe are all treated as the corresponding vectors in this    gure.) the bellman operator takes a value
function in the plane to one outside, which can then be projected back. if you iteratively applied the bellman
operator outside the space (shown in gray above) you would reach the true value function, as in conventional
id145. if instead you kept projecting back into the subspace at each step, as in the lower step
shown in gray, then the    xed point would be the point of vector-zero pbe.

because, as discussed in section 9.2, some states are more important than others because they occur
more frequently or because we are more interested in them (section 9.10). as in section 9.2, let us use
the weighting    : s     r to specify the degree to which we care about di   erent states being accurately
valued (often taken to be the on-policy distribution). we can then de   ne the distance between value
functions using the norm

(cid:107)v(cid:107)2

  

.

=(cid:88)s   s

  (s)v(s)2.

(11.11)

note that the ve from section 9.2 can be written simply using this norm as ve(w) = (cid:107)vw     v  (cid:107)2
  . for
any value function v, the operation of    nding its closest value function in the subspace of representable
value functions is a projection operation. we de   ne a projection operator    that takes an arbitrary
value function to the representable function that is closest in our norm:

  v

.
= vw where w = arg min

w (cid:107)v     vw(cid:107)2
   .

(11.12)

the representable value function that is closest to the true value function v   is thus its projection,   v  ,
as suggested in figure 11.3. this is the solution asymptotically found by monte carlo methods, albeit
often very slowly. the projection operation is discussed more fully in the box on the next page.

accordingtoastationarydecisionmakingpolicy   :s   a![0,1]where   (s,a)istheid203thatat=agiventhatst=s,forallt.tosolvethemdpisto   ndanoptimalpolicy      ,de   nedasapolicythatmaximizestheexpected -discountedrewardreceivedfromeachstate:      =argmax   v   (s),8s2s,wherev   (s)=e      rt+1+ rt+2+ 2rt+3+        st=s   ,8s2s,(1)where 2[0,1)isknownasthediscount-rateparameter,andthesubscriptontheeindicatesthattheexpectationisconditionalonthepolicy   beingusedtoselectactions.thefunctionv   iscalledthestate-valuefunctionforpolicy   .akeysubproblemunderlyingalmostalle cientsolutionstrategiesformdpsispolicyevaluation,thecomputationorestimationofv   foragivenpolicy   .forexample,thepopulardpalgorithmknownaspolicyiterationinvolvescomputingthevaluefunctionforasequenceofpolicies,eachofwhichisbetterthantheprevious,untilanoptimalpolicyisfound.intdl,algorithmssuchastd( )areusedtoapproximatethevaluefunctionforthecurrentpolicy,forexampleaspartofactor   criticmethods.ifthestatespaceis   nite,thentheestimatedvaluefunctionmayberepresentedinacomputerasalargearraywithoneentryforeachstateandtheentriesdirectlyupdatedtoformtheestimate.suchtabularmethodscanhandlelargestatespaces,evencontinuousones,throughdiscretization,stateaggregation,andinterpolation,butasthedimensionalityofthestatespaceincreases,thesemethodsrapidlybecomecomputationallyinfeasibleorine   ective.thisisthee   ectwhichgaverisetothephrase   thecurseofdimensionality.   amoregeneraland   exibleapproachistorepresentthevaluefunctionbyafunctionalformof   xedsizeand   xedstructurewithmanyvariableparametersorweights.theweightsarethenchangedtoreshapetheapproximatevaluefunctiontobettermatchthetruevaluefunction.wedenotetheparameterizedvaluefunctionapproximatorasv   (s)   v   (s),8s2s,(2)where   2rn,withn   |s|,istheweight/parametervector.theapproximatevaluefunctioncanhavearbitraryformaslongasitiseverywheredi   erentiablewithrespecttotheweights.forexample,itcouldbeacubicspline,oritcouldimplementedbyamulti-layerneuralnetworkwhere   istheconcatenationofalltheconnectionweights.henceforthreferto   exclusivelyastheweights,orweightvector,andreservetheword   parameter   forthingslikethediscount-rateparameter, ,andstep-sizeparameters.animportantspecialcaseisthatinwhichtheapproximatevaluefunctionislinearintheweightsandinfeaturesofthestate:v   (s)=   > (s),(3)wherethe (s)2rn,8s2s,arefeaturevectorscharacterizingeachstates,andx>ydenotestheinnerproductoftwovectorsxandy.2the subspace of all value functions representable as bellman error (be)theothertwogoalsforapproximationarerelatedtothebellmanequation,whichcanbewrittencompactlyinvectorformasv   =b   v   ,(7)whereb   :r|s|!r|s|isthebellmanoperatorforpolicy   ,de   nedby(b   v)(s)=xa2a   (s,a)"r(s,a)+ xs02sp(s0|s,a)v(s0)#,8s2s,8v:s!r.(8)(ifthestateandactionspacesarecontinuous,thenthesumsarereplacedbyintegralsandthefunctionp(  |s,a)istakentobeaid203density.)thetruevaluefunctionv   istheuniquesolutiontothebellmanequation;thebellmanequationcanbeviewedasanalternatewayofde   ningv   .foranyvaluefunctionv:s!rnotequaltov   ,therewillalwaysbeatleastonestatesatwhichv(s)6=(b   v)(s).thediscrepancybetweenthetwosidesofthebellmanequation,v    b   v   ,isanerrorvector,andreducingitisthebasisforoursecondandthirdgoalsforapproximation.thesecondgoalistominimizetheerrorvector   slengthinthed-metric.thatis,tominimizethemean-squaredbellmanerror:be(   )=xs2sd(s)   (b   v   )(s) v   (s)   2.(9)notethatifv   isnotrepresentable,thenitisnotbepossibletoreducethebellmanerrortozero.foranyv   ,thecorrespondingb   v   willgenerallynotberepresentable;itwilllieoutsidethespaceofrepresentablefunctions,assuggestedbythe   gure...finally,inourthirdgoalofapproximation,we   rstprojectthebellmanerrorandthenminimizeitslength.thatis,weminimizetheerrornotinthebellmanequation(7)butinitsprojectedform:v   =   b   v   ,(10)unliketheoriginalbellmanequation,formostfunctionapproximators(e.g.,linearones)theprojectedbellmanequationcanbesolvedexactly.ifitcan   tbesolvedexactly,youcanminimizethemean-squaredprojectedbellmanerror:pbe(   )=xs2sd(s)   (   (b   v    v   ))(s)   2.(11)theminimumisachievedattheprojection   xpoint,atwhichxs2sd(s)   (b   v   )(s) v   (s)   r   v   (s)=~0.(12)pvepbeppbe   v   =v      vev      pbev      be42.2bellmanerrorthesecondgoalforapproximationistoapproximatelysolvethebellmanequation:v   =b   v   ,(8)whereb   :r|s|!r|s|isthebellmanoperatorforpolicy   ,de   nedby(b   v)(s)=xa2a   (s,a)"r(s,a)+ xs02sp(s0|s,a)v(s0)#,8s2s,8v:s!r.(9)(ifthestateandactionspacesarecontinuous,thenthesumsarereplacedbyintegralsandthefunctionp(  |s,a)istakentobeaid203density.)thetruevaluefunctionv   istheuniquesolutiontothebellmanequation,andinthissensethebellmanequationcanbeviewedasanalternatewayofde   ningv   .foranyvaluefunctionv   notequaltov   ,wecanaskthebellmanequationtoholdapproximately,v      b   v   .thatis,wecanminimizethebellmanerror:be(   )=||v    b   v   ||,(10)thoughwecannotexpecttodriveittozeroifv   isoutsidetherepresentablesubspace.figure1showsthegeometricrelationships;notethatthebellmanoperatorisshownastakingvaluefunctionsinsidethesubspaceoutsidetosomethingthatisnotrepresentable,andthatthepointofminimumbeisingeneraldi   erentfromthatofminimumve.thebewas   rstproposedasanobjectivefunctionfordpbyschweitzerandseidmann(1985).baird(1995,1999)extendedittotdlbasedonstochasticgradientdescent,andengel,mannor,andmeir(2003)extendedittoleastsquares(o(n2))methodsknownasgaussianprocesstdl.intheliterature,beminimizationisoftenreferredtoasbellmanresidualminimization.2.3projectedbellmanerrorthethirdgoalforapproximationistoapproximatelysolvetheprojectedbellmanequation:v   =   b   v   .(11)unliketheoriginalbellmanequation,formostfunctionapproximators(e.g.,linearones)theprojectedbellmanequationcanbesolvedexactly.theoriginaltdlmethods(sutton1988,dayan1992)convergetothissolution,asdoesleast-squarestdl(bradke&barto1996,boyan1999).thegoalofachieving(11)exactlyiscommon;lesscommonistoconsiderapproximatingitasanobjective.theearlyworkongradient-td(e.g.,suttonetal.2009)appearstobe   rsttohaveexplicitlyproposedminimizingthed-weightednormoftheerrorin(11),whichweherecalltheprojectedbellmanerror:pbe(   )=||v       b   v   ||.(12)thisobjectiveisbestunderstoodbylookingattheleftsideoffigure1.startingatv   ,thebellmanoperatortakesusoutsidethesubspace,andtheprojectionoperatortakesusbackintoit.thedistancebetweenwhereweendupandwherewestartedisthepbe.thedistanceisminimal(zero)whenthetripupandbackleavesusinthesameplace.8value error (ve)w1w2vwvwb   vw   b   vw(minve=kvek2  )minbe=kbek2  wtdtde=0pbe=~0the 3d space of all value functions over 3 states218

chapter 11. *off-policy methods with approximation

the projection matrix

for a linear function approximator, the projection operation is linear, which implies that it can
be represented as an |s|    |s| matrix:

x(cid:62)d,

(11.13)

  

.

= x(cid:0)x(cid:62)dx(cid:1)   1

where, as in section 9.4, d denotes the |s|    |s| diagonal matrix with the   (s) on the diagonal,
and x denotes the |s|    d matrix whose rows are the feature vectors x(s)(cid:62), one for each state
s. if the inverse in does not exist, then the pseudoinverse is substituted. using these matrices,
the norm of a vector can be written

(cid:107)v(cid:107)2

   = v(cid:62)dv,

and the approximate linear value function can be written

vw = xw.

(11.14)

(11.15)

td methods    nd di   erent solutions. to understand their rationale, recall that the bellman equation

for value function v   is

v  (s) =(cid:88)a

  (a|s)(cid:88)s(cid:48),r

p(s(cid:48), r|s, a) [r +   v  (s(cid:48))] ,

for all s     s.

(11.16)

v   is the only value function that solves this equation exactly. if an approximate value function vw
were substituted for v  , the di   erence between the right and left sides of the modi   ed equation could
be used as a measure of how far o    vw is from v  . we call this the bellman error at state s:

    w(s)

.

  (a|s)(cid:88)s(cid:48),r

=      (cid:88)a
p(s(cid:48), r|s, a) [r +   vw(s(cid:48))]           vw(s)
= e(cid:2)rt+1 +   vw(st+1)     vw(st)(cid:12)(cid:12) st = s, at       (cid:3) ,

which shows clearly the relationship of the bellman error to the td error (11.3). the bellman error is
the expectation of the td error.

the vector of all the bellman errors, at all states,     w     r|s|, is called the bellman error vector
(shown as be in figure 11.3). the overall size of this vector, in the norm, is an overall measure of the
error in the value function, called the mean squared bellman error :

(11.17)

(11.18)

(11.19)

it is not possible in general to reduce the be to zero (at which point vw = v  ), but for linear func-
tion approximation there is a unique value of w for which the be is minimized. this point in the
representable-function subspace (labeled min be in figure 11.3) is di   erent in general from that which
minimizes the ve (shown as   v  ). methods that seek to minimize the be are discussed in the next
two sections.

the bellman error vector is shown in figure 11.3 as the result of applying the bellman operator

b   : r|s|     r|s| to the approximate value function. the bellman operator is de   ned by

p(s(cid:48), r|s, a) [r +   v(s(cid:48))] ,

(11.20)

(b  v)(s)

.

=(cid:88)a

  (a|s)(cid:88)s(cid:48),r

for all s     s and v : s     r. the bellman error vector for v can be written     w = b  vw     vw.

be(w) =(cid:13)(cid:13)    w(cid:13)(cid:13)2

   .

11.5. stochastic id119 in the bellman error

219

if the bellman operator is applied to a value function in the representable subspace, then, in general,
it will produce a new value function that is outside the subspace, as suggested in the    gure. in dynamic
programming (without function approximation), this operator is applied repeatedly to the points outside
the representable space, as suggested by the gray arrows in the top of figure 11.3. eventually that
process converges to the true value function v  , the only    xed point for the bellman operator, the only
value function for which

v   = b  v  ,

(11.21)

which is just another way of writing the bellman equation for    (11.16).

with function approximation, however, the intermediate value functions lying outside the subspace
cannot be represented. the gray arrows in the upper part of figure 11.3 cannot be followed because
after the    rst update (dark line) the value function must be projected back into something representable.
the next iteration then begins within the subspace; the value function is again taken outside of the
subspace by the bellman operator and then mapped back by the projection operator, as suggested by
the lower gray arrow and line. following these arrows is a dp-like process with approximation.

in this case we are interested in the projection of the bellman error vector back into the representable
space. this is the projected bellman error vector       vw , shown in figure 11.3 as pbe. the size of this
vector, in the norm, is another measure of error in the approximate value function. for any approximate
value function v, we de   ne the mean square projected bellman error, denoted pbe, as

(11.22)

with linear function approximation there always exists an approximate value function (within the
subspace) with zero pbe; this is the td    xed point, wtd, introduced in section 9.4. as we have seen,
this point is not always stable under semi-gradient td methods and o   -policy training. as shown in
the    gure, this value function is generally di   erent from those minimizing ve or be. methods that are
guaranteed to converge to it are discussed in sections 11.7 and 11.8.

pbe(w) =(cid:13)(cid:13)      w(cid:13)(cid:13)2

   .

11.5 stochastic id119 in the bellman error

armed with a better understanding of value function approximation and its various objectives, we
return now to the challenge of stability in o   -policy learning. we would like to apply the approach of
stochastic id119 (sgd, section 9.3), in which updates are made that in expectation are equal
to the negative gradient of an objective function. these methods always go downhill (in expectation)
in the objective and because of this are typically stable with excellent convergence properties. among
the algorithms investigated so far in this book, only the monte carlo methods are true sgd methods.
these methods converge robustly under both on-policy and o   -policy training as well as for general
non-linear (di   erentiable) function approximators, though they are often slower than semi-gradient
methods with id64, which are not sgd methods. semi-gradient methods may diverge under
o   -policy training, as we have seen earlier in this chapter, and under contrived cases of non-linear
function approximation (tsitsiklis and van roy, 1997). with a true sgd method such divergence
would not be possible.

the appeal of sgd is so strong that great e   ort has gone into    nding a practical way of harnessing
it for id23. the starting place of all such e   orts is the choice of an error or objective
function to optimize.
in this and the next section we explore the origins and limits of the most
popular proposed objective function, that based on the bellman error introduced in the previous section.
although this has been a popular and in   uential approach, the conclusion that we reach here is that
it is a misstep and yields no good learning algorithms. on the other hand, this approach fails in an
interesting way that o   ers insight into what might constitute a good approach.

220

chapter 11. *off-policy methods with approximation

to begin, let us consider not the bellman error, but something more immediate and naive. temporal
di   erence learning is driven by the td error. why not take the minimization of the expected square of
the td error as the objective? in the general function-approximation case, the one-step td error with
discounting is

  t = rt+1 +     v(st+1,wt)       v(st,wt).

a possible objective function then is what one might call the mean squared td error :

tde(w) =(cid:88)s   s
  (s)e(cid:2)  2
=(cid:88)s   s
  (s)e(cid:2)  t  2
t(cid:3) .
= eb(cid:2)  t  2

t (cid:12)(cid:12) st = s, at      (cid:3)
t (cid:12)(cid:12) st = s, at    b(cid:3)

the last equation is of the form needed for sgd; it gives the objective as an expectation that can be
sampled from experience (remember the experience is due to the behavior policy b. thus, following the
standard sgd approach, one can derive the per-step update based on a sample of this expected value:

(if    is the distribution encountered under b)

1
2

wt+1 = wt    

     (  t  2
t )
= wt         t  t     t

= wt +     t  t(cid:0)     v(st,wt)            v(st+1,wt)(cid:1),

(11.23)

which you will recognize as the same as the semi-gradient td algorithm (11.2) except for the additional
   nal term. this term completes the gradient and makes this a true sgd algorithm with excellent
convergence guarantees. let us call this algorithm the naive residual-gradient algorithm (after baird,
1993).

although the naive residual-gradient algorithm converges robustly, it does not always converge to
a desirable place, as the a-split example in the box shows. in this example a tabular representation
is used, so the true state values can be exactly represented, yet the naive residual-gradient algorithm
   nds di   erent values, and these values have lower tde than do the true values. minimizing the tde
is naive; by penalizing all td errors it achieves something more like temporal smoothing than accurate
prediction.

a-split example, showing the naivet  e of the naive residual gradient algorithm

consider the following three-state episodic mrp:

episodes begin in state a and then    split    stochastically, half the time going to b and then
invariably going on to terminate with a reward of 1, and half the time going to state c and then
invariably terminating with a reward of zero. reward for the    rst transition, out of a, is always
zero whichever way the episode goes. as this is an episodic problem, we can take    to be 1. we
also assume on-policy training, so that   t is always 1, and tabular function approximation, so
that the learning algorithms are free to give arbitrary, independent values to all three states. so
it should be an easy problem.

abc000111.5. stochastic id119 in the bellman error

221

what should the values be? from a, half the time the return is 1, and half the time the
return is 0; a should have value 1
2 . from b the return is always 1, so its value should be 1, and
similarly from c the return is always 0, so its value should be 0. these are the true values and,
as this is a tabular problem, all the methods presented previously converge to them exactly.

however, the naive residual-gradient algorithm    nds di   erent values for b and c. it converges
2 ). these are in

4 (a converges correctly to 1

4 and c having a value of 1

with b having a value of 3
fact the values that minimize the tde.

2 to b   s 3

4 , a change of 1

4 , or down from a   s 1

let us compute the tde for these values. the    rst transition of each episode is either up
from a   s 1
4 . because
the reward is zero on these transitions, and    = 1, these changes are the td errors, and thus
the squared td error is always 1
16 on the    rst transition. the second transition is similar; it is
either up from b   s 3
4 to
a reward of 0 (again with a terminal state of value 0). thus, the td error is always    1
4 , for a
squared error of 1
16 on the second step. thus, for this set of values, the tde on both steps is
1
16 .

4 to a reward of 1 (and a terminal state of value 0), or down from c   s 1

4 , a change of     1

2 to c   s 1

2 up to 1, at b, or from 1

2 and the squared error is 1

now let   s compute the tde for the true values (b at 1, c at 0, and a at 1

2 ). in this case
the    rst transition is either from 1
2 down to 0, at c; in either case the
absolute error is 1
4 . the second transition has zero error because
the starting value, either 1 or 0 depending on whether the transition is from b or c, always
exactly matches the immediate reward and return. thus the squared td error is 1
4 on the    rst
transition and 0 on the second, for a mean reward over the two transitions of 1
8 is bigger
that 1
16 , this solution is worse according to the tde. on this simple problem, the true values
do not have the smallest tde.

8 . as 1

a better idea would seem to be minimizing the bellman error. if the exact values are learned, the
bellman error is zero everywhere. thus, a bellman-error-minimizing algorithm should have no trouble
with the a-split example. we cannot expect to achieve zero bellman error in general, as it would involve
   nding the true value function, which we presume is outside the space of representable value functions.
but getting close to this ideal is a natural-seeming goal. as we have seen, the bellman error is also
closely related to the td error. the bellman error for a state is the expected td error in that state.
so let   s repeat the derivation above with the expected td error (all expectations here are implicitly
conditional on st):

1
2
1
2

     (e  [  t]2)
     (eb[  t  t]2)

wt+1 = wt    
= wt    
= wt       eb[  t  t]   eb[  t  t]
= wt       eb(cid:2)  t(rt+1 +     v(st+1,w)       v(st,w))(cid:3) eb[  t     t]
= wt +   (cid:104)eb(cid:2)  t(rt+1 +     v(st+1,w))(cid:3)       v(st,w)(cid:105)(cid:104)     v(st,w)       eb(cid:2)  t     v(st+1,w)(cid:3)(cid:105).

this update and various ways of sampling it are referred to as the residual gradient algorithm. if you
simply used the sample values in all the expectations, then the equation above reduces almost exactly to
(11.23), the naive residual-gradient algorithm.1 but this is naive, because the equation above involves
the next state, st+1, appearing in two expectations that are multiplied together. to get an unbiased

1for state values there remains a small di   erence in the treatment of the importance sampling ratio   t. in the analagous
action-value case (which is the most important case for control algorithms), the residual gradient algorithm would reduce
exactly to the naive version.

222

chapter 11. *off-policy methods with approximation

sample of the product, two independent samples of the next state are required, but during normal
interaction with an external environment only one is obtained. one expectation or the other can be
sampled, but not both.

there are two ways to make the residual gradient algorithm work. one is in the case of deterministic
environments. if the transition to the next state is deterministic, then the two samples will necessarily
be the same, and the naive algorithm is valid. the other way is to obtain two independent samples
of the next state, st+1, from st, one for the    rst expectation and another for the second expectation.
in real interaction with an environment, this would not seem possible, but when interacting with a
simulated environment, it is. one simply rolls back to the previous state and obtains an alternate next
state before proceeding forward from the    rst next state. in either of these cases the residual gradient
algorithm is guaranteed to converge to a minimum of the be under the usual conditions on the step-size
parameter. as a true sgd method, this convergence is robust, applying to both linear and non-linear
function approximators. in the linear case, convergence is always to the unique w that minimizes the
be.

however, there remain at least three ways in which the convergence of the residual gradient method
is unsatisfactory. the    rst of these is that empirically it is slow, much slower that semi-gradient meth-
ods. indeed, proponents of this method have proposed increasing its speed by combining it with faster
semi-gradient methods initially, then gradually switching over to residual gradient for the convergence
guarantee (baird and moore, 1999). the second way in which the residual-gradient algorithm is un-
satisfactory is that it still seems to converge to the wrong values. it does get the right values in all
tabular cases, such as the a-split example, as for those an exact solution to the bellman equation is
possible. but if we examine examples with genuine function approximation, then the residual-gradient
algorithm, and indeed the be objective, seem to    nd the wrong value functions. one of the most telling
such examples is the variation on the a-split example shown in the box. on the a-presplit example
the residual-gradient algorithm    nds the same poor solution as its naive version. this example shows
intuitively that minimizing the be (which the residual-gradient algorithm surely does) may not be a
desirable goal.

a-presplit example, a counterexample for the be

consider the following three-state episodic mrp:

episodes start in either a1 or a2, with equal id203. these two states look exactly the
same to the function approximator, like a single state a whose feature representation is distinct
from and unrelated to the feature representation of the other two states, b and c, which are
also distinct from each other. speci   cally, the parameter of the function approximator has three
components, one giving the value of state b, one giving the value of state c, and one giving
the value of both states a1 and a2. other than the selection of the initial state, the system
if it starts in a1, then it transitions to b with a reward of 0 and then on
is deterministic.
if it starts in a2, then it transitions to c, and then to
to termination with a reward of 1.
termination, with both rewards zero.

to a learning algorithm, seeing only the features, the system looks identical to the a-split
example. the system seems to always start in a, followed by either b or c with equal id203,
and then terminating with a 1 or a 0 depending deterministically on the previous state. as in

a1bc0001a2a11.6. the bellman error is not learnable

223

the a-split example, the true values of b and c are 1 and 0, and the best shared value of a1 and
a2 is 1

2 , by symmetry.

because this problem appears externally identical to the a-split example, we already know
what values will be found by the algorithms. semi-gradient td converges to the ideal values
just mentioned, while the naive residual-gradient algorithm converges to values of 3
4 for
b and c respectively. all state transitions are deterministic, so the non-naive residual-gradient
algorithm will also converge to these values (it is the same algorithm in this case). it follows
then that this    naive    solution must also be the one that minimizes the be, and so it is. on a
deterministic problem, the bellman errors and td errors are all the same, so the be is always
the same as the tde. optimizing the be on this example gives rise to the same failure mode
as with the naive residual-gradient algorithm on the a-split example.

4 and 1

the third way in which the convergence of the residual-gradient algorithm is not satisfactory is
explained in the next section. like the second way, the third way is also a problem with the be
objective itself rather than with any particular algorithm for achieving it.

11.6 the bellman error is not learnable

the concept of learnability that we introduce in this section is di   erent from that commonly used in
machine learning. there, a hypothesis is said to be    learnable    if it is e   ciently learnable, meaning
that it can be learned within a polynomial rather than an exponential number of examples. here we
use the term in a more basic way, to mean learnable at all, with any amount of experience. it turns out
many quantities of apparent interest in id23 cannot be learned even from an in   nite
amount of experiential data. these quantities are well de   ned and can be computed given knowledge
of the internal structure of the environment, but cannot be computed or estimated from the observed
sequence of feature vectors, actions, and rewards.2 we say that they are not learnable. it will turn out
that the bellman error objective (be) introduced in the last two sections is not learnable in this sense.
that the bellman error objective cannot be learned from the observable data is probably the strongest
reason not to seek it.

to make the concept of learnability clear, let   s start with some simple examples. consider the two

markov reward processes3 (mrps) diagrammed below:

where two edges leave a state, both transitions are assumed to occur with equal id203, and the
numbers indicate the reward received. all the states appear the same; they all produce the same single-
component feature vector x = 1 and have approximated value w. thus, the only varying part of the
data trajectory is the reward sequence. the left mrp stays in the same state and emits an endless
stream of 0s and 2s at random, each with 0.5 id203. the right mrp, on every step, either stays
in its current state or switches to the other, with equal id203. the reward is deterministic in this
mrp, always a 0 from one state and always a 2 from the other, but because the each state is equally
likely on each step, the observable data is again an endless stream of 0s and 2s at random, identical to
that produced by the left mrp. (we can assume the right mrp starts in one of two states at random

2they would of course be estimated if the state sequence were observed rather than only the corresponding feature

vectors.

3all mrps can be considered mdps with a single action in all states; what we conclude about mrps here applies as

well to mdps.

020220www224

chapter 11. *off-policy methods with approximation

with equal id203.) thus, even given even an in   nite amount of data, it would not be possible to
tell which of these two mrps was generating it. in particular, we could not tell if the mrp has one
state or two, is stochastic or deterministic. these things are not learnable.

this pair of mrps also illustrates that the ve objective (9.1) is not learnable. if    = 0, then the
true values of the three states (in both mrps), left to right, are 1, 0, and 2. suppose w = 1. then the
ve is 0 for the left mrp and 1 for the right mrp. because the ve is di   erent in the two problems,
yet the data generated has the same distribution, the ve cannot be learned. the ve is not a unique
function of the data distribution. and if it cannot be learned, then how could the ve possibly be useful
as an objective for learning?

if an objective cannot be learned, it does indeed draw its utility into question. in the case of the
ve, however, there is a way out. note that the same solution, w = 1, is optimal for both mrps above
(assuming    is the same for the two indistinguishable states in the right mrp). is this a coincidence,
or could it be generally true that all mdps with the same data distribution also have the same optimal
parameter vector? if this is true   and we will show next that it is   then the ve remains a usable
objective. the ve is not learnable, but the parameter that optimizes it is!

to understand this, it is useful to bring in another natural objective function, this time one that is
clearly learnable. one error that is always observable is that between the value estimate at each time
and the return from that time. the mean square return error, denoted re, is the expectation, under
  , of the square of this error. in the on-policy case the re can be written

re(w) = e(cid:104)(cid:0)gt       v(st,w)(cid:1)2(cid:105)

= ve(w) + e(cid:104)(cid:0)gt     v  (st)(cid:1)2(cid:105) .

thus, the two objectives are the same except for a variance term that does not depend on the parameter
vector. the two objectives must therefore have the same optimal parameter value w   . the overall
relationships are summarized in figure 11.4.

(11.24)

figure 11.4: causal relationships among the data distribution, mdps, and errors for monte-carlo objectives.
two di   erent mdps can produce the same data distribution yet also produce di   erent ves, proving that the
ve objective cannot be determined from data and is not learnable. however, all such ves must have the same
optimal parameter vector, w   ! moreover, this same w    can be determined from another objective, the re,
which is uniquely determined from the data distribution. thus w    and the re are learnable even though the
ves are not.

   exercise 11.3 prove (11.24). hint: write the re as an expectation over possible states s of the
expectation of the squared error given that st = s. then add and subtract the true value of state s

mdp1mdp2msve1msve2msredatadistributionpolicytogethercompletelydeterminetheid203distributionoverdatatrajectories.assumeforthemomentthatthestate,action,andrewardsetsareall   nite.then,forany   nitesequence   = 0,a0,r1,...,rk, k,thereisawellde   nedid203(pos-siblyzero)ofitoccuringastheinitialportionofatrajectory,whichwemaydenotedp(   )=pr{ (s0)= 0,a0=a0,r1=r1,...,rk=rk, (sk)= k}.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata,butitisstilllessthanknowingthemdp.inparticular,theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3,butthesecannotbedeterminedfrompalone.   1   2   3   4be1be2mdp1mdp2pbe            1      2      3      4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple,pomdp-likeexamples,inwhichtheobservabledataproducedbytwodi   erentmdpsisidenticalineveryrespect,yetthebeisdi   erent.insuchacasethebeisliterallynotafunctionofthedata,andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow:ba10-1ba0-1b(cid:1)01-1thesemdpshaveonlyoneaction(or,equivalently,noactions),sotheyareine   ectmarkovchains.wheretwoedgesleaveastate,bothpossibilitiesareassumedtooccurwithequalid203.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly;eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates,twoofwhich,bandb0,arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbythe   rstcomponentof   andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0,thensomenumberofbseachfollowedbya 1,exceptthelastwhichisfollowedbya1,thenwestartalloveragainwithasingleaanda0,etc.allthedetailsarethesameaswell;inbothmdps,theid203ofastringofkbsis2 k.nowconsiderthevaluefunctionv   =~0.inthe   rstmdp,thisisanexactsolution,andtheoverallbeiszero.inthesecondmdp,thissolutionproducesanerrorinbothbandb0of1,foranoverallbeofpd(b)+d(b0),orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps,whichgeneratethesamedata,havedi   erentbes.thus,thebecannotbeestimatedfromdataalone;knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover,thetwomdpshavedi   erentminimal-bevaluefunctions.2forthe   rstmdp,theminimal-bevaluefunctionistheexactsolutionv   =~0forany .forthesecondmdp,2.thisisacriticalobservation,asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample,thisiswhathappenswiththeve.theveisnotobservablefromdata,butits20w   ve1ve2re11.6. the bellman error is not learnable

225

figure 11.5: causal relationships among the data distribution, mdps, and errors for id64 objectives.
two di   erent mdps can produce the same data distribution yet also produce di   erent bes and have di   erent
minimizing parameter vectors; these are not learnable from the data distribution. the pbe and tde objectives
and their (di   erent) minima can be directly determined from data and thus are learnable.

from the error (before squaring), grouping the subtracted true value with the return and the added true
value with the estimated value. then, if you expand the square, the most complex term will end up
being zero, leaving you with (11.24).

now let us return to the be. the be is like the ve in that it can be computed

from knowledge of
the mdp but is not learnable from data. but it is not like the ve in that its minimum solution is not
learnable. the box on the next page presents a counterexample   two mrps that generate the same data
distribution but whose minimizing parameter vector is di   erent, proving that the optimal parameter
vector is not a function of the data and thus cannot be learned from it. the other id64
objectives that we have considered, the pbe and tde, can be determined from data (are learnable)
and determine optimal solutions that are in general di   erent from each other and the be minimums.
the general case is summarized in figure 11.5.

thus, the be is not learnable; it cannot be estimated from feature vectors and other observable data.
this limits the be to model-based settings. there can be no algorithm that minimizes the be without
access to the underlying mdp states beyond the feature vectors. the residual-gradient algorithm is
only able to minimize be because it is allowed to double sample from the same state   not a state that
has the same feature vector, but one that is guaranteed to be the same underlying state. we can see
now that there is no way around this. minimizing the be requires some such access to the nominal,
underlying mdp. this is an important limitation of the be beyond that identi   ed in the a-presplit
example on page 222. all this directs more attention toward the pbe.

mdp1mdp2msbe1msbe2policytogethercompletelydeterminetheid203distributionoverdatatrajectories.assumeforthemomentthatthestate,action,andrewardsetsareall   nite.then,forany   nitesequence   = 0,a0,r1,...,rk, k,thereisawellde   nedid203(pos-siblyzero)ofitoccuringastheinitialportionofatrajectory,whichwemaydenotedp(   )=pr{ (s0)= 0,a0=a0,r1=r1,...,rk=rk, (sk)= k}.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata,butitisstilllessthanknowingthemdp.inparticular,theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3,butthesecannotbedeterminedfrompalone.   1   2   3   4be1be2mdp1mdp2      1      2      3      4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple,pomdp-likeexamples,inwhichtheobservabledataproducedbytwodi   erentmdpsisidenticalineveryrespect,yetthebeisdi   erent.insuchacasethebeisliterallynotafunctionofthedata,andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow:ba10-1ba0-1b(cid:1)01-1thesemdpshaveonlyoneaction(or,equivalently,noactions),sotheyareine   ectmarkovchains.wheretwoedgesleaveastate,bothpossibilitiesareassumedtooccurwithequalid203.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly;eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates,twoofwhich,bandb0,arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbythe   rstcomponentof   andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0,thensomenumberofbseachfollowedbya 1,exceptthelastwhichisfollowedbya1,thenwestartalloveragainwithasingleaanda0,etc.allthedetailsarethesameaswell;inbothmdps,theid203ofastringofkbsis2 k.nowconsiderthevaluefunctionv   =~0.inthe   rstmdp,thisisanexactsolution,andtheoverallbeiszero.inthesecondmdp,thissolutionproducesanerrorinbothbandb0of1,foranoverallbeofpd(b)+d(b0),orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps,whichgeneratethesamedata,havedi   erentbes.thus,thebecannotbeestimatedfromdataalone;knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover,thetwomdpshavedi   erentminimal-bevaluefunctions.2forthe   rstmdp,theminimal-bevaluefunctionistheexactsolutionv   =~0forany .forthesecondmdp,2.thisisacriticalobservation,asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample,thisiswhathappenswiththeve.theveisnotobservablefromdata,butits20policytogethercompletelydeterminetheid203distributionoverdatatrajectories.assumeforthemomentthatthestate,action,andrewardsetsareall   nite.then,forany   nitesequence   = 0,a0,r1,...,rk, k,thereisawellde   nedid203(pos-siblyzero)ofitoccuringastheinitialportionofatrajectory,whichwemaydenotedp(   )=pr{ (s0)= 0,a0=a0,r1=r1,...,rk=rk, (sk)= k}.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata,butitisstilllessthanknowingthemdp.inparticular,theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3,butthesecannotbedeterminedfrompalone.   1   2   3   4be1be2mdp1mdp2      1      2      3      4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple,pomdp-likeexamples,inwhichtheobservabledataproducedbytwodi   erentmdpsisidenticalineveryrespect,yetthebeisdi   erent.insuchacasethebeisliterallynotafunctionofthedata,andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow:ba10-1ba0-1b(cid:1)01-1thesemdpshaveonlyoneaction(or,equivalently,noactions),sotheyareine   ectmarkovchains.wheretwoedgesleaveastate,bothpossibilitiesareassumedtooccurwithequalid203.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly;eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates,twoofwhich,bandb0,arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbythe   rstcomponentof   andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0,thensomenumberofbseachfollowedbya 1,exceptthelastwhichisfollowedbya1,thenwestartalloveragainwithasingleaanda0,etc.allthedetailsarethesameaswell;inbothmdps,theid203ofastringofkbsis2 k.nowconsiderthevaluefunctionv   =~0.inthe   rstmdp,thisisanexactsolution,andtheoverallbeiszero.inthesecondmdp,thissolutionproducesanerrorinbothbandb0of1,foranoverallbeofpd(b)+d(b0),orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps,whichgeneratethesamedata,havedi   erentbes.thus,thebecannotbeestimatedfromdataalone;knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover,thetwomdpshavedi   erentminimal-bevaluefunctions.2forthe   rstmdp,theminimal-bevaluefunctionistheexactsolutionv   =~0forany .forthesecondmdp,2.thisisacriticalobservation,asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample,thisiswhathappenswiththeve.theveisnotobservablefromdata,butits20mspbepolicytogethercompletelydeterminetheid203distributionoverdatatrajectories.assumeforthemomentthatthestate,action,andrewardsetsareall   nite.then,forany   nitesequence   = 0,a0,r1,...,rk, k,thereisawellde   nedid203(pos-siblyzero)ofitoccuringastheinitialportionofatrajectory,whichwemaydenotedp(   )=pr{ (s0)= 0,a0=a0,r1=r1,...,rk=rk, (sk)= k}.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata,butitisstilllessthanknowingthemdp.inparticular,theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3,butthesecannotbedeterminedfrompalone.   1   2   3   4be1be2mdp1mdp2      1      2      3      4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple,pomdp-likeexamples,inwhichtheobservabledataproducedbytwodi   erentmdpsisidenticalineveryrespect,yetthebeisdi   erent.insuchacasethebeisliterallynotafunctionofthedata,andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow:ba10-1ba0-1b(cid:1)01-1thesemdpshaveonlyoneaction(or,equivalently,noactions),sotheyareine   ectmarkovchains.wheretwoedgesleaveastate,bothpossibilitiesareassumedtooccurwithequalid203.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly;eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates,twoofwhich,bandb0,arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbythe   rstcomponentof   andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0,thensomenumberofbseachfollowedbya 1,exceptthelastwhichisfollowedbya1,thenwestartalloveragainwithasingleaanda0,etc.allthedetailsarethesameaswell;inbothmdps,theid203ofastringofkbsis2 k.nowconsiderthevaluefunctionv   =~0.inthe   rstmdp,thisisanexactsolution,andtheoverallbeiszero.inthesecondmdp,thissolutionproducesanerrorinbothbandb0of1,foranoverallbeofpd(b)+d(b0),orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps,whichgeneratethesamedata,havedi   erentbes.thus,thebecannotbeestimatedfromdataalone;knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover,thetwomdpshavedi   erentminimal-bevaluefunctions.2forthe   rstmdp,theminimal-bevaluefunctionistheexactsolutionv   =~0forany .forthesecondmdp,2.thisisacriticalobservation,asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample,thisiswhathappenswiththeve.theveisnotobservablefromdata,butits20mstdepolicytogethercompletelydeterminetheid203distributionoverdatatrajectories.assumeforthemomentthatthestate,action,andrewardsetsareall   nite.then,forany   nitesequence   = 0,a0,r1,...,rk, k,thereisawellde   nedid203(pos-siblyzero)ofitoccuringastheinitialportionofatrajectory,whichwemaydenotedp(   )=pr{ (s0)= 0,a0=a0,r1=r1,...,rk=rk, (sk)= k}.thedistributionpthenisacompletecharacterizationofasourceofdatatrajectories.toknowpistoknoweverythingaboutthestatisticsofthedata,butitisstilllessthanknowingthemdp.inparticular,theveandbeobjectivesarereadilycomputedfromthemdpasdescribedinsection3,butthesecannotbedeterminedfrompalone.   1   2   3   4be1be2mdp1mdp2      1      2      3      4be1be2mdp1mdp2tderevetheproblemcanbeseeninverysimple,pomdp-likeexamples,inwhichtheobservabledataproducedbytwodi   erentmdpsisidenticalineveryrespect,yetthebeisdi   erent.insuchacasethebeisliterallynotafunctionofthedata,andthusthereisnowaytoestimateitfromdata.oneofthesimplestexamplesisthepairofmdpsshownbelow:ba10-1ba0-1b(cid:1)01-1thesemdpshaveonlyoneaction(or,equivalently,noactions),sotheyareine   ectmarkovchains.wheretwoedgesleaveastate,bothpossibilitiesareassumedtooccurwithequalid203.thenumbersontheedgesindicatetherewardemittedifthatedgeistraversed.themdponthelefthastwostatesthatarerepresenteddistinctly;eachhasaseparateweightsothattheycantakeonanyvalue.themdpontherighthasthreestates,twoofwhich,bandb0,arerepresentedidenticallyandmustbegiventhesameapproximatevalue.wecanimaginethatthevalueofstateaisgivenbythe   rstcomponentof   andthevalueofbandb0isgivenbythesecond.noticethattheobservabledataisidenticalforthetwomdps.inbothcasestheagentwillseesingleoccurrencesofafollowedbya0,thensomenumberofbseachfollowedbya 1,exceptthelastwhichisfollowedbya1,thenwestartalloveragainwithasingleaanda0,etc.allthedetailsarethesameaswell;inbothmdps,theid203ofastringofkbsis2 k.nowconsiderthevaluefunctionv   =~0.inthe   rstmdp,thisisanexactsolution,andtheoverallbeiszero.inthesecondmdp,thissolutionproducesanerrorinbothbandb0of1,foranoverallbeofpd(b)+d(b0),orp2/3ifthethreestatesareequallyweightedbyd.thetwomdps,whichgeneratethesamedata,havedi   erentbes.thus,thebecannotbeestimatedfromdataalone;knowledgeofthemdpbeyondwhatisrevealedinthedataisrequired.moreover,thetwomdpshavedi   erentminimal-bevaluefunctions.2forthe   rstmdp,theminimal-bevaluefunctionistheexactsolutionv   =~0forany .forthesecondmdp,2.thisisacriticalobservation,asitispossibleforanerrorfunctiontobeunobservableandyetstillbeperfectlysatisfactoryforuseinlearningsettingsbecausethevaluethatminimizesitcanbedeterminedfromdata.forexample,thisiswhathappenswiththeve.theveisnotobservablefromdata,butits20datadistributionw   1w   2w   3w   4be2be1pbetde226

chapter 11. *off-policy methods with approximation

counterexample to the learnability of the be and its minima

to show the full range of possibilities we need a slightly more complex pair of markov reward
processes (mrps) than those considered earlier. consider the following two mrps:

where two edges leave a state, both transitions are assumed to occur with equal id203,
and the numbers indicate the reward received. the mrp on the left has two states that are
represented distinctly. the mrp on the right has three states, two of which, b and b(cid:48), appear
the same and must be given the same approximate value. speci   cally, w has two components
and the value of state a is given by the    rst component and the value of b and b(cid:48) is given by
the second. the second mrp has been designed so that equal time is spent in all three states,
so we can take   (s) = 1

3 , for all s.

note that the observable data distribution is identical for the two mrps. in both cases the
agent will see single occurrences of a followed by a 0, then some number of apparent bs, each
followed by a    1 except the last, which is followed by a 1, then we start all over again with
a single a and a 0, etc. all the statistical details are the same as well; in both mrps, the
id203 of a string of k bs is 2   k.

now suppose w = 0.

in
the second mrp, this solution produces a squared error in both b and b(cid:48) of 1, such that
be =   (b)1 +   (b(cid:48))1 = 2
3 . these two mrps, which generate the same data distribution, have
di   erent bes; the be is not learnable.

in the    rst mrp, this is an exact solution, and the be is zero.

moreover (and unlike the earlier example for the ve) the minimizing value of w is di   erent
for the two mrps. for the    rst mrp, w = 0 minimizes the be for any   . for the second mrp,
the minimizing w is a complicated function of   , but in the limit, as        1, it is (    1
2 , 0)(cid:62). thus
the solution that minimizes be cannot be estimated from data alone; knowledge of the mrp
beyond what is revealed in the data is required. in this sense, it is impossible in principle to
pursue the be as an objective for learning.

it may be surprising that in the second mrp the be-minimizing value of a is so far from
zero. recall that a has a dedicated weight and thus its value is unconstrained by function
approximation. a is followed by a reward of 0 and transition to a state with a value of nearly 0,
which suggests vw(a) should be 0; why is its optimal value substantially negative rather than
0? the answer is that making the value of a negative reduces the error upon arriving in a from
b. the reward on this deterministic transition is 1, which implies that b should have a value
1 more than a. because b   s value is approximately zero, a   s value is driven toward    1. the
be-minimizing value of         1
2 for a is a compromise between reducing the errors on leaving and
on entering a.

ba10-1ba0-1b'01-111.7. gradient-td methods

227

11.7 gradient-td methods

we now consider sgd methods for minimizing the pbe. as true sgd methods, these gradient-td
methods have robust convergence properties even under o   -policy training and non-linear function
approximation. remember that in the linear case there is always an exact solution, the td    xed point
wtd, at which the pbe is zero. this solution could be found by least-squares methods (section 9.7),
but only by methods of quadratic o(d2) complexity in the number of parameters. we seek instead an
sgd method, which should be o(d) and have robust convergence properties. gradient-td methods
come close to achieving these goals, at the cost of a rough doubling of computational complexity.

to derive an sgd method for the pbe (assuming linear function approximation) we begin by ex-

panding and rewriting the objective (11.22) in matrix terms:

pbe(w) =(cid:13)(cid:13)      w(cid:13)(cid:13)2

  

= (      w)(cid:62)d      w
=     (cid:62)w  (cid:62)d      w

x(cid:62)d    w

(using (11.13) and the identity   (cid:62)d   = dx(cid:0)x(cid:62)dx(cid:1)   1

=     (cid:62)wdx(cid:0)x(cid:62)dx(cid:1)   1
=(cid:0)x(cid:62)d    w(cid:1)(cid:62)(cid:0)x(cid:62)dx(cid:1)   1(cid:0)x(cid:62)d    w(cid:1).

the gradient with respect to w is

x(cid:62)d)

(from (11.14))

(11.25)

(11.26)

   pbe(w) = 2   (cid:2)x(cid:62)d    w(cid:3)(cid:62)(cid:0)x(cid:62)dx(cid:1)   1(cid:0)x(cid:62)d    w(cid:1).

to turn this into an sgd method, we have to sample something on every time step that has this quantity
as its expected value. let us take    to be the distribution of states visited under the behavior policy.
all three of the factors above can then be written in terms of expectations under this distribution. for
example, the last factor can be written

x(cid:62)d    w =(cid:88)s

  (s)x(s)    w(s) = e[  t  txt] ,

which is just the expectation of the semi-gradient td(0) update (11.2). the    rst factor is the transpose
of the gradient of this update:

finally, the middle factor is the inverse of the expected outer-product matrix of the feature vectors:

(using episodic   t)

   e[  t  txt](cid:62) = e(cid:2)  t     (cid:62)t x(cid:62)t(cid:3)

= e(cid:2)  t   (rt+1 +   w(cid:62)xt+1     w(cid:62)xt)(cid:62)x(cid:62)t(cid:3)
= e(cid:2)  t(  xt+1     xt)x(cid:62)t(cid:3) .
  (s)xsx(cid:62)s = e(cid:2)xtx(cid:62)t(cid:3) .

x(cid:62)dx =(cid:88)s

substituting these expectations for the three factors in our expression for the gradient of the pbe, we
get

   pbe(w) = 2e(cid:2)  t(  xt+1     xt)x(cid:62)t(cid:3) e(cid:2)xtx(cid:62)t(cid:3)   1 e[  t  txt] .

it might not be obvious that we have made any progress by writing the gradient in this form. it is a
product of three expressions and the    rst and last are not independent. they both depend on the next

(11.27)

228

chapter 11. *off-policy methods with approximation

feature vector xt+1; we cannot simply sample both of these expectations and then multiply the samples.
this would give us a biased estmate of the gradient just as in the naive residual-gradient algorithm.

another idea would be to estimate the three expectations separately and then combine them to
produce an unbiased estimate of the gradient. this would work, but would require a lot of computational
resources, particularly to store the    rst two expectations, which are d   d matrices, and to compute the
inverse of the second. this idea can be improved. if two of the three expectations are estimated and
stored, then the third could be sampled and used in conjunction with the two stored quantities. for
example, you could store estimates of the second two quantities (using the increment inverse-updating
techniques in section 9.7) and then sample the    rst expression. unfortunately, the overall algorithm
would still be of quadratic complexity (of order o(d2)).

the idea of storing some estimates separately and then combining them with samples is a good one
and is also used in gradient-td methods. gradient-td methods estimate and store the product of the
second two factors in (11.27). these factors are a d    d matrix and a d-vector, so their product is just
a d-vector, like w itself. we denote this second learned vector as v:

this form is familiar to students of linear supervised learning. it is the solution to a linear least-squares
problem that tries to approximate   t  t from the features. the standard sgd method for incrementally
is known as the least

   nding the vector v that minimizes the expected squared error(cid:0)v(cid:62)xt       t  t(cid:1)2

mean square (lms) rule:

(11.28)

v     e(cid:2)xtx(cid:62)t(cid:3)   1 e[  t  txt] .

vt+1 = vt +     t(cid:0)  t     v(cid:62)t xt(cid:1) xt,

where    > 0 is another step-size parameter. we can use this method to e   ectively achieve (11.28) with
o(d) storage and per-step computation.

given a stored estimate vt approximating (11.28), we can update our main parameter vector wt

using sgd methods based on (11.27). the simplest such rule is

wt+1 = wt    
= wt    

1
2
1
2

     pbe(wt)

  2e(cid:2)  t(  xt+1     xt)x(cid:62)t(cid:3) e(cid:2)xtx(cid:62)t(cid:3)   1 e[  t  txt]
= wt +   e(cid:2)  t(xt       xt+1)x(cid:62)t(cid:3) e(cid:2)xtx(cid:62)t(cid:3)   1 e[  t  txt]
= wt +   e(cid:2)  t(xt       xt+1)x(cid:62)t(cid:3) vt
= wt +     t (xt       xt+1) x(cid:62)t vt.

(the general sgd rule)

(from (11.27))

(11.29)

(based on (11.28))

(sampling)

this algorithm is called gtd2. note that if the    nal inner product (x(cid:62)t vt) is done    rst, then the entire
algorithm is of o(d) complexity.

a slightly better algorithm can be derived by doing a few more analytic steps before substituting in

vt. continuing from (11.29):

wt+1 = wt +   e(cid:2)  t(xt       xt+1)x(cid:62)t(cid:3) e(cid:2)xtx(cid:62)t(cid:3)   1 e[  t  txt]

= wt +   (cid:0)e(cid:2)  txtx(cid:62)t(cid:3)       e(cid:2)  txt+1x(cid:62)t(cid:3)(cid:1) e(cid:2)xtx(cid:62)t(cid:3)   1 e[  t  txt]
= wt +   (cid:16)e[xt  t  t]       e(cid:2)  txt+1x(cid:62)t(cid:3) e(cid:2)xtx(cid:62)t(cid:3)   1 e[  t  txt](cid:17)
= wt +   (cid:0)e[xt  t  t]       e(cid:2)  txt+1x(cid:62)t(cid:3) vt(cid:1)
= wt +     t(cid:0)  txt       xt+1x(cid:62)t vt(cid:1) ,

(sampling)

11.7. gradient-td methods

229

which again is o(d) if the    nal product (x(cid:62)t vt) is done    rst. this algorithm is known as either td(0)
with gradient correction (tdc) or, alternatively, as gtd(0).

figure 11.6 shows a sample and the expected behavior of tdc on baird   s counterexample. as
intended, the pbe falls to zero, but note that the individual components of the parameter vector do
not approach zero. in fact, these values are still far from an optimal solution,   v(s) = 0, for all s, for
which w would have to be proportional to (1, 1, 1, 1, 1, 1, 4,   2)(cid:62). after 1000 iterations we are still far
from an optimal solution, as we can see from the ve, which remains almost 2. the system is actually
converging to an optimal solution, but progress is extremely slow because the pbe is already so close
to zero.

figure 11.6: the behavior of the tdc algorithm on baird   s counterexample. on the left is shown a typical sin-
gle run, and on the right is shown the expected behavior of this algorithm if the updates are done synchronously
(analogous to (11.9), except for the two tdc parameter vectors). the step sizes were    = 0.005 and    = 0.05.

gtd2 and tdc both involve two learning processes, a primary one for w and a secondary one for
v. the logic of the primary learning process relies on the secondary learning process having    nished,
at least approximately, whereas the secondary learning process proceeds without being in   uenced by
the    rst. we call this sort of asymmetrical dependence a cascade. in cascades we often assume that the
secondary learning process is proceeding faster and thus is always at its asymptotic value, ready and
accurate to assist the primary learning process. the convergence proofs for these methods often make
this assumption explicitly. these are called two-time-scale proofs. the fast time scale is that of the
secondary learning process, and the slower time scale is that of the primary learning process. if    is
the step size of the primary learning process, and    is the step size of the secondary learning process,
then these convergence proofs will typically require that in the limit        0 and   

gradient-td methods are currently the most well understood and widely used stable o   -policy
methods. there are extensions to action values and control (gq, maei et al., 2010), to eligibility traces
(gtd(  ) and gq(  ), maei, 2011; maei and sutton, 2010), and to nonlinear function approximation
(maei et al., 2009). there have also been proposed hybrid algorithms midway between semi-gradient
td and gradient td. the hybrid td (htd, hackman, 2012; white and white, 2016) algorithm
behaves like gtd in states where the target and behavior policies are very di   erent, and behaves like
semi-gradient td in states where the target and behavior policies are the same. finally, the gradient-

       0.

w1   w6w8w8pvepveppbeppbestepsw7w1   w6tdcexpected tdcsweepsw7230

chapter 11. *off-policy methods with approximation

td idea has been combined with the ideas of proximal methods and control variates to produce more
e   cient methods (mahadevan et al., 2014).

11.8 emphatic-td methods

we turn now to the second major strategy that has been extensively explored for obtaining a cheap
and e   cient o   -policy learning method with function approximation. recall that linear semi-gradient
td methods are e   cient and stable when trained under the on-policy distribution, and that we showed
in section 9.4 that this has to do with the positive de   niteness of the matrix a (9.11) and the match
between the on-policy state distribution      and the state-transition probabilities p(s|s, a) under the
target policy. in o   -policy learning, we reweight the state transitions using importance weighting so
that they become appropriate for learning about the target policy, but the state distribution is still
that of the behavior policy. there is a mismatch. a natural idea is to somehow reweight the states,
emphasizing some and de-emphasizing others, so as to return the distribution of updates to the on-
policy distribution. there would then be a match, and stability and convergence would follow from
existing results. this is the idea of emphatic-td methods,    rst introduced for on-policy training in
section 9.10.

actually, the notion of    the on-policy distribution    is not quite right, as there are many on-policy
distributions, and any one of these is su   cient to guarantee stability. consider an undiscounted episodic
problem. the way episodes terminate is fully determined by the transition probabilities, but there may
be several di   erent ways the episodes might begin. however the episodes start, if all state transitions
are due to the target policy, then the state distribution that results is an on-policy distribution. you
might start close to the terminal state and visit only a few states with high id203 before ending
the episode. or you might start far away and pass through many states before terminating. both are
on-policy distributions, and training on both with a linear semi-gradient method would be guaranteed to
be stable. however the process starts, an on-policy distribution results as long as all states encountered
are updated up until termination.

if there is discounting, it can be treated as partial or probabilistic termination for these purposes.
if    = 0.9, then we can consider that with id203 0.1 the process terminates on every time step
and then immediately restarts in the state that is transitioned to. a discounted problem is one that is
continually terminating and restarting with id203 1       on every step. this way of thinking about
discounting is an example of a more general notion of pseudo termination   termination that does not
a   ect the sequence of state transitions, but does a   ect the learning process and the quantities being
learned. this kind of pseudo termination is important to o   -policy learning because the restarting is
optional   remember we can start any way we want to   and the termination relieves the need to keep
including encountered states within the on-policy distribution. that is, if we don   t consider the new
states as restarts, then discounting quickly give us a limited on-policy distribution.

the one-step emphatic-td algorithm for learning episodic state values is de   ned by:

  t = rt+1 +     v(st+1,wt)       v(st,wt),
wt+1 = wt +   mt  t  t     v(st,wt),
mt =     t   1mt   1 + it,

with it, the interest, being arbitrary and mt, the emphasis, being initialized to mt   1 = 0. how does this
algorithm perform on baird   s counterexample? figure 11.7 shows the trajectory in expectation of the
components of the parameter vector (for the case in which it = 1, for all t). there are some oscillations
but eventually everything converges and the ve goes to zero. these trajectories are obtained by
iteratively computing the expectation of the parameter vector trajectory without any of the variance
due to sampling of transitions and rewards. we do not show the results of applying the emphatic-td

11.9. reducing variance

231

figure 11.7: the behavior of the one-step emphatic-td algorithm in expectation on baird   s counterexample.
the step size was    = 0.03.

algorithm directly because its variance on baird   s counterexample is so high that it is nigh impossible to
get consistent results in computational experiments. the algorithm converges to the optimal solution
in theory on this problem, but in practice it does not. we turn to the topic of reducing the variance of
all these algorithms in the next section.

11.9 reducing variance

o   -policy learning is inherently of greater variance than on-policy learning. this is not surprising; if
you receive data less closely related to a policy, you should expect to learn less about the policy   s values.
in the extreme, one may be able to learn nothing. you can   t expect to learn how to drive by cooking
dinner, for example. only if the target and behavior policies are related, if they visit similar states and
take similar actions, should one be able to make signi   cant progress in o   -policy training.

on the other hand, any policy has many neighbors, many similar policies with considerable overlap
in states visited and actions chosen, and yet which are not identical. the raison d     etre of o   -policy
learning is to enable generalization to this vast number of related-but-not-identical policies. the problem
remains of how to make the best use of the experience. now that we have some methods that are stable
in expected value (if the step sizes are set right), attention naturally turns to reducing the variance
of the estimates. there are many possible ideas, and we can just touch on of a few of them in this
introductory text.

why is controlling variance especially critical in o   -policy methods based on importance sampling?
as we have seen, importance sampling often involves products of policy ratios. the ratios are always
one in expectation (5.11), but their actual values may be very high or as low as zero. successive ratios
are uncorrelated, so their products are also always one in expected value, but they can be of very high
variance. recall that these ratios multiply the step size in sgd methods, so high variance means taking
steps that vary greatly in their sizes. this is problematic for sgd because of the occasional very large
steps. they must not be so large as to take the parameter to a part of the space with a very di   erent
gradient. sgd methods rely on averaging over multiple steps to get a good sense of the gradient, and
if they make large moves from single samples they become unreliable. if the step-size parameter is set
small enough to prevent this, then the expected step can end up being very small, resulting in very
slow learning. the notions of momentum (derthick, 1984), of polyak-ruppert averaging (polyak, 1991;

sweepsw1   w6w7w8pve232

chapter 11. *off-policy methods with approximation

ruppert, 1988; polyak and juditsky, 1992), or further extensions of these ideas may signi   cantly help.
methods for adaptively setting separate step sizes for di   erent components of the parameter vector
are also pertinent (e.g., jacobs, 1988; sutton, 1992), as are the    importance weight aware    updates of
karampatziakis and langford (2010).

in chapter 5 we saw how weighted importance sampling is signi   cantly better behaved, with lower
variance updates, than ordinary importance sampling. however, adapting weighted importance sam-
pling to function approximation is challenging and can probably only be done approximately with o(d)
complexity (mahmood and sutton, 2015).

the tree backup algorithm (section 7.5) shows that it is possible to perform some o   -policy learning
without using importance sampling. this idea has been extended to the o   -policy case to produce
stable and more e   cient methods by munos, stepleton, harutyunyan, and bellemare (2016) and by
mahmood, yu and sutton (2017).

another, complementary strategy is to allow the target policy to be determined in part by the
behavior policy, in such a way that it never can be so di   erent from it to create large importance
sampling ratios. for example, the target policy can be de   ned by reference to the behavior policy, as
in the    recognizers    proposed by precup et al. (2005).

11.10

summary

o   -policy learning is a tempting challenge, testing our ingenuity in designing stable and e   cient learning
algorithms. tabular id24 makes o   -policy learning seem easy, and it has natural generalizations
to expected sarsa and to the tree backup algorithm. but as we have seen in this chapter, the extension
of these ideas to signi   cant function approximation, even linear function approximation, involves new
challenges and forces us to deepen our understanding of id23 algorithms.

why go to such lengths? one reason to seek o   -policy algorithms is to give    exibility in dealing
with the tradeo    between exploration and exploitation. another is to free behavior from learning, and
avoid the tyranny of the target policy. td learning appears to hold out the possibility of learning about
multiple things in parallel, of using one stream of experience to solve many tasks simultaneously. we
can certainly do this in special cases, just not in every case that we would like to or as e   ciently as we
would like to.

in this chapter we divided the challenge of o   -policy learning into two parts. the    rst part, correcting
the targets of learning for the behavior policy, is straightforwardly dealt with using the techniques
devised earlier for the tabular case, albeit at the cost of increasing the variance of the updates and
thereby slowing learning. high variance will probably always remains a challenge for o   -policy learning.

the second part of the challenge of o   -policy learning emerges as the instability of semi-gradient td
methods that involve id64. we seek powerful function approximation, o   -policy learning, and
the e   ciency and    exibility of id64 td methods, but it is challenging to combine all three
aspects of this deadly triad in one algorithm without introducing the potential for instability. there
have been several attempts. the most popular has been to seek to perform true stochastic gradient
descent (sgd) in the bellman error (a.k.a. the bellman residual). however, our analysis concludes
that this is not an appealing goal in many cases, and that anyway it is impossible to achieve with a
learning algorithm   the gradient of the be is not learnable from experience that reveals only feature
vectors and not underlying states. another approach, gradient-td methods, performs sgd in the
projected bellman error. the gradient of the pbe is learnable with o(d) complexity, but at the cost
of a second parameter vector with a second step size. the newest family of methods, emphatic-td
methods, re   ne an old idea for reweighting updates, emphasizing some and de-emphasizing others. in
this way they restore the special properties that make on-policy learning stable with computationally
simple semi-gradient methods.

11.10. summary

233

the whole area of o   -policy learning is relatively new and unsettled. which methods are best or even
adequate is not yet clear. are the complexities of the new methods introduced at the end of this chapter
really necessary? which of them can be combined e   ectively with variance reductions methods? the
potential for o   -policy learning remains tantalizing, the best way to achieve it still a mystery.

bibliographical and historical remarks

11.1

11.2

11.3

11.4

11.5

the    rst semi-gradient method was linear td(  ) (sutton, 1988). the name    semi-gradient    is
more recent (sutton, 2015a). semi-gradient o   -policy td(0) with general importance-sampling
ratio may not have been explicitly stated until sutton, mahmood, and white (2016), but the
action-value forms were introduced by precup, sutton, and singh (2000), who also did eligibility
trace forms of these algorithms (see chapter 12). their continuing, undiscounted forms have
not been signi   cantly explored. the atomic multi-step forms given here are new.

the earliest w-to-2w example was given by tsitsiklis and van roy (1996), who also introduced
the speci   c counterexample in the box on page 214. baird   s counterexample is due to baird
(1995), though the version we present here is slightly modi   ed. averaging methods for function
approximation were developed by gordon (1995, 1996). other examples of instability with o   -
policy dp methods and more complex methods of function approximation are given by boyan
and moore (1995). bradtke (1993) gives an example in which id24 using linear function
approximation in a linear quadratic regulation problem converges to a destabilizing policy.

the deadly triad was    rst identi   ed by sutton (1995b) and thoroughly analyzed by tsitsiklis
and van roy (1997). the name    deadly triad    is due to sutton (2015a).

this kind of linear analysis was pioneered by tsitsiklis and van roy (1996; 1997), including
the id145 operator. diagrams like figure 11.3 were introduced by lagoudakis
and parr (2003).

the be was    rst proposed as an objective function for id145 by schweitzer and
seidmann (1985). baird (1995, 1999) extended it to td learning based on stochastic gradient
descent, and engel, mannor, and meir (2003) extended it to least squares (o(d2)) methods
known as gaussian process td learning. in the literature, be minimization is often referred
to as bellman residual minimization.

the earliest a-split example is due to dayan (1992). the two forms given here were introduced
by sutton et al. (2009).

11.6

the contents of this section are new to this text.

11.7 gradient-td methods were introduced by sutton, szepesv  ari, and maei (2009). the methods
highlighted in this section were introduced by sutton et al. (2009) and mahmood et al. (2014).
the most sensitive empirical investigations to date of gradient-td and related methods are
given by geist and scherrer (2014), dann, neumann, and peters (2014), and white (2015).

11.8

emphatic-td methods were introduced by sutton, mahmood, and white (2016). full conver-
gence proofs and other theory were later established by yu (2015a; 2015b; yu, mahmood, and
sutton, 2017) and hallak, tamar, munos, and mannor (2015).

234

chapter 11. *off-policy methods with approximation

chapter 12

eligibility traces

eligibility traces are one of the basic mechanisms of id23. for example, in the popular
td(  ) algorithm, the    refers to the use of an eligibility trace. almost any temporal-di   erence (td)
method, such as id24 or sarsa, can be combined with eligibility traces to obtain a more general
method that may learn more e   ciently.

eligibility traces unify and generalize td and monte carlo methods. when td methods are aug-
mented with eligibility traces, they produce a family of methods spanning a spectrum that has monte
carlo methods at one end (   = 1) and one-step td methods at the other (   = 0).
in between are
intermediate methods that are often better than either extreme method. eligibility traces also provide
a way of implementing monte carlo methods online and on continuing problems without episodes.

of course, we have already seen one way of unifying td and monte carlo methods: the n-step td
methods of chapter 7. what eligibility traces o   er beyond these is an elegant algorithmic mechanism
with signi   cant computational advantages. the mechanism is a short-term memory vector, the eligibility
trace zt     rd, that parallels the long-term weight vector wt     rd. the rough idea is that when a
component of wt participates in producing an estimated value, then the corresponding component of
zt is bumped up and then begins to fade away. learning will then occur in that component of wt if
a nonzero td error occurs before the trace falls back to zero. the trace-decay parameter        [0, 1]
determines the rate at which the trace falls.

the primary computational advantage of eligibility traces over n-step methods is that only a single
trace vector is required rather than a store of the last n feature vectors. learning also occurs continually
and uniformly in time rather than being delayed and then catching up at the end of the episode. in
addition learning can occur and a   ect behavior immediately after a state is encountered rather than
being delayed n steps.

eligibility traces illustrate that a learning algorithm can sometimes be implemented in a di   erent way
to obtain computational advantages. many algorithms are most naturally formulated and understood
as an update of a state   s value based on events that follow that state over multiple future time steps.
for example, monte carlo methods (chapter 5) update a state based on all the future rewards, and
n-step td methods (chapter 7) update based on the next n rewards and state n steps in the future.
such formulations, based on looking forward from the updated state, are called forward views. forward
views are always somewhat complex to implement because the update depends on later things that are
not available at the time. however, as we show in this chapter it is often possible to achieve nearly
the same updates   and sometimes exactly the same updates   with an algorithm that uses the current
td error, looking backward to recently visited states using an eligibility trace. these alternate ways of
looking at and implementing learning algorithms are called backward views. backward views, transfor-
mations between forward-views and backward-views, and equivalences between them date back to the

235

236

chapter 12. eligibility traces

introduction of temporal di   erence learning, but have become much more powerful and sophisticated
since 2014. here we present the basics of the modern view.

as usual,    rst we fully develop the ideas for state values and prediction, then extend them to action
values and control. we develop them    rst for the on-policy case then extend them to o   -policy learning.
our treatment pays special attention to the case of linear function approximation, for which the results
with eligibility traces are stronger. all these results apply also to the tabular and state aggregation
cases because these are special cases of linear function approximation.

12.1 the   -return

in chapter 7 we de   ned an n-step return as the sum of the    rst n rewards plus the estimated value of
the state reached in n steps, each appropriately discounted (7.1). the general form of that equation,
for any parameterized function approximator, is

gt:t+n

.
= rt+1 +   rt+2 +        +   n   1rt+n +   n  v(st+n,wt+n   1), 0     t     t     n.

(12.1)

we noted in chapter 7 that each n-step return, for n     1, is a valid update target for a tabular learning
update, just as it is for an approximate sgd learning update such as (9.7).

2 gt:t+2 + 1

now we note that a valid update can be done not just toward any n-step return, but toward any
average of n-step returns. for example, an update can be done toward a target that is half of a two-step
return and half of a four-step return: 1
2 gt:t+4. any set of n-step returns can be averaged
in this way, even an in   nite set, as long as the weights on the component returns are positive and
sum to 1. the composite return possesses an error reduction property similar to that of individual
n-step returns (7.3) and thus can be used to construct updates with guaranteed convergence properties.
averaging produces a substantial new range of algorithms. for example, one could average one-step and
in   nite-step returns to obtain another way of interrelating td and monte carlo methods. in principle,
one could even average experience-based updates with dp updates to get a simple combination of
experience-based and model-based methods (cf. chapter 8).

an update that averages simpler component updates is called a compound update. the
backup diagram for a compound update consists of the backup diagrams for each of the
component updates with a horizontal line above them and the weighting fractions below.
for example, the compound update for the case mentioned at the start of this section,
mixing half of a two-step return and half of a four-step return, has the diagram shown
to the right. a compound update can only be done when the longest of its component
updates is complete. the update at the right, for example, could only be done at time
t + 4 for the estimate formed at time t. in general one would like to limit the length of
the longest component update because of the corresponding delay in the updates.

the td(  ) algorithm can be understood as one particular way of averaging n-step
updates. this average contains all the n-step updates, each weighted proportional to
  n   1, where        [0, 1], and is normalized by a factor of 1        to ensure that the weights
sum to 1 (see figure 12.1). the resulting update is toward a return, called the   -return,
de   ned in its state-based form by

g  
t

.
= (1       )

   (cid:88)n=1

  n   1gt:t+n.

(12.2)

figure 12.2 further illustrates the weighting on the sequence of n-step returns in the   -
return. the one-step return is given the largest weight, 1      ; the two-step return is given
the next largest weight, (1       )  ; the three-step return is given the weight (1       )  2; and so on. the

121212.1. the   -return

237

figure 12.1: the update digram for td(  ). if    = 0, then the overall update reduces to its    rst component,
the one-step td update, whereas if    = 1, then the overall update reduces to its last component, the monte
carlo update.

figure 12.2: weighting given in the   -return to each of the n-step returns.

weight fades by    with each additional step. after a terminal state has been reached, all subsequent
n-step returns are equal to gt. if we want, we can separate these post-termination terms from the main
sum, yielding

g  
t = (1       )

t   t   1(cid:88)n=1

  n   1gt:t+n +   t   t   1gt,

(12.3)

as indicated in the    gures. this equation makes it clearer what happens when    = 1. in this case the
main sum goes to zero, and the remaining term reduces to the conventional return, gt. thus, for    = 1,
updating according to the   -return is a monte carlo algorithm. on the other hand, if    = 0, then the
  -return reduces to gt:t+1, the one-step return. thus, for    = 0, updating according to the   -return is
a one-step td method.

exercise 12.1 just as the return can be written recursively in terms of the    rst reward and itself
one-step later (3.9), so can the   -return. derive the analogous recursive relationship from (12.2) and

1  (1  ) (1  ) 2 t t 1            statat+1at 1st+1rt+1strt      st+2rt+2at+2td( )x=11!"weight given tothe 3-step returndecay by "weight given toactual, final returntttimeweighttotal area = 1is(1  ) 2is t t 1weighting238

chapter 12. eligibility traces

(12.1).

(cid:3)
exercise 12.2 the parameter    characterizes how fast the exponential weighting in figure 12.2 falls
o   , and thus how far into the future the   -return algorithm looks in determining its update. but a
rate factor such as    is sometimes an awkward way of characterizing the speed of the decay. for some
purposes it is better to specify a time constant, or half-life. what is the equation relating    and the
(cid:3)
half-life,     , the time by which the weighting sequence will have fallen to half of its initial value?
we are now ready to de   ne our    rst learning algorithm based on the   -return: the o   -line   -return
algorithm. as an o   -line algorithm, it makes no changes to the weight vector during the episode.
then, at the end of the episode, a whole sequence of o   -line updates are made according to our usual
semi-gradient rule, using the   -return as the target:

wt+1

.

= wt +   (cid:104)g  

t       v(st,wt)(cid:105)     v(st,wt),

t = 0, . . . , t     1.

(12.4)

the   -return gives us an alternative way of moving smoothly between monte carlo and one-step td
methods that can be compared with the n-step td way of chapter 7. there we assessed e   ectiveness
on a 19-state random walk task (example 7.1). figure 12.3 shows the performance of the o   -line   -
return algorithm on this task alongside that of the n-step methods (repeated from figure 7.2). the
experiment was just as described earlier except that for the   -return algorithm we varied    instead of
n. the performance measure used is the estimated root-mean-squared error between the correct and
estimated values of each state measured at the end of the episode, averaged over the    rst 10 episodes
and the 19 states. note that overall performance of the o   -line   -return algorithms is comparable to
that of the n-step algorithms. in both cases we get best performance with an intermediate value of the
id64 parameter, n for n-step methods and    for the o   ine   -return algorithm.

figure 12.3: 19-state random walk results (example 7.1): performance of the o   ine   -return algorithm
alongside that of the n-step td methods. in both case, intermediate values of the id64 parameter (  
or n) performed best. the results with the o   -line   -return algorithm are slightly better at the best values of
   and   , and at high   .

the approach that we have been taking so far is what we call the theoretical, or forward, view of a
learning algorithm. for each state visited, we look forward in time to all the future rewards and decide
how best to combine them. we might imagine ourselves riding the stream of states, looking forward
from each state to determine its update, as suggested by figure 12.4. after looking forward from and
updating one state, we move on to the next and never have to work with the preceding state again.

   n=1n=2n=4n=8n=16n=32n=32n=64128512256n-step td methods(from chapter 7)off-line   -return algorithm   rms errorat the end of the episodeover the    rst10 episodes  =0  =.4  =.8  =.9  =.95  =.975  =.99  =1  =.9512.2. td(  )

239

figure 12.4: the forward view. we decide how to update each state by looking forward to future rewards and
states.

future states, on the other hand, are viewed and processed repeatedly, once from each vantage point
preceding them.

12.2 td(  )

td(  ) is one of the oldest and most widely used algorithms in id23. it was the    rst
algorithm for which a formal relationship was shown between a more theoretical forward view and a
more computationally congenial backward view using eligibility traces. here we will show empirically
that it approximates the o   -line   -return algorithm presented in the previous section.

td(  ) improves over the o   -line   -return algorithm in three ways. first it updates the weight vector
on every step of an episode rather than only at the end, and thus its estimates may be better sooner.
second, its computations are equally distributed in time rather that all at the end of the episode. and
third, it can be applied to continuing problems rather than just episodic problems. in this section we
present the semi-gradient version of td(  ) with function approximation.

with function approximation, the eligibility trace is a vector zt     rd with the same number of
components as the weight vector wt. whereas the weight vector is a long-term memory, accumulating
over the lifetime of the system, the eligibility trace is a short-term memory, typically lasting less time
than the length of an episode. eligibility traces assist in the learning process; their only consequence is
that they a   ect the weight vector, and then the weight vector determines the estimated value.

in td(  ), the eligibility trace vector is initialized to zero at the beginning of the episode, is incre-

mented on each time step by the value gradient, and then fades away by     :

.
= 0,
z   1
.
=     zt   1 +      v(st,wt),
zt

0     t     t,

(12.5)

where    is the discount rate and    is the parameter introduced in the previous section. the eligibility
trace keeps track of which components of the weight vector have contributed, positively or negatively,
to recent state valuations, where    recent    is de   ned in terms     . the trace is said to indicate the
eligibility of each component of the weight vector for undergoing learning changes should a reinforcing
event occur. the reinforcing events we are concerned with are the moment-by-moment one-step td
errors. the td error for state-value prediction is

  t

.
= rt+1 +     v(st+1,wt)       v(st,wt).

(12.6)

in td(  ), the weight vector is updated on each step proportional to the scalar td error and the vector
eligibility trace:

wt+1

.
= wt +     t zt,

(12.7)

timert+3rt+2rt+1rtst+1st+2st+3stst+1stst+2st+3rt+3rt+2rt+1rt240

chapter 12. eligibility traces

complete pseudocode for td(  ) is given in the box, and a picture of its operation is suggested by
figure 12.5.

semi-gradient td(  ) for estimating   v     v  
input: the policy    to be evaluated
input: a di   erentiable function   v : s+    rd     r such that   v(terminal,  ) = 0
initialize value-function weights w arbitrarily (e.g., w = 0)
repeat (for each episode):

(a d-dimensional vector)

initialize s
z     0
repeat (for each step of episode):
. choose a       (  |s)
. take action a, observe r, s(cid:48)
z         z +      v(s,w)
.
       r +     v(s(cid:48),w)       v(s,w)
.
. w     w +      z
. s     s(cid:48)
until s(cid:48) is terminal

td(  ) is oriented backward in time. at each moment we look at the current td error and assign
it backward to each prior state according to how much that state contributed to the current eligibility
trace at that time. we might imagine ourselves riding along the stream of states, computing td errors,
and shouting them back to the previously visited states, as suggested by figure 12.5. where the td
error and traces come together, we get the update given by (12.7).

to better understand the backward view, consider what happens at various values of   . if    = 0,
then by (12.5) the trace at t is exactly the value gradient corresponding to st. thus the td(  ) update
(12.7) reduces to the one-step semi-gradient td update treated in chapter 9 (and, in the tabular case,
to the simple td rule (6.2)). this is why that algorithm was called td(0). in terms of figure 12.5,
td(0) is the case in which only the one state preceding the current one is changed by the td error. for
larger values of   , but still    < 1, more of the preceding states are changed, but each more temporally
distant state is changed less because the corresponding eligibility trace is smaller, as suggested by the
   gure. we say that the earlier states are given less credit for the td error.

figure 12.5: the backward or mechanistic view. each update depends on the current td error combined with
the current eligibility traces of past events.

!tetetetettimestst+1st-1st-2st-3stst+1st-1st-2st-3 tztztztzt12.2. td(  )

241

figure 12.6: 19-state random walk results (example 7.1): performance of td(  ) alongside that of the o   -line
  -return algorithm. the two algorithms performed virtually identically at low (less than optimal)    values, but
td(  ) was worse at high    values.

if    = 1, then the credit given to earlier states falls only by    per step. this turns out to be just
the right thing to do to achieve monte carlo behavior. for example, remember that the td error,   t,
includes an undiscounted term of rt+1. in passing this back k steps it needs to be discounted, like any
reward in a return, by   k, which is just what the falling eligibility trace achieves. if    = 1 and    = 1,
then the eligibility traces do not decay at all with time. in this case the method behaves like a monte
carlo method for an undiscounted, episodic task. if    = 1, the algorithm is also known as td(1).

td(1) is a way of implementing monte carlo algorithms that is more general than those presented
earlier and that signi   cantly increases their range of applicability. whereas the earlier monte carlo
methods were limited to episodic tasks, td(1) can be applied to discounted continuing tasks as well.
moreover, td(1) can be performed incrementally and on-line. one disadvantage of monte carlo meth-
ods is that they learn nothing from an episode until it is over. for example, if a monte carlo control
method takes an action that produces a very poor reward but does not end the episode, then the agent   s
tendency to repeat the action will be undiminished during the episode. on-line td(1), on the other
hand, learns in an n-step td way from the incomplete ongoing episode, where the n steps are all the
way up to the current step. if something unusually good or bad happens during an episode, control
methods based on td(1) can learn immediately and alter their behavior on that same episode.

it is revealing to revisit the 19-state random walk example (example 7.1) to see how well td(  )
does in approximating the o   -line   -return algorithm. the results for both algorithms are shown in
figure 12.6. for each    value, if    is selected optimally for it (or smaller), then the two algorithms
perform virtually identically. if    is chosen larger than is optimal, however, then the   -return algorithm
is only a little worse whereas td(  ) is much worse and may even be unstable. this is not catastrophic
for td(  ) on this problem, as these higher parameter values are not what one would want to use
anyway, but for other problems it can be a signi   cant weakness.

linear td(  ) has been proved to converge in the on-policy case if the step-size parameter is reduced
over time according to the usual conditions (2.7). just as discussed in section 9.4, convergence is not
to the minimum-error weight vector, but to a nearby weight vector that depends on   . the bound on
solution quality presented in that section (9.14) can now be generalized to apply to any   . for the
continuing discounted case,

ve(w   )    

1         
1       

min

w

ve(w).

(12.8)

off-line   -return algorithm(from the previous section)     =0  =.4  =.8  =.9  =.95  =.975  =.99  =1  =.95  =0  =.4  =.8  =.9  =.95.975.991td(  )     =.8  =.9rms errorat the end of the episodeover the    rst10 episodes242

chapter 12. eligibility traces

that is, the asymptotic error is no more than 1       
1      times the smallest possible error. as    approaches
1, the bound approaches the minimum error (and it is loosest at    = 0). in practice, however,    = 1 is
often the poorest choice, as will be illustrated later in figure 12.14.

exercise 12.3 some insight into how td(  ) can closely approximate the o   -line   -return algorithm
can be gained by seeing that the latter   s error term (from (12.4)) can be written as the sum of td
errors (12.6) for a single    xed w. show this, following the pattern of (6.6), and using the recursive
(cid:3)
relationship for the   -return you obtained in exercise 12.1.
   exercise 12.4 although online td(  ) is not equivalent to the   -return algorithm, perhaps there   s
a slightly di   erent online td method that would maintain equivalence. one idea is to de   ne the td
.
error instead as   (cid:48)t
= rt+1 +   vt(st+1)    vt   1(st). show that in this case the modi   ed td(  ) algorithm
would then achieve exactly

   vt(st) =   (cid:104)g  

t     vt   1(st)(cid:105),

even in the case of on-line updating with large   . in what ways might this modi   ed td(  ) be better
or worse than the conventional one described in the text? describe an experiment to assess the relative
(cid:3)
merits of the two algorithms.

12.3 n-step truncated   -return methods

the o   -line   -return algorithm is an important ideal, but it   s of limited utility because it uses the
  -return (12.2), which is not known until the end of the episode. in the continuing case, the   -return
is technically never known, as it depends on n-step returns for arbitrarily large n, and thus on rewards
arbitrarily far in the future. however, the dependence gets weaker for long-delayed rewards, falling by
     for each step of delay. a natural approximation then would be to truncate the sequence after some
number of steps. our existing notion of n-step returns provides a natural way to do this in which the
missing rewards are replaced with estimated values.

in general, we de   ne the truncated   -return for time t, given data only up to some later horizon, h,

as

g  
t:h

.
= (1       )

h   t   1(cid:88)n=1

  n   1gt:t+n +   h   t   1gt:h,

0     t < h     t.

(12.9)

if you compare this equation with the   -return (12.3), it is clear that the horizon h is playing the same
role as was previously played by t , the time of termination. whereas in the   -return there is a residual
weighting given to the true return, here it is given to the longest available n-step return, the (h   t)-step
return (figure 12.2).

the truncated   -return immediately gives rise to a family of n-step   -return algorithms similar to
the n-step methods of chapter 7. in all these algorithms, updates are delayed by n steps and only take
into account the    rst n rewards, but now all the k-step returns are included for 1     k     n (whereas the
earlier n-step algorithms used only the n-step return), weighted geometrically as in figure 12.2. in the
state-value case, this family of algorithms is known as truncated td(  ), or ttd(  ). the compound
backup diagram, shown in figure 12.7, is similar to that for td(  ) (figure 12.1) except that the longest
component update is at most n steps rather than always going all the way to the end of the episode.
ttd(  ) is de   ned by (cf. (9.15)):

wt+n

.

= wt+n   1 +   (cid:2)g  

t:t+n       v(st,wt+n   1)(cid:3)     v(st,wt+n   1),

this algorithm can be implemented e   ciently so that per-step computation does not scale with n
(though of course memory must). much as in n-step td methods, no updates are made on the    rst

0     t < t.

(12.10)

12.4. redoing updates: the online   -return algorithm

243

figure 12.7: the backup diagram for truncated td(  ).

n     1 time steps, and n     1 additional updates are made upon termination. e   cient implementation
relies on the fact that the k-step   -return can be written exactly as

g  

t:t+k =   v(st,wt   1) +

t+k   1(cid:88)i=t

(    )i   t  (cid:48)i,

where
  (cid:48)t

.
= rt+1 +     v(st+1,wt)       v(st,wt   1).

(12.11)

(12.12)

exercise 12.5 several times in this book (often in exercises) we have established that returns can be
written as sums of td errors if the value function is held constant. why is (12.11) another instance of
(cid:3)
this? prove (12.11).

12.4 redoing updates: the online   -return algorithm

choosing the truncation parameter n in truncated td(  ) involves a tradeo   . n should be large so that
the method closely approximates the o   -line   -return algorithm, but it should also be small so that the
updates can be made sooner and can in   uence behavior sooner. can we get the best of both? well,
yes, in principle we can, albeit at the cost of computational complexity.

the idea is that, on each time step as you gather a new increment of data, you go back and redo all
the updates since the beginning of the current episode. the new updates will be better than the ones
you previously made because now they can take into account the time step   s new data. that is, the
updates are always towards an n-step truncated   -return target, but they always use the latest horizon.
in each pass over that episode you can use a slightly longer horizon and obtain slightly better results.
recall that the n-step truncated   -return is de   ned by

g  
t:h

.
= (1       )

h   t   1(cid:88)n=1

  n   1gt:t+n +   h   t   1gt:h.

(12.9)

1  (1  ) (1  ) 2 t t 1or,ift+n t                   n 1statat+1at 1st+nrt+nst+1rt+1strtat+n 1n-steptruncatedtd( )244

chapter 12. eligibility traces

let us step through how this target could ideally be used if computational complexity was not an
issue. the episode begins with an estimate at time 0 using the weights w0 from the end of the previous
episode. learning begins when the data horizon is extended to time step 1. the target for the estimate
at step 0, given the data up to horizon 1, could only be the one-step return g0:1, which includes r1
and bootstraps from the estimate   v(s1,w0). note that this is exactly what g  
0:1 is, with the sum in
the    rst term of (12.9) degenerating to zero. using this update target, we construct w1. then, after
advancing the data horizon to step 2, what do we do? we have new data in the form of r2 and s2, as
well as the new w1, so now we can construct a better update target g  
0:2 for the    rst update from s0 as
well as a better update target g  
0:2 for the second update from s1. we perform both of these updates
in sequence to produce w2. now we advance the horizon to step 3 and repeat, going all the way back
to produce three new updates and    nally w3, and so on.

this conceptual algorithm involves multiple passes over the episode, one at each horizon, each gen-
erating a di   erent sequence of weight vectors. to describe it clearly we have to distinguish between the
weight vectors computed at the di   erent horizons. let us use wh
t to denote the weights used to generate
the value at time t in the sequence at horizon h. the    rst weight vector wh
0 in each sequence is that
inherited from the previous episode, and the last weight vector wh
h in each sequence de   nes the ultimate
weight-vector sequence of the algorithm. at the    nal horizon h = t we obtain the    nal weights wt
t
which will be passed on to form the initial weights of the next episode. with these conventions, the
three    rst sequences described in the previous paragraph can be given explicitly:

h = 1 : w1
1

h = 2 : w2
1
w2
2

h = 3 : w3
1
w3
2
w3
3

.
= w1

.
= w2
.
= w2

.
= w3
.
= w3
.
= w3

0)(cid:3)     v(s0,w1
0)(cid:3)     v(s0,w2
1)(cid:3)     v(s1,w2
0)(cid:3)     v(s0,w3
1)(cid:3)     v(s1,w3
2)(cid:3)     v(s2,w3

0),

0),
1),

0),
1),
2).

0:1       v(s0,w1

0:2       v(s0,w2
1:2       v(s1,w2

0:3       v(s0,w3
1:3       v(s1,w3
2:3       v(s2,w3

0 +   (cid:2)g  
0 +   (cid:2)g  
1 +   (cid:2)g  
0 +   (cid:2)g  
1 +   (cid:2)g  
2 +   (cid:2)g  
t:h       v(st,wh
.
= wt

the general form for the update is

wh

t+1

.
= wh

t +   (cid:2)g  

t )(cid:3)     v(st,wh

t ), 0     t < h     t.

this update, together with wt

t de   nes the online   -return algorithm.

(12.13)

the online   -return algorithm is fully online, determining a new weight vector wt at each step t during
an episode, using only information available at time t. it   s main drawback is that it is computationally
complex, passing over the entire episode so far on every step. note that it is strictly more complex
than the o   -line   -return algorithm, which passes through all the steps at the time of termination but
does not make any updates during the episode. in return, the online algorithm can be expected to
perform better than the o   -line one, not only during the episode when it makes an update while the
o   -line algorithm makes none, but also at the end of the episode because the weight vector used in
id64 (in g  
t:h) has had a greater number of informative updates. this e   ect can be seen if one
looks carefully at figure 12.8, which compares the two algorithms on the 19-state random walk task.

12.5. true online td(  )

245

figure 12.8: 19-state random walk results (example 7.1): performance of online and o   -line   -return algo-
rithms. the performance measure here is the ve at the end of the episode, which should be the best case for
the o   -line algorithm. nevertheless, the on-line algorithm performs subtlely better. for comparison, the    = 0
line is the same for both methods.

12.5 true online td(  )

the on-line   -return algorithm just presented is currently the best performing temporal-di   erence
algorithm. it is an ideal which online td(  ) only approximates. as presented, however, the on-line
  -return algorithm is very complex. is there a way to invert this forward-view algorithm to produce
an e   cient backward-view algorithm using eligibility traces? it turns out that there is indeed an
exact computationally congenial implementation of the on-line   -return algorithm for the case of linear
function approximation. this implementation is known as the true online td(  ) algorithm because it
is    truer    to the ideal of the online   -return algorithm than the td(  ) algorithm is.

the derivation of true on-line td(  ) is a little too complex to present here (see the next section and
the appendix to the paper by van seijen et al., 2016) but its strategy is simple. the sequence of weight
vectors produced by the on-line   -return algorithm can be arranged in a triangle:

w0
0
0 w1
w1
1
1 w2
0 w2
w2
2
2 w3
1 w3
0 w3
w3
3
...
...
0 wt
wt

1 wt

2 wt
3

...

...

. . .
       wt

t

(12.14)

t, are really needed. the    rst, w0

one row of this triangle is produced on each time step. it turns out that only the weight vectors on the
diagonal, the wt
t , is the output, and each
weight vector along the way, wt
t, plays a role in id64 in the n-step returns of the updates.
in the    nal algorithm the diagonal weight vectors are renamed without a superscript, wt
t. the
strategy then is to    nd a compact, e   cient way of computing each wt
t from the one before. if this is
done, for the linear case in which   v(s,w) = w(cid:62)x(s), then we arrive at the true online td(  ) algorithm:

0, is the input, the last, wt

.
= wt

wt+1

.

= wt +     t zt +   (cid:0)w(cid:62)t xt     w(cid:62)t   1xt(cid:1) (zt     xt),

(12.15)

rms errorover    rst10 episodesoff-line   -return algorithm     =0  =.4  =.8  =.9  =.95  =.975  =.99  =1  =.95on-line   -return algorithm= true online td(  )     =0  =.4  =.8  =.9  =.95  =.975  =.99  =1  =.95246

chapter 12. eligibility traces

where we have used the shorthand xt

.
= x(st),   t is de   ned as in td(  ) (12.6), and zt is de   ned by

(12.16)

zt

.

=     zt   1 +(cid:0)1           z(cid:62)t   1xt(cid:1) xt.

this algorithm has been proven to produce exactly the same sequence of weight vectors, wt, 0     t     t ,
as the on-line   -return algorithm (van siejen et al. 2016). thus the results on the random walk task
on the left of figure 12.8 are also its results on that task. now, however, the algorithm is much
less expensive. the memory requirements of true online td(  ) are identical to those of conventional
td(  ), while the per-step computation is increased by about 50% (there is one more inner product in
the eligibility-trace update). overall, the per-step computational complexity remains of o(d), the same
as td(  ). pseudocode for the complete algorithm is given in the box.

true online td(  ) for estimating w(cid:62)x     v  
input: the policy    to be evaluated

initialize value-function weights w arbitrarily (e.g., w = 0)
repeat (for each episode):

(an d-dimensional vector)
(a scalar temporary variable)

initialize state and obtain initial feature vector x
z     0
vold     0
repeat (for each step of episode):
| choose a       
| take action a, observe r, x(cid:48) (feature vector of the next state)
| v     w(cid:62)x
| v (cid:48)     w(cid:62)x(cid:48)
       r +   v (cid:48)     v
|
z         z +(cid:0)1           z(cid:62)x(cid:1) x
|
| w     w +   (   + v     vold)z       (v     vold)x
| vold     v (cid:48)
| x     x(cid:48)
until x(cid:48) = 0 (signaling arrival at a terminal state)

the eligibility trace (12.16) used in true online td(  ) is called a dutch trace to distinguish it from
the trace (12.5) used in td(  ), which is called an accumulating trace. earlier work often used a third
kind of trace called the replacing trace, de   ned only for the tabular case or for binary feature vectors
such as those produced by tile coding. the replacing trace is de   ned on a component-by-component
basis depending on whether the component of the feature vector was 1 or 0:

zi,t

.

=(cid:26) 1

    zi,t   1

if xi,t = 1
otherwise.

(12.17)

nowadays, use of the replacing trace is deprecated; a dutch trace should almost always be used instead.

12.6. dutch traces in monte carlo learning

247

12.6 dutch traces in monte carlo learning

although eligibility traces are closely associated historically with td learning, in fact they have nothing
to do with it. in fact, eligibility traces arise even in monte carlo learning, as we show in this section.
we show that the linear mc algorithm (chapter 9), taken as a forward view, can be used to derive an
equivalent yet computationally cheaper backward-view algorithm using dutch traces. this is the only
equivalence of forward- and backward-views that we explicitly demonstrate in this book. it gives some
of the    avor of the proof of equivalence of true online td(  ) and the on-line   -return algorithm, but is
much simpler.

the linear version of the gradient monte carlo prediction algorithm (page 165) makes the following

sequence of updates, one for each time step of the episode:

wt+1

.

= wt +   (cid:2)g     w(cid:62)t xt(cid:3) xt,

0     t < t.

(12.18)

to make the example simpler, we assume here that the return g is a single reward received at the end
of the episode (this is why g is not subscripted by time) and that there is no discounting. in this case
the update is also known as the least mean square (lms) rule. as a monte carlo algorithm, all the
updates depend on the    nal reward/return, so none can be made until the end of the episode. the
mc algorithm is an o   ine algorithm and we do not seek to improve this aspect of it. rather we seek
merely an implementation of this algorithm with computational advantages. we will still update the
weight vector only at the end of the episode, but we will do some computation during each step of
the episode and less at its end. this will give a more equal distribution of computation   o(d) per
step   and also remove the need to store the feature vectors at each step for use later at the end of each
episode. instead, we will introduce an additional vector memory, the eligibility trace, keeping in it a
summary of all the feature vectors seen so far. this will be su   cient to e   ciently recreate exactly the
same overall update as the sequence of mc updates (12.18), by the end of the episode:

wt = wt   1 +   (cid:0)g     w(cid:62)t   1xt   1(cid:1) xt   1
= wt   1 +   xt   1(cid:0)   x(cid:62)t   1wt   1(cid:1) +   gxt   1
=(cid:0)i       xt   1x(cid:62)t   1(cid:1) wt   1 +   gxt   1
.
= i       xtx(cid:62)t

= ft   1wt   1 +   gxt   1

where ft

is a forgetting, or fading, matrix. now, recursing,

= ft   1 (ft   2wt   2 +   gxt   2) +   gxt   1
= ft   1ft   2wt   2 +   g (ft   1xt   2 + xt   1)
= ft   1ft   2 (ft   3wt   3 +   gxt   3) +   g (ft   1xt   2 + xt   1)
= ft   1ft   2ft   3wt   3 +   g (ft   1ft   2xt   3 + ft   1xt   2 + xt   1)
...

= ft   1ft   2        f0w0

ft   1ft   2        fk+1xk

(cid:124)

at   1

(cid:123)(cid:122)

= at   1 +   gzt   1 ,

+   g

t   1(cid:88)k=0
(cid:124)

(cid:125)

zt   1

(cid:123)(cid:122)

(cid:125)

(12.19)

where at   1 and zt   1 are the values at time t     1 of two auxilary memory vectors that can be updated
incrementally without knowledge of g and with o(d) complexity per time step. the zt vector is in fact

248

chapter 12. eligibility traces

a dutch-style eligibility trace. it is initialized to z0 = x0 and then updated according to

ftft   1        fk+1xk,

1     t < t

.
=

zt

=

t(cid:88)k=0
t   1(cid:88)k=0
t   1(cid:88)k=0

ftft   1        fk+1xk + xt

= ft

ft   1ft   2        fk+1xk + xt

= ftzt   1 + xt

=(cid:0)i       xtx(cid:62)t(cid:1) zt   1 + xt
= zt   1       xtx(cid:62)t zt   1 + xt
= zt   1       (cid:0)z(cid:62)t   1xt(cid:1) xt + xt
= zt   1 +(cid:0)1       z(cid:62)t   1xt(cid:1) xt,

which is the dutch trace for the case of      = 1 (cf. eq. 12.16). the at auxilary vector is initialized to
a0 = w0 and then updated according to

at

.
= ftft   1        f0w0 = ftat   1 = at   1       xtx(cid:62)t at   1,

1     t < t.

(12.20)

the auxiliary vectors, at and zt, are updated on each time step t < t and then, at time t when g
is observed, they are used in (12.19) to compute wt . in this way we achieve exactly the same    nal
result as the mc/lms algorithm with poor computational properties (12.18), but with an incremental
algorithm whose time and memory complexity per step is o(d). this is surprising and intriguing
because the notion of an eligibility trace (and the dutch trace in particular) has arisen in a setting
without temporal-di   erence (td) learning (in contrast to van seijen and sutton, 2014).
it seems
eligibility traces are not speci   c to td learning at all; they are more fundamental than that. the
need for eligibility traces seems to arise whenever one tries to learn long-term predictions in an e   cient
manner.

12.7 sarsa(  )

very few changes in the ideas already presented in this chapter are required in order to extend eligibility-
traces to action-value methods. to learn approximate action values,   q(s, a, w), rather than approximate
state values,   v(s,w), we need to use the action-value form of the n-step return, from chapter 10:

gt:t+n

.
= rt+1 +        +   n   1rt+n +   n   q(st+n, at+n, wt+n   1),

(10.4)

for all n and t such that n     1 and 0     t < t     n. using this, we can form the action-value form of the
truncated   -return, which is otherwise identical to the state-value form (12.9). the action-value form
of the o   -line   -return algorithm (12.4) simply uses   q rather than   v:

wt+1

.

= wt +   (cid:104)g  

t       q(st, atwt)(cid:105)     q(st, at, wt),

t = 0, . . . , t     1,

(12.21)

.
= g  

where g  
t:   . the compound backup diagram for this forward view is shown in figure 12.9. notice
t
the similarity to the diagram of the td(  ) algorithm (figure 12.1). the    rst update looks ahead one
full step, to the next state   action pair, the second looks ahead two steps, to the second state   action

12.7. sarsa(  )

249

figure 12.9: sarsa(  )   s backup diagram. compare with figure 12.1.

pair, and so on. a    nal update is based on the complete return. the weighting of each n-step update
in the   -return is just as in td(  ) and the   -return algorithm (12.3).

the temporal-di   erence method for action values, known as sarsa(  ), approximates this forward

view. it has the same update rule as given earlier for td(  ):

wt+1

.
= wt +     t zt,

except, naturally, using the action-value form of the td error:

  t

.
= rt+1 +      q(st+1, at+1, wt)       q(st, at, wt),

and the action-value form of the eligibility trace:

.
z   1
= 0,
.
=     zt   1 +      q(st, at, wt),
zt

0     t     t

(12.7)

(12.22)

(12.23)

(or, alternatively, the replacing trace given by (12.17)). complete pseudocode for sarsa(  ) with linear
function approximation, binary features, and either accumulating or replacing traces is given in the box
on the next page. this pseudocode highlights a few optimizations possible in the special case of binary
features (features are either active (=1) or inactive (=0).

1  (1  ) (1  ) 2 t t 1            statat+1at 1st+1rt+1strt      st+2rt+2at+2x=1sarsa( )250

chapter 12. eligibility traces

sarsa(  ) with binary features and linear function approximation
for estimating   q     q   or   q     q   
input: a function f(s, a) returning the set of (indices of) active features for s, a
input: a policy    to be evaluated, if any

initialize parameter vector w = (w1, . . . , wn) arbitrarily (e.g., w = 0)
loop for each episode:

initialize s
choose a       (  |s) or   -greedy according to   q(s,  , w)
z     0
loop for each step of episode:

take action a, observe r, s(cid:48)
       r
loop for i in f(s, a):

              wi
zi     zi + 1
or zi     1
w     w +      z
go to next episode

if s(cid:48) is terminal then:

(accumulating traces)
(replacing traces)

choose a(cid:48)       (  |s(cid:48)) or near greedily       q(s(cid:48),  , w)
loop for i in f(s(cid:48), a(cid:48)):           +   wi
w     w +      z
z         z
s     s(cid:48); a     a(cid:48)

example 12.1: traces in gridworld the use of eligibility traces can substantially increase the
e   ciency of control algorithms over one-step methods and even over n-step methods. the reason for
this is illustrated by the gridworld example below.

the    rst panel shows the path taken by an agent in a single episode. the initial estimated values were
zero, and all rewards were zero except for a positive reward at the goal location marked by g. the arrows
in the other panels show, for various algorithms, which action-values would be increased, and by how
much, upon reaching the goal. a one-step method would increment only the last action value, whereas an
n-step method would equally increment the last n action   s values, and an eligibility trace method would
update all the action values up to the beginning of the episode to di   erent degrees, fading with recency.
the fading strategy is often the best tradeo   , strongly learning how to reach the goal from the right, yet
not as strongly learning the roundabout path to the goal from the left that was taken in this episode.

exercise 12.6 modi   y the pseudocode for sarsa(  ) to use dutch traces (12.16) without the other
(cid:3)
features of a true online algorithm. assume linear function approximation and binary features.

path takenaction values increasedby one-step sarsaaction values increasedby sarsa(!) with !=0.9gggpath takenaction values increasedby one-step sarsaaction values increasedby sarsa(!) with !=0.9by 10-step sarsaggg12.7. sarsa(  )

251

example 12.2: sarsa(  ) on mountain car figure 12.10 (left) shows results with sarsa(  ) on
the mountain car task introduced in example 10.1. the function approximation, action selection, and
environmental details were exactly as in chapter 10, and thus it is appropriate to numerically compare
these results with the chapter 10 results for n-step sarsa (right side of the    gure). the earlier results
varied the update length n whereas here for sarsa(  ) we vary the trace parameter   , which plays a
similar role. the fading-trace id64 strategy of sarsa(  ) appears to result in more e   cient
learning on this problem.

figure 12.10: early performance on the mountain car task of sarsa(  ) with replacing traces and n-step sarsa
(copied from figure 10.4) as a function of the step size,   .

there is also an action-value version of our ideal td method, the online   -return algorithm presented
in section 12.4. everything in that section goes through without change other than to use the action-
value form of the n-step return given at the beginning of this section. in the case of linear function
approximation, the ideal algorithm again has an exact, e   cient o(d) implementation, called true online
sarsa(  ). the analyses in sections 12.5 and 12.6 carry through without change other than to use state   
action feature vectors xt = x(st, at) instead of state feature vectors xt = x(st). the pseudocode for
this algorithm is given in the box on the next page. figure 12.11 compares the performance of various
versions of sarsa(  ) on the mountain car example.

22024026030000.511.5mountain carsteps per episodeaveraged over   rst 50 episodesand 100 runs280200180    number of tilings (8)     =.96  =.92  =.99  =.84  =.68  =022024026030000.511.528000.511.5n=1n=2n=4n=8n=16n=8n=4n=2n=16n=1  =.98    number of tilings (8)   sarsa(  ) with replacing tracesn-step sarsa  =.92  =.84252

chapter 12. eligibility traces

true online sarsa(  ) for estimating w(cid:62)x     q   or q   
input: a feature function x : s+    a     rd s.t. x(terminal,  ) = 0
input: the policy    to be evaluated, if any

initialize parameter w arbitrarily (e.g., w = 0)
loop for each episode:

(a scalar temporary variable)

initialize s
choose a       (  |s) or near greedily from s using w
x     x(s, a)
z     0
qold     0
loop for each step of episode:
| take action a, observe r, s(cid:48)
| choose a(cid:48)       (  |s(cid:48)) or near greedily from s(cid:48) using w
| x(cid:48)     x(s(cid:48), a(cid:48))
| q     w(cid:62)x
| q(cid:48)     w(cid:62)x(cid:48)
       r +   q(cid:48)     q
|
z         z +(cid:0)1           z(cid:62)x(cid:1) x
|
| w     w +   (   + q     qold)z       (q     qold)x
| qold     q(cid:48)
| x     x(cid:48)
| a     a(cid:48)
until s(cid:48) is terminal

figure 12.11: summary comparison of sarsa(  ) algorithms on the mountain car task. true online sarsa(  )
performed better than regular sarsa(  ) with both accumulating and replacing traces. also included is a version
of sarsa(  ) with replacing traces in which, on each time step, the traces for the state and the actions not selected
were set to zero.

trueonlinetd( )00.511.500.20.40.60.81step   sizerms           error         td(  ), accumulating traces     task 1   = 0   = 0.975   = 1   = 0.95   = 0.2   = 0.100.511.500.20.40.60.81step   sizerms           error         td(  ), replacing traces     task 1   = 0   = 100.511.500.20.40.60.81step   sizerms           error         true online td(  )     task 1   = 0   = 100.511.500.20.40.60.81step   sizerms           error         td(  ), accumulating traces     task 2   = 1   = 000.511.500.20.40.60.81step   sizerms          error         td(  ), replacing traces     task 200.511.500.20.40.60.81step   sizerms           error         true online td(  )     task 2   = 1   = 0figure2.rmserrorofstatevaluesattheendofeachepisode,averagedoverthe   rst10episodes,aswellas100independentruns,fordifferentvaluesof   and .figure4comparestrueonlinesarsa( )withthetraditionalsarsa( )implementationonthestandardmountaincartask(sutton&barto,1998)using10tilingsofeach10   10tiles.resultsareplottedfor =0.9and   =   0/10,for   0from0.2to2.0withstepsof0.2.clearing/noclearingreferstowhetherthetracevaluesofnon-selectedactionsaresetto0(clearing)ornot(noclearing),incaseofreplacingtraces.theresultssuggestthatthetrueonlineprincipleisalsoeffectiveinacontrolsetting.6.conclusionwepresentedforthe   rsttimeanonlineversionofthefor-wardviewwhichformsthetheoreticalandintuitivefoun-dationforthetd( )algorithm.inaddition,wehavepre-sentedanewvariantoftd( ),withthesamecompu-tationalcomplexityastheclassicalalgorithm,whichwecalltrueonlinetd( ).weprovedthattrueonlinetd( )matchesthenewonlineforwardviewexactly,incontrasttoclassicalonlinetd( ),whichonlyapproximatesitsforwardview.inaddition,wedemonstratedempiricallythattrueonlinetd( )outperformsconventionaltd( )onthreebenchmarkproblems.itseems,byadheringmoretrulytotheoriginalgoaloftd( )   matchinganintuitivelyclearforwardviewevenintheonlinecase   thatwehavefoundanewalgorithmthatsimplyimprovesontd( ).0.20.40.60.811.21.41.61.82   550   500   450   400   350   300   250   200   150  0return  sarsa(  ), replacing, clearingsarsa(  ), replacing, no clearingsarsa(  ), accumulatingtrue online sarsa(  )figure4.averagereturnover   rst20episodesonmountaincartaskfor =0.9anddifferent   0.resultsareaveragedover100independentruns.acknowledgementstheauthorsthankhadovanhasseltandrupammahmoodforextensivediscussionsleadingtothere   nementoftheseideas.thisworkwassupportedbygrantsfromalbertainnovates   technologyfuturesandthenationalscienceandengineeringresearchcouncilofcanada.mountain carreward per episodeaveraged over   rst 20 episodesand 100 runs    number of tilings (8)   true online sarsa(  )sarsa(  ) with replacing tracessarsa(  ) with replacing tracesand clearing the traces of other actionssarsa(  ) with accumulating traces12.8. variable    and   

253

12.8 variable    and   

we are starting now to reach the end of our development of fundamental td learning algorithms.
to present the    nal algorithms in their most general forms, it is useful to generalize the degree of
id64 and discounting beyond constant parameters to functions potentially dependent on the
state and action. that is, each time step will have a di   erent    and   , denoted   t and   t. we change
notation now so that    : s    a     [0, 1] is now a whole function from states and actions to the unit
.
=   (st, at), and similarly,    : s     [0, 1] is a function from states to the unit
interval such that   t
.
interval such that   t
=   (st).

the latter generalization, to state-dependent discounting, is particularly signi   cant because it changes
the return, the fundamental random variable whose expectation we seek to estimate. now the return
is de   ned more generally as

gt

.
= rt+1 +   t+1gt+1
= rt+1 +   t+1rt+2 +   t+1  t+2rt+3 +   t+1  t+2  t+3rt+4 +       

   (cid:88)k=t

k(cid:89)i=t+1

=

rk+1

  i,

(12.24)

where, to assure the sums are    nite, we require that(cid:81)   k=t   k = 0 with id203 one for all t. one

convenient aspect of this de   nition is that it allows us to dispense with episodes, start and terminal
states, and t as special cases and quantities. a terminal state just becomes a state at which   (s) = 0
and which transitions to the start state. in that way (and by choosing   (  ) as a constant function) we
can recover the classical episodic setting as a special case. state dependent discounting includes other
prediction cases such as soft termination, when we seek to predict a quantity that becomes complete
but does not alter the    ow of the markov process. discounted returns themselves can be thought of as
such a quantity, and state dependent discounting is a deep uni   cation of the episodic and discounted-
continuing cases. (the undiscounted-continuing case still needs some special treatment.)

the generalization to variable id64 is not a change in the problem, like discounting, but a
change in the solution strategy. the generalization a   ects the   -returns for states and actions. the
new state-based   -return can be written recursively as

g  s
t

.

= rt+1 +   t+1(cid:0)(1       t+1)  v(st+1,wt) +   t+1g  s
t+1(cid:1) ,

where now we have added the    s    to the superscript    to remind us that this is a return that bootstraps
from state values, distinguishing it from returns that bootstrap from action values, which we present
below with    a    in the superscript. this equation says that the   -return is the    rst reward, undiscounted
and una   ected by id64, plus possibly a second term to the extent that we are not discounting
at the next state (that is, according to   t+1; recall that this is zero if the next state is terminal). to the
extent that we aren   t terminating at the next state, we have a second term which is itself divided into
two cases depending on the degree of id64 in the state. to the extent we are id64,
this term is the estimated value at the state, whereas, to the extent that we not id64, the
term is the   -return for the next time step. the action-based   -return is either the sarsa form

(12.25)

or the expected sarsa form,

g  a

t

g  a

t

.

.

= rt+1 +   t+1(cid:16)(1       t+1)  q(st+1, at+1, wt) +   t+1g  a
t+1(cid:17),
= rt+1 +   t+1(cid:16)(1       t+1)   qt+1 +   t+1g  a
t+1(cid:17),
=(cid:88)a

  (a|st)  q(st, a, wt   1).

.

where

  qt

(12.26)

(12.27)

(12.28)

254

chapter 12. eligibility traces

exercise 12.7 generalize the three recursive equations above to their truncated versions, de   ning
(cid:3)
g  s

t:h and g  a
t:h.

12.9 o   -policy eligibility traces

the    nal step is to incorporate importance sampling. unlike in the case of n-step methods, for full
non-truncated   -returns one does not have a practical option in which the importance sampling is done
outside the target return. instead, we move directly to the id64 generalization of per-reward
importance sampling (section 7.4). in the state case, our    nal de   nition of the   -return generalizes
(12.25), after the model of (7.10), to

g  s
t

.

=   t(cid:16)rt+1 +   t+1(cid:0)(1       t+1)  v(st+1,wt) +   t+1g  s

t+1(cid:1)(cid:17) + (1       t)  v(st,wt)

where   t =   (at|st)
b(at|st) is the usual single-step importance sampling ratio. much like the other returns we
have seen in this book, the truncated version of this return can be approximated simply in terms of
sums of the state-based td error,

(12.29)

  s
t = rt+1 +   t+1  v(st+1,wt)       v(st,wt),

as

g  s
t       v(st,wt) +   t

  s
k

   (cid:88)k=t

k(cid:89)i=t+1

  i  i  i

(12.30)

(12.31)

with the approximation becoming exact if the approximate value function does not change.

exercise 12.8 prove that (12.31) becomes exact if the value function does not change. to save writing,
(cid:3)
consider the case of t = 0, and use the notation vk
exercise 12.9 the truncated version of the general o   -policy return is denoted g  s
t:h. guess the correct
(cid:3)
equation, based on (12.31).

.
=   v(sk,w).

the above form of the   -return (12.31) is convenient to use in a forward-view update,

wt+1 = wt +   (cid:0)g  s
    wt +     t(cid:32)    (cid:88)k=t

t       v(st,wt)(cid:1)     v(st,wt)

  i  i  i(cid:33)     v(st,wt),

  s
k

k(cid:89)i=t+1

which to the experienced eye looks like an eligibility-based td update   the product is like an eligibility
trace and it is multiplied by td errors. but this is just one time step of a forward view. the relationship
that we are looking for is that the forward-view update, summed over time, is approximately equal to
a backward-view update, summed over time (this relationship is only approximate because again we
ignore changes in the value function). the sum of the forward-view update over time is

   (cid:88)t=1

(wt+1     wt)    

=

=

   (cid:88)k=t
k(cid:88)t=1

    s
k

   (cid:88)t=1
   (cid:88)k=1
   (cid:88)k=1

    t  s

k     v(st,wt)

    t     v(st,wt)  s

k(cid:88)t=1

  t     v(st,wt)

k

  i  i  i

  i  i  i

k(cid:89)i=t+1
k(cid:89)i=t+1
(using the summation rule: (cid:80)y
k(cid:89)i=t+1

  i  i  i,

t=x(cid:80)y

k=t =(cid:80)y

k=x(cid:80)k

t=x)

12.9. off-policy eligibility traces

255

which would be in the form of the sum of a backward-view td update if the entire expression from the
second sum left could be written and updated incrementally as an eligibility trace, which we now show
can be done. that is, we show that if this expression was the trace at time k, then we could update it
from its value at time k     1 by:

  i  i  i

  i  i  i +   k     v(sk,wk)

  i  i  i

+   k     v(sk,wk)

(cid:125)

zk =

=

k(cid:88)t=1
k   1(cid:88)t=1

  t     v(st,wt)

  t     v(st,wt)

k(cid:89)i=t+1
k(cid:89)i=t+1

zk   1

=   k  k  k

  t     v(st,wt)

k   1(cid:88)t=1
(cid:124)

k   1(cid:89)i=t+1
=   k(cid:0)  k  kzk   1 +      v(sk,wk)(cid:1),
=   t(cid:0)  t  tzt   1 +      v(st,wt)(cid:1),

(cid:123)(cid:122)

.

zt

which, changing the index from k to t, is the general accumulating trace update for state values:

this eligibility trace, together with the usual semi-gradient parameter-update rule for td(  ) (12.7),
forms a general td(  ) algorithm that can be applied to either on-policy or o   -policy data.
in the
on-policy case, the algorithm is exactly td(  ) because   t is alway 1 and (12.32) becomes the usual
accumulating trace (12.5) (extended to variable    and   ). in the o   -policy case, the algorithm often
works well but, as an semi-gradient method, is not guaranteed to be stable. in the next few sections
we will consider extensions of it that do guarantee stability.

a very similar series of steps can be followed to derive the o   -policy eligibility traces for action-value
methods and corresponding general sarsa(  ) algorithms. one could start with either recursive form
for the general action-based   -return, (12.26) or (12.27), but the former works out to be simpler. we
extend (12.26) to the o   -policy case after the model of (7.11) to produce

g  a

t

.

= rt+1 +   t+1(cid:16)(1       t+1)(cid:0)  t+1   q(st+1, at+1, wt) + (1       t+1)   qt+1(cid:1)
= rt+1 +   t+1(cid:16)(1       t+1)  t+1   q(st+1, at+1, wt) +   t+1  t+1g  a

t+1 + (1       t+1)   qt+1(cid:1)(cid:17)

+   t+1(cid:0)  t+1g  a

t+1 + (1       t+1)   qt+1)(cid:17)

(12.33)

where   qt+1 is as given by (12.28). again the   -return can be written approximately as the sum of td
errors,

g  a
t       q(st, at, wt) +

  a
k

   (cid:88)k=t

k(cid:89)i=t+1

  i  i  i,

using a novel form of the td error:

  a

t = rt+1 +   t+1(cid:16)  t+1   q(st+1, at+1, wt) + (1       t+1)   qt+1(cid:17)       q(st, at, wt).

as before, the approximation becomes exact if the approximate value function does not change.

exercise 12.10 prove that (12.34) becomes exact if the value function does not change. to save
writing, consider the case of t = 0, and use the notation qk =   q(sk, ak, w). hint: start by writing out
(cid:3)
  a
0 and g  a

0 , then g  a

0     q0.

(12.32)

(12.34)

(12.35)

256

chapter 12. eligibility traces

exercise 12.11 the truncated version of the general o   -policy return is denoted g  a
correct equation, based on (12.34).

t:h. guess the
(cid:3)
using steps entirely analogous to those for the state case, one can write a forward-view update based
on (12.34), transform the sum of the updates using the summation rule, and    nally derive the following
form for the eligibility trace for action values:

zt

.
=   t  t  tzt   1 +      q(st, at, wt).

(12.36)

this eligibility trace, together with the usual semi-gradient parameter-update rule (12.7), forms a
general sarsa(  ) algorithm that can be applied to either on-policy or o   -policy data. in the on-policy
case with constant    and    this algorithm is identical to the sarsa(  ) algorithm presented in section 12.7.
in the o   -policy case this algorithm is not stable unless combined with one of the methods presented
in the following sections.

exercise 12.12 show in detail the steps outlined above for deriving (12.36) from (12.34). start with
t , then follow similar steps as led to (12.32). (cid:3)
the update (12.21), substitute g  a
   exercise 12.13 show how similar steps can be followed starting from the expected sarsa form of the
action-based   -return (12.27) and (12.28) to derive the same eligibility trace algorithm as (12.36), but
with a di   erent td error:

from (12.33) for g  

t

  a
t = rt+1 +   t+1

  qt+1       q(st, at, wt) +   t+1  t+1  t+1(cid:0)  q(st+1, at+1, wt)       qt+1(cid:1). (cid:3)

at    = 1, these algorithms become closely related to corresponding monte carlo algorithms. one
might expect that an exact equivalence would hold for episodic problems and o   -line updating, but in
fact the relationship is subtler and slightly weaker than that. under these most favorable conditions still
there is not an episode by episode equivalence of updates, only of their expectations. this should not be
surprising as these method make irrevocable updates as a trajectory unfolds, whereas true monte carlo
methods would make no update for a trajectory if any action within it has zero id203 under the
target policy. in particular, all of these methods, even at    = 1, still bootstrap in the sense that their
targets depend on the current value estimates   its just that the dependence cancels out in expected
value. whether this is a good or bad property in practice is another question. recently methods have
been proposed that do achieve an exact equivalence (sutton, mahmood, precup and van hasselt, 2014).
these methods require an additional vector of    provisional weights    that keep track of updates which
have been made but may need to be retracted (or emphasized) depending on the actions taken later.
the state and state   action versions of these methods are called ptd(  ) and pq(  ) respectively, where
the    p    stands for provisional.

the practical consequences of all these new o   -policy methods have not yet been established. un-
doubtedly, issues of high variance will arise as they do in all o   -policy methods using importance
sampling (section 11.9).

if    < 1, then all these o   -policy algorithms involve id64 and the deadly triad applies (sec-
tion 11.3), meaning that they can be guaranteed stable only for the tabular case, for state aggregation,
and for other limited forms of function approximation. for linear and more-general forms of function
approximation the parameter vector may diverge to in   nity as in the examples in chapter 11. as
we discussed there, the challenge of o   -policy learning has two parts. o   -policy eligibility traces deal
e   ectively with the    rst part of the challenge, correcting for the expected value of the targets, but not
at all with the second part of the challenge, having to do with the distribution of updates. algorithmic
strategies for meeting the second part of the challenge of o   -policy learning with eligibility traces are
summarized in section 12.11.

exercise 12.14 what are the dutch-trace and replacing-trace versions of o   -policy eligibility traces
(cid:3)
for state-value and action-value methods?

12.10. watkins   s q(  ) to tree-backup(  )

257

12.10 watkins   s q(  ) to tree-backup(  )

several methods have been proposed over the years to extend id24 to eligibility traces. the
original is watkins   s q(  ), which decays its eligibility traces in the usual way as long as a greedy
action was taken, then cuts the traces to zero after the    rst non-greedy action. the backup diagram
for watkins   s q(  ) is shown in figure 12.12. in chapter 6, we uni   ed id24 and expected sarsa
in the o   -policy version of the latter, which includes id24 as a special case, and generalizes it
to arbitrary target policies, and in the previous section of this chapter we completed our treatment of
expected sarsa by generalizing it to o   -policy eligibility traces. in chapter 7, however, we distinguished
multi-step expected sarsa from multi-step tree backup, where the latter retained the property of not
using importance sampling.
it remains then to present the eligibility trace version of tree backup,
which we well call tree-backup(  ), or tb(  ) for short. this is arguably the true successor to q-
learning because it retains its appealing absence of importance sampling even though it can be applied
to o   -policy data.

the concept of tb(  ) is straightforward. as shown in its backup diagram in figure 12.13, the
tree-backup updates of each length (from section 7.5) are weighted in the usual way dependent on
the id64 parameter   . to get the detailed equations, with the right indexes on the general
id64 and discounting parameters, it is best to start with a recursive form (12.27) for the
  -return using action values, and then expand the id64 case of the target after the model of

figure 12.12: the backup diagram for watkins   s q(  ). the series of component updates ends either with the
end of the episode or with the    rst nongreedy action, whichever comes    rst.

1  (1  ) (1  ) 2 t t 1      statat+1st+1rt+1strt      st+2rt+2at+2x=1or            st+nrt+nfirstnon-greedyaction n 1watkins   sq( )258

chapter 12. eligibility traces

figure 12.13: the backup diagram for the    version of the tree backup algorithm.

(7.13):

g  a

t

.

= rt+1 +   t+1(cid:18)(1       t+1)   qt+1 +   t+1(cid:16)

(cid:88)a(cid:54)=at+1

  (a|st+1)  q(st+1, a, wt) +   (at+1|st+1)g  a

= rt+1 +   t+1(cid:18)   qt+1 +   t+1  (at+1|st+1)(cid:16)g  a

t+1       q(st+1, at+1, wt)(cid:17)(cid:19)

t+1(cid:17)(cid:19)

as per the usual pattern, it can also be written approximately (ignoring changes in the approximate
value function) as a sum of td errors,

g  a
t       q(st, at, wt) +

  a
k

   (cid:88)k=t

k(cid:89)i=t+1

  i  i  (ai|si),

using the expectation form of the action-based td error:

  a
t = rt+1 +   t+1

  qt+1       q(st, at, wt).

(12.37)

(12.38)

following the same steps as in the previous section, we arrive at a special eligibility trace update

involving the target-policy probabilities of the selected actions,

zt

.
=   t  t  (at|st)zt   1 +      q(st, at, wt).

(12.39)

this, together with the usual parameter-update rule (12.7), de   nes the tb(  ) algorithm. like all semi-
gradient algorithms, tb(  ) is not guaranteed to be stable when used with o   -policy data and with
a powerful function approximator. for that it would have to be combined with one of the methods
presented in the next section.

1  (1  ) (1  ) 2 t t 1      statat+1at 1st+1rt+1strt      st+2rt+2at+2x=1      treebackup( )st 112.11.

stable off-policy methods with traces

259

exercise 12.15 (programming) de asis (personal communication) has proposed a new algorithm
he calls    rigid tree backup    that combines (12.38) with (12.36) in the usual way (12.7). implement
this algorithm and compare it empirically with tb(  ) and general sarsa(  ) on the 19-state random
(cid:3)
walk.

12.11

stable o   -policy methods with traces

several methods using eligibility traces have been proposed that achieve guarantees of stability under
o   -policy training, and here we present four of the most important using this book   s standard notation,
including general id64 and discounting functions. all are based on either the gradient-td
or the emphatic-td ideas presented in sections 11.7 and 11.8. all the algorithms assume linear
function approximation, though extensions to nonlinear function approximation can also be found in
the literature.

gtd(  ) is the eligibility-trace algorithm analogous to tdc, the better of the two state-value
its goal is to learn a parameter wt
its

gradient-td prediction algorithms discussed in section 11.7.
such that   v(s,w)
update is

.
= w(cid:62)t x(s)     v  (s) even from data that is due to following another policy b.
t zt         t+1(1       t+1)(cid:0)z(cid:62)t vt(cid:1) xt+1,

t , zt, and   t de   ned in the usual ways for state values (12.30) (12.32) (11.1), and

.
= wt +     s

with   s

(12.40)

wt+1

.
=     s

t zt       (cid:0)v(cid:62)t xt(cid:1) xt,

vt+1

(12.41)
where, as in section 11.7, v     rd is a vector of the same dimension as w, initialized to v0 = 0, and
   > 0 is a second step-size parameter.

gq(  ) is the gradient-td algorithm for action values with eligibility traces. its goal is to learn a
.
= w(cid:62)t x(s, a)     q  (s, a) from o   -policy data. if the target policy
parameter wt such that   q(s, a, wt)
is   -greedy, or otherwise biased toward the greedy policy for   q, then gq(  ) can be used as a control
algorithm. its update is

wt+1

.
= wt +     a

t zt         t+1(1       t+1)(cid:0)z(cid:62)t vt(cid:1)   xt+1,

where   xt is the average feature vector for st under the target policy,

.

  (a|st)x(st, a),

=(cid:88)a
.
= rt+1 +   t+1w(cid:62)t   xt+1     w(cid:62)t xt,

  xt

  a
t

  a
t is the expectation form of the td error, which can be written,

(12.42)

(12.43)

(12.44)

zt is de   ned in the usual ways for action values (12.36), and the rest is as in gtd(  ), including the
update for vt (12.41).

htd(  ) is a hybrid state-value algorithm combining aspects of gtd(  ) and td(  ). its most ap-
pealing feature is that it is a strict generalization of td(  ) to o   -policy learning, meaning that if the
behavior policy happens to be the same as the target policy, then htd(  ) becomes the same as td(  ),
which is not true for gtd(  ). this is appealing because td(  ) is often faster than gtd(  ) when both
algorithms converge, and td(  ) requires setting only a single step size. htd(  ) is de   ned by

wt+1
vt+1
zt
zb
t

.
= wt +     s
.
= vt +     s
.
.
=   t  tzb

t zt +   (cid:0)(zt     zb
t zt       (cid:16)zb
=   t(cid:0)  t  tzt   1 + xt(cid:1),

t   1 + xt,

t

t)(cid:62)vt(cid:1) (xt       t+1xt+1),

(cid:62)vt(cid:17) (xt       t+1xt+1),

.
= 0,

.
= 0,
.
= 0,

v0
z   1
zb
   1

(12.45)

260

chapter 12. eligibility traces

where    > 0 again is a second step-size parameter that becomes irrelevant in the on-policy case in which
b =   . in addition to the second set of weights, vt, htd(  ) also has a second set of eligibility traces,
zb
t. these are a conventional accumulating eligibility trace for the behavior policy and become equal to
zt if all the   t are 1, which causes the second term in the wt update to be zero and the overall update
to reduce to td(  ).

emphatic td(  ) is the extension of the one-step emphatic-td algorithm from section 11.8 to eligibil-
ity traces. the resultant algorithm retains strong o   -policy convergence guarantees while enabling any
degree of id64, albeit at the cost of high variance and potentially slow convergence. emphatic
td(  ) is de   ned by

  t+1
  t
zt
mt
ft

.
=   t +     tzt
.
= rt+1 +   t+1  (cid:62)t xt+1       (cid:62)t xt
=   t(cid:0)  t  tzt   1 + mtxt(cid:1),
.
.
=   t it + (1       t)ft
.
=   t   1  tft   1 + it,

with z   1

.
= 0

with f0

.
= i(s0),

(12.46)

where mt     0 is the general form of emphasis, ft     0 is termed the followon trace, and it     0 is
the interest, as described in section 11.8. note that mt, like   t, is not really an additional memory
variable. it can be removed from the algorithm by substituting its de   nition into the eligibility-trace
equation. pseudocode and software for the true online version of emphatic-td(  ) are available on the
web (sutton, 2015b).

in the on-policy case (  t = 0, for all t), emphatic-td(  ) is similar to conventional td(  ), but
still signi   cantly di   erent. in fact, whereas emphatic-td(  ) is guaranteed to converge for all state-
dependent    functions, td(  ) is not. td(  ) is guaranteed convergent only for all constant   . see yu   s
counterexample (ghiassian, ra   ee, and sutton, 2016).

12.12

implementation issues

it might at    rst appear that methods using eligibility traces are much more complex than one-step
methods. a naive implementation would require every state (or state   action pair) to update both its
value estimate and its eligibility trace on every time step. this would not be a problem for implemen-
tations on single-instruction, multiple-data, parallel computers or in plausible neural implementations,
but it is a problem for implementations on conventional serial computers. fortunately, for typical values
of    and    the eligibility traces of almost all states are almost always nearly zero; only those that have
recently been visited will have traces signi   cantly greater than zero. in practice, only these few states
need to be updated to closely approximate these algorithms.

in practice, then, implementations on conventional computers may keep track of and update only
the few states with nonzero traces. using this trick, the computational expense of using traces is
typically just a few times that of a one-step method. the exact multiple of course depends on   
and    and on the expense of the other computations. note that the tabular case is in some sense
the worst case for the computational complexity of eligibility traces. when function approximation is
used, the computational advantages of not using traces generally decrease. for example, if arti   cial
neural networks and id26 are used, then eligibility traces generally cause only a doubling
of the required memory and computation per step. truncated   -return methods (section 12.3) can
be computationally e   cient on conventional computers though they always require some additional
memory.

12.13. conclusions

261

12.13 conclusions

eligibility traces in conjunction with td errors provide an e   cient, incremental way of shifting and
choosing between monte carlo and td methods. the atomic multi-step methods of chapter 7 also
enabled this, but eligibility trace methods are more general, often faster to learn, and o   er di   erent
computational complexity tradeo   s. this chapter has o   ered an introduction to the elegant, emerging
theoretical understanding of eligibility traces for on- and o   -policy learning and for variable bootstrap-
ping and discounting. one aspect of this elegant theory is true online methods, which exactly reproduce
the behavior of expensive ideal methods while retaining the computational congeniality of conventional
td methods. another aspect is the possibility of derivations that automatically convert from intuitive
forward-view methods to more e   cient incremental backward-view algorithms. we illustrated this gen-
eral idea in a derivation that started with a classical, expensive monte carlo algorithm and ended with
a cheap incremental non-td implementation using the same novel eligibility trace used in true online
td methods.

as we mentioned in chapter 5, monte carlo methods may have advantages in non-markov tasks
because they do not bootstrap. because eligibility traces make td methods more like monte carlo
methods, they also can have advantages in these cases. if one wants to use td methods because of
their other advantages, but the task is at least partially non-markov, then the use of an eligibility trace
method is indicated. eligibility traces are the    rst line of defense against both long-delayed rewards
and non-markov tasks.

by adjusting   , we can place eligibility trace methods anywhere along a continuum from monte
carlo to one-step td methods. where shall we place them? we do not yet have a good theoretical
answer to this question, but a clear empirical answer appears to be emerging. on tasks with many
steps per episode, or many steps within the half-life of discounting, it appears signi   cantly better to use
eligibility traces than not to (e.g., see figure 12.14). on the other hand, if the traces are so long as to
produce a pure monte carlo method, or nearly so, then performance degrades sharply. an intermediate
mixture appears to be the best choice. eligibility traces should be used to bring us toward monte carlo
methods, but not all the way there. in the future it may be possible to vary the trade-o    between td
and monte carlo methods more    nely by using variable   , but at present it is not clear how this can
be done reliably and usefully.

methods using eligibility traces require more computation than one-step methods, but in return they
o   er signi   cantly faster learning, particularly when rewards are delayed by many steps. thus it often
makes sense to use eligibility traces when data are scarce and cannot be repeatedly processed, as is
often the case in on-line applications. on the other hand, in o   -line applications in which data can be
generated cheaply, perhaps from an inexpensive simulation, then it often does not pay to use eligibility
traces. in these cases the objective is not to get more out of a limited amount of data, but simply to
process as much data as possible as quickly as possible. in these cases the speedup per datum due to
traces is typically not worth their computational cost, and one-step methods are favored.

   exercise 12.16 how might double expected sarsa be extended to eligibility traces?

(cid:3)

262

chapter 12. eligibility traces

figure 12.14: the e   ect of    on id23 performance in four di   erent test problems. in all cases,
lower numbers represent better performance. the two left panels are applications to simple continuous-state
control tasks using the sarsa(  ) algorithm and tile coding, with either replacing or accumulating traces (sutton,
1996). the upper-right panel is for policy evaluation on a random walk task using td(  ) (singh and sutton,
1996). the lower right panel is unpublished data for the pole-balancing task (example 3.4) from an earlier
study (sutton, 1984).

bibliographical and historical remarks

eligibility traces came into id23 via the fecund ideas of klopf (1972). our use of
eligibility traces is based on klopf   s work (sutton, 1978a, 1978b, 1978c; barto and sutton, 1981a,
1981b; sutton and barto, 1981a; barto, sutton, and anderson, 1983; sutton, 1984). we may have been
the    rst to use the term    eligibility trace    (sutton and barto, 1981). the idea that stimuli produce
aftere   ects in the nervous system that are important for learning is very old. see chapter 14. some of
the earliest uses of eligibility traces were in the actor   critic methods discussed in chapter 13 (barto,
sutton, and anderson, 1983; sutton, 1984).

12.1

the   -return and its error-reduction properties were introduced by watkins (1989) and further
developed by jaakkola, jordan and singh (1994). the random walk results in this and sub-
sequent sections are new to this text, as are the terms    forward view    and    backward view.   
the notion of a   -return algorithm was introduced in the    rst edition of this text. the more
re   ned treatment presented here was developed in conjunction with harm van seijen (e.g., van
seijen and sutton, 2014).

12.2

td(  ) with accumulating traces was introduced by sutton (1988, 1984). convergence in the
mean was proved by dayan (1992), and with id203 1 by many researchers, including
peng (1993), dayan and sejnowski (1994), and tsitsiklis (1994) and gurvits, lin, and hanson

accumulatingtraces  0.20.30.40.500.20.40.60.81!random walk50100150200250300failures per100,000 steps00.20.40.60.81!cart and pole400450500550600650700 steps perepisode00.20.40.60.81!mountain carreplacingtraces150160170180190200210220230240cost perepisode00.20.40.60.81!puddle worldreplacingtracesaccumulatingtraces replacingtracesaccumulatingtracesrms error12.13. conclusions

263

(1994). the bound on the error of the asymptotic   -dependent solution of linear td(  ) is due
to tsitsiklis and van roy (1997).

12.3-5 truncated td methods were developed by cichosz (1995) and van seijen (2016). true online
td(  ) and the other ideas presented in these sections are primarily due to work of van seijen
(van seijen and sutton, 2014; van seijen et al., 2016) replacing traces are due to singh and
sutton (1996).

12.6

the material in this section is from van hasselt and sutton (2015).

12.7

sarsa(  ) with accumulating traces was    rst explored as a control method by rummery and
niranjan (1994; rummery, 1995). true online sarsa(  ) was introduced by van seijen and
sutton (2014). the algorithm on page 252 was adapted from van seijen et al. (2016). the
mountain car results were made new for this text, except for figure 12.11 which is adapted
from van seijen and sutton (2014).

12.8

perhaps the    rst published discussion of variable    was by watkins (1989), who pointed out
that the cutting o    of the update sequence (figure 12.12) in his q(  ) when a nongreedy action
was selected could be implemented by temporarily setting    to 0.

variable    was introduced in the    rst edition of this text. the roots of variable    are in
the work on options (sutton, precup, and singh, 1999) and its precursors (sutton, 1995a),
becoming explicit in the gq(  ) paper (maei and sutton, 2010), which also introduced some of
these recursive forms for the   -returns.

a di   erent notion of variable    has been developed by yu (2012).

12.9

o   -policy eligibility traces were introduced by precup et al. (2000, 2001), then further developed
by bertsekas and yu (2009), maei (2011; maei and sutton, 2010), yu (2012), and by sutton,
mahmood, precup, and van hasselt (2014). the latter reference in particular gives a powerful
forward view for o   -policy td methods with general state-dependent    and   . the presentation
here seems to be new.

12.10 watkins   s q(  ) is due to watkins (1989). convergence has still not been proved for any control

method for 0 <    < 1. tree backup(  ) is due to precup, sutton, and singh (2000).

12.11 gtd(  ) is due to maei (2011). gq(  ) is due to maei and sutton (2010). htd(  ) is due
to white and white (2016) based on the one-step htd algorithm introduced by hackman
(2012). emphatic td(  ) was introduced by sutton, mahmood, and white (2016), who proved
its stability, then was proved to be convergent by yu (2015a,b), and developed further by
hallak, tamar, munos, and mannor (2016).

264

chapter 12. eligibility traces

chapter 13

id189

in this chapter we consider something new. so far in this book almost all the methods have learned the
values of actions and then selected actions based on their estimated action values1; their policies would
not even exist without the action-value estimates. in this chapter we consider methods that instead
learn a parameterized policy that can select actions without consulting a value function. a value function
may still be used to learn the policy parameter, but is not required for action selection. we use the

notation        rd(cid:48) for the policy   s parameter vector. thus we write   (a|s,   ) = pr{at = a | st = s,   t =   }

for the id203 that action a is taken at time t given that the environment is in state s at time t
with parameter   . if a method uses a learned value function as well, then the value function   s weight
vector is denoted w     rd as usual, as in   v(s,w).

in this chapter we consider methods for learning the policy parameter based on the gradient of some
performance measure j(  ) with respect to the policy parameter. these methods seek to maximize
performance, so their updates approximate gradient ascent in j:

  t+1 =   t +    (cid:92)   j(  t),

(13.1)

where (cid:92)   j(  t) is a stochastic estimate whose expectation approximates the gradient of the performance

measure with respect to its argument   t. all methods that follow this general schema we call policy
gradient methods, whether or not they also learn an approximate value function. methods that learn
approximations to both policy and value functions are often called actor   critic methods, where    actor   
is a reference to the learned policy, and    critic    refers to the learned value function, usually a state-value
function. first we treat the episodic case, in which performance is de   ned as the value of the start state
under the parameterized policy, before going on to consider the continuing case, in which performance is
de   ned as the average reward rate, as in section 10.3. in the end we are able to express the algorithms
for both cases in very similar terms.

13.1 policy approximation and its advantages

in id189, the policy can be parameterized in any way, as long as   (a|s,   ) is di   er-
entiable with respect to its parameters, that is, as long as        (a|s,   ) exists and is always    nite. in
practice, to ensure exploration we generally require that the policy never becomes deterministic (i.e.,
that   (a|s,   )     (0, 1), for all s, a,   ). in this section we introduce the most common parameterization
1the lone exception is the gradient bandit algorithms of section 2.8. in fact, that section goes through many of the
same steps, in the single-state bandit case, as we go through here for full mdps. reviewing that section would be good
preparation for fully understanding this chapter.

265

266

chapter 13. id189

for discrete action spaces and point out the advantages it o   ers over action-value methods. policy-
based methods also o   er useful ways of dealing with continuous action spaces, as we describe later in
section 13.7.

if the action space is discrete and not too large, then a natural kind of parameterization is to form
parameterized numerical preferences h(s, a,   )     r for each state   action pair. the actions with the
highest preferences in each state are given the highest probabilities of being selected, for example,
according to an exponential softmax distribution:

  (a|s,   ) =

exp(h(s, a,   ))

(cid:80)b exp(h(s, b,   ))

,

(13.2)

where exp(x) = ex, where e     2.71828 is the base of the natural logarithm. note that the denominator
here is just what is required so that the action probabilities in each state sum to one. the preferences
themselves can be parameterized arbitrarily. for example, they might be computed by a deep neural
network, where    is the vector of all the connection weights of the network (as in the alphago system
described in section 16.6). or the preferences could simply be linear in features,

h(s, a,   ) =   (cid:62)x(s, a),

(13.3)

using feature vectors x(s, a)     rd(cid:48) constructed by any of the methods described in chapter 9.

an immediate advantage of selecting actions according to the softmax in action preferences (13.2) is
that the approximate policy can approach a deterministic policy, whereas with   -greedy action selection
over action values there is always an    id203 of selecting a random action. of course, one could
select according to a softmax over action values, but this alone would not allow the policy to approach
a deterministic policy. instead, the action-value estimates would converge to their corresponding true
values, which would di   er by a    nite amount, translating to speci   c probabilities other than 0 and 1.
if the softmax included a temperature parameter, then the temperature could be reduced over time
to approach determinism, but in practice it would be di   cult to choose the reduction schedule, or
even the initial temperature, without more prior knowledge of the true action values than we would
like to assume. action preferences are di   erent because they do not approach speci   c values; instead
they are driven to produce the optimal stochastic policy. if the optimal policy is deterministic, then
the preferences of the optimal actions will be driven in   nitely higher than all suboptimal actions (if
permited by the parameterization).

in problems with signi   cant function approximation, the best approximate policy may be stochastic.
for example, in card games with imperfect information the optimal play is often to do two di   erent
things with speci   c probabilities, such as when blu   ng in poker. action-value methods have no natural
way of    nding stochastic optimal policies, whereas policy approximating methods can, as shown in
example 13.1. this is a second signi   cant advantage of policy-based methods.

exercise 13.1 use your knowledge of the gridworld and its dynamics to determine an exact symbolic
expression for the optimal id203 of selecting the right action in example 13.1.

perhaps the simplest advantage that policy parameterization may have over action-value parameter-
ization is that the policy may be a simpler function to approximate. problems vary in the complexity
of their policies and action-value functions. for some, the action-value function is simpler and thus
easier to approximate. for others, the policy is simpler. in the latter case a policy-based method will
typically learn faster and yield a superior asymptotic policy (as seems to be the case with tetris; see
s  im  sek, alg  orta, and kothiyal, 2016).

finally, we note that the choice of policy parameterization is sometimes a good way of injecting prior
knowledge about the desired form of the policy into the id23 system. this is often
the most important reason for using a policy-based learning method.

13.1. policy approximation and its advantages

267

example 13.1 short corridor with switched actions

consider the small corridor gridworld shown inset in the graph below. the reward is    1 per
step, as usual. in each of the three nonterminal states there are only two actions, right and left.
these actions have their usual consequences in the    rst and third states (left causes no movement
in the    rst state), but in the second state they are reversed, so that right moves to the left and
left moves to the right. the problem is di   cult because all the states appear identical under the
function approximation. in particular, we de   ne x(s, right) = [1, 0](cid:62) and x(s, left) = [0, 1](cid:62), for
all s. an action-value method with   -greedy action selection is forced to choose between just
two policies: choosing right with high id203 1       /2 on all steps or choosing left with the
same high id203 on all time steps. if    = 0.1, then these two policies achieve a value (at
the start state) of less than    44 and    82, respectively, as shown in the graph. a method can
do signi   cantly better if it can learn a speci   c id203 with which to select right. the best
id203 is about 0.59, which achieves a value of about    11.6.

id203 of right action-11.60.10.2-20-40-60-80-1000.30.400.60.70.80.90.51 -greedy left  -greedy right optimalstochasticpolicy j(   )=v      (s)gs268

chapter 13. id189

13.2 the policy gradient theorem

in addition to the practical advantages of policy parameterization over   -greedy action selection, there
is also an important theoretical advantage. with continuous policy parameterization the action proba-
bilities change smoothly as a function of the learned parameter, whereas in   -greedy selection the action
probabilities may change dramatically for an arbitrarily small change in the estimated action values,
if that change results in a di   erent action having the maximal value. largely because of this stronger
convergence guarantees are available for policy-gradient methods than for action-value methods.
in
particular, it is the continuity of the policy dependence on the parameters that enables policy-gradient
methods to approximate gradient ascent (13.1).

the episodic and continuing cases de   ne the performance measure, j(  ), di   erently and thus have to
be treated separately to some extent. nevertheless, we will try to present both cases uniformly, and we
develop a notation so that the major theoretical results can be decribed with a single set of equations.

in this section we treat the episodic case, for which we de   ne the performance measure as the value
of the start state of the episode. we can simplify the notation without losing any meaningful generality
by assuming that every episode starts in some particular (non-random) state s0. then, in the episodic
case we de   ne performance as

j(  )

.
= v     (s0),

(13.4)

where v     is the true value function for     , the policy determined by   . from here on in our discussion
we will assume no discounting (   = 1) for the episodic case, although for completeness we do include
the possibility of discounting in the boxed algorithms.

with function approximation, it may seem challenging to change the policy parameter in a way that
ensures improvement. the problem is that performance depends on both the action selections and the
distribution of states in which those selections are made, and that both of these are a   ected by the
policy parameter. given a state, the e   ect of the policy parameter on the actions, and thus on reward,
can be computed in a relatively straightforward way from knowledge of the parameterization. but the
e   ect of the policy on the state distribution is a function of the environment and is typically unknown.
how can we estimate the performance gradient with respect to the policy parameter when the gradient
depends on the unknown e   ect of policy changes on the state distribution?

fortunately, there is an excellent theoretical answer to this challenge in the form of the policy gradient
theorem, which provides us an analytic expression for the gradient of performance with respect to the
policy parameter (which is what we need to approximate for gradient ascent (13.1)) that does not
involve the derivative of the state distribution. the policy gradient theorem establishes that

   j(  )    (cid:88)s

  (s)(cid:88)a

q  (s, a)       (a|s,   ),

(13.5)

where the gradients are column vectors of partial derivatives with respect to the components of   , and
   denotes the policy corresponding to parameter vector   . the symbol     here means    proportional
to   . in the episodic case, the constant of proportionality is the average length of an episode, and in the
continuing case it is 1, so that the relationship is actually an equality. the distribution    here (as in
chapters 9 and 10) is the on-policy distribution under    (see page 163). the policy gradient theorem
is proved for the episodic case in the box on the next page.

13.2. the policy gradient theorem

269

proof of the policy gradient theorem (episodic case)

with just elementary calculus and re-arranging terms we can prove the policy gradient theorem
from    rst principles. to keep the notation simple, we leave it implicit in all cases that    is a
function of   , and all gradients are also implicitly with respect to   . first note that the gradient
of the state-value function can be written in terms of the action-value function as

   v  (s) =    (cid:34)(cid:88)a

  (a|s)q  (s, a)(cid:35) ,

for all s     s

=(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)   q  (s, a)(cid:105)
=(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)   (cid:88)s(cid:48),r
=(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)(cid:88)s(cid:48)
=(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)(cid:88)s(cid:48)

p(s(cid:48), r|s, a)(cid:0)r + v  (s(cid:48))(cid:1)(cid:105)
p(s(cid:48)|s, a)   v  (s(cid:48))(cid:105)
p(s(cid:48)|s, a)

(cid:88)a(cid:48) (cid:2)     (a(cid:48)|s(cid:48))q  (s(cid:48), a(cid:48)) +   (a(cid:48)|s(cid:48))(cid:88)s(cid:48)(cid:48)
   (cid:88)k=0
pr(s    x, k,   )(cid:88)a

     (a|x)q  (x, a),

=(cid:88)x   s

p(s(cid:48)(cid:48)|s(cid:48), a(cid:48))   v  (s(cid:48)(cid:48))(cid:3)(cid:105)

(exercise 3.15)

(product rule)

(eq. 3.4)

(unrolling)

(exercise 3.16 and equation 3.2)

after repeated unrolling, where pr(s    x, k,   ) is the id203 of transitioning from state s to
state x in k steps under policy   . it is then immediate that

   j(  ) =    v  (s0)

     (a|s)q  (s, a)

=(cid:88)s (cid:32)    (cid:88)k=0
pr(s0    s, k,   )(cid:33)(cid:88)a
  (s)(cid:88)a
=(cid:88)s
=(cid:32)(cid:88)s
  (s)(cid:33)(cid:88)s
  (s)(cid:88)a
   (cid:88)s

(cid:80)s   (s)(cid:88)a

     (a|s)q  (s, a).

  (s)

     (a|s)q  (s, a)

(box page 163)

     (a|s)q  (s, a)

q.e.d.

(eq. 9.3)

270

chapter 13. id189

13.3 reinforce: monte carlo policy gradient

we are now ready for our    rst policy-gradient learning algorithm. recall our overall strategy of stochas-
tic gradient ascent (13.1), which requires a way to obtain samples such that the expectation of the
sample gradient is proportional to the actual gradient of the performance measure as a function of the
parameter. the sample gradients need only be proportional to the gradient because any constant of
proportionality can be absorbed into the step size   , which is otherwise arbitrary. the policy gradient
theorem gives an exact expression proportional to the gradient; all that is needed is some way of sam-
pling whose expectation equals or approximates this expression. notice that the right-hand side of the
policy gradient theorem is a sum over states weighted by how often the states occur under the target
policy   ; if    is followed, then states will be encountered in these proportions. thus

q  (s, a)       (a|s,   ),

   j(  )    (cid:88)s
  (s)(cid:88)a
= e  (cid:34)(cid:88)a
q  (st, a)       (a|st,   )(cid:35) .

(13.5)

this is good progress, and we would like to carry it further and handle the action in the same way
(replacing a with the sample action at). the remaining part of the expectation above is a sum over
actions; if only each term were weighted by the id203 of selecting the actions, that is, according
to   (a|st,   ), then the replacement could be done. we can arrange for this by multiplying and dividing
by this id203. continuing from the previous equation, this gives

   j(  ) = e  (cid:34)(cid:88)a

  (a|st,   ) (cid:35)
  (a|st,   )q  (st, a)       (a|st,   )

  (at|st,   ) (cid:21)
= e  (cid:20)q  (st, at)       (at|st,   )
  (at|st,   ) (cid:21) ,
= e  (cid:20)gt        (at|st,   )

(replacing a by the sample at       )
(because e  [gt|st, at] = q  (st, at))

where gt is the return as usual. the    nal expression in the brackets is exactly what is needed, a
quantity that can be sampled on each time step whose expectation is equal to the gradient. using this
sample to instantiate our generic stochastic gradient ascent algorithm (13.1), yields the update

  t+1

=   t +   gt        (at|st,   t)
.
  (at|st,   t)

.

(13.6)

we call this algorithm reinforce (after williams, 1992). its update has an intuitive appeal. each
increment is proportional to the product of a return gt and a vector, the gradient of the id203
of taking the action actually taken divided by the id203 of taking that action. the vector is the
direction in parameter space that most increases the id203 of repeating the action at on future
visits to state st. the update increases the parameter vector in this direction proportional to the
return, and inversely proportional to the action id203. the former makes sense because it causes
the parameter to move most in the directions that favor actions that yield the highest return. the latter
makes sense because otherwise actions that are selected frequently are at an advantage (the updates
will be more often in their direction) and might win out even if they do not yield the highest return.

note that reinforce uses the complete return from time t, which includes all future rewards up
until the end of the episode. in this sense reinforce is a monte carlo algorithm and is well de   ned
only for the episodic case with all updates made in retrospect after the episode is completed (like the
monte carlo algorithms in chapter 5). this is shown explicitly in the boxed pseudocode below.

13.3. reinforce: monte carlo policy gradient

271

reinforce, a monte-carlo policy-gradient method (episodic)

input: a di   erentiable policy parameterization   (a|s,   )
initialize policy parameter        rd(cid:48)
repeat forever:

generate an episode s0, a0, r1, . . . , st   1, at   1, rt , following   (  |  ,   )
for each step of the episode t = 0, . . . , t     1:

g     return from step t
          +     t g      ln   (at|st,   )

figure 13.1 shows the performance of reinforce, averaged over 100 runs, on the gridworld from

example 13.1.

the vector         (at|st,  t)
  (at|st,  t)

in the reinforce update is the only place the policy parameterization
appears in the algorithm. this vector has been given several names and notations in the literature;
we will refer to it simply as the eligibility vector. the eligibility vector is often written in the compact
form       ln   (at|st,   t), using the identity     ln x =    x
x . this form is used in all the boxed pseudocode
in this chapter. in earlier examples in this chapter we considered exponential softmax policies (13.2)
with linear action preferences (13.3). for this parameterization, the eligibility vector is

      ln   (a|s,   ) = x(s, a)    (cid:88)b

  (b|s,   )x(s, b).

(13.7)

as a stochastic gradient method, reinforce has good theoretical convergence properties. by
construction, the expected update over an episode is in the same direction as the performance gradient.2
this assures an improvement in expected performance for su   ciently small   , and convergence to a

2technically, this is only true if each episode   s updates are done o   -line, meaning they are accumulated on the side
during the episode and only used to change    by their sum at the episode   s end. however, this would probably be a
worse algorithm in practice, and its desirable theoretical properties would probably be shared by the algorithm as given
(although this has not been proved).

figure 13.1: reinforce on the short-corridor gridworld (example 13.1). with a good step size, the total
reward per episode approaches the optimal value of the start state.

   =2 13   =2 12episode10008006004002001-80-90-60-40-20-10total rewardper episodeg0v   (s0)   =2 14272

chapter 13. id189

local optimum under standard stochastic approximation conditions for decreasing   . however, as a
monte carlo method reinforce may be of high variance and thus produce slow learning.

exercise 13.2 prove (13.7) using the de   nitions and elementary calculus.

(cid:3)

13.4 reinforce with baseline

the policy gradient theorem (13.5) can be generalized to include a comparison of the action value to
an arbitrary baseline b(s):

   j(  )    (cid:88)s

  (s)(cid:88)a (cid:16)q  (s, a)     b(s)(cid:17)       (a|s,   ).

(13.8)

the baseline can be any function, even a random variable, as long as it does not vary with a; the
equation remains valid because the subtracted quantity is zero:

(cid:88)a

b(s)       (a|s,   ) = b(s)     (cid:88)a

  (a|s,   ) = b(s)     1 = 0.

the policy gradient theorem with baseline (13.8) can be used to derive an update rule using similar
steps as in the previous section. the update rule that we end up with is a new version of reinforce
that includes a general baseline:

  t+1

.

=   t +   (cid:16)gt     b(st)(cid:17)       (at|st,   t)

  (at|st,   t)

.

(13.9)

because the baseline could be uniformly zero, this update is a strict generalization of reinforce. in
general, the baseline leaves the expected value of the update unchanged, but it can have a large e   ect
on its variance. for example, we saw in section 2.8 that an analogous baseline can signi   cantly reduce
the variance (and thus speed the learning) of gradient bandit algorithms. in the bandit algorithms the
baseline was just a number (the average of the rewards seen so far), but for mdps the baseline should
vary with state. in some states all actions have high values and we need a high baseline to di   erentiate
the higher valued actions from the less highly valued ones; in other states all actions will have low values
and a low baseline is appropriate.

one natural choice for the baseline is an estimate of the state value,   v(st,w), where w     rm is a
weight vector learned by one of the methods presented in previous chapters. because reinforce is
a monte carlo method for learning the policy parameter,   , it seems natural to also use a monte carlo
method to learn the state-value weights, w. a complete pseudocode algorithm for reinforce with
baseline is given in the box on the next page using such a learned state-value function as the baseline.
this algorithm has two step sizes, denoted      and   w (where      is the    in (13.9)). the step size
for values (here   w) is relatively easy; in the linear case we have rules of thumb for setting it, such as

  w = 0.1/e(cid:2)(cid:107)   w   v(st,w)(cid:107)2

  (cid:3). it is much less clear how to set the step size      for the policy parameters.

it depends on the range of variation of the rewards and on the policy parameterization.

13.5. actor   critic methods

273

reinforce with baseline (episodic)

input: a di   erentiable policy parameterization   (a|s,   )
input: a di   erentiable state-value parameterization   v(s,w)
parameters: step sizes      > 0,   w > 0
initialize policy parameter        rd(cid:48) and state-value weights w     rd

repeat forever:

generate an episode s0, a0, r1, . . . , st   1, at   1, rt , following   (  |  ,   )
for each step of the episode t = 0, . . . , t     1:

gt     return from step t
       gt       v(st,w)
w     w +   w   t      w   v(st,w)
          +        t         ln   (at|st,   )

figure 13.2 compares the behavior of reinforce with and without a baseline on the short-corridor
gridword (example 13.1). here the approximate state-value function used in the baseline is   v(s,w) = w.
that is, w is a single component, w. the step size used for the baseline was    = 0.1.

figure 13.2: adding a baseline to reinforce can make it learn much faster, as illustrated here on the
short-corridor gridworld (example 13.1). the step size used here for plain reinforce is that at which it
performs best (to the nearest power of two; see figure 13.1). each line is an average over 100 independent runs.

13.5 actor   critic methods

although the reinforce-with-baseline method learns both a policy and a state-value function, we
do not consider it to be an actor   critic method because its state-value function is used only as a
baseline, not as a critic. that is, it is not used for id64 (updating the value estimate for
a state from the estimated values of subsequent states), but only as a baseline for the state whose
estimate is being updated. this is a useful distinction, for only through id64 do we introduce

   =2 13episode10008006004002001-80-90-60-40-20-10total rewardper episodeg0v   (s0)reinforcereinforce with baseline   =2 9   =2 9, =2 6274

chapter 13. id189

bias and an asymptotic dependence on the quality of the function approximation. as we have seen,
the bias introduced through id64 and reliance on the state representation is often bene   cial
because it reduces variance and accelerates learning. reinforce with baseline is unbiased and
will converge asymptotically to a local minimum, but like all monte carlo methods it tends to learn
slowly (produce estimates of high variance) and to be inconvenient to implement online or for continuing
problems. as we have seen earlier in this book, with temporal-di   erence methods we can eliminate these
inconveniences, and through multi-step methods we can    exibly choose the degree of id64. in
order to gain these advantages in the case of id189 we use actor   critic methods with
a id64 critic.

first consider one-step actor   critic methods, the analog of the td methods introduced in chapter 6
such as td(0), sarsa(0), and id24. the main appeal of one-step methods is that they are fully
online and incremental, yet avoid the complexities of eligibility traces. they are a special case of the
eligibility trace methods, and not as general, but easier to understand. one-step actor   critic methods
replace the full return of reinforce (13.9) with the one-step return (and use a learned state-value
function as the baseline) as follows:

  t+1

.

=   t +   (cid:16)gt:t+1       v(st,w)(cid:17)       (at|st,   t)
=   t +   (cid:16)rt+1 +     v(st+1,w)       v(st,w)(cid:17)       (at|st,   t)

  (at|st,   t)

  (at|st,   t)

=   t +     t       (at|st,   t)
  (at|st,   t)

.

(13.10)

(13.11)

(13.12)

the natural state-value-function learning method to pair with this is semi-gradient td(0). pseudocode
for the complete algorithm is given in the box below. note that it is now a fully online, incremental
algorithm, with states, actions, and rewards processed as they occur and then never revisited.

one-step actor   critic (episodic)

input: a di   erentiable policy parameterization   (a|s,   )
input: a di   erentiable state-value parameterization   v(s,w)
parameters: step sizes      > 0,   w > 0
initialize policy parameter        rd(cid:48) and state-value weights w     rd

repeat forever:

initialize s (   rst state of episode)
i     1
while s is not terminal:

a       (  |s,   )
take action a, observe s(cid:48), r
       r +      v(s(cid:48),w)       v(s,w)
w     w +   w i      w   v(s,w)
          +      i         ln   (a|s,   )
i       i
s     s(cid:48)

(if s(cid:48) is terminal, then   v(s(cid:48),w)

.
= 0)

the generalizations to the forward view of multi-step methods and then to a   -return algorithm are
straightforward. the one-step return in (13.10) is merely replaced by g  
t respectively. the
backward views are also straightforward, using separate eligibility traces for the actor and critic, each
after the patterns in chapter 12. pseudocode for the complete algorithm is given in the box below.

t:t+k and g  

13.6. policy gradient for continuing problems

275

actor   critic with eligibility traces (episodic)

input: a di   erentiable policy parameterization   (a|s,   )
input: a di   erentiable state-value parameterization   v(s,w)
parameters: trace-decay rates          [0, 1],   w     [0, 1]; step sizes      > 0,   w > 0
initialize policy parameter        rd(cid:48) and state-value weights w     rd

repeat forever (for each episode):

initialize s (   rst state of episode)
z       0 (d(cid:48)-component eligibility trace vector)
zw     0 (d-component eligibility trace vector)
i     1
while s is not terminal (for each time step):

(if s(cid:48) is terminal, then   v(s(cid:48),w)

.
= 0)

a       (  |s,   )
take action a, observe s(cid:48), r
       r +      v(s(cid:48),w)       v(s,w)
zw         wzw + i   w   v(s,w)
z             z   + i      ln   (a|s,   )
w     w +   w    zw
          +         z  
i       i
s     s(cid:48)

13.6 policy gradient for continuing problems

as discussed in section 10.3, for continuing problems without episode boundaries we need to de   ne
performance in terms of the average rate of reward per time step:

(13.13)

j(  )

.
= r(  )

e[rt | a0:t   1       ]

.
= lim
h      
= lim
t      

=(cid:88)s

h(cid:88)t=1
1
h
e[rt | a0:t   1       ]
  (s)(cid:88)a
  (a|s)(cid:88)s(cid:48),r

p(s(cid:48), r|s, a)r,
.
= limt       pr{st = s|a0:t       }, which is assumed
where    is the steady-state distribution under   ,   (s)
to exist and to be independent of s0 (an ergodicity assumption). remember that this is the special
distribution under which, if you select actions according to   , you remain in the same distribution:

(cid:88)s

  (s)(cid:88)a

  (a|s,   )p(s(cid:48)|s, a) =   (s(cid:48)).

(13.14)

we also de   ne values, v  (s)
di   erential return:

.
= e  [gt|st = s] and q  (s, a)

.
= e  [gt|st = s, at = a], with respect to the

gt

.
= rt+1       (  ) + rt+2       (  ) + rt+3       (  ) +        .

(13.15)

with these alternate de   nitions, the policy gradient theorem as given for the episodic case (13.5)
remains true for the continuing case. a proof is given in the box on the next page. the forward and
backward view equations also remain the same. complete pseudocode for the actor   critic algorithm in
the continuing case (backward view) is given in the box on page 277.

276

chapter 13. id189

proof of the policy gradient theorem (continuing case)

the proof of the policy gradient theorem for the continuing case begins similarly to the episodic
case. again we leave it implicit in all cases that    is a function of    and that the gradients
are with respect to   . recall that in the continuing case j(  ) = r(  ) (13.13) and that v   and
q   denote values with respect to the di   erential return (13.15). the gradient of the state-value
function can be written, for any s     s, as

(exercise 3.15)

(product rule of calculus)

   v  (s) =    (cid:34)(cid:88)a

  (a|s)q  (s, a)(cid:35) ,

for all s     s

=(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)   q  (s, a)(cid:105)
=(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)   (cid:88)s(cid:48),r
=(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)(cid:2)      r(  ) +(cid:88)s(cid:48)
   r(  ) =(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)(cid:88)s(cid:48)

p(s(cid:48), r|s, a)(cid:0)r     r(  ) + v  (s(cid:48))(cid:1)(cid:105)
p(s(cid:48)|s, a)   v  (s(cid:48))(cid:3)(cid:105).
p(s(cid:48)|s, a)   v  (s(cid:48))(cid:105)        v  (s).

after re-arranging terms, we obtain

notice that the left-hand side can be written    j(  ) and that it does not depend on s. thus the
right-hand side does not depend on s either, and we can safely sum it over all s     s, weighted

p(s(cid:48)|s, a)   v  (s(cid:48))(cid:105)        v  (s)

     (a|s)q  (s, a)

by   (s), without changing it (because(cid:80)s   (s) = 1). thus
   j(  ) =(cid:88)s
=(cid:88)s
=(cid:88)s

  (s)(cid:88)a (cid:104)     (a|s)q  (s, a) +   (a|s)(cid:88)s(cid:48)
  (s)(cid:88)a
+   (s)(cid:88)a
  (s)(cid:88)a
+(cid:88)s(cid:48) (cid:88)s
(cid:124)
  (s)(cid:88)a
  (s)(cid:88)a

  (a|s)(cid:88)s(cid:48)
p(s(cid:48)|s, a)   v  (s(cid:48))       (s)(cid:88)a
   v  (s(cid:48))    (cid:88)s
  (s)(cid:88)a
  (a|s)p(s(cid:48)|s, a)
(cid:123)(cid:122)
(cid:125)
     (a|s)q  (s, a) +(cid:88)s(cid:48)
  (s(cid:48))   v  (s(cid:48))    (cid:88)s

=(cid:88)s
=(cid:88)s

     (a|s)q  (s, a).

     (a|s)q  (s, a)

  (s(cid:48)) (13.14)

q.e.d.

   v  (s)

  (s)   v  (s)

  (s)   v  (s)

13.7. policy parameterization for continuous actions

277

actor   critic with eligibility traces (continuing)

input: a di   erentiable policy parameterization   (a|s,   )
input: a di   erentiable state-value parameterization   v(s,w)
parameters: trace-decay rates          [0, 1],   w     [0, 1]; step sizes      > 0,   w > 0,    > 0
z       0 (d(cid:48)-component eligibility trace vector)
zw     0 (d-component eligibility trace vector)
initialize   r     r (e.g., to 0)
initialize policy parameter        rd(cid:48) and state-value weights w     rd (e.g., to 0)
initialize s     s (e.g., to s0)
repeat forever:
a       (  |s,   )
take action a, observe s(cid:48), r
       r       r +   v(s(cid:48),w)       v(s,w)
  r       r +      
zw       wzw +    w   v(s,w)
z           z   +       ln   (a|s,   )
w     w +   w    zw
          +         z  
s     s(cid:48)

(if s(cid:48) is terminal, then   v(s(cid:48),w)

.
= 0)

13.7 policy parameterization for continuous actions

policy-based methods o   er practical ways of dealing with large actions spaces, even continuous spaces
with an in   nite number of actions. instead of computing learned probabilities for each of the many
actions, we instead learn statistics of the id203 distribution. for example, the action set might be
the real numbers, with actions chosen from a normal (gaussian) distribution.

the id203 density function for the normal distribution is conventionally written

p(x)

.
=

1

     2  

exp(cid:18)   

(x       )2

2  2 (cid:19) ,

(13.16)

where    and    here are the mean and standard deviation of the normal distribution, and of course   
here is just the number        3.14159. the id203 density functions for several di   erent means and
standard deviations are shown in figure 13.3. the value p(x) is the density of the id203 at x, not
the id203. it can be greater than 1; it is the total area under p(x) that must sum to 1. in general,
one can take the integral under p(x) for any range of x values to get the id203 of x falling within
that range.

to produce a policy parameterization, the policy can be de   ned as the normal id203 den-
sity over a real-valued scalar action, with mean and standard deviation given by parametric function
approximators that depend on the state. that is,
(a       (s,   ))2

(13.17)

.
=

1

  (a|s,   )

  (s,   )   2  

exp(cid:18)   

2  (s,   )2 (cid:19) ,

where    : s    rd(cid:48)     r and    : s    rd(cid:48)     r+ are two parameterized function approximators. to
complete the example we need only give a form for these approximators. for this we divide the policy   s
parameter vector into two parts,    = [    ,     ](cid:62), one part to be used for the approximation of the mean
and one part for the approximation of the standard deviation. the mean can be approximated as a

278

chapter 13. id189

figure 13.3: the id203 density function of the normal distribution for di   erent means and variances.

linear function. the standard deviation must always be positive and is better approximated as the
exponential of a linear function. thus

  (s,   )

.
=     (cid:62)x(s)

and

  (s,   )

.

= exp(cid:16)    (cid:62)x(s)(cid:17) ,

(13.18)

where x(s) is a state feature vector perhaps constructed by one of the methods described in chapter 9.
with these de   nitions, all the algorithms described in the rest of this chapter can be applied to learn
to select real-valued actions.

exercise 13.3 a bernoulli-logistic unit is a stochastic neuron-like unit used in some arti   cial neural
networks (section 9.6). its input at time t is a feature vector x(st); its output, at, is a random variable
having two values, 0 and 1, with pr{at = 1} = pt and pr{at = 0} = 1    pt (the bernoulli distribution).
let h(s, 0,   ) and h(s, 1,   ) be the preferences in state s for the unit   s two actions given policy parameter
  . assume that the di   erence between the preferences is given by a weighted sum of the unit   s input
vector, that is, assume that h(s, 1,   )     h(s, 0,   ) =   (cid:62)x(s), where    is the unit   s weight vector.
(a) show that if the exponential softmax distribution (13.2) is used to convert preferences to policies,

then pt =   (1|st,   t) = 1/(1 + exp(     (cid:62)t x(st))) (the logistic function).

(b) what is the monte-carlo reinforce update of   t to   t+1 upon receipt of return gt?

(c) express the eligibility       ln   (a|s,   ) for a bernoulli-logistic unit, in terms of a, x(s), and   (a|s,   )

by calculating the gradient.

hint: separately for each action compute the derivative of the logorithm    rst with respect to pt =
  (a|s,   t), combine the two results into one expression that depends on a and pt, and then use the
(cid:3)
chain rule, noting that the derivative of the logistic function f (x) is f (x)(1     f (x)).

p(x).=1 p2   exp    (x   )22 2       ,  2(0.80.60.40.20.0   5   3135x1.0   1024   2   4x)0,  =0,  =0,  =   2,  =20.2,  =21.0,  =25.0,  =20.5,  =p(x).=1 p2   exp    (x   )22 2   13.8. summary

13.8 summary

279

prior to this chapter, this book focused on action-value methods   meaning methods that learn action
values and then use them to determine action selections. in this chapter, on the other hand, we con-
sidered methods that learn a parameterized policy that enables actions to be taken without consulting
action-value estimates   though action-value estimates may still be learned and used to update the
policy parameter. in particular, we have considered policy-gradient methods   meaning methods that
update the policy parameter on each step in the direction of an estimate of the gradient of performance
with respect to the policy parameter.

methods that learn and store a policy parameter have many advantages. they can learn speci   c
probabilities for taking the actions. they can learn appropriate levels of exploration and approach
deterministic policies asymptotically. they can naturally handle continuous state spaces. all these
things are easy for policy-based methods but awkward or impossible for   -greedy methods and for
action-value methods in general. in addition, on some problems the policy is just simpler to represent
parametrically than the value function; these problems are more suited to parameterized policy methods.

parameterized policy methods also have an important theoretical advantage over action-value meth-
ods in the form of the policy gradient theorem, which gives an exact formula for how performance is
a   ected by the policy parameter that does not involve derivatives of the state distribution. this theorem
provides a theoretical foundation for all id189.

the reinforce method follows directly from the policy gradient theorem. adding a state-value
function as a baseline reduces reinforce   s variance without introducing bias. using the state-value
function for id64 introduces bias but is often desirable for the same reason that id64
td methods are often superior to monte carlo methods (substantially reduced variance). the state-
value function assigns credit to   critizes   the policy   s action selections, and accordingly the former is
termed the critic and the latter the actor, and these overall methods are sometimes termed actor   critic
methods.

overall, policy-gradient methods provide a signi   cantly di   erent set of strengths, and weaknesses
than action-value methods. today they are less well understood in some respects, but a subject of
excitement and ongoing research.

bibliographical and historical remarks

methods that we now see as related to policy gradients were actually some of the earliest to be studied
in id23 (witten, 1977; barto, sutton, and anderson, 1983; sutton, 1984; williams,
1987, 1992) and in predecessor    elds (phansalkar and thathachar, 1995). they were largely supplanted
in the 1990s by the action-value methods that are the focus of the other chapters of this book.
in
recent years, however, attention has returned to actor   critic methods and to policy-gradient methods
in general. among the further developments beyond what we cover here are natural-gradient methods
(amari, 1998; kakade, 2002, peters, vijayakumar and schaal, 2005; peters and schall, 2008; park,
kim and kang, 2005; bhatnagar, sutton, ghavamzadeh and lee, 2009; see grondman, busoniu, lopes
and babuska, 2012), and deterministic policy gradient (silver et al., 2014). major applications include
acrobatic helicopter autopilots and alphago (see section 16.6).

our presentation in this chapter is based primarily on that by sutton, mcallester, singh, and man-
sour (2000, see also sutton, singh, and mcallester, 2000), who introduced the term    policy gradient
methods.    a useful overview is provided by bhatnagar et al. (2003). one of the earliest related works
is by aleksandrov, sysoyev, and shemeneva (1968).

13.1

example 13.1 and the results with it in this chapter were developed with eric graves.

280

13.2

the policy gradient theorem here and on page 276 was    rst obtained by marbach and tsitsiklis
(1998, 2001) and then independently by sutton et al. (2000). a similar expression was obtained
by cao and chen (1997). other early results are due to konda and tsitsiklis (2000, 2003) and
baxter and bartlett (2000). some additional results are developed by sutton

13.3

reinforce is due to williams (1987, 1992).

phansalkar and thathachar (1995) proved both local and global convergence theorems for
modi   ed versions of reinforce algorithms.

13.4

the baseline was introduced in williams   s (1987, 1992) original work. greensmith, bartlett,
and baxter (2004) analyzed an arguably better baseline (see dick, 2015).

13.5   6 actor   critic methods were among the earliest to be investigated in id23 (wit-
ten, 1977; barto, sutton, and anderson, 1983; sutton, 1984). the algorithms presented here
and in section 13.6 are based on the work of degris, white, and sutton (2012), who also
introduced the study of o   -policy policy-gradient methods.

13.7

the    rst to show how continuous actions could be handled this way appears to have been
williams (1987, 1992).

part iii: looking deeper

in this last part of the book we look beyond the standard id23 ideas presented in
the    rst two parts of the book to brie   y survey their relationships with psychology and neuroscience, a
sampling of id23 applications, and some of the active frontiers for future reinforcement
learning research.

281

282

chapter 14

psychology

in previous chapters we developed ideas for algorithms based on computational considerations alone. in
this chapter we look at some of these algorithms from another perspective: the perspective of psychology
and its study of how animals learn. the goals of this chapter are,    rst, to discuss ways that reinforcement
learning ideas and algorithms correspond to what psychologists have discovered about animal learning,
and second, to explain the in   uence id23 is having on the study of animal learning.
the clear formalism provided by id23 that systemizes tasks, returns, and algorithms
is proving to be enormously useful in making sense of experimental data, in suggesting new kinds of
experiments, and in pointing to factors that may be critical to manipulate and to measure. the idea
of optimizing return over the long term that is at the core of id23 is contributing to
our understanding of otherwise puzzling features of animal learning and behavior.

some of the correspondences between id23 and psychological theories are not sur-
prising because the development of id23 drew inspiration from psychological learning
theories. however, as developed in this book, id23 explores idealized situations from
the perspective of an arti   cial intelligence researcher or engineer, with the goal of solving computa-
tional problems with e   cient algorithms, rather than to to replicate or explain in detail how animals
learn. as a result, some of the correspondences we describe connect ideas that arose independently
in their respective    elds. we believe these points of contact are specially meaningful because they
expose computational principles important to learning, whether it is learning by arti   cial or by natural
systems.

for the most part, we describe correspondences between id23 and learning theories
developed to explain how animals like rats, pigeons, and rabbits learn in controlled laboratory exper-
iments. thousands of these experiments were conducted throughout the 20th century, and many are
still being conducted today. although sometimes dismissed as irrelevant to wider issues in psychology,
these experiments probe subtle properties of animal learning, often motivated by precise theoretical
questions. as psychology shifted its focus to more cognitive aspects of behavior, that is, to mental
processes such as thought and reasoning, animal learning experiments came to play less of a role in
psychology than they once did. but this experimentation led to the discovery of learning principles that
are elemental and widespread throughout the animal kingdom, principles that should not be neglected
in designing arti   cial learning systems. in addition, as we shall see, some aspects of cognitive processing
connect naturally to the computational perspective provided by id23.

this chapter   s    nal section includes references relevant to the connections we discuss as well as to
connections we neglect. we hope this chapter encourages readers to probe all of these connections more
deeply. also included in this    nal section is a discussion of how the terminology used in reinforcement
learning relates to that of psychology. many of the terms and phrases used in id23 are

283

284

chapter 14. psychology

borrowed from animal learning theories, but the computational/engineering meanings of these terms
and phrases do not always coincide with their meanings in psychology.

14.1 prediction and control

the algorithms we describe in this book fall into two broad categories: algorithms for prediction and
algorithms for control. these categories arise naturally in solution methods for the reinforcement
learning problem presented in chapter 3. in many ways these categories respectively correspond to
categories of learning extensively studied by psychologists: classical, or pavlovian, conditioning and
instrumental, or operant, conditioning. these correspondences are not completely accidental because
of psychology   s in   uence on id23, but they are nevertheless striking because they
connect ideas arising from di   erent objectives.

the prediction algorithms presented in this book estimate quantities that depend on how features of
an agent   s environment are expected to unfold over the future. we speci   cally focus on estimating the
amount of reward an agent can expect to receive over the future while it interacts with its environment.
in this role, prediction algorithms are policy evaluation algorithms, which are integral components of
algorithms for improving policies. but prediction algorithms are not limited to predicting future reward;
they can predict any feature of the environment (see, for example, modayil, white, and sutton, 2014).
the correspondence between prediction algorithms and classical conditioning rests on their common
property of predicting upcoming stimuli, whether or not those stimuli are rewarding (or punishing).

the situation in an instrumental, or operant, conditioning experiment is di   erent. here, the exper-
imental apparatus is set up so that an animal is given something it likes (a reward) or something it
dislikes (a penalty) depending on what the animal did. the animal learns to increase its tendency to
produce rewarded behavior and to decrease its tendency to produce penalized behavior. the reinforcing
stimulus is said to be contingent on the animal   s behavior, whereas in classical conditioning it is not
(although it is di   cult to remove all behavior contingencies in a classical conditioning experiment).
instrumental conditioning experiments are like those that inspired thorndike   s law of e   ect that we
brie   y discuss in chapter 1. control is at the core of this form of learning, which corresponds to the
operation of id23   s policy-improvement algorithms.1

thinking of classical conditioning in terms of prediction, and instrumental conditioning in terms of
control, is a starting point for connecting our computational view of id23 to animal
learning, but in reality, the situation is more complicated than this. there is more to classical condi-
tioning than prediction; it also involves action, and so is a mode of control, sometimes called pavlovian
control. further, classical and instrumental conditioning interact in interesting ways, with both sorts
of learning likely being engaged in most experimental situations. despite these complications, align-
ing the classical/instrumental distinction with the prediction/control distinction is a convenient    rst
approximation in connecting id23 to animal learning.

in psychology, the term reinforcement is used to describe learning in both classical and instrumental
conditioning. originally referring only to the strengthening of a pattern of behavior, it is frequently also
used for the weakening of a pattern of behavior. a stimulus considered to be the cause of the change in
behavior is called a reinforcer, wether or not it is contingent on the animal   s previous behavior. at the
end of this chapter we discuss this terminology in more detail and how it relates to terminology used
in machine learning.

1what control means for us is di   erent from what it typically means in animal learning theories; there the environment

controls the agent instead of the other way around. see our comments on terminology at the end of this chapter.

14.2. classical conditioning

285

14.2 classical conditioning

while studying the activity of the digestive system, the celebrated russian physiologist ivan pavlov
found that an animal   s innate responses to certain triggering stimuli can come to be triggered by other
stimuli that are quite unrelated to the inborn triggers. his experimental subjects were dogs that had
undergone minor surgery to allow the intensity of their salivary re   ex to be accurately measured. in
one case he describes, the dog did not salivate under most circumstances, but about 5 seconds after
being presented with food it produced about six drops of saliva over the next several seconds. after
several repetitions of presenting another stimulus, one not related to food, in this case the sound of a
metronome, shortly before the introduction of food, the dog salivated in response to the sound of the
metronome in the same way it did to the food.    the activity of the salivary gland has thus been called
into play by impulses of sound   a stimulus quite alien to food    (pavlov, 1927, p. 22). summarizing the
signi   cance of this    nding, pavlov wrote:

it is pretty evident that under natural conditions the normal animal must respond not only
to stimuli which themselves bring immediate bene   t or harm, but also to other physical or
chemical agencies   waves of sound, light, and the like   which in themselves only signal the
approach of these stimuli; though it is not the sight and sound of the beast of prey which is
in itself harmful to the smaller animal, but its teeth and claws. (pavlov, 1927, p. 14)

connecting new stimuli to innate re   exes in this way is now called classical, or pavlovian, condi-
tioning. pavlov (or more exactly, his translators) called inborn responses (e.g., salivation in his demon-
stration described above)    unconditioned responses    (urs), their natural triggering stimuli (e.g., food)
   unconditioned stimuli    (uss), and new responses triggered by predictive stimuli (e.g., here also sali-
vation)    conditioned responses    (crs). a stimulus that is initially neutral, meaning that it does not
normally elicit strong responses (e.g., the metronome sound), becomes a    conditioned stimulus    (cs)
as the animal learns that it predicts the us and so comes to produce a cr in response to the cs. these
terms are still used in describing classical conditioning experiments (though better translations would
have been    conditional    and    unconditional    instead of conditioned and unconditioned). the us is
called a reinforcer because it reinforces producing a cr in response to the cs.

figure 14.1 shows the arrangement of stimuli in two types of classical conditioning experiments: in
delay conditioning, the cs extends throughout the interstimulus interval, or isi, which is the time
interval between the cs onset and the us onset (with the cs ending when the us ends in a common
version shown here).
in trace conditioning, the us begins after the cs ends, and the time interval
between cs o   set and us onset is called the trace interval.

the salivation of pavlov   s dogs to the sound of a metronome is just one example of classical condition-
ing, which has been intensively studied across many response systems of many species of animals. urs
are often preparatory in some way, like the salivation of pavlov   s dog, or protective in some way, like
an eye blink in response to something irritating to the eye, or freezing in response to seeing a predator.
experiencing the cs-us predictive relationship over a series of trials causes the animal to learn that
the cs predicts the us so that the animal can respond to the cs with a cr that prepares the animal
for, or protects it from, the predicted us. some crs are similar to the ur but begin earlier and di   er
in ways that increase their e   ectiveness. in one intensively studied type of experiment, for example,
a tone cs reliably predicts a pu    of air (the us) to a rabbit   s eye, triggering a ur consisting of the
closure of a protective inner eyelid called the nictitating membrane. after one or more trials, the tone
comes to trigger a cr consisting of membrane closure that begins before the air pu    and eventually
becomes timed so that peak closure occurs just when the air pu    is likely to occur. this cr, being
initiated in anticipation of the air pu    and appropriately timed, o   ers better protection than simply
initiating closure as a reaction to the irritating us. the ability to act in anticipation of important events
by learning about predictive relationships among stimuli is so bene   cial that it is widely present across
the animal kingdom.

286

chapter 14. psychology

figure 14.1: arrangement of stimuli in two types of classical conditioning experiments. in delay conditioning,
the cs extends throughout the interstimulus interval, or isi, which is the time interval between the cs onset
and the us onset (often with the cs and us ending at the same time as shown here). in trace conditioning,
there is a time interval, called the trace interval, between cs o   set and us onset.

14.2.1 blocking and higher-order conditioning

many interesting properties of classical conditioning have been observed in experiments. beyond the
anticipatory nature of crs, two widely observed properties    gured prominently in the development
of classical conditioning models: blocking and higher-order conditioning. blocking occurs when an
animal fails to learn a cr when a potential cs in presented along with another cs that had been used
previously to condition the animal to produce that cr. for example, in the    rst stage of a blocking
experiment involving rabbit nictitating membrane conditioning, a rabbit is    rst conditioned with a tone
cs and an air pu    us to produce the cr of closing its nictitating membrane in anticipation of the
air pu   . the experiment   s second stage consists of additional trials in which a second stimulus, say a
light, is added to the tone to form a compound tone/light cs followed by the same air pu    us. in the
experiment   s third phase, the second stimulus alone   the light   is presented to the rabbit to see if the
rabbit has learned to respond to it with a cr. it turns out that the rabbit produces very few, or no,
crs in response to the light:
learning to the light had been blocked by the previous learning to the
tone.2 blocking results like this challenged the idea that conditioning depends only on simple temporal
contiguity, that is, that a necessary and su   cient condition for conditioning is that a us frequently
follows a cs closely in time. in the next section we describe the rescorla   wagner model (rescorla and
wagner, 1972) that o   ered an in   uential explanation for blocking.

higher-order conditioning occurs when a previously-conditioned cs acts as a us in conditioning an-
other initially neutral stimulus. pavlov described an experiment in which his assistant    rst conditioned
a dog to salivate to the sound of a metronome that predicted a food us, as described above. after this
stage of conditioning, a number of trials were conducted in which a black square, to which the dog was
initially indi   erent, was placed in the dog   s line of vision followed by the sound of the metronome   
and this was not followed by food. in just ten trials, the dog began to salivate merely upon seeing
the black square, despite the fact that the sight of it had never been followed by food. the sound of

2comparison with a control group is necessary to show that the previous conditioning to the tone is responsible for
blocking learning to the light. this is done by trials with the tone/light cs but with no prior conditioning to the tone.
learning to the light in this case is unimpaired. moore and schmajuk (2008) give a full account of this procedure.

ttrace conditioningdelay conditioningcsuscsusisi14.2. classical conditioning

287

the metronome itself acted as a us in conditioning a salivation cr to the black square cs. this was
second-order conditioning. if the black square had been used as a us to establish salivation crs to
another otherwise neutral cs, it would have been third-order conditioning, and so on. higher-order
conditioning is di   cult to demonstrate, especially above the second order, in part because a higher-
order reinforcer loses its reinforcing value due to not being repeatedly followed by the original us during
higher-order conditioning trials. but under the right conditions, such as intermixing    rst-order trials
with higher-order trials or by providing a general energizing stimulus, higher-order conditioning beyond
the second order can be demonstrated. as we describe below, the td model of classical conditioning
uses the id64 idea that is central to our approach to extend the rescorla   wagner model   s
account of blocking to include both the anticipatory nature of crs and higher-order conditioning.

higher-order instrumental conditioning occurs as well. in this case, a stimulus that consistently pre-
dicts primary reinforcement becomes a reinforcer itself, where reinforcement is primary if its rewarding
or penalizing quality has been built into the animal by evolution. the predicting stimulus becomes
a secondary reinforcer, or more generally, a higher-order or conditioned reinforcer    the latter being a
better term when the predicted reinforcing stimulus is itself a secondary, or an even higher-order, rein-
forcer. a conditioned reinforcer delivers conditioned reinforcement: conditioned reward or conditioned
penalty. conditioned reinforcement acts like primary reinforcement in increasing an animal   s tendency
to produce behavior that leads to conditioned reward, and to decrease an animal   s tendency to produce
behavior that leads to conditioned penalty. (see our comments at the end of this chapter that explain
how our terminology sometimes di   ers, as it does here, from terminology used in psychology.)

conditioned reinforcement is a key phenomenon that explains, for instance, why we work for the
conditioned reinforcer money, whose worth derives solely from what is predicted by having it. in actor   
critic methods described in section 13.5 (and discussed in the context of neuroscience in sections 15.7
and 15.8), the critic uses a td method to evaluate the actor   s policy, and its value estimates provide
conditioned reinforcement to the actor, allowing the actor to improve its policy. this analog of higher-
order instrumental conditioning helps address the credit-assignment problem mentioned in section 1.7
because the critic gives moment-by-moment reinforcement to the actor when the primary reward signal
is delayed. we discuss this more below in section 14.4.

14.2.2 the rescorla   wagner model

rescorla and wagner created their model mainly to account for blocking. the core idea of the rescorla   
wagner model is that an animal only learns when events violate its expectations, in other words, only
when the animal is surprised (although without necessarily implying any conscious expectation or
emotion). we    rst present rescorla and wagner   s model using their terminology and notation before
shifting to the terminology and notation we use to describe the td model.

here is how rescorla and wagner described their model. the model adjusts the    associative strength   
of each component stimulus of a compound cs, which is a number representing how strongly or reliably
that component is predictive of a us. when a compound cs consisting of several component stimuli is
presented in a classical conditioning trial, the associative strength of each component stimulus changes
in a way that depends on an associative strength associated with the entire stimulus compound, called
the    aggregate associative strength,    and not just on the associative strength of each component itself.

rescorla and wagner considered a compound cs ax, consisting of component stimuli a and x, where
the animal may have already experienced stimulus a, and stimulus x might be new to the animal. let
va, vx, and vax respectively denote the associative strengths of stimuli a, x, and the compound ax.
suppose that on a trial the compound cs ax is followed by a us, which we label stimulus y. then
the associative strengths of the stimulus components change according to these expressions:

   va =   a  y(ry     vax)
   vx =   x  y(ry     vax),

288

chapter 14. psychology

where   a  y and   x  y are the step-size parameters, which depend on the identities of the cs compo-
nents and the us, and ry is the asymptotic level of associative strength that the us y can support.
(rescorla and wagner used    here instead of r, but we use r to avoid confusion with our use of   
and because we usually think of this as the magnitude of a reward signal, with the caveat that the us
in classical conditioning is not necessarily rewarding or penalizing.) a key assumption of the model is
that the aggregate associative strength vax is equal to va + vx. the associative strengths as changed
by these    s become the associative strengths at the beginning of the next trial.

to be complete, the model needs a response-generation mechanism, which is a way of mapping values
of v s to crs. since this mapping would depend on details of the experimental situation, rescorla and
wagner did not specify a mapping but simply assumed that larger v s would produce stronger or more
likely crs, and that negative v s would mean that there would be no crs.

the rescorla   wagner model accounts for the acquisition of crs in a way that explains blocking. as
long as the aggregate associative strength, vax, of the stimulus compound is below the asymptotic level
of associative strength, ry, that the us y can support, the prediction error ry     vax is positive. this
means that over successive trials the associative strengths va and vx of the component stimuli increase
until the aggregate associative strength vax equals ry, at which point the associative strengths stop
changing (unless the us changes). when a new component is added to a compound cs to which the
animal has already been conditioned, further conditioning with the augmented compound produces little
or no increase in the associative strength of the added cs component because the error has already been
reduced to zero, or to a low value. the occurrence of the us is already predicted nearly perfectly, so
little or no error   or surprise   is introduced by the new cs component. prior learning blocks learning
to the new component.

to transition from rescorla and wagner   s model to the td model of classical conditioning (which
we just call the td model), we    rst recast their model in terms of the concepts that we are using
throughout this book. speci   cally, we match the notation we use for learning with linear function
approximation (section 9.4), and we think of the conditioning process as one of learning to predict the
   magnitude of the us    on a trial on the basis of the compound cs presented on that trial, where the
magnitude of a us y is the ry of the rescorla   wagner model as given above. we also introduce states.
because the rescorla   wagner model is a trial-level model, meaning that it deals with how associative
strengths change from trial to trial without considering any details about what happens within and
between trials, we do not have to consider how states change during a trial until we present the full td
model in the following section. instead, here we simply think of a state as a way of labeling a trial in
terms of the collection of component css that are present on the trial.

therefore, assume that trial-type, or state, s is described by a real-valued vector of features x(s) =
(x1(s), x2(s), . . . , xd(s))(cid:62) where xi(s) = 1 if csi, the ith component of a compound cs, is present on
the trial and 0 otherwise. then if the d-dimensional vector of associative strengths is w, the aggregate
associative strength for trial-type s is

  v(s,w) = w(cid:62)x(s).

(14.1)

this corresponds to a value estimate in id23, and we think of it as the us prediction.

now temporally let t denote the number of a complete trial and not its usual meaning as a time step
(we revert to t   s usual meaning when we extend this to the td model below), and assume that st is
the state corresponding to trial t. conditioning trial t updates the associative strength vector wt to
wt+1 as follows:

wt+1 = wt +     t x(st),

(14.2)

where    is the step-size parameter, and   because here we are describing the rescorla   wagner model     t
is the prediction error

  t = rt       v(st,wt).

(14.3)

14.2. classical conditioning

289

rt is the target of the prediction on trial t, that is, the magnitude of the us, or in rescorla and wagner   s
terms, the associative strength that the us on the trial can support. note that because of the factor
x(st) in (14.2), only the associative strengths of cs components present on a trial are adjusted as a
result of that trial. you can think of the prediction error as a measure of surprise, and the aggregate
associative strength as the animal   s expectation that is violated when it does not match the target us
magnitude.

from the perspective of machine learning, the rescorla   wagner model is an error-correction super-
vised learning rule. it is essentially the same as the least mean square (lms), or widrow-ho   , learning
rule (widrow and ho   , 1960) that    nds the weights   here the associative strengths   that make the
average of the squares of all the errors as close to zero as possible. it is a    curve-   tting,    or regression,
algorithm that is widely used in engineering and scienti   c applications (see section 9.4).3

the rescorla   wagner model was very in   uential in the history of animal learning theory because it
showed that a    mechanistic    theory could account for the main facts about blocking without resorting
to more complex cognitive theories involving, for example, an animal   s explicit recognition that another
stimulus component had been added and then scanning its short-term memory backward to reassess
the predictive relationships involving the us. the rescorla   wagner model showed how traditional
contiguity theories of conditioning   that temporal contiguity of stimuli was a necessary and su   cient
condition for learning   could be adjusted in a simple way to account for blocking (moore and schmajuk,
2008).

the rescorla   wagner model provides a simple account of blocking and some other features of classical
conditioning, but it is not a complete or perfect model of classical conditioning. di   erent ideas account
for a variety of other observed e   ects, and progress is still being made toward understanding the many
subtleties of classical conditioning. the td model, which we describe next, though also not a complete
or perfect model model of classical conditioning, extends the rescorla   wagner model to address how
within-trial and between-trial timing relationships among stimuli can in   uence learning and how higher-
order conditioning might arise.

14.2.3 the td model

the td model is a real-time model, as opposed to a trial-level model like the rescorla   wagner model.
a single step t in the our formulation of rescorla and wagner   s model above represents an entire
conditioning trial. the model does not apply to details about what happens during the time a trial
is taking place, or what might happen between trials. within each trial an animal might experience
various stimuli whose onsets occur at particular times and that have particular durations. these
timing relationships strongly in   uence learning. the rescorla   wagner model also does not include
a mechanism for higher-order conditioning, whereas for the td model, higher-order conditioning is a
natural consequence of the id64 idea that is at the base of td algorithms.

to describe the td model we begin with the formulation of the rescorla   wagner model above, but
t now labels time steps within or between trials instead of complete trials. think of the time between
t and t + 1 as a small time interval, say .01 second, and think of a trial as a sequences of states,
one associated with each time step, where the state at step t now represents details of how stimuli
are represented at t instead of just a label for the cs components present on a trial. in fact, we can
completely abandon the idea of trials. from the point of view of the animal, a trial is just a fragment
of its continuing experience interacting with its world. following our usual view of an agent interacting
with its environment, imagine that the animal is experiencing an endless sequence of states s, each
represented by a feature vector x(s). that said, it is still often convenient to refer to trials as fragments

3the only di   erences between the lms rule and the rescorla   wagner model are that for lms the input vectors xt
can have any real numbers as components, and   at least in the simplest version of the lms rule   the step-size parameter
   does not depend on the input vector or the identity of the stimulus setting the prediction target.

290

chapter 14. psychology

of time during which patterns of stimuli repeat in an experiment.

state features are not restricted to describing the external stimuli that an animal experiences; they can
describe neural activity patterns that external stimuli produce in an animal   s brain, and these patterns
can be history-dependent, meaning that they can be persistent patterns produced by sequences of
external stimuli. of course, we do not know exactly what these neural activity patterns are, but a real-
time model like the td model allows one to explore the consequences on learning of di   erent hypotheses
about the internal representations of external stimuli. for these reasons, the td model does not commit
to any particular state representation. in addition, because the td model includes discounting and
eligibility traces that span time intervals between stimuli, the model also makes it possible to explore
how discounting and eligibility traces interact with stimulus representations in making predictions about
the results of classical conditioning experiments.

below we describe some of the state representations that have been used with the td model and
some of their implications, but for the moment we stay agnostic about the representation and just
assume that each state s is represented by a feature vector x(s) = (x1(s), x2(s), . . . , xn(s))(cid:62). then
the aggregate associative strength corresponding to a state s is given by (14.1), the same as for the
rescorla-wgner model, but the td model updates the associative strength vector, w, di   erently. with
t now labeling a time step instead of a complete trial, the td model governs learning according to this
update:

wt+1 = wt +     t zt,

(14.4)

which replaces xt(st) in the rescorla   wagner update (14.2) with zt, a vector of eligibility traces, and
instead of the   t of (14.3), here   t is a td error:

  t = rt+1 +     v(st+1,wt)       v(st,wt),

(14.5)

where    is a discount factor (between 0 and 1), rt is the prediction target at time t, and   v(st+1,wt)
and   v(st,wt) are aggregate associative strengths at t + 1 and t as de   ned by (14.1).

each component i of the eligibility-trace vector zt increments or decrements according to the com-

ponent xi(st) of the feature vector x(st), and otherwise decays with a rate determined by     :

zt+1 =     zt + x(st).

(14.6)

here    is the usual eligibility trace decay parameter.

note that if    = 0, the td model reduces to the rescorla   wagner model with the exceptions that:
the meaning of t is di   erent in each case (a trial number for the rescorla   wagner model and a time
step for the td model), and in the td model there is a one-time-step lead in the prediction target r.
the td model is equivalent to the backward view of the semi-gradient td(  ) algorithm with linear
function approximation (chapter 12), except that rt in the model does not have to be a reward signal
as it does when the td algorithm is used to learn a value function for policy-improvement.

14.2.4 td model simulations

real-time conditioning models like the td model are interesting primarily because they make predic-
tions for a wide range of situations that cannot be represented by trial-level models. these situations
involve the timing and durations of conditionable stimuli, the timing of these stimuli in relation to the
timing of the us, and the timing and shapes of crs. for example, the us generally must begin after
the onset of a neutral stimulus for conditioning to occur, with the rate and e   ectiveness of learning
depending on the inter-stimulus interval, or isi, the interval between the onsets of the cs and the us.
when crs appear, they generally begin before the appearance of the us and their temporal pro   les
change during learning. in conditioning with compound css, the component stimuli of the compound

14.2. classical conditioning

291

css may not all begin and end at the same time, sometimes forming what is called a serial compound
in which the component stimuli occur in a sequence over time. timing considerations like these make
it important to consider how stimuli are represented, how these representations unfold over time during
and between trials, and how they interact with discounting and eligibility traces.

figure 14.2 shows three of the stimulus representations that have been used in exploring the behavior
of the td model: the complete serial compound (csc), the microstimulus (ms), and the presence
representations (ludvig, sutton, and kehoe, 2012). these representations di   er in the degree to which
they force generalization among nearby time points during which a stimulus is present.

the simplest of the representations shown in figure 14.2 is the presence representation in the    gure   s
right column. this representation has a single feature for each component cs present on a trial,
where the feature has value 1 whenever that component is present, and 0 otherwise.4 the presence
representation is not a realistic hypothesis about how stimuli are represented in an animal   s brain, but as
we describe below, the td model with this representation can produce many of the timing phenomena
seen in classical conditioning.

for the csc representation (left column of figure 14.2), the onset of each external stimulus initiates
a sequence of precisely-timed short-duration internal signals that continues until the external stimulus

4in our formalism, there is a di   erent state, st, for each time step t during a trial, and for a trial in which a compound
cs consists of n component css of various durations occurring at various times throughout the trial, there is a feature, xi,
for each component csi, i = 1, . . . , n, where xi(st) = 1 for all times t when the csi is present, and equals zero otherwise.

figure 14.2: three stimulus representations (in columns) sometimes used with the td model. each row
represents one element of the stimulus representation. the three representations vary along a temporal gen-
eralization gradient, with no generalization between nearby time points in the complete serial compound (left
column) and complete generalization between nearby time points in the presence representation (right column).
the microstimulus representation occupies a middle ground. the degree of temporal generalization determines
the temporal granularity with which us predictions are learned. adapted with minor changes from learning
& behavior, evaluating the td model of classical conditioning, volume 40, 2012, p. 311, e. a. ludvig, r. s.
sutton, e. j. kehoe. with permission of springer.

presencecomplete serial compoundrewardstimulusstimulus representation temporal generalization gradientmicrostimulicsus292

chapter 14. psychology

ends.5 this is like assuming the animal   s nervous system has a clock that keeps precise track of
time during stimulus presentations; it is what engineers call a    tapped delay line.    like the presence
representation, the csc representation is unrealistic as a hypothesis about how the brain internally
represents stimuli, but ludvig et al. (2012) call it a    useful    ction    because it can reveal details of
how the td model works when relatively unconstrained by the stimulus representation. the csc
representation is also used in most td models of dopamine-producing neurons in the brain, a topic we
take up in chapter 15. the csc representation is often viewed as an essential part of the td model,
although this view is mistaken.

the ms representation (center column of figure 14.2) is like the csc representation in that each
external stimulus initiates a cascade of internal stimuli, but in this case the internal stimuli   the
microstimuli   are not of such limited and non-overlapping form; they are extended over time and
overlap. as time elapses from stimulus onset, di   erent sets of microstimuli become more or less active,
and each subsequent microstimulus becomes progressively wider in time and reaches a lower maximal
level. of course, there are many ms representations depending on the nature of the microstimuli, and
a number of examples of ms representations have been studied in the literature, in some cases along
with proposals for how an animal   s brain might generate them (see the bibliographic and historical
comments at the end of this chapter). ms representations are more realistic than the presence or csc
representations as hypotheses about neural representations of stimuli, and they allow the behavior of
the td model to be related to a broader collection of phenomena observed in animal experiments. in
particular, by assuming that cascades of microstimuli are initiated by uss as well as by css, and by
studying the signi   cant e   ects on learning of interactions between microstimuli, eligibility traces, and
discounting, the td model is helping to frame hypotheses to account for many of the subtle phenomena
of classical conditioning and how an animal   s brain might produce them. we say more about this below,
particularly in chapter 15 where we discuss id23 and neuroscience.

even with the simple presence representation, however, the td model produces all the basic prop-
erties of classical conditioning that are accounted for by the rescorla   wagner model, plus features of
conditioning that are beyond the scope of trial-level models. for example, as we have already men-
tioned, a conspicuous feature of classical conditioning is that the us generally must begin after the
onset of a neutral stimulus for conditioning to occur, and that after conditioning, the cr begins before
the appearance of the us. in other words, conditioning generally requires a positive isi, and the cr
generally anticipates the us. how the strength of conditioning (e.g., the percentage of crs elicited by
a cs) depends on the isi varies substantially across species and response systems, but it typically has
the following properties: it is negligible for a zero or negative isi, i.e., when the us onset occurs simul-
taneously with, or earlier than, the cs onset (although research has found that associative strengths
sometimes increase slightly or become negative with negative isis); it increases to a maximum at a
positive isi where conditioning is most e   ective; and it then decreases to zero after an interval that
varies widely with response systems. the precise shape of this dependency for the td model depends
on the values of its parameters and details of the stimulus representation, but these basic features of
isi-dependency are core properties of the td model.

one of the theoretical issues arising with serial-compound conditioning, that is, conditioning with a
compound cs whose components occur in a sequence, concerns the facilitation of remote associations.
it has been found that if the empty trace interval between the cs and the us is    lled with a second cs
to form a serial-compound stimulus, then conditioning to the    rst cs is facilitated. figure 14.3 shows
the behavior of the td model with the presence representation in a simulation of such an experiment
whose timing details are shown at the top of the    gure. consistent with the experimental results
(kehoe, 1982), the model shows facilitation of both the rate of conditioning and the asymptotic level

5in our formalism, for each cs component csi present on a trial, and for each time step t during a trial, there is a
i(st(cid:48) ) = 1 if t = t(cid:48) for any t(cid:48) at which csi is present, and equals 0 otherwise. this is di   erent
separate feature xt
from the csc representation in sutton and barto (1990) in which there are the same distinct features for each time step
but no reference to external stimuli; hence the name complete serial compound.

i, where xt

14.2. classical conditioning

293

of conditioning of the    rst cs due to the presence of the second cs.

figure 14.3: facilitation of a remote association by an intervening stimulus in the td model. top: temporal
relationships among stimuli within a trial. bottom: behavior over trials of csa   s associative strength when
csa is presented in a serial compound as shown in the top panel, and when presented in an identical temporal
relationship to the us, only without csb. adapted from sutton and barto (1990).

a well-known demonstration of the e   ects on conditioning of temporal relationships among stimuli
within a trial is an experiment by egger and miller (1962) that involved two overlapping css in a
delay con   guration as shown in the top panel of figure 14.4. although csb was in a better temporal
relationship with the us, the presence of csa substantially reduced conditioning to csb as compared
to controls in which csa was absent. the bottom panel of figure 14.4 shows the same result being
generated by the td model in a simulation of this experiment with the presence representation.

the td model accounts for blocking because it is an error-correcting learning rule like the rescorla   
wagner model. beyond accounting for basic blocking results, however, the td model predicts (with the
presence representation and more complex representations a well) that blocking is reversed if the blocked
stimulus is moved earlier in time so that its onset occurs before the onset of the blocking stimulus. this
feature of the td model   s behavior deserves attention because it had not been observed at the time of
the model   s introduction. recall that in blocking, if an animal has already learned that one cs predicts
a us, then learning that a newly-added second cs also predicts the us is much reduced, i.e., is blocked.
but if the newly-added second cs begins earlier than the pretrained cs, then   according to the td
model    learning to the newly-added cs is not blocked. in fact, as training continues and the newly-
added cs gains associative strength, and the pretrained cs loses associative strength. the behavior
of the td model under these conditions is shown in figure 14.5. this simulation experiment di   ered
from the egger-miller experiment of figure 14.4 in that the shorter cs with the later onset was given
prior training until it was fully associated with the us. this surprising prediction led kehoe, scheurs,
and graham (1987) to conduct the experiment using the well-studied rabbit nictitating membrane
preparation. their results con   rmed the model   s prediction, and they noted that non-td models have
considerable di   culty explaining their data.

with the td model, an earlier predictive stimulus takes precedence over a later predictive stimulus

wcsa294

chapter 14. psychology

figure 14.4: the egger-miller, or primacy, e   ect in the td model. top: temporal relationships among stimuli
within a trial. bottom: behavior over trials of csb   s associative strength when csb is presented with and
without csa. adapted from sutton and barto (1990).

because, like all the prediction methods described in this book, the td model is based on the backing-
up or id64 idea: updates to associative strengths shift the strengths at a particular state
toward the strength at later states. another consequence of id64 is that the td model
provides an account of higher-order conditioning, a feature of classical conditioning that is beyond the
scope of the rescoral-wagner and similar models. as we described above, higher-order conditioning
is the phenomenon in which a previously-conditioned cs can act as a us in conditioning another
initially neutral stimulus. figure 14.6 shows the behavior of the td model (again with the presence
representation) in a higher-order conditioning experiment   in this case it is second-order conditioning.
in the    rst phase (not shown in the    gure), csb is trained to predict a us so that its associative strength
increases, here to 1.6. in the second phase, csa is paired with csb in the absence of the us, in the
sequential arrangement shown at the top of the    gure. csa acquires associative strength even though
it is never paired with the us. with continued training, csa   s associative strength reaches a peak and
then decreases because the associative strength of csb, the secondary reinforcer, decreases so that it
loses its ability to provide secondary reinforcement. csb   s associative strength decreases because the
us does not occur in these higher-order conditioning trials. these are extinction trials for csb because
its predictive relationship to the us is disrupted so that its ability to act as a reinforcer decreases. this
same pattern is seen in animal experiments. this extinction of conditioned reinforcement in higher-
order conditioning trials makes it di   cult to demonstrate higher-order conditioning unless the original
predictive relationships are periodically refreshed by occasionally inserting    rst-order trials.

the td model produces an analog of second- and higher-order conditioning because     v(st+1,wt)    
  v(st,wt) appears in the td error   t (14.5). this means that as a result of previous learning,     v(st+1,wt)
can di   er from   v(st,wt), making   t non-zero (a temporal di   erence). this di   erence has the same status
as rt+1 in (14.5), implying that as far as learning is concerned there is no di   erence between a temporal
di   erence and the occurrence of a us. in fact, this feature of the td algorithm is one of the major
reasons for its development, which we now understand through its connection to id145
as described in chapter 6. id64 values is intimately related to second-order, and higher-order,
conditioning.

in the examples of the td model   s behavior described above, we examined only the changes in

wcsb14.2. classical conditioning

295

figure 14.5: temporal primacy overriding blocking in the td model. top: temporal relationships between
stimuli. bottom: behavior over trials of csb   s associative strength when csb is presented with and without
csa. the only di   erence between this simulation and that shown in figure 14.4 was that here csb started
out fully conditioned   csb   s associative strength was initially set to 1.653, the    nal level reached when csb
was presented alone for 80 trials, as in the    csa-absent    case in figure 14.4. adapted from sutton and barto
(1990).

the associative strengths of the cs components; we did not look at what the model predicts about
properties of an animal   s conditioned responses (crs): their timing, shape, and how they develop
over conditioning trials. these properties depend on the species, the response system being observed,
and parameters of the conditioning trials, but in many experiments with di   erent animals and di   erent
response systems, the magnitude of the cr, or the id203 of a cr, increases as the expected time of
the us approaches. for example, in classical conditioning of a rabbit   s nictitating membrane response
that we mentioned above, over conditioning trials the delay from cs onset to when the nictitating
membrane begins to move across the eye decreases over trials, and the amplitude of this anticipatory
closure gradually increases over the interval between the cs and the us until the membrane reaches
maximal closure at the expected time of the us. the timing and shape of this cr is critical to its
adaptive signi   cance   covering the eye too early reduces vision (even though the nictitating membrane
is translucent), while covering it too late is of little protective value. capturing cr features like these
is challenging for models of classical conditioning.

the td model does not include as part of its de   nition any mechanism for translating the time
course of the us prediction,   v(st,wt), into a pro   le that can be compared with the properties of an
animal   s cr. the simplest choice is to let the time course of a simulated cr equal the time course of
the us prediction. in this case, features of simulated crs and how they change over trials depend only
on the stimulus representation chosen and the values of the model   s parameters   ,   , and   .

figure 14.7 shows the time courses of us predictions at di   erent points during learning with the
three representations shown in figure 14.2. for these simulations the us occurred 25 time steps after
the onset of the cs, and    = .05,    = .95 and    = .97. with the csc representation (figure 14.7 left),
the curve of the us prediction formed by the td model increases exponentially throughout the interval
between the cs and the us until it reaches a maximum exactly when the us occurs (at time step 25).
this exponential increase is the result of discounting in the td model learning rule. with the presence
representation (figure 14.7 middle), the us prediction is nearly constant while the stimulus is present

wcsb296

chapter 14. psychology

figure 14.6: second-order conditioning with the td model. top: temporal relationships between stimuli.
bottom: behavior of the associative strengths associated with csa and csb over trials. the second stimulus,
csb, has an initial associative strength of 1.653 at the beginning of the simulation. adapted from sutton and
barto (1990).

because there is only one weight, or associative strength, to be learned for each stimulus. consequently,
the td model with the presence representation cannot recreate many features of cr timing. with
an ms representation (figure 14.7 right), the development of the td model   s us prediction is more
complicated. after 200 trials the prediction   s pro   le is a reasonable approximation of the us prediction
curve produced with the csc representation.

the us prediction curves shown in figure 14.7 were not intended to precisely match pro   les of crs
as they develop during conditioning in any particular animal experiment, but they illustrate the strong
in   uence that the stimulus representation has on predictions derived from the td model. further,
although we can only mention it here, how the stimulus representation interacts with discounting and
eligibility traces is important in determining properties of the us prediction pro   les produced by the
td model. another dimension beyond what we can discuss here is the in   uence of di   erent response-
generation mechanisms that translate us predictions into cr pro   les; the pro   les shown in figure 14.7
are    raw    us prediction pro   les. even without any special assumption about how an animal   s brain
might produce overt responses from us predictions, however, the pro   les in figure 14.7 for the csc
and ms representations increase as the time of the us approaches and reach a maximum at the time
of the us, as is seen in many animal conditioning experiments.

the td model, when combined with particular stimulus representations and response-generation
mechanisms, is able to account for a surprisingly-wide range of phenomena observed in animal classical
conditioning experiments, but it is far from being a perfect model. to generate other details of classical
conditioning the model needs to be extended, perhaps by adding model-based elements and mechanisms
for adaptively altering some of its parameters. other approaches to modeling classical conditioning
depart signi   cantly from the rescorla   wagner-style error-correction process. bayesian models, for
example, work within a probabilistic framework in which experience revises id203 estimates. all
of these models usefully contribute to our understanding of classical conditioning.

perhaps the most notable feature of the td model is that it is based on a theory   the theory we
have described in this book   that suggests an account of what an animal   s nervous system is trying to
do while undergoing conditioning: it is trying to form accurate long-term predictions, consistent with
the limitations imposed by the way stimuli are represented and how the nervous system works.
in
other words, it suggests a normative account of classical conditioning in which long-term, instead of

w14.3.

instrumental conditioning

297

figure 14.7: time course of us prediction over the course of acquisition for the td model with three dif-
ferent stimulus representations. left: with the complete serial compound (csc), the us prediction increases
exponentially through the interval, peaking at the time of the us. at asymptote (trial 200), the us prediction
peaks at the us intensity (1 in these simulations). middle: with the presence representation, the us prediction
converges to an almost constant level. this constant level is determined by the us intensity and the length of
the cs   us interval. right: with the microstimulus representation, at asymptote, the td model approximates
the exponentially increasing time course depicted with the csc through a linear combination of the di   erent
microstimuli. adapted with minor changes from learning & behavior, evaluating the td model of classical
conditioning, volume 40, 2012, e. a. ludvig, r. s. sutton, e. j. kehoe. with permission of springer.

immediate, prediction is a key feature.

the development of the td model of classical conditioning is one instance in which the explicit
goal was to model some of the details of animal learning behavior. in addition to its standing as an
algorithm, then, td learning is also the basis of this model of aspects of biological learning. as we
discuss in chapter 15, td learning has also turned out to underlie an in   uential model of the activity of
neurons that produce dopamine, a chemical in the brain of mammals that is deeply involved in reward
processing. these are instances in which id23 theory makes detailed contact with
animal behavioral and neural data.

we now turn to considering correspondences between id23 and animal behavior
in instrumental conditioning experiments, the other major type of laboratory experiment studied by
animal learning psychologists.

14.3 instrumental conditioning

in instrumental conditioning experiments learning depends on the consequences of behavior: the de-
livery of a reinforcing stimulus is contingent on what the animal does. in classical conditioning ex-
periments, in contrast, the reinforcing stimulus   the us   is delivered independently of the animal   s
behavior. instrumental conditioning is usually considered to be the same as operant conditioning, the
term b. f. skinner (1938, 1961) introduced for experiments with behavior-contingent reinforcement,
though the experiments and theories of those who use these two terms di   er in a number of ways, some
of which we touch on below. we will exclusively use the term instrumental conditioning for experiments
in which reinforcement is contingent upon behavior. the roots of instrumental conditioning go back
to experiments performed by the american psychologist edward thorndike one hundred years before
publication of the    rst edition of this book.

thorndike observed the behavior of cats when they were placed in    puzzle boxes    from which they
could escape by appropriate actions (figure 14.8). for example, a cat could open the door of one
box by performing a sequence of three separate actions: depressing a platform at the back of the box,
pulling a string by clawing at it, and pushing a bar up or down. when    rst placed in a puzzle box,

  v  v  v298

chapter 14. psychology

figure 14.8: one of thorndike   s puzzle boxes. reprinted from thorndike, animal intelligence: an experimen-
tal study of the associative processes in animals, the psychological review, series of monograph supplements,
ii(4), macmillan, new york, 1898.

with food visible outside, all but a few of thorndike   s cats displayed    evident signs of discomfort    and
extraordinarily vigorous activity    to strive instinctively to escape from con   nement    (thorndike, 1898).

in experiments with di   erent cats and boxes with di   erent escape mechanisms, thorndike recorded
the amounts of time each cat took to escape over multiple experiences in each box. he observed that
the time almost invariably decreased with successive experiences, for example, from 300 seconds to 6
or 7 seconds. he described cats    behavior in a puzzle box like this:

the cat that is clawing all over the box in her impulsive struggle will probably claw the
string or loop or button so as to open the door. and gradually all the other non-successful
impulses will be stamped out and the particular impulse leading to the successful act will
be stamped in by the resulting pleasure, until, after many trials, the cat will, when put in
the box, immediately claw the button or loop in a de   nite way. (thorndike 1898, p. 13)

these and other experiments (some with dogs, chicks, monkeys, and even    sh) led thorndike to for-
mulate a number of    laws    of learning, the most in   uential being the law of e   ect, a version of which
we quoted in chapter 1. this law describes what is generally known as learning by trial and error.
as mentioned in chapter 1, many aspects of the law of e   ect have generated controversy, and its
details have been modi   ed over the years. still the law   in one form or another   expresses an enduring
principle of learning.

essential features of id23 algorithms correspond to features of animal learning
described by the law of e   ect. first, id23 algorithms are selectional, meaning that
they try alternatives and select among them by comparing their consequences. second, reinforcement
learning algorithms are associative, meaning that the alternatives found by selection are associated with
particular situations, or states, to form the agent   s policy. like learning described by the law of e   ect,
id23 is not just the process of    nding actions that produce a lot of reward, but also of
connecting these actions to situations or states. thorndike used the phrase learning by    selecting and
connecting    (hilgard, 1956). natural selection in evolution is a prime example of a selectional process,
but it is not associative (at least as it is commonly understood); supervised learning is associative,
but it is not selectional because it relies on instructions that directly tell the agent how to change its
behavior.

in computational terms, the law of e   ect describes an elementary way of combining search and
memory: search in the form of trying and selecting among many actions in each situation, and memory
in the form of associations linking situations with the actions found   so far   to work best in those

14.3.

instrumental conditioning

299

situations. search and memory are essential components of all id23 algorithms,
whether memory takes the form of an agent   s policy, value function, or environment model.

a id23 algorithm   s need to search means that it has to explore in some way. animals
clearly explore as well, and early animal learning researchers disagreed about the degree of guidance an
animal uses in selecting its actions in situations like thorndike   s puzzle boxes. are actions the result
of    absolutely random, blind groping    (woodworth, 1938, p. 777), or is there some degree of guidance,
either from prior learning, reasoning, or other means? although some thinkers, including thorndike,
seem to have taken the former position, others favored more deliberate exploration. reinforcement
learning algorithms allow wide latitude for how much guidance an agent can employ in selecting actions.
the forms of exploration we have used in the algorithms presented in this book, such as  -greedy and
upper-con   dence-bound action selection, are merely among the simplest. more sophisticated methods
are possible, with the only stipulation being that there has to be some form of exploration for the
algorithms to work e   ectively.

the feature of our treatment of id23 allowing the set of actions available at any
time to depend on the environment   s current state echoes something thorndike observed in his cats   
puzzle-box behaviors. the cats selected actions from those that they instinctively perform in their
current situation, which thorndike called their    instinctual impulses.    first placed in a puzzle box, a
cat instinctively scratches, claws, and bites with great energy: a cat   s instinctual responses to    nding
itself in a con   ned space. successful actions are selected from these and not from every possible action
or activity. this is like the feature of our formalism where the action selected from a state s belongs to
a set of admissible actions, a(s). specifying these sets is an important aspect of id23
because it can radically simplify learning. they are like an animal   s instinctual impulses. on the other
hand, thorndike   s cats might have been exploring according to an instinctual context-speci   c ordering
over actions rather than by just selecting from a set of instinctual impulses. this is another way to
make id23 easier.

among the most prominent animal learning researchers in   uenced by the law of e   ect were clark
hull (e.g., hull, 1943) and b. f. skinner (e.g., skinner, 1938). at the center of their research was
the idea of selecting behavior on the basis of its consequences. id23 has features
in common with hull   s theory, which included eligibility-like mechanisms and secondary reinforcement
to account for the ability to learn when there is a signi   cant time interval between an action and the
consequent reinforcing stimulus (see section 14.4). randomness also played a role in hull   s theory
through what he called    behavioral oscillation    to introduce exploratory behavior.

skinner did not fully subscribe to the memory aspect of the law of e   ect. being averse to the
idea of associative linkages, he instead emphasized selection from spontaneously-emitted behavior. he
introduced the term    operant    to emphasize the key role of an action   s e   ects on an animal   s environ-
ment. unlike the experiments of thorndike and others, which consisted of sequences of separate trials,
skinner   s operant conditioning experiments allowed animal subjects to behave for extended periods of
time without interruption. he invented the operant conditioning chamber, now called a    skinner box,   
the most basic version of which contains a lever or key that an animal can press to obtain a reward,
such as food or water, which would be delivered according to a well-de   ned rule, called a reinforcement
schedule. by recording the cumulative number of lever presses as a function of time, skinner and his
followers could investigate the e   ect of di   erent reinforcement schedules on the animal   s rate of lever-
pressing. modeling results from experiments likes these using the id23 principles we
present in this book is not well developed, but we mention some exceptions in the bibliographic and
historical remarks section at the end of this chapter.

another of skinner   s contributions resulted from his recognition of the e   ectiveness of training an
animal by reinforcing successive approximations of the desired behavior, a process he called shaping. al-
though this technique had been used by others, including skinner himself, its signi   cance was impressed
upon him when he and colleagues were attempting to train a pigeon to bowl by swiping a wooden ball

300

chapter 14. psychology

with its beak. after waiting for a long time without seeing any swipe that they could reinforce, they

... decided to reinforce any response that had the slightest resemblance to a swipe   perhaps,
at    rst, merely the behavior of looking at the ball   and then to select responses which more
closely approximated the    nal form. the result amazed us.
in a few minutes, the ball
was caroming o    the walls of the box as if the pigeon had been a champion squash player.
(skinner, 1958, p. 94)

not only did the pigeon learn a behavior that is unusual for pigeons, it learned quickly through an
interactive process in which its behavior and the reinforcement contingencies changed in response to each
other. skinner compared the process of altering reinforcement contingencies to the work of a sculptor
shaping clay into a desired form. shaping is a powerful technique for computational reinforcement
learning systems as well. when it is di   cult for an agent to receive any non-zero reward signal at all,
either due to sparseness of rewarding situations or their inaccessibility given initial behavior, starting
with an easier problem and incrementally increasing its di   culty as the agent learns can be an e   ective,
and sometimes indispensable, strategy.

a concept from psychology that is especially relevant in the context of instrumental conditioning is
motivation, which refers to processes that in   uence the direction and strength, or vigor, of behavior.
thorndike   s cats, for example, were motivated to escape from puzzle boxes because they wanted the
food that was sitting just outside. obtaining this goal was rewarding to them and reinforced the actions
allowing them to escape. it is di   cult to link the concept of motivation, which has many dimensions,
in a precise way to id23   s computational perspective, but there are clear links with
some of its dimensions.

in one sense, a id23 agent   s reward signal is at the base of its motivation: the agent
is motivated to maximize the total reward it receives over the long run. a key facet of motivation,
then, is what makes an agent   s experience rewarding. in id23, reward signals depend
on the state of the id23 agent   s environment and the agent   s actions. further, as
pointed out in chapter 1, the state of the agent   s environment not only includes information about
what is external to the machine, like an organism or a robot, that houses the agent, but also what
is internal to this machine. some internal state components correspond to what psychologists call an
animal   s motivational state, which in   uences what is rewarding to the animal. for example, an animal
will be more rewarded by eating when it is hungry than when it has just    nished a satisfying meal. the
concept of state dependence is broad enough to allow for many types of modulating in   uences on the
generation of reward signals.

value functions provide a further link to psychologists    concept of motivation.

if the most basic
motive for selecting an action is to obtain as much reward as possible, for a id23
agent that selects actions using a value function, a more proximal motive is to ascend the gradient of
its value function, that is, to select actions expected to lead to the most highly-valued next states (or
what is essentially the same thing, to select actions with the greatest action-values). for these agents,
value functions are the main driving force determining the direction of their behavior.

another dimension of motivation is that an animal   s motivational state not only in   uences learning,
but also in   uences the strength, or vigor, of the animal   s behavior after learning. for example, after
learning to    nd food in the goal box of a maze, a hungry rat will run faster to the goal box than one
that is not hungry. this aspect of motivation does not link so cleanly to the id23
framework we present here, but in the bibliographical and historical remarks section at the end of this
chapter we cite several publications that propose theories of behavioral vigor based on reinforcement
learning.

we turn now to the subject of learning when reinforcing stimuli occur well after the events they
reinforce. the mechanisms used by id23 algorithms to enable learning with de-
layed reinforcement   eligibility traces and td learning   closely correspond to psychologists    hypotheses

14.4. delayed reinforcement

301

about how animals can learn under these conditions.

14.4 delayed reinforcement

the law of e   ect requires a backward e   ect on connections, and some early critics of the law could not
conceive of how the present could a   ect something that was in the past. this concern was ampli   ed
by the fact that learning can even occur when there is a considerable delay between an action and the
consequent reward or penalty. similarly, in classical conditioning, learning can occur when us onset
follows cs o   set by a non-negligible time interval. we call this the problem of delayed reinforcement,
which is related to what minsky (1961) called the    credit-assignment problem for learning systems   : how
do you distribute credit for success among the many decisions that may have been involved in producing
it? the id23 algorithms presented in this book include two basic mechanisms for
addressing this problem. the    rst is the use of eligibility traces, and the second is the use of td
methods to learn value functions that provide nearly immediate evaluations of actions (in tasks like
instrumental conditioning experiments) or that provide immediate prediction targets (in tasks like
classical conditioning experiments). both of these methods correspond to similar mechanisms proposed
in theories of animal learning.

pavlov (1927) pointed out that every stimulus must leave a trace in the nervous system that persists
for some time after the stimulus ends, and he proposed that stimulus traces make learning possible
when there is a temporal gap between the cs o   set and the us onset. to this day, conditioning under
these conditions is called trace conditioning (figure 14.1). assuming a trace of the cs remains when
the us arrives, learning occurs through the simultaneous presence of the trace and the us. we discuss
some proposals for trace mechanisms in the nervous system in chapter 15.

stimulus traces were also proposed as a means for bridging the time interval between actions and
consequent rewards or penalties in instrumental conditioning. in hull   s in   uential learning theory, for
example,    molar stimulus traces    accounted for what he called an animal   s goal gradient, a description
of how the maximum strength of an instrumentally-conditioned response decreases with increasing delay
of reinforcement (hull, 1932, 1943). hull hypothesized that an animal   s actions leave internal stimuli
whose traces decay exponentially as functions of time since an action was taken. looking at the animal
learning data available at the time, he hypothesized that the traces e   ectively reach zero after 30 to 40
seconds.

the eligibility traces used in the algorithms described in this book are like hull   s traces: they are
decaying traces of past state visitations, or of past state   action pairs. eligibility traces were introduced
by klopf (1972) in his neuronal theory in which they are temporally-extended traces of past activity at
synapses, the connections between neurons. klopf   s traces are more complex than the exponentially-
decaying traces our algorithms use, and we discuss this more when we take up his theory in section 15.9.

to account for goal gradients that extend over longer time periods than spanned by stimulus traces,
hull (1943) proposed that longer gradients result from conditioned reinforcement passing backwards
from the goal, a process acting in conjunction with his molar stimulus traces. animal experiments
showed that if conditions favor the development of conditioned reinforcement during a delay period,
learning does not decrease with increased delay as much as it does under conditions that obstruct
secondary reinforcement. conditioned reinforcement is favored if there are stimuli that regularly occur
during the delay interval. then it is as if reward is not actually delayed because there is more immediate
conditioned reinforcement. hull therefore envisioned that there is a primary gradient based on the delay
of the primary reinforcement mediated by stimulus traces, and that this is progressively modi   ed, and
lengthened, by conditioned reinforcement.

algorithms presented in this book that use both eligibility traces and value functions to enable
learning with delayed reinforcement correspond to hull   s hypothesis about how animals are able to

302

chapter 14. psychology

learn under these conditions. the actor   critic architecture discussed in sections 13.5, 15.7, and 15.8
illustrates this correspondence most clearly. the critic uses a td algorithm to learn a value function
associated with the system   s current behavior, that is, to predict the current policy   s return. the
actor updates the current policy based on the critic   s predictions, or more exactly, on changes in the
critic   s predictions. the td error produced by the critic acts as a conditioned reinforcement signal
for the actor, providing an immediate evaluation of performance even when the primary reward signal
itself is considerably delayed. algorithms that estimate action-value functions, such as id24 and
sarsa, similarly use td learning principles to enable learning with delayed reinforcement by means
of conditioned reinforcement. the close parallel between td learning and the activity of dopamine
producing neurons that we discuss in chapter 15 lends additional support to links between reinforcement
learning algorithms and this aspect of hull   s learning theory.

14.5 cognitive maps

model-based id23 algorithms use environment models that have elements in common
with what psychologists call cognitive maps. recall from our discussion of planning and learning in
chapter 8 that by an environment model we mean anything an agent can use to predict how its
environment will respond to its actions in terms of state transitions and rewards, and by planning we
mean any process that computes a policy from such a model. environment models consist of two parts:
the state-transition part encodes knowledge about the e   ect of actions on state changes, and the reward-
model part encodes knowledge about the reward signals expected for each state or each state   action
pair. a model-based algorithm selects actions by using a model to predict the consequences of possible
courses of action in terms of future states and the reward signals expected to arise from those states.
the simplest kind of planning is to compare the predicted consequences of collections of    imagined   
sequences of decisions.

questions about whether or not animals use environment models, and if so, what are the models like
and how are they learned, have played in   uential roles in the history of animal learning research. some
researchers challenged the then-prevailing stimulus-response (s   r) view of learning and behavior, which
corresponds to the simplest model-free way of learning policies, by demonstrating latent learning. in the
earliest latent learning experiment, two groups of rats were run in a maze. for the experimental group,
there was no reward during the    rst stage of the experiment, but food was suddenly introduced into
the goal box of the maze at the start of the second stage. for the control group, food was in the goal
box throughout both stages. the question was whether or not rats in the experimental group would
have learned anything during the    rst stage in the absence of food reward. although the experimental
rats did not appear to learn much during the    rst, unrewarded, stage, as soon as they discovered the
food that was introduced in the second stage, they rapidly caught up with the rats in the control
group.
it was concluded that    during the non-reward period, the rats [in the experimental group]
were developing a latent learning of the maze which they were able to utilize as soon as reward was
introduced    (blodgett, 1929).

latent learning is most closely associated with the psychologist edward tolman, who interpreted this
result, and others like it, as showing that animals could learn a    cognitive map of the environment    in
the absence of rewards or penalties, and that they could use the map later when they were motivated
to reach a goal (tolman, 1948). a cognitive map could also allow a rat to plan a route to the goal that
was di   erent from the route the rat had used in its initial exploration. explanations of results like these
led to the enduring controversy lying at the heart of the behaviorist/cognitive dichotomy in psychology.
in modern terms, cognitive maps are not restricted to models of spatial layouts but are more generally
environment models, or models of an animal   s    task space    (e.g., wilson, takahashi, schoenbaum, and
niv, 2014). the cognitive map explanation of latent learning experiments is analogous to the claim
that animals use model-based algorithms, and that environment models can be learned even without

14.6. habitual and goal-directed behavior

303

explicit rewards or penalties. models are then used for planning when the animal is motivated by the
appearance of rewards or penalties.

tolman   s account of how animals learn cognitive maps was that they learn stimulus-stimulus, or s   s,
associations by experiencing successions of stimuli as they explore an environment. in psychology this is
called expectancy theory: given s   s associations, the occurrence of a stimulus generates an expectation
about the stimulus to come next. this is much like what control engineers call system identi   cation,
in which a model of a system with unknown dynamics is learned from labeled training examples. in
the simplest discrete-time versions, training examples are s   s(cid:48) pairs, where s is a state and s(cid:48), the
subsequent state, is the label. when s is observed, the model creates the    expectation    that s(cid:48) will
be observed next. models more useful for planning involve actions as well, so that examples look like
sa   s(cid:48), where s(cid:48) is expected when action a is executed in state s. it is also useful to learn how the
environment generates rewards. in this case, examples are of the form s   r or sa   r, where r is a
reward signal associated with s or the sa pair. these are all forms of supervised learning by which
an agent can acquire cognitive-like maps whether or not it receives any non-zero reward signals while
exploring its environment.

14.6 habitual and goal-directed behavior

the distinction between model-free and model-based id23 algorithms corresponds
to the distinction psychologists make between habitual and goal-directed control of learned behavioral
patterns. habits are behavior patterns triggered by appropriate stimuli and then performed more-or-
less automatically. goal-directed behavior, according to how psychologists use the phrase, is purposeful
in the sense that it is controlled by knowledge of the value of goals and the relationship between actions
and their consequences. habits are sometimes said to be controlled by antecedent stimuli, whereas goal-
directed behavior is said to be controlled by its consequences (dickinson, 1980, 1985). goal-directed
control has the advantage that it can rapidly change an animal   s behavior when the environment changes
its way of reacting to the animal   s actions. while habitual behavior responds quickly to input from an
accustomed environment, it is unable to quickly adjust to changes in the environment. the development
of goal-directed behavioral control was likely a major advance in the evolution of animal intelligence.

figure 14.9 illustrates the di   erence between model-free and model-based decision strategies in a
hypothetical task in which a rat has to navigate a maze that has distinctive goal boxes, each delivering
an associated reward of the magnitude shown (figure 14.9 top). starting at s1, the rat has to    rst
select left (l) or right (r) and then has to select l or r again at s2 or s3 to reach one of the goal boxes.
the goal boxes are the terminal states of each episode of the rat   s episodic task. a model-free strategy
(figure 14.9 lower left) relies on stored values for state   action pairs. these action values (q-values)
are estimates of the highest return the rat can expect for each action taken from each (nonterminal)
state. they are obtained over many trials of running the maze from start to    nish. when the action
values have become good enough estimates of the optimal returns, the rat just has to select at each
state the action with the largest action value in order to make optimal decisions. in this case, when
the action-value estimates become accurate enough, the rat selects l from s1 and r from s2 to obtain
the maximum return of 4. a di   erent model-free strategy might simply rely on a cached policy instead
of action values, making direct links from s1 to l and from s2 to r. in neither of these strategies do
decisions rely on an environment model. there is no need to consult a state-transition model, and no
connection is required between the features of the goal boxes and the rewards they deliver.

figure 14.9 (lower right) illustrates a model-based strategy. it uses an environment model consisting
of a state-transition model and a reward model. the state-transition model is shown as a decision tree,
and the reward model associates the distinctive features of the goal boxes with the rewards to be found
in each. (the rewards associated with states s1, s2, and s3 are also part of the reward model, but here
they are zero and are not shown.) a model-based agent can decide which way to turn at each state

304

chapter 14. psychology

figure 14.9: model-based and model-free strategies to solve a hypothetical sequential action-selection problem.
top: a rat navigates a maze with distinctive goal boxes, each associated with a reward having the value shown.
lower left: a model-free strategy relies on stored action values for all the state   action pairs obtained over many
learning trials. to make decisions the rat just has to select at each state the action with the largest action
value for that state. lower right: in a model-based strategy, the rat learns an environment model, consisting
of knowledge of state   action-next-state transitions and a reward model consisting of knowledge of the reward
associated with each distinctive goal box. the rat can decide which way to turn at each state by using the
model to simulate sequences of action choices to    nd a path yielding the highest return. adapted from trends in
cognitive science, volume 10, number 8, y. niv, d. joel, and p. dayan, a normative perspective on motivation,
p. 376, 2006, with permission from elsevier.

by using the model to simulate sequences of action choices to    nd a path yielding the highest return.
in this case the return is the reward obtained from the outcome at the end of the path. here, with a
su   ciently accurate model, the rat would select l and then r to obtain reward of 4. comparing the
predicted returns of simulated paths is a simple form of planning, which can be done in a variety of
ways as discussed in chapter 8.

when the environment of a model-free agent changes the way it reacts to the agent   s actions, the
agent has to acquire new experience in the changed environment during which it can update its policy
and/or value function.
in the model-free strategy shown in figure 14.9 (lower left), for example, if
one of the goal boxes were to somehow shift to delivering a di   erent reward, the rat would have to
traverse the maze, possibly many times, to experience the new reward upon reaching that goal box, all
the while updating either its policy or its action-value function (or both) based on this experience. the
key point is that for a model-free agent to change the action its policy speci   es for a state, or to change
an action value associated with a state, it has to move to that state, act from it, possibly many times,
and experience the consequences of its actions.

s1, ls2, ls2, rs3, ls3, rs1, r430432model-free= 4= 0= 2= 3rewardmodel-baseds1lrs2s3lrlr(4)s2s3s1402314.6. habitual and goal-directed behavior

305

a model-based agent can accommodate changes in its environment without this kind of    personal
experience    with the states and actions a   ected by the change. a change in its model automatically
(through planning) changes its policy. planning can determine the consequences of changes in the
environment that have never been linked together in the agent   s own experience. for example, again
referring to the maze task of figure 14.9, imagine that a rat with a previously learned transition and
reward model is placed directly in the goal box to the right of s2 to    nd that the reward available
there now has value 1 instead of 4. the rat   s reward model will change even though the action choices
required to    nd that goal box in the maze were not involved. the planning process will bring knowledge
of the new reward to bear on maze running without the need for additional experience in the maze; in
this case changing the policy to right turns at both s1 and s3 to obtain a return of 3.

exactly this logic is the basis of outcome-devaluation experiments with animals. results from these
experiments provide insight into whether an animal has learned a habit or if its behavior is under
goal-directed control. outcome-devaluation experiments are like latent-learning experiments in that
the reward changes from one stage to the next. after an initial rewarded stage of learning, the reward
value of an outcome is changed, including being shifted to zero or even to a negative value.

an early important experiment of this type was conducted by adams and dickinson (1981). they
trained rats via instrumental conditioning until the rats energetically pressed a lever for sucrose pellets
in a training chamber. the rats were then placed in the same chamber with the lever retracted and
allowed non-contingent food, meaning that pellets were made available to them independently of their
actions. after 15-minutes of this free-access to the pellets, rats in one group were injected with the
nausea-inducing poison lithium chloride. this was repeated for three sessions, in the last of which none
of the injected rats consumed any of the non-contingent pellets, indicating that the reward value of the
pellets had been decreased   the pellets had been devalued. in the next stage taking place a day later,
the rats were again placed in the chamber and given a session of extinction training, meaning that the
response lever was back in place but disconnected from the pellet dispenser so that pressing it did not
release pellets. the question was whether the rats that had the reward value of the pellets decreased
would lever-press less than rats that did not have the reward value of the pellets decreased, even without
experiencing the devalued reward as a result of lever-pressing. it turned out that the injected rats had
signi   cantly lower response rates than the non-injected rats right from the start of the extinction trials.

adams and dickinson concluded that the injected rats associated lever pressing with consequent
nausea by means of a cognitive map linking lever pressing to pellets, and pellets to nausea. hence, in
the extinction trials, the rats    knew    that the consequences of pressing the lever would be something
they did not want, and so they reduced their lever-pressing right from the start. the important point
is that they reduced lever-pressing without ever having experienced lever-pressing directly followed by
being sick: no lever was present when they were made sick. they seemed able to combine knowledge
of the outcome of a behavioral choice (pressing the lever will be followed by getting a pellet) with the
reward value of the outcome (pellets are to be avoided) and hence could alter their behavior accordingly.
not every psychologist agrees with this    cognitive    account of this kind of experiment, and it is not the
only possible way to explain these results, but the model-based planning explanation is widely accepted.

nothing prevents an agent from using both model-free and model-based algorithms, and there are
good reasons for using both. we know from our own experience that with enough repetition, goal-
directed behavior tends to turn into habitual behavior. experiments show that this happens for rats
too. adams (1982) conducted an experiment to see if extended training would convert goal-directed
behavior into habitual behavior. he did this by comparing the e   ect of outcome devaluation on rats
that experienced di   erent amounts of training.
if extended training made the rats less sensitive to
devaluation compared to rats that received less training, this would be evidence that extended training
made the behavior more habitual. adams    experiment closely followed the adams and dickinson
(1981) experiment just described. simplifying a bit, rats in one group were trained until they made
100 rewarded lever-presses, and rats in the other group   the overtrained group   were trained until
they made 500 rewarded lever-presses. after this training, the reward value of the pellets was decreased

306

chapter 14. psychology

(using lithium chloride injections) for rats in both groups. then both groups of rats were given a session
of extinction training. adams    question was whether devaluation would e   ect the rate of lever-pressing
for the overtrained rats less than it would for the non-overtrained rats, which would be evidence that
extended training reduces sensitivity to outcome devaluation. it turned out that devaluation strongly
decreased the lever-pressing rate of the non-overtrained rats. for the overtrained rats, in contrast,
devaluation had little e   ect on their lever-pressing; in fact, if anything, it made it more vigorous.
(the full experiment included control groups showing that the di   erent amounts of training did not by
themselves signi   cantly e   ect lever-pressing rates after learning.) this result suggested that while the
non-overtrained rats were acting in a goal-directed manner sensitive to their knowledge of the outcome
of their actions, the overtrained rats had developed a lever-pressing habit.

viewing this and other results like it from a computational perspective provides insight as to why
one might expect animals to behave habitually in some circumstances, in a goal-directed way in others,
and why they shift from one mode of control to another as they continue to learn. while animals
undoubtedly use algorithms that do not exactly match those we have presented in this book, one
can gain insight into animal behavior by considering the tradeo   s that various id23
algorithms imply. an idea developed by computational neuroscientists daw, niv, and dayan (2005)
is that animals use both model-free and model-based processes. each process proposes an action, and
the action chosen for execution is the one proposed by the process judged to be the more trustworthy
of the two as determined by measures of con   dence that are maintained throughout learning. early in
learning the planning process of a model-based system is more trustworthy because it chains together
short-term predictions which can become accurate with less experience than long-term predictions of the
model-free process. but with continued experience, the model-free process becomes more trustworthy
because planning is prone to making mistakes due to model inaccuracies and short-cuts necessary to
make planning feasible, such as various forms of    tree-pruning   : the removal of unpromising search
tree branches. according to this idea one would expect a shift from goal-directed behavior to habitual
behavior as more experience accumulates. other ideas have been proposed for how animals arbitrate
between goal-directed and habitual control, and both behavioral and neuroscience research continues
to examine this and related questions.

the distinction between model-free and model-based algorithms is proving to be useful for this re-
search. one can examine the computational implications of these types of algorithms in abstract settings
that expose basic advantages and limitations of each type. this serves both to suggest and to sharpen
questions that guide the design of experiments necessary for increasing psychologists    understanding of
habitual and goal-directed behavioral control.

14.7 summary

our goal in this chapter has been to discuss correspondences between id23 and the
experimental study of animal learning in psychology. we emphasized at the outset that reinforcement
learning as described in this book is not intended to model details of animal behavior. it is an abstract
computational framework that explores idealized situations from the perspective of arti   cial intelligence
and engineering. but many of the basic id23 algorithms were inspired by psychological
theories, and in some cases, these algorithms have contributed to the development of new animal learning
models. this chapter described the most conspicuous of these correspondences.

the distinction in id23 between algorithms for prediction and algorithms for control
parallels animal learning theory   s distinction between classical, or pavlovian, conditioning and instru-
mental conditioning. the key di   erence between instrumental and classical conditioning experiments
is that in the former the reinforcing stimulus is contingent upon the animal   s behavior, whereas in
the latter it is not. learning to predict via a td algorithm corresponds to classical conditioning, and
we described the td model of classical conditioning as one instance in which id23

14.7. summary

307

principles account for some details of animal learning behavior. this model generalizes the in   uen-
tial rescorla   wagner model by including the temporal dimension where events within individual trials
in   uence learning, and it provides an account of second-order conditioning, where predictors of rein-
forcing stimuli become reinforcing themselves. it also is the basis of an in   uential view of the activity
of dopamine neurons in the brain, something we take up in chapter 15.

learning by trial and error is at the base of the control aspect of id23. we presented
some details about thorndike   s experiments with cats and other animals that led to his law of e   ect,
which we discussed here and in chapter 1. we pointed out that in id23, exploration
does not have to be limited to    blind groping   ; trials can be generated by sophisticated methods using
innate and previously learned knowledge as long as there is some exploration. we discussed the training
method b. f. skinner called shaping in which reward contingencies are progressively altered to train
an animal to successively approximate a desired behavior. shaping is not only indispensable for animal
training, it is also an e   ective tool for training id23 agents. there is also a connection
to the idea of an animal   s motivational state, which in   uences what an animal will approach or avoid
and what events are rewarding or punishing for the animal.

the id23 algorithms presented in this book include two basic mechanisms for
addressing the problem of delayed reinforcement: eligibility traces and value functions learned via td
algorithms. both mechanisms have antecedents in theories of animal learning. eligibility traces are
similar to stimulus traces of early theories, and value functions correspond to the role of secondary
reinforcement in providing nearly immediate evaluative feedback.

the next correspondence the chapter addressed is that between id23   s environment
models and what psychologists call cognitive maps. experiments conducted in the mid 20th century
purported to demonstrate the ability of animals to learn cognitive maps as alternatives to, or as additions
to, state   action associations, and later use them to guide behavior, especially when the environment
changes unexpectedly. environment models in id23 are like cognitive maps in that
they can be learned by supervised learning methods without relying on reward signals, and then they
can be used later to plan behavior.

id23   s distinction between model-free and model-based algorithms corresponds to
the distinction in psychology between habitual and goal-directed behavior. model-free algorithms make
decisions by accessing information that has been strored in a policy or an action-value function, whereas
model-based methods select actions as the result of planning ahead using a model of the agent   s envi-
ronment. outcome-devaluation experiments provide information about whether an animal   s behavior
is habitual or under goal-directed control. id23 theory has helped clarify thinking
about these issues.

animal learning clearly informs id23, but as a type of machine learning, reinforce-
ment learning is directed toward designing and understanding e   ective learning algorithms, not toward
replicating or explaining details of animal behavior. we focused on aspects of animal learning that
relate in clear ways to methods for solving prediction and control problems, highlighting the fruitful
two-way    ow of ideas between id23 and psychology without venturing deeply into
many of the behavioral details and controversies that have occupied the attention of animal learning
researchers. future development of id23 theory and algorithms will likely exploit
links to many other features of animal learning as the computational utility of these features becomes
better appreciated. we expect that a    ow of ideas between id23 and psychology will
continue to bear fruit for both disciplines.

many connections between id23 and areas of psychology and other behavioral
sciences are beyond the scope of this chapter. we largely omit discussing links to the psychology of
decision making, which focuses on how actions are selected, or how decisions are made, after learning
has taken place. we also do not discuss links to ecological and evolutionary aspects of behavior studied
by ethologists and behavioral ecologists: how animals relate to one another and to their physical

308

chapter 14. psychology

surroundings, and how their behavior contributes to evolutionary    tness. optimization, mdps, and
id145    gure prominently in these    elds, and our emphasis on agent interaction with
dynamic environments connects to the study of agent behavior in complex    ecologies.    multi-agent
id23, omitted in this book, has connections to social aspects of behavior. despite
the lack of treatment here, id23 should by no means be interpreted as dismissing
evolutionary perspectives. nothing about id23 implies a tabula rasa view of learning
and behavior.
indeed, experience with engineering applications has highlighted the importance of
building into id23 systems knowledge that is analogous to what evolution provides to
animals.

bibliographical and historical remarks

ludvig, bellemare, and pearson (2011) and shah (2012) review id23 in the contexts
of psychology and neuroscience. these publications are useful companions to this chapter and the
following chapter on id23 and neuroscience.

14.1

dayan, niv, seymour, and daw (2006) focused on interactions between classical and instru-
mental conditioning, particularly situations where classically-conditioned and instrumental re-
sponses are in con   ict. they proposed a id24 framework for modeling aspects of this
interaction. modayil and sutton (2014) used a mobile robot to demonstrate the e   ectiveness of
a control method combining a    xed response with online prediction learning. calling this pavlo-
vian control , they emphasized that it di   ers from the usual control methods of reinforcement
learning, being based on predictively executing    xed responses and not on reward maximiza-
tion. the electro-mechanical machine of ross (1933) and especially the learning version of
walter   s turtle (walter, 1951) were very early illustrations of pavlovian control. what is now
called pavlovian-instrumental transfer was    rst observed by estes (1943, 1948).

14.2.1 kamin (1968)    rst reported blocking, now commonly known as kamin blocking, in classical
conditioning. moore and schmajuk (2008) provide an excellent summary of the blocking phe-
nomenon, the research it stimulated, and its lasting in   uence on animal learning theory. gibbs,
cool, land, kehoe, and gormezano (1991) describe second-order conditioning of the rabbit   s
nictitating membrane response and its relationship to conditioning with serial-compound stim-
uli. finch and culler (1934) reported obtaining    fth-order conditioning of a dog   s foreleg
withdrawal    when the motivation of the animal is maintained through the various orders.   

14.2.2 the idea built into the rescorla   wagner model that learning occurs when animals are sur-
prised is derived from kamin (1969). models of classical conditioning other than rescorla and
wagner   s include the models of klopf (1988), grossberg (1975), mackintosh (1975), moore and
stickney (1980), pearce and hall (1980), and courville, daw, and touretzky (2006). schmajuk
(2008) review models of classical conditioning.

14.2.3 an early version of the td model of classical conditioning appeared in sutton and barto (1981),
which also included the early model   s prediction that temporal primacy overrides blocking, later
shown by kehoe, scheurs, and graham (1987) to occur in the rabbit nictitating membrane
preparation. sutton and barto (1981) contains the earliest recognition of the near identity be-
tween the rescorla   wagner model and the least-mean-square (lms), or widrow-ho   , learning
rule (widrow and ho   , 1960). this early model was revised following sutton   s development
of the td algorithm (sutton, 1984, 1988) and was    rst presented as the td model in sutton
and barto (1987) and more completely in sutton and barto (1990), upon which this section is
largely based. additional exploration of the td model and its possible neural implementation

14.7. summary

309

was conducted by moore and colleagues (moore, desmond, berthier, blazis, sutton, and barto,
1986; moore and blazis, 1989; moore, choi, and brunzell, 1998; moore, marks, castagna, and
polewan, 2001). klopf   s (1988) drive-reinforcement theory of classical conditioning extends
the td model to address additional experimental details, such as the s-shape of acquisition
curves. in some of these publications td is taken to mean time derivative instead of temporal
di   erence.

14.2.4 ludvig, sutton, and kehoe (2012) evaluated the performance of the td model in previously
unexplored tasks involving classical conditioning and examined the in   uence of various stim-
ulus representations, including the microstimulus representation that they introduced earlier
(ludvig, sutton, and kehoe, 2008). earlier investigations of the in   uence of various stimulus
representations and their possible neural implementations on response timing and topography
in the context of the td model are those of moore and colleagues cited above. although not in
the context of the td model, representations like the microstimulus representation of ludvig et
al. (2012) have been proposed and studied by grossberg and schmajuk (1989), brown, bullock,
and grossberg (1999), buhusi and schmajuk (1999), and machado (1997).

14.4

14.4

14.5

14.6

section 1.7 includes comments on the history of trial-and-error learning and the law of e   ect.
the idea that thorndikes cats might have been exploring according to an instinctual context-
speci   c ordering over actions rather than by just selecting from a set of instinctual impulses
was suggested by peter dayan (personal communication). selfridge, sutton, and barto (1985)
illustrated the e   ectiveness of shaping in a pole-balancing id23 task. other
examples of shaping in id23 are gullapalli and barto (1992), mahadevan
and connell (1992), mataric (1994), dorigo and colombette (1994), saksida, raymond, and
touretzky (1997), and randl  v and alstr  m (1998). ng (2003) and ng, harada, and russell
(1999) used the term shaping in a sense somewhat di   erent from skinner   s, focussing on the
problem of how to alter the reward signal without altering the set of optimal policies.

dickinson and balleine (2002) discuss the complexity of the interaction between learning and
motivation. wise (2004) provides an overview of id23 and its relation to
motivation. daw and shohamy (2008) link motivation and learning to aspects of reinforcement
learning theory. see also mcclure, daw, and montague (2003), niv, joel, and dayan (2006),
rangel et al. (2008), and dayan and berridge (2014). mcclure et al. (2003), niv, daw, and
dayan (2005), and niv, daw, joel, and dayan (2007) present theories of behavioral vigor
related to the id23 framework.

spence, hull   s student and collaborator at yale, elaborated the role of higher-order reinforce-
ment in addressing the problem of delayed reinforcement (spence, 1947). learning over very
long delays, as in taste-aversion conditioning with delays up to several hours, led to interference
theories as alternatives to decaying-trace theories (e.g., revusky and garcia, 1970; boakes and
costa, 2014). other views of learning under delayed reinforcement invoke roles for awareness
and working memory (e.g., clark and squire, 1998; seo, barraclough, and lee, 2007).

thistlethwaite (1951) is an extensive review of latent learning experiments up to the time of its
publication. ljung (1998) is an overview of model learning, or system identi   cation, techniques
in engineering. gopnik, glymour, sobel, schulz, kushnir, and danks (2004) present a bayesian
theory about how children learn models.

connections between habitual and goal-directed behavior and model-free and model-based re-
inforcement learning were    rst proposed by daw, niv, and dayan (2005). the hypothetical
maze task used to explain habitual and goal-directed behavioral control is based on the expla-
nation of niv, joel, and dayan (2006). dolan and dayan (2013) review four generations of

310

chapter 14. psychology

experimental research related to this issue and discuss how it can move forward on the basis of
id23   s model-free/model-based distinction. dickinson (1980, 1985) and dick-
inson and balleine (2002) discuss experimental evidence related to this distinction. donahoe
and burgos (2000) alternatively argue that model-free processes can account for the results of
outcome-devaluation experiments. dayan and berridge (2014) argue that classical conditioning
involves model-based processes. rangel, camerer, and montague (2008) review many of the
outstanding issues involving habitual, goal-directed, and pavlovian modes of control.

comments on terminology    the traditional meaning of reinforcement in psychology is the strength-
ening of a pattern of behavior (by increasing either its intensity or frequency) as a result of an animal
receiving a stimulus (or experiencing the omission of a stimulus) in an appropriate temporal relation-
ship with another stimulus or with a response. reinforcement produces changes that remain in future
behavior. sometimes in psychology reinforcement refers to the process of producing lasting changes in
behavior, whether the changes strengthen or weaken a behavior pattern (mackintosh, 1983). letting
reinforcement refer to weakening in addition to strengthening is at odds with the everyday meaning of
reinforce, and its traditional use in psychology, but it is a useful extension that we have adopted here.
in either case, a stimulus considered to be the cause of the behavioral change is called a reinforcer.

psychologists do not generally use the speci   c phrase id23 as we do. animal
learning pioneers probably regarded reinforcement and learning as being synonymous, so it would be
redundant to use both words. our use of the phrase follows its use in computational and engineering re-
search, in   uenced mostly by minsky (1961). but the phrase is lately gaining currency in psychology and
neuroscience, likely because strong parallels have surfaced between id23 algorithms
and animal learning   parallels described in this chapter and the next.

according to common usage, a reward is an object or event that an animal will approach and work
for. a reward may be given to an animal in recognition of its    good    behavior, or given in order to make
the animal   s behavior    better.    similarly, a penalty is an object or event that the animal usually avoids
and that is given as a consequence of    bad    behavior, usually in order to change that behavior. primary
reward is reward due to machinery built into an animal   s nervous system by evolution to improve its
chances of survival and reproduction, e.g., reward produced by the taste of nourishing food, sexual
contact, successful escape, and many other stimuli and events that predicted reproductive success over
the animal   s ancestral history. as explained in section 14.2.1, higher-order reward is reward delivered
by stimuli that predict primary reward, either directly or indirectly by predicting other stimuli that
predict primary reward. reward is secondary if its rewarding quality is the result of directly predicting
primary reward.

i this book we call rt the    reward signal at time t    or sometimes just the    reward at time t,    but we
do not think of it as an object or event in the agent   s environment. because rt is a number   not an
object or an event   it is more like a reward signal in neuroscience, which is a signal internal to the
brain, like the activity of neurons, that in   uences decision making and learning. this signal might be
triggered when the animal perceives an attractive (or an aversive) object, but it can also be triggered
by things that do not physically exist in the animal   s external environment, such as memories, ideas, or
hallucinations. because our rt can be positive, negative, or zero, it might be better to call a negative
rt a penalty, and an rt equal to zero a neutral signal, but for simplicity we generally avoid these terms.
in id23, the process that generates all the rts de   nes the problem the agent is
trying to solve. the agent   s objective is to keep the magnitude of rt as large as possible over time. in
this respect, rt is like primary reward for an animal if we think of the problem the animal faces as the
problem of obtaining as much primary reward as possible over its lifetime (and thereby, through the
prospective    wisdom    of evolution, improve its chances of solving its real problem, which is to pass its
genes on to future generations). however, as we suggest in chapter 15, it is unlikely that there is a
single    master    reward signal like rt in an animal   s brain.

not all reinforcers are rewards or penalties. sometimes reinforcement is not the result of an animal

14.7. summary

311

receiving a stimulus that evaluates its behavior by labeling the behavior good or bad. a behavior
pattern can be reinforced by a stimulus that arrives to an animal no matter how the animal behaved. as
described in section 14.1, whether the delivery of reinforcer depends, or does not depend, on preceding
behavior is the de   ning di   erence between instrumental, or operant, conditioning experiments and
classical, or pavlovian, conditioning experiments. reinforcement is at work in both types of experiments,
but only in the former is it feedback that evaluates past behavior. (though it has often been pointed
out that even when the reinforcing us in a classical conditioning experiment is not contingent on the
subject   s preceding behavior, its reinforcing value can be in   uenced by this behavior, an example being
that a closed eye makes an air pu    to the eye less aversive.)

the distinction between reward signals and reinforcement signals is a crucial point when we discuss
neural correlates of these signals in the next chapter. like a reward signal, for us, the reinforcement
signal at any speci   c time is a positive or negative number, or zero. a reinforcement signal is the major
factor directing changes a learning algorithm makes in an agent   s policy, value estimates, or environment
models. the de   nition that makes the most sense to us is that a reinforcement signal at any time is a
number that multiplies (possibly along with some constants) a vector to determine parameter updates
in some learning algorithm.

for some algorithms, the reward signal alone is the critical multiplier in the parameter-update equa-
tion. for these algorithms the reinforcement signal is the same as the reward signal. but for most of
the algorithms we discuss in this book, reinforcement signals include terms in addition to the reward
signal, an example being a td error   t = rt+1 +   v (st+1)     v (st), which is the reinforcement signal
for td state-value learning (and analogous td errors for action-value learning). in this reinforcement
signal, rt+1 is the primary reinforcement contribution, and the temporal di   erence in predicted values,
  v (st+1)     v (st) (or an analogous temporal di   erence for action values), is the conditioned reinforce-
ment contribution. thus, whenever   v (st+1)    v (st) = 0,   t signals    pure    primary reinforcement; and
whenever rt+1 = 0, it signals    pure    conditioned reinforcement, but it often signals a mixture of these.
note as we mentioned in section 6.1, this   t is not available until time t + 1. we therefore think of   t as
the reinforcement signal at time t + 1, which is    tting because it reinforces predictions and/or actions
made earlier at step t.

a possible source of confusion is the terminology used by the famous psychologist b.f. skinner and
his followers. for skinner, positive reinforcement occurs when the consequences of an animal   s behavior
increase the frequency of that behavior; punishment occurs when the behavior   s consequences decrease
that behavior   s frequency. negative reinforcement occurs when behavior leads to the removal of an
aversive stimulus (that is, a stimulus the animal does not like), thereby increasing the frequency of
that behavior. negative punishment, on the other hand, occurs when behavior leads to the removal of
an appetitive stimulus (that is, a stimulus the animal likes), thereby decreasing the frequency of that
behavior. we    nd no critical need for these distinctions because our approach is more abstract than
this, with both reward and reinforcement signals allowed to take on both positive and negative values.
(but note especially that when our reinforcement signal is negative, it is not the same as skinner   s
negative reinforcement.)

on the other hand, it has often been pointed out that using a single number as a reward or a
penalty signal, depending only on its sign, is at odds with the fact that animals    appetitive and aversive
systems have qualitatively di   erent properties and involve di   erent brain mechanisms. this points to
a direction in which the id23 framework might be developed in the future to exploit
computational advantages of separate appetitive and aversive systems, but for now we are passing over
these possibilities.

another discrepancy in terminology is how we use the word action. to many cognitive scientists, an
action is purposeful in the sense of being the result of an animal   s knowledge about the relationship
between the behavior in question and the consequences of that behavior. an action is goal-directed and
the result of a decision, in contrast to a response, which is triggered by a stimulus; the result of a re   ex or

312

chapter 14. psychology

a habit. we use the word action without di   erentiating among what others call actions, decisions, and
responses. these are important distinctions, but for us they are encompassed by di   erences between
model-free and model-based id23 algorithms, which we discussed above in relation to
habitual and goal-directed behavior in section 14.6. dickinson (1985) discusses the distinction between
responses and actions.

a term used a lot in this book is control. what we mean by control is entirely di   erent from what it
means to animal learning psychologists. by control we mean that an agent in   uences its environment to
bring about states or events that the agent prefers: the agent exerts control over its environment. this
is the sense of control used by control engineers. in psychology, on the other hand, control typically
means that an animal   s behavior is in   uenced by   is controlled by   the stimuli the animal receives
(stimulus control) or the reinforcement schedule it experiences. here the environment is controlling the
agent. control in this sense is the basis of behavior modi   cation therapy. of course, both of these
directions of control are at play when an agent interacts with its environment, but our focus is on the
agent as controller; not the environment as controller. a view equivalent to ours, and perhaps more
illuminating, is that the agent is actually controlling the input it receives from its environment (powers,
1973). this is not what psychologists mean by stimulus control.

sometimes id23 is understood to refer solely to learning policies directly from
rewards (and penalties) without the involvement of value functions or environment models. this is
what psychologists call stimulus-response, or s-r, learning. but for us, along with most of today   s
psychologists, id23 is much broader than this, including in addition to s-r learn-
ing, methods involving value functions, environment models, planning, and other processes that are
commonly thought to belong to the more cognitive side of mental functioning.

chapter 15

neuroscience

neuroscience is the multidisciplinary study of nervous systems: how they regulate bodily functions;
control behavior; change over time as a result of development, learning, and aging; and how cellular and
molecular mechanisms make these functions possible. one of the most exciting aspects of reinforcement
learning is the mounting evidence from neuroscience that the nervous systems of humans and many other
animals implement algorithms that correspond in striking ways to id23 algorithms.
the main objective of this chapter is to explain these parallels and what they suggest about the neural
basis of reward-related learning in animals.

the most remarkable point of contact between id23 and neuroscience involves
dopamine, a chemical deeply involved in reward processing in the brains of mammals. dopamine
appears to convey temporal-di   erence (td) errors to brain structures where learning and decision
making take place. this parallel is expressed by the reward prediction error hypothesis of dopamine
neuron activity, a hypothesis that resulted from the convergence of computational id23
and results of neuroscience experiments. in this chapter we discuss this hypothesis, the neuroscience
   ndings that led to it, and why it is a signi   cant contribution to understanding brain reward systems.
we also discuss parallels between id23 and neuroscience that are less striking than
this dopamine/td-error parallel but that provide useful conceptual tools for thinking about reward-
based learning in animals. other elements of id23 have the potential to impact the
study of nervous systems, but their connections to neuroscience are still relatively undeveloped. we
discuss several of these evolving connections that we think will grow in importance over time.

as we outlined in the history section of this book   s introductory chapter (section 1.7), many aspects of
id23 were in   uenced by neuroscience. a second objective of this chapter is to acquaint
readers with ideas about brain function that have contributed to our approach to id23.
some elements of id23 are easier to understand when seen in light of theories of brain
function. this is particularly true for the idea of the eligibility trace, one of the basic mechanisms of
id23, that originated as a conjectured property of synapses, the structures by which
nerve cells   neurons   communicate with one another.

in this chapter we do not delve very deeply into the enormous complexity of the neural systems
underlying reward-based learning in animals: this chapter is too short, and we are not neuroscientists.
we do not try to describe   or even to name   the very many brain structures and pathways, or any
of the molecular mechanisms, believed to be involved in these processes. we also do not do justice to
hypotheses and models that are alternatives to those that align so well with id23. it
should not be surprising that there are di   ering views among experts in the    eld. we can only provide
a glimpse into this fascinating and developing story. we hope, though, that this chapter convinces
you that a very fruitful channel has emerged connecting id23 and its theoretical

313

314

chapter 15. neuroscience

underpinnings to the neuroscience of reward-based learning in animals.

many excellent publications cover links between id23 and neuroscience, some of
which we cite in this chapter   s    nal section. our treatment di   ers from most of these because we assume
familiarity with id23 as presented in the earlier chapters of this book, but we do not
assume knowledge of neuroscience. we begin with a brief introduction to the neuroscience concepts
needed for a basic understanding of what is to follow.

15.1 neuroscience basics

some basic information about nervous systems is helpful for following what we cover in this chapter.
terms that we refer to later are italicized. skipping this section will not be a problem if you already
have an elementary knowledge of neuroscience.

neurons, the main components of nervous systems, are cells specialized for processing and transmit-
ting information using electrical and chemical signals. they come in many forms, but a neuron typically
has a cell body, dendrites, and a single axon. dendrites are structures that branch from the cell body
to receive input from other neurons (or to also receive external signals in the case of sensory neurons).
a neuron   s axon is a    ber that carries the neuron   s output to other neurons (or to muscles or glands).
a neuron   s output consists of sequences of electrical pulses called action potentials that travel along the
axon. action potentials are also called spikes, and a neuron is said to    re when it generates a spike.
in models of neural networks it is common to use real numbers to represent a neuron   s    ring rate, the
average number of spikes per some unit of time.

a neuron   s axon can branch widely so that the neuron   s action potentials reach many targets. the
branching structure of a neuron   s axon is called the neuron   s axonal arbor. because the conduction
of an action potential is an active process, not unlike the burning of a fuse, when an action potential
reaches an axonal branch point it    lights up    action potentials on all of the outgoing branches (although
propagation to a branch can sometimes fail). as a result, the activity of a neuron with a large axonal
arbor can in   uence many target sites.

a synapse is a structure generally at the termination of an axon branch that mediates the com-
munication of one neuron to another. a synapse transmits information from the presynaptic neuron   s
axon to a dendrite or cell body of the postsynaptic neuron. with a few exceptions, synapses release a
chemical neurotransmitter upon the arrival of an action potential from the presynaptic neuron. (the
exceptions are cases of direct electric coupling between neurons, but these will not concern us here.)
neurotransmitter molecules released from the presynaptic side of the synapse di   use across the synaptic
cleft, the very small space between the presynaptic ending and the postsynaptic neuron, and then bind
to receptors on the surface of the postsynaptic neuron to excite or inhibit its spike-generating activity,
or to modulate its behavior in other ways. a particular neurotransmitter may bind to several di   erent
types of receptors, with each producing a di   erent e   ect on the postsynaptic neuron. for example,
there are at least    ve di   erent receptor types by which the neurotransmitter dopamine can a   ect a
postsynaptic neuron. many di   erent chemicals have been identi   ed as neurotransmitters in animal
nervous systems.

a neuron   s background activity is its level of activity, usually its    ring rate, when the neuron does
not appear to be driven by synaptic input related to the task of interest to the experimenter, for
example, when the neuron   s activity is not correlated with a stimulus delivered to a subject as part
of an experiment. background activity can be irregular due to input from the wider network, or due
to noise within the neuron or its synapses. sometimes background activity is the result of dynamic
processes intrinsic to the neuron. a neuron   s phasic activity, in contrast to its background activity,
consists of bursts of spiking activity usually caused by synaptic input. activity that varies slowly and
often in a graded manner, whether as background activity or not, is called a neuron   s tonic activity.

15.2. reward signals, reinforcement signals, values, and prediction
errors

315

the strength or e   ectiveness by which the neurotransmitter released at a synapse in   uences the post-
synaptic neuron is the synapse   s e   cacy. one way a nervous system can change through experience is
through changes in synaptic e   cacies as a result of combinations of the activities of the presynaptic and
postsynaptic neurons, and sometimes by the presence of a neuromodulator, which is a neurotransmitter
having e   ects other than, or in addition to, direct fast excitation or inhibition.

brains contain several di   erent neuromodulation systems consisting of clusters of neurons with widely
branching axonal arbors, with each system using a di   erent neurotransmitter. neuromodulation can
alter the function of neural circuits, mediate motivation, arousal, attention, memory, mood, emotion,
sleep, and body temperature. important here is that a neuromodulatory system can distribute some-
thing like a scalar signal, such as a reinforcement signal, to alter the operation of synapses in widely
distributed sites critical for learning.

the ability of synaptic e   cacies to change is called synaptic plasticity. it is one of the primary mecha-
nisms responsible for learning. the parameters, or weights, adjusted by learning algorithms correspond
to synaptic e   cacies. as we detail below, modulation of synaptic plasticity via the neuromodulator
dopamine is a plausible mechanism for how the brain might implement learning algorithms like many
of those described in this book.

15.2 reward signals, reinforcement signals, values, and pre-

diction errors

links between neuroscience and computational id23 begin as parallels between signals
in the brain and signals playing prominent roles in id23 theory and algorithms. in
chapter 3 we said that any problem of learning goal-directed behavior can be reduced to the three
signals representing actions, states, and rewards. however, to explain links that have been made
between neuroscience and id23, we have to be less abstract than this and consider
other id23 signals that correspond, in certain ways, to signals in the brain. in addition
to reward signals, these include reinforcement signals (which we argue are di   erent from reward signals),
value signals, and signals conveying prediction errors. when we label a signal by its function in this
way, we are doing it in the context of id23 theory in which the signal corresponds to
a term in an equation or an algorithm. on the other hand, when we refer to a signal in the brain, we
mean a physiological event such as a burst of action potentials or the secretion of a neurotransmitter.
labeling a neural signal by its function, for example calling the phasic activity of a dopamine neuron a
reinforcement signal, means that the neural signal behaves like, and is conjectured to function like, the
corresponding theoretical signal.

uncovering evidence for these correspondences involves many challenges. neural activity related to
reward processing can be found in nearly every part of the brain, and it is di   cult to interpret results
unambiguously because representations of di   erent reward-related signals tend to be highly correlated
with one another. experiments need to be carefully designed to allow one type of reward-related signal
to be distinguished with any degree of certainty from others   or from an abundance of other signals not
related to reward processing. despite these di   culties, many experiments have been conducted with
the aim of reconciling aspects of id23 theory and algorithms with neural signals, and
some compelling links have been established. to prepare for examining these links, in the rest of this
section we remind the reader of what various reward-related signals mean according to reinforcement
learning theory.

in our comments on terminology at the end of the previous chapter, we said that rt is like a reward
signal in an animal   s brain and not as an object or event in the animal   s environment. in reinforcement
learning, the reward signal (along with an agent   s environment) de   nes the problem a reinforcement
learning agent is trying to solve. it this respect, rt is like a signal in an animal   s brain that distributes

316

chapter 15. neuroscience

primary reward to sites throughout the brain. but it is unlikely that a unitary master reward signal
like rt exists in an animal   s brain. it is best to think of rt as an abstraction summarizing the overall
e   ect of a multitude of neural signals generated by many systems in the brain that assess the rewarding
or punishing qualities of sensations and states.

reinforcement signals in id23 are di   erent from reward signals. the function of
a reinforcement signal is to direct the changes a learning algorithm makes in an agent   s policy, value
estimates, or environment models. for a td method, for instance, the reinforcement signal at time t
is the td error   t   1 = rt +   v (st)     v (st   1).1 the reinforcement signal for some algorithms could
be just the reward signal, but for most of the algorithms we consider the reinforcement signal is the
reward signal adjusted by other information, such as the value estimates in td errors.

estimates of state values or of action values, that is, v or q, specify what is good or bad for the agent
over the long run. they are predictions of the total reward an agent can expect to accumulate over the
future. agents make good decisions by selecting actions leading to states with the largest estimated
state values, or by selecting actions with the largest estimated action values.

prediction errors measure discrepancies between expected and actual signals or sensations. reward
prediction errors (rpes) speci   cally measure discrepancies between the expected and the received re-
ward signal, being positive when the reward signal is greater than expected, and negative otherwise.
td errors like (6.5) are special kinds rpes that signal discrepancies between current and earlier expec-
tations of reward over the long-term. when neuroscientists refer to rpes they generally (though not
always) mean td rpes, which we simply call td errors throughout this chapter. also in this chapter,
a td error is generally one that does not depend on actions, as opposed to td errors used in learning
action-values by algorithms like sarsa and id24. this is because the most well-known links to
neuroscience are stated in terms of action-free td errors, but we do not mean to rule out possible sim-
ilar links involving action-dependent td errors. (td errors for predicting signals other than rewards
are useful too, but that case will not concern us here. see, for example, modayil, white, and sutton,
2014.)

one can ask many questions about links between neuroscience data and these theoretically-de   ned
signals. is an observed signal more like a reward signal, a value signal, a prediction error, a reinforcement
signal, or something altogether di   erent? and if it is an error signal, is it an rpe, a td error, or a
simpler error like the rescorla   wagner error (14.3)? and if it is a td error, does it depend on actions like
the td error of id24 or sarsa? as indicated above, probing the brain to answer questions like these
is extremely di   cult. but experimental evidence suggests that one neurotransmitter, speci   cally the
neurotransmitter dopamine, signals rpes, and further, that the phasic activity of dopamine-producing
neurons in fact conveys td errors (see section 15.1 for a de   nition of phasic activity). this evidence
led to the reward prediction error hypothesis of dopamine neuron activity, which we describe next.

15.3 the reward prediction error hypothesis

the reward prediction error hypothesis of dopamine neuron activity proposes that one of the functions of
the phasic activity of dopamine-producing neurons in mammals is to deliver an error between an old and
a new estimate of expected future reward to target areas throughout the brain. this hypothesis (though
not in these exact words) was    rst explicitly stated by montague, dayan, and sejnowski (1996), who
showed how the td error concept from id23 accounts for many features of the phasic
activity of dopamine neurons in mammals. the experiments that led to this hypothesis were performed
in the 1980s and early 1990s in the laboratory of neuroscientist wolfram schultz. section 15.5 describes

1 as we mentioned in section 6.1,   t in our notation is de   ned to be rt+1 +   v (st+1)     v (st), so   t is not available
until time t + 1. the td error available at t is actually   t   1 = rt +   v (st)     v (st   1). since we are thinking of time
steps as very small, or even in   nitesimal, time intervals, one should not attribute undue importance to this one-step time
shift.

15.3. the reward prediction error hypothesis

317

these in   uential experiments, section 15.6 explains how the results of these experiments align with td
errors, and the bibliographical and historical remarks section at the end of this chapter includes a
guide to the literature surrounding the development of this in   uential hypothesis.

montague et al. (1996) compared the td errors of the td model of classical conditioning with the
phasic activity of dopamine-producing neurons during classical conditioning experiments. recall from
section 14.2 that the td model of classical conditioning is basically the semi-gradient-descent td(  )
algorithm with linear function approximation. montague et al. made several assumptions to set up this
comparison. first, since a td error can be negative but neurons cannot have a negative    ring rate,
they assumed that the quantity corresponding to dopamine neuron activity is   t   1 + bt, where bt is
the background    ring rate of the neuron. a negative td error corresponds to a drop in a dopamine
neuron   s    ring rate below its background rate.2

a second assumption was needed about the states visited in each classical conditioning trial and
how they are represented as inputs to the learning algorithm. this is the same issue we discussed in
section 14.2.4 for the td model. montague et al. chose a complete serial compound (csc) represen-
tation as shown in the left column of figure 14.2, but where the sequence of short-duration internal
signals continues until the onset of the us, which here is the arrival of a non-zero reward signal. this
representation allows the td error to mimic the fact that dopamine neuron activity not only predicts
a future reward, but that it is also sensitive to when after a predictive cue that reward is expected
to arrive. there has to be some way to keep track of the time between sensory cues and the arrival
of reward. if a stimulus initiates a sequence of internal signals that continues after the stimulus ends,
and if there is a di   erent signal for each time step following the stimulus, then each time step after the
stimulus is represented by a distinct state. thus, the td error, being state-dependent, can be sensitive
to the timing of events within a trial.

in simulated trials with these assumptions about background    ring rate and input representation,
td errors of the td model are remarkably similar to dopamine neuron phasic activity. previewing our
description of details about these similarities in section 15.5 below, the td errors parallel the following
features of dopamine neuron activity: 1) the phasic response of a dopamine neuron only occurs when
a rewarding event is unpredicted; 2) early in learning, neutral cues that precede a reward do not cause
substantial phasic dopamine responses, but with continued learning these cues gain predictive value
and come to elicit phasic dopamine responses; 3) if an even earlier cue reliably precedes a cue that has
already acquired predictive value, the phasic dopamine response shifts to the earlier cue, ceasing for
the later cue; and 3) if after learning, the predicted rewarding event is omitted, a dopamine neuron   s
response decreases below its baseline level shortly after the expected time of the rewarding event.

although not every dopamine neuron monitored in the experiments of schultz and colleagues behaved
in all of these ways, the striking correspondence between the activities of most of the monitored neurons
and td errors lends strong support to the reward prediction error hypothesis. there are situations,
however, in which predictions based on the hypothesis do not match what is observed in experiments.
the choice of input representation is critical to how closely td errors match some of the details of
dopamine neuron activity, particularly details about the timing of dopamine neuron responses. di   erent
ideas, some of which we discuss below, have been proposed about input representations and other
features of td learning to make the td errors    t the data better, though the main parallels appear
with the csc representation that montague et al. used. overall, the reward prediction error hypothesis
has received wide acceptance among neuroscientists studying reward-based learning, and it has proven
to be remarkably resilient in the face of accumulating results from neuroscience experiments.

to prepare for our description of the neuroscience experiments supporting the reward prediction error
hypothesis, and to provide some context so that the signi   cance of the hypothesis can be appreciated,
we next present some of what is known about dopamine, the brain structures it in   uences, and how it

2in the literature relating td errors to the activity of dopamine neurons, their   t is the same as our   t   1 = rt +

  v (st)     v (st   1).

318

chapter 15. neuroscience

is involved in reward-based learning.

15.4 dopamine

dopamine is produced as a neurotransmitter by neurons whose cell bodies lie mainly in two clusters
of neurons in the midbrain of mammals: the substantia nigra pars compacta (snpc) and the ventral
tegmental area (vta). dopamine plays essential roles in many processes in the mammalian brain.
prominent among these are motivation, learning, action-selection, most forms of addiction, and the
disorders schizophrenia and parkinson   s disease. dopamine is called a neuromodulator because it per-
forms many functions other than direct fast excitation or inhibition of targeted neurons. although
much remains unknown about dopamine   s functions and details of its cellular e   ects, it is clear that it
is fundamental to reward processing in the mammalian brain. dopamine is not the only neuromodulator
involved in reward processing, and its role in aversive situations   punishment   remains controversial.
dopamine also can function di   erently in non-mammals. but no one doubts that dopamine is essential
for reward-related processes in mammals, including humans.

an early, traditional view is that dopamine neurons broadcast a reward signal to multiple brain
regions implicated in learning and motivation. this view followed from a famous 1954 paper by james
olds and peter milner that described the e   ects of electrical stimulation on certain areas of a rat   s
brain. they found that electrical stimulation to particular regions acted as a very powerful reward in
controlling the rat   s behavior:    ... the control exercised over the animal   s behavior by means of this
reward is extreme, possibly exceeding that exercised by any other reward previously used in animal
experimentation    (olds and milner, 1954). later research revealed that the sites at which stimulation
was most e   ective in producing this rewarding e   ect excited dopamine pathways, either directly or
indirectly, that ordinarily are excited by natural rewarding stimuli. e   ects similar to these with rats
were also observed with human subjects. these observations strongly suggested that dopamine neuron
activity signals reward.

but if the reward prediction error hypothesis is correct   even if it accounts for only some features
of a dopamine neuron   s activity   this traditional view of dopamine neuron activity is not entirely
correct: phasic responses of dopamine neurons signal reward prediction errors, not reward itself. in
id23   s terms, a dopamine neuron   s phasic response at a time t corresponds to   t   1 =
rt +   v (st)     v (st   1), not to rt.

id23 theory and algorithms help reconcile the reward-prediction-error view with
the conventional notion that dopamine signals reward. in many of the algorithms we discuss in this
book,    functions as a reinforcement signal, meaning that it is the main driver of learning. for example,
   is the critical factor in the td model of classical conditioning, and    is the reinforcement signal for
learning both a value function and a policy in an actor   critic architecture (sections 13.5 and 15.7).
action-dependent forms of    are reinforcement signals for id24 and sarsa. the reward signal rt
is a crucial component of   t   1, but it is not the complete determinant of its reinforcing e   ect in these
algorithms. the additional term   v (st)     v (st   1) is the higher-order reinforcement part of   t   1, and
even if reward occurs (rt (cid:54)= 0), the td error can be silent if the reward is fully predicted (which is fully
explained in section 15.6 below).

a closer look at olds    and milner   s 1954 paper, in fact, reveals that it is mainly about the reinforcing
e   ect of electrical stimulation in an instrumental conditioning task. electrical stimulation not only
energized the rats    behavior   through dopamine   s e   ect on motivation   it also led to the rats quickly
learning to stimulate themselves by pressing a lever, which they would do frequently for long periods
of time. the activity of dopamine neurons triggered by electrical stimulation reinforced the rats    lever
pressing.

more recent experiments using optogenetic methods clinch the role of phasic responses of dopamine

15.4. dopamine

319

neurons as reinforcement signals. these methods allow neuroscientists to precisely control the activity
of selected neuron types at a millisecond timescale in awake behaving animals. optogenetic methods
introduce light-sensitive proteins into selected neuron types so that these neurons can be activated
or silenced by means of    ashes of laser light. the    rst experiment using optogenetic methods to
study dopamine neurons showed that optogenetic stimulation producing phasic activation of dopamine
neurons in mice was enough to condition the mice to prefer the side of a chamber where they received
this stimulation as compared to the chamber   s other side where they received no, or lower-frequency,
stimulation (tsai et al. 2009). in another example, steinberg et al. (2013) used optogenetic activation
of dopamine neurons to create arti   cial bursts of dopamine neuron activity in rats at the times when
rewarding stimuli were expected but omitted   times when dopamine neuron activity normally pauses.
with these pauses replaced by arti   cial bursts, responding was sustained when it would ordinarily
decrease due to lack of reinforcement (in extinction trials), and learning was enabled when it would
ordinarily be blocked due to the reward being already predicted (the blocking paradigm; section 14.2.1).

additional evidence for the reinforcing function of dopamine comes from optogenetic experiments
with fruit    ies, except in these animals dopamine   s e   ect is the opposite of its e   ect in mammals:
optically triggered bursts of dopamine neuron activity act just like electric foot shock in reinforcing
avoidance behavior, at least for the population of dopamine neurons activated (claridge-chang et al.
2009). although none of these optogenetic experiments showed that phasic dopamine neuron activity
is speci   cally like a td error, they convincingly demonstrated that phasic dopamine neuron activity
acts just like    acts (or perhaps like minus    acts in fruit    ies) as the reinforcement signal in algorithms
for both prediction (classical conditioning) and control (instrumental conditioning).

dopamine neurons are particularly well suited to broadcasting a reinforcement signal to many areas
of the brain. these neurons have huge axonal arbors, each releasing dopamine at 100 to 1,000 times
more synaptic sites than reached by the axons of typical neurons. figure 15.1 shows the axonal arbor
of a single dopamine neuron whose cell body is in the snpc of a rat   s brain. each axon of a snpc or
vta dopamine neuron makes roughly 500,000 synaptic contacts on the dendrites of neurons in targeted
brain areas.

figure 15.1: axonal arbor of a single neuron producing dopamine as a neurotransmitter whose cell body is in
the snpc of a rat   s brain. these axons make synaptic contacts with a huge number of dendrites of neurons in
targeted brain areas. adapted from journal of neuroscience, matsuda, furuta, nakamura, hioki, fujiyama,
arai, and kaneko, volume 29, 2009, page 451.

320

chapter 15. neuroscience

if dopamine neurons broadcast a reinforcement signal like id23   s   , then since this
is a scalar signal, i.e., a single number, all dopamine neurons in both the snpc and vta would be
expected to activate more-or-less identically so that they would act in near synchrony to send the same
signal to all of the sites their axons target. although it has been a common belief that dopamine neurons
do act together like this, modern evidence is pointing to the more complicated picture that di   erent
subpopulations of dopamine neurons respond to input di   erently depending on the structures to which
they send their signals and the di   erent ways these signals act on their target structures. dopamine
has functions other than signaling rpes, and even for dopamine neurons that do signal rpes, it can
make sense to send di   erent rpes to di   erent structures depending on the roles these structures play
in producing reinforced behavior. this is beyond what we treat in any detail in this book, but vector-
valued rpe signals make sense from the perspective of id23 when decisions can be
decomposed into separate sub-decisions, or more generally, as a way to address the structural version
of the credit assignment problem: how do you distribute credit for success (or blame for failure) of a
decision among the many component structures that could have been involved in producing it? we say
a bit more about this in section 15.10 below.

the axons of most dopamine neurons make synaptic contact with neurons in the frontal cortex and
the basal ganglia, areas of the brain involved in voluntary movement, decision making, learning, and
cognitive functions such a planning. since most ideas relating dopamine to id23 focus
on the basal ganglia, and the connections from dopamine neurons are particularly dense there, we focus
on the basal ganglia here. the basal ganglia are a collection of neuron groups, or nuclei, lying at the base
of the forebrain. the main input structure of the basal ganglia is called the striatum. essentially all
of the cerebral cortex, among other structures, provides input to the striatum. the activity of cortical
neurons conveys a wealth of information about sensory input, internal states, and motor activity. the
axons of cortical neurons make synaptic contacts on the dendrites of the main input/output neurons
of the striatum, called medium spiny neurons. output from the striatum loops back via other basal
ganglia nuclei and the thalamus to frontal areas of cortex, and to motor areas, making it possible for
the striatum to in   uence movement, abstract decision processes, and reward processing. two main
subdivisions of the striatum are important for id23: the dorsal striatum, primarily
implicated in in   uencing action selection, and the ventral striatum, thought to be critical for di   erent
aspects of reward processing, including the assignment of a   ective value to sensations.

the dendrites of medium spiny neurons are covered with spines on whose tips the axons of neurons
in the cortex make synaptic contact. also making synaptic contact with these spines   in this case
contacting the spine stems   are axons of dopamine neurons (figure 15.2). this arrangement brings
together presynaptic activity of cortical neurons, postsynaptic activity of medium spiny neurons, and
input from dopamine neurons. what actually occurs at these spines is complex and not completely
understood. figure 15.2 hints at the complexity by showing two types of receptors for dopamine,
receptors for glutamate   the neurotransmitter of the cortical inputs   and multiple ways that the various
signals can interact. but evidence is mounting that changes in the e   cacies of the synapses on the
pathway from the cortex to the striatum, which neuroscientists call corticostriatal synapses, depend
critically on appropriately-timed dopamine signals.

15.5. experimental support for the reward prediction error hypothesis321

figure 15.2: spine of a striatal neuron showing input from both cortical and dopamine neurons. axons of
cortical neurons in   uence striatal neurons via corticostriatal synapses releasing the neurotransmitter glutamate
at the tips of spines covering the dendrites of striatal neurons. an axon of a vta or snpc dopamine neuron is
shown passing by the spine (from the lower right).    dopamine varicosities    on this axon release dopamine at or
near the spine stem, in an arrangement that brings together presynaptic input from cortex, postsynaptic activity
of the striatal neuron, and dopamine, making it possible that several types of learning rules govern the plasticity
of corticostriatal synapses. each axon of a dopamine neuron makes synaptic contact with the stems of roughly
500,000 spines. some of the complexity omitted from our discussion is shown here by other neurotransmitter
pathways and multiple receptor types, such as d1 an d2 dopamine receptors by which dopamine can produce
di   erent e   ects at spines and other postsynaptic sites. from journal of neurophysiology, w. schultz, vol. 80,
1998, page 10.

15.5 experimental support for the reward prediction error

hypothesis

dopamine neurons respond with bursts of activity to intense, novel, or unexpected visual and auditory
stimuli that trigger eye and body movements, but very little of their activity is related to the move-
ments themselves. this is surprising because degeneration of dopamine neurons is a cause of parkinson   s
disease, whose symptoms include motor disorders, particularly de   cits in self-initiated movement. moti-
vated by the weak relationship between dopamine neuron activity and stimulus-triggered eye and body
movements, romo and schultz (1990) and schultz and romo (1990) took the    rst steps toward the
reward prediction error hypothesis by recording the activity of dopamine neurons and muscle activity
while monkeys moved their arms.

they trained two monkeys to reach from a resting hand position into a bin containing a bit of apple,
a piece of cookie, or a raisin, when the monkey saw and heard the bin   s door open. the monkey could
then grab and bring the food to its mouth. after a monkey became good at this, it was trained on two
additional tasks. the purpose of the    rst task was to see what dopamine neurons do when movements
are self-initiated. the bin was left open but covered from above so that the monkey could not see
inside but could reach in from below. no triggering stimuli were presented, and after the monkey
reached for and ate the food morsel, the experimenter usually (though not always), silently and unseen
by the monkey, replaced food in the bin by sticking it onto a rigid wire. here too, the activity of the
dopamine neurons romo and schultz monitored was not related to the monkey   s movements, but a

322

chapter 15. neuroscience

large percentage of these neurons produced phasic responses whenever the monkey    rst touched a food
morsel. these neurons did not respond when the monkey touched just the wire or explored the bin
when no food was there. this was good evidence that the neurons were responding to the food and not
to other aspects of the task.

the purpose of romo and schultz   s second task was to see what happens when movements are
triggered by stimuli. this task used a di   erent bin with a movable cover. the sight and sound of
the bin opening triggered reaching movements to the bin. in this case, romo and schultz found that
after some period of training, the dopamine neurons no longer responded to the touch of the food but
instead responded to the sight and sound of the opening cover of the food bin. the phasic responses
of these neurons had shifted from the reward itself to stimuli predicting the availability of the reward.
in a followup study, romo and schultz found that most of the dopamine neurons whose activity they
monitored did not respond to the sight and sound of the bin opening outside the context of the behavioral
task. these observations suggested that the dopamine neurons were responding neither to the initiation
of a movement nor to the sensory properties of the stimuli, but were rather signaling an expectation of
reward.

schultz   s group conducted many additional studies involving both snpc and vta dopamine neurons.
a particular series of experiments was in   uential in suggesting that the phasic responses of dopamine
neurons correspond to td errors and not to simpler errors like those in the rescorla   wagner model
(14.3). in the    rst of these experiments (ljungberg, apicella, and schultz, 1992), monkeys were trained
to depress a lever after a light was illuminated as a    trigger cue    to obtain a drop of apple juice. as
romo and schultz had observed earlier, many dopamine neurons initially responded to the reward   the
drop of juice (figure 15.3, top panel). but many of these neurons lost that reward response as training
continued and developed responses instead to the illumination of the light that predicted the reward
(figure 15.3, middle panel). with continued training, lever pressing became faster while the number of
dopamine neurons responding to the trigger cue decreased.

figure 15.3: the response of dopamine neurons shifts from initial responses to primary reward to earlier
predictive stimuli. these are plots of the number of action potentials produced by monitored dopamine neurons
within small time intervals, averaged over all the monitored dopamine neurons (ranging from 23 to 44 neurons
for these data). top: dopamine neurons are activated by the unpredicted delivery of drop of apple juice.
middle: with learning, dopamine neurons developed responses to the reward-predicting trigger cue and lost
responsiveness to the delivery of reward. bottom: with the addition of an instruction cue preceding the trigger
cue by 1 second, dopamine neurons shifted their responses from the trigger cue to the earlier instruction cue.
from schultz et al. (1995), mit press.

15.6. td error/dopamine correspondence

323

following this study, the same monkeys were trained on a new task (schultz, apicella, and ljungberg,
1993). here the monkeys faced two levers, each with a light above it. illuminating one of these lights
was an    instruction cue    indicating which of the two levers would produce a drop of apple juice. in this
task, the instruction cue preceded the trigger cue of the previous task by a    xed interval of 1 second.
the monkeys learned to withhold reaching until seeing the trigger cue, and dopamine neuron activity
increased, but now the responses of the monitored dopamine neurons occurred almost exclusively to the
earlier instruction cue and not to the trigger cue (figure 15.3, bottom panel). here again the number of
dopamine neurons responding to the instruction cue was much reduced when the task was well learned.
during learning across these tasks, dopamine neuron activity shifted from initially responding to the
reward to responding to the earlier predictive stimuli,    rst progressing to the trigger stimulus then
to the still earlier instruction cue. as responding moved earlier in time it disappeared from the later
stimuli. this shifting of responses to earlier reward predictors, while losing responses to later predictors
is a hallmark of td learning (see, for example, figure 14.5).

the task just described revealed another property of dopamine neuron activity shared with td
learning. the monkeys sometimes pressed the wrong key, that is, the key other than the instructed
one, and consequently received no reward. in these trials, many of the dopamine neurons showed a
sharp decrease in their    ring rates below baseline shortly after the reward   s usual time of delivery,
and this happened without the availability of any external cue to mark the usual time of reward
delivery (figure 15.4). somehow the monkeys were internally keeping track of the timing of the reward.
(response timing is one area where the simplest version of td learning needs to be modi   ed to account
for some of the details of the timing of dopamine neuron responses. we consider this issue in the
following section.)

the observations from the studies described above led schultz and his group to conclude that
dopamine neurons respond to unpredicted rewards, to the earliest predictors of reward, and that
dopamine neuron activity decreases below baseline if a reward, or a predictor of reward, does not
occur at its expected time. researchers familiar with id23 were quick to recognize
that these results are strikingly similar to how the td error behaves as the reinforcement signal in a td
algorithm. the next section explores this similarity by working through a speci   c example in detail.

15.6 td error/dopamine correspondence

this section explains the correspondence between the td error    and the phasic responses of dopamine
neurons observed in the experiments just described. we examine how    changes over the course of
learning in a task something like the one described above where a monkey    rst sees an instruction cue
and then a    xed time later has to respond correctly to a trigger cue in order to obtain reward. we use
a simple idealized version of this task, but we go into a lot more detail than is usual because we want
to emphasize the theoretical basis of the parallel between td errors and dopamine neuron activity.

the    rst simplifying assumption is that the agent has already learned the actions required to obtain
reward. then its task is just to learn accurate predictions of future reward for the sequence of states it
experiences. this is then a prediction task, or more technically, a policy-evaluation task: learning the
value function for a    xed policy (sections 4.1 and 6.1). the value function to be learned assigns to each
state a value that predicts the return that will follow that state if the agent selects actions according
to the given policy, where the return is the (possibly discounted) sum of all the future rewards. this is
unrealistic as a model of the monkey   s situation because the monkey would likely learn these predictions
at the same time that it is learning to act correctly (as would a id23 algorithm that
learns policies as well as value functions, such as an actor   critic algorithm), but this scenario is simpler
to describe than one in which a policy and a value function are learned simultaneously.

now imagine that the agent   s experience divides into multiple trials, in each of which the same
sequence of states repeats, with a distinct state occurring on each time step during the trial. further

324

chapter 15. neuroscience

figure 15.4: the response of dopamine neurons drops below baseline shortly after the time when an expected
reward fails to occur. top: dopamine neurons are activated by the unpredicted delivery of a drop of apple juice.
middle: dopamine neurons respond to a conditioned stimulus (cs) that predicts reward and do not respond
to the reward itself. bottom: when the reward predicted by the cs fails to occur, the activity of dopamine
neurons drops below baseline shortly after the time the reward is expected to occur. at the top of each of these
panels is shown the average number of action potentials produced by monitored dopamine neurons within small
time intervals around the indicated times. the raster plots below show the activity patterns of the individual
dopamine neurons that were monitored; each dot represents an action potential. from schultz, dayan, and
montague, a neural substrate of prediction and reward, science, vol. 275, issue 5306, pages 1593-1598, march
14, 1997. reprinted with permission from aaas.

imagine that the return being predicted is limited to the return over a trial, which makes a trial
analogous to a id23 episode as we have de   ned it. in reality, of course, the returns
being predicted are not con   ned to single trials, and the time interval between trials is an important
factor in determining what an animal learns. this is true for td learning as well, but here we assume
that returns do not accumulate over multiple trials. given this, then, a trial in experiments like those
conducted by schultz and colleagues is equivalent to an episode of id23. (though in
this discussion, we will use the term trial instead of episode to relate better to the experiments.)

as usual, we also need to make an assumption about how states are represented as inputs to the
learning algorithm, an assumption that in   uences how closely the td error corresponds to dopamine
neuron activity. we discuss this issue later, but for now we assume the same csc representation used
by montague et al. (1996) in which there is a separate internal stimulus for each state visited at each

15.6. td error/dopamine correspondence

325

time step in a trial. this reduces the process to the tabular case covered in the    rst part of this book.
finally, we assume that the agent uses td(0) to learn a value function, v , stored in a lookup table
initialized to be zero for all the states. we also assume that this is a deterministic task and that the
discount factor,   , is very nearly one so that we can ignore it.

figure 15.5 shows the time courses of r, v , and    at several stages of learning in this policy-evaluation
task. the time axes represent the time interval over which a sequence of states is visited in a trial (where
for clarity we omit showing individual states). the reward signal is zero throughout each trial except
when the agent reaches the rewarding state, shown near the right end of the time line, when the reward
signal becomes some positive number, say r(cid:63). the goal of td learning is to predict the return for each
state visited in a trial, which in this undiscounted case and given our assumption that predictions are
con   ned to individual trials, is simply r(cid:63) for each state.

preceding the rewarding state is a sequence of reward-predicting states, with the earliest reward-
predicting state shown near the left end of the time line. this is like the state near the start of a trial,
for example like the state marked by the instruction cue in a trial of the monkey experiment of schultz
et al. (1993) described above. it is the    rst state in a trial that reliably predicts that trial   s reward.
(of course, in reality states visited on preceding trials are even earlier reward-predicting states, but
since we are con   ning predictions to individual trials, these do not qualify as predictors of this trial   s
reward. below we give a more satisfactory, though more abstract, description of an earliest reward-
predicting state.) the latest reward-predicting state in a trial is the state immediately preceding the
trial   s rewarding state. this is the state near the far right end of the time line in figure 15.5. note that
the rewarding state of a trial does not predict the return for that trial: the value of this state would
come to predict the return over all the following trials, which here we are assuming to be zero in this
episodic formulation.

figure 15.5 shows the    rst-trial time courses of v and    as the graphs labeled    early in learning.   

figure 15.5: the behavior of the td error    during td learning is consistent with features of the phasic
activation of dopamine neurons. (here    is the td error available at time t, i.e.,   t   1). top: a sequence of
states, shown as an interval of regular predictors, is followed by a non-zero reward r(cid:63). early in learning: the
initial value function, v , and initial   , which at    rst is equal to r(cid:63). learning complete: the value function
accurately predicts future reward,    is positive at the earliest predictive state, and    = 0 at the time of the
non-zero reward. r(cid:63) omitted : at the time the predicted reward is omitted,    becomes negative. see text for a
complete explanation of why this happens.

rrrrrtr t 1=rt+vt vt 1rvv   early inlearninglearningcompleteomittedrregular predictors of     over this intervalrr?326

chapter 15. neuroscience

because the reward signal is zero throughout the trial except when the rewarding state is reached, and
all the v -values are zero, the td error is also zero until it becomes r(cid:63) at the rewarding state. this
follows because   t   1 = rt + vt     vt   1 = rt + 0     0 = rt, which is zero until it equals r(cid:63) when the
reward occurs. here vt and vt   1 are respectively the estimated values of the states visited at times t
and t   1 in a trial. the td error at this stage of learning is analogous to a dopamine neuron responding
to an unpredicted reward, e.g., a drop apple juice, at the start of training.

throughout this    rst trial and all successive trials, td(0) updates occur at each state transition as
described in chapter 6. this successively increases the values of the reward-predicting states, with the
increases spreading backwards from the rewarding state, until the values converge to the correct return
predictions. in this case (since we are assuming no discounting) the correct predictions are equal to r(cid:63)
for all the reward-predicting states. this can be seen in figure 15.5 as the graph of v labeled    learning
complete    where the values of all the states from the earliest to the latest reward-predicting states all
equal r(cid:63). the values of the states preceding the earliest reward-predicting state remain low (which
figure 15.5 shows as zero) because they are not reliable predictors of reward.

when learning is complete, that is, when v attains its correct values, the td errors associated with
transitions from any reward-predicting state are zero because the predictions are now accurate. this
is because for a transition from a reward-predicting state to another reward-predicting state, we have
  t   1 = rt +vt   vt   1 = 0+r(cid:63)   r(cid:63) = 0, and for the transition from the latest reward-predicting state to
the rewarding state, we have   t   1 = rt + vt    vt   1 = r(cid:63) + 0    r(cid:63) = 0. on the other hand, the td error
on a transition from any state to the earliest reward-predicting state is positive because of the mismatch
between this state   s low value and the larger value of the following reward-predicting state. indeed, if
the value of a state preceding the earliest reward-predicting state were zero, then after the transition to
the earliest reward-predicting state, we would have that   t   1 = rt + vt     vt   1 = 0 + r(cid:63)     0 = r(cid:63). the
   learning complete    graph of    in figure 15.5 shows this positive value at the earliest reward-predicting
state, and zeros everywhere else.

the positive td error upon transitioning to the earliest reward-predicting state is analogous to
the persistence of dopamine responses to the earliest stimuli predicting reward. by the same token,
when learning is complete, a transition from the latest reward-predicting state to the rewarding state
produces a zero td error because the latest reward-predicting state   s value, being correct, cancels the
reward. this parallels the observation that fewer dopamine neurons generate a phasic response to a
fully predicted reward than to an unpredicted reward.

after learning, if the reward is suddenly omitted, the td error goes negative at the usual time of
reward because the value of the latest reward-predicting state is then too high:   t   1 = rt + vt     vt   1 =
0 + 0     r(cid:63) =    r(cid:63), as shown at the right end of the    r omitted    graph of    in figure 15.5. this is like
dopamine neuron activity decreasing below baseline at the time an expected reward is omitted as seen
in the experiment of schultz et al. (1993) described above and shown in figure 15.4.

the idea of an earliest reward-predicting state deserves more attention. in the scenario described
above, since experience is divided into trials, and we assumed that predictions are con   ned to indi-
vidual trials, the earliest reward-predicting state is always the    rst state of a trial. clearly this is
arti   cial. a more general way to think of an earliest reward-predicting state is that it is an unpredicted
predictor of reward, and there can be many such states. in an animal   s life, many di   erent states may
precede an earliest reward-predicting state. however, because these states are more often followed by
other states that do not predict reward, their reward-predicting powers, that is, their values, remain
low. a td algorithm, if operating throughout the animal   s life, would update the values of these
states too, but the updates would not consistently accumulate because, by assumption, none of these
states reliably precedes an earliest reward-predicting state. if any of them did, they would be reward-
predicting states as well. this might explain why with overtraining, dopamine responses decrease to
even the earliest reward-predicting stimulus in a trial. with overtraining one would expect that even a
formerly-unpredicted predictor state would become predicted by stimuli associated with earlier states:

15.6. td error/dopamine correspondence

327

the animal   s interaction with its environment both inside and outside of an experimental task would
become commonplace. upon breaking this routine with the introduction of a new task, however, one
would see td errors reappear, as indeed is observed in dopamine neuron activity.

the example described above explains why the td error shares key features with the phasic activity
of dopamine neurons when the animal is learning in a task similar to the idealized task of our example.
but not every property of the phasic activity of dopamine neurons coincides so neatly with properties
of   . one of the most troubling discrepancies involves what happens when a reward occurs earlier than
expected. we have seen that the omission of an expected reward produces a negative prediction error
at the reward   s expected time, which corresponds to the activity of dopamine neurons decreasing below
baseline when this happens. if the reward arrives later than expected, it is then an unexpected reward
and generates a positive prediction error. this happens with both td errors and dopamine neuron
responses. but when reward arrives earlier than expected, dopamine neurons do not do what the td
error does   at least with the csc representation used by montague et al. (1996) and by us in our
example. dopamine neurons do respond to the early reward, which is consistent with a positive td
error because the reward is not predicted to occur then. however, at the later time when the reward is
expected but omitted, the td error is negative whereas, in contrast to this prediction, dopamine neuron
activity does not drop below baseline in the way the td model predicts (hollerman and schultz, 1998).
something more complicated is going on in the animal   s brain than simply td learning with a csc
representation.

some of the mismatches between the td error and dopamine neuron activity can be addressed by
selecting suitable parameter values for the td algorithm and by using stimulus representations other
than the csc representation. for instance, to address the early-reward mismatch just described, suri
and schultz (1999) proposed a csc representation in which the sequences of internal signals initiated
by earlier stimuli are cancelled by the occurrence of a reward. another proposal by daw, courville,
and touretzky (2006) is that the brain   s td system uses representations produced by statistical mod-
eling carried out in sensory cortex rather than simpler representations based on raw sensory input.
ludvig, sutton, and kehoe (2008) found that td learning with a microstimulus (ms) representation
(figure 14.2)    ts the activity of dopamine neurons in the early-reward and other situations better than
when a csc representation is used. pan, schmidt, wickens, and hyland (2005) found that even with
the csc representation, prolonged eligibility traces improve the    t of the td error to some aspects of
dopamine neuron activity. in general, many    ne details of td-error behavior depend on subtle interac-
tions between eligibility traces, discounting, and stimulus representations. findings like these elaborate
and re   ne the reward prediction error hypothesis without refuting its core claim that the phasic activity
of dopamine neurons is well characterized as signaling td errors.

on the other hand, there are other discrepancies between the td theory and experimental data that
are not so easily accommodated by selecting parameter values and stimulus representations (we mention
some of these discrepancies in the bibliographical and historical remarks section at the end of this
chapter), and more mismatches are likely to be discovered as neuroscientists conduct ever more re   ned
experiments. but the reward prediction error hypothesis has been functioning very e   ectively as a
catalyst for improving our understanding of how the brain   s reward system works. intricate experiments
have been designed to validate or refute predictions derived from the hypothesis, and experimental
results have, in turn, led to re   nement and elaboration of the td error/dopamine hypothesis.

a remarkable aspect of these developments is that the id23 algorithms and theory
that connect so well with properties of the dopamine system were developed from a computational
perspective in total absence of any knowledge about the relevant properties of dopamine neurons   
remember, td learning and its connections to optimal control and id145 were devel-
oped many years before any of the experiments were conducted that revealed the td-like nature of
dopamine neuron activity. this unplanned correspondence, despite not being perfect, suggests that the
td error/dopamine parallel captures something signi   cant about brain reward processes.

328

chapter 15. neuroscience

in addition to accounting for many features of the phasic activity of dopamine neurons, the reward
prediction error hypothesis links neuroscience to other aspects of id23, in particular,
to learning algorithms that use td errors as reinforcement signals. neuroscience is still far from
reaching complete understanding of the circuits, molecular mechanisms, and functions of the phasic
activity of dopamine neurons, but evidence supporting the reward prediction error hypothesis, along
with evidence that phasic dopamine responses are reinforcement signals for learning, suggest that the
brain might implement something like an actor   critic algorithm in which td errors play critical roles.
other id23 algorithms are plausible candidates too, but actor   critic algorithms    t
the anatomy and physiology of the mammalian brain particularly well, as we describe in the following
two sections.

15.7 neural actor   critic

actor   critic algorithms learn both policies and value functions. the    actor    is the component that learns
policies, and the    critic    is the component that learns about whatever policy is currently being followed
by the actor in order to    criticize    the actor   s action choices. the critic uses a td algorithm to learn
the state-value function for the actor   s current policy. the value function allows the critic to critique
the actor   s action choices by sending td errors,   , to the actor. a positive    means that the action
was    good    because it led to a state with a better-than-expected value; a negative    means that the
action was    bad    because it led to a state with a worse-than-expected value. based on these critiques,
the actor continually updates its policy.

two distinctive features of actor   critic algorithms are responsible for thinking that the brain might
implement an algorithm like this. first, the two components of an actor   critic algorithm   the actor and
the critic   suggest that two parts of the striatum   the dorsal and ventral subdivisions (section 15.4),
both critical for reward-based learning   may function respectively something like an actor and a critic.
a second property of actor   critic algorithms that suggests a brain implementation is that the td error
has the dual role of being the reinforcement signal for both the actor and the critic, though it has a
di   erent in   uence on learning in each of these components. this    ts well with several properties of
the neural circuitry: axons of dopamine neurons target both the dorsal and ventral subdivisions of the
striatum; dopamine appears to be critical for modulating synaptic plasticity in both structures; and
how a neuromodulator such as dopamine acts on a target structure depends on properties of the target
structure and not just on properties of the neuromodulator.

section 13.5 presents actor   critic algorithms as id189, but the actor   critic algo-
rithm of barto, sutton, and anderson (1983) was simpler and was presented as an arti   cial neural
network. here we describe an arti   cial neural network implementation something like that of barto
et al., and we follow takahashi, schoenbaum, and niv (2008) in giving a schematic proposal for how
this arti   cial neural network might be implemented by real neural networks in the brain. we postpone
discussion of the actor and critic learning rules until section 15.8, where we present them as special cases
of the policy-gradient formulation and discuss what they suggest about how dopamine might modulate
synaptic plasticity.

figure 15.6a shows an implementation of an actor   critic algorithm as an arti   cial neural network
with component networks implementing the actor and the critic. the critic consists of a single neuron-
like unit, v , whose output activity represents state values, and a component shown as the diamond
labeled td that computes td errors by combining v    s output with reward signals and with previous
state values (as suggested by the loop from the td diamond to itself). the actor network has a single
layer of k actor units labeled ai, i = 1, . . . , k. the output of each actor unit is a component of a
k-dimensional action vector. an alternative is that there are k separate actions, one commanded by
each actor unit, that compete with one another to be executed, but here we will think of the entire
a-vector as an action.

15.7. neural actor   critic

329

figure 15.6: actor   critic arti   cial neural network and a hypothetical neural implementation. a) actor   critic
algorithm as an arti   cial neural network. the actor adjusts a policy based on the td error    it receives from the
critic; the critic adjusts state-value parameters using the same   . the critic produces a td error from the reward
signal, r, and the current change in its estimate of state values. the actor does not have direct access to the
reward signal, and the critic does not have direct access to the action. b) hypothetical neural implementation
of an actor   critic algorithm. the actor and the value-learning part of the critic are respectively placed in the
ventral and dorsal subdivisions of the striatum. the td error is transmitted by dopamine neurons located
in the vta and snpc to modulate changes in synaptic e   cacies of input from cortical areas to the ventral
and dorsal striatum. adapted from frontiers in neuroscience, vol. 2(1), 2008, y. takahashi, g. schoenbaum,
and y. niv, silencing the critics: understanding the e   ects of cocaine sensitization on dorsolateral and ventral
striatum in the context of an actor/critic model.

both the critic and actor networks receive input consisting of multiple features representing the state
of the agent   s environment. (recall from chapter 1 that the environment of a id23
agent includes components both inside and outside of the    organism    containing the agent.) the    gure
shows these features as the circles labeled x1, x2, . . . , xn, shown twice just to keep the    gure simple. a
weight representing the e   cacy of a synapse is associated with each connection from each feature xi to
the critic unit, v , and to each of the action units, ai. the weights in the critic network parameterize
the value function, and the weights in the actor network parameterize the policy. the networks learn as
these weights change according to the critic and actor learning rules that we describe in the following
section.

the td error produced by circuitry in the critic is the reinforcement signal for changing the weights
in both the critic and the actor networks. this is shown in figure 15.6a by the line labeled    td error
      extending across all of the connections in the critic and actor networks. this aspect of the network
implementation, together with the reward prediction error hypothesis and the fact that the activity of
dopamine neurons is so widely distributed by the extensive axonal arbors of these neurons, suggests
that an actor   critic network something like this may not be too farfetched as a hypothesis about how
reward-related learning might happen in the brain.

figure 15.6b suggests   very schematically   how the arti   cial neural network on the    gure   s left
might map onto structures in the brain according to the hypothesis of takahashi et al. (2008). the

rewardactortd. . .. . .critic. . .   actionsstates/stimulitd errorenvironmentactorvtasncs2s1sn. . .. . .critic. . .s1sns2actionsstates/stimulidopaminestriatumventraldorsal striatumcortex (multiple areas)environmentreward 1 2 n 1 2 na1a2a3akv (a)(b)330

chapter 15. neuroscience

hypothesis puts the actor and the value-learning part of the critic respectively in the dorsal and ventral
subdivisions of the striatum, the input structure of the basal ganglia. recall from section 15.4 that
the dorsal striatum is primarily implicated in in   uencing action selection, and the ventral striatum is
thought to be critical for di   erent aspects of reward processing, including the assignment of a   ective
value to sensations. the cerebral cortex, along with other structures, sends input to the striatum
conveying information about stimuli, internal states, and motor activity.

in this hypothetical actor   critic brain implementation, the ventral striatum sends value information
to the vta and snpc, where dopamine neurons in these nuclei combine it with information about
reward to generate activity corresponding to td errors (though exactly how dopaminergic neurons
calculate these errors is not yet understood). the    td error       line in figure 15.6a becomes the line
labeled    dopamine    in figure 15.6b, which represents the widely branching axons of dopamine neurons
whose cell bodies are in the vta and snpc. referring back to figure 15.2, these axons make synaptic
contact with the spines on the dendrites of medium spiny neurons, the main input/output neurons of
both the dorsal and ventral divisions of the striatum. axons of the cortical neurons that send input
to the striatum make synaptic contact on the tips of these spines. according to the hypothesis, it is
at these spines where changes in the e   cacies of the synapses from cortical regions to the stratum are
governed by learning rules that critically depend on a reinforcement signal supplied by dopamine.

an important implication of the hypothesis illustrated in figure 15.6b is that the dopamine signal is
not the    master    reward signal like the scalar rt of id23. in fact, the hypothesis implies
that one should not necessarily be able to probe the brain and record any signal like rt in the activity
of any single neuron. many interconnected neural systems generate reward-related information, with
di   erent structures being recruited depending on di   erent types of rewards. dopamine neurons receive
information from many di   erent brain areas, so the input to the snpc and vta labeled    reward    in
figure 15.6b should be thought of as vector of reward-related information arriving to neurons in these
nuclei along multiple input channels. what the theoretical scalar reward signal rt might correspond
to, then, is the net contribution of all reward-related information to dopamine neuron activity. it is the
result of a pattern of activity across many neurons in di   erent areas of the brain.

although the actor   critic neural implementation illustrated in figure 15.6b may be correct on some
counts, it clearly needs to be re   ned, extended, and modi   ed to qualify as a full-   edged model of the
function of the phasic activity of dopamine neurons. the historical and bibliographic remarks section
at the end of this chapter cites publications that discuss in more detail both empirical support for this
hypothesis and places where it falls short. we now look in detail at what the actor and critic learning
algorithms suggest about the rules governing changes in synaptic e   cacies of corticostriatal synapses.

15.8 actor and critic learning rules

if the brain does implement something like the actor   critic algorithm   and assuming populations of
dopamine neurons broadcast a common reinforcement signal to the corticostriatal synapses of both
the dorsal and ventral striatum as illustrated in figure 15.6b (which is likely an oversimpli   cation as
we mentioned above)   then this reinforcement signal a   ects the synapses of these two structures in
di   erent ways. the learning rules for the critic and the actor use the same reinforcement signal, the
td error   , but its e   ect on learning is di   erent for these two components. the td error (combined
with eligibility traces) tells the actor how to update action probabilities in order to reach higher-valued
states. learning by the actor is like instrumental conditioning using a law-of-e   ect-type learning rule
(section 1.7): the actor works to keep    as positive as possible. on the other hand, the td error (when
combined with eligibility traces) tells the critic the direction and magnitude in which to change the
parameters of the value function in order to improve its predictive accuracy. the critic works to reduce
     s magnitude to be as close to zero as possible using a learning rule like the td model of classical
conditioning (section 14.2). the di   erence between the critic and actor learning rules is relatively

15.8. actor and critic learning rules

331

simple, but this di   erence has a profound e   ect on learning and is essential to how the actor   critic
algorithm works. the di   erence lies solely in the eligibility traces each type of learning rule uses.

more than one set of learning rules can be used in actor   critic neural networks like those in fig-
ure 15.6b but, to be speci   c, here we focus on the actor   critic algorithm for continuing problems with
eligibility traces presented in section 13.6. on each transition from state st to state st+1, taking action
at and receiving action rt+1, that algorithm computes the td error (  ) and then updates the eligibility
trace vectors (zw

t ) and the parameters for the critic and actor (w and   ), according to

t and z  

t   1 +    w   v(st,w),
t   1 +       ln   (at|st,   ),

  t = rt+1 +      v(st+1,w)       v(st,w),
t =   wzw
zw
t =     z  
z  
w     w +   w   t zw
t ,
          +         z  
t ,

where        [0, 1) is a discount-rate parameter,   wc     [0, 1] and   wa     [0, 1] are id64 parameters
for the critic and the actor respectively, and   w > 0 and      > 0 are analogous step-size parameters.

think of the approximate value function   v as the output of a single linear neuron-like unit, called
the critic unit and labeled v in figure 15.6a. then the value function is a linear function of the
feature-vector representation of state s, x(s) = (x1(s), . . . , xn(s))(cid:62), parameterized by a weight vector
w = (w1, . . . , wn)(cid:62):

  v(s,w) = w(cid:62)x(s).

(15.1)

each xi(s) is like the presynaptic signal to a neuron   s synapse whose e   cacy is wi. the weights of
the critic are incremented according to the rule above by   w  tzw
t , where the reinforcement signal,   t,
corresponds to a dopamine signal being broadcast to all of the critic unit   s synapses. the eligibility
trace vector, zw
t , for the critic unit is a trace (average of recent values) of    w   v(st,w). because   v(s,w)
is linear in the weights,    w   v(st,w) = x(st).
in neural terms, this means that each synapse has its own eligibility trace, which is one component
of the vector zw
t . a synapse   s eligibility trace accumulates according to the level of activity arriving at
that synapse, that is, the level of presynaptic activity, represented here by the component of the feature
vector x(st) arriving at that synapse. the trace otherwise decays toward zero at a rate governed
by the fraction   w. a synapse is eligible for modi   cation as long as its eligibility trace is non-zero.
how the synapse   s e   cacy is actually modi   ed depends on the reinforcement signals that arrive while
the synapse is eligible. we call eligibility traces like these of the critic unit   s synapses non-contingent
eligibility traces because they only depend on presynaptic activity and are not contingent in any way
on postsynaptic activity.

the non-contingent eligibility traces of the critic unit   s synapses mean that the critic unit   s learning
rule is essentially the td model of classical conditioning described in section 14.2. with the de   nition
we have given above of the critic unit and its learning rule, the critic in figure 15.6a is the same as
the critic in the neural network actor   critic of barto et al. (1983). clearly, a critic like this consisting
of just one linear neuron-like unit is the simplest starting point; this critic unit is a proxy for a more
complicated neural network able to learn value functions of greater complexity.

the actor in figure 15.6a is a one-layer network of k neuron-like actor units, each receiving at time
t the same feature vector, x(st), that the critic unit receives. each actor unit j, j = 1, . . . , k, has its
own weight vector,   j, but since the actor units are all identical, we describe just one of the units and
omit the subscript. one way for these units to follow the actor   critic algorithm given in the equations
above is for each to be a bernoulli-logistic unit. this means that the output of each actor unit at
each time is a random variable, at, taking value 0 or 1. think of value 1 as the neuron    ring, that
is, emitting an action potential. the weighted sum,   (cid:62)x(st), of a unit   s input vector determines the

332

chapter 15. neuroscience

unit   s action probabilities via the exponential softmax distribution (13.2), which for two actions is the
logistic function:

  (1|s,   ) = 1       (0|s,   ) =

1

1 + exp(     (cid:62)x(s))

.

(15.2)

the weights of each actor unit are incremented, as above, by:           +        t z  

t , where    again
corresponds to the dopamine signal: the same reinforcement signal that is sent to all the critic unit   s
synapses. figure 15.6a shows   t being broadcast to all the synapses of all the actor units (which makes
this actor network a team of id23 agents, something we discuss in section 15.10
below). the actor eligibility trace vector z  
t is a trace (average of recent values) of       ln   (at|st,   ).
to understand this eligibility trace refer to exercise 13.3, which de   nes this kind of unit and asks you
to give a learning rule for it. that exercise asked you to express       ln   (a|s,   ) in terms of a, x(s),
and   (a|s,   ) (for arbitrary state s and action a) by calculating the gradient. for the action and state
actually occurring at time t, the answer is the answer we were looking for is:

(15.3)

       (at|st,   ) =(cid:0)at       (at|st,   )(cid:1)x(st).

unlike the non-contingent eligibility trace of a critic synapse that only accumulates the presynaptic
activity x(st), the eligibility trace of an actor unit   s synapse in addition depends on the activity of
the actor unit itself. we call this a contingent eligibility trace because it is contingent on this postsy-
naptic activity. the eligibility trace at each synapse continually decays, but increments or decrements
depending on the activity of the presynaptic neuron and whether or not the postsynaptic neuron    res.
the factor at       (at|st,   ) in (15.3) is positive when at = 1 and negative otherwise. the postsy-
naptic contingency in the eligibility traces of actor units is the only di   erence between the critic and
actor learning rules. by keeping information about what actions were taken in what states, contin-
gent eligibility traces allow credit for reward (positive   ), or blame for punishment (negative   ), to be
apportioned among the policy parameters (the e   cacies of the actor units    synapses) according to the
contributions these parameters made to the units    outputs that could have in   uenced later values of   .
contingent eligibility traces mark the synapses as to how they should be modi   ed to alter the units   
future responses to favor positive values of   .

what do the critic and actor learning rules suggest about how e   cacies of corticostriatal synapses
change? both learning rules are related to donald hebb   s classic proposal that whenever a presynaptic
signal participates in activating the postsynaptic neuron, the synapse   s e   cacy increases (hebb, 1949).
the critic and actor learning rules share with hebb   s proposal the idea that changes in a synapse   s
e   cacy depend on the interaction of several factors. in the critic learning rule the interaction is between
the reinforcement signal    and eligibility traces that depend only on presynaptic signals. neuroscientists
call this a two-factor learning rule because the interaction is between two signals or quantities. the
actor learning rule, on the other hand, is a three-factor learning rule because, in addition to depending
on   , its eligibility traces depend on both presynaptic and postsynaptic activity. unlike hebb   s proposal,
however, the relative timing of the factors is critical to how synaptic e   cacies change, with eligibility
traces intervening to allow the reinforcement signal to a   ect synapses that were active in the recent
past.

some subtleties about signal timing for the actor and critic learning rules deserve closer attention.
in de   ning the neuron-like actor and critic units, we ignored the small amount of time it takes synaptic
input to e   ect the    ring of a real neuron. when an action potential from the presynaptic neuron
arrives at a synapse, neurotransmitter molecules are released that di   use across the synaptic cleft
to the postsynaptic neuron, where they bind to receptors on the postsynaptic neuron   s surface; this
activates molecular machinery that causes the postsynaptic neuron to    re (or to inhibit its    ring in
the case of inhibitory synaptic input). this process can take several tens of milliseconds. according
to (15.1) and (15.2), though, the input to a critic and actor unit instantaneously produces the unit   s

15.8. actor and critic learning rules

333

output. ignoring activation time like this is common in abstract models of hebbian-style plasticity in
which synaptic e   cacies change according to a simple product of simultaneous pre- and postsynaptic
activity. more realistic models must take activation time into account.

activation time is especially important for a more realistic actor unit because it in   uences how
contingent eligibility traces have to work in order to properly apportion credit for reinforcement to

the presynaptic factor x(st). this works because by ignoring activation time, the presynaptic activity

the appropriate synapses. the expression(cid:0)at       (at|st,   )(cid:1)x(st) de   ning contingent eligibility traces
for the actor unit   s learning rule given above includes the postsynaptic factor (cid:0)at       (at|st,   )(cid:1) and
x(st) participates in causing the postsynaptic activity appearing in(cid:0)at      (at|st,   )(cid:1). to assign credit

for reinforcement correctly, the presynaptic factor de   ning the eligibility trace must be a cause of the
postsynaptic factor that also de   nes the trace. contingent eligibility traces for a more realistic actor
unit would have to take activation time into account. (activation time should not be confused with
the time required for a neuron to receive a reinforcement signal in   uenced by that neuron   s activity.
the function of eligibility traces is to span this time interval which is generally much longer than the
activation time. we discuss this further in the following section.)

there are hints from neuroscience for how this process might work in the brain. neuroscientists have
discovered a form of hebbian plasticity called spike-timing-dependent plasticity (stdp) that lends plau-
sibility to the existence of actor-like synaptic plasticity in the brain. stdp is a hebbian-style plasticity,
but changes in a synapse   s e   cacy depend on the relative timing of presynaptic and postsynaptic action
potentials. the dependence can take di   erent forms, but in the one most studied, a synapse increases
in strength if spikes incoming via that synapse arrive shortly before the postsynaptic neuron    res. if
the timing relation is reversed, with a presynaptic spike arriving shortly after the postsynaptic neuron
   res, then the strength of the synapse decreases. stdp is a type of hebbian plasticity that takes the
activation time of a neuron into account, which is one of the ingredients needed for actor-like learning.

the discovery of stdp has led neuroscientists to investigate the possibility of a three-factor form of
stdp in which neuromodulatory input must follow appropriately-timed pre- and postsynaptic spikes.
this form of synaptic plasticity, called reward-modulated stdp, is much like the actor learning rule
discussed here. synaptic changes that would be produced by regular stdp only occur if there is neuro-
modulatory input within a time window after a presynaptic spike is closely followed by a postsynaptic
spike. evidence is accumulating that reward-modulated stdp occurs at the spines of medium spiny
neurons of the dorsal striatum, with dopamine providing the neuromodulatory factor   the sites where
actor learning takes place in the hypothetical neural implementation of an actor   critic algorithm il-
lustrated in figure 15.6b. experiments have demonstrated reward-modulated stdp in which lasting
changes in the e   cacies of corticostriatal synapses occur only if a neuromodulatory pulse arrives within
a time window that can last up to 10 seconds after a presynaptic spike is closely followed by a post-
synaptic spike (yagishita et al. 2014). although the evidence is indirect, these experiments point to
the existence of contingent eligibility traces having prolonged time courses. the molecular mechanisms
producing these traces, as well as the much shorter traces that likely underly stdp, are not yet un-
derstood, but research focusing on time-dependent and neuromodulator-dependent synaptic plasticity
is continuing.

the neuron-like actor unit that we have described here, with its law-of-e   ect-style learning rule,
appeared in somewhat simpler form in the actor   critic network of barto et al. (1983). that network
was inspired by the    hedonistic neuron    hypothesis proposed by physiologist a. h. klopf (1972, 1982).
not all the details of klopf   s hypothesis are consistent with what has been learned about synaptic
plasticity, but the discovery of stdp and the growing evidence for a reward-modulated form of stdp
suggest that klopf   s ideas may not have been far o    the mark. we discuss klopf   s hedonistic neuron
hypothesis next.

334

chapter 15. neuroscience

15.9 hedonistic neurons

in his hedonistic neuron hypothesis, klopf (1972, 1982) conjectured that individual neurons seek to max-
imize the di   erence between synaptic input treated as rewarding and synaptic input treated as punishing
by adjusting the e   cacies of their synapses on the basis of rewarding or punishing consequences of their
own action potentials. in other words, individual neurons can be trained with response-contingent rein-
forcement like an animal can be trained in an instrumental conditioning task. his hypothesis included
the idea that rewards and punishments are conveyed to a neuron via the same synaptic input that
excites or inhibits the neuron   s spike-generating activity. (had klopf known what we know today about
neuromodulatory systems, he might have assigned the reinforcing role to neuromodulatory input, but
he wanted to avoid any centralized source of training information.) synaptically-local traces of past
pre- and postsynaptic activity had the key function in klopf   s hypothesis of making synapses eligible   
the term he introduced   for modi   cation by later reward or punishment. he conjectured that these
traces are implemented by molecular mechanisms local to each synapse and therefore di   erent from the
electrical activity of both the pre- and the postsynaptic neurons. in the bibliographical and historical
remarks section of this chapter we bring attention to some similar proposals made by others.

klopf speci   cally conjectured that synaptic e   cacies change in the following way. when a neuron
   res an action potential, all of its synapses that were active in contributing to that action potential
become eligible to undergo changes in their e   cacies.
if the action potential is followed within an
appropriate time period by an increase of reward, the e   cacies of all the eligible synapses increase.
symmetrically, if the action potential is followed within an appropriate time period by an increase of
punishment, the e   cacies of eligible synapses decrease. this is implemented by triggering an eligibility
trace at a synapse upon a coincidence of presynaptic and postsynaptic activity (or more exactly, upon
pairing of presynaptic activity with the postsynaptic activity that that presynaptic activity participates
in causing)   what we call a contingent eligibility trace. this is essentially the three-factor learning rule
of an actor unit described in the previous section.

the shape and time course of an eligibility trace in klopf   s theory re   ects the durations of the many
feedback loops in which the neuron is embedded, some of which lie entirely within the brain and body
of the organism, while others extend out through the organism   s external environment as mediated
by its motor and sensory systems. his idea was that the shape of a synaptic eligibility trace is like
a histogram of the durations of the feedback loops in which the neuron is embedded. the peak of
an eligibility trace would then occur at the duration of the most prevalent feedback loops in which
that neuron participates. the eligibility traces used by algorithms described in this book are simpli   ed
versions of klopf   s original idea, being exponentially (or geometrically) decreasing functions controlled
by the parameters    and   . this simpli   es simulations as well as theory, but we regard these simple
eligibility traces as a placeholders for traces closer to klopf   s original conception, which would have
computational advantages in complex id23 systems by re   ning the credit-assignment
process.

klopf   s hedonistic neuron hypothesis is not as implausible as it may at    rst appear. a well-studied
example of a single cell that seeks some stimuli and avoids others is the bacterium escherichia coli.
the movement of this single-cell organism is in   uenced by chemical stimuli in its environment, behavior
known as chemotaxis. it swims in its liquid environment by rotating hairlike structures called    agella
attached to its surface.
(yes, it rotates them!) molecules in the bacterium   s environment bind to
receptors on its surface. binding events modulate the frequency with which the bacterium reverses
   agellar rotation. each reversal causes the bacterium to tumble in place and then head o    in a random
new direction. a little chemical memory and computation causes the frequency of    agellar reversal
to decrease when the bacterium swims toward higher concentrations of molecules it needs to survive
(attractants) and increase when the bacterium swims toward higher concentrations of molecules that
are harmful (repellants). the result is that the bacterium tends to persist in swimming up attractant
gradients and tends to avoid swimming up repellant gradients.

15.10. collective id23

335

the chemotactic behavior just described is called klinokinesis. it is a kind of trial-and-error behavior,
although it is unlikely that learning is involved: the bacterium needs a modicum of short-term memory
to detect molecular concentration gradients, but it probably does not maintain long-term memories.
arti   cial intelligence pioneer oliver selfridge called this strategy    run and twiddle,    pointing out its
utility as a basic adaptive strategy:    keep going in the same way if things are getting better, and
otherwise move around    (selfridge, 1978, 1984). similarly, one might think of a neuron    swimming   
(not literally of course) in a medium composed of the complex collection of feedback loops in which
it is embedded, acting to obtain one type of input signal and to avoid others. unlike the bacterium,
however, the neuron   s synaptic strengths retain information about its past trial-and-error behavior. if
this view of the behavior of a neuron (or just one type of neuron) is plausible, then the closed-loop
nature of how the neuron interacts with its environment is important for understanding its behavior,
where the neuron   s environment consists of the rest of the animal together with the environment with
which the animal as a whole interacts.

klopf   s hedonistic neuron hypothesis extended beyond the idea that individual neurons are reinforce-
ment learning agents. he argued that many aspects of intelligent behavior can be understood as the
result of the collective behavior of a population of self-interested hedonistic neurons interacting with one
another in an immense society or economic system making up an animal   s nervous system. whether or
not this view of nervous systems is useful, the collective behavior of id23 agents has
implications for neuroscience. we take up this subject next.

15.10 collective id23

the behavior of populations of id23 agents is deeply relevant to the study of social and
economic systems, and if anything like klopf   s hedonistic neuron hypothesis is correct, to neuroscience
as well. the hypothesis described above about how an actor   critic algorithm might be implemented in
the brain only narrowly addresses the implications of the fact that the dorsal and ventral subdivisions
of the striatum, the respective locations of the actor and the critic according to the hypothesis, each
contain millions of medium spiny neurons whose synapses undergo change modulated by phasic bursts
of dopamine neuron activity.

the actor in figure 15.6a is a single-layer network of k actor units. the actions produced by this
network are vectors (a1, a2,       , ak)(cid:62) presumed to drive the animal   s behavior. changes in the e   cacies
of the synapses of all of these units depend on the reinforcement signal   . because actor units attempt to
make    as large as possible,    e   ectively acts as a reward signal for them (so in this case reinforcement is
the same as reward). thus, each actor unit is itself a id23 agent   a hedonistic neuron
if you will. now, to make the situation as simple as possible, assume that each of these units receives
the same reward signal at the same time (although, as indicated above, the assumption that dopamine
is released at all the corticostriatal synapses under the same conditions and at the same times is likely
an oversimpli   cation).

what can id23 theory tell us about what happens when all members of a population
of id23 agents learn according to a common reward signal? the    eld of multi-agent
id23 considers many aspects of learning by populations of id23
agents. although this    eld is beyond the scope of this book, we believe that some of its basic concepts
and results are relevant to thinking about the brain   s di   use neuromodulatory systems. in multi-agent
id23 (and in game theory), the scenario in which all the agents try to maximize
a common reward signal that they simultaneously receive is known as a cooperative game or a team
problem.

what makes a team problem interesting and challenging is that the common reward signal sent to
each agent evaluates the pattern of activity produced by the entire population, that is, it evaluates the
collective action of the team members. this means that any individual agent has only limited ability

336

chapter 15. neuroscience

to a   ect the reward signal because any single agent contributes just one component of the collective
action evaluated by the common reward signal. e   ective learning in this scenario requires addressing a
structural credit assignment problem: which team members, or groups of team members, deserve credit
for a favorable reward signal, or blame for an unfavorable reward signal? it is a cooperative game, or
a team problem, because the agents are united in seeking to increase the same reward signal: there
are no con   icts of interest among the agents. the scenario would be a competitive game if di   erent
agents receive di   erent reward signals, where each reward signal again evaluates the collective action of
the population, and the objective of each agent is to increase its own reward signal. in this case there
might be con   icts of interest among the agents, meaning that actions that are good for some agents are
bad for others. even deciding what the best collective action should be is a non-trivial aspect of game
theory. this competitive setting might be relevant to neuroscience too (for example, to account for
heterogeneity of dopamine neuron activity), but here we focus only on the cooperative, or team, case.

how can each id23 agent in a team learn to    do the right thing    so that the collective
action of the team is highly rewarded? an interesting result is that if each agent can learn e   ectively
despite its reward signal being corrupted by a large amount of noise, and despite its lack of access to
complete state information, then the population as a whole will learn to produce collective actions that
improve as evaluated by the common reward signal, even when the agents cannot communicate with
one another. each agent faces its own id23 task in which its in   uence on the reward
signal is deeply buried in the noise created by the in   uences of other agents. in fact, for any agent, all
the other agents are part of its environment because its input, both the part conveying state information
and the reward part, depends on how all the other agents are behaving. furthermore, lacking access
to the actions of the other agents, indeed lacking access to the parameters determining their policies,
each agent can only partially observe the state of its environment. this makes each team member   s
learning task very di   cult, but if each uses a id23 algorithm able to increase a reward
signal even under these di   cult conditions, teams of id23 agents can learn to produce
collective actions that improve over time as evaluated by the team   s common reward signal.

if the team members are neuron-like units, then each unit has to have the goal of increasing the
amount of reward it receives over time, as the actor unit does that we described in section 15.8. each
unit   s learning algorithm has to have two essential features. first, it has to use contingent eligibility
traces. recall that a contingent eligibility trace, in neural terms, is initiated (or increased) at a synapse
when its presynaptic input participates in causing the postsynaptic neuron to    re. a non-contingent
eligibility trace, in contrast, is initiated or increased by presynaptic input independently of what the
postsynaptic neuron does. as explained in section 15.8, by keeping information about what actions were
taken in what states, contingent eligibility traces allow credit for reward, or blame for punishment, to be
apportioned to an agent   s policy parameters according to the contribution the values of these parameters
made in determining the agent   s action. by similar reasoning, a team member must remember its recent
action so that it can either increase or decrease the likelihood of producing that action according to
the reward signal that is subsequently received. the action component of a contingent eligibility trace
implements this action memory. because of the complexity of the learning task, however, contingent
eligibility is merely a preliminary step in the credit assignment process: the relationship between a
single team member   s action and changes in the team   s reward signal is a statistical correlation that
has to be estimated over many trials. contingent eligibility is an essential but preliminary step in this
process.

learning with non-contingent eligibility traces does not work at all in the team setting because
it does not provide a way to correlate actions with consequent changes in the reward signal. non-
contingent eligibility traces are adequate for learning to predict, as the critic component of the actor   
critic algorithm does, but they do not support learning to control, as the actor component must do.
the members of a population of critic-like agents may still receive a common reinforcement signal, but
they would all learn to predict the same quantity (which in the case of an actor   critic method, would
be the expected return for the current policy). how successful each member of the population would

15.11. model-based methods in the brain

337

be in learning to predict the expected return would depend on the information it receives, which could
be very di   erent for di   erent members of the population. there would be no need for the population
to produce di   erentiated patterns of activity. this is not a team problem as de   ned here.

a second requirement for collective learning in a team problem is that there has to be variability in
the actions of the team members in order for the team to explore the space of collective actions. the
simplest way for a team of id23 agents to do this is for each member to independently
explore its own action space through persistent variability in its output. this will cause the team as a
whole to vary its collective actions. for example, a team of the actor units described in section 15.8
explores the space of collective actions because the output of each unit, being a bernoulli-logistic
unit, probabilistically depends on the weighted sum of its input vector   s components. the weighted
sum biases    ring id203 up or down, but there is always variability. because each unit uses a
reinforce policy gradient algorithm (chapter 13), each unit adjusts its weights with the goal of
maximizing the average reward rate it experiences while stochastically exploring its own action space.
one can show, as williams (1992) did, that a team of bernoulli-logistic reinforce units implements
a policy gradient algorithm as a whole with respect to average rate of the team   s common reward signal,
where the actions are the collective actions of the team.

further, williams (1992) showed that a team of bernoulli-logistic units using reinforce ascends
the average reward gradient when the units in the team are interconnected to form a multilayer neural
network. in this case, the reward signal is broadcast to all the units in the network, though reward
may depend only on the collective actions of the network   s output units. this means that a multilayer
team of bernoulli-logistic reinforce units learns like a multilayer network trained by the widely-
used error id26 method, but in this case the id26 process is replaced by the
broadcasted reward signal. in practice, the error id26 method is considerably faster, but
the id23 team method is more plausible as a neural mechanism, especially in light of
what is being learned about reward-modulated stdp as discussed in section 15.8.

exploration through independent exploration by team members is only the simplest way for a team to
explore; more sophisticated methods are possible if the team members communicate with one another so
that they can coordinate their actions to focus on particular parts of the collective action space. there
are also mechanisms more sophisticated than contingent eligibility traces for addressing structural credit
assignment, which is easier in a team problem when the set of possible collective actions is restricted in
some way. an extreme case is a winner-take-all arrangement (for example, the result of lateral inhibition
in the brain) that restricts collective actions to those to which only one, or a few, team members
contribute. in this case the winners get the credit or blame for resulting reward or punishment.

details of learning in cooperative games (or team problems) and non-cooperative game problems
are beyond the scope of this book. the bibliographical and historical remarks section at the end of
this chapter cites a selection of the relevant publications, including extensive references to research on
implications for neuroscience of collective id23.

15.11 model-based methods in the brain

id23   s distinction between model-free and model-based algorithms is proving to be
useful for thinking about animal learning and decision processes. section 14.6 discusses how this dis-
tinction aligns with that between habitual and goal-directed animal behavior. the hypothesis discussed
above about how the brain might implement an actor   critic algorithm is relevant only to an animal   s ha-
bitual mode of behavior because the basic actor   critic method is model-free. what neural mechanisms
are responsible for producing goal-directed behavior, and how do they interact with those underlying
habitual behavior?

one way to investigate questions about the brain structures involved in these modes of behavior is

338

chapter 15. neuroscience

to inactivate an area of a rat   s brain and then observe what the rat does in an outcome-devaluation
experiment (section 14.6). results from experiments like these indicate that the actor   critic hypothesis
described above is too simple in placing the actor in the dorsal striatum. inactivating one part of the
dorsal striatum, the dorsolateral striatum (dls), impairs habit learning, causing the animal to rely more
on goal-directed processes. on the other hand, inactivating the dorsomedial striatum (dms) impairs
goal-directed processes, requiring the animal to rely more on habit learning. results like these support
the view that the dls in rodents is more involved in model-free processes, whereas their dms is more
involved in model-based processes. results of studies with human subjects in similar experiments using
functional neuroimaging, and with non-human primates, support the view that the analogous structures
in the primate brain are di   erentially involved in habitual and goal-directed modes of behavior.

other studies identify activity associated with model-based processes in the prefrontal cortex of
the human brain, the front-most part of the frontal cortex implicated in executive function, including
planning and decision making. speci   cally implicated is the orbitofrontal cortex (ofc), the part of the
prefrontal cortex immediately above the eyes. functional neuroimaging in humans, and also recordings
of the activities of single neurons in monkeys, reveals strong activity in the ofc related to the subjective
reward value of biologically signi   cant stimuli, as well as activity related to the reward expected as a
consequence of actions. although not free of controversy, these results suggest signi   cant involvement
of the ofc in goal-directed choice. it may be critical for the reward part of an animal   s environment
model.

another structure involved in model-based behavior is the hippocampus, a structure critical for
memory and spatial navigation. a rat   s hippocampus plays a critical role in the rat   s ability to navigate
a maze in the goal-directed manner that led tolman to the idea that animals use models, or cognitive
maps, in selecting actions (section 14.5). the hippocampus may also be a critical component of our
human ability to imagine new experiences (hassabis and maguire, 2007;   olafsd  ottir, barry, saleem,
hassabis, and spiers, 2105).

the    ndings that most directly implicate the hippocampus in planning   the process needed to enlist
an environment model in making decisions   come from experiments that decode the activity of neurons
in the hippocampus to determine what part of space hippocampal activity is representing on a moment-
to-moment basis. when a rat pauses at a choice point in a maze, the representation of space in the
hippocampus sweeps forward (and not backwards) along the possible paths the animal can take from
that point (johnson and redish, 2007). furthermore, the spatial trajectories represented by these
sweeps closely correspond to the rat   s subsequent navigational behavior (pfei   er and foster, 2013).
these results suggest that the hippocampus is critical for the state-transition part of an animal   s
environment model, and that it is part of a system that uses the model to simulate possible future state
sequences to assess the consequences of possible courses of action: a form of planning.

the results described above add to a voluminous literature on neural mechanisms underlying goal-
directed, or model-based, learning and decision making, but many questions remain unanswered. for
example, how can areas as structurally similar as the dls and dms be essential components of modes
of learning and behavior that are as di   erent as model-free and model-based algorithms? are separate
structures responsible for (what we call) the transition and reward components of an environment
model? is all planning conducted at decision time via simulations of possible future courses of action as
the forward sweeping activity in the hippocampus suggests? in other words, is all planning something
like a rollout algorithm (section 8.10)? or are models sometimes engaged in the background to re   ne or
recompute value information as illustrated by the dyna architecture (section 8.2)? how does the brain
arbitrate between the use of the habit and goal-directed systems? is there, in fact, a clear separation
between the neural substrates of these systems?

the evidence is not pointing to a positive answer to this last question. summarizing the situation,
doll, simon, and daw (2012) wrote that    model-based in   uences appear ubiquitous more or less wher-
ever the brain processes reward information,    and this is true even in the regions thought to be critical

15.12. addiction

339

for model-free learning. this includes the dopamine signals themselves, which can exhibit the in   u-
ence of model-based information in addition to the reward prediction errors thought to be the basis of
model-free processes.

continuing neuroscience research informed by id23   s model-free and model-based
distinction has the potential to sharpen our understanding of habitual and goal-directed processes in
the brain. a better grasp of these neural mechanisms may lead to algorithms combining model-free and
model-based methods in ways that have not yet been explored in computational id23.

15.12 addiction

understanding the neural basis of drug abuse is a high-priority goal of neuroscience with the potential
to produce new treatments for this serious public health problem. one view is that drug craving is the
result of the same motivation and learning processes that lead us to seek natural rewarding experiences
that serve our biological needs. addictive substances, by being intensely reinforcing, e   ectively co-opt
our natural mechanisms of learning and decision making. this is plausible given that many   though not
all   drugs of abuse increase levels of dopamine either directly or indirectly in regions around terminals
of dopamine neuron axons in the striatum, a brain structure    rmly implicated in normal reward-
based learning (section 15.7). but the self-destructive behavior associated with drug addiction is not
characteristic of normal learning. what is di   erent about dopamine-mediated learning when the reward
is the result of an addictive drug? is addiction the result of normal learning in response to substances
that were largely unavailable throughout our evolutionary history, so that evolution could not select
against their damaging e   ects? or do addictive substances somehow interfere with normal dopamine-
mediated learning?

the reward prediction error hypothesis of dopamine neuron activity and its connection to td learning
are the basis of a model due to redish (2004) of some   but certainly not all   features of addiction.
the model is based on the observation that administration of cocaine and some other addictive drugs
produces a transient increase in dopamine. in the model, this dopamine surge is assumed to increase
the td error,   , in a way that cannot be cancelled out by changes in the value function.
in other
words, whereas    is reduced to the degree that a normal reward is predicted by antecedent events
(section 15.6), the contribution to    due to an addictive stimulus does not decrease as the reward signal
becomes predicted: drug rewards cannot be    predicted away.    the model does this by preventing
   from ever becoming negative when the reward signal is due to an addictive drug, thus eliminating
the error-correcting feature of td learning for states associated with administration of the drug. the
result is that the values of these states increase without bound, making actions leading to these states
preferred above all others.

addictive behavior is much more complicated than this result from redish   s model, but the model   s
main idea may be a piece of the puzzle. or the model might be misleading. dopamine appears not
to play a critical role in all forms of addiction, and not everyone is equally susceptible to developing
addictive behavior. moreover, the model does not include the changes in many circuits and brain regions
that accompany chronic drug taking, for example, changes that lead to a drug   s diminishing e   ect with
repeated use.
it is also likely that addiction involves model-based processes. still, redish   s model
illustrates how id23 theory can be enlisted in the e   ort to understand a major health
problem. in a similar manner, id23 theory has been in   uential in the development
of the new    eld of computational psychiatry, which aims to improve understanding of mental disorders
through mathematical and computational methods.

340

chapter 15. neuroscience

15.13

summary

the neural pathways involved in the brain   s reward system are complex and incompletely understood,
but neuroscience research directed toward understanding these pathways and their roles in behavior
is progressing rapidly. this research is revealing striking correspondences between the brain   s reward
system and the theory of id23 as presented in this book.

the reward prediction error hypothesis of dopamine neuron activity was proposed by scientists who
recognized striking parallels between the behavior of td errors and the activity of neurons that pro-
duce dopamine, a neurotransmitter essential in mammals for reward-related learning and behavior.
experiments conducted in the late 1980s and 1990s in the laboratory of neuroscientist wolfram schultz
showed that dopamine neurons respond to rewarding events with substantial bursts of activity, called
phasic responses, only if the animal does not expect those events, suggesting that dopamine neurons
are signaling reward prediction errors instead of reward itself. further, these experiments showed that
as an animal learns to predict a rewarding event on the basis of preceding sensory cues, the phasic
activity of dopamine neurons shifts to earlier predictive cues while decreasing to later predictive cues.
this parallels the backing-up e   ect of the td error as a id23 agent learns to predict
reward.

other experimental results    rmly establish that the phasic activity of dopamine neurons is a rein-
forcement signal for learning that reaches multiple areas of the brain by means of profusely branching
axons of dopamine producing neurons. these results are consistent with the distinction we make be-
tween a reward signal, rt, and a reinforcement signal, which is the td error   t in most of the algorithms
we present. phasic responses of dopamine neurons are reinforcement signals, not reward signals.

a prominent hypothesis is that the brain implements something like an actor   critic algorithm. two
structures in the brain (the dorsal and ventral subdivisions of the striatum), both of which play critical
roles in reward-based learning, may function respectively like an actor and a critic. that the td error
is the reinforcement signal for both the actor and the critic    ts well with the facts that dopamine neuron
axons target both the dorsal and ventral subdivisions of the striatum; that dopamine appears to be
critical for modulating synaptic plasticity in both structures; and that the e   ect on a target structure
of a neuromodulator such as dopamine depends on properties of the target structure and not just on
properties of the neuromodulator.

the actor and the critic can be implemented by arti   cial neural networks consisting of neuron-like
units having learning rules based on the policy-gradient actor   critic method described in section 13.5.
each connection in these networks is like a synapse between neurons in the brain, and the learning
rules correspond to rules governing how synaptic e   cacies change as functions of the activities of the
presynaptic and the postsynaptic neurons, together with neuromodulatory input corresponding to input
from dopamine neurons. in this setting, each synapse has its own eligibility trace that records past
activity involving that synapse. the only di   erence between the actor and critic learning rules is that
they use di   erent kinds of eligibility traces: the critic unit   s traces are non-contingent because they do
not involve the critic unit   s output, whereas the actor unit   s traces are contingent because in addition
to the actor unit   s input, they depend on the actor unit   s output. in the hypothetical implementation
of an actor   critic system in the brain, these learning rules respectively correspond to rules governing
plasticity of corticostriatal synapses that convey signals from the cortex to the principal neurons in the
dorsal and ventral striatal subdivisions, synapses that also receive inputs from dopamine neurons.

the learning rule of an actor unit in the actor   critic network closely corresponds to reward-modulated
spike-timing-dependent plasticity. in spike-timing-dependent plasticity (stdp), the relative timing of
pre- and postsynaptic activity determines the direction of synaptic change. in reward-modulated stdp,
changes in synapses in addition depend on a neuromodulator, such as dopamine, arriving within a time
window that can last up to 10 seconds after the conditions for stdp are met. evidence accumulating
that reward-modulated stdp occurs at corticostriatal synapses, where the actor   s learning takes place

15.13.

summary

341

in the hypothetical neural implementation of an actor   critic system, adds to the plausibility of the
hypothesis that something like an actor   critic system exists in the brains of some animals.

the idea of synaptic eligibility and basic features of the actor learning rule derive from klopf   s
hypothesis of the    hedonistic neuron    (klopf, 1972, 1981). he conjectured that individual neurons
seek to obtain reward and to avoid punishment by adjusting the e   cacies of their synapses on the
basis of rewarding or punishing consequences of their action potentials. a neuron   s activity can a   ect
its later input because the neuron is embedded in many feedback loops, some within the animal   s
nervous system and body and others passing through the animal   s external environment. klopf   s idea
of eligibility is that synapses are temporarily marked as eligible for modi   cation if they participated in
the neuron   s    ring (making this the contingent form of eligibility trace). a synapse   s e   cacy is modi   ed
if a reinforcing signal arrives while the synapse is eligible. we alluded to the chemotactic behavior of
a bacterium as an example of a single cell that directs its movements in order to seek some molecules
and to avoid others.

a conspicuous feature of the dopamine system is that    bers releasing dopamine project widely to
multiple parts of the brain. although it is likely that only some populations of dopamine neurons
broadcast the same reinforcement signal, if this signal reaches the synapses of many neurons involved in
actor-type learning, then the situation can be modeled as a team problem. in this type of problem, each
agent in a collection of id23 agents receives the same reinforcement signal, where that
signal depends on the activities of all members of the collection, or team. if each team member uses a
su   ciently capable learning algorithm, the team can learn collectively to improve performance of the
entire team as evaluated by the globally-broadcast reinforcement signal, even if the team members do not
directly communicate with one another. this is consistent with the wide dispersion of dopamine signals
in the brain and provides a neurally plausible alternative to the widely-used error-id26
method for training multilayer networks.

the distinction between model-free and model-based id23 is helping neuroscientists
investigate the neural bases of habitual and goal-directed learning and decision making. research so
far points to their being some brain regions more involved in one type of process than the other, but
the picture remains unclear because model-free and model-based processes do not appear to be neatly
separated in the brain. many questions remain unanswered. perhaps most intriguing is evidence that
the hippocampus, a structure traditionally associated with spatial navigation and memory, appears
to be involved in simulating possible future courses of action as part of an animal   s decision-making
process. this suggests that it is part of a system that uses an environment model for planning.

id23 theory is also in   uencing thinking about neural processes underlying drug
abuse. a model of some features of drug addiction is based on the reward prediction error hypothesis. it
proposes that an addicting stimulant, such as cocaine, destabilizes td learning to produce unbounded
growth in the values of actions associated with drug intake. this is far from a complete model of
addiction, but it illustrates how a computational perspective suggests theories that can be tested with
further research. the new    eld of computational psychiatry similarly focuses on the use of computational
models, some derived from id23, to better understand mental disorders.

this chapter only touched the surface of how the neuroscience of id23 and the
development of id23 in computer science and engineering have in   uenced one an-
other. most features of id23 algorithms owe their design to purely computational
considerations, but some have been in   uenced by hypotheses about neural learning mechanisms. re-
markably, as experimental data has accumulated about the brain   s reward processes, many of the purely
computationally-motivated features of id23 algorithms are turning out to be consis-
tent with neuroscience data. other features of computational id23, such eligibility
traces and the ability of teams of id23 agents to learn to act collectively under the
in   uence of a globally-broadcast reinforcement signal, may also turn out to parallel experimental data
as neuroscientists continue to unravel the neural basis of reward-based animal learning and behavior.

342

chapter 15. neuroscience

bibliographical and historical remarks

the number of publications treating parallels between the neuroscience of learning and decision making
and the approach to id23 presented in this book is enormous. we can cite only a
small selection. niv (2009), dayan and niv (2008), gimcher (2011), ludvig, bellemare, and pearson
(2011), and shah (2012) are good places to start.

together with economics, evolutionary biology, and mathematical psychology, id23
theory is helping to formulate quantitative models of the neural mechanisms of choice in humans and
non-human primates. with its focus on learning, this chapter only lightly touches upon the neuroscience
of decision making. glimcher (2003) introduced the    eld of    neuroeconomics,    in which reinforcement
learning contributes to the study of the neural basis of decision making from an economics perspec-
tive. see also glimcher and fehr (2013). the text on computational and mathematical modeling in
neuroscience by dayan and abbott (2001) includes id23   s role in these approaches.
sterling and laughlin (2015) examined the neural basis of learning in terms of general design principles
that enable e   cient adaptive behavior.

15.1

15.2

15.3

there are many good expositions of basic neuroscience. kandel, schwartz, jessell, siegelbaum,
and hudspeth (2013) is an authoritative and very comprehensive source.

berridge and kringelbach (2008) reviewed the neural basis of reward and pleasure, pointing out
that reward processing has many dimensions and involves many neural systems. space prevents
discussion of the in   uential research of berridge and robinson (1998), who distinguish between
the hedonic impact of a stimulus, which they call    liking,    and the motivational e   ect, which
they call    wanting.    hare, o   doherty, camerer, schultz, and rangel (2008) examined the
neural basis of value-related signals from an economic perspective, distinguishing between goal
values, decision values, and prediction errors. decision value is goal value minus action cost.
see also rangel, camerer, and montague (2008), rangel and hare (2010), and peters and
b  uchel (2010).

the reward prediction error hypothesis of dopamine neuron activity is most prominently dis-
cussed by schultz, montague, and dayan (1997). the hypothesis was    rst explicitly put forward
by montague, dayan, and sejnowski (1996). as they stated the hypothesis, it referred to re-
ward prediction errors (rpes) but not speci   cally to td errors; however, their development
of the hypothesis made it clear that they were referring to td errors. the earliest recogni-
tion of the td-error/dopamine connection of which we are aware is that of montague, dayan,
nowlan, pouget, and sejnowski (1992), who proposed a td-error-modulated hebbian learning
rule motivated by results on dopamine signaling from schultz   s group. the connection was
also pointed out in an abstract by quartz, dayan, montague, and sejnowski (1992). mon-
tague and sejnowski (1994) emphasized the importance of prediction in the brain and outlined
how predictive hebbian learning modulated by td errors could be implemented via a di   use
neuromodulatory system, such as the dopamine system. friston, tononi, reeke, sporns, and
edelman (1994) presented a model of value-dependent learning in the brain in which synaptic
changes are mediated by a td-like error provided by a global neuromodulatory signal (al-
though they did not single out dopamine). montague, dayan, person, and sejnowski (1995)
presented a model of honeybee foraging using the td error. the model is based on research
by hammer, menzel, and colleagues (hammer and menzel, 1995; hammer, 1997) showing that
the neuromodulator octopamine acts as a reinforcement signal in the honeybee. montague et
al. (1995) pointed out that dopamine likely plays a similar role in the vertebrate brain. barto
(1995) related the actor   critic architecture to basal-ganglionic circuits and discussed the re-
lationship between td learning and the main results from schultz   s group. houk, adams,
and barto (1995) suggested how td learning and the actor   critic architecture might map onto

15.13.

summary

343

the anatomy, physiology, and molecular mechanism of the basal ganglia. doya and sejnowski
(1998) extended their earlier paper on a model of birdsong learning (doya and sejnowski, 1994)
by including a td-like error identi   ed with dopamine to reinforce the selection of auditory in-
put to be memorized. o   reilly and frank (2006) and o   reilly, frank, hazy, and watz (2007)
argued that phasic dopamine signals are rpes but not td errors. in support of their theory
they cited results with variable interstimulus intervals that do not match predictions of a sim-
ple td model, as well as the observation that higher-order conditioning beyond second-order
conditioning is rarely observed, while td learning is not so limited. dayan and niv (2008)
discussed    the good, the bad, and the ugly    of how id23 theory and the
reward prediction error hypothesis align with experimental data. glimcher (2011) reviewed
the empirical    ndings that support the reward prediction error hypothesis and emphasized the
signi   cance of the hypothesis for contemporary neuroscience.

15.4 graybiel (2000) is a brief primer on the basal ganglia. the experiments mentioned that involve
optogenetic activation of dopamine neurons were conducted by tsai, zhang, adamantidis,
stuber, bonci, de lecea, and deisseroth (2009), steinberg, kei   in, boivin, witten, deisseroth,
and janak (2013), and claridge-chang, roorda, vrontou, sjulson, li, hirsh, and miesenb  ock
(2009). fiorillo, yun, and song (2013), lammel, lim, and malenka (2014), and saddoris,
cacciapaglia, wightmman, and carelli (2015) are among studies showing that the signaling
properties of dopamine neurons are specialized for di   erent target regions. rpe-signaling
neurons may belong to one among multiple populations of dopamine neurons having di   erent
targets and subserving di   erent functions. eshel, tian, bukwich, and uchida (2016) found
homogeneity of reward prediction error responses of dopamine neurons in the lateral vta during
classical conditioning in mice, though their results do not rule out response diversity across wider
areas. gershman, pesaran, and daw (2009) studied id23 tasks that can be
decomposed into independent components with separate reward signals,    nding evidence in
human neuroimaging data suggesting that the brain exploits this kind of structure.

15.5

15.6

15.7

schultz   s 1998 survey article (schultz, 1998) is a good entr  ee into the very extensive literature
on reward predicting signaling of dopamine neurons. berns, mcclure, pagnoni, and montague
(2001), breiter, aharon, kahneman, dale, and shizgal (2001), pagnoni, zink, montague, and
berns (2002), and o   doherty, dayan, friston, critchley, and dolan (2003) described functional
brain imaging studies supporting the existence of signals like td errors in the human brain.

this section roughly follows barto (1995) in explaining how td errors mimic the main results
from schultz   s group on the phasic responses of dopamine neurons.

this section is largely based on takahashi, schoenbaum, and niv (2008) and niv (2009). to
the best of our knowledge, barto (1995) and houk, adams, and barto (1995)    rst speculated
about possible implementations of actor   critic algorithms in the basal ganglia. on the basis
of functional magnetic resonance imaging of human subjects while engaged in instrumental
conditioning, o   doherty, dayan, schultz, deichmann, friston, and dolan (2004) suggested
that the actor and the critic are most likely located respectively in the dorsal and ventral
striatum. gershman, moustafa, and ludvig (2013) focused on how time is represented in
id23 models of the basal ganglia, discussing evidence for, and implications
of, various computational approaches to time representation.

the hypothetical neural implementation of the actor   critic architecture described in this sec-
tion includes very little detail about known basal ganglia anatomy and physiology. in addition
to the more detailed hypothesis of houk, adams, and barto (1995), a number of other hypothe-
ses include more speci   c connections to anatomy and physiology and are claimed to explain
additional data. these include hypotheses proposed by suri and schultz (1998, 1999), brown,

344

15.8

chapter 15. neuroscience

bullock, and grossberg (1999), contreras-vidal and schultz (1999), suri, bargas, and arbib
(2001), o   reilly and frank (2006), and o   reilly, frank, hazy, and watz (2007). joel, niv, and
ruppin (2002) critically evaluated the anatomical plausibility of several of these models and
present an alternative intended to accommodate some neglected features of basal ganglionic
circuitry.

the actor learning rule discussed here is more complicated than the one in the early actor   
critic network of barto et al. (1983). actor-unit eligibility traces in that network were traces
of just at    x(st) instead of the full (at       (at|st,   ))x(st). that work did not bene   t from
the policy-gradient theory presented in chapter 13 or the contributions of williams (1986,
1992), who showed how an arti   cial neural network of bernoulli-logistic units could implement
a policy-gradient method.

reynolds and wickens (2002) proposed a three-factor rule for synaptic plasticity in the cor-
ticostriatal pathway in which dopamine modulates changes in corticostriatal synaptic e   cacy.
they discussed the experimental support for this kind of learning rule and its possible molecular
basis. the de   nitive demonstration of spike-timing-dependent plasticity (stdp) is attributed
to markram, l  ubke, frotscher, and sakmann (1997), with evidence from earlier experiments
by levy and steward (1983) and others that the relative timing of pre- and postsynaptic spikes
is critical for inducing changes in synaptic e   cacy. rao and sejnowski (2001) suggested how
stdp could be the result of a td-like mechanism at synapses with non-contingent eligibility
traces lasting about 10 milliseconds. dayan (2002) commented that this would require an error
as in sutton and barto   s (1981) early model of classical conditioning and not a true td er-
ror. representative publications from the extensive literature on reward-modulated stdp are
wickens (1990), reynolds and wickens (2002), and calabresi, picconi, tozzi and di filippo
(2007). pawlak and kerr (2008) showed that dopamine is necessary to induce stdp at the
corticostriatal synapses of medium spiny neurons. see also pawlak, wickens, kirkwood, and
kerr (2010). yagishita, hayashi-takagi, ellis-davies, urakubo, ishii, and kasai (2014) found
that dopamine promotes spine enlargement of the medium spiny neurons of mice only during a
time window of from 0.3 to 2 seconds after stdp stimulation. izhikevich (2007) proposed and
explored the idea of using stdp timing conditions to trigger contingent eligibility traces.

15.9

klopf   s hedonistic neuron hypothesis (klopf 1972, 1982) inspired our actor   critic algorithm
implemented as an arti   cial neural network with a single neuron-like unit, called the actor
unit, implementing a law-of-e   ect-like learning rule (barto, sutton, and anderson, 1983).
ideas related to klopf   s synaptically-local eligibility have been proposed by others. crow (1968)
proposed that changes in the synapses of cortical neurons are sensitive to the consequences of
neural activity. emphasizing the need to address the time delay between neural activity and
its consequences in a reward-modulated form of synaptic plasticity, he proposed a contingent
form of eligibility, but associated with entire neurons instead of individual synapses. according
to his hypothesis, a wave of neuronal activity

leads to a short-term change in the cells involved in the wave such that they are
picked out from a background of cells not so activated.
... such cells are rendered
sensitive by the short-term change to a reward signal ... in such a way that if such a
signal occurs before the end of the decay time of the change the synaptic connexions
between the cells are made more e   ective. (crow, 1968)

crow argued against previous proposals that reverberating neural circuits play this role by
pointing out that the e   ect of a reward signal on such a circuit would    ...establish the synaptic
connexions leading to the reverberation (that is to say, those involved in activity at the time
of the reward signal) and not those on the path which led to the adaptive motor output.   

15.13.

summary

345

crow further postulated that reward signals are delivered via a    distinct neural    ber system,   
presumably the one into which olds and milner (1954) tapped, that would transform synaptic
connections    from a short into a long-term form.   

in another farsighted hypothesis, miller (1981) proposed a law-of-e   ect-like learning rule that
includes synaptically-local contingent eligibility traces:

... it is envisaged that in a particular sensory situation neurone b, by chance,    res a
   meaningful burst    of activity, which is then translated into motor acts, which then
change the situation. it must be supposed that the meaningful burst has an in   uence,
at the neuronal level, on all of its own synapses which are active at the time ... thereby
making a preliminary selection of the synapses to be strengthened, though not yet
actually strengthening them. ...the strengthening signal ... makes the    nal selection
... and accomplishes the de   nitive change in the appropriate synapses. (miller, 1981,
p. 81)

miller   s hypothesis also included a critic-like mechanism, which he called a    sensory analyzer
unit,    that worked according to classical conditioning principles to provide reinforcement sig-
nals to neurons so that they would learn to move from lower- to higher-valued states, thus
anticipating the use of the td error as a reinforcement signal in the actor   critic architecture.
miller   s idea not only parallels klopf   s (with the exception of its explicit invocation of a distinct
   strengthening signal   ), it also anticipated the general features of reward-modulated stdp.

a related though di   erent idea, which seung (2003) called the    hedonistic synapse,    is that
synapses individually adjust the id203 that they release neurotransmitter in the manner
of the law of e   ect: if reward follows release, the release id203 increases, and decreases
if reward follows failure to release. this is essentially the same as the learning scheme minsky
used in his 1954 princeton ph.d. dissertation (minsky, 1954), where he called the synapse-like
learning element a snarc (stochastic neural-analog reinforcement calculator). contingent
eligibility is involved in these ideas too, although it is contingent on the activity of an individual
synapse instead of the postsynaptic neuron.

frey and morris (1997) proposed the idea of a    synaptic tag    for the induction of long-lasting
strengthening of synaptic e   cacy. though not unlike klopf   s eligibility, their tag was hypoth-
esized to consist of a temporary strengthening of a synapse that could be transformed into a
long-lasting strengthening by subsequent neuron activation. the model of o   reilly and frank
(2006) and o   reilly, frank, hazy, and watz (2007) uses working memory to bridge temporal
intervals instead of eligibility traces. wickens and kotter (1995) discuss possible mechanisms
for synaptic eligibility. he, huertas, hong, tie, hell, shouval, kirkwood (2015) provide evi-
dence supporting the existence of contingent eligibility traces in synapses of cortical neurons
with time courses like those of the eligibility traces klopf postulated.

the metaphor of a neuron using a learning rule related to bacterial chemotaxis was discussed
by barto (1989). koshland   s extensive study of bacterial chemotaxis was in part motivated by
similarities between features of bacteria and features of neurons (koshland, 1980). see also berg
(1975). shimansky (2009) proposed a synaptic learning rule somewhat similar to seung   s men-
tioned above in which each synapse individually acts like a chemotactic bacterium. in this case
a collection of synapses    swims    toward attractants in the high-dimensional space of synaptic
weight values. montague, dayan, person, and sejnowski (1995) proposed a chemotactic-like
model of the bee   s foraging behavior involving the neuromodulator octopamine.

15.10 research on the behavior of id23 agents in team and game problems has a long
history roughly occurring in three phases. to the best of our knowledge, the    rst phase began
with investigations by the russian mathematician and physicist m. l. tsetlin. a collection of

346

chapter 15. neuroscience

his work was published as tsetlin (1973) after his death in 1966. our sections 1.7 and 4.8 refer
to his study of learning automata in connection to bandit problems. the tsetlin collection
also includes studies of learning automata in team and game problems, which led to later
work in this area using stochastic learning automata as described by narendra and thathachar
(1974), viswanathan and narendra (1974), lakshmivarahan and narendra (1982), narendra
and wheeler (1983), narendra (1989), and thathachar and sastry (2002). thathachar and
sastry (2011) is a more recent comprehensive account. these studies were mostly restricted to
non-associative learning automata, meaning that they did not address associative, or contextual,
bandit problems (section 2.9).

the second phase began with the extension of learning automata to the associative, or contex-
tual, case. barto, sutton, and brouwer (1981) and barto and sutton (1981) experimented with
associative stochastic learning automata in single-layer arti   cial neural networks to which a
global reinforcement signal was broadcast. they called neuron-like elements implementing this
kind of learning associative search elements (ases). barto and anandan (1985) introduced a
more sophisticated associative id23 algorithm called the associative reward-
penalty (ar   p) algorithm. they proved a convergence result by combining theory of stochastic
learning automata with theory of pattern classi   cation. barto (1985, 1986) and barto and
jordan (1987) described results with teams of ar   p units connected into multi-layer neural
networks, showing that they could learn nonlinear functions, such as xor and others, with a
globally-broadcast reinforcement signal. barto (1985) extensively discussed this approach to
arti   cial neural networks and how this type of learning rule is related to others in the literature
at that time. williams (1992) mathematically analyzed and broadened this class of learning
rules and related their use to the error id26 method for training multilayer arti   cial
neural networks. williams (1988) described several ways that id26 and reinforce-
ment learning can be combined for training arti   cial neural networks. williams (1992) showed
that a special case of the ar   p algorithm is a reinforce algorithm, although better results
were obtained with the general ar   p algorithm (barto,1985).
the third phase of interest in teams of id23 agents was in   uenced by increased
understanding of the role of dopamine as a widely broadcast neuromodulator and speculation
about the existence of reward-modulated stdp. much more so than earlier research, this
research considers details of synaptic plasticity and other constraints from neuroscience. pub-
lications include the following (chronologically and alphabetically): bartlett and baxter (1999,
2000), xie and seung (2004), baras and meir (2007), farries and fairhall (2007), florian (2007),
izhikevich (2007), pecevski, maass, and legenstein (2007), legenstein, pecevski, and maass
(2008), kolodziejski, porr, and w  org  otter (2009), urbanczik and senn (2009), and vasilaki,
fr  emaux, urbanczik, senn, and gerstner (2009). now  e, vrancx, and de hauwere (2012) re-
viewed more recent developments in the wider    eld of multi-agent id23

15.11 yin and knowlton (2006) reviewed    ndings from outcome-devaluation experiments with rodents
supporting the view that habitual and goal-directed behavior (as psychologists use the phrase)
are respectively most associated with processing in the dorsolateral striatum (dls) and the
dorsomedial striatum (dms). results of functional imaging experiments with human subjects
in the outcome-devaluation setting by valentin, dickinson, and o   doherty (2007) suggest that
the orbitofrontal cortex (ofc) is an important component of goal-directed choice. single unit
recordings in monkeys by padoa-schioppa and assad (2006) support the role of the ofc in
encoding values guiding choice behavior. rangel, camerer, and montague (2008) and rangel
and hare (2010) reviewed    ndings from the perspective of neuroeconomics about how the
brain makes goal-directed decisions. pezzulo, van der meer, lansink, and pennartz (2014)
reviewed the neuroscience of internally generated sequences and presented a model of how
these mechanisms might be components of model-based planning. daw and shohamy (2008)

15.13.

summary

347

proposed that while dopamine signaling connects well to habitual, or model-free, behavior,
other processes are involved in goal-directed, or model-based, behavior. data from experiments
by bromberg-martin, matsumoto, hong, and hikosaka (2010) indicate that dopamine signals
contain information pertinent to both habitual and goal-directed behavior. doll, simon, and
daw (2012) argued that there may not a clear separation in the brain between mechanisms
that subserve habitual and goal-directed learning and choice.

15.12 kei   in and janak (2015) reviewed connections between td errors and addiction. nutt, lingford-
hughes, erritzoe, and stokes (2015) critically evaluated the hypothesis that addiction is due to
a disorder of the dopamine system. montague, dolan, friston, and dayan (2012) outlined the
goals and early e   orts in the    eld of computational psychiatry, and adams, huys, and roiser
(2015) reviewed more recent progress.

348

chapter 15. neuroscience

chapter 16

applications and case studies

in this    nal chapter we present a few case studies of id23. several of these are
substantial applications of potential economic signi   cance. one, samuel   s checkers player, is primarily
of historical interest. our presentations are intended to illustrate some of the trade-o   s and issues that
arise in real applications. for example, we emphasize how domain knowledge is incorporated into the
formulation and solution of the problem. we also highlight the representation issues that are so often
critical to successful applications. the algorithms used in some of these case studies are substantially
more complex than those we have presented in the rest of the book. applications of reinforcement
learning are still far from routine and typically require as much art as science. making applications
easier and more straightforward is one of the goals of current research in id23.

16.1 td-gammon

one of the most impressive applications of id23 to date is that by gerald tesauro
to the game of backgammon (tesauro, 1992, 1994, 1995, 2002). tesauro   s program, td-gammon,
required little backgammon knowledge, yet learned to play extremely well, near the level of the world   s
strongest grandmasters. the learning algorithm in td-gammon was a straightforward combination of
the td(  ) algorithm and nonlinear function approximation using a multilayer neural network trained
by backpropagating td errors.

backgammon is a major game in the sense that it is played throughout the world, with numerous
tournaments and regular world championship matches. it is in part a game of chance, and it is a popular
vehicle for waging signi   cant sums of money. there are probably more professional backgammon players
than there are professional chess players. the game is played with 15 white and 15 black pieces on a
board of 24 locations, called points. figure 16.1 shows a typical position early in the game, seen from
the perspective of the white player.

in this    gure, white has just rolled the dice and obtained a 5 and a 2. this means that he can move
one of his pieces 5 steps and one (possibly the same piece) 2 steps. for example, he could move two
pieces from the 12 point, one to the 17 point, and one to the 14 point. white   s objective is to advance
all of his pieces into the last quadrant (points 19   24) and then o    the board. the    rst player to remove
all his pieces wins. one complication is that the pieces interact as they pass each other going in di   erent
directions. for example, if it were black   s move in figure 16.1, he could use the dice roll of 2 to move a
piece from the 24 point to the 22 point,    hitting    the white piece there. pieces that have been hit are
placed on the    bar    in the middle of the board (where we already see one previously hit black piece),
from whence they reenter the race from the start. however, if there are two pieces on a point, then the
opponent cannot move to that point; the pieces are protected from being hit. thus, white cannot use

349

350

chapter 16. applications and case studies

figure 16.1: a backgammon position

his 5   2 dice roll to move either of his pieces on the 1 point, because their possible resulting points are
occupied by groups of black pieces. forming contiguous blocks of occupied points to block the opponent
is one of the elementary strategies of the game.

backgammon involves several further complications, but the above description gives the basic idea.
with 30 pieces and 24 possible locations (26, counting the bar and o   -the-board) it should be clear
that the number of possible backgammon positions is enormous, far more than the number of memory
elements one could have in any physically realizable computer. the number of moves possible from each
position is also large. for a typical dice roll there might be 20 di   erent ways of playing. in considering
future moves, such as the response of the opponent, one must consider the possible dice rolls as well.
the result is that the game tree has an e   ective branching factor of about 400. this is far too large to
permit e   ective use of the conventional heuristic search methods that have proved so e   ective in games
like chess and checkers.

on the other hand, the game is a good match to the capabilities of td learning methods. although
the game is highly stochastic, a complete description of the game   s state is available at all times. the
game evolves over a sequence of moves and positions until    nally ending in a win for one player or
the other, ending the game. the outcome can be interpreted as a    nal reward to be predicted. on
the other hand, the theoretical results we have described so far cannot be usefully applied to this task.
the number of states is so large that a lookup table cannot be used, and the opponent is a source of
uncertainty and time variation.

td-gammon used a nonlinear form of td(  ). the estimated value,   v(s,w), of any state (board
position) s was meant to estimate the id203 of winning starting from state s. to achieve this,
rewards were de   ned as zero for all time steps except those on which the game is won. to implement the
value function, td-gammon used a standard multilayer neural network, much as shown in figure 16.2.
(the real network had two additional units in its    nal layer to estimate the id203 of each player   s
winning in a special way called a    gammon    or    backgammon.   ) the network consisted of a layer
of input units, a layer of hidden units, and a    nal output unit. the input to the network was a
representation of a backgammon position, and the output was an estimate of the value of that position.

in the    rst version of td-gammon, td-gammon 0.0, backgammon positions were represented to
the network in a relatively direct way that involved little backgammon knowledge. it did, however,
involve substantial knowledge of how neural networks work and how information is best presented to
them. it is instructive to note the exact representation tesauro chose. there were a total of 198 input
units to the network. for each point on the backgammon board, four units indicated the number of
white pieces on the point. if there were no white pieces, then all four units took on the value zero. if
there was one piece, then the    rst unit took on the value 1. this encoded the elementary concept of a
   blot,    i.e., a piece that can be hit by the opponent. if there were two or more pieces, then the second

white pieces move    counterclockwise123456789101112181716151413192021222324    black pieces move clockwise16.1. td-gammon

351

unit was set to 1. this encoded the basic concept of a    made point    on which the opponent cannot
land. if there were exactly three pieces on the point, then the third unit was set to 1. this encoded
the basic concept of a    single spare,    i.e., an extra piece in addition to the two pieces that made the
point. finally, if there were more than three pieces, the fourth unit was set to a value proportionate
to the number of additional pieces beyond three. letting n denote the total number of pieces on the
point, if n > 3, then the fourth unit took on the value (n     3)/2. this encoded a linear representation
of    multiple spares    at the given point.

with four units for white and four for black at each of the 24 points, that made a total of 192 units.
two additional units encoded the number of white and black pieces on the bar (each took the value
n/2, where n is the number of pieces on the bar), and two more encoded the number of black and white
pieces already successfully removed from the board (these took the value n/15, where n is the number
of pieces already borne o   ). finally, two units indicated in a binary fashion whether it was white   s or
black   s turn to move. the general logic behind these choices should be clear. basically, tesauro tried
to represent the position in a straightforward way, while keeping the number of units relatively small.
he provided one unit for each conceptually distinct possibility that seemed likely to be relevant, and
he scaled them to roughly the same range, in this case between 0 and 1.

given a representation of a backgammon position, the network computed its estimated value in the
standard way. corresponding to each connection from an input unit to a hidden unit was a real-valued
weight. signals from each input unit were multiplied by their corresponding weights and summed at
the hidden unit. the output, h(j), of hidden unit j was a nonlinear sigmoid function of the weighted
sum:

h(j) =   (cid:32)(cid:88)i

wijxi(cid:33) =

1 + e   

(cid:80)

1

i wij xi

,

where xi is the value of the ith input unit and wij is the weight of its connection to the jth hidden unit
(all the weights in the network together make up the parameter vector w). the output of the sigmoid
is always between 0 and 1, and has a natural interpretation as a id203 based on a summation
of evidence. the computation from hidden units to the output unit was entirely analogous. each
connection from a hidden unit to the output unit had a separate weight. the output unit formed the
weighted sum and then passed it through the same sigmoid nonlinearity.

td-gammon used the semi-gradient form of the td(  ) algorithm described in section 12.2, with the
gradients computed by the error id26 algorithm (rumelhart, hinton, and williams, 1986).

figure 16.2: the neural network used in td-gammon

vt+1! vthidden units (40-80)backgammon position (198 input units)predicted id203of winning, vttd error,. . .. . .. . .. . .. . .. . .15.1.td-gammon263gationalgorithm(rumelhart,hinton,andwilliams,1986).recallthatthegeneralupdateruleforthiscaseiswt+1=wt+   hrt+1+   v(st+1,wt)   v(st,wt)iet,(15.1)wherewtisthevectorofallmodi   ableparameters(inthiscase,theweightsofthenetwork)andetisavectorofeligibilitytraces,oneforeachcomponentofwt,updatedbyet=  et 1+rwt  v(st,wt),withe0=0.thegradientinthisequationcanbecomputede cientlybytheid26procedure.forthebackgammonapplication,inwhich =1andtherewardisalwayszeroexceptuponwinning,thetderrorportionofthelearningruleisusuallyjust  v(st+1,w)   v(st,w),assuggestedinfigure15.2.toapplythelearningruleweneedasourceofbackgammongames.tesauroobtainedanunendingsequenceofgamesbyplayinghislearningbackgammonplayeragainstitself.tochooseitsmoves,td-gammonconsideredeachofthe20orsowaysitcouldplayitsdicerollandthecorrespondingpositionsthatwouldresult.theresultingpositionsareafterstatesasdiscussedinsection6.8.thenetworkwasconsultedtoestimateeachoftheirvalues.themovewasthenselectedthatwouldleadtothepositionwiththehighestestimatedvalue.continuinginthisway,withtd-gammonmakingthemovesforbothsides,itwaspossibletoeasilygeneratelargenumbersofbackgammongames.eachgamewastreatedasanepisode,withthesequenceofpositionsactingasthestates,s0,s1,s2,....tesauroappliedthenonlineartdrule(15.1)fullyincrementally,thatis,aftereachindividualmove.theweightsofthenetworkweresetinitiallytosmallrandomvalues.theinitialevaluationswerethusentirelyarbitrary.sincethemoveswereselectedonthebasisoftheseevaluations,theinitialmoveswereinevitablypoor,andtheinitialgamesoftenlastedhundredsorthousandsofmovesbeforeonesideortheotherwon,almostbyaccident.afterafewdozengameshowever,performanceimprovedrapidly.afterplayingabout300,000gamesagainstitself,td-gammon0.0asde-scribedabovelearnedtoplayapproximatelyaswellasthebestpreviousbackgammoncomputerprograms.thiswasastrikingresultbecausealltheprevioushigh-performancecomputerprogramshadusedextensivebackgam-monknowledge.forexample,thereigningchampionprogramatthetimewas,arguably,neurogammon,anotherprogramwrittenbytesaurothatusedaneuralnetworkbutnottdlearning.neurogammon   snetworkwastrainedonalargetrainingcorpusofexemplarymovesprovidedbybackgammonex-perts,and,inaddition,startedwithasetoffeaturesspeciallycraftedfortd error15.1.td-gammon263gationalgorithm(rumelhart,hinton,andwilliams,1986).recallthatthegeneralupdateruleforthiscaseiswt+1=wt+   hrt+1+   v(st+1,wt)   v(st,wt)iet,(15.1)wherewtisthevectorofallmodi   ableparameters(inthiscase,theweightsofthenetwork)andetisavectorofeligibilitytraces,oneforeachcomponentofwt,updatedbyet=  et 1+rwt  v(st,wt),withe0=0.thegradientinthisequationcanbecomputede cientlybytheid26procedure.forthebackgammonapplication,inwhich =1andtherewardisalwayszeroexceptuponwinning,thetderrorportionofthelearningruleisusuallyjust  v(st+1,w)   v(st,w),assuggestedinfigure15.2.toapplythelearningruleweneedasourceofbackgammongames.tesauroobtainedanunendingsequenceofgamesbyplayinghislearningbackgammonplayeragainstitself.tochooseitsmoves,td-gammonconsideredeachofthe20orsowaysitcouldplayitsdicerollandthecorrespondingpositionsthatwouldresult.theresultingpositionsareafterstatesasdiscussedinsection6.8.thenetworkwasconsultedtoestimateeachoftheirvalues.themovewasthenselectedthatwouldleadtothepositionwiththehighestestimatedvalue.continuinginthisway,withtd-gammonmakingthemovesforbothsides,itwaspossibletoeasilygeneratelargenumbersofbackgammongames.eachgamewastreatedasanepisode,withthesequenceofpositionsactingasthestates,s0,s1,s2,....tesauroappliedthenonlineartdrule(15.1)fullyincrementally,thatis,aftereachindividualmove.theweightsofthenetworkweresetinitiallytosmallrandomvalues.theinitialevaluationswerethusentirelyarbitrary.sincethemoveswereselectedonthebasisoftheseevaluations,theinitialmoveswereinevitablypoor,andtheinitialgamesoftenlastedhundredsorthousandsofmovesbeforeonesideortheotherwon,almostbyaccident.afterafewdozengameshowever,performanceimprovedrapidly.afterplayingabout300,000gamesagainstitself,td-gammon0.0asde-scribedabovelearnedtoplayapproximatelyaswellasthebestpreviousbackgammoncomputerprograms.thiswasastrikingresultbecausealltheprevioushigh-performancecomputerprogramshadusedextensivebackgam-monknowledge.forexample,thereigningchampionprogramatthetimewas,arguably,neurogammon,anotherprogramwrittenbytesaurothatusedaneuralnetworkbutnottdlearning.neurogammon   snetworkwastrainedonalargetrainingcorpusofexemplarymovesprovidedbybackgammonex-perts,and,inaddition,startedwithasetoffeaturesspeciallycraftedfor352

chapter 16. applications and case studies

recall that the general update rule for this case is

(16.1)

wt+1

.

= wt +   (cid:104)rt+1 +     v(st+1,wt)       v(st,wt)(cid:105)zt,

where wt is the vector of all modi   able parameters (in this case, the weights of the network) and zt is
a vector of eligibility traces, one for each component of wt, updated by

zt

.
=     zt   1 +      v(st,wt),
.
with z0
= 0. the gradient in this equation can be computed e   ciently by the id26 pro-
cedure. for the backgammon application, in which    = 1 and the reward is always zero except upon
winning, the td error portion of the learning rule is usually just   v(st+1,w)       v(st,w), as suggested in
figure 16.2.

to apply the learning rule we need a source of backgammon games. tesauro obtained an unending
sequence of games by playing his learning backgammon player against itself. to choose its moves, td-
gammon considered each of the 20 or so ways it could play its dice roll and the corresponding positions
that would result. the resulting positions are afterstates as discussed in section 6.8. the network was
consulted to estimate each of their values. the move was then selected that would lead to the position
with the highest estimated value. continuing in this way, with td-gammon making the moves for both
sides, it was possible to easily generate large numbers of backgammon games. each game was treated
as an episode, with the sequence of positions acting as the states, s0, s1, s2, . . .. tesauro applied the
nonlinear td rule (16.1) fully incrementally, that is, after each individual move.

the weights of the network were set initially to small random values. the initial evaluations were
thus entirely arbitrary. since the moves were selected on the basis of these evaluations, the initial moves
were inevitably poor, and the initial games often lasted hundreds or thousands of moves before one side
or the other won, almost by accident. after a few dozen games however, performance improved rapidly.

after playing about 300,000 games against itself, td-gammon 0.0 as described above learned to play
approximately as well as the best previous backgammon computer programs. this was a striking result
because all the previous high-performance computer programs had used extensive backgammon knowl-
edge. for example, the reigning champion program at the time was, arguably, neurogammon, another
program written by tesauro that used a neural network but not td learning. neurogammon   s network
was trained on a large training corpus of exemplary moves provided by backgammon experts, and, in
addition, started with a set of features specially crafted for backgammon. neurogammon was a highly
tuned, highly e   ective backgammon program that decisively won the world backgammon olympiad in
1989. td-gammon 0.0, on the other hand, was constructed with essentially zero backgammon knowl-
edge. that it was able to do as well as neurogammon and all other approaches is striking testimony to
the potential of self-play learning methods.

the tournament success of td-gammon 0.0 with zero expert backgammon knowledge suggested an
obvious modi   cation: add the specialized backgammon features but keep the self-play td learning
method. this produced td-gammon 1.0. td-gammon 1.0 was clearly substantially better than all
previous backgammon programs and found serious competition only among human experts. later
versions of the program, td-gammon 2.0 (40 hidden units) and td-gammon 2.1 (80 hidden units),
were augmented with a selective two-ply search procedure. to select moves, these programs looked
ahead not just to the positions that would immediately result, but also to the opponent   s possible dice
rolls and moves. assuming the opponent always took the move that appeared immediately best for him,
the expected value of each candidate move was computed and the best was selected. to save computer
time, the second ply of search was conducted only for candidate moves that were ranked highly after
the    rst ply, about four or    ve moves on average. two-ply search a   ected only the moves selected; the
learning process proceeded exactly as before. the    nal versions of the program, td-gammon 3.0 and
3.1, used 160 hidden units and a selective three-ply search. td-gammon illustrates the combination of
learned value functions and decision-time search as in heuristic search and mcts methods. in follow-on

16.2. samuel   s checkers player

353

work, tesauro and galperin (1997) explored trajectory sampling methods as an alternative to full-width
search, which reduced the error rate of live play by large numerical factors (4x   6x) while keeping the
think time reasonable at     5   10 seconds per move.

program

td-gammon 0.0
td-gammon 1.0
td-gammon 2.0
td-gammon 2.1
td-gammon 3.0

hidden training
units
games
300,000
300,000
800,000
1,500,000
1,500,000

40
80
40
80
80

opponents

results

other programs

tied for best

robertie, magriel, ...    13 pts / 51 games
various grandmasters    7 pts / 38 games
   1 pt / 40 games
+6 pts / 20 games

robertie
kazaros

table 16.1: summary of td-gammon results

during the 1990s, tesauro was able to play his programs in a signi   cant number of games against
world-class human players. a summary of the results is given in table 16.1. based on these results and
analyses by backgammon grandmasters (robertie, 1992; see tesauro, 1995), td-gammon 3.0 appeared
to play at close to, or possibly better than, the playing strength of the best human players in the
world. tesauro reported in a subsequent article (tesauro, 2002) the results of an extensive rollout
analysis of the move decisions and doubling decisions of td-gammon relative to top human players.
the conclusion was that td-gammon 3.1 had a    lopsided advantage    in piece-movement decisions,
and a    slight edge    in doubling decisions, over top humans.

td-gammon had a signi   cant impact on the way the best human players play the game. for example,
it learned to play certain opening positions di   erently than was the convention among the best human
players. based on td-gammon   s success and further analysis, the best human players now play these
positions as td-gammon does (tesauro, 1995). the impact on human play was greatly accelerated
when several other self-teaching neural net backgammon programs inspired by td-gammon, such
as jelly   sh, snowie, and gnubackgammon, became widely available. these programs enabled wide
dissemination of new knowledge generated by the neural nets, resulting in great improvements in the
overall caliber of human tournament play (tesauro, 2002).

16.2 samuel   s checkers player

an important precursor to tesauro   s td-gammon was the seminal work of arthur samuel (1959,
1967) in constructing programs for learning to play checkers. samuel was one of the    rst to make
e   ective use of heuristic search methods and of what we would now call temporal-di   erence learning.
his checkers players are instructive case studies in addition to being of historical interest. we emphasize
the relationship of samuel   s methods to modern id23 methods and try to convey some
of samuel   s motivation for using them.

samuel    rst wrote a checkers-playing program for the ibm 701 in 1952. his    rst learning program
was completed in 1955 and was demonstrated on television in 1956. later versions of the program
achieved good, though not expert, playing skill. samuel was attracted to game-playing as a domain for
studying machine learning because games are less complicated than problems    taken from life    while
still allowing fruitful study of how heuristic procedures and learning can be used together. he chose to
study checkers instead of chess because its relative simplicity made it possible to focus more strongly
on learning.

samuel   s programs played by performing a lookahead search from each current position. they used
what we now call heuristic search methods to determine how to expand the search tree and when to stop
searching. the terminal board positions of each search were evaluated, or    scored,    by a value function,

354

chapter 16. applications and case studies

or    scoring polynomial,    using linear function approximation. in this and other respects samuel   s work
seems to have been inspired by the suggestions of shannon (1950). in particular, samuel   s program
was based on shannon   s minimax procedure to    nd the best move from the current position. working
backward through the search tree from the scored terminal positions, each position was given the score
of the position that would result from the best move, assuming that the machine would always try
to maximize the score, while the opponent would always try to minimize it. samuel called this the
   backed-up score    of the position. when the minimax procedure reached the search tree   s root   the
current position   it yielded the best move under the assumption that the opponent would be using
the same evaluation criterion, shifted to its point of view. some versions of samuel   s programs used
sophisticated search control methods analogous to what are known as    alpha-beta    cuto   s (e.g., see
pearl, 1984).

samuel used two main learning methods, the simplest of which he called rote learning. it consisted
simply of saving a description of each board position encountered during play together with its backed-
up value determined by the minimax procedure. the result was that if a position that had already
been encountered were to occur again as a terminal position of a search tree, the depth of the search
was e   ectively ampli   ed since this position   s stored value cached the results of one or more searches
conducted earlier. one initial problem was that the program was not encouraged to move along the
most direct path to a win. samuel gave it a    a sense of direction    by decreasing a position   s value a
small amount each time it was backed up a level (called a ply) during the minimax analysis.    if the
program is now faced with a choice of board positions whose scores di   er only by the ply number, it
will automatically make the most advantageous choice, choosing a low-ply alternative if winning and
a high-ply alternative if losing    (samuel, 1959, p. 80). samuel found this discounting-like technique
essential to successful learning. rote learning produced slow but continuous improvement that was
most e   ective for opening and endgame play. his program became a    better-than-average novice    after
learning from many games against itself, a variety of human opponents, and from book games in a
supervised learning mode.

rote learning and other aspects of samuel   s work strongly suggest the essential idea of temporal-
di   erence learning   that the value of a state should equal the value of likely following states. samuel
came closest to this idea in his second learning method, his    learning by generalization    procedure for
modifying the parameters of the value function. samuel   s method was the same in concept as that
used much later by tesauro in td-gammon. he played his program many games against another
version of itself and performed an update after each move. the idea of samuel   s update is suggested
by the diagram in figure 16.3. each open circle represents a position where the program moves next,
an on-move position, and each solid circle represents a position where the opponent moves next. an
update was made to the value of each on-move position after a move by each side, resulting in a second
on-move position. the update was toward the minimax value of a search launched from the second
on-move position. thus, the overall e   ect was that of a backing-up over one full move of real events
and then a search over possible events, as suggested by figure 16.3. samuel   s actual algorithm was
signi   cantly more complex than this for computational reasons, but this was the basic idea.

samuel did not include explicit rewards. instead, he    xed the weight of the most important feature,
the piece advantage feature, which measured the number of pieces the program had relative to how
many its opponent had, giving higher weight to kings, and including re   nements so that it was better
to trade pieces when winning than when losing. thus, the goal of samuel   s program was to improve its
piece advantage, which in checkers is highly correlated with winning.

however, samuel   s learning method may have been missing an essential part of a sound temporal-
di   erence algorithm. temporal-di   erence learning can be viewed as a way of making a value function
consistent with itself, and this we can clearly see in samuel   s method. but also needed is a way of tying
the value function to the true value of the states. we have enforced this via rewards and by discounting
or giving a    xed value to the terminal state. but samuel   s method included no rewards and no special
treatment of the terminal positions of games. as samuel himself pointed out, his value function could

16.3. watson   s daily-double wagering

355

figure 16.3: the backup diagram for samuel   s checkers player.

have become consistent merely by giving a constant value to all positions. he hoped to discourage such
solutions by giving his piece-advantage term a large, nonmodi   able weight. but although this may
decrease the likelihood of    nding useless evaluation functions, it does not prohibit them. for example,
a constant function could still be attained by setting the modi   able weights so as to cancel the e   ect
of the nonmodi   able one.

since samuel   s learning procedure was not constrained to    nd useful evaluation functions, it should
have been possible for it to become worse with experience. in fact, samuel reported observing this during
extensive self-play training sessions. to get the program improving again, samuel had to intervene and
set the weight with the largest absolute value back to zero. his interpretation was that this drastic
intervention jarred the program out of local optima, but another possibility is that it jarred the program
out of evaluation functions that were consistent but had little to do with winning or losing the game.

despite these potential problems, samuel   s checkers player using the generalization learning method
approached    better-than-average    play. fairly good amateur opponents characterized it as    tricky but
beatable    (samuel, 1959). in contrast to the rote-learning version, this version was able to develop
a good middle game but remained weak in opening and endgame play. this program also included
an ability to search through sets of features to    nd those that were most useful in forming the value
function. a later version (samuel, 1967) included re   nements in its search procedure, such as alpha-beta
pruning, extensive use of a supervised learning mode called    book learning,    and hierarchical lookup
tables called signature tables (gri   th, 1966) to represent the value function instead of linear function
approximation. this version learned to play much better than the 1959 program, though still not at a
master level. samuel   s checkers-playing program was widely recognized as a signi   cant achievement in
arti   cial intelligence and machine learning.

16.3 watson   s daily-double wagering

ibm watson1 is the system developed by a team of ibm researchers to play the popular tv quiz
show jeopardy!.2 it gained fame in 2011 by winning    rst prize in an exhibition match against human
champions. although the main technical achievement demonstrated by watson was its ability to
quickly and accurately answer natural language questions over broad areas of general knowledge, its

1registered trademark of ibm corp.
2registered trademark of jeopardy productions inc.

hypothetical eventsactual eventsbackup356

chapter 16. applications and case studies

winning jeopardy! performance also relied on sophisticated decision-making strategies for critical parts
of the game. tesauro, gondek, lechner, fan, and prager (2012, 2013) adapted tesauro   s td-gammon
system described above to create the strategy used by watson in    daily-double    (dd) wagering in its
celebrated winning performance against human champions. these authors report that the e   ectiveness
of this wagering strategy went well beyond what human players are able to do in live game play, and
that it, along with other advanced strategies, was an important contributor to watson   s impressive
winning performance. here we focus only on dd wagering because it is the component of watson
that owes the most to id23.

jeopardy! is played by three contestants who face a board showing 30 squares, each of which hides a
clue and has a dollar value. the squares are arranged in six columns, each corresponding to a di   erent
category. a contestant selects a square, the host reads the square   s clue, and each contestant may
choose to respond to the clue by sounding a buzzer (   buzzing in   ). the    rst contestant to buzz in gets
to try responding to the clue. if this contestant   s response is correct, their score increases by the dollar
value of the square; if their response is not correct, or if they do not respond within    ve seconds, their
score decreases by that amount, and the other contestants get a chance to buzz in to respond to the
same clue. one or two squares (depending on the game   s current round) are special dd squares. a
contestant who selects one of these gets an exclusive opportunity to respond to the square   s clue and has
to decide   before the clue is revealed   on how much to wager, or bet. the bet has to be greater than
   ve dollars but not greater than the contestant   s current score. if the contestant responds correctly to
the dd clue, their score increases by the bet amount; otherwise it decreases by the bet amount. at
the end of each game is a    final jeopardy    (fj) round in which each contestant writes down a sealed
bet and then writes an answer after the clue is read. the contestant with the highest score after three
rounds of play (where a round consists of revealing all 30 clues) is the winner. the game has many
other details, but these are enough to appreciate the importance of dd wagering. winning or losing
often depends on a contestant   s dd wagering strategy.

whenever watson selected a dd square, it chose its bet by comparing action values,   q(s, bet), that
estimated the id203 of a win from the current game state, s, for each round-dollar legal bet. except
for some risk-abatement measures described below, watson selected the bet with the maximum action
value. action values were computed whenever a betting decision was needed by using two types of
estimates that were learned before any live game play took place. the    rst were estimated values of the
afterstates (section 6.8) that would result from selecting each legal bet. these estimates were obtained
from a state-value function,   v(  ,w), de   ned by parameters w, that gave estimates of the id203
of a win for watson from any game state. the second estimates used to compute action values gave
the    in-category dd con   dence,    pdd, which estimated the likelihood that watson would respond
correctly to the as-yet unrevealed dd clue.

tesauro et al. used the id23 approach of td-gammon described above to learn
  v(  ,w): a straightforward combination of nonlinear td(  ) using a multilayer neural network with
weights w trained by backpropagating td errors during many simulated games. states were represented
to the network by feature vectors speci   cally designed for jeopardy!. features included the current
scores of the three players, how many dds remained, the total dollar value of the remaining clues,
and other information related to the amount of play left in the game. unlike td-gammon, which
learned by self-play, watson   s   v was learned over millions of simulated games against carefully-crafted
models of human players. in-category con   dence estimates were conditioned on the number of right
responses r and wrong responses w that watson gave in previously-played clues in the current category.
the dependencies on (r, w) were estimated from watson   s actual accuracies over many thousands of
historical categories.

with the previously learned value function   v and in-category dd con   dence pdd, watson computed

  q(s, bet) for each legal round-dollar bet as follows:

  q(s, bet) = pdd      v(sw + bet,. . .) + (1     pdd)      v(sw     bet,. . .),

(16.2)

16.3. watson   s daily-double wagering

357

where sw is watson   s current score, and   v gives the estimated value for the game state after watson   s
response to the dd clue, which is either correct or incorrect. computing an action value this way
corresponds to the insight from exercise 3.16 that an action value is the expected next state value given
the action (except that here it is the expected next afterstate value because the full next state of the
entire game depends on the next square selection).

tesauro et al. found that selecting bets by maximizing action values incurred    a frightening amount of
risk,    meaning that if watson   s response to the clue happened to be wrong, the loss could be disastrous
for its chances of winning. to decrease the downside risk of a wrong answer, tesauro et al. adjusted
(16.2) by subtracting a small fraction of the standard deviation over watson   s correct/incorrect af-
terstate evaluations. they further reduced risk by prohibiting bets that would cause the wrong-answer
afterstate value to decrease below a certain limit. these measures slightly reduced watson   s expecta-
tion of winning, but they signi   cantly reduced downside risk, not only in terms of average risk per dd
bet, but even more so in extreme-risk scenarios where a risk-neutral watson would bet most or all of
its bankroll.

why was the td-gammon method of self-play not used to learn the critical value function   v?
learning from self-play in jeopardy! would not have worked very well because watson was so di   erent
from any human contestant. self-play would have led to exploration of state space regions that are
not typical for play against human opponents, particularly human champions.
in addition, unlike
backgammon, jeopardy! is a game of imperfect information because contestants do not have access to
all the information in   uencing their opponents    play. in particular, jeopardy! contestants do not know
how much con   dence their opponents have for responding to clues in the various categories. self-play
would have been something like playing poker with someone who is holding the same cards that you
hold.

as a result of these complications, much of the e   ort in developing watson   s dd-wagering strategy
was devoted to creating good models of human opponents. the models did not address the natural
language aspect of the game, but were instead stochastic process models of events that can occur during
play. statistics were extracted from an extensive fan-created archive of game information from the
beginning of the show to the present day. the archive includes information such as the ordering of the
clues, right and wrong contestant answers, dd locations, and dd and fj bets for nearly 300,000 clues.
three models were constructed: an average contestant model (based on all the data), a champion
model (based on statistics from games with the 100 best players), and a grand champion model (based
on statistics from games with the 10 best players). in addition to serving as opponents during learning,
the models were used to asses the bene   ts produced by the learned dd-wagering strategy. watson   s
win rate in simulation when it used a baseline heuristic dd-wagering strategy was 61%; when it used the
learned values and a default con   dence value, its win rate increased to 64%; and with live in-category
con   dence, it was 67%. tesauro et al. regarded this as a signi   cant improvement, given that the dd
wagering was needed only about 1.5 to 2 times in each game.

because watson had only a few seconds to bet, as well as to select squares and decide whether or
not to buzz in, the computation time needed to make these decisions was a critical factor. the neural
network implementation of   v allowed dd bets to be made quickly enough to meet the time constraints of
live play. however, once games could be simulated fast enough through improvements in the simulation
software, near the end of a game it was feasible to estimate the value of bets by averaging over many
monte-carlo trials in which the consequence of each bet was determined by simulating play to the
game   s end. selecting endgame dd bets in live play based on monte-carlo trials instead of the neural
network signi   cantly improved watson   s performance because errors in value estimates in endgames
could seriously a   ect its chances of winning. making all the decisions via monte-carlo trials might have
led to better wagering decisions, but this was simply impossible given the complexity of the game and
the time constraints of live play.

although its ability to quickly and accurately answer natural language questions stands out as

358

chapter 16. applications and case studies

watson   s major achievement, all of its sophisticated decision strategies contributed to its impressive
defeat of human champions. according to tesauro et al. (2012):

... it is plainly evident that our strategy algorithms achieve a level of quantitative precision
and real-time performance that exceeds human capabilities. this is particularly true in
the cases of dd wagering and endgame buzzing, where humans simply cannot come close
to matching the precise equity and con   dence estimates and complex decision calculations
performed by watson.

16.4 optimizing memory control

most computers use dynamic random access memory (dram) as their main memory because of its
low cost and high capacity. the job of a dram memory controller is to e   ciently use the interface
between the processor chip and an o   -chip dram system to provide the high-bandwidth and low-
latency data transfer necessary for high-speed program execution. a memory controller needs to deal
with dynamically changing patterns of read/write requests while adhering to a large number of timing
and resource constraints required by the hardware. this is a formidable scheduling problem, especially
with modern processors with multiple cores sharing the same dram.

  ipek, mutlu, mart    nez, and caruana (2008) (also mart    nez and   ipek, 2009) designed a reinforcement
learning memory controller and demonstrated that it can signi   cantly improve the speed of program
execution over what was possible with conventional controllers at the time of their research. they
were motivated by limitations of existing state-of-the-art controllers that used policies that did not take
advantage of past scheduling experience and did not account for long-term consequences of scheduling
decisions.   ipek et al.   s project was carried out by means of simulation, but they designed the controller
at the detailed level of the hardware needed to implement it   including the learning algorithm   directly
on a processor chip.

accessing dram involves a number of steps that have to be done according to strict time constraints.
dram systems consist of multiple dram chips, each containing multiple rectangular arrays of storage
cells arranged in rows and columns. each cell stores a bit as the charge on a capacitor. since the
charge decreases over time, each dram cell needs to be recharged   refreshed   every few milliseconds
to prevent memory content from being lost. this need to refresh the cells is why dram is called
   dynamic.   

each cell array has a row bu   er that holds a row of bits that can be transferred into or out of one of
the array   s rows. an activate command    opens a row,    which means moving the contents of the row
whose address is indicated by the command into the row bu   er. with a row open, the controller can
issue read and write commands to the cell array. each read command transfers a word (a short sequence
of consecutive bits) in the row bu   er to the external data bus, and each write command transfers a
word in the external data bus to the row bu   er. before a di   erent row can be opened, a precharge
command must be issued which transfers the (possibly updated) data in the row bu   er back into the
addressed row of the cell array. after this, another activate command can open a new row to be accessed.
read and write commands are column commands because they sequentially transfer bits into or out of
columns of the row bu   er; multiple bits can be transferred without re-opening the row. read and write
commands to the currently-open row can be carried out more quickly than accessing a di   erent row,
which would involve additional row commands: precharge and activate; this is sometimes referred to as
   row locality.    a memory controller maintains a memory transaction queue that stores memory-access
requests from the processors sharing the memory system. the controller has to process requests by
issuing commands to the memory system while adhering to a large number of timing constraints.

a controller   s policy for scheduling access requests can have a large e   ect on the performance of the
memory system, such as the average latency with which requests can be satis   ed and the throughput

16.4. optimizing memory control

359

the system is capable of achieving. the simplest scheduling strategy handles access requests in the order
in which they arrive by issuing all the commands required by the request before beginning to service
the next one. but if the system is not ready for one of these commands, or executing a command
would result in resources being underutilized (e.g., due to timing constraints arising from servicing
that one command), it makes sense to begin servicing a newer request before    nishing the older one.
policies can gain e   ciency by reordering requests, for example, by giving priority to read requests over
write requests, or by giving priority to read/write commands to already open rows. the policy called
first-ready, first-come-first-serve (fr-fcfs), gives priority to column commands (read and write)
over row commands (activate and precharge), and in case of a tie gives priority to the oldest command.
fr-fcfs was shown to outperform other scheduling policies in terms of average memory-access latency
under conditions commonly encountered (rixner, 2004).

figure 16.4 is a high-level view of   ipek et al.   s id23 memory controller. they
modeled the dram access process as an mdp whose states are the contents of the transaction queue
and whose actions are commands to the dram system: precharge, activate, read, write, and noop.
the reward signal is 1 whenever the action is read or write, and otherwise it is 0. state transitions were
considered to be stochastic because the next state of the system not only depends on the scheduler   s
command, but also on aspects of the system   s behavior that the scheduler cannot control, such as the
workloads of the processor cores accessing the dram system.

critical to this mdp are constraints on the actions available in each state. recall from chapter 3
that the set of available actions can depend on the state: at     a(st), where at is the action at time
step t and a(st) is the set of actions available in state st.
in this application, the integrity of the
dram system was assured by not allowing actions that would violate timing or resource constraints.
although   ipek et al. did not make it explicit, they e   ectively accomplished this by pre-de   ning the sets
a(st) for all possible states st.

these constraints explain why the mdp has a noop action and why the reward signal is 0 except
when a read or write command is issued. noop is issued when it is the sole legal action in a state.
to maximize utilization of the memory system, the controller   s task is to drive the system to states in
which either a read or a write action can be selected: only these actions result in sending data over
the external data bus, so it is only these that contribute to the throughput of the system. although
precharge and activate produce no immediate reward, the agent needs to select these actions to make
it possible to later select the rewarded read and write actions.

the scheduling agent used sarsa (section 6.4) to learn an action-value function. states were rep-

figure 16.4: high-level view of the id23 dram controller. the scheduler is the reinforcement
learning agent. its environment is represented by features of the transaction queue, and its actions are commands
to the dram system. c(cid:13)2009 ieee. reprinted, with permission, from j. f. mart    nez and e.   ipek, dynamic
multicore resource management: a machine learning approach, micro, ieee, 29(5), p. 12.

360

chapter 16. applications and case studies

resented by six integer-valued features. to approximate the action-value function, the algorithm used
linear function approximation implemented by tile coding with hashing (section 9.5.4). the tile coding
had 32 tilings, each storing 256 action values as 16-bit    xed point numbers. exploration was  -greedy
with   = 0.05.

state features included the number of read requests in the transaction queue, the number of write
requests in the transaction queue, the number of write requests in the transaction queue waiting for
their row to be opened, and the number of read requests in the transaction queue waiting for their row
to be opened that are the oldest issued by their requesting processors. (the other features depended on
how the dram interacts with cache memory, details we omit here.) the selection of the state features
was based on   ipek et al.   s understanding of factors that impact dram performance. for example,
balancing the rate of servicing reads and writes based on how many of each are in the transaction
queue can help avoid stalling the dram system   s interaction with cache memory. the authors in fact
generated a relatively long list of potential features, and then pared them down to a handful using
simulations guided by stepwise feature selection.

an interesting aspect of this formulation of the scheduling problem as an mdp is that the features
input to the tile coding for de   ning the action-value function were di   erent from the features used to
specify the action-constraint sets a(st). whereas the tile coding input was derived from the contents
of the transaction queue, the constraint sets depended on a host of other features related to timing and
resource constraints that had to be satis   ed by the hardware implementation of the entire system. in
this way, the action constraints ensured that the learning algorithm   s exploration could not endanger
the integrity of the physical system, while learning was e   ectively limited to a    safe    region of the much
larger state space of the hardware implementation.

since an objective of this work was that the learning controller could be implemented on a chip so
that learning could occur on-line while a computer is running, hardware implementation details were
important considerations. the design included two    ve-stage pipelines to calculate and compare two
action values at every processor clock cycle, and to update the appropriate action value. this included
accessing the tile coding which was stored on-chip in static ram. for the con   guration   ipek et al.
simulated, which was a 4ghz 4-core chip typical of high-end workstations at the time of their research,
there were 10 processor cycles for every dram cycle. considering the cycles needed to    ll the pipes,
  ipek et al. found that the number of legal
up to 12 actions could be evaluated in each dram cycle.
commands for any state was rarely greater than this, and that performance loss was negligible if enough
time was not always available to consider all legal commands. these and other clever design details
made it feasible to implement the complete controller and learning algorithm on a multi-processor chip.
  ipek et al. evaluated their learning controller in simulation by comparing it with three other con-
trollers: 1) the fr-fcfs controller mentioned above that produces the best on-average performance,
2) a conventional controller that processes each request in order, and 3) an unrealizable ideal controller,
called the optimistic controller, able to sustain 100% dram throughput if given enough demand by
ignoring all timing and resource constraints, but otherwise modeling dram latency (as row bu   er hits)
and bandwidth. they simulated nine memory-intensive parallel workloads consisting of scienti   c and
data-mining applications. figure 16.5 shows the performance (the inverse of execution time normal-
ized to the performance of fr-fcfs) of each controller for the nine applications, together with the
geometric mean of their performances over the applications. the learning controller, labeled rl in the
   gure, improved over that of fr-fcfs by from 7% to 33% over the nine applications, with an average
improvement of 19%. of course, no realizable controller can match the performance of optimistic,
which ignores all timing and resource constraints, but the learning controller   s performance closed the
gap with optimistic   s upper bound by an impressive 27%.

because the rationale for on-chip implementation of the learning algorithm was to allow the schedul-
ing policy to adapt on-line to changing workloads,   ipek et al. analyzed the impact of on-line learning
compared to a previously-learned    xed policy. they trained their controller with data from all nine

16.5. human-level video game play

361

figure 16.5: performances of four controllers over a suite of 9 simulated benchmark applications. the controllers
are: the simplest    in-order    controller, fr-fcfs, the learning controller rl, and the unrealizable optimistic
controller which ignores all timing and resource constraints to provide a performance upper bound. perfor-
mance, normalized to that of fr-fcfs, is the inverse of execution time. at far right is the geometric mean of
performances over the 9 benchmark applications for each controller. controller rl comes closest to the ideal
performance. c(cid:13)2009 ieee. reprinted, with permission, from j. f. mart    nez and e.   ipek, dynamic multicore
resource management: a machine learning approach, micro, ieee, 29(5), p. 13.

benchmark applications and then held the resulting action values    xed throughout the simulated ex-
ecution of the applications. they found that the average performance of the controller that learned
on-line was 8% better than that of the controller using the    xed policy, leading them to conclude that
on-line learning is an important feature of their approach.

this learning memory controller was never committed to physical hardware because of the large
cost of fabrication. nevertheless,   ipek et al. could convincingly argue on the basis of their simulation
results that a memory controller that learns on-line via id23 has the potential to
improve performance to levels that would otherwise require more complex and more expensive memory
systems, while removing from human designers some of the burden required to manually design e   cient
scheduling policies. mukundan and mart    nez (2012) took this project forward by investigating learning
controllers with additional actions, other performance criteria, and more complex reward functions
derived using id107. they considered additional performance criteria related to energy
e   ciency. the results of these studies surpassed the earlier results described above and signi   cantly
surpassed the 2012 state-of-the-art for all of the performance criteria they considered. the approach is
especially promising for developing sophisticated power-aware dram interfaces.

16.5 human-level video game play

one of the greatest challenges in applying id23 to real-world problems is deciding
how to represent and store value functions and/or policies. unless the state set is    nite and small
enough to allow exhaustive representation by a lookup table   as in many of our illustrative examples   
one must use a parameterized function approximation scheme. whether linear or non-linear, function
approximation relies on features that have to be readily accessible to the learning system and able to
convey the information necessary for skilled performance. most successful applications of reinforcement
learning owe much to sets of features carefully handcrafted based on human knowledge and intuition
about the speci   c problem to be tackled.

a team of researchers at google deepmind developed an impressive demonstration that a deep multi-
layer arti   cial neural network (ann) can automate the feature design process (mnih et al., 2015, 2013).
multi-layer anns have been used for function approximation in id23 ever since the
1986 popularization of the id26 algorithm as a method for learning internal representations
(rumelhart, hinton, and williams, 1986; see section 9.6). striking results have been obtained by

362

chapter 16. applications and case studies

coupling id23 with id26. the results obtained by tesauro and colleages
with td-gammon and watson discussed above are notable examples. these and other applications
bene   ted from the ability of multi-layer anns to learn task-relevant features. however, in all the
examples of which we are aware, the most impressive demonstrations required the network   s input
to be represented in terms of specialized features handcrafted for the given problem. this is vividly
apparent in the td-gammon results. td-gammon 0.0, whose network input was essentially a    raw   
representation of he backgammon board, meaning that it involved very little knowledge of backgammon,
learned to play approximately as well as the best previous backgammon computer programs. adding
specialized backgammon features produced td-gammon 1.0 which was substantially better than all
previous backgammon programs and competed well against human experts.

mnih et al. developed a id23 agent called deep q-network (id25) that combined
id24 with a deep convolutional ann, a many-layered, or deep, ann specialized for processing
spatial arrays of data such as images. we describe deep convolutional anns in section 9.6. by the
time of mnih et al.   s work with id25, deep anns, including deep convolutional anns, had produced
impressive results in many applications, but they had not been widely used in id23.

mnih et al. used id25 to show how a single id23 agent can achieve high levels
of performance in many di   erent problems without relying on di   erent problem-speci   c feature sets.
to demonstrate this, they let id25 learn to play 49 di   erent atari 2600 video games by interacting
with a game emulator. for learning each game, id25 used the same raw input, the same network
architecture, and the same parameter values (e.g., step-size, discount rate, exploration parameters, and
many more speci   c to the implementation). id25 achieved levels of play at or beyond human level on
a large fraction of these games. although the games were alike in being played by watching streams
of video images, they varied widely in other respects. their actions had di   erent e   ects, they had
di   erent state-transition dynamics, and they needed di   erent policies for earning high scores. the
deep convolutional ann learned to transform the raw input common to all the games into features
specialized for representing the action values required for playing at the high level id25 achieved for
most of the games.

the atari 2600 is a home video game console that was sold in various versions by atari inc. from
1977 to 1992. it introduced or popularized many arcade video games that are now considered classics,
such as pong, breakout, space invaders, and asteroids. although much simpler than modern video
games, atari 2600 games are still entertaining and challenging for human players, and they have been
attractive as testbeds for developing and evaluating id23 methods (diuk, cohen,
littman, 2008; naddaf, 2010; cobo, zang, isbell, and thomaz, 2011; bellemare, veness, and bowling,
2013). bellemare, naddaf, veness, and bowling (2012) developed the publicly available arcade learning
environment (ale) to encourage and simplify using atari 2600 games to study learning and planning
algorithms.

these previous studies and the availability of ale made the atari 2600 game collection a good choice
for mnih et al.   s demonstration, which was also in   uenced by the impressive human-level performance
that td-gammon was able to achieve in backgammon. id25 is similar to td-gammon in using a
multi-layer ann as the function approximation method for a semi-gradient form of a td algorithm,
with the gradients computed by the id26 algorithm. however, instead of using td(  )
as td-gammon did, id25 used the semi-gradient form of id24. td-gammon estimated the
values of afterstates, which were easily obtained from the rules for making backgammon moves. to
use the same algorithm for the atari games would have required generating the next states for each
possible action (which would not have been afterstates in that case). this could have been done by
using the game emulator to run single-step simulations for all the possible actions (which ale makes
possible). or a model of each game   s state-transition function could have been learned and used to
predict next states (oh, guo, lee, lewis, and singh, 2015). while these methods might have produced
results comparable to id25   s, they would have been more complicated to implement and would have
signi   cantly increased the time needed for learning. another motivation for using id24 was that

16.5. human-level video game play

363

id25 used the experience replay method, described below, which requires an o   -policy algorithm. being
model-free and o   -policy made id24 a natural choice.

before describing the details of id25 and how the experiments were conducted, we look at the skill
levels id25 was able to achieve. mnih et al. compared the scores of id25 with the scores of the best
performing learning system in the literature at the time, the scores of a professional human games
tester, and the scores of an agent that selected actions at random. the best system from the literature
used linear function approximation with features hand designed using some knowledge about atari 2600
games (bellemare, naddaf, veness, and bowling, 2013). id25 learned on each game by interacting with
the game emulator for 50 million frames, which corresponds to about 38 days of experience with the
game. at the start of learning on each game, the weights of id25   s network were reset to random values.
to evaluate id25   s skill level after learning, its score was averaged over 30 sessions on each game, each
lasting up to 5 minutes and beginning with a random initial game state. the professional human tester
played using the same emulator (with the sound turned o    to remove any possible advantage over id25
which did not process audio). after 2 hours of practice, the human played about 20 episodes of each
game for up to 5 minutes each and was not allowed to take any break during this time. id25 learned
to play better than the best previous id23 systems on all but 6 of the games, and
played better than the human player on 22 of the games. by considering any performance that scored
at or above 75% of the human score to be comparable to, or better than, human-level play, mnih et al.
concluded that the levels of play id25 learned reached or exceeded human level on 29 of the 46 games.
see mnih et al. (2015) for a more detailed account of these results.

for an arti   cial learning system to achieve these levels of play would be impressive enough, but what
makes these results remarkable   and what many at the time considered to be breakthrough results
for arti   cial intelligence   is that the very same learning system achieved these levels of play on widely
varying games without relying on any game-speci   c modi   cations.

a human playing any of these 49 atari games sees 210   160 pixel image frames with 128 colors
at 60hz. in principle, exactly these images could have formed the raw input to id25, but to reduce
memory and processing requirements, mnih et al. preprocessed each frame to produce an 84  84 array
of luminance values. since the full states of many of the atari games are not completely observable
from the image frames, mnih et al.    stacked    the four most recent frames so that the inputs to the
network had dimension 84  84  4. this did not eliminate partial observability for all of the games, but
it was helpful in making many of them more markovian.

an essential point here is that these preprocessing steps were exactly the same for all 46 games. no
game-speci   c prior knowledge was involved beyond the general understanding that it should still be
possible to learn good policies with this reduced dimension and that stacking adjacent frames should
help with the partial observability of some of the games. since no game-speci   c prior knowledge beyond
this minimal amount was used in preprocessing the image frames, we can think of the 84  84  4 input
vectors as being    raw    input to id25.

the basic architecture of id25 is similar to the deep convolutional ann illustrated in figure 9.15
(though unlike that network, subsampling in id25 is treated as part of each convolutional layer, with
feature maps consisting of units having only a selection of the possible receptive    elds). id25 has three
hidden convolutional layers, followed by one fully connected hidden layer, followed by the output layer.
the three successive hidden convolutional layers of id25 produce 32 20   20 feature maps, 64 9   9
feature maps, and 64 7  7 feature maps. the activation function of the units of each feature map is a
recti   er nonlinearity (max(0, x)). the 3,136 (64  7  7) units in this third convolutional layer all connect
to each of 512 units in the fully connected hidden layer, which then each connect to all 18 units in the
output layer, one for each possible action in an atari game.

the activation levels of id25   s output units were the estimated optimal action values (optimal q-
values) of the corresponding state   action pairs, for the state represented by the network   s input. the
assignment of output units to a game   s actions varied from game to game, and since the number of

364

chapter 16. applications and case studies

valid actions varied between 4 and 18 for the games, not all output units had functional roles in all of
the games. it helps to think of the network as if it were 18 separate networks, one for estimating the
optimal action value of each possible action. in reality, these networks shared their initial layers, but
the output units learned to use the features extracted by these layers in di   erent ways.

id25   s reward signal indicated how a games   s score changed from one time step to the next: +1
whenever it increased,    1 whenever it decreased, and 0 otherwise. this standardized the reward signal
across the games and made a single step-size parameter work well for all the games despite their varying
ranges of scores. id25 used an  -greedy policy, with   decreasing linearly over the    rst million frames
and remaining at a low value for the rest of the learning session. the values of the various other
parameters, such as the learning step-size, discount rate, and others speci   c to the implementation,
were selected by performing informal searches to see which values worked best for a small selection of
the games. these values were then held    xed for all of the games.

after id25 selected an action, the action was executed by the game emulator, which returned a
reward and the next video frame. the frame was preprocessed and added to the four-frame stack that
became the next input to the network. skipping for the moment the changes to the basic id24
procedure made by mnih et al., id25 used the following semi-gradient form of id24 to update
the network   s weights:

(16.3)

wt+1 = wt +   (cid:104)rt+1 +    max

a

  q(st+1, a, wt)       q(st, at, wt)(cid:105)   wt   q(st, at, wt),

where wt is the vector of the network   s weights, at is the action selected at time step t, and st and
st+1 are respectively the preprocessed image stacks input to the network at time steps t and t + 1.

the gradient in (16.3) was computed by id26. imagining again that there was a separate
network for each action, for the update at time step t, id26 was applied only to the network
corresponding to at. mnih et al. took advantage of techniques shown to improve the basic backpropa-
gation algorithm when applied to large networks. they used a mini-batch method that updated weights
only after accumulating gradient information over a small batch of images (here after 32 images). this
yielded smoother sample gradients compared to the usual procedure that updates weights after each
action. they also used a gradient-ascent algorithm called rmsprop (tieleman and hinton, 2012) that
accelerates learning by adjusting the step-size parameter for each weight based on a running average of
the magnitudes of recent gradients for that weight.

mnih et al. modi   ed the basic id24 procedure in three ways. first, they used a method called
experience replay    rst studied by lin (1992). this method stores the agent   s experience at each time
step in a replay memory that is accessed to perform the weight updates. it worked like this in id25.
after the game emulator executed action at in a state represented by the image stack st, and returned
reward rt+1 and image stack st+1, it added the tuple (st, at, rt+1, st+1) to the replay memory. this
memory accumulated experiences over many plays of the same game. at each time step multiple q-
learning updates   a mini-batch   were performed based on experiences sampled uniformly at random
from the replay memory. instead of st+1 becoming the new st for the next update as it would in the
usual form of id24, a new unconnected experience was drawn from the replay memory to supply
data for the next update. since id24 is an o   -policy algorithm, it does not need to be applied
along connected trajectories.

id24 with experience replay provided several advantages over the usual form of id24.
the ability to use each stored experience for many updates allowed id25 to learn more e   ciently from
its experiences. experience replay reduced the variance of the updates because successive updates were
not correlated with one another as they would be with standard id24. and by removing the
dependence of successive experiences on the current weights, experience replay eliminated one source
of instability.

mnih et al. modi   ed standard id24 in a second way to improve its stability. as in other methods
that bootstrap, the target for a id24 update depends on the current action-value function estimate.

16.6. mastering the game of go

365

when a parameterized function approximation method is used to represent action values, the target
is a function of the same parameters that are being updated. for example, the target in the update
given by (16.3) is    maxa   q(st+1, a, wt). its dependence on wt complicates the process compared to
the simpler supervised-learning situation in which the targets do not depend on the parameters being
updated. as discussed in chapter 11 this can lead to oscillations and/or divergence.

to address this problem mnih et al. used a technique that brought id24 closer to the simpler
supervised-learning case while still allowing it to bootstrap. whenever a certain number, c, of updates
had been done to the weights w of the action-value network, they inserted the network   s current weights
into another network and held these duplicate weights    xed for the next c updates of w. the outputs
of this duplicate network over the next c updates of w were used as the id24 targets. letting   q
denote the output of this duplicate network, then instead of (16.3) the update rule was:

wt+1 = wt +   (cid:104)rt+1 +    max

a

  q(st+1, a, wt)       q(st, at, wt)(cid:105)   wt   q(st, at, wt).

a    nal modi   cation of standard id24 was also found to improve stability. they clipped the error
term rt+1 +    maxa   q(st+1, a, wt)       q(st, at, wt) so that it remained in the interval [   1, 1].

mnih et al. conducted a large number of learning runs on 5 of the games to gain insight into the
e   ect that various of id25   s design features had on its performance. they ran id25 with the four
combinations of experience replay and the duplicate target network being included or not included.
although the results varied from game to game, each of these features alone signi   cantly improved
performance, and very dramatically improved performance when used together. mnih et al. also studied
the role played by the deep convolutional ann in id25   s learning ability by comparing the deep
convolutional version of id25 with a version having a network of just one linear layer, both receiving
the same stacked preprocessed video frames. here, the improvement of the deep convolutional version
over the linear version was particularly striking across all 5 of the test games.

creating arti   cial agents that excel over a diverse collection of challenging tasks has been an enduring
goal of arti   cial intelligence. the promise of machine learning as a means for achieving this has been
frustrated by the need to craft problem-speci   c representations. deepmind   s id25 stands as a major
step forward by demonstrating that a single agent can learn problem-speci   c features enabling it to
acquire human-competitive skills over a range of tasks. but as mnih et al. point out, id25 is not a
complete solution to the problem of task-independent learning. although the skills needed to excel on
the atari games were markedly diverse, all the games were played by observing video images, which
made a deep convolutional ann a natural choice for this collection of tasks.
in addition, id25   s
performance on some of the atari 2600 games fell considerably short of human skill levels on these
games. the games most di   cult for id25   especially montezuma   s revenge on which id25 learned to
perform about as well as the random player   require deep planning beyond what id25 was designed to
do. further, learning control skills through extensive practice, like id25 learned how to play the atari
games, is just one of the types of learning humans routinely accomplish. despite these limitations,
id25 advanced the state-of-the-art in machine learning by impressively demonstrating the promise of
combining id23 with modern methods of deep learning.

16.6 mastering the game of go

the ancient chinese game of go has challenged arti   cial intelligence researchers for many decades.
methods that achieve human-level skill, or even superhuman-level skill, in other games have not been
successful in producing strong go programs. thanks to a very active community of go programmers
and international competitions, the level of go program play has improved signi   cantly over the years.
until recently, however, no go program had been able to play anywhere near the level of a human go
master.

366

chapter 16. applications and case studies

a team at deepmind (silver et al., 2016) developed the program alphago that broke this barrier
by combining deep arti   cial neural networks (deep anns, section 9.6), supervised learning, monte
carlo tree search (mcts, section 8.11), and id23. by the time of silver et al.   s
2016 publication, alphago had been shown to be decisively stronger than other current go programs,
and it had defeated the european go champion fan hui 5 games to 0. these were the    rst victories
of a go program over a professional human go player without handicap in full go games. shortly
thereafter, a similar version of alphago won stunning victories over the 18-time world champion lee
sedol, winning 4 out of a 5 games in a challenge match, making worldwide headline news. arti   cial
intelligence researchers thought that it would be many more years, perhaps decades, before a program
reached this level of play.

here we describe alphago and a successor program called alphago zero (silver et al. 2017). where
in addition to id23, alphago relied on supervised learning from a large database of
expert human moves, alphago zero used only id23 and no human data or guidance
beyond the basic rules of the game (hence the zero in its name). we    rst describe alphago in some
detail in order to highlight the relatively simplicity of alphago zero, which is both higher-performing
and more of a pure id23 program.

in many ways, both alphago and alphago zero are descendants of tesauo   s td-gammon (sec-
tion 16.1), itself a descendant of samuel   s checkers player (section 16.2). all these programs included
id23 over simulated games of self-play. alphago and alphago zero also built upon
the progress made by deepmind on playing atari games with the program id25 (section 16.5) that
used deep convolutional anns to approximate optimal value functions.

go is a game between two players who alternately
place black and white    stones    on unoccupied intersec-
tions, or    points,    on a board with a grid of 19 hor-
izontal and 19 vertical lines to produce positions like
that shown to the right. the game   s goal is to cap-
ture an area of the board larger than that captured by
the opponent. stones are captured according to simple
rules. a player   s stones are captured if they are com-
pletely surrounded by the other player   s stones, meaning
that there is no horizontally or vertically adjacent point
that is unoccupied. for example, figure 16.6 shows on
the left three white stones with an unoccupied adjacent
point (labeled x). if player black places a stone on x,
the three white stones are captured and taken o    the
board (figure 16.6 middle). however, if player white
were to place a stone on point x    rst, than the possibil-
ity of this capture would be blocked (figure 16.6 right).
other rules are needed to prevent in   nite capturing/re-
capturing loops. the game ends when neither player wishes to place another stone. these rules are
simple, but they produce a very complex game that has had wide appeal for thousands of years.

a go board con   guration

figure 16.6: go capturing rule. left: the three white stones are not surrounded because point x is unoccupied.
middle: if black places a stone on x, the three white stones are captured and removed from the board. right:
if white places a stone on point x    rst, the capture is blocked.

x16.6. mastering the game of go

367

methods that produce strong play for other games, such as chess, have not worked as well for go.
the search space for go is signi   cantly larger than that of chess because go has a larger number of
legal moves per position than chess (    250 versus     35) and go games tend to involve more moves
than chess games (    150 versus     80). but the size of the search space is not the major factor that
makes go so di   cult. exhaustive search is infeasible for both chess and go, and go on smaller boards,
e.g., 9    9, has proven to be exceedingly di   cult as well. experts agree that the major stumbling
block to creating stronger-than-amateur go programs is the di   culty of de   ning an adequate position
evaluation function. a good evaluation function allows search to be truncated at a feasible depth by
providing relatively easy-to-compute predictions of what deeper search would likely yield. according
to m  uller (2002):    no simple yet reasonable evaluation function will ever be found for go.    a major
step forward was the introduction of mcts to go programs. the strongest programs at the time of
alphago   s development all included mcts, but master-level skill remained elusive.

recall from section 8.11 that mcts is a decision-time planning procedure that does not attempt
to learn and store a global evaluation function. like a rollout algorithm (section 8.10), it runs many
monte carlo simulations of entire episodes (here, entire go games) to select each action (here, each
go move: where to place a stone or to resign). unlike a simple rollout algorithm, however, mcts is
an iterative procedure that incrementally extends a search tree whose root node represents the current
environment state. as illustrated in figure 8.11, each iteration traverses the tree by simulating actions
guided by statistics associated with the tree   s edges. in its basic version, when a simulation reaches a
leaf node of the search tree, mcts expands the tree by adding some, or all, of the leaf node   s children
to the tree. from the leaf node, or one of its newly added child notes, a rollout is executed: a simulation
that typically proceeds all the way to a terminal state, with actions selected by a rollout policy. when
the rollout completes, the statistics associated with the search tree   s edges that were traversed in this
iteration are updated by backing up the return produced by the rollout. mcts continues this process,
starting each time at the search tree   s root at the current state, for as many iterations as possible given
the time constraints. then,    nally, an action from the root node (which still represents the current
environment state) is selected according to statistics accumulated in the root node   s outgoing edges.
this is the action the agent takes. after the environment transitions to its next state, mcts is executed
again with the root node set to represent the new current state. the search tree at the start of this
next execution might be just this new root node, or it might include descendants of this node left over
from mcts   s previous execution. the remainder of the tree is discarded.

16.6.1 alphago

the main innovation that made alphago such a strong player is that it selected moves by a novel version
of mcts that was guided by both a policy and a value function learned by id23 with
function approximation provided by deep convolutional anns. another key feature is that instead of
id23 starting from random network weights, it started from weights that were the
result of previous supervised learning from a large collection of human expert moves.

the deepmind team called alphago   s modi   cation of basic mcts    asynchronous policy and value
mcts,    or apv-mcts. it selected actions via basic mcts as described above but with some twists
in how it extended its search tree and how it evaluated action edges.
in contrast to basic mcts,
which expands its current search tree by using stored action values to select an unexplored edge from a
leaf node, apv-mcts, as implemented in alphago, expanded its tree by choosing an edge according
to probabilities supplied by a 13-layer deep convolutional ann, called the sl-policy network, trained
previously by supervised learning to predict moves contained in a database of nearly 30 million human
expert moves.

then, also in contrast to basic mcts, which evaluates the newly-added state node solely by the
return of a rollout initiated from it, apv-mcts evaluated the node in two ways: by this return of the
rollout, but also by a value function, v  , learned previously by a id23 method. if s

368

chapter 16. applications and case studies

was the newly-added node, its value became

v(s) = (1       )v  (s) +   g,

(16.4)

where g was the return of the rollout and    controlled the mixing of the values resulting from these two
evaluation methods. in alphago, these values were supplied by the value network, another 13-layer deep
convolutional ann that was trained as we describe below to output estimated values of board positions.
apv-mcts   s rollouts in alphago were simulated games with both players using a fast rollout policy
provided by a simple linear network, also trained by supervised learning before play. throughout its
execution, apv-mcts kept track of how many simulations passed through each edge of the search
tree, and when its execution completed, the most-visited edge from the root node was selected as the
action to take, here the move alphago actually made in a game.

the value network had the same structure as the deep convolutional sl policy network except that
it had a single output unit that gave estimated values of game positions instead of the sl policy
network   s id203 distributions over legal actions. ideally, the value network would output optimal
state values, and it might have been possible to approximate the optimal value function along the lines
of td-gammon described above: self-play with nonlinear td(  ) coupled to a deep convolutional ann.
but the deepmind team took a di   erent approach that held more promise for a game as complex as
go. they divided the process of training the value network into two stages. in the    rst stage, they
created the best policy they could by using id23 to train an rl policy network. this
was a deep convolutional ann with the same structure as the sl policy network. it was initialized
with the    nal weights of the sl policy network that were learned via supervised learning, and then
policy-gradient id23 was used to improve upon the sl policy. in the second stage of
training the value network, the team used monte carlo policy evaluation on data obtained from a large
number of simulated self-play games with moves selected by the rl policy network.

figure 16.7 illustrates the networks used by alphago and the steps taken to train them in what the
deepmind team called the    alphago pipeline.    all these networks were trained before any live game
play took place, and their weights remained    xed throughout live play.

here is some more detail about alphago   s anns and their training. the identically-structured sl
and rl policy networks were similar to id25   s deep convolutional network described in section 16.5

figure 16.7: alphago pipeline. adapted with permission from macmillan publishers ltd: nature, vol.
529(7587), p. 485, copyright (2016).

rollout policy         sl policy network                    rl policy network          value networkpolicy gradientsupervised learningmc policy evaluationself playsupervised learning]]networksdata16.6. mastering the game of go

369

for playing atari games, except that they had 13 convolutional layers with the    nal layer consisting
of a softmax unit for each point on the 19    19 go board. the networks    input was a 19    19   
48 image stack in which each point on the go board was represented by the values of 48 binary or
integer-valued features. for example, for each point, one feature indicated if the point was occupied
by one of alphago   s stones, one of its opponent   s stones, or was unoccupied, thus providing the    raw   
representation of the board con   guration. other features were based on the rules of go, such as the
number of adjacent points that were empty, the number of opponent stones that would be captured by
placing a stone there, the number of turns since a stone was placed there, and other features that the
design team considered to be important.

training the sl policy network took approximately 3 weeks using a distributed implementation
of stochastic gradient ascent on 50 processors. the network achieved 57% accuracy, where the best
accuracy achieved by other groups at the time of publication was 44.4%. training the rl policy
network was done by policy gradient id23 over simulated games between the rl
policy network   s current policy and opponents using policies randomly selected from policies produced by
earlier iterations of the learning algorithm. playing against a randomly selected collection of opponents
prevented over   tting to the current policy. the reward signal was +1 if the current policy won,    1 if
it lost, and zero otherwise. these games directly pitted the two policies against one another without
involving mcts. by simulating many games in parallel on 50 processors, the deepmind team trained
the rl policy network on a million games in a single day. in testing the    nal rl policy, they found
that it won more than 80% of games played against the sl policy, and it won 85% of games played
against a go program using mcts that simulated 100,000 games per move.

the value network, whose structure was similar to that of the sl and rl policy networks except for
its single output unit, received the same input as the sl and rl policy networks with the exception that
there was an additional binary feature giving the current color to play. monte carlo policy evaluation
was used to train the network from data obtained from a large number of self-play games played using
the rl policy. to avoid over   tting and instability due to the strong correlations between positions
encountered in self-play, the deepmind team constructed a data set of 30 million positions each chosen
randomly from a unique self-play game. then training was done using 50 million mini-batches each of
32 positions drawn from this data set. training took one week on 50 gpus.

the rollout policy was learned learned prior to play by a simple linear network trained by supervised
learning from a corpus of 8 million human moves. the rollout policy network had to output actions
quickly while still being reasonably accurate. in principle, the sl or rl policy networks could have
been used in the rollouts, but the forward propagation through these deep networks took too much time
for either of them to be used in rollout simulations, a great many of which had to be carried out for
each move decision during live play. for this reason, the rollout policy network was less complex than
the other policy networks, and its input features could be computed more quickly than the features
used for the policy networks. the rollout policy network allowed approximately 1,000 complete game
simulations per second to be run on each of the processing threads that alphago used.

one may wonder why the sl policy was used instead of the better rl policy to select actions in
the expansion phase of apv-mcts. these policies took the same amount of time to compute since
they used the same network architecture. the team actually found that alphago played better against
human opponents when apv-mcts used as the sl policy instead of the rl policy. they conjectured
that the reason for this was that the latter was tuned to respond to optimal moves rather than to the
broader set of moves characteristic of human play. interestingly, the situation was reversed for the value
function used by apv-mcts. they found that when apv-mcts used the value function derived from
the rl policy, it performed better than if it used the value function derived from the sl policy.

several methods worked together to produce alphago   s impressive playing skill. the deepmind
team evaluated di   erent versions of alphago in order to asses the contributions made by these various
components. the parameter    in (16.4) controlled the mixing of game state evaluations produced by

370

chapter 16. applications and case studies

the value network and by rollouts. with    = 0, alphago used just the value network without rollouts,
and with    = 1, evaluation relied just on rollouts. they found that alphago using just the value
network played better than the rollout-only alphago, and in fact played better than the strongest of
all other go programs existing at the time. the best play resulted from setting    = 0.5, indicating
that combining the value network with rollouts was particularly important to alphago   s success. these
evaluation methods complemented one another: the value network evaluated the high-performance rl
policy that was too slow to be used in live play, while rollouts using the weaker but much faster rollout
policy were able to add precision to the value network   s evaluations for speci   c states that occurred
during games.

overall, alphago   s remarkable success fueled a new round of enthusiasm for the promise of arti   cial
intelligence, speci   cally for systems combining id23 with deep anns, to address
problems in other challenging domains.

16.6.2 alphago zero

building upon the experience with alphago, a deepmind team developed alphago zero (silver et al.
2017). in contrast to alphago, this program used no human data or guidance beyond the basic rules
of the game (hence the zero in its name). it learned exclusively from self-play id23,
with input giving just    raw    descriptions of the placements of stones on the go board. alphago
zero implemented a form of policy iteration (section 4.3), interleaving policy evaluation with policy
improvement. figure 16.8 is an overview of alphago zero   s algorithm. a signi   cant di   erence between
alphago zero and alphago is that alphago zero used mcts to select moves throughout self-play
id23, whereas alphago used mcts for live play after   but not during   learning.
other di   erences besides not using any human data or human-crafted features are that alphago zero
used only one deep convolutional ann and used a simpler version of mcts.

alphago zero   s mcts was simpler than the version used by alphago in that it did not include
rollouts of complete games, and therefore did not need a rollout policy. each iteration of alphago
zero   s mcts ran a simulation that ended at a leaf node of the current search tree instead of at the
terminal position of a complete game simulation. but as in alphago, each iteration of mcts in
alphago zero was guided by the output of a deep convolutional network, labeled f   in figure 16.7,
were    is the network   s weight vector. the input to the network, whose architecture we describe below,
consisted of raw representations of board positions, and its output had two parts: a scaler value, v, an
estimate of the id203 that the current player will win from from the current board position, and
a vector, p, of move probabilities, one for each possible stone placement on the current board, plus the
pass, or resign, move.

instead of selecting self-play actions according to the probabilities p, however, alphago zero used
these probabilities, together with the network   s value output, to direct each execution of mcts, which
returned new move probabilities, shown in figure 16.7 as the policies   i. these policies bene   tted from
the many simulations that mcts conducted each time it executed. the result was that the policy
actually followed by alphago zero was an improvement over the policy given by the network   s outputs
p. silver et al. (2017) wrote that    mcts may therefore be viewed as a powerful policy improvement
operator.   

here is more detail about alphago zero   s ann and how it was trained. the network took as input
a 19    19    17 image stack consisting of 17 binary feature planes. the    rst 8 feature planes were
raw representations of the positions of the current player   s stones in the current and seven past board
con   gurations: a feature value was 1 if a player   s stone was on the corresponding point, and was 0
otherwise. the next 8 feature planes similarly coded the positions of the opponent   s stones. a    nal
input feature plane had a constant value indicating the color of the current play: 1 for black; 0 for
white. because repetition is not allowed in go and one player is given some number of    compensation

16.6. mastering the game of go

371

figure 16.8: alphago zero self-play id23. a) the program played many games against itself,
one shown here as a sequence of board positions si, i = 1, 2, . . . , t , with moves ai, i = 1, 2, . . . , t , and winner
z. each move ai was determined by action probabilities   i returned by mcts executed from root node si and
guided by a deep convolutional network, here labeled f  , with latest weights   . shown here for just one position
s but repeated for all si, the network   s inputs were raw representations of board positions si (together with
several past position, though not shown here), and its outputs were vectors p of move probabilities that guided
mcts   s forward searches, and scalar values v that estimated the id203 of the current player winning from
each position si. b) deep convolutional network training. training examples were randomly sampled steps from
recent self-play games. weights    were updated to move the policy vector p toward the probabilities    returned
by mcts, and to include the winners z in the estimated win id203 v. reprinted from draft of silver et
al. (2017) with permission of the authors and deepmind.

points    for not getting the    rst move, the current board position is not a markov state of go. this is
why features describing past board positions and the color feature were needed.

the network was    two-headed,    meaning that after a number of initial layers, the network split into
in this
two separate    heads    of additional layers that separately fed into two sets of output units.
case, one head fed 362 output units producing 192 + 1 move probabilities p, one for each possible stone
placement plus pass; the other head fed just one output unit producing the scalar v, an estimate of
the id203 that the current player will win from the current board position. the network before
the split consisted of 41 convolutional layers, each followed by batch id172, and with skip
connections added to implement residual learning by pairs of layers (see section 9.6). overall, move
probabilities and values were computed by 43 and 44 layers respectively.

starting with random weights, the network was trained by stochastic id119 (with momen-
tum, id173, and step-size parameter decreasing as training continues) using batches of examples
sampled uniformly at random from all the steps of the most recent 500,000 games of self-play with the
current best policy. extra noise was added to the network   s output p to encourage exploration of all
possible moves. at periodic checkpoints during training, which silver et al. (2017) chose to be at every

figure1:self-playreinforcementlearninginalphagozero.atheprogramplaysagames1,...,stagainstitself.ineachpositionst,amonte-carlotreesearch(mcts)isexecuted(seefigure2)usingthelatestneuralnetworkf   .movesareselectedaccordingtothesearchprobabil-itiescomputedbythemcts,at            t.theterminalpositionstisscoredtocomputethegamewinnerz.bneuralnetworktraininginalphagozero.theneuralnetworktakestherawboardpositionsasitsinput,passesitthroughmanyconvolutionallayerswithparameters   ,andoutputsbothavectorp,representingaid203distributionovermoves,andascalarvaluev,represent-ingtheid203ofthecurrentplayerwinninginpositions.theneuralnetworkistrainedonrandomlysampledstepsfromrecentgamesofself-play,(s,         ,z).theparameters   areupdatedsoastomaximisethesimilarityofthepolicyvectorptothesearchprobabilities         ,andtominimisetheerrorbetweenthepredictedwinnervandthegamewinnerz(seeequation1).4372

chapter 16. applications and case studies

1,000 training steps, the policy output by the ann with the latest weights was evaluated by simulating
400 games (using mcts with 1,600 iterations to select each move) against the current best policy. if
the new policy won (by a margin set to reduce noise in the outcome), then it became the best policy
to be used in subsequent self-play. the network   s weights were updated to make the network   s policy
output p more closely match the policy returned by mcts, and to make its value output, v, more
closely match the id203 that the current best policy wins from the board position represented by
the network   s input.

the deepmind team trained alphago zero over 4.9 million games of self-play, which took about 3
days. each move of each game was selected by running mcts for 1,600 iterations, taking approximately
0.4 second per move. network weights were updated over 700,000 batches each consisting of 2,048 board
con   gurations. they then ran tournaments with the trained alphago zero playing against the version
of alphago that defeated fan hui by 5 games to 0, and against the version that defeated lee sedol by
4 games to 1. they used the elo rating system to evaluate the relative performances of the programs.
the di   erence between two elo ratings is meant to predict the outcome of games between the players.
the elo ratings of alphago zero, the version of alphago that played against fan hui, and the version
that played against lee sedol were respectively 4,308, 3,144, and 3,739. the gaps in these elo ratings
translate into predictions that alphago zero would defeat these other programs with probabilities very
close to one.
in a match of 100 games between alphago zero, trained as described, and the exact
version of alphago that defeated lee sedol held under the same conditions that were used in that
match, alphago zero defeated alphago in all 100 games.

the deepmind team also compared alphago zero with a program using an ann with the same
architecture but trained by supervised learning to predict human moves in a data set containing nearly
30 million positions from 160,000 games. they found that the supervised-learning player initially played
better than alphago zero, and was better at predicting human expert moves, but played less well after
alphago zero was trained for a day. this suggested that alphago zero had discovered a strategy for
playing that was di   erent from how humans play. in fact, alphago zero discovered, and came to prefer,
some novel variations of classical move sequences.

final tests of alphago zero   s algorithm were conducted with a version having a larger ann and
trained over 29 million self-play games, which took about 40 days, again starting with random weights.
this version achieved an elo rating of 5,185. the team pitted this version of alphago zero against
a program called alphago master , the strongest program at the time, that was identical to alphago
zero but, like alphago, used human data and features. alphago master    s elo rating was 4,858, and
it had defeated the strongest human professional players 60 to 0 in online games.
in a 100 game
match, alphago zero with the larger network and more extensive learning defeated alphago master
89 games to 11, thus providing a convincing demonstration of the problem-solving power of alphago
zero   s algorithm.

alphago zero soundly demonstrated that superhuman performance can be achieved by pure re-
inforcement learning, augmented by a simple version of mcts, and deep anns with very minimal
knowledge of the domain and no reliance on human data or guidance. we will surely see systems in-
spired by the deepmind accomplishments of both alphago and alphago zero applied to challenging
problems in other domains.

16.7 personalized web services

personalizing web services such as the delivery of news articles or advertisements is one approach to
increasing users    satisfaction with a website or to increase the yield of a marketing campaign. a policy
can recommend content considered to be the best for each particular user based on a pro   le of that
user   s interests and preferences inferred from their history of online activity. this is a natural domain
for machine learning, and in particular, for id23. a id23 system

16.7. personalized web services

373

can improve a recommendation policy by making adjustments in response to user feedback. one way
to obtain user feedback is by means of website satisfaction surveys, but for acquiring feedback in real
time it is common to monitor user clicks as indicators of interest in a link.

a method long used in marketing called a/b testing is a simple type of id23 used to
decide which of two versions, a or b, of a website users prefer. because it is non-associative, like a two-
armed bandit problem, this approach does not personalize content delivery. adding context consisting
of features describing individual users and the content to be delivered allows personalizing service. this
has been formalized as a contextual bandit problem (or an associative id23 problem,
section 2.9) with the objective of maximizing the total number of user clicks. li, chu, langford, and
schapire (2010) applied a contextual bandit algorithm to the problem of personalizing the yahoo! front
page today webpage (one of the most visited pages on the internet at the time of their research) by
selecting the news story to feature. their objective was to maximize the click-through rate (ctr),
which is the ratio of the total number of clicks all users make on a webpage to the total number of
visits to the page. their contextual bandit algorithm improved over a standard non-associative bandit
algorithm by 12.5%.

theocharous, thomas, and ghavamzadeh (2015) argued that better results are possible by for-
mulating personalized recommendation as a markov decision problem (mdp) with the objective of
maximizing the total number of clicks users make over repeated visits to a website. policies derived
from the contextual bandit formulation are greedy in the sense that they do not take long-term e   ects
of actions into account. these policies e   ectively treat each visit to a website as if it were made by a
new visitor uniformly sampled from the population of the website   s visitors. by not using the fact that
many users repeatedly visit the same websites, greedy policies do not take advantage of possibilities
provided by long-term interactions with individual users.

as an example of how a marketing strategy might take advantage of long-term user interaction,
theocharous et al. contrasted a greedy policy with a longer-term policy for displaying ads for buying
a product, say a car. the ad displayed by the greedy policy might o   er a discount if the user buys the
car immediately. a user either takes the o   er or leaves the website, and if they ever return to the site,
they would likely see the same o   er. a longer-term policy, on the other hand, can transition the user
   down a sales funnel    before presenting the    nal deal. it might start by describing the availability of
favorable    nancing terms, then praise an excellent service department, and then, on the next visit, o   er
the    nal discount. this type of policy can result in more clicks by a user over repeated visits to the
site, and if the policy is suitably designed, more eventual sales.

working at adobe systems incorporated, theocharous et al. conducted experiments to see if policies
designed to maximize clicks over the long term could in fact improve over short-term greedy policies.
the adobe marketing cloud, a set of tools that many companies use to to run digital marketing
campaigns, provides infrastructure for automating user-targed advertising and fund-raising campaigns.
actually deploying novel policies using these tools entails signi   cant risk because a new policy may end
up performing poorly. for this reason, the research team needed to assess what a policy   s performance
would be if it were to be actually deployed, but to do so on the basis of data collected under the
execution of other policies. a critical aspect of this research, then, was o   -policy evaluation. further,
the team wanted to do this with high con   dence to reduce the risk of deploying a new policy. although
high con   dence o   -policy evaluation was a central component of this research (see also thomas, 2015;
thomas, theocharous, and ghavamzadeh, 2015), here we focus only on the algorithms and their results.

theocharous et al. compared the results of two algorithms for learning ad recommendation policies.
the    rst algorithm, which they called greedy optimization, had the goal of maximizing only the proba-
bility of immediate clicks. as in the standard contextual bandit formulation, this algorithm did not take
the long-term e   ects of recommendations into account. the other algorithm, a id23
algorithm based on an mdp formulation, aimed at improving the number of clicks users made over
multiple visits to a website. they called this latter algorithm life-time value (ltv) optimization. both

374

chapter 16. applications and case studies

algorithms faced challenging problems because the reward signal in this domain is very sparse since
users usually do not click on ads, and user clicking is very random so that returns have high variance.

data sets from the banking industry were used for training and testing these algorithms. the data
sets consisted of many complete trajectories of customer interaction with a bank   s website that showed
each customer one out of a collection of possible o   ers.
if a customer clicked, the reward was 1,
and otherwise it was 0. one data set contained approximately 200,000 interactions from a month of a
bank   s campaign that randomly o   ered one of 7 o   ers. the other data set from another bank   s campaign
contained 4,000,000 interactions involving 12 possible o   ers. all interactions included customer features
such as the time since the customer   s last visit to the website, the number of their visits so far, the
last time the customer clicked, geographic location, one of a collection of interests, and features giving
demographic information.

greedy optimization was based on a mapping estimating the id203 of a click as a function of
user features. the mapping was learned via supervised learning from one of the data sets by means
of a id79 (rf) algorithm (breiman, 2001). rf algorithms have been widely used for large-
scale applications in industry because they are e   ective predictive tools that tend not to over   t and
are relatively insensitive to outliers and noise. theocharous et al. then used the mapping to de   ne
an  -greedy policy that selected with id203 1-  the o   er predicted by the rf algorithm to have
the highest id203 of producing a click, and otherwise selected from the other o   ers uniformly at
random.

ltv optimization used a batch-mode id23 algorithm called    tted q iteration (fqi).
it is a variant of    tted value iteration (gordon, 1999) adapted to id24. batch mode means
that the entire data set for learning is available from the start, as opposed to the on-line mode of
the algorithms we focus on in this book in which data are acquired sequentially while the learning
algorithm executes. batch-mode id23 algorithms are sometimes necessary when on-
line learning is not practical, and they can use any batch-mode supervised learning regression algorithm,
including algorithms known to scale well to high-dimensional spaces. the convergence of fqi depends
on properties of the function approximation algorithm (gordon, 1999). for their application to ltv
optimization, theocharous et al. used the same rf algorithm they used for the greedy optimization
approach. since in this case fqi convergence is not monotonic, theocharous et al. kept track of the
best fqi policy by o   -policy evaluation using a validation training set. the    nal policy for testing
the ltv approach was the  -greedy policy based on the best policy produced by fqi with the initial
action-value function set to the mapping produced by the rf for the greedy optimization approach.

to measure the performance of the policies produced by the greedy and ltv approaches, theocharous
et al. used the ctr metric and a metric they called the ltv metric. these metrics are similar, except
that the ltv metric critically distinguishes between individual website visitors:

ctr =

total # of clicks
total # of visits

,

ltv =

total # of clicks
total # of visitors

.

figure 16.9 illustrates how these metrics di   er. each circle represents a user visit to the site; black
circles are visits at which the user clicks. each row represents visits by a particular user. by not
distinguishing between visitors, the ctr for these sequences is 0.35, whereas the ltv is 1.5. because
ltv is larger than ctr to the extent that individual users revisit the site, it is an indicator of how
successful a policy is in encouraging users to engage in extended interactions with the site.

testing the policies produced by the greedy and ltv approaches was done using a high con   dence
o   -policy evaluation method on a test data set consisting of real-world interactions with a bank website
served by a random policy. as expected, results showed that greedy optimization performed best as
measured by the ctr metric, while ltv optimization performed best as measured by the ltv metric.

16.8. thermal soaring

375

figure 16.9: click through rate (ctr) versus life-time value (ltv). each circle represents a user visit; black
circles are visits at which the user clicks. adapted from theocharous et al. (2015).

furthermore   although we have omitted its details   the high con   dence o   -policy evaluation method
provided probabilistic guarantees that the ltv optimization method would, with high id203, pro-
duce policies that improve upon policies currently deployed. assured by these probabilistic guarantees,
adobe announced in 2016 that the new ltv algorithm would be a standard component of the adobe
marketing cloud so that a retailer could issue a sequence of o   ers following a policy likely to yield
higher return than a policy that is insensitive to long-term results.

16.8 thermal soaring

birds and gliders take advantage of upward air currents   thermals   to gain altitude in order to maintain
   ight while expending little, or no, energy. thermal soaring, as this behavior is called, is a complex skill
requiring responding to subtle environmental cues to increase altitude by exploiting a rising column of
air for as long as possible. reddy, celani, sejnowski, and vergassola (2016) used id23
to investigate thermal soaring policies that are e   ective in the strong atmospheric turbulence usually
accompanying rising air currents. their primary goal was to provide insight into the cues birds sense
and how they use them to achieve their impressive thermal soaring performance, but the results also
contribute to technology relevant to autonomous gliders. id23 had previously been
applied to the problem of navigating e   ciently to the vicinity of a thermal updraft (woodbury, dunn,
and valasek, 2014) but not to the more challenging problem of soaring within the turbulence of the
updraft itself.

reddy et al. modeled the soaring problem as an mdp. the agent interacted with a detailed model
of a glider    ying in turbulent air. they devoted signi   cant e   ort toward making the model generate
realistic thermal soaring conditions, including investigating several di   erent approaches to atmospheric
modeling. for the learning experiments, air    ow in a three-dimensional box with one kilometer sides,
one of which was at ground level, was modeled by a sophisticated physics-based set of partial di   erential
equations involving air velocity, temperature, and pressure. introducing small random perturbations
into the numerical simulation caused the model to produce analogs of thermal updrafts and accom-
panying turbulence (figure 16.10 left) glider    ight was modeled by aerodynamic equations involving
velocity, lift, drag, and other factors governing powerless    ight of a    xed-wing aircraft. maneuvering
the glider involved changing its angle of attack (the angle between the glider   s wing and the direction
of air    ow) and its bank angle (figure 16.10 right).

the interface between the agent and the environment required de   ning the agent   s actions, the state
information the agent receives from the environment, and the reward signal. by experimenting with
various possibilities, reddy et al. decided that three actions each for the angle of attack and the bank
angle were enough for their purposes:
increment or decrement the current bank angle and angle of
attack by 5    and 2.5   , respectively, or leave them unchanged. this resulted in 32 possible actions. the

376

chapter 16. applications and case studies

figure 16.10: thermal soaring model: left: snapshot of the vertical velocity    eld of the simulated cube of air:
in light (dark) grey is a region of large upward (downward)    ow. right: diagram of powerless    ight showing
bank angle    and angle of attack   . adapted with permission from pnas vol. 113(22), p. e4879, 2016, reddy,
celani, sejnowski, and vergassola, learning to soar in turbulent environments.

bank angle was bounded to remain between    15    and +15   .

because a goal of their study was to try to determine what minimal set of sensory cues are necessary
for e   ective soaring, both to shed light on the cues birds might use for soaring and to minimize the
sensing complexity required for automated glider soaring, the authors tried various sets of signals as
input to the id23 agent. they started by using state aggregation (chapter 9) of
a four-dimensional state space with dimensions giving local vertical wind speed, local vertical wind
acceleration, torque depending on the di   erence between the vertical wind velocities at the left and
right wing tips, and the local temperature. each dimension was discretized into three bins: positive
high, negative high, and small. results, described below, showed that only two of these dimensions
were critical for e   ective soaring behavior.

the overall objective of thermal soaring is to gain as much altitude as possible from each rising
column of air. reddy et al. tried a straightforward reward signal that rewarded the agent at the end of
each episode based on the altitude gained over the episode, a large negative reward signal if the glider
touched the ground, and zero otherwise. they found that learning was not successful with this reward
signal for episodes of realistic duration and that eligibility traces did not help. by experimenting with
various reward signals, they found that learning was best with a reward signal that at each time step
linearly combined the vertical wind velocity and vertical wind acceleration observed on the previous
time step.

learning was by sarsa with action selection using softmax applied to action values normalized to
the interval [0, 1]. the temperature parameter was initialized to 2.0 and incrementally decreased to 0.2
during learning. the step-size and discount-rate parameters were    xed at 0.1 and 0.98 respectively. each
learning episode took place with the agent controlling simulated    ight in an independently generated
period of simulated turbulent air currents. each episode lasted 2.5 minutes simulated with a 1 second
time step. learning e   ectively converged after a few hundred episodes. the left panel of figure 16.11
shows a sample trajectory before learning where the agent selects actions randomly. starting at the top
of the volume shown, the glider   s trajectory is in the direction indicated by the arrow and quickly loses
altitude. figure 16.11   s right panel is a trajectory after learning. the glider starts at the same place
(here appearing at the bottom of the volume) and gains altitude by spiraling within the rising column
of air. although reddy at al. found that performance varied widely over di   erent simulated periods
of air    ow, the number of times the glider touched the ground consistently decreased to nearly zero as

contributesignificantlyandmoreexploratorystrategiesarepreferred.thesarsaalgorithmfindstheoptimalpolicybyestimatingforeverystate   actionpairitsqfunctiondefinedastheexpectedsumoffuturerewardsgiventhecurrentstatesandtheactiona.ateachstep,theqfunctionisupdatedasfollows:q  s,a     q  s,a  +    r+  q  s   ,a        q  s,a    ,[5]whereristhereceivedrewardand  isthelearningrate.theupdateismadeonlineanddoesnotrequireanypriormodeloftheflowortheflight.thisfeatureisparticularlyrelevantinmodelingdecision-makingprocessesinanimals.whenthealgo-rithmisclosetoconvergence,theqfunctionapproachesthesolutiontobellman   sdynamicprogrammingequations(12).thepolicy  as,whichencodestheid203ofchoosingactionaatstates,approachestheoptimalone  pandisobtainedfromtheqfunctionviaaboltzmann-likeexpression:  as   exp    ^q  s,a     temp ,[6]^q  s,a  =maxa   q  s,a        q  s,a  maxa   q  s,a        mina   q  s,a     .[7]here,  tempisaneffective   temperature   :when  temp 1,ac-tionsareonlyweaklydependentontheassociatedqfunction;conversely,for  tempsmall,thepolicygreedilychoosestheactionwiththelargestq.thetemperatureparameterisinitiallychosenlargeandloweredastrainingprogressestocreateanannealingeffect,therebypreventingthepolicyfromgettingstuckinlocalextrema.parametersusedinoursimulationscanbefoundintables1.inthesequel,weshallqualifythepolicyidentifiedbysarsaasoptimal.itshouldbeunderstood,however,thatthesarsaalgorithm(asotherreinforcementlearningalgorithms)typicallyidentifiesanapproximatelyoptimalpolicyand   approximately   isskippedonlyforthesakeofconciseness.resultssensorimotorcuesandrewardfunctionforeffectivelearning.keyaspectsofthelearningforthesoaringproblemarethesensori-motorcuesthattheglidercansense(statespace)andthechoiceoftherewardusedtotraintheglidertoascendquickly.asthestateandactionspacesarecontinuousandhigh-dimensional,itisnecessarytodiscretizethem,whichwerealizeherebyastandardlookuptablerepresentation.theheightascendedpertrial,averagedoverdifferentrealizationsoftheflow,servesasourperformancecriterion.thegliderisallowedcontroloveritsangleofattackanditsbankangle(fig.1b).controlovertheangleofattackfeaturestworegimes:(i)atsmallanglesofattack,thehorizontalspeedislargeandtheclimbrateissmall(theglidersinksquickly);(ii)atlargeanglesofattackbutbelowthestallangle,thehorizontalspeedissmall,whereastheclimbrateislarge.thebankanglecontrolstheheadingoftheglider,andweallowforarangeofvariationbetween   15  and15  .exploringvariouspossibilities,wefoundthatthreeactionsareminimallysufficient:increasing,decreasing,orpreservingtheangleofattackandthebankangle.theangleofattackandbankanglewereincremented/decre-mentedinstepsof2.5  and5  ,respectively.insummary,theglidercanchoose32possibleactionstocontrolitsnavigationinresponsetothesensorimotorcuesdescribedhereafter.ourrationaleinthechoiceofthestatespacewastryingtominimizebiologicalorelectronicsensorydevicesnecessaryforcontrol.wetesteddifferentcombinationsoflocalsensorimotoraczylift lzxlift ldrag dvelocity directionwing directionbank angleglide angleangle of attackbdfig.1.snapshotsoftheverticalvelocity(a)andthetemperaturefields(b)inournumericalsimulationsof3drayleigh   b  nardconvection.fortheverticalvelocityfield,theredandbluecolorsindicateregionsoflargeupwardanddownwardflow,respectively.forthetemperaturefield,theredandbluecolorsindicateregionsofhighandlowtemperature,respectively.noticethatthehotandcoldregionsdrivetheupwardanddownwardbranchesoftheconvectivecell,inagreementwiththebasicphysicsofconvection.(c)theforce-bodydiagramofflightwithnothrust,thatis,withoutanyengineorflappingofwings.thefigurealsoshowsthebankangle  (blue),theangleofattack  (green),andtheglideangle  (red).(d)therangeofhorizontalspeedsandclimbratesaccessiblebycontrollingtheangleofattack.atsmallanglesofattack,theglidermovesfastbutalsosinksfast,whereasatlargerangles,theglidermovesandsinksmoreslowly.iftheangleofattackistoohigh,atabout16  ,thegliderstalls,leadingtoasuddendropinlift.theverticalblackdashedlineshowsthefixedangleofattackformostofthesimulations(results,controlovertheangleofattack).reddyetal.pnas|publishedonlineaugust1,2016|e4879neurosciencephysicspnaspluscontributesignificantlyandmoreexploratorystrategiesarepreferred.thesarsaalgorithmfindstheoptimalpolicybyestimatingforeverystate   actionpairitsqfunctiondefinedastheexpectedsumoffuturerewardsgiventhecurrentstatesandtheactiona.ateachstep,theqfunctionisupdatedasfollows:q  s,a     q  s,a  +    r+  q  s   ,a        q  s,a    ,[5]whereristhereceivedrewardand  isthelearningrate.theupdateismadeonlineanddoesnotrequireanypriormodeloftheflowortheflight.thisfeatureisparticularlyrelevantinmodelingdecision-makingprocessesinanimals.whenthealgo-rithmisclosetoconvergence,theqfunctionapproachesthesolutiontobellman   sdynamicprogrammingequations(12).thepolicy  as,whichencodestheid203ofchoosingactionaatstates,approachestheoptimalone  pandisobtainedfromtheqfunctionviaaboltzmann-likeexpression:  as   exp    ^q  s,a     temp ,[6]^q  s,a  =maxa   q  s,a        q  s,a  maxa   q  s,a        mina   q  s,a     .[7]here,  tempisaneffective   temperature   :when  temp 1,ac-tionsareonlyweaklydependentontheassociatedqfunction;conversely,for  tempsmall,thepolicygreedilychoosestheactionwiththelargestq.thetemperatureparameterisinitiallychosenlargeandloweredastrainingprogressestocreateanannealingeffect,therebypreventingthepolicyfromgettingstuckinlocalextrema.parametersusedinoursimulationscanbefoundintables1.inthesequel,weshallqualifythepolicyidentifiedbysarsaasoptimal.itshouldbeunderstood,however,thatthesarsaalgorithm(asotherreinforcementlearningalgorithms)typicallyidentifiesanapproximatelyoptimalpolicyand   approximately   isskippedonlyforthesakeofconciseness.resultssensorimotorcuesandrewardfunctionforeffectivelearning.keyaspectsofthelearningforthesoaringproblemarethesensori-motorcuesthattheglidercansense(statespace)andthechoiceoftherewardusedtotraintheglidertoascendquickly.asthestateandactionspacesarecontinuousandhigh-dimensional,itisnecessarytodiscretizethem,whichwerealizeherebyastandardlookuptablerepresentation.theheightascendedpertrial,averagedoverdifferentrealizationsoftheflow,servesasourperformancecriterion.thegliderisallowedcontroloveritsangleofattackanditsbankangle(fig.1b).controlovertheangleofattackfeaturestworegimes:(i)atsmallanglesofattack,thehorizontalspeedislargeandtheclimbrateissmall(theglidersinksquickly);(ii)atlargeanglesofattackbutbelowthestallangle,thehorizontalspeedissmall,whereastheclimbrateislarge.thebankanglecontrolstheheadingoftheglider,andweallowforarangeofvariationbetween   15  and15  .exploringvariouspossibilities,wefoundthatthreeactionsareminimallysufficient:increasing,decreasing,orpreservingtheangleofattackandthebankangle.theangleofattackandbankanglewereincremented/decre-mentedinstepsof2.5  and5  ,respectively.insummary,theglidercanchoose32possibleactionstocontrolitsnavigationinresponsetothesensorimotorcuesdescribedhereafter.ourrationaleinthechoiceofthestatespacewastryingtominimizebiologicalorelectronicsensorydevicesnecessaryforcontrol.wetesteddifferentcombinationsoflocalsensorimotoraczylift lzxlift ldrag dvelocity directionwing directionbank angleglide angleangle of attackbdfig.1.snapshotsoftheverticalvelocity(a)andthetemperaturefields(b)inournumericalsimulationsof3drayleigh   b  nardconvection.fortheverticalvelocityfield,theredandbluecolorsindicateregionsoflargeupwardanddownwardflow,respectively.forthetemperaturefield,theredandbluecolorsindicateregionsofhighandlowtemperature,respectively.noticethatthehotandcoldregionsdrivetheupwardanddownwardbranchesoftheconvectivecell,inagreementwiththebasicphysicsofconvection.(c)theforce-bodydiagramofflightwithnothrust,thatis,withoutanyengineorflappingofwings.thefigurealsoshowsthebankangle  (blue),theangleofattack  (green),andtheglideangle  (red).(d)therangeofhorizontalspeedsandclimbratesaccessiblebycontrollingtheangleofattack.atsmallanglesofattack,theglidermovesfastbutalsosinksfast,whereasatlargerangles,theglidermovesandsinksmoreslowly.iftheangleofattackistoohigh,atabout16  ,thegliderstalls,leadingtoasuddendropinlift.theverticalblackdashedlineshowsthefixedangleofattackformostofthesimulations(results,controlovertheangleofattack).reddyetal.pnas|publishedonlineaugust1,2016|e4879neurosciencephysicspnasplusglide angle16.8. thermal soaring

377

figure 16.11: sample thermal soaring trajectories, with arrows showing the direction of    ight from the same
starting point (note that the altitude scales are shifted). left: before learning: the agent selects actions randomly
and the glider descends. right: after learning: the glider gains altitude by following a spiral trajectory. adapted
with permission from pnas vol. 113(22), p. e4879, 2016, reddy, celani, sejnowski, and vergassola, learning
to soar in turbulent environments.

learning progressed.

after experimenting with di   erent sets of features available to the learning agent, it turned out that
the combination of just vertical wind acceleration and torques worked best. the authors conjectured
that because these features give information about the gradient of vertical wind velocity in two di   erent
directions, they allow the controller to select between turning by changing the bank angle or continuing
along the same course by leaving the bank angle alone. this allows the glider to stay within a rising
column of air. vertical wind velocity is indicative of the strength of the thermal but does not help in
staying within the    ow. they found that sensitivity to temperature was of little help. they also found
that controlling the angle of attack is not helpful in staying within a particular thermal, being useful
instead for traveling between thermals when covering large distances, as in cross-country gliding and
bird migration.

since soaring in di   erent levels of turbulence requires di   erent policies, training was done in conditions
ranging from weak to strong turbulence. in strong turbulence the rapidly changing wind and glider
velocities allowed less time for the controller to react. this reduced the amount of control possible
compared to what was possible for maneuvering when    uctuations were weak. reddy at al. examined
the policies sarsa learned under these di   erent conditions. common to policies learned in all regimes
were these features: when sensing negative wind acceleration, bank sharply in the direction of the wing
with the higher lift; when sensing large positive wind acceleration and no torque, do nothing. however,
di   erent levels of turbulence led to policy di   erences. policies learned in strong turbulence were more
conservative in that they preferred small bank angles, whereas in weak turbulence, the best action was
to turn as much as possible by banking sharply. systematic study of the bank angles preferred by the
policies learned under the di   erent conditions led the authors to suggest that by detecting when vertical
wind acceleration crosses a certain threshold the controller can adjust its policy to cope with di   erent
turbulence regimes.

reddy et al. also conducted experiments to investigate the e   ect of the discount-rate parameter    on
the performance of the learned policies. they found that the altitude gained in an episode increased as
   increased, reaching a maximum for    = .99, suggesting that e   ective thermal soaring requires taking
into account long-term e   ects of control decisions.

(a)(b)378

chapter 16. applications and case studies

this computational study of thermal soaring illustrates how id23 can further
progress toward di   erent kinds of objectives. learning policies having access to di   erent sets of environ-
mental cues and control actions contributes to both the engineering objective of designing autonomous
gliders and the scienti   c objective of improving understanding of the soaring skills of birds. in both
cases, hypotheses resulting from the learning experiments can be tested in the    eld by instrumenting
real gliders and by comparing predictions with observed bird soaring behavior.

chapter 17

frontiers

in this    nal chapter we touch on some topics that are beyond the scope of this book but that we see as
particularly important for the future of id23. many of these topics bring us beyond
what is reliably known, and some bring us beyond the mdp framework.

17.1 general value functions and auxiliary tasks

over the course of this book, our notion of value function has become quite general. with o   -policy
learning we allowed a value function to be conditional on an arbitrary target policy. then in section 12.8
we generalized discounting to a termination function    : s (cid:55)    [0, 1], so that a di   erent discount rate could
be applied at each time step in determining the return (12.24). this allowed us to express predictions
about how much reward we will get over an arbitrary, state-dependent horizon. the next, and perhaps
   nal, step is to generalize beyond rewards to permit predictions about arbitrary other signals. rather
than predicting the sum of future rewards, we might predict the sum of the future values of a sound or
color sensation, or of an internal, highly processed signal such as another prediction. whatever signal
is added up in this way in a value-function-like prediction, we call it the cumulant of that prediction.
we formalize it in a cumulant signal ct     r. using this, a general value function, or gvf, is written

v  ,  ,c(s) = e(cid:34)    (cid:88)k=t

ct+1

k(cid:89)i=t+1

st = s, at:         (cid:35) .

  (si)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as with conventional value functions (such as v   or q   ) this is an ideal function that we seek to
approximate with a parameterized form, which we might continue to denote   v(s,w), although of course
there would have to be a di   erent w for each prediction, that is, for each choice of   ,   , and ct. because
a gvf has no necessory connection to reward, it is perhaps a misnomer to call it a value function.
one could call it simply a prediction or, to make it more distinctive, a forecast (ring, in preparation).
whatever it is called, it is in the form of a value function and thus can be learned in the usual ways
using the methods developed in this book for learning approximate value functions. along with the
learned predictions, we might also learn policies to maximize the predictions in the usual gpi ways by
greedi   cation, or by actor   critic methods. in this way an agent could learn to predict and control great
numbers of signals, not just long-term reward.

why might it be useful to predict and control signals other than long-term reward? these are
auxiliary tasks in that they are extra, in-addition-to, the main task of maximizing reward. one answer
is that the ability to predict and control a diverse multitude of signals can constitute a powerful kind of
environmental model. as we saw in chapter 8, a good model can enable the agent to get reward more

379

(17.1)

380

chapter 17. frontiers

e   ciently. it takes a couple of further concepts to develop this answer clearly, so we postpone it to the
next section. first let   s consider two simpler ways in which a multitude of diverse predictions can be
helpful to a id23 agent.

one simple way in which auxiliary tasks can help on the main task is that they may require some of
the same representations as are needed on the main task. some of the auxiliary tasks may be easier, with
less delay and a clearer connection between actions and outcomes. if good features can be found early
on easy auxilary tasks, then those features may signi   cantly speed learning on the main task. there is
no necessary reason why this has to be true, but in many cases it seems plausible. for example, if you
learn to predict and control your sensors over short time scales, say seconds, then you might plausibly
come up with part of the idea of objects, which would then greatly help with the prediction and control
of long-term reward.

we might imagine an arti   cial neural network in which the last layer is split into multiple parts,
or heads, each working on a di   erent task. one head might produce the approximate value function
for the main task (with reward as its cumulant) whereas the others would produce solutions to various
auxilary tasks. all heads could propagate errors by stochastic id119 into the same body   the
shared preceding part of the network   which would then try to form representations, in its next-to-last
layer, to support all the heads. researchers have experimented with auxiliary tasks such as predicting
change in pixels, predicting the next-time-step   s reward, and predicting the distribution of the return.
in many cases this approach has been shown to greatly accelerate learning on the main task (jaderberg
et al., 2017). multiple predictions have similarly been repeatedly proposed as a way of directing the
construction of state estimates (see section 17.3).

another simple way in which the learning of auxiliary tasks can improve performance is best ex-
plained by analogy to the psychological phenomena of classical conditioning (section 14.2). one way of
understanding classical conditioning is that evolution has built in a re   exive (non-learned) association
to a particular action from the prediction of a particular signal. for example, humans and many other
animals appear to have a built-in re   ex to blink whenever their prediction of being poked in the eye
exceeds some threshold. the prediction is learned, but the association from prediction to blinking is
built in, and thus the animal is saved many pokes in its eye. similarly, the association from fear to
increased heart rate, or to freezing, can be built in. agent designers can do something similar, connect-
ing by design (without learning) predictions of speci   c events to predetermined actions. for example,
a self-driving car that learns to predict whether going forward will produce a collision could be given a
built-in re   ex to stop, or to turn away, whenever the prediction is above some threshold. or consider
a vacuum-cleaning robot that learned to predict whether it might run out of battery power before
returning to the charger, and re   exively headed back to the charger whenever the prediction became
non-zero. the correct prediction would depend on the size of the house, the room the robot was in, and
the age of the battery, all of which would be hard for the robot designer to know. it would be di   cult
for the designer to build in a reliable algorithm for deciding whether to head back to the charger in
sensory terms, but it might be easy to do this in terms of the learned prediction. we foresee many
possible ways like this in which learned predictions might combine usefully with built-in algorithms for
controlling behavior.

finally, perhaps the most important role for auxiliary tasks is in moving beyond the assumption
we have made throughout this book that the state representation is    xed and given to the agent. to
explain this role, we    rst have to take a few steps back to appreciate the magnitude of this assumption
and the implications of removing it. we do that in section 17.3.

17.2 temporal abstraction via options

an appealing aspect of the mdp formalism is that it can be applied usefully to tasks at many di   erent
time scales. one can use it to formalize the task of deciding which muscles to twitch to grasp an object,

17.2. temporal abstraction via options

381

which airplane    ight to take to arrive conveniently at a distant city, and which job to take to lead a
satisfying life. these tasks di   er greatly in their time scales, yet each can be usefully formulated as
an mdp that can be solved by planning or learning processes as described in this book. all involve
interaction with the world, sequential decision making, and a goal usefully conceived of as accumulating
rewards over time, and so all can be formulated as mdps.

although all these tasks can be formulated as mdps, one might think that they cannot be formulated
as a single mdp. they involve such di   erent time scales, such di   erent notions of choice and action! it
would be no good, for example, to plan a    ight across a continent at the level of muscle twitches. yet
for other tasks, grasping, throwing darts, or hitting a baseball, low-level muscle twitches may be just
the right level. people do all these things seaid113ssly without appearing to switch between levels. can
the mdp framework be stretched to cover all the levels simultaneously?

perhaps it can. one popular idea is to formalize an mdp at a detailed level, with a small time step,
yet enable planning at higher levels using extended courses of action that correspond to many base-level
time steps. to do this we need a notion of course of action that extends over many time steps and
includes a notion of termination. a general way to formulate these two ideas are as a policy,   , and a
state-dependent termination function,   , as in gvfs. we de   ne a pair of these as a generalized notion of
action termed an option. to execute an option    =(cid:104)    ,     (cid:105) at time t is to obtain the action to take, at,
from     (  |st), then terminate at time t + 1 with id203     (st+1). if the option does not terminate,
then at+1 is selected from     (  |st+1), the option terminates at t + 2 with id203     (st+2), and
so on until eventual termination. such options are a strict generalization of low-level actions, which
correspond to options (cid:104)    ,     (cid:105) in which the policy always picks the same action (    (s) = a for all s     s)
and in which the termination condition always terminates (    (s) = 0 for all s     s+). options e   ectively
extend the action space. the agent can either select a primitive action, terminating after one time step,
or select an extended option that might execute for many time steps before terminating.

options are designed so that they are interchangable with (primitive) actions. for example, the
notion of an action-value function q   naturally generalizes to an option-value function that takes a state
and option as input and returns the expected return starting from that state, executing that option
to termination, and thereafter following the policy,   . we can also generalize the notion of policy
to a hierarchical policy that selects from options rather than actions, where options, when selected,
execute until termination. with these ideas, many of the algorithms in this book can be generalized to
learn approximate option-value functions and hierarchical policies. in the simplest case, the learning
process    jumps    from option initiation to option termination, with an update only occurring when an
option terminates. more subtly, updates can be made on each time step, using intra-option learning
algorithms, which in general require o   -policy learning.

perhaps the most important generalization made possible by option ideas is that of the environmental
model as developed in chapters 3, 4 and 8. the conventional model of an action is the state-transition
probabilities and the expected immediate reward for taking the action in each state. how do conven-
tional action models generalize to option models? for options, the appropriate model is again of two
parts, one corresponding to the state transition resulting from executing the option and one correspond-
ing to the expected cumulative reward along the way. the reward part of an option model, analogous
to the expected reward for state   action pairs (3.5), is

r(s,   )

.

= e(cid:2)r1 +   r2 +   2r3 +        +        1r   (cid:12)(cid:12) s0 = s, a0:     1        ,            (cid:3) ,

for all options    and all states s     s, where    is the random time step at which the option terminates
according to     . note the role of the overall discounting parameter    in this equation   discounting is
according to   , but termination of the option is according to     . the state-transition part of an option
model is a little more subtle. this part of the model characterizes the id203 of each possible
resulting state (as in (3.4)), but now this state may result after various numbers of time steps, each of
which must be discounted di   erently. the model for option    speci   es, for each state s that    might

(17.2)

382

chapter 17. frontiers

start executing in, and for each state s(cid:48) that    might terminate in,

p(s(cid:48)|s,   )

.
=

   (cid:88)k=1

pr{sk = s(cid:48),    = k | s0 = s, a0:k   1        ,            }  k .

(17.3)

note that this p(s(cid:48)|s,   ) is no longer a transition id203 and no longer sums to one over all values
of s(cid:48). (nevertheless, we continue to use the    |    notation in p.)

the above de   nition of the transition part of an option model allows us to formulate bellman equa-
tions and id145 algorithms that apply to all options, including primitive actions as a
special case. for example, the general bellman equation for the state values of a hierarchical policy   
is

v  (s) = (cid:88)        (s)

  (  |s)(cid:34)r(s,   ) +(cid:88)s(cid:48)

p(s(cid:48)|s,   )v  (s(cid:48))(cid:35) ,

(17.4)

where    (s) denotes the set of options available in state s. if    (s) includes only the primitive actions, then
this equation reduces to a version of the usual bellman equation (3.14), except of course    is included
in the new p (17.3) and thus does not appear. similarly, the corresponding planning algorithms also
have no   . for example, the value iteration algorithm with options, analogous to (4.10), is

vk+1(s)

.
= max

        (s)(cid:34)r(s,   ) +(cid:88)s(cid:48)

p(s(cid:48)|s,   )vk(s(cid:48))(cid:35) , for all s     s.

(17.5)

if    (s) includes all the primitive actions available in each s, then this algorithm converges to the
conventional v   , from which the optimal policy can be computed. however, it is particularly useful to
plan with options when only a subset of the possible options are considered (in    (s)) in each state.
value iteration will then converge to the best hierarchical policy limited to the restricted set of options.
although this policy may be sub-optimal, convergence can be much faster because fewer options are
considered and because each option can jump over many time steps.

to plan with options, one must either be given the option models, or learn them. one natural way
to learn an option model is to formulate it as a collection of gvfs (as de   ned in the preceding section)
and then learn the gvfs using the methods presented in this book. it is not di   cult to see how this
could be done for the reward part of the option model. one merely chooses one gvf   s cumulant to be
the reward (ct = rt), its policy to be the the option   s policy (   =     ), and its termination function
to be the discount rate times the option   s termination function (  (s) =           (s)). the true gvf then
equals the reward part of the option model, v  ,  ,c(s) = r(s,   ), and the learning methods described in
this book can be used to approximate it. the state-transition part of the option model is only a little
more complicated. one needs to allocate one gvf for each state that the option might terminate in.
we don   t want these gvfs to accumulate anything except when the option terminates, and then only
when the termination is in the appropriate state. this can be achieved by choosing the cumulant of the
gvf that predicts transition to state s(cid:48) to be ct =   (st)    1st=s(cid:48). the gvf   s policy and termination
functions are chosen the same as for the reward part of the option model. the true gvf then equals the
s(cid:48) portion of the option   s state-transition model, v  ,  ,c(s) = p(s(cid:48)|s,   ), and again this book   s methods
could be employed to learn it. although each of these steps is seemingly natural, putting them all
together (including function approximation and other essential components) is quite challenging and
beyond the current state of the art.

exercise 17.1 this section has presented options for the discounted case, but discounting is arguably
inappropriate for control when using function approximation (section 10.4). what is the natural bell-
man equation for a hierarchical policy, analogous to (17.4), but for the average reward setting (sec-
tion 10.3)? what are the two parts of the option model, analogous to (17.2) and (17.3), for the average
reward setting?

17.3. observations and state

383

17.3 observations and state

throughout this book we have written the learned approximate value functions (and the policies in
chapter 13) as functions of the environment   s state. this is a signi   cant limitation of the methods
presented in part i, in which the learned value function was implemented as a table such that any
value function could be exactly approximated; that case is tantamount to assuming that the state of
the environment is completely observed by the agent. but in many cases of interest, and certainly in
the lives of all natural intelligences, the sensory input gives only partial information about the state
of the world. some objects may be ocluded by others, or behind the agent, or miles away. in these
cases, potentially important aspects of the environment   s state are not directly observable, and it is a
strong, unrealistic, and limiting assumption to assume that the learned value function is implemented
as a table over the environment   s state space.

on the other hand, the framework of parametric function approximation that we developed in part
ii is far less restrictive and, arguably, is no limitation at all. in part ii we retained the assumption
that the learned value functions (and policies) are functions of the environment   s state, but allowed
these functions to be arbitrarily restricted by the parameterization. it is somewhat surprising and not
widely recognized, but function approximation includes important aspects of partial observability. for
example, if there is some state variable that is not observable, then the parameterization can be chosen
such that the approximate value does not depend on that state variable. the e   ect is just as if that state
variable was not observable. because of this, all the results obtained for the parameterized case apply
to partial observability without change. in this sense, the case of parameterized function approximation
includes the case of partial observability.

nevertheless, there are many issues that cannot be investigated without a more explicit treatment of
partial observability. although we cannot give them a full treatment here, we can outline the changes
that would be needed to do so. there are four steps.

first, we would change the problem. the environment would emit not its states, but only obser-
vations   signals that depend on its state but, like a robot   s sensors, provide only partial information
about it. for convenience, without loss of generality, we assume that the reward is a direct, known
function of the observation (perhaps the observation is a vector, and the reward is one of is components).
the environmental interaction would then have no explicit states or rewards, but would simply be an
alternating sequence of actions at     a and observations ot     o:

a0, o1, a1, o2, a2, o3, a3, o4, . . . ,

going on forever (cf. equation 3.1) or forming episodes each ending with a special terminal observation.

second, we can recover the idea of state as used in this book from the sequence of observations and
actions. let us use the word history, and the notation ht, for an initial portion of the trajectory up to
.
an observation: ht
= a0, o1, . . . , at   1, ot. the history represents the most that we can know about
the past without looking outside of the data stream (because the history is the whole past data stream).
of course, the history grows with t and can become large and unwieldy. the idea of state is that of
some compact summary of the history that is as useful as the actual history for predicting the future.
let us be clear about exactly what this means. to be a summary of the history, the state must be a
function of history, st = f (ht), and to be as useful for predicting the future as the whole history is
known as the markov property. formally, this is a property of the function f . a function f has the
markov property if and only if any two histories h and h(cid:48) that are mapped by f to the same state
(f (h) = f (h(cid:48))) also have the same probabilities for their next observation,

f (h) = f (h(cid:48))     pr{ot = o|ht = h, at = a} = pr{ot+1 = o|ht = h(cid:48), at = a},

(17.6)
for all o     o and a     a. if f is markov, then st = f (ht) is a state as we have used the term in this
book. let us henceforth call it a markov state to distinguish it from states that are summaries of the
history but fall short of the markov property (which we will consider shortly).

384

chapter 17. frontiers

a markov state is a good basis for predicting the next observation (17.6) but, more importantly, it is
also a good basis for predicting or controlling anything. for example, let a test be any speci   c sequence
of alternating actions and observations that might occur in the future. for example, a three-step test
is denoted    = a1 o1 a2, o2, a3, o3. the id203 of this test given a speci   c history h is de   ned as

p(  |h)

.
= pr{ot+1 = o1, ot+2 = o2, ot+3 = o3 | ht = h, at = a1, at+1 = a2, at+2 = a3}.

(17.7)

if f is markov and h and h(cid:48) are any two histories that map to the same state under f , then for any test
   of any length, its probabilities given the two histories must also be the same:

f (h) = f (h(cid:48))     p(  |h) = p(  |h(cid:48)).

(17.8)

in other words, a markov state summarizes all the information in the history necessary for determining
any test   s id203. in fact, it summarizes all that is necessary for making any prediction, including
any gvf, and for behaving optimally (if f is markov, then there is always a deterministic function   
such that choosing at

.
=   (f (ht)) is optimal).

the third step in extending id23 to partial observability is to deal with certain
computational considerations. in particular, we want the state to be a compact summary of the his-
tory. for example, the identity function completely satis   es the conditions for a markov f , but would
nevertheless be of little use because the corresponding state st = ht would grow with time and becomes
unwieldy, as mentioned earlier, but more fundamentally because it would never recur; the agent would
never encounter the same state twice (in a continuing task) and thus could never bene   t from a tab-
ular learning method. we want our states to be compact as well as markov. there is a similar issue
regarding how state is obtained and updated. we don   t really want a function f that takes whole his-
tories. instead, for computational reasons we prefer to obtain the same e   ect as f with an incremental,
recursive update that computes st+1 from st, incorporating the next increment of data, at and ot+1:

st+1 = u(st, at, ot+1), for all t     0,

(17.9)

with the    rst state s0 given. the function u is called the state-update function. for example, if f were
the identity (st = ht), then u would merely extend st by appending at and ot+1 to it. given f , it is
always possible to construct a corresponding u, but it may not be computationally convenient and, as
in the identity example, it may not produce a compact state. the state-update function is a central
part of any agent architecture that handles partial observability. it must be e   ciently computatible, as
no actions or predictions can be made until the state is available.

an example of obtaining markov states through a state-update function is provided by the popular
bayesian approach known as partially observable mdps, or pomdps. in this approach the environ-
ment is assumed to have a well de   ned latent state xt that underlies and produces the environment   s
observations, but is never available to the agent (and is not to be confused with the state st used by the
agent to make predictions and decisions). the natural markov state st for a pomdp is the distribution
over the latent states given the history, called the belief state. for concreteness, assume the usual case
in which there are a    nite number of hidden states, xt     {1, 2, . . . , d}. then the belief state is the
vector st

.
= st     rd with components
.
= pr{xt = i | ht}, for all possible latent states i     {1, 2, . . . , d}.

(17.10)

st[i]

the belief state remains the same size (same number of components) however t grows. it can also be
incrementally updated by bayes rule, assuming one has complete knowledge of the internal workings of
the environment. speci   cally, the ith component of the belief-state update function is

u(s, a, o)[i] =

(cid:80)d
(cid:80)d
x=1(cid:80)d

x=1 s[x]p(i, o|x, a)

x(cid:48)=1 s[x]p(x(cid:48), o|x, a)

,

(17.11)

17.3. observations and state

385

for all a     a, o     o, and belief states s     rd with components s[x], where the four-argument p function
here is not the usual one for mdps (as in chapter 3), but the analogous one for pomdps, in terms
.
= pr{xt = x(cid:48), ot = o | xt   1 = x, at   1 = a}. this approach is popu-
of the latent state: p(x(cid:48), o|x, a)
lar in theoretical work and has many signi   cant applications, but its assumptions and computational
complexity scale poorly and we do not recommend it as an approach to arti   cial intelligence.

another example of markov states is provided by predictive state representations, or psrs. psrs
address a weakness of the pomdp approach that the semantics of its agent state st are grounded in
the environment state, xt, which is never observed and thus is di   cult to learn about. in psrs and
related approaches, the semantics of the agent state is instead grounded in predictions about future
observations and actions, which are readily observable.
in psrs, a markov state is de   ned as a d-
vector of the probabilities of d specially chosen    core    tests as de   ned above (17.7). the vector is then
updated by a state-update function u that is analogous to bayes rule, but with a semantics grounded
in observable data, which arguably makes it easier to learn. this approach has been extended in
many ways, including end-tests, compositional tests, powerful    spectral    methods, and closed-loop and
temporally abstract tests learned by td methods. some of the best theoretical developments are for
systems known as observable operator models (ooms) and sequential systems (thon, 2017).

the fourth and    nal step in our brief outline of how to handle partial observability in reinforcement
learning is to re-introduce approximation. as discussed in the introduction to part ii, to approach
arti   cial intelligence ambitiously one must embrace approximation. this is just as true for states as it
is for value functions. we must accept and work with an approximate notion of state. the approximate
state will play the same role in our algorithms as before, so we continue use the notation st for the
state used by the agent, even though it may not be markov.

perhaps the simplest example of an approximate state is just the latest observation, st

.
= ot. of course
this approach cannot handle any hidden state information. better is to use the last k observations and
.
= ot, at   1, ot   1, . . . , ot   k, for some k     1, which can be achieved by a state-update
actions, st
function that just shifts the new data in and the oldest data out. this kth-order history approach is
still very simple, but can greatly increase the agent   s capabilities compared to trying to use the single
immediate observation directly as the state.

what happens when the markov property (17.6) is only approximately satis   ed? prediction per-
formance can degrade dramatically. longer-term tests, gvfs, and state-update functions may all
approximate poorly with an approximate state even if the one-step predictions (17.6) de   ning the
markov property are well approximated with it. there are essentially no useful theoretical guarantees
at present.

nevertheless, there are still reasons to think that the general idea outlined in this section applies
to the approximate case. the general idea is that a state good for some predictions is also good for
others (in particular, that a markov state, su   cient for one-step predictions, is also su   cient for all
others). if we step back from that speci   c result for the markov case, the general idea is similar to
what we discussed in section 17.1 with multi-headed learning and auxiliary tasks. we discussed how
representations that were good for the auxiliary tasks were often also good for the main task. taken
together, these suggest an approach to both partial observability and representation learning in which
multiple predictions are pursued and used to direct the contruction of state features. the guarantee
provided by the perfect-but-impractical markov property is replaced by the heuristic that what   s good
for some predictions may be good for others. this approach scales well with computational resources.
with a large machine one could experiment with large numbers of predictions, perhaps favoring those
that are most similar to the ones of ultimate interest, that are easiest to learn reliably, or by other
criteria. key here is to move beyond selecting the predictions manually. the agent should do it. this
would require a general language for predictions, so that the agent can systematically explore a large
space of possible predictions, sifting through them for the ones that are most useful.

386

chapter 17. frontiers

17.4 designing reward signals

a major advantage of id23 over supervised learning is that id23 does
not rely on detailed instructional information: designing reward signals does not depend on knowing
what the correct actions should be. but not all reward signals are created equal. the success of a
id23 application often strongly depends on how well the reward signal frames the
problem and how well it assesses progress in solving it. future applications can bene   t from a better
understanding of how reward signals a   ect learning and from improved methods for designing them.

the usual way to use id23 to solve a problem is to reward the agent according to
its success in solving the problem. this is relatively easy for many problems, but some problems have
goals that are di   cult to translate into reward signals. this is especially true when the problem is to get
an agent to skillfully perform a complex task. even when there is a simple goal that is easy to identify,
the problem of sparse rewards often arises. delivering non-zero reward frequently enough to allow the
agent to achieve the goal once, let alone to learn to achieve it e   ciently from multiple initial conditions,
can be a daunting challenge. further, id23 agents can discover unexpected ways to
make their environments deliver reward, some of which might be undesirable, or even dangerous. for
these reasons, designing reward signals is a critical part of any id23 application.

in practice, designing reward signals is often left to an informal trial-and-error search for a signal
that produces acceptable results.
if the agent fails to learn, learns too slowly, or learns the wrong
thing, the designer tweaks the reward signal and tries again. to do this, the designer judges the agent   s
performance by criteria that he or she is attempting to translate into reward signals so that the the
agent   s goal matches his or her own. some more sophisticated ways to    nd good reward signals have
been proposed, but the subject has interesting and relatively unexplored dimensions.

it is tempting to address the sparse reward problem by rewarding the agent for achieving subgoals that
the designer thinks are important way stations to the overall goal. but augmenting the reward signal
with well-intentioned supplemental rewards may lead the agent to behave very di   erently from what
is intended; the agent may end up not achieving the overall goal at all. a better way to provide such
guidance is to leave the reward signal alone and instead augment the value-function approximation with
an initial guess of what it should ultimately be. for example, suppose one wants to o   er v0 : s     r as
an initial guess at the true optimal value function v   , and that one is using linear function approximation
with feature x : s     rd. then one would de   ne the approximate value function as

  v(s,w)

.
= w(cid:62)x(s) + v0(s),

(17.12)

and update the weights w as usual. if the initial weight vector is 0, then the initial value function will
be v0, but the asymptotic solution quality will be determined by the feature vectors as usual. this
initialization works for arbitrary nonlinear approximators and arbitrary forms of v0. wiewiora (2003)
showed that this initialization is equivalent to the more complex    potential-based shaping    technique
for changing rewards described by ng, harada, and russell (1999).

what if one has no idea what the rewards should be but there is another agent, perhaps a person, who
is already expert at the task and whose behavior can be observed? in this case one can use a variety of
methods known variously as    imitation learning,       learning from demonstration,    and    apprenticeship
learning.    the idea here is to bene   t from the expert agent but leave open the possibility of eventually
performing even better. learning from an expert   s behavior can be done either by learning directly
by supervised learning or by extracting a reward signal using what is known as    inverse reinforcement
learning    and then using a id23 algorithm with that reward function to learn a
policy. the task of inverse id23 as explored by ng and russell (2000) is to recover
the expert   s reward signal (within a scalar constant) from the expert   s behavior alone. this cannot be
done exactly because a policy can be optimal with respect to many di   erent reward signals (always
including any reward signal that gives the same reward for all states and actions), but it is possible
to    nd plausible reward-signal candidates. however, some strong assumptions are required, including

17.4. designing reward signals

387

knowledge of the feature vectors in which the reward signal is linear and complete knowledge of the
environment   s dynamics. the method also requires completely solving the problem (e.g., by dynamic
programming methods) multiple times. these di   culties notwithstanding, abbeel and ng (2004) argue
that the inverse reinforcement approach can sometimes be more e   ective than supervised learning for
bene   ting from the behavior of an expert.

another approach to    nding a good reward signal is based on automating the trial-and-error search
for such a signal that we mentioned above. from an engineering perspective, the reward signal is
a parameter of the learning algorithm. as is true for other algorithm parameters, the search for
a good reward signal can be automated by de   ning a space of feasible candidates and applying an
optimization algorithm. the optimization algorithm evaluates each candidate reward signal by running
the id23 system with that signal for some number of steps, and then scoring the
overall result by a    high-level    objective function intended to encode the designer   s true goal, ignoring
the limitations of the agent. in some cases, reward signals can be improved via online gradient ascent,
again as evaluated by a high-level objective function (sorg, lewis, and singh, 2010). relating this to
the natural world, the the algorithm for optimizing the high-level objective function is analogous to
evolution, where the high-level objective function is like an animal   s evolutionary    tness determined by
the number of its o   spring that survive to reproductive age.

experiments with this bilevel optimization approach (singh, lewis, and barto, 2009) con   rmed that
intuition alone is not always adequate to devise good reward signals. the performance of a reinforcement
learning agent as evaluated by the high-level objective function can be very sensitive to details of the
agent   s reward signal in subtle ways determined by the agent   s limitations and the environments in
which it acts and learns. these experiments also demonstrated that an agent   s goal should not always
be the same as the goal of the agent   s designer.

at    rst this seems counterintuitive, but it may be impossible for the agent to achieve the designer   s
goal no matter what its reward signal is because the agent has to learn under various kinds of constraints,
such as having limited computational power, limited access to information about its environment, or
limited time to learn. when there are constraints like these, learning to achieve a goal that is di   erent
from the designer   s goal can sometimes end up getting closer to the designer   s goal than if that goal were
pursued directly (sorg, singh, and lewis, 2010; sorg, 2011). there are also abundant examples of this
in the natural world. since we cannot directly detect the nutritional value of most foods, evolution   the
designer of our reward signal   produced a reward signal that makes us seek certain tastes. though
certainly not infallible (indeed, possibly detrimental in environments that di   er in certain ways from
the ancestral environments), this compensates for many of our limitations: our limited sensory abilities,
the limited time over which we can learn, and the risks involved in    nding a diet through personal
id23. similarly, since an animal cannot observe its own evolutionary    tness, that
evaluation function does not work as a reward signal for learning (although predictors of evolutionary
   tness certainly can be observed and    gure prominently in animals    reward signals).

another dimension to devising reward signals is whether the agent is to learn to solve a speci   c
problem, or if instead, it is to learn skills that can be useful across many di   erent problems that the
agent is likely to face in the future. pursuing the latter goal has led to the idea of implementing in
id23 something like what psychologists call    intrinsic motivation.    where    extrinsic
motivation    means doing something because of some speci   c rewarding outcome,    intrinsic motivation   
refers to doing something    for its own sake.    intrinsic motivation leads animals to engage in exploration,
play, and other behavior driven by curiosity in the absence of problem-speci   c rewards. the true value
of what is learned via intrinsically-motivated behavior (which for an animal would be the value of the
evolutionary advantage it confers) emerges over long-term experience with many di   erent speci   c tasks.

giving an agent something analogous to intrinsic motivation can be done by devising a reward signal
that helps an agent learn widely useful skills, including skills that aid the learning process itself. reward
signals can depend on such things as a general ability to cause changes in the environment, assessments

388

chapter 17. frontiers

of general progress in learning, or other measures that do not depend on a goal of performing a speci   c
task. an example is the    bonus reward    described in section 8.3. instead of being tied to a speci   c task,
this reward signal encourages exploration in general, which bene   ts the learning of many speci   c tasks.
another example is the proposal by schmidhuber (1991a, b) for how something like curiosity would
result if reward signals were a function of how quickly an agent   s environment model was improving
in predicting state transitions. many preliminary studies of such computational curiosity have been
conducted and are exciting topics of ongoing research.

17.5 remaining issues

in this book we have presented the foundations of a id23 approach to arti   cial in-
telligence. roughly speaking, that approach is based on model-free and model-based methods working
together, as in the dyna architecture of chapter 8, combined with function approximation as developed
in part ii. the focus has been on online and incremental algorithms, which we see as fundamental even
to model-based methods, and on how these can be applied in o   -policy training situations. the full
rationale for the latter has been presented only in this last chapter. that is, we have all along pre-
sented o   -policy learning as an appealing way to deal with the explore/exploit dilemma, but only in this
chapter have we talked about learning about many diverse auxiliary tasks simultaneously with gvfs,
and about understanding the world hierarchically in terms of temporally-abstract option models, both
of which seem to ineluctably involve o   -policy learning. much remains to be worked out, as we have
indicated throughout the book and as evidenced by the directions for additional research discussed in
this chapter. but suppose we are generous and grant the broad outlines of everything that we have
done in the book and everthing that has been outlined in this chapter. what would remain even after
that? of course, we can   t know for sure what will be required, but we can make some guesses. in this
section we highlight four further issues which it seems to us will still need to be addressed by future
research.

first, we still need powerful parametric function approximation methods that work well in fully
incremental and online settings. methods based on deep learning and arti   cial neural networks are
a major step in this direction, but rely on batch training with large data sets, extensive o   -line self
play, or learning asynchronously from multiple simultaneous streams agent-environment interaction.
these and other techniques are ways of working around a basic limitation of today   s deep learning
methods, which struggle to learn rapidly in the incremental, online settings that are most natural for
id23 settings and that we have emphasized in this book. the problem is sometimes
described as one of    correlated data    or    catastrophic interference   . when something new is learned
it tends to replace what has previously been learned rather than adding to it, such that the bene   t
of the older learning is lost. techniques such as    replay bu   ers    are often used to retain and replay
old data so that its bene   ts are not permanently lost. an honest assessment has to be that current
deep learning methods just don   t learn well online. we don   t see any reason why they couldn   t, but the
learning algorithms to do this have not yet been devised, and the bulk of current research is directed
toward working around this limitation of current algorithms rather than to removing it.

second, and perhaps closely related, we still need methods for learning features such that subsequent
learning generalizes well. this issue is an instance of a general problem variously called    representation
learning,       constructive induction,    and    meta-learning      how can we use experience not just to learn
a given a desired function, but to learn inductive biases such that future learning generalizes better and
is thus faster? this is an old problem, dating back to the origins of arti   cial intelligence and pattern
recognition in the 1950s and 1960s. (some would claim that deep learning solves this problem, but we
consider it still unsolved.) such age should give one pause. perhaps there is no solution? but just as
likely the time has not previously been ripe for any solution being found and being shown e   ective.
today machine learning is conducted at a far larger scale and the bene   ts of a good representation

17.5. remaining issues

389

learning method are potentially much more apparent. we note that a new annual conference   the
international conference on learning representations   has been exploring this and related topics every
year since 2013. it is also new to explore representation learning in a id23 context.
id23 brings some new possibilities to this old issue, such as the auxiliary tasks
discussed in section 17.1.
in id23, the problem of representation learning can be
identi   ed with the problem of learning the state-update function discussed in section 17.3.

third, we still need scalable methods for planning with learned models. planning methods have
proven extremely e   ective in applications such as alphago zero and computer chess in which the
model of the environment is known from the rules of the game or can otherwise be designed in by
people. but cases of full model-based id23, in which the environmental model is
learned from data and then used for planning, are rare. the dyna system described in chapter 8 is one
example, but as described there and in most subsequent work it used a tabular model without function
approximation of any sort, which greatly limits its applicability. there have been only a few studies
with learned linear models, and even fewer that have also tried to incorporate temporally abstract
models using options as discussed in section 17.2.

these limitations are a problem because they greatly limit the e   ectiveness of planning. in particular,
model making needs to be selective because the contents of the model strongly a   ect planning e   ciency.
if the model is focused on the key consequences of the most important possible options, then planning
can be e   cient and rapid, but if the model details the unimportant consequences of options that are
unlikely to be chosen, then planning may be almost useless. environmental models must be constructed
judiciously in both their states and dynamics so as to optimize the planning process. the various parts
of the model will have to be continually monitored for the degree to which they contribute to or detract
from planning e   ciency. the    eld has not yet come to grips with this complex of issues or designed
model-learning methods that take into account their implications. to make a good model that supports
planning is analogous to obtaining a true understanding of the environment that enables reasoning to
obtain a goal. as such it would be a signi   cant milestone in arti   cial intelligence.

the fourth and    nal issue that strikes us as as needing to be addressed in future research is that
of automating the choice of subproblems on which an agent works and which it uses to structure its
developing mind.
in machine learning, designers are used to setting the problems or tasks for the
learning agent. because these tasks are    xed, we build them into the code for the learning algorithm.
however, looking ahead we will want the agent to make its own choices about what tasks to work on.
these tasks may be like the auxiliary tasks or the gvfs discussed in section 17.1. in forming a gvf,
for example, what should the cumulant, the policy, and the termination function be? the current state
of the art is to select these manually, but far greater power and generality would come from making
these task choices automatically, particularly when they are from things previously constructed by the
agent as a result of representation learning or previous subproblems.
if gvf design is automated,
then the design choices themselves will have to be explicitly represented. rather than the task choices
being in the mind of the designer and built into the code, they will have to be in the machine itself in
such a way that they can be set and changed, monitored,    ltered, and searched among automatically.
tasks could then be built hierarchically upon one each other much like features are in a neural network.
the tasks are the questions and the contents of the neural network the answers to those questions.
we expect there will need to be a full hierarchy of questions to match the hierarchy of the answers in
modern deep learning.

390

chapter 17. frontiers

17.6 id23 and the future of arti   cial in-

telligence

when we were writing the    rst edition of this book in the mid-1990s, arti   cial intelligence was making
signi   cant progress and was having an impact on society, though it was mostly still the promise of
arti   cial intelligence that was inspiring developments. machine learning was part of that outlook, but
it had not yet become indispensable to arti   cial intelligence. by today that promise has transitioned
to applications that are changing the lives of millions of people, and machine learning has come into
its own as a key technology. as we write this second edition, some of the most remarkable develop-
ments in arti   cial intelligence have involved id23, most notably    deep reinforcement
learning      id23 with function approximation by deep neural networks. we are at
the beginning of a wave of real-world applications of arti   cial intelligence, many of which will include
id23, deep and otherwise, that will impact our lives in ways that are hard to predict.

but an abundance of successful real-world applications does not mean that true arti   cial intelli-
gence has arrived. despite great progress in many areas, the gulf between arti   cial intelligence and
the intelligence of humans, and even of other animals, remains great. superhuman performance can be
achieved in some domains, even formidable domains like go, but it remains a signi   cant challenge to
develop systems that are like us in being complete, interactive agents having general adaptability and
problem-solving skills, emotional sophistication, creativity, and the ability to learn quickly from expe-
rience. with its focus on learning by interacting with dynamic environments, id23,
as it develops over the future, will be a critical component of agents with these abilities.

id23   s connections to psychology and neuroscience (chapters 14 and 15) under-
score its relevance to another longstanding goal of arti   cial intelligence: shedding light on fundamental
questions about the mind and how it emerges from the brain. id23 theory is al-
ready contributing to our understanding of natural reward, motivation, and decision-making systems,
understanding that can contribute to improving human abilities to learn, to remain motivated, and to
make decisions. there is also good reason to believe that through its links to computational psychiatry,
id23 theory will contribute to methods for treating mental disorders, including drug
abuse and addiction.

another contribution that id23 can make over the future is as an aid to human
decision making. policies derived by id23 in simulated environments can advise
human decision makers in such areas as education, healthcare, transportation, energy, and public-sector
resource allocation. particularly relevant is the key feature of id23 that it takes long-
term consequences of decisions into account. this is very clear in games like backgammon and go,
where some of the most impressive results of id23 have been demonstrated, but it
is also a property of many high-stakes decisions that a   ect our lives and our planet. reinforcement
learning follows related methods for advising human decision making that have been developed in the
past by decision analysts in many disciplines. with advanced function approximation methods and
massive computational power, id23 methods have the potential to overcome some of
the di   culties of scaling up traditional decision-support methods to larger and more complex problems.

the rapid pace of advances in arti   cial intelligence has led to warnings that arti   cial intelligence
poses serious threats to our societies, even to humanity itself. the renowned scientist and arti   cial
intelligence pioneer herbert simon anticipated the warnings we are hearing today in a presentation at
the earthware symposium at cmu in 2000 (simon, 2000). he spoke of the eternal con   ict between the
promise and perils of any new knowledge, reminding us of the greek myths of prometheus, the hero
of modern science, who stole    re from the gods for the bene   t of mankind, and pandora, whose box
could be opened by a small and innocent action to release untold perils on the world. while accepting
that this con   ict is inevitable, simon urged us to recognize that as designers of our future and not
simply as spectators, the decisions we make can tilt the scale in prometheus    favor. this is certainly

17.6. id23 and the future of ai

391

true for id23, which can bene   t society but can also produce undesirable outcomes if
it is carelessly deployed. thus, the safety of arti   cial intelligence applications involving reinforcement
learning is a topic that deserves careful attention.

a id23 agent can learn by interacting with either the real world or with a simulation
of some piece of the real world, or by a mixture of these two sources of experience. simulators provide
safe environments in which an agent can explore and learn without risking real damage to itself or to
its environment. in most current applications, policies are learned from simulated experience instead
of direct interaction with the real world. in addition to avoiding undesirable real-world consequences,
learning from simulated experience can make virtually unlimited data available for learning, generally
at less cost than needed to obtain real experience, and since simulations typically run much faster than
real time, learning can often occur more quickly than if it relied on real experience.

nevertheless, the full potential of id23 requires id23 agents to
be embedded into the    ow of real-world experience, where they act, explore, and learn in our world,
and not just in their worlds. after all, id23 algorithms   at least those upon which
we focus in this book   are designed to learn online, and they emulate many aspects of how animals are
able to survive in nonstationary and hostile environments. embedding id23 agents
in the real world can be transformative in realizing the promises of arti   cial intelligence to amplify and
extend human abilities.

a major reason for wanting a id23 agent to act and learn in the real world is that it
is often di   cult, sometimes impossible, to simulate real-world experience with enough    delity to make
the resulting policies, whether derived by id23 or by other methods, work well   and
safely   when directing real actions. this is especially true for environments whose dynamics depend on
the behavior of humans, such as in education, healthcare, transportation, and public policy, domains
that can surely bene   t from improved decision making. however, it is for real-world embedded agents
that warnings about potential dangers of arti   cial intelligence need to be heeded.

id23 is a collection of optimization methods, so it inherits the pluses and minuses
of all optimization methods. on the minus side is the problem we mentioned at several places above:
how do you devise objective functions, or reward signals in the case of id23, so that
optimization produces the desired results while avoiding undesirable results? this is hardly a new
problem with id23; recognition of it has a long history in literature and engineering.
the founder of cybernetics, norbert weiner, for one, warned of this more than half a century ago by
relating the supernatural story of    the monkey   s paw    (weiner, 1964): wishes are granted but come
with unacceptable cost. the problem has also been discussed at length in a modern context by nick
bostrom (2014). anyone having experience with id23 has likely seen their systems
discover unexpected ways to obtain a lot of reward. sometimes the unexpected behavior is good:
it
solves a problem in a nice new way. in other instances, what the agent learns violates considerations
that the system designer may never have thought about. careful design of reward signals is essential if
an agent is to act in the real world with no opportunity for human vetting of its actions or means to
easily interrupt its behavior.

despite the possibility of unintended negative consequences, optimization has been used for hundreds
of years by engineers, architects, and others whose designs have positively impacted the world. many
approaches have been developed to mitigate the risk of optimization, such as adding hard and soft
constraints, restricting optimization to robust and risk-sensitive policies, and optimizing with multiple
objective functions. some of these have been adapted to id23. we owe much that
is good in our environment to the application of optimization methods. still, the problem of ensuring
that a id23 agent   s goal is attuned to our own remains a challenge.

another challenge if id23 agents are to act and learn in the real world is not just
about what they might eventually learn, but about how they will behave while they are learning. how
do you make sure that an agent gets enough experience to learn a high-performing policy, all the while

392

chapter 17. frontiers

not harming its environment, other agents, or itself (or more realistically, while keeping the id203
of harm acceptably low)? this problem is also not novel or unique to id23. risk
management and mitigation for embedded id23 is similar to what control engineers
have had to confront from the beginning of using automatic control in situations where a controller   s
behavior can have unacceptable, possibly catastrophic, consequences, as in the control of an aircraft
or a delicate chemical process. control applications rely on careful system modeling, model validation,
and extensive testing, and there is a highly-developed body of theory aimed at ensuring convergence
and stability of adaptive controllers designed for use when the dynamics of the system to be controlled
are not fully known. theoretical guarantees are never iron-clad because they depend on the validity of
the assumptions underlying the mathematics, but without this theory, combined with risk-management
and mitigation practices, automatic control   adaptive and otherwise   would not be as bene   cial as it
is today in improving the quality, e   ciency, and cost-e   ectiveness of processes on which we have come
to rely. some of this theory has been adapted to id23 to help prevent unwanted
behavior during, and after, learning, but many future applications of id23 are likely
to be in domains that are less constrained than those to which control theory and practice readily
apply. developing methods to make it acceptably safe to fully embed id23 agents
into physical environments is one of the most pressing areas for future research.

in closing, we return to simon   s call for us to recognize that we are designers of our future and
not simply spectators. by decisions we make as individuals, and by the in   uence we can exert on
how our societies are governed, we can work toward ensuring that the bene   ts made possible by a
new technology outweigh the harm it can cause. there is ample opportunity to do this in the case
of id23, which can help improve the quality, fairness, and sustainability of life on
our planet, but which can also release new perils. a threat already here is the displacement of jobs
caused by applications of arti   cial intelligence. still there are good reasons to believe that the bene   ts
of arti   cial intelligence can outweigh the disruption it causes. as to safety, hazards possible with
id23 are not completely di   erent from those that have been managed successfully for
related applications of optimization and control methods. as id23 moves out into
the real world in future applications, developers have an obligation to follow best practices that have
evolved for similar technologies, while at the same time extending them to make sure that prometheus
keeps the upper hand.

bibliographical and historical remarks

17.1 general value functions were    rst explicitly identi   ed by sutton and colleagues (sutton, 1995;
sutton et al., 2011; modayil, white and sutton, 2013). ring (in preparation) developed an
extensive thought experiment with gvfs (   forecasts   ) that has been in   uential despite not yet
having been published.

the    rst demonstrations of multi-headed learning in id23 were by jaderberg
et alia (2017). bellemare, dabney and munos (2017) showed that predicting more things about
the distribution of reward could signi   cantly speed learning to optimize its expectation, an
instance of auxiliary tasks. many others have since taken up this line of research.

the view of classical conditioning as learned predictions together with built-in, re   exive reac-
tions to the predictions has not to our knowledge been clearly articulated in the psychological
literature. modayil and sutton (2014) describe it as an approach to the engineering of robots
and other agents, calling it    pavlovian control    to allude to its roots in classical conditioning.

17.2

the formalization of temporally abstract courses of action as options was introduced by sutton,
precup, and singh (1999), building on prior work by parr (1998) and sutton (1995a), and on
classical work on semi-mdps (e.g., see puterman, 1994). precup   s (2000) phd thesis developed

17.6. id23 and the future of ai

393

option ideas fully. an important limitation of these early works is that they treated either the
tabular case or the on-policy case. the general case of intra-option learning involves o   -policy
learning, which could not be done reliably with function approximation at that time. although
now we have a variety of stable o   -policy learning methods using function approximation, their
combination with option ideas had not been signi   cantly explored at the time of publication of
this book.

using gvfs to implement option models has not previously been described. our presentation
uses the trick introduced by modayil, white and sutton (2014) for predicting signals at the
termination of policies.

among the few works that have learned option models with function approximation are those
by bacon, harb, and precup (2017).

the extension of options and option models to the average-reward setting has not yet been
developed in the literature.

17.3

for a good intuitive discussion of the system-theoretic concept of state, see minsky (1967). a
good presentation of the pomdp approach is given by monahan (1982). psrs and tests were
introduced by littman, sutton and singh (2002). ooms were introduced by jaeger (1997, 1998,
2000). sequential systems, which unify psrs, ooms, and many other works, were introduced
in the phd thesis of michael thon (2017; thon and jaeger, 2015).

the theory of id23 with a non-markov state representation was developed
explicitly by singh, jaakkola, and jordan (1994).

17.5

the problem of catastrophic interference in arti   cial neural networks was developed by mc-
closkey and cohen (1989), ratcli    (1990), and french (1999). the idea of a replay bu   er was
introduced by lin (1992) and used prominently in deep learning in the atari game playing
system (section 16.5, minh et al., 2013, 2015).

minsky (1961) was one of the    rst to identify the problem of representation learning.

among the few works to consider planning with learned, approximate models are those by
kuvayev and sutton (1998), sutton, szepesvari, geramifard, and bowling (2008), and nouri
and littman (2009).

the need to be selective in model construction to avoid slowing planning is well known in
arti   cial intelligence. some of the classic work is by minton (1990) and tambe, newell, and
rosenbloom (1990). hauskrecht, mieuleau, boutilier, kaelbling, and dean (1998) showed this
e   ect in mdps with deterministic options.

394

chapter 17. frontiers

references

abbeel, p., and ng, a. y. (2004). apprenticeship learning via inverse id23. in proceedings of

the twenty-   rst international conference on machine learning. acm.

abramson, b. (1990). expected-outcome: a general model of static evaluation. ieee transactions on pattern

analysis and machine intelligence 12 (2):182   193.

adams, c. (1982). variations in the sensitivity of instrumental responding to reinforcer devaluation. the

quarterly journal of experimental psychology, 34(2):77   98.

adams, c. d. and dickinson, a. (1981).

instrumental responding following reinforcer devaluation. the

quarterly journal of experimental psychology 33(2):109   121.

adams, r. a., huys, q. j. m., and roiser, j. p. (2015). computational psychiatry: towards a mathe-
journal of neurology, neurosurgery & psychiatry,

matically informed understanding of mental illness.
doi:10.1136/jnnp-2015-310737.

agrawal, r. (1995). sample mean based index policies with o(logn) regret for the multi-armed bandit problem.

advances in applied id203, 27:1054   1078.

agre, p. e. (1988). the dynamic structure of everyday life. ph.d. thesis, massachusetts institute of

technology. ai-tr 1085, mit arti   cial intelligence laboratory.

agre, p. e., chapman, d. (1990). what are plans for? robotics and autonomous systems, 6:17   34.
aizerman, m. a., braverman, e.   i., and rozonoer, l. i. (1964). id203 problem of pattern recognition

learning and potential functions method. avtomat. i telemekh 25 (9):1307   1323.

albus, j. s. (1971). a theory of cerebellar function. mathematical biosciences, 10:25   61.

albus, j. s. (1981). brain, behavior, and robotics. byte books, peterborough, nh.

aleksandrov, v. m., sysoev, v. i., shemeneva, v. v. (1968). stochastic optimization of systems. izv. akad.

nauk sssr, tekh. kibernetika, 14   19.

amari, s. i. (1998). natural gradient works e   ciently in learning. neural computation 10 (2), 251   276.

an, p.-c. e. (1991). an improved multi-dimensional cmac neural network: receptive field function and
placement (doctoral dissertation, phd thesis, dept. electrical and computer engineering, new hampshire
univ., new hampshire, usa).

an, p. c. e., miller, w. t., parks, p. c. (1991). design improvements in associative memories for cerebellar
model articulation controllers (cmac). arti   cial neural networks, pp. 1207   1210, elsvier north-holland.

anderson, c. w. (1986). learning and problem solving with multilayer connectionist systems. ph.d. thesis,

university of massachusetts, amherst.

anderson, c. w. (1987). strategy learning with multilayer connectionist representations. proceedings of the

fourth international workshop on machine learning, pp. 103   114. morgan kaufmann, san mateo, ca.

anderson, c. w. (1989). learning to control an inverted pendulum using neural networks.

ieee control

systems magazine 9 (3):31   37.

anderson, j. a., silverstein, j. w., ritz, s. a., jones, r. s. (1977). distinctive features, categorical perception,

and id203 learning: some applications of a neural model. psychological review, 84:413   451.

andreae, j. h. (1963). stella: a scheme for a learning machine. in proceedings of the 2nd ifac congress,

basle, pp. 497   502. butterworths, london.

395

396

references

andreae, j. h. (1969a). a learning machine with monologue. international journal of man   machine studies,

1:1   20.

andreae, j. h. (1969b). learning machines   a uni   ed view.

in a. r. meetham and r. a. hudson (eds.),

encyclopedia of information, linguistics, and control, pp. 261   270. pergamon, oxford.

andreae, j. h. (1977). thinking with the teachable machine. academic press, london.

arthur, w. b. (1991). designing economic agents that act like human agents: a behavioral approach to bounded

rationality. the american economic review 81 (2):353-359.

atkeson, c. g. (1992). memory-based approaches to approximating continuous functions. in sante fe institute

studies in the sciences of complexity, proceedings vol. 12, pp. 521   521. addison-wesley publishing co.

atkeson, c. g., moore, a. w., and schaal, s. (1997). locally weighted learning. arti   cial intelligence review

11 :11   73.

auer, p., cesa-bianchi, n., fischer, p. (2002). finite-time analysis of the multiarmed bandit problem. machine

learning, 47(2-3):235   256.

bacon, p. l., harb, j., and precup, d. (2017). the option-critic architecture. in proceedings of the association

for the advancement of arti   cial intelligence, pp. 1726   1734.

bae, j., chhatbar, p., francis, j. t., sanchez, j. c., and principe, j. c. (2011). id23 via
kernel temporal di   erence. in annual international conference of the ieee engineering in medicine and
biology society, pp. 5662   5665. ieee.

baird, l. c. (1995). residual algorithms: id23 with function approximation. in proceedings
of the twelfth international conference on machine learning, pp. 30   37. morgan kaufmann, san francisco.

baird, l. c., and klopf, a. h. (1993). id23 with high-dimensional, continuous actions.

wright laboratory, wright-patterson air force base, tech. rep. wl-tr-93-1147.

baird, l., moore, a. w. (1999). id119 for general id23. advances in neural

information processing systems, pp. 968   974.

baldassarre, g. and mirolli, m., editors (2013).

intrinsically motivated learning in natural and arti   cial

systems. springer-verlag, berlin.

balke, a., pearl, j. (1994). counterfactual probabilities: computational methods, bounds and applications.
in proceedings of the tenth international conference on uncertainty in arti   cial intelligence (pp. 46-54).
morgan kaufmann.

bao, g., cassandras, c. g., djaferis, t. e., gandhi, a. d., looze, d. p. (1994). elevator dispatchers for down

peak tra   c. technical report. ece department, university of massachusetts, amherst.

baras, d. and meir, r. (2007). id23, spike-time-dependent plasticity, and the bcm rule.

neural computation, 19(8):2245   2279.

barnard, e. (1993). temporal-di   erence methods and markov models. ieee transactions on systems, man,

and cybernetics 23:357   365.

barnhill, r. e. (1977). representation and approximation of surfaces. mathematical software 3 :69   120.

barreto, a. s., precup, d., and pineau, j. (2011). id23 using kernel-based stochastic factor-

ization. in advances in neural information processing systems, pp. 720   728.

bartlett, p. l. and baxter, j. (1999). hebbian synaptic modi   cations in spiking neurons that learn. technical

report, research school of information sciences and engineering, australian national university.

bartlett, p. l. and baxter, j. (2000). a biologically plausible and locally optimal learning algorithm for spiking

neurons. rapport technique, australian national university.

barto, a. g. (1985). learning by statistical cooperation of self-interested neuron-like computing elements.

human neurobiology, 4:229   256.

barto, a. g. (1986). game-theoretic cooperativity in networks of self-interested units. in j. s. denker (ed.),

neural networks for computing, pp. 41   46. american institute of physics, new york.

barto, a. g. (1989). from chemotaxis to cooperativity: abstract exercises in neuronal learning strategies. in
durbin, r., maill, r., and mitchison, g., editors, the computing neuron, pages 73   98. addison-wesley,
reading, ma.

references

397

barto, a. g. (1990). connectionist learning for control: an overview.

in t. miller, r. s. sutton, and

p. j. werbos (eds.), neural networks for control, pp. 5   58. mit press, cambridge, ma.

barto, a. g. (1991). some learning tasks from a control perspective. in l. nadel and d. l. stein (eds.), 1990

lectures in complex systems, pp. 195   223. addison-wesley, redwood city, ca.

barto, a. g. (1992). id23 and adaptive critic methods.

in d. a. white and d. a. sofge
(eds.), handbook of intelligent control: neural, fuzzy, and adaptive approaches, pp. 469   491. van nostrand
reinhold, new york.

barto, a. g. (1995a). adaptive critics and the basal ganglia.

in j. c. houk, j. l. davis, and d. g. beiser

(eds.), models of information processing in the basal ganglia, pp. 215   232. mit press, cambridge, ma.

barto, a. g. (1995b). id23.

in m. a. arbib (ed.), handbook of brain theory and neural

networks, pp. 804   809. mit press, cambridge, ma.

barto, a. g. (2011). adaptive real-time id145.

in sammut, c. and webb, g. i. (eds.)

encyclopedia of machine learning, pp. 19   22. springer science and business media.

barto, a. g. (2013). intrinsic motivation and id23. in intrinsically motivated learning in

natural and arti   cial systems, pp. 17   47. springer berlin heidelberg

barto, a. g., anandan, p. (1985). pattern recognizing stochastic learning automata. ieee transactions on

systems, man, and cybernetics, 15:360   375.

barto, a. g., anderson, c. w. (1985). structural learning in connectionist systems. in program of the seventh

annual conference of the cognitive science society, pp. 43   54.

barto, a. g., anderson, c. w., sutton, r. s. (1982). synthesis of nonlinear control surfaces by a layered

associative search network. biological cybernetics, 43:175   185.

barto, a. g., bradtke, s. j., singh, s. p. (1991). real-time learning and control using asynchronous dynamic
programming. technical report 91-57. department of computer and information science, university of
massachusetts, amherst.

barto, a. g., bradtke, s. j., singh, s. p. (1995). learning to act using real-time id145.

arti   cial intelligence, 72:81   138.

barto, a. g., du   , m. (1994). monte carlo matrix inversion and id23.

in j. d. cohen,
g. tesauro, and j. alspector (eds.), advances in neural information processing systems: proceedings of the
1993 conference, pp. 687   694. morgan kaufmann, san francisco.

barto, a. g., jordan, m. i. (1987). gradient following without back-propagation in layered networks.

in
m. caudill and c. butler (eds.), proceedings of the ieee first annual conference on neural networks,
pp. ii629   ii636. sos printing, san diego, ca.

barto, a. g., and mahadevan, s. (2003). recent advances in hierarchical id23. discrete event

dynamic systems 13 (4):341   379.

barto, a. g., singh, s., and chentanez, n. (2004). intrinsically motivated learning of hierarchical collections

of skills. in international conference on developmental learning (icdl), lajolla, ca.

barto, a. g., sutton, r. s. (1981a). goal seeking components for adaptive intelligence: an initial assessment.
technical report afwal-tr-81-1070. air force wright aeronautical laboratories/avionics laboratory,
wright-patterson afb, oh.

barto, a. g., sutton, r. s. (1981b). landmark learning: an illustration of associative search. biological

cybernetics, 42:1   8.

barto, a. g., sutton, r. s. (1982). simulation of anticipatory responses in classical conditioning by a neuron-like

adaptive element. behavioural brain research, 4:221   235.

barto, a. g., sutton, r. s., anderson, c. w. (1983). neuronlike elements that can solve di   cult learning control
problems. ieee transactions on systems, man, and cybernetics, 13:835   846. reprinted in j. a. anderson
and e. rosenfeld (eds.), neurocomputing: foundations of research, pp. 535   549. mit press, cambridge,
ma, 1988.

barto, a. g., sutton, r. s., brouwer, p. s. (1981). associative search network: a id23

associative memory. biological cybernetics, 40:201   211.

barto, a. g., sutton, r. s., and watkins, c. j. c. h. (1990). learning and sequential decision making.

398

references

in m. gabriel and j. moore (eds.), learning and computational neuroscience: foundations of adaptive
networks, pp. 539   602. mit press, cambridge, ma.

bellemare, m. g., dabney, w., and munos, r. (2017). a distributional perspective on id23.

arxiv preprint arxiv:1707.06887.

bellemare, m. g., naddaf, y., veness, j., and bowling, m. (2013). the arcade learning environment: an

evaluation platform for general agents. journal of arti   cial intelligence research, 47:253   279.

bellemare, m. g., veness, j., and bowling, m. (2012).

investigating contingency awareness using atari 2600
games. in proceedings of the twenty-sixth aaai conference on arti   cial intelligence (aaai 2012), pages
864   871, palo alto, ca. the aaai press.

bellman, r. e. (1956). a problem in the sequential design of experiments. sankhya, 16:221   229.

bellman, r. e. (1957a). id145. princeton university press, princeton.

bellman, r. e. (1957b). a markov decision process. journal of mathematical mechanics, 6:679   684.

bellman, r. e., dreyfus, s. e. (1959). functional approximations and id145. mathematical

tables and other aids to computation, 13:247   251.

bellman, r. e., kalaba, r., kotkin, b. (1973). polynomial approximation   a new computational technique in

id145: allocation processes. mathematical computation, 17:155   161.

bengio, y. (2009). learning deep architectures for ai. foundations and trends in machine learning, 2(1):1   27.

bengio, y., courville, a. c., and vincent, p. (2012). unsupervised id171 and deep learning: a review

and new perspectives. corr 1, arxiv 1206.5538.

bentley, j. l. (1975). multidimensional binary search trees used for associative searching. communications of

the acm 18 (9):509   517.

berg, h. c. (1975). chemotaxis in bacteria. annual review of biophysics and bioengineering, 4(1):119   136.

bernoulli, d. (1954). exposition of a new theory on the measurement of risk. econometrica, 22(1):23   36.

english translation of the 1738 paper.

berns, g. s., mcclure, s. m., pagnoni, g., and montague, p. r. (2001). predictability modulates human brain

response to reward. the journal of neuroscience, 21(8):2793   2798.

berridge, k. c. and kringelbach, m. l. (2008). a   ective neuroscience of pleasure: reward in humans and

animals. psychopharmacology, 199(3):457   480.

berridge, k. c. and robinson, t. e. (1998). what is the role of dopamine in reward: hedonic impact, reward

learning, or incentive salience? brain research reviews, 28(3):309   369.

berry, d. a., fristedt, b. (1985). bandit problems. chapman and hall, london.

bertsekas, d. p. (1982). distributed id145.

ieee transactions on automatic control,

27:610   616.

bertsekas, d. p. (1983). distributed asynchronous computation of    xed points. mathematical programming,

27:107   120.

bertsekas, d. p. (1987). id145: deterministic and stochastic models. prentice-hall, engle-

wood cli   s, nj.

bertsekas, d. p. (2005). id145 and optimal control, volume 1,

third edition. athena

scienti   c, belmont, ma.

bertsekas, d. p. (2012). id145 and optimal control, volume 2: approximate dynamic

programming, fourth edition. athena scienti   c, belmont, ma.

bertsekas, d. p. (2013). rollout algorithms for discrete optimization: a survey. in handbook of combinatorial

optimization, pp. 2989   3013. springer new york.

bertsekas, d. p., tsitsiklis, j. n. (1989). parallel and distributed computation: numerical methods. prentice-

hall, englewood cli   s, nj.

bertsekas, d. p., tsitsiklis, j. n. (1996). neuro-id145. athena scienti   c, belmont, ma.

bertsekas, d. p., tsitsiklis, j. n., and wu, c. (1997). rollout algorithms for combinatorial optimization.

journal of heuristics 3 (3):245   262.

references

399

bertsekas, d. p., yu, h. (2009). projected equation methods for approximate solution of large linear systems.

journal of computational and applied mathematics, 227(1):27   50.

bhat, n., farias, v., and moallemi, c. c. (2012). non-parametric approximate id145 via the

kernel method. in advances in neural information processing systems, pp. 386   394.

bhatnagar, s., sutton, r., ghavamzadeh, m., lee, m. (2009). natural actor   critic algorithms. automatica

45 (11).

biermann, a. w., fair   eld, j. r. c., beres, t. r. (1982).

signature table systems and learning.

ieee

transactions on systems, man, and cybernetics, 12:635   648.

bishop, c. m. (1995). neural networks for pattern recognition. clarendon, oxford.

bishop, c. m. (2006). pattern recognition and machine learning. springer.

blodgett, h. c. (1929). the e   ect of the introduction of reward upon the maze performance of rats. university

of california publications in psychology, 4:113   134.

boakes, r. a. and costa, d. s. j. (2014). temporal contiguity in associative learning: iinterference and
decay from an historical perspective. journal of experimental psychology: animal learning and cognition,
40(4):381   400.

booker, l. b. (1982). intelligent behavior as an adaptation to the task environment. ph.d. thesis, university

of michigan, ann arbor.

boone, g. (1997). minimum-time control of the acrobot.

in 1997 international conference on robotics and

automation, pp. 3281   3287. ieee robotics and automation society.

bottou, l., and vapnik, v. (1992). local learning algorithms. neural computation 4 (6):888   900.

boutilier, c., dearden, r., goldszmidt, m. (1995). exploiting structure in policy construction. in proceedings of
the fourteenth international joint conference on arti   cial intelligence, pp. 1104   1111. morgan kaufmann.

boyan, j. a., (1999). least-squares temporal di   erence learning. international conference on machine learning

16, pp. 49   56.

boyan, j. (2002). technical update: least-squares temporal di   erence learning. machine learning 49:233   246.

boyan, j. a., moore, a. w. (1995). generalization in id23: safely approximating the value
function. in g. tesauro, d. s. touretzky, and t. leen (eds.), advances in neural information processing
systems: proceedings of the 1994 conference, pp. 369   376. mit press, cambridge, ma.

bradtke, s. j. (1993). id23 applied to linear quadratic regulation.

in s. j. hanson,
j. d. cowan, and c. l. giles (eds.), advances in neural information processing systems: proceedings of the
1992 conference, pp. 295   302. morgan kaufmann, san mateo, ca.

bradtke, s. j. (1994). incremental id145 for on-line adaptive optimal control. ph.d. thesis,

university of massachusetts, amherst. appeared as cmpsci technical report 94-62.

bradtke, s. j., barto, a. g. (1996). linear least   squares algorithms for temporal di   erence learning. machine

learning, 22:33   57.

bradtke, s. j., ydstie, b. e., barto, a. g. (1994). adaptive linear quadratic control using policy iteration.
in proceedings of the american control conference, pp. 3475   3479. american automatic control council,
evanston, il.

bradtke, s. j., du   , m. o. (1995). id23 methods for continuous-time markov decision
in g. tesauro, d. touretzky, and t. leen (eds.), advances in neural information processing

problems.
systems: proceedings of the 1994 conference, pp. 393   400. mit press, cambridge, ma.

brafman, r. i., tennenholtz, m. (2003). r-max     a general polynomial time algorithm for near-optimal rein-

forcement learning. journal of machine learning research, 3, 213   231.

breiter, h. c., aharon, i., kahneman, d., dale, a., and shizgal, p. (2001). functional imaging of neural

responses to expectancy and experience of monetary gains and losses. neuron, 30(2):619   639.

breland, k. and breland, m. (1961). the misbehavior of organisms. american psychologist, 16(11):681   684.

bridle, j. s. (1990). training stochastic model recognition algorithms as networks can lead to maximum mutual
information estimates of parameters. in d. s. touretzky (ed.), advances in neural information processing
systems: proceedings of the 1989 conference, pp. 211   217. morgan kaufmann, san mateo, ca.

400

references

broomhead, d. s., lowe, d. (1988). multivariable functional interpolation and adaptive networks. complex

systems, 2:321   355.

bromberg-martin, e. s., matsumoto, m., hong, s., and hikosaka, o. (2010). a pallidus-habenula-dopamine

pathway signals inferred stimulus values. journal of neurophysiology, 104(2):1068   1076.

browne, c.b., powley, e., whitehouse, d., lucas, s.m., cowling, p.i., rohlfshagen, p., tavener, s., perez, d.,
samothrakis, s. and colton, s. (2012). a survey of id169 methods. ieee transactions
on computational intelligence and ai in games 4 (1):1   43.

brown, j., bullock, d., and grossberg, s. (1999). how the basal ganglia use parallel excitatory and inhibitory
learning pathways to selectively respond to unexpected rewarding cues. the journal of neuroscience,
19(23):10502   10511.

bryson, a. e., jr. (1996). optimal control   1950 to 1985. ieee control systems, 13(3):26   33.

buchanan, b. g., mitchell, t., smith, r. g., and jr., c. r. j. (1978). models of learning systems. encyclopeadia

of computer science and technology, 11.

buhusi, c. v. and schmajuk, n. a. (1999). timing in simple conditioning and occasion setting: a neural

network approach. behavioural processes, 45(1):33   57.

burke, c. j., dreher, j.-c., seymour, b., and tobler, p. n. (2014). state-dependent value representation:

evidence from the stiatum. frontiers in neuroscience, 8.

bu  soniu, l., lazaric, a., ghavamzadeh, m., munos, r., babu  ska, r., and de schutter, b. (2012). least-squares
methods for policy iteration. in wiering and van otterlo (eds.) id23: state-of-the art,
pp. 75   109. springer berlin heidelberg.

bush, r. r., mosteller, f. (1955). stochastic models for learning. wiley, new york.

byrne, j. h., gingrich, k. j., baxter, d. a. (1990). computational capabilities of single neurons: relationship
in r. d. hawkins and g. h. bower

to simple forms of associative and nonassociative learning in aplysia.
(eds.), computational models of learning, pp. 31   63. academic press, new york.

calabresi, p., picconi, b., tozzi, a., and filippo, m. d. (2007). dopamine-mediated regulation of corticostriatal

synaptic plasticity. trends in neuroscience, 30(5):211   219.

camerer, c. (2003). behavioral game theory: experiments in strategic interaction. princeton university press.

campbell, d. t. (1960). blind variation and selective survival as a general strategy in knowledge-processes. in

m. c. yovits and s. cameron (eds.), self-organizing systems, pp. 205   231. pergamon, new york.

cao, x. r. (2009). stochastic learning and optimization   a sensitivity-based approach. annual reviews in

control 33 (1):11   24.

carlstr  om, j., nordstr  om, e. (1997). control of self-similar atm call tra   c by id23.

in
proceedings of the international workshop on applications of neural networks to telecommunications 3,
pp. 54   62. erlbaum, hillsdale, nj.

chapman, d., kaelbling, l. p. (1991). input generalization in delayed id23: an algorithm and
performance comparisons. in proceedings of the twelfth international conference on arti   cial intelligence,
pp. 726   731. morgan kaufmann, san mateo, ca.

chaslot, g., bakkes, s., szita, i., and spronck, p. (2008). monte-carlo tree search: a new framework for game
ai. in proceedings of the aaai conference on arti   cial intelligence and interactive digital entertainment.

chow, c.-s., tsitsiklis, j. n. (1991). an optimal one-way multigrid algorithm for discrete-time stochastic

control. ieee transactions on automatic control, 36:898   914.

chrisman, l. (1992). id23 with perceptual aliasing: the perceptual distinctions approach.
in proceedings of the tenth national conference on arti   cial intelligence, pp. 183   188. aaai/mit press,
menlo park, ca.

christensen, j., korf, r. e. (1986). a uni   ed theory of heuristic evaluation functions and its application to
in proceedings of the fifth national conference on arti   cial intelligence, pp. 148   152. morgan

learning.
kaufmann, san mateo, ca.

cichosz, p. (1995). truncating temporal di   erences: on the e   cient implementation of td(  ) for reinforcement

learning. journal of arti   cial intelligence research, 2:287   318.

references

401

claridge-chang, a., roorda, r. d., vrontou, e., sjulson, l., li, h., hirsh, j., and miesenb  ock, g. (2009).

writing memories with light-addressable reinforcement circuitry. cell, 139(2):405   415.

clark, r. e. and squire, l. r. (1998). classical conditioning and brain systems: the role of awareness. science,

280(5360):77   81.

clark, w. a., farley, b. g. (1955). generalization of pattern recognition in a self-organizing system.

in

proceedings of the 1955 western joint computer conference, pp. 86   91.

clouse, j. (1996). on integrating apprentice learning and id23 title2. ph.d. thesis,

university of massachusetts, amherst. appeared as cmpsci technical report 96-026.

clouse, j., utgo   , p. (1992). a teaching method for id23 systems.

in proceedings of the

ninth international machine learning conference, pp. 92   101. morgan kaufmann, san mateo, ca.

cobo, l. c., zang, p., isbell, c. l., and thomaz, a. l. (2011). automatic state abstraction from demonstration.

in ijcaai proceedings: international joint conference on arti   cial intelligence, volume 22, page 1243.

cohen, j. y., haesler, s., vong, l., lowell, b. b., and uchida, n. (2012). neuron-type-speci   c signals for

reward and punishment in the ventral tegmental area. nature 482 (7383):85   88.

colombetti, m., dorigo, m. (1994). training agent to perform sequential behavior. adaptive behavior,

2(3):247   275.

connell, j. (1989). a colony architecture for an arti   cial creature. technical report ai-tr-1151. mit

arti   cial intelligence laboratory, cambridge, ma.

connell, j., mahadevan, s. (1993). robot learning. kluwer academic, boston.

connell, m. e., and utgo   , p. e. (1987). learning to control a dynamic physical system. computational

intelligence 3 (1):330   337.

contreras-vidal, j. l. and schultz, w. (1999). a predictive reinforcement model of dopamine neurons for

learning approach behavior. journal of computational neuroscience, 6(3):191   214.

coulom, r. (2006). e   cient selectivity and backup operators in monte-carlo tree search. in proceedings of the

5th international conference on computers and games, pp. 72   83.

courville, a. c., daw, n. d., and touretzky, d. s. (2006). bayesian theories of conditioning in a changing

world. trends in cognitive science, 10(7):294   300.

craik, k. j. w. (1943). the nature of explanation. cambridge university press, cambridge.

crites, r. h. (1996).

large-scale dynamic optimization using teams of id23 agents.

ph.d. thesis, university of massachusetts, amherst.

crites, r. h., barto, a. g. (1996).

in
d. s. touretzky, m. c. mozer, and m. e. hasselmo (eds.), advances in neural information processing
systems: proceedings of the 1995 conference, pp. 1017   1023. mit press, cambridge, ma.

improving elevator performance using id23.

cross, j. g. (1973). a stochastic learning model of economic behavior. the quarterly journal of economics

87 (2):239-266.

crow, t. j. (1968). cortical synapses and reinforcement: a hypothesis. nature, 219:736   737.

curtiss, j. h. (1954). a theoretical comparison of the e   ciencies of two classical methods and a monte carlo
method for computing one component of the solution of a set of id202ic equations. in h. a. meyer
(ed.), symposium on monte carlo methods, pp. 191   233. wiley, new york.

cybenko, g. (1989). approximation by superpositions of a sigmoidal function. mathematics of control, signals

and systems, 2(4):303   314.

cziko, g. (1995). without miracles: universal selection theory and the second darvinian revolution. mit

press, cambridge, ma.

daniel, j. w. (1976). splines and e   ciency in id145. journal of mathematical analysis and

applications, 54:402   407.

dann, c., neumann, g., peters, j. (2014). policy evaluation with temporal di   erences: a survey and compari-

son. journal of machine learning research 15 :809   883.

daw, n. d., courville, a. c., and touretzky, d. s. (2003). timing and partial observability in the dopamine

system. in advances in neural information processing systems, pages 99   106.

402

references

daw, n. d., courville, a. c., and touretzky, d. s. (2006). representation and timing in theories of the

dopamine system. neural computation, 18(7):1637   1677.

daw, n., niv, y., and dayan, p. (2005). uncertainty based competition between prefrontal and dorsolateral

striatal systems for behavioral control. nature neuroscience, 8(12):1704   1711.

daw, n. d. and shohamy, d. (2008). the cognitive neuroscience of motivation and learning. social cognition,

26(5):593   620.

dayan, p. (1991). reinforcement comparison. in d. s. touretzky, j. l. elman, t. j. sejnowski, and g. e. hinton
(eds.), connectionist models: proceedings of the 1990 summer school, pp. 45   51. morgan kaufmann, san
mateo, ca.

dayan, p. (1992). the convergence of td(  ) for general   . machine learning, 8:341   362.

dayan, p. (2008). the role of value systems in decision making. in engel, c. and singer, w., editors, better than
conscious?: decision making, the human mind, and implications for institutions (str  ungmann forum
reports), pages 51   70. mit press, cambridge, ma.

dayan, p. and abbott, l. f. (2001). theoretical neuroscience: computational and mathematical modeling of

neural systems. mit press, cambridge, ma.

dayan, p., and berridge, k. c. (2014). model-based and model-free pavlovian reward learning: revaluation,

revision, and revaluation. cognitive, a   ective, & behavioral neuroscience, 14(2):473   492.

dayan, p., and hinton, g. e. (1993). feudal id23.

in s. j. hanson, j. d. cohen, and
c. l. giles (eds.), advances in neural information processing systems: proceedings of the 1992 conference,
pp. 271   278. morgan kaufmann, san mateo, ca.

dayan, p. and niv, y. (2008). id23: the good, the bad and the ugly. current opinion in

neurobiology, 18(2):185   196.

dayan, p., niv, y., seymour, b., and daw, n. d. (2006). the misbehavior of value and the discipline of the

will. neural networks 19 (8):1153   1160.

dayan, p., and sejnowski, t. (1994). td(  ) converges with id203 1. machine learning, 14:295   301.

de asis, k., hernandez-garcia, j. f., holland, g. z., and sutton, r. s. (2017). multi-step reinforcement

learning: a unifying algorithm. arxiv preprint arxiv:1703.01327.

dean, t., lin, s.-h. (1995). decomposition techniques for planning in stochastic domains.

in proceedings of
the fourteenth international joint conference on arti   cial intelligence, pp. 1121   1127. morgan kaufmann.
see also technical report cs-95-10, brown university, department of computer science, 1995.

degris, t., white, m., sutton, r. s. (2012). o   -policy actor   critic. proceedings of the 29th international

conference on machine learning.

dejong, g., spong, m. w. (1994). swinging up the acrobot: an example of intelligent control. in proceedings
of the american control conference, pp. 2158   2162. american automatic control council, evanston, il.

denardo, e. v. (1967). contraction mappings in the theory underlying id145. siam review,

9:165   177.

dennett, d. c. (1978). brainstorms, pp. 71   89. bradford/mit press, cambridge, ma.

derthick, m. (1984). variations on the id82 learning algorithm. carnegie-mellon university

department of computer science technical report no. cmu-cs-84-120.

deutsch, j. a. (1953). a new type of behaviour theory. british journal of psychology. general section,

44(4):304   317.

deutsch, j. a. (1954). a machine with insight. quarterly journal of experimental psychology, 6(1):6   11.

dick, t. (2015). policy gradient id23 without regret. msc thesis, university of alberta.

dickinson, a. (1980). contemporary animal learning theory. cambridge university press, cambridge.

dickinson, a. (1985). actions and habits: the development of behavioral autonomy. phil. trans. r. soc.

lond. b, 308(1135):67   78.

dickinson, a. and balleine, b. w. (2002). the role of learning in motivation. in gallistel, c. r., editor, stevens

handbook of experimental psychology, volume 3, pages 497   533. wiley, ny.

references

403

dietterich, t. and buchanan, b. g. (1984). the role of the critic in learning systems.

in selfridge, o. g.,
rissland, e. l., and arbib, m. a., editors, adaptive control of ill-de   ned systems, pages 127   147. plenum
press, ny. proceedings of the nato advanced research institute on adaptive control of ill-de   ned
systems, nato conference series ii, systems science, vol. 16.

dietterich, t. g., flann, n. s. (1995). explanation-based learning and id23: a uni   ed
view. in a. prieditis and s. russell (eds.), proceedings of the twelfth international conference on machine
learning, pp. 176   184. morgan kaufmann, san francisco.

dietterich, t. g. and wang, x. (2002). batch value function approximation via support vectors. in advances

in neural information processing systems 14, pp. 1491   1498. cambridge, ma: mit press.

diuk, c., cohen, a., and littman, m. l. (2008). an object-oriented representation for e   cient reinforcement
in proceedings of the 25th international conference on machine learning, pages 240   247. acm

learning.
new york, ny.

dolan, r. j. and dayan, p. (2013). goals and habits in the brain. neuron, 80(2):312   325.

doll, b. b., simon, d. a., and daw, n. d. (2012). the ubiquity of model-based id23. current

opinion in neurobiology, 22:1   7.

donahoe, j. w. and burgos, j. e. (2000). behavior analysis and revaluation. journal of the experimental

analysis of behavior, 74(3):331   346.

dorigo, m. and colombetti, m. (1994). robot shaping: developing autonomous agents through learning.

arti   cial intelligence, 71(2):321   370.

doya, k. (1996). temporal di   erence learning in continuous time and space. in d. s. touretzky, m. c. mozer,
and m. e. hasselmo (eds.), advances in neural information processing systems: proceedings of the 1995
conference, pp. 1073   1079. mit press, cambridge, ma.

doya, k. and sejnowski, t. j. (1995). a novel reinforcement model of birdsong vocalization learning.

in
tesauro, g., touretzky, d. s., and leen, t., editors, advances in neural information processing systems:
proceedings of the 1994 conference, pages 101   108, cambridge, ma. mit press.

doya, k. and sejnowski, t. j. (1998). a computational model of birdsong learning by auditory experience and

auditory feedback. in central auditory processing and neural modeling, pages 77   88. springer us.

doyle, p. g., snell, j. l. (1984). id93 and electric networks. the mathematical association of

america. carus mathematical monograph 22.

dreyfus, s. e., law, a. m. (1977). the art and theory of id145. academic press, new york.

duda, r. o., hart, p. e. (1973). pattern classi   cation and scene analysis. wiley, new york.

du   , m. o. (1995). id24 for bandit problems.

in a. prieditis and s. russell (eds.), proceedings of the

twelfth international conference on machine learning, pp. 209   217. morgan kaufmann, san francisco.

egger, d. m. and miller, n. e. (1962). secondary reinforcement in rats as a function of information value and

reliability of the stimulus. journal of experimental psychology, 64:97   104.

eshel, n., bukwich, m., rao, v., hemmelder, v., tian, j., and uchida, n. (2015). arithmetic and local circuitry

underlying dopamine prediction errors. nature 525 (7568):243   246.

eshel, n., tian, j., bukwich, m., and uchida, n. (2016). dopamine neurons share common response function

for reward prediction error. nature neuroscience 19 (3):479   486.

estes, w. k. (1943). discriminative conditioning. i. a discriminative property of conditioned anticipation.

journal of experimental psychology 32 (2):150   155.

estes, w. k. (1948). discriminative conditioning. ii. e   ects of a pavlovian conditioned stimulus upon a subse-

quently established operant response. journal of experimental psychology 38 (2):173   177.

estes, w. k. (1950). toward a statistical theory of learning. psychololgical review, 57:94   107.

farley, b. g., clark, w. a. (1954). simulation of self-organizing systems by digital computer. ire transactions

on id205, 4:76   84.

farries, m. a. and fairhall, a. l. (2007). id23 with modulated spike timingdependent

synaptic plasticity. journal of neurophysiology, 98(6):3648   3665.

feldbaum, a. a. (1965). optimal control systems. academic press, new york.

404

references

finch, g., and culler, e. (1934). higher order conditioning with constant motivation. the american journal

of psychology, 596   602.

finnsson, h., bj  ornsson, y. (2008). simulation-based approach to general game playing. in proceedings of the

association for the advancement of arti   cial intelligence, 259   264.

fiorillo, c. d., tobler, p. n., and schultz, w. (2003). discrete coding of reward id203 and uncertainty by

dopamine neurons. science, 299(5614):1898   1902.

fiorillo, c. d., yun, s. r., and song, m. r. (2013). diversity and homogeneity in responses of midbrain

dopamine neurons. the journal of neuroscience, 33(11):4693   4709.

florian, r. v. (2007). id23 through modulation of spike-timing-dependent synaptic plasticity.

neural computation, 19(6):1468   1502.

fogel, l. j., owens, a. j., walsh, m. j. (1966). arti   cial intelligence through simulated evolution. john wiley

and sons.

french, r. m. (1999). catastrophic forgetting in connectionist networks. trends in cognitive sciences 3 (4),

128   135.

frey, u. and morris, r. g. m. (1997). synaptic tagging and long-term potentiation. nature, 385(6616):533   536.

friedman, j. h., bentley, j. l., and finkel, r. a. (1977). an algorithm for    nding best matches in logarithmic

expected time. acm transactions on mathematical software 3 (3):209   226.

friston, k. j., tononi, g., reeke, g. n., sporns, o., edelman, g. m. (1994). value-dependent selection in the

brain: simulation in a synthetic neural model. neuroscience, 59:229   243.

fu, k. s. (1970). learning control systems   review and outlook. ieee transactions on automatic control,

15:210   221.

galanter, e., gerstenhaber, m. (1956). on thought: the extrinsic theory. psychological review, 63:218   227.

gallant, s. i. (1993). neural network learning and id109. mit press, cambridge, ma.

gallistel, c. r. (2005). deconstructing the law of e   ect. games and economic behavior 52 (2), 410-423.

g  allmo, o., asplund, l. (1995). id23 by construction of hypothetical targets. in j. alspector,
r. goodman, and t. x. brown (eds.), proceedings of the international workshop on applications of neural
networks to telecommunications 2, pp. 300   307. erlbaum, hillsdale, nj.

gardner, m. (1973). mathematical games. scienti   c american, 228(1):108   115.

geist, m., scherrer, b. (2014). o   -policy learning with eligibility traces: a survey. journal of machine learning

research 15 :289   333.

gelperin, a., hop   eld, j. j., tank, d. w. (1985). the logic of limax learning. in a. selverston (ed.), model

neural networks and behavior, pp. 247   261. plenum press, new york.

genesereth, m., thielscher, m. (2014). general game playing. synthesis lectures on arti   cial intelligence and

machine learning, 8(2), 1   229.

gershman, s. j., moustafa, a. a., and ludvig, e. a. (2013). time representation in id23

models of the basal ganglia. frontiers in computational neuroscience, 7.

gershman, s. j., pesaran, b., and daw, n. d. (2009). human id23 subdivides structured

action spaces by learning e   ector-speci   c values. journal of neuroscience 29 (43):13524   13531.

gershman, s. j. and niv, y. (2010). learning latent structure: carving nature at its joints. current opinions

in neurobiology, 20:251   256.

ghiassian, s., ra   ee, b., sutton, r. s. (2016). a    rst empirical study of emphatic temporal di   erence learning.
workshop on continual learning and deep learning at the conference on neural information processing
systems. arxiv:1705.04185.

gibbs, c. m., cool, v., land, t., kehoe, e. j., and gormezano, i. (1991). second-order conditioning of the

rabbits nictitating membrane response. integrative physiological and behavioral science 26 (4):282   295.

gittins, j. c., jones, d. m. (1974). a dynamic allocation index for the sequential design of experiments.

in
j. gani, k. sarkadi, and i. vincze (eds.), progress in statistics, pp. 241   266. north-holland, amsterdam   
london.

references

405

glimcher, p. w. (2011). understanding dopamine and id23: the dopamine reward prediction

error hypothesis. proceedings of the national academy of sciences, 108(supplement 3):15647   15654.

glimcher, p. w. (2003). decisions, uncertainty, and the brain: the science of neuroeconomics. mit press,

cambridge, ma.

glimcher, p. w. and fehr, e., editors (2013). neuroeconomics: decision making and the brain, second edition.

academic press.

goldberg, d. e. (1989). id107 in search, optimization, and machine learning. addison-wesley,

reading, ma.

goldstein, h. (1957). classical mechanics. addison-wesley, reading, ma.

goodfellow, i., bengio, y., and courville, a. (2016). deep learning. mit press.

goodwin, g. c., sin, k. s. (1984). adaptive filtering prediction and control. prentice-hall, englewood cli   s,

nj.

gopnik, a., glymour, c., sobel, d., schulz, l. e., kushnir, t., and danks, d. (2004). a theory of causal

learning in children: causal maps and bayes nets. psychological review, 111(1):3   32.

gordon, g. j. (1995). stable function approximation in id145. in a. prieditis and s. russell
(eds.), proceedings of the twelfth international conference on machine learning, pp. 261   268. morgan
kaufmann, san francisco. an expanded version was published as technical report cmu-cs-95-103.
carnegie mellon university, pittsburgh, pa, 1995.

gordon, g. j. (1996a). chattering in sarsa(  ). cmu learning lab internal report.

gordon, g. j. (1996b). stable    tted id23. in d. s. touretzky, m. c. mozer, m. e. hasselmo
(eds.), advances in neural information processing systems: proceedings of the 1995 conference, pp. 1052   
1058. mit press, cambridge, ma.

gordon, g. j. (1999). approximate solutions to id100. phd thesis, school of computer

science, carnegie mellon university, pittsburgh, pa.

gordon, g. j. (2001). id23 with function approximation converges to a region. advances in

neural information processing systems.

graybiel, a. m. (2000). the basal ganglia. current biology, 10(14):r509   r511.

greensmith, e., bartlett, p. l., baxter, j. (2001). variance reduction techniques for gradient estimates in
in advances in neural information processing systems: proceedings of the 2000

id23.
conference, pp. 1507   1514.

greensmith, e., bartlett, p. l., baxter, j. (2004). variance reduction techniques for gradient estimates in

id23. journal of machine learning research 5 (nov), 1471   1530.

gri   th, a. k. (1966). a new machine learning technique applied to the game of checkers. technical report

project mac, arti   cial intelligence memo 94. massachusetts institute of technology, cambridge, ma.

gri   th, a. k. (1974). a comparison and evaluation of three machine learning procedures as applied to the

game of checkers. arti   cial intelligence, 5:137   148.

grondman, i., busoniu, l., lopes, g. a., babuska, r. (2012). a survey of actor   critic id23:
ieee transactions on systems, man, and cybernetics, part c

standard and natural policy gradients.
(applications and reviews) 42 (6), 1291   1307.

grossberg, s. (1975). a neural model of attention, reinforcement, and discrimination learning. international

review of neurobiology, 18:263   327.

grossberg, s. and schmajuk, n. a. (1989). neural dynamics of adaptive timing and temporal discrimination

during associative learning. neural networks, 2(2):79   102.

gullapalli, v. (1990). a stochastic reinforcement algorithm for learning real-valued functions. neural networks,

3:671   692.

gullapalli, v. and barto, a. g. (1992).

shaping as a method for accelerating id23.
proceedings of the 1992 ieee international symposium on intelligent control, pages 554   559. ieee.

in

gurney, k., prescott, t. j., and redgrave, p. (2001). a computational model of action selection in the basal

ganglia i. a new functional anatomy. biological cybernetics, 84(6):401   410.

406

references

gurvits, l., lin, l.-j., hanson, s. j. (1994). incremental learning of evaluation functions for absorbing markov

chains: new methods and theorems. preprint.

hackman, l. (2012). faster gradient-td algorithms (msc dissertation, university of alberta).

hallak, a., tamar, a., munos, r., mannor, s. (2016). generalized emphatic temporal di   erence learning:

bias-variance analysis. in thirtieth aaai conference on arti   cial intelligence.

hammer, m. (1997). the neural basis of associative reward learning in honeybees. trends in neuroscience,

20:245   252.

hammer, m. and menzel, r. (1995).

learning and memory in the honeybee.

journal of neuroscience,

15(3):1617   1630.

hampson, s. e. (1983). a neural model of adaptive behavior. ph.d. thesis, university of california, irvine.

hampson, s. e. (1989). connectionist problem solving: computational aspects of biological learning. birkhauser,

boston.

hare, t. a., o   doherty, j., camerer, c. f., schultz, w., and rangel, a. (2008). dissociating the role of the
orbitofrontal cortex and the striatum in the computation of goal values and prediction errors. the journal
of neuroscience, 28(22):5623   5630.

hassabis, d. and maguire, e. a. (2007). deconstructing episodic memory with construction. trends in cognitive

sciences, 11(7):299   306.

hawkins, r. d., kandel, e. r. (1984). is there a cell-biological alphabet for simple forms of learning? psycho-

logical review, 91:375   391.

he, k., huertas, m., hong, s. z., tie, x., hell, j. w., shouval, h., and kirkwood, a. (2015). distinct eligibility

traces for ltp and ltd in cortical synapses. neuron, 88(3):528   538.

he, k., zhang, x., ren, s., and sun, j. (2016). deep residual learning for image recognition. in proceedings of

the 1992 ieee conference on id161 and pattern recognition, pages 770   778.

hebb, d. o. (1949). the organization of behavior: a neuropsychological theory. john wiley and sons inc.,

new york. reissued by lawrence erlbaum associates inc., mahwah nj, 2002.

hengst, b. (2012). hierarchical approaches.

in wiering and van otterlo (eds.) id23:

state-of-the art, pp. 293   323. springer berlin heidelberg.

herrnstein, r. j. (1970). on the law of e   ect. journal of the experimental analysis of behavior 13 (2),

243-266.

hersh, r., griego, r. j. (1969). brownian motion and potential theory. scienti   c american, 220:66   74.

hester, t., and stone, p. (2012). learning and using models. in wiering and van otterlo (eds.) reinforcement

learning: state-of-the art, pp. 111   141. springer berlin heidelberg.

hesterberg, t. c. (1988), advances in importance sampling, ph.d. dissertation, statistics department, stanford

university.

hilgard, e. r. (1956). theories of learning, second edition. appleton-century-cofts, inc., new york.

hilgard, e. r., bower, g. h. (1975). theories of learning. prentice-hall, englewood cli   s, nj.

hinton, g. e. (1984). distributed representations. technical report cmu-cs-84-157. department of computer

science, carnegie-mellon university, pittsburgh, pa.

hinton, g. e., osindero, s., and teh, y. (2006). a fast learning algorithm for deep belief nets. neural

computation, 18(7):1527   1554.

hochreiter, s., schmidhuber, j. (1997). ltsm can solve hard time lag problems.

in advances in neural
information processing systems: proceedings of the 1996 conference, pp. 473   479. mit press, cambridge,
ma.

holland, j. h. (1975). adaptation in natural and arti   cial systems. university of michigan press, ann arbor.

holland, j. h. (1976). adaptation. in r. rosen and f. m. snell (eds.), progress in theoretical biology, vol. 4,

pp. 263   293. academic press, new york.

holland, j. h. (1986). escaping brittleness: the possibility of general-purpose learning algorithms applied to
rule-based systems. in r. s. michalski, j. g. carbonell, and t. m. mitchell (eds.), machine learning: an

references

407

arti   cial intelligence approach, vol. 2, pp. 593   623. morgan kaufmann, san mateo, ca.

hollerman, j. r. and schultz, w. (1998). dopmine neurons report an error in the temporal prediction of reward

during learning. nature neuroscience, 1:304   309.

houk, j. c., adams, j. l., barto, a. g. (1995). a model of how the basal ganglia generates and uses neural
signals that predict reinforcement. in j. c. houk, j. l. davis, and d. g. beiser (eds.), models of information
processing in the basal ganglia, pp. 249   270. mit press, cambridge, ma.

howard, r. (1960). id145 and markov processes. mit press, cambridge, ma.

hull, c. l. (1932). the goal-gradient hypothesis and maze learning. psychological review, 39(1):25   43.

hull, c. l. (1943). principles of behavior. appleton-century, new york.

hull, c. l. (1952). a behavior system. wiley, new york.

io   e, s., and szegedy, c. (2015). batch id172: accelerating deep network training by reducing internal

covariate shift. arxiv:1502.03167.

  ipek, e., mutlu, o., mart    nez, j. f., and caruana, r. (2008). self-optimizing memory controllers: a rein-
forcement learning approach. in 35th international symposium on computer architecture, isca   08, pages
39   50. ieee.

izhikevich, e. m. (2007). solving the distal reward problem through linkage of stdp and dopamine signaling.

cerebral cortex, 17(10):2443   2452.

jaakkola, t., jordan, m. i., singh, s. p. (1994). on the convergence of stochastic iterative id145

algorithms. neural computation, 6:1185   1201.

jaakkola, t., singh, s. p., jordan, m. i. (1995). id23 algorithm for partially observable
markov decision problems. in g. tesauro, d. s. touretzky, t. leen (eds.), advances in neural information
processing systems: proceedings of the 1994 conference, pp. 345   352. mit press, cambridge, ma.

jaderberg, m., mnih, v., czarnecki, w. m., schaul, t., leibo, j. z., silver, d., and kavukcuoglu, k. (2016).

id23 with unsupervised auxiliary tasks. arxiv preprint arxiv:1611.05397.

jaeger, h. (1997). observable operator models and conditioned continuation representa- tions. arbeitspapiere

der gmd 1043, gmd forschungszentrum informationstechnik, sankt augustin, germany.

jaeger, h. (1998). discrete time, discrete valued observable operator models: a tutorial. gmd-forschungszentrum

informationstechnik.

jaeger, h. (2000). observable operator models for discrete stochastic time series. neural computation 12 (6),

1371   1398.

jaeger, h. (2002). tutorial on training recurrent neural networks, covering bppt, rtrl, ekf and the    echo
state network    approach. german national research center for information technology, technical report
gmd report 159, 2002.

joel, d., niv, y., and ruppin, e. (2002). actor   critic models of the basal ganglia: new anatomical and

computational perspectives. neural networks, 15(4):535   547.

johanson, e. b., killeen, p. r., russell, v. a., tripp, g., wickens, j. r., tannock, r., williams, j., and
sagvolden, t. (2009). origins of altered reinforcement e   ects in adhd. behavioral and brain functions,
5(7).

johnson, a. and redish, a. d. (2007). neural ensembles in ca3 transiently encode paths forward of the animal

at a decision point. the journal of neuroscience, 27(45):12176   12189.

kaelbling, l. p. (1993a). hierarchical learning in stochastic domains: preliminary results. in proceedings of the

tenth international conference on machine learning, pp. 167   173. morgan kaufmann, san mateo, ca.

kaelbling, l. p. (1993b). learning in embedded systems. mit press, cambridge, ma.

kaelbling, l. p. (ed.) (1996). special triple issue on id23, machine learning 22 (1/2/3).

kaelbling, l. p., littman, m. l., moore, a. w. (1996). id23: a survey. journal of arti   cial

intelligence research, 4:237   285.

kahneman, d. and tversky, a. (1979). prospect theory: an analysis of decision under risk. econometrica:

journal of the econometric society, 47:263   291.

408

references

kakade, s. (2002). a natural policy gradient. advances in neural information processing systems 2, 1531   1538.

kakade, s. m. (2003). on the sample complexity of id23 (doctoral dissertation, university

of london).

kakutani, s. (1945). markov processes and the dirichlet problem. proceedings of the japan academy, 21:227   

233.

kalos, m. h., whitlock, p. a. (1986). monte carlo methods. wiley, new york.

kamin, l. j. (1968).

in jones, m. r., editor, miami
symposium on the prediction of behavior, 1967: aversive stimulation, pages 9   31. university of miami
press, coral gables, florida.

   attention-like    processes in classical conditioning.

kamin, l. j. (1969). predictability, surprise, attention, and conditioning.

in campbell, b. a. and church,
r. m., editors, punishment and aversive behavior, pages 279   296. appleton-century-crofts, new york,
ny.

kandel, e. r., schwartz, j. h., jessell, t. m., siegelbaum, s. a., and hudspeth, a. j., editors (2013). principles

of neural science, fifth edition. mcgraw-hill companies, inc.

kanerva, p. (1988). sparse distributed memory. mit press, cambridge, ma.

kanerva, p. (1993). sparse distributed memory and related models. in m. h. hassoun (ed.), associative neural

memories: theory and implementation, pp. 50   76. oxford university press, new york.

karampatziakis, n., and langford, j. (2010). online importance weight aware updates. arxiv:1011.1576.

kashyap, r. l., blaydon, c. c., fu, k. s. (1970). stochastic approximation. in j. m. mendel and k. s. fu (eds.),
adaptive, learning, and pattern recognition systems: theory and applications, pp. 329   355. academic
press, new york.

kearns, m., singh, s. (2002). near-optimal id23 in polynomial time. machine learning,

49(2-3), 209   232.

keerthi, s. s., ravindran, b. (1997). id23. in e. fiesler and r. beale (eds.), handbook of

neural computation, c3. oxford university press, new york.

kehoe, e. j. (1982). conditioning with serial compound stimuli: theoretical and empirical issues. experimental

animal behavior, 1:30   65.

kehoe, e. j., schreurs, b. g., and graham, p. (1987). temporal primacy overrides prior training in se-
rial compound conditioning of the rabbits nictitating membrane response. animal learning & behavior,
15(4):455   464.

kei   in, r. and janak, p. h. (2015). dopamine prediction errors in reward learning and addiction: ffrom theory

to neural circuitry. neuron, 88(2):247    263.

kimble, g. a. (1961). hilgard and marquis    conditioning and learning. appleton-century-crofts, new york.

kimble, g. a. (1967). foundations of conditioning and learning. appleton-century-crofts, new york.

klopf, a. h. (1972). brain function and adaptive systems   a heterostatic theory. technical report afcrl-
72-0164, air force cambridge research laboratories, bedford, ma. a summary appears in proceedings
of the international conference on systems, man, and cybernetics. ieee systems, man, and cybernetics
society, dallas, tx, 1974.

klopf, a. h. (1975). a comparison of natural and arti   cial intelligence. sigart newsletter, 53:11   13.

klopf, a. h. (1982). the hedonistic neuron: a theory of memory, learning, and intelligence. hemisphere,

washington, dc.

klopf, a. h. (1988). a neuronal model of classical conditioning. psychobiology, 16:85   125.

kober, j. and peters, j. (2012). id23 in robotics: a survey. in wiering, m. and van otterlo,

m., editors, id23: state-of-the-art, pages 579   610. springer-verlag, berlin.

kocsis, l., szepesv  ari, cs.

(2006). bandit based monte-carlo planning.

in proceedings of the european

conference on machine learning, 282   293. springer berlin heidelberg.

kohonen, t. (1977). associative memory: a system theoretic approach. springer-verlag, berlin.

koller, d., friedman, n. (2009). probabilistic id114: principles and techniques. mit press, 2009.

references

409

kolodziejski, c., porr, b., and w  org  otter, f. (2009). on the asymptotic equivalence between di   erential

hebbian and temporal di   erence learning. neural computation, 21(4):1173   1202.

kolter, j. z. (2011). the    xed points of o   -policy td. advances in neural information processing systems 24,

pp. 2169   2177.

konidaris, g. d., osentoski, s., thomas, p. s. (2011). value function approximation in id23
using the fourier basis, proceedings of the twenty-fifth conference of the association for the advancement
of arti   cial intelligence, pp. 380   385.

korf, r. e. (1988). optimal path    nding algorithms. in l. n. kanal and v. kumar (eds.), search in arti   cial

intelligence, pp. 223   267. springer verlag, berlin.

korf, r. e. (1990). real-time heuristic search. arti   cial intelligence 42 (2   3), 189   211.

koshland, d. e. (1980). bacterial chemotaxis as a model bhavioral system. raven press, new york.

koza, j. r. (1992). genetic programming: on the programming of computers by means of natural selection

(vol. 1). mit press.

kraft, l. g., campagna, d. p. (1990). a summary comparison of cmac neural network and traditional
adaptive control systems. in t. miller, r. s. sutton, and p. j. werbos (eds.), neural networks for control,
pp. 143   169. mit press, cambridge, ma.

kraft, l. g., miller, w. t., dietz, d. (1992). development and application of cmac neural network-based
control. in d. a. white and d. a. sofge (eds.), handbook of intelligent control: neural, fuzzy, and adaptive
approaches, pp. 215   232. van nostrand reinhold, new york.

kumar, p. r., varaiya, p. (1986).

stochastic systems: estimation, identi   cation, and adaptive control.

prentice-hall, englewood cli   s, nj.

kumar, p. r. (1985). a survey of some results in stochastic adaptive control. siam journal of control and

optimization, 23:329   380.

kumar, v., kanal, l. n. (1988). the cdp: a unifying formulation for heuristic search, id145,
in l. n. kanal and v. kumar (eds.), search in arti   cial intelligence, pp. 1   37.

and branch-and-bound.
springer-verlag, berlin.

kushner, h. j., dupuis, p. (1992). numerical methods for stochastic control problems in continuous time.

springer-verlag, new york.

lagoudakis, m., parr, r. (2003). least squares policy iteration. journal of machine learning research 4 :1107   

1149.

lai, t. l., robbins, h. (1985). asymptotically e   cient adaptive allocation rules. advances in applied mathe-

matics, 6(1):4   22.

lakshmivarahan, s. and narendra, k. s. (1982). learning algorithms for two-person zero-sum stochastic games
with incomplete information: a uni   ed approach. siam journal of control and optimization, 20:541   552.

lammel, s., lim, b. k., and malenka, r. c. (2014). reward and aversion in a heterogeneous midbrain dopamine

system. neuropharmacology, 76:353   359.

lane, s. h., handelman, d. a., gelfand, j. j. (1992). theory and development of higher-order cmac neural

networks. ieee control systems 12 (2):23   30.

lang, k. j., waibel, a. h., hinton, g. e. (1990). a time-delay neural network architecture for isolated word

recognition. neural networks, 3:33   43.

lange, s., gabel, t., and riedmiller, m. (2012). batch id23. in wiering and van otterlo

(eds.) id23: state-of-the art, pp. 45   73. springer berlin heidelberg.

lecun, y. (1985). une procdure d   apprentissage pour rseau a seuil asymmetrique (a learning scheme for

asymmetric threshold networks). in proceedings of cognitiva 85, paris, france.

lecun, y., bottou, l., bengio, y., and ha   ner, p. (1998). gradient-based learning applied to document

recognition. proceedings of the ieee, 86(11):2278   2324.

legenstein, r. and andw. maass, d. p. (2008). a learning theory for reward-modulated spike-timing-dependent

plasticity with application to biofeedback. plos computational biology, 4(10).

levy, w. b. and steward, d. (1983). temporal contiguity requirements for long-term associative potentia-

410

references

tion/depression in thehippocampus. neuroscience, 8:791   797.

lewis, f. l., liu, d. (eds.).

(2013). id23 and approximate id145 for

feedback control. john wiley and sons.

lewis, r. l., howes, a., and singh, s. (2014). computational rationality: linking mechanism and behavior

through utility maximization. topics in cognitive science, 6(2):279   311.

li, l. (2012). sample complexity bounds of exploration. in wiering and van otterlo (eds.) reinforcement

learning: state-of-the art, pp. 175   204. springer berlin heidelberg.

li, l., chu, w., langford, j., and schapire, r. e. (2010). a contextual-bandit approach to personalized news
in proceedings of the 19th international conference on world wide web, pages

article recommendation.
661   670. acm.

lin, c.-s., kim, h. (1991). cmac-based adaptive critic self-learning control. ieee transactions on neural

networks, 2:530   533.

lin, l.-j. (1992).

self-improving reactive agents based on id23, planning and teaching.

machine learning, 8:293   321.

lin, l.-j., mitchell, t. (1992). id23 with hidden states.

in proceedings of the second
international conference on simulation of adaptive behavior: from animals to animats, pp. 271   280.
mit press, cambridge, ma.

littman, m. l. (1994). markov games as a framework for multi-agent id23. in proceedings of
the eleventh international conference on machine learning, pp. 157   163. morgan kaufmann, san francisco.

littman, m. l., cassandra, a. r., kaelbling, l. p. (1995). learning policies for partially observable environ-
ments: scaling up. in a. prieditis and s. russell (eds.), proceedings of the twelfth international conference
on machine learning, pp. 362   370. morgan kaufmann, san francisco.

littman, m. l., dean, t. l., kaelbling, l. p. (1995). on the complexity of solving markov decision problems.

in proceedings of the eleventh annual conference on uncertainty in arti   cial intelligence, pp. 394   402.

littman, m. l., sutton, r. s., singh (2002). predictive representations of state.
information processing systems (pp. 1555-1561).

in advances in neural

liu, j. s. (2001). monte carlo strategies in scienti   c computing. berlin, springer-verlag.

liu, w., pokharel, p. p., and principe, j. c. (2008). the kernel least-mean-square algorithm. ieee transactions

on signal processing 56 (2):543   554.

ljung, l. (1998). system identi   cation. in proch  azka, a., uhl      r, j., rayner, p. w. j., and kingsbury, n. g.,
editors, signal analysis and prediction, pages 163   173. springer science + business media new york, llc.

ljung, l., s  oderstrom, t. (1983). theory and practice of recursive identi   cation. mit press, cambridge,

ma.

ljungberg, t., apicella, p., and schultz, w. (1992). responses of monkey dopamine neurons during learning

of behavioral reactions. journal of neurophysiology, 67(1):145   163.

lovejoy, w. s. (1991). a survey of algorithmic methods for partially observed id100.

annals of operations research, 28:47   66.

luce, d. (1959). individual choice behavior. wiley, new york.

ludvig, e. a., bellemare, m. g., and pearson, k. g. (2011). a primer on id23 in the
brain: psychological, computational, and neural perspectives.
in alonso, e. and mondrag  on, e., editors,
computational neuroscience for advancing arti   cial intelligence: models, methods and applications, pages
111   44. medical information science reference, hershey pa.

ludvig, e. a., sutton, r. s., and kehoe, e. j. (2008). stimulus representation and the timing of reward-

prediction errors in models of the dopamine system. neural computation, 20(12):3034   3054.

ludvig, e. a., sutton, r. s., and kehoe, e. j. (2012). evaluating the td model of classical conditioning.

learning & behavior, 40(3):305   319.

machado, a. (1997). learning the temporal dynamics of behavior. psychological review, 104(2):241   265.

mackintosh, n. j. (1975). a theory of attention: variations in the associability of stimuli with reinforcement.

psychological review, 82(4):276   298.

references

411

mackintosh, n. j. (1983). conditioning and associative learning. oxford: clarendon press.

maclin, r., and shavlik, j. w. (1994).

in
proceedings of the twelfth national conference on arti   cial intelligence, pp. 694   699. aaai press, menlo
park, ca.

incorporating advice into agents that learn from reinforcements.

maei, h. r. (2011). gradient temporal-di   erence learning algorithms. phd thesis, university of alberta.

maei, h. r., and sutton, r. s. (2010). gq(  ): a general gradient algorithm for temporal-di   erence prediction
learning with eligibility traces. in proceedings of the third conference on arti   cial general intelligence,
pp. 91   96.

maei, h. r., szepesv  ari, cs., bhatnagar, s., precup, d., silver, d., and sutton, r. s. (2009). convergent
temporal-di   erence learning with arbitrary smooth function approximation. in advances in neural infor-
mation processing systems, pp. 1204   1212.

maei, h. r., szepesv  ari, cs., bhatnagar, s., and sutton, r. s. (2010). toward o   -policy learning control
with function approximation. in proceedings of the 27th international conference on machine learning,
pp. 719   726).

mahadevan, s. (1996). average reward id23: foundations, algorithms, and empirical results.

machine learning, 22:159   196.

mahadevan, s., liu, b., thomas, p., dabney, w., giguere, s., jacek, n., gemp, i., liu, j. (2014). proximal
id23: a new theory of sequential decision making in primal-dual spaces. arxiv preprint
arxiv:1405.6757.

mahadevan, s., and connell, j. (1992). automatic programming of behavior-based robots using reinforcement

learning. arti   cial intelligence, 55:311   365.

mahmood, a. r. (2017). incremental o   -policy id23 algorithms. university of alberta

phd thesis.

mahmood, a. r., and sutton, r. s. (2015). o   -policy learning based on weighted importance sampling with
linear computational complexity. in proceedings of the 31st conference on uncertainty in arti   cial intelli-
gence, amsterdam, netherlands.

mahmood, a. r., sutton, r. s., degris, t., and pilarski, p. m. (2012). tuning-free step-size adaptation. in
acoustics, speech and signal processing (icassp), 2012 ieee international conference on (pp. 2121   
2124). ieee.

mahmood, a. r., yu, h, sutton, r. s. (2017). multi-step o   -policy learning without importance sampling

ratios. arxiv 1702.03006.

mahmood, a. r., van hasselt, h., and sutton, r. s. (2014). weighted importance sampling for o   -policy

learning with linear function approximation. advances in neural information processing systems 27.

marbach, p., tsitsiklis, j. n. (2001). simulation-based optimization of markov reward processes. ieee trans-

actions on automatic control 46 (2), 191   209. also mit technical report lids-p-2411 (1998).

markey, k. l. (1994). e   cient learning of multiple degree-of-freedom control problems with quasi-independent
q-agents. in m. c. mozer, p. smolensky, d. s. touretzky, j. l. elman, and a. s. weigend (eds.), proceedings
of the 1990 connectionist models summer school. erlbaum, hillsdale, nj.

markram, h., l  ubke, j., frotscher, m., and sakmann, b. (1997). regulation of synaptic e   cacy by coincidence

of postsynaptic aps and epsps. science, 275:213   215.

mart    nez, j. f. and   ipek, e. (2009). dynamic multicore resource management: a machine learning approach.

micro, ieee, 29(5):8   17.

mataric, m. j. (1994). reward functions for accelerated learning.

in machine learning: proceedings of the

eleventh international conference, pages 181   189.

matsuda, w., furuta, t., nakamura, k. c., hioki, h., fujiyama, f., arai, r., and kaneko, t. (2009). sin-
gle nigrostriatal dopaminergic neurons form widely spread and highly dense axonal arborizations in the
neostriatum. the journal of neuroscience, 29(2):444   453.

mazur, j. e. (1994). learning and behavior, 3rd ed. prentice-hall, englewood cli   s, nj.

mccallum, a. k. (1993). overcoming incomplete perception with utile distinction memory.

in proceedings
of the tenth international conference on machine learning, pp. 190   196. morgan kaufmann, san mateo,

412

ca.

references

mccallum, a. k. (1995). id23 with selective perception and hidden state. ph.d. thesis,

university of rochester, rochester, ny.

mccloskey, m., and cohen, n. j. (1989). catastrophic interference in connectionist networks: the sequential

learning problem. psychology of learning and motivation 24, 109   165.

mcculloch, w. s., and pitts, w. (1943). a logical calculus of the ideas immanent in nervous activity. bulletin

of mathematical biophysics 5 (4):115   133.

mcmahan, h. b., gordon, g. j. (2005). fast exact planning in id100. in proceedings of

the international conference on automated planning and scheduling (pp. 151-160).

melo, f. s., meyn, s. p., ribeiro, m. i. (2008). an analysis of id23 with function approxima-

tion. in proceedings of the 25th international conference on machine learning (pp. 664   671).

mendel, j. m. (1966). a survey of learning control systems. isa transactions, 5:297   303.

mendel, j. m., mclaren, r. w. (1970). id23 control and pattern recognition systems.
in j. m. mendel and k. s. fu (eds.), adaptive, learning and pattern recognition systems: theory and
applications, pp. 287   318. academic press, new york.

michie, d. (1961). trial and error. in s. a. barnett and a. mclaren (eds.), science survey, part 2, pp. 129   145.

penguin, harmondsworth.

michie, d. (1963). experiments on the mechanisation of game learning. 1. characterization of the model and

its parameters. computer journal, 1:232   263.

michie, d. (1974). on machine intelligence. edinburgh university press, edinburgh.

michie, d., chambers, r. a. (1968). boxes: an experiment in adaptive control.

in e. dale and d. michie

(eds.), machine intelligence 2, pp. 137   152. oliver and boyd, edinburgh.

miller, r. (1981). meaning and purpose in the intact brain: a philosophical, psychological, and biological

account of conscious process. clarendon press, oxford.

miller, w. t., an, e., glanz, f., carter, m. (1990). the design of cmac neural networks for control. adaptive

and learning systems 1 :140   145.

miller, w. t., glanz, f. h. (1996). unh cmac verison 2.1: the university of new hampshire implementation
of the cerebellar model arithmetic computer - cmac. robotics laboratory technical report, university
of new hampshire, durham, new hampshire.

miller, s., williams, r. j. (1992). learning to control a bioreactor using a neural net dyna-q system.

in
proceedings of the seventh yale workshop on adaptive and learning systems, pp. 167   172. center for
systems science, dunham laboratory, yale university, new haven.

miller, w. t., scalera, s. m., kim, a. (1994). neural network control of dynamic balance for a biped walking
robot. in proceedings of the eighth yale workshop on adaptive and learning systems, pp. 156   161. center
for systems science, dunham laboratory, yale university, new haven.

minton, s. (1990). quantitative results concerning the utility of explanation-based learning. arti   cial intelli-

gence 42 (2   3), 363   391.

minsky, m. l. (1954). theory of neural-analog reinforcement systems and its application to the brain-model

problem. ph.d. thesis, princeton university.

minsky, m. l. (1961). steps toward arti   cial intelligence. proceedings of the institute of radio engineers,
49:8   30. reprinted in e. a. feigenbaum and j. feldman (eds.), computers and thought, pp. 406   450.
mcgraw-hill, new york, 1963.

minsky, m. l. (1967). computation: finite and in   nite machines. prentice-hall, englewood cli   s, nj.

mnih, v., kavukcuoglu, k., silver, d., graves, a., antonoglou, i., wierstra, d., and riedmiller, m. (2013).

playing atari with deep id23. arxiv preprint arxiv:1312.5602.

mnih, v., kavukcuoglu, k., silver, d., rusu, a. a., veness, j., bellemare, m. g., graves, a., riedmiller, m.,
fidjeland, a. k., ostrovski, g., petersen, s., beattie, c., sadik, a., antonoglou, i., king, h., kumaran, d.,
wierstra, d., legg, s., and hassabis, d. (2015). human-level control through deep id23.
nature, 518(7540):529   533.

references

413

modayil, j., and sutton, r. s. (2014). prediction driven behavior: learning predictions that drive    xed

responses. in aaai-14 workshop on arti   cial intelligence and robotics, quebec city, canada.

modayil, j., white, a., and sutton, r. s. (2014). multi-timescale nexting in a id23 robot.

adaptive behavior, 22(2):146   160.

monahan, g. e. (1982). state of the art   a survey of partially observable id100: theory,

models, and algorithms. management science 28 (1), 1   16.

montague, p. r., dayan, p., nowlan, s. j., pouget, a., and sejnowski, t. j. (1992). using aperiodic rein-
forcement for directed self-organization during development. in advances in neural information processing
systems 5, pages 969   976.

montague, p. r., dayan, p., person, c., and sejnowski, t. j. (1995). bee foraging in uncertain environments

using predictive hebbian learning. nature, 377(6551):725   728.

montague, p. r., dayan, p., sejnowski, t. j. (1996). a framework for mesencephalic dopamine systems based

on predictive hebbian learning. journal of neuroscience, 16:1936   1947.

montague, p. r., dolan, r. j., friston, k. j., and dayan, p. (2012). computational psychiatry. trends in

cognitive sciences 16 (1):72   80.

montague, p. r. and sejnowski, t. j. (1994). the predictive brain: temporal coincidence and temporal order

in synaptic learningmechanisms. learning & memory, 1:1   33.

moore, a. w. (1990). e   cient memory-based learning for robot control. ph.d. thesis, university of

cambridge.

moore, a. w. (1994). the parti-game algorithm for variable resolution id23 in multidi-
in j. d. cohen, g. tesauro and j. alspector (eds.), advances in neural information

mensional spaces.
processing systems: proceedings of the 1993 conference, pp. 711   718. morgan kaufmann, san francisco.

moore, a. w., atkeson, c. g. (1993). prioritized sweeping: id23 with less data and less real

time. machine learning, 13:103   130.

moore, a. w., schneider, j., and deng, k. (1997). e   cient locally weighted polynomial regression predictions.

in proceedings of the 1997 international machine learning conference. morgan kaufmann.

moore, j. w. and blazis, d. e. j. (1989).

implementation of the sutton-barto-desmond model.
models of plasticity, pages 187   207. academic press, san diego, ca.

simulation of a classically conditioned response: a cerebellar
in byrne, j. h. and berry, w. o., editors, neural

moore, j. w., choi, j.-s., and brunzell, d. h. (1998). predictive timing under temporal uncertainty: the time
derivative model of the conditioned response. in rosenbaum, d. a. and collyer, c. e., editors, timing of
behavior, pages 3   34. mit press, cambridge, ma.

moore, j. w., desmond, j. e., berthier, n. e., blazis, e. j., sutton, r. s., and barto, a. g. (1986). simulation
of the classically conditioned nictitating membrane response by a neuron-like adaptive element: i. response
topography, neuronal    ring, and interstimulus intervals. behavioural brain research, 21:143   154.

moore, j. w., marks, j. s., castagna, v. e., and polewan, r. j. (2001). parameter stability in the td model

of complex cr topographies. society for neuroscience abstract 642.2.

moore, j. w. and schmajuk, n. a. (2008). kamin blocking. scholarpedia, 3(5):3542.

moore, j. w. and stickney, k. j. (1980). formation of attentional-associative networks in real time:role of the

hippocampus and implications for conditioning. physiological psychology, 8(2):207   217.

mukundan, j. and mart    nez, j. f. (2012). morse: multi-objective recon   gurable self-optimizing memory
scheduler. in ieee 18th international symposium on high performance computer architecture (hpca),
pages 1   12.

m  uller, m. (2002). computer go. arti   cial intelligence, 134(1):145   179.

munos, r., stepleton, t., harutyunyan, a., and bellemare, m. (2016). safe and e   cient o   -policy reinforcement

learning. in advances in neural information processing systems, pp. 1046   1054.

naddaf, y. (2010). game-independent ai agents for playing atari 2600 console games. phd thesis, university

of alberta.

narendra, k. s., thathachar, m. a. l. (1974). learning automata   a survey. ieee transactions on systems,

414

references

man, and cybernetics, 4:323   334.

narendra, k. s., thathachar, m. a. l. (1989). learning automata: an introduction. prentice-hall, englewood

cli   s, nj.

narendra, k. s. and wheeler, r. m. (1983). an n-player sequential stochastic game with identical payo   s.

ieee transactions on systems, man, and cybernetics, 13:1154   1158.

narendra, k. s., wheeler, r. m. (1986). decentralized learning in    nite markov chains. ieee transactions

on automatic control, ac31(6):519   526.

nedi  c, a., bertsekas, d. p. (2003). least squares policy evaluation algorithms with linear function approxima-

tion. discrete event dynamic systems 13 (1-2):79   110.

ng, a. y. (2003). shaping and policy search in id23. phd thesis, university of california,

berkeley, berkeley, ca.

ng, a. y., harada, d., and russell, s. (1999). policy invariance under reward transformations: theory
in bratko, i. and dzeroski, s., editors, proceedings of the sixteenth

and application to reward shaping.
international conference on machine learning (icml 1999), volume 99, pp. 278   287.

ng, a. y., and russell, s. j. (2000). algorithms for inverse id23. in international conference

on machine learning, pp. 663   670.

nie, j., haykin, s. (1996). a dynamic channel assignment policy through id24. crl report 334.

communications research laboratory, mcmaster university, hamilton, ontario.

niv, y. (2009). id23 in the brain. journal of mathematical psychology, 53(3):139   154.

niv, y., daw, n. d., and dayan, p. (2005). how fast to work: response vigor, motivation and tonic dopamine.
in yeiss, y., sch  olkopft, b., and platt, j., editors, advances in neural information processing systems 18
(nips 2005), pages 1019   1026. mit press, cambridge, ma.

niv, y., daw, n. d., joel, d., and dayan, p. (2007). tonic dopamine: opportunity costs and the control of

response vigor. psychopharmacology, 191(3):507   520.

niv, y., joel, d., and dayan, p. (2006). a normative perspective on motivation. trends in cognitive sciences,

10(8):375   381.

nouri, a., and littman, m. l. (2009). multi-resolution exploration in continuous spaces. in advances in neural

information processing systems (pp. 1209   1216).

now  e, a., vrancx, p., and hauwere, y.-m. d. (2012). game theory and multi-agent id23.
in wiering, m. and van otterlo, m., editors, id23: state-of-the-art, pages 441   467.
springer-verlag, berlin.

nutt, d. j., lingford-hughes, a., erritzoe, d., and stokes, p. r. a. (2015). the dopamine theory of addiction:

40 years of highs and lows. nature reviews neuroscience, 16:305   312.

o   doherty, j. p., dayan, p., friston, k., critchley, h., and dolan, r. j. (2003). temporal di   erence models

and reward-related learning in the human brain. neuron, 38(2):329   337.

o   doherty, j. p., dayan, p., schultz, j., deichmann, r., friston, k., and dolan, r. j. (2004). dissociable roles

of ventral and dorsal striatum in instrumental conditioning. science, 304(5669):452   454.

  olafsd  ottir, h. f., barry, c., saleem, a. b., hassabis, d., and spiers, h. j. (2015). hippocampal place cells

construct reward related sequences through unexplored space. elife, 4:e06063.

oh, j., guo, x., lee, h., lewis, r. l., and singh, s. (2015). action-conditional video prediction using deep

networks in atari games. in advances in neural information processing systems, pages 2845   2853.

olds, j. and milner, p. (1954). positive reinforcement produced by electrical stimulation of the septal area and

other regions of rat brain. journal of comparative and physiological psychology, 47(6):419   427.

oliehoek, f. a. (2012). decentralized pomdps. in wiering and van otterlo (eds.) id23:

state-of-the art, pp. 471   503. springer berlin heidelberg.

o   reilly, r. c. and frank, m. j. (2006). making working memory work: a computational model of learning in

the prefrontal cortex and basal ganglia. neural computation, 18(2):283   328.

o   reilly, r. c., frank, m. j., hazy, t. e., and watz, b. (2007). pvlv: the primary value and learned value

pavlovian learning algorithm. behavioral neuroscience, 121(1):31   49.

references

415

omohundro, s. m. (1987). e   cient algorithms with neural network behavior. technical report, department

of computer science, university of illinois at urbana-champaign.

orenstein, j. a. (1982). multidimensional tries used for associative searching. information processing letters

14 (4):150   157.

ormoneit, d., and sen,   s. (2002). kernel-based id23. machine learning 49 (2   3):161   178.
oudeyer, p.-y. and kaplan, f. (2007). what is intrinsic motivation? a typology of computational approaches.

frontiers in neurorobotics, 1.

oudeyer, p.-y., kaplan, f., and hafner, v. v. (2007).

intrinsic motivation systems for autonomous mental

development. ieee transactions on evolutionary computation, 11(2):265   286.

padoa-schioppa, c., and assad, j. a. (2006). neurons in the orbitofrontal cortex encode economic value. nature

441 (7090):223   226.

page, c. v. (1977). heuristics for signature table analysis as a pattern recognition technique. ieee transactions

on systems, man, and cybernetics, 7:77   86.

pagnoni, g., zink, c. f., montague, p. r., and berns, g. s. (2002). activity in human ventral striatum locked

to errors of reward prediction. nature neuroscience, 5(2):97   98.

pan, w.-x., schmidt, r., wickens, j. r., and hyland, b. i. (2005). dopamine cells respond to predicted events
during classical conditioning: evidence for eligibility traces in the reward-learning network. the journal of
neuroscience, 25(26):6235   6242.

park, j., kim, j., kang, d. (2005). an rls-based natural actor   critic algorithm for locomotion of a two-linked

robot arm. computational intelligence and security, 65   72.

parker, d. b. (1985). learning logic. ???

parks, p. c., militzer, j. (1991).

improved allocation of weights for associative memory storage in learning

control systems. ifac design methods of control systems, zurich, switzerland, 507   512.

parr, r. (1988). hierarchical control and learning for id100. phd thesis, university of

california at berkeley.

parr, r., russell, s. (1995). approximating optimal policies for partially observable stochastic domains.
in proceedings of the fourteenth international joint conference on arti   cial intelligence, pp. 1088   1094.
morgan kaufmann.

pavlov, p. i. (1927). conditioned re   exes. oxford university press, london.

pawlak, v. and kerr, j. n. d. (2008). dopamine receptor activation is required for corticostriatal spike-timing-

dependent plasticity. the journal of neuroscience, 28(10):2435   2446.

pawlak, v., wickens, j. r., kirkwood, a., and kerr, j. n. d. (2010). timing is not everything: neuromodulation

opens the stdp gate. frontiers in synaptic neuroscience, 2.

pearce, j. m. and hall, g. (1980). a model for pavlovian learning: variation in the e   ectiveness of conditioning

but not unconditioned stimuli. psychological review, 87(6):532   552.

pearl, j. (1984). heuristics: intelligent search strategies for computer problem solving. addison-wesley,

reading, ma.

pearl, j. (1995). causal diagrams for empirical research. biometrika, 82(4), 669-688.

pecevski, d., maass, w., and legenstein, r. a. (2007). theoretical analysis of learning with reward-modulated

spike-timing-dependent plasticity. in advances in neural information processing systems, pp. 881   888.

peng, j. (1993). e   cient id145-based learning for control. ph.d. thesis, northeastern

university, boston.

peng, j. (1995). e   cient memory-based id145. in 12th international conference on machine

learning, pp. 438   446.

peng, j., williams, r. j. (1993). e   cient learning and planning within the dyna framework. adaptive

behavior, 1(4):437   454.

peng, j., williams, r. j. (1994).

in w. w. cohen and h. hirsh (eds.),
proceedings of the eleventh international conference on machine learning, pp. 226   232. morgan kaufmann,
san francisco.

incremental multi-step id24.

416

references

peng, j., williams, r. j. (1996). incremental multi-step id24. machine learning, 22:283   290.

perkins, t. j., pendrith, m. d. (2002). on the existence of    xed points for id24 and sarsa in partially

observable domains. in proceedings of the international conference on machine learning, pp. 490   497.

perkins, t. j., precup, d. (2003). a convergent form of approximate policy iteration. in advances in neural

information processing systems, proceedings of the 2002 conference, pp. 1595   1602.

peters, j. and b  uchel, c. (2010). neural representations of subjective reward value. behavioral brain research,

213(2):135   141.

peters, j., schaal, s. (2008). natural actor   critic. neurocomputing 71 (7), 1180   1190.

peters, j., vijayakumar, s., schaal, s. (2005). natural actor   critic.

in european conference on machine

learning (pp. 280   291). springer berlin heidelberg.

peterson, g. b. (2004). a day of great illumination: b.f. skinner   s discovery of shaping.

journal of the

experimental analysis of behavior, 82(3):317   28.

pezzulo, g., van der meer, m. a. a., lansink, c. s., and pennartz, c. m. a. (2014).

internally generated

sequences in learning and executing goal-directed behavior. trends in cognitive science, 18(12):647   657.

pfei   er, b. e. and foster, d. j. (2013). hippocampal place-cell sequences depict future paths to remembered

goals. nature, 497(7447):74   79.

phansalkar, v. v., thathachar, m. a. l. (1995). local and global optimization algorithms for generalized

learning automata. neural computation, 7:950   973.

poggio, t., girosi, f. (1989). a theory of networks for approximation and learning. a.i. memo 1140. arti   cial

intelligence laboratory, massachusetts institute of technology, cambridge, ma.

poggio, t., girosi, f. (1990). id173 algorithms for learning that are equivalent to multilayer networks.

science, 247:978   982.

polyak, b. t. (1990). new stochastic approximation type procedures. automat. i telemekh 7 (98-107), 2 (in

russian).

polyak, b. t., juditsky, a. b. (1992). acceleration of stochastic approximation by averaging. siam journal

on control and optimization 30 (4), 838-855.

powell, m. j. d. (1987). radial basis functions for multivariate interpolation: a review. in j. c. mason and

m. g. cox (eds.), algorithms for approximation, pp. 143   167. clarendon press, oxford.

powell, w. b. (2011). approximate id145: solving the curses of dimensionality, second

edition. john wiley and sons.

powers, w. t. (1973). behavior: the control of perception. aldine de gruyter, chicago. 2nd expanded

edition 2005.

precup, d., sutton, r. s., dasgupta, s. (2001). o   -policy temporal-di   erence learning with function approxi-

mation. in proceedings of the 18th international conference on machine learning.

precup, d., sutton, r. s., paduraru, c., koop, a., and singh, s. (2005). o   -policy learning with options and

recognizers. in advances in neural processing systems, pp. 1097   1104.

precup, d., sutton, r. s., singh, s. (2000). eligibility traces for o   -policy policy evaluation. in proceedings of

the 17th international conference on machine learning, pp. 759   766. morgan kaufmann.

puterman, m. l. (1994). markov decision problems. wiley, new york.

puterman, m. l., shin, m. c. (1978). modi   ed policy iteration algorithms for discounted markov decision

problems. management science, 24:1127   1137.

quartz, s., dayan, p., montague, p. r., and sejnowski, t. j. (1992). expectation learning in the brain using

di   use ascending connections. in society for neuroscience abstracts, volume 18, page 1210.

randl  v, j. and alstr  m, p. (1998). learning to drive a bicycle using id23 and shaping. in

proceedings of the fifteenth international conference on machine learning, pages 463   471.

rangel, a., camerer, c., and montague, p. r. (2008). a framework for studying the neurobiology of value-based

decision making. nature reviews neuroscience, 9(7):545   556.

rangel, a. and hare, t. (2010). neural computations associated with goal-directed choice. current opinion in

references

417

neurobiology, 20(2):262   270.

ratcli   , r. (1990). connectionist models of recognition memory: constraints imposed by learning and forgetting

functions. psychological review 97 (2), 285   308.

reddy, g., celani, a., sejnowski, t. j., and vergassola, m. (2016). learning to soar in turbulent environments.

proceedings of the national academy of sciences, 113(33):e4877   e4884.

redgrave, p. and gurney, k. (2006). the short-latency dopamine signal: a role in discovering novel actions?

nature reviews neuroscience, 7:967   975.

redish, d. a. (2004). addiction as a computational process gone awry. science, 306(5703):1944   1947.

reetz, d. (1977). approximate solutions of a discounted markovian decision process. bonner mathematische

schriften, 98:77   92.

rescorla, r. a. and wagner, a. r. (1972). a theory of pavlovian conditioning: variations in the e   ectiveness
of reinforcement and nonreinforcement. in black, a. h. and prokasy, w. f., editors, classical conditioning
ii, pages 64   99. appleton-century-crofts, new york.

revusky, s. and garcia, j. (1970). learned associations over long delays. in bower, g., editor, the psychology

of learning and motivation, volume 4, pages 1   84. academic press, inc., new york.

reynolds, j. n. j. and wickens, j. r. (2002). dopamine-dependent plasticity of corticostriatal synapses. neural

networks, 15(4):507   521.

ring, m. b. (1994). continual learning in reinforcement environments. ph.d. thesis, university of texas,

austin.

ripley, b. d. (2007). pattern recognition and neural networks. cambridge university press.

rivest, r. l., schapire, r. e. (1987). diversity-based id136 of    nite automata. in proceedings of the twenty-
eighth annual symposium on foundations of computer science, pp. 78   87. computer society press of the
ieee, washington, dc.

rixner, s. (2004). memory controller optimizations for web servers.

in proceedings of the 37th annual

ieee/acm international symposium on microarchitecture, pages 355   366. ieee computer society.

robbins, h. (1952). some aspects of the sequential design of experiments. bulletin of the american mathe-

matical society, 58:527   535.

robertie, b. (1992). carbon versus silicon: matching wits with td-gammon. inside backgammon, 2:14   22.

roesch, m. r., calu, d. j., and schoenbaum, g. (2007). dopamine neurons encode the better option in rats

deciding between di   erently delayed or sized rewards. nature neuroscience, 10(12):1615   1624.

romo, r. and schultz, w. (1990). dopamine neurons of the monkey midbrain: contingencies of responses to

active touch during self-initiated arm movements. journal of neurophysiology, 63(3):592   624.

rosenblatt, f. (1962). principles of neurodynamics: id88s and the theory of brain mechanisms. spartan

books, washington, dc.

ross, s. (1983). introduction to stochastic id145. academic press, new york.

ross, t. (1933). machines that think. scienti   c american, pages 206   208.

rubinstein, r. y. (1981). simulation and the monte carlo method. wiley, new york.

rumelhart, d. e., hinton, g. e., williams, r. j. (1986). learning internal representations by error propaga-
tion. in d. e. rumelhart and j. l. mcclelland (eds.), parallel distributed processing: explorations in the
microstructure of cognition, vol. i, foundations. bradford/mit press, cambridge, ma.

rummery, g. a. (1995). problem solving with id23. ph.d. thesis, cambridge university.

rummery, g. a., niranjan, m. (1994). on-line id24 using connectionist systems. technical report

cued/f-infeng/tr 166. engineering department, cambridge university.

ruppert, d. (1988). e   cient estimations from a slowly convergent robbins-monro process. cornell university

operations research and industrial engineering technical report no. 781.

russell, s., norvig, p. (2010). arti   cial intelligence: a modern approach, 3rd edition. prentice-hall, englewood

cli   s, nj.

rust, j. (1996). numerical id145 in economics.

in h. amman, d. kendrick, and j. rust

418

references

(eds.), handbook of computational economics, pp. 614   722. elsevier, amsterdam.

ryan, r. m. and deci, e. l. (2000). intrinsic and extrinsic motivations: classic de   nitions and new directions.

contemporary educational psychology, 25(1):54   67.

saddoris, m. p., cacciapaglia, f., wightmman, r. m., and carelli, r. m. (2015). di   erential dopamine release
dynamics in the nucleus accumbens core and shell reveal complementary signals for error prediction and
incentive motivation. the journal of neuroscience, 35(33):11572   11582.

saksida, l. m., raymond, s. m., and touretzky, d. s. (1997). shaping robot behavior using principles from

instrumental conditioning. robotics and autonomous systems, 22(3):231   249.

samuel, a. l. (1959). some studies in machine learning using the game of checkers. ibm journal on research
and development, 3:211   229. reprinted in e. a. feigenbaum and j. feldman (eds.), computers and
thought, pp. 71   105. mcgraw-hill, new york, 1963.

samuel, a. l. (1967). some studies in machine learning using the game of checkers. ii   recent progress. ibm

journal on research and development, 11:601   617.

schaal, s., and atkeson, c. g. (1994). robot juggling: implementation of memory-based learning.

ieee

control systems 14 (1):57   71.

schmajuk, n. a. (2008). computational models of classical conditioning. scholarpedia, 3(3):1664.

schmidhuber, j. (1991a). adaptive con   dence and adaptive curiosity. technical report fki-149-91, institut

f  ur informatik, technische universit  at m  unchen, arcisstr. 21, 800 m  unchen 2, germany.

schmidhuber, j. (1991b). a possibility for implementing curiosity and boredom in model-building neural
controllers. in from animals to animats: proceedings of the first international conference on simulation
of adaptive behavior, pages 222   227, cambridge, ma. mit press.

schmidhuber, j. (2009). driven by compression progress: a simple principle explains essential aspects of
subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes.
in pezzulo, g., butz, m. v., sigaud, o., and baldassarre, g., editors, anticipatory behavior in adaptive
learning systems. from psychological theories to arti   cial cognitive systems, pages 48   76. springer,
berlin.

schmidhuber, j. (2015). deep learning in neural networks: an overview. neural networks 61, 85   117.

schmidhuber, j., storck, j., and hochreiter, s. (1994). reinforcement driven information acquisition in non-
deterministic environments. technical report, fakult  at f  ur informatik, technische universit  at m  unchen,
m  unchen, germany.

schultz, d. g., melsa, j. l. (1967). state functions and linear control systems. mcgraw-hill, new york.

schultz, w. (1998). predictive reward signal of dopamine neurons. journal of neurophysiology, 80:1   27.

schultz, w., apicella, p., and ljungberg, t. (1993). responses of monkey dopamine neurons to reward and
conditioned stimuli during successive steps of learning a delayed response task. journal of neuroscience
13 (3):900   913.

schultz, w., dayan, p., montague, p. r. (1997). a neural substrate of prediction and reward.

science,

275:1593   1598.

schultz, w. and romo, r. (1990). dopamine neurons of the monkey midbrain: contingencies of responses to

stimuli eliciting immediate behavioral reactions. journal of neurophysiology, 63(3):607   624.

schultz, w., romo, r., ljungberg, t., mirenowicz, j., hollerman, j. r., and dickinson, a. (1995). reward-
in houk, davis, and beiser (eds.) models of information

related signals carried by dopamine neurons.
processing in the basal ganglia, pp. 233   248. mit press cambridge ma.

schumaker, l. l. (1976). fitting surfaces to scattered data. university of texas at austin, dept. of mathe-

matics.

schwartz, a. (1993). a id23 method for maximizing undiscounted rewards.

in proceedings
of the tenth international conference on machine learning, pp. 298   305. morgan kaufmann, san mateo,
ca.

schweitzer, p. j., seidmann, a. (1985). generalized polynomial approximations in markovian decision processes.

journal of mathematical analysis and applications, 110:568   582.

references

419

selfridge, o. g. (1978). tracking and trailing: adaptation in movement strategies. technical report, bolt

beranek and newman, inc. unpublished report.

selfridge, o. g. (1984). some themes and primitives in ill-de   ned systems.

in selfridge, o. g., rissland,
e. l., and arbib, m. a., editors, adaptive control of ill-de   ned systems, pages 21   26. plenum press, ny.
proceedings of the nato advanced research institute on adaptive control of ill-de   ned systems, nato
conference series ii, systems science, vol. 16.

selfridge, o. j., sutton, r. s., barto, a. g. (1985). training and tracking in robotics.

in a. joshi (ed.),
proceedings of the ninth international joint conference on arti   cial intelligence, pp. 670   672. morgan
kaufmann, san mateo, ca.

seo, h., barraclough, d., and lee, d. (2007). dynamic signals related to choices and outcomes in the dorsolateral

prefrontal cortex. cerebral cortex, 17(suppl 1):110   117.

seung, h. s. (2003). learning in spiking neural networks by reinforcement of stochastic synaptic transmission.

neuron, 40(6):1063   1073.

shah, a. (2012). psychological and neuroscienti   c connections with id23.

in wiering, m.
and van otterlo, m., editors, id23: state of the art, pages 507   537. springer-verlag,
berlin.

shannon, c. e. (1950). programming a computer for playing chess. philosophical magazine, 41:256   275.

shannon, c. e. (1951). presentation of a maze-solving machine.

in forester, h. v., editor, cybernetics.

transactions of the eighth conference, pages 173   180. josiah macy jr. foundation.

shannon, c. e. (1952).    theseus    maze-solving mouse. http://cyberneticzoo.com/mazesolvers/1952--theseus-

maze-solving-mouse--claude-shannon-american/.

shelton, c. r. (2001). importance sampling for id23 with multiple objectives. phd thesis,

massachusetts institute of technology.

shepard, d. (1968). a two-dimensional interpolation function for irregularly-spaced data. in proceedings of the

23rd acm national conference, pp. 517   524. acm.

sherman, j., morrison, w. j. (1949). adjustment of an inverse matrix corresponding to changes in the elements
of a given column or a given row of the original matrix (abstract). annals of mathematical statistics 20 :621.

shewchuk, j., dean, t. (1990). towards learning time-varying functions with high input dimensionality.

in
proceedings of the fifth ieee international symposium on intelligent control, pp. 383   388. ieee computer
society press, los alamitos, ca.

shimansky, y. p. (2009). biologically plausible learning in neural networks: a lesson from bacterial chemotaxis.

biological cybernetics, 101(5-6):379   385.

si, j., barto, a., powell, w., wunsch, d. (eds.).

(2004). handbook of learning and approximate dynamic

programming. john wiley and sons.

silver, d. (2009). id23 and simulation based search in the game of go. university of alberta

doctoral dissertation.

silver, d., huang, a., maddison, c. j., guez, a., sifre, l., van den driessche, g., schrittwieser, j., antonoglou,
i., panneershelvam, v., lanctot, m., dieleman, s., grewe, d., nham, j., kalchbrenner, n., sutskever, i.,
lillicrap, t., leach, m., kavukcuoglu, k., graepel, t., and hassabis, d. (2016). mastering the game of go
with deep neural networks and tree search. nature, 529(7587):484   489.

silver, d., lever, g., heess, n., degris, t., wierstra, d., riedmiller, m. (2014). deterministic policy gradient
algorithms. in proceedings of the 31st international conference on machine learning (icml-14) (pp. 387   
395).

s  im  sek,   o., alg  orta, s., and kothiyal, a. (2016). why most decisions are easy in tetris-and perhaps in other

sequential decision problems, as well. proceedings of 33rd international conference on machine learning.

simon, h. (2000). lecture at the earthware symposium, carnegie mellon university. https://www.youtube.com/watch?v=ezhyi-

8dbjc

singh, s. p. (1992a). id23 with a hierarchy of abstract models. in proceedings of the tenth

national conference on arti   cial intelligence, pp. 202   207. aaai/mit press, menlo park, ca.

singh, s. p. (1992b). scaling id23 algorithms by learning variable temporal resolution models.

420

references

in proceedings of the ninth international machine learning conference, pp. 406   415. morgan kaufmann,
san mateo, ca.

singh, s. p. (1993). learning to solve markovian decision processes. ph.d. thesis, university of massachusetts,

amherst. appeared as cmpsci technical report 93-77.

singh, s. p. (ed.) (2002). special double issue on id23, machine learning 49 (2/3).

singh, s., barto, a. g., and chentanez, n. (2005).

in ad-
vances in neural information processing systems 17: proceedings of the 2004 conference, pages 1281   1288,
cambridge ma. mit press.

intrinsically motivated id23.

singh, s. p., bertsekas, d. (1997). id23 for dynamic channel allocation in cellular telephone
in advances in neural information processing systems: proceedings of the 1996 conference,

systems.
pp. 974   980. mit press, cambridge, ma.

singh, s. p., jaakkola, t., jordan, m. i. (1994). learning without state-estimation in partially observable
markovian decision problems. in w. w. cohen and h. hirsch (eds.), proceedings of the eleventh interna-
tional conference on machine learning, pp. 284   292. morgan kaufmann, san francisco.

singh, s., jaakkola, t., littman, m. l., and szepesvri, c. (2000). convergence results for single-step on-policy

reinforcement-learning algorithms. machine learning 38 (3), 287   308.

singh, s. p., jaakkola, t., jordan, m. i. (1995). reinforcement learing with soft state aggregation.

in
g. tesauro, d. s. touretzky, t. leen (eds.), advances in neural information processing systems: proceedings
of the 1994 conference, pp. 359   368. mit press, cambridge, ma.

singh, s., lewis, r. l., and barto, a. g. (2009). where do rewards come from? in taatgen, n. and van rijn,
h., editors, proceedings of the 31st annual conference of the cognitive science society, pages 2601   2606.
cognitive science society.

singh, s., lewis, r. l., barto, a. g., and sorg, j. (2010). intrinsically motivated id23: an
evolutionary perspective. ieee transactions on autonomous mental development, 2(2):7082. special issue
on active learning and intrinsically motivated exploration in robots: advances and challenges.

singh, s. p., sutton, r. s. (1996). id23 with replacing eligibility traces. machine learning,

22:123   158.

sivarajan, k. n., mceliece, r. j., ketchum, j. w. (1990). dynamic channel assignment in cellular radio. in

proceedings of the 40th vehicular technology conference, pp. 631   637.

skinner, b. f. (1938). the behavior of organisms: an experimental analysis. appleton-century, new york.

skinner, b. f. (1958). reinforcement today. american psychologist, 13(3):94   99.

skinner, b. f. (1981). selection by consequences. science 213 (4507):501   504.

smith, k. s. and greybiel, a. m. (2013). a dual operator view of habitual behavior re   ecting cortical and

striatal dynamics. neuron, 79(2):361   374.

sofge, d. a., white, d. a. (1992). applied learning: optimal control for manufacturing. in d. a. white and
d. a. sofge (eds.), handbook of intelligent control: neural, fuzzy, and adaptive approaches, pp. 259   281.
van nostrand reinhold, new york.

sorg, j. d. (2011). the optimal reward problem:designing e   ective reward for bounded agents. phd thesis,

computer science and engineering, the university of michigan.

sorg, j., lewis, r. l., and singh, s. p. (2010). reward design via online gradient ascent. in advances in neural

information processing systems, pp. 2190   2198.

sorg, j., singh, s., and lewis, r. (2010). internal rewards mitigate agent boundedness. in proceedings of the

27th international conference on machine learning (icml), pages 1007   1014.

spaan, m. t. (2012). partially observable id100.

in wiering and van otterlo (eds.)

id23: state-of-the art, pp. 387   414. springer berlin heidelberg.

spence, k. w. (1947). the role of secondary reinforcement in delayed reward learning. psychological review,

54(1):1   8.

spong, m. w. (1994). swing up control of the acrobot. in proceedings of the 1994 ieee conference on robotics

and automation, pp. 2356-2361. ieee computer society press, los alamitos, ca.

references

421

srivastava, n., hinton, g., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2014). dropout: a simple way
to prevent neural networks from over   tting. the journal of machine learning research, 15(1):1929   1958.

staddon, j. e. r. (1983). adaptive behavior and learning. cambridge university press, cambridge.

stan   ll, c., and waltz, d. (1986). toward memory-based reasoning. communications of the acm 29 (12):1213   

1228.

steinberg, e. e., kei   in, r., boivin, j. r., witten, i. b., deisseroth, k., and janak, p. h. (2013). a causal link

between prediction errors, dopamine neurons and learning. nature neuroscience, 16(7):966   973.

sterling, p. and laughlin, s. (2015). principles of neural design. mit press, cambridge, ma.

storck, j., hochreiter, s., and schmidhuber, j. (1995). reinforcement-driven information acquisition in non-

deterministic environments. in proceedings of icann   95, paris, france, volume 2, pages 159   164.

sugiyama, m., hachiya, h., morimura, t. (2013). statistical id23: modern machine learning

approaches. chapman & hall/crc.

suri, r. e., bargas, j., and arbib, m. a. (2001). modeling functions of striatal dopamine modulation in learning

and planning. neuroscience, 103(1):65   85.

suri, r. e. and schultz, w. (1998). learning of sequential movements by neural network model with dopamine-

like reinforcement signal. experimental brain research, 121(3):350   354.

suri, r. e. and schultz, w. (1999). a neural network model with dopamine-like reinforcement signal that learns

a spatial delayed response task. neuroscience, 91(3):871   890.

sutton, r. s. (1978a). learning theory support for a single channel theory of the brain. unpublished report.

sutton, r. s. (1978b). single channel theory: a neuronal theory of learning. brain theory newsletter, 4:72   75.

center for systems neuroscience, university of massachusetts, amherst, ma.

sutton, r. s. (1978c). a uni   ed theory of expectation in classical and instrumental conditioning. bachelors

thesis, stanford university.

sutton, r. s. (1984). temporal credit assignment in id23. ph.d. thesis, university of

massachusetts, amherst.

sutton, r. s. (1988). learning to predict by the method of temporal di   erences. machine learning, 3:9   44.

sutton, r. s. (1990).

integrated architectures for learning, planning, and reacting based on approximating
in proceedings of the seventh international conference on machine learning,

id145.
pp. 216   224. morgan kaufmann, san mateo, ca.

sutton, r. s. (1991a). dyna, an integrated architecture for learning, planning, and reacting. sigart bulletin,

2:160   163. acm press.

sutton, r. s. (1991b). planning by incremental id145. in l. a. birnbaum and g. c. collins
(eds.), proceedings of the eighth international workshop on machine learning, pp. 353   357. morgan kauf-
mann, san mateo, ca.

sutton, r. s. (ed.) (1992). id23. kluwer academic press. reprinting of a special double

issue on id23, machine learning 8 (3/4).

sutton, r. s. (1995a). td models: modeling the world at a mixture of time scales.

in a. prieditis and
s. russell (eds.), proceedings of the twelfth international conference on machine learning, pp. 531   539.
morgan kaufmann, san francisco.

sutton, r. s. (1995b). on the virtues of linear learning and trajectory distributions. proceedings of the workshop

on value function approximation at the international conference on machine learning.

sutton, r. s. (1996). generalization in id23: successful examples using sparse coarse coding.
in d. s. touretzky, m. c. mozer and m. e. hasselmo (eds.), advances in neural information processing
systems: proceedings of the 1995 conference, pp. 1038   1044. mit press, cambridge, ma.

sutton, r. s. (2009). the grand challenge of predictive empirical abstract knowledge. working notes of the

ijcai-09 workshop on grand challenges for reasoning from experiences.

sutton, r. s. (2015a) introduction to id23 with function approximation. tutorial at the

conference on neural information processing systems, montreal, december 7, 2015.

sutton, r. s. (2015b) true online emphatic td(  ): quick reference and implementation guide. arxiv:1507.07147.

422

references

code is available in python and c++ by downloading the source    les of this arxiv paper as a zip archive.

sutton, r. s., barto, a. g. (1981a). toward a modern theory of adaptive networks: expectation and prediction.

psychological review, 88:135   170.

sutton, r. s., barto, a. g. (1981b). an adaptive network that constructs and uses an internal model of its

world. cognition and brain theory, 3:217   246.

sutton, r. s., barto, a. g. (1987). a temporal-di   erence model of classical conditioning. in proceedings of the

ninth annual conference of the cognitive science society, pp. 355-378. erlbaum, hillsdale, nj.

sutton, r. s., barto, a. g. (1990). time-derivative models of pavlovian reinforcement.

in m. gabriel and
j. moore (eds.), learning and computational neuroscience: foundations of adaptive networks, pp. 497   537.
mit press, cambridge, ma.

sutton, r. s., maei, h. r., precup, d., bhatnagar, s., silver, d., szepesv  ari, cs., and wiewiora, e. (2009). fast
gradient-descent methods for temporal-di   erence learning with linear function approximation. in proceedings
of the 26th annual international conference on machine learning, pp. 993   1000. acm.

sutton, r. s., maei, h. r., and szepesv  ari, cs. (2009). a convergent o(d2) temporal-di   erence algorithm
for o   -policy learning with linear function approximation. in advances in neural information processing
systems, pp. 1609   1616.

sutton, r. s., mahmood, a. r., precup, d., van hasselt, h. (2014). a new q(  ) with interim forward view

and monte carlo equivalence. international conference on machine learning 31. jmlr w&cp 32 (2).

sutton, r. s., mahmood, a. r., white, m. (2016). an emphatic approach to the problem of o   -policy temporal-

di   erence learning. journal of machine learning research 17 (73):1-29.

sutton, r. s., mcallester, d. a., singh, s. p., mansour, y. (2000). id189 for reinforcement
learning with function approximation. in advances in neural information processing systems 99, pp. 1057   
1063.

sutton, r. s., modayil, j., delp, m., degris, t., pilarski, p. m., white, a., precup, d. (2011). horde: a scalable
real-time architecture for learning knowledge from unsupervised sensorimotor interaction. in proceedings of
the tenth international conference on autonomous agents and multiagent systems, pp. 761   768, taipei,
taiwan.

sutton, r. s., pinette, b. (1985). the learning of world models by connectionist networks. in proceedings of

the seventh annual conference of the cognitive science society, pp. 54   64.

sutton, r. s., precup, d., and singh, s. (1999). between mdps and semi-mdps: a framework for temporal

abstraction in id23. arti   cial intelligence 112 (1   2), 181   211.

sutton, r. s., singh, s. (1994). on bias and step size in temporal-di   erence learning.

in proceedings of the
eighth yale workshop on adaptive and learning systems, pp. 91   96. center for systems science, dunham
laboratory, yale university, new haven.

sutton, r. s., singh, s. p., and mcallester, d. a. (2000). comparing policy-gradient algorithms.

sutton, r. s., whitehead, d. s. (1993). online learning with random representations.

in proceedings of the

tenth international machine learning conference, pp. 314   321. morgan kaufmann, san mateo, ca.

szepesv  ari, c. (2010). algorithms for id23. synthesis lectures on arti   cial intelligence and

machine learning 4(1), 1   103.

szita, i. (2012). id23 in games. in id23 (pp. 539-577). springer berlin

heidelberg.

tadepalli, p., ok, d. (1994). h-learning: a id23 method to optimize undiscounted average

reward. technical report 94-30-01. oregon state university, computer science department, corvallis.

tadepalli, p., and ok, d. (1996). scaling up average reward id23 by approximating the

domain models and the value function. in international conference on machine learning, pp. 471   479.

takahashi, y., schoenbaum, g., and niv, y. (2008). silencing the critics: understanding the e   ects of cocaine
sensitization on dorsolateral and ventral striatum in the context of an actor/critic model. frontiers in
neuroscience, 2(1):86   99.

tambe, m., newell, a., and rosenbloom, p. s. (1990). the problem of expensive chunks and its solution by

restricting expressiveness. machine learning 5 (3), 299   348.

references

423

tan, m. (1991). learning a cost-sensitive internal representation for id23. in l. a. birnbaum
and g. c. collins (eds.), proceedings of the eighth international workshop on machine learning, pp. 358   
362. morgan kaufmann, san mateo, ca.

tan, m. (1993). multi-agent id23: independent vs. cooperative agents. in proceedings of the

tenth international conference on machine learning, pp. 330   337. morgan kaufmann, san mateo, ca.

taylor, g., and parr, r. (2009). kernelized value function approximation for id23.
proceedings of the 26th annual international conference on machine learning, pp. 1017   1024. acm.

in

taylor, m. e., and stone, p. (2009). id21 for id23 domains: a survey. journal

of machine learning research 10 :1633   1685.

tesauro, g. j. (1986). simple neural models of classical conditioning. biological cybernetics, 55:187   200.

tesauro, g. j. (1992). practical issues in temporal di   erence learning. machine learning, 8:257   277.

tesauro, g. j. (1994). td-gammon, a self-teaching backgammon program, achieves master-level play. neural

computation, 6(2):215   219.

tesauro, g. j. (1995). temporal di   erence learning and td-gammon. communications of the acm, 38:58   68.

tesauro, g. (2002).

programming backgammon using self-teaching neural nets.

arti   cial intelligence,

134(1):181   199.

tesauro, g. j., galperin, g. r. (1997). on-line policy improvement using monte-carlo search.

in advances
in neural information processing systems: proceedings of the 1996 conference, pp. 1068   1074. mit press,
cambridge, ma.

tesauro, g., gondek, d. c., lechner, j., fan, j., and prager, j. m. (2012). simulation, learning, and optimiza-
tion techniques in watson   s game strategies. ibm journal of research and development, 56(3.4):16   1   16   11.
tesauro, g., gondek, d. c., lenchner, j., fan, j., and prager, j. m. (2013). analysis of watson   s strategies

for playing jeopardy! journal of arti   cial intelligence research, 21:205   251.

tham, c. k. (1994). modular on-line function approximation for scaling up id23. phd

thesis, cambridge university.

thathachar, m. a. l. and sastry, p. s. (1985). a new approach to the design of reinforcement schemes for

learning automata. ieee transactions on systems, man, and cybernetics, 15:168   175.

thathachar, m. and sastry, p. s. (2002). varieties of learning automata: an overview. ieee transactions on

systems, man, and cybernetics, part b: cybernetics, 36(6):711   722.

thathachar, m. and sastry, p. s. (2011). networks of learning automata: techniques for online stochastic

optimization. springer science & business media.

theocharous, g., thomas, p. s., and ghavamzadeh, m. (2015). personalized ad recommendation for life-time
in proceedings of the twenty-fourth international joint conference on

value optimization guarantees.
arti   cial intelligence (ijcai-15).

thistlethwaite, d. (1951). a critical review of latent learning and related experiments. psychological bulletin,

48(2):97   129.

thomas, p. (2014). bias in natural actor   critic algorithms. international conference on machine learning 31.

jmlr w&cp 32 (1):441   448.

thomas, p. s. (2015). safe id23. phd thesis, university of massachusetts amherst.

thomas, p. s., theocharous, g., and ghavamzadeh, m. (2015). high-con   dence o   -policy evaluation.

in
proceedings of the twenty-ninth aaai conference on arti   cial intelligence, pages 3000   3006. the aaai
press, palo alto, ca.

thompson, w. r. (1933). on the likelihood that one unknown id203 exceeds another in view of the

evidence of two samples. biometrika, 25:285   294.

thompson, w. r. (1934). on the theory of apportionment. american journal of mathematics, 57:450   457.

thon, m., and jaeger, h. (2015). links between multiplicity automata, observable operator models and predic-
tive state representations: a uni   ed learning framework. the journal of machine learning research 16 (1),
103   147.

thorndike, e. l. (1898). animal intelligence: an experimental study of the associative processes in animals.

424

references

the psychological review, series of monograph supplements, ii(4).

thorndike, e. l. (1911). animal intelligence. hafner, darien, ct.

thorp, e. o. (1966). beat the dealer: a winning strategy for the game of twenty-one. random house, new

york.

tian, t. (in preparation) an empirical study of sliding-step methods in temporal di   erence learning. uni-

versity of alberta msc thesis.

tieleman, t. and hinton, g. (2012). lecture 6.5-rmsprop. coursera: neural networks for machine learning.

tobler, p. n., fiorillo, c. d., and schultz, w. (2005). adaptive coding of reward value by dopamine neurons.

science, 307(5715):1642   1645.

tolman, e. c. (1932). purposive behavior in animals and men. century, new york.

tolman, e. c. (1948). cognitive maps in rats and men. psychological review, 55(4):189   208.

tsai, h.-s., zhang, f., adamantidis, a., stuber, g. d., bonci, a., de lecea, l., and deisseroth, k. (2009).
phasic    ring in dopaminergic neurons is su   cient for behavioral conditioning. science, 324(5930):1080   1084.

tsetlin, m. l. (1973). automaton theory and modeling of biological systems. academic press, new york.

tsitsiklis, j. n. (1994). asynchronous stochastic approximation and id24. machine learning, 16:185   202.

tsitsiklis, j. n. (2002). on the convergence of optimistic policy iteration.

journal of machine learning

research, 3:59   72.

tsitsiklis, j. n. and van roy, b. (1996). feature-based methods for large scale id145. machine

learning, 22:59   94.

tsitsiklis, j. n., van roy, b. (1997). an analysis of temporal-di   erence learning with function approximation.

ieee transactions on automatic control, 42:674   690.

tsitsiklis, j. n., van roy, b. (1999). average cost temporal-di   erence learning. automatica, 35:1799   1808.

turing, a. m. (1950). computing machinery and intelligence. mind 433   460.

turing, a. m. (1948). intelligent machinery, a heretical theory. the turing test: verbal behavior as the

hallmark of intelligence, 105.

ungar, l. h. (1990). a bioreactor benchmark for adaptive network-based process control.

in w. t. miller,
r. s. sutton, and p. j. werbos (eds.), neural networks for control, pp. 387   402. mit press, cambridge,
ma.

urbanczik, r. and senn, w. (2009). id23 in populations of spiking neurons. nature

neuroscience, 12(3):250   252.

urbanowicz, r. j., moore, j. h. (2009). learning classi   er systems: a complete introduction, review, and

roadmap. journal of arti   cial evolution and applications.

valentin, v. v., dickinson, a., and o   doherty, j. p. (2007). determining the neural substrates of goal-directed

learning in the human brain. the journal of neuroscience, 27(15):4019   4026.

van hasselt, h. (2010). double id24. in advances in neural information processing systems, pp. 2613   

2621.

van hasselt, h. (2011).

insights in id23: formal analysis and empircal evaluation of

temporal-di   erence learning. siks dissertation series number 2011-04.

van hasselt, h. (2012). id23 in continuous state and action spaces.

in wiering and van

otterlo (eds.) id23: state-of-the art, pp. 207   251. springer berlin heidelberg.

van hasselt, h., and sutton, r. s. (2015). learning to predict independent of span. arxiv 1508.04582.

van otterlo, m. (2009). the logic of adaptive behavior. ios press.

van otterlo, m. (2012). solving relational and    rst-order logical id100: a survey.

in
wiering and van otterlo (eds.) id23: state-of-the art, pp. 253   292. springer berlin
heidelberg.

van roy, b., bertsekas, d. p., lee, y., tsitsiklis, j. n. (1997). a neuro-id145 approach to
retailer inventory management. in proceedings of the 36th ieee conference on decision and control, vol. 4,
pp. 4052   4057.

references

425

van seijen, h. (2016). e   ective multi-step temporal-di   erence learning for non-linear function approximation.

arxiv preprint arxiv:1608.05151.

van seijen, h., and sutton, r. s. (2014). true online td(  ). in proceedings of the 31st international conference

on machine learning. jmlr w&cp 32(1):692   700.

van seijen, h., mahmood, a. r., pilarski, p. m., machado, m. c., and sutton, r. s. (2016). true online

temporal-di   erence learning. journal of machine learning research 17 (145), 1   40.

van seijen, h., van hasselt, h., whiteson, s., wiering, m. (2009). a theoretical and empirical analysis of
in ieee symposium on adaptive id145 and id23,

expected sarsa.
pp. 177   184.

varga, r. s. (1962). matrix iterative analysis. englewood cli   s, nj: prentice-hall.

vasilaki, e., fr  emaux, n., urbanczik, r., senn, w., and gerstner, w. (2009).

spike-based reinforcement
learning in continuous state and action space: when id189 fail. plos computational
biology, 5(12).

viswanathan, r. and narendra, k. s. (1974). games of stochastic automata. ieee transactions on systems,

man, and cybernetics, 4:131   135.

vlassis, n., ghavamzadeh, m., mannor, s., and poupart, p. (2012). bayesian id23. in wiering
and van otterlo (eds.) id23: state-of-the art, pp. 359   386. springer berlin heidelberg.

walter, w. g. (1950). an imitation of life. scienti   c american, pages 42   45.

walter, w. g. (1951). a machine that learns. scienti   c american, 185(2):60   63.

waltz, m. d., fu, k. s. (1965). a heuristic approach to id23 control systems.

ieee

transactions on automatic control, 10:390   398.

watkins, c. j. c. h. (1989). learning from delayed rewards. ph.d. thesis, cambridge university.

watkins, c. j. c. h., dayan, p. (1992). id24. machine learning, 8:279   292.

wiering, m., van otterlo, m. (2012). id23. springer berlin heidelberg.

werbos, p. (1974). beyond regression: new tools for prediction and analysis in the behavioral sciences. phd

thesis, harvard university, cambridge, massachusetts.

werbos, p. j. (1977). advanced forecasting methods for global crisis warning and models of intelligence. general

systems yearbook, 22:25   38.

werbos, p. j. (1982). applications of advances in nonlinear sensitivity analysis. in r. f. drenick and f. kozin

(eds.), system modeling and optimization, pp. 762   770. springer-verlag, berlin.

werbos, p. j. (1987). building and understanding adaptive systems: a statistical/numerical approach to factory

automation and brain research. ieee transactions on systems, man, and cybernetics, 17:7   20.

werbos, p. j. (1988). generalization of back propagation with applications to a recurrent gas market model.

neural networks, 1:339   356.

werbos, p. j. (1989). neural networks for control and system identi   cation.

in proceedings of the 28th

conference on decision and control, pp. 260   265. ieee control systems society.

werbos, p. j. (1990). consistency of hdp applied to a simple id23 problem. neural networks,

3:179   189.

werbos, p. j. (1992). approximate id145 for real-time control and neural modeling.

in
d. a. white and d. a. sofge (eds.), handbook of intelligent control: neural, fuzzy, and adaptive ap-
proaches, pp. 493   525. van nostrand reinhold, new york.

werbos, p. j. (1994). the roots of id26: from ordered derivatives to neural networks and political

forecasting (vol. 1). john wiley and sons.

white, a. (2015). developing a predictive approach to knowledge. phd thesis, university of alberta.

white, d. j. (1969). id145. holden-day, san francisco.

white, d. j. (1985). real applications of id100. interfaces, 15:73   83.

white, d. j. (1988). further real applications of id100. interfaces, 18:55   61.

white, d. j. (1993). a survey of applications of id100. journal of the operational research

426

society, 44:1073   1096.

references

white, a., and white, m. (2016). investigating practical linear temporal di   erence learning. in proceedings of

the 2016 international conference on autonomous agents and multiagent systems, pp. 494   502.

whitehead, s. d., ballard, d. h. (1991). learning to perceive and act by trial and error. machine learning,

7:45   83.

whitt, w. (1978). approximations of dynamic programs i. mathematics of operations research, 3:231   243.

whittle, p. (1982). optimization over time, vol. 1. wiley, new york.

whittle, p. (1983). optimization over time, vol. 2. wiley, new york.

wickens, j. and k  otter, r. (1995). cellular models of reinforcement.

in houk, j. c., davis, j. l., and
beiser, d. g., editors, models of information processing in the basal ganglia, pages 187   214. mit press,
cambridge, ma.

widrow, b., gupta, n. k., maitra, s. (1973). punish/reward: learning with a critic in adaptive threshold

systems. ieee transactions on systems, man, and cybernetics, 3:455   465.

widrow, b., ho   , m. e. (1960). adaptive switching circuits.

in 1960 wescon convention record part
iv, pp. 96   104. institute of radio engineers, new york. reprinted in j. a. anderson and e. rosenfeld,
neurocomputing: foundations of research, pp. 126   134. mit press, cambridge, ma, 1988.

widrow, b., smith, f. w. (1964). pattern-recognizing control systems. in j. t. tou and r. h. wilcox (eds.),

computer and information sciences, pp. 288   317. spartan, washington, dc.

widrow, b., stearns, s. d. (1985). adaptive signal processing. prentice-hall, englewood cli   s, nj.

wiewiora, e. (2003). potential-based shaping and q-value initialization are equivalent. journal of arti   cial

intelligence research 19 :205   208.

williams, r. j. (1986). id23 in connectionist networks: a mathematical analysis. technical

report ics 8605. institute for cognitive science, university of california at san diego, la jolla.

williams, r. j. (1987). reinforcement-learning connectionist systems. technical report nu-ccs-87-3. college

of computer science, northeastern university, boston.

williams, r. j. (1988). on the use of id26 in associative id23. in proceedings of
the ieee international conference on neural networks, pp. i263   i270. ieee san diego section and ieee
tab neural network committee.

williams, r. j. (1992). simple statistical gradient-following algorithms for connectionist id23.

machine learning, 8:229   256.

williams, r. j., baird, l. c. (1990). a mathematical analysis of actor   critic architectures for learning optimal
controls through incremental id145. in proceedings of the sixth yale workshop on adaptive
and learning systems, pp. 96   101. center for systems science, dunham laboratory, yale university, new
haven.

wilson, r. c., takahashi, y. k., schoenbaum, g., and niv, y. (2014). orbitofrontal cortex as a cognitive map

of task space. neuron, 81(2):267   279.

wilson, s. w. (1994). zcs: a zeroth order classi   er system. evolutionary computation, 2:1   18.

wise, r. a. (2004). dopamine, learning, and motivation. nature reviews neuroscience, 5(6):1   12.

witten, i. h. (1976). the apparent con   ict between estimation and control   a survey of the two-armed

problem. journal of the franklin institute, 301:161   189.

witten, i. h. (1977). an adaptive optimal controller for discrete-time markov environments. information and

control, 34:286   295.

witten, i. h., corbin, m. j. (1973). human operators and automatic adaptive controllers: a comparative study

on a particular control task. international journal of man   machine studies, 5:75   104.

woodbury, t., dunn, c., and valasek, j. (2014). autonomous soaring using id23 for trajec-

tory generation. in 52nd aerospace sciences meeting, page 0990.

woodworth, r. s., schlosberg, h. (1938). experimental psychology. new york: henry holt and company.

xie, x. and seung, h. s. (2004). learning in neural networks by reinforcement of irregular spiking. physical

references

review e, 69(4).

427

xu, x., xie, t., hu, d., and lu, x. (2005). kernel least-squares temporal di   erence learning. international

journal of information technology 11 (9):54   63.

yagishita, s., hayashi-takagi, a., ellis-davies, g. c. r., urakubo, h., ishii, s., and kasai, h. (2014). a critical
time window for dopamine actions on the structural plasticity of dendritic spines. science, 345(6204):1616   
1619.

yee, r. c., saxena, s., utgo   , p. e., barto, a. g. (1990). explaining temporal di   erences to create useful
concepts for evaluating states. in proceedings of the eighth national conference on arti   cial intelligence,
pp. 882   888. aaai press, menlo park, ca.

yin, h. h. and knowlton, b. j. (2006). the role of the basal ganglia in habit formation. nature reviews

neuroscience, 7(6):464   476.

young, p. (1984). recursive estimation and time-series analysis. springer-verlag, berlin.

yu, h. (2010). convergence of least squares temporal di   erence methods under general conditions. international

conference on machine learning 27, pp. 1207   1214.

yu, h. (2012). least squares temporal di   erence methods: an analysis under general conditions. siam journal

on control and optimization, 50(6), 3310   3343.

yu, h. (2015a). on convergence of emphatic temporal-di   erence learning. arxiv:1506.02582. a shorter version

appeared in conference on learning theory 18, jmlr w&cp 40.

yu, h. (2015b). weak convergence properties of constrained emphatic temporal-di   erence learning with constant

and slowly diminishing stepsize. arxiv:1511.07471.

zhang, m., yum, t. p. (1989). comparisons of channel-assignment strategies in cellular mobile telephone

systems. ieee transactions on vehicular technology, 38:211-215.

zhang, w. (1996). id23 for job-shop scheduling. ph.d. thesis, oregon state university.

technical report cs-96-30-1.

zhang, w., dietterich, t. g. (1995). a id23 approach to job-shop scheduling.

in pro-
ceedings of the fourteenth international joint conference on arti   cial intelligence, pp. 1114   1120. morgan
kaufmann.

zhang, w., dietterich, t. g. (1996). high-performance job-shop scheduling with a time   delay td(  ) network.
in d. s. touretzky, m. c. mozer, m. e. hasselmo (eds.), advances in neural information processing systems:
proceedings of the 1995 conference, pp. 1024   1030. mit press, cambridge, ma.

zweben, m., daun, b., deale, m. (1994). scheduling and rescheduling with iterative repair. in m. zweben and

m. s. fox (eds.), intelligent scheduling, pp. 241   255. morgan kaufmann, san francisco.

