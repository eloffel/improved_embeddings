constructing and evaluating

id27s

dr marek rei and dr ekaterina kochmar

computer laboratory
university of cambridge

representing words as vectors

let   s represent words (or any objects) as vectors. 
we want to construct them so that similar words have similar vectors.

sequence
i live in cambridge

i live in paris

i live in tallinn

i live in yellow

representing words as vectors

let   s represent words (or any objects) as vectors. 
we want to construct them so that similar words have similar vectors.

sequence
i live in cambridge

i live in paris

i live in tallinn

i live in yellow

count

19

68

0

0

representing words as vectors

let   s represent words (or any objects) as vectors. 
we want to construct them so that similar words have similar vectors.

sequence
i live in cambridge

i live in paris

i live in tallinn

i live in yellow

count

19

68

0

0

1-hot vectors

how can we represent words as vectors?

option 1: each element represents a different word. 

also known as    1-hot    or    1-of-v    representation.

bear

cat

frog

bear

1

0

0

cat

0

1

0

frog

0

0

1

         bear=[1.0, 0.0, 0.0]          cat=[0.0, 1.0, 0.0]  

1-hot vectors

when using 1-hot vectors, we can   t fit many and they tell us very little.

need a separate dimension for every word we want to represent.

distributed vectors

option 2: each element represents a property, and they are shared between the 
words.

also known as    distributed    representation.

bear

cat

frog

furry

0.9

0.85

0

dangerous

mammal

0.85

0.15

0.05

1

1

0

                 bear = [0.9, 0.85, 1.0]    cat = [0.85, 0.15, 1.0]

distributed vectors

furry

dangerous

0.9

0.85

0.0

0.85

0.8

0.85

0.15

0.8

0.9

0.15

bear
cat
cobra
lion
dog

distributed vectors group similar words/objects together

distributed vectors

cos(lion, bear) = 0.998

cos(lion, dog) = 0.809

cos(cobra, dog) = 0.727

can use cosine to calculate similarity between two words

distributed vectors

we can infer some information, based only on the vector of the word

we don   t even need to know the labels on the vector elements

distributional hypothesis

words which are similar in meaning occur in similar contexts.

(harris, 1954)

you shall know a word by the company it keeps 

(firth, 1957)

he is reading a magazine

i was reading a newspaper

this magazine published my story

the newspaper published an article

she buys a magazine every month

he buys this newspaper every day

count-based vectors

one way of creating a vector for a word:
let   s count how often a word occurs together with specific other words.

he is reading a magazine

i was reading a newspaper

this magazine published my story

the newspaper published an article

she buys a magazine every month

he buys this newspaper every day

reading

magazine

newspaper

1

1

a

2

1

this

published my buys

the

an

every month

day

1

1

1

1

1

0

1

1

0

1

0

1

1

1

1

0

0

1

count-based vectors

    more frequent words dominate the vectors.

can use a weighting scheme like pmi or tf-idf.

    large number of sparse features

can use matrix decomposition like singular value decomposition (svd) or 
id44 (lda).

neural id27s

neural networks will automatically try to discover useful features in the data, given 
a specific task.

idea: let   s allocate a number of parameters for each word and allow the neural 
network to automatically learn what the useful values should be.

often referred to as    id27s   , as we are embedding the words into a 
real-valued low-dimensional space.

embeddings through language modelling

predict the next word in a sequence, 
based on the previous words.

use this to guide the training for 
id27s.

bengio et. al. 2003. a neural 
probabilistic language model.

i read at my         desk

i study at my        desk

embeddings through error detection

take a grammatically correct 
sentence and create a corrupted 
counterpart.

train the neural network to assign 
a higher score to the correct 
version of each sentence.

collobert et. al. 2011. natural 
language processing (almost) 
from scratch.

my cat climbed a tree

my cat bridge  a tree

embedding matrix

two ways of thinking about the embedding matrix.

1. each row contains a id27, which we need to extract

2.

it is a normal weight matrix, working with a 1-hot input vector

id97

a popular tool for creating id27s.

available from: https://code.google.com/archive/p/id97/

can also download embeddings that are pretrained on 100 billion words.

preprocess the data!

    tokenise
    lowercase (usually)

./id97 -train input.txt -output vectors.txt -cbow 0 -size 100 
-window 5 -negative 5 -hs 0 -sample 1e-3 -threads 8

continuous bag-of-words (cbow) model

predict the current word, based 
on the surrounding words

mikolov et. al. 2013. efficient 
estimation of word 
representations in vector space.

skip-gram model

predict the surrounding words, 
based on the current word.

mikolov et. al. 2013. efficient 
estimation of word 
representations in vector space.

word similarity

collobert et. al. 2011. natural language processing (almost) from scratch.

word similarity

joseph turian

word similarity

joseph turian

word similarity

joseph turian

analogy recovery

the task of analogy recovery. questions in the form:

a is to b as c is to d

the system is given words a, b, c, and it needs to find d. for example:

   apple    is to    apples    as    car    is to ?

or

   man    is to    woman    as    king    is to ?

mikolov et. al. 2013. efficient estimation of word representations in vector 
space.

analogy recovery

task: a is to b as c is to d

idea: the direction of the relation 
should remain the same.

analogy recovery

task: a is to b as c is to d

idea: the offset of vectors should 
reflect their relation.

analogy recovery

example output using id97 vectors.

id27s in practice

id97 is often used for pretraining.

    it will help your models start from an informed position
    requires only plain text - which we have a lot
    is very fast and easy to use
    already pretrained vectors also available (trained on 100b words)

however, for best performance it is important to continue training (fine-tuning).

raw id97 vectors are good for predicting the surrounding words, but not 
necessarily for your specific task.

simply treat the embeddings the same as other parameters in your model and 
keep updating them during training.

problems with id27s

id27s allow us to learn similar representations for semantically or 
functionally similar words.

but

1.

if a token has not been seen during training, we have to use a generic oov 
(out-of-vocabulary) token to represent it.

2.

infrequent words have very low-quality embeddings, due to lack of data.

3. morphological and character-level information is ignored when treating words 

as atomic units.

character-based representations

we can augment id27s by learning character-based representations. 

rei et al. (2016)

multimodal embeddings
we can map text and images into the same space

kiros et al. (2014, 2015)

conclusion
id27s are the building blocks for higher-level models

questions?

